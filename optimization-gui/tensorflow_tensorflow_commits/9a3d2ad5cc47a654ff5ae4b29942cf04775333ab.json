{
    "author": "nhatleSummer22",
    "message": "Merge branch 'tensorflow:master' into nhatle/fix_fused_bn_bug_tf_serving",
    "sha": "9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
    "files": [
        {
            "sha": "32898953f8973e5d227361ed72e361ae92cf9d1e",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/ifrt/ifrt_backend_compiler_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Fifrt_backend_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Fifrt_backend_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Fifrt_backend_compiler_test.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -111,7 +111,6 @@ class IfrtBackendCompilerTest : public ::testing::Test {\n \n namespace {\n using ::testing::HasSubstr;\n-using ::tsl::testing::StatusIs;\n \n struct IfrtBackendCompilerTestParams {\n   std::string mlir_file_name;"
        },
        {
            "sha": "7d535bcd5cbe3658df2400a19db09352a31b619e",
            "filename": "tensorflow/core/public/version.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/tensorflow%2Fcore%2Fpublic%2Fversion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/tensorflow%2Fcore%2Fpublic%2Fversion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fpublic%2Fversion.h?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -93,7 +93,7 @@ limitations under the License.\n \n #define TF_GRAPH_DEF_VERSION_MIN_PRODUCER 0\n #define TF_GRAPH_DEF_VERSION_MIN_CONSUMER 0\n-#define TF_GRAPH_DEF_VERSION 2340  // Updated: 2025/9/4\n+#define TF_GRAPH_DEF_VERSION 2341  // Updated: 2025/9/5\n \n // Checkpoint compatibility versions (the versions field in SavedSliceMeta).\n //"
        },
        {
            "sha": "c8dd29135e96cabfc69a5fd1289928f197c9f0cf",
            "filename": "tensorflow/dtensor/mlir/layout_propagation_v2.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/tensorflow%2Fdtensor%2Fmlir%2Flayout_propagation_v2.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/tensorflow%2Fdtensor%2Fmlir%2Flayout_propagation_v2.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Flayout_propagation_v2.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -1022,7 +1022,7 @@ class LayoutPrinter : public mlir::OpAsmPrinter {\n     os_ << symbolRef;\n   };\n \n-  void printNamedAttribute(mlir::NamedAttribute attr) {\n+  void printNamedAttribute(mlir::NamedAttribute attr) override {\n     os_ << attr.getName().strref() << \" = \";\n     printAttribute(attr.getValue());\n   }"
        },
        {
            "sha": "62b2557290f7805e6d7c9fe1a31f29dbed690b6c",
            "filename": "tensorflow/python/_pywrap_tensorflow.def",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/tensorflow%2Fpython%2F_pywrap_tensorflow.def",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/tensorflow%2Fpython%2F_pywrap_tensorflow.def",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2F_pywrap_tensorflow.def?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -217,7 +217,6 @@ EXPORTS\n   ?Build@KernelDefBuilder@tensorflow@@QEAAPEBVKernelDef@2@XZ\n   ?Canonicalize@FunctionParameterCanonicalizer@tensorflow@@QEAA_NPEAU_object@@0V?$Span@PEAU_object@@@lts_20250512@absl@@@Z\n   ?Capture@StackTrace@tensorflow@@SA?AV?$shared_ptr@VStackTrace@tensorflow@@@std@@H@Z\n-  ?CatPieces@internal@strings@tsl@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$initializer_list@V?$basic_string_view@DU?$char_traits@D@std@@@std@@@5@@Z\n   ?CatPieces@strings_internal@lts_20250512@absl@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$initializer_list@V?$basic_string_view@DU?$char_traits@D@std@@@std@@@5@@Z\n   ?Check@PyListChecker@py_dispatch@tensorflow@@UEAA?AW4MatchType@PyTypeChecker@23@PEAU_object@@@Z\n   ?Check@PyUnionChecker@py_dispatch@tensorflow@@UEAA?AW4MatchType@PyTypeChecker@23@PEAU_object@@@Z"
        },
        {
            "sha": "8a5005ba8acfed6135fd80117bb13fe5a9cfc959",
            "filename": "tensorflow/python/compat/compat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/tensorflow%2Fpython%2Fcompat%2Fcompat.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/tensorflow%2Fpython%2Fcompat%2Fcompat.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Fcompat%2Fcompat.py?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -29,7 +29,7 @@\n # This value changes every day with an automatic CL. It can be modified in code\n # via `forward_compatibility_horizon()` or with the environment variable\n # TF_FORWARD_COMPATIBILITY_DELTA_DAYS, which is added to the compatibility date.\n-_FORWARD_COMPATIBILITY_HORIZON = datetime.date(2025, 9, 4)\n+_FORWARD_COMPATIBILITY_HORIZON = datetime.date(2025, 9, 5)\n _FORWARD_COMPATIBILITY_DELTA_DAYS_VAR_NAME = \"TF_FORWARD_COMPATIBILITY_DELTA_DAYS\"\n _FORWARD_COMPATIBILITY_DATE_NUMBER = None\n "
        },
        {
            "sha": "af40ce5521bfa875712d68388003392f7a4be405",
            "filename": "third_party/xla/build_tools/ci/build.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fbuild_tools%2Fci%2Fbuild.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fbuild_tools%2Fci%2Fbuild.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fci%2Fbuild.py?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -52,7 +52,10 @@\n     \"//build_tools/...\",\n     \"@local_tsl//tsl/...\",\n )\n-_XLA_ONEAPI_TARGET_PATTERNS = (\"//xla/stream_executor/sycl/...\",)\n+_XLA_ONEAPI_TARGET_PATTERNS = (\n+    \"//xla/stream_executor/sycl/...\",\n+    \"//xla/service/gpu/...\",\n+)\n _XLA_CPU_PRESUBMIT_BENCHMARKS_DEFAULT_TARGET_PATTERNS = (\n     \"//xla/tools/multihost_hlo_runner:hlo_runner_main\",\n     \"//xla/tools:compute_xspace_stats_main\","
        },
        {
            "sha": "ae305ed6b551ce114eebf1e48ffb808317b329a8",
            "filename": "third_party/xla/build_tools/ci/golden_commands.txt",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fbuild_tools%2Fci%2Fgolden_commands.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fbuild_tools%2Fci%2Fgolden_commands.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fci%2Fgolden_commands.txt?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -65,8 +65,8 @@ bazel test --build_tag_filters=-no_oss,requires-gpu-nvidia,gpu,-rocm-only,-oneap\n bazel analyze-profile profile.json.gz\n # END BuildType.XLA_LINUX_X86_GPU_L4_GITHUB_ACTIONS\n # BEGIN BuildType.XLA_LINUX_X86_GPU_ONEAPI_GITHUB_ACTIONS\n-parallel --ungroup --retries 3 --delay 15 --nonall -- bazel build --build_tag_filters=oneapi-only,requires-gpu-intel,-requires-gpu-amd,-requires-gpu-nvidia,-no_oss,-cuda-only,-rocm-only,-no-oneapi --test_tag_filters=oneapi-only,-requires-gpu-intel,-requires-gpu-amd,-requires-gpu-nvidia,-no_oss,-cuda-only,-rocm-only,-no-oneapi --config=sycl --config=sycl_hermetic --config=icpx_clang --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --//xla/tsl:ci_build --nobuild -- //xla/stream_executor/sycl/...\n-bazel build --build_tag_filters=oneapi-only,requires-gpu-intel,-requires-gpu-amd,-requires-gpu-nvidia,-no_oss,-cuda-only,-rocm-only,-no-oneapi --test_tag_filters=oneapi-only,-requires-gpu-intel,-requires-gpu-amd,-requires-gpu-nvidia,-no_oss,-cuda-only,-rocm-only,-no-oneapi --config=sycl --config=sycl_hermetic --config=icpx_clang --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --//xla/tsl:ci_build -- //xla/stream_executor/sycl/...\n+parallel --ungroup --retries 3 --delay 15 --nonall -- bazel build --build_tag_filters=oneapi-only,requires-gpu-intel,-requires-gpu-amd,-requires-gpu-nvidia,-no_oss,-cuda-only,-rocm-only,-no-oneapi --test_tag_filters=oneapi-only,-requires-gpu-intel,-requires-gpu-amd,-requires-gpu-nvidia,-no_oss,-cuda-only,-rocm-only,-no-oneapi --config=sycl --config=sycl_hermetic --config=icpx_clang --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --//xla/tsl:ci_build --nobuild -- //xla/stream_executor/sycl/... //xla/service/gpu/...\n+bazel build --build_tag_filters=oneapi-only,requires-gpu-intel,-requires-gpu-amd,-requires-gpu-nvidia,-no_oss,-cuda-only,-rocm-only,-no-oneapi --test_tag_filters=oneapi-only,-requires-gpu-intel,-requires-gpu-amd,-requires-gpu-nvidia,-no_oss,-cuda-only,-rocm-only,-no-oneapi --config=sycl --config=sycl_hermetic --config=icpx_clang --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --//xla/tsl:ci_build -- //xla/stream_executor/sycl/... //xla/service/gpu/...\n bazel analyze-profile profile.json.gz\n # END BuildType.XLA_LINUX_X86_GPU_ONEAPI_GITHUB_ACTIONS\n # BEGIN BuildType.XLA_MACOS_ARM64_CPU_KOKORO"
        },
        {
            "sha": "2511d1ac41acdf12238217b2858917ad492753b1",
            "filename": "third_party/xla/third_party/tsl/tsl/platform/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2FBUILD?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -458,7 +458,6 @@ filegroup(\n         \"statusor.h\",\n         \"str_util.cc\",\n         \"str_util.h\",\n-        \"strcat.cc\",\n         \"strcat.h\",\n         \"stringpiece.h\",\n         \"stringprintf.cc\",\n@@ -774,7 +773,6 @@ cc_library(\n \n cc_library(\n     name = \"strcat\",\n-    srcs = [\"strcat.cc\"],\n     hdrs = [\"strcat.h\"],\n     deps = [\n         \":numbers\",\n@@ -1368,23 +1366,6 @@ tsl_cc_test(\n     ],\n )\n \n-tsl_cc_test(\n-    name = \"strcat_test\",\n-    size = \"small\",\n-    srcs = [\n-        \"strcat_test.cc\",\n-    ],\n-    deps = [\n-        \":bfloat16\",\n-        \":strcat\",\n-        \":stringprintf\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@local_xla//xla/tsl/platform:test\",\n-        \"@local_xla//xla/tsl/platform:types\",\n-    ],\n-)\n-\n tsl_cc_test(\n     name = \"stringpiece_test\",\n     size = \"small\","
        },
        {
            "sha": "24f533bed5a173877374b9ab6f2b40a9b4486598",
            "filename": "third_party/xla/third_party/tsl/tsl/platform/strcat.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 267,
            "changes": 267,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/682659f1d20f53d82008992cc183a09a70ee2a66/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstrcat.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/682659f1d20f53d82008992cc183a09a70ee2a66/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstrcat.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstrcat.cc?ref=682659f1d20f53d82008992cc183a09a70ee2a66",
            "patch": "@@ -1,267 +0,0 @@\n-/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"tsl/platform/strcat.h\"\n-\n-#include <stdarg.h>\n-#include <stdint.h>\n-#include <stdio.h>\n-#include <string.h>\n-\n-#include <algorithm>\n-#include <initializer_list>\n-#include <string>\n-\n-#include \"absl/meta/type_traits.h\"\n-#include \"absl/strings/str_cat.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"xla/tsl/platform/logging.h\"\n-\n-namespace tsl {\n-namespace strings {\n-\n-\n-// ----------------------------------------------------------------------\n-// StrCat()\n-//    This merges the given strings or integers, with no delimiter.  This\n-//    is designed to be the fastest possible way to construct a string out\n-//    of a mix of raw C strings, StringPieces, strings, and integer values.\n-// ----------------------------------------------------------------------\n-\n-// Append is merely a version of memcpy that returns the address of the byte\n-// after the area just overwritten.  It comes in multiple flavors to minimize\n-// call overhead.\n-static char* Append1(char* out, const absl::AlphaNum& x) {\n-  if (x.data() == nullptr) return out;\n-\n-  memcpy(out, x.data(), x.size());\n-  return out + x.size();\n-}\n-\n-static char* Append2(char* out, const absl::AlphaNum& x1,\n-                     const absl::AlphaNum& x2) {\n-  if (x1.data() != nullptr) {\n-    memcpy(out, x1.data(), x1.size());\n-    out += x1.size();\n-  }\n-\n-  if (x2.data() == nullptr) return out;\n-\n-  memcpy(out, x2.data(), x2.size());\n-  return out + x2.size();\n-}\n-\n-static char* Append4(char* out, const absl::AlphaNum& x1,\n-                     const absl::AlphaNum& x2, const absl::AlphaNum& x3,\n-                     const absl::AlphaNum& x4) {\n-  if (x1.data() != nullptr) {\n-    memcpy(out, x1.data(), x1.size());\n-    out += x1.size();\n-  }\n-\n-  if (x2.data() != nullptr) {\n-    memcpy(out, x2.data(), x2.size());\n-    out += x2.size();\n-  }\n-\n-  if (x3.data() != nullptr) {\n-    memcpy(out, x3.data(), x3.size());\n-    out += x3.size();\n-  }\n-\n-  if (x4.data() == nullptr) return out;\n-\n-  memcpy(out, x4.data(), x4.size());\n-  return out + x4.size();\n-}\n-\n-std::string StrCat(const absl::AlphaNum& a) {\n-  return std::string(a.data(), a.size());\n-}\n-\n-std::string StrCat(const absl::AlphaNum& a, const absl::AlphaNum& b) {\n-  std::string result(a.size() + b.size(), '\\0');\n-  char *const begin = &*result.begin();\n-  char *out = Append2(begin, a, b);\n-  DCHECK_EQ(out, begin + result.size());\n-  return result;\n-}\n-\n-std::string StrCat(const absl::AlphaNum& a, const absl::AlphaNum& b,\n-                   const absl::AlphaNum& c) {\n-  std::string result(a.size() + b.size() + c.size(), '\\0');\n-  char *const begin = &*result.begin();\n-  char *out = Append2(begin, a, b);\n-  out = Append1(out, c);\n-  DCHECK_EQ(out, begin + result.size());\n-  return result;\n-}\n-\n-std::string StrCat(const absl::AlphaNum& a, const absl::AlphaNum& b,\n-                   const absl::AlphaNum& c, const absl::AlphaNum& d) {\n-  std::string result(a.size() + b.size() + c.size() + d.size(), '\\0');\n-  char *const begin = &*result.begin();\n-  char *out = Append4(begin, a, b, c, d);\n-  DCHECK_EQ(out, begin + result.size());\n-  return result;\n-}\n-\n-namespace {\n-// HasMember is true_type or false_type, depending on whether or not\n-// T has a __resize_default_init member. Resize will call the\n-// __resize_default_init member if it exists, and will call the resize\n-// member otherwise.\n-template <typename string_type, typename = void>\n-struct ResizeUninitializedTraits {\n-  using HasMember = std::false_type;\n-  static void Resize(string_type *s, size_t new_size) { s->resize(new_size); }\n-};\n-\n-// __resize_default_init is provided by libc++ >= 8.0.\n-template <typename string_type>\n-struct ResizeUninitializedTraits<\n-    string_type, absl::void_t<decltype(std::declval<string_type &>()\n-                                           .__resize_default_init(237))> > {\n-  using HasMember = std::true_type;\n-  static void Resize(string_type *s, size_t new_size) {\n-    s->__resize_default_init(new_size);\n-  }\n-};\n-\n-static inline void STLStringResizeUninitialized(std::string* s,\n-                                                size_t new_size) {\n-  ResizeUninitializedTraits<std::string>::Resize(s, new_size);\n-}\n-\n-// Used to ensure exponential growth so that the amortized complexity of\n-// increasing the string size by a small amount is O(1), in contrast to\n-// O(str->size()) in the case of precise growth.\n-// TODO(b/217943845): Would be better to use absl::strings so we don't need to\n-// keep cherry-picking performance fixes.\n-template <typename string_type>\n-void STLStringReserveAmortized(string_type *s, size_t new_size) {\n-  const size_t cap = s->capacity();\n-  if (new_size > cap) {\n-    // Make sure to always grow by at least a factor of 2x.\n-    s->reserve((std::max)(new_size, 2 * cap));\n-  }\n-}\n-\n-// Like STLStringResizeUninitialized(str, new_size), except guaranteed to use\n-// exponential growth so that the amortized complexity of increasing the string\n-// size by a small amount is O(1), in contrast to O(str->size()) in the case of\n-// precise growth.\n-template <typename string_type>\n-void STLStringResizeUninitializedAmortized(string_type *s, size_t new_size) {\n-  STLStringReserveAmortized(s, new_size);\n-  STLStringResizeUninitialized(s, new_size);\n-}\n-\n-}  // namespace\n-namespace internal {\n-\n-// Do not call directly - these are not part of the public API.\n-std::string CatPieces(std::initializer_list<absl::string_view> pieces) {\n-  size_t total_size = 0;\n-  for (const absl::string_view piece : pieces) total_size += piece.size();\n-  std::string result(total_size, '\\0');\n-\n-  char *const begin = &*result.begin();\n-  char *out = begin;\n-  for (const absl::string_view piece : pieces) {\n-    const size_t this_size = piece.size();\n-    memcpy(out, piece.data(), this_size);\n-    out += this_size;\n-  }\n-  DCHECK_EQ(out, begin + result.size());\n-  return result;\n-}\n-\n-// It's possible to call StrAppend with a StringPiece that is itself a fragment\n-// of the string we're appending to.  However the results of this are random.\n-// Therefore, check for this in debug mode.  Use unsigned math so we only have\n-// to do one comparison.\n-#define DCHECK_NO_OVERLAP(dest, src) \\\n-  DCHECK_GE(uintptr_t((src).data() - (dest).data()), uintptr_t((dest).size()))\n-\n-void AppendPieces(std::string* result,\n-                  std::initializer_list<absl::string_view> pieces) {\n-  size_t old_size = result->size();\n-  size_t total_size = old_size;\n-  for (const absl::string_view piece : pieces) {\n-    DCHECK_NO_OVERLAP(*result, piece);\n-    total_size += piece.size();\n-  }\n-  STLStringResizeUninitializedAmortized(result, total_size);\n-\n-  char *const begin = &*result->begin();\n-  char *out = begin + old_size;\n-  for (const absl::string_view piece : pieces) {\n-    const size_t this_size = piece.size();\n-    memcpy(out, piece.data(), this_size);\n-    out += this_size;\n-  }\n-  DCHECK_EQ(out, begin + result->size());\n-}\n-\n-}  // namespace internal\n-\n-void StrAppend(std::string* dest, const absl::AlphaNum& a) {\n-  DCHECK_NO_OVERLAP(*dest, a);\n-  dest->append(a.data(), a.size());\n-}\n-\n-void StrAppend(std::string* dest, const absl::AlphaNum& a,\n-               const absl::AlphaNum& b) {\n-  DCHECK_NO_OVERLAP(*dest, a);\n-  DCHECK_NO_OVERLAP(*dest, b);\n-  std::string::size_type old_size = dest->size();\n-  STLStringResizeUninitializedAmortized(dest, old_size + a.size() + b.size());\n-  char* const begin = &*dest->begin();\n-  char *out = Append2(begin + old_size, a, b);\n-  DCHECK_EQ(out, begin + dest->size());\n-}\n-\n-void StrAppend(std::string* dest, const absl::AlphaNum& a,\n-               const absl::AlphaNum& b, const absl::AlphaNum& c) {\n-  DCHECK_NO_OVERLAP(*dest, a);\n-  DCHECK_NO_OVERLAP(*dest, b);\n-  DCHECK_NO_OVERLAP(*dest, c);\n-  std::string::size_type old_size = dest->size();\n-  STLStringResizeUninitializedAmortized(\n-      dest, old_size + a.size() + b.size() + c.size());\n-  char* const begin = &*dest->begin();\n-  char *out = Append2(begin + old_size, a, b);\n-  out = Append1(out, c);\n-  DCHECK_EQ(out, begin + dest->size());\n-}\n-\n-void StrAppend(std::string* dest, const absl::AlphaNum& a,\n-               const absl::AlphaNum& b, const absl::AlphaNum& c,\n-               const absl::AlphaNum& d) {\n-  DCHECK_NO_OVERLAP(*dest, a);\n-  DCHECK_NO_OVERLAP(*dest, b);\n-  DCHECK_NO_OVERLAP(*dest, c);\n-  DCHECK_NO_OVERLAP(*dest, d);\n-  std::string::size_type old_size = dest->size();\n-  STLStringResizeUninitializedAmortized(\n-      dest, old_size + a.size() + b.size() + c.size() + d.size());\n-  char* const begin = &*dest->begin();\n-  char *out = Append4(begin + old_size, a, b, c, d);\n-  DCHECK_EQ(out, begin + dest->size());\n-}\n-\n-}  // namespace strings\n-}  // namespace tsl"
        },
        {
            "sha": "f82190957c1b0a3b6b1305cb2fdb2a94ffca4776",
            "filename": "third_party/xla/third_party/tsl/tsl/platform/strcat.h",
            "status": "modified",
            "additions": 38,
            "deletions": 47,
            "changes": 85,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstrcat.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstrcat.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstrcat.h?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -20,7 +20,6 @@ limitations under the License.\n #ifndef TENSORFLOW_TSL_PLATFORM_STRCAT_H_\n #define TENSORFLOW_TSL_PLATFORM_STRCAT_H_\n \n-#include <initializer_list>\n #include <string>\n \n #include \"absl/base/attributes.h\"\n@@ -106,44 +105,30 @@ using AlphaNum ABSL_DEPRECATE_AND_INLINE() = absl::AlphaNum;\n // ----------------------------------------------------------------------\n \n // For performance reasons, we have specializations for <= 4 args.\n-ABSL_DEPRECATED(\"Use absl::StrCat() instead.\")\n-std::string StrCat(const absl::AlphaNum& a) TF_MUST_USE_RESULT;\n-ABSL_DEPRECATED(\"Use absl::StrCat() instead.\")\n-std::string StrCat(const absl::AlphaNum& a,\n-                   const absl::AlphaNum& b) TF_MUST_USE_RESULT;\n-ABSL_DEPRECATED(\"Use absl::StrCat() instead.\")\n-std::string StrCat(const absl::AlphaNum& a, const absl::AlphaNum& b,\n-                   const absl::AlphaNum& c) TF_MUST_USE_RESULT;\n-ABSL_DEPRECATED(\"Use absl::StrCat() instead.\")\n-std::string StrCat(const absl::AlphaNum& a, const absl::AlphaNum& b,\n-                   const absl::AlphaNum& c,\n-                   const absl::AlphaNum& d) TF_MUST_USE_RESULT;\n-\n-namespace internal {\n-\n-// Do not call directly - this is not part of the public API.\n-std::string CatPieces(std::initializer_list<absl::string_view> pieces);\n-void AppendPieces(std::string *dest,\n-                  std::initializer_list<absl::string_view> pieces);\n-\n-}  // namespace internal\n+ABSL_DEPRECATE_AND_INLINE()\n+inline std::string StrCat(const absl::AlphaNum& a) { return absl::StrCat(a); }\n+ABSL_DEPRECATE_AND_INLINE()\n+inline std::string StrCat(const absl::AlphaNum& a, const absl::AlphaNum& b) {\n+  return absl::StrCat(a, b);\n+}\n+ABSL_DEPRECATE_AND_INLINE()\n+inline std::string StrCat(const absl::AlphaNum& a, const absl::AlphaNum& b,\n+                          const absl::AlphaNum& c) {\n+  return absl::StrCat(a, b, c);\n+}\n+ABSL_DEPRECATE_AND_INLINE()\n+inline std::string StrCat(const absl::AlphaNum& a, const absl::AlphaNum& b,\n+                          const absl::AlphaNum& c, const absl::AlphaNum& d) {\n+  return absl::StrCat(a, b, c, d);\n+}\n \n // Support 5 or more arguments\n-template <typename... AV>\n-ABSL_DEPRECATED(\"Use absl::StrCat() instead.\")\n-std::string\n-    StrCat(const absl::AlphaNum& a, const absl::AlphaNum& b,\n-           const absl::AlphaNum& c, const absl::AlphaNum& d,\n-           const absl::AlphaNum& e, const AV&... args) TF_MUST_USE_RESULT;\n-\n template <typename... AV>\n ABSL_DEPRECATED(\"Use absl::StrCat() instead.\")\n std::string StrCat(const absl::AlphaNum& a, const absl::AlphaNum& b,\n                    const absl::AlphaNum& c, const absl::AlphaNum& d,\n                    const absl::AlphaNum& e, const AV&... args) {\n-  return internal::CatPieces(\n-      {a.Piece(), b.Piece(), c.Piece(), d.Piece(), e.Piece(),\n-       static_cast<const absl::AlphaNum&>(args).Piece()...});\n+  return absl::StrCat(a, b, c, d, e, args...);\n }\n \n // ----------------------------------------------------------------------\n@@ -167,18 +152,26 @@ std::string StrCat(const absl::AlphaNum& a, const absl::AlphaNum& b,\n //    worked around as consecutive calls to StrAppend are quite efficient.\n // ----------------------------------------------------------------------\n \n-ABSL_DEPRECATED(\"Use absl::StrAppend() instead.\")\n-void StrAppend(std::string* dest, const absl::AlphaNum& a);\n-ABSL_DEPRECATED(\"Use absl::StrAppend() instead.\")\n-void StrAppend(std::string* dest, const absl::AlphaNum& a,\n-               const absl::AlphaNum& b);\n-ABSL_DEPRECATED(\"Use absl::StrAppend() instead.\")\n-void StrAppend(std::string* dest, const absl::AlphaNum& a,\n-               const absl::AlphaNum& b, const absl::AlphaNum& c);\n-ABSL_DEPRECATED(\"Use absl::StrAppend() instead.\")\n-void StrAppend(std::string* dest, const absl::AlphaNum& a,\n-               const absl::AlphaNum& b, const absl::AlphaNum& c,\n-               const absl::AlphaNum& d);\n+ABSL_DEPRECATE_AND_INLINE()\n+inline void StrAppend(std::string* dest, const absl::AlphaNum& a) {\n+  absl::StrAppend(dest, a);\n+}\n+ABSL_DEPRECATE_AND_INLINE()\n+inline void StrAppend(std::string* dest, const absl::AlphaNum& a,\n+                      const absl::AlphaNum& b) {\n+  absl::StrAppend(dest, a, b);\n+}\n+ABSL_DEPRECATE_AND_INLINE()\n+inline void StrAppend(std::string* dest, const absl::AlphaNum& a,\n+                      const absl::AlphaNum& b, const absl::AlphaNum& c) {\n+  absl::StrAppend(dest, a, b, c);\n+}\n+ABSL_DEPRECATE_AND_INLINE()\n+inline void StrAppend(std::string* dest, const absl::AlphaNum& a,\n+                      const absl::AlphaNum& b, const absl::AlphaNum& c,\n+                      const absl::AlphaNum& d) {\n+  absl::StrAppend(dest, a, b, c, d);\n+}\n \n // Support 5 or more arguments\n template <typename... AV>\n@@ -187,9 +180,7 @@ inline void StrAppend(std::string* dest, const absl::AlphaNum& a,\n                       const absl::AlphaNum& b, const absl::AlphaNum& c,\n                       const absl::AlphaNum& d, const absl::AlphaNum& e,\n                       const AV&... args) {\n-  internal::AppendPieces(dest,\n-                         {a.Piece(), b.Piece(), c.Piece(), d.Piece(), e.Piece(),\n-                          static_cast<const absl::AlphaNum&>(args).Piece()...});\n+  absl::StrAppend(dest, a, b, c, d, e, args...);\n }\n \n }  // namespace strings"
        },
        {
            "sha": "63930612a2aa622bcbfa4d9180a67bfd33e683b4",
            "filename": "third_party/xla/third_party/tsl/tsl/platform/strcat_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 394,
            "changes": 394,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/682659f1d20f53d82008992cc183a09a70ee2a66/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstrcat_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/682659f1d20f53d82008992cc183a09a70ee2a66/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstrcat_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstrcat_test.cc?ref=682659f1d20f53d82008992cc183a09a70ee2a66",
            "patch": "@@ -1,394 +0,0 @@\n-/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"tsl/platform/strcat.h\"\n-\n-#include <cstddef>\n-#include <cstdint>\n-#include <string>\n-\n-#include \"absl/strings/str_cat.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"xla/tsl/platform/test.h\"\n-#include \"xla/tsl/platform/types.h\"\n-#include \"tsl/platform/bfloat16.h\"\n-#include \"tsl/platform/stringprintf.h\"\n-\n-#ifdef _MSC_VER\n-// ssize_t is not a standard C++ type.\n-typedef ptrdiff_t ssize_t;\n-#endif\n-\n-namespace tsl {\n-namespace strings {\n-\n-// Test StrCat of ints and longs of various sizes and signdedness.\n-TEST(StrCat, Ints) {\n-  const int16_t s = -1;\n-  const uint16 us = 2;\n-  const int i = -3;\n-  const unsigned int ui = 4;\n-  const int32_t l = -5;\n-  const uint32 ul = 6;\n-  const int64_t ll = -7;\n-  const uint64 ull = 8;\n-  const ptrdiff_t ptrdiff = -9;\n-  const size_t size = 10;\n-  const ssize_t ssize = -11;\n-  const intptr_t intptr = -12;\n-  const uintptr_t uintptr = 13;\n-  string answer;\n-  answer = StrCat(s, us);\n-  EXPECT_EQ(answer, \"-12\");\n-  answer = StrCat(i, ui);\n-  EXPECT_EQ(answer, \"-34\");\n-  answer = StrCat(l, ul);\n-  EXPECT_EQ(answer, \"-56\");\n-  answer = StrCat(ll, ull);\n-  EXPECT_EQ(answer, \"-78\");\n-  answer = StrCat(ptrdiff, size);\n-  EXPECT_EQ(answer, \"-910\");\n-  answer = StrCat(ssize, intptr);\n-  EXPECT_EQ(answer, \"-11-12\");\n-  answer = StrCat(uintptr, 0);\n-  EXPECT_EQ(answer, \"130\");\n-}\n-\n-TEST(StrCat, Floats) {\n-  const int s = 0;\n-  const float f = 1.5f;\n-  const double d = 1.5;\n-\n-  string answer;\n-  answer = StrCat(s, f);\n-  EXPECT_EQ(answer, \"01.5\");\n-  answer = StrCat(s, d);\n-  EXPECT_EQ(answer, \"01.5\");\n-}\n-\n-TEST(StrCat, Nulls) {\n-  string result;\n-  // When passed to StrCat the below will produce a NULL data pointer\n-  absl::string_view v;\n-\n-  string strs[] = {\"Hello\", \"Cruel\", \"World\"};\n-\n-  result = StrCat(v);\n-  EXPECT_EQ(result, \"\");\n-\n-  result = StrCat(strs[0], v);\n-  EXPECT_EQ(result, \"Hello\");\n-\n-  result = StrCat(v, strs[0]);\n-  EXPECT_EQ(result, \"Hello\");\n-\n-  result = StrCat(v, strs[0], strs[1]);\n-  EXPECT_EQ(result, \"HelloCruel\");\n-\n-  result = StrCat(strs[0], v, strs[1]);\n-  EXPECT_EQ(result, \"HelloCruel\");\n-\n-  result = StrCat(strs[0], strs[1], v);\n-  EXPECT_EQ(result, \"HelloCruel\");\n-\n-  result = StrCat(v, strs[0], strs[1], strs[2]);\n-  EXPECT_EQ(result, \"HelloCruelWorld\");\n-\n-  result = StrCat(strs[0], v, strs[1], strs[2]);\n-  EXPECT_EQ(result, \"HelloCruelWorld\");\n-\n-  result = StrCat(strs[0], strs[1], v, strs[2]);\n-  EXPECT_EQ(result, \"HelloCruelWorld\");\n-\n-  result = StrCat(strs[0], strs[1], strs[2], v);\n-  EXPECT_EQ(result, \"HelloCruelWorld\");\n-}\n-\n-TEST(StrCat, Basics) {\n-  string result;\n-\n-  string strs[] = {\"Hello\", \"Cruel\", \"World\"};\n-\n-  absl::string_view pieces[] = {\"Hello\", \"Cruel\", \"World\"};\n-\n-  const char *c_strs[] = {\"Hello\", \"Cruel\", \"World\"};\n-\n-  int32 i32s[] = {'H', 'C', 'W'};\n-  uint64 ui64s[] = {12345678910LL, 10987654321LL};\n-\n-  result = StrCat(false, true, 2, 3);\n-  EXPECT_EQ(result, \"0123\");\n-\n-  result = StrCat(-1);\n-  EXPECT_EQ(result, \"-1\");\n-\n-  result = StrCat(0.5);\n-  EXPECT_EQ(result, \"0.5\");\n-\n-  result = StrCat(strs[1], pieces[2]);\n-  EXPECT_EQ(result, \"CruelWorld\");\n-\n-  result = StrCat(strs[0], \", \", pieces[2]);\n-  EXPECT_EQ(result, \"Hello, World\");\n-\n-  result = StrCat(strs[0], \", \", strs[1], \" \", strs[2], \"!\");\n-  EXPECT_EQ(result, \"Hello, Cruel World!\");\n-\n-  result = StrCat(pieces[0], \", \", pieces[1], \" \", pieces[2]);\n-  EXPECT_EQ(result, \"Hello, Cruel World\");\n-\n-  result = StrCat(c_strs[0], \", \", c_strs[1], \" \", c_strs[2]);\n-  EXPECT_EQ(result, \"Hello, Cruel World\");\n-\n-  result = StrCat(\"ASCII \", i32s[0], \", \", i32s[1], \" \", i32s[2], \"!\");\n-  EXPECT_EQ(result, \"ASCII 72, 67 87!\");\n-\n-  result = StrCat(ui64s[0], \", \", ui64s[1], \"!\");\n-  EXPECT_EQ(result, \"12345678910, 10987654321!\");\n-\n-  string one = \"1\";  // Actually, it's the size of this string that we want; a\n-                     // 64-bit build distinguishes between size_t and uint64,\n-                     // even though they're both unsigned 64-bit values.\n-  result = StrCat(\"And a \", one.size(), \" and a \", &result[2] - &result[0],\n-                  \" and a \", one, \" 2 3 4\", \"!\");\n-  EXPECT_EQ(result, \"And a 1 and a 2 and a 1 2 3 4!\");\n-\n-  // result = StrCat(\"Single chars won't compile\", '!');\n-  // result = StrCat(\"Neither will NULLs\", NULL);\n-  result = StrCat(\"To output a char by ASCII/numeric value, use +: \", '!' + 0);\n-  EXPECT_EQ(result, \"To output a char by ASCII/numeric value, use +: 33\");\n-\n-  float f = 100000.5;\n-  result = StrCat(\"A hundred K and a half is \", f);\n-  EXPECT_EQ(result, \"A hundred K and a half is 100000\");\n-\n-  double d = f;\n-  d *= d;\n-  result = StrCat(\"A hundred K and a half squared is \", d);\n-  EXPECT_EQ(result, \"A hundred K and a half squared is 1.00001e+10\");\n-\n-  result = StrCat(1, 2, 333, 4444, 55555, 666666, 7777777, 88888888, 999999999);\n-  EXPECT_EQ(result, \"12333444455555666666777777788888888999999999\");\n-}\n-\n-TEST(StrCat, MaxArgs) {\n-  string result;\n-  // Test 10 up to 26 arguments, the current maximum\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\");\n-  EXPECT_EQ(result, \"123456789a\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\");\n-  EXPECT_EQ(result, \"123456789ab\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\");\n-  EXPECT_EQ(result, \"123456789abc\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\");\n-  EXPECT_EQ(result, \"123456789abcd\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\");\n-  EXPECT_EQ(result, \"123456789abcde\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\");\n-  EXPECT_EQ(result, \"123456789abcdef\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\");\n-  EXPECT_EQ(result, \"123456789abcdefg\");\n-  result =\n-      StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\");\n-  EXPECT_EQ(result, \"123456789abcdefgh\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n-                  \"h\", \"i\");\n-  EXPECT_EQ(result, \"123456789abcdefghi\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n-                  \"h\", \"i\", \"j\");\n-  EXPECT_EQ(result, \"123456789abcdefghij\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n-                  \"h\", \"i\", \"j\", \"k\");\n-  EXPECT_EQ(result, \"123456789abcdefghijk\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n-                  \"h\", \"i\", \"j\", \"k\", \"l\");\n-  EXPECT_EQ(result, \"123456789abcdefghijkl\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n-                  \"h\", \"i\", \"j\", \"k\", \"l\", \"m\");\n-  EXPECT_EQ(result, \"123456789abcdefghijklm\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n-                  \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\");\n-  EXPECT_EQ(result, \"123456789abcdefghijklmn\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n-                  \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\");\n-  EXPECT_EQ(result, \"123456789abcdefghijklmno\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n-                  \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\");\n-  EXPECT_EQ(result, \"123456789abcdefghijklmnop\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n-                  \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\");\n-  EXPECT_EQ(result, \"123456789abcdefghijklmnopq\");\n-  // No limit thanks to C++11's variadic templates\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\",\n-                  \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\",\n-                  \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\", \"D\",\n-                  \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\",\n-                  \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\");\n-  EXPECT_EQ(result,\n-            \"12345678910abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\");\n-}\n-\n-TEST(StrAppend, Basics) {\n-  string result = \"existing text\";\n-\n-  string strs[] = {\"Hello\", \"Cruel\", \"World\"};\n-\n-  absl::string_view pieces[] = {\"Hello\", \"Cruel\", \"World\"};\n-\n-  const char *c_strs[] = {\"Hello\", \"Cruel\", \"World\"};\n-\n-  int32 i32s[] = {'H', 'C', 'W'};\n-  uint64 ui64s[] = {12345678910LL, 10987654321LL};\n-\n-  string::size_type old_size = result.size();\n-  StrAppend(&result, strs[0]);\n-  EXPECT_EQ(result.substr(old_size), \"Hello\");\n-\n-  old_size = result.size();\n-  StrAppend(&result, strs[1], pieces[2]);\n-  EXPECT_EQ(result.substr(old_size), \"CruelWorld\");\n-\n-  old_size = result.size();\n-  StrAppend(&result, strs[0], \", \", pieces[2]);\n-  EXPECT_EQ(result.substr(old_size), \"Hello, World\");\n-\n-  old_size = result.size();\n-  StrAppend(&result, strs[0], \", \", strs[1], \" \", strs[2], \"!\");\n-  EXPECT_EQ(result.substr(old_size), \"Hello, Cruel World!\");\n-\n-  old_size = result.size();\n-  StrAppend(&result, pieces[0], \", \", pieces[1], \" \", pieces[2]);\n-  EXPECT_EQ(result.substr(old_size), \"Hello, Cruel World\");\n-\n-  old_size = result.size();\n-  StrAppend(&result, c_strs[0], \", \", c_strs[1], \" \", c_strs[2]);\n-  EXPECT_EQ(result.substr(old_size), \"Hello, Cruel World\");\n-\n-  old_size = result.size();\n-  StrAppend(&result, \"ASCII \", i32s[0], \", \", i32s[1], \" \", i32s[2], \"!\");\n-  EXPECT_EQ(result.substr(old_size), \"ASCII 72, 67 87!\");\n-\n-  old_size = result.size();\n-  StrAppend(&result, ui64s[0], \", \", ui64s[1], \"!\");\n-  EXPECT_EQ(result.substr(old_size), \"12345678910, 10987654321!\");\n-\n-  string one = \"1\";  // Actually, it's the size of this string that we want; a\n-                     // 64-bit build distinguishes between size_t and uint64,\n-                     // even though they're both unsigned 64-bit values.\n-  old_size = result.size();\n-  StrAppend(&result, \"And a \", one.size(), \" and a \", &result[2] - &result[0],\n-            \" and a \", one, \" 2 3 4\", \"!\");\n-  EXPECT_EQ(result.substr(old_size), \"And a 1 and a 2 and a 1 2 3 4!\");\n-\n-  // result = StrCat(\"Single chars won't compile\", '!');\n-  // result = StrCat(\"Neither will NULLs\", NULL);\n-  old_size = result.size();\n-  StrAppend(&result,\n-            \"To output a char by ASCII/numeric value, use +: \", '!' + 0);\n-  EXPECT_EQ(result.substr(old_size),\n-            \"To output a char by ASCII/numeric value, use +: 33\");\n-\n-  float f = 100000.5;\n-  old_size = result.size();\n-  StrAppend(&result, \"A hundred K and a half is \", f);\n-  EXPECT_EQ(result.substr(old_size), \"A hundred K and a half is 100000\");\n-\n-  double d = f;\n-  d *= d;\n-  old_size = result.size();\n-  StrAppend(&result, \"A hundred K and a half squared is \", d);\n-  EXPECT_EQ(result.substr(old_size),\n-            \"A hundred K and a half squared is 1.00001e+10\");\n-\n-  // Test 9 arguments, the old maximum\n-  old_size = result.size();\n-  StrAppend(&result, 1, 22, 333, 4444, 55555, 666666, 7777777, 88888888, 9);\n-  EXPECT_EQ(result.substr(old_size), \"1223334444555556666667777777888888889\");\n-\n-  // No limit thanks to C++11's variadic templates\n-  old_size = result.size();\n-  StrAppend(&result, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, \"a\", \"b\", \"c\", \"d\", \"e\",\n-            \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\",\n-            \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\", \"D\", \"E\",\n-            \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\",\n-            \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\",\n-            \"No limit thanks to C++11's variadic templates\");\n-  EXPECT_EQ(result.substr(old_size),\n-            \"12345678910abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n-            \"No limit thanks to C++11's variadic templates\");\n-}\n-\n-TEST(StrAppend, Death) {\n-  string s = \"self\";\n-  EXPECT_DEBUG_DEATH(StrAppend(&s, s.c_str() + 1), \"Check failed:\");\n-  EXPECT_DEBUG_DEATH(StrAppend(&s, s), \"Check failed:\");\n-}\n-\n-static void CheckHex64(uint64 v) {\n-  string actual = strings::StrCat(absl::Hex(v, absl::kZeroPad16));\n-  string expected = Printf(\"%016llx\", static_cast<unsigned long long>(v));\n-  EXPECT_EQ(expected, actual) << \" decimal value \" << v;\n-\n-  actual = strings::StrCat(absl::Hex(v, absl::kZeroPad8));\n-  expected = Printf(\"%08llx\", static_cast<unsigned long long>(v));\n-  EXPECT_EQ(expected, actual) << \" decimal value \" << v;\n-\n-  actual = strings::StrCat(absl::Hex(v));\n-  expected = Printf(\"%llx\", static_cast<unsigned long long>(v));\n-  EXPECT_EQ(expected, actual) << \" decimal value \" << v;\n-}\n-\n-static void CheckHex32(uint32 v) {\n-  string actual = strings::StrCat(absl::Hex(v, absl::kZeroPad8));\n-  string expected = Printf(\"%08x\", v);\n-  EXPECT_EQ(expected, actual) << \" decimal value \" << v;\n-\n-  actual = strings::StrCat(absl::Hex(v));\n-  expected = Printf(\"%x\", v);\n-  EXPECT_EQ(expected, actual) << \" decimal value \" << v;\n-}\n-\n-static void CheckHexSigned32(int32_t v) {\n-  string actual = strings::StrCat(absl::Hex(v, absl::kZeroPad8));\n-  string expected = Printf(\"%08x\", v);\n-  EXPECT_EQ(expected, actual) << \" decimal value \" << v;\n-\n-  actual = strings::StrCat(absl::Hex(v));\n-  expected = Printf(\"%x\", v);\n-  EXPECT_EQ(expected, actual) << \" decimal value \" << v;\n-}\n-\n-static void TestFastPrints() {\n-  // Test min int to make sure that works\n-  for (int i = 0; i < 10000; i++) {\n-    CheckHex64(i);\n-    CheckHex32(i);\n-    CheckHexSigned32(i);\n-    CheckHexSigned32(-i);\n-  }\n-  CheckHex64(0x123456789abcdef0ull);\n-  CheckHex32(0x12345678);\n-\n-  int8_t minus_one_8bit = -1;\n-  EXPECT_EQ(\"ff\", strings::StrCat(absl::Hex(minus_one_8bit)));\n-\n-  int16_t minus_one_16bit = -1;\n-  EXPECT_EQ(\"ffff\", strings::StrCat(absl::Hex(minus_one_16bit)));\n-}\n-\n-TEST(Numbers, TestFunctionsMovedOverFromNumbersMain) { TestFastPrints(); }\n-\n-}  // namespace strings\n-}  // namespace tsl"
        },
        {
            "sha": "78185cf9e2169b6038d47e96de96bb49fe658ff7",
            "filename": "third_party/xla/xla/backends/autotuner/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2FBUILD?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -113,6 +113,7 @@ cc_library(\n         \":autotuner_cache_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings:string_view\",\n     ],\n )\n \n@@ -149,6 +150,7 @@ xla_cc_test(\n     name = \"file_based_autotuner_cache_test\",\n     srcs = [\"file_based_autotuner_cache_test.cc\"],\n     deps = [\n+        \":autotuner_cache_interface\",\n         \":autotuner_cache_proto_cc\",\n         \":file_based_autotuner_cache\",\n         \"//xla:literal_util\","
        },
        {
            "sha": "08caf3e190a38454759297c92bf068673f7fe763",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -77,13 +77,13 @@ absl::Status Autotuner::Autotune(HloInstruction* instr) {\n absl::StatusOr<Autotuner::Config> Autotuner::GetBestConfig(\n     HloInstruction* instr) {\n   if (cache_) {\n-    auto cached_entry = cache_->Lookup(instr);\n-    if (cached_entry.has_value()) {\n-      VLOG(1) << \"Found cached entry for HLO: \" << instr->ToString();\n+    auto cached_config = cache_->Lookup(instr);\n+    if (cached_config.has_value()) {\n+      VLOG(1) << \"Found cached config for HLO: \" << instr->ToString();\n       for (auto& codegen_backend : codegen_backends_) {\n-        if (codegen_backend->name() == cached_entry->codegen_backend()) {\n+        if (codegen_backend->name() == cached_config->codegen_backend_name) {\n           auto backend_config = std::make_unique<google::protobuf::Any>(\n-              cached_entry->backend_config());\n+              cached_config->backend_config);\n           return Config{codegen_backend.get(), std::move(backend_config)};\n         }\n       }\n@@ -278,11 +278,11 @@ absl::StatusOr<Autotuner::Config> Autotuner::ProfileAndPickBest(\n   VLOG(1) << \"Picked config: \" << best_config->codegen_backend->name() << \" \"\n           << best_config->backend_config->ShortDebugString();\n \n-  AutotunerCacheEntry cache_entry;\n-  cache_entry.set_codegen_backend(min_duration_config->codegen_backend->name());\n-  *cache_entry.mutable_backend_config() = *best_config->backend_config;\n+  AutotunerCacheInterface::Config config;\n+  config.codegen_backend_name = (best_config->codegen_backend->name());\n+  config.backend_config = *best_config->backend_config;\n   if (cache_) {\n-    TF_RETURN_IF_ERROR(cache_->Insert(instr, cache_entry));\n+    TF_RETURN_IF_ERROR(cache_->Insert(instr, config));\n   }\n   return std::move(*best_config);\n }"
        },
        {
            "sha": "763c8a3acc130b3f6a82221448b52bfcd5cab6d9",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner_cache_interface.h",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_cache_interface.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_cache_interface.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_cache_interface.h?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <optional>\n \n #include \"absl/status/status.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"xla/backends/autotuner/autotuner_cache.pb.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n \n@@ -29,13 +30,18 @@ namespace xla {\n // AutotunerCacheInterface implementations may have different cache keys.\n class AutotunerCacheInterface {\n  public:\n+  // Serializable config. Will be changed to a proto in the future.\n+  struct Config {\n+    absl::string_view codegen_backend_name;\n+    google::protobuf::Any backend_config;\n+  };\n+\n   virtual ~AutotunerCacheInterface() = default;\n \n-  virtual std::optional<AutotunerCacheEntry> Lookup(\n-      const HloInstruction* instr) = 0;\n+  virtual std::optional<Config> Lookup(const HloInstruction* instr) = 0;\n \n   virtual absl::Status Insert(const HloInstruction* instr,\n-                              AutotunerCacheEntry& entry) = 0;\n+                              Config& best_config) = 0;\n };\n \n }  // namespace xla"
        },
        {
            "sha": "3b8750909092c33cffc8dc563e04b23a52335a33",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -112,10 +112,11 @@ class MockProfiler : public Profiler {\n \n class MockAutotunerCache : public AutotunerCacheInterface {\n  public:\n-  MOCK_METHOD(std::optional<AutotunerCacheEntry>, Lookup,\n+  MOCK_METHOD(std::optional<AutotunerCacheInterface::Config>, Lookup,\n               (const HloInstruction* instr), (override));\n   MOCK_METHOD(absl::Status, Insert,\n-              (const HloInstruction* instr, AutotunerCacheEntry& entry),\n+              (const HloInstruction* instr,\n+               AutotunerCacheInterface::Config& best_config),\n               (override));\n };\n \n@@ -384,13 +385,13 @@ TEST_F(AutotunerTest, AutotuneModuleWithDuplicateInstructions) {\n \n TEST_F(AutotunerTest, CacheHit) {\n   auto cache_manager = std::make_unique<MockAutotunerCache>();\n-  AutotunerCacheEntry entry;\n-  entry.set_codegen_backend(\"mock_backend\");\n+  AutotunerCacheInterface::Config config;\n+  config.codegen_backend_name = \"mock_backend\";\n   TestConfig test_config;\n   GetTestConfig(\"test_config_2\")->UnpackTo(&test_config);\n-  entry.mutable_backend_config()->PackFrom(test_config);\n+  config.backend_config.PackFrom(test_config);\n \n-  EXPECT_CALL(*cache_manager, Lookup(_)).WillOnce(Return(entry));\n+  EXPECT_CALL(*cache_manager, Lookup(_)).WillOnce(Return(config));\n \n   auto backend = std::make_unique<MockCodegenBackend>();\n   EXPECT_CALL(*backend, name()).WillRepeatedly(Return(\"mock_backend\"));"
        },
        {
            "sha": "1c6f1923871e34bc5561ab38488cf01378cc92e9",
            "filename": "third_party/xla/xla/backends/autotuner/file_based_autotuner_cache.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -128,7 +128,7 @@ absl::StatusOr<AutotunerCacheKey> FileBasedAutotunerCache::GetProtoKey(\n   return key;\n }\n \n-std::optional<AutotunerCacheEntry> FileBasedAutotunerCache::Lookup(\n+std::optional<AutotunerCacheInterface::Config> FileBasedAutotunerCache::Lookup(\n     const HloInstruction* instr) {\n   absl::StatusOr<std::string> map_key = GetMapKey(instr);\n   if (!map_key.ok()) {\n@@ -140,19 +140,25 @@ std::optional<AutotunerCacheEntry> FileBasedAutotunerCache::Lookup(\n   if (it == in_memory_cache_.end()) {\n     return std::nullopt;\n   }\n-  return it->second;\n+  AutotunerCacheInterface::Config config;\n+  config.codegen_backend_name = it->second.codegen_backend();\n+  config.backend_config = it->second.backend_config();\n+  return config;\n }\n \n-absl::Status FileBasedAutotunerCache::Insert(const HloInstruction* instr,\n-                                             AutotunerCacheEntry& entry) {\n+absl::Status FileBasedAutotunerCache::Insert(\n+    const HloInstruction* instr, AutotunerCacheInterface::Config& best_config) {\n   if (cache_config_.autotune_cache_mode ==\n       FileBasedCacheConfig::CacheMode::READ) {\n     return absl::OkStatus();\n   }\n   TF_ASSIGN_OR_RETURN(const std::string map_key, GetMapKey(instr));\n   TF_ASSIGN_OR_RETURN(AutotunerCacheKey proto_key, GetProtoKey(instr));\n   absl::MutexLock lock(&mutex_);\n+  AutotunerCacheEntry entry;\n   *entry.mutable_key() = proto_key;\n+  entry.set_codegen_backend(best_config.codegen_backend_name);\n+  *entry.mutable_backend_config() = best_config.backend_config;\n   in_memory_cache_[map_key] = entry;\n   if (!cache_config_.autotune_cache_dir.empty()) {\n     return Save(map_key, entry);"
        },
        {
            "sha": "42066e9c584750442da63a57b532c409a8d7b260",
            "filename": "third_party/xla/xla/backends/autotuner/file_based_autotuner_cache.h",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.h?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -79,11 +79,10 @@ class FileBasedAutotunerCache : public AutotunerCacheInterface {\n   static absl::StatusOr<std::unique_ptr<AutotunerCacheInterface>> Create(\n       const FileBasedCacheConfig& cache_config);\n \n-  std::optional<AutotunerCacheEntry> Lookup(\n-      const HloInstruction* instr) override ABSL_LOCKS_EXCLUDED(mutex_);\n+  std::optional<Config> Lookup(const HloInstruction* instr) override\n+      ABSL_LOCKS_EXCLUDED(mutex_);\n \n-  absl::Status Insert(const HloInstruction* instr,\n-                      AutotunerCacheEntry& entry) override\n+  absl::Status Insert(const HloInstruction* instr, Config& best_config) override\n       ABSL_LOCKS_EXCLUDED(mutex_);\n \n  private:"
        },
        {
            "sha": "4f3b09544c28b7ccc07b1344708d1cca20959a58",
            "filename": "third_party/xla/xla/backends/autotuner/file_based_autotuner_cache_test.cc",
            "status": "modified",
            "additions": 71,
            "deletions": 45,
            "changes": 116,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache_test.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -26,22 +26,22 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n #include \"xla/backends/autotuner/autotuner_cache.pb.h\"\n+#include \"xla/backends/autotuner/autotuner_cache_interface.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/tsl/util/proto/proto_matchers.h\"\n #include \"tsl/platform/path.h\"\n \n namespace xla {\n namespace {\n \n+using Config = ::xla::AutotunerCacheInterface::Config;\n using ::testing::Eq;\n using ::testing::Optional;\n-using ::tsl::proto_testing::EqualsProto;\n \n // Helper to create a dummy DeviceDescription.\n se::DeviceDescription CreateDummyDeviceDescription(\n@@ -103,6 +103,32 @@ class FileBasedAutotunerCacheTest : public ::testing::Test {\n   }\n };\n \n+// Matcher for Config.\n+MATCHER_P(ConfigEq, expected_config, \"\") {\n+  const Config& actual_config = arg;\n+  if (actual_config.codegen_backend_name !=\n+      expected_config.codegen_backend_name) {\n+    *result_listener << \"codegen_backend mismatch: expected \"\n+                     << expected_config.codegen_backend_name << \", got \"\n+                     << actual_config.codegen_backend_name;\n+    return false;\n+  }\n+  // Compare backend_config (google::protobuf::Any)\n+  if (actual_config.backend_config.type_url() !=\n+      expected_config.backend_config.type_url()) {\n+    *result_listener << \"backend_config type_url mismatch: expected \"\n+                     << expected_config.backend_config.type_url() << \", got \"\n+                     << actual_config.backend_config.type_url();\n+    return false;\n+  }\n+  if (actual_config.backend_config.value() !=\n+      expected_config.backend_config.value()) {\n+    *result_listener << \"backend_config value mismatch\";\n+    return false;\n+  }\n+  return true;\n+}\n+\n TEST_F(FileBasedAutotunerCacheTest, CreateEmpty) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto cache, FileBasedAutotunerCache::Create(\n@@ -118,27 +144,27 @@ TEST_F(FileBasedAutotunerCacheTest, InsertAndLookup) {\n                       GetConfig(CreateDummyDeviceDescription(),\n                                 FileBasedCacheConfig::CacheMode::READ_WRITE)));\n   auto instr = CreateDummyInstr(\"hlo1\");\n-  AutotunerCacheEntry entry;\n-  entry.set_codegen_backend(\"TestBackend\");\n-  *entry.mutable_backend_config() = CreateDummyBackendConfig();\n+  Config config;\n+  config.codegen_backend_name = \"TestBackend\";\n+  config.backend_config = CreateDummyBackendConfig();\n \n-  TF_ASSERT_OK(cache->Insert(instr.get(), entry));\n-  EXPECT_THAT(cache->Lookup(instr.get()), Optional(EqualsProto(entry)));\n+  TF_ASSERT_OK(cache->Insert(instr.get(), config));\n+  EXPECT_THAT(cache->Lookup(instr.get()), Optional(ConfigEq(config)));\n }\n \n TEST_F(FileBasedAutotunerCacheTest, SaveAndLoad) {\n   auto instr = CreateDummyInstr(\"hlo2\");\n-  AutotunerCacheEntry entry;\n-  entry.set_codegen_backend(\"TestBackend\");\n-  *entry.mutable_backend_config() = CreateDummyBackendConfig();\n+  Config config;\n+  config.codegen_backend_name = \"TestBackend\";\n+  config.backend_config = CreateDummyBackendConfig();\n \n   // Create cache, insert, and let it save.\n   {\n     TF_ASSERT_OK_AND_ASSIGN(auto cache,\n                             FileBasedAutotunerCache::Create(GetConfig(\n                                 CreateDummyDeviceDescription(),\n                                 FileBasedCacheConfig::CacheMode::READ_WRITE)));\n-    TF_ASSERT_OK(cache->Insert(instr.get(), entry));\n+    TF_ASSERT_OK(cache->Insert(instr.get(), config));\n   }\n \n   // Create a new cache, which should load from disk.\n@@ -147,23 +173,23 @@ TEST_F(FileBasedAutotunerCacheTest, SaveAndLoad) {\n                             FileBasedAutotunerCache::Create(GetConfig(\n                                 CreateDummyDeviceDescription(),\n                                 FileBasedCacheConfig::CacheMode::READ_WRITE)));\n-    EXPECT_THAT(cache->Lookup(instr.get()), Optional(EqualsProto(entry)));\n+    EXPECT_THAT(cache->Lookup(instr.get()), Optional(ConfigEq(config)));\n   }\n }\n \n TEST_F(FileBasedAutotunerCacheTest, LoadWithDifferentDevice) {\n   auto instr = CreateDummyInstr(\"hlo2\");\n-  AutotunerCacheEntry entry;\n-  entry.set_codegen_backend(\"TestBackend\");\n-  *entry.mutable_backend_config() = CreateDummyBackendConfig();\n+  Config config;\n+  config.codegen_backend_name = \"TestBackend\";\n+  config.backend_config = CreateDummyBackendConfig();\n \n   // Create cache, insert, and let it save.\n   {\n     TF_ASSERT_OK_AND_ASSIGN(auto cache,\n                             FileBasedAutotunerCache::Create(GetConfig(\n                                 CreateDummyDeviceDescription(),\n                                 FileBasedCacheConfig::CacheMode::READ_WRITE)));\n-    TF_ASSERT_OK(cache->Insert(instr.get(), entry));\n+    TF_ASSERT_OK(cache->Insert(instr.get(), config));\n   }\n \n   // Create a new cache with different device, should not load the entry.\n@@ -178,43 +204,43 @@ TEST_F(FileBasedAutotunerCacheTest, LoadWithDifferentDevice) {\n \n TEST_F(FileBasedAutotunerCacheTest, LoadWithDifferentVersion) {\n   auto instr = CreateDummyInstr(\"hlo2\");\n-  AutotunerCacheEntry entry;\n-  entry.set_codegen_backend(\"TestBackend\");\n-  *entry.mutable_backend_config() = CreateDummyBackendConfig();\n+  Config config;\n+  config.codegen_backend_name = \"TestBackend\";\n+  config.backend_config = CreateDummyBackendConfig();\n \n   // Create cache, insert, and let it save.\n   {\n     TF_ASSERT_OK_AND_ASSIGN(auto cache,\n                             FileBasedAutotunerCache::Create(GetConfig(\n                                 CreateDummyDeviceDescription(),\n                                 FileBasedCacheConfig::CacheMode::READ_WRITE)));\n-    TF_ASSERT_OK(cache->Insert(instr.get(), entry));\n+    TF_ASSERT_OK(cache->Insert(instr.get(), config));\n   }\n \n   // Create a new cache with different version, should not load the entry.\n   {\n-    auto config = GetConfig(CreateDummyDeviceDescription(),\n-                            FileBasedCacheConfig::CacheMode::READ_WRITE);\n-    config.cache_version = \"2\";\n+    auto cache_config = GetConfig(CreateDummyDeviceDescription(),\n+                                  FileBasedCacheConfig::CacheMode::READ_WRITE);\n+    cache_config.cache_version = \"2\";\n     TF_ASSERT_OK_AND_ASSIGN(auto cache,\n-                            FileBasedAutotunerCache::Create(config));\n+                            FileBasedAutotunerCache::Create(cache_config));\n     EXPECT_THAT(cache->Lookup(instr.get()), Eq(std::nullopt));\n   }\n }\n \n TEST_F(FileBasedAutotunerCacheTest, ReadOnlyMode) {\n   auto instr = CreateDummyInstr(\"hlo3\");\n-  AutotunerCacheEntry entry;\n-  entry.set_codegen_backend(\"TestBackend\");\n-  *entry.mutable_backend_config() = CreateDummyBackendConfig();\n+  Config config;\n+  config.codegen_backend_name = \"TestBackend\";\n+  config.backend_config = CreateDummyBackendConfig();\n \n   // Create in READ_WRITE mode to pre-populate the cache file.\n   {\n     TF_ASSERT_OK_AND_ASSIGN(auto cache,\n                             FileBasedAutotunerCache::Create(GetConfig(\n                                 CreateDummyDeviceDescription(),\n                                 FileBasedCacheConfig::CacheMode::READ_WRITE)));\n-    TF_ASSERT_OK(cache->Insert(instr.get(), entry));\n+    TF_ASSERT_OK(cache->Insert(instr.get(), config));\n   }\n \n   // Create in READ mode.\n@@ -223,14 +249,14 @@ TEST_F(FileBasedAutotunerCacheTest, ReadOnlyMode) {\n                       GetConfig(CreateDummyDeviceDescription(),\n                                 FileBasedCacheConfig::CacheMode::READ)));\n   // Lookup should work.\n-  EXPECT_THAT(cache->Lookup(instr.get()), Optional(EqualsProto(entry)));\n+  EXPECT_THAT(cache->Lookup(instr.get()), Optional(ConfigEq(config)));\n \n   // Insert a new entry.\n   auto instr2 = CreateDummyInstr(\"hlo4\");\n-  AutotunerCacheEntry entry2;\n-  entry2.set_codegen_backend(\"AnotherBackend\");\n-  *entry2.mutable_backend_config() = CreateDummyBackendConfig();\n-  TF_ASSERT_OK(cache->Insert(instr2.get(), entry2));\n+  Config config2;\n+  config2.codegen_backend_name = \"AnotherBackend\";\n+  config2.backend_config = CreateDummyBackendConfig();\n+  TF_ASSERT_OK(cache->Insert(instr2.get(), config2));\n   EXPECT_THAT(cache->Lookup(instr2.get()), Eq(std::nullopt));\n \n   // Create a new cache, key2 should not be present as it wasn't saved.\n@@ -239,7 +265,7 @@ TEST_F(FileBasedAutotunerCacheTest, ReadOnlyMode) {\n                             FileBasedAutotunerCache::Create(GetConfig(\n                                 CreateDummyDeviceDescription(),\n                                 FileBasedCacheConfig::CacheMode::READ_WRITE)));\n-    EXPECT_THAT(cache2->Lookup(instr.get()), Optional(EqualsProto(entry)));\n+    EXPECT_THAT(cache2->Lookup(instr.get()), Optional(ConfigEq(config)));\n     EXPECT_THAT(cache2->Lookup(instr2.get()), Eq(std::nullopt));\n   }\n }\n@@ -251,17 +277,17 @@ TEST_F(FileBasedAutotunerCacheTest, OverwriteEntry) {\n                                 FileBasedCacheConfig::CacheMode::READ_WRITE)));\n   auto instr = CreateDummyInstr(\"hlo5\");\n \n-  AutotunerCacheEntry entry1;\n-  entry1.set_codegen_backend(\"BackendV1\");\n-  *entry1.mutable_backend_config() = CreateDummyBackendConfig();\n-  TF_ASSERT_OK(cache->Insert(instr.get(), entry1));\n-  EXPECT_THAT(cache->Lookup(instr.get()), Optional(EqualsProto(entry1)));\n-\n-  AutotunerCacheEntry entry2;\n-  entry2.set_codegen_backend(\"BackendV2\");\n-  *entry2.mutable_backend_config() = CreateDummyBackendConfig();\n-  TF_ASSERT_OK(cache->Insert(instr.get(), entry2));\n-  EXPECT_THAT(cache->Lookup(instr.get()), Optional(EqualsProto(entry2)));\n+  Config config1;\n+  config1.codegen_backend_name = \"BackendV1\";\n+  config1.backend_config = CreateDummyBackendConfig();\n+  TF_ASSERT_OK(cache->Insert(instr.get(), config1));\n+  EXPECT_THAT(cache->Lookup(instr.get()), Optional(ConfigEq(config1)));\n+\n+  Config config2;\n+  config2.codegen_backend_name = \"BackendV2\";\n+  config2.backend_config = CreateDummyBackendConfig();\n+  TF_ASSERT_OK(cache->Insert(instr.get(), config2));\n+  EXPECT_THAT(cache->Lookup(instr.get()), Optional(ConfigEq(config2)));\n }\n \n }  // namespace"
        },
        {
            "sha": "3c84afec5bb04e3fc3a9641b0c5b5ae4c657f2c0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -371,6 +371,7 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service:algorithm_util\",\n+        \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -571,6 +572,7 @@ xla_test(\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest\","
        },
        {
            "sha": "e210a97d6af9dd49e5932a7b5e7131965537a2b4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -31,6 +31,7 @@ limitations under the License.\n #include \"mlir/Dialect/Math/IR/Math.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/Support/LLVM.h\"\n@@ -42,6 +43,7 @@ limitations under the License.\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/algorithm_util.h\"\n+#include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -128,6 +130,38 @@ std::vector<Value> SplitF32(EmitterLocOpBuilder b, Value input,\n   return split_inputs;\n }\n \n+absl::StatusOr<ttir::ScaleDotElemType> GetScaleDotElemType(Type value) {\n+  auto type = getElementTypeOrSelf(value);\n+  if (type == mlir::Float8E4M3FNType::get(value.getContext())) {\n+    return ttir::ScaleDotElemType::E4M3;\n+  }\n+  if (type == mlir::Float8E5M2Type::get(value.getContext())) {\n+    return ttir::ScaleDotElemType::E5M2;\n+  }\n+  if (type == mlir::Float4E2M1FNType::get(value.getContext())) {\n+    return ttir::ScaleDotElemType::E2M1;\n+  }\n+  return absl::InvalidArgumentError(\n+      absl::StrCat(\"Unsupported type: \", llvm_ir::DumpToString(type)));\n+}\n+\n+absl::StatusOr<Value> ScaledDot(EmitterLocOpBuilder b,\n+                                ScaledDotOperands& operands) {\n+  TF_ASSIGN_OR_RETURN(auto lhs_dot_elem_type,\n+                      GetScaleDotElemType(operands.lhs.getType()));\n+  TF_ASSIGN_OR_RETURN(auto rhs_dot_elem_type,\n+                      GetScaleDotElemType(operands.rhs.getType()));\n+\n+  auto lhs_scale = Bitcast(b, operands.lhs_scale, b.getI8Type());\n+  auto rhs_scale = Bitcast(b, operands.rhs_scale, b.getI8Type());\n+\n+  // make type with the same shape as the scale but with i8 type\n+  return b.create<ttir::DotScaledOp>(\n+      operands.accumulator.getType(), operands.lhs, operands.rhs,\n+      operands.accumulator, lhs_scale, rhs_scale, lhs_dot_elem_type,\n+      rhs_dot_elem_type, true);\n+}\n+\n Value IEEEDot(EmitterLocOpBuilder b, Value lhs, Value rhs, Value acc) {\n   return b.create<ttir::DotOp>(lhs, rhs, acc,\n                                /*inputPrecision=*/ttir::InputPrecision::IEEE,\n@@ -488,6 +522,12 @@ absl::StatusOr<Value> EmitSingleTileDot(EmitterLocOpBuilder b,\n   return result;\n }\n \n+absl::StatusOr<Value> EmitSingleTileScaledDot(\n+    EmitterLocOpBuilder b, const HloScaledDotInstruction& scaled_dot,\n+    ScaledDotOperands dot_operands) {\n+  return ScaledDot(b, dot_operands);\n+}\n+\n }  // namespace triton\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "211fe354c924ac774594d275c48eb419ded1cf06",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms.h",
            "status": "modified",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.h?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include \"mlir/IR/Value.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n \n namespace xla {\n namespace gpu {\n@@ -34,6 +35,16 @@ struct DotOperands {\n   ::mlir::Value accumulator;\n };\n \n+// Carries named `Value`s corresponding to `scaled-dot` operands. This includes\n+// an accumulator and their respective scaling factors.\n+struct ScaledDotOperands {\n+  ::mlir::Value lhs;\n+  ::mlir::Value lhs_scale;\n+  ::mlir::Value rhs;\n+  ::mlir::Value rhs_scale;\n+  ::mlir::Value accumulator;\n+};\n+\n // Returns the type to use for accumulation for the given `dot` instruction.\n // This also handles the case where the algorithm is `ALG_UNSET`.\n absl::StatusOr<::mlir::Type> GetDotAccumulatorType(\n@@ -46,6 +57,19 @@ absl::StatusOr<::mlir::Value> EmitSingleTileDot(EmitterLocOpBuilder b,\n                                                 const HloDotInstruction& dot,\n                                                 DotOperands dot_operands);\n \n+// Emits a single-tile scaled-dot, considering the given `scaled-dot`\n+// instruction's operand precisions. Raises an `InvalidArgumentError` if the\n+// operand types are not supported.\n+absl::StatusOr<::mlir::Value> EmitSingleTileScaledDot(\n+    EmitterLocOpBuilder b, const HloScaledDotInstruction& scaled_dot,\n+    ScaledDotOperands dot_operands);\n+\n+namespace internal {\n+absl::StatusOr<mlir::triton::ScaleDotElemType> GetScaleDotElemType(\n+    mlir::Type value);\n+\n+}  // namespace internal\n+\n }  // namespace triton\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "6cf6c1d36649fb3c308a2496c2d94be966fb7e75",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -106,6 +106,10 @@ absl::StatusOr<Type> TritonType(EmitterLocOpBuilder& b, PrimitiveType t) {\n       return b.getType<mlir::Float8E5M2Type>();\n     case F8E4M3FN:\n       return b.getType<mlir::Float8E4M3FNType>();\n+    case F8E8M0FNU:\n+      return b.getType<mlir::Float8E8M0FNUType>();\n+    case F4E2M1FN:\n+      return b.getType<mlir::Float4E2M1FNType>();\n     default:\n       return absl::UnimplementedError(\n           absl::StrCat(\"This type is not supported yet: \",\n@@ -114,6 +118,7 @@ absl::StatusOr<Type> TritonType(EmitterLocOpBuilder& b, PrimitiveType t) {\n }\n \n absl::StatusOr<PrimitiveType> GetPrimitiveType(Type t) {\n+  // NOLINTBEGIN(google-readability-braces-around-statements)\n   if (t.isF64()) return F64;\n   if (t.isF32()) return F32;\n   if (t.isF16()) return F16;\n@@ -126,6 +131,8 @@ absl::StatusOr<PrimitiveType> GetPrimitiveType(Type t) {\n   if (t.isInteger(1)) return PRED;\n   if (mlir::isa<mlir::Float8E5M2Type>(t)) return F8E5M2;\n   if (mlir::isa<mlir::Float8E4M3FNType>(t)) return F8E4M3FN;\n+  if (mlir::isa<mlir::Float8E8M0FNUType>(t)) return F8E8M0FNU;\n+  // NOLINTEND(google-readability-braces-around-statements)\n   return absl::UnimplementedError(\"Unsupported type in getPrimitiveType.\\n\");\n }\n \n@@ -502,4 +509,10 @@ absl::StatusOr<ScalarOrTensor> EmitConstant(EmitterLocOpBuilder& b,\n   return CreateConst(b, ty, ScalarConstantValue<double>(constant, F64), shape);\n }\n \n+Value Bitcast(EmitterLocOpBuilder& b, Value value, Type type) {\n+  auto value_type = value.getType();\n+  value_type = mlir::dyn_cast<ShapedType>(value_type).clone(type);\n+  return b.create<mlir::arith::BitcastOp>(value_type, value);\n+}\n+\n }  // namespace xla::gpu::triton"
        },
        {
            "sha": "45ca2c2acdbb77a45d5969c654bdf1513b441f19",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -211,6 +211,8 @@ absl::StatusOr<mlir::Value> EmitElementwise(\n     const se::DeviceDescription& device_info, const HloInstruction& hlo,\n     mlir::ValueRange inputs);\n \n+mlir::Value Bitcast(EmitterLocOpBuilder& b, mlir::Value value, mlir::Type type);\n+\n }  // namespace xla::gpu::triton\n \n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_EMITTER_HELPERS_H_"
        },
        {
            "sha": "024b8e7fefa05b9f7b18af7be710aee75d9d930e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -103,7 +103,8 @@ TritonFusion::GenerateTritonKernelAndWrapper(\n   TritonWrapperResult triton_wrapper_result;\n \n   if (fusion_kind == kTritonFusionKind ||\n-      fusion_kind == kTritonNestedGemmFusionKind) {\n+      fusion_kind == kTritonNestedGemmFusionKind ||\n+      fusion_kind == kTritonScaledDotFusionKind) {\n     std::optional<LaunchConfig> launch_config = this->launch_config();\n     if (!launch_config.has_value()) {\n       return absl::InvalidArgumentError(absl::StrCat(\n@@ -175,7 +176,8 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n \n     LaunchDimensions launch_dimensions;\n     if (fusion_kind == kTritonFusionKind ||\n-        fusion_kind == kTritonNestedGemmFusionKind) {\n+        fusion_kind == kTritonNestedGemmFusionKind ||\n+        fusion_kind == kTritonScaledDotFusionKind) {\n       std::optional<LaunchConfig> launch_config = this->launch_config();\n       // This check should be enforced by `GenerateTritonKernelWrapper`.\n       CHECK(launch_config.has_value());"
        },
        {
            "sha": "aaab980ec45285e5f54ca13e5b81cace5efa2cdf",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 174,
            "deletions": 14,
            "changes": 188,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -172,6 +172,10 @@ using ::xla::gpu::triton::TritonType;\n \n namespace {\n \n+Value MakeIndex(EmitterLocOpBuilder& b, int64_t value) {\n+  return b.create<arith::ConstantIndexOp>(value);\n+}\n+\n // Emit a value as Index clamped to [lower, upper].\n Value EmitClampedIndex(EmitterLocOpBuilder b, Value value, int64_t lower,\n                        int64_t upper) {\n@@ -757,8 +761,7 @@ absl::StatusOr<int64_t> GetDotLoopIterationCount(\n   // - size from the shape of the operand\n   // - tile size from the tiling of the nested fusion root\n   // using the contracting dimension from the dot instruction.\n-  const HloDotInstruction& dot =\n-      *::xla::Cast<HloDotInstruction>(tiled_dot.hlo());\n+  const HloInstruction& dot = *tiled_dot.hlo();\n   const auto& dims = dot.dot_dimension_numbers();\n   if (dims.lhs_contracting_dimensions_size() != 1) {\n     return absl::UnimplementedError(\n@@ -866,11 +869,11 @@ absl::StatusOr<Value> MaskDotOperand(EmitterLocOpBuilder b,\n // Returns `shape` without all its unit dimensions, as well as the index of the\n // remaining dimensions in the original `shape`.\n std::pair<SmallVector<int64_t>, SmallVector<int64_t>> CollapseUnitDims(\n-    llvm::ArrayRef<int64_t> shape) {\n+    llvm::ArrayRef<int64_t> shape, bool scaled_dot = false) {\n   SmallVector<int64_t> shape_without_unit_dims;\n   SmallVector<int64_t> non_unit_dims_indices;\n   for (auto [i, size] : llvm::enumerate(shape)) {\n-    if (size != 1) {\n+    if (size != 1 || scaled_dot) {\n       shape_without_unit_dims.push_back(size);\n       non_unit_dims_indices.push_back(i);\n     }\n@@ -889,11 +892,12 @@ enum class DotOperandSide { kLhs, kRhs };\n absl::StatusOr<Value> CanonicalizeDotOperand(EmitterLocOpBuilder b,\n                                              Value operand,\n                                              int64_t contracting_dim_idx,\n-                                             DotOperandSide side) {\n+                                             DotOperandSide side,\n+                                             bool scaled_dot = false) {\n   llvm::ArrayRef<int64_t> shape =\n       mlir::cast<ShapedType>(operand.getType()).getShape();\n   auto [shape_without_unit_dims, non_unit_dims_indices] =\n-      CollapseUnitDims(shape);\n+      CollapseUnitDims(shape, scaled_dot);\n \n   if (shape_without_unit_dims.size() != 2) {\n     return absl::FailedPreconditionError(\n@@ -992,14 +996,12 @@ absl::StatusOr<ScalarOrTensor> EmitDot(\n       CreateConst(b, accumulator_type, 0.0f, padded_tile_sizes_no_unit_dims)\n           .UnwrapTensor();\n \n-  auto cindex = [&](int64_t value) -> Value {\n-    return b.create<arith::ConstantIndexOp>(value);\n-  };\n   TF_ASSIGN_OR_RETURN(int64_t loop_iteration_count,\n                       GetDotLoopIterationCount(tiled_hlo_dot));\n   auto for_op = b.create<mlir::scf::ForOp>(\n-      /*lowerBound=*/cindex(0), /*upperBound=*/cindex(loop_iteration_count),\n-      /*step=*/cindex(1), SmallVector<Value>{accumulator});\n+      /*lowerBound=*/MakeIndex(b, 0),\n+      /*upperBound=*/MakeIndex(b, loop_iteration_count),\n+      /*step=*/MakeIndex(b, 1), ValueRange{accumulator});\n   {  // Loop body.\n     mlir::OpBuilder::InsertionGuard g(b);\n     b.setInsertionPointToStart(for_op.getBody());\n@@ -1089,6 +1091,150 @@ absl::StatusOr<ScalarOrTensor> EmitDot(\n   return ScalarOrTensor(result);\n }\n \n+absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n+    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n+    const se::DeviceDescription& device_info,\n+    const HloFusionInstruction* fusion,\n+    const TiledHloInstruction& tiled_hlo_dot, mlir::FunctionOpInterface fn,\n+    Value pid,\n+    absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values) {\n+  VLOG(2) << \"EmitScaledDot: \" << tiled_hlo_dot.ToString();\n+  const HloScaledDotInstruction& scaled_dot =\n+      *::xla::Cast<HloScaledDotInstruction>(tiled_hlo_dot.hlo());\n+  if (!absl::c_all_of(tiled_hlo_dot.operands(),\n+                      [](const TiledHloInstruction* operand) {\n+                        return operand->hlo()->opcode() == HloOpcode::kFusion;\n+                      })) {\n+    return absl::FailedPreconditionError(\"Expected dot operands to be fusions\");\n+  }\n+\n+  SmallVector<int64_t> padded_tile_sizes =\n+      GetPaddedTileSizes(tiled_hlo_dot.tile_sizes());\n+\n+  // Sanity check: Triton historically did not support non-2D dots (and still\n+  // doesn't support arbitrary nD dots), so we require that the dot is tiled\n+  // with exactly two non-unit tile sizes. This anyway matches the hardware's\n+  // expectations, so seems like a reasonable requirement.\n+  // TODO(b/393299275): this needs to be enforced in tiling.\n+  if (padded_tile_sizes.size() != 2) {\n+    return absl::FailedPreconditionError(\n+        \"Expected dot to be tiled with exactly two non-unit tile sizes\");\n+  }\n+\n+  Type accumulator_type = b.getF32Type();\n+  Value accumulator =\n+      CreateConst(b, accumulator_type, 0.0f, padded_tile_sizes).UnwrapTensor();\n+\n+  TF_ASSIGN_OR_RETURN(int64_t loop_iteration_count,\n+                      GetDotLoopIterationCount(tiled_hlo_dot));\n+  auto for_op = b.create<mlir::scf::ForOp>(\n+      /*lowerBound=*/MakeIndex(b, 0),\n+      /*upperBound=*/MakeIndex(b, loop_iteration_count),\n+      /*step=*/MakeIndex(b, 1), SmallVector<Value>{accumulator});\n+  {  // Loop body.\n+    mlir::OpBuilder::InsertionGuard g(b);\n+    b.setInsertionPointToStart(for_op.getBody());\n+    SmallVector<TensorValue> dot_args;\n+    Value ki = for_op.getInductionVar();\n+    // Nested fusions are tiled with indexing map\n+    // (pid * loop_iteration_count_value + loop index) -> ....\n+    auto pid_dim = b.getAffineDimExpr(0);\n+    auto ki_symbol = b.getAffineSymbolExpr(0);\n+    IndexingMap computation_index_map{\n+        AffineMap::get(1, 1, {pid_dim * loop_iteration_count + ki_symbol}),\n+        {IndexingMap::Variable{\n+            tiled_hlo_dot.tile_offsets_indexing()->GetDimensionBound(0),\n+            \"pid\"}},\n+        {IndexingMap::Variable{{0, loop_iteration_count - 1}, \"k\"}},\n+        /*rt_vars=*/{}};\n+\n+    Value computation_index = b.create<xla::ApplyIndexingOp>(\n+                                   ValueRange{pid, ki}, computation_index_map)\n+                                  .getResult(0);\n+    for (const TiledHloInstruction* operand : tiled_hlo_dot.operands()) {\n+      VLOG(3) << \"Emitting scaled dot operand: \" << operand->ToString();\n+      const TiledHloFusionInstruction* tiled_fusion_operand =\n+          static_cast<const TiledHloFusionInstruction*>(operand);\n+      TF_ASSIGN_OR_RETURN(\n+          std::vector<ScalarOrTensor> result,\n+          EmitTiledComputation(\n+              b, libdevice_path, device_info,\n+              ::xla::Cast<HloFusionInstruction>(tiled_fusion_operand->hlo()),\n+              *tiled_fusion_operand->called_computation(), fn,\n+              computation_index, values));\n+      if (result.size() != 1) {\n+        return absl::InternalError(absl::StrCat(\n+            \"Expected nested fusion computation to emit a single value, got \",\n+            result.size()));\n+      }\n+      dot_args.push_back(result.front().UnwrapTensor());\n+    }\n+    Value acc = for_op.getRegionIterArgs().front();\n+    int64_t lhs_contracting_dim_idx =\n+        scaled_dot.dot_dimension_numbers().lhs_contracting_dimensions(0);\n+\n+    int64_t rhs_contracting_dim_idx =\n+        scaled_dot.dot_dimension_numbers().rhs_contracting_dimensions(0);\n+\n+    // TODO(b/393299275): masking is only necessary during the last iteration of\n+    // the loop. We should evaluate whether adding a conditional mask helps or\n+    // hinders performance for Triton.\n+    Value ki_i32 = Cast(b, ki, b.getI32Type());\n+    TF_ASSIGN_OR_RETURN(\n+        Value lhs, MaskDotOperand(b, *tiled_hlo_dot.operand(0), dot_args[0],\n+                                  ki_i32, lhs_contracting_dim_idx));\n+    TF_ASSIGN_OR_RETURN(\n+        Value lhs_scale,\n+        MaskDotOperand(b, *tiled_hlo_dot.operand(1), dot_args[1], ki_i32,\n+                       lhs_contracting_dim_idx));\n+\n+    TF_ASSIGN_OR_RETURN(\n+        Value rhs, MaskDotOperand(b, *tiled_hlo_dot.operand(2), dot_args[2],\n+                                  ki_i32, rhs_contracting_dim_idx));\n+\n+    TF_ASSIGN_OR_RETURN(\n+        Value rhs_scale,\n+        MaskDotOperand(b, *tiled_hlo_dot.operand(3), dot_args[3], ki_i32,\n+                       rhs_contracting_dim_idx));\n+\n+    // Canonicalize the dot operands to match Triton's/the hardware's\n+    // expectations.\n+    TF_ASSIGN_OR_RETURN(\n+        lhs, CanonicalizeDotOperand(b, lhs, lhs_contracting_dim_idx,\n+                                    DotOperandSide::kLhs, /*scaled_dot=*/true));\n+    TF_ASSIGN_OR_RETURN(\n+        lhs_scale,\n+        CanonicalizeDotOperand(b, lhs_scale, lhs_contracting_dim_idx,\n+                               DotOperandSide::kLhs, /*scaled_dot=*/true));\n+    TF_ASSIGN_OR_RETURN(\n+        rhs, CanonicalizeDotOperand(b, rhs, rhs_contracting_dim_idx,\n+                                    DotOperandSide::kRhs, /*scaled_dot=*/true));\n+    TF_ASSIGN_OR_RETURN(\n+        rhs_scale,\n+        CanonicalizeDotOperand(b, rhs_scale, rhs_contracting_dim_idx,\n+                               DotOperandSide::kRhs, /*scaled_dot=*/true));\n+\n+    TF_ASSIGN_OR_RETURN(\n+        Value acc_next,\n+        triton::EmitSingleTileScaledDot(\n+            b, scaled_dot,\n+            triton::ScaledDotOperands{lhs, lhs_scale, rhs, rhs_scale, acc}));\n+    b.create<mlir::scf::YieldOp>(acc_next);\n+  }\n+\n+  // The output of the loop may not match the expected output type of the dot.\n+  // We make sure to issue a conversion if necessary.\n+  TF_ASSIGN_OR_RETURN(Type dot_output_type,\n+                      TritonType(b, scaled_dot.shape().element_type()));\n+\n+  Value result = for_op.getResult(0);\n+  if (dot_output_type != accumulator_type) {\n+    result = Cast(b, result, dot_output_type);\n+  }\n+\n+  return ScalarOrTensor(result);\n+}\n+\n absl::StatusOr<ScalarOrTensor> EmitConcatenate(\n     EmitterLocOpBuilder b, absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info,\n@@ -1328,6 +1474,11 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n                    values);\n   }\n \n+  if (hlo->opcode() == HloOpcode::kScaledDot) {\n+    return EmitScaledDot(b, libdevice_path, device_info, fusion, tiled_hlo, fn,\n+                         pid, values);\n+  }\n+\n   if (hlo->opcode() == HloOpcode::kConstant) {\n     if (ShapeUtil::IsEffectiveScalar(hlo->shape())) {\n       return EmitConstant(b, *hlo);\n@@ -1412,6 +1563,12 @@ absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n     // Skip generating nested fusions, they are emitted by their consumer.\n     if (hlo->parent()->IsFusionComputation() &&\n         hlo->opcode() == HloOpcode::kFusion) {\n+      if (hlo->GetModule()\n+              ->config()\n+              .debug_options()\n+              .xla_gpu_experimental_scaled_dot_with_triton()) {\n+        continue;\n+      }\n       CodegenDecision decision = IsTritonSupportedInstruction(\n           *hlo, device_info.gpu_compute_capability());\n       if (!decision.CanFuse()) {\n@@ -1515,7 +1672,8 @@ absl::StatusOr<Tiling> TilingFromAnnotatedFusion(\n   for (const auto& [hlo, num_tiling_parameters] :\n        symbolic_tile_analysis.GetTilingSpecification().parameter_mapping()) {\n     // TODO(b/419026602): handle reductions.\n-    if (hlo->opcode() == HloOpcode::kDot) {\n+    if (hlo->opcode() == HloOpcode::kDot ||\n+        hlo->opcode() == HloOpcode::kScaledDot) {\n       const HloInstruction* lhs = hlo->operand(0);\n       // When encountering a `dot`, we always expect its operands to be nests.\n       auto backend_config = lhs->backend_config<GpuBackendConfig>();\n@@ -1858,7 +2016,8 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n     TF_RETURN_IF_ERROR(EmitMatMul(b, libdevice_path, device_info, fusion, fn,\n                                   block_level_parameters));\n   } else if (fusion_kind == kTritonFusionKind ||\n-             fusion_kind == kTritonNestedGemmFusionKind) {\n+             fusion_kind == kTritonNestedGemmFusionKind ||\n+             fusion_kind == kTritonScaledDotFusionKind) {\n     TF_RETURN_IF_ERROR(EmitGeneric(b, libdevice_path, device_info, fusion, fn,\n                                    block_level_parameters));\n   } else {\n@@ -2030,7 +2189,8 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n   }\n \n   pm.addPass(mlir::triton::xla::CreateTritonXLAExtractInsertToTritonPass(\n-      device_info, block_level_parameters.is_tma_allowed));\n+      block_level_parameters.is_tma_allowed &&\n+      stream_executor::gpu::IsTmaAvailableForDevice(device_info)));\n \n   // Lower affine expressions into arithmetic ops.\n   pm.addPass(mlir::createLowerAffinePass());"
        },
        {
            "sha": "c3a13687f5644e16b85e08abcdc7bec1a1f37e37",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_port_test.cc",
            "status": "modified",
            "additions": 189,
            "deletions": 1,
            "changes": 190,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include <cstdlib>\n #include <memory>\n+#include <ostream>\n #include <string>\n #include <utility>\n #include <variant>\n@@ -23,8 +24,10 @@ limitations under the License.\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_replace.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/strings/substitute.h\"\n #include \"llvm/IR/LLVMContext.h\"\n@@ -3229,7 +3232,192 @@ ENTRY e {\n ; CHECK-SAME: __triton_nested_gemm_fusion\n   )\");\n }\n-\n }  // namespace\n+\n+struct ScaleDotTestParams {\n+  std::string lhs_type;\n+  std::string lhs_scale_type;\n+  std::string rhs_type;\n+  std::string rhs_scale_type;\n+  std::string output_type;\n+  std::string expected_triton_type;\n+\n+  std::string PrepareHloText(absl::string_view hlo_template) const {\n+    return absl::StrReplaceAll(hlo_template,\n+                               {{\"$lhs_type\", lhs_type},\n+                                {\"$lhs_scale_type\", lhs_scale_type},\n+                                {\"$rhs_type\", rhs_type},\n+                                {\"$rhs_scale_type\", rhs_scale_type},\n+                                {\"$output_type\", output_type}});\n+  }\n+  static std::string ToString(\n+      const ::testing::TestParamInfo<ScaleDotTestParams>& info) {\n+    const ScaleDotTestParams& params = info.param;\n+    auto name = absl::StrCat(params.lhs_type, \"_\", params.lhs_scale_type, \"_\",\n+                             params.rhs_type, \"_\", params.rhs_scale_type, \"_\",\n+                             params.output_type);\n+    absl::StrReplaceAll({{\"[\", \"_\"}, {\"]\", \"_\"}, {\",\", \"x\"}}, &name);\n+    return name;\n+  }\n+};\n+\n+std::ostream& operator<<(std::ostream& stream, const ScaleDotTestParams& tc) {\n+  return stream << \"{\\n\\tlhs_type:\" << tc.lhs_type\n+                << \",\\n\\tlhs_scale_type:\" << tc.lhs_scale_type\n+                << \",\\n\\trhs_type:\" << tc.rhs_type\n+                << \",\\n\\trhs_scale_type:\" << tc.rhs_scale_type\n+                << \",\\n\\toutput_type:\" << tc.output_type << \"\\n}\";\n+}\n+\n+class TritonScaledDotGemmTest\n+    : public TritonGemmTest,\n+      public ::testing::WithParamInterface<ScaleDotTestParams> {};\n+\n+TEST_P(TritonScaledDotGemmTest, Fp8ScaledDotDoesNotCrash) {\n+  const ScaleDotTestParams& params = GetParam();\n+  constexpr absl::string_view kHloTextTemplate = R\"hlo(\n+HloModule m\n+flhs (p0: $lhs_type) -> $lhs_type {\n+  ROOT p0 = $lhs_type{1,0} parameter(0)\n+}\n+flhs_scale (p0: $lhs_scale_type) -> $lhs_scale_type {\n+  ROOT p0 = $lhs_scale_type{1,0} parameter(0)\n+}\n+frhs (p0: $rhs_type) -> $rhs_type {\n+  ROOT p0 = $rhs_type{1,0} parameter(0)\n+}\n+frhs_scale (p0: $rhs_scale_type) -> $rhs_scale_type {\n+  ROOT p0 = $rhs_scale_type{1,0} parameter(0)\n+}\n+\n+triton_dot {\n+  lhs = $lhs_type parameter(0)\n+  lhs1 = $lhs_type{1,0} fusion(lhs),\n+    kind=kCustom,\n+    calls=flhs,\n+    backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"16\",\"32\"]}],\n+          \"num_warps\":\"1\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\",\n+        }\n+      }\n+    }\n+  lhs_scale = $lhs_scale_type parameter(1)\n+  lhs_scale1 = $lhs_scale_type{1,0} fusion(lhs_scale),\n+    kind=kCustom,\n+    calls=flhs_scale,\n+    backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"16\",\"1\"]}],\n+          \"num_warps\":\"1\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\",\n+        }\n+      }\n+    }\n+  rhs = $rhs_type parameter(2)\n+  rhs1 = $rhs_type{1,0} fusion(rhs),\n+    kind=kCustom,\n+    calls=frhs,\n+    backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"16\",\"32\"]}],\n+          \"num_warps\":\"1\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\",\n+        }\n+      }\n+    }\n+  rhs_scale = $rhs_scale_type parameter(3)\n+  rhs_scale1 = $rhs_scale_type{1,0} fusion(rhs_scale),\n+    kind=kCustom,\n+    calls=frhs_scale,\n+    backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"16\",\"1\"]}],\n+          \"num_warps\":\"1\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\",\n+        }\n+      }\n+    }\n+  ROOT _ = $output_type{1,0} scaled-dot(lhs1, lhs_scale1, rhs1, rhs_scale1),\n+    lhs_contracting_dims={1},\n+    rhs_contracting_dims={0}\n+}\n+\n+ENTRY e {\n+  p0 = $lhs_type{1,0} parameter(0)\n+  p1 = $lhs_scale_type{1,0} parameter(1)\n+  p2 = $rhs_type{1,0} parameter(2)\n+  p3 = $rhs_scale_type{1,0} parameter(3)\n+  ROOT _ = $output_type{1,0} fusion(p0, p1, p2, p3),\n+    kind=kCustom,\n+    calls=triton_dot,\n+    backend_config={\n+      \"fusion_backend_config\": {\n+        kind: \"__triton_scaled_dot_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"16\", \"16\"]}],\n+          \"num_warps\":\"1\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\"\n+        }\n+      }\n+    }\n+}\n+)hlo\";\n+\n+  auto hlo_text = params.PrepareHloText(kHloTextTemplate);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_text));\n+\n+  auto debug_options = module->config().debug_options();\n+  debug_options.set_xla_gpu_experimental_scaled_dot_with_triton(true);\n+  module->mutable_config().set_debug_options(debug_options);\n+\n+  constexpr absl::string_view kExpectedTritonIrTmpl = R\"(\n+      CHECK: tt.dot_scaled\n+      CHECK: tensor<16x32x$triton_type>, tensor<16x1xi8>\n+      CHECK: tensor<32x16x$triton_type>, tensor<1x16xi8>\n+      CHECK: -> tensor<16x16xf32>\n+  )\";\n+  auto expected_triton_ir = absl::StrReplaceAll(\n+      kExpectedTritonIrTmpl, {{\"$triton_type\", params.expected_triton_type}});\n+  EXPECT_THAT(\n+      CreateTritonIrAndFileCheck(*module->GetComputationWithName(\"triton_dot\"),\n+                                 /*block_level_parameters=*/\n+                                 {\n+                                     {{16, 16}},\n+                                     1,\n+                                     1,\n+                                     1,\n+                                     true,\n+                                 },\n+                                 expected_triton_ir),\n+      absl_testing::IsOk());\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    TritonScaledDotGemmTest, TritonScaledDotGemmTest,\n+    ::testing::Values(ScaleDotTestParams{\"f8e4m3fn[64,512]\", \"f8e8m0fnu[64,16]\",\n+                                         \"f8e4m3fn[512,64]\", \"f8e8m0fnu[16,64]\",\n+                                         \"f32[64,64]\", \"f8E4M3FN\"},\n+                      ScaleDotTestParams{\"f8e5m2[64,512]\", \"f8e8m0fnu[64,16]\",\n+                                         \"f8e5m2[512,64]\", \"f8e8m0fnu[16,64]\",\n+                                         \"f32[64,64]\", \"f8E5M2\"}),\n+    ScaleDotTestParams::ToString);\n+\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "788f5812481a48b7349623522842c9f20a814b2a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -31,8 +31,7 @@ namespace mlir::triton::xla {\n \n std::unique_ptr<mlir::Pass> CreateTritonXLAExtractInsertToTritonPass();\n std::unique_ptr<mlir::Pass> CreateTritonXLAExtractInsertToTritonPass(\n-    const stream_executor::DeviceDescription& device_description,\n-    bool tma_enabled);\n+    bool allow_tma);\n std::unique_ptr<mlir::Pass> CreateTritonXLASqueezeDimsPass();\n std::unique_ptr<mlir::Pass> CreateTritonXLAFoldTransposePass();\n std::unique_ptr<mlir::Pass> CreateGeneralizeKernelSignaturePass();"
        },
        {
            "sha": "a9206709deaf6bafb6689cb73e94b2f4a82eac43",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.td",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -25,6 +25,10 @@ def TritonXLAExtractInsertToTritonPass : Pass<\"triton-xla-extract-insert-to-trit\n     Triton ops. It also rewrites `func` args to `tt.ptr` types and removes\n     function return args.\n   }];\n+  let options = [\n+    Option<\"allow_tma_\", \"allow_tma\", \"bool\", \"false\",\n+           \"Whether to permit lowering to TMA.\">,\n+  ];\n   let dependentDialects = [\n     \"triton::TritonDialect\",\n     \"::xla::XlaDialect\""
        },
        {
            "sha": "9d2649bf63751420252e8330fa47f6a1d178fa5b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_extract_insert_to_triton.mlir",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_extract_insert_to_triton.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_extract_insert_to_triton.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_extract_insert_to_triton.mlir?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -1,9 +1,9 @@\n // RUN: xla-opt %s -split-input-file \\\n-// RUN: -triton-xla-extract-insert-to-triton=\"gpu_device_info='cuda_compute_capability {major: 6}' tma_enabled=0\" \\\n+// RUN: -triton-xla-extract-insert-to-triton \\\n // RUN: | FileCheck %s\n \n // RUN: xla-opt %s -split-input-file \\\n-// RUN: -triton-xla-extract-insert-to-triton=\"gpu_device_info='cuda_compute_capability {major: 9}' tma_enabled=1\" \\\n+// RUN: -triton-xla-extract-insert-to-triton=allow_tma=1 \\\n // RUN: | FileCheck %s --check-prefix=CHECK-TMA\n \n func.func @lower_extract_insert(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {"
        },
        {
            "sha": "95b713354f971541c342ec4794518fb9eee80df8",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_extract_insert_to_triton_pass.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 80,
            "changes": 96,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -34,7 +34,6 @@ limitations under the License.\n #include \"llvm/ADT/ArrayRef.h\"\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n-#include \"llvm/Support/CommandLine.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n@@ -59,7 +58,6 @@ limitations under the License.\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/codegen/emitters/ir/xla_ops.h\"\n #include \"xla/permutation_util.h\"\n-#include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n \n namespace mlir::triton::xla {\n@@ -150,15 +148,12 @@ bool IsOffsetDivisibilityGuaranteed(mlir::Value offset_val,\n //      minor tile dimension (in bytes) must be divisible by 16, it is\n //      sufficient to check that the offset in the minor dimension (in bytes) is\n //      divisible by 16.\n-bool CanUseTma(bool tma_enabled,\n-               const stream_executor::DeviceDescription& device_description,\n-               const ArrayRef<int64_t>& original_shape,\n+bool CanUseTma(bool allow_tma, const ArrayRef<int64_t>& original_shape,\n                const ArrayRef<int64_t>& tile_shape,\n                const ArrayRef<int64_t>& tile_strides, ValueRange offsets,\n                const TypedValue<PointerType>& pointer,\n                const ArrayRef<int64_t>& minor_to_major_layout) {\n-  if (!tma_enabled ||\n-      !stream_executor::gpu::IsTmaAvailableForDevice(device_description)) {\n+  if (!allow_tma) {\n     return false;\n   }\n \n@@ -500,12 +495,8 @@ static std::pair<Value, Value> CreateTensorOfPointersAndMask(\n \n class RewriteExtract : public mlir::OpRewritePattern<ExtractOp> {\n  public:\n-  RewriteExtract(mlir::MLIRContext* context,\n-                 const stream_executor::DeviceDescription* device_description,\n-                 bool tma_enabled)\n-      : OpRewritePattern(context),\n-        device_description_(device_description),\n-        tma_enabled_(tma_enabled) {}\n+  RewriteExtract(mlir::MLIRContext* context, bool allow_tma)\n+      : OpRewritePattern(context), allow_tma_(allow_tma) {}\n   using OpRewritePattern::OpRewritePattern;\n \n  private:\n@@ -532,8 +523,8 @@ class RewriteExtract : public mlir::OpRewritePattern<ExtractOp> {\n     auto sizes = op.getStaticSizes();\n     auto strides = to_vector(op.getStaticStrides());\n \n-    if (CanUseTma(tma_enabled_, *device_description_, src_shape, sizes, strides,\n-                  offsets, op.getSrc(), src_layout)) {\n+    if (CanUseTma(allow_tma_, src_shape, sizes, strides, offsets, op.getSrc(),\n+                  src_layout)) {\n       if (auto result = CanonicalizeTileStrides(strides, sizes, src_shape);\n           !result.ok()) {\n         return rewriter.notifyMatchFailure(op, result.message());\n@@ -594,18 +585,13 @@ class RewriteExtract : public mlir::OpRewritePattern<ExtractOp> {\n     return mlir::success();\n   }\n \n-  const stream_executor::DeviceDescription* device_description_;\n-  const bool tma_enabled_;\n+  const bool allow_tma_;\n };\n \n class RewriteInsert : public mlir::OpRewritePattern<InsertOp> {\n  public:\n-  RewriteInsert(mlir::MLIRContext* context,\n-                const stream_executor::DeviceDescription* device_description,\n-                bool tma_enabled)\n-      : OpRewritePattern(context),\n-        device_description_(device_description),\n-        tma_enabled_(tma_enabled) {}\n+  RewriteInsert(mlir::MLIRContext* context, bool allow_tma)\n+      : OpRewritePattern(context), allow_tma_(allow_tma) {}\n   using OpRewritePattern::OpRewritePattern;\n \n  private:\n@@ -640,8 +626,8 @@ class RewriteInsert : public mlir::OpRewritePattern<InsertOp> {\n     SmallVector<unsigned> reduced_dims = to_vector(*reduction_mask);\n     absl::c_sort(reduced_dims);\n \n-    if (CanUseTma(tma_enabled_, *device_description_, dst_shape, sizes, strides,\n-                  offsets, op.getDst(), dst_layout)) {\n+    if (CanUseTma(allow_tma_, dst_shape, sizes, strides, offsets, op.getDst(),\n+                  dst_layout)) {\n       if (auto result = CanonicalizeTileStrides(strides, sizes, dst_shape);\n           !result.ok()) {\n         return rewriter.notifyMatchFailure(op, result.message());\n@@ -685,8 +671,7 @@ class RewriteInsert : public mlir::OpRewritePattern<InsertOp> {\n     return mlir::success();\n   }\n \n-  const stream_executor::DeviceDescription* device_description_;\n-  const bool tma_enabled_;\n+  const bool allow_tma_;\n };\n \n // Rewriting tensor::InsertOp as tt.store.\n@@ -737,58 +722,18 @@ class RewriteScalarExtract : public mlir::OpRewritePattern<tensor::ExtractOp> {\n   }\n };\n \n-class DeviceDescriptionParser\n-    : public llvm::cl::parser<stream_executor::DeviceDescription> {\n- public:\n-  using parser::parser;\n-\n-  bool parse(llvm::cl::Option& option, StringRef arg_name, StringRef arg_value,\n-             stream_executor::DeviceDescription& value) {\n-    if (arg_value.empty()) {\n-      value = stream_executor::DeviceDescription();\n-      return false;\n-    }\n-    stream_executor::GpuDeviceInfoProto proto;\n-    if (!tsl::protobuf::TextFormat::ParseFromString(arg_value.str(), &proto)) {\n-      return option.error(\"failed to parse GpuDeviceInfoProto from string: \" +\n-                          arg_value);\n-    }\n-    absl::StatusOr<stream_executor::DeviceDescription> device_description =\n-        stream_executor::DeviceDescription::FromProto(proto);\n-    if (!device_description.ok()) {\n-      return option.error(device_description.status().message());\n-    }\n-    value = *device_description;\n-    return false;\n-  }\n-\n-  static void print(raw_ostream& os,\n-                    const stream_executor::DeviceDescription& value) {\n-    os << value.ToString();\n-  }\n-};\n-\n class TritonXLAExtractInsertToTritonPass\n     : public impl::TritonXLAExtractInsertToTritonPassBase<\n           TritonXLAExtractInsertToTritonPass> {\n  public:\n   using Base::Base;\n-  TritonXLAExtractInsertToTritonPass(\n-      const TritonXLAExtractInsertToTritonPass& other)\n-      : Base(other) {}\n-  explicit TritonXLAExtractInsertToTritonPass(\n-      const stream_executor::DeviceDescription& device_description,\n-      bool tma_enabled) {\n-    device_description_ = device_description;\n-    tma_enabled_ = tma_enabled;\n-  }\n \n  private:\n   void runOnOperation() override {\n     mlir::MLIRContext* mlir_context = &getContext();\n     mlir::RewritePatternSet patterns(mlir_context);\n-    patterns.add<RewriteExtract, RewriteInsert>(\n-        mlir_context, &device_description_.getValue(), tma_enabled_.getValue());\n+    patterns.add<RewriteExtract, RewriteInsert>(mlir_context,\n+                                                allow_tma_.getValue());\n     patterns.add<RewriteScalarExtract, RewriteScalarInsert>(mlir_context);\n     if (mlir::failed(\n             mlir::applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n@@ -802,14 +747,6 @@ class TritonXLAExtractInsertToTritonPass\n       return signalPassFailure();\n     }\n   }\n-\n-  Option<stream_executor::DeviceDescription, DeviceDescriptionParser>\n-      device_description_{\n-          *this, \"gpu_device_info\",\n-          ::llvm::cl::desc(\"Serialized stream_executor::GPUDeviceInfo proto\")};\n-  Option<bool> tma_enabled_{*this, \"tma_enabled\",\n-                            ::llvm::cl::desc(\"Flag to enable/disable TMA\"),\n-                            ::llvm::cl::init(false)};\n };\n \n }  // namespace\n@@ -819,10 +756,9 @@ std::unique_ptr<mlir::Pass> CreateTritonXLAExtractInsertToTritonPass() {\n }\n \n std::unique_ptr<mlir::Pass> CreateTritonXLAExtractInsertToTritonPass(\n-    const stream_executor::DeviceDescription& device_description,\n-    bool tma_enabled) {\n+    bool allow_tma) {\n   return std::make_unique<TritonXLAExtractInsertToTritonPass>(\n-      device_description, tma_enabled);\n+      TritonXLAExtractInsertToTritonPassOptions{allow_tma});\n }\n \n }  // namespace mlir::triton::xla"
        },
        {
            "sha": "9467c5f996019bf1da399e8a5988349d0483c0e9",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 14,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -13,6 +13,8 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include \"xla/backends/gpu/runtime/ragged_all_to_all.h\"\n+\n #include <algorithm>\n #include <array>\n #include <cstddef>\n@@ -89,10 +91,6 @@ absl::Status RunRaggedAllToAllKernel(\n \n   se::StreamExecutor* executor = stream->parent();\n   static constexpr size_t kThreads = 128;\n-  static constexpr size_t kMaxBlocksPerUpdate = 1024;\n-\n-  // blockIdx.x is the index of the update.\n-  int64_t num_blocks_x = num_updates_per_output * num_outputs;\n \n   int64_t num_vectorized_row_elements = num_row_elements;\n   int64_t vector_size_bytes = xla::primitive_util::BitWidth(element_type) / 8;\n@@ -102,18 +100,25 @@ absl::Status RunRaggedAllToAllKernel(\n     vector_size_bytes *= 2;\n   }\n \n-  // blockIdx.y and threadIdx.x are used to iterate over the elements of the\n-  // update. Since the size of each update is not known at compile time, the\n-  // kernel assumes the worst case of `num_input_rows * num_row_elements`\n-  // elements per update and uses a loop up to `send_size * num_row_elements` to\n-  // terminate early.\n-  size_t num_blocks_y =\n-      std::min(CeilOfRatio<size_t>(num_input_rows * num_vectorized_row_elements,\n-                                   kThreads),\n-               kMaxBlocksPerUpdate);\n+  int64_t num_updates_per_block = 1;\n+  int64_t num_block_clusters = num_updates_per_output;\n+\n+  // Decide how many updates should each block process. In the kernel, N blocks\n+  // process N updates. This is done to reduce imbalance in data transfer per\n+  // block if updates happen to be unevenly distributed. The numbers were\n+  // chosen empirically in Sep 2025 and can change in the future.\n+  const int64_t max_num_updates_per_block =\n+      std::min<int64_t>(CeilOfRatio<int64_t>(num_input_rows, 16), 64);\n+\n+  while (num_updates_per_block < max_num_updates_per_block &&\n+         num_block_clusters % 2 == 0) {\n+    num_block_clusters /= 2;\n+    num_updates_per_block *= 2;\n+  }\n \n   se::ThreadDim thread_dims(kThreads, 1, 1);\n-  se::BlockDim block_dims(num_blocks_x, num_blocks_y, 1);\n+  se::BlockDim block_dims(num_outputs, num_block_clusters,\n+                          num_updates_per_block);\n \n   std::array<void*, stream_executor::gpu::kMaxNumRaggedAllToAllOutputPtrs>\n       output_ptrs;"
        },
        {
            "sha": "315fd1b885f7638644907795851212ab56348190",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -450,6 +450,7 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_gpu_experimental_enable_command_buffer_on_thunks(true);\n   opts.set_xla_detect_unstable_reductions(\n       DebugOptions::UNSTABLE_REDUCTION_DETECTION_MODE_NONE);\n+  opts.set_xla_gpu_experimental_scaled_dot_with_triton(false);\n   return opts;\n }\n "
        },
        {
            "sha": "b731d194f7b838d7f85411cda7d76c0d5505881c",
            "filename": "third_party/xla/xla/hlo/ir/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2FBUILD?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -291,8 +291,10 @@ cc_library(\n     hdrs = [\"hlo_instruction_utils.h\"],\n     deps = [\n         \":hlo\",\n+        \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings\",\n     ],\n )"
        },
        {
            "sha": "842e1540767920ca4fdd70752e5713ff3e78728b",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instruction_utils.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction_utils.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -21,10 +21,12 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_join.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/primitive_util.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla {\n@@ -37,6 +39,17 @@ bool IsUnstridedSlice(const HloInstruction* hlo) {\n                         [](int64_t stride) { return stride == 1; });\n }\n \n+bool KeepsBitwidth(const HloInstruction& hlo) {\n+  CHECK(hlo.shape().IsArray());\n+  if (absl::c_any_of(hlo.operands(), [&](const HloInstruction* operand) {\n+        return primitive_util::BitWidth(operand->shape().element_type()) !=\n+               primitive_util::BitWidth(hlo.shape().element_type());\n+      })) {\n+    return false;\n+  }\n+  return true;\n+}\n+\n using Interval = std::pair<int64_t, int64_t>;\n void AddOrUpdateVectorOfPairsAsAttribute(HloInstruction* instr,\n                                          std::string attr_name,"
        },
        {
            "sha": "c4cd55784778c111708c674c7fd17c3757e0adee",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instruction_utils.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction_utils.h?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -29,6 +29,9 @@ namespace hlo_instruction_utils {\n // all dimensions.\n bool IsUnstridedSlice(const HloInstruction* hlo);\n \n+// Checks that all instruction operands have the same bitwidth as its output.\n+bool KeepsBitwidth(const HloInstruction&);\n+\n // Adds or updates the attributes for an instruction. If the attribute is\n // already present, then it is overwritten. Otherwise, this is added as another\n // attribute."
        },
        {
            "sha": "fb7fc571ee2dd3da237b1dc7535493185f7daeff",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instruction_utils_test.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction_utils_test.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -56,6 +56,20 @@ TEST_F(HloInstructionUtilsTest, TestIsUnstridedSlice) {\n   EXPECT_FALSE(IsUnstridedSlice(strided_slice));\n }\n \n+TEST_F(HloInstructionUtilsTest, KeepsBitwidth) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> m,\n+                          ParseAndReturnVerifiedModule(R\"(\n+e {\n+  a = s8[2] parameter(0)\n+  b = s16[] bitcast(a)\n+  c = s16[] add(b, b)\n+})\"));\n+  const HloInstruction& root = *m->entry_computation()->root_instruction();\n+  EXPECT_TRUE(KeepsBitwidth(root));\n+  EXPECT_FALSE(KeepsBitwidth(*root.operand(0)));\n+  EXPECT_TRUE(KeepsBitwidth(*root.operand(0)->operand(0)));\n+}\n+\n TEST_F(HloInstructionUtilsTest, TestAddOrUpdateVectorOfPairsAsAttribute) {\n   const char* hlo = R\"(\n     HloModule test"
        },
        {
            "sha": "563439030d00ef231aa388fbcf1b7bc50408e776",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/BUILD",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2FBUILD?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -1706,6 +1706,38 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"slice_hoister\",\n+    srcs = [\"slice_hoister.cc\"],\n+    hdrs = [\"slice_hoister.h\"],\n+    deps = [\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/pass:hlo_pass\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"slice_hoister_test\",\n+    srcs = [\"slice_hoister_test.cc\"],\n+    deps = [\n+        \":slice_hoister\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/hlo/testlib:pattern_matcher_gmock\",\n+        \"//xla/service:hlo_cse\",\n+        \"//xla/service:pattern_matcher\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"unflatten_call_graph\",\n     srcs = [\"unflatten_call_graph.cc\"],"
        },
        {
            "sha": "c8307e04ec9dbc3670bdda3c2075db0915c2b436",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/slice_hoister.cc",
            "status": "added",
            "additions": 134,
            "deletions": 0,
            "changes": 134,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fslice_hoister.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fslice_hoister.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fslice_hoister.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -0,0 +1,134 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/hlo/transforms/simplifiers/slice_hoister.h\"\n+\n+#include <vector>\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+\n+namespace {\n+\n+// Helper function to attempt hoisting a slice through an add operation.\n+// Returns true if a change was made.\n+absl::StatusOr<bool> TryHoistSliceThroughAdd(HloInstruction* instruction,\n+                                             HloComputation* computation) {\n+  if (instruction->opcode() != HloOpcode::kSlice) {\n+    return false;\n+  }\n+  HloInstruction* operand = instruction->mutable_operand(0);\n+  if (operand->opcode() != HloOpcode::kAdd) {\n+    return false;\n+  }\n+\n+  HloInstruction* add = operand;\n+  HloInstruction* lhs = add->mutable_operand(0);\n+  HloInstruction* rhs = add->mutable_operand(1);\n+\n+  if (lhs->shape() != rhs->shape()) {\n+    VLOG(1) << \" Operand shapes do not match: \" << lhs->shape() << \" and \"\n+            << rhs->shape();\n+    return false;\n+  }\n+  if (lhs->shape().element_type() != instruction->shape().element_type()) {\n+    VLOG(1) << \" Slice element type does not match operand element type: \"\n+            << lhs->shape().element_type() << \" and \"\n+            << instruction->shape().element_type();\n+    return false;\n+  }\n+  if (instruction->shape().element_type() != add->shape().element_type()) {\n+    VLOG(1) << \" Slice element type does not match add element type: \"\n+            << instruction->shape().element_type() << \" and \"\n+            << add->shape().element_type();\n+    return false;\n+  }\n+\n+  // All checks passed, perform the hoisting.\n+  HloInstruction* lhs_slice =\n+      computation->AddInstruction(HloInstruction::CreateSlice(\n+          instruction->shape(), lhs, instruction->slice_starts(),\n+          instruction->slice_limits(), instruction->slice_strides()));\n+  HloInstruction* rhs_slice =\n+      computation->AddInstruction(HloInstruction::CreateSlice(\n+          instruction->shape(), rhs, instruction->slice_starts(),\n+          instruction->slice_limits(), instruction->slice_strides()));\n+  TF_RETURN_IF_ERROR(computation->ReplaceWithNewInstruction(\n+      instruction,\n+      HloInstruction::CreateBinary(instruction->shape(), HloOpcode::kAdd,\n+                                   lhs_slice, rhs_slice)));\n+  return true;\n+}\n+\n+// As slices reduce the size of the input, it can be beneficial to hoist\n+// slices as high in the graph as possible, ideally right after parameter\n+// reads, which could reduce both compute and memory costs.\n+//\n+// Currently, this pass hoists slice operations through add operations.\n+// Note that this pass can create redundant slices, which can be removed by\n+// running CSE.\n+//\n+// Note that algebraic simplifier also has `HandleSlice` function.\n+absl::StatusOr<bool> HoistSliceOperations(HloComputation* computation) {\n+  bool changed = false;\n+  bool changed_on_last_iteration = false;\n+  // TODO(b/434724820): Generalize to element-wise operations.\n+  // TODO(b/434724820): Consider also other operations like broadcast, reduce,\n+  // transpose, etc.\n+  // TODO(b/434724820): Make this more efficient by e.g. using a worklist or a\n+  // topological sort.\n+  do {\n+    changed |= changed_on_last_iteration;\n+    changed_on_last_iteration = false;\n+    std::vector<HloInstruction*> instructions =\n+        computation->MakeInstructionPostOrder();\n+    for (HloInstruction* instruction : instructions) {\n+      TF_ASSIGN_OR_RETURN(bool instruction_changed,\n+                          TryHoistSliceThroughAdd(instruction, computation));\n+      if (instruction_changed) {\n+        changed_on_last_iteration = true;\n+        break;\n+      }\n+    }\n+  } while (changed_on_last_iteration);\n+\n+  return changed;\n+}\n+}  // anonymous namespace\n+\n+absl::StatusOr<bool> SliceHoister::Run(\n+    HloModule* module,\n+    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n+  bool changed = false;\n+  for (HloComputation* computation :\n+       module->MakeNonfusionComputations(execution_threads)) {\n+    TF_ASSIGN_OR_RETURN(bool changed_computation,\n+                        HoistSliceOperations(computation));\n+    changed |= changed_computation;\n+  }\n+  return changed;\n+}\n+\n+}  // namespace xla"
        },
        {
            "sha": "f66aeee1f7685e03577da2a6fd6c0371ac15bc7c",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/slice_hoister.h",
            "status": "added",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fslice_hoister.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fslice_hoister.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fslice_hoister.h?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -0,0 +1,41 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_HLO_TRANSFORMS_SIMPLIFIERS_SLICE_HOISTER_H_\n+#define XLA_HLO_TRANSFORMS_SIMPLIFIERS_SLICE_HOISTER_H_\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/pass/hlo_pass_interface.h\"\n+\n+namespace xla {\n+\n+// An HLO pass that hoists slice operations through add operations.\n+class SliceHoister : public HloModulePass {\n+ public:\n+  SliceHoister() = default;\n+\n+  absl::string_view name() const override { return \"slice-hoister\"; }\n+  using HloPassInterface::Run;\n+  absl::StatusOr<bool> Run(\n+      HloModule* module,\n+      const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n+};\n+\n+}  // namespace xla\n+\n+#endif  // XLA_HLO_TRANSFORMS_SIMPLIFIERS_SLICE_HOISTER_H_"
        },
        {
            "sha": "49a39a47569e120e59d0c50e7b38176a098fd104",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/slice_hoister_test.cc",
            "status": "added",
            "additions": 184,
            "deletions": 0,
            "changes": 184,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fslice_hoister_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fslice_hoister_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fslice_hoister_test.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -0,0 +1,184 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/hlo/transforms/simplifiers/slice_hoister.h\"\n+\n+#include <memory>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/hlo/testlib/pattern_matcher_gmock.h\"\n+#include \"xla/service/hlo_cse.h\"\n+#include \"xla/service/pattern_matcher.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+namespace {\n+\n+namespace m = xla::match;\n+using ::testing::ElementsAre;\n+\n+class SliceHoisterTest : public HloHardwareIndependentTestBase {\n+ public:\n+  SliceHoisterTest()\n+      : HloHardwareIndependentTestBase(\n+            /*verifier_layout_sensitive=*/false,\n+            /*allow_mixed_precision_in_hlo_verifier=*/true) {};\n+};\n+\n+TEST_F(SliceHoisterTest, HoistSliceThroughAdd) {\n+  absl::string_view module_str = R\"(\n+    HloModule module\n+    ENTRY main {\n+      add_op = f32[8,9] add(f32[8,9] parameter(0), f32[8,9] parameter(1))\n+      ROOT slice_op = f32[2,9] slice(f32[8,9] add_op), slice={[0:2], [0:9]}\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(module_str));\n+\n+  SliceHoister slice_hoister;\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed,\n+                          RunHloPass(&slice_hoister, module.get()));\n+\n+  SCOPED_TRACE(module->ToString());\n+  EXPECT_TRUE(changed);\n+  HloInstruction* root_instruction =\n+      module->entry_computation()->root_instruction();\n+  const HloInstruction* param_0_slice = nullptr;\n+  const HloInstruction* param_1_slice = nullptr;\n+  EXPECT_THAT(root_instruction,\n+              GmockMatch(m::Add(m::Slice(&param_0_slice, m::Parameter(0)),\n+                                m::Slice(&param_1_slice, m::Parameter(1)))));\n+  EXPECT_THAT(param_0_slice->slice_starts(), ElementsAre(0, 0));\n+  EXPECT_THAT(param_0_slice->slice_limits(), ElementsAre(2, 9));\n+  EXPECT_THAT(param_0_slice->slice_strides(), ElementsAre(1, 1));\n+  EXPECT_THAT(param_1_slice->slice_starts(), ElementsAre(0, 0));\n+  EXPECT_THAT(param_1_slice->slice_limits(), ElementsAre(2, 9));\n+  EXPECT_THAT(param_1_slice->slice_strides(), ElementsAre(1, 1));\n+}\n+\n+TEST_F(SliceHoisterTest, HoistSliceThroughMultipleAdds) {\n+  absl::string_view module_str = R\"(\n+    HloModule module\n+    ENTRY main {\n+      param_0 = f32[8,9] parameter(0)\n+      param_1 = f32[8,9] parameter(1)\n+      add_op_1 = f32[8,9] add(f32[8,9] param_0, f32[8,9] param_1)\n+      add_op_2 = f32[8,9] add(f32[8,9] add_op_1, f32[8,9] param_1)\n+      ROOT slice_op = f32[2,9] slice(f32[8,9] add_op_2), slice={[0:2], [0:9]}\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(module_str));\n+\n+  SliceHoister slice_hoister;\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed,\n+                          RunHloPass(&slice_hoister, module.get()));\n+\n+  SCOPED_TRACE(module->ToString());\n+  EXPECT_TRUE(changed);\n+\n+  HloCSE cse = HloCSE(false);\n+  TF_ASSERT_OK_AND_ASSIGN(changed, RunHloPass(&cse, module.get()));\n+\n+  SCOPED_TRACE(module->ToString());\n+  EXPECT_TRUE(changed);\n+\n+  HloInstruction* root_instruction =\n+      module->entry_computation()->root_instruction();\n+  const HloInstruction* param_0_slice = nullptr;\n+  const HloInstruction* param_1_first_slice = nullptr;\n+  const HloInstruction* param_1_second_slice = nullptr;\n+  EXPECT_THAT(\n+      root_instruction,\n+      GmockMatch(m::Add(m::Add(m::Slice(&param_0_slice, m::Parameter(0)),\n+                               m::Op(&param_1_first_slice)),\n+                        m::Op(&param_1_second_slice))));\n+  // The slice of param_1 should be evaluated only once and reused.\n+  EXPECT_EQ(param_1_first_slice, param_1_second_slice);\n+  EXPECT_THAT(param_1_first_slice, GmockMatch(m::Slice(m::Parameter(1))));\n+  EXPECT_THAT(param_0_slice->slice_starts(), ElementsAre(0, 0));\n+  EXPECT_THAT(param_0_slice->slice_limits(), ElementsAre(2, 9));\n+  EXPECT_THAT(param_0_slice->slice_strides(), ElementsAre(1, 1));\n+  EXPECT_THAT(param_1_first_slice->slice_starts(), ElementsAre(0, 0));\n+  EXPECT_THAT(param_1_first_slice->slice_limits(), ElementsAre(2, 9));\n+  EXPECT_THAT(param_1_first_slice->slice_strides(), ElementsAre(1, 1));\n+}\n+\n+TEST_F(SliceHoisterTest, DoesNotHoistSliceThroughAddIfElementTypesDoNotMatch) {\n+  absl::string_view module_str = R\"(\n+    HloModule module\n+    ENTRY main {\n+      add_op = f32[8,9] add(f16[8,9] parameter(0), f32[8,9] parameter(1))\n+      ROOT slice_op = f32[2,9] slice(f32[8,9] add_op), slice={[0:2], [0:9]}\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(module_str));\n+\n+  SliceHoister slice_hoister;\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed,\n+                          RunHloPass(&slice_hoister, module.get()));\n+\n+  SCOPED_TRACE(module->ToString());\n+  EXPECT_FALSE(changed);\n+}\n+\n+TEST_F(SliceHoisterTest,\n+       DoesNotHoistSliceThroughAddIfAddTypeDoesNotMatchSliceType) {\n+  absl::string_view module_str = R\"(\n+    HloModule module\n+    ENTRY main {\n+      add_op = f32[8,9] add(f32[8,9] parameter(0), f32[8,9] parameter(1))\n+      ROOT slice_op = f16[2,9] slice(f32[8,9] add_op), slice={[0:2], [0:9]}\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(module_str));\n+\n+  SliceHoister slice_hoister;\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed,\n+                          RunHloPass(&slice_hoister, module.get()));\n+\n+  SCOPED_TRACE(module->ToString());\n+  EXPECT_FALSE(changed);\n+}\n+\n+TEST_F(SliceHoisterTest,\n+       DoesNotHoistSliceThroughAddIfAddTypeDoesNotMatchOperandsType) {\n+  absl::string_view module_str = R\"(\n+    HloModule module\n+    ENTRY main {\n+      add_op = f32[8,9] add(f16[8,9] parameter(0), f16[8,9] parameter(1))\n+      ROOT slice_op = f32[2,9] slice(f32[8,9] add_op), slice={[0:2], [0:9]}\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(module_str));\n+\n+  SliceHoister slice_hoister;\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed,\n+                          RunHloPass(&slice_hoister, module.get()));\n+\n+  SCOPED_TRACE(module->ToString());\n+  EXPECT_FALSE(changed);\n+}\n+}  // anonymous namespace\n+}  // namespace xla"
        },
        {
            "sha": "c3c4cbbf29ae3dc32c5c2c8f4397451744cc4b4b",
            "filename": "third_party/xla/xla/pjrt/pjrt_future.h",
            "status": "modified",
            "additions": 28,
            "deletions": 4,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_future.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_future.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_future.h?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -470,6 +470,17 @@ class PjRtFuture : public internal::PjRtFutureBase<absl::StatusOr<T>> {\n     friend class PjRtFuture;\n   };\n \n+  // This is a temporary class to support migration from CreatePromise() to\n+  // MakePromise() and an end goal of making Promise move-only type.\n+  class MoveOnlyPromise : public Promise {\n+   public:\n+    using Promise::Promise;\n+    using Promise::Set;\n+\n+    MoveOnlyPromise(MoveOnlyPromise&&) = default;\n+    MoveOnlyPromise& operator=(MoveOnlyPromise&&) = default;\n+  };\n+\n   // Returns a Promise that can be used to construct a PjRtFuture, and then Set\n   // later.\n   static Promise CreatePromise() {\n@@ -478,8 +489,9 @@ class PjRtFuture : public internal::PjRtFutureBase<absl::StatusOr<T>> {\n \n   // Returns a pair of connected Promise and PjRtFuture<T>. Setting the returned\n   // promise will fulfill the connected future.\n-  static std::pair<Promise, PjRtFuture<T>> MakePromise() {\n-    Promise promise(tsl::MakeUnconstructedAsyncValueRef<absl::StatusOr<T>>());\n+  static std::pair<MoveOnlyPromise, PjRtFuture<T>> MakePromise() {\n+    MoveOnlyPromise promise(\n+        tsl::MakeUnconstructedAsyncValueRef<absl::StatusOr<T>>());\n     PjRtFuture<T> future(promise);\n     return std::make_pair(std::move(promise), std::move(future));\n   }\n@@ -728,6 +740,17 @@ class PjRtFuture<void> : public internal::PjRtFutureBase<absl::Status> {\n     friend class PjRtFuture<void>;\n   };\n \n+  // This is a temporary class to support migration from CreatePromise() to\n+  // MakePromise() and an end goal of making Promise move-only type.\n+  class MoveOnlyPromise : public Promise {\n+   public:\n+    using Promise::Promise;\n+    using Promise::Set;\n+\n+    MoveOnlyPromise(MoveOnlyPromise&&) = default;\n+    MoveOnlyPromise& operator=(MoveOnlyPromise&&) = default;\n+  };\n+\n   // Returns a Promise that can be used to construct a PjRtFuture, and then Set\n   // later.\n   static Promise CreatePromise() {\n@@ -736,8 +759,9 @@ class PjRtFuture<void> : public internal::PjRtFutureBase<absl::Status> {\n \n   // Returns a pair of connected Promise and PjRtFuture<>. Setting the returned\n   // promise will fulfill the connected future.\n-  static std::pair<Promise, PjRtFuture<>> MakePromise() {\n-    Promise promise(tsl::MakeUnconstructedAsyncValueRef<absl::Status>());\n+  static std::pair<MoveOnlyPromise, PjRtFuture<>> MakePromise() {\n+    MoveOnlyPromise promise(\n+        tsl::MakeUnconstructedAsyncValueRef<absl::Status>());\n     PjRtFuture<> future(promise);\n     return std::make_pair(std::move(promise), std::move(future));\n   }"
        },
        {
            "sha": "d5848250be34c4a7ad1137de966b360eca2064b5",
            "filename": "third_party/xla/xla/pjrt/pjrt_future_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_future_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_future_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_future_test.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -176,13 +176,6 @@ TEST(PjRtFutureTest, PromiseIsUnique) {\n   // else, and the promise becomes unique.\n   promise.Set();\n   EXPECT_TRUE(promise.IsUniqueReference());\n-\n-  {  // Making a copy of the promise makes it not unique.\n-    auto copy = promise;\n-    EXPECT_FALSE(promise.IsUniqueReference());\n-    EXPECT_FALSE(copy.IsUniqueReference());\n-  }\n-  EXPECT_TRUE(promise.IsUniqueReference());\n }\n \n TEST(PjRtFutureTest, MapCopyableFuture) {"
        },
        {
            "sha": "585b31ccaeb2a9003bcdcf04174b19f3658a4199",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 51,
            "deletions": 0,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -172,6 +172,7 @@ xla_test(\n     srcs = [\"custom_call_test.cc\"],\n     backends = [\"gpu\"],\n     local_defines = if_cuda_is_configured([\"GOOGLE_CUDA=1\"]),\n+    tags = [\"no-oneapi\"],  # TODO(intel-tf): Remove it when macro substitutions for SYCL are available in xla/stream_executor/sycl/*.\n     deps = [\n         \"//xla:debug_options_flags\",\n         \"//xla:shape_util\",\n@@ -2577,6 +2578,7 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/hlo/analysis:hlo_dataflow_analysis\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/ir:hlo_instruction_utils\",\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service:instruction_fusion\",\n         \"//xla/stream_executor:device_description\",\n@@ -3141,3 +3143,52 @@ cc_library(\n         \"@local_tsl//tsl/profiler/lib:traceme_encode\",\n     ],\n )\n+\n+cc_library(\n+    name = \"intel_gpu_compiler\",\n+    srcs = [\n+        \"intel_gpu_compiler_registration.cc\",\n+    ],\n+    tags = [\n+        \"gpu\",\n+        \"oneapi-only\",\n+    ],\n+    deps = [\n+        \":intel_gpu_compiler_impl\",\n+        \"//xla/service:compiler\",\n+        \"//xla/stream_executor/sycl:sycl_platform_id\",\n+    ],\n+    alwayslink = True,  # Contains compiler registration\n+)\n+\n+cc_library(\n+    name = \"intel_gpu_compiler_impl\",\n+    srcs = [\n+        \"intel_gpu_compiler.cc\",\n+    ],\n+    hdrs = [\n+        \"intel_gpu_compiler.h\",\n+    ],\n+    tags = [\n+        \"gpu\",\n+        \"oneapi-only\",\n+    ],\n+    deps = [\n+        \":gpu_compiler\",\n+    ],\n+)\n+\n+xla_test(\n+    name = \"intel_gpu_compiler_test\",\n+    srcs = [\"intel_gpu_compiler_test.cc\"],\n+    backends = [\"gpu\"],\n+    tags = [\n+        \"gpu\",\n+        \"oneapi-only\",\n+    ],\n+    deps = [\n+        \":intel_gpu_compiler\",\n+        \"//xla/tests:hlo_test_base\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)"
        },
        {
            "sha": "f14023ac5444df5237766866a33a8193ca802149",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -259,6 +259,7 @@ xla_test(\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/time\",\n@@ -383,6 +384,7 @@ xla_cc_test(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/parser:hlo_parser\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor:device_description_proto_cc\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\","
        },
        {
            "sha": "f6fbe1c9eff5d201781e028f26806f4a631dcade",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotune_cache_key_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key_test.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_clone_context.h\"\n #include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/device_description.pb.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\""
        },
        {
            "sha": "4278b9d63f561cc9f9ca04f785d417abac3e00d7",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -39,6 +39,7 @@ limitations under the License.\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n+#include \"xla/xla.pb.h\"\n \n namespace xla {\n namespace gpu {"
        },
        {
            "sha": "455d493bf5769c898c7babf71f9bcb40d2995d0a",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\""
        },
        {
            "sha": "0c07533c3469e40b69c376f6ec81f7c42185e63c",
            "filename": "third_party/xla/xla/service/gpu/gpu_fusible.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -33,6 +33,7 @@ limitations under the License.\n #include \"xla/hlo/analysis/hlo_dataflow_analysis.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instruction_utils.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/permutation_util.h\"\n@@ -391,6 +392,7 @@ bool IsUniversallyLoopFusible(const HloInstruction& instr) {\n       return instr.fusion_kind() == HloInstruction::FusionKind::kLoop;\n \n     case HloOpcode::kBitcast:\n+      return hlo_instruction_utils::KeepsBitwidth(instr);\n     case HloOpcode::kBroadcast:\n     case HloOpcode::kConcatenate:\n     case HloOpcode::kDynamicSlice:\n@@ -425,18 +427,6 @@ bool IsLoopFusibleAsProducer(const HloInstruction& instr) {\n   }\n }\n \n-static bool AllSatisfy(const HloInstruction& instr,\n-                       const HloPredicate& predicate) {\n-  if (instr.opcode() != HloOpcode::kFusion) {\n-    return predicate(&instr);\n-  }\n-\n-  return absl::c_all_of(\n-      instr.fused_instructions(), [&](const HloInstruction* i) {\n-        return i->opcode() == HloOpcode::kParameter || predicate(i);\n-      });\n-}\n-\n FusionDecision CanEmitInputFusedScatter(const HloInstruction& producer,\n                                         const HloInstruction& consumer) {\n   if (IsInputFusibleScatter(producer)) {"
        },
        {
            "sha": "c9c4c85b7af1017c4f2025cd61ddabb16b1fef50",
            "filename": "third_party/xla/xla/service/gpu/gpu_fusible_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible_test.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -1281,6 +1281,17 @@ TEST_F(GpuFusibleTest, ProducerConsumerFusionInPlaceOperation) {\n   EXPECT_TRUE(ShapesCompatibleForMultiOutputFusion(*dus, *transpose));\n }\n \n+TEST_F(GpuFusibleTest, BitwidthChangingBitcastIsNotFusible) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+e {\n+  a = s32[7,2]{1,0} parameter(0)\n+  b = s16[7]{0} bitcast(a)\n+})\")\n+                    .value();\n+  EXPECT_FALSE(IsProducerMultiOutputFusible(\n+      *module->entry_computation()->root_instruction()));\n+}\n+\n TEST_F(GpuFusibleTest, ChooseFusionKind) {\n   auto module = ParseAndReturnVerifiedModule(R\"(\n HloModule module"
        },
        {
            "sha": "f1a772db6aef3d8623d65f1ea45a35cbb3a55341",
            "filename": "third_party/xla/xla/service/gpu/hlo_fusion_analysis.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -113,7 +113,8 @@ HloFusionAnalysis::EmitterFusionKind GetEmitterFusionKind(\n \n   if (fusion_backend_config.kind() == kTritonFusionKind ||\n       fusion_backend_config.kind() == kTritonGemmFusionKind ||\n-      fusion_backend_config.kind() == kTritonNestedGemmFusionKind) {\n+      fusion_backend_config.kind() == kTritonNestedGemmFusionKind ||\n+      fusion_backend_config.kind() == kTritonScaledDotFusionKind) {\n     return HloFusionAnalysis::EmitterFusionKind::kTriton;\n   }\n "
        },
        {
            "sha": "1666428c06765dc3cebc4af66bfa8e2404855741",
            "filename": "third_party/xla/xla/service/gpu/intel_gpu_compiler.cc",
            "status": "added",
            "additions": 52,
            "deletions": 0,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fintel_gpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fintel_gpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fintel_gpu_compiler.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -0,0 +1,52 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/service/gpu/intel_gpu_compiler.h\"\n+\n+#include \"xla/service/gpu/target_constants.h\"\n+#include \"xla/stream_executor/sycl/sycl_platform_id.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+IntelGpuCompiler::IntelGpuCompiler()\n+    : GpuCompiler(stream_executor::sycl::kSyclPlatformId, spir::TargetTriple(),\n+                  spir::DataLayout()) {}\n+\n+absl::Status IntelGpuCompiler::OptimizeHloConvolutionCanonicalization(\n+    HloModule* hlo_module, se::GpuComputeCapability gpu_version,\n+    se::dnn::VersionInfo dnn_version,\n+    const se::SemanticVersion& toolkit_version) {\n+  // Note: this is a stub.\n+  return absl::OkStatus();\n+}\n+\n+absl::StatusOr<GpuCompiler::BackendCompileResult>\n+IntelGpuCompiler::CompileTargetBinary(\n+    const HloModuleConfig& module_config, llvm::Module* llvm_module,\n+    const stream_executor::DeviceDescription& device_description,\n+    bool relocatable, const HloModule* debug_module,\n+    const CompileOptions& options, std::optional<int> shard_number) {\n+  // Note: this is a stub.\n+  return BackendCompileResult{};\n+}\n+\n+std::vector<std::string> IntelGpuCompiler::GetLLVMCommandLineOptions(\n+    const DebugOptions& debug_options) const {\n+  return {};\n+}\n+\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "11bc3eefc8184035ce0547acae7a1c2904f4f5e3",
            "filename": "third_party/xla/xla/service/gpu/intel_gpu_compiler.h",
            "status": "added",
            "additions": 59,
            "deletions": 0,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fintel_gpu_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fintel_gpu_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fintel_gpu_compiler.h?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -0,0 +1,59 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_SERVICE_GPU_INTEL_GPU_COMPILER_H_\n+#define XLA_SERVICE_GPU_INTEL_GPU_COMPILER_H_\n+\n+#include <string>\n+#include <vector>\n+\n+#include \"absl/status/statusor.h\"\n+#include \"llvm/IR/Module.h\"\n+#include \"xla/hlo/analysis/hlo_dataflow_analysis.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/service/gpu/gpu_compiler.h\"\n+#include \"xla/stream_executor/semantic_version.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+class IntelGpuCompiler : public GpuCompiler {\n+ public:\n+  IntelGpuCompiler();\n+\n+  absl::Status OptimizeHloConvolutionCanonicalization(\n+      HloModule* hlo_module, se::GpuComputeCapability gpu_version,\n+      se::dnn::VersionInfo dnn_version,\n+      const se::SemanticVersion& toolkit_version) override;\n+\n+  absl::StatusOr<BackendCompileResult> CompileTargetBinary(\n+      const HloModuleConfig& module_config, llvm::Module* llvm_module,\n+      const stream_executor::DeviceDescription& device_description,\n+      bool relocatable, const HloModule* debug_module,\n+      const CompileOptions& options, std::optional<int> shard_number) override;\n+\n+  std::vector<std::string> GetLLVMCommandLineOptions(\n+      const DebugOptions& debug_options) const override;\n+\n+ private:\n+  IntelGpuCompiler(const IntelGpuCompiler&) = delete;\n+  IntelGpuCompiler& operator=(const IntelGpuCompiler&) = delete;\n+};\n+\n+}  // namespace gpu\n+}  // namespace xla\n+\n+#endif  // XLA_SERVICE_GPU_INTEL_GPU_COMPILER_H_"
        },
        {
            "sha": "b765599237cc4675170de561fc9cc3a72cef0961",
            "filename": "third_party/xla/xla/service/gpu/intel_gpu_compiler_registration.cc",
            "status": "added",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fintel_gpu_compiler_registration.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fintel_gpu_compiler_registration.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fintel_gpu_compiler_registration.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -0,0 +1,25 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/service/gpu/intel_gpu_compiler.h\"\n+#include \"xla/stream_executor/sycl/sycl_platform_id.h\"\n+\n+static bool InitCompilerModule() {\n+  xla::Compiler::RegisterCompilerFactory(\n+      stream_executor::sycl::kSyclPlatformId,\n+      []() { return std::make_unique<xla::gpu::IntelGpuCompiler>(); });\n+  return true;\n+}\n+static bool compiler_module_initialized = InitCompilerModule();"
        },
        {
            "sha": "809121f35fe2695c851e0569f2ab366e86b27b50",
            "filename": "third_party/xla/xla/service/gpu/intel_gpu_compiler_test.cc",
            "status": "added",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fintel_gpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fintel_gpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fintel_gpu_compiler_test.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -0,0 +1,33 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <gtest/gtest.h>\n+#include \"xla/stream_executor/sycl/sycl_platform_id.h\"\n+#include \"xla/tests/hlo_test_base.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+class IntelGpuCompilerTest : public HloTestBase {};\n+\n+TEST_F(IntelGpuCompilerTest, CheckCompiler) {\n+  auto compiler = backend().compiler();\n+  EXPECT_EQ(compiler->PlatformId(), stream_executor::sycl::kSyclPlatformId);\n+}\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "7d87370ec462f57708d5792f8460ff0aa6281575",
            "filename": "third_party/xla/xla/service/gpu/tests/BUILD",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -78,6 +78,7 @@ xla_test(\n     backends = [\"gpu\"],\n     local_defines = if_cuda_is_configured([\"GOOGLE_CUDA=1\"]),\n     tags = [\n+        \"no-oneapi\",  # TODO(intel-tf): Enable this test for SYCL when IntelGpuCompiler is implemented.\n         \"notsan\",\n         \"test_migrated_to_hlo_runner_pjrt\",\n     ],  # TODO(b/345034145): Fix tsan error.\n@@ -655,15 +656,18 @@ lit_test_suite(\n #     srcs = [\"xla-opt.cc\"],\n #     deps = [\n #         \"@llvm-project//mlir:AllExtensions\",\n+#         \"@llvm-project//mlir:BuiltinToLLVMIRTranslation\",\n #         \"@llvm-project//mlir:FuncDialect\",\n+#         \"@llvm-project//mlir:FuncExtensions\",\n+#         \"@llvm-project//mlir:LLVMIRTransforms\",\n+#         \"@llvm-project//mlir:LLVMToLLVMIRTranslation\",\n #         \"@llvm-project//mlir:MlirOptLib\",\n #         \"@llvm-project//mlir:RegisterAllExtensions\",\n #         \"@llvm-project//mlir:TensorDialect\",\n #         \"//xla/backends/gpu/codegen/emitters/transforms:passes\",\n #         \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n #         \"//xla/backends/gpu/codegen/triton/transforms:passes\",\n-#         # Needed for xla_ops.h\n-#         \"//xla/codegen/emitters/ir:xla\",  # buildcleaner: keep\n+#         \"//xla/codegen/emitters/ir:xla\",\n #         \"//xla/codegen/emitters/transforms:passes\",\n #         \"@triton//:AllPassesAndDialects\",\n #         \"@triton//third_party/amd:TestAMDAnalysis\","
        },
        {
            "sha": "716249eba4420507a9834a88bfb2f293be96daa4",
            "filename": "third_party/xla/xla/service/gpu/tests/xla-opt.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fxla-opt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fxla-opt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fxla-opt.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -13,20 +13,28 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include \"mlir/Dialect/Func/Extensions/InlinerExtension.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/Dialect/LLVMIR/Transforms/InlinerInterfaceImpl.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/InitAllExtensions.h\"\n+#include \"mlir/Target/LLVMIR/Dialect/Builtin/BuiltinToLLVMIRTranslation.h\"\n+#include \"mlir/Target/LLVMIR/Dialect/LLVMIR/LLVMToLLVMIRTranslation.h\"\n #include \"mlir/Tools/mlir-opt/MlirOptMain.h\"\n #include \"xla/backends/gpu/codegen/emitters/transforms/passes.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n-#include \"xla/codegen/emitters/ir/xla_ops.h\"\n+#include \"xla/codegen/emitters/ir/xla_dialect.h\"\n #include \"xla/codegen/emitters/transforms/passes.h\"\n #include \"third_party/triton/bin/RegisterTritonDialects.h\"\n \n int main(int argc, char **argv) {\n   mlir::DialectRegistry registry;\n   mlir::registerAllExtensions(registry);\n+  registerBuiltinDialectTranslation(registry);\n+  registerLLVMDialectTranslation(registry);\n+  mlir::LLVM::registerInlinerInterface(registry);\n+  mlir::func::registerInlinerExtension(registry);\n   registerTritonDialects(registry);  // This registers all passes as well.\n   registry.insert<mlir::func::FuncDialect, mlir::tensor::TensorDialect,\n                   mlir::triton::xla::XlaTritonDialect, xla::XlaDialect>();"
        },
        {
            "sha": "b336093d04e619606d994fdfc08b8b438494d004",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -2111,6 +2111,7 @@ cc_library(\n         \"//xla/backends/gpu/codegen/triton:support\",\n         \"//xla/hlo/analysis:hlo_dfs_reachability\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/ir:hlo_instruction_utils\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service:dump\","
        },
        {
            "sha": "b52ad2d5721377e2676fbec83020dfc8c675ee1d",
            "filename": "third_party/xla/xla/service/gpu/transforms/priority_fusion.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -46,6 +46,7 @@ limitations under the License.\n #include \"xla/hlo/analysis/hlo_dfs_reachability.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instruction_utils.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_print_options.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n@@ -97,11 +98,12 @@ bool IsFusible(const HloInstruction& instr) {\n     case HloOpcode::kFusion:\n       return IsGenericTritonFusion(instr) ||\n              instr.fusion_kind() != HloInstruction::FusionKind::kCustom;\n+    case HloOpcode::kBitcast:\n+      return hlo_instruction_utils::KeepsBitwidth(instr);\n     case HloOpcode::kCopy:\n     case HloOpcode::kIota:\n     case HloOpcode::kConstant:\n     case HloOpcode::kReduce:\n-    case HloOpcode::kBitcast:\n     case HloOpcode::kBroadcast:\n     case HloOpcode::kConcatenate:\n     case HloOpcode::kDynamicSlice:"
        },
        {
            "sha": "f5d3b21d6f804286b172af708c34da876158a05a",
            "filename": "third_party/xla/xla/service/gpu/transforms/priority_fusion_test.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion_test.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -213,6 +213,20 @@ CHECK-NEXT: ROOT %{{.*}} = (f32[512]{0}, s32[512]{0}) tuple(%[[FUSION_F32]], %[[\n   )\");\n }\n \n+TEST_F(PriorityFusionTest, DoNotFuseBitWidthChangingBitcast) {\n+  EXPECT_TRUE(RunAndCheckHloRewrite(R\"(\n+e {\n+  a = s8[3,5,2]{2,1,0} parameter(0)\n+  n = s8[3,5,2]{2,1,0} negate(a)\n+  b = s16[3,5]{1,0} bitcast(n)\n+  m = s16[3,5]{1,0} multiply(b, b)\n+})\",\n+                                    std::move(priority_fusion_),\n+                                    /*expect_change=*/false)\n+                  .status()\n+                  .ok());\n+}\n+\n TEST_F(PriorityFusionTest, FuseConvertIntoReduce) {\n   absl::string_view kHlo = R\"(\n     HloModule test_module"
        },
        {
            "sha": "cdba67ae3b5a89f9dea397b7bb5e49f5127efd15",
            "filename": "third_party/xla/xla/service/hlo_verifier.cc",
            "status": "modified",
            "additions": 55,
            "deletions": 31,
            "changes": 86,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -2754,37 +2754,6 @@ bool IsOtherCollective(const HloInstruction* instruction) {\n   }\n }\n \n-absl::Status VerifyNoCollectiveDeadlocksRecursive(\n-    const HloComputation* computation, DfaState& current_state,\n-    absl::flat_hash_set<const HloSendInstruction*>& send_instructions,\n-    absl::flat_hash_set<const HloRecvInstruction*>& recv_instructions) {\n-  for (const HloInstruction* instruction : computation->instructions()) {\n-    if (instruction->called_computations().empty()) {\n-      if (instruction->opcode() == HloOpcode::kSend) {\n-        TF_RETURN_IF_ERROR(CheckDeadlocksForSend(\n-            DynCast<HloSendInstruction>(instruction), current_state,\n-            send_instructions, recv_instructions));\n-      } else if (instruction->opcode() == HloOpcode::kRecv) {\n-        TF_RETURN_IF_ERROR(CheckDeadlocksForRecv(\n-            DynCast<HloRecvInstruction>(instruction), current_state,\n-            send_instructions, recv_instructions));\n-      } else if (IsOtherCollective(instruction)) {\n-        TF_RETURN_IF_ERROR(CheckDeadlocksForOtherCollectives(\n-            instruction, current_state, send_instructions, recv_instructions));\n-      } else {\n-        continue;\n-      }\n-    } else {\n-      for (const HloComputation* computation :\n-           instruction->called_computations()) {\n-        TF_RETURN_IF_ERROR(VerifyNoCollectiveDeadlocksRecursive(\n-            computation, current_state, send_instructions, recv_instructions));\n-      }\n-    }\n-  }\n-  return absl::OkStatus();\n-}\n-\n absl::Status CheckPendingSendRecvDeadlocks(\n     absl::flat_hash_set<const HloSendInstruction*>& send_instructions,\n     absl::flat_hash_set<const HloRecvInstruction*>& recv_instructions) {\n@@ -2822,6 +2791,61 @@ absl::Status CheckPendingSendRecvDeadlocks(\n   return absl::OkStatus();\n }\n \n+absl::Status VerifyNoCollectiveDeadlocksRecursive(\n+    const HloComputation* computation, DfaState& current_state,\n+    absl::flat_hash_set<const HloSendInstruction*>& send_instructions,\n+    absl::flat_hash_set<const HloRecvInstruction*>& recv_instructions) {\n+  for (const HloInstruction* instruction : computation->instructions()) {\n+    if (instruction->called_computations().empty()) {\n+      if (instruction->opcode() == HloOpcode::kSend) {\n+        TF_RETURN_IF_ERROR(CheckDeadlocksForSend(\n+            DynCast<HloSendInstruction>(instruction), current_state,\n+            send_instructions, recv_instructions));\n+      } else if (instruction->opcode() == HloOpcode::kRecv) {\n+        TF_RETURN_IF_ERROR(CheckDeadlocksForRecv(\n+            DynCast<HloRecvInstruction>(instruction), current_state,\n+            send_instructions, recv_instructions));\n+      } else if (IsOtherCollective(instruction)) {\n+        TF_RETURN_IF_ERROR(CheckDeadlocksForOtherCollectives(\n+            instruction, current_state, send_instructions, recv_instructions));\n+      } else {\n+        continue;\n+      }\n+    } else {\n+      for (const HloComputation* computation :\n+           instruction->called_computations()) {\n+        // special handling for grouped multi-op async collectives\n+        if (computation->IsAsyncComputation() &&\n+            !computation->CanExpandIntoSingleInstruction()) {\n+          // Reset the state machine for async-grouped send and recv. This block\n+          // essentially calls the main VerifyNoCollectiveDeadlocks function\n+          // on the async computation without recursion. This is necessary for\n+          // async-wrapped send and recv instructions that are sandwiched in\n+          // between partially pipelined collectives.\n+          DfaState async_comp_current_state = DfaState::kNoExpectation;\n+          absl::flat_hash_set<const HloSendInstruction*>\n+              async_comp_send_instructions;\n+          absl::flat_hash_set<const HloRecvInstruction*>\n+              async_comp_recv_instructions;\n+          TF_RETURN_IF_ERROR(VerifyNoCollectiveDeadlocksRecursive(\n+              computation, async_comp_current_state,\n+              async_comp_send_instructions, async_comp_recv_instructions));\n+          if (current_state != DfaState::kNoExpectation) {\n+            TF_RETURN_IF_ERROR(CheckPendingSendRecvDeadlocks(\n+                async_comp_send_instructions, async_comp_recv_instructions));\n+          }\n+        } else {\n+          // normal case\n+          TF_RETURN_IF_ERROR(VerifyNoCollectiveDeadlocksRecursive(\n+              computation, current_state, send_instructions,\n+              recv_instructions));\n+        }\n+      }\n+    }\n+  }\n+  return absl::OkStatus();\n+}\n+\n absl::Status VerifyNoCollectiveDeadlocks(const HloModule& module) {\n   DfaState current_state = DfaState::kNoExpectation;\n   absl::flat_hash_set<const HloSendInstruction*> send_instructions;"
        },
        {
            "sha": "496f56bc3e85de7fdf440e86544aa0ce962a4ac5",
            "filename": "third_party/xla/xla/service/hlo_verifier_test.cc",
            "status": "modified",
            "additions": 68,
            "deletions": 0,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_verifier_test.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -4676,6 +4676,74 @@ ENTRY main {\n   EXPECT_THAT(verifier().Run(module.get()), IsOkAndHolds(false));\n }\n \n+TEST_F(HloVerifierTestForCollectiveDeadlocks,\n+       VerifyAsyncComputationPartiallyPipelined) {\n+  const char* const hlo = R\"(\n+HloModule nccl_group_send_recv_no_loop_x4, is_scheduled=true\n+\n+wrapped_send_recv {\n+  param0 = f32[] parameter(0)\n+  param1 = token[] parameter(1)\n+  send1 = (f32[], u32[], token[]) send(param0, param1), channel_id=0,\n+    frontend_attributes={_xla_send_recv_source_target_pairs={{0,1},{1,2}}}\n+  param2 = f32[] parameter(2)\n+  param3 = token[] parameter(3)\n+  send2 = (f32[], u32[], token[]) send(param2, param3), channel_id=0,\n+    frontend_attributes={_xla_send_recv_source_target_pairs={{2,3}}}\n+  param4 = token[] parameter(4)\n+  recv1 = (f32[], u32[], token[]) recv(param4), channel_id=0,\n+    frontend_attributes={_xla_send_recv_source_target_pairs={{0,1},{1,2}}}\n+  param5 = token[] parameter(5)\n+  recv2 = (f32[], u32[], token[]) recv(param5), channel_id=0,\n+    frontend_attributes={_xla_send_recv_source_target_pairs={{2,3}}}\n+  ROOT out = ((f32[], u32[], token[]), (f32[], u32[], token[]),\n+    (f32[], u32[], token[]), (f32[], u32[], token[]))\n+    tuple(send1, send2, recv1, recv2)\n+}\n+\n+ENTRY main {\n+  data1 = f32[] constant(10)\n+  after-all1 = token[] after-all()\n+  data2 = f32[] constant(20)\n+  after-all2 = token[] after-all()\n+  data3 = f32[] constant(30)\n+  after-all3 = token[] after-all()\n+  bwd_send = (f32[], u32[], token[]) send(data3, after-all3), channel_id=0,\n+    frontend_attributes={_xla_send_recv_source_target_pairs={{1,0}}}\n+  bwd_send_done = token[] send-done(bwd_send), channel_id=0\n+  async-comp-start = ((f32[], token[], f32[], token[], token[], token[]),\n+    ((f32[], u32[], token[]), (f32[], u32[], token[]), (f32[], u32[], token[]),\n+    (f32[], u32[], token[])), s32[]) async-start(data1, after-all1,\n+    data2, after-all2, after-all1, after-all2), calls=wrapped_send_recv\n+  async-comp-done = ((f32[], u32[], token[]), (f32[], u32[], token[]),\n+    (f32[], u32[], token[]), (f32[], u32[], token[])) async-done(async-comp-start)\n+  bwd_recv = (f32[], u32[], token[]) recv(after-all3), channel_id=0,\n+    frontend_attributes={_xla_send_recv_source_target_pairs={{1,0}}}\n+  bwd_recv_done = (f32[], token[]) recv-done(bwd_recv), channel_id=0\n+  unpack-recv-done1 = (f32[], u32[], token[]) get-tuple-element(async-comp-done), index=2\n+  recv-done-data1 = f32[] get-tuple-element(unpack-recv-done1), index=0\n+  recv-done-token1 = token[] get-tuple-element(unpack-recv-done1), index=2\n+  recv-done1 = (f32[], token[]) tuple(recv-done-data1, recv-done-token1),\n+    control-predecessors={async-comp-start}\n+  data-out1 = f32[] get-tuple-element(recv-done1), index=0\n+  unpack-recv-done2 = (f32[], u32[], token[]) get-tuple-element(async-comp-done), index=3\n+  recv-done-data2 = f32[] get-tuple-element(unpack-recv-done2), index=0\n+  recv-done-token2 = token[] get-tuple-element(unpack-recv-done2), index=2\n+  recv-done2 = (f32[], token[]) tuple(recv-done-data2, recv-done-token2),\n+    control-predecessors={async-comp-start}\n+  data-out2 = f32[] get-tuple-element(recv-done2), index=0\n+  ROOT out = (f32[], f32[]) tuple(data-out1, data-out2)\n+  unpack-send-done1 = (f32[], u32[], token[]) get-tuple-element(async-comp-done), index=0\n+  send-done1 = token[] get-tuple-element(unpack-send-done1), index=2\n+  unpack-send-done2 = (f32[], u32[], token[]) get-tuple-element(async-comp-done), index=1\n+  send-done2 = token[] get-tuple-element(unpack-send-done2), index=2\n+}\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<xla::HloModule> module,\n+                          ParseAndReturnUnverifiedModule(hlo));\n+  EXPECT_THAT(verifier().Run(module.get()), IsOkAndHolds(false));\n+}\n+\n TEST_F(HloVerifierTest, VerifyMatchingSendSameChannel) {\n   const char* const hlo = R\"(\n   HloModule module"
        },
        {
            "sha": "761932320675a1c31557396d1aa93e8f29bcd8fd",
            "filename": "third_party/xla/xla/service/scatter_determinism_expander.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fscatter_determinism_expander.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fscatter_determinism_expander.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fscatter_determinism_expander.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -904,13 +904,20 @@ bool CheckOutputDependency(HloComputation* to_apply, int operand_size) {\n   return true;\n }\n \n+bool IsSupportedIndicesType(PrimitiveType primitive_type) {\n+  return primitive_type == S32 || primitive_type == S64;\n+}\n+\n }  // namespace\n \n bool ScatterDeterminismExpander::InstructionMatchesPattern(\n     HloInstruction* inst) {\n   auto* scatter = DynCast<HloScatterInstruction>(inst);\n \n   return (scatter != nullptr) && !IsScatterDeterministic(scatter) &&\n+         scatter->scatter_operand_count() == 1 &&\n+         IsSupportedIndicesType(\n+             scatter->scatter_indices()->shape().element_type()) &&\n          CheckOutputDependency(scatter->to_apply(),\n                                scatter->scatter_operands().size());\n }"
        },
        {
            "sha": "496e9c261d72191f68c08d41e76fd333dc3c32c4",
            "filename": "third_party/xla/xla/service/scatter_determinism_expander_test.cc",
            "status": "modified",
            "additions": 65,
            "deletions": 0,
            "changes": 65,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fscatter_determinism_expander_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fscatter_determinism_expander_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fscatter_determinism_expander_test.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -1084,5 +1084,70 @@ TEST_F(ScatterDeterminismExpanderTest, ScalarUpdateChangesVectorDim) {\n   EXPECT_TRUE(result);\n }\n \n+TEST_F(ScatterDeterminismExpanderTest, UnsupportedScatterIndicesType) {\n+  const char* const kModuleStr = R\"(\n+    HloModule m\n+\n+    update_s32 (lhs: s32[], rhs: s32[]) -> s32[] {\n+      lhs = s32[] parameter(0)\n+      ROOT rhs = s32[] parameter(1)\n+    }\n+\n+    ENTRY main {\n+      operand = s32[129,3]{1,0} parameter(0)\n+      indices = u8[6,2]{1,0} parameter(1)\n+      updates = s32[6,1,1]{2,1,0} parameter(2)\n+      ROOT scatter = s32[129,3]{1,0} scatter(operand, indices, updates),\n+          to_apply=update_s32,\n+          update_window_dims={1,2},\n+          inserted_window_dims={},\n+          scatter_dims_to_operand_dims={0,1},\n+          index_vector_dim=1\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(kModuleStr));\n+\n+  ScatterDeterminismExpander scatter_determinism_expander;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      bool result, RunHloPass(&scatter_determinism_expander, module.get()));\n+  EXPECT_FALSE(result);\n+}\n+\n+TEST_F(ScatterDeterminismExpanderTest, UnsupportedVariadicScatter) {\n+  const char* const kModuleStr = R\"(\n+    HloModule MultioutputScatter\n+\n+    update {\n+      lhs0 = s32[] parameter(0)\n+      lhs1 = f32[] parameter(1)\n+      rhs0 = s32[] parameter(2)\n+      rhs1 = f32[] parameter(3)\n+      ROOT tuple = (s32[], f32[]) tuple(rhs0, rhs1)\n+    }\n+\n+    ENTRY main {\n+      operand0 = s32[3,3,2] parameter(0)\n+      operand1 = f32[3,3,2] parameter(1)\n+      indices = s32[2,2] parameter(2)\n+      updates0 = s32[2,2] parameter(3)\n+      updates1 = f32[2,2] parameter(4)\n+      ROOT scatter = (s32[3,3,2], f32[3,3,2]) scatter(operand0, operand1, indices, updates0, updates1),\n+          to_apply=update,\n+          update_window_dims={1},\n+          inserted_window_dims={0,1},\n+          scatter_dims_to_operand_dims={0,1},\n+          index_vector_dim=1\n+    }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(kModuleStr));\n+\n+  ScatterDeterminismExpander scatter_determinism_expander;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      bool result, RunHloPass(&scatter_determinism_expander, module.get()));\n+  EXPECT_FALSE(result);\n+}\n+\n }  // namespace\n }  // namespace xla"
        },
        {
            "sha": "07bb582f76f1be1c5991aa37fd3a3d70f51ece64",
            "filename": "third_party/xla/xla/service/spmd/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2FBUILD?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -30,6 +30,7 @@ cc_library(\n     hdrs = [\n         \"convolution_handler.h\",\n         \"custom_call_handler.h\",\n+        \"dot_handler.h\",\n         \"spmd_partitioner.h\",\n         \"spmd_partitioner_util.h\",\n     ],\n@@ -435,10 +436,12 @@ xla_cc_test(\n     srcs = [\"dot_handler_test.cc\"],\n     deps = [\n         \":stateful_rng_spmd_partitioner\",\n+        \"//xla:literal_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass_pipeline\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/hlo/utils:hlo_matchers\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service:hlo_verifier\",\n         \"//xla/service:sharding_propagation\","
        },
        {
            "sha": "885fd1dbdba6c5a651d09f0004b3fa05d657fac6",
            "filename": "third_party/xla/xla/service/spmd/convolution_handler.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 61,
            "changes": 93,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fconvolution_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fconvolution_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fconvolution_handler.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -52,10 +52,7 @@ namespace {\n absl::StatusOr<HloInstruction*> PartitionConvolutionWithBatchGroupCount(\n     PartitionedHlo lhs, PartitionedHlo rhs, const Shape& output_base_shape,\n     const HloSharding& output_sharding,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_conv,\n+    CreateShardedConvolutionFunctor& create_sharded_conv,\n     const Window& conv_window, HloInstruction* original_hlo,\n     int64_t num_partitions, SpmdBuilder* b) {\n   TF_RET_CHECK(original_hlo->opcode() == HloOpcode::kConvolution);\n@@ -135,9 +132,8 @@ absl::StatusOr<HloInstruction*> PartitionConvolutionWithBatchGroupCount(\n       lhs.sharding(), lhs_to_output_indices);\n \n   // Create partitioned convolution.\n-  TF_ASSIGN_OR_RETURN(\n-      auto sharded_conv,\n-      create_sharded_conv(lhs.hlo(), rhs.hlo(), b, conv_window));\n+  TF_ASSIGN_OR_RETURN(auto sharded_conv,\n+                      create_sharded_conv(lhs, rhs, b, conv_window));\n   sharded_conv->set_sharding(aligned_output_sharding);\n   return PartitionedHlo(sharded_conv, output_base_shape, lhs.state())\n       .Reshard(output_sharding)\n@@ -148,10 +144,7 @@ absl::StatusOr<HloInstruction*> PartitionConvolutionWithBatchGroupCount(\n absl::StatusOr<HloInstruction*> PartitionConvolutionWithFeatureGroupCount(\n     PartitionedHlo lhs, PartitionedHlo rhs, const Shape& output_base_shape,\n     const HloSharding& output_sharding,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_conv,\n+    CreateShardedConvolutionFunctor& create_sharded_conv,\n     const Window& conv_window, HloInstruction* original_hlo,\n     int64_t num_partitions, SpmdBuilder* b) {\n   TF_RET_CHECK(original_hlo->opcode() == HloOpcode::kConvolution);\n@@ -231,9 +224,8 @@ absl::StatusOr<HloInstruction*> PartitionConvolutionWithFeatureGroupCount(\n   auto aligned_output_sharding = hlo_sharding_util::TransposeSharding(\n       lhs.sharding(), lhs_to_output_indices);\n \n-  TF_ASSIGN_OR_RETURN(\n-      auto sharded_conv,\n-      create_sharded_conv(lhs.hlo(), rhs.hlo(), b, conv_window));\n+  TF_ASSIGN_OR_RETURN(auto sharded_conv,\n+                      create_sharded_conv(lhs, rhs, b, conv_window));\n   sharded_conv->set_sharding(aligned_output_sharding);\n   return PartitionedHlo(sharded_conv, output_base_shape, lhs.state())\n       .Reshard(output_sharding)\n@@ -246,10 +238,7 @@ absl::StatusOr<HloInstruction*>\n PartitionConvolutionWithSpatialDimensionHaloExchangeOnRHS(\n     PartitionedHlo lhs, PartitionedHlo rhs, const Shape& output_base_shape,\n     const HloSharding& output_sharding,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_conv,\n+    CreateShardedConvolutionFunctor& create_sharded_conv,\n     const Window& conv_window, HloInstruction* original_hlo,\n     HloInstruction* partition_id, HloModule* module, SpmdBuilder* b) {\n   TF_RET_CHECK(original_hlo->opcode() == HloOpcode::kConvolution);\n@@ -518,7 +507,11 @@ PartitionConvolutionWithSpatialDimensionHaloExchangeOnRHS(\n   }\n \n   TF_ASSIGN_OR_RETURN(\n-      auto conv, create_sharded_conv(conv_lhs, rhs_with_halo, b, new_window));\n+      auto conv,\n+      create_sharded_conv(\n+          PartitionedHlo(conv_lhs, lhs.base_shape(), lhs.state()),\n+          PartitionedHlo(rhs_with_halo, rhs.base_shape(), rhs.state()), b,\n+          new_window));\n \n   auto ar = collective_ops_creator.create_cross_partition_all_reduce(\n       b, conv, MakeBinaryAdd(original_hlo->shape().element_type(), module), {},\n@@ -535,10 +528,7 @@ absl::StatusOr<HloInstruction*>\n PartitionConvolutionWithSpatialDimensionHaloExchangeOnLHS(\n     PartitionedHlo lhs, PartitionedHlo rhs, const Shape& output_base_shape,\n     const HloSharding& output_sharding,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_conv,\n+    CreateShardedConvolutionFunctor& create_sharded_conv,\n     const Window& conv_window, HloInstruction* original_hlo,\n     HloInstruction* partition_id, HloModule* module, SpmdBuilder* b) {\n   TF_RET_CHECK(original_hlo->opcode() == HloOpcode::kConvolution);\n@@ -745,7 +735,10 @@ PartitionConvolutionWithSpatialDimensionHaloExchangeOnLHS(\n   }\n \n   TF_ASSIGN_OR_RETURN(\n-      auto conv, create_sharded_conv(lhs_with_halo, rhs.hlo(), b, new_window));\n+      auto conv,\n+      create_sharded_conv(\n+          PartitionedHlo(lhs_with_halo, lhs.base_shape(), lhs.state()), rhs, b,\n+          new_window));\n   auto ar =\n       lhs.state().collective_ops_creator.create_cross_partition_all_reduce(\n           b, conv, MakeBinaryAdd(output_base_shape.element_type(), module), {},\n@@ -761,10 +754,7 @@ PartitionConvolutionWithSpatialDimensionHaloExchangeOnLHS(\n absl::StatusOr<HloInstruction*> PartitionConvolutionTiledOutput(\n     PartitionedHlo lhs, PartitionedHlo rhs, const Shape& output_base_shape,\n     const HloSharding& output_sharding,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_conv,\n+    CreateShardedConvolutionFunctor& create_sharded_conv,\n     const Window& conv_window, HloInstruction* original_hlo, SpmdBuilder* b) {\n   TF_RET_CHECK(original_hlo->opcode() == HloOpcode::kConvolution);\n   const auto& dnums = original_hlo->convolution_dimension_numbers();\n@@ -833,8 +823,10 @@ absl::StatusOr<HloInstruction*> PartitionConvolutionTiledOutput(\n \n   TF_ASSIGN_OR_RETURN(\n       auto sharded_conv,\n-      create_sharded_conv(resharded_operand_and_window->sharded_input,\n-                          rhs.hlo(), b, new_window));\n+      create_sharded_conv(\n+          PartitionedHlo(resharded_operand_and_window->sharded_input,\n+                         lhs.base_shape(), lhs.state()),\n+          rhs, b, new_window));\n \n   auto shard_shape = MakePartitionedShape(output_base_shape, output_sharding);\n   if (!resharded_operand_and_window->dynamic_slice_index_on_output\n@@ -852,10 +844,7 @@ absl::StatusOr<HloInstruction*> PartitionConvolutionTiledOutput(\n absl::StatusOr<HloInstruction*> PartitionConvolutionBaseCase(\n     const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_conv,\n+    CreateShardedConvolutionFunctor& create_sharded_conv,\n     const Window& conv_window, HloInstruction* original_hlo,\n     int64_t num_partitions, const SpmdPartitionerOptions& options,\n     HloInstruction* partition_id, HloModule* module, SpmdBuilder* b) {\n@@ -925,6 +914,8 @@ absl::StatusOr<HloInstruction*> PartitionConvolutionBaseCase(\n   return nullptr;\n }\n \n+}  // namespace\n+\n absl::StatusOr<std::unique_ptr<HloInstruction>> CreateShardedConvolution(\n     const HloInstruction& conv,\n     const dot_as_convolution_util::DotConvolutionDimsInfo& dot_dnums,\n@@ -1007,17 +998,12 @@ absl::StatusOr<std::unique_ptr<HloInstruction>> CreateShardedConvolution(\n       batch_group_count, window, conv_dnums, conv.precision_config());\n }\n \n-}  // namespace\n-\n // Partition convolution.\n absl::StatusOr<HloInstruction*> PartitionConvolution(\n     const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const dot_as_convolution_util::DotConvolutionDimsInfo& dims_mapping,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_conv,\n+    CreateShardedConvolutionFunctor& create_sharded_conv,\n     const Window& conv_window, HloInstruction* original_hlo,\n     int64_t num_partitions, const SpmdPartitionerOptions& options,\n     HloInstruction* partition_id, HloModule* module, SpmdBuilder* b) {\n@@ -1039,28 +1025,13 @@ absl::Status SpmdPartitioningVisitor::HandleConvolution(HloInstruction* hlo) {\n   if (hlo->sharding().HasUniqueDevice()) {\n     return DefaultAction(hlo);\n   }\n-  const auto dims_info = dot_as_convolution_util::ParseConvolutionDimsInfo(hlo);\n-\n-  auto create_sharded_conv =\n-      [&](HloInstruction* lhs_hlo, HloInstruction* rhs_hlo,\n-          spmd::SpmdBuilder* b,\n-          const Window& conv_window) -> absl::StatusOr<HloInstruction*> {\n-    if (dims_info.conv_spatial_dims.empty() &&\n-        hlo->feature_group_count() == 1 && hlo->batch_group_count() == 1) {\n-      TF_ASSIGN_OR_RETURN(\n-          auto sharded_conv,\n-          dot_as_convolution_util::CreateShardedConvForDotGeneralConvolution(\n-              *hlo, dims_info, lhs_hlo, rhs_hlo));\n-      return b->AddInstruction(std::move(sharded_conv));\n-    } else {\n-      TF_ASSIGN_OR_RETURN(auto sharded_conv,\n-                          CreateShardedConvolution(*hlo, dims_info, lhs_hlo,\n-                                                   rhs_hlo, conv_window));\n-      return b->AddInstruction(std::move(sharded_conv));\n-    }\n-  };\n+  const dot_as_convolution_util::DotConvolutionDimsInfo dims_info =\n+      dot_as_convolution_util::ParseConvolutionDimsInfo(hlo);\n+\n+  CreateShardedConvolutionFunctor create_sharded_conv_functor(hlo, dims_info);\n \n-  return HandleDotHelper(hlo, dims_info, create_sharded_conv);\n+  return HandleDotHelper<CreateShardedConvolutionFunctor>(\n+      hlo, dims_info, create_sharded_conv_functor);\n }\n \n }  // namespace spmd"
        },
        {
            "sha": "c52c42885d1cf0396059b36709f2ed3613fc9cbf",
            "filename": "third_party/xla/xla/service/spmd/convolution_handler.h",
            "status": "modified",
            "additions": 44,
            "deletions": 4,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fconvolution_handler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fconvolution_handler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fconvolution_handler.h?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n #include \"xla/service/dot_as_convolution_util.h\"\n+#include \"xla/service/spmd/dot_handler.h\"\n #include \"xla/service/spmd/spmd_partitioner.h\"\n #include \"xla/shape.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -36,14 +37,53 @@ absl::StatusOr<HloInstruction*> PartitionConvolution(\n     const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const dot_as_convolution_util::DotConvolutionDimsInfo& dims_mapping,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_conv,\n+    CreateShardedConvolutionFunctor& create_sharded_conv,\n     const Window& conv_window, HloInstruction* original_hlo,\n     int64_t num_partitions, const SpmdPartitionerOptions& options,\n     HloInstruction* partition_id, HloModule* module, SpmdBuilder* b);\n \n+absl::StatusOr<std::unique_ptr<HloInstruction>> CreateShardedConvolution(\n+    const HloInstruction& conv,\n+    const dot_as_convolution_util::DotConvolutionDimsInfo& dot_dnums,\n+    HloInstruction* sharded_lhs_hlo, HloInstruction* sharded_rhs_hlo,\n+    const Window& conv_window);\n+\n+// Functor class for creating sharded convolutions with operands of type\n+// PartitionedHlo.\n+class CreateShardedConvolutionFunctor final\n+    : public CreateShardedFunctorBase<PartitionedHlo> {\n+ public:\n+  CreateShardedConvolutionFunctor(\n+      HloInstruction* conv,\n+      const dot_as_convolution_util::DotConvolutionDimsInfo& dims_info)\n+      : conv_(conv), dims_info_(dims_info) {}\n+\n+  // Implements the creation of sharded convolutions.\n+  absl::StatusOr<HloInstruction*> CreateSharded(\n+      const PartitionedHlo& ll, const PartitionedHlo& rr, spmd::SpmdBuilder* b,\n+      const Window& conv_window) const override {\n+    HloInstruction* l = ll.hlo();\n+    HloInstruction* r = rr.hlo();\n+    if (dims_info_.conv_spatial_dims.empty() &&\n+        conv_->feature_group_count() == 1 && conv_->batch_group_count() == 1) {\n+      TF_ASSIGN_OR_RETURN(\n+          auto sharded_conv,\n+          dot_as_convolution_util::CreateShardedConvForDotGeneralConvolution(\n+              *conv_, dims_info_, l, r));\n+      return b->AddInstruction(std::move(sharded_conv));\n+    } else {\n+      TF_ASSIGN_OR_RETURN(\n+          auto sharded_conv,\n+          CreateShardedConvolution(*conv_, dims_info_, l, r, conv_window));\n+      return b->AddInstruction(std::move(sharded_conv));\n+    }\n+  }\n+\n+ private:\n+  HloInstruction* conv_;\n+  const dot_as_convolution_util::DotConvolutionDimsInfo& dims_info_;\n+};\n+\n }  // namespace spmd\n }  // namespace xla\n "
        },
        {
            "sha": "3fb82561ec710cf95e61e343c6fd5d4e6c85f77e",
            "filename": "third_party/xla/xla/service/spmd/custom_call_handler.cc",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fcustom_call_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fcustom_call_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fcustom_call_handler.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -481,6 +481,39 @@ absl::Status SpmdPartitioningVisitor::HandleCustomCall(HloInstruction* hlo) {\n     return absl::OkStatus();\n   }\n \n+  // Block-scaled dot with MX operands.\n+  if (hlo->custom_call_target() == \"__op$block_scaled_dot\") {\n+    // Evaluate the dimension numbers of the block-scaled dot.\n+    int dimensions_size = hlo->operand(0)->shape().dimensions_size();\n+    TF_RET_CHECK(dimensions_size == 2 || dimensions_size == 3);\n+    DotDimensionNumbers dimension_numbers;\n+    dimension_numbers.add_lhs_contracting_dimensions(dimensions_size - 1);\n+    dimension_numbers.add_rhs_contracting_dimensions(dimensions_size - 1);\n+    if (dimensions_size == 3) {\n+      dimension_numbers.add_lhs_batch_dimensions(0);\n+      dimension_numbers.add_rhs_batch_dimensions(0);\n+    }\n+\n+    HloCustomCallInstruction* block_scaled_dot =\n+        Cast<HloCustomCallInstruction>(hlo);\n+    CreateShardedScaledDotFunctor create_sharded_scaled_dot_functor(\n+        block_scaled_dot, dimension_numbers);\n+\n+    // Create a regular dot with equivalent operand and output shape to compute\n+    // the mapping for HandleDotHelper.\n+    PrecisionConfig precision_config;\n+    precision_config.mutable_operand_precision()->Resize(\n+        2, PrecisionConfig::DEFAULT);\n+    std::unique_ptr<HloInstruction> dot = HloInstruction::CreateDot(\n+        hlo->shape(), hlo->mutable_operand(0), hlo->mutable_operand(1),\n+        dimension_numbers, precision_config);\n+    dot_as_convolution_util::DotConvolutionDimsInfo mapping =\n+        dot_as_convolution_util::ParseDotGeneralFromDot(dot.get());\n+\n+    return HandleDotHelper<CreateShardedScaledDotFunctor>(\n+        hlo, mapping, create_sharded_scaled_dot_functor);\n+  }\n+\n   return DefaultAction(hlo);\n }\n "
        },
        {
            "sha": "bfb61daf4ff4678363b12634e5c30b55dc61a30f",
            "filename": "third_party/xla/xla/service/spmd/custom_call_handler.h",
            "status": "modified",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fcustom_call_handler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fcustom_call_handler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fcustom_call_handler.h?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -20,6 +20,10 @@ limitations under the License.\n #include <memory>\n \n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/shape_inference.h\"\n+#include \"xla/service/spmd/dot_handler.h\"\n+#include \"xla/service/spmd/spmd_partitioner.h\"\n \n namespace xla {\n namespace spmd {\n@@ -30,6 +34,40 @@ namespace spmd {\n std::unique_ptr<HloInstruction> CreateCustomCallSPMDInternal_RotateRight(\n     HloInstruction* input, int64_t dim, int64_t amount);\n \n+// Functor class for creating sharded block-scaled dots with operands of type\n+// PartitionedHloMX.\n+class CreateShardedScaledDotFunctor final\n+    : public CreateShardedFunctorBase<PartitionedHloMX> {\n+ public:\n+  CreateShardedScaledDotFunctor(HloCustomCallInstruction* block_scaled_dot,\n+                                const DotDimensionNumbers& dimension_numbers)\n+      : block_scaled_dot_(block_scaled_dot),\n+        dimension_numbers_(dimension_numbers) {}\n+\n+  // Implements the creation of sharded block-scaled dots.\n+  absl::StatusOr<HloInstruction*> CreateSharded(\n+      const PartitionedHloMX& ll, const PartitionedHloMX& rr, SpmdBuilder* b,\n+      const Window& conv_window) const override {\n+    HloInstruction* l = ll.operand().hlo();\n+    HloInstruction* r = rr.operand().hlo();\n+    HloInstruction* l_scale = ll.scale().hlo();\n+    HloInstruction* r_scale = rr.scale().hlo();\n+    TF_ASSIGN_OR_RETURN(Shape sharded_scaled_dot_shape,\n+                        ShapeInference::InferDotOpShape(\n+                            l->shape(), r->shape(), dimension_numbers_,\n+                            /*preferred_element_type=*/\n+                            block_scaled_dot_->shape().element_type()));\n+\n+    return b->AddInstruction(HloInstruction::CreateCustomCall(\n+        sharded_scaled_dot_shape, {l, r, l_scale, r_scale},\n+        \"__op$block_scaled_dot\", \"\"));\n+  }\n+\n+ private:\n+  HloCustomCallInstruction* block_scaled_dot_;\n+  const DotDimensionNumbers& dimension_numbers_;\n+};\n+\n }  // namespace spmd\n }  // namespace xla\n "
        },
        {
            "sha": "6ed7a62bad82785c7d4572e2ac12eb7adf6163a1",
            "filename": "third_party/xla/xla/service/spmd/dot_handler.cc",
            "status": "modified",
            "additions": 544,
            "deletions": 433,
            "changes": 977,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -13,6 +13,8 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include \"xla/service/spmd/dot_handler.h\"\n+\n #include <algorithm>\n #include <cstdint>\n #include <deque>\n@@ -40,7 +42,6 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_input_output_alias_config.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n #include \"xla/hlo/utils/hlo_sharding_util.h\"\n@@ -50,6 +51,7 @@ limitations under the License.\n #include \"xla/service/shape_inference.h\"\n #include \"xla/service/sharding_propagation.h\"\n #include \"xla/service/spmd/convolution_handler.h\"\n+#include \"xla/service/spmd/custom_call_handler.h\"\n #include \"xla/service/spmd/spmd_partitioner.h\"\n #include \"xla/service/spmd/spmd_partitioner_util.h\"\n #include \"xla/shape.h\"\n@@ -66,27 +68,103 @@ namespace xla {\n namespace spmd {\n \n namespace {\n+\n using dot_as_convolution_util::DotConvolutionDimsInfo;\n using hlo_sharding_util::GroupedSharding;\n+\n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n+absl::StatusOr<HloInstruction*> PartitionDot(\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n+    const Shape& output_base_shape, const HloSharding& output_sharding,\n+    const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n+    const SpmdPartitionerOptions& options, SpmdBuilder* b,\n+    std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n+        windowed_dot_general_loops,\n+    SpmdPartitioningVisitor* visitor);\n+\n+PartitionedHlo MakePartitionedHloMaybeMX(\n+    const PartitionedHlo& p, const Shape& base_shape,\n+    const PartitionedHlo::PartitioningState& state) {\n+  return PartitionedHlo(p.hlo(), base_shape, state);\n+}\n+\n+PartitionedHloMX MakePartitionedHloMaybeMX(\n+    const PartitionedHloMX& p, const std::pair<Shape, Shape>& base_shapes,\n+    const PartitionedHlo::PartitioningState& state) {\n+  return PartitionedHloMX(\n+      PartitionedHlo(p.operand().hlo(), base_shapes.first, state),\n+      PartitionedHlo(p.scale().hlo(), base_shapes.second, state));\n+}\n+\n+PartitionedHlo ReplicatePartiallySharded(\n+    const PartitionedHlo& partitioned, absl::Span<const int64_t> batch_dims,\n+    const GroupedSharding& grouped, SpmdBuilder* b,\n+    const PartitionedHlo::PartitioningState& state) {\n+  HloInstruction* partially_sharded = PerGroupSliceFromReplicated(\n+      partitioned.Replicate().hlo(), partitioned.state().partition_id,\n+      grouped.device_groups, batch_dims, grouped.group_dim_sizes, b);\n+  partially_sharded->set_sharding(HloSharding::Replicate());\n+  return PartitionedHlo(partially_sharded, partially_sharded->shape(), state);\n+}\n+\n+PartitionedHloMX ReplicatePartiallySharded(\n+    const PartitionedHloMX& partitioned, absl::Span<const int64_t> batch_dims,\n+    const GroupedSharding& grouped, SpmdBuilder* b,\n+    const PartitionedHlo::PartitioningState& state) {\n+  return PartitionedHloMX(\n+      ReplicatePartiallySharded(partitioned.operand(), batch_dims, grouped, b,\n+                                state),\n+      ReplicatePartiallySharded(partitioned.scale(), batch_dims, grouped, b,\n+                                state));\n+}\n+\n }  // namespace\n \n-absl::Status SpmdPartitioningVisitor::HandleDot(HloInstruction* hlo) {\n-  DotConvolutionDimsInfo mapping =\n-      dot_as_convolution_util::ParseDotGeneralFromDot(hlo);\n+std::pair<Shape, Shape> GetPerGroupBaseShape(\n+    const hlo_sharding_util::GroupedSharding& grouped_sharding,\n+    const PartitionedHloMX::ShapesMX& original_base_shapes) {\n+  std::pair<Shape, Shape> pair(std::move(original_base_shapes));\n+  return std::make_pair(GetPerGroupBaseShape(grouped_sharding, pair.first),\n+                        GetPerGroupBaseShape(grouped_sharding, pair.second));\n+}\n \n-  auto create_sharded_dot =\n-      [&](HloInstruction* l, HloInstruction* r, SpmdBuilder* b,\n-          const Window& conv_window) -> absl::StatusOr<HloInstruction*> {\n+// Functor class for creating sharded dots with operands of type PartitionedHlo.\n+class CreateShardedDotFunctor final\n+    : public CreateShardedFunctorBase<PartitionedHlo> {\n+ public:\n+  CreateShardedDotFunctor(HloDotInstruction* dot) : dot_(dot) {}\n+\n+  // Implements the creation of sharded dots.\n+  absl::StatusOr<HloInstruction*> CreateSharded(\n+      const PartitionedHlo& ll, const PartitionedHlo& rr, SpmdBuilder* b,\n+      const Window& conv_window) const override {\n+    HloInstruction* l = ll.hlo();\n+    HloInstruction* r = rr.hlo();\n     TF_ASSIGN_OR_RETURN(\n         auto sharded_dot_shape,\n         ShapeInference::InferDotOpShape(\n-            l->shape(), r->shape(), hlo->dot_dimension_numbers(),\n-            /*preferred_element_type=*/hlo->shape().element_type()));\n+            l->shape(), r->shape(), dot_->dot_dimension_numbers(),\n+            /*preferred_element_type=*/dot_->shape().element_type()));\n     return b->AddInstruction(HloInstruction::CreateDot(\n-        sharded_dot_shape, l, r, hlo->dot_dimension_numbers(),\n-        hlo->precision_config()));\n-  };\n-  return HandleDotHelper(hlo, mapping, create_sharded_dot);\n+        sharded_dot_shape, l, r, dot_->dot_dimension_numbers(),\n+        dot_->precision_config()));\n+  }\n+\n+ private:\n+  HloDotInstruction* dot_;\n+};\n+\n+absl::Status SpmdPartitioningVisitor::HandleDot(HloInstruction* hlo) {\n+  DotConvolutionDimsInfo mapping =\n+      dot_as_convolution_util::ParseDotGeneralFromDot(hlo);\n+\n+  HloDotInstruction* dot = Cast<HloDotInstruction>(hlo);\n+\n+  CreateShardedDotFunctor create_sharded_dot_functor(dot);\n+  return HandleDotHelper<CreateShardedDotFunctor>(hlo, mapping,\n+                                                  create_sharded_dot_functor);\n }\n \n namespace {\n@@ -278,6 +356,7 @@ bool should_enable_windowed_einsum_with_threshold(\n   }\n }\n \n+template <typename CreateShardedFunctor>\n std::optional<WindowedEinsumConfig> GetWindowedEinsumConfiguration(\n     int64_t num_partitions, int64_t output_lhs_non_contracting_partitions,\n     int64_t output_rhs_non_contracting_partitions,\n@@ -298,10 +377,7 @@ std::optional<WindowedEinsumConfig> GetWindowedEinsumConfiguration(\n     const HloInstruction* original_hlo = nullptr,\n     const PartitionedHlo* const partitioned_lhs = nullptr,\n     const PartitionedHlo* const partitioned_rhs = nullptr,\n-    std::optional<absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>>\n-        create_sharded_dot = std::nullopt,\n+    std::optional<CreateShardedFunctor> create_sharded_dot = std::nullopt,\n     SpmdBuilder* b = nullptr, HloModule* module = nullptr,\n     SpmdPartitioningVisitor* visitor = nullptr) {\n   if (num_partitions > max_iterations) {\n@@ -394,8 +470,7 @@ std::optional<WindowedEinsumConfig> GetWindowedEinsumConfiguration(\n                                           partitioned_rhs->state())\n                                .Replicate()\n                          : *partitioned_rhs;\n-      dot = (*create_sharded_dot)(new_lhs.hlo(), new_rhs.hlo(), b, conv_window)\n-                .value();\n+      dot = (*create_sharded_dot)(new_lhs, new_rhs, b, conv_window).value();\n       computation_time_in_ms = visitor->GetComputationTimeInMilliSec(dot);\n \n       collective = lhs_needs_ag ? new_lhs.hlo() : new_rhs.hlo();\n@@ -434,8 +509,7 @@ std::optional<WindowedEinsumConfig> GetWindowedEinsumConfiguration(\n       new_lhs = new_lhs.PadWithZero();\n       new_rhs = new_rhs.PadWithZero();\n \n-      dot = (*create_sharded_dot)(new_lhs.hlo(), new_rhs.hlo(), b, conv_window)\n-                .value();\n+      dot = (*create_sharded_dot)(new_lhs, new_rhs, b, conv_window).value();\n       computation_time_in_ms = visitor->GetComputationTimeInMilliSec(dot);\n \n       std::vector<int64_t> lhs_contracting_dims;\n@@ -750,15 +824,12 @@ std::vector<ReplicaGroup> GetLoopReplicaGroups(HloInstruction* while_loop) {\n // is tiled in other dimensions. Or both operands are partitioned in the same\n // way along contracting dimensions, but the output is partitioned along\n // non-contracting dimensions.\n+template <typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> EmitWindowedDotGeneral(\n     PartitionedHlo lhs, PartitionedHlo rhs, const Shape& output_base_shape,\n     const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    int64_t loop_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n+    int64_t loop_partitions, const CreateShardedFunctor& create_sharded_dot,\n     const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n@@ -1163,9 +1234,12 @@ absl::StatusOr<HloInstruction*> EmitWindowedDotGeneral(\n     }\n \n     // The generated original dot will not be used.\n-    TF_ASSIGN_OR_RETURN(auto original_dot,\n-                        create_sharded_dot(original_dot_lhs, original_dot_rhs,\n-                                           &body_b, conv_window));\n+    TF_ASSIGN_OR_RETURN(\n+        auto original_dot,\n+        create_sharded_dot(\n+            PartitionedHlo(original_dot_lhs, lhs.base_shape(), lhs.state()),\n+            PartitionedHlo(original_dot_rhs, rhs.base_shape(), rhs.state()),\n+            &body_b, conv_window));\n     VLOG(2) << original_dot->ToString();\n \n     // Generate the correct shape of the new dot/conv.\n@@ -1334,7 +1408,10 @@ absl::StatusOr<HloInstruction*> EmitWindowedDotGeneral(\n       }\n     }\n     TF_ASSIGN_OR_RETURN(\n-        auto dot, create_sharded_dot(dot_lhs, dot_rhs, &body_b, conv_window));\n+        auto dot, create_sharded_dot(\n+                      PartitionedHlo(dot_lhs, lhs.base_shape(), lhs.state()),\n+                      PartitionedHlo(dot_rhs, rhs.base_shape(), rhs.state()),\n+                      &body_b, conv_window));\n     if (windowed_at_contracting_dims || operands_sharded_at_contracting_dims) {\n       // Accumulate the partial output to the result buffer.\n       o = body_b.AddInstruction(\n@@ -1709,15 +1786,13 @@ absl::StatusOr<HloInstruction*> EmitWindowedDotGeneral(\n // one at a time. The base shapes and shardings can be changed during the\n // recursion as we group devices together. So refer to the passed in shapes and\n // shardings for inputs and output, and do not use shape inference.\n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionBaseCase(\n-    PartitionedHlo lhs, PartitionedHlo rhs, const Shape& output_base_shape,\n-    const HloSharding& output_sharding,\n+    PartitionedHloMaybeMX lhs, PartitionedHloMaybeMX rhs,\n+    const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    const CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n@@ -1782,8 +1857,7 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n   if (lhs_batch_partitions == rhs_batch_partitions &&\n       rhs_batch_partitions == num_partitions &&\n       lhs_sharding_transposed_to_match_rhs == rhs_sharding) {\n-    TF_ASSIGN_OR_RETURN(\n-        auto dot, create_sharded_dot(lhs.hlo(), rhs.hlo(), b, conv_window));\n+    TF_ASSIGN_OR_RETURN(auto dot, create_sharded_dot(lhs, rhs, b, conv_window));\n     dot->set_sharding(*lhs_sharding_transposed_to_match_output);\n     return PartitionedHlo(dot, output_base_shape, lhs.state())\n         .Reshard(output_sharding)\n@@ -1808,7 +1882,7 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n         return nullptr;\n       }\n       auto resharded_rhs = rhs.Reshard(*lhs_sharding_transposed_to_match_rhs);\n-      return create_sharded_dot(lhs.hlo(), resharded_rhs.hlo(), b, conv_window);\n+      return create_sharded_dot(lhs, resharded_rhs, b, conv_window);\n     }\n     // RHS and output are batch partitioned in the same way.\n     if (rhs_batch_partitions == num_partitions &&\n@@ -1822,7 +1896,7 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n         return nullptr;\n       }\n       auto resharded_lhs = lhs.Reshard(*rhs_sharding_transposed_to_match_lhs);\n-      return create_sharded_dot(resharded_lhs.hlo(), rhs.hlo(), b, conv_window);\n+      return create_sharded_dot(resharded_lhs, rhs, b, conv_window);\n     }\n     return nullptr;\n   };\n@@ -1842,10 +1916,10 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n   // the current dot. We also skip any reshape operand as long as it only has\n   // the lhs or rhs of the dot as the only user since reshape ops won't change\n   // the functional meaning of the pattern.\n-  auto has_reshape_operand = [](PartitionedHlo& hlo) -> bool {\n-    return hlo.hlo()->opcode() == HloOpcode::kReshape ||\n-           hlo.hlo()->opcode() == HloOpcode::kBitcast ||\n-           hlo.hlo()->opcode() == HloOpcode::kTranspose;\n+  auto has_reshape_operand = [](auto& partitioned_hlo_maybe_mx) -> bool {\n+    return partitioned_hlo_maybe_mx.hlo()->opcode() == HloOpcode::kReshape ||\n+           partitioned_hlo_maybe_mx.hlo()->opcode() == HloOpcode::kBitcast ||\n+           partitioned_hlo_maybe_mx.hlo()->opcode() == HloOpcode::kTranspose;\n   };\n   const auto& attrs = original_hlo->frontend_attributes().map();\n   bool should_skip_windowed_einsum =\n@@ -1874,42 +1948,55 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n   }\n \n   std::optional<WindowedEinsumConfig> e_config = std::nullopt;\n-  if (!should_skip_windowed_einsum) {\n-    e_config = GetWindowedEinsumConfiguration(\n-        num_partitions, output_lhs_non_contracting_partitions,\n-        output_rhs_non_contracting_partitions, rhs_contracting_partitions,\n-        rhs_non_contracting_partitions, rhs_batch_partitions,\n-        lhs_contracting_partitions, lhs_non_contracting_partitions,\n-        lhs_batch_partitions, ShapeSizeInBytes(rhs.base_shape()),\n-        ShapeSizeInBytes(lhs.base_shape()), ShapeSizeInBytes(output_base_shape),\n-        options, output_sharding_transposed_to_match_lhs,\n-        output_sharding_transposed_to_match_rhs,\n-        lhs_sharding_transposed_to_match_rhs,\n-        rhs_sharding_transposed_to_match_lhs, lhs_sharding, rhs_sharding,\n-        output_sharding, conv_window, dims_mapping, indices_map,\n-        visitor->call_graph(), options.max_windowed_einsum_iteration,\n-        original_hlo, &lhs, &rhs, create_sharded_dot, b, module, visitor);\n-  }\n-  if (e_config) {\n-    int64_t loop_partitions = 1;\n-    for (int64_t dim : e_config->windowing_dims) {\n-      loop_partitions *= lhs_sharding.tile_assignment().dim(dim);\n-    }\n-    if (e_config->windowing_dims.empty()) {\n-      loop_partitions = num_partitions;\n-    }\n-\n-    VLOG(2) << \"Emit windowed dot.\";\n-    return EmitWindowedDotGeneral(\n-        lhs, rhs, output_base_shape, output_sharding, dims_mapping,\n-        num_partitions, loop_partitions, create_sharded_dot, conv_window,\n-        module, original_hlo, options, b, windowed_dot_general_loops, *e_config,\n-        indices_map, lhs_sharding_transposed_to_match_output,\n-        rhs_sharding_transposed_to_match_output,\n-        rhs_sharding_transposed_to_match_lhs,\n-        lhs_sharding_transposed_to_match_rhs,\n-        output_sharding_transposed_to_match_rhs,\n-        output_sharding_transposed_to_match_lhs);\n+  // Disable windowed einsums for block-scaled dot.\n+  if constexpr (std::is_same_v<PartitionedHloMaybeMX, PartitionedHlo>) {\n+    if (!should_skip_windowed_einsum) {\n+      e_config = GetWindowedEinsumConfiguration<CreateShardedFunctor>(\n+          num_partitions, output_lhs_non_contracting_partitions,\n+          output_rhs_non_contracting_partitions, rhs_contracting_partitions,\n+          rhs_non_contracting_partitions, rhs_batch_partitions,\n+          lhs_contracting_partitions, lhs_non_contracting_partitions,\n+          lhs_batch_partitions, ShapeSizeInBytes(rhs.base_shape()),\n+          ShapeSizeInBytes(lhs.base_shape()),\n+          ShapeSizeInBytes(output_base_shape), options,\n+          output_sharding_transposed_to_match_lhs,\n+          output_sharding_transposed_to_match_rhs,\n+          lhs_sharding_transposed_to_match_rhs,\n+          rhs_sharding_transposed_to_match_lhs, lhs_sharding, rhs_sharding,\n+          output_sharding, conv_window, dims_mapping, indices_map,\n+          visitor->call_graph(), options.max_windowed_einsum_iteration,\n+          original_hlo, &lhs, &rhs, create_sharded_dot, b, module, visitor);\n+    }\n+    if (e_config) {\n+      int64_t loop_partitions = 1;\n+      for (int64_t dim : e_config->windowing_dims) {\n+        loop_partitions *= lhs_sharding.tile_assignment().dim(dim);\n+      }\n+      if (e_config->windowing_dims.empty()) {\n+        loop_partitions = num_partitions;\n+      }\n+      if (e_config) {\n+        int64_t loop_partitions = 1;\n+        for (int64_t dim : e_config->windowing_dims) {\n+          loop_partitions *= lhs_sharding.tile_assignment().dim(dim);\n+        }\n+        if (e_config->windowing_dims.empty()) {\n+          loop_partitions = num_partitions;\n+        }\n+\n+        VLOG(2) << \"Emit windowed dot.\";\n+        return EmitWindowedDotGeneral(\n+            lhs, rhs, output_base_shape, output_sharding, dims_mapping,\n+            num_partitions, loop_partitions, create_sharded_dot, conv_window,\n+            module, original_hlo, options, b, windowed_dot_general_loops,\n+            *e_config, indices_map, lhs_sharding_transposed_to_match_output,\n+            rhs_sharding_transposed_to_match_output,\n+            rhs_sharding_transposed_to_match_lhs,\n+            lhs_sharding_transposed_to_match_rhs,\n+            output_sharding_transposed_to_match_rhs,\n+            output_sharding_transposed_to_match_lhs);\n+      }\n+    }\n   }\n \n   {\n@@ -1942,8 +2029,7 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n \n     lhs = lhs.PadWithZero();\n     rhs = rhs.PadWithZero();\n-    TF_ASSIGN_OR_RETURN(\n-        auto dot, create_sharded_dot(lhs.hlo(), rhs.hlo(), b, conv_window));\n+    TF_ASSIGN_OR_RETURN(auto dot, create_sharded_dot(lhs, rhs, b, conv_window));\n     std::vector<int64_t> lhs_contracting_dims;\n     lhs_contracting_dims.reserve(dims_mapping.contracting_dims.size());\n     for (const auto& cd : dims_mapping.contracting_dims) {\n@@ -1963,14 +2049,14 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n   if (lhs_non_contracting_partitions == num_partitions &&\n       output_lhs_non_contracting_partitions == num_partitions &&\n       lhs_sharding_transposed_to_match_output == output_sharding) {\n-    return create_sharded_dot(lhs.hlo(), rhs.Replicate().hlo(), b, conv_window);\n+    return create_sharded_dot(lhs, rhs.Replicate(), b, conv_window);\n   }\n \n   // RHS and output have the same partitioned non-contracting dimensions.\n   if (rhs_non_contracting_partitions == num_partitions &&\n       output_rhs_non_contracting_partitions == num_partitions &&\n       rhs_sharding_transposed_to_match_output == output_sharding) {\n-    return create_sharded_dot(lhs.Replicate().hlo(), rhs.hlo(), b, conv_window);\n+    return create_sharded_dot(lhs.Replicate(), rhs, b, conv_window);\n   }\n \n   if (may_reshard_if_mismatch) {\n@@ -1980,24 +2066,21 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n           lhs.Reshard(*output_sharding_transposed_to_match_lhs);\n       auto resharded_rhs =\n           rhs.Reshard(*output_sharding_transposed_to_match_rhs);\n-      return create_sharded_dot(resharded_lhs.hlo(), resharded_rhs.hlo(), b,\n-                                conv_window);\n+      return create_sharded_dot(resharded_lhs, resharded_rhs, b, conv_window);\n     }\n     // Output is partitioned along LHS non-contracting dimensions.\n     if (output_lhs_non_contracting_partitions == num_partitions) {\n       auto resharded_lhs =\n           lhs.Reshard(*output_sharding_transposed_to_match_lhs);\n       auto replicated_rhs = rhs.Replicate();\n-      return create_sharded_dot(resharded_lhs.hlo(), replicated_rhs.hlo(), b,\n-                                conv_window);\n+      return create_sharded_dot(resharded_lhs, replicated_rhs, b, conv_window);\n     }\n     // Output is partitioned along RHS non-contracting dimensions.\n     if (output_rhs_non_contracting_partitions == num_partitions) {\n       auto replicated_lhs = lhs.Replicate();\n       auto resharded_rhs =\n           rhs.Reshard(*output_sharding_transposed_to_match_rhs);\n-      return create_sharded_dot(replicated_lhs.hlo(), resharded_rhs.hlo(), b,\n-                                conv_window);\n+      return create_sharded_dot(replicated_lhs, resharded_rhs, b, conv_window);\n     }\n   }\n \n@@ -2033,8 +2116,7 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n   lhs = lhs.PadWithZero();\n   rhs = rhs.PadWithZero();\n \n-  TF_ASSIGN_OR_RETURN(auto dot,\n-                      create_sharded_dot(lhs.hlo(), rhs.hlo(), b, conv_window));\n+  TF_ASSIGN_OR_RETURN(auto dot, create_sharded_dot(lhs, rhs, b, conv_window));\n \n   std::vector<int64_t> lhs_contracting_dims;\n   lhs_contracting_dims.reserve(dims_mapping.contracting_dims.size());\n@@ -2047,42 +2129,29 @@ absl::StatusOr<HloInstruction*> PartitionBaseCase(\n       MakeBinaryAdd(output_base_shape.element_type(), module));\n }\n \n-absl::StatusOr<HloInstruction*> PartitionDot(\n-    const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n-    const Shape& output_base_shape, const HloSharding& output_sharding,\n-    const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n-    const SpmdPartitionerOptions& options, SpmdBuilder* b,\n-    std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n-        windowed_dot_general_loops,\n-    SpmdPartitioningVisitor* visitor);\n+}  // namespace\n \n+namespace {\n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionDotGroupOnBatchImpl(\n-    PartitionedHlo lhs, PartitionedHlo rhs, const Shape& output_base_shape,\n-    const HloSharding& output_sharding,\n+    PartitionedHloMaybeMX lhs, PartitionedHloMaybeMX rhs,\n+    const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n     int64_t lhs_contracting_partitions, int64_t rhs_contracting_partitions,\n     int64_t lhs_non_contracting_partitions,\n     int64_t rhs_non_contracting_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     bool require_matching_devices_to_group,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n     SpmdPartitioningVisitor* visitor) {\n-  std::vector<std::pair<HloInstruction*, HloSharding>>\n+  std::vector<std::pair<PartitionedHloMaybeMX, HloSharding>>\n       top_level_sharding_to_reset;\n-  absl::Cleanup cleaner = [&] {\n+  absl::Cleanup cleaner = [&top_level_sharding_to_reset] {\n     for (auto& to_reset : top_level_sharding_to_reset) {\n-      to_reset.first->set_sharding(to_reset.second);\n+      to_reset.first.set_sharding(to_reset.second);\n     }\n   };\n   std::vector<int64_t> lhs_dims;\n@@ -2128,8 +2197,8 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnBatchImpl(\n   }\n   auto output_grouped =\n       hlo_sharding_util::GroupShardingOnDims(output_sharding, output_dims);\n-  PartitionedHlo per_group_lhs = lhs;\n-  PartitionedHlo per_group_rhs = rhs;\n+  PartitionedHloMaybeMX per_group_lhs = lhs;\n+  PartitionedHloMaybeMX per_group_rhs = rhs;\n   if (lhs_rhs_dims_matching) {\n     auto lhs_grouped =\n         hlo_sharding_util::GroupShardingOnDims(lhs.sharding(), lhs_dims);\n@@ -2154,38 +2223,34 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnBatchImpl(\n         lhs_grouped);\n     auto per_group_partitioner_state = CreatePerGroupPartitioningState(\n         lhs.state(), lhs_grouped.device_groups, b);\n-    top_level_sharding_to_reset.emplace_back(lhs.hlo(), lhs.sharding());\n-    lhs.hlo()->set_sharding(lhs_grouped.sharding);\n-    top_level_sharding_to_reset.emplace_back(rhs.hlo(), rhs.sharding());\n-    rhs.hlo()->set_sharding(rhs_grouped.sharding);\n+    top_level_sharding_to_reset.emplace_back(lhs, lhs.sharding());\n+    lhs.set_sharding(lhs_grouped.sharding);\n+    top_level_sharding_to_reset.emplace_back(rhs, rhs.sharding());\n+    rhs.set_sharding(rhs_grouped.sharding);\n     CHECK(lhs.hlo() != rhs.hlo() ||\n           lhs_grouped.sharding == rhs_grouped.sharding);\n-    per_group_lhs = PartitionedHlo(\n-        lhs.hlo(), GetPerGroupBaseShape(lhs_grouped, lhs.base_shape()),\n+    per_group_lhs = MakePartitionedHloMaybeMX(\n+        lhs, GetPerGroupBaseShape(lhs_grouped, lhs.base_shape()),\n         per_group_partitioner_state);\n-    per_group_rhs = PartitionedHlo(\n-        rhs.hlo(), GetPerGroupBaseShape(rhs_grouped, rhs.base_shape()),\n+    per_group_rhs = MakePartitionedHloMaybeMX(\n+        rhs, GetPerGroupBaseShape(rhs_grouped, rhs.base_shape()),\n         per_group_partitioner_state);\n   } else {\n     auto per_group_partitioner_state = CreatePerGroupPartitioningState(\n         lhs.state(), output_grouped.device_groups, b);\n     auto reshard_to_output_batch =\n-        [&](const PartitionedHlo& operand, absl::Span<const int64_t> batch_dims,\n+        [&](const PartitionedHloMaybeMX& operand,\n+            absl::Span<const int64_t> batch_dims,\n             absl::Span<const int64_t> contracting_dims,\n             absl::Span<const int64_t> non_contracting_dims,\n             int64_t contracting_dim_partitions,\n             int64_t non_contracting_dim_partitions,\n             int64_t other_contracting_dim_partitions,\n             std::vector<int64_t>* sharding_dims_adjusted_to_output)\n-        -> std::optional<PartitionedHlo> {\n+        -> std::optional<PartitionedHloMaybeMX> {\n       if (operand.sharding().IsTileMaximal()) {\n-        auto partially_sharded = PerGroupSliceFromReplicated(\n-            operand.Replicate().hlo(), operand.state().partition_id,\n-            output_grouped.device_groups, batch_dims,\n-            output_grouped.group_dim_sizes, b);\n-        partially_sharded->set_sharding(HloSharding::Replicate());\n-        return PartitionedHlo(partially_sharded, partially_sharded->shape(),\n-                              per_group_partitioner_state);\n+        return ReplicatePartiallySharded(operand, batch_dims, output_grouped, b,\n+                                         per_group_partitioner_state);\n       }\n       auto& original_tiling = operand.sharding().tile_assignment();\n       // It's possible that the operand is not initially sharded on batch\n@@ -2245,12 +2310,11 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnBatchImpl(\n         return std::nullopt;\n       }\n       auto resharded = operand.Reshard(UngroupSharding(grouped));\n-      top_level_sharding_to_reset.emplace_back(resharded.hlo(),\n-                                               resharded.sharding());\n-      resharded.hlo()->set_sharding(grouped.sharding);\n-      return PartitionedHlo(resharded.hlo(),\n-                            GetPerGroupBaseShape(grouped, operand.base_shape()),\n-                            per_group_partitioner_state);\n+      top_level_sharding_to_reset.emplace_back(resharded, resharded.sharding());\n+      resharded.set_sharding(grouped.sharding);\n+      return MakePartitionedHloMaybeMX(\n+          resharded, GetPerGroupBaseShape(grouped, operand.base_shape()),\n+          per_group_partitioner_state);\n     };\n     std::vector<int64_t> lhs_contracting_dims;\n     std::vector<int64_t> rhs_contracting_dims;\n@@ -2446,31 +2510,29 @@ GetNonContractingPartitionGroupedShardingForOtherOperand(\n   return std::nullopt;\n }\n \n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionDotGroupOnNonContractingImpl(\n-    bool lhs_matching, PartitionedHlo matching, PartitionedHlo other,\n-    int64_t matching_contracting_partitions,\n+    bool lhs_matching, PartitionedHloMaybeMX matching,\n+    PartitionedHloMaybeMX other, int64_t matching_contracting_partitions,\n     int64_t other_contracting_partitions,\n     absl::Span<const DotConvolutionDimsInfo::DimNums>\n         partitioned_non_contracting_dims,\n     int64_t other_non_contracting_partitions,\n     int64_t output_other_non_contracting_partitions,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     bool require_matching_devices_to_group,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n     SpmdPartitioningVisitor* visitor) {\n-  std::vector<std::pair<HloInstruction*, HloSharding>>\n+  std::vector<std::pair<PartitionedHloMaybeMX, HloSharding>>\n       top_level_sharding_to_reset;\n-  absl::Cleanup cleaner = [&] {\n+  absl::Cleanup cleaner = [&top_level_sharding_to_reset] {\n     for (auto& to_reset : top_level_sharding_to_reset) {\n-      to_reset.first->set_sharding(to_reset.second);\n+      to_reset.first.set_sharding(to_reset.second);\n     }\n   };\n \n@@ -2491,7 +2553,7 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnNonContractingImpl(\n   }\n \n   auto try_sharding_for_other_operand = [&](const HloSharding& sharding) {\n-    PartitionedHlo other_reshard = other.Reshard(sharding);\n+    PartitionedHloMaybeMX other_reshard = other.Reshard(sharding);\n     std::optional<GroupedSharding> grouped_sharding =\n         GetNonContractingPartitionGroupedShardingForOtherOperand(\n             lhs_matching, output_base_shape, other_reshard.hlo()->shape(),\n@@ -2526,21 +2588,20 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnNonContractingImpl(\n   matching = matching.Reshard(UngroupSharding(matching_grouped));\n   auto per_group_partitioner_state = CreatePerGroupPartitioningState(\n       matching.state(), matching_grouped.device_groups, b);\n-  top_level_sharding_to_reset.emplace_back(matching.hlo(), matching.sharding());\n-  matching.hlo()->set_sharding(matching_grouped.sharding);\n-  auto matching_p = PartitionedHlo(\n-      matching.hlo(),\n-      GetPerGroupBaseShape(matching_grouped, matching.base_shape()),\n+  top_level_sharding_to_reset.emplace_back(matching, matching.sharding());\n+  matching.set_sharding(matching_grouped.sharding);\n+  PartitionedHloMaybeMX matching_p = MakePartitionedHloMaybeMX(\n+      matching, GetPerGroupBaseShape(matching_grouped, matching.base_shape()),\n       per_group_partitioner_state);\n \n-  auto partially_replicated_other = other.hlo();\n+  PartitionedHloMaybeMX partially_replicated_other = other;\n   if (other_grouped && other_grouped->group_dims.size() == 1 &&\n       other_grouped->group_dims[0] == other.base_shape().dimensions().size()) {\n     // Group on replication dim.\n     other = other.Reshard(UngroupSharding(*other_grouped));\n-    partially_replicated_other = other.hlo();\n-    top_level_sharding_to_reset.emplace_back(other.hlo(), other.sharding());\n-    partially_replicated_other->set_sharding(other_grouped->sharding);\n+    partially_replicated_other = other;\n+    top_level_sharding_to_reset.emplace_back(other, other.sharding());\n+    partially_replicated_other.set_sharding(other_grouped->sharding);\n   } else if (other_grouped && !other.sharding().IsReplicated()) {\n     HloSharding target_sharding = UngroupSharding(*other_grouped);\n     GroupedSharding target_group_sharding =\n@@ -2557,16 +2618,15 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnNonContractingImpl(\n       other = other.Reshard(target_sharding);\n     }\n     partially_replicated_other =\n-        other\n-            .Reshard(hlo_sharding_util::PartiallyReplicateTiledShardingOnDims(\n-                other.sharding(), other_grouped->group_dims))\n-            .hlo();\n+        other.Reshard(hlo_sharding_util::PartiallyReplicateTiledShardingOnDims(\n+            other.sharding(), other_grouped->group_dims));\n     top_level_sharding_to_reset.emplace_back(\n-        partially_replicated_other, partially_replicated_other->sharding());\n-    partially_replicated_other->set_sharding(other_grouped->sharding);\n+        partially_replicated_other, partially_replicated_other.sharding());\n+    partially_replicated_other.set_sharding(other_grouped->sharding);\n   }\n \n-  auto other_p = PartitionedHlo(partially_replicated_other, other.base_shape(),\n+  PartitionedHloMaybeMX other_p =\n+      MakePartitionedHloMaybeMX(partially_replicated_other, other.base_shape(),\n                                 per_group_partitioner_state);\n   return PartitionDot(\n       lhs_matching ? matching_p : other_p, lhs_matching ? other_p : matching_p,\n@@ -2709,9 +2769,10 @@ GetDotGroupPartitionContractingOutputShardings(\n   return std::make_pair(inner_output_sharding, outer_output_tmp_sharding);\n }\n \n+template <typename PartitionedHloMaybeMX>\n std::pair<HloSharding, HloSharding>\n GetDotGroupPartitionContractingLhsRhsShardings(\n-    const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n     absl::Span<const DotConvolutionDimsInfo::DimNums>\n         partitioned_contracting_dims) {\n   HloSharding lhs_sharding = lhs.sharding();\n@@ -2743,30 +2804,28 @@ GetDotGroupPartitionContractingLhsRhsShardings(\n   return std::make_pair(lhs_sharding, rhs_sharding);\n }\n \n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionDotGroupOnContractingImpl(\n-    PartitionedHlo lhs, PartitionedHlo rhs,\n+    PartitionedHloMaybeMX lhs, PartitionedHloMaybeMX rhs,\n     absl::Span<const DotConvolutionDimsInfo::DimNums>\n         partitioned_contracting_dims,\n     int64_t output_batch_partitions,\n     int64_t output_lhs_non_contracting_partitions,\n     int64_t output_rhs_non_contracting_partitions,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     bool require_matching_devices_to_group,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n     SpmdPartitioningVisitor* visitor) {\n-  std::vector<std::pair<HloInstruction*, HloSharding>>\n+  std::vector<std::pair<PartitionedHloMaybeMX, HloSharding>>\n       top_level_sharding_to_reset;\n-  absl::Cleanup cleaner = [&] {\n+  absl::Cleanup cleaner = [&top_level_sharding_to_reset] {\n     for (auto& to_reset : top_level_sharding_to_reset) {\n-      to_reset.first->set_sharding(to_reset.second);\n+      to_reset.first.set_sharding(to_reset.second);\n     }\n   };\n   std::vector<int64_t> lhs_dims;\n@@ -2806,10 +2865,10 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnContractingImpl(\n   lhs = lhs.PadWithZeroOnSpecifiedDims(lhs_dims);\n   rhs = rhs.PadWithZeroOnSpecifiedDims(rhs_dims);\n \n-  top_level_sharding_to_reset.emplace_back(lhs.hlo(), lhs_sharding);\n-  lhs.hlo()->set_sharding(lhs_grouped.sharding);\n-  top_level_sharding_to_reset.emplace_back(rhs.hlo(), rhs_sharding);\n-  rhs.hlo()->set_sharding(rhs_grouped.sharding);\n+  top_level_sharding_to_reset.emplace_back(lhs, lhs_sharding);\n+  lhs.set_sharding(lhs_grouped.sharding);\n+  top_level_sharding_to_reset.emplace_back(rhs, rhs_sharding);\n+  rhs.set_sharding(rhs_grouped.sharding);\n \n   HloSharding inner_output_sharding = HloSharding::Replicate();\n   HloSharding outer_output_tmp_sharding = HloSharding::Replicate();\n@@ -2837,11 +2896,21 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnContractingImpl(\n         hlo_sharding_util::PartiallyReplicateTiledShardingOnDims(\n             output_sharding, get_non_slice_dims()));\n   }\n-  auto inner_creator =\n-      [&](HloInstruction* l, HloInstruction* r, SpmdBuilder* b,\n-          const Window& conv_window) -> absl::StatusOr<HloInstruction*> {\n-    TF_ASSIGN_OR_RETURN(auto inner_dot,\n-                        create_sharded_dot(l, r, b, conv_window));\n+\n+  std::function<absl::StatusOr<HloInstruction*>(const PartitionedHloMaybeMX&,\n+                                                const PartitionedHloMaybeMX&,\n+                                                SpmdBuilder*, const Window&)>\n+      inner_creator =\n+          [&](const PartitionedHloMaybeMX& l, const PartitionedHloMaybeMX& r,\n+              SpmdBuilder* b,\n+              const Window& conv_window) -> absl::StatusOr<HloInstruction*> {\n+    // inner_creator will become create_sharded_dot's operator() target. Call\n+    // create_sharded_dot's original CreateSharded function here by setting\n+    // call_custom_create_sharded to false.\n+    TF_ASSIGN_OR_RETURN(\n+        auto inner_dot,\n+        create_sharded_dot(l, r, b, conv_window,\n+                           /*call_custom_create_sharded=*/false));\n     HloInstruction* result = inner_dot;\n     if (!output_slice_dims.empty()) {\n       // Create an AllReduce along slice dims first to allow a reduce-scatter.\n@@ -2868,7 +2937,7 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnContractingImpl(\n       if (output_replicate_dim_grouped) {\n         result = lhs.state().partitioner->AllReduceAlongShardingDims(\n             b, result, outer_output_tmp_sharding, lhs.state().next_channel_id,\n-            {output_base_shape.dimensions_size()},\n+            {static_cast<int64_t>(output_base_shape.dimensions().size())},\n             lhs.state().collective_ops_creator,\n             MakeBinaryAdd(output_base_shape.element_type(), module));\n       }\n@@ -2897,31 +2966,35 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnContractingImpl(\n     TF_ASSIGN_OR_RETURN(\n         maybe_windowed_dot,\n         PartitionDot(\n-            PartitionedHlo(lhs.hlo(),\n-                           GetPerGroupBaseShape(lhs_grouped, lhs.base_shape()),\n-                           inner_state),\n-            PartitionedHlo(rhs.hlo(),\n-                           GetPerGroupBaseShape(rhs_grouped, rhs.base_shape()),\n-                           inner_state),\n+            MakePartitionedHloMaybeMX(\n+                lhs, GetPerGroupBaseShape(lhs_grouped, lhs.base_shape()),\n+                inner_state),\n+            MakePartitionedHloMaybeMX(\n+                rhs, GetPerGroupBaseShape(rhs_grouped, rhs.base_shape()),\n+                inner_state),\n             predicted_inner_output_base_shape, inner_output_sharding,\n             dims_mapping, num_partitions / group_count, predicted_inner_creator,\n             conv_window, module, original_hlo, options, b,\n             windowed_dot_general_loops, visitor));\n   }\n   int new_num_windowed_loops = windowed_dot_general_loops->size();\n \n+  // create_sharded_dot's operator() will call inner_creator instead of\n+  // its CreateSharded function.\n+  create_sharded_dot.SetCustomCreateSharded(std::move(inner_creator));\n+\n   TF_ASSIGN_OR_RETURN(\n       auto inner_dot,\n-      PartitionDot(\n-          PartitionedHlo(lhs.hlo(),\n-                         GetPerGroupBaseShape(lhs_grouped, lhs.base_shape()),\n-                         inner_state),\n-          PartitionedHlo(rhs.hlo(),\n-                         GetPerGroupBaseShape(rhs_grouped, rhs.base_shape()),\n-                         inner_state),\n-          inner_output_base_shape, inner_output_sharding, dims_mapping,\n-          num_partitions / group_count, inner_creator, conv_window, module,\n-          original_hlo, options, b, windowed_dot_general_loops, visitor));\n+      PartitionDot(MakePartitionedHloMaybeMX(\n+                       lhs, GetPerGroupBaseShape(lhs_grouped, lhs.base_shape()),\n+                       inner_state),\n+                   MakePartitionedHloMaybeMX(\n+                       rhs, GetPerGroupBaseShape(rhs_grouped, rhs.base_shape()),\n+                       inner_state),\n+                   inner_output_base_shape, inner_output_sharding, dims_mapping,\n+                   num_partitions / group_count, create_sharded_dot,\n+                   conv_window, module, original_hlo, options, b,\n+                   windowed_dot_general_loops, visitor));\n \n   // Reenables the inner reshard if there is an inner dot and no actual\n   // windowed_dot_general_loops generated.\n@@ -2943,10 +3016,9 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnContractingImpl(\n   }\n \n   maybe_windowed_dot->set_sharding(outer_output_tmp_sharding);\n-  auto d = PartitionedHlo(maybe_windowed_dot, output_base_shape, lhs.state())\n-               .Reshard(output_sharding)\n-               .hlo();\n-  return d;\n+  return PartitionedHlo(maybe_windowed_dot, output_base_shape, lhs.state())\n+      .Reshard(output_sharding)\n+      .hlo();\n }\n \n DotConvolutionDimsInfo ConvertDimNumsWithFeatureGroupCount(\n@@ -3117,7 +3189,7 @@ EstimateWindowedEinsumIterationsForNonContractingPartitioning(\n     const int64_t new_num_partitions =\n         num_partitions / matching_non_contracting_partitions;\n     std::optional<WindowedEinsumConfig> e_config =\n-        GetWindowedEinsumConfiguration(\n+        GetWindowedEinsumConfiguration<CreateShardedDotFunctor>(\n             new_num_partitions, output_matching_non_contracting_partitions,\n             output_other_non_contracting_partitions,\n             other_contracting_partitions, other_non_contracting_partitions,\n@@ -3154,6 +3226,7 @@ EstimateWindowedEinsumIterationsForNonContractingPartitioning(\n // The general idea is similar as the one in\n // LhsIsBestMatchForNonContractingPartitioning with one all-gather replaced by\n // reduce-scatter.\n+template <typename CreateShardedFunctor>\n bool PrioritizeContractingDimensionsPartitioning(\n     const DotConvolutionDimsInfo& dims_mapping, const PartitionedHlo& lhs,\n     const PartitionedHlo& rhs, const Shape& output_base_shape,\n@@ -3165,11 +3238,7 @@ bool PrioritizeContractingDimensionsPartitioning(\n     int64_t output_rhs_non_contracting_partitions, int64_t lhs_batch_partitions,\n     int64_t rhs_batch_partitions, int64_t output_batch_partitions,\n     bool require_matching_devices_to_group, SpmdBuilder* b,\n-    const Window& conv_window,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n+    const Window& conv_window, const CreateShardedFunctor& create_sharded_dot,\n     SpmdPartitioningVisitor* visitor) {\n   const bool may_group_on_lhs_non_contracting =\n       lhs_non_contracting_partitions == output_lhs_non_contracting_partitions &&\n@@ -3301,20 +3370,21 @@ bool PrioritizeContractingDimensionsPartitioning(\n       hlo_sharding_util::TransposeShardingWithCollapsedDims(\n           rhs_sharding, indices_map.rhs_to_lhs_indices,\n           indices_map.lhs_to_rhs_indices);\n-  std::optional<WindowedEinsumConfig> e_config = GetWindowedEinsumConfiguration(\n-      new_num_partitions, new_output_lhs_non_contracting_partitions,\n-      new_output_rhs_non_contracting_partitions, 1,\n-      rhs_non_contracting_partitions, rhs_batch_partitions, 1,\n-      lhs_non_contracting_partitions, lhs_batch_partitions,\n-      ShapeSizeInBytes(GetPerGroupBaseShape(rhs_grouped, rhs.base_shape())),\n-      ShapeSizeInBytes(GetPerGroupBaseShape(lhs_grouped, lhs.base_shape())),\n-      ShapeSizeInBytes(inner_output_base_shape), options,\n-      output_sharding_transposed_to_match_lhs,\n-      output_sharding_transposed_to_match_rhs,\n-      lhs_sharding_transposed_to_match_rhs,\n-      rhs_sharding_transposed_to_match_lhs, lhs_grouped.sharding,\n-      output_sharding, rhs_grouped.sharding, conv_window, dims_mapping,\n-      indices_map, visitor->call_graph());\n+  std::optional<WindowedEinsumConfig> e_config =\n+      GetWindowedEinsumConfiguration<CreateShardedFunctor>(\n+          new_num_partitions, new_output_lhs_non_contracting_partitions,\n+          new_output_rhs_non_contracting_partitions, 1,\n+          rhs_non_contracting_partitions, rhs_batch_partitions, 1,\n+          lhs_non_contracting_partitions, lhs_batch_partitions,\n+          ShapeSizeInBytes(GetPerGroupBaseShape(rhs_grouped, rhs.base_shape())),\n+          ShapeSizeInBytes(GetPerGroupBaseShape(lhs_grouped, lhs.base_shape())),\n+          ShapeSizeInBytes(inner_output_base_shape), options,\n+          output_sharding_transposed_to_match_lhs,\n+          output_sharding_transposed_to_match_rhs,\n+          lhs_sharding_transposed_to_match_rhs,\n+          rhs_sharding_transposed_to_match_lhs, lhs_grouped.sharding,\n+          output_sharding, rhs_grouped.sharding, conv_window, dims_mapping,\n+          indices_map, visitor->call_graph());\n   if (!e_config) {\n     return false;\n   }\n@@ -3359,9 +3429,14 @@ bool PrioritizeContractingDimensionsPartitioning(\n   *other_hlo->mutable_shape() =\n       GetPerGroupBaseShape(other_grouped, other_base_shape);\n   HloInstruction* dot =\n-      create_sharded_dot(lhs_matching_iterations ? lhs.hlo() : other_hlo,\n-                         lhs_matching_iterations ? other_hlo : rhs.hlo(), b,\n-                         conv_window)\n+      create_sharded_dot(\n+          lhs_matching_iterations\n+              ? lhs\n+              : PartitionedHlo(other_hlo, other_base_shape, rhs.state()),\n+          lhs_matching_iterations\n+              ? PartitionedHlo(other_hlo, other_base_shape, lhs.state())\n+              : rhs,\n+          b, conv_window)\n           .value();\n   const double computation_time_in_ms =\n       visitor->GetComputationTimeInMilliSec(dot);\n@@ -3394,21 +3469,20 @@ bool PrioritizeContractingDimensionsPartitioning(\n \n // Return if it would be better to match the LHS operand or RHS operand\n // of a dot for non-contracting partitioning.\n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n bool LhsIsBestMatchForNonContractingPartitioning(\n-    const DotConvolutionDimsInfo& dims_mapping, const PartitionedHlo& lhs,\n-    const PartitionedHlo& rhs, const Shape& output_base_shape,\n-    const HloSharding& output_sharding, const SpmdPartitionerOptions& options,\n-    int64_t num_partitions, int64_t lhs_non_contracting_partitions,\n+    const DotConvolutionDimsInfo& dims_mapping,\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n+    const Shape& output_base_shape, const HloSharding& output_sharding,\n+    const SpmdPartitionerOptions& options, int64_t num_partitions,\n+    int64_t lhs_non_contracting_partitions,\n     int64_t rhs_non_contracting_partitions, int64_t lhs_matching_partitions,\n     int64_t rhs_matching_partitions, int64_t lhs_contracting_partitions,\n     int64_t rhs_contracting_partitions,\n     int64_t output_lhs_non_contracting_partitions,\n     int64_t output_rhs_non_contracting_partitions, int64_t lhs_batch_partitions,\n     int64_t rhs_batch_partitions, SpmdBuilder* b, const Window& conv_window,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n+    const CreateShardedFunctor& create_sharded_dot,\n     SpmdPartitioningVisitor* visitor) {\n   const bool may_group_on_lhs_non_contracting =\n       lhs_non_contracting_partitions == output_lhs_non_contracting_partitions &&\n@@ -3432,94 +3506,102 @@ bool LhsIsBestMatchForNonContractingPartitioning(\n   // with the smaller all_gather as it has potentially smaller extra\n   // collective-permute overhead outside of the while loop; 2) Otherwise, we\n   // choose the all_gather with longer runtime to overlap with.\n-  if (may_group_on_lhs_non_contracting && may_group_on_rhs_non_contracting &&\n-      options.choose_faster_windowed_einsum_over_mem) {\n-    const DotDimensionIndexMapping indices_map = ComputeDimensionIndexMapping(\n-        dims_mapping, lhs.base_shape().dimensions().size(),\n-        rhs.base_shape().dimensions().size(),\n-        output_base_shape.dimensions().size());\n-    std::optional<int64_t> lhs_matching_iterations;\n-    std::optional<int64_t> rhs_matching_iterations;\n-    std::tie(lhs_matching_iterations, rhs_matching_iterations) =\n-        EstimateWindowedEinsumIterationsForNonContractingPartitioning(\n-            dims_mapping, lhs, rhs, output_base_shape, output_sharding, options,\n-            num_partitions, lhs_non_contracting_partitions,\n-            rhs_non_contracting_partitions, lhs_matching_partitions,\n-            rhs_matching_partitions, lhs_contracting_partitions,\n-            rhs_contracting_partitions, output_lhs_non_contracting_partitions,\n-            output_rhs_non_contracting_partitions, lhs_batch_partitions,\n-            rhs_batch_partitions, conv_window, visitor);\n-    if (lhs_matching_iterations && rhs_matching_iterations) {\n-      const int64_t lhs_all_gather_bytes =\n-          ShapeUtil::ByteSizeOf(lhs.hlo()->shape()) *\n-          rhs_non_contracting_partitions;\n-      const int64_t rhs_all_gather_bytes =\n-          ShapeUtil::ByteSizeOf(rhs.hlo()->shape()) *\n-          lhs_non_contracting_partitions;\n-      auto lhs_grouped =\n-          GetNonContractingPartitionGroupedShardingForMatchedOperand(\n-              /*lhs_matching=*/true, lhs.sharding(), output_sharding,\n-              dims_mapping.lhs_non_contracting_dims);\n-      auto lhs_all_gather_subgroups = lhs_grouped.device_groups;\n-      auto rhs_grouped =\n-          GetNonContractingPartitionGroupedShardingForMatchedOperand(\n-              /*lhs_matching=*/false, rhs.sharding(), output_sharding,\n-              dims_mapping.rhs_non_contracting_dims);\n-      auto rhs_all_gather_subgroups = rhs_grouped.device_groups;\n-      const double lhs_all_gather_time_in_ms =\n-          visitor->GetCommunicationTimeInMilliSec(\n-              lhs_all_gather_bytes,\n-              CollectiveDeviceList(\n-                  visitor->CreateReplicaGroups(lhs_all_gather_subgroups)));\n-      const double rhs_all_gather_time_in_ms =\n-          visitor->GetCommunicationTimeInMilliSec(\n-              rhs_all_gather_bytes,\n-              CollectiveDeviceList(\n-                  visitor->CreateReplicaGroups(rhs_all_gather_subgroups)));\n-\n-      HloInstruction* compute_lhs = lhs.hlo();\n-      Shape lhs_original_shape = compute_lhs->shape();\n-      *compute_lhs->mutable_shape() =\n-          GetPerGroupBaseShape(lhs_grouped, lhs.base_shape());\n-      HloInstruction* compute_rhs = rhs.hlo();\n-      Shape rhs_original_shape = compute_rhs->shape();\n-      *compute_rhs->mutable_shape() =\n-          GetPerGroupBaseShape(rhs_grouped, rhs.base_shape());\n-      HloInstruction* dot =\n-          create_sharded_dot(compute_lhs, compute_rhs, b, conv_window).value();\n-      const double computation_time_in_ms =\n-          visitor->GetComputationTimeInMilliSec(dot);\n-      *compute_lhs->mutable_shape() = lhs_original_shape;\n-      *compute_rhs->mutable_shape() = rhs_original_shape;\n-\n-      VLOG(2) << \"lhs: \" << lhs.hlo()->ToString() << \"\\n\"\n-              << \"rhs: \" << rhs.hlo()->ToString() << \"\\n\"\n-              << \"lhs_non_contracting_partitions: \"\n-              << lhs_non_contracting_partitions\n-              << \" rhs_non_contracting_partitions: \"\n-              << rhs_non_contracting_partitions << \"\\n\"\n-              << \"lhs_matching_iterations: \" << *lhs_matching_iterations\n-              << \" rhs_matching_iterations: \" << *rhs_matching_iterations\n-              << \"\\n\"\n-              << \"lhs_all_gather_bytes: \" << lhs_all_gather_bytes\n-              << \" rhs_all_gather_bytes: \" << rhs_all_gather_bytes << \"\\n\"\n-              << \"lhs_all_gather_time_in_ms: \" << lhs_all_gather_time_in_ms\n-              << \" rhs_all_gather_time_in_ms: \" << rhs_all_gather_time_in_ms\n-              << \"\\n\"\n-              << \"dot: \" << dot->ToString() << \"\\n\"\n-              << \"computation_time_in_ms: \" << computation_time_in_ms;\n-      if (computation_time_in_ms == 0.0 || lhs_all_gather_time_in_ms == 0.0 ||\n-          rhs_all_gather_time_in_ms == 0.0) {\n-        lhs_matching = *lhs_matching_iterations < *rhs_matching_iterations;\n-      } else if ((computation_time_in_ms <= lhs_all_gather_time_in_ms) &&\n-                 (computation_time_in_ms <= rhs_all_gather_time_in_ms)) {\n-        lhs_matching = lhs_all_gather_bytes / rhs_non_contracting_partitions >\n-                       rhs_all_gather_bytes / lhs_non_contracting_partitions;\n+  //\n+  // Disable windowed einsums for block-scaled dot.\n+  if constexpr (std::is_same_v<PartitionedHloMaybeMX, PartitionedHlo>) {\n+    if (may_group_on_lhs_non_contracting && may_group_on_rhs_non_contracting &&\n+        options.choose_faster_windowed_einsum_over_mem) {\n+      const DotDimensionIndexMapping indices_map = ComputeDimensionIndexMapping(\n+          dims_mapping, lhs.base_shape().dimensions().size(),\n+          rhs.base_shape().dimensions().size(),\n+          output_base_shape.dimensions().size());\n+      std::optional<int64_t> lhs_matching_iterations;\n+      std::optional<int64_t> rhs_matching_iterations;\n+      std::tie(lhs_matching_iterations, rhs_matching_iterations) =\n+          EstimateWindowedEinsumIterationsForNonContractingPartitioning(\n+              dims_mapping, lhs, rhs, output_base_shape, output_sharding,\n+              options, num_partitions, lhs_non_contracting_partitions,\n+              rhs_non_contracting_partitions, lhs_matching_partitions,\n+              rhs_matching_partitions, lhs_contracting_partitions,\n+              rhs_contracting_partitions, output_lhs_non_contracting_partitions,\n+              output_rhs_non_contracting_partitions, lhs_batch_partitions,\n+              rhs_batch_partitions, conv_window, visitor);\n+      if (lhs_matching_iterations && rhs_matching_iterations) {\n+        const int64_t lhs_all_gather_bytes =\n+            ShapeUtil::ByteSizeOf(lhs.hlo()->shape()) *\n+            rhs_non_contracting_partitions;\n+        const int64_t rhs_all_gather_bytes =\n+            ShapeUtil::ByteSizeOf(rhs.hlo()->shape()) *\n+            lhs_non_contracting_partitions;\n+        auto lhs_grouped =\n+            GetNonContractingPartitionGroupedShardingForMatchedOperand(\n+                /*lhs_matching=*/true, lhs.sharding(), output_sharding,\n+                dims_mapping.lhs_non_contracting_dims);\n+        auto lhs_all_gather_subgroups = lhs_grouped.device_groups;\n+        auto rhs_grouped =\n+            GetNonContractingPartitionGroupedShardingForMatchedOperand(\n+                /*lhs_matching=*/false, rhs.sharding(), output_sharding,\n+                dims_mapping.rhs_non_contracting_dims);\n+        auto rhs_all_gather_subgroups = rhs_grouped.device_groups;\n+        const double lhs_all_gather_time_in_ms =\n+            visitor->GetCommunicationTimeInMilliSec(\n+                lhs_all_gather_bytes,\n+                CollectiveDeviceList(\n+                    visitor->CreateReplicaGroups(lhs_all_gather_subgroups)));\n+        const double rhs_all_gather_time_in_ms =\n+            visitor->GetCommunicationTimeInMilliSec(\n+                rhs_all_gather_bytes,\n+                CollectiveDeviceList(\n+                    visitor->CreateReplicaGroups(rhs_all_gather_subgroups)));\n+\n+        HloInstruction* compute_lhs = lhs.hlo();\n+        Shape lhs_original_shape = compute_lhs->shape();\n+        *compute_lhs->mutable_shape() =\n+            GetPerGroupBaseShape(lhs_grouped, lhs.base_shape());\n+        HloInstruction* compute_rhs = rhs.hlo();\n+        Shape rhs_original_shape = compute_rhs->shape();\n+        *compute_rhs->mutable_shape() =\n+            GetPerGroupBaseShape(rhs_grouped, rhs.base_shape());\n+        HloInstruction* dot =\n+            create_sharded_dot(\n+                PartitionedHlo(compute_lhs, lhs.base_shape(), lhs.state()),\n+                PartitionedHlo(compute_rhs, rhs.base_shape(), rhs.state()), b,\n+                conv_window)\n+                .value();\n+        const double computation_time_in_ms =\n+            visitor->GetComputationTimeInMilliSec(dot);\n+        *compute_lhs->mutable_shape() = lhs_original_shape;\n+        *compute_rhs->mutable_shape() = rhs_original_shape;\n+\n+        VLOG(2) << \"lhs: \" << lhs.hlo()->ToString() << \"\\n\"\n+                << \"rhs: \" << rhs.hlo()->ToString() << \"\\n\"\n+                << \"lhs_non_contracting_partitions: \"\n+                << lhs_non_contracting_partitions\n+                << \" rhs_non_contracting_partitions: \"\n+                << rhs_non_contracting_partitions << \"\\n\"\n+                << \"lhs_matching_iterations: \" << *lhs_matching_iterations\n+                << \" rhs_matching_iterations: \" << *rhs_matching_iterations\n+                << \"\\n\"\n+                << \"lhs_all_gather_bytes: \" << lhs_all_gather_bytes\n+                << \" rhs_all_gather_bytes: \" << rhs_all_gather_bytes << \"\\n\"\n+                << \"lhs_all_gather_time_in_ms: \" << lhs_all_gather_time_in_ms\n+                << \" rhs_all_gather_time_in_ms: \" << rhs_all_gather_time_in_ms\n+                << \"\\n\"\n+                << \"dot: \" << dot->ToString() << \"\\n\"\n+                << \"computation_time_in_ms: \" << computation_time_in_ms;\n+        if (computation_time_in_ms == 0.0 || lhs_all_gather_time_in_ms == 0.0 ||\n+            rhs_all_gather_time_in_ms == 0.0) {\n+          lhs_matching = *lhs_matching_iterations < *rhs_matching_iterations;\n+        } else if ((computation_time_in_ms <= lhs_all_gather_time_in_ms) &&\n+                   (computation_time_in_ms <= rhs_all_gather_time_in_ms)) {\n+          lhs_matching = lhs_all_gather_bytes / rhs_non_contracting_partitions >\n+                         rhs_all_gather_bytes / lhs_non_contracting_partitions;\n+        } else {\n+          lhs_matching = lhs_all_gather_time_in_ms > rhs_all_gather_time_in_ms;\n+        }\n       } else {\n-        lhs_matching = lhs_all_gather_time_in_ms > rhs_all_gather_time_in_ms;\n+        lhs_matching = lhs_matching_iterations.has_value();\n       }\n-    } else {\n-      lhs_matching = lhs_matching_iterations.has_value();\n     }\n   }\n   return lhs_matching;\n@@ -3530,10 +3612,7 @@ PartitionConvOnBatchOrFeatureGroupedDims(\n     const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n+    CreateShardedConvolutionFunctor& create_sharded_dot,\n     const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n@@ -3653,8 +3732,7 @@ PartitionConvOnBatchOrFeatureGroupedDims(\n             resharded_rhs.Reshard(*lhs_sharding_transposed_to_match_rhs);\n         TF_ASSIGN_OR_RETURN(\n             sharded_conv,\n-            create_sharded_dot(resharded_lhs.hlo(), resharded_rhs.hlo(), b,\n-                               conv_window));\n+            create_sharded_dot(resharded_lhs, resharded_rhs, b, conv_window));\n         auto lhs_sharding_transposed_to_match_output =\n             hlo_sharding_util::TransposeShardingWithCollapsedDims(\n                 resharded_lhs.sharding(), indices_map.lhs_to_output_indices,\n@@ -3672,8 +3750,7 @@ PartitionConvOnBatchOrFeatureGroupedDims(\n             resharded_lhs.Reshard(*rhs_sharding_transposed_to_match_lhs);\n         TF_ASSIGN_OR_RETURN(\n             sharded_conv,\n-            create_sharded_dot(resharded_lhs.hlo(), resharded_rhs.hlo(), b,\n-                               conv_window));\n+            create_sharded_dot(resharded_lhs, resharded_rhs, b, conv_window));\n         auto rhs_sharding_transposed_to_match_output =\n             hlo_sharding_util::TransposeShardingWithCollapsedDims(\n                 resharded_rhs.sharding(), indices_map.rhs_to_output_indices,\n@@ -3698,8 +3775,7 @@ PartitionConvOnBatchOrFeatureGroupedDims(\n             resharded_rhs.Reshard(*output_sharding_transposed_to_match_rhs);\n         TF_ASSIGN_OR_RETURN(\n             sharded_conv,\n-            create_sharded_dot(resharded_lhs.hlo(), resharded_rhs.hlo(), b,\n-                               conv_window));\n+            create_sharded_dot(resharded_lhs, resharded_rhs, b, conv_window));\n         sharded_conv->set_sharding(target_output_sharding);\n       }\n \n@@ -3716,10 +3792,7 @@ absl::StatusOr<std::optional<HloInstruction*>> PartitionConv(\n     const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n+    CreateShardedConvolutionFunctor& create_sharded_dot,\n     const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n@@ -3791,15 +3864,13 @@ absl::StatusOr<std::optional<HloInstruction*>> PartitionConv(\n   return std::nullopt;\n }\n \n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionDotGroupOnBatchDims(\n-    const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n@@ -3837,15 +3908,13 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnBatchDims(\n   return nullptr;\n }\n \n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionDotGroupOnNonContractingDims(\n-    const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n@@ -3910,16 +3979,21 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnNonContractingDims(\n       }\n     }\n   }\n-  const bool prioritize_contracting_for_faster_windowed_einsum =\n-      PrioritizeContractingDimensionsPartitioning(\n-          dims_mapping, lhs, rhs, output_base_shape, output_sharding, options,\n-          num_partitions, lhs_non_contracting_partitions,\n-          rhs_non_contracting_partitions, lhs_contracting_partitions,\n-          rhs_contracting_partitions, output_lhs_non_contracting_partitions,\n-          output_rhs_non_contracting_partitions, lhs_batch_partitions,\n-          rhs_batch_partitions, output_batch_partitions,\n-          require_matching_devices_to_group, b, conv_window, create_sharded_dot,\n-          visitor);\n+\n+  bool prioritize_contracting_for_faster_windowed_einsum = false;\n+  // Disable windowed einsum path for block-scaled dot.\n+  if constexpr (std::is_same_v<PartitionedHloMaybeMX, PartitionedHlo>) {\n+    prioritize_contracting_for_faster_windowed_einsum =\n+        PrioritizeContractingDimensionsPartitioning(\n+            dims_mapping, lhs, rhs, output_base_shape, output_sharding, options,\n+            num_partitions, lhs_non_contracting_partitions,\n+            rhs_non_contracting_partitions, lhs_contracting_partitions,\n+            rhs_contracting_partitions, output_lhs_non_contracting_partitions,\n+            output_rhs_non_contracting_partitions, lhs_batch_partitions,\n+            rhs_batch_partitions, output_batch_partitions,\n+            require_matching_devices_to_group, b, conv_window,\n+            create_sharded_dot, visitor);\n+  }\n   if (!(matching_dims.empty() ||\n         prioritize_contracting_for_faster_windowed_einsum)) {\n     TF_ASSIGN_OR_RETURN(\n@@ -3946,15 +4020,13 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnNonContractingDims(\n   return nullptr;\n }\n \n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionDotGroupOnContractingDims(\n-    const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n@@ -3987,6 +4059,7 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnContractingDims(\n       return dot;\n     }\n   }\n+\n   if (lhs_contracting_partitions > 1 && rhs_contracting_partitions > 1) {\n     // If part of contracting dims match, try them.\n     std::vector<DotConvolutionDimsInfo::DimNums> matching_dims;\n@@ -4015,33 +4088,32 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnContractingDims(\n   return nullptr;\n }\n \n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionDotRemovingOutputPartialReplication(\n-    const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n     bool require_matching_devices_to_group, SpmdPartitioningVisitor* visitor) {\n   if (lhs.sharding().IsReplicated() && rhs.sharding().IsReplicated() &&\n       output_sharding.ReplicateOnLastTileDim()) {\n     auto grouped_output = hlo_sharding_util::GroupShardingOnDims(\n-        output_sharding, {output_base_shape.dimensions_size()});\n+        output_sharding,\n+        {static_cast<int64_t>(output_base_shape.dimensions().size())});\n     auto inner_state = CreatePerGroupPartitioningState(\n         lhs.state(), grouped_output.device_groups, b);\n     TF_ASSIGN_OR_RETURN(\n         auto dot,\n-        PartitionDot(PartitionedHlo(lhs.hlo(), lhs.base_shape(), inner_state),\n-                     PartitionedHlo(rhs.hlo(), rhs.base_shape(), inner_state),\n-                     output_base_shape, grouped_output.sharding, dims_mapping,\n-                     output_sharding.NumTiles(), create_sharded_dot,\n-                     conv_window, module, original_hlo, options, b,\n-                     windowed_dot_general_loops, visitor));\n+        PartitionDot(\n+            MakePartitionedHloMaybeMX(lhs, lhs.base_shape(), inner_state),\n+            MakePartitionedHloMaybeMX(rhs, rhs.base_shape(), inner_state),\n+            output_base_shape, grouped_output.sharding, dims_mapping,\n+            output_sharding.NumTiles(), create_sharded_dot, conv_window, module,\n+            original_hlo, options, b, windowed_dot_general_loops, visitor));\n     if (dot) {\n       return dot;\n     }\n@@ -4052,15 +4124,13 @@ absl::StatusOr<HloInstruction*> PartitionDotRemovingOutputPartialReplication(\n // Recursive partitioning function. If there are partial dimensions matching\n // in the operands and output, group the devices and recursively partition\n // the in-group dot.\n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionDot(\n-    const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     bool require_matching_devices_to_group,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n@@ -4071,14 +4141,18 @@ absl::StatusOr<HloInstruction*> PartitionDot(\n   // Case 0: Try partition the purely spatially-partitioned convolution with\n   // convolution spatial dimension partitioned or depthwise parallel\n   // dimension partitioned.\n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<HloInstruction*> partitioned_conv,\n-      PartitionConv(lhs, rhs, output_base_shape, output_sharding, dims_mapping,\n-                    num_partitions, create_sharded_dot, conv_window, module,\n-                    original_hlo, options, b, windowed_dot_general_loops,\n-                    require_matching_devices_to_group, visitor));\n-  if (partitioned_conv.has_value()) {\n-    return partitioned_conv.value();\n+  if constexpr (std::is_same_v<CreateShardedFunctor,\n+                               CreateShardedConvolutionFunctor>) {\n+    TF_ASSIGN_OR_RETURN(\n+        std::optional<HloInstruction*> partitioned_conv,\n+        PartitionConv(lhs, rhs, output_base_shape, output_sharding,\n+                      dims_mapping, num_partitions, create_sharded_dot,\n+                      conv_window, module, original_hlo, options, b,\n+                      windowed_dot_general_loops,\n+                      require_matching_devices_to_group, visitor));\n+    if (partitioned_conv.has_value()) {\n+      return partitioned_conv.value();\n+    }\n   }\n \n   HloInstruction* partitioned_dot;\n@@ -4160,15 +4234,13 @@ absl::StatusOr<HloInstruction*> PartitionDot(\n }\n \n // Reshard the LHS and RHS to match the output sharding.\n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> ReshardLHSRHSToMatchOutputSharding(\n-    const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n     const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, SpmdBuilder* b) {\n+    const CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    SpmdBuilder* b) {\n   const bool consider_other_operand = false;\n   const bool may_combine_partial_sharding = false;\n   const HloSharding infered_lhs_sharding =\n@@ -4180,20 +4252,17 @@ absl::StatusOr<HloInstruction*> ReshardLHSRHSToMatchOutputSharding(\n           &output_sharding, &rhs.sharding(), 1, dims_mapping,\n           consider_other_operand, may_combine_partial_sharding);\n \n-  return create_sharded_dot(lhs.Reshard(infered_lhs_sharding).hlo(),\n-                            rhs.Reshard(infered_rhs_sharding).hlo(), b,\n-                            conv_window);\n+  return create_sharded_dot(lhs.Reshard(infered_lhs_sharding),\n+                            rhs.Reshard(infered_rhs_sharding), b, conv_window);\n }\n \n+template <typename PartitionedHloMaybeMX, typename CreateShardedFunctor>\n absl::StatusOr<HloInstruction*> PartitionDot(\n-    const PartitionedHlo& lhs, const PartitionedHlo& rhs,\n+    const PartitionedHloMaybeMX& lhs, const PartitionedHloMaybeMX& rhs,\n     const Shape& output_base_shape, const HloSharding& output_sharding,\n     const DotConvolutionDimsInfo& dims_mapping, int64_t num_partitions,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot,\n-    const Window& conv_window, HloModule* module, HloInstruction* original_hlo,\n+    CreateShardedFunctor& create_sharded_dot, const Window& conv_window,\n+    HloModule* module, HloInstruction* original_hlo,\n     const SpmdPartitionerOptions& options, SpmdBuilder* b,\n     std::vector<SpmdPartitioningVisitor::WindowedDotGeneralLoop>*\n         windowed_dot_general_loops,\n@@ -4202,7 +4271,7 @@ absl::StatusOr<HloInstruction*> PartitionDot(\n   // resharding the groups.\n   for (bool require_matching_devices_to_group : {true, false}) {\n     TF_ASSIGN_OR_RETURN(\n-        auto try_partition,\n+        HloInstruction * try_partition,\n         PartitionDot(lhs, rhs, output_base_shape, output_sharding, dims_mapping,\n                      num_partitions, create_sharded_dot, conv_window, module,\n                      original_hlo, require_matching_devices_to_group, options,\n@@ -4220,8 +4289,8 @@ absl::StatusOr<HloInstruction*> PartitionDot(\n \n   // Default action.\n   TF_ASSIGN_OR_RETURN(\n-      auto dot, create_sharded_dot(lhs.Replicate().hlo(), rhs.Replicate().hlo(),\n-                                   b, conv_window));\n+      HloInstruction * dot,\n+      create_sharded_dot(lhs.Replicate(), rhs.Replicate(), b, conv_window));\n   dot->set_sharding(HloSharding::Replicate());\n   return PartitionedHlo(dot, output_base_shape, lhs.state())\n       .Reshard(output_sharding)\n@@ -4230,37 +4299,79 @@ absl::StatusOr<HloInstruction*> PartitionDot(\n \n }  // namespace\n \n+template <typename CreateShardedFunctor>\n absl::Status SpmdPartitioningVisitor::HandleDotHelper(\n     HloInstruction* hlo, const DotConvolutionDimsInfo& dims_mapping,\n-    absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-        HloInstruction*, HloInstruction*, SpmdBuilder*,\n-        const Window& conv_window)>\n-        create_sharded_dot) {\n+    CreateShardedFunctor& create_sharded_dot) {\n   if (hlo->sharding().HasUniqueDevice()) {\n     return DefaultAction(hlo);\n   }\n-  PartitionedHlo& lhs = GetPartitionedHlo(hlo->operand(0));\n-  PartitionedHlo& raw_rhs = GetPartitionedHlo(hlo->operand(1));\n-  // If lhs and rhs are the same instruction, make a copy for rhs.\n-  const PartitionedHlo& rhs =\n-      (lhs.hlo() == raw_rhs.hlo())\n-          ? MakeACopyAndReturnItsPartitionedHlo(raw_rhs, builder())\n-          : raw_rhs;\n-\n+  HloInstruction* partitioned_dot;\n   Window conv_window;\n-  if (hlo->opcode() == HloOpcode::kConvolution) {\n-    conv_window = hlo->window();\n-  }\n+  if constexpr (std::is_same_v<CreateShardedFunctor,\n+                               CreateShardedScaledDotFunctor>) {\n+    HloCustomCallInstruction* block_scaled_dot =\n+        Cast<HloCustomCallInstruction>(hlo);\n+    PartitionedHlo& lhs_operand =\n+        GetPartitionedHlo(block_scaled_dot->operand(0));\n+    PartitionedHlo& lhs_scale = GetPartitionedHlo(block_scaled_dot->operand(2));\n+    PartitionedHlo& raw_rhs_operand =\n+        GetPartitionedHlo(block_scaled_dot->operand(1));\n+    PartitionedHlo& raw_rhs_scale =\n+        GetPartitionedHlo(block_scaled_dot->operand(3));\n+    // If lhs and rhs are the same instruction, make a copy for rhs.\n+    const PartitionedHlo& rhs_operand =\n+        (lhs_operand.hlo() == raw_rhs_operand.hlo())\n+            ? MakeACopyAndReturnItsPartitionedHlo(raw_rhs_operand, builder())\n+            : raw_rhs_operand;\n+    const PartitionedHlo& rhs_scale =\n+        (lhs_operand.hlo() == raw_rhs_operand.hlo())\n+            ? MakeACopyAndReturnItsPartitionedHlo(raw_rhs_scale, builder())\n+            : raw_rhs_scale;\n+\n+    PartitionedHloMX lhs(lhs_operand, lhs_scale);\n+    PartitionedHloMX rhs(rhs_operand, rhs_scale);\n \n-  TF_ASSIGN_OR_RETURN(\n-      auto partitioned_dot,\n-      PartitionDot(lhs, rhs, hlo->shape(), hlo->sharding(), dims_mapping,\n-                   num_partitions_, create_sharded_dot, conv_window, module_,\n-                   hlo, options_, &b_, &windowed_dot_general_loops_, this));\n-  SetPartitionedHlo(hlo, [&] { return partitioned_dot; });\n+    TF_ASSIGN_OR_RETURN(\n+        partitioned_dot,\n+        PartitionDot(lhs, rhs, hlo->shape(), hlo->sharding(), dims_mapping,\n+                     num_partitions_, create_sharded_dot, conv_window, module_,\n+                     hlo, options_, &b_, &windowed_dot_general_loops_, this));\n+  } else {\n+    PartitionedHlo lhs = GetPartitionedHlo(hlo->operand(0));\n+    PartitionedHlo raw_rhs = GetPartitionedHlo(hlo->operand(1));\n+    // If lhs and rhs are the same instruction, make a copy for rhs.\n+    const PartitionedHlo rhs =\n+        (lhs.hlo() == raw_rhs.hlo())\n+            ? MakeACopyAndReturnItsPartitionedHlo(raw_rhs, builder())\n+            : raw_rhs;\n+\n+    if (hlo->opcode() == HloOpcode::kConvolution) {\n+      conv_window = hlo->window();\n+    }\n+\n+    TF_ASSIGN_OR_RETURN(\n+        partitioned_dot,\n+        PartitionDot(lhs, rhs, hlo->shape(), hlo->sharding(), dims_mapping,\n+                     num_partitions_, create_sharded_dot, conv_window, module_,\n+                     hlo, options_, &b_, &windowed_dot_general_loops_, this));\n+  }\n+  SetPartitionedHlo(hlo, [partitioned_dot] { return partitioned_dot; });\n   return absl::OkStatus();\n }\n \n+template absl::Status\n+SpmdPartitioningVisitor::HandleDotHelper<CreateShardedDotFunctor>(\n+    HloInstruction*, const DotConvolutionDimsInfo&, CreateShardedDotFunctor&);\n+template absl::Status\n+SpmdPartitioningVisitor::HandleDotHelper<CreateShardedScaledDotFunctor>(\n+    HloInstruction*, const DotConvolutionDimsInfo&,\n+    CreateShardedScaledDotFunctor&);\n+template absl::Status\n+SpmdPartitioningVisitor::HandleDotHelper<CreateShardedConvolutionFunctor>(\n+    HloInstruction*, const DotConvolutionDimsInfo&,\n+    CreateShardedConvolutionFunctor&);\n+\n namespace {\n \n // Finds a cluster of nodes that produce the inputs for `hlo` which only"
        },
        {
            "sha": "0cc25656a83f30db0bcaa796da90e5d3d5c060da",
            "filename": "third_party/xla/xla/service/spmd/dot_handler.h",
            "status": "added",
            "additions": 73,
            "deletions": 0,
            "changes": 73,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.h?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -0,0 +1,73 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_SERVICE_SPMD_DOT_HANDLER_H_\n+#define XLA_SERVICE_SPMD_DOT_HANDLER_H_\n+\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/spmd/spmd_partitioner.h\"\n+#include \"xla/service/spmd/spmd_partitioner_util.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla {\n+namespace spmd {\n+\n+class CreateShardedConvolutionFunctor;\n+class CreateShardedDotFunctor;\n+class CreateShardedScaledDotFunctor;\n+\n+// Abstract base class for functors creating sharded dots, block-scaled dots and\n+// convolutions.\n+template <typename PartitionedHloMaybeMX>\n+class CreateShardedFunctorBase {\n+ public:\n+  virtual ~CreateShardedFunctorBase() = default;\n+\n+  // Implemented in derived classes to create sharded dots, block-scaled dots\n+  // and convolutions.\n+  virtual absl::StatusOr<HloInstruction*> CreateSharded(\n+      const PartitionedHloMaybeMX& ll, const PartitionedHloMaybeMX& rr,\n+      SpmdBuilder* b, const Window& conv_window) const = 0;\n+\n+  void SetCustomCreateSharded(\n+      std::function<absl::StatusOr<HloInstruction*>(\n+          const PartitionedHloMaybeMX&, const PartitionedHloMaybeMX&,\n+          SpmdBuilder*, const Window&)>&& custom_create_sharded) {\n+    custom_create_sharded_ = std::move(custom_create_sharded);\n+  }\n+\n+  absl::StatusOr<HloInstruction*> operator()(\n+      const PartitionedHloMaybeMX& ll, const PartitionedHloMaybeMX& rr,\n+      SpmdBuilder* builder, const Window& conv_window,\n+      bool call_custom_create_sharded = true) const {\n+    if (call_custom_create_sharded && custom_create_sharded_) {\n+      return custom_create_sharded_(ll, rr, builder, conv_window);\n+    }\n+    return CreateSharded(ll, rr, builder, conv_window);\n+  }\n+\n+ private:\n+  // May hold a function which can be optionally called instead of\n+  // CreateSharded.\n+  std::function<absl::StatusOr<HloInstruction*>(const PartitionedHloMaybeMX&,\n+                                                const PartitionedHloMaybeMX&,\n+                                                SpmdBuilder*, const Window&)>\n+      custom_create_sharded_;\n+};\n+\n+}  // namespace spmd\n+}  // namespace xla\n+\n+#endif  // XLA_SERVICE_SPMD_DOT_HANDLER_H_"
        },
        {
            "sha": "49c8b0e744e91e097717161e9509438fdd45259d",
            "filename": "third_party/xla/xla/service/spmd/dot_handler_test.cc",
            "status": "modified",
            "additions": 214,
            "deletions": 0,
            "changes": 214,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler_test.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -26,6 +26,8 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/hlo/utils/hlo_matchers.h\"\n+#include \"xla/literal_util.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/hlo_verifier.h\"\n #include \"xla/service/sharding_propagation.h\"\n@@ -38,6 +40,8 @@ namespace xla {\n namespace spmd {\n namespace {\n \n+namespace op = xla::testing::opcode_matchers;\n+\n class DotHandlerTest : public HloHardwareIndependentTestBase {\n  public:\n   absl::StatusOr<std::unique_ptr<HloModule>> PartitionComputation(\n@@ -338,6 +342,216 @@ ENTRY main {\n   }\n }\n \n+TEST_F(DotHandlerTest, MXCustomCall_BatchAndBatch) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[8,128,512]{2,1,0} parameter(0), sharding={devices=[8,1,1]<=[8]}\n+  lhs_scale = f8e8m0fnu[8,128,16] parameter(1), sharding={devices=[8,1,1]<=[8]}\n+  rhs = f8e4m3fn[8,1024,512]{2,1,0} parameter(2), sharding={devices=[8,1,1]<=[8]}\n+  rhs_scale = f8e8m0fnu[8,1024,16] parameter(3), sharding={devices=[8,1,1]<=[8]}\n+  ROOT block_scaled_dot = f32[8,128,1024]{2,1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[1,8,1]<=[8]}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              op::Reshape(op::Transpose(op::AllToAll(\n+                  op::Reshape(op::CustomCall({\"__op$block_scaled_dot\"}))))));\n+}\n+\n+TEST_F(DotHandlerTest, MXCustomCall_BatchAndNonContracting) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[8,128,512]{2,1,0} parameter(0), sharding={devices=[8,1,1]<=[8]}\n+  lhs_scale = f8e8m0fnu[8,128,16]{2,1,0} parameter(1), sharding={devices=[8,1,1]<=[8]}\n+  rhs = f8e4m3fn[8,1024,512]{2,1,0} parameter(2), sharding={devices=[1,8,1]<=[8]}\n+  rhs_scale = f8e8m0fnu[8,32,512]{2,1,0} parameter(3), sharding={devices=[1,8,1]<=[8]}\n+  ROOT block_scaled_dot = f32[8,128,1024]{2,1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[8,1,1]<=[8]}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              op::CustomCall({\"__op$block_scaled_dot\"}, op::Parameter(0),\n+                             op::Reshape(op::Transpose(op::AllToAll())),\n+                             op::Parameter(1),\n+                             op::Reshape(op::Transpose(op::AllToAll()))));\n+}\n+\n+TEST_F(DotHandlerTest, MXCustomCall_ContractingAndContracting) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[128,512]{1,0} parameter(0), sharding={devices=[1,8]<=[8]}\n+  lhs_scale = f8e8m0fnu[128,16]{1,0} parameter(1), sharding={devices=[1,8]<=[8]}\n+  rhs = f8e4m3fn[1024,512]{1,0} parameter(2), sharding={devices=[1,8]<=[8]}\n+  rhs_scale = f8e8m0fnu[1024,16]{1,0} parameter(3), sharding={devices=[1,8]<=[8]}\n+  ROOT block_scaled_dot = f32[128,1024]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[8,1]<=[8]}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(\n+      module->entry_computation()->root_instruction(),\n+      op::DynamicSlice(\n+          op::AllReduce(op::CustomCall({\"__op$block_scaled_dot\"})),\n+          op::Reshape(op::DynamicSlice(op::Constant(LiteralUtil::CreateR1<int>(\n+                                           {0, 16, 32, 48, 64, 80, 96, 112})),\n+                                       op::PartitionId())),\n+          op::Constant(LiteralUtil::CreateR0<int>(0))));\n+}\n+\n+TEST_F(DotHandlerTest, MXCustomCall_NonContractingAndContracting) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[128,512]{1,0} parameter(0), sharding={devices=[8,1]<=[8]}\n+  lhs_scale = f8e8m0fnu[128,16]{1,0} parameter(1), sharding={devices=[8,1]<=[8]}\n+  rhs = f8e4m3fn[1024,512]{1,0} parameter(2), sharding={devices=[1,8]<=[8]}\n+  rhs_scale = f8e8m0fnu[1024,16]{1,0} parameter(3), sharding={devices=[1,8]<=[8]}\n+  ROOT block_scaled_dot = f32[128,1024]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[8,1]<=[8]}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(\n+      module->entry_computation()->root_instruction(),\n+      op::CustomCall({\"__op$block_scaled_dot\"}, op::Parameter(0),\n+                     op::AllGather(), op::Parameter(1), op::AllGather()));\n+}\n+\n+TEST_F(DotHandlerTest, MXCustomCall_ContractingAndReplicated) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[1024,512]{1,0} parameter(0), sharding={devices=[1,8]<=[8]}\n+  lhs_scale = f8e4m3fn[1024,16]{1,0} parameter(1), sharding={devices=[1,8]<=[8]}\n+  rhs = f8e4m3fn[128,512]{1,0} parameter(2), sharding={replicated}\n+  rhs_scale = f8e8m0fnu[128,16]{1,0} parameter(3), sharding={replicated}\n+  ROOT block_scaled_dot = f32[1024,128]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={replicated}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              op::AllReduce(op::CustomCall({\"__op$block_scaled_dot\"})));\n+}\n+\n+TEST_F(DotHandlerTest, MXCustomCall_BatchNonContractingAndBatchNonContracting) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[8,1024,512]{2,1,0} parameter(0), sharding={devices=[4,2,1]7,6,5,4,3,2,1,0}\n+  lhs_scale = f8e8m0fnu[8,1024,16]{2,1,0} parameter(1), sharding={devices=[4,2,1]7,6,5,4,3,2,1,0}\n+  rhs = f8e4m3fn[8,128,512]{2,1,0} parameter(2), sharding={devices=[4,2,1]0,1,2,3,4,5,6,7}\n+  rhs_scale = f8e8m0fnu[8,128,16]{2,1,0} parameter(3), sharding={devices=[4,2,1]0,1,2,3,4,5,6,7}\n+  ROOT block_scaled_dot = f32[8,1024,128]{2,1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[4,2,1]0,1,2,3,4,5,6,7}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              op::CollectivePermute(op::CustomCall({\"__op$block_scaled_dot\"})));\n+}\n+\n+TEST_F(DotHandlerTest,\n+       MXCustomCall_ContractingNonContractingAndContractingNonContracting0) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[1024,512]{1,0} parameter(0), sharding={devices=[4,2]0,1,2,3,4,5,6,7}\n+  lhs_scale = f8e8m0fnu[1024,16]{1,0} parameter(1), sharding={devices=[4,2]0,1,2,3,4,5,6,7}\n+  rhs = f8e4m3fn[128,512]{1,0} parameter(2), sharding={devices=[2,4]0,1,2,3,4,5,6,7}\n+  rhs_scale = f8e8m0fnu[128,16] parameter(3), sharding={devices=[2,4]0,1,2,3,4,5,6,7}\n+  ROOT block_scaled_dot = f32[1024,128]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[4,2]0,1,2,3,4,5,6,7}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(\n+      module->entry_computation()->root_instruction(),\n+      op::CustomCall({\"__op$block_scaled_dot\"}, op::AllGather(),\n+                     op::AllGather(), op::AllGather(), op::AllGather()));\n+}\n+\n+TEST_F(DotHandlerTest,\n+       MXCustomCall_ContractingNonContractingAndContractingNonContracting1) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[1024,512]{1,0} parameter(0), sharding={devices=[4,2]0,1,2,3,4,5,6,7}\n+  lhs_scale = f8e8m0fnu[1024,16]{1,0} parameter(1), sharding={devices=[4,2]0,1,2,3,4,5,6,7}\n+  rhs = f8e4m3fn[128,512]{1,0} parameter(2), sharding={devices=[4,2]0,1,2,3,4,5,6,7}\n+  rhs_scale = f8e8m0fnu[128,16]{1,0} parameter(3), sharding={devices=[4,2]0,1,2,3,4,5,6,7}\n+  ROOT block_scaled_dot = f32[1024,128]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={replicated}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              op::AllReduce(op::CustomCall({\"__op$block_scaled_dot\"})));\n+}\n+\n+TEST_F(DotHandlerTest, MXCustomCall_ReplicatedAndReplicated0) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[1024,512]{1,0} parameter(0), sharding={replicated}\n+  lhs_scale = f8e8m0fnu[1024,16]{1,0} parameter(1), sharding={replicated}\n+  rhs = f8e4m3fn[128,512]{1,0} parameter(2), sharding={replicated}\n+  rhs_scale = f8e8m0fnu[128,16]{1,0} parameter(3), sharding={replicated}\n+  ROOT block_scaled_dot = f32[1024,128]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[2,1,4]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(\n+      module->entry_computation()->root_instruction(),\n+      op::CustomCall({\"__op$block_scaled_dot\"}, op::DynamicSlice(),\n+                     op::Parameter(2), op::DynamicSlice(), op::Parameter(3)));\n+}\n+\n+TEST_F(DotHandlerTest, MXCustomCall_ReplicatedAndReplicated1) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[1024,512]{1,0} parameter(0), sharding={replicated}\n+  lhs_scale = f8e8m0fnu[1024,16]{1,0} parameter(1), sharding={replicated}\n+  rhs = f8e4m3fn[128,512]{1,0} parameter(2), sharding={replicated}\n+  rhs_scale = f8e8m0fnu[128,16]{1,0} parameter(3), sharding={replicated}\n+  ROOT block_scaled_dot = f32[1024,128]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[8,1]0,1,2,3,4,5,6,7}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  VLOG(1) << module->ToString();\n+  EXPECT_THAT(\n+      module->entry_computation()->root_instruction(),\n+      op::CustomCall({\"__op$block_scaled_dot\"}, op::DynamicSlice(),\n+                     op::Parameter(2), op::DynamicSlice(), op::Parameter(3)));\n+}\n+\n }  // namespace\n }  // namespace spmd\n }  // namespace xla"
        },
        {
            "sha": "4bec21743c0155c006db3059a821d224a287baff",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.h",
            "status": "modified",
            "additions": 91,
            "deletions": 8,
            "changes": 99,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -489,8 +489,6 @@ class PartitionedHlo {\n   };\n   PartitionedHlo(HloInstruction* hlo, Shape base_shape, PartitioningState state)\n       : hlo_(hlo), base_shape_(base_shape), state_(std::move(state)) {\n-    CHECK(hlo->has_sharding())\n-        << \"PartitionedHlo is missing sharding:\" << hlo->ToString();\n   }\n \n   PartitionedHlo(PartitionedHlo&& other) = default;\n@@ -545,8 +543,14 @@ class PartitionedHlo {\n   // Returns the sharding of the SPMD instruction.\n   const HloSharding& sharding() const { return hlo_->sharding(); }\n \n-  // Returns the SPMD instruction's number of dimensions.\n-  int64_t num_dimensions() const { return base_shape_.dimensions().size(); }\n+  void set_sharding(const HloSharding& sharding) {\n+    hlo_->set_sharding(sharding);\n+  }\n+\n+  // Returns the rank of the SPMD instruction.\n+  const int64_t num_dimensions() const {\n+    return base_shape_.dimensions().size();\n+  }\n \n   int64_t NewChannel() const { return (*state_.next_channel_id)++; }\n \n@@ -624,6 +628,87 @@ class PartitionedHlo {\n   PartitioningState state_;\n };\n \n+// Combines two identically sharded PartitionedHlo instructions describing the\n+// operand and scale tensors in OCP microscaling (MX) formats used in\n+// block-scaled dots. See\n+// https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf.\n+class PartitionedHloMX {\n+ public:\n+  class ShapesMX {\n+   public:\n+    ShapesMX(const Shape& operand_shape, const Shape& scale_shape)\n+        : shapes_(operand_shape, scale_shape) {};\n+\n+    absl::Span<const int64_t> dimensions() const {\n+      return shapes_.first.dimensions();\n+    }\n+\n+    operator Shape() const { return shapes_.first; }\n+\n+    operator std::pair<Shape, Shape>() const { return shapes_; }\n+\n+   private:\n+    std::pair<Shape, Shape> shapes_;\n+  };\n+\n+  PartitionedHloMX(const PartitionedHlo& operand, const PartitionedHlo& scale)\n+      : operand_(operand.hlo(), operand.base_shape(), operand.state()),\n+        scale_(scale.hlo(), scale.base_shape(), scale.state()) {\n+    CHECK_EQ(operand.sharding(), scale.sharding())\n+        << \"Operand and scale must be identically sharded.\";\n+  };\n+\n+  PartitionedHlo operand() const { return operand_; }\n+\n+  PartitionedHlo scale() const { return scale_; }\n+\n+  const HloSharding& sharding() const { return operand_.sharding(); }\n+\n+  void set_sharding(const HloSharding& sharding) {\n+    operand_.set_sharding(sharding);\n+    scale_.set_sharding(sharding);\n+  }\n+\n+  const ShapesMX base_shape() const {\n+    return ShapesMX(operand_.base_shape(), scale_.base_shape());\n+  }\n+\n+  const PartitionedHlo::PartitioningState& state() const {\n+    return operand_.state();\n+  }\n+\n+  // Returns the operand instruction.\n+  HloInstruction* hlo() const { return operand_.hlo(); }\n+\n+  PartitionedHloMX Replicate() const {\n+    return PartitionedHloMX(operand_.Replicate(), scale_.Replicate());\n+  }\n+\n+  PartitionedHloMX Reshard(const HloSharding& target) const {\n+    return PartitionedHloMX(operand_.Reshard(target), scale_.Reshard(target));\n+  }\n+\n+  PartitionedHloMX PadWithZero(\n+      absl::Span<const int64_t> left_padded_dims = {},\n+      absl::Span<const int64_t> skipped_dims = {}) const {\n+    return PartitionedHloMX(\n+        operand_.PadWithZero(left_padded_dims, skipped_dims),\n+        scale_.PadWithZero(left_padded_dims, skipped_dims));\n+  }\n+\n+  PartitionedHloMX PadWithZeroOnSpecifiedDims(\n+      absl::Span<const int64_t> dims,\n+      absl::Span<const int64_t> left_padded_dims = {}) const {\n+    return PartitionedHloMX(\n+        operand_.PadWithZeroOnSpecifiedDims(dims, left_padded_dims),\n+        scale_.PadWithZeroOnSpecifiedDims(dims, left_padded_dims));\n+  }\n+\n+ private:\n+  PartitionedHlo operand_;\n+  PartitionedHlo scale_;\n+};\n+\n class SpmdPartitioningVisitor : public DfsHloVisitorWithDefault {\n  public:\n   SpmdPartitioningVisitor(\n@@ -677,13 +762,11 @@ class SpmdPartitioningVisitor : public DfsHloVisitorWithDefault {\n   absl::Status HandleWhile(HloInstruction* hlo) override;\n \n   // Implementation of dot partitioning given DotGeneralDimsMapping.\n+  template <typename CreateShardedFunctor>\n   absl::Status HandleDotHelper(\n       HloInstruction* hlo,\n       const dot_as_convolution_util::DotConvolutionDimsInfo& dims_mapping,\n-      absl::FunctionRef<absl::StatusOr<HloInstruction*>(\n-          HloInstruction*, HloInstruction*, SpmdBuilder*,\n-          const Window& conv_window)>\n-          create_sharded_dot);\n+      CreateShardedFunctor& create_sharded_dot);\n \n   // Common handle for elementwise HLOs.\n   absl::Status HandleElementwise(HloInstruction* hlo);"
        },
        {
            "sha": "704a69ff96a095d9bb10ba7e6d647a99036d4aa9",
            "filename": "third_party/xla/xla/stream_executor/cuda/cudnn_api_wrappers.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcudnn_api_wrappers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcudnn_api_wrappers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcudnn_api_wrappers.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -14,6 +14,7 @@ limitations under the License.\n ==============================================================================*/\n \n #include \"xla/stream_executor/cuda/cudnn_api_wrappers.h\"\n+\n #include <sys/resource.h>\n \n #include <string>\n@@ -23,7 +24,12 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n #include \"third_party/gpus/cuda/include/library_types.h\"\n+#include \"third_party/gpus/cudnn/cudnn_version.h\"\n+#if CUDNN_VERSION >= 90000\n #include \"third_party/gpus/cudnn/cudnn_graph.h\"\n+#else\n+#include \"third_party/gpus/cudnn/cudnn_ops_infer.h\"\n+#endif\n #include \"xla/stream_executor/semantic_version.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\""
        },
        {
            "sha": "146b784e38ae6e6ead302de54fa5a0fd08865ad2",
            "filename": "third_party/xla/xla/stream_executor/cuda/cudnn_api_wrappers.h",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcudnn_api_wrappers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcudnn_api_wrappers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcudnn_api_wrappers.h?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -19,7 +19,12 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"third_party/gpus/cudnn/cudnn_version.h\"\n+#if CUDNN_VERSION >= 90000\n #include \"third_party/gpus/cudnn/cudnn_graph.h\"\n+#else\n+#include \"third_party/gpus/cudnn/cudnn_ops_infer.h\"\n+#endif\n #include \"xla/stream_executor/semantic_version.h\"\n \n namespace stream_executor {"
        },
        {
            "sha": "4717bda5385d05fcfeb39c13e35aaa2cd090531a",
            "filename": "third_party/xla/xla/stream_executor/gpu/ragged_all_to_all_kernel_lib.cu.h",
            "status": "modified",
            "additions": 27,
            "deletions": 15,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fragged_all_to_all_kernel_lib.cu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fragged_all_to_all_kernel_lib.cu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fragged_all_to_all_kernel_lib.cu.h?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -53,8 +53,11 @@ struct alignas(kSize) Vec {\n //  update_slice = input[input_offset: input_offset + send_size]\n //  output_ptrs[j][output_offset : output_offset + send_size] = update_slice\n //\n+// `num_updates_per_block` blocks cooperate to process `num_updates_per_block`\n+// updates. This is done to reduce imbalance in data transfer per block.\n+//\n // Launch parameters:\n-//  - Block grid: (N*num_updates_per_rank, num_blocks_per_update, 1)\n+//  - Block grid: (num_ranks, num_block_clusters, num_updates_per_block)\n //  - Thread grid: (num_threads_per_update, 1, 1)\n template <int64_t kVectorSize>\n __global__ void __launch_bounds__(128) RaggedAllToAllKernelImpl(\n@@ -66,25 +69,34 @@ __global__ void __launch_bounds__(128) RaggedAllToAllKernelImpl(\n     int64_t num_updates_per_replica, int64_t num_row_elements) {\n   using T = Vec<kVectorSize>;\n \n-  int64_t update_idx = blockIdx.x;\n-  int64_t output_idx = update_idx / num_updates_per_replica;\n-\n   const T* typed_input_ptr = static_cast<const T* __restrict__>(input_ptr);\n-  T* output_ptr = static_cast<T* __restrict__>(output_ptrs[output_idx]);\n+  T* output_ptr = static_cast<T* __restrict__>(output_ptrs[blockIdx.x]);\n+\n+  int64_t num_updates_to_process = gridDim.z;\n+\n+  for (int64_t i = 0; i < num_updates_to_process; ++i) {\n+    const int64_t update_idx =\n+        blockIdx.x * num_updates_per_replica + blockIdx.y * gridDim.z + i;\n+\n+    const int64_t input_offset = input_offsets_ptr[update_idx];\n+    const int64_t send_size = send_sizes_ptr[update_idx];\n+    const int64_t output_offset = output_offsets_ptr[update_idx];\n \n-  int64_t input_offset = input_offsets_ptr[update_idx];\n-  int64_t send_size = send_sizes_ptr[update_idx];\n-  int64_t output_offset = output_offsets_ptr[update_idx];\n+    const int64_t input_offset_start = input_offset * num_row_elements;\n+    const int64_t output_offset_start = output_offset * num_row_elements;\n \n-  int64_t input_offset_start = input_offset * num_row_elements;\n-  int64_t output_offset_start = output_offset * num_row_elements;\n+    const int64_t update_size = send_size * num_row_elements;\n \n-  int64_t update_size = send_size * num_row_elements;\n+    int64_t offset_update_batch_idx = blockIdx.z + i;\n+    if (offset_update_batch_idx >= num_updates_to_process) {\n+      offset_update_batch_idx -= num_updates_to_process;\n+    }\n \n-  for (int64_t i = threadIdx.x + blockIdx.y * blockDim.x; i < update_size;\n-       i += blockDim.x * gridDim.y) {\n-    output_ptr[output_offset_start + i] =\n-        typed_input_ptr[input_offset_start + i];\n+    for (int64_t j = threadIdx.x + offset_update_batch_idx * blockDim.x;\n+         j < update_size; j += num_updates_to_process * blockDim.x) {\n+      output_ptr[output_offset_start + j] =\n+          typed_input_ptr[input_offset_start + j];\n+    }\n   }\n }\n }  // namespace stream_executor::gpu"
        },
        {
            "sha": "99cbdb4268dfe425600b7d5edf92d3029a6b274f",
            "filename": "third_party/xla/xla/stream_executor/gpu/tma_metadata.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Ftma_metadata.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Ftma_metadata.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Ftma_metadata.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -462,9 +462,11 @@ absl::StatusOr<TmaMetadata> TmaMetadata::FromProto(\n \n bool IsTmaAvailableForDevice(\n     const stream_executor::DeviceDescription& device_info) {\n-  bool is_cuda = std::holds_alternative<stream_executor::CudaComputeCapability>(\n-      device_info.gpu_compute_capability());\n-  return is_cuda && device_info.cuda_compute_capability().IsAtLeastHopper();\n+  if (auto* cuda_cc = std::get_if<stream_executor::CudaComputeCapability>(\n+          &device_info.gpu_compute_capability())) {\n+    return cuda_cc->IsAtLeastHopper();\n+  }\n+  return false;\n }\n \n // Limitations of TMA:"
        },
        {
            "sha": "c883db7ca98a2a40867ce4c092f3014cf97a9cad",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test.cc",
            "status": "modified",
            "additions": 104,
            "deletions": 0,
            "changes": 104,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -1672,6 +1672,110 @@ ENTRY entry {\n   CollectiveOpsCompareShardedUnsharded(hlo_text, /*num_partitions=*/4);\n }\n \n+TEST_F(CollectiveOpsTestE2EShardedUnsharded, BlockScaledDotBatchAndBatch) {\n+  const std::string hlo_text = R\"(\n+HloModule module, entry_computation_layout={(f8e4m3fn[4,16,64]{2,1,0}, f8e8m0fnu[4,16,2]{2,1,0}, f8e4m3fn[4,4,64]{2,1,0}, f8e8m0fnu[4,4,2]{2,1,0})->f32[4,16,4]{2,1,0}}, num_partitions=2\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[4,16,64]{2,1,0} parameter(0), sharding={devices=[2,1,1]<=[2]}\n+  lhs_scale = f8e8m0fnu[4,16,2]{2,1,0} parameter(1), sharding={devices=[2,1,1]<=[2]}\n+  rhs = f8e4m3fn[4,4,64]{2,1,0} parameter(2), sharding={devices=[2,1,1]<=[2]}\n+  rhs_scale = f8e8m0fnu[4,4,2]{2,1,0} parameter(3), sharding={devices=[2,1,1]<=[2]}\n+  ROOT block_scaled_dot = f32[4,16,4]{2,1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[1,2,1]<=[2]}\n+})\";\n+  CollectiveOpsCompareShardedUnsharded(hlo_text);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EShardedUnsharded,\n+       BlockScaledDotBatchAndNonContracting) {\n+  const std::string hlo_text = R\"(\n+HloModule module, entry_computation_layout={(f8e4m3fn[4,16,64]{2,1,0}, f8e8m0fnu[4,16,2]{2,1,0}, f8e4m3fn[4,4,64]{2,1,0}, f8e8m0fnu[4,4,2]{2,1,0})->f32[4,16,4]{2,1,0}}, num_partitions=2\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[4,16,64]{2,1,0} parameter(0), sharding={devices=[2,1,1]<=[2]}\n+  lhs_scale = f8e8m0fnu[4,16,2]{2,1,0} parameter(1), sharding={devices=[2,1,1]<=[2]}\n+  rhs = f8e4m3fn[4,4,64]{2,1,0} parameter(2), sharding={devices=[1,2,1]<=[2]}\n+  rhs_scale = f8e8m0fnu[4,4,2]{2,1,0} parameter(3), sharding={devices=[1,2,1]<=[2]}\n+  ROOT block_scaled_dot = f32[4,16,4]{2,1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[2,1,1]<=[2]}\n+})\";\n+  CollectiveOpsCompareShardedUnsharded(hlo_text);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EShardedUnsharded,\n+       BlockScaledDotContractingAndContracting) {\n+  const std::string hlo_text = R\"(\n+HloModule module, entry_computation_layout={(f8e4m3fn[16,64]{1,0}, f8e8m0fnu[16,2]{1,0}, f8e4m3fn[4,64]{1,0}, f8e8m0fnu[4,2]{1,0})->f32[16,4]{1,0}}, num_partitions=2\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[16,64]{1,0} parameter(0), sharding={devices=[1,2]<=[2]}\n+  lhs_scale = f8e8m0fnu[16,2]{1,0} parameter(1), sharding={devices=[1,2]<=[2]}\n+  rhs = f8e4m3fn[4,64]{1,0} parameter(2), sharding={devices=[1,2]<=[2]}\n+  rhs_scale = f8e8m0fnu[4,2]{1,0} parameter(3), sharding={devices=[1,2]<=[2]}\n+  ROOT block_scaled_dot = f32[16,4]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[2,1]<=[2]}\n+})\";\n+  CollectiveOpsCompareShardedUnsharded(hlo_text);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EShardedUnsharded,\n+       BlockScaledDotNonContractingAndContracting) {\n+  const std::string hlo_text = R\"(\n+HloModule module, entry_computation_layout={(f8e4m3fn[16,128]{1,0}, f8e8m0fnu[16,4]{1,0}, f8e4m3fn[4,128]{1,0}, f8e8m0fnu[4,4]{1,0})->f32[16,4]{1,0}}, num_partitions=2\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[16,128]{1,0} parameter(0), sharding={devices=[2,1]<=[2]}\n+  lhs_scale = f8e8m0fnu[16,4]{1,0} parameter(1), sharding={devices=[2,1]<=[2]}\n+  rhs = f8e4m3fn[4,128]{1,0} parameter(2), sharding={devices=[1,2]<=[2]}\n+  rhs_scale = f8e8m0fnu[4,4]{1,0} parameter(3), sharding={devices=[1,2]<=[2]}\n+  ROOT block_scaled_dot = f32[16,4]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[2,1]<=[2]}\n+})\";\n+  CollectiveOpsCompareShardedUnsharded(hlo_text);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EShardedUnsharded,\n+       BlockScaledDotContractingAndReplicated) {\n+  const std::string hlo_text = R\"(\n+HloModule module, entry_computation_layout={(f8e4m3fn[16,128]{1,0}, f8e8m0fnu[16,4]{1,0}, f8e4m3fn[4,128]{1,0}, f8e8m0fnu[4,4]{1,0})->f32[16,4]{1,0}}, num_partitions=2\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[16,128]{1,0} parameter(0), sharding={devices=[1,2]<=[2]}\n+  lhs_scale = f8e8m0fnu[16,4]{1,0} parameter(1), sharding={devices=[1,2]<=[2]}\n+  rhs = f8e4m3fn[4,128]{1,0} parameter(2), sharding={replicated}\n+  rhs_scale = f8e8m0fnu[4,4]{1,0} parameter(3), sharding={replicated}\n+  ROOT block_scaled_dot = f32[16,4]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[2,1]<=[2]}\n+})\";\n+  CollectiveOpsCompareShardedUnsharded(hlo_text);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EShardedUnsharded,\n+       BlockScaledDotReplicatedAndReplicated) {\n+  const std::string hlo_text = R\"(\n+HloModule module, entry_computation_layout={(f8e4m3fn[4,128]{1,0}, f8e8m0fnu[4,4], f8e4m3fn[1,128]{1,0}, f8e8m0fnu[1,4]{1,0})->f32[4,1]{1,0}}, num_partitions=2\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[4,128]{1,0} parameter(0), sharding={replicated}\n+  lhs_scale = f8e8m0fnu[4,4]{1,0} parameter(1), sharding={replicated}\n+  rhs = f8e4m3fn[1,128]{1,0} parameter(2), sharding={replicated}\n+  rhs_scale = f8e8m0fnu[1,4]{1,0} parameter(3), sharding={replicated}\n+  ROOT block_scaled_dot = f32[4,1]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[2,1]<=[2]}\n+})\";\n+  CollectiveOpsCompareShardedUnsharded(hlo_text);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EShardedUnsharded,\n+       BlockScaledDotContractingNonContractingAndContractingNonContracting) {\n+  const std::string hlo_text = R\"(\n+HloModule module, entry_computation_layout={(f8e4m3fn[8,128]{1,0}, f8e8m0fnu[8,4]{1,0}, f8e4m3fn[4,128]{1,0}, f8e8m0fnu[4,4]{1,0})->f32[8,4]{1,0}}, num_partitions=4\n+\n+ENTRY entry {\n+  lhs = f8e4m3fn[8,128]{1,0} parameter(0), sharding={devices=[2,2]<=[4]}\n+  lhs_scale = f8e8m0fnu[8,4]{1,0} parameter(1), sharding={devices=[2,2]<=[4]}\n+  rhs = f8e4m3fn[4,128]{1,0} parameter(2), sharding={devices=[2,2]<=[4]}\n+  rhs_scale = f8e8m0fnu[4,4]{1,0} parameter(3), sharding={devices=[2,2]<=[4]}\n+  ROOT dot = f32[8,4]{1,0} custom-call(lhs, rhs, lhs_scale, rhs_scale), custom_call_target=\"__op$block_scaled_dot\", sharding={devices=[2,2]<=[4]}\n+})\";\n+  CollectiveOpsCompareShardedUnsharded(hlo_text, /*num_partitions=*/4);\n+}\n+\n // E2E tests comparing the results of windowed einsum and non-windowed cases.\n class CollectiveOpsTestE2EWindowedNonWindowed : public CollectiveOpsTestE2E {\n  public:"
        },
        {
            "sha": "690fc663b070865ea2a04c83f64569a7d4a3edee",
            "filename": "third_party/xla/xla/tools/multihost_hlo_runner/BUILD",
            "status": "modified",
            "additions": 57,
            "deletions": 3,
            "changes": 60,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2FBUILD?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -4,6 +4,7 @@ load(\"@local_config_rocm//rocm:build_defs.bzl\", \"if_rocm\")\n load(\"//xla:xla.default.bzl\", \"xla_cc_binary\")\n load(\"//xla/tests:build_defs.bzl\", \"xla_test\")\n load(\"//xla/tsl:tsl.bzl\", \"if_cuda_or_rocm\", \"if_google\")\n+load(\"//xla/tsl:tsl.default.bzl\", \"tsl_pybind_extension\")\n load(\"//xla/tsl/platform:build_config_root.bzl\", \"tf_gpu_tests_tags\")\n load(\"//xla/tsl/platform:rules_cc.bzl\", \"cc_library\")\n \n@@ -37,9 +38,7 @@ cc_library(\n     testonly = True,\n     srcs = [\"hlo_runner_main.cc\"],\n     compatible_with = None,\n-    tags = [\n-        \"no_mac\",\n-    ],\n+    tags = [\"no_mac\"],\n     deps = [\n         \":create_client\",\n         \":functional_hlo_runner\",\n@@ -280,3 +279,58 @@ xla_test(\n         \"@local_tsl//tsl/platform:protobuf\",\n     ],\n )\n+\n+tsl_pybind_extension(\n+    name = \"py_hlo_multihost_runner\",\n+    srcs = [\"python_hlo_runner.cc\"],\n+    deps = [\n+        \":create_client\",\n+        \":functional_hlo_runner\",\n+        \":hlo_input_output_format\",\n+        \":profiler_interface\",\n+        \"//xla:debug_options_flags\",\n+        \"//xla:status_macros\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/ffi\",\n+        \"//xla/ffi:ffi_api\",\n+        \"//xla/ffi/api:c_api\",\n+        \"//xla/pjrt:pjrt_client\",\n+        \"//xla/pjrt:status_casters\",\n+        \"//xla/pjrt/distributed\",\n+        \"//xla/pjrt/distributed:client\",\n+        \"//xla/pjrt/distributed:key_value_store_interface\",\n+        \"//xla/pjrt/distributed:service\",\n+        \"//xla/pjrt/plugin/xla_gpu:xla_gpu_allocator_config\",\n+        \"//xla/pjrt/plugin/xla_gpu:xla_gpu_client_options\",\n+        \"//xla/python:logging\",\n+        \"//xla/service:cpu_plugin\",\n+        \"//xla/service:custom_call_target_registry\",\n+        \"//xla/service:hlo_module_util\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/util:command_line_flags\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/time\",\n+        \"@local_tsl//tsl/platform:errors\",\n+        \"@local_tsl//tsl/platform:logging\",\n+        \"@local_tsl//tsl/platform:platform_port\",\n+        \"@local_tsl//tsl/platform:status\",\n+        \"@local_tsl//tsl/platform:statusor\",\n+        \"@nanobind\",\n+    ] + if_cuda_or_rocm([\n+        \"//xla/service:gpu_plugin\",\n+        \"//xla/backends/profiler/gpu:cupti_tracer\",\n+        \"//xla/backends/profiler/gpu:device_tracer\",\n+    ]) + if_cuda([\n+        \"//xla/stream_executor:cuda_platform\",\n+    ] + if_google(\n+        [\n+            \"//third_party/py/jax/jaxlib/cuda:cuda_gpu_kernels\",  # fixdeps: keep\n+        ],\n+    )) + if_rocm([\n+        \"//xla/stream_executor:rocm_platform\",\n+    ]),\n+)"
        },
        {
            "sha": "e71110b21decb61dfb160e116bccdcb77f62690d",
            "filename": "third_party/xla/xla/tools/multihost_hlo_runner/python_hlo_runner.cc",
            "status": "added",
            "additions": 450,
            "deletions": 0,
            "changes": 450,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Fpython_hlo_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Fpython_hlo_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Fpython_hlo_runner.cc?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -0,0 +1,450 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <optional>\n+#include <string>\n+\n+#include \"nanobind/stl/shared_ptr.h\"  // IWYU pragma: keep\n+#include \"nanobind/stl/string.h\"  // IWYU pragma: keep\n+#include \"nanobind/stl/string_view.h\"  // IWYU pragma: keep\n+#include \"nanobind/stl/vector.h\"  // IWYU pragma: keep\n+#include \"xla/ffi/api/c_api.h\"\n+#include \"xla/ffi/ffi.h\"\n+#include \"xla/ffi/ffi_api.h\"\n+#include \"xla/pjrt/plugin/xla_gpu/xla_gpu_allocator_config.h\"\n+#include \"xla/pjrt/plugin/xla_gpu/xla_gpu_client_options.h\"\n+#include \"xla/pjrt/status_casters.h\"\n+#include \"xla/python/logging.h\"\n+#include \"xla/service/custom_call_target_registry.h\"\n+#include \"xla/tools/multihost_hlo_runner/create_client.h\"\n+#include \"xla/tools/multihost_hlo_runner/functional_hlo_runner.h\"\n+#include \"xla/tools/multihost_hlo_runner/hlo_input_output_format.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace nb = ::nanobind;\n+\n+namespace xla {\n+\n+enum DeviceType {\n+  kHost = 0,\n+  kGpu = 1,\n+};\n+\n+struct PyHloRunnerConfig {\n+  InputFormat input_format = InputFormat::kProtoText;\n+  FunctionalHloRunner::ModuleOutputMode output_mode =\n+      FunctionalHloRunner::ModuleOutputMode::kReturnOutputs;\n+  bool should_run = true;\n+  bool enable_mock_nccl = false;\n+  std::string dump_output_literal_to = \"\";\n+  int task_id = 0;\n+  int num_nodes = 1;\n+  DeviceType device_type = DeviceType::kGpu;\n+  std::string address = \"\";\n+  int32_t num_replicas = -1;\n+  int32_t num_partitions = 1;\n+  bool log_output = false;\n+  FunctionalHloRunner::HloPassesMode hlo_pass_mode =\n+      FunctionalHloRunner::HloPassesMode::kStandardCompile;\n+  FunctionalHloRunner::SpmdMode spmd_mode =\n+      FunctionalHloRunner::SpmdMode::kNotUseSpmdPartitioning;\n+  bool is_spmd_partitioned_module = false;\n+  std::string xla_dump_to = \"\";\n+  bool xla_dump_as_text = false;\n+  bool xla_dump_as_proto = false;\n+  FunctionalHloRunner::ModuleArgumentMode hlo_argument_mode =\n+      FunctionalHloRunner::ModuleArgumentMode::kUseRandomInputs;\n+  int32_t while_execution_count = -1;\n+  bool remove_infeed_outfeed = true;\n+  bool compile_as_stablehlo = false;\n+  bool use_layouts_from_hlo_module = false;\n+  bool force_auto_layout = false;\n+  int32_t num_repeats = 1;\n+  std::string execution_options_path = \"\";\n+  int64_t gpu_client_initialization_timeout_sec = 300;\n+  float gpu_client_mem_fraction = GpuAllocatorConfig{}.memory_fraction;\n+  bool profile_execution = false;\n+  std::string xla_gpu_dump_xspace_to = \"\";\n+};\n+\n+absl::StatusOr<FunctionalHloRunner::PreprocessingOptions>\n+PreprocessingOptionsFromFlags(const PyHloRunnerConfig& opts) {\n+  FunctionalHloRunner::PreprocessingOptions out;\n+  out.spmd_partitioned_mode =\n+      opts.is_spmd_partitioned_module\n+          ? FunctionalHloRunner::SpmdPartitionedMode::kIsSpmdPartitionedModule\n+          : FunctionalHloRunner::SpmdPartitionedMode::\n+                kIsNotSpmdPartitionedModule;\n+  out.while_execution_count =\n+      opts.while_execution_count > 0\n+          ? std::make_optional(opts.while_execution_count)\n+          : std::nullopt;\n+  out.remove_infeed_outfeed = opts.remove_infeed_outfeed;\n+  return out;\n+}\n+\n+absl::StatusOr<FunctionalHloRunner::RunningOptions> RunningOptionsFromFlags(\n+    const PyHloRunnerConfig& opts) {\n+  FunctionalHloRunner::RunningOptions out;\n+  out.module_argument_mode = opts.hlo_argument_mode;\n+  out.module_output_mode = opts.output_mode;\n+  out.num_repeats = static_cast<size_t>(opts.num_repeats);\n+  out.log_input_output_mode =\n+      opts.log_output ? FunctionalHloRunner::LogOutputMode::kLogOutput\n+                      : FunctionalHloRunner::LogOutputMode::kNotLogOutput;\n+  return out;\n+}\n+\n+absl::StatusOr<FunctionalHloRunner::RawCompileOptions>\n+RawCompileOptionsFromFlags(const PyHloRunnerConfig& opts) {\n+  FunctionalHloRunner::RawCompileOptions out;\n+  out.hlo_passes_mode = opts.hlo_pass_mode;\n+  out.spmd_mode = opts.spmd_mode;\n+  if (!opts.execution_options_path.empty()) {\n+    TF_ASSIGN_OR_RETURN(\n+        out.execution_options,\n+        FunctionalHloRunner::LoadExecutionOptions(opts.execution_options_path));\n+  }\n+  out.num_replicas = opts.num_replicas < 0\n+                         ? std::nullopt\n+                         : std::optional<int>(opts.num_replicas);\n+  out.num_partitions = opts.num_partitions < 0\n+                           ? std::nullopt\n+                           : std::optional<int>(opts.num_partitions);\n+  out.xla_dump_to = opts.xla_dump_to;\n+  out.xla_text_dump_mode =\n+      opts.xla_dump_as_text\n+          ? FunctionalHloRunner::XlaTextDumpMode::kDumpAsText\n+          : FunctionalHloRunner::XlaTextDumpMode::kNotDumpAsText;\n+  out.xla_proto_dump_mode =\n+      opts.xla_dump_as_proto\n+          ? FunctionalHloRunner::XlaProtoDumpMode::kDumpAsProto\n+          : FunctionalHloRunner::XlaProtoDumpMode::kNotDumpAsProto;\n+  out.xla_gpu_dump_xspace_to = opts.xla_gpu_dump_xspace_to;\n+  return out;\n+}\n+\n+absl::Status RunHloFiles(const std::vector<std::string>& hlo_files,\n+                         const PyHloRunnerConfig& opts) {\n+  TF_ASSIGN_OR_RETURN(FunctionalHloRunner::PreprocessingOptions preproc_options,\n+                      PreprocessingOptionsFromFlags(opts));\n+  preproc_options.annotate_while_loop_trip_count = true;\n+  TF_ASSIGN_OR_RETURN(\n+      FunctionalHloRunner::RawCompileOptions raw_compile_options,\n+      RawCompileOptionsFromFlags(opts));\n+  TF_ASSIGN_OR_RETURN(FunctionalHloRunner::RunningOptions running_options,\n+                      RunningOptionsFromFlags(opts));\n+\n+  // tsl::Flags::Parse() leaves unknown flags in argv, we assume that those are\n+  // HLO files to run. Note that argv[0] is the binary name and is excluded.\n+  if (hlo_files.size() == 0) {\n+    return absl::InvalidArgumentError(\"No HLO files provided.\");\n+  }\n+\n+  if (!opts.dump_output_literal_to.empty() && hlo_files.size() > 1) {\n+    return absl::InvalidArgumentError(\n+        \"Can only dump output literal when single input file is specified.\");\n+  }\n+\n+  if (opts.gpu_client_mem_fraction < 0.0 ||\n+      opts.gpu_client_mem_fraction > 1.0) {\n+    return absl::InvalidArgumentError(\n+        \"Invalid GPU client memory fraction. Must be in range [0.0, 1.0]\");\n+  }\n+\n+  PjRtEnvironment env;\n+  std::unique_ptr<HLORunnerProfiler> hlo_runner_profiler;\n+  if (opts.device_type == DeviceType::kGpu) {\n+    GpuClientOptions gpu_options;\n+    gpu_options.node_id = opts.task_id;\n+    gpu_options.num_nodes = opts.num_nodes;\n+    gpu_options.enable_mock_nccl = opts.enable_mock_nccl;\n+    gpu_options.allocator_config.memory_fraction = opts.gpu_client_mem_fraction;\n+    TF_ASSIGN_OR_RETURN(\n+        env, GetPjRtEnvironmentForGpu(\n+                 opts.address, gpu_options,\n+                 absl::Seconds(opts.gpu_client_initialization_timeout_sec)));\n+  } else {\n+    QCHECK(opts.device_type == DeviceType::kHost) << \"Invalid device type\";\n+    TF_ASSIGN_OR_RETURN(env, GetPjRtEnvironmentForHostCpu());\n+  }\n+\n+  CHECK(env.client != nullptr);\n+  if (!opts.xla_gpu_dump_xspace_to.empty()) {\n+    TF_ASSIGN_OR_RETURN(hlo_runner_profiler,\n+                        HLORunnerProfiler::Create(opts.xla_gpu_dump_xspace_to,\n+                                                  /*keep_xspace=*/false));\n+    running_options.profiler = hlo_runner_profiler.get();\n+  }\n+\n+  for (const auto& hlo_file : hlo_files) {\n+    std::vector<ExecutionProfile> execution_profiles;\n+    if (opts.profile_execution) {\n+      running_options.execution_profiles = &execution_profiles;\n+    }\n+    if (opts.should_run) {\n+      TF_RETURN_IF_ERROR(FunctionalHloRunner::LoadAndRunAndDump(\n+          *env.client, GetDebugOptionsFromFlags(), preproc_options,\n+          raw_compile_options, running_options, hlo_file, opts.input_format,\n+          opts.dump_output_literal_to, opts.task_id));\n+    } else {\n+      TF_RETURN_IF_ERROR(FunctionalHloRunner::LoadAndCompile(\n+          *env.client, GetDebugOptionsFromFlags(), preproc_options,\n+          raw_compile_options, hlo_file, opts.input_format, opts.task_id));\n+    }\n+    for (int i = 0; i < execution_profiles.size(); ++i) {\n+      LOG(INFO) << \"## Execution time, file=\" << hlo_file << \" repeat=\" << i\n+                << \" duration=\" << execution_profiles[i].compute_time_ns()\n+                << \"ns\";\n+    }\n+  }\n+  return absl::OkStatus();\n+}\n+\n+absl::Status RegisterCustomCallTarget(const std::string& fn_name, nb::object fn,\n+                                      const std::string& platform,\n+                                      int api_version,\n+                                      XLA_FFI_Handler_Traits traits) {\n+  // Register legacy custom call target (untyped void* API).\n+  if (api_version == 0) {\n+    if (traits != 0) {\n+      return absl::InvalidArgumentError(\n+          \"Custom call target registration with traits is not supported for \"\n+          \"api_version=0\");\n+    }\n+\n+    nb::capsule capsule;\n+    if (!nb::try_cast<nb::capsule>(fn, capsule)) {\n+      return absl::InvalidArgumentError(\n+          \"Custom call target registration with api_version=0 requires a \"\n+          \"PyCapsule fn object\");\n+    }\n+\n+    CustomCallTargetRegistry::Global()->Register(fn_name, capsule.data(),\n+                                                 platform);\n+    return absl::OkStatus();\n+  }\n+\n+  if (api_version == 1) {\n+    // Register a single execute handler\n+    nb::capsule capsule;\n+    if (nb::try_cast<nb::capsule>(fn, capsule)) {\n+      return ffi::TakeStatus(ffi::Ffi::RegisterStaticHandler(\n+          ffi::GetXlaFfiApi(), fn_name, platform,\n+          reinterpret_cast<XLA_FFI_Handler*>(capsule.data())));\n+    }\n+\n+    // Register a bundle of handlers\n+    nb::dict bundle;\n+    if (nb::try_cast<nb::dict>(fn, bundle)) {\n+      auto handler = [&](const char* name) -> absl::StatusOr<XLA_FFI_Handler*> {\n+        if (!bundle.contains(name)) {\n+          return nullptr;\n+        }\n+\n+        nb::capsule capsule;\n+        if (nb::try_cast<nb::capsule>(bundle[name], capsule)) {\n+          return reinterpret_cast<XLA_FFI_Handler*>(capsule.data());\n+        }\n+        return absl::InvalidArgumentError(\n+            \"Custom call target registration with api_version=1 requires a \"\n+            \"PyCapsule fn object for all dict keys\");\n+      };\n+\n+      XLA_FFI_Handler_Bundle bundle;\n+      TF_ASSIGN_OR_RETURN(bundle.instantiate, handler(\"instantiate\"));\n+      TF_ASSIGN_OR_RETURN(bundle.prepare, handler(\"prepare\"));\n+      TF_ASSIGN_OR_RETURN(bundle.initialize, handler(\"initialize\"));\n+      TF_ASSIGN_OR_RETURN(bundle.execute, handler(\"execute\"));\n+\n+      return ffi::TakeStatus(ffi::Ffi::RegisterStaticHandler(\n+          ffi::GetXlaFfiApi(), fn_name, platform, bundle, traits));\n+    }\n+\n+    return absl::InvalidArgumentError(\n+        \"Unsupported custom call target type for api_version=1\");\n+  }\n+\n+  return absl::UnimplementedError(absl::StrFormat(\n+      \"API version %d is not supported by RegisterCustomCallTarget. Supported \"\n+      \"versions are 0 and 1.\",\n+      api_version));\n+}\n+\n+nb::dict GetRegisteredCustomCallTargets(const std::string& platform) {\n+  nb::dict targets;\n+\n+  // version 0 handlers\n+  for (const auto& [name, target] :\n+       CustomCallTargetRegistry::Global()->registered_symbols(platform)) {\n+    targets[nb::str(name.data(), name.size())] = nb::capsule(target);\n+  }\n+\n+  // version 1 handlers\n+  auto ffi_handlers = ffi::StaticRegisteredHandlers(platform);\n+  if (!ffi_handlers.ok()) {\n+    return targets;\n+  }\n+\n+  for (const auto& [name, registration] : *ffi_handlers) {\n+    nb::dict bundle;\n+    auto export_handler = [&](absl::string_view name, XLA_FFI_Handler* h) {\n+      if (h != nullptr) {\n+        bundle[nb::str(name.data(), name.size())] =\n+            nb::capsule(reinterpret_cast<void*>(h));\n+      }\n+    };\n+    export_handler(\"instantiate\", registration.bundle.instantiate);\n+    export_handler(\"prepare\", registration.bundle.prepare);\n+    export_handler(\"initialize\", registration.bundle.initialize);\n+    export_handler(\"execute\", registration.bundle.execute);\n+    targets[nb::str(name.data(), name.size())] = std::move(bundle);\n+  }\n+  return targets;\n+}\n+\n+absl::Status RegisterCustomTypeId(absl::string_view type_name,\n+                                  nb::object type_id) {\n+  nb::capsule capsule;\n+  if (!nb::try_cast<nb::capsule>(type_id, capsule)) {\n+    return absl::InvalidArgumentError(\n+        \"The type_id argument to register_custom_call_type_id must be a \"\n+        \"PyCapsule object holding a pointer to a XLA_FFI_TypeId.\");\n+  }\n+  XLA_FFI_TypeId* type_id_ptr =\n+      reinterpret_cast<XLA_FFI_TypeId*>(static_cast<void*>(capsule.data()));\n+  return ffi::TakeStatus(ffi::Ffi::RegisterTypeId(xla::ffi::GetXlaFfiApi(),\n+                                                  type_name, type_id_ptr));\n+}\n+\n+NB_MODULE(py_hlo_multihost_runner, m) {\n+  InitializeAbslLogging();\n+\n+  m.def(\"RunHloFiles\", ThrowIfErrorWrapper(RunHloFiles));\n+  m.def(\n+      \"register_custom_call_target\",\n+      [](const std::string& fn_name, nb::object fn, const std::string& platform,\n+         int api_version, XLA_FFI_Handler_Traits traits) {\n+        ThrowIfError(RegisterCustomCallTarget(fn_name, std::move(fn), platform,\n+                                              api_version, traits));\n+      },\n+      nb::arg(\"fn_name\"), nb::arg(\"fn\"), nb::arg(\"platform\"),\n+      nb::arg(\"api_version\") = 0, nb::arg(\"traits\") = 0);\n+  m.def(\"custom_call_targets\", GetRegisteredCustomCallTargets,\n+        nb::arg(\"platform\"));\n+  m.def(\n+      \"register_custom_type_id\",\n+      [](absl::string_view type_name, nb::object type_id) {\n+        xla::ThrowIfError(RegisterCustomTypeId(type_name, type_id));\n+      },\n+      nb::arg(\"type_name\"), nb::arg(\"type_id\"));\n+\n+  nb::class_<PyHloRunnerConfig>(m, \"PyHloRunnerConfig\")\n+      .def(nb::init<>())\n+      .def_rw(\"input_format\", &PyHloRunnerConfig::input_format)\n+      .def_rw(\"output_mode\", &PyHloRunnerConfig::output_mode)\n+      .def_rw(\"should_run\", &PyHloRunnerConfig::should_run)\n+      .def_rw(\"enable_mock_nccl\", &PyHloRunnerConfig::enable_mock_nccl)\n+      .def_rw(\"dump_output_literal_to\",\n+              &PyHloRunnerConfig::dump_output_literal_to)\n+      .def_rw(\"task_id\", &PyHloRunnerConfig::task_id)\n+      .def_rw(\"num_nodes\", &PyHloRunnerConfig::num_nodes)\n+      .def_rw(\"device_type\", &PyHloRunnerConfig::device_type)\n+      .def_rw(\"address\", &PyHloRunnerConfig::address)\n+      .def_rw(\"num_replicas\", &PyHloRunnerConfig::num_replicas)\n+      .def_rw(\"num_partitions\", &PyHloRunnerConfig::num_partitions)\n+      .def_rw(\"log_output\", &PyHloRunnerConfig::log_output)\n+      .def_rw(\"hlo_pass_mode\", &PyHloRunnerConfig::hlo_pass_mode)\n+      .def_rw(\"spmd_mode\", &PyHloRunnerConfig::spmd_mode)\n+      .def_rw(\"is_spmd_partitioned_module\",\n+              &PyHloRunnerConfig::is_spmd_partitioned_module)\n+      .def_rw(\"xla_dump_to\", &PyHloRunnerConfig::xla_dump_to)\n+      .def_rw(\"xla_dump_as_text\", &PyHloRunnerConfig::xla_dump_as_text)\n+      .def_rw(\"xla_dump_as_proto\", &PyHloRunnerConfig::xla_dump_as_proto)\n+      .def_rw(\"hlo_argument_mode\", &PyHloRunnerConfig::hlo_argument_mode)\n+      .def_rw(\"while_execution_count\",\n+              &PyHloRunnerConfig::while_execution_count)\n+      .def_rw(\"remove_infeed_outfeed\",\n+              &PyHloRunnerConfig::remove_infeed_outfeed)\n+      .def_rw(\"compile_as_stablehlo\", &PyHloRunnerConfig::compile_as_stablehlo)\n+      .def_rw(\"use_layouts_from_hlo_module\",\n+              &PyHloRunnerConfig::use_layouts_from_hlo_module)\n+      .def_rw(\"force_auto_layout\", &PyHloRunnerConfig::force_auto_layout)\n+      .def_rw(\"num_repeats\", &PyHloRunnerConfig::num_repeats)\n+      .def_rw(\"gpu_client_initialization_timeout_sec\",\n+              &PyHloRunnerConfig::gpu_client_initialization_timeout_sec)\n+      .def_rw(\"gpu_client_mem_fraction\",\n+              &PyHloRunnerConfig::gpu_client_mem_fraction)\n+      .def_rw(\"profile_execution\", &PyHloRunnerConfig::profile_execution)\n+      .def_rw(\"xla_gpu_dump_xspace_to\",\n+              &PyHloRunnerConfig::xla_gpu_dump_xspace_to);\n+\n+  nb::enum_<InputFormat>(m, \"InputFormat\")\n+      .value(\"Text\", InputFormat::kText)\n+      .value(\"ProtoText\", InputFormat::kProtoText)\n+      .value(\"ProtoBinary\", InputFormat::kProtoBinary)\n+      .value(\"SnapshotProtoBinary\", InputFormat::kSnapshotProtoBinary)\n+      .value(\"UnoptimizedSnapshotProtoBinary\",\n+             InputFormat::kUnoptimizedSnapshotProtoBinary)\n+      .value(\"UnoptimizedSnapshotProtoText\",\n+             InputFormat::kUnoptimizedSnapshotProtoText)\n+      .value(\"SerializedPjrtExecutable\",\n+             InputFormat::kSerializedPjRtExecutable);\n+\n+  nb::enum_<FunctionalHloRunner::ModuleOutputMode>(m, \"ModuleOutputMode\")\n+      .value(\"ReturnOutputs\",\n+             FunctionalHloRunner::ModuleOutputMode::kReturnOutputs)\n+      .value(\"NotReturnOutputs\",\n+             FunctionalHloRunner::ModuleOutputMode::kNotReturnOutputs)\n+      .value(\"ReturnDevice0Outputs\",\n+             FunctionalHloRunner::ModuleOutputMode::kReturnDevice0Outputs);\n+\n+  nb::enum_<FunctionalHloRunner::ModuleArgumentMode>(m, \"ModuleArgumentMode\")\n+      .value(\"UseDeviceIdAsInput\",\n+             FunctionalHloRunner::ModuleArgumentMode::kUseDeviceIdAsInput)\n+      .value(\"UseRandomInputs\",\n+             FunctionalHloRunner::ModuleArgumentMode::kUseRandomInputs)\n+      .value(\"UseSharedRandomInputs\",\n+             FunctionalHloRunner::ModuleArgumentMode::kUseSharedRandomInputs)\n+      .value(\"UseZerosAsInput\",\n+             FunctionalHloRunner::ModuleArgumentMode::kUseZerosAsInput)\n+      .value(\"Uninitialized\",\n+             FunctionalHloRunner::ModuleArgumentMode::kUninitialized);\n+\n+  nb::enum_<FunctionalHloRunner::HloPassesMode>(m, \"HloPassesMode\")\n+      .value(\"RunXLABackendOnly\",\n+             FunctionalHloRunner::HloPassesMode::kRunXLABackendOnly)\n+      .value(\"DisableAllHloPasses\",\n+             FunctionalHloRunner::HloPassesMode::kDisableAllHloPasses)\n+      .value(\"StandardCompile\",\n+             FunctionalHloRunner::HloPassesMode::kStandardCompile);\n+\n+  nb::enum_<FunctionalHloRunner::SpmdMode>(m, \"SpmdMode\")\n+      .value(\"UseSpmdPartitioning\",\n+             FunctionalHloRunner::SpmdMode::kUseSpmdPartitioning)\n+      .value(\"UseShardyPartitioning\",\n+             FunctionalHloRunner::SpmdMode::kUseShardyPartitioning)\n+      .value(\"NotUseSpmdPartitioning\",\n+             FunctionalHloRunner::SpmdMode::kNotUseSpmdPartitioning);\n+\n+  nb::enum_<DeviceType>(m, \"DeviceType\")\n+      .value(\"Host\", DeviceType::kHost)\n+      .value(\"Gpu\", DeviceType::kGpu);\n+}\n+\n+}  // namespace xla"
        },
        {
            "sha": "eb27b5210cce3e93268284b5f4ff94bae208ae56",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9a3d2ad5cc47a654ff5ae4b29942cf04775333ab/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=9a3d2ad5cc47a654ff5ae4b29942cf04775333ab",
            "patch": "@@ -1330,10 +1330,14 @@ message DebugOptions {\n   // layouts.\n   optional bool xla_early_exit_with_layouts = 397;\n \n+  // If true, the triton fusion emitter will ignore IsTritonSupportedInstruction\n+  // We need this to enable triton scaled dot emitter for testing.\n+  optional bool xla_gpu_experimental_scaled_dot_with_triton = 410;\n+\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 410\n+  // Next id: 411\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 4807,
        "additions": 3299,
        "deletions": 1508
    }
}