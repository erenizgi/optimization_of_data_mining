{
    "author": "WillFroom",
    "message": "[XLA][XTile] Use xtile entry, extract & insert in triton emitter.\n\nPiperOrigin-RevId: 822020719",
    "sha": "0f4c3f55b5a87b9457bafb24a1da6382dbefffbb",
    "files": [
        {
            "sha": "14626df0d23d763dc2679665ff1730a904fdc74a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=0f4c3f55b5a87b9457bafb24a1da6382dbefffbb",
            "patch": "@@ -137,6 +137,7 @@ cc_library(\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:LLVMDialect\",\n         \"@llvm-project//mlir:MathDialect\",\n+        \"@llvm-project//mlir:NVVMDialect\",\n         \"@llvm-project//mlir:Support\",\n         \"@triton//:TritonDialects\",\n     ],"
        },
        {
            "sha": "f94f4d3321363166114c5fc81d2883be420d3e89",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=0f4c3f55b5a87b9457bafb24a1da6382dbefffbb",
            "patch": "@@ -33,7 +33,6 @@ limitations under the License.\n #include \"llvm/Support/MathExtras.h\"\n #include \"llvm/TargetParser/Triple.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMAttrs.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Dialect/Math/IR/Math.h\"\n #include \"mlir/IR/Builders.h\"\n@@ -617,20 +616,8 @@ absl::StatusOr<stream_executor::gpu::TmaMetadata> ExtractTmaMetadata(\n   return tma_metadata;\n }\n \n-::mlir::triton::PointerType GetPointerType(mlir::MemRefType memref_type) {\n-  int address_space = 0;\n-\n-  mlir::Attribute memory_space_attr = memref_type.getMemorySpace();\n-  if (auto int_memory_space_attr =\n-          mlir::dyn_cast_if_present<mlir::IntegerAttr>(memory_space_attr)) {\n-    address_space = int_memory_space_attr.getInt();\n-  } else if (auto llvm_memory_space_attr = mlir::dyn_cast_if_present<\n-                 mlir::LLVM::LLVMAddrSpaceAttrInterface>(memory_space_attr)) {\n-    address_space = llvm_memory_space_attr.getAddressSpace();\n-  }\n-\n-  return ::mlir::triton::PointerType::get(memref_type.getElementType(),\n-                                          address_space);\n+mt::PointerType GetGlobalPointerType(mlir::Type element_type) {\n+  return mlir::cast<mt::PointerType>(mt::getPointerTypeToElement(element_type));\n }\n \n }  // namespace xla::gpu::triton"
        },
        {
            "sha": "8f7d83e809775979afb4a053fa7821a1ff8ce38a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h?ref=0f4c3f55b5a87b9457bafb24a1da6382dbefffbb",
            "patch": "@@ -231,9 +231,9 @@ absl::StatusOr<stream_executor::gpu::TmaMetadata> ExtractTmaMetadata(\n absl::StatusOr<stream_executor::ThreadDim> ExtractThreadDims(\n     mlir::ModuleOp triton_module, mlir::LLVM::LLVMFuncOp func_op);\n \n-// Returns the triton pointer type that corresponds to the given memref type,\n-// i.e. has the same element type and address space.\n-::mlir::triton::PointerType GetPointerType(mlir::MemRefType memref_type);\n+// Returns the triton pointer type with global memory space and the given\n+// element type.\n+::mlir::triton::PointerType GetGlobalPointerType(mlir::Type element_type);\n \n }  // namespace xla::gpu::triton\n "
        },
        {
            "sha": "f53919af9213cf3b2e28c8ce93930df444b44782",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 52,
            "deletions": 61,
            "changes": 113,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=0f4c3f55b5a87b9457bafb24a1da6382dbefffbb",
            "patch": "@@ -43,6 +43,7 @@ limitations under the License.\n #include \"llvm/IR/Metadata.h\"\n #include \"llvm/IR/Module.h\"\n #include \"llvm/Linker/Linker.h\"\n+#include \"llvm/Support/Debug.h\"\n #include \"llvm/Support/FileSystem.h\"\n #include \"llvm/Support/LogicalResult.h\"\n #include \"llvm/Support/raw_ostream.h\"\n@@ -108,6 +109,8 @@ limitations under the License.\n #include \"xla/codegen/tiling/tiled_hlo_fusion_instruction.h\"\n #include \"xla/codegen/tiling/tiled_hlo_instruction.h\"\n #include \"xla/codegen/tiling/tiled_hlo_schedule.h\"\n+#include \"xla/codegen/xtile/ir/xtile_dialect.h\"\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n #include \"xla/hlo/builder/xla_builder.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n@@ -148,7 +151,6 @@ limitations under the License.\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/path.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n-#include \"triton/Dialect/Triton/IR/Types.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n namespace xla {\n@@ -341,25 +343,21 @@ ScalarOrTensor Iota(EmitterLocOpBuilder b, int32_t limit) {\n }\n \n ScalarOrTensor EmitParameterExtract(EmitterLocOpBuilder b,\n-                                    const TileInfo& tile_info,\n-                                    Value parent_base_ptr) {\n-  // For a pointer to a scalar or a zero-dimensional tensor, load the base\n-  // pointer directly. This shortcut is necessary because Triton does not\n-  // support 0-D tensors.\n-  // TODO(csigg): This should be handled in the extract/insert rewrite.\n-  if (tile_info.padded_tile_sizes().empty()) {\n-    return ScalarOrTensor(ttir::LoadOp::create(\n-        b, parent_base_ptr, ttir::CacheModifier::NONE,\n-        ttir::EvictionPolicy::NORMAL, /*isVolatile=*/false));\n-  }\n-\n-  return ScalarOrTensor(mtx::ExtractOp::create(\n-      b,\n-      mlir::RankedTensorType::get(tile_info.padded_tile_sizes(),\n-                                  tile_info.storage_type()),\n-      parent_base_ptr, tile_info.offsets(), tile_info.padded_tile_sizes(),\n-      tile_info.tile_strides(), tile_info.original_shape(),\n-      tile_info.minor_to_major_layout()));\n+                                    const TileInfo& tile_info, Value arg) {\n+  auto tensor_type = mlir::RankedTensorType::get(tile_info.padded_tile_sizes(),\n+                                                 tile_info.storage_type());\n+\n+  mlir::Value extracted_tensor = b.create<xla::xtile::ExtractTileOp>(\n+      tensor_type, arg, tile_info.offsets(), tile_info.padded_tile_sizes(),\n+      tile_info.tile_strides());\n+\n+  if (tensor_type.getRank() == 0) {\n+    // Triton does not support 0-D tensors so we must extract the scalar value.\n+    // TODO(csigg): This should be handled in the extract/insert rewrite.\n+    return ScalarOrTensor(b.create<mlir::tensor::ExtractOp>(extracted_tensor));\n+  }\n+\n+  return ScalarOrTensor(extracted_tensor);\n }\n \n absl::StatusOr<ScalarOrTensor> EmitScope(\n@@ -1736,7 +1734,7 @@ absl::Status EmitGeneric(mlir::OpBuilder builder,\n                          absl::string_view libdevice_path,\n                          const se::DeviceDescription& device_info,\n                          const HloFusionInstruction* fusion,\n-                         mlir::FunctionOpInterface fn,\n+                         xtile::EntryFuncOp fn,\n                          const BlockLevelParameters& block_level_parameters,\n                          SymbolicExprContext* symbolic_expr_context) {\n   if (VLOG_IS_ON(6)) {\n@@ -1823,19 +1821,14 @@ absl::Status EmitGeneric(mlir::OpBuilder builder,\n   VLOG(3) << \"EmitGeneric: tiled HLO computation:\\n\"\n           << tiled_hlo_computation.ToString();\n \n-  // TODO(b/389955087): we can decide whether to sign extend by understanding if\n-  // we need 64 bits to encode indices or if 32 bits are enough. For now, just\n-  // use 64 bits to avoid issues.\n-  Value pid_i64 = Cast(b, b.create<ttir::GetProgramIdOp>(ttir::ProgramIDDim::X),\n-                       b.getI64Type());\n-  Value pid = Cast(b, pid_i64, b.getIndexType());\n+  Value tile_id = fn.getTileId();\n   absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor> values;\n   TF_ASSIGN_OR_RETURN(\n       auto results,\n       EmitTiledComputation(b, libdevice_path, device_info, fusion,\n-                           tiled_hlo_computation, fn, pid, values));\n+                           tiled_hlo_computation, fn, tile_id, values));\n \n-  for (auto [root, result, parent_base_ptr] :\n+  for (auto [root, result, arg] :\n        llvm::zip(tiled_hlo_computation.GetRoots(), results,\n                  fn.getArguments().drop_front(computation->num_parameters()))) {\n     // Some types are stored using different types, e.g. i1 is stored in memory\n@@ -1849,28 +1842,25 @@ absl::Status EmitGeneric(mlir::OpBuilder builder,\n           ScalarOrTensor(Cast(b, result.UnwrapUnsafe(), result_storage_type));\n     }\n \n+    mlir::Value input_tensor;\n     if (result.IsScalar()) {\n       // TODO(csigg): Handle this in extract/insert rewrite.\n-      ttir::StoreOp::create(b, parent_base_ptr, result.UnwrapScalar(),\n-                            /*mask=*/nullptr);\n-      continue;\n+      mlir::Value scalar_value = result.UnwrapScalar();\n+      auto tensor_type =\n+          mlir::RankedTensorType::get({}, scalar_value.getType());\n+      input_tensor =\n+          b.create<mlir::tensor::FromElementsOp>(tensor_type, scalar_value);\n+    } else {\n+      input_tensor = result.UnwrapTensor();\n     }\n \n-    CHECK(root->hlo()->shape().IsArray() &&\n-          !root->hlo()->shape().dimensions().empty());\n     TF_ASSIGN_OR_RETURN(\n         auto tile_info,\n-        TileInfo::Construct(b, pid, /*runtime_values=*/{}, *root));\n-\n-    // Should not be scalar at this point.\n-    CHECK(!tile_info.padded_tile_sizes().empty())\n-        << \"Unexpected scalar encountered. Expected padded_tile_sizes() to be \"\n-           \"non-empty.\";\n+        TileInfo::Construct(b, tile_id, /*runtime_values=*/{}, *root));\n \n-    mtx::InsertOp::create(b, result.UnwrapTensor(), parent_base_ptr,\n-                          tile_info.offsets(), tile_info.padded_tile_sizes(),\n-                          tile_info.tile_strides(), tile_info.original_shape(),\n-                          tile_info.minor_to_major_layout());\n+    b.create<xtile::InsertTileOp>(input_tensor, arg, tile_info.offsets(),\n+                                  tile_info.padded_tile_sizes(),\n+                                  tile_info.tile_strides());\n   }\n \n   return absl::OkStatus();\n@@ -1884,7 +1874,8 @@ void LoadMlirDialectsForTriton(mlir::MLIRContext& mlir_context) {\n       mlir::arith::ArithDialect, mlir::affine::AffineDialect,\n       mlir::LLVM::LLVMDialect, xla::XlaDialect, xla::gpu::XlaGpuDialect,\n       ttir::xla::XlaTritonDialect, mlir::func::FuncDialect,\n-      mlir::tensor::TensorDialect, stablehlo::StablehloDialect>();\n+      mlir::tensor::TensorDialect, xla::xtile::XTileDialect,\n+      mlir::NVVM::NVVMDialect, stablehlo::StablehloDialect>();\n   mlir::DialectRegistry registry;\n   mlir::func::registerInlinerExtension(registry);\n   mlir::LLVM::registerInlinerInterface(registry);\n@@ -1929,13 +1920,15 @@ absl::Status CreateInternalError(absl::string_view message,\n   return absl::InternalError(err);\n }\n \n-// Legacy emitter works with tt.func. New emitter works with func.func.\n-// TODO(b/393299275): Remove legacy optionality once migration is complete.\n-void AppendFuncArgType(absl::Span<const int64_t> dims, Type ir_type,\n-                       SmallVector<Type>& fn_arg_types) {\n-  fn_arg_types.push_back(ttir::PointerType::get(\n-      StorageType(ir_type),\n-      static_cast<unsigned>(mlir::NVVM::NVVMMemorySpace::Global)));\n+mlir::MemRefType GetMemRefType(const Shape& shape, mlir::Type element_type) {\n+  mlir::MLIRContext* context = element_type.getContext();\n+\n+  mlir::Type storage_type = StorageType(element_type);\n+  auto minor_to_major_attr =\n+      mlir::DenseI64ArrayAttr::get(context, shape.layout().minor_to_major());\n+  auto layout = mtx::LayoutAttr::get(context, minor_to_major_attr);\n+\n+  return mlir::MemRefType::get(shape.dimensions(), storage_type, layout);\n }\n \n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n@@ -2271,18 +2264,15 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n     } else {\n       TF_ASSIGN_OR_RETURN(ir_type, TritonType(b, type));\n     }\n-\n-    AppendFuncArgType(p->shape().dimensions(), ir_type, fn_arg_types);\n+    fn_arg_types.push_back(GetMemRefType(p->shape(), ir_type));\n   }\n \n-  for (const ShapeUtil::IndexedShape& s :\n-       ShapeUtil::GetLeafShapes(fusion->shape())) {\n-    TF_ASSIGN_OR_RETURN(Type triton_ty, TritonType(b, s.shape.element_type()));\n-    AppendFuncArgType(s.shape.dimensions(), triton_ty, fn_arg_types);\n+  for (const auto& [index, shape] : ShapeUtil::GetLeafShapes(fusion->shape())) {\n+    TF_ASSIGN_OR_RETURN(Type triton_ty, TritonType(b, shape.element_type()));\n+    fn_arg_types.push_back(GetMemRefType(shape, triton_ty));\n   }\n \n-  mlir::FunctionOpInterface fn = b.create<mlir::func::FuncOp>(\n-      fn_name, b.getFunctionType(fn_arg_types, {}));\n+  auto fn = b.create<xtile::EntryFuncOp>(fn_name, fn_arg_types);\n \n   fn.addEntryBlock();\n   b.setInsertionPointToStart(&fn.front());\n@@ -2311,7 +2301,7 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n     return Internal(\"Unsupported fusion kind: %s\", fusion_kind);\n   }\n \n-  b.create<mlir::func::ReturnOp>();\n+  b.create<xtile::EntryFuncReturnOp>();\n \n   return triton_module;\n }\n@@ -2324,6 +2314,7 @@ absl::Status LowerXTileToTriton(mlir::ModuleOp xtile_dialect_module,\n     // Disable verifier because the Triton code may be invalid due to the\n     // unsupported types.\n     pm.enableVerifier(/*enabled=*/false);\n+    pm.addPass(mlir::triton::xla::CreateTritonXLALowerXTilePass());\n     pm.addPass(mlir::triton::xla::CreateTensorLowerToTritonPass());\n     pm.addPass(mlir::triton::xla::CreateStableHLOLowerToTritonPass());\n     if (mlir::failed(pm.run(xtile_dialect_module))) {"
        },
        {
            "sha": "e4983c85b2b175df72d02dcbdf00afae6efe3aee",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 16,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=0f4c3f55b5a87b9457bafb24a1da6382dbefffbb",
            "patch": "@@ -939,7 +939,7 @@ ENTRY main {\n       CreateXTileIrAndFileCheck(this, kHloText, \"triton_reduction_computation\",\n                                 R\"(\n ; Make sure input reduction tile is padded with a neutral value.\n-CHECK:  %[[LOAD:.*]] = triton_xla.extract\n+CHECK:  %[[LOAD:.*]] = xtile.extract\n CHECK:  %[[RANGE:.*]] = stablehlo.iota\n CHECK:  %[[BROADCAST:.*]] = stablehlo.broadcast_in_dim %[[RANGE]]\n CHECK:  %[[CMPI:.*]] = arith.cmpi slt, %[[BROADCAST]]\n@@ -1848,9 +1848,11 @@ ENTRY main {\n       CreateXTileIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n // #xla.indexing_map<\"(pid_0) -> (pid_0 * 32), domain: pid_0 in [0, 1]\n \n-// CHECK: func @{{.*}}(%[[IN:.*]]: !tt.ptr<f32>, %[[OUT:.*]]: !tt.ptr<f32>)\n+// CHECK: xtile.entry_func @{{.*}}(\n+// CHECK-SAME: %[[IN:.*]]: memref<17xf32\n+// CHECK-SAME: %[[OUT:.*]]: memref<49xf32\n \n-// CHECK: %[[EXTRACT:.*]] = triton_xla.extract from %[[IN]] {{.*}}\n+// CHECK: %[[EXTRACT:.*]] = xtile.extract %[[IN]]{{.*}}\n // CHECK: %[[PAD_VALUE:.*]] = arith.constant 1.000000e+00 : f32\n // CHECK: %[[TILE_OFFSET:.*]] = xla.apply_indexing\n // CHECK: %[[IOTA_VAL:.*]] = stablehlo.iota dim = 0 : tensor<32xi32>\n@@ -1865,7 +1867,7 @@ ENTRY main {\n // CHECK: %[[PAD_SPLAT:.*]] = stablehlo.broadcast_in_dim %[[FROM_ELEMENTS_PAD_VALUE]], dims = []\n // CHECK: %[[SELECT:.*]] = arith.select %[[MASK]], %[[EXTRACT]], %[[PAD_SPLAT]]\n \n-// CHECK:   triton_xla.insert %[[SELECT]] into %[[OUT]]\n+// CHECK:   xtile.insert %[[SELECT]] into %[[OUT]]\n           )\"));\n \n   TF_ASSERT_OK(LowerXTileIrToTritonAndFileCheck(\n@@ -1920,7 +1922,7 @@ ENTRY main {\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto xtile_module_and_hlo_module,\n       CreateXTileIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n-// CHECK: triton_xla.extract {{.*}} : tensor<32x16xf32>\n+// CHECK: xtile.extract {{.*}} -> tensor<32x16xf32>\n // CHECK: stablehlo.iota dim = 0 : tensor<32xi32>\n // CHECK: stablehlo.broadcast_in_dim\n // CHECK: arith.cmpi\n@@ -2151,11 +2153,11 @@ ENTRY entry_computation {\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto xtile_module_and_hlo_module,\n       CreateXTileIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n-CHECK:     triton_xla.extract\n+CHECK:     xtile.extract\n CHECK-NOT: stablehlo.transpose\n CHECK:     tt.reshape\n CHECK-NOT: stablehlo.transpose\n-CHECK:     triton_xla.insert\n+CHECK:     xtile.insert\n           )\"));\n \n   TF_ASSERT_OK(LowerXTileIrToTritonAndFileCheck(\n@@ -2195,11 +2197,11 @@ ENTRY entry_computation {\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto xtile_module_and_hlo_module,\n       CreateXTileIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n-CHECK:     triton_xla.extract\n+CHECK:     xtile.extract\n CHECK:     stablehlo.transpose\n CHECK:     tt.reshape\n CHECK-NOT: stablehlo.transpose\n-CHECK:     triton_xla.insert\n+CHECK:     xtile.insert\n           )\"));\n \n   TF_ASSERT_OK(LowerXTileIrToTritonAndFileCheck(\n@@ -2239,11 +2241,11 @@ ENTRY entry_computation {\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto xtile_module_and_hlo_module,\n       CreateXTileIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n-CHECK:     triton_xla.extract\n+CHECK:     xtile.extract\n CHECK-NOT: stablehlo.transpose\n CHECK:     tt.reshape\n CHECK:     stablehlo.transpose\n-CHECK:     triton_xla.insert\n+CHECK:     xtile.insert\n           )\"));\n \n   TF_ASSERT_OK(LowerXTileIrToTritonAndFileCheck(\n@@ -2284,11 +2286,11 @@ ENTRY entry_computation {\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto xtile_module_and_hlo_module,\n       CreateXTileIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n-CHECK:     triton_xla.extract\n+CHECK:     xtile.extract\n CHECK:     stablehlo.transpose\n CHECK:     tt.reshape\n CHECK:     stablehlo.transpose\n-CHECK:     triton_xla.insert\n+CHECK:     xtile.insert\n           )\"));\n \n   TF_ASSERT_OK(LowerXTileIrToTritonAndFileCheck(\n@@ -2328,11 +2330,11 @@ ENTRY entry_computation {\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto xtile_module_and_hlo_module,\n       CreateXTileIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n-CHECK:     triton_xla.extract\n+CHECK:     xtile.extract\n CHECK:     stablehlo.transpose\n CHECK-NOT: tt.reshape\n CHECK-NOT: stablehlo.transpose\n-CHECK:     triton_xla.insert\n+CHECK:     xtile.insert\n           )\"));\n \n   TF_ASSERT_OK(LowerXTileIrToTritonAndFileCheck(\n@@ -2529,7 +2531,7 @@ ENTRY main {\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto xtile_module_and_hlo_module,\n       CreateXTileIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n-CHECK:      %[[TILE:.*]] = triton_xla.extract {{.*}} : tensor<8x4x1xf32>\n+CHECK:      %[[TILE:.*]] = xtile.extract {{.*}} -> tensor<8x4x1xf32>\n CHECK:      stablehlo.transpose %[[TILE]], dims = [2, 0, 1] : (tensor<8x4x1xf32>) -> tensor<1x8x4xf32>\n           )\"));\n "
        },
        {
            "sha": "c997671af8682e00fe0486d6d9cf6c91ced9dd0b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_legacy_matmul.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 4,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul.cc?ref=0f4c3f55b5a87b9457bafb24a1da6382dbefffbb",
            "patch": "@@ -56,7 +56,9 @@ limitations under the License.\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/gpu/codegen/triton/dot_algorithms.h\"\n #include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n+#include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"xla/comparison_util.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n@@ -1063,8 +1065,21 @@ class MatMulEmitterHelper {\n   // bases: The base pointers of each argument.\n   absl::StatusOr<Value> EmitTensorPointer(\n       EmitterLocOpBuilder b, const HloInstruction* hlo, const Side& side,\n-      const ValueRange& bases, Value pid_k,\n+      const ValueRange& args, Value pid_k,\n       std::vector<int32_t>& boundary_checks) {\n+    llvm::SmallVector<mlir::Value> bases;\n+    bases.reserve(hlo->operand_count());\n+    for (mlir::Value arg : args) {\n+      if (mlir::MemRefType memref_type =\n+              mlir::dyn_cast<mlir::MemRefType>(arg.getType())) {\n+        auto ptr_type =\n+            triton::GetGlobalPointerType(memref_type.getElementType());\n+        bases.push_back(b.create<mt::xla::MemrefToPtrOp>(ptr_type, arg));\n+      } else {\n+        bases.push_back(arg);\n+      }\n+    }\n+\n     Value base;\n \n     // Concatenations of parameters are handled during generation of block\n@@ -1107,6 +1122,7 @@ class MatMulEmitterHelper {\n       // Load of a scalar.\n       return base;\n     }\n+\n     auto tensor_ptr = mlir::cast<Value>(\n         b.create<mt::MakeTensorPtrOp>(\n              base, tensor_params.bounds, tensor_params.strides,\n@@ -1876,8 +1892,7 @@ absl::Status EmitMatMul(EmitterLocOpBuilder& b,\n                         absl::string_view libdevice_path,\n                         const se::DeviceDescription& device_info,\n                         const HloFusionInstruction* fusion,\n-                        mlir::FunctionOpInterface fn,\n-                        const BlockLevelParameters&) {\n+                        xtile::EntryFuncOp fn, const BlockLevelParameters&) {\n   TF_ASSIGN_OR_RETURN(TritonGemmConfig config, GetTritonGemmConfig(fusion));\n   TF_ASSIGN_OR_RETURN(auto analysis,\n                       TritonFusionAnalysis::Execute(\n@@ -1989,7 +2004,8 @@ absl::Status EmitMatMul(EmitterLocOpBuilder& b,\n \n   // Emit tensor store operations for all outputs.\n   for (int i = 0;\n-       i < fn.getNumArguments() - dot_instr->parent()->num_parameters(); ++i) {\n+       i < fn.getBufferArgs().size() - dot_instr->parent()->num_parameters();\n+       ++i) {\n     const HloInstruction* producer =\n         root->shape().IsTuple() ? root->operand(i) : root;\n     std::vector<int32_t> boundary_checks;"
        },
        {
            "sha": "cf765afb7474e45ce398166956577061fd3ef4cb",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_legacy_matmul.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul.h?ref=0f4c3f55b5a87b9457bafb24a1da6382dbefffbb",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"mlir/Interfaces/FunctionInterfaces.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n@@ -43,8 +44,7 @@ absl::Status EmitMatMul(EmitterLocOpBuilder& builder,\n                         absl::string_view libdevice_path,\n                         const se::DeviceDescription& device_info,\n                         const HloFusionInstruction* fusion,\n-                        mlir::FunctionOpInterface fn,\n-                        const BlockLevelParameters&);\n+                        xtile::EntryFuncOp fn, const BlockLevelParameters&);\n \n }  // namespace xla::gpu\n "
        },
        {
            "sha": "13e60202238d0d9cbff096e9f529fd32c764895f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_legacy_matmul_stub.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul_stub.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul_stub.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_legacy_matmul_stub.cc?ref=0f4c3f55b5a87b9457bafb24a1da6382dbefffbb",
            "patch": "@@ -19,10 +19,12 @@ limitations under the License.\n #include \"mlir/Interfaces/FunctionInterfaces.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter_legacy_matmul.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/service/gpu/triton_fusion_analysis.h\"\n #include \"xla/stream_executor/device_description.h\"\n \n@@ -39,8 +41,7 @@ absl::Status EmitMatMul(EmitterLocOpBuilder& builder,\n                         absl::string_view libdevice_path,\n                         const se::DeviceDescription& device_info,\n                         const HloFusionInstruction* fusion,\n-                        mlir::FunctionOpInterface fn,\n-                        const BlockLevelParameters&) {\n+                        xtile::EntryFuncOp fn, const BlockLevelParameters&) {\n   return absl::UnimplementedError(\"not supported for this build configuration\");\n }\n "
        },
        {
            "sha": "25a44c2eda4ac766e7ff77552e02c728b762f679",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD?ref=0f4c3f55b5a87b9457bafb24a1da6382dbefffbb",
            "patch": "@@ -58,7 +58,6 @@ cc_library(\n         \"//xla/codegen:emitter_loc_op_builder\",\n         \"//xla/codegen/emitters/ir:xla\",\n         \"//xla/codegen/xtile/ir:xtile\",\n-        \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/stream_executor/gpu:collective_kernel_metadata\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n@@ -67,7 +66,6 @@ cc_library(\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\","
        },
        {
            "sha": "c3db109f89a685be1ce3da1a4063690cca64b1df",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_lower_xtile_pass.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f4c3f55b5a87b9457bafb24a1da6382dbefffbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_xtile_pass.cc?ref=0f4c3f55b5a87b9457bafb24a1da6382dbefffbb",
            "patch": "@@ -49,6 +49,7 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/Triton/IR/Types.h\"\n \n namespace mlir::triton::xla {\n \n@@ -67,7 +68,8 @@ llvm::SmallVector<mlir::Type> GetPtrArgTypes(mlir::ValueRange args) {\n   arg_types.reserve(args.size());\n   for (auto arg : args) {\n     mlir::MemRefType memref_type = mlir::cast<mlir::MemRefType>(arg.getType());\n-    arg_types.push_back(::xla::gpu::triton::GetPointerType(memref_type));\n+    arg_types.push_back(\n+        ::xla::gpu::triton::GetGlobalPointerType(memref_type.getElementType()));\n   }\n   return arg_types;\n }\n@@ -124,9 +126,9 @@ absl::StatusOr<llvm::SmallVector<int64_t>> getPermutationMinorToMajor(\n \n MemrefToPtrOp CreateMemrefToPtr(mlir::OpBuilder& builder,\n                                 mlir::TypedValue<mlir::MemRefType> memref) {\n-  mlir::MemRefType memref_type = memref.getType();\n-  return builder.create<MemrefToPtrOp>(\n-      memref.getLoc(), ::xla::gpu::triton::GetPointerType(memref_type), memref);\n+  PointerType ptr_type = ::xla::gpu::triton::GetGlobalPointerType(\n+      memref.getType().getElementType());\n+  return builder.create<MemrefToPtrOp>(memref.getLoc(), ptr_type, memref);\n }\n \n // Rewrite a xtile entry to a func.func with the same body, but with memref"
        }
    ],
    "stats": {
        "total": 216,
        "additions": 107,
        "deletions": 109
    }
}