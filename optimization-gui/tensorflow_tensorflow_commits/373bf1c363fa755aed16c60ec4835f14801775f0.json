{
    "author": "majiddadashi",
    "message": "Add a pattern to push DRQ FQ forward through pad\n\nThis CL adds a new rewrite pattern, PushForwardDrqFQ, to push a DRQ fake quant op forward through a pad op.\n\nThis pattern transforms the graph from:\ndrq_fake_quant(input) -> pad -> consumer\nto:\ninput -> pad -> drq_fake_quant -> consumer\n\nThis enables the DRQ fake quant op to be fused with its consuming op (e.g., a convolution), leading to a more efficient quantized model.\n\nPiperOrigin-RevId: 798286947",
    "sha": "373bf1c363fa755aed16c60ec4835f14801775f0",
    "files": [
        {
            "sha": "5c321a39d8c89031ee671a74a40b1ad9dc6889d8",
            "filename": "tensorflow/compiler/mlir/lite/tests/quantize-strict.mlir",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/373bf1c363fa755aed16c60ec4835f14801775f0/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Fquantize-strict.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/373bf1c363fa755aed16c60ec4835f14801775f0/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Fquantize-strict.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Fquantize-strict.mlir?ref=373bf1c363fa755aed16c60ec4835f14801775f0",
            "patch": "@@ -18,6 +18,28 @@ func.func @QuantizeConvDRQ(%arg0: tensor<1x4x4x3xf32>) -> (tensor<1x4x4x1xf32>)\n \n // -----\n \n+// CHECK-LABEL: QuantizeConvDrqWithPad\n+func.func @QuantizeConvDrqWithPad(%arg0: tensor<1x4x4x3xf32>) -> (tensor<1x6x6x1xf32>) {\n+  %cst = arith.constant dense<0.000000e+00> : tensor<1xf32>\n+  %cst_0 = arith.constant dense<[[[[1.76285899, -0.257785767, 0.20429258], [1.16310906, 0.23124367, 0.529797196]], [[0.348971426, -0.319283515, -0.772461354], [0.316666812, 1.88180697, -1.78054631]]]]> : tensor<1x2x2x3xf32>\n+  %0 = stablehlo.composite \"quant.fake_quant\" %arg0 {composite_attributes = {dtype = \"i8\", narrow_range = false, quantization_dimension = 0 : i32, scale = dense<> : tensor<0xf64>, zero_point = dense<> : tensor<0xi64>}, decomposition = @XlaCallModule_quant.fake_quant.impl_0} : (tensor<1x4x4x3xf32>) -> tensor<1x4x4x3xf32>\n+  %paddings = arith.constant dense<[[0, 0], [1, 1], [1, 1], [0, 0]]> : tensor<4x2xi32>\n+  %1 = \"tfl.pad\"(%0, %paddings) : (tensor<1x4x4x3xf32>, tensor<4x2xi32>) -> tensor<1x6x6x3xf32>\n+  %2 = \"tfl.quantize\"(%cst_0) <{qtype = tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>}> : (tensor<1x2x2x3xf32>) -> tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>\n+  %3 = \"tfl.dequantize\"(%2) : (tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>) -> tensor<1x2x2x3xf32>\n+  %4 = \"tfl.conv_2d\"(%1, %3, %cst) <{dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}> : (tensor<1x6x6x3xf32>, tensor<1x2x2x3xf32>, tensor<1xf32>) -> tensor<1x6x6x1xf32>\n+  return %4 : tensor<1x6x6x1xf32>\n+\n+// CHECK-LITERAL:    %cst = arith.constant dense<[[0, 0], [1, 1], [1, 1], [0, 0]]> : tensor<4x2xi32>\n+// CHECK:    %cst_0 = arith.constant dense<0.000000e+00> : tensor<1xf32>\n+// CHECK:    %0 = \"tfl.pad\"(%arg0, %cst) : (tensor<1x4x4x3xf32>, tensor<4x2xi32>) -> tensor<1x6x6x3xf32>\n+// CHECK{LITERAL}:    %1 = \"tfl.pseudo_qconst\"() <{qtype = tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>, value = dense<[[[[119, -17, 14], [78, 16, 36]], [[24, -22, -52], [21, 127, -120]]]]> : tensor<1x2x2x3xi8>}> : () -> tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>\n+// CHECK:    %2 = \"tfl.conv_2d\"(%0, %1, %cst_0) <{dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32}> : (tensor<1x6x6x3xf32>, tensor<1x2x2x3x!quant.uniform<i8:f32, 0.014817377552390099>>, tensor<1xf32>) -> tensor<1x6x6x1xf32>\n+// CHECK:    return %2 : tensor<1x6x6x1xf32>\n+}\n+\n+// -----\n+\n // CHECK-LABEL: QuantizeConvWithBiasDRQ\n func.func @QuantizeConvWithBiasDRQ(%arg0: tensor<1x4x4x3xf32>) -> (tensor<1x4x4x1xf32>) {\n   %cst = arith.constant dense<1.14751196> : tensor<1xf32>"
        },
        {
            "sha": "d0c143d73914c96d4007fddf38d7fecea0639cca",
            "filename": "tensorflow/compiler/mlir/lite/transforms/quantize.cc",
            "status": "modified",
            "additions": 53,
            "deletions": 1,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/373bf1c363fa755aed16c60ec4835f14801775f0/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fquantize.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/373bf1c363fa755aed16c60ec4835f14801775f0/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fquantize.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fquantize.cc?ref=373bf1c363fa755aed16c60ec4835f14801775f0",
            "patch": "@@ -176,6 +176,57 @@ class RemoveUnusedFQ : public OpRewritePattern<stablehlo::CompositeOp> {\n   }\n };\n \n+// Pushes a drq fake quant op forward through a pad op.\n+// This is to allow DRQ FQ to be fused into the DRQ op.\n+// drq_fake_quant(input) -> pad -> output\n+// becomes\n+// input -> pad -> drq_fake_quant -> output\n+class PushForwardDrqFQ : public OpRewritePattern<stablehlo::CompositeOp> {\n+ public:\n+  using OpRewritePattern<stablehlo::CompositeOp>::OpRewritePattern;\n+\n+  LogicalResult matchAndRewrite(stablehlo::CompositeOp drq_fq_op,\n+                                PatternRewriter& rewriter) const final {\n+    if (!IsDrqFakeQuant(drq_fq_op)) {\n+      return rewriter.notifyMatchFailure(drq_fq_op,\n+                                         \"is not a drq fake quant op.\");\n+    }\n+\n+    if (!drq_fq_op.getResult(0).hasOneUse()) {\n+      return rewriter.notifyMatchFailure(\n+          drq_fq_op, \"drq fake quant op does not have one use.\");\n+    }\n+    auto pad_op =\n+        llvm::dyn_cast<TFL::PadOp>(*drq_fq_op.getResult(0).user_begin());\n+    if (!pad_op) {\n+      return rewriter.notifyMatchFailure(drq_fq_op,\n+                                         \"user is not a tfl.pad op.\");\n+    }\n+\n+    // The input to the new pad op is the float input to the drq fake quant op.\n+    Value float_input = drq_fq_op.getOperand(drq_fq_op.getNumOperands() - 1);\n+\n+    // Create a new pad op.\n+    auto new_pad_op = rewriter.create<TFL::PadOp>(\n+        pad_op.getLoc(), pad_op.getType(), float_input, pad_op.getPadding());\n+\n+    // Create a new drq fake quant op.\n+    // Operands are the same, except for the last one.\n+    SmallVector<Value> new_drq_operands;\n+    for (Value operand : drq_fq_op.getOperands().drop_back()) {\n+      new_drq_operands.push_back(operand);\n+    }\n+    new_drq_operands.push_back(new_pad_op.getResult());\n+\n+    auto new_drq_fq_op = rewriter.create<stablehlo::CompositeOp>(\n+        drq_fq_op.getLoc(), pad_op.getType(), new_drq_operands,\n+        drq_fq_op->getAttrs());\n+\n+    rewriter.replaceOp(pad_op, new_drq_fq_op.getResult(0));\n+    return success();\n+  }\n+};\n+\n class StrictQuantizationPattern : public RewritePattern {\n  public:\n   using BaseType = StrictQuantizationPattern;\n@@ -693,7 +744,8 @@ void QuantizePass::runOnOperation() {\n \n   if (quant_specs.qdq_conversion_mode == QDQConversionMode::kQDQStrict) {\n     patterns.add<StrictQuantizationPattern>(ctx, quant_params);\n-    patterns.add<RemoveUnusedFQ, SquashDqQ, FuseDqQToRequant>(ctx);\n+    patterns.add<RemoveUnusedFQ, SquashDqQ, FuseDqQToRequant, PushForwardDrqFQ>(\n+        ctx);\n   } else if (quant_specs.weight_quantization ||\n              quant_specs.use_fake_quant_num_bits ||\n              quant_specs.qdq_conversion_mode =="
        }
    ],
    "stats": {
        "total": 76,
        "additions": 75,
        "deletions": 1
    }
}