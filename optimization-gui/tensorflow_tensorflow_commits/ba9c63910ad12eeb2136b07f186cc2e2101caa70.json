{
    "author": "WillFroom",
    "message": "[XLA:CPU][XTile] Create simple lowering for tiled ops.\n\nPiperOrigin-RevId: 820160792",
    "sha": "ba9c63910ad12eeb2136b07f186cc2e2101caa70",
    "files": [
        {
            "sha": "af42391e513e20d43dc0812aee18575a69c4e93d",
            "filename": "third_party/xla/xla/backends/cpu/codegen/BUILD",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -146,13 +146,15 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/backends/cpu/codegen/emitters/ir:xla_cpu\",\n         \"//xla/backends/cpu/codegen/emitters/transforms:passes\",\n+        \"//xla/backends/cpu/codegen/tiled/transforms:passes\",\n         \"//xla/codegen:llvm_ir_kernel_source\",\n         \"//xla/codegen:mlir_kernel_source\",\n         \"//xla/codegen:trace_pass_instrumentation\",\n         \"//xla/codegen/emitters/ir:xla\",\n         \"//xla/codegen/emitters/ir:xla_attrs_inc_gen\",\n         \"//xla/codegen/emitters/transforms:pass_pipelines\",\n         \"//xla/codegen/emitters/transforms:passes\",\n+        \"//xla/codegen/xtile/ir:xtile\",\n         \"//xla/mlir/tools/mlir_replay/public:compiler_trace_proto_cc\",\n         \"//xla/mlir_hlo\",\n         \"//xla/service/gpu/model/experimental:symbolic_expr\",\n@@ -182,6 +184,7 @@ cc_library(\n         \"@llvm-project//mlir:LLVMToLLVMIRTranslation\",\n         \"@llvm-project//mlir:MathDialect\",\n         \"@llvm-project//mlir:MathToLLVM\",\n+        \"@llvm-project//mlir:MemRefToLLVM\",\n         \"@llvm-project//mlir:MemRefTransforms\",\n         \"@llvm-project//mlir:Pass\",\n         \"@llvm-project//mlir:ReconcileUnrealizedCasts\",\n@@ -191,7 +194,10 @@ cc_library(\n         \"@llvm-project//mlir:TensorDialect\",\n         \"@llvm-project//mlir:ToLLVMIRTranslation\",\n         \"@llvm-project//mlir:Transforms\",\n+        \"@llvm-project//mlir:UBToLLVM\",\n         \"@llvm-project//mlir:VectorDialect\",\n+        \"@llvm-project//mlir:VectorToLLVM\",\n+        \"@llvm-project//mlir:VectorToSCF\",\n         \"@local_tsl//tsl/profiler/lib:traceme\",\n         \"@local_tsl//tsl/profiler/lib:traceme_encode\",\n         \"@stablehlo//:stablehlo_passes\","
        },
        {
            "sha": "40d37cbf958054e24244853dbae49ad562a51572",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_compiler.cc",
            "status": "modified",
            "additions": 109,
            "deletions": 36,
            "changes": 145,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -38,8 +38,13 @@ limitations under the License.\n #include \"mlir/Conversion/AffineToStandard/AffineToStandard.h\"\n #include \"mlir/Conversion/ComplexToStandard/ComplexToStandard.h\"\n #include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n+#include \"mlir/Conversion/MemRefToLLVM/MemRefToLLVM.h\"\n #include \"mlir/Conversion/ReconcileUnrealizedCasts/ReconcileUnrealizedCasts.h\"\n #include \"mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h\"\n+#include \"mlir/Conversion/UBToLLVM/UBToLLVM.h\"\n+#include \"mlir/Conversion/VectorToLLVM/ConvertVectorToLLVM.h\"\n+#include \"mlir/Conversion/VectorToLLVM/ConvertVectorToLLVMPass.h\"\n+#include \"mlir/Conversion/VectorToSCF/VectorToSCF.h\"\n #include \"mlir/Dialect/Affine/IR/AffineOps.h\"\n #include \"mlir/Dialect/Affine/Passes.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n@@ -58,6 +63,7 @@ limitations under the License.\n #include \"mlir/IR/Attributes.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/Operation.h\"\n #include \"mlir/IR/Visitors.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Support/LLVM.h\"\n@@ -70,6 +76,7 @@ limitations under the License.\n #include \"xla/backends/cpu/codegen/emitters/ir/xla_cpu_dialect.h\"\n #include \"xla/backends/cpu/codegen/emitters/transforms/passes.h\"\n #include \"xla/backends/cpu/codegen/kernel_api_ir_builder.h\"\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h\"\n #include \"xla/codegen/emitters/ir/xla_attrs.h.inc\"\n #include \"xla/codegen/emitters/ir/xla_dialect.h\"\n #include \"xla/codegen/emitters/ir/xla_ops.h\"\n@@ -78,6 +85,8 @@ limitations under the License.\n #include \"xla/codegen/llvm_ir_kernel_source.h\"\n #include \"xla/codegen/mlir_kernel_source.h\"\n #include \"xla/codegen/trace_pass_instrumentation.h\"\n+#include \"xla/codegen/xtile/ir/xtile_dialect.h\"\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"xla/mlir/tools/mlir_replay/public/compiler_trace.pb.h\"\n #include \"xla/mlir_hlo/mhlo/IR/hlo_ops.h\"\n #include \"xla/status_macros.h\"\n@@ -114,6 +123,34 @@ static std::unique_ptr<::mlir::Pass> CreateConvertMathToLLVMPass() {\n   return mlir::createConvertMathToLLVMPass(options);\n }\n \n+// The final lowering passes common to both scalar and tiled kernels.\n+// These passes are primarily responsible for lowering individual ops to\n+// their LLVM equivalent.\n+static void AddGenericLoweringPasses(mlir::OpPassManager& pm) {\n+  pm.addPass(emitters::CreateSimplifyAffinePass());\n+  pm.addPass(mlir::createCanonicalizerPass());\n+\n+  // simplify-affine lowers most affine.apply ops, but if it can't prove a\n+  // division or modulo is unsigned, affine.apply ops will remain.\n+  pm.addPass(mlir::createLowerAffinePass());\n+\n+  pm.addPass(mlir::createLoopInvariantCodeMotionPass());\n+  pm.addPass(mlir::createSymbolDCEPass());\n+  pm.addPass(mlir::createCSEPass());\n+\n+  pm.addNestedPass<mlir::func::FuncOp>(cpu::CreateExpandFloatOpsPass());\n+  pm.addPass(emitters::CreateExpandFloatOpsPass(/*aproximate_tanh=*/false));\n+  pm.addPass(emitters::CreateEraseDeadFunctionsPass());\n+  pm.addPass(mlir::createLowerAffinePass());\n+  pm.addPass(mlir::createSCFToControlFlowPass());\n+  pm.addPass(emitters::CreateLowerXlaIntrinsicLibPass());\n+  pm.addNestedPass<mlir::func::FuncOp>(CreateConvertMathToLLVMPass());\n+  pm.addPass(emitters::CreateLowerToLLVMPass(/*target_type=*/\"cpu\"));\n+  pm.addPass(mlir::createReconcileUnrealizedCastsPass());\n+  pm.addPass(mlir::createCanonicalizerPass());\n+  pm.addPass(mlir::createCSEPass());\n+}\n+\n static std::unique_ptr<::mlir::Pass> CreateInlinerAndCsePass() {\n   return mlir::createCompositeFixedPointPass(\n       \"Inliner\", [](mlir::OpPassManager& pm) {\n@@ -124,8 +161,12 @@ static std::unique_ptr<::mlir::Pass> CreateInlinerAndCsePass() {\n       });\n }\n \n-static void AddLoopTransformationPasses(mlir::OpPassManager& pm,\n+// Optimizations passes for the \"hero\" emitters, e.g. loop emitter.\n+// It is expected that the input has a simple nested loop structure that works\n+// on scalar instructions extracted/inserted from tensor types.\n+static void AddScalarOptimizationPasses(mlir::OpPassManager& pm,\n                                         int32_t vector_width) {\n+  emitters::RegisterOptimizationPasses(pm);\n   pm.addPass(CreateAddReductionFastMathFlagsPass());\n   pm.addPass(CreateInlinerAndCsePass());\n   pm.addNestedPass<mlir::func::FuncOp>(CreatePeelWorkgroupLoopPass());\n@@ -154,8 +195,12 @@ static void AddLoopTransformationPasses(mlir::OpPassManager& pm,\n   pm.addNestedPass<mlir::func::FuncOp>(CreateAddLoopUnrollFlagsPass());\n }\n \n-static void AddLoweringPasses(mlir::OpPassManager& pm, int32_t vector_width,\n-                              bool fast_min_max) {\n+// Lowering passes for the \"hero\" emitters, e.g. loop emitter.\n+// It is expected that the input has a simple nested loop structure that works\n+// on scalar instructions extracted/inserted from tensor types.\n+// The resulting IR can then be translated to native LLVM.\n+static void AddScalarLoweringPasses(mlir::OpPassManager& pm,\n+                                    int32_t vector_width, bool fast_min_max) {\n   pm.addNestedPass<mlir::func::FuncOp>(\n       emitters::CreateConvertPureCallOpsPass());\n   pm.addPass(cpu::createLowerToLLVMPass(\n@@ -170,28 +215,32 @@ static void AddLoweringPasses(mlir::OpPassManager& pm, int32_t vector_width,\n   pm.addPass(mlir::createCSEPass());\n   pm.addNestedPass<mlir::func::FuncOp>(\n       emitters::CreateSimplifyArithPass(fast_min_max));\n-  pm.addPass(emitters::CreateSimplifyAffinePass());\n-  pm.addPass(mlir::createCanonicalizerPass());\n+  AddGenericLoweringPasses(pm);\n+}\n \n-  // simplify-affine lowers most affine.apply ops, but if it can't prove a\n-  // division or modulo is unsigned, affine.apply ops will remain.\n-  pm.addPass(mlir::createLowerAffinePass());\n+// Optimizations passes for the tiled emitter.\n+// This is currently very simple but will grow to include tiled optimizations\n+// such as transpose hoisting and dimension reduction.\n+static void AddTiledOptimizationPasses(mlir::OpPassManager& pm) {\n+  emitters::RegisterOptimizationPasses(pm);\n+}\n \n-  pm.addPass(mlir::createLoopInvariantCodeMotionPass());\n-  pm.addPass(mlir::createSymbolDCEPass());\n-  pm.addPass(mlir::createCSEPass());\n+// Lowering passes for the tiled emitter.\n+// The input IR is from the xtile dialect which uses tensors that are converted\n+// first to the vector dialect and then to LLVM.\n+static void AddTiledLoweringPasses(mlir::OpPassManager& pm) {\n+  pm.addPass(CreateXTileToVectorPass());\n+  pm.addPass(CreateElementalTensorToVectorPass());\n+  pm.addPass(CreateShloToVectorPass());\n+  pm.addPass(CreateLowerXTileEntryPass());\n+  pm.addPass(cpu::createLowerToLLVMPass());\n+  pm.addPass(mlir::createConvertVectorToSCFPass(\n+      mlir::VectorTransferToSCFOptions().enableFullUnroll(false)));\n+  pm.addPass(mlir::createConvertVectorToLLVMPass());\n \n-  pm.addNestedPass<mlir::func::FuncOp>(cpu::CreateExpandFloatOpsPass());\n-  pm.addPass(emitters::CreateExpandFloatOpsPass(/*aproximate_tanh=*/false));\n-  pm.addPass(emitters::CreateEraseDeadFunctionsPass());\n-  pm.addPass(mlir::createLowerAffinePass());\n-  pm.addPass(mlir::createSCFToControlFlowPass());\n-  pm.addPass(emitters::CreateLowerXlaIntrinsicLibPass());\n-  pm.addNestedPass<mlir::func::FuncOp>(CreateConvertMathToLLVMPass());\n-  pm.addPass(emitters::CreateLowerToLLVMPass(/*target_type=*/\"cpu\"));\n-  pm.addPass(mlir::createReconcileUnrealizedCastsPass());\n-  pm.addPass(mlir::createCanonicalizerPass());\n-  pm.addPass(mlir::createCSEPass());\n+  pm.addPass(mlir::createConvertComplexToStandardPass());\n+\n+  AddGenericLoweringPasses(pm);\n }\n \n static int GetLlvmFunctionDefCount(mlir::ModuleOp m) {\n@@ -223,18 +272,31 @@ FusionCompiler::FusionCompiler(mlir::MLIRContext* context, Options options,\n                                CompilationHooks hooks)\n     : options_(std::move(options)),\n       hooks_(std::move(hooks)),\n-      optimization_pass_manager_(\n+      scalar_optimization_pass_manager_(\n+          mlir::PassManager::on<mlir::ModuleOp>(context)),\n+      tiled_optimization_pass_manager_(\n+          mlir::PassManager::on<mlir::ModuleOp>(context)),\n+      scalar_lowering_pass_manager_(\n           mlir::PassManager::on<mlir::ModuleOp>(context)),\n-      lowering_pass_manager_(mlir::PassManager::on<mlir::ModuleOp>(context)) {\n-  emitters::RegisterOptimizationPasses(optimization_pass_manager_);\n-  AddLoopTransformationPasses(optimization_pass_manager_,\n+      tiled_lowering_pass_manager_(\n+          mlir::PassManager::on<mlir::ModuleOp>(context)) {\n+  // Scalar passes.\n+  AddScalarOptimizationPasses(scalar_optimization_pass_manager_,\n                               options_.vector_width);\n-  optimization_pass_manager_.addInstrumentation(\n-      std::make_unique<TraceInstrumentation>());\n+  AddScalarLoweringPasses(scalar_lowering_pass_manager_, options_.vector_width,\n+                          options_.fast_min_max);\n+\n+  // Tiled passes.\n+  AddTiledOptimizationPasses(tiled_optimization_pass_manager_);\n+  AddTiledLoweringPasses(tiled_lowering_pass_manager_);\n \n-  AddLoweringPasses(lowering_pass_manager_, options_.vector_width,\n-                    options_.fast_min_max);\n-  lowering_pass_manager_.addInstrumentation(\n+  scalar_optimization_pass_manager_.addInstrumentation(\n+      std::make_unique<TraceInstrumentation>());\n+  scalar_lowering_pass_manager_.addInstrumentation(\n+      std::make_unique<TraceInstrumentation>());\n+  tiled_optimization_pass_manager_.addInstrumentation(\n+      std::make_unique<TraceInstrumentation>());\n+  tiled_lowering_pass_manager_.addInstrumentation(\n       std::make_unique<TraceInstrumentation>());\n }\n \n@@ -252,6 +314,14 @@ absl::StatusOr<std::unique_ptr<llvm::Module>> FusionCompiler::Compile(\n     });\n     return count;\n   };\n+\n+  bool is_tiled = !mlir_module.getBody()->getOps<xtile::EntryFuncOp>().empty();\n+  mlir::PassManager& optimization_pm = is_tiled\n+                                           ? tiled_optimization_pass_manager_\n+                                           : scalar_optimization_pass_manager_;\n+  mlir::PassManager& lowering_pm =\n+      is_tiled ? tiled_lowering_pass_manager_ : scalar_lowering_pass_manager_;\n+\n   VLOG(1) << \"Compiling MLIR module: \" << module_name << \", with \"\n           << get_module_op_count() << \" operations.\";\n   XLA_SCOPED_LOGGING_TIMER_LEVEL(\n@@ -266,15 +336,15 @@ absl::StatusOr<std::unique_ptr<llvm::Module>> FusionCompiler::Compile(\n   if (hooks_.pre_optimization) {\n     hooks_.pre_optimization(mlir_module);\n   }\n-  TF_RETURN_IF_ERROR(RunPassPipeline(mlir_module, optimization_pass_manager_,\n-                                     nullptr, options_.verification_level));\n+  TF_RETURN_IF_ERROR(RunPassPipeline(mlir_module, optimization_pm, nullptr,\n+                                     options_.verification_level));\n \n   if (hooks_.post_optimization) {\n     hooks_.post_optimization(mlir_module);\n   }\n \n-  TF_RETURN_IF_ERROR(RunPassPipeline(mlir_module, lowering_pass_manager_,\n-                                     nullptr, options_.verification_level));\n+  TF_RETURN_IF_ERROR(RunPassPipeline(mlir_module, lowering_pm, nullptr,\n+                                     options_.verification_level));\n \n   if (hooks_.post_lowering) {\n     hooks_.post_lowering(mlir_module);\n@@ -347,14 +417,17 @@ std::unique_ptr<mlir::MLIRContext> FusionCompiler::CreateContext() {\n                        xla::cpu::XlaCpuDialect, mlir::mhlo::MhloDialect,\n                        mlir::scf::SCFDialect, mlir::LLVM::LLVMDialect,\n                        mlir::tensor::TensorDialect, mlir::vector::VectorDialect,\n-                       xla::XlaDialect>();\n+                       xla::XlaDialect, xla::xtile::XTileDialect>();\n \n   mlir::DialectRegistry registry;\n   mlir::LLVM::registerInlinerInterface(registry);\n   mlir::func::registerInlinerExtension(registry);\n   mlir::registerLLVMDialectTranslation(registry);\n   mlir::registerBuiltinDialectTranslation(registry);\n   mlir::registerConvertMathToLLVMInterface(registry);\n+  mlir::registerConvertMemRefToLLVMInterface(registry);\n+  mlir::ub::registerConvertUBToLLVMInterface(registry);\n+  mlir::vector::registerConvertVectorToLLVMInterface(registry);\n   context->appendDialectRegistry(registry);\n \n   return context;"
        },
        {
            "sha": "5c301e1fe96145d2720bba40bab4be1dadfc11de",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_compiler.h",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.h?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -67,11 +67,19 @@ class FusionCompiler {\n  private:\n   Options options_;\n   CompilationHooks hooks_;\n+  // The reason we have 4 distinct pass managers is because:\n+  //   - We have 2 stages: optimization and lowering, this is to enable dumping\n+  //     of the intermediate optimized MLIR.\n+  //   - We have 2 distinct pipelines for scalar and tiled kernels, this is\n+  //     because they differ slightly in their semantics, ideally these would be\n+  //     unified but this is a larger change.\n   // Pass manager that holds the optimization & loop transformation passes.\n-  mlir::PassManager optimization_pass_manager_;\n+  mlir::PassManager scalar_optimization_pass_manager_;\n+  mlir::PassManager tiled_optimization_pass_manager_;\n   // Pass manager that holds the passes responsible for lowering the module from\n   // MLIR to LLVM.\n-  mlir::PassManager lowering_pass_manager_;\n+  mlir::PassManager scalar_lowering_pass_manager_;\n+  mlir::PassManager tiled_lowering_pass_manager_;\n };\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "1eb3c4d2935fe1563383fdf3b2b12e01f1e12bf5",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/BUILD",
            "status": "added",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2FBUILD?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -0,0 +1,22 @@\n+load(\"//xla:py_strict.bzl\", \"py_strict_test\")\n+\n+package(\n+    # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n+    licenses = [\"notice\"],\n+)\n+\n+py_strict_test(\n+    name = \"tiled_kernel_test\",\n+    srcs = [\"tiled_kernel_test.py\"],\n+    main = \"tiled_kernel_test.py\",\n+    tags = [\n+        \"no_oss\",\n+    ],\n+    deps = [\n+        \"//third_party/py/numpy\",\n+        \"//xla:xla_data_proto_py\",\n+        \"//xla/backends/cpu/testlib\",\n+        \"//xla/codegen/testlib\",\n+        \"@absl_py//absl/testing:absltest\",\n+    ],\n+)"
        },
        {
            "sha": "ac6c36dd7f4340addcee3adab693413255f723fe",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/tiled_kernel_test.py",
            "status": "added",
            "additions": 144,
            "deletions": 0,
            "changes": 144,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_kernel_test.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_kernel_test.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_kernel_test.py?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -0,0 +1,144 @@\n+# Copyright 2024 The OpenXLA Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+from collections.abc import Callable, Iterable\n+\n+from absl.testing import absltest\n+import numpy as np\n+\n+from xla.backends.cpu import testlib as cpu_testlib\n+from xla.codegen import testlib as base_testlib\n+from xla.codegen.testlib import utilities as testlib_utilities\n+\n+create_literal = testlib_utilities.create_literal_from_np\n+\n+\n+def compare_kernel(\n+    ir: str,\n+    kernel_name: str,\n+    num_workgroups: int,\n+    input_shapes: Iterable[tuple[int, ...]],\n+    output_shape: tuple[int, ...],\n+    dtype,\n+    expected_output: Callable[[np.ndarray, ...], np.ndarray],\n+) -> None:\n+  mlir_emitter = cpu_testlib.MlirTestKernelEmitter(\n+      ir, kernel_name, (num_workgroups, 1, 1)\n+  )\n+  kernel_definition = mlir_emitter.emit_kernel_definition()\n+\n+  runner = cpu_testlib.KernelRunner.create(\n+      kernel_definition,\n+      cpu_testlib.JitCompiler(base_testlib.HloModuleConfig()),\n+  )\n+  inputs = [np.random.rand(*shape).astype(dtype) for shape in input_shapes]\n+\n+  input_tensors = [create_literal(input) for input in inputs]\n+  output_tensor = create_literal(np.zeros(output_shape, dtype=dtype))\n+  runner.call(input_tensors + [output_tensor])\n+\n+  np.testing.assert_array_equal(\n+      np.asarray(output_tensor), expected_output(*inputs)\n+  )\n+\n+\n+class XtileLoweringTest(absltest.TestCase):\n+\n+  def test_slice(self):\n+    # Check that masked extract / insert works.\n+    ir = \"\"\"\n+      module @tiled_slice {\n+        xtile.entry_func @tiled_slice(\n+            %input: memref<5x5xf32>,\n+            %output: memref<5x5xf32>,\n+            %tile_id: index) attributes {xtile.tiling_info = #xtile.tiling_info<tile_count:1, tiles_per_workgroup:1>} {\n+          %offset = arith.constant 0 : index\n+          %input_tile = xtile.extract %input[%offset, %offset][64, 64][1, 1] : memref<5x5xf32> -> tensor<64x64xf32>\n+          %transposed_tile = stablehlo.transpose %input_tile, dims = [1, 0] : (tensor<64x64xf32>) -> tensor<64x64xf32>\n+          xtile.insert %transposed_tile into %output[%offset, %offset][64, 64][1, 1] : tensor<64x64xf32> -> memref<5x5xf32>\n+          xtile.return\n+        }\n+      }\n+    \"\"\"\n+\n+    compare_kernel(\n+        ir,\n+        \"tiled_slice\",\n+        1,\n+        [(5, 5)],\n+        (5, 5),\n+        np.float32,\n+        lambda arg: arg.transpose(),\n+    )\n+\n+  def test_transpose(self):\n+    ir = \"\"\"\n+      module @tiled_transpose {\n+        xtile.entry_func @tiled_transpose(\n+            %input: memref<4096x4096xf32>,\n+            %output: memref<4096x4096xf32>,\n+            %tile_id: index) attributes {xtile.tiling_info = #xtile.tiling_info<tile_count:262144, tiles_per_workgroup:32768>} {\n+          %offset_0 = xla.apply_indexing #xla.indexing_map<\"(tid) -> ((tid mod 512) * 8), domain: tid in [0, 262144]\">(%tile_id)\n+          %offset_1 = xla.apply_indexing #xla.indexing_map<\"(tid) -> ((tid floordiv 512) * 8), domain: tid in [0, 262144]\">(%tile_id)\n+          %input_tile = xtile.extract %input[%offset_0, %offset_1][8, 8][1, 1] : memref<4096x4096xf32> -> tensor<8x8xf32>\n+          %transposed_tile = stablehlo.transpose %input_tile, dims = [1, 0] : (tensor<8x8xf32>) -> tensor<8x8xf32>\n+          xtile.insert %transposed_tile into %output[%offset_1, %offset_0][8, 8][1, 1] : tensor<8x8xf32> -> memref<4096x4096xf32>\n+          xtile.return\n+        }\n+      }\n+    \"\"\"\n+\n+    compare_kernel(\n+        ir,\n+        \"tiled_transpose\",\n+        8,\n+        [(4096, 4096)],\n+        (4096, 4096),\n+        np.float32,\n+        lambda arg: arg.transpose(),\n+    )\n+\n+  def test_add_tranpose(self):\n+    ir = \"\"\"\n+      module @add_tranpose {\n+        xtile.entry_func @add_tranpose(\n+            %input: memref<4096x4096xf32>,\n+            %output: memref<4096x4096xf32>,\n+            %tile_id: index) attributes {xtile.tiling_info = #xtile.tiling_info<tile_count:262144, tiles_per_workgroup:32768>} {\n+          %offset_0 = xla.apply_indexing #xla.indexing_map<\"(tid) -> ((tid mod 512) * 8), domain: tid in [0, 262144]\">(%tile_id)\n+          %offset_1 = xla.apply_indexing #xla.indexing_map<\"(tid) -> ((tid floordiv 512) * 8), domain: tid in [0, 262144]\">(%tile_id)\n+          %input_tile_0 = xtile.extract %input[%offset_0, %offset_1][8, 8][1, 1] : memref<4096x4096xf32> -> tensor<8x8xf32>\n+          %input_tile_1 = xtile.extract %input[%offset_1, %offset_0][8, 8][1, 1] : memref<4096x4096xf32> -> tensor<8x8xf32>\n+          %transposed_tile = stablehlo.transpose %input_tile_0, dims = [1, 0] : (tensor<8x8xf32>) -> tensor<8x8xf32>\n+          %added_tile = arith.addf %input_tile_1, %transposed_tile : tensor<8x8xf32>\n+          xtile.insert %added_tile into %output[%offset_1, %offset_0][8, 8][1, 1] : tensor<8x8xf32> -> memref<4096x4096xf32>\n+          xtile.return\n+        }\n+      }\n+    \"\"\"\n+\n+    compare_kernel(\n+        ir,\n+        \"add_tranpose\",\n+        8,\n+        [(4096, 4096)],\n+        (4096, 4096),\n+        np.float32,\n+        lambda arg: arg + arg.transpose(),\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+  absltest.main()"
        },
        {
            "sha": "6315951b9fb23cf52db18b19f55d5b66e44a6997",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/BUILD",
            "status": "added",
            "additions": 66,
            "deletions": 0,
            "changes": 66,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -0,0 +1,66 @@\n+load(\"@llvm-project//mlir:tblgen.bzl\", \"gentbl_cc_library\")\n+load(\"//xla/tsl:tsl.default.bzl\", \"get_compatible_with_portable\")\n+load(\"//xla/tsl/platform:rules_cc.bzl\", \"cc_library\")\n+\n+package(\n+    # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n+    default_visibility = [\":friends\"],\n+    licenses = [\"notice\"],\n+)\n+\n+package_group(\n+    name = \"friends\",\n+    includes = [\n+        \"//xla:friends\",\n+    ],\n+)\n+\n+gentbl_cc_library(\n+    name = \"passes_inc_gen\",\n+    compatible_with = get_compatible_with_portable(),\n+    tbl_outs = {\"passes.h.inc\": [\n+        \"-gen-pass-decls\",\n+        \"-name=XTileCpuTransforms\",\n+    ]},\n+    tblgen = \"@llvm-project//mlir:mlir-tblgen\",\n+    td_file = \"passes.td\",\n+    visibility = [\"//visibility:private\"],\n+    deps = [\"@llvm-project//mlir:PassBaseTdFiles\"],\n+)\n+\n+cc_library(\n+    name = \"passes\",\n+    srcs = [\n+        \"elemental_tensor_to_vector.cc\",\n+        \"lower_xtile_entry.cc\",\n+        \"shlo_to_vector.cc\",\n+        \"xtile_to_vector.cc\",\n+    ],\n+    hdrs = [\"passes.h\"],\n+    deps = [\n+        \":passes_inc_gen\",\n+        \"//xla/backends/cpu/codegen/emitters/ir:xla_cpu\",\n+        \"//xla/codegen/emitters/ir:xla\",\n+        \"//xla/codegen/xtile/ir:xtile\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//mlir:ArithDialect\",\n+        \"@llvm-project//mlir:ArithOpsIncGen\",\n+        \"@llvm-project//mlir:DataLayoutInterfaces\",\n+        \"@llvm-project//mlir:FuncDialect\",\n+        \"@llvm-project//mlir:FuncTransforms\",\n+        \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:LLVMDialect\",\n+        \"@llvm-project//mlir:MathDialect\",\n+        \"@llvm-project//mlir:MathOpsIncGen\",\n+        \"@llvm-project//mlir:Pass\",\n+        \"@llvm-project//mlir:SCFDialect\",\n+        \"@llvm-project//mlir:Support\",\n+        \"@llvm-project//mlir:TensorDialect\",\n+        \"@llvm-project//mlir:TransformUtils\",\n+        \"@llvm-project//mlir:VectorDialect\",\n+        \"@stablehlo//:stablehlo_ops\",\n+    ],\n+)"
        },
        {
            "sha": "d457bc29ae3f45612669567948f8bba37205a61b",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/elemental_tensor_to_vector.cc",
            "status": "added",
            "additions": 171,
            "deletions": 0,
            "changes": 171,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Felemental_tensor_to_vector.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Felemental_tensor_to_vector.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Felemental_tensor_to_vector.cc?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -0,0 +1,171 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cassert>\n+#include <memory>\n+#include <utility>\n+\n+#include \"llvm/Support/LogicalResult.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/Func/Transforms/FuncConversions.h\"\n+#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/BuiltinTypeInterfaces.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/Types.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/Visitors.h\"\n+#include \"mlir/Interfaces/DataLayoutInterfaces.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h\"\n+\n+namespace xla::cpu {\n+\n+#define GEN_PASS_DECL_ELEMENTALTENSORTOVECTORPASS\n+#define GEN_PASS_DEF_ELEMENTALTENSORTOVECTORPASS\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h.inc\"\n+\n+namespace {\n+\n+// This converter defines the rules for mapping types from the source (tensors)\n+// to the target (vectors).\n+class TensorToVectorTypeConverter : public mlir::TypeConverter {\n+ public:\n+  TensorToVectorTypeConverter() {\n+    // Keep all non-tensor types as-is.\n+    addConversion([](mlir::Type type) { return type; });\n+\n+    // Convert RankedTensorType to VectorType.\n+    addConversion([](mlir::RankedTensorType type) -> mlir::Type {\n+      // We can only convert tensors with a static shape to vectors.\n+      if (!type.hasStaticShape()) {\n+        return nullptr;  // Return null if the type cannot be converted.\n+      }\n+      return mlir::VectorType::get(type.getShape(), type.getElementType());\n+    });\n+  }\n+};\n+\n+// A generic pattern to convert an elemental op from tensor-based to\n+// vector-based.\n+template <typename ElementalOp>\n+class ElementalOpConversion : public mlir::OpConversionPattern<ElementalOp> {\n+ public:\n+  using mlir::OpConversionPattern<ElementalOp>::OpConversionPattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      ElementalOp op, typename ElementalOp::Adaptor adaptor,\n+      mlir::ConversionPatternRewriter& rewriter) const override {\n+    llvm::SmallVector<mlir::Type> new_result_types;\n+    mlir::LogicalResult results_ok = this->getTypeConverter()->convertTypes(\n+        op->getResultTypes(), new_result_types);\n+    if (results_ok.failed()) {\n+      return rewriter.notifyMatchFailure(op, \"could not convert result type\");\n+    }\n+\n+    rewriter.replaceOpWithNewOp<ElementalOp>(\n+        op, new_result_types, adaptor.getOperands(), op->getAttrs());\n+    return mlir::success();\n+  }\n+};\n+\n+// We need to specify the ConstantOp conversion explicitly as it doesn't follow\n+// the simple operands & results of the other Arith ops.\n+template <>\n+class ElementalOpConversion<mlir::arith::ConstantOp>\n+    : public mlir::OpConversionPattern<mlir::arith::ConstantOp> {\n+ public:\n+  using mlir::OpConversionPattern<mlir::arith::ConstantOp>::OpConversionPattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      mlir::arith::ConstantOp op,\n+      typename mlir::arith::ConstantOp::Adaptor adaptor,\n+      mlir::ConversionPatternRewriter& rewriter) const override {\n+    mlir::Type new_type = getTypeConverter()->convertType(op.getType());\n+    mlir::ShapedType shaped_type = mlir::dyn_cast<mlir::ShapedType>(new_type);\n+    if (!shaped_type) {\n+      return rewriter.notifyMatchFailure(op, \"could not convert result type\");\n+    }\n+\n+    auto dense_attr = mlir::dyn_cast<mlir::DenseElementsAttr>(op.getValue());\n+    rewriter.replaceOpWithNewOp<mlir::arith::ConstantOp>(\n+        op, new_type, dense_attr.reshape(shaped_type));\n+    return mlir::success();\n+  }\n+};\n+\n+template <typename... ElementalOps>\n+void AddAElementalOpConversionsImpl(\n+    mlir::ConversionTarget& target, mlir::RewritePatternSet& patterns,\n+    TensorToVectorTypeConverter& typeConverter) {\n+  target.addDynamicallyLegalOp<ElementalOps...>(\n+      [&](mlir::Operation* op) { return typeConverter.isLegal(op); });\n+  patterns.add<ElementalOpConversion<ElementalOps>...>(typeConverter,\n+                                                       patterns.getContext());\n+}\n+\n+void AddArithOpConversions(mlir::ConversionTarget& target,\n+                           mlir::RewritePatternSet& patterns,\n+                           TensorToVectorTypeConverter& typeConverter) {\n+  AddAElementalOpConversionsImpl<\n+#define GET_OP_LIST\n+#include \"mlir/Dialect/Arith/IR/ArithOps.cpp.inc\"\n+#undef GET_OP_LIST\n+      >(target, patterns, typeConverter);\n+}\n+\n+void AddMathOpConversions(mlir::ConversionTarget& target,\n+                          mlir::RewritePatternSet& patterns,\n+                          TensorToVectorTypeConverter& typeConverter) {\n+  AddAElementalOpConversionsImpl<\n+#define GET_OP_LIST\n+#include \"mlir/Dialect/Math/IR/MathOps.cpp.inc\"\n+#undef GET_OP_LIST\n+      >(target, patterns, typeConverter);\n+}\n+\n+struct ElementalTensorToVectorPass\n+    : public impl::ElementalTensorToVectorPassBase<\n+          ElementalTensorToVectorPass> {\n+  void runOnOperation() override {\n+    auto* context = &getContext();\n+    mlir::ModuleOp module = getOperation();\n+\n+    mlir::ConversionTarget target(*context);\n+    mlir::RewritePatternSet patterns(context);\n+    TensorToVectorTypeConverter typeConverter;\n+    AddArithOpConversions(target, patterns, typeConverter);\n+    AddMathOpConversions(target, patterns, typeConverter);\n+\n+    mlir::ConversionConfig config;\n+    config.buildMaterializations = false;\n+    if (failed(applyPartialConversion(module, target, std::move(patterns),\n+                                      config))) {\n+      signalPassFailure();\n+    }\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<mlir::Pass> CreateElementalTensorToVectorPass() {\n+  return std::make_unique<ElementalTensorToVectorPass>();\n+}\n+\n+}  // namespace xla::cpu"
        },
        {
            "sha": "43d19ae0444eb93927d3f0a9b3617bf630610cd7",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/lower_xtile_entry.cc",
            "status": "added",
            "additions": 225,
            "deletions": 0,
            "changes": 225,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flower_xtile_entry.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flower_xtile_entry.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Flower_xtile_entry.cc?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -0,0 +1,225 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cassert>\n+#include <cstdint>\n+#include <memory>\n+#include <string>\n+#include <utility>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/ADT/StringRef.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMAttrs.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"  // IWYU pragma: keep\n+#include \"mlir/Dialect/SCF/IR/SCF.h\"\n+#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n+#include \"mlir/IR/Attributes.h\"\n+#include \"mlir/IR/Block.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/TypeRange.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/Visitors.h\"\n+#include \"mlir/Interfaces/DataLayoutInterfaces.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Support/WalkResult.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"xla/backends/cpu/codegen/emitters/ir/xla_cpu_ops.h\"\n+#include \"xla/backends/cpu/codegen/emitters/ir/xla_cpu_types.h\"\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h\"\n+#include \"xla/codegen/emitters/ir/xla_ops.h\"\n+#include \"xla/codegen/xtile/ir/xtile_attrs.h\"\n+#include \"xla/codegen/xtile/ir/xtile_dialect.h\"  // IWYU pragma: keep\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n+\n+namespace xla::cpu {\n+\n+#define GEN_PASS_DECL_LOWERXTILEENTRYPASS\n+#define GEN_PASS_DEF_LOWERXTILEENTRYPASS\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h.inc\"\n+\n+namespace {\n+\n+struct LowerXtileEntry : mlir::OpRewritePattern<xtile::EntryFuncOp> {\n+  using OpRewritePattern::OpRewritePattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      xtile::EntryFuncOp op, mlir::PatternRewriter& rewriter) const override {\n+    llvm::SmallVector<mlir::NamedAttribute> filtered_attrs;\n+    for (const auto& attr : op->getAttrs()) {\n+      if (!absl::c_linear_search(mlir::func::FuncOp::getAttributeNames(),\n+                                 attr.getName())) {\n+        filtered_attrs.push_back(attr);\n+      }\n+    }\n+\n+    auto new_func_op = rewriter.create<mlir::func::FuncOp>(\n+        op->getLoc(), op.getSymName(), op.getFunctionType(), filtered_attrs);\n+    new_func_op.setArgAttrsAttr(op.getArgAttrsAttr());\n+\n+    // Move the region from the old function to the new one.\n+    rewriter.inlineRegionBefore(op.getBody(), new_func_op.getBody(),\n+                                new_func_op.getBody().end());\n+\n+    // Replace the original operation. Since a function definition does not\n+    // produce any results, we replace it with an empty list of values.\n+    rewriter.replaceOp(op, new_func_op);\n+\n+    return mlir::success();\n+  }\n+};\n+\n+struct LowerXTileEntryReturn\n+    : mlir::OpRewritePattern<xtile::EntryFuncReturnOp> {\n+  using OpRewritePattern::OpRewritePattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      xtile::EntryFuncReturnOp op,\n+      mlir::PatternRewriter& rewriter) const override {\n+    rewriter.replaceOp(op, rewriter.create<mlir::func::ReturnOp>(op->getLoc()));\n+    return mlir::success();\n+  }\n+};\n+\n+class LowerXTileEntryPass\n+    : public impl::LowerXTileEntryPassBase<LowerXTileEntryPass> {\n+ public:\n+  using LowerXTileEntryPassBase::LowerXTileEntryPassBase;\n+\n+  void runOnOperation() override {\n+    mlir::ModuleOp module = getOperation();\n+    mlir::MLIRContext* context = &getContext();\n+    mlir::RewritePatternSet patterns(context);\n+    if (WrapInCallFrame(module).failed()) {\n+      signalPassFailure();\n+      return;\n+    }\n+\n+    patterns.add<LowerXtileEntry, LowerXTileEntryReturn>(context);\n+    if (mlir::failed(\n+            mlir::applyPatternsGreedily(module, std::move(patterns)))) {\n+      signalPassFailure();\n+      return;\n+    }\n+  }\n+\n+ private:\n+  // Wrap the entry function in another func that abides by the XLA:CPU ABI.\n+  mlir::LogicalResult WrapInCallFrame(mlir::ModuleOp module) {\n+    mlir::MLIRContext* context = module.getContext();\n+    mlir::ImplicitLocOpBuilder builder(module->getLoc(), module);\n+\n+    for (auto entry_func : module.getOps<xtile::EntryFuncOp>()) {\n+      if (!entry_func.symbolKnownUseEmpty(module)) {\n+        module->emitError() << \"entry function is itself called.\";\n+        return mlir::failure();\n+      }\n+\n+      llvm::StringRef kernel_name = entry_func.getName();\n+      std::string kernel_impl_name =\n+          absl::StrCat(absl::AlphaNum(entry_func.getName()), \"_impl\");\n+      entry_func.setName(kernel_impl_name);\n+      entry_func.setPrivate();\n+      entry_func->setAttr(\n+          \"llvm.linkage\",\n+          mlir::LLVM::LinkageAttr::get(context, mlir::LLVM::Linkage::Internal));\n+      entry_func->setAttr(\"always_inline\", builder.getUnitAttr());\n+\n+      auto call_frame_type = CallFrameType::get(context);\n+      auto error_type = ErrorType::get(context);\n+      builder.setInsertionPointToStart(module.getBody());\n+      mlir::func::FuncOp kernel_func = builder.create<mlir::func::FuncOp>(\n+          kernel_name,\n+          builder.getFunctionType({call_frame_type}, {error_type}));\n+\n+      builder.setInsertionPointToStart(kernel_func.addEntryBlock());\n+\n+      auto call_frame = mlir::cast<mlir::TypedValue<CallFrameType>>(\n+          kernel_func.getArgument(0));\n+      llvm::SmallVector<mlir::Value> call_args;\n+      for (const auto& [idx, arg] :\n+           llvm::enumerate(entry_func.getBufferArgs())) {\n+        LoadOp load = builder.create<LoadOp>(arg.getType(), call_frame, idx);\n+        call_args.push_back(load);\n+      }\n+\n+      auto tile_info = entry_func->getAttrOfType<xla::xtile::TilingInfoAttr>(\n+          \"xtile.tiling_info\");\n+\n+      if (!tile_info) {\n+        entry_func->emitError() << \"missing tiling info.\";\n+        return mlir::failure();\n+      }\n+      int32_t tile_count = tile_info.getTileCount();\n+      int32_t tiles_per_workgroup = tile_info.getTilesPerWorkgroup();\n+\n+      mlir::Value tile_count_value =\n+          builder.create<mlir::arith::ConstantIndexOp>(tile_count);\n+      mlir::Value tiles_per_workgroup_value =\n+          builder.create<mlir::arith::ConstantIndexOp>(tiles_per_workgroup);\n+      mlir::Value workgroup_id = builder.create<ExtractWorkgroupIdOp>(\n+          builder.getIndexType(), call_frame, WorkGroupDimension::x);\n+\n+      mlir::Value start_tile_id = builder.create<mlir::arith::MulIOp>(\n+          builder.getIndexType(), workgroup_id, tiles_per_workgroup_value);\n+      mlir::Value bounded_start_tile_id = builder.create<mlir::arith::MinSIOp>(\n+          builder.getIndexType(), start_tile_id, tile_count_value);\n+\n+      mlir::Value end_tile_id = builder.create<mlir::arith::AddIOp>(\n+          builder.getIndexType(), start_tile_id, tiles_per_workgroup_value);\n+      mlir::Value bounded_end_tile_id = builder.create<mlir::arith::MinSIOp>(\n+          builder.getIndexType(), end_tile_id, tile_count_value);\n+\n+      mlir::Value step = builder.create<mlir::arith::ConstantIndexOp>(1);\n+\n+      auto for_op = builder.create<mlir::scf::ForOp>(bounded_start_tile_id,\n+                                                     bounded_end_tile_id, step);\n+      {\n+        mlir::ImplicitLocOpBuilder body_builder(entry_func->getLoc(),\n+                                                entry_func);\n+        body_builder.setInsertionPointToStart(for_op.getBody());\n+\n+        call_args.push_back(for_op.getInductionVar());\n+\n+        body_builder.create<mlir::func::CallOp>(kernel_impl_name,\n+                                                mlir::TypeRange(), call_args);\n+      }\n+\n+      auto error = builder.create<cpu::SuccessOp>(error_type);\n+      builder.create<mlir::func::ReturnOp>(error.getResult());\n+    }\n+\n+    return mlir::success();\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<mlir::Pass> CreateLowerXTileEntryPass() {\n+  return std::make_unique<LowerXTileEntryPass>();\n+}\n+\n+}  // namespace xla::cpu"
        },
        {
            "sha": "e851effe4200a17b9e13a8982a17b281ab377d75",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/passes.h",
            "status": "added",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.h?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -0,0 +1,46 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_CPU_CODEGEN_TILED_TRANSFORMS_PASSES_H_\n+#define XLA_BACKENDS_CPU_CODEGEN_TILED_TRANSFORMS_PASSES_H_\n+\n+#include <memory>\n+\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"  // IWYU pragma: keep\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"  // IWYU pragma: keep\n+#include \"mlir/Dialect/Math/IR/Math.h\"  // IWYU pragma: keep\n+#include \"mlir/Dialect/SCF/IR/SCF.h\"  // IWYU pragma: keep\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"  // IWYU pragma: keep\n+#include \"mlir/Dialect/Vector/IR/VectorOps.h\"  // IWYU pragma: keep\n+#include \"mlir/Pass/Pass.h\"\n+#include \"xla/backends/cpu/codegen/emitters/ir/xla_cpu_dialect.h\"  // IWYU pragma: keep\n+#include \"xla/codegen/xtile/ir/xtile_dialect.h\"  // IWYU pragma: keep\n+\n+namespace xla::cpu {\n+\n+#define GEN_PASS_DECL\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h.inc\"\n+\n+std::unique_ptr<mlir::Pass> CreateElementalTensorToVectorPass();\n+std::unique_ptr<mlir::Pass> CreateLowerXTileEntryPass();\n+std::unique_ptr<mlir::Pass> CreateShloToVectorPass();\n+std::unique_ptr<mlir::Pass> CreateXTileToVectorPass();\n+\n+#define GEN_PASS_REGISTRATION\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h.inc\"\n+\n+}  // namespace xla::cpu\n+\n+#endif  // XLA_BACKENDS_CPU_CODEGEN_TILED_TRANSFORMS_PASSES_H_"
        },
        {
            "sha": "77d68083a3c0315030d30b037383e011493fc701",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/passes.td",
            "status": "added",
            "additions": 67,
            "deletions": 0,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -0,0 +1,67 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+include \"mlir/Pass/PassBase.td\"\n+\n+def XTileToVectorPass : Pass<\"xtile-cpu-xtile-to-vector\", \"mlir::ModuleOp\"> {\n+  let summary = \"Lowering xtile ops to vector ops\";\n+\n+  let constructor = \"CreateXTileToVectorPass()\";\n+\n+  let dependentDialects = [\n+    \"mlir::vector::VectorDialect\",\n+    \"xla::xtile::XTileDialect\",\n+  ];\n+}\n+\n+def LowerXTileEntryPass : Pass<\"xtile-cpu-lower-xtile-entry\", \"mlir::ModuleOp\"> {\n+  let summary = \"Lowers the entry function into the form required by the CPU runtime\";\n+\n+  let constructor = \"CreateLowerXTileEntryPass()\";\n+\n+  let dependentDialects = [\n+    \"mlir::func::FuncDialect\",\n+    \"mlir::LLVM::LLVMDialect\",\n+    \"mlir::scf::SCFDialect\",\n+    \"xla::cpu::XlaCpuDialect\",\n+    \"xla::xtile::XTileDialect\"\n+  ];\n+}\n+\n+def ShloToVectorPass : Pass<\"xtile-cpu-shlo-to-vector\", \"mlir::ModuleOp\"> {\n+  let summary = \"Lowering satble hlo ops to vector ops\";\n+\n+  let constructor = \"CreateShloToVectorPass()\";\n+\n+  let dependentDialects = [\n+    \"mlir::tensor::TensorDialect\",\n+    \"mlir::vector::VectorDialect\",\n+    \"mlir::stablehlo::StablehloDialect\",\n+  ];\n+}\n+\n+def ElementalTensorToVectorPass : Pass<\"xtile-cpu-elemental-tensor-to-vector\",\n+                                       \"mlir::ModuleOp\"> {\n+  let summary = \"Lowering arith & math ops with tensor types to vector types\";\n+\n+  let constructor = \"CreateElementalTensorToVectorPass()\";\n+\n+  let dependentDialects = [\n+    \"mlir::arith::ArithDialect\",\n+    \"mlir::math::MathDialect\",\n+    \"mlir::tensor::TensorDialect\",\n+    \"mlir::vector::VectorDialect\",\n+  ];\n+}"
        },
        {
            "sha": "086d5b42c103c42f65a85d834f9ef6a9ae8fc6d5",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/shlo_to_vector.cc",
            "status": "added",
            "additions": 97,
            "deletions": 0,
            "changes": 97,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fshlo_to_vector.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fshlo_to_vector.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fshlo_to_vector.cc?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -0,0 +1,97 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cassert>\n+#include <memory>\n+#include <utility>\n+\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"  // IWYU pragma: keep\n+#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n+#include \"mlir/IR/AffineExpr.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/Visitors.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"stablehlo/dialect/StablehloOps.h\"\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h\"\n+\n+namespace xla::cpu {\n+\n+#define GEN_PASS_DECL_SHLOTOVECTORPASS\n+#define GEN_PASS_DEF_SHLOTOVECTORPASS\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h.inc\"\n+\n+namespace {\n+\n+struct LowerTranspose : mlir::OpRewritePattern<mlir::stablehlo::TransposeOp> {\n+  using OpRewritePattern::OpRewritePattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      mlir::stablehlo::TransposeOp op,\n+      mlir::PatternRewriter& rewriter) const override {\n+    mlir::RankedTensorType source_tensor_type = op.getOperand().getType();\n+    auto source_vector_type = mlir::VectorType::get(\n+        source_tensor_type.getShape(), source_tensor_type.getElementType());\n+    mlir::Value source_vector =\n+        rewriter\n+            .create<mlir::UnrealizedConversionCastOp>(\n+                op->getLoc(), source_vector_type, op.getOperand())\n+            .getResult(0);\n+\n+    mlir::Value dest_vector = rewriter.create<mlir::vector::TransposeOp>(\n+        op->getLoc(), source_vector, op.getPermutation());\n+\n+    mlir::RankedTensorType dest_tensor_type = op.getResult().getType();\n+    mlir::Value dest_tensor =\n+        rewriter\n+            .create<mlir::UnrealizedConversionCastOp>(\n+                op->getLoc(), dest_tensor_type, dest_vector)\n+            .getResult(0);\n+\n+    rewriter.replaceAllUsesWith(op, dest_tensor);\n+    return mlir::success();\n+  }\n+};\n+\n+class ShloToVectorPass : public impl::ShloToVectorPassBase<ShloToVectorPass> {\n+ public:\n+  using ShloToVectorPassBase::ShloToVectorPassBase;\n+\n+  void runOnOperation() override {\n+    mlir::MLIRContext* context = &getContext();\n+    mlir::RewritePatternSet patterns(context);\n+    patterns.add<LowerTranspose>(context);\n+    if (mlir::failed(\n+            mlir::applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n+      signalPassFailure();\n+      return;\n+    }\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<mlir::Pass> CreateShloToVectorPass() {\n+  return std::make_unique<ShloToVectorPass>();\n+}\n+\n+}  // namespace xla::cpu"
        },
        {
            "sha": "10228fcc460af821c5685541d937e47bb011a65e",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/BUILD",
            "status": "added",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2FBUILD?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -0,0 +1,16 @@\n+load(\"//xla:lit.bzl\", \"lit_test_suite\")\n+\n+package(\n+    # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n+    licenses = [\"notice\"],\n+)\n+\n+lit_test_suite(\n+    name = \"tests\",\n+    srcs = glob([\"*.mlir\"]),\n+    cfg = \"//xla:lit.cfg.py\",\n+    tools = [\n+        \"//xla/codegen/tools:emitters_opt\",\n+        \"@llvm-project//llvm:FileCheck\",\n+    ],\n+)"
        },
        {
            "sha": "9378fc77c0dff5c06e6de6ac7d69db302bd0bda0",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/arith_to_vector.mlir",
            "status": "added",
            "additions": 383,
            "deletions": 0,
            "changes": 383,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Farith_to_vector.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Farith_to_vector.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Farith_to_vector.mlir?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -0,0 +1,383 @@\n+// RUN: emitters_opt %s --xtile-cpu-elemental-tensor-to-vector -split-input-file | FileCheck %s\n+\n+func.func @addf(%lhs : tensor<1024xf32>, %rhs : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: arith.addf %{{.*}}, %{{.*}} : vector<1024xf32>\n+  %add = arith.addf %lhs, %rhs : tensor<1024xf32>\n+  return %add : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @addi(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.addi %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %add = arith.addi %lhs, %rhs : tensor<1024xi32>\n+  return %add : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @addiu(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> (tensor<1024xi32>, tensor<1024xi1>) {\n+  // CHECK: arith.addui_extended %{{.*}}, %{{.*}} : vector<1024xi32>, vector<1024xi1>\n+  %add, %carry = arith.addui_extended %lhs, %rhs : tensor<1024xi32>, tensor<1024xi1>\n+  return %add, %carry : tensor<1024xi32>, tensor<1024xi1>\n+}\n+\n+// -----\n+\n+func.func @andi(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.andi %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %and = arith.andi %lhs, %rhs : tensor<1024xi32>\n+  return %and : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @bitcast(%arg0 : tensor<1024xi32>) -> tensor<1024xf32> {\n+  // CHECK: arith.bitcast %{{.*}} : vector<1024xi32> to vector<1024xf32>\n+  %cast = arith.bitcast %arg0 : tensor<1024xi32> to tensor<1024xf32>\n+  return %cast : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @ceildivsi(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.ceildivsi %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %div = arith.ceildivsi %lhs, %rhs : tensor<1024xi32>\n+  return %div : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @ceildivui(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.ceildivui %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %div = arith.ceildivui %lhs, %rhs : tensor<1024xi32>\n+  return %div : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @cmpf(%lhs : tensor<1024xf32>, %rhs : tensor<1024xf32>) -> tensor<1024xi1> {\n+  // CHECK: arith.cmpf oeq, %{{.*}}, %{{.*}} : vector<1024xf32>\n+  %cmp = arith.cmpf oeq, %lhs, %rhs : tensor<1024xf32>\n+  return %cmp : tensor<1024xi1>\n+}\n+\n+// -----\n+\n+func.func @cmpi(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi1> {\n+  // CHECK: arith.cmpi eq, %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %cmp = arith.cmpi eq, %lhs, %rhs : tensor<1024xi32>\n+  return %cmp : tensor<1024xi1>\n+}\n+\n+// -----\n+\n+func.func @constant() -> tensor<1024xf32> {\n+  // CHECK: arith.constant dense<1.000000e+00> : vector<1024xf32>\n+  %const = arith.constant dense<1.0> : tensor<1024xf32>\n+  return %const : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @divf(%lhs : tensor<1024xf32>, %rhs : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: arith.divf %{{.*}}, %{{.*}} : vector<1024xf32>\n+  %div = arith.divf %lhs, %rhs : tensor<1024xf32>\n+  return %div : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @divsi(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.divsi %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %div = arith.divsi %lhs, %rhs : tensor<1024xi32>\n+  return %div : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @divui(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.divui %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %div = arith.divui %lhs, %rhs : tensor<1024xi32>\n+  return %div : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @extf(%arg0 : tensor<1024xf32>) -> tensor<1024xf64> {\n+  // CHECK: arith.extf %{{.*}} : vector<1024xf32> to vector<1024xf64>\n+  %ext = arith.extf %arg0 : tensor<1024xf32> to tensor<1024xf64>\n+  return %ext : tensor<1024xf64>\n+}\n+\n+// -----\n+\n+func.func @extsi(%arg0 : tensor<1024xi16>) -> tensor<1024xi32> {\n+  // CHECK: arith.extsi %{{.*}} : vector<1024xi16> to vector<1024xi32>\n+  %ext = arith.extsi %arg0 : tensor<1024xi16> to tensor<1024xi32>\n+  return %ext : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @extui(%arg0 : tensor<1024xi16>) -> tensor<1024xi32> {\n+  // CHECK: arith.extui %{{.*}} : vector<1024xi16> to vector<1024xi32>\n+  %ext = arith.extui %arg0 : tensor<1024xi16> to tensor<1024xi32>\n+  return %ext : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @fptosi(%arg0 : tensor<1024xf32>) -> tensor<1024xi32> {\n+  // CHECK: arith.fptosi %{{.*}} : vector<1024xf32> to vector<1024xi32>\n+  %cast = arith.fptosi %arg0 : tensor<1024xf32> to tensor<1024xi32>\n+  return %cast : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @fptoui(%arg0 : tensor<1024xf32>) -> tensor<1024xi32> {\n+  // CHECK: arith.fptoui %{{.*}} : vector<1024xf32> to vector<1024xi32>\n+  %cast = arith.fptoui %arg0 : tensor<1024xf32> to tensor<1024xi32>\n+  return %cast : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @floordivsi(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.floordivsi %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %div = arith.floordivsi %lhs, %rhs : tensor<1024xi32>\n+  return %div : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @index_cast(%arg0 : tensor<1024xi32>) -> tensor<1024xindex> {\n+  // CHECK: arith.index_cast %{{.*}} : vector<1024xi32> to vector<1024xindex>\n+  %cast = arith.index_cast %arg0 : tensor<1024xi32> to tensor<1024xindex>\n+  return %cast : tensor<1024xindex>\n+}\n+\n+// -----\n+\n+func.func @index_castui(%arg0 : tensor<1024xi32>) -> tensor<1024xindex> {\n+  // CHECK: arith.index_castui %{{.*}} : vector<1024xi32> to vector<1024xindex>\n+  %cast = arith.index_castui %arg0 : tensor<1024xi32> to tensor<1024xindex>\n+  return %cast : tensor<1024xindex>\n+}\n+\n+// -----\n+\n+func.func @maxnumf(%lhs : tensor<1024xf32>, %rhs : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: arith.maxnumf %{{.*}}, %{{.*}} : vector<1024xf32>\n+  %max = arith.maxnumf %lhs, %rhs : tensor<1024xf32>\n+  return %max : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @maxsi(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.maxsi %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %max = arith.maxsi %lhs, %rhs : tensor<1024xi32>\n+  return %max : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @maxui(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.maxui %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %max = arith.maxui %lhs, %rhs : tensor<1024xi32>\n+  return %max : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @maximumf(%lhs : tensor<1024xf32>, %rhs : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: arith.maximumf %{{.*}}, %{{.*}} : vector<1024xf32>\n+  %max = arith.maximumf %lhs, %rhs : tensor<1024xf32>\n+  return %max : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @minnumf(%lhs : tensor<1024xf32>, %rhs : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: arith.minnumf %{{.*}}, %{{.*}} : vector<1024xf32>\n+  %min = arith.minnumf %lhs, %rhs : tensor<1024xf32>\n+  return %min : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @minsi(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.minsi %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %min = arith.minsi %lhs, %rhs : tensor<1024xi32>\n+  return %min : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @minui(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.minui %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %min = arith.minui %lhs, %rhs : tensor<1024xi32>\n+  return %min : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @minimumf(%lhs : tensor<1024xf32>, %rhs : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: arith.minimumf %{{.*}}, %{{.*}} : vector<1024xf32>\n+  %min = arith.minimumf %lhs, %rhs : tensor<1024xf32>\n+  return %min : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @mulf(%lhs : tensor<1024xf32>, %rhs : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: arith.mulf %{{.*}}, %{{.*}} : vector<1024xf32>\n+  %mul = arith.mulf %lhs, %rhs : tensor<1024xf32>\n+  return %mul : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @muli(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.muli %{{.*}}, %{{.*}} overflow<nsw, nuw> : vector<1024xi32>\n+  %mul = arith.muli %lhs, %rhs overflow<nsw, nuw> : tensor<1024xi32>\n+  return %mul : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @mului_ext(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> (tensor<1024xi32>, tensor<1024xi32>) {\n+  // CHECK: arith.mulsi_extended %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %low, %high = arith.mulsi_extended %lhs, %rhs : tensor<1024xi32>\n+  return %low, %high : tensor<1024xi32>, tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @negf(%arg0 : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: arith.negf %{{.*}} : vector<1024xf32>\n+  %neg = arith.negf %arg0 : tensor<1024xf32>\n+  return %neg : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @ori(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.ori %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %or = arith.ori %lhs, %rhs : tensor<1024xi32>\n+  return %or : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @remf(%lhs : tensor<1024xf32>, %rhs : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: arith.remf %{{.*}}, %{{.*}} : vector<1024xf32>\n+  %rem = arith.remf %lhs, %rhs : tensor<1024xf32>\n+  return %rem : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @remsi(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.remsi %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %rem = arith.remsi %lhs, %rhs : tensor<1024xi32>\n+  return %rem : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @remui(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.remui %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %rem = arith.remui %lhs, %rhs : tensor<1024xi32>\n+  return %rem : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @sitofp(%arg0 : tensor<1024xi32>) -> tensor<1024xf32> {\n+  // CHECK: arith.sitofp %{{.*}} : vector<1024xi32> to vector<1024xf32>\n+  %cast = arith.sitofp %arg0 : tensor<1024xi32> to tensor<1024xf32>\n+  return %cast : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @select(%cond : tensor<1024xi1>, %true_val : tensor<1024xf32>, %false_val : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: arith.select %{{.*}}, %{{.*}}, %{{.*}} : vector<1024xi1>, vector<1024xf32>\n+  %sel = arith.select %cond, %true_val, %false_val : tensor<1024xi1>, tensor<1024xf32>\n+  return %sel : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @shli(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.shli %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %shl = arith.shli %lhs, %rhs : tensor<1024xi32>\n+  return %shl : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @shrsi(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.shrsi %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %shr = arith.shrsi %lhs, %rhs : tensor<1024xi32>\n+  return %shr : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @shrui(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.shrui %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %shr = arith.shrui %lhs, %rhs : tensor<1024xi32>\n+  return %shr : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @subf(%lhs : tensor<1024xf32>, %rhs : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: arith.subf %{{.*}}, %{{.*}} : vector<1024xf32>\n+  %sub = arith.subf %lhs, %rhs : tensor<1024xf32>\n+  return %sub : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @subi(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.subi %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %sub = arith.subi %lhs, %rhs : tensor<1024xi32>\n+  return %sub : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @truncf(%arg0 : tensor<1024xf64>) -> tensor<1024xf32> {\n+  // CHECK: arith.truncf %{{.*}} : vector<1024xf64> to vector<1024xf32>\n+  %trunc = arith.truncf %arg0 : tensor<1024xf64> to tensor<1024xf32>\n+  return %trunc : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @trunci(%arg0 : tensor<1024xi32>) -> tensor<1024xi16> {\n+  // CHECK: arith.trunci %{{.*}} : vector<1024xi32> to vector<1024xi16>\n+  %trunc = arith.trunci %arg0 : tensor<1024xi32> to tensor<1024xi16>\n+  return %trunc : tensor<1024xi16>\n+}\n+\n+// -----\n+\n+func.func @uitofp(%arg0 : tensor<1024xi32>) -> tensor<1024xf32> {\n+  // CHECK: arith.uitofp %{{.*}} : vector<1024xi32> to vector<1024xf32>\n+  %cast = arith.uitofp %arg0 : tensor<1024xi32> to tensor<1024xf32>\n+  return %cast : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @xori(%lhs : tensor<1024xi32>, %rhs : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: arith.xori %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %xor = arith.xori %lhs, %rhs : tensor<1024xi32>\n+  return %xor : tensor<1024xi32>\n+}\n\\ No newline at end of file"
        },
        {
            "sha": "716fc5676637d867fe26bfabde84282eb0a5e4d4",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/lower_xtile_entry.mlir",
            "status": "added",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Flower_xtile_entry.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Flower_xtile_entry.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Flower_xtile_entry.mlir?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -0,0 +1,38 @@\n+// RUN: emitters_opt %s --xtile-cpu-lower-xtile-entry -split-input-file | FileCheck %s\n+\n+xtile.entry_func @simple_wrap(%input: memref<1024xf32> {xla.some_attr = 1},\n+                             %output: memref<32xf64>,\n+                             %tile_id: index) attributes {xtile.tiling_info = #xtile.tiling_info<tile_count:1012, tiles_per_workgroup:64>} {\n+  xtile.return\n+}\n+\n+// CHECK: func.func @simple_wrap(%[[CALL_FRAME:.*]]: !xla_cpu.call_frame) -> !xla_cpu.error {\n+\n+// CHECK-DAG: %[[STEP:.*]] = arith.constant 1 : index\n+// CHECK-DAG: %[[TILES_PER_WORKGROUP:.*]] = arith.constant 64 : index\n+// CHECK-DAG: %[[TILE_COUNT:.*]] = arith.constant 1012 : index\n+\n+// CHECK: %[[INPUT:.*]] = xla_cpu.load %[[CALL_FRAME]], 0 : memref<1024xf32>\n+// CHECK: %[[OUTPUT:.*]] = xla_cpu.load %[[CALL_FRAME]], 1 : memref<32xf64>\n+// CHECK: %[[WORKGROUP_ID:.*]] = xla_cpu.extract_workgroup_id %[[CALL_FRAME]], x\n+\n+// CHECK: %[[START_IDX:.*]] = arith.muli %[[WORKGROUP_ID]], %[[TILES_PER_WORKGROUP]] : index\n+// CHECK: %[[CLAMPED_START_IDX:.*]] = arith.minsi %[[START_IDX]], %[[TILE_COUNT]] : index\n+// CHECK: %[[END_IDX:.*]] = arith.addi %[[START_IDX]], %[[TILES_PER_WORKGROUP]] : index\n+// CHECK: %[[CLAMPED_END_IDX:.*]] = arith.minsi %[[END_IDX]], %[[TILE_COUNT]] : index\n+// CHECK: scf.for %[[IDX:.*]] = %[[CLAMPED_START_IDX]] to %[[CLAMPED_END_IDX]] step %[[STEP]] {\n+// CHECK:   func.call @[[IMPL_FUNC:.*]](%[[INPUT]], %[[OUTPUT]], %[[IDX]]) : (memref<1024xf32>, memref<32xf64>, index) -> ()\n+// CHECK: }\n+\n+// CHECK: %[[SUCCESS:.*]] = xla_cpu.success : !xla_cpu.error\n+// CHECK: return %[[SUCCESS]] : !xla_cpu.error\n+\n+// CHECK: func.func @[[IMPL_FUNC]](\n+// CHECK-SAME: %{{.*}}: memref<1024xf32> {xla.some_attr = 1 : i64},\n+// CHECK-SAME: %{{.*}}: memref<32xf64>,\n+// CHECK-SAME: %{{.*}}: index)\n+// CHECK-SAME: attributes {always_inline, llvm.linkage = #llvm.linkage<internal>\n+// CHECK: return\n+\n+\n+// -----"
        },
        {
            "sha": "c49d37474d7d847ea76fa4b4f823f07be4f8710f",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/math_to_vector.mlir",
            "status": "added",
            "additions": 367,
            "deletions": 0,
            "changes": 367,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fmath_to_vector.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fmath_to_vector.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fmath_to_vector.mlir?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -0,0 +1,367 @@\n+// RUN: emitters_opt %s --xtile-cpu-elemental-tensor-to-vector -split-input-file | FileCheck %s\n+\n+func.func @absf(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.absf %{{.*}} : vector<1024xf32>\n+  %abs = math.absf %input : tensor<1024xf32>\n+  return %abs : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @absi(%input : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: math.absi %{{.*}} : vector<1024xi32>\n+  %abs = math.absi %input : tensor<1024xi32>\n+  return %abs : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @acos(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.acos %{{.*}} : vector<1024xf32>\n+  %res = math.acos %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @acosh(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.acosh %{{.*}} : vector<1024xf32>\n+  %res = math.acosh %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @asin(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.asin %{{.*}} : vector<1024xf32>\n+  %res = math.asin %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @asinh(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.asinh %{{.*}} : vector<1024xf32>\n+  %res = math.asinh %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @atan2(%input1 : tensor<1024xf32>, %input2 : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.atan2 %{{.*}}, %{{.*}} : vector<1024xf32>\n+  %res = math.atan2 %input1, %input2 : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @atan(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.atan %{{.*}} : vector<1024xf32>\n+  %res = math.atan %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @atanh(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.atanh %{{.*}} : vector<1024xf32>\n+  %res = math.atanh %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @cbrt(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.cbrt %{{.*}} : vector<1024xf32>\n+  %res = math.cbrt %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @ceil(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.ceil %{{.*}} : vector<1024xf32>\n+  %res = math.ceil %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @clampf(%input : tensor<1024xf32>, %low : tensor<1024xf32>, %high : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.clampf %{{.*}} to [%{{.*}}, %{{.*}}] : vector<1024xf32>\n+  %res = math.clampf %input to [%low, %high] : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @copysign(%mag : tensor<1024xf32>, %sign : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.copysign %{{.*}}, %{{.*}} : vector<1024xf32>\n+  %res = math.copysign %mag, %sign : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @cos(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.cos %{{.*}} : vector<1024xf32>\n+  %res = math.cos %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @cosh(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.cosh %{{.*}} : vector<1024xf32>\n+  %res = math.cosh %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @ctlz(%input : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: math.ctlz %{{.*}} : vector<1024xi32>\n+  %res = math.ctlz %input : tensor<1024xi32>\n+  return %res : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @cttz(%input : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: math.cttz %{{.*}} : vector<1024xi32>\n+  %res = math.cttz %input : tensor<1024xi32>\n+  return %res : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @ctpop(%input : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: math.ctpop %{{.*}} : vector<1024xi32>\n+  %res = math.ctpop %input : tensor<1024xi32>\n+  return %res : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @erf(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.erf %{{.*}} : vector<1024xf32>\n+  %res = math.erf %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @erfc(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.erfc %{{.*}} : vector<1024xf32>\n+  %res = math.erfc %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @exp2(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.exp2 %{{.*}} : vector<1024xf32>\n+  %res = math.exp2 %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @expm1(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.expm1 %{{.*}} : vector<1024xf32>\n+  %res = math.expm1 %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @exp(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.exp %{{.*}} : vector<1024xf32>\n+  %res = math.exp %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @fpowi(%base : tensor<1024xf32>, %exp : tensor<1024xi32>) -> tensor<1024xf32> {\n+  // CHECK: math.fpowi %{{.*}}, %{{.*}} : vector<1024xf32>\n+  %res = math.fpowi %base, %exp : tensor<1024xf32>, tensor<1024xi32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @floor(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.floor %{{.*}} : vector<1024xf32>\n+  %res = math.floor %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @fma(%a : tensor<1024xf32>, %b : tensor<1024xf32>, %c : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.fma %{{.*}}, %{{.*}}, %{{.*}} : vector<1024xf32>\n+  %res = math.fma %a, %b, %c : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @ipowi(%base : tensor<1024xi32>, %exp : tensor<1024xi32>) -> tensor<1024xi32> {\n+  // CHECK: math.ipowi %{{.*}}, %{{.*}} : vector<1024xi32>\n+  %res = math.ipowi %base, %exp : tensor<1024xi32>\n+  return %res : tensor<1024xi32>\n+}\n+\n+// -----\n+\n+func.func @isfinite(%input : tensor<1024xf32>) -> tensor<1024xi1> {\n+  // CHECK: math.isfinite %{{.*}} : vector<1024xf32>\n+  %res = math.isfinite %input : tensor<1024xf32>\n+  return %res : tensor<1024xi1>\n+}\n+\n+// -----\n+\n+func.func @isinf(%input : tensor<1024xf32>) -> tensor<1024xi1> {\n+  // CHECK: math.isinf %{{.*}} : vector<1024xf32>\n+  %res = math.isinf %input : tensor<1024xf32>\n+  return %res : tensor<1024xi1>\n+}\n+\n+// -----\n+\n+func.func @isnan(%input : tensor<1024xf32>) -> tensor<1024xi1> {\n+  // CHECK: math.isnan %{{.*}} : vector<1024xf32>\n+  %res = math.isnan %input : tensor<1024xf32>\n+  return %res : tensor<1024xi1>\n+}\n+\n+// -----\n+\n+func.func @isnormal(%input : tensor<1024xf32>) -> tensor<1024xi1> {\n+  // CHECK: math.isnormal %{{.*}} : vector<1024xf32>\n+  %res = math.isnormal %input : tensor<1024xf32>\n+  return %res : tensor<1024xi1>\n+}\n+\n+// -----\n+\n+func.func @log10(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.log10 %{{.*}} : vector<1024xf32>\n+  %res = math.log10 %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @log1p(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.log1p %{{.*}} : vector<1024xf32>\n+  %res = math.log1p %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @log2(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.log2 %{{.*}} : vector<1024xf32>\n+  %res = math.log2 %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @log(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.log %{{.*}} : vector<1024xf32>\n+  %res = math.log %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @powf(%base : tensor<1024xf32>, %exp : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.powf %{{.*}}, %{{.*}} : vector<1024xf32>\n+  %res = math.powf %base, %exp : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @roundeven(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.roundeven %{{.*}} : vector<1024xf32>\n+  %res = math.roundeven %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @round(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.round %{{.*}} : vector<1024xf32>\n+  %res = math.round %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @rsqrt(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.rsqrt %{{.*}} : vector<1024xf32>\n+  %res = math.rsqrt %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @sin(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.sin %{{.*}} : vector<1024xf32>\n+  %res = math.sin %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @sincos(%input : tensor<1024xf32>) -> (tensor<1024xf32>, tensor<1024xf32>) {\n+  // CHECK: math.sincos %{{.*}} : vector<1024xf32>\n+  %sin, %cos = math.sincos %input : tensor<1024xf32>\n+  return %sin, %cos : tensor<1024xf32>, tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @sinh(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.sinh %{{.*}} : vector<1024xf32>\n+  %res = math.sinh %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @sqrt(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.sqrt %{{.*}} : vector<1024xf32>\n+  %res = math.sqrt %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @tan(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.tan %{{.*}} : vector<1024xf32>\n+  %res = math.tan %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @tanh(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.tanh %{{.*}} : vector<1024xf32>\n+  %res = math.tanh %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n+\n+// -----\n+\n+func.func @trunc(%input : tensor<1024xf32>) -> tensor<1024xf32> {\n+  // CHECK: math.trunc %{{.*}} : vector<1024xf32>\n+  %res = math.trunc %input : tensor<1024xf32>\n+  return %res : tensor<1024xf32>\n+}\n\\ No newline at end of file"
        },
        {
            "sha": "440f8375918087d4eb01e757afc64de2f71cd943",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/shlo_to_vector.mlir",
            "status": "added",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fshlo_to_vector.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fshlo_to_vector.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fshlo_to_vector.mlir?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -0,0 +1,8 @@\n+// RUN: emitters_opt %s --xtile-cpu-shlo-to-vector -split-input-file | FileCheck %s\n+\n+func.func @transpose(%input : tensor<1024x32xf32>) -> tensor<32x1024xf32> {\n+  // CHECK: vector.transpose %{{.*}}, [1, 0] : vector<1024x32xf32> to vector<32x1024xf32>\n+  %transposed = stablehlo.transpose %input, dims = [1, 0] : (tensor<1024x32xf32>) -> tensor<32x1024xf32>\n+  return %transposed : tensor<32x1024xf32>\n+}\n+// -----"
        },
        {
            "sha": "6c16cdee5637d4a0eb0abbf806480b198e0bea87",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/xtile_to_vector.mlir",
            "status": "added",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fxtile_to_vector.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fxtile_to_vector.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fxtile_to_vector.mlir?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -0,0 +1,30 @@\n+// RUN: emitters_opt %s --xtile-cpu-xtile-to-vector -split-input-file | FileCheck %s\n+\n+// CHECK-LABEL: @simple_insert_extract\n+// CHECK-SAME: (%[[INPUT:.*]]: memref<1024xf32>, %[[OUTPUT:.*]]: memref<1024xf32>, %[[TILE_ID:.*]]: index)\n+xtile.entry_func @simple_insert_extract(%input: memref<1024xf32>, %output: memref<1024xf32>, %tile_id: index) {\n+  // CHECK-DAG: %[[POISON:.*]] = ub.poison : f32\n+  // CHECK: %[[EXTRACT:.*]] = vector.transfer_read %[[INPUT]][%[[TILE_ID]]], %[[POISON]] : memref<1024xf32>, vector<1xf32>\n+  %tile = xtile.extract %input[%tile_id][1][1] : memref<1024xf32> -> tensor<1xf32>\n+  // CHECK: vector.transfer_write %[[EXTRACT]], %[[OUTPUT]][%[[TILE_ID]]] : vector<1xf32>, memref<1024xf32>\n+  xtile.insert %tile into %output[%tile_id][1][1] : tensor<1xf32> -> memref<1024xf32>\n+  xtile.return\n+}\n+\n+\n+// -----\n+\n+// CHECK: #[[MAP:.*]] = affine_map<(d0, d1) -> (d0)>\n+// CHECK: @reduce_dimension(%[[INPUT:.*]]: memref<16x1024xf32>, %[[OUTPUT:.*]]: memref<16x1024xf32>, %[[TILE_ID:.*]]: index)\n+xtile.entry_func @reduce_dimension(%input: memref<16x1024xf32>, %output: memref<16x1024xf32>, %tile_id: index) {\n+  // CHECK: %[[OFFSET:.*]] = arith.constant 0 : index\n+  %offset = arith.constant 0 : index\n+  // CHECK: %[[POISON:.*]] = ub.poison : f32\n+  // CHECK: %[[EXTRACT:.*]] = vector.transfer_read %[[INPUT]][%[[OFFSET]], %[[TILE_ID]]], %[[POISON]] {in_bounds = [true], permutation_map = #[[MAP]]} : memref<16x1024xf32>, vector<10xf32>\n+  // CHECK: vector.transfer_write %[[EXTRACT]], %[[OUTPUT]][%[[OFFSET]], %[[TILE_ID]]] {in_bounds = [true], permutation_map = #[[MAP]]} : vector<10xf32>, memref<16x1024xf32>\n+  %tile = xtile.extract %input[%offset, %tile_id][10, 1][1, 1] : memref<16x1024xf32> -> tensor<10xf32>\n+  xtile.insert %tile into %output[%offset, %tile_id][10, 1][1, 1] : tensor<10xf32> -> memref<16x1024xf32>\n+  xtile.return\n+}\n+\n+// -----"
        },
        {
            "sha": "a177bb72251e3b4e1c9736ea4a203748993c5d1a",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/xtile_to_vector.cc",
            "status": "added",
            "additions": 127,
            "deletions": 0,
            "changes": 127,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fxtile_to_vector.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fxtile_to_vector.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fxtile_to_vector.cc?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -0,0 +1,127 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cassert>\n+#include <memory>\n+#include <optional>\n+#include <utility>\n+\n+#include \"llvm/ADT/DenseSet.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"  // IWYU pragma: keep\n+#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n+#include \"mlir/IR/AffineExpr.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/Visitors.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h\"\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n+\n+namespace xla::cpu {\n+\n+#define GEN_PASS_DECL_XTILETOVECTORPASS\n+#define GEN_PASS_DEF_XTILETOVECTORPASS\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h.inc\"\n+\n+namespace {\n+\n+mlir::AffineMap GetFilteredDims(mlir::MLIRContext* context, unsigned rank,\n+                                llvm::SmallDenseSet<unsigned> reduced_dims) {\n+  return mlir::AffineMap::getFilteredIdentityMap(\n+      context, rank, [&reduced_dims](mlir::AffineDimExpr dim) {\n+        return !reduced_dims.contains(dim.getPosition());\n+      });\n+}\n+\n+struct LowerExtractTile : mlir::OpRewritePattern<xtile::ExtractTileOp> {\n+  using OpRewritePattern::OpRewritePattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      xtile::ExtractTileOp op, mlir::PatternRewriter& rewriter) const override {\n+    mlir::RankedTensorType dest_tensor_type = op.getResult().getType();\n+    auto vector_type = mlir::VectorType::get(dest_tensor_type.getShape(),\n+                                             dest_tensor_type.getElementType());\n+\n+    // TODO(willfroom): Add support for inBounds attr.\n+    mlir::Value vector_value = rewriter.create<mlir::vector::TransferReadOp>(\n+        op->getLoc(), vector_type, op.getSource(), op.getOffsets(),\n+        /*padding=*/std::nullopt,\n+        GetFilteredDims(rewriter.getContext(),\n+                        op.getSource().getType().getRank(),\n+                        op.getReducedDimensions()));\n+    mlir::UnrealizedConversionCastOp cast =\n+        rewriter.create<mlir::UnrealizedConversionCastOp>(\n+            op->getLoc(), op.getResult().getType(), vector_value);\n+    rewriter.replaceOp(op, cast);\n+    return mlir::success();\n+  }\n+};\n+\n+struct LowerInsertTile : mlir::OpRewritePattern<xtile::InsertTileOp> {\n+  using OpRewritePattern::OpRewritePattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      xtile::InsertTileOp op, mlir::PatternRewriter& rewriter) const override {\n+    mlir::RankedTensorType source_tensor_type = op.getSource().getType();\n+    auto vector_type = mlir::VectorType::get(\n+        source_tensor_type.getShape(), source_tensor_type.getElementType());\n+    mlir::Value cast = rewriter\n+                           .create<mlir::UnrealizedConversionCastOp>(\n+                               op->getLoc(), vector_type, op.getSource())\n+                           .getResult(0);\n+    // TODO(willfroom): Add support for inBounds attr.\n+    mlir::vector::TransferWriteOp transfer_write =\n+        rewriter.create<mlir::vector::TransferWriteOp>(\n+            op->getLoc(), cast, op.getDestination(), op.getOffsets(),\n+            GetFilteredDims(rewriter.getContext(),\n+                            op.getDestination().getType().getRank(),\n+                            op.getReducedDimensions()));\n+\n+    rewriter.replaceOp(op, transfer_write);\n+    return mlir::success();\n+  }\n+};\n+\n+class XTileToVectorPass\n+    : public impl::XTileToVectorPassBase<XTileToVectorPass> {\n+ public:\n+  using XTileToVectorPassBase::XTileToVectorPassBase;\n+\n+  void runOnOperation() override {\n+    mlir::MLIRContext* context = &getContext();\n+    mlir::RewritePatternSet patterns(context);\n+    patterns.add<LowerExtractTile, LowerInsertTile>(context);\n+    if (mlir::failed(\n+            mlir::applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n+      signalPassFailure();\n+      return;\n+    }\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<mlir::Pass> CreateXTileToVectorPass() {\n+  return std::make_unique<XTileToVectorPass>();\n+}\n+\n+}  // namespace xla::cpu"
        },
        {
            "sha": "a9f9937c72f65de5602fd5e1d86e66a2bb1010f7",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2FBUILD?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -119,6 +119,7 @@ cc_library(\n         \"@llvm-project//mlir:MathDialect\",\n         \"@llvm-project//mlir:MathToLLVM\",\n         \"@llvm-project//mlir:MathTransforms\",\n+        \"@llvm-project//mlir:MemRefToLLVM\",\n         \"@llvm-project//mlir:NVVMDialect\",\n         \"@llvm-project//mlir:Pass\",\n         \"@llvm-project//mlir:ROCDLDialect\",\n@@ -128,6 +129,7 @@ cc_library(\n         \"@llvm-project//mlir:Support\",\n         \"@llvm-project//mlir:TensorDialect\",\n         \"@llvm-project//mlir:TransformUtils\",\n+        \"@llvm-project//mlir:UBToLLVM\",\n         \"@llvm-project//mlir:VectorDialect\",\n         \"@llvm-project//mlir:VectorToLLVM\",\n         \"@llvm-project//mlir:VectorTransforms\","
        },
        {
            "sha": "0cd9eed233a03ac018788f9e47d67cf5b761cf61",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/lower_to_llvm.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Flower_to_llvm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Flower_to_llvm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Flower_to_llvm.cc?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -29,7 +29,9 @@ limitations under the License.\n #include \"mlir/Conversion/LLVMCommon/ConversionTarget.h\"\n #include \"mlir/Conversion/LLVMCommon/TypeConverter.h\"\n #include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n+#include \"mlir/Conversion/MemRefToLLVM/MemRefToLLVM.h\"\n #include \"mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h\"\n+#include \"mlir/Conversion/UBToLLVM/UBToLLVM.h\"\n #include \"mlir/Conversion/VectorToLLVM/ConvertVectorToLLVM.h\"\n #include \"mlir/Dialect/AMDGPU/Utils/Chipset.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n@@ -129,6 +131,9 @@ class LowerToLLVMPass : public impl::LowerToLLVMPassBase<LowerToLLVMPass> {\n       }\n     }\n     mlir::populateFuncToLLVMConversionPatterns(type_converter, patterns);\n+    mlir::populateFinalizeMemRefToLLVMConversionPatterns(type_converter,\n+                                                         patterns);\n+    mlir::ub::populateUBToLLVMConversionPatterns(type_converter, patterns);\n     mlir::populateVectorToLLVMConversionPatterns(type_converter, patterns);\n     mlir::cf::populateControlFlowToLLVMConversionPatterns(type_converter,\n                                                           patterns);"
        },
        {
            "sha": "4ded1759ef098f35429bd14955d6b1192fa9f53f",
            "filename": "third_party/xla/xla/codegen/tools/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fcodegen%2Ftools%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fcodegen%2Ftools%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftools%2FBUILD?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -24,6 +24,7 @@ xla_cc_binary(\n     deps = [\n         \"//xla/backends/cpu/codegen/emitters/ir:xla_cpu\",\n         \"//xla/backends/cpu/codegen/emitters/transforms:passes\",\n+        \"//xla/backends/cpu/codegen/tiled/transforms:passes\",\n         \"//xla/backends/gpu/codegen/emitters:emitter_base\",\n         \"//xla/backends/gpu/codegen/emitters/ir:xla_gpu\",\n         \"//xla/backends/gpu/codegen/emitters/transforms:passes\",\n@@ -52,6 +53,7 @@ xla_cc_binary(\n         \"@llvm-project//mlir:TensorDialect\",\n         \"@llvm-project//mlir:Transforms\",\n         \"@llvm-project//mlir:VectorDialect\",\n+        \"@stablehlo//:stablehlo_ops\",\n     ],\n )\n "
        },
        {
            "sha": "0fa5f64c7ff4b5078cd6c2022dee9957c9f67277",
            "filename": "third_party/xla/xla/codegen/tools/emitters_opt.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fcodegen%2Ftools%2Femitters_opt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ba9c63910ad12eeb2136b07f186cc2e2101caa70/third_party%2Fxla%2Fxla%2Fcodegen%2Ftools%2Femitters_opt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftools%2Femitters_opt.cc?ref=ba9c63910ad12eeb2136b07f186cc2e2101caa70",
            "patch": "@@ -36,8 +36,10 @@ limitations under the License.\n #include \"mlir/Support/LogicalResult.h\"\n #include \"mlir/Tools/mlir-opt/MlirOptMain.h\"\n #include \"mlir/Transforms/Passes.h\"\n+#include \"stablehlo/dialect/StablehloOps.h\"\n #include \"xla/backends/cpu/codegen/emitters/ir/xla_cpu_dialect.h\"\n #include \"xla/backends/cpu/codegen/emitters/transforms/passes.h\"\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h\"\n #include \"xla/backends/gpu/codegen/emitters/emitter_base.h\"\n #include \"xla/backends/gpu/codegen/emitters/ir/xla_gpu_ops.h\"\n #include \"xla/backends/gpu/codegen/emitters/transforms/passes.h\"\n@@ -50,15 +52,15 @@ limitations under the License.\n \n int main(int argc, char** argv) {\n   mlir::DialectRegistry registry;\n-  registry.insert<mlir::DLTIDialect, mlir::LLVM::LLVMDialect,\n-                  mlir::NVVM::NVVMDialect, mlir::affine::AffineDialect,\n-                  mlir::arith::ArithDialect, mlir::complex::ComplexDialect,\n-                  mlir::func::FuncDialect, mlir::gpu::GPUDialect,\n-                  mlir::math::MathDialect, mlir::mhlo::MhloDialect,\n-                  mlir::mhlo::MhloDialect, mlir::scf::SCFDialect,\n-                  mlir::tensor::TensorDialect, mlir::vector::VectorDialect,\n-                  xla::XlaDialect, xla::cpu::XlaCpuDialect,\n-                  xla::gpu::XlaGpuDialect, xla::xtile::XTileDialect>();\n+  registry.insert<\n+      mlir::DLTIDialect, mlir::LLVM::LLVMDialect, mlir::NVVM::NVVMDialect,\n+      mlir::affine::AffineDialect, mlir::arith::ArithDialect,\n+      mlir::complex::ComplexDialect, mlir::func::FuncDialect,\n+      mlir::gpu::GPUDialect, mlir::math::MathDialect, mlir::mhlo::MhloDialect,\n+      mlir::mhlo::MhloDialect, mlir::scf::SCFDialect,\n+      mlir::tensor::TensorDialect, mlir::vector::VectorDialect, xla::XlaDialect,\n+      xla::cpu::XlaCpuDialect, xla::gpu::XlaGpuDialect,\n+      xla::xtile::XTileDialect, mlir::stablehlo::StablehloDialect>();\n   mlir::func::registerAllExtensions(registry);\n   mlir::LLVM::registerInlinerInterface(registry);\n   mlir::registerCanonicalizerPass();\n@@ -67,6 +69,7 @@ int main(int argc, char** argv) {\n   xla::emitters::registerTransformsPasses();\n   xla::gpu::registerGpuFusionTransformsPasses();\n   xla::cpu::registerXlaCpuTransformsPasses();\n+  xla::cpu::registerXTileCpuTransformsPasses();\n   mlir::registerPassPipeline(\n       \"xla-test-optimize\",\n       \"Test pipeline of passes up to inlining. No vectorization, also does not \""
        }
    ],
    "stats": {
        "total": 2000,
        "additions": 1953,
        "deletions": 47
    }
}