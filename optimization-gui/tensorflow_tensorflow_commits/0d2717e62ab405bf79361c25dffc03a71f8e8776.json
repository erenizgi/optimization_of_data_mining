{
    "author": "tensorflower-gardener",
    "message": "[XLA:GPU] Pass peer pointers for each kernel parameter.\n\nPiperOrigin-RevId: 828345912",
    "sha": "0d2717e62ab405bf79361c25dffc03a71f8e8776",
    "files": [
        {
            "sha": "a684ef0b72b540ed1cddde63f96914458af69fd0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/triton_xla_ops.td",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0d2717e62ab405bf79361c25dffc03a71f8e8776/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0d2717e62ab405bf79361c25dffc03a71f8e8776/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td?ref=0d2717e62ab405bf79361c25dffc03a71f8e8776",
            "patch": "@@ -401,22 +401,26 @@ def TTXLA_GetRankOp : TTXLA_Op<\"get_rank\", [Pure]> {\n def TTXLA_GetPeerPtrOp : TTXLA_Op<\"get_peer_ptr\", [Pure]> {\n   let summary = [{\n     Extract the pointer to the given symmetric memory `address` on the given\n-    `peer` device using the symmetric memory `metadata`.\n-    For this an operation first calculates an offset of the `address` to the\n-    current rank symmetric memory range, and the adds this offset to the \n-    symmetric memory range of the `peer` device.\n+    `peer` device. An `address` should point to the memory of the given kernel\n+    argument with `argument_index`. The result is calculated using the symmetric\n+    memory `metadata` constructed at the runtime.\n+    To calculate offsets operation also need to know the number of devices\n+    participating in the collective operation (`world_size`).\n   }];\n   let arguments = (ins\n     Arg<TT_PtrLike, \"\",\n       [MemRead<GlobalMemory>]>:$address,\n     I64:$peer_id,\n     Arg<TT_PtrLike, \"\",\n-      [MemRead<GlobalMemory>]>:$metadata);\n+      [MemRead<GlobalMemory>]>:$metadata,\n+    I32Attr:$argument_index,\n+    // The number of devices participating in the collective operation.\n+    I32Attr:$world_size);\n \n   let results = (outs Arg<TT_PtrLike, \"\", [MemRead<GlobalMemory>]>:$result);\n \n   let assemblyFormat = [{\n-    $address `,` $peer_id `,` $metadata attr-dict `:`\n+    $address `,` $peer_id `,` $metadata `,` attr-dict `:`\n     functional-type(operands, results)\n   }];\n }"
        },
        {
            "sha": "3f571820d261a22d16b4969c5792bc5b057badf1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_remote_access.mlir",
            "status": "modified",
            "additions": 59,
            "deletions": 12,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0d2717e62ab405bf79361c25dffc03a71f8e8776/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_remote_access.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0d2717e62ab405bf79361c25dffc03a71f8e8776/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_remote_access.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_remote_access.mlir?ref=0d2717e62ab405bf79361c25dffc03a71f8e8776",
            "patch": "@@ -12,39 +12,86 @@ tt.func @get_rank(\n }\n \n tt.func @get_peer_ptr(\n-  %arg0: !tt.ptr<i64>, %peer_id: i64, %metadata: !tt.ptr<i64>\n+  %arg0: !tt.ptr<i64>, %arg1: !tt.ptr<i64>, %peer_id: i64, %metadata: !tt.ptr<i64>\n ) -> !tt.ptr<i64> {\n   // CHECK-NOT: triton_xla.get_peer_ptr\n-  // Byte size of a pointer.\n+  // An offset from the beginning of metadata to the peer pointers for the %arg1\n+  // offset(param_to_peers) + sizeof(uint64_t) * 2 = 20\n+  // CHECK: %c24_i64 = arith.constant 24 : i64\n+  // Size of the uint64_t.\n   // CHECK: %c8_i64 = arith.constant 8 : i64\n \n   // Load metadata->rank\n-  // CHECK-NEXT: %0 = tt.load %arg2 : !tt.ptr<i64>\n+  // CHECK-NEXT: %0 = tt.load %arg3 : !tt.ptr<i64>\n \n   // Calculate offset to current base pointer.\n   // CHECK-NEXT: %1 = arith.muli %0, %c8_i64 : i64\n \n-  // Load metadata->buffer_root_ptrs[metadata->rank].\n+  // Load metadata->param_to_peers[argument_offset + metadata->rank].\n+  // Here argument_offset = 0 since %arg0 is the first argument.\n   // CHECK-NEXT: %2 = arith.addi %1, %c8_i64 : i64\n-  // CHECK-NEXT: %3 = tt.addptr %arg2, %2 : !tt.ptr<i64>, i64\n+  // CHECK-NEXT: %3 = tt.addptr %arg3, %2 : !tt.ptr<i64>, i64\n   // CHECK-NEXT: %4 = tt.load %3 : !tt.ptr<i64>\n \n   // Calculate offset to address.\n   // CHECK-NEXT: %5 = tt.ptr_to_int %arg0 : !tt.ptr<i64> -> i64\n   // CHECK-NEXT: %6 = arith.subi %5, %4 : i64\n \n   // Calculate offset to peer base pointer.\n-  // CHECK-NEXT: %7 = arith.muli %arg1, %c8_i64 : i64\n+  // CHECK-NEXT: %7 = arith.muli %arg2, %c8_i64 : i64\n   // CHECK-NEXT: %8 = arith.addi %7, %c8_i64 : i64\n \n-  // Load metadata->buffer_root_ptrs[peer_id].\n-  // CHECK-NEXT: %9 = tt.addptr %arg2, %8 : !tt.ptr<i64>, i64\n+  // Load metadata->peer_base_ptrs[argument_offset + peer_id].\n+  // CHECK-NEXT: %9 = tt.addptr %arg3, %8 : !tt.ptr<i64>, i64\n   // CHECK-NEXT: %10 = tt.load %9 : !tt.ptr<i64>\n \n-  // Load metadata->buffer_root_ptrs[peer_id] + offset.\n+  // Load metadata->buffer_root_ptrs[argument_offset + peer_id] + offset.\n   // CHECK-NEXT: %11 = arith.addi %10, %6 : i64\n   // CHECK-NEXT: %12 = tt.int_to_ptr %11 : i64 -> !tt.ptr<i64>\n-  // CHECK-NEXT: tt.return %12 : !tt.ptr<i64>\n-  %peer_ptr = triton_xla.get_peer_ptr %arg0, %peer_id, %metadata : (!tt.ptr<i64>, i64, !tt.ptr<i64>) -> !tt.ptr<i64>\n-  tt.return %peer_ptr : !tt.ptr<i64>\n+  %arg_0_peer_ptr = triton_xla.get_peer_ptr %arg0, %peer_id, %metadata,\n+     { argument_index = 0 : i32, world_size = 2 : i32 } :\n+     (!tt.ptr<i64>, i64, !tt.ptr<i64>) -> !tt.ptr<i64>\n+\n+  // Load metadata->rank\n+  // CHECK-NEXT: %13 = tt.load %arg3 : !tt.ptr<i64>\n+  // Calculate offset to current base pointer.\n+  // CHECK-NEXT: %14 = arith.muli %13, %c8_i64 : i64\n+  // Load metadata->param_to_peers[argument_offset + metadata->rank].\n+  // CHECK-NEXT: %15 = arith.addi %14, %c24_i64 : i64\n+  // CHECK-NEXT: %16 = tt.addptr %arg3, %15 : !tt.ptr<i64>, i64\n+  // CHECK-NEXT: %17 = tt.load %16 : !tt.ptr<i64>\n+  // Calculate offset to address.\n+  // CHECK-NEXT: %18 = tt.ptr_to_int %arg1 : !tt.ptr<i64> -> i64\n+  // CHECK-NEXT: %19 = arith.subi %18, %17 : i64\n+\n+  // Calculate offset to peer base pointer.\n+  // CHECK-NEXT: %20 = arith.muli %arg2, %c8_i64 : i64\n+  // CHECK-NEXT: %21 = arith.addi %20, %c24_i64 : i64\n+\n+  // Load metadata->peer_base_ptrs[argument_offset + peer_id].\n+  // CHECK-NEXT: %22 = tt.addptr %arg3, %21 : !tt.ptr<i64>, i64\n+  // CHECK-NEXT: %23 = tt.load %22 : !tt.ptr<i64>\n+\n+  // Load metadata->buffer_root_ptrs[argument_offset + peer_id] + offset.\n+  // CHECK-NEXT: %24 = arith.addi %23, %19 : i64\n+  // CHECK-NEXT: %25 = tt.int_to_ptr %24 : i64 -> !tt.ptr<i64>\n+\n+  %arg_1_peer_ptr = triton_xla.get_peer_ptr %arg1, %peer_id, %metadata,\n+     { argument_index = 1 : i32, world_size = 2 : i32 } :\n+     (!tt.ptr<i64>, i64, !tt.ptr<i64>) -> !tt.ptr<i64>\n+  \n+  // Avoid optimizing away the get_peer_ptr calls, by returning xor of the two\n+  // peer pointers.\n+  // \n+  // CHECK-NEXT: %26 = tt.ptr_to_int %12 : !tt.ptr<i64> -> i64\n+  %int_arg0 = tt.ptr_to_int %arg_0_peer_ptr : !tt.ptr<i64> -> i64\n+  // CHECK-NEXT: %27 = tt.ptr_to_int %25 : !tt.ptr<i64> -> i64\n+  %int_arg1 = tt.ptr_to_int %arg_1_peer_ptr : !tt.ptr<i64> -> i64\n+\n+  // CHECK-NEXT: %28 = arith.ori %26, %27 : i64\n+  %result_int = arith.ori %int_arg0, %int_arg1 : i64\n+  // CHECK-NEXT: %29 = tt.int_to_ptr %28 : i64 -> !tt.ptr<i64>\n+  %result_ptr = tt.int_to_ptr %result_int : i64 -> !tt.ptr<i64>\n+  // CHECK-NEXT: tt.return %29 : !tt.ptr<i64>\n+  tt.return %result_ptr : !tt.ptr<i64>\n }\n\\ No newline at end of file"
        },
        {
            "sha": "579eb21390ca8968fa4a8eade8acc666fd8516a7",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_lower_remote_access_pass.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 10,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0d2717e62ab405bf79361c25dffc03a71f8e8776/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_remote_access_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0d2717e62ab405bf79361c25dffc03a71f8e8776/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_remote_access_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_remote_access_pass.cc?ref=0d2717e62ab405bf79361c25dffc03a71f8e8776",
            "patch": "@@ -67,8 +67,14 @@ LogicalResult LowerGetRankOp(GetRankOp get_rank, PatternRewriter& rewriter) {\n \n // The peer address should be computed as follows:\n //\n-// offset = address - metadata->buffer_root_ptrs[metadata->rank].\n-// peer_address = metadata->buffer_root_ptrs[peer_id] + offset.\n+// argument_offset = world_size * argument_index\n+// argument_base = metadata->param_to_peers[argument_offset + metadata->rank]\n+// offset = address - argument_base\n+// peer_base = metadata->param_to_peers[argument_offset + peer_id]\n+// peer_address = peer_base + offset\n+//\n+// For more details regarding peer pointers layout see comments in the:\n+// `stream_executor::gpu::CollectiveKernelMetadata`.\n LogicalResult LowerGetPeerPtrOp(GetPeerPtrOp get_peer_ptr,\n                                 PatternRewriter& rewriter) {\n   Value metadata = get_peer_ptr.getMetadata();\n@@ -94,16 +100,26 @@ LogicalResult LowerGetPeerPtrOp(GetPeerPtrOp get_peer_ptr,\n   // 1. Load metadata->rank.\n   Value current_rank_load_op = builder.create<GetRankOp>(metadata);\n \n-  // 2. Load metadata->buffer_root_ptrs[metadata->rank].\n+  // 2. Calculate argument_offset = num_ranks * argument_index.\n+  const int32_t argument_index = get_peer_ptr.getArgumentIndex();\n+  const int32_t world_size = get_peer_ptr.getWorldSize();\n+  const int32_t argument_offset =\n+      world_size * argument_index * sizeof(uint64_t);\n+\n+  // 3. Load metadata->param_to_peers[argument_offset + metadata->rank].\n   Value local_buffers_ptrs_offset = builder.create<arith::ConstantIntOp>(\n-      type_i64, offsetof(CollectiveKernelMetadata, buffer_root_ptrs));\n+      type_i64, offsetof(CollectiveKernelMetadata, param_to_peers));\n \n   Value rank_offset =\n       builder.create<arith::ExtUIOp>(type_i64, current_rank_load_op);\n+  Value argument_offset_bytes =\n+      builder.create<arith::ConstantIntOp>(type_i64, argument_offset);\n   Value current_rank_offset_bytes =\n       builder.create<arith::MulIOp>(rank_offset, pointer_size_bytes_const);\n+  Value argument_ptr_offset_bytes = builder.create<arith::AddIOp>(\n+      local_buffers_ptrs_offset, argument_offset_bytes);\n   Value current_ptr_offset_bytes = builder.create<arith::AddIOp>(\n-      local_buffers_ptrs_offset, current_rank_offset_bytes);\n+      argument_ptr_offset_bytes, current_rank_offset_bytes);\n \n   Value current_range_address = builder.create<AddPtrOp>(\n       metadata.getType(), metadata, current_ptr_offset_bytes);\n@@ -115,19 +131,19 @@ LogicalResult LowerGetPeerPtrOp(GetPeerPtrOp get_peer_ptr,\n       EvictionPolicyAttr::get(ctx, EvictionPolicy::NORMAL),\n       /*isVolatile=*/rewriter.getBoolAttr(false));\n \n-  // 3. Calculate offset =\n-  //      address - metadata->buffer_root_ptrs[metadata->rank].\n+  // 4. Calculate offset =\n+  //      address - metadata->param_to_peers[argument_offset + metadata->rank].\n   Value current_range_address_int =\n       builder.create<PtrToIntOp>(type_i64, address);\n   Value offsetInt = builder.create<arith::SubIOp>(current_range_address_int,\n                                                   current_range_address_value);\n \n-  // 4. Load metadata->buffer_root_ptrs[peer_id].\n+  // 5. Load metadata->param_to_peers[argument_offset + peer_id].\n   Value peer_index = builder.create<arith::ExtUIOp>(type_i64, peer_id);\n   Value peer_index_offset_bytes =\n       builder.create<arith::MulIOp>(peer_index, pointer_size_bytes_const);\n   Value peer_range_offset_bytes = builder.create<arith::AddIOp>(\n-      local_buffers_ptrs_offset, peer_index_offset_bytes);\n+      argument_ptr_offset_bytes, peer_index_offset_bytes);\n   Value peer_range_address = builder.create<AddPtrOp>(\n       metadata.getType(), metadata, peer_range_offset_bytes);\n \n@@ -138,7 +154,7 @@ LogicalResult LowerGetPeerPtrOp(GetPeerPtrOp get_peer_ptr,\n       EvictionPolicyAttr::get(ctx, EvictionPolicy::NORMAL),\n       /*isVolatile=*/rewriter.getBoolAttr(false));\n \n-  // 5. Calculate the result address: peerBasePtr + offset.\n+  // 6. Calculate the result address: peerBasePtr + offset.\n   Value result_int =\n       builder.create<arith::AddIOp>(peer_range_address_value, offsetInt);\n   Value result_address = builder.create<IntToPtrOp>(result_type, result_int);"
        },
        {
            "sha": "e3967642047daf4309b679eb174ce57e90e62147",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_test.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 5,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0d2717e62ab405bf79361c25dffc03a71f8e8776/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0d2717e62ab405bf79361c25dffc03a71f8e8776/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc?ref=0d2717e62ab405bf79361c25dffc03a71f8e8776",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/all_reduce.h\"\n \n #include <algorithm>\n+#include <cstddef>\n #include <cstdint>\n #include <memory>\n #include <tuple>\n@@ -150,15 +151,22 @@ class AllReduceKernelTest : public ::testing::Test,\n     }\n \n     std::vector<se::DeviceMemoryBase> metadata_buffers;\n+    // One for signal and one for input parameters.\n+    constexpr int kNumPeerParameters = 2;\n+    size_t param_to_peers_size =\n+        sizeof(uint64_t) * kNumPeerParameters * num_ranks;\n+    std::vector<uint64_t> param_to_peers_ptrs;\n+    for (const auto& local_input_buffer : local_input_buffers) {\n+      param_to_peers_ptrs.push_back((uint64_t)local_input_buffer.opaque());\n+    }\n+    for (const auto& signal_flags_buffer : signal_flags_buffers) {\n+      param_to_peers_ptrs.push_back((uint64_t)signal_flags_buffer.opaque());\n+    }\n \n     for (int i = 0; i < num_ranks; ++i) {\n       CollectiveKernelMetadata metadata;\n       metadata.rank = i;\n \n-      for (int j = 0; j < num_ranks; ++j) {\n-        metadata.buffer_root_ptrs[j] = (uint64_t)allocated_buffers[j].opaque();\n-      }\n-\n       if (params_.all_reduce_strategy == AllReduceStrategy::kMultimem) {\n         stream_executor::gpu::GpuExecutor* gpu_executor =\n             dynamic_cast<stream_executor::gpu::GpuExecutor*>(executors[i]);\n@@ -171,11 +179,21 @@ class AllReduceKernelTest : public ::testing::Test,\n         metadata.multicast_buffer_ptr = 0;\n       }\n \n+      // First map from parameter to peer ptrs and then metadata.\n       metadata_buffers.emplace_back(executors[i]->AllocateArray<uint64_t>(\n-          sizeof(CollectiveKernelMetadata)));\n+          sizeof(CollectiveKernelMetadata) + param_to_peers_size));\n+\n+      se::DeviceMemoryBase param_to_peers_ptrs_buffer =\n+          metadata_buffers[i].GetByteSlice(sizeof(CollectiveKernelMetadata),\n+                                           param_to_peers_size);\n+      metadata.param_to_peers =\n+          reinterpret_cast<uint64_t*>(param_to_peers_ptrs_buffer.opaque());\n \n       TF_RETURN_IF_ERROR(streams[i]->Memcpy(&metadata_buffers[i], &metadata,\n                                             sizeof(CollectiveKernelMetadata)));\n+      TF_RETURN_IF_ERROR(streams[i]->Memcpy(&param_to_peers_ptrs_buffer,\n+                                            param_to_peers_ptrs.data(),\n+                                            param_to_peers_size));\n     }\n \n     for (int i = 0; i < num_ranks; ++i) {"
        },
        {
            "sha": "41067ff0ed491fe956863547c0e459d99c75e2bd",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.cc",
            "status": "modified",
            "additions": 55,
            "deletions": 27,
            "changes": 82,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0d2717e62ab405bf79361c25dffc03a71f8e8776/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0d2717e62ab405bf79361c25dffc03a71f8e8776/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc?ref=0d2717e62ab405bf79361c25dffc03a71f8e8776",
            "patch": "@@ -163,6 +163,7 @@ int64_t CollectiveKernelThunk::GetInputSizeBytes() const {\n struct BaseRangePtrRendezvousValue {\n   RankId rank;\n   se::DeviceMemoryBase buffer_ptr;\n+  se::DeviceMemoryBase signal_ptr;\n \n   bool operator<(const BaseRangePtrRendezvousValue& other) const {\n     return rank < other.rank;\n@@ -179,7 +180,8 @@ absl::Status CollectiveKernelThunk::ExchangeStateMetadata(\n       << \"Device \" << params.collective_params->global_device_id\n       << \"is not in the clique.\";\n   rendezvous_value.rank = rank.value();\n-  rendezvous_value.buffer_ptr = state.local_buffer.memory();\n+  rendezvous_value.buffer_ptr = state.local_buffers_handle.memory();\n+  rendezvous_value.signal_ptr = state.signal_buffers_handle.memory();\n \n   auto rendezvous_fn =\n       [](absl::Span<const BaseRangePtrRendezvousValue* const> values) {\n@@ -203,24 +205,44 @@ absl::Status CollectiveKernelThunk::ExchangeStateMetadata(\n                           /*value=*/rendezvous_value, /*num_threads=*/num_ranks,\n                           rendezvous_fn));\n \n-  if (rendezvous_values->size() > CollectiveKernelMetadata::kMaxNumDevices) {\n-    return absl::InvalidArgumentError(\n-        absl::StrFormat(\"Multi-device kernels require at most %d peers.\",\n-                        CollectiveKernelMetadata::kMaxNumDevices));\n+  if (rendezvous_values->size() > num_ranks) {\n+    return absl::InvalidArgumentError(absl::StrFormat(\n+        \"Multi-device kernels require at most %d peers.\", num_ranks));\n   }\n   CollectiveKernelMetadata metadata;\n   metadata.rank = rank.value().value();\n-  for (int i = 0; i < rendezvous_values->size(); ++i) {\n-    metadata.buffer_root_ptrs[i] = reinterpret_cast<uint64_t>(\n-        rendezvous_values->at(i).buffer_ptr.opaque());\n-  }\n   metadata.multicast_buffer_ptr =\n       reinterpret_cast<uint64_t>(state.multicast_device_ptr);\n \n-  se::DeviceMemoryBase metadata_ptr =\n-      params.executor->Allocate(sizeof(CollectiveKernelMetadata), 0);\n+  std::vector<uint64_t> param_to_peers_ptrs;\n+  param_to_peers_ptrs.reserve(rendezvous_values->size() * 2);\n+  for (const auto& value : *rendezvous_values) {\n+    param_to_peers_ptrs.push_back(\n+        reinterpret_cast<uint64_t>(value.buffer_ptr.opaque()));\n+  }\n+  for (const auto& value : *rendezvous_values) {\n+    param_to_peers_ptrs.push_back(\n+        reinterpret_cast<uint64_t>(value.signal_ptr.opaque()));\n+  }\n+\n+  size_t param_to_peers_ptrs_size_bytes =\n+      param_to_peers_ptrs.size() * sizeof(uint64_t);\n+  se::DeviceMemoryBase metadata_ptr = params.executor->Allocate(\n+      sizeof(CollectiveKernelMetadata) + param_to_peers_ptrs_size_bytes, 0);\n+  se::DeviceMemoryBase param_to_peers_ptrs_buffer = metadata_ptr.GetByteSlice(\n+      sizeof(CollectiveKernelMetadata), param_to_peers_ptrs_size_bytes);\n+  VLOG(3) << \"[\" << params.executor->device_ordinal() << \"]\"\n+          << \" ExchangeStateMetadata: metadata_ptr = \" << metadata_ptr.opaque()\n+          << \", param_to_peers_ptrs_buffer = \"\n+          << param_to_peers_ptrs_buffer.opaque()\n+          << \", param_to_peers_ptrs_size = \" << param_to_peers_ptrs.size();\n+  metadata.param_to_peers =\n+      reinterpret_cast<uint64_t*>(param_to_peers_ptrs_buffer.opaque());\n   TF_RETURN_IF_ERROR(params.stream->Memcpy(&metadata_ptr, (void*)&metadata,\n                                            sizeof(CollectiveKernelMetadata)));\n+  TF_RETURN_IF_ERROR(params.stream->Memcpy(&param_to_peers_ptrs_buffer,\n+                                           param_to_peers_ptrs.data(),\n+                                           param_to_peers_ptrs_size_bytes));\n   TF_RETURN_IF_ERROR(params.stream->BlockHostUntilDone());\n \n   state.metadata = metadata_ptr;\n@@ -265,9 +287,9 @@ absl::Status CollectiveKernelThunk::SetupMultimem(\n   // Wait for all devices to register the multicast object.\n   TF_RETURN_IF_ERROR(Barrier(device_number, clique_key));\n \n-  TF_ASSIGN_OR_RETURN(\n-      state.multicast_device_ptr,\n-      multicast_memory_->MapMemory(state.local_buffer.memory(), gpu_executor));\n+  TF_ASSIGN_OR_RETURN(state.multicast_device_ptr,\n+                      multicast_memory_->MapMemory(\n+                          state.local_buffers_handle.memory(), gpu_executor));\n \n   return absl::OkStatus();\n }\n@@ -299,11 +321,16 @@ absl::Status CollectiveKernelThunk::Initialize(const InitializeParams& params) {\n           kNumSignalFlags * sizeof(int32_t), kXlaAllocatedBufferAlignBytes);\n       const int64_t kLocalBufferSize = xla::RoundUpTo<uint64_t>(\n           buffers_[0].source_buffer.size(), kXlaAllocatedBufferAlignBytes);\n+\n+      TF_ASSIGN_OR_RETURN(\n+          se::DeviceMemoryHandle local_buffers_handle,\n+          AllocateMemory(params.executor, kLocalBufferSize * kNumBuffers,\n+                         \"Local buffers\"));\n+\n       TF_ASSIGN_OR_RETURN(\n-          se::DeviceMemoryHandle local_buffer_alloc,\n-          AllocateMemory(params.executor,\n-                         (kSignalBufferSize + kLocalBufferSize) * kNumBuffers,\n-                         \"Local and Signal buffers\"));\n+          se::DeviceMemoryHandle signal_buffers_handle,\n+          AllocateMemory(params.executor, kLocalBufferSize * kNumBuffers,\n+                         \"Signal buffers\"));\n \n       // Step2: We needs 1 atomic flag per block per device on each device.\n       // One-shot kernel expects that the signal flags buffer is zeroed out.\n@@ -312,7 +339,8 @@ absl::Status CollectiveKernelThunk::Initialize(const InitializeParams& params) {\n       // correct state after use, so we don't need to zero out after\n       // initialization.\n       TF_RETURN_IF_ERROR(params.executor->SynchronousMemZero(\n-          local_buffer_alloc.memory_ptr(), local_buffer_alloc.memory().size()));\n+          signal_buffers_handle.memory_ptr(),\n+          signal_buffers_handle.memory().size()));\n       // Create a kernel for execution.\n       std::unique_ptr<se::Kernel> kernel = nullptr;\n       // If PTX is provided, we create a kernel from it.\n@@ -327,22 +355,22 @@ absl::Status CollectiveKernelThunk::Initialize(const InitializeParams& params) {\n           params.executor,\n           std::make_unique<StreamState>(\n               params.executor->device_ordinal(), rank.value(),\n-              std::move(local_buffer_alloc), std::move(kernel)));\n+              std::move(local_buffers_handle), std::move(signal_buffers_handle),\n+              std::move(kernel)));\n \n       state = per_stream_state_.at(params.executor).get();\n \n       // NB: This is a double buffer allocation. So size of a single buffer is\n       // half of the total allocation.\n       for (int i = 0; i < kNumBuffers; ++i) {\n-        uint64_t offset = i * (kLocalBufferSize + kSignalBufferSize);\n         state->remote_buffer_ptrs[i] =\n-            state->local_buffer.memory_ptr()->GetByteSlice(\n-                /*offset_bytes=*/offset,\n+            state->local_buffers_handle.memory_ptr()->GetByteSlice(\n+                /*offset_bytes=*/i * kLocalBufferSize,\n                 /*size_bytes=*/kLocalBufferSize);\n \n         state->signal_buffer_ptrs[i] =\n-            state->local_buffer.memory_ptr()->GetByteSlice(\n-                /*offset_bytes=*/offset + kLocalBufferSize,\n+            state->signal_buffers_handle.memory_ptr()->GetByteSlice(\n+                /*offset_bytes=*/i * kSignalBufferSize,\n                 /*size_bytes=*/kSignalBufferSize);\n       }\n     }\n@@ -416,8 +444,8 @@ absl::Status CollectiveKernelThunk::ExecuteOnStream(\n   se::DeviceMemoryBase signal_buffer_ptr =\n       state->signal_buffer_ptrs[buffer_index];\n   VLOG(3) << \"[\" << device_ordinal\n-          << \"] input_buffer_ptr: \" << (uint64_t)input_buffer_ptr.opaque()\n-          << \" signal_buffer_ptr: \" << (uint64_t)signal_buffer_ptr.opaque();\n+          << \"] input_buffer_ptr: \" << input_buffer_ptr.opaque()\n+          << \" signal_buffer_ptr: \" << signal_buffer_ptr.opaque();\n   VLOG(3) << \"[\" << device_ordinal\n           << \"] launch dimensions: \" << launch_dimensions.num_blocks() << \"x\"\n           << launch_dimensions.num_threads_per_block()"
        },
        {
            "sha": "9fd6b941f7eb33b1a5ed038a3d16ef7d36a08d51",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.h",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0d2717e62ab405bf79361c25dffc03a71f8e8776/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0d2717e62ab405bf79361c25dffc03a71f8e8776/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h?ref=0d2717e62ab405bf79361c25dffc03a71f8e8776",
            "patch": "@@ -99,15 +99,19 @@ class CollectiveKernelThunk : public Thunk {\n   struct StreamState {\n     int device_ordinal;\n     RankId rank;\n-    // Buffers and signal flags allocated for the collective.\n+    // Buffers allocated for the collective.\n     // Buffers are double buffered to allow for consecutive invocation\n     // of the kernel on different GPUs.\n     // - GPUs sync on Buffer 0 on first invocation.\n     // - GPUs sync on Buffer 1 on second invocation.\n     //   This implies that all GPUs must have finished the first invocation\n     //   before they can sync on the second invocation.\n     // - Alternate back to Buffer 0 on third invocation. And so on.\n-    se::DeviceMemoryHandle local_buffer;\n+    se::DeviceMemoryHandle local_buffers_handle;\n+\n+    // Signal buffers allocated for the collective.\n+    // Also double buffered for the same reason as local buffers.\n+    se::DeviceMemoryHandle signal_buffers_handle;\n \n     // Pointer to the collective kernel metadata on device.\n     se::DeviceMemoryBase metadata;\n@@ -126,11 +130,13 @@ class CollectiveKernelThunk : public Thunk {\n     // Constructor to make OSS builds happy.\n     StreamState() = default;\n     StreamState(int device_ordinal_arg, RankId rank_arg,\n-                se::DeviceMemoryHandle local_buffer_arg,\n+                se::DeviceMemoryHandle local_buffers_handle_arg,\n+                se::DeviceMemoryHandle signal_buffers_handle_arg,\n                 std::unique_ptr<se::Kernel> kernel_arg)\n         : device_ordinal(device_ordinal_arg),\n           rank(rank_arg),\n-          local_buffer(std::move(local_buffer_arg)),\n+          local_buffers_handle(std::move(local_buffers_handle_arg)),\n+          signal_buffers_handle(std::move(signal_buffers_handle_arg)),\n           kernel(std::move(kernel_arg)) {}\n   };\n "
        },
        {
            "sha": "ee8a367f17080bf479942f5f76c753c1ee906af5",
            "filename": "third_party/xla/xla/stream_executor/gpu/all_reduce_kernel_lib.cu.h",
            "status": "modified",
            "additions": 29,
            "deletions": 17,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0d2717e62ab405bf79361c25dffc03a71f8e8776/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fall_reduce_kernel_lib.cu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0d2717e62ab405bf79361c25dffc03a71f8e8776/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fall_reduce_kernel_lib.cu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fall_reduce_kernel_lib.cu.h?ref=0d2717e62ab405bf79361c25dffc03a71f8e8776",
            "patch": "@@ -86,17 +86,24 @@ __device__ __forceinline__ void VecOp(Vec<T>& res, const Vec<T>& vec) {\n \n template <typename T>\n __device__ __forceinline__ RestrictedPtr<T> GetPeerPtr(\n-    void* ptr, int64_t peer_rank, const CollectiveKernelMetadata& metadata) {\n-  uint64_t current_base = metadata.buffer_root_ptrs[metadata.rank];\n+    void* ptr, int64_t peer_rank, int64_t argument_index, int num_ranks,\n+    const CollectiveKernelMetadata& metadata) {\n+  uint64_t argument_offset = num_ranks * argument_index;\n+  uint64_t current_base =\n+      metadata.param_to_peers[argument_offset + metadata.rank];\n+  uint64_t peer_base = metadata.param_to_peers[argument_offset + peer_rank];\n   uint64_t offset = (uint64_t)ptr - current_base;\n \n-  return (RestrictedPtr<T>)(metadata.buffer_root_ptrs[peer_rank] + offset);\n+  return (RestrictedPtr<T>)(peer_base + offset);\n }\n \n template <typename T>\n __device__ __forceinline__ RestrictedPtr<T> GetMultimemPtr(\n-    void* ptr, const CollectiveKernelMetadata& metadata) {\n-  uint64_t current_base = metadata.buffer_root_ptrs[metadata.rank];\n+    void* ptr, int64_t argument_index, int num_ranks,\n+    const CollectiveKernelMetadata& metadata) {\n+  uint64_t argument_offset = num_ranks * argument_index;\n+  uint64_t current_base =\n+      metadata.param_to_peers[argument_offset + metadata.rank];\n   uint64_t offset = (uint64_t)ptr - current_base;\n \n   return (RestrictedPtr<T>)(metadata.multicast_buffer_ptr + offset);\n@@ -134,11 +141,13 @@ __device__ __forceinline__ void OneShotAllReduceKernelImpl(\n   __shared__ std::array<RestrictedPtr<T>, kMaxNumAllReduceInputPtrs>\n       remote_input_buffers;\n \n-  if (threadIdx.x < kMaxNumAllReduceInputPtrs) {\n-    signal_flags_buffers[threadIdx.x] = GetPeerPtr<uint32_t>(\n-        args.symmetric_signal_ptrs, threadIdx.x, *args.metadata);\n+  if (threadIdx.x < args.num_ranks) {\n     remote_input_buffers[threadIdx.x] =\n-        GetPeerPtr<T>(args.symmetric_input_ptrs, threadIdx.x, *args.metadata);\n+        GetPeerPtr<T>(args.symmetric_input_ptrs, threadIdx.x,\n+                      /*argument_index=*/0, args.num_ranks, *args.metadata);\n+    signal_flags_buffers[threadIdx.x] = GetPeerPtr<uint32_t>(\n+        args.symmetric_signal_ptrs, threadIdx.x, /*argument_index=*/1,\n+        args.num_ranks, *args.metadata);\n   }\n \n   __syncthreads();\n@@ -190,9 +199,10 @@ __device__ __forceinline__ void MultimemAllReduceKernelImpl(\n   __shared__ std::array<RestrictedPtr<uint32_t>, kMaxNumAllReduceInputPtrs>\n       signal_flags_buffers;\n \n-  if (threadIdx.x < kMaxNumAllReduceInputPtrs) {\n+  if (threadIdx.x < args.num_ranks) {\n     signal_flags_buffers[threadIdx.x] = GetPeerPtr<uint32_t>(\n-        args.symmetric_signal_ptrs, threadIdx.x, *args.metadata);\n+        args.symmetric_signal_ptrs, threadIdx.x, /*argument_index=*/1,\n+        args.num_ranks, *args.metadata);\n   }\n \n   int64_t offset =\n@@ -208,8 +218,8 @@ __device__ __forceinline__ void MultimemAllReduceKernelImpl(\n                               args.signal_value);\n   __syncthreads();\n \n-  RestrictedPtr<T> multimem_ptr =\n-      GetMultimemPtr<T>(args.symmetric_input_ptrs, *args.metadata);\n+  RestrictedPtr<T> multimem_ptr = GetMultimemPtr<T>(\n+      args.symmetric_input_ptrs, 0, args.num_ranks, *args.metadata);\n   if (args.metadata->rank == 0) {\n     for (int i = offset; i < args.num_elements; i += stride) {\n       T* multimem_element_ptr = multimem_ptr + i;\n@@ -254,11 +264,13 @@ __device__ __forceinline__ void TwoShotAllReduceKernelImpl(\n   __shared__ std::array<RestrictedPtr<T>, kMaxNumAllReduceInputPtrs>\n       remote_input_buffers;\n \n-  if (threadIdx.x < kMaxNumAllReduceInputPtrs) {\n-    signal_flags_buffers[threadIdx.x] = GetPeerPtr<uint32_t>(\n-        args.symmetric_signal_ptrs, threadIdx.x, *args.metadata);\n+  if (threadIdx.x < args.num_ranks) {\n     remote_input_buffers[threadIdx.x] =\n-        GetPeerPtr<T>(args.symmetric_input_ptrs, threadIdx.x, *args.metadata);\n+        GetPeerPtr<T>(args.symmetric_input_ptrs, threadIdx.x,\n+                      /*argument_index=*/0, args.num_ranks, *args.metadata);\n+    signal_flags_buffers[threadIdx.x] = GetPeerPtr<uint32_t>(\n+        args.symmetric_signal_ptrs, threadIdx.x, /*argument_index=*/1,\n+        args.num_ranks, *args.metadata);\n   }\n \n   __syncthreads();"
        },
        {
            "sha": "631421bea2f262d1c056b446c2157e8a7ad70eee",
            "filename": "third_party/xla/xla/stream_executor/gpu/collective_kernel_metadata.h",
            "status": "modified",
            "additions": 13,
            "deletions": 9,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0d2717e62ab405bf79361c25dffc03a71f8e8776/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fcollective_kernel_metadata.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0d2717e62ab405bf79361c25dffc03a71f8e8776/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fcollective_kernel_metadata.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fcollective_kernel_metadata.h?ref=0d2717e62ab405bf79361c25dffc03a71f8e8776",
            "patch": "@@ -21,18 +21,22 @@ limitations under the License.\n // Metadata parameter which is passed to the collective kernel.\n // The metadata allows to compute the address of a peer's buffer in the\n // collective kernel and get the current rank of a peer device.\n-// Right now two root pointers are getting passed. One is used for buffers\n-// allocated by the buffer assignment and allows kernel to address input and\n-// output buffers. The second one is used for buffers allocated within the\n-// collective kernel thunk.\n+// For each kernel parameter `param_to_peers` contains the N peer pointers to\n+// the same parameter at the peer device, where N is the number of devices\n+// participating in the collective kernel.\n+// This information is structured as the\n+// single dimentional array with the following layout:\n+// [\n+//   param0_peer0, param0_peer1, ..., param0_peerN,\n+//   param1_peer0, param1_peer1, ..., param1_peerN,\n+//   ...\n+// ]\n struct CollectiveKernelMetadata {\n-  constexpr static int kMaxNumDevices = 8;\n-  int64_t rank;\n-  // Root pointer for buffers.\n-  int64_t buffer_root_ptrs[kMaxNumDevices];\n+  uint64_t rank;\n+  uint64_t* param_to_peers;\n \n   // Root pointer for multicast buffer for current device.\n-  int64_t multicast_buffer_ptr;\n+  uint64_t multicast_buffer_ptr;\n };\n \n #endif  // XLA_STREAM_EXECUTOR_GPU_COLLECTIVE_KERNEL_METADATA_H_"
        }
    ],
    "stats": {
        "total": 315,
        "additions": 225,
        "deletions": 90
    }
}