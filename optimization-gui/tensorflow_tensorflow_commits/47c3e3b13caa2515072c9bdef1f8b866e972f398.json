{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 809678355",
    "sha": "47c3e3b13caa2515072c9bdef1f8b866e972f398",
    "files": [
        {
            "sha": "d606f58a7d7a5ddacf0779537b7ef99f09c4fa50",
            "filename": "third_party/xla/xla/tsl/distributed_runtime/coordination/client_server_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/47c3e3b13caa2515072c9bdef1f8b866e972f398/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fclient_server_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/47c3e3b13caa2515072c9bdef1f8b866e972f398/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fclient_server_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fclient_server_test.cc?ref=47c3e3b13caa2515072c9bdef1f8b866e972f398",
            "patch": "@@ -252,15 +252,15 @@ TEST_F(ClientServerTest, ConnectAndShutdownAreBarriers) {\n       return connect_count == node_id;\n     };\n     {\n-      absl::MutexLock lock(&mu);\n+      absl::MutexLock lock(mu);\n       mu.Await(absl::Condition(&my_connect_turn));\n       ++connect_count;\n     }\n     TF_RETURN_IF_ERROR(client->Connect());\n     // Verify that all of the threads have called Connect() by the time we get\n     // here.\n     {\n-      absl::MutexLock lock(&mu);\n+      absl::MutexLock lock(mu);\n       if (connect_count != num_nodes) {\n         return absl::InternalError(absl::StrCat(\n             \"Connect count is \", connect_count, \" but expected \", num_nodes));\n@@ -273,13 +273,13 @@ TEST_F(ClientServerTest, ConnectAndShutdownAreBarriers) {\n       return shutdown_count == node_id;\n     };\n     {\n-      absl::MutexLock lock(&mu);\n+      absl::MutexLock lock(mu);\n       mu.Await(absl::Condition(&my_shutdown_turn));\n       ++shutdown_count;\n     }\n     TF_RETURN_IF_ERROR(client->Shutdown());\n     {\n-      absl::MutexLock lock(&mu);\n+      absl::MutexLock lock(mu);\n       if (shutdown_count != num_nodes) {\n         return absl::InternalError(absl::StrCat(\n             \"Shutdown count is \", shutdown_count, \" but expected \", num_nodes));"
        },
        {
            "sha": "fc94b64c2fada66f98f2ea7832421355d97e4388",
            "filename": "third_party/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/47c3e3b13caa2515072c9bdef1f8b866e972f398/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fcoordination_service.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/47c3e3b13caa2515072c9bdef1f8b866e972f398/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fcoordination_service.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fcoordination_service.cc?ref=47c3e3b13caa2515072c9bdef1f8b866e972f398",
            "patch": "@@ -137,7 +137,7 @@ void CoordinationService::TaskState::SetConnected(\n   state_ = CoordinatedTaskState::TASKSTATE_CONNECTED;\n   status_ = absl::OkStatus();\n   task_incarnation_ = task_incarnation;\n-  absl::MutexLock l(&last_heartbeat_mu_);\n+  absl::MutexLock l(last_heartbeat_mu_);\n   last_heartbeat_us_ = Env::Default()->NowMicros();\n }\n \n@@ -161,7 +161,7 @@ absl::Status CoordinationService::TaskState::RecordHeartbeat(\n   if (!status_.ok()) return status_;\n   // Record heartbeat.\n   if (task_incarnation_ == task_incarnation) {\n-    absl::MutexLock l(&last_heartbeat_mu_);\n+    absl::MutexLock l(last_heartbeat_mu_);\n     last_heartbeat_us_ = Env::Default()->NowMicros();\n     return absl::OkStatus();\n   }\n@@ -179,7 +179,7 @@ absl::Status CoordinationService::TaskState::RecordHeartbeat(\n }\n \n int64_t CoordinationService::TaskState::TimeSinceLastHeartbeatMs() {\n-  absl::MutexLock l(&last_heartbeat_mu_);\n+  absl::MutexLock l(last_heartbeat_mu_);\n   return (Env::Default()->NowMicros() - last_heartbeat_us_) / 1000;\n }\n \n@@ -233,7 +233,7 @@ CoordinationService::CoordinationService(\n void CoordinationService::CheckHeartbeatTimeout() {\n   absl::Status status = absl::OkStatus();\n   std::vector<absl::string_view> stale_task_names;\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   for (const auto& [task_name, task_state] : cluster_state_) {\n     // Skip tasks that are not registered or in error state.\n     if (task_state->GetState() != CoordinatedTaskState::TASKSTATE_CONNECTED) {\n@@ -277,7 +277,7 @@ void CoordinationService::CheckHeartbeatTimeout() {\n \n absl::flat_hash_map<std::string, int>\n CoordinationService::GetCountOfOutOfSyncTasksPerBarrier() {\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   absl::flat_hash_map<std::string, absl::flat_hash_set<std::string>>\n       unsynced_tasks_to_barriers;\n   absl::flat_hash_map<std::string, int> out_of_sync_tasks_per_barrier;\n@@ -338,7 +338,7 @@ void CoordinationService::CheckBarrierStatusWithRecoverableTasks() {\n   // be removed from the unsynced_tasks set.\n   auto out_of_sync_tasks_per_barrier = GetCountOfOutOfSyncTasksPerBarrier();\n \n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   // Gather barriers which are ready to pass except for the recoverable tasks\n   // reconnected during the barrier. When the flag\n   // leave_barriers_on_recoverable_agent_restart is set, the recoverable tasks\n@@ -383,7 +383,7 @@ void CoordinationService::CheckBarrierStatusWithRecoverableTasks() {\n void CoordinationService::CheckBarrierTimeout() {\n   absl::flat_hash_map<std::string, BarrierState*> expired_barriers;\n   uint64_t current_time_micros = Env::Default()->NowMicros();\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   // Gather barriers which have timed out.\n   for (absl::string_view barrier_id : ongoing_barriers_) {\n     auto* barrier = &barriers_[barrier_id];\n@@ -427,7 +427,7 @@ void CoordinationService::CheckStaleness() {\n   // Used to store stale tasks and barriers.\n   while (true) {\n     {\n-      absl::MutexLock l(&state_mu_);\n+      absl::MutexLock l(state_mu_);\n       check_staleness_thread_cv_.WaitWithTimeout(&state_mu_, absl::Seconds(1));\n       if (shutting_down_) {\n         return;\n@@ -568,7 +568,7 @@ void CoordinationService::RegisterTaskAsync(const CoordinatedTask& task,\n   const std::string task_name = GetTaskName(task);\n \n   std::string error_message;\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   if (ServiceHasStopped()) {\n     done(MakeCoordinationError(absl::InternalError(absl::StrCat(\n         \"Coordination service has stopped. RegisterTask() from task: \",\n@@ -684,7 +684,7 @@ void CoordinationService::WaitForAllTasks(const CoordinatedTask& task,\n                                           const DeviceInfo& devices,\n                                           StatusCallback done) {\n   {\n-    absl::MutexLock l(&state_mu_);\n+    absl::MutexLock l(state_mu_);\n     if (ServiceHasStopped()) {\n       done(MakeCoordinationError(absl::InternalError(\n           \"Coordination service has stopped. WaitForAllTasks() failed.\")));\n@@ -734,7 +734,7 @@ void CoordinationService::ShutdownTaskAsync(const CoordinatedTask& task,\n   } else {\n     absl::Status status;\n     {\n-      absl::MutexLock l(&state_mu_);\n+      absl::MutexLock l(state_mu_);\n       if (ServiceHasStopped()) {\n         status = MakeCoordinationError(absl::InternalError(\n             \"Coordination service has stopped. ShutdownTaskAsync() failed.\"));\n@@ -748,7 +748,7 @@ void CoordinationService::ShutdownTaskAsync(const CoordinatedTask& task,\n }\n \n absl::Status CoordinationService::ResetTask(const CoordinatedTask& task) {\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   return DisconnectTask(task);\n }\n \n@@ -793,7 +793,7 @@ IncarnationId CoordinationService::GetServiceIncarnation() {\n absl::Status CoordinationService::ReportTaskError(const CoordinatedTask& task,\n                                                   const absl::Status& error) {\n   const std::string task_name = GetTaskName(task);\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   if (ServiceHasStopped()) {\n     return MakeCoordinationError(absl::InternalError(\n         \"Coordination service has stopped. ReportTaskError() failed.\"));\n@@ -831,7 +831,7 @@ std::vector<CoordinatedTaskStateInfo> CoordinationService::GetTaskState(\n   std::vector<CoordinatedTaskStateInfo> states_info;\n   states_info.reserve(tasks.size());\n \n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   for (const auto& task : tasks) {\n     states_info.push_back(\n         CreateTaskStateInfo(task, *cluster_state_[GetTaskName(task)]));\n@@ -867,7 +867,7 @@ void CoordinationService::ClusterStateUpdated() {\n void CoordinationService::WatchJobState(absl::string_view job_name,\n                                         std::optional<int64_t> version_number,\n                                         WatchJobStateCallback callback) {\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   int64_t v = version_number.value_or(-1);\n   CHECK_GE(cluster_state_version_number_, v);\n   if (cluster_state_version_number_ == v) {\n@@ -883,7 +883,7 @@ absl::Status CoordinationService::RecordHeartbeat(const CoordinatedTask& task,\n                                                   IncarnationId incarnation) {\n   const std::string task_name = GetTaskName(task);\n   absl::Status s = absl::OkStatus();\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   if (ServiceHasStopped()) {\n     return MakeCoordinationError(absl::InternalError(absl::StrCat(\n         \"Coordination service has stopped. RecordHeartbeat() from task: \",\n@@ -1113,7 +1113,7 @@ void CoordinationService::PollForErrorAsync(const CoordinatedTask& task,\n   const std::string task_name = GetTaskName(task);\n   VLOG(3) << \"Task \" << task_name << \" invoked PollForErrorAsync().\";\n \n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   if (ServiceHasStopped()) {\n     done(MakeCoordinationError(absl::InternalError(\n         \"PollForError requested after coordination service has shut down.\")));\n@@ -1284,7 +1284,7 @@ void CoordinationService::BarrierAsync(\n     const CoordinatedTask& task,\n     const std::vector<CoordinatedTask>& participating_tasks,\n     BarrierCallback done) {\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   return BarrierAsyncLocked(barrier_id, counter, timeout, task,\n                             participating_tasks, std::move(done));\n };\n@@ -1410,7 +1410,7 @@ absl::Status CoordinationService::CancelBarrier(\n     // completes, which would invalidate the `string_view`.\n     std::string barrier_id, int64_t counter, const CoordinatedTask& task) {\n   std::string barrier_name = BarrierName(barrier_id, counter);\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   if (ServiceHasStopped()) {\n     return MakeBarrierError(\n         absl::InternalError(absl::StrCat(\n@@ -1593,7 +1593,7 @@ void CoordinationService::GetAliveTasksAsync(\n   }\n \n   // Find the corresponding AlivenessState, creating a new one if needed.\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   auto it = std::find_if(aliveness_states_.begin(), aliveness_states_.end(),\n                          [&task_set](const AlivenessState& state) {\n                            return TaskSetEqual(state.tasks, task_set);\n@@ -1832,7 +1832,7 @@ void CoordinationService::DisconnectAllNonRecoverableTasks() {\n }\n \n std::vector<CoordinatedTask> CoordinationService::GetTasksForShutdownBarrier() {\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   if (shutdown_barrier_tasks_.empty()) {\n     for (const auto& [task_name, task_state] : cluster_state_) {\n       if (!task_state->IsRecoverable()) {"
        },
        {
            "sha": "15918e330241a016cec4f6e0d6b6fdb34a1fb404",
            "filename": "third_party/xla/xla/tsl/distributed_runtime/coordination/coordination_service.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/47c3e3b13caa2515072c9bdef1f8b866e972f398/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fcoordination_service.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/47c3e3b13caa2515072c9bdef1f8b866e972f398/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fcoordination_service.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fcoordination_service.h?ref=47c3e3b13caa2515072c9bdef1f8b866e972f398",
            "patch": "@@ -101,7 +101,7 @@ class CoordinationService {\n                       std::unique_ptr<CoordinationClientCache> client_cache);\n \n   ~CoordinationService() {\n-    absl::MutexLock lock(&state_mu_);\n+    absl::MutexLock lock(state_mu_);\n     Stop();\n   }\n "
        },
        {
            "sha": "84536d9d67c00298496a86da1aaf5c73b8de8ff8",
            "filename": "third_party/xla/xla/tsl/distributed_runtime/coordination/coordination_service_agent.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 22,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/47c3e3b13caa2515072c9bdef1f8b866e972f398/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fcoordination_service_agent.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/47c3e3b13caa2515072c9bdef1f8b866e972f398/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fcoordination_service_agent.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fcoordination_service_agent.cc?ref=47c3e3b13caa2515072c9bdef1f8b866e972f398",
            "patch": "@@ -117,7 +117,7 @@ absl::Status CoordinationServiceAgent::Initialize(\n     std::unique_ptr<CoordinationClient> leader_client,\n     StatusCallback error_fn) {\n   enabled_usage_metric->GetCell()->Set(true);\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   if (state_ != CoordinatedTaskState::TASKSTATE_UNINITIALIZED) {\n     return MakeCoordinationError(absl::FailedPreconditionError(\n         \"Coordination service agent has already been initialized.\"));\n@@ -141,23 +141,23 @@ absl::Status CoordinationServiceAgent::Initialize(\n }\n \n bool CoordinationServiceAgent::IsInitialized() {\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   return state_ != CoordinatedTaskState::TASKSTATE_UNINITIALIZED;\n }\n \n bool CoordinationServiceAgent::IsConnected() {\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   return state_ == CoordinatedTaskState::TASKSTATE_CONNECTED;\n }\n \n bool CoordinationServiceAgent::IsError() {\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   return state_ == CoordinatedTaskState::TASKSTATE_ERROR;\n }\n \n void CoordinationServiceAgent::StopHeartbeat() {\n   {\n-    absl::MutexLock l(&shutdown_mu_);\n+    absl::MutexLock l(shutdown_mu_);\n     shutting_down_ = true;\n   }\n   heartbeat_thread_ = nullptr;\n@@ -175,7 +175,7 @@ void CoordinationServiceAgent::ResetCancellationManager() {\n absl::Status CoordinationServiceAgent::Connect() {\n   VLOG(3) << \"Agent has started trying to Connect().\";\n   {\n-    absl::MutexLock l(&state_mu_);\n+    absl::MutexLock l(state_mu_);\n     if (state_ != CoordinatedTaskState::TASKSTATE_DISCONNECTED) {\n       return MakeCoordinationError(absl::FailedPreconditionError(\n           \"Coordination service agent is not in DISCONNECTED state.\"));\n@@ -209,7 +209,7 @@ absl::Status CoordinationServiceAgent::Connect() {\n           if (s.ok()) {\n             leader_incarnation_ = response.leader_incarnation();\n             {\n-              absl::MutexLock l(&state_mu_);\n+              absl::MutexLock l(state_mu_);\n               state_ = CoordinatedTaskState::TASKSTATE_CONNECTED;\n             }\n           }\n@@ -289,7 +289,7 @@ void CoordinationServiceAgent::StartSendingHeartbeats() {\n       // inflight heartbeats sent during shutdown and can be ignored.\n       absl::SleepFor(absl::Seconds(1));\n       {\n-        absl::MutexLock l(&shutdown_mu_);\n+        absl::MutexLock l(shutdown_mu_);\n \n         if (shutting_down_) {\n           return;\n@@ -305,7 +305,7 @@ void CoordinationServiceAgent::StartSendingHeartbeats() {\n     }\n     // Send next heartbeat after an interval.\n     {\n-      absl::MutexLock l(&shutdown_mu_);\n+      absl::MutexLock l(shutdown_mu_);\n       shutdown_mu_.AwaitWithTimeout(absl::Condition(&shutting_down_),\n                                     absl::Milliseconds(heartbeat_interval_ms));\n       if (shutting_down_) {\n@@ -478,7 +478,7 @@ CoordinationServiceAgent::WatchJobState(absl::string_view job_name,\n \n absl::Status CoordinationServiceAgent::ReportError(const absl::Status& error) {\n   {\n-    absl::MutexLock l(&state_mu_);\n+    absl::MutexLock l(state_mu_);\n     if (state_ == CoordinatedTaskState::TASKSTATE_UNINITIALIZED) {\n       return MakeCoordinationError(absl::FailedPreconditionError(\n           \"Coordination service agent must be initialized first before \"\n@@ -525,7 +525,7 @@ absl::Status CoordinationServiceAgent::ShutdownInternal() {\n   absl::Status status = absl::OkStatus();\n   bool is_connected = false;\n   {\n-    absl::MutexLock l(&state_mu_);\n+    absl::MutexLock l(state_mu_);\n     is_connected = state_ == CoordinatedTaskState::TASKSTATE_CONNECTED;\n   }\n   // Disconnect agent from service.\n@@ -574,7 +574,7 @@ absl::Status CoordinationServiceAgent::ShutdownInternal() {\n   StopHeartbeat();\n   StopErrorPolling();\n   {\n-    absl::MutexLock l(&state_mu_);\n+    absl::MutexLock l(state_mu_);\n     if (status.ok() && state_ == CoordinatedTaskState::TASKSTATE_ERROR) {\n       const std::string status_message = absl::StrCat(\n           \"Shutdown() was called while coordination agent is in error state, \"\n@@ -600,7 +600,7 @@ absl::Status CoordinationServiceAgent::ShutdownInternal() {\n \n absl::Status CoordinationServiceAgent::Reset() {\n   {\n-    absl::MutexLock l(&state_mu_);\n+    absl::MutexLock l(state_mu_);\n     if (state_ != CoordinatedTaskState::TASKSTATE_ERROR) {\n       return MakeCoordinationError(absl::FailedPreconditionError(\n           \"Reset() failed: coordination service agent is not in ERROR state.\"));\n@@ -630,11 +630,11 @@ absl::Status CoordinationServiceAgent::Reset() {\n   StopErrorPolling();\n   ResetCancellationManager();\n   {\n-    absl::MutexLock l(&state_mu_);\n+    absl::MutexLock l(state_mu_);\n     state_ = CoordinatedTaskState::TASKSTATE_DISCONNECTED;\n   }\n   {\n-    absl::MutexLock l(&shutdown_mu_);\n+    absl::MutexLock l(shutdown_mu_);\n     shutting_down_ = false;\n   }\n \n@@ -861,7 +861,7 @@ absl::Status CoordinationServiceAgent::StopWatchKey(absl::string_view key) {\n \n void CoordinationServiceAgent::SetError(const absl::Status& error) {\n   assert(!error.ok());\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   if (state_ == CoordinatedTaskState::TASKSTATE_ERROR) return;\n   absl::Status trimmed_error = TrimCoordinationErrorMessage(error);\n \n@@ -901,7 +901,7 @@ void CoordinationServiceAgent::WaitAtBarrierAsync(\n   auto request = std::make_shared<BarrierRequest>();\n   auto response = std::make_shared<BarrierResponse>();\n   {\n-    absl::MutexLock l(&state_mu_);\n+    absl::MutexLock l(state_mu_);\n \n     // Prevent multiple concurrent invocations with the same id.\n     // This usually indicates a bug in the user code. They should wait till the\n@@ -944,7 +944,7 @@ void CoordinationServiceAgent::WaitAtBarrierAsync(\n       call_opts.get(), request.get(), response.get(),\n       [call_opts, request, response, done = std::move(done), barrier_id, this,\n        &cm = cancellation_manager_, token](const absl::Status& s) mutable {\n-        absl::MutexLock l(&state_mu_);\n+        absl::MutexLock l(state_mu_);\n         // Allow the same barrier id to be invoked after this counter's\n         // completion.\n         ongoing_barriers_.erase(barrier_id);\n@@ -986,7 +986,7 @@ void CoordinationServiceAgent::CancelBarrierAsync(absl::string_view barrier_id,\n     done(agent_running_status);\n     return;\n   }\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   if (!barrier_counter_.contains(barrier_id)) {\n     done(MakeCoordinationError(absl::FailedPreconditionError(absl::StrCat(\n         \"Tried to cancel non-existent barrier \", barrier_id, \".\"))));\n@@ -1042,7 +1042,7 @@ CoordinationServiceAgent::GetAliveTasks(\n     return status;\n   }\n   {\n-    absl::MutexLock lock(&incarnations_mu_);\n+    absl::MutexLock lock(incarnations_mu_);\n     for (int i = 0; i < response->alive_tasks_size(); ++i) {\n       incarnations_[response->alive_tasks(i).task_id()] =\n           response->incarnations(i);\n@@ -1054,7 +1054,7 @@ CoordinationServiceAgent::GetAliveTasks(\n \n absl::StatusOr<std::vector<IncarnationId>>\n CoordinationServiceAgent::Incarnations(absl::Span<const int> tasks) const {\n-  absl::MutexLock lock(&incarnations_mu_);\n+  absl::MutexLock lock(incarnations_mu_);\n   std::vector<IncarnationId> incarnations;\n   for (const auto& task_id : tasks) {\n     auto it = incarnations_.find(task_id);\n@@ -1070,7 +1070,7 @@ CoordinationServiceAgent::Incarnations(absl::Span<const int> tasks) const {\n // Returns an error if agent is not running.\n absl::Status CoordinationServiceAgent::ValidateRunningAgent(\n     bool allow_disconnected) {\n-  absl::MutexLock l(&state_mu_);\n+  absl::MutexLock l(state_mu_);\n   switch (state_) {\n     case CoordinatedTaskState::TASKSTATE_CONNECTED:\n       return absl::OkStatus();"
        },
        {
            "sha": "2b07f5c2279b8fee2c0f11bbcae9dbe23f5cddec",
            "filename": "third_party/xla/xla/tsl/distributed_runtime/coordination/coordination_service_recoverable_job_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/47c3e3b13caa2515072c9bdef1f8b866e972f398/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fcoordination_service_recoverable_job_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/47c3e3b13caa2515072c9bdef1f8b866e972f398/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fcoordination_service_recoverable_job_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fcoordination_service_recoverable_job_test.cc?ref=47c3e3b13caa2515072c9bdef1f8b866e972f398",
            "patch": "@@ -51,12 +51,12 @@ constexpr char kServiceLeader[] = \"/job:parameter_server/replica:0/task:0\";\n class TestCoordinationClientCache : public CoordinationClientCache {\n  public:\n   void AddTask(const std::string& target, CoordinationClient* client) {\n-    absl::MutexLock l(&clients_mu_);\n+    absl::MutexLock l(clients_mu_);\n     clients_.emplace(target, client);\n   }\n \n   CoordinationClient* GetClient(const std::string& target) override {\n-    absl::MutexLock l(&clients_mu_);\n+    absl::MutexLock l(clients_mu_);\n     if (auto it = clients_.find(target); it != clients_.end()) {\n       return it->second;\n     }"
        },
        {
            "sha": "63aca3bfdce7d84c0826fc6d549816b3d93e5fb3",
            "filename": "third_party/xla/xla/tsl/distributed_runtime/coordination/coordination_service_rpc_handler.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/47c3e3b13caa2515072c9bdef1f8b866e972f398/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fcoordination_service_rpc_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/47c3e3b13caa2515072c9bdef1f8b866e972f398/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fcoordination_service_rpc_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fcoordination_service_rpc_handler.cc?ref=47c3e3b13caa2515072c9bdef1f8b866e972f398",
            "patch": "@@ -44,20 +44,20 @@ using tensorflow::KeyValueEntry;\n \n void CoordinationServiceRpcHandler::SetAgentInstance(\n     CoordinationServiceAgent* agent) {\n-  absl::MutexLock l(&mu_);\n+  absl::MutexLock l(mu_);\n   agent_ = agent;\n }\n \n void CoordinationServiceRpcHandler::SetServiceInstance(\n     CoordinationService* service) {\n-  absl::MutexLock l(&mu_);\n+  absl::MutexLock l(mu_);\n   service_ = service;\n }\n \n void CoordinationServiceRpcHandler::RegisterTaskAsync(\n     const tensorflow::RegisterTaskRequest* request,\n     tensorflow::RegisterTaskResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (service_ == nullptr) {\n     done(MakeCoordinationError(\n         absl::InternalError(\"Coordination service is not enabled.\")));\n@@ -73,7 +73,7 @@ void CoordinationServiceRpcHandler::RegisterTaskAsync(\n void CoordinationServiceRpcHandler::HeartbeatAsync(\n     const tensorflow::HeartbeatRequest* request,\n     tensorflow::HeartbeatResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (service_ == nullptr) {\n     done(MakeCoordinationError(\n         absl::InternalError(\"Coordination service is not enabled.\")));\n@@ -94,7 +94,7 @@ void CoordinationServiceRpcHandler::HeartbeatAsync(\n void CoordinationServiceRpcHandler::WaitForAllTasksAsync(\n     const tensorflow::WaitForAllTasksRequest* request,\n     tensorflow::WaitForAllTasksResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (service_ == nullptr) {\n     done(MakeCoordinationError(\n         absl::InternalError(\"Coordination service is not enabled.\")));\n@@ -114,7 +114,7 @@ void CoordinationServiceRpcHandler::WaitForAllTasksAsync(\n void CoordinationServiceRpcHandler::ShutdownTaskAsync(\n     const tensorflow::ShutdownTaskRequest* request,\n     tensorflow::ShutdownTaskResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (service_ == nullptr) {\n     done(MakeCoordinationError(\n         absl::InternalError(\"Coordination service is not enabled.\")));\n@@ -127,7 +127,7 @@ void CoordinationServiceRpcHandler::ShutdownTaskAsync(\n void CoordinationServiceRpcHandler::ResetTaskAsync(\n     const tensorflow::ResetTaskRequest* request,\n     tensorflow::ResetTaskResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (service_ == nullptr) {\n     done(MakeCoordinationError(\n         absl::InternalError(\"Coordination service is not enabled.\")));\n@@ -139,7 +139,7 @@ void CoordinationServiceRpcHandler::ResetTaskAsync(\n void CoordinationServiceRpcHandler::ReportErrorToTaskAsync(\n     const tensorflow::ReportErrorToTaskRequest* request,\n     tensorflow::ReportErrorToTaskResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (agent_ == nullptr) {\n     done(MakeCoordinationError(absl::InternalError(\n         \"CoordinationServiceAgent is uninitialized or has already shutdown.\")));\n@@ -160,7 +160,7 @@ void CoordinationServiceRpcHandler::ReportErrorToTaskAsync(\n void CoordinationServiceRpcHandler::ReportErrorToServiceAsync(\n     const tensorflow::ReportErrorToServiceRequest* request,\n     tensorflow::ReportErrorToServiceResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (service_ == nullptr) {\n     done(MakeCoordinationError(\n         absl::InternalError(\"Coordination service is not enabled.\")));\n@@ -178,7 +178,7 @@ void CoordinationServiceRpcHandler::ReportErrorToServiceAsync(\n void CoordinationServiceRpcHandler::GetTaskStateAsync(\n     const tensorflow::GetTaskStateRequest* request,\n     tensorflow::GetTaskStateResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (service_ == nullptr) {\n     done(MakeCoordinationError(\n         absl::InternalError(\"Coordination service is not enabled.\")));\n@@ -194,7 +194,7 @@ void CoordinationServiceRpcHandler::GetTaskStateAsync(\n void CoordinationServiceRpcHandler::WatchJobStateAsync(\n     const tensorflow::WatchJobStateRequest* request,\n     tensorflow::WatchJobStateResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (service_ == nullptr) {\n     done(MakeCoordinationError(\n         absl::InternalError(\"Coordination service is not enabled.\")));\n@@ -219,7 +219,7 @@ void CoordinationServiceRpcHandler::WatchJobStateAsync(\n void CoordinationServiceRpcHandler::InsertKeyValueAsync(\n     const tensorflow::InsertKeyValueRequest* request,\n     tensorflow::InsertKeyValueResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (service_ == nullptr) {\n     done(MakeCoordinationError(\n         absl::InternalError(\"Coordination service is not enabled.\")));\n@@ -232,7 +232,7 @@ void CoordinationServiceRpcHandler::InsertKeyValueAsync(\n void CoordinationServiceRpcHandler::GetKeyValueAsync(\n     const tensorflow::GetKeyValueRequest* request,\n     tensorflow::GetKeyValueResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (service_ == nullptr) {\n     done(MakeCoordinationError(\n         absl::InternalError(\"Coordination service is not enabled.\")));\n@@ -254,7 +254,7 @@ void CoordinationServiceRpcHandler::GetKeyValueAsync(\n void CoordinationServiceRpcHandler::TryGetKeyValueAsync(\n     const tensorflow::TryGetKeyValueRequest* request,\n     tensorflow::TryGetKeyValueResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (service_ == nullptr) {\n     done(MakeCoordinationError(\n         absl::InternalError(\"Coordination service is not enabled.\")));\n@@ -273,7 +273,7 @@ void CoordinationServiceRpcHandler::TryGetKeyValueAsync(\n void CoordinationServiceRpcHandler::IncrementKeyValueAsync(\n     const tensorflow::IncrementKeyValueRequest* request,\n     tensorflow::IncrementKeyValueResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (service_ == nullptr) {\n     done(MakeCoordinationError(\n         absl::InternalError(\"Coordination service is not enabled.\")));\n@@ -293,7 +293,7 @@ void CoordinationServiceRpcHandler::IncrementKeyValueAsync(\n void CoordinationServiceRpcHandler::GetKeyValueDirAsync(\n     const tensorflow::GetKeyValueDirRequest* request,\n     tensorflow::GetKeyValueDirResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (service_ == nullptr) {\n     done(MakeCoordinationError(\n         absl::InternalError(\"Coordination service is not enabled.\")));\n@@ -309,7 +309,7 @@ void CoordinationServiceRpcHandler::GetKeyValueDirAsync(\n void CoordinationServiceRpcHandler::DeleteKeyValueAsync(\n     const tensorflow::DeleteKeyValueRequest* request,\n     tensorflow::DeleteKeyValueResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (service_ == nullptr) {\n     done(MakeCoordinationError(\n         absl::InternalError(\"Coordination service is not enabled.\")));\n@@ -321,7 +321,7 @@ void CoordinationServiceRpcHandler::DeleteKeyValueAsync(\n void CoordinationServiceRpcHandler::BarrierAsync(\n     const tensorflow::BarrierRequest* request,\n     tensorflow::BarrierResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (service_ == nullptr) {\n     done(MakeCoordinationError(\n         absl::InternalError(\"Coordination service is not enabled.\")));\n@@ -342,7 +342,7 @@ void CoordinationServiceRpcHandler::BarrierAsync(\n void CoordinationServiceRpcHandler::CancelBarrierAsync(\n     const tensorflow::CancelBarrierRequest* request,\n     tensorflow::CancelBarrierResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (service_ == nullptr) {\n     done(MakeCoordinationError(\n         absl::InternalError(\"Coordination service is not enabled.\")));\n@@ -355,7 +355,7 @@ void CoordinationServiceRpcHandler::CancelBarrierAsync(\n void CoordinationServiceRpcHandler::GetAliveTasksAsync(\n     const tensorflow::GetAliveTasksRequest* request,\n     tensorflow::GetAliveTasksResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (service_ == nullptr) {\n     done(MakeCoordinationError(\n         absl::InternalError(\"Coordination service is not enabled.\")));\n@@ -382,7 +382,7 @@ void CoordinationServiceRpcHandler::GetAliveTasksAsync(\n void CoordinationServiceRpcHandler::PollForErrorAsync(\n     const tensorflow::PollForErrorRequest* request,\n     tensorflow::PollForErrorResponse* response, StatusCallback done) {\n-  absl::ReaderMutexLock l(&mu_);\n+  absl::ReaderMutexLock l(mu_);\n   if (service_ == nullptr) {\n     done(MakeCoordinationError(\n         absl::InternalError(\"Coordination service is not enabled.\")));"
        },
        {
            "sha": "65258f3a4419cfd66351022c5422fda53340515e",
            "filename": "third_party/xla/xla/tsl/distributed_runtime/coordination/coordination_service_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/47c3e3b13caa2515072c9bdef1f8b866e972f398/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fcoordination_service_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/47c3e3b13caa2515072c9bdef1f8b866e972f398/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fcoordination_service_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fcoordination_service_test.cc?ref=47c3e3b13caa2515072c9bdef1f8b866e972f398",
            "patch": "@@ -95,7 +95,7 @@ class TestCoordinationClient : public CoordinationClient {\n   TestCoordinationClient() = default;\n \n   absl::Status GetStatus() {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     return status_;\n   }\n \n@@ -109,7 +109,7 @@ class TestCoordinationClient : public CoordinationClient {\n                               const ReportErrorToTaskRequest* request,\n                               ReportErrorToTaskResponse* response,\n                               StatusCallback done) override {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     status_ = absl::Status(static_cast<absl::StatusCode>(request->error_code()),\n                            request->error_message());\n     done(absl::OkStatus());"
        },
        {
            "sha": "6598becd9ef47268ac5757b421945e0ad002de64",
            "filename": "third_party/xla/xla/tsl/distributed_runtime/coordination/key_value_store.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/47c3e3b13caa2515072c9bdef1f8b866e972f398/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fkey_value_store.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/47c3e3b13caa2515072c9bdef1f8b866e972f398/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fkey_value_store.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fdistributed_runtime%2Fcoordination%2Fkey_value_store.cc?ref=47c3e3b13caa2515072c9bdef1f8b866e972f398",
            "patch": "@@ -33,7 +33,7 @@ limitations under the License.\n namespace tsl {\n \n KeyValueStore::~KeyValueStore() {\n-  absl::MutexLock l(&mu_);\n+  absl::MutexLock l(mu_);\n \n   absl::Status cancelled = absl::CancelledError(\"KeyValueStore destructed\");\n   for (auto& [key, callbacks] : callbacks_) {\n@@ -45,7 +45,7 @@ KeyValueStore::~KeyValueStore() {\n \n absl::Status KeyValueStore::Put(absl::string_view key, absl::string_view value,\n                                 bool allow_overwrite) {\n-  absl::MutexLock l(&mu_);\n+  absl::MutexLock l(mu_);\n \n   if (allow_overwrite) {\n     data_[key] = value;\n@@ -63,7 +63,7 @@ absl::Status KeyValueStore::Put(absl::string_view key, absl::string_view value,\n }\n \n std::optional<std::string> KeyValueStore::Get(absl::string_view key) {\n-  absl::MutexLock l(&mu_);\n+  absl::MutexLock l(mu_);\n   auto it = data_.find(key);\n   if (it == data_.end()) {\n     return std::nullopt;\n@@ -73,7 +73,7 @@ std::optional<std::string> KeyValueStore::Get(absl::string_view key) {\n \n absl::StatusOr<std::string> KeyValueStore::IncrementBy(absl::string_view key,\n                                                        int64_t increment) {\n-  absl::MutexLock l(&mu_);\n+  absl::MutexLock l(mu_);\n   auto [it, inserted] = data_.try_emplace(key, \"0\");\n   int val;\n   if (!absl::SimpleAtoi(it->second, &val)) {\n@@ -87,7 +87,7 @@ absl::StatusOr<std::string> KeyValueStore::IncrementBy(absl::string_view key,\n \n std::vector<tensorflow::KeyValueEntry> KeyValueStore::GetPrefix(\n     absl::string_view prefix) {\n-  absl::MutexLock l(&mu_);\n+  absl::MutexLock l(mu_);\n \n   std::vector<tensorflow::KeyValueEntry> entries;\n   for (auto it = data_.lower_bound(prefix); it != data_.end(); ++it) {\n@@ -104,12 +104,12 @@ std::vector<tensorflow::KeyValueEntry> KeyValueStore::GetPrefix(\n }\n \n void KeyValueStore::Delete(absl::string_view key) {\n-  absl::MutexLock l(&mu_);\n+  absl::MutexLock l(mu_);\n   data_.erase(key);\n }\n \n void KeyValueStore::DeletePrefix(absl::string_view prefix) {\n-  absl::MutexLock l(&mu_);\n+  absl::MutexLock l(mu_);\n \n   auto begin = data_.lower_bound(prefix);\n   auto it = begin;\n@@ -124,7 +124,7 @@ void KeyValueStore::DeletePrefix(absl::string_view prefix) {\n \n void KeyValueStore::AddCallbackForKey(absl::string_view key,\n                                       Callback callback) {\n-  absl::MutexLock l(&mu_);\n+  absl::MutexLock l(mu_);\n \n   if (auto it = data_.find(key); it != data_.end()) {\n     callback(it->second);"
        }
    ],
    "stats": {
        "total": 162,
        "additions": 81,
        "deletions": 81
    }
}