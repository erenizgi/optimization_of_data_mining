{
    "author": "mrguenther",
    "message": "Integrate StableHLO at openxla/stablehlo@3f27c53c\n\nPiperOrigin-RevId: 829497796",
    "sha": "151894fa84cb54bf11b58fe7c4fefc56fa831a4b",
    "files": [
        {
            "sha": "7c73eeb9cdd7024437ce9cf0e69f1b65677e92f1",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 36,
            "deletions": 632,
            "changes": 668,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/151894fa84cb54bf11b58fe7c4fefc56fa831a4b/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/151894fa84cb54bf11b58fe7c4fefc56fa831a4b/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=151894fa84cb54bf11b58fe7c4fefc56fa831a4b",
            "patch": "@@ -1,130 +1,42 @@\n-diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir b/stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir\n---- stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir\n-+++ stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir\n-@@ -768,7 +768,7 @@\n- // CHECK-PRIMITIVE: %[[MAP:.+]] = linalg.map\n- // CHECK-PRIMITIVE-SAME: ins(%[[ARG0]], %[[ARG1]]\n- // CHECK-PRIMITIVE-SAME: outs(%[[INIT]] : tensor<?xi1>)\n--// CHECK-PRIMITIVE-NEXT: (%[[A:.+]]: complex<f32>, %[[B:.+]]: complex<f32>) {\n-+// CHECK-PRIMITIVE-NEXT: (%[[A:.+]]: complex<f32>, %[[B:.+]]: complex<f32>, %{{.+}}: i1) {\n- // CHECK-PRIMITIVE: %[[RE1:.+]] = complex.re %[[A]] : complex<f32>\n- // CHECK-PRIMITIVE: %[[RE2:.+]] = complex.re %[[B]] : complex<f32>\n- // CHECK-PRIMITIVE: %[[CMP:.+]] = arith.cmpf oeq, %[[RE1]], %[[RE2]] : f32\n-diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir b/stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir\n---- stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir\n-+++ stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir\n-@@ -714,7 +714,7 @@\n- // CHECK-PRIMITIVE: linalg.map\n- // CHECK-PRIMITIVE-SAME: ins(\n- // CHECK-PRIMITIVE-SAME: outs(\n--// CHECK-PRIMITIVE-NEXT: (%[[LHS_IN:[a-zA-Z0-9]*]]: bf16, %[[RHS_IN:.*]]: bf16) {\n-+// CHECK-PRIMITIVE-NEXT: (%[[LHS_IN:[a-zA-Z0-9]*]]: bf16, %[[RHS_IN:.*]]: bf16, %[[RESULT_OUT:.*]]: i1) {\n- // CHECK-PRIMITIVE-NEXT:   %[[LHS_INT:.*]] = arith.bitcast %[[LHS_IN]] : bf16 to i16\n- // CHECK-PRIMITIVE-NEXT:   %[[LHS_CMP:.*]] = arith.cmpi slt, %[[LHS_INT]], %[[C0]] : i16\n- // CHECK-PRIMITIVE-NEXT:   %[[LHS_SUB:.*]] = arith.subi %[[C32767]], %[[LHS_INT]] : i16\n-@@ -937,7 +937,7 @@\n- // CHECK-PRIMITIVE-SAME:   ins(%[[LHS]], %[[RHS]] : tensor<2x?xf32>, tensor<2x?xf32>)\n- // CHECK-PRIMITIVE-SAME:   outs(%[[DST]] : tensor<2x?xf32>)\n- // CHECK-PRIMITIVE-SAME:   {someattr}\n--// CHECK-PRIMITIVE:      (%[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32) {\n-+// CHECK-PRIMITIVE:      (%[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32, %[[RESULT_OUT:.*]]: f32) {\n- // CHECK-PRIMITIVE:        %[[RES:.*]] = arith.select %[[PRED_ELEM]], %[[LHS_]], %[[RHS_]] : f32\n- // CHECK-PRIMITIVE:        linalg.yield %[[RES]]\n- \n-@@ -978,7 +978,7 @@\n- // CHECK-PRIMITIVE-SAME:   ins(%[[LHS]], %[[RHS]] : tensor<2x?xf32>, tensor<2x?xf32>)\n- // CHECK-PRIMITIVE-SAME:   outs(%[[DST]] : tensor<2x?xf32>)\n- // CHECK-PRIMITIVE-SAME:   {someattr}\n--// CHECK-PRIMITIVE:      (%[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32) {\n-+// CHECK-PRIMITIVE:      (%[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32, %[[RESULT_OUT:.*]]: f32) {\n- // CHECK-PRIMITIVE:        linalg.yield %[[LHS_]]\n- \n- // -----\n-@@ -1416,7 +1416,7 @@\n- \n- // CHECK-PRIMITIVE: %[[INIT:.*]] = tensor.empty\n- // CHECK-PRIMITIVE: %[[RESULT:.*]] = linalg.map ins(%[[LB]], %[[X]], %[[UB]] : tensor<4xf32>, tensor<4xf32>, tensor<4xf32>) outs(%[[INIT]] : tensor<4xf32>)\n--// CHECK-PRIMITIVE: (%[[SCALAR_LB:.*]]: f32, %[[SCALAR_X:.*]]: f32, %[[SCALAR_UB:.*]]: f32)\n-+// CHECK-PRIMITIVE: (%[[SCALAR_LB:.*]]: f32, %[[SCALAR_X:.*]]: f32, %[[SCALAR_UB:.*]]: f32, %[[RESULT_OUT:.*]]: f32)\n- // CHECK-PRIMITIVE:   %[[MAX:.*]] = arith.maximumf %[[SCALAR_LB]], %[[SCALAR_X]] : f32\n- // CHECK-PRIMITIVE:   %[[MIN:.*]] = arith.minimumf %[[MAX]], %[[SCALAR_UB]] : f32\n- // CHECK-PRIMITIVE:   linalg.yield %[[MIN]]\n-@@ -1478,7 +1478,7 @@\n- // CHECK-PRIMITIVE-DAG: %[[SCALAR_LB:.*]] = tensor.extract %[[LB]]\n- // CHECK-PRIMITIVE-DAG: %[[SCALAR_UB:.*]] = tensor.extract %[[UB]]\n- // CHECK-PRIMITIVE: %[[RESULT:.*]] = linalg.map ins(%[[X]] : tensor<?xf32>) outs(%[[INIT]] : tensor<?xf32>)\n--// CHECK-PRIMITIVE: (%[[SCALAR_X:.*]]: f32)\n-+// CHECK-PRIMITIVE: (%[[SCALAR_X:.*]]: f32, %[[RESULT_OUT:.*]]: f32)\n- // CHECK-PRIMITIVE:   %[[MAX:.*]] = arith.maximumf %[[SCALAR_LB]], %[[SCALAR_X]] : f32\n- // CHECK-PRIMITIVE:   %[[MIN:.*]] = arith.minimumf %[[MAX]], %[[SCALAR_UB]] : f32\n- // CHECK-PRIMITIVE:   linalg.yield %[[MIN]]\n-@@ -1554,7 +1554,7 @@\n-   // CHECK:   linalg.yield %[[V_NOT]] : i32\n-   // CHECK-PRIMITIVE: %[[CST_N1:.+]] = arith.constant -1 : i32\n-   // CHECK-PRIMITIVE: linalg.map\n--  // CHECK-PRIMITIVE:   (%[[IN:.+]]: i32)\n-+  // CHECK-PRIMITIVE:   (%[[IN:.+]]: i32, %[[RESULT_OUT:.+]]: i32)\n-   // CHECK-PRIMITIVE:   %[[V_NOT:.+]] = arith.xori %[[IN]], %[[CST_N1]] : i32\n-   // CHECK-PRIMITIVE:   linalg.yield %[[V_NOT]] : i32\n-   %0 = \"stablehlo.not\"(%arg) : (tensor<2x2xi32>) -> tensor<2x2xi32>\n-diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp\n---- stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp\n-+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp\n-@@ -1748,6 +1748,12 @@\n- \n-     rewriter.applySignatureConversion(&region.front(), signatureConverter,\n-                                       getTypeConverter());\n-+    auto& blocks = linalgOp.getMapper().getBlocks();\n-+    if (blocks.empty()) {\n-+      return rewriter.notifyMatchFailure(op, \"expected at least one block\");\n-+    }\n-+    blocks.front().addArgument(resultType.getElementType(), loc);\n-+\n-     auto result = rewriter.createOrFold<tensor::CastOp>(loc, resultType,\n-                                                         linalgOp.getResults());\n-     rewriter.replaceOp(op, result);\n-diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp\n---- stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp\n-+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp\n-@@ -33,6 +33,7 @@\n- \n- template <typename OpTy>\n- struct ScalarHloToFuncPatterns final : OpConversionPattern<OpTy> {\n-+  // NOLINTNEXTLINE(clang-diagnostic-shadow-field)\n-   ScalarHloToFuncPatterns(TypeConverter& typeConverter, MLIRContext* context,\n-                           PatternBenefit benefit = 1)\n-       : OpConversionPattern<OpTy>(typeConverter, context, benefit) {}\n-@@ -51,6 +52,7 @@\n- template <typename OpTy>\n- struct ScalarHloToArithmeticPattern final : OpConversionPattern<OpTy> {\n-   ScalarHloToArithmeticPattern(\n-+      // NOLINTNEXTLINE(clang-diagnostic-shadow-field)\n-       TypeConverter& typeConverter, MLIRContext* context,\n-       llvm::function_ref<bool(Operation*)> filterFn = nullptr,\n-       PatternBenefit benefit = 1)\n-diff --ruN a/stablehlo/stablehlo/dialect/Base.td b/stablehlo/stablehlo/dialect/Base.td\n---- stablehlo/stablehlo/dialect/Base.td\n-+++ stablehlo/stablehlo/dialect/Base.td\n-@@ -152,7 +152,7 @@\n-     AnyTypeOf<[HLO_PerAxisQuantizedSignedInt, HLO_PerAxisQuantizedUnsignedInt], \"per-axis integer quantized\">;\n- \n- // Token type.\n--def HLO_Token : Type<CPred<\"::llvm::isa<::mlir::stablehlo::TokenType>($_self)\">, \"token\">;\n-+def HLO_Token : Type<CPred<\"::llvm::isa<TokenType>($_self)\">, \"token\">;\n- \n- // Any integer tensor types\n- def HLO_IntTensor : RankedTensorOf<[HLO_Int]>;\n diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/dialect/StablehloOps.cpp\n --- stablehlo/stablehlo/dialect/StablehloOps.cpp\n +++ stablehlo/stablehlo/dialect/StablehloOps.cpp\n-@@ -3164,6 +3164,7 @@\n- using mlir::hlo::printVariadicOperandWithAttribute;\n- using mlir::hlo::printVariadicSameOperandsAndResultType;\n- \n-+using mlir::stablehlo::TokenType;\n- #define GET_OP_CLASSES\n- #include \"stablehlo/dialect/StablehloOps.cpp.inc\"\n- \n+@@ -3275,12 +3275,12 @@\n+ // Entry point for Attribute printing, TableGen generated code will handle the\n+ // dispatch to the individual classes.\n+ void StablehloDialect::printAttribute(Attribute attr,\n+-                                      DialectAsmPrinter& os) const {\n++                                      DialectAsmPrinter& printer) const {\n+   if (auto type_extensions = dyn_cast<TypeExtensionsAttr>(attr)) {\n+-    hlo::printTypeExtensions(cast<hlo::BoundedAttrInterface>(attr), os);\n++    hlo::printTypeExtensions(cast<hlo::BoundedAttrInterface>(attr), printer);\n+     return;\n+   }\n+-  LogicalResult result = generatedAttributePrinter(attr, os);\n++  LogicalResult result = generatedAttributePrinter(attr, printer);\n+   (void)result;\n+   assert(succeeded(result));\n+ }\n+diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.h b/stablehlo/stablehlo/dialect/StablehloOps.h\n+--- stablehlo/stablehlo/dialect/StablehloOps.h\n++++ stablehlo/stablehlo/dialect/StablehloOps.h\n+@@ -93,13 +93,14 @@\n+   Type parseType(DialectAsmParser& parser) const override;\n+ \n+   // Prints a type registered to this dialect.\n+-  void printType(Type type, DialectAsmPrinter& os) const override;\n++  void printType(Type type, DialectAsmPrinter& printer) const override;\n+ \n+   // Parses an attribute registered to this dialect.\n+   Attribute parseAttribute(DialectAsmParser& parser, Type type) const override;\n+ \n+   // Prints an attribute registered to this dialect.\n+-  void printAttribute(Attribute attr, DialectAsmPrinter& os) const override;\n++  void printAttribute(Attribute attr,\n++                      DialectAsmPrinter& printer) const override;\n+ \n+   // Get the set dialect version.\n+   std::optional<StablehloDialectVersion> getVersion() const;\n diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp\n --- stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp\n +++ stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp\n@@ -149,71 +61,6 @@ diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp b/\n  #include \"llvm/Support/raw_ostream.h\"\n  #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n  #include \"mlir/IR/BuiltinOps.h\"\n-diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp\n---- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp\n-+++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp\n-@@ -29,9 +29,35 @@\n- #include \"stablehlo/dialect/TypeInference.h\"\n- #include \"stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h\"\n- #include \"stablehlo/integrations/cpp/builder/MlirBuilder.h\"\n-+#include \"third_party/llvm/llvm-project/mlir/include/mlir/IR/Attributes.h\"\n- \n- namespace mlir {\n- namespace stablehlo {\n-+\n-+///////////////\n-+// Dialect Helpers\n-+///////////////\n-+\n-+MlirOp AttachFrontendAttribute(MlirBuilder& builder, MlirOp op, StringRef name,\n-+                               Attribute value) {\n-+  constexpr char kFrontendAttrName[] = \"mhlo.frontend_attributes\";\n-+  Operation* mlirOp = unwrap(op).getDefiningOp();\n-+  SmallVector<NamedAttribute> attrs;\n-+  DictionaryAttr frontendAttr =\n-+      mlirOp->getAttrOfType<DictionaryAttr>(kFrontendAttrName);\n-+  if (frontendAttr) {\n-+    for (NamedAttribute attr : frontendAttr.getValue()) {\n-+      // Populate all non-conflicting names.\n-+      if (attr.getName() != name) {\n-+        attrs.push_back(attr);\n-+      }\n-+    }\n-+  }\n-+  attrs.emplace_back(name, value);\n-+  mlirOp->setAttr(kFrontendAttrName,\n-+                  DictionaryAttr::get(&builder.getContext(), attrs));\n-+  return op;\n-+}\n- \n- /////////////////\n- // MANUAL APIs\n-diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h\n---- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h\n-+++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h\n-@@ -27,9 +27,22 @@\n- #include \"stablehlo/dialect/StablehloOps.h\"\n- #include \"stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h\"\n- #include \"stablehlo/integrations/cpp/builder/MlirBuilder.h\"\n-+#include \"third_party/llvm/llvm-project/mlir/include/mlir/IR/Attributes.h\"\n- \n- namespace mlir {\n- namespace stablehlo {\n-+\n-+///////////////\n-+// Dialect Helpers\n-+///////////////\n-+\n-+// Appends or overwrites an entry in the `mhlo.frontend_attributes` attribute\n-+//\n-+// of the given op.\n-+// Ex:\n-+//   stablehlo.abs %0 { mhlo.frontend_attributes = { \"foo\" = 123 } }\n-+MlirOp AttachFrontendAttribute(MlirBuilder& builder, MlirOp op, StringRef name,\n-+                               Attribute value);\n- \n- /////////////////\n- // MANUAL APIs\n diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp\n --- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp\n +++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp\n@@ -226,447 +73,4 @@ diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.c\n  #include \"mlir/IR/BuiltinAttributes.h\"\n  #include \"mlir/IR/BuiltinOps.h\"\n  #include \"mlir/IR/DialectRegistry.h\"\n-@@ -1592,5 +1592,57 @@\n-   EXPECT_EQ(expected, debugString(*module));\n- }\n- \n-+TEST(MlirBuilderTest, FrontendAttributesAppend) {\n-+  std::string expected = R\"mlir(module {\n-+  func.func @main(%arg0: tensor<2xf32>) -> tensor<2xf32> {\n-+    %0 = stablehlo.exponential %arg0 {mhlo.frontend_attributes = {bar = \"hello\", foo = 123 : i32}} : tensor<2xf32>\n-+    return %0 : tensor<2xf32>\n-+  }\n-+})mlir\";\n-+\n-+  StablehloModuleBuilder mb;\n-+  {\n-+    Location funcLoc = fileLineColLoc(mb->getContext(), \"main.mlir\", 1, 1);\n-+    func::FunctionBuilder fb(mb.get(), \"main\", funcLoc);\n-+    auto type = makeTensorType(fb.getContext(), {2}, ElementType::F32);\n-+    auto arg0 = func::Argument(fb, type);\n-+    auto exp = Exp(arg0);\n-+    stablehlo::AttachFrontendAttribute(\n-+        fb, exp, \"foo\", fb.getOpBuilder().getI32IntegerAttr(123));\n-+    stablehlo::AttachFrontendAttribute(\n-+        fb, exp, \"bar\", fb.getOpBuilder().getStringAttr(\"hello\"));\n-+    func::Return(fb, {exp});\n-+  }\n-+\n-+  OwningOpRef<ModuleOp> module = mb->build();\n-+  EXPECT_EQ(expected, debugString(*module));\n-+}\n-+\n-+TEST(MlirBuilderTest, FrontendAttributesOverwrite) {\n-+  std::string expected = R\"mlir(module {\n-+  func.func @main(%arg0: tensor<2xf32>) -> tensor<2xf32> {\n-+    %0 = stablehlo.exponential %arg0 {mhlo.frontend_attributes = {foo = 456 : i32}} : tensor<2xf32>\n-+    return %0 : tensor<2xf32>\n-+  }\n-+})mlir\";\n-+\n-+  StablehloModuleBuilder mb;\n-+  {\n-+    Location funcLoc = fileLineColLoc(mb->getContext(), \"main.mlir\", 1, 1);\n-+    func::FunctionBuilder fb(mb.get(), \"main\", funcLoc);\n-+    auto type = makeTensorType(fb.getContext(), {2}, ElementType::F32);\n-+    auto arg0 = func::Argument(fb, type);\n-+    auto exp = Exp(arg0);\n-+    stablehlo::AttachFrontendAttribute(\n-+        fb, exp, \"foo\", fb.getOpBuilder().getI32IntegerAttr(123));\n-+    stablehlo::AttachFrontendAttribute(\n-+        fb, exp, \"foo\", fb.getOpBuilder().getI32IntegerAttr(456));\n-+    func::Return(fb, {exp});\n-+  }\n-+\n-+  OwningOpRef<ModuleOp> module = mb->build();\n-+  EXPECT_EQ(expected, debugString(*module));\n-+}\n-+\n- }  // namespace stablehlo\n- }  // namespace mlir\n-diff --ruN a/stablehlo/stablehlo/reference/InterpreterOps.cpp b/stablehlo/stablehlo/reference/InterpreterOps.cpp\n---- stablehlo/stablehlo/reference/InterpreterOps.cpp\n-+++ stablehlo/stablehlo/reference/InterpreterOps.cpp\n-@@ -46,6 +46,7 @@\n- #include \"stablehlo/reference/ProcessGrid.h\"\n- #include \"stablehlo/reference/Value.h\"\n- \n-+using mlir::stablehlo::TokenType;\n- #define GET_OP_CLASSES\n- #include \"stablehlo/reference/InterpreterOps.cpp.inc\"\n- \n-diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n---- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n-+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n-@@ -22,21 +22,24 @@\n- // CHECK-LABEL: func.func @broadcast_in_dim_fold_splat\n- // CHECK-SAME:   ([[ARG0:%.+]]: tensor<3x3xi32>)\n- func.func @broadcast_in_dim_fold_splat(%arg0: tensor<3x3xi32>)\n--  -> (tensor<6xi32>, tensor<3xf32>, tensor<3x3xi32>) {\n-+  -> (tensor<6xi32>, tensor<3xf32>, tensor<5xcomplex<f32>>, tensor<3x3xi32>) {\n-   %c0 = stablehlo.constant dense<5> : tensor<i32>\n-   %c1 = stablehlo.constant dense<3.0> : tensor<f32>\n--  %c2 = stablehlo.constant dense<1> : tensor<1x3xi32>\n-+  %c2 = stablehlo.constant dense<(1.0,2.0)> : tensor<complex<f32>>\n-+  %c3 = stablehlo.constant dense<1> : tensor<1x3xi32>\n- \n-   %0 = stablehlo.broadcast_in_dim %c0, dims = [] : (tensor<i32>) -> tensor<6xi32>\n-   %1 = stablehlo.broadcast_in_dim %c1, dims = [] : (tensor<f32>) -> tensor<3xf32>\n--  %2 = stablehlo.broadcast_in_dim %c2, dims = [1, 0] : (tensor<1x3xi32>) -> tensor<3x3xi32>\n-+  %2 = stablehlo.broadcast_in_dim %c2, dims = [] : (tensor<complex<f32>>) -> tensor<5xcomplex<f32>>\n-+  %3 = stablehlo.broadcast_in_dim %c3, dims = [1, 0] : (tensor<1x3xi32>) -> tensor<3x3xi32>\n- \n-   // CHECK-DAG:  [[R0:%.+]] = stablehlo.constant dense<5> : tensor<6xi32>\n-   // CHECK-DAG:  [[R1:%.+]] = stablehlo.constant dense<3.000000e+00> : tensor<3xf32>\n--  // CHECK-DAG:  [[R2:%.+]] = stablehlo.constant dense<1> : tensor<3x3xi32>\n--\n--  // CHECK-NEXT: return [[R0]], [[R1]], [[R2]]\n--  return %0, %1, %2 : tensor<6xi32>, tensor<3xf32>, tensor<3x3xi32>\n-+  // CHECK-DAG:  [[R2:%.+]] = stablehlo.constant dense<(1.0{{.*}},2.0{{.*}})> : tensor<5xcomplex<f32>>\n-+  // CHECK-DAG:  [[R3:%.+]] = stablehlo.constant dense<1> : tensor<3x3xi32>\n-+\n-+  // CHECK-NEXT: return [[R0]], [[R1]], [[R2]], [[R3]]\n-+  return %0, %1, %2, %3 : tensor<6xi32>, tensor<3xf32>, tensor<5xcomplex<f32>>, tensor<3x3xi32>\n- }\n- \n- // -----\n-diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n---- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n-+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n-@@ -134,6 +134,35 @@\n-   return %0, %5 : tensor<1x3x6xi32>, tensor<3x6x1xi32>\n- }\n- \n-+// CHECK-LABEL: func.func @broadcast_in_dim_prefer_nested_reshape\n-+// CHECK-SAME:   ([[ARG0:%[^ ]+]]: tensor<3x4xi32>)\n-+func.func @broadcast_in_dim_prefer_nested_reshape(%arg0: tensor<3x4xi32>) -> (tensor<2x3x4x3xi32>, tensor<2x3x4x3xi32>) {\n-+  // When `broadcast_in_dim(broadcast_in_dim(x))` could be optimized into either\n-+  // `broadcast_in_dim(reshape(x))` or `broadcast_in_dim(x)`, we want to select\n-+  // the former pattern.\n-+  //\n-+  // (We accomplish this by blocking the merge-composition pattern if the inner\n-+  // op can be replaced with a `reshape`. Simply adding benefit to the\n-+  // replace-with-reshape pattern isn't sufficient here because the outermost\n-+  // op, which only matches the merge-composition pattern, is traversed first.)\n-+\n-+  // CHECK-DAG: [[INNER_RESHAPE:%[^ ]+]] = stablehlo.reshape [[ARG0]] : (tensor<3x4xi32>) -> tensor<3x1x4xi32>\n-+  // CHECK-DAG: [[BROADCAST_OF_RESHAPE:%[^ ]+]] = stablehlo.broadcast_in_dim [[INNER_RESHAPE]], dims = [1, 0, 2] : (tensor<3x1x4xi32>) -> tensor<2x3x4x3xi32>\n-+  %0 = stablehlo.broadcast_in_dim %arg0, dims = [0, 2] : (tensor<3x4xi32>) -> tensor<3x1x4xi32>\n-+  %1 = stablehlo.broadcast_in_dim %0, dims = [1, 0, 2] : (tensor<3x1x4xi32>) -> tensor<2x3x4x3xi32>\n-+\n-+  // When the inner op doesn't qualify for replacement with a `reshape` op,\n-+  // however (particularly when it meets some conditions but not others), ensure\n-+  // that we allow the merge-composition pattern to match.\n-+\n-+  // CHECK-DAG: [[MERGED_BROADCAST:%[^ ]+]] = stablehlo.broadcast_in_dim [[ARG0]], dims = [3, 2] : (tensor<3x4xi32>) -> tensor<2x3x4x3xi32>\n-+  %2 = stablehlo.broadcast_in_dim %arg0, dims = [2, 1] : (tensor<3x4xi32>) -> tensor<1x4x3xi32>\n-+  %3 = stablehlo.broadcast_in_dim %2, dims = [0, 2, 3] : (tensor<1x4x3xi32>) -> tensor<2x3x4x3xi32>\n-+\n-+  // CHECK-DAG: return [[BROADCAST_OF_RESHAPE]], [[MERGED_BROADCAST]]\n-+  return %1, %3 : tensor<2x3x4x3xi32>, tensor<2x3x4x3xi32>\n-+}\n-+\n- // CHECK-LABEL: func.func @broadcast_in_dim_not_identity_broadcasts\n- func.func @broadcast_in_dim_not_identity_broadcasts(%arg0: tensor<1x2xf32>) -> tensor<2x2xf32> {\n-   // CHECK: stablehlo.broadcast_in_dim\n-@@ -208,6 +237,18 @@\n-   // CHECK-NEXT: return [[C1]], [[C0]], [[C1]], [[C0]], [[R0]], [[R1]], [[R2]], [[R3]]\n-   return %0, %1, %2, %3, %4, %5, %6, %7 :\n-          tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>\n-+}\n-+\n-+// CHECK-LABEL: func.func @compare_op_bool_simplify\n-+// CHECK-SAME:   ([[ARG0:%.+]]: tensor<i1>)\n-+func.func @compare_op_bool_simplify(%arg0: tensor<i1>) -> (tensor<i1>, tensor<i1>) {\n-+  %false = stablehlo.constant dense<false> : tensor<i1>\n-+  %true = stablehlo.constant dense<true> : tensor<i1>\n-+  // CHECK-NOT: stablehlo.compare\n-+  %0 = stablehlo.compare NE, %arg0, %false, UNSIGNED : (tensor<i1>, tensor<i1>) -> tensor<i1>\n-+  %1 = stablehlo.compare EQ, %arg0, %true, UNSIGNED : (tensor<i1>, tensor<i1>) -> tensor<i1>\n-+  // CHECK: return [[ARG0]], [[ARG0]]\n-+  func.return %0, %1 : tensor<i1>, tensor<i1>\n- }\n- \n- // -----\n-@@ -1021,6 +1062,18 @@\n-   // CHECK-NOT: stablehlo.pad\n-   %1 = stablehlo.pad %arg0, %0, low = [0, 0], high = [0, 0], interior = [0, 0] : (tensor<256x1024xbf16>, tensor<bf16>) -> tensor<256x1024xbf16>\n-   return %1 : tensor<256x1024xbf16>\n-+}\n-+\n-+// We don't want to delete `pad` ops that move a tensor's values around without\n-+// affecting its dimensions.\n-+//\n-+// CHECK-LABEL: @pad_rotate_tensor_no_dim_change\n-+func.func @pad_rotate_tensor_no_dim_change(%arg0: tensor<50x50xf32>) -> tensor<50x50xf32> {\n-+  // CHECK: %[[RES:.+]] = stablehlo.pad\n-+  // CHECK: return %[[RES]]\n-+  %cst = stablehlo.constant dense<0.0> : tensor<f32>\n-+  %0 = stablehlo.pad %arg0, %cst, low = [0, -1], high = [0, 1], interior = [0, 0] : (tensor<50x50xf32>, tensor<f32>) -> tensor<50x50xf32>\n-+  return %0 : tensor<50x50xf32>\n- }\n- \n- // -----\n-@@ -1810,6 +1863,15 @@\n-   return %0 : tensor<2x4x1x5xf32>\n- }\n- \n-+// CHECK-LABEL: @transpose_of_transpose\n-+func.func @transpose_of_transpose(%arg0 : tensor<1x2x3x4xf32>) -> tensor<1x2x3x4xf32> {\n-+  %0 = stablehlo.transpose %arg0, dims = [3,2,1,0] : (tensor<1x2x3x4xf32>) -> tensor<4x3x2x1xf32>\n-+  %1 = stablehlo.transpose %0, dims = [3,2,1,0] : (tensor<4x3x2x1xf32>) -> tensor<1x2x3x4xf32>\n-+  // CHECK-NOT: stablehlo.transpose\n-+  // CHECK: return %arg0\n-+  return %1 : tensor<1x2x3x4xf32>\n-+}\n-+\n- // -----\n- \n- ////////\n-diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir\n---- stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir\n-+++ stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir\n-@@ -752,7 +752,7 @@\n-     %2 = call @refine_call_callee(%arg0_different_i32, %1) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>\n-     return %2 : tensor<?xf32>\n-   }\n--  // expected-error@+1{{'func.func' op refined with invompatible refinement keys}}\n-+  // expected-error@+1{{'func.func' op refined with incompatible refinement keys}}\n-   func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {\n-     return %arg1 : tensor<?xf32>\n-   }\n-@@ -770,7 +770,7 @@\n-     %2 = call @refine_call_callee(%arg0_different, %1) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>\n-     return %2 : tensor<?xf32>\n-   }\n--  // expected-error@+1{{'func.func' op refined with invompatible refinement keys}}\n-+  // expected-error@+1{{'func.func' op refined with incompatible refinement keys}}\n-   func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {\n-     return %arg1 : tensor<?xf32>\n-   }\n-@@ -789,7 +789,7 @@\n-     %4 = call @refine_call_callee(%arg0_new, %3) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>\n-     return %4 : tensor<?xf32>\n-   }\n--  // expected-error@+1{{'func.func' op refined with invompatible refinement keys}}\n-+  // expected-error@+1{{'func.func' op refined with incompatible refinement keys}}\n-   func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {\n-     return %arg1 : tensor<?xf32>\n-   }\n-diff --ruN a/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp b/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp\n---- stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp\n-+++ stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp\n-@@ -461,7 +461,7 @@\n-   LogicalResult emitDifferentRefinementContextError(func::FuncOp func,\n-                                                     RefinementKey key,\n-                                                     RefinementKey prevKey) {\n--    return func.emitOpError() << \"refined with invompatible refinement keys:\"\n-+    return func.emitOpError() << \"refined with incompatible refinement keys:\"\n-                               << \"\\n  curr=\" << key.toString()\n-                               << \"\\n  prev=\" << prevKey.toString();\n-   }\n-diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n---- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n-+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n-@@ -530,10 +530,15 @@\n-   using FoldOpRewritePattern<OpType>::matchAndRewrite;\n-   using FoldOpRewritePattern<OpType>::options;\n- \n-+  // TODO: Generalize all relevant folder patterns to support complex data\n-+  // types, then hard-code `allowComplex` to `true`.\n-   LogicalResult validateShapeFoldDtype(PatternRewriter& rewriter, OpType op,\n--                                       ShapedType resultType) const {\n-+                                       ShapedType resultType,\n-+                                       bool allowComplex = false) const {\n-     if (resultType.getElementType().isInteger()) return success();\n--    if (options.optimizeFloat && isa<FloatType>(resultType.getElementType()))\n-+    if (options.optimizeFloat &&\n-+        (allowComplex ? isa<FloatType, ComplexType>(resultType.getElementType())\n-+                      : isa<FloatType>(resultType.getElementType())))\n-       return success();\n-     return rewriter.notifyMatchFailure(op, \"skipping fold of shape op dtype\");\n-   }\n-@@ -605,7 +610,8 @@\n-                                 PatternRewriter& rewriter) const override {\n-     auto resultType = op.getType();\n-     if (failed(validateStaticShapeResult(rewriter, op, resultType)) ||\n--        failed(validateShapeFoldDtype(rewriter, op, resultType)))\n-+        failed(validateShapeFoldDtype(rewriter, op, resultType,\n-+                                      /*allowComplex=*/true)))\n-       return failure();\n- \n-     SplatElementsAttr cstAttr;\n-@@ -825,7 +831,8 @@\n-     RankedTensorType resultType = op.getType();\n- \n-     if (failed(validateStaticShapeResult(rewriter, op, resultType)) ||\n--        failed(validateShapeFoldDtype(rewriter, op, resultType)))\n-+        failed(validateShapeFoldDtype(rewriter, op, resultType)) ||\n-+        failed(validateElementCountForFold(rewriter, op, resultType)))\n-       return failure();\n- \n-     auto operandElemType = getElementTypeOrSelf(operand.getType());\n-@@ -1104,7 +1111,7 @@\n-         failed(validateShapeFoldDtype(rewriter, op, resultType)))\n-       return failure();\n- \n--    DenseIntOrFPElementsAttr attr;\n-+    DenseElementsAttr attr;\n-     if (!matchPattern(op.getOperand(), m_Constant(&attr)))\n-       return rewriter.notifyMatchFailure(op, \"expected constant operand\");\n-     rewriter.replaceOpWithNewOp<ConstantOp>(op, attr.reshape(resultType));\n-diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n---- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n-+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n-@@ -1309,10 +1309,20 @@\n- // TransposeOp\n- /////////////////////////////////\n- \n-+DenseI64ArrayAttr getMergedTransposePermutation(OpBuilder& b,\n-+                                                ArrayRef<int64_t> childPerm,\n-+                                                ArrayRef<int64_t> parentPerm) {\n-+  SmallVector<int64_t> mergedPerm;\n-+  mergedPerm.reserve(parentPerm.size());\n-+  for (int64_t parentIdx : parentPerm) {\n-+    mergedPerm.push_back(childPerm[parentIdx]);\n-+  }\n-+  return b.getDenseI64ArrayAttr(mergedPerm);\n-+}\n-+\n- // Pattern: transpose(X, [no_mem_layout_change...]) -> reshape(X)\n- struct TransposeIsReshape final : SimplifyOpRewritePattern<TransposeOp> {\n-   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n--\n-   LogicalResult matchAndRewrite(TransposeOp op,\n-                                 PatternRewriter& rewriter) const override {\n-     auto input = op.getOperand();\n-diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n---- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n-+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n-@@ -43,6 +43,14 @@\n-     CPred<\"llvm::cast<ShapedType>($0.getType()).getNumElements() == llvm::cast<ShapedType>($1.getType()).getNumElements()\">,\n-     \"same number of elements\">;\n- \n-+def BroadcastNotReducibleToReshape : Constraint<\n-+    CPred<\"llvm::isa<stablehlo::BroadcastInDimOp>($0.getDefiningOp()) && \"\n-+          \"!(\"\n-+            \"llvm::is_sorted($0.getDefiningOp<stablehlo::BroadcastInDimOp>().getBroadcastDimensions()) && \"\n-+            \"llvm::cast<ShapedType>($0.getType()).getNumElements() == llvm::cast<ShapedType>($1.getType()).getNumElements()\"\n-+          \")\">,\n-+    \"is a broadcast_in_dim op that cannot be simplified to a reshape op\">;\n-+\n- def OperandsEqual : Constraint<CPred<\"$0 == $1\">, \"operands are equal\">;\n- \n- def RankEqual : Constraint<\n-@@ -61,6 +69,10 @@\n- def AnyZero : AttrConstraint<\n-     CPred<\"::mlir::matchPattern($_self, m_AnyAttrOf(m_Zero(), m_AnyZeroFloat()))\">,\n-     \"is int or float zero\">;\n-+\n-+def ZeroArrayI64 : AttrConstraint<\n-+    CPred<\"::llvm::all_of(::llvm::cast<DenseI64ArrayAttr>($_self).asArrayRef(), [](int64_t val) { return val == 0; })\">,\n-+    \"is an array of zeros\">;\n- \n- def DenseIntElementsAttr : AttrConstraint<\n-     CPred<\"llvm::isa<DenseIntElementsAttr>($_self)\">,\n-@@ -120,6 +132,8 @@\n- \n- def MergeBroadcastDims : NativeCodeCall<\"getMergedBroadcastDimensions($_builder, $0, $1)\">;\n- \n-+def MergePermutations : NativeCodeCall<\"getMergedTransposePermutation($_builder, $0, $1)\">;\n-+\n- def StableHLO_ConvertOpWithShape : NativeCodeCall<\n-     \"$_builder.create<stablehlo::ConvertOp>($_loc, $0.getType(), $1)\">;\n- \n-@@ -178,18 +192,23 @@\n- \n- // Pattern: broadcast_in_dim(broadcast_in_dim(X, [dimsA...]), [dimsB...])\n- //       -> broadcast_in_dim(X, merge(dimsA, dimsB))\n-+//          [if the nested broadcast can't be simplified to a reshape]\n- def BroadcastInDimOp_MergeComposition\n--  : Pat<(StableHLO_BroadcastInDimOp\n--            (StableHLO_BroadcastInDimOp $operand, $dims_parent), $dims),\n-+  : Pat<(StableHLO_BroadcastInDimOp:$outer_op\n-+            (StableHLO_BroadcastInDimOp:$inner_op $operand, $inner_dims),\n-+            $outer_dims),\n-         (StableHLO_BroadcastInDimOp\n--            $operand, (MergeBroadcastDims $dims, $dims_parent))>;\n-+            $operand, (MergeBroadcastDims $outer_dims, $inner_dims)),\n-+        [(BroadcastNotReducibleToReshape $inner_op, $operand)]>;\n- \n- // Pattern: broadcast_in_dim(X, [sorted...]) -> reshape(X, [sorted...])\n- //          [if same numel]\n- def BroadcastInDimOp_ReplaceWithReshape\n-   : Pat<(StableHLO_BroadcastInDimOp:$op $operand, SortedDims:$dims),\n-         (StableHLO_ReshapeOpWithShape $op, $operand),\n--        [(NumberOfElementsEqual $op, $operand)]>;\n-+        [(NumberOfElementsEqual $op, $operand)],\n-+        [],\n-+        (addBenefit 1)>;\n- \n- // Pattern: broadcast_in_dim(X, [dims...]) -> transpose(X, [dims...])\n- //          [if same numel & rank]\n-@@ -197,6 +216,36 @@\n-   : Pat<(StableHLO_BroadcastInDimOp:$op $operand, $dims),\n-         (StableHLO_TransposeOp $operand, (InvertBroadcastDims $dims)),\n-         [(NumberOfElementsEqual $op, $operand), (RankEqual $op, $operand)]>;\n-+\n-+////////\n-+// CompareOp\n-+\n-+// The canonical form has the constant operand as the RHS.\n-+class StableHLO_ComparisonDirectionValue<string enumStr> :\n-+  ConstantAttr<StableHLO_ComparisonDirectionAttr, \"::mlir::stablehlo::ComparisonDirection::\" # enumStr>;\n-+\n-+// Pattern: compare(NE, X, False) : i1 -> X\n-+def CompareOp_NeBooleanFalse\n-+  : Pat<(StableHLO_CompareOp\n-+            $lhs,\n-+            (StableHLO_ConstantOp:$cst IntZero:$value),\n-+            StableHLO_ComparisonDirectionValue<\"NE\">,\n-+            $type),\n-+        (replaceWithValue $lhs),\n-+        [(HLO_PredTensor $cst)]>;\n-+\n-+// Pattern: compare(EQ, X, True) : i1 -> X\n-+def CompareOp_EqBooleanTrue\n-+  : Pat<(StableHLO_CompareOp\n-+            $lhs,\n-+            (StableHLO_ConstantOp:$cst IntOne:$value),\n-+            StableHLO_ComparisonDirectionValue<\"EQ\">,\n-+            $type),\n-+        (replaceWithValue $lhs),\n-+        [(HLO_PredTensor $cst)]>;\n-+\n-+// TODO: compare(EQ, X, False) : i1 -> not(X)\n-+// TODO: compare(NE, X, True) : i1 -> not(X)\n- \n- ////////\n- // ConvertOp\n-@@ -424,9 +473,9 @@\n-   : Pat<(StableHLO_PadOp:$pad\n-             $operand,\n-             $padding_value,\n--            $edge_padding_low,\n--            $edge_padding_high,\n--            $interior_padding),\n-+            ZeroArrayI64:$edge_padding_low,\n-+            ZeroArrayI64:$edge_padding_high,\n-+            ZeroArrayI64:$interior_padding),\n-         (replaceWithValue $operand),\n-         [(TypesEqual $pad, $operand)]>;\n- \n-@@ -539,6 +588,12 @@\n-   : Pat<(StableHLO_TransposeOp $lhs, IotaDims:$dims),\n-         (replaceWithValue $lhs)>;\n- \n-+// Pattern: transpose(transpose(X)) -> transpose(X)\n-+def TransposeOp_TransposeOfTranspose\n-+  : Pat<(StableHLO_TransposeOp\n-+          (StableHLO_TransposeOp $child, $child_dims), $dims),\n-+        (StableHLO_TransposeOp $child, (MergePermutations $child_dims, $dims))>;\n-+\n- ////////\n- // GetTupleElementOp\n- \n "
        },
        {
            "sha": "7548b489858693b4f918d047dad1cc6e28989598",
            "filename": "third_party/xla/third_party/stablehlo/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/151894fa84cb54bf11b58fe7c4fefc56fa831a4b/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/151894fa84cb54bf11b58fe7c4fefc56fa831a4b/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Fworkspace.bzl?ref=151894fa84cb54bf11b58fe7c4fefc56fa831a4b",
            "patch": "@@ -4,8 +4,8 @@ load(\"//third_party:repo.bzl\", \"tf_http_archive\", \"tf_mirror_urls\")\n \n def repo():\n     # LINT.IfChange\n-    STABLEHLO_COMMIT = \"baaf7475f8925cb0c5f9580408b3c0385f888487\"\n-    STABLEHLO_SHA256 = \"c4b96f94d9d4aaa8b2dc88104579aab662aa33d59b79e77a9b75c8e0af3d9461\"\n+    STABLEHLO_COMMIT = \"3f27c53c20b9021ccab8b5f673e2c72e5b9cd6aa\"\n+    STABLEHLO_SHA256 = \"915e05e79d9764c048557a929c64e090ab58a5c7334da2c2650cd6378aa4d166\"\n     # LINT.ThenChange(Google-internal path)\n \n     tf_http_archive("
        }
    ],
    "stats": {
        "total": 672,
        "additions": 38,
        "deletions": 634
    }
}