{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 845697709",
    "sha": "960635397c7be3d84fd4afa9b876b5d824435769",
    "files": [
        {
            "sha": "c63c07a394b6c6af5a4a08ad8854bfc978b0b8e7",
            "filename": "tensorflow/core/kernels/quantize_op.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantize_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantize_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fquantize_op.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -67,7 +67,7 @@ class QuantizeV2Op : public OpKernel {\n             : (static_cast<double>(std::numeric_limits<T>::max()) -\n                static_cast<double>(std::numeric_limits<T>::min()) + 1) /\n                   2.0f;\n-    string mode_string;\n+    std::string mode_string;\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"mode\", &mode_string));\n     OP_REQUIRES(ctx,\n                 (mode_string == \"MIN_COMBINED\" || mode_string == \"MIN_FIRST\" ||\n@@ -83,7 +83,7 @@ class QuantizeV2Op : public OpKernel {\n       mode_ = QUANTIZE_MODE_SCALED;\n     }\n \n-    string round_mode_string;\n+    std::string round_mode_string;\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"round_mode\", &round_mode_string));\n     OP_REQUIRES(ctx,\n                 (round_mode_string == \"HALF_AWAY_FROM_ZERO\" ||"
        },
        {
            "sha": "ec486ba87dc990fa5ca429cde937f9ef069a2ac6",
            "filename": "tensorflow/core/kernels/quantize_op_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantize_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantize_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fquantize_op_test.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -62,7 +62,7 @@ TEST_F(QuantizedOpTest, QuantizeV2) {\n template <typename T>\n std::vector<T> ScalePerSliceAlongAxis(std::vector<int64_t> dims, int axis,\n                                       const std::vector<T>& data) {\n-  uint32 seed = 123;\n+  uint32_t seed = 123;\n   std::minstd_rand rng(seed);\n   int64_t out_size = 1;\n   for (int dim : dims) {\n@@ -373,14 +373,14 @@ TEST_F(QuantizedOpTest, QuantizeV2_32Bit) {\n   Tensor expected(allocator(), DT_QINT32, TensorShape({element_count}));\n   test::FillValues<qint32>(&expected,\n                            {\n-                               std::numeric_limits<int32>::min(),\n+                               std::numeric_limits<int32_t>::min(),\n                                0,\n-                               static_cast<int32>(1.0f * (1 << 23)),\n-                               static_cast<int32>(1.25f * (1 << 23)),\n-                               static_cast<int32>(1.75f * (1 << 23)),\n-                               static_cast<int32>(127.0f * (1 << 23)),\n-                               static_cast<int32>(255.0f * (1 << 23)),\n-                               std::numeric_limits<int32>::max(),\n+                               static_cast<int32_t>(1.0f * (1 << 23)),\n+                               static_cast<int32_t>(1.25f * (1 << 23)),\n+                               static_cast<int32_t>(1.75f * (1 << 23)),\n+                               static_cast<int32_t>(127.0f * (1 << 23)),\n+                               static_cast<int32_t>(255.0f * (1 << 23)),\n+                               std::numeric_limits<int32_t>::max(),\n                            });\n   // We expect there will be some fuzziness in the lower bits, since this is\n   // converting from float."
        },
        {
            "sha": "e8904e8a088395a17e30006ca291b5690c48d9f1",
            "filename": "tensorflow/core/kernels/quantized_add_op.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_add_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_add_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fquantized_add_op.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -149,7 +149,7 @@ void ScalarAddition(OpKernelContext* context, const quint8* full_input,\n     full_input_in_output_range_64 =\n         std::min(full_input_in_output_range_64, highest_quantized);\n     const int32_t full_input_in_output_range =\n-        static_cast<int32>(full_input_in_output_range_64);\n+        static_cast<int32_t>(full_input_in_output_range_64);\n     output[i] = full_input_in_output_range + scalar_in_output_range;\n   }\n }\n@@ -272,13 +272,15 @@ void VectorAddition(OpKernelContext* context, const quint8* x_data, float min_x,\n     int64_t x_in_output_range_64 = x_0_int64 + (x_value * x_mult_int32);\n     x_in_output_range_64 = std::max(x_in_output_range_64, lowest_quantized);\n     x_in_output_range_64 = std::min(x_in_output_range_64, highest_quantized);\n-    const int32_t x_in_output_range = static_cast<int32>(x_in_output_range_64);\n+    const int32_t x_in_output_range =\n+        static_cast<int32_t>(x_in_output_range_64);\n \n     const int64_t y_value = static_cast<int64_t>(y_data[i]);\n     int64_t y_in_output_range_64 = y_0_int64 + (y_value * y_mult_int32);\n     y_in_output_range_64 = std::max(y_in_output_range_64, lowest_quantized);\n     y_in_output_range_64 = std::min(y_in_output_range_64, highest_quantized);\n-    const int32_t y_in_output_range = static_cast<int32>(y_in_output_range_64);\n+    const int32_t y_in_output_range =\n+        static_cast<int32_t>(y_in_output_range_64);\n \n     output[i] = x_in_output_range + y_in_output_range;\n   }\n@@ -430,7 +432,7 @@ void VectorTensorAddition(const quint8* vector_data, float min_vector,\n     vector_in_output_range_64 =\n         std::min(vector_in_output_range_64, highest_quantized);\n     const int32_t vector_in_output_range =\n-        static_cast<int32>(vector_in_output_range_64);\n+        static_cast<int32_t>(vector_in_output_range_64);\n \n     const int64_t tensor_value = static_cast<int64_t>(tensor_data[i]);\n     int64_t tensor_in_output_range_64 =\n@@ -440,7 +442,7 @@ void VectorTensorAddition(const quint8* vector_data, float min_vector,\n     tensor_in_output_range_64 =\n         std::min(tensor_in_output_range_64, highest_quantized);\n     const int32_t tensor_in_output_range =\n-        static_cast<int32>(tensor_in_output_range_64);\n+        static_cast<int32_t>(tensor_in_output_range_64);\n \n     output[i] = vector_in_output_range + tensor_in_output_range;\n   }"
        },
        {
            "sha": "613fef99ea67c9921e089400c47f548b81853932",
            "filename": "tensorflow/core/kernels/quantized_concat_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_concat_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_concat_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fquantized_concat_op.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -183,7 +183,7 @@ class QuantizedConcatOp : public OpKernel {\n         errors::InvalidArgument(\n             \"Concat dim tensor should be a scalar integer, but got shape \",\n             concat_dim_tensor->shape().DebugString()));\n-    const int32_t concat_dim = concat_dim_tensor->scalar<int32>()();\n+    const int32_t concat_dim = concat_dim_tensor->scalar<int32_t>()();\n     OpInputList values;\n     OP_REQUIRES_OK(context, context->input_list(\"values\", &values));\n     const size_t N = values.size();"
        },
        {
            "sha": "cebe247f77f460030bdaf55f6359d8f8a135f953",
            "filename": "tensorflow/core/kernels/quantized_concat_op_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_concat_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_concat_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fquantized_concat_op_test.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -88,7 +88,7 @@ void QuantizedConcatTest::TestInvalidMinMax(const Tensor& first_min,\n   Tensor second_quantized(DT_QUINT8, {1});\n   test::FillValues<quint8>(&second_quantized, {1});\n \n-  AddInputFromArray<int32>(TensorShape({}), {0});\n+  AddInputFromArray<int32_t>(TensorShape({}), {0});\n   AddInputFromArray<quint8>(first_quantized.shape(),\n                             first_quantized.flat<quint8>());\n   AddInputFromArray<quint8>(second_quantized.shape(),\n@@ -144,7 +144,7 @@ void QuantizedConcatTest::TestSmall8Bit(float first_min, float first_max,\n                           {1,  2,  3,  4,  5,  6,  7,  8,  9,  10, 11, 12,\n                            13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24});\n \n-  AddInputFromArray<int32>(TensorShape({}), {0});\n+  AddInputFromArray<int32_t>(TensorShape({}), {0});\n   AddInputFromArray<quint8>(first_quantized.shape(),\n                             first_quantized.flat<quint8>());\n   AddInputFromArray<quint8>(second_quantized.shape(),\n@@ -210,7 +210,7 @@ void QuantizedConcatTest::TestSmall32Bit(float first_min, float first_max,\n       {100,  200,  300,  400,  500,  600,  700,  800,  900,  1000, 1100, 1200,\n        1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400});\n \n-  AddInputFromArray<int32>(TensorShape({}), {0});\n+  AddInputFromArray<int32_t>(TensorShape({}), {0});\n   AddInputFromArray<qint32>(first_quantized.shape(),\n                             first_quantized.flat<qint32>());\n   AddInputFromArray<qint32>(second_quantized.shape(),\n@@ -272,7 +272,7 @@ void QuantizedConcatTest::TestSecondDim8Bit(float first_min, float first_max,\n                           {1, 2, 3, 4,  5,  6,  13, 14, 15, 16, 17, 18,\n                            7, 8, 9, 10, 11, 12, 19, 20, 21, 22, 23, 24});\n \n-  AddInputFromArray<int32>(TensorShape({}), {1});\n+  AddInputFromArray<int32_t>(TensorShape({}), {1});\n   AddInputFromArray<quint8>(first_quantized.shape(),\n                             first_quantized.flat<quint8>());\n   AddInputFromArray<quint8>(second_quantized.shape(),\n@@ -303,7 +303,7 @@ static void ConcatHelper(::testing::benchmark::State& state,\n   const int kDim1 = 100;\n   TensorShape shape({kDim1, dim2});\n \n-  Tensor concat_dim = test::AsScalar<int32>(concat_dimension);\n+  Tensor concat_dim = test::AsScalar<int32_t>(concat_dimension);\n   Tensor in0(dt, shape);\n   in0.flat<T>().setRandom();\n   Tensor in1(dt, shape);"
        },
        {
            "sha": "14072547b310e74ce4753fdcff000af7d6210b55",
            "filename": "tensorflow/core/kernels/quantized_conv_ops.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_conv_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_conv_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fquantized_conv_ops.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -62,8 +62,9 @@ class ReferenceConvFunctor {\n                   int output_shift, int output_offset, int output_mult) {\n     // Set up some constants we need for the output down-shifting and\n     // saturation.\n-    const int32_t highest = static_cast<int32>(Eigen::NumTraits<T3>::highest());\n-    const int32_t lowest = static_cast<int32>(Eigen::NumTraits<T3>::lowest());\n+    const int32_t highest =\n+        static_cast<int32_t>(Eigen::NumTraits<T3>::highest());\n+    const int32_t lowest = static_cast<int32_t>(Eigen::NumTraits<T3>::lowest());\n \n     // When we're converting the 32 bit accumulator to a lower bit depth, we\n     // need to add on 0.5 in fixed-point terms to make the operation round half\n@@ -150,7 +151,7 @@ class ReferenceConvFunctor {\n                     // We're promoting the T1 type to a higher bit depth here as\n                     // we do the subtraction.\n                     input_value =\n-                        static_cast<int32>(input_source_value) - input_offset;\n+                        static_cast<int32_t>(input_source_value) - input_offset;\n                   } else {\n                     input_value = 0;\n                   }\n@@ -161,7 +162,7 @@ class ReferenceConvFunctor {\n                                   (in_channel * filter_count) + out_channel];\n                   // Another promotion to 32 bit, as above.\n                   const int32_t filter_value =\n-                      static_cast<int32>(filter_source_value) - filter_offset;\n+                      static_cast<int32_t>(filter_source_value) - filter_offset;\n                   total += (input_value * filter_value);\n                 }\n               }\n@@ -406,9 +407,9 @@ class Im2ColConvFunctor {\n         // The gemmlowp optimized library only works for a particular set of\n         // data types, so check if we meet those requirements and fall back to a\n         // slower reference implementation if not.\n-        const uint8* im2col_data_as_uint8 = &(im2col_buffer->value);\n-        const uint8* filter_data_as_uint8 = &(filter_data->value);\n-        int32* output_data_as_int32 = &(chunk_output_data->value);\n+        const uint8_t* im2col_data_as_uint8 = &(im2col_buffer->value);\n+        const uint8_t* filter_data_as_uint8 = &(filter_data->value);\n+        int32_t* output_data_as_int32 = &(chunk_output_data->value);\n         // All of the transpose_* variables are currently compile-time consts,\n         // so we could just hard-code these values too, but that would break if\n         // anybody changed those values in the future (e.g. to match the ability\n@@ -472,7 +473,7 @@ class QuantizedConv2DOp : public OpKernel {\n         context, (strides_[0] == 1 && strides_[3] == 1),\n         errors::InvalidArgument(\"Current implementation does not yet support \"\n                                 \"strides in the batch and depth dimensions.\"));\n-    std::vector<int32> dilations;\n+    std::vector<int32_t> dilations;\n     OP_REQUIRES_OK(context, context->GetAttr(\"dilations\", &dilations));\n     OP_REQUIRES(context, dilations.size() == 4,\n                 errors::InvalidArgument(\"Dilations field must \"\n@@ -612,7 +613,7 @@ class QuantizedConv2DOp : public OpKernel {\n   }\n \n  private:\n-  std::vector<int32> strides_;\n+  std::vector<int32_t> strides_;\n   Padding padding_;\n };\n "
        },
        {
            "sha": "5f7143e183991ab4b37c7e6797dc85adb3e6bf1d",
            "filename": "tensorflow/core/kernels/quantized_matmul_op.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_matmul_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_matmul_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fquantized_matmul_op.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -38,9 +38,9 @@ template <bool TransposeA, bool TransposeB, bool TransposeC>\n void GemmlowpMultiply(OpKernelContext* op_context, const quint8* a_data,\n                       const quint8* b_data, qint32* c_data, int m, int n, int k,\n                       int offset_a, int offset_b, int lda, int ldb, int ldc) {\n-  const uint8* a_data_as_uint8 = &(a_data->value);\n-  const uint8* b_data_as_uint8 = &(b_data->value);\n-  int32* c_data_as_int32 = &(c_data->value);\n+  const uint8_t* a_data_as_uint8 = &(a_data->value);\n+  const uint8_t* b_data_as_uint8 = &(b_data->value);\n+  int32_t* c_data_as_int32 = &(c_data->value);\n   static const gemmlowp::MapOrder ResultOrder =\n       !TransposeC ? gemmlowp::MapOrder::RowMajor : gemmlowp::MapOrder::ColMajor;\n   static const gemmlowp::MapOrder LhsOrder ="
        },
        {
            "sha": "9028137e49949dd27029326fa2dc246dff420103",
            "filename": "tensorflow/core/kernels/quantized_mul_op.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_mul_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_mul_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fquantized_mul_op.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -38,9 +38,9 @@ void ScalarMultiply(OpKernelContext* context, const T* full_input,\n                     T scalar_input, int32_t scalar_input_offset,\n                     Toutput* output) {\n   const int32_t scalar_minus_offset =\n-      static_cast<int32>(scalar_input) - scalar_input_offset;\n+      static_cast<int32_t>(scalar_input) - scalar_input_offset;\n   for (int i = 0; i < num_elements; ++i) {\n-    output[i] = (static_cast<int32>(full_input[i]) - full_input_offset) *\n+    output[i] = (static_cast<int32_t>(full_input[i]) - full_input_offset) *\n                 scalar_minus_offset;\n   }\n }\n@@ -115,8 +115,8 @@ void VectorMultiply(OpKernelContext* context, const T* x_data, int32_t offset_x,\n                     const T* y_data, int32_t offset_y, int64_t num_elements,\n                     Toutput* output) {\n   for (int i = 0; i < num_elements; ++i) {\n-    output[i] = (static_cast<int32>(x_data[i]) - offset_x) *\n-                (static_cast<int32>(y_data[i]) - offset_y);\n+    output[i] = (static_cast<int32_t>(x_data[i]) - offset_x) *\n+                (static_cast<int32_t>(y_data[i]) - offset_y);\n   }\n }\n \n@@ -193,8 +193,8 @@ void VectorTensorMultiply(const T* vector_data, int32_t vector_offset,\n                           Toutput* output) {\n   for (int i = 0; i < tensor_num_elements; ++i) {\n     const int64_t vector_i = i % vector_num_elements;\n-    output[i] = (static_cast<int32>(vector_data[vector_i]) - vector_offset) *\n-                (static_cast<int32>(tensor_data[i]) - tensor_offset);\n+    output[i] = (static_cast<int32_t>(vector_data[vector_i]) - vector_offset) *\n+                (static_cast<int32_t>(tensor_data[i]) - tensor_offset);\n   }\n }\n "
        },
        {
            "sha": "5a05d1635c1d6baa4192332c4400410e906cb6ab",
            "filename": "tensorflow/core/kernels/quantized_pooling_ops.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_pooling_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_pooling_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fquantized_pooling_ops.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -95,8 +95,9 @@ class QuantizedAvgPoolingOp : public OpKernel {\n                    params.forward_output_shape(&params_forward_output_shape));\n     OP_REQUIRES_OK(context, context->allocate_output(\n                                 0, params_forward_output_shape, &output));\n-    const int32_t highest = static_cast<int32>(Eigen::NumTraits<T>::highest());\n-    const int32_t lowest = static_cast<int32>(Eigen::NumTraits<T>::lowest());\n+    const int32_t highest =\n+        static_cast<int32_t>(Eigen::NumTraits<T>::highest());\n+    const int32_t lowest = static_cast<int32_t>(Eigen::NumTraits<T>::lowest());\n \n     // TODO(vrv): Switch this to the Eigen::Tensor version of\n     // SpatialAvgPooling once that version is running quickly.\n@@ -105,12 +106,12 @@ class QuantizedAvgPoolingOp : public OpKernel {\n     Tensor int32_output(DT_INT32, params_forward_output_shape);\n     // Cast input to int32 tensor and call SpatialAvgPool.\n     Tensor int32_input(DT_INT32, tensor_in.shape());\n-    int32_input.flat<int32>() = tensor_in.flat<T>().template cast<int32>();\n-    SpatialAvgPool<Device, int32>(context, &int32_output, int32_input, params,\n-                                  padding_);\n+    int32_input.flat<int32_t>() = tensor_in.flat<T>().template cast<int32_t>();\n+    SpatialAvgPool<Device, int32_t>(context, &int32_output, int32_input, params,\n+                                    padding_);\n \n     // Clamp the int32 output back into quantized space.\n-    output->flat<T>() = int32_output.flat<int32>()\n+    output->flat<T>() = int32_output.flat<int32_t>()\n                             .cwiseMax(lowest)\n                             .cwiseMin(highest)\n                             .template cast<T>();\n@@ -124,8 +125,8 @@ class QuantizedAvgPoolingOp : public OpKernel {\n   }\n \n  private:\n-  std::vector<int32> ksize_;\n-  std::vector<int32> stride_;\n+  std::vector<int32_t> ksize_;\n+  std::vector<int32_t> stride_;\n   Padding padding_;\n };\n "
        },
        {
            "sha": "a2c7b60bbc71db20171dfafe1733bd9aede74540",
            "filename": "tensorflow/core/kernels/quantized_reshape_op_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_reshape_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_reshape_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fquantized_reshape_op_test.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -56,7 +56,7 @@ TEST_F(QuantizedReshapeTest, Reshape) {\n     expected.flat<quint8>()(i) = quint8(i);\n   }\n   AddInputFromArray<quint8>(input.shape(), input.flat<quint8>());\n-  AddInputFromList<int32>({3}, {5, 10, 4});  // shape\n+  AddInputFromList<int32_t>({3}, {5, 10, 4});  // shape\n   AddInputFromArray<float>(TensorShape({1}), {-10});\n   AddInputFromArray<float>(TensorShape({1}), {20});\n   TF_ASSERT_OK(RunOpKernel());"
        },
        {
            "sha": "4e6f072973b3e1ca0e4e61b95feac4bd77eefe82",
            "filename": "tensorflow/core/kernels/quantized_resize_bilinear_op.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_resize_bilinear_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_resize_bilinear_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fquantized_resize_bilinear_op.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -132,7 +132,7 @@ inline T ComputeLerp(const T top_left, const T top_right, const T bottom_left,\n       MulOffset<T, T_SCALE, T_CALC>(bottom_right, bottom_left, x_lerp);\n   const T_CALC out = top + (bottom - top) / RESOLUTION_MULT * y_lerp;\n   return static_cast<T>(\n-      static_cast<int32>((out + RESOLUTION_MULT / 2) / RESOLUTION_MULT));\n+      static_cast<int32_t>((out + RESOLUTION_MULT / 2) / RESOLUTION_MULT));\n }\n \n #ifdef QUANTIZED_RESIZE_BILINEAR_USE_NEON\n@@ -266,7 +266,7 @@ inline void OutputLerpForChannels(const InterpolationCache<T_SCALE>& xs,\n }\n \n template <int RES>\n-inline void OutputLerp8x8x1(const InterpolationCache<int16>& xs,\n+inline void OutputLerp8x8x1(const InterpolationCache<int16_t>& xs,\n                             const int64_t x_start, const int16_t ys_ilerp,\n                             const float min, const float max,\n                             const quint8* const ys_input_lower_ptr,\n@@ -284,15 +284,15 @@ inline void OutputLerp8x8x1(const InterpolationCache<int16>& xs,\n \n #else\n   for (int x = x_start; x < x_start + 8; ++x) {\n-    OutputLerpForChannels<RES, quint8, int16, int16>(\n+    OutputLerpForChannels<RES, quint8, int16_t, int16_t>(\n         xs, x, ys_ilerp, 1, min, max, ys_input_lower_ptr, ys_input_upper_ptr,\n         output_y_ptr);\n   }\n #endif\n }\n \n template <int RES>\n-inline void OutputLerp8x8x3(const InterpolationCache<int16>& xs,\n+inline void OutputLerp8x8x3(const InterpolationCache<int16_t>& xs,\n                             const int64_t x_start, const int16_t ys_ilerp,\n                             const float min, const float max,\n                             const quint8* const ys_input_lower_ptr,\n@@ -325,15 +325,15 @@ inline void OutputLerp8x8x3(const InterpolationCache<int16>& xs,\n \n #else\n   for (int x = x_start; x < x_start + 8; ++x) {\n-    OutputLerpForChannels<RES, quint8, int16, int16>(\n+    OutputLerpForChannels<RES, quint8, int16_t, int16_t>(\n         xs, x, ys_ilerp, 3, min, max, ys_input_lower_ptr, ys_input_upper_ptr,\n         output_y_ptr);\n   }\n #endif\n }\n \n template <int RESOLUTION>\n-inline void OutputLerp32x4x1(const InterpolationCache<int32>& xs,\n+inline void OutputLerp32x4x1(const InterpolationCache<int32_t>& xs,\n                              const int64_t x_start, const int32_t ys_ilerp,\n                              const float min, const float max,\n                              const qint32* const ys_input_lower_ptr,\n@@ -373,15 +373,15 @@ inline void OutputLerp32x4x1(const InterpolationCache<int32>& xs,\n \n #else\n   for (int x = x_start; x < x_start + 4; ++x) {\n-    OutputLerpForChannels<RESOLUTION, qint32, int32, int64_t>(\n+    OutputLerpForChannels<RESOLUTION, qint32, int32_t, int64_t>(\n         xs, x, ys_ilerp, 1, min, max, ys_input_lower_ptr, ys_input_upper_ptr,\n         output_y_ptr);\n   }\n #endif\n }\n \n template <int RESOLUTION>\n-inline void OutputLerp32x4x3(const InterpolationCache<int32>& xs,\n+inline void OutputLerp32x4x3(const InterpolationCache<int32_t>& xs,\n                              const int64_t x_start, const int32_t ys_ilerp,\n                              const float min, const float max,\n                              const qint32* const ys_input_lower_ptr,\n@@ -458,7 +458,7 @@ inline void OutputLerp32x4x3(const InterpolationCache<int32>& xs,\n \n #else\n   for (int x = x_start; x < x_start + 4; ++x) {\n-    OutputLerpForChannels<RESOLUTION, qint32, int32, int64_t>(\n+    OutputLerpForChannels<RESOLUTION, qint32, int32_t, int64_t>(\n         xs, x, ys_ilerp, 3, min, max, ys_input_lower_ptr, ys_input_upper_ptr,\n         output_y_ptr);\n   }\n@@ -543,10 +543,10 @@ void ResizeImage<qint32>(typename TTypes<qint32, 4>::ConstTensor images,\n \n   CHECK_NOTNULL(output);\n \n-  const InterpolationCache<int32> xs =\n-      BuildLerpCache<int32>(out_width, in_width, width_scale, channels,\n-                            RESOLUTION, half_pixel_centers);\n-  const InterpolationCache<int32> ys = BuildLerpCache<int32>(\n+  const InterpolationCache<int32_t> xs =\n+      BuildLerpCache<int32_t>(out_width, in_width, width_scale, channels,\n+                              RESOLUTION, half_pixel_centers);\n+  const InterpolationCache<int32_t> ys = BuildLerpCache<int32_t>(\n       out_height, in_height, height_scale, 1, RESOLUTION, half_pixel_centers);\n \n   const int64_t in_row_size = in_width * channels;\n@@ -581,7 +581,7 @@ void ResizeImage<qint32>(typename TTypes<qint32, 4>::ConstTensor images,\n         }\n       }\n       for (; x < out_width; ++x) {\n-        OutputLerpForChannels<RESOLUTION, qint32, int32, int64_t>(\n+        OutputLerpForChannels<RESOLUTION, qint32, int32_t, int64_t>(\n             xs, x, ys_ilerp, channels, in_min, in_max, ys_input_lower_ptr,\n             ys_input_upper_ptr, output_y_ptr);\n       }\n@@ -606,10 +606,10 @@ void ResizeImage<quint8>(typename TTypes<quint8, 4>::ConstTensor images,\n \n   CHECK_NOTNULL(output);\n \n-  const InterpolationCache<int16> xs =\n-      BuildLerpCache<int16>(out_width, in_width, width_scale, channels,\n-                            RESOLUTION, half_pixel_centers);\n-  const InterpolationCache<int16> ys = BuildLerpCache<int16>(\n+  const InterpolationCache<int16_t> xs =\n+      BuildLerpCache<int16_t>(out_width, in_width, width_scale, channels,\n+                              RESOLUTION, half_pixel_centers);\n+  const InterpolationCache<int16_t> ys = BuildLerpCache<int16_t>(\n       out_height, in_height, height_scale, 1, RESOLUTION, half_pixel_centers);\n \n   const int64_t in_row_size = in_width * channels;\n@@ -646,7 +646,7 @@ void ResizeImage<quint8>(typename TTypes<quint8, 4>::ConstTensor images,\n         }\n       }\n       for (; x < out_width; ++x) {\n-        OutputLerpForChannels<RESOLUTION, quint8, int16, int16>(\n+        OutputLerpForChannels<RESOLUTION, quint8, int16_t, int16_t>(\n             xs, x, ys_ilerp, channels, in_min, in_max, ys_input_lower_ptr,\n             ys_input_upper_ptr, output_y_ptr);\n       }"
        },
        {
            "sha": "8c2426ee6621b781126e3f78a40705725b1923b7",
            "filename": "tensorflow/core/kernels/quantized_resize_bilinear_op_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_resize_bilinear_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fquantized_resize_bilinear_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fquantized_resize_bilinear_op_test.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -171,8 +171,8 @@ void CheckTensorValue(const T* in_data, const T* out_data, const int batch_size,\n           const float val = QuantizedToFloat<T>(qval, min, max);\n           if (!relative) {\n             const int q_tolerance = std::round(tolerance);\n-            EXPECT_TRUE(std::abs(static_cast<int32>(ref_qval) -\n-                                 static_cast<int32>(qval)) <= q_tolerance)\n+            EXPECT_TRUE(std::abs(static_cast<int32_t>(ref_qval) -\n+                                 static_cast<int32_t>(qval)) <= q_tolerance)\n                 << \"ref = \" << ref_val << \", val = \" << val << \", \" << b << \", \"\n                 << y << \", \" << x << \", \" << c << \", qval = \" << qval\n                 << \", ref qval = \" << ref_qval << \", \" << q_tolerance;\n@@ -197,7 +197,7 @@ void TestResizeBilinear(const Tensor& image_tensor, const DataType dt,\n   Scope root = Scope::NewRootScope();\n \n   Output placeholder = ops::Placeholder(root.WithOpName(\"placeholder\"), dt);\n-  Output size = ops::Const<int32>(root.WithOpName(\"size\"), new_size);\n+  Output size = ops::Const<int32_t>(root.WithOpName(\"size\"), new_size);\n   Output in_min = ops::Const<float>(root.WithOpName(\"min\"), min);\n   Output in_max = ops::Const<float>(root.WithOpName(\"max\"), max);\n "
        },
        {
            "sha": "e55693b4d540d489dac18091038e4ec414262a53",
            "filename": "tensorflow/core/kernels/queue_base.h",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fqueue_base.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fqueue_base.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fqueue_base.h?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -46,7 +46,7 @@ class QueueBase : public QueueInterface {\n   //   name: A name to use for the queue.\n   QueueBase(int32_t capacity, const DataTypeVector& component_dtypes,\n             const std::vector<TensorShape>& component_shapes,\n-            const string& name);\n+            const std::string& name);\n \n   // Implementations of QueueInterface methods --------------------------------\n   const DataTypeVector& component_dtypes() const override {\n@@ -64,7 +64,7 @@ class QueueBase : public QueueInterface {\n     return component_shapes_;\n   }\n \n-  int32 capacity() const { return capacity_; }\n+  int32_t capacity() const { return capacity_; }\n \n   bool is_closed() const override {\n     mutex_lock lock(mu_);\n@@ -103,7 +103,7 @@ class QueueBase : public QueueInterface {\n   };\n \n   // Returns the number of components in a queue-element tuple.\n-  int32 num_components() const { return component_dtypes_.size(); }\n+  int32_t num_components() const { return component_dtypes_.size(); }\n \n   // True if shapes were specified.  If so, inputs will be validated\n   // against them, etc.\n@@ -135,26 +135,27 @@ class QueueBase : public QueueInterface {\n   ~QueueBase() override;\n \n   // Helpers for implementing MatchesNodeDef().\n-  static string ShapeListString(const absl::Span<const TensorShape>& shapes);\n+  static std::string ShapeListString(\n+      const absl::Span<const TensorShape>& shapes);\n   absl::Status MatchesNodeDefOp(const NodeDef& node_def,\n-                                const string& op) const;\n+                                const std::string& op) const;\n   absl::Status MatchesNodeDefCapacity(const NodeDef& node_def,\n                                       int32_t capacity) const;\n   absl::Status MatchesNodeDefTypes(const NodeDef& node_def) const;\n   absl::Status MatchesNodeDefShapes(const NodeDef& node_def) const;\n \n  protected:\n-  const int32 capacity_;\n+  const int32_t capacity_;\n   const DataTypeVector component_dtypes_;\n   const std::vector<TensorShape> component_shapes_;\n-  const string name_;\n+  const std::string name_;\n   mutable mutex mu_;\n   bool closed_ TF_GUARDED_BY(mu_);\n \n   struct Attempt;\n   typedef std::function<RunResult(Attempt*)> RunCallback;\n   struct Attempt {\n-    int32 elements_requested;\n+    int32_t elements_requested;\n     DoneCallback done_callback;  // must be run outside mu_\n     OpKernelContext* context;\n     CancellationManager* cancellation_manager;  // not owned"
        },
        {
            "sha": "4c5c1ee10b04332d6a5b9d697d326b8bc338d8af",
            "filename": "tensorflow/core/kernels/queue_op.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fqueue_op.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fqueue_op.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fqueue_op.h?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -39,7 +39,7 @@ class QueueOp : public ResourceOpKernel<QueueInterface> {\n \n  protected:\n   // Variables accessible by subclasses\n-  int32 capacity_;\n+  int32_t capacity_;\n   DataTypeVector component_types_;\n \n  private:"
        },
        {
            "sha": "9a951af9017a3650e81f733c4e0dcc09fcd65662",
            "filename": "tensorflow/core/kernels/ragged_range_op_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fragged_range_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fragged_range_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fragged_range_op_test.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -90,10 +90,10 @@ TEST_F(RaggedRangeOpTest, RangeSizeOverflow) {\n }\n \n TEST_F(RaggedRangeOpTest, RangeSizeOverflow2) {\n-  BuildRaggedRangeGraph<int64>();\n-  AddInputFromArray<int64>(TensorShape({}), {static_cast<int64_t>(5e18)});\n-  AddInputFromArray<int64>(TensorShape({}), {static_cast<int64_t>(-5e18)});\n-  AddInputFromArray<int64>(TensorShape({}), {-1});\n+  BuildRaggedRangeGraph<int64_t>();\n+  AddInputFromArray<int64_t>(TensorShape({}), {static_cast<int64_t>(5e18)});\n+  AddInputFromArray<int64_t>(TensorShape({}), {static_cast<int64_t>(-5e18)});\n+  AddInputFromArray<int64_t>(TensorShape({}), {-1});\n \n   EXPECT_EQ(absl::StrCat(\"Requires ((limit - start) / delta) <= \",\n                          std::numeric_limits<int64_t>::max()),"
        },
        {
            "sha": "ffb186af87ece45c8ef8d23cbd7af5da83cce607",
            "filename": "tensorflow/core/kernels/ragged_tensor_to_sparse_kernel.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_sparse_kernel.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_sparse_kernel.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_sparse_kernel.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -228,8 +228,8 @@ class RaggedTensorToSparseOp : public OpKernel {\n \n REGISTER_KERNEL_BUILDER(Name(\"RaggedTensorToSparse\")\n                             .Device(DEVICE_CPU)\n-                            .TypeConstraint<int32>(\"Tsplits\"),\n-                        RaggedTensorToSparseOp<int32>);\n+                            .TypeConstraint<int32_t>(\"Tsplits\"),\n+                        RaggedTensorToSparseOp<int32_t>);\n \n REGISTER_KERNEL_BUILDER(Name(\"RaggedTensorToSparse\")\n                             .Device(DEVICE_CPU)"
        },
        {
            "sha": "28820593a4b5c5a198f78afc3e06b829427d9b31",
            "filename": "tensorflow/core/kernels/ragged_tensor_to_tensor_op.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_tensor_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_tensor_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_tensor_op.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -445,8 +445,8 @@ void copy_array<tstring, int64_t>(tstring* dst, const tstring* src,\n }\n \n template <>\n-void copy_array<tstring, int32>(tstring* dst, const tstring* src,\n-                                int32_t size) {\n+void copy_array<tstring, int32_t>(tstring* dst, const tstring* src,\n+                                  int32_t size) {\n   slow_copy_array(dst, src, size);\n }\n \n@@ -460,8 +460,8 @@ void copy_array<Eigen::half, int64_t>(Eigen::half* dst, const Eigen::half* src,\n }\n \n template <>\n-void copy_array<Eigen::half, int32>(Eigen::half* dst, const Eigen::half* src,\n-                                    int32_t size) {\n+void copy_array<Eigen::half, int32_t>(Eigen::half* dst, const Eigen::half* src,\n+                                      int32_t size) {\n   slow_copy_array(dst, src, size);\n }\n "
        },
        {
            "sha": "e23a2c07ed861bd9d7fb2a13ca57d4cad007b577",
            "filename": "tensorflow/core/kernels/ragged_tensor_to_tensor_op_test.cc",
            "status": "modified",
            "additions": 84,
            "deletions": 63,
            "changes": 147,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_tensor_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_tensor_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_tensor_op_test.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -51,7 +51,8 @@ class RaggedTensorToTensorOpTest : public ::tensorflow::OpsTestBase {\n   // Builds the tensorflow test graph for RaggedTensorToTensor.\n   template <typename VALUE_TYPE, typename INDEX_TYPE>\n   void BuildRaggedTensorToTensorGraph(\n-      const TensorShape& shape, const std::vector<string>& row_partition_types,\n+      const TensorShape& shape,\n+      const std::vector<std::string>& row_partition_types,\n       const ShapeAndValues<VALUE_TYPE>& values,\n       const ShapeAndValues<VALUE_TYPE>& default_value,\n       const std::vector<ShapeAndValues<INDEX_TYPE>>& row_partition_tensors) {\n@@ -95,12 +96,13 @@ TEST_F(RaggedTensorToTensorOpTest, RaggedTensorToTensor) {\n   // indices = [2, 1, 0, 3]\n   // params = [[.1, .2, .3], [], [.4, .5, .6, .7], [.8, .9]]\n   // params.shape = [4, None]\n-  BuildRaggedTensorToTensorGraph<float, int32>(\n+  BuildRaggedTensorToTensorGraph<float, int32_t>(\n       TensorShape({4, 4}),                 // shape\n       {\"FIRST_DIM_SIZE\", \"VALUE_ROWIDS\"},  // row_partition_types\n       createVector<float>({.1, .2, .3, .4, .5, .6, .7, .8, .9}),  // values\n       createScalar<float>(1.5),  // default_value\n-      {createScalar<int32>(4), createVector<int32>({0, 0, 0, 2, 2, 2, 2, 3, 3})}\n+      {createScalar<int32_t>(4),\n+       createVector<int32_t>({0, 0, 0, 2, 2, 2, 2, 3, 3})}\n       // row_partition_tensors\n   );\n \n@@ -117,12 +119,12 @@ TEST_F(RaggedTensorToTensorOpTest, RaggedTensorToTensor) {\n TEST_F(RaggedTensorToTensorOpTest, RaggedTensorToTensorRowSplits) {\n   // indices = [2, 1, 0, 3]\n   // params = [[.1, .2, .3], [], [.4, .5, .6, .7], [.8, .9]]\n-  BuildRaggedTensorToTensorGraph<float, int32>(\n+  BuildRaggedTensorToTensorGraph<float, int32_t>(\n       TensorShape({4, 4}),  // shape\n       {\"ROW_SPLITS\"},       // row_partition_types\n       createVector<float>({.1, .2, .3, .4, .5, .6, .7, .8, .9}),  // values\n-      createScalar<float>(1.5),               // default_value\n-      {createVector<int32>({0, 3, 3, 7, 9})}  // row_partition_tensors\n+      createScalar<float>(1.5),                 // default_value\n+      {createVector<int32_t>({0, 3, 3, 7, 9})}  // row_partition_tensors\n   );\n \n   TF_ASSERT_OK(RunOpKernel());\n@@ -143,16 +145,16 @@ TEST_F(RaggedTensorToTensorOpTest, RaggedTensorToTensor_3DParams) {\n   //           [[.4, .5], [.6, .7, .8]],\n   //           [[.9]]\n   //          ]\n-  BuildRaggedTensorToTensorGraph<float, int32>(\n+  BuildRaggedTensorToTensorGraph<float, int32_t>(\n       TensorShape({5, 2, 3}),  // shape\n       {\"FIRST_DIM_SIZE\", \"VALUE_ROWIDS\",\n        \"VALUE_ROWIDS\"},  // row_partition_types\n       createVector<float>({.1, .2, .3, .4, .5, .6, .7, .8, .9}),  // values\n       createScalar<float>(1.5),  // default_value\n       {\n-          createScalar<int32>(5),\n-          createVector<int32>({0, 1, 1, 3, 3, 4}),\n-          createVector<int32>({1, 1, 2, 3, 3, 4, 4, 4, 5}),\n+          createScalar<int32_t>(5),\n+          createVector<int32_t>({0, 1, 1, 3, 3, 4}),\n+          createVector<int32_t>({1, 1, 2, 3, 3, 4, 4, 4, 5}),\n       }  // row_partition_tensors\n   );\n   TF_ASSERT_OK(RunOpKernel());\n@@ -181,14 +183,14 @@ TEST_F(RaggedTensorToTensorOpTest, RaggedTensorToTensor_3DParamsRowSplits) {\n   //           [[.4, .5], [.6, .7, .8]],\n   //           [[.9]]\n   //          ]\n-  BuildRaggedTensorToTensorGraph<float, int32>(\n+  BuildRaggedTensorToTensorGraph<float, int32_t>(\n       TensorShape({5, 2, 3}),        // shape\n       {\"ROW_SPLITS\", \"ROW_SPLITS\"},  // row_partition_types\n       createVector<float>({.1, .2, .3, .4, .5, .6, .7, .8, .9}),  // values\n       createScalar<float>(1.5),  // default_value\n       {\n-          createVector<int32>({0, 1, 3, 3, 5, 6}),\n-          createVector<int32>({0, 0, 2, 3, 5, 8, 9}),\n+          createVector<int32_t>({0, 1, 3, 3, 5, 6}),\n+          createVector<int32_t>({0, 0, 2, 3, 5, 8, 9}),\n       }  // row_partition_tensors\n   );\n   TF_ASSERT_OK(RunOpKernel());\n@@ -249,15 +251,16 @@ TEST_F(RaggedTensorToTensorOpTest, RaggedTensorToTensor_4DParams) {\n   //            []\n   // ]\n   // params.shape = [3, 2, 3, 2]\n-  BuildRaggedTensorToTensorGraph<int32, int32>(\n+  BuildRaggedTensorToTensorGraph<int32_t, int32_t>(\n       TensorShape({4, 2, 3, 2}),  // shape\n       {\"FIRST_DIM_SIZE\", \"VALUE_ROWIDS\", \"VALUE_ROWIDS\",\n-       \"VALUE_ROWIDS\"},                               // row_partition_types\n-      createVector<int32>({1, 2, 3, 4, 5, 6, 7, 8}),  // values\n-      createScalar<int32>(15),                        // default_value\n-      {createScalar<int32>(5), createVector<int32>({0, 1, 1}),\n-       createVector<int32>({1, 1, 1, 2}),\n-       createVector<int32>({0, 0, 1, 1, 2, 2, 3, 3})}  // row_partition_tensors\n+       \"VALUE_ROWIDS\"},                                 // row_partition_types\n+      createVector<int32_t>({1, 2, 3, 4, 5, 6, 7, 8}),  // values\n+      createScalar<int32_t>(15),                        // default_value\n+      {createScalar<int32_t>(5), createVector<int32_t>({0, 1, 1}),\n+       createVector<int32_t>({1, 1, 1, 2}),\n+       createVector<int32_t>({0, 0, 1, 1, 2, 2, 3, 3})}\n+      // row_partition_tensors\n   );\n \n   TF_ASSERT_OK(RunOpKernel());\n@@ -277,9 +280,9 @@ TEST_F(RaggedTensorToTensorOpTest, RaggedTensorToTensor_4DParams) {\n   //             [[15,15],[15,15],[15,15]],\n   //           ]\n   // params.shape = [3, 2, 3, 2]\n-  test::ExpectTensorEqual<int32>(\n+  test::ExpectTensorEqual<int32_t>(\n       *GetOutput(0),\n-      test::AsTensor<int32>(\n+      test::AsTensor<int32_t>(\n           {15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 1,  2,  3,  4,\n            5,  6,  7,  8,  15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n            15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15},\n@@ -296,14 +299,14 @@ TEST_F(RaggedTensorToTensorOpTest, RaggedTensorToTensor_4DParamsRowSplit) {\n   //            []\n   // ]\n   // params.shape = [3, 2, 3, 2]\n-  BuildRaggedTensorToTensorGraph<int32, int32>(\n+  BuildRaggedTensorToTensorGraph<int32_t, int32_t>(\n       TensorShape({4, 2, 3, 2}),  // shape\n       {\"ROW_SPLITS\", \"ROW_SPLITS\", \"ROW_SPLITS\"},\n       // row_partition_types\n-      createVector<int32>({1, 2, 3, 4, 5, 6, 7, 8}),  // values\n-      createScalar<int32>(15),                        // default_value\n-      {createVector<int32>({0, 1, 3}), createVector<int32>({0, 0, 3, 4}),\n-       createVector<int32>({0, 2, 4, 6, 8})}  // row_partition_tensors\n+      createVector<int32_t>({1, 2, 3, 4, 5, 6, 7, 8}),  // values\n+      createScalar<int32_t>(15),                        // default_value\n+      {createVector<int32_t>({0, 1, 3}), createVector<int32_t>({0, 0, 3, 4}),\n+       createVector<int32_t>({0, 2, 4, 6, 8})}  // row_partition_tensors\n   );\n \n   TF_ASSERT_OK(RunOpKernel());\n@@ -323,9 +326,9 @@ TEST_F(RaggedTensorToTensorOpTest, RaggedTensorToTensor_4DParamsRowSplit) {\n   //             [[15,15],[15,15],[15,15]],\n   //           ]\n   // params.shape = [3, 2, 3, 2]\n-  test::ExpectTensorEqual<int32>(\n+  test::ExpectTensorEqual<int32_t>(\n       *GetOutput(0),\n-      test::AsTensor<int32>(\n+      test::AsTensor<int32_t>(\n           {15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 1,  2,  3,  4,\n            5,  6,  7,  8,  15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n            15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15},\n@@ -334,12 +337,13 @@ TEST_F(RaggedTensorToTensorOpTest, RaggedTensorToTensor_4DParamsRowSplit) {\n \n TEST_F(RaggedTensorToTensorOpTest, RaggedTensorToTensorContractExpanded) {\n   // params = [[.1, .2, .3], [], [.4, .5, .6, .7], [.8, .9]]\n-  BuildRaggedTensorToTensorGraph<float, int32>(\n+  BuildRaggedTensorToTensorGraph<float, int32_t>(\n       TensorShape({3, 5}),                 // shape\n       {\"FIRST_DIM_SIZE\", \"VALUE_ROWIDS\"},  // row_partition_types\n       createVector<float>({.1, .2, .3, .4, .5, .6, .7, .8, .9}),  // values\n       createScalar<float>(1.5),  // default_value\n-      {createScalar<int32>(4), createVector<int32>({0, 0, 0, 2, 2, 2, 2, 3, 3})}\n+      {createScalar<int32_t>(4),\n+       createVector<int32_t>({0, 0, 0, 2, 2, 2, 2, 3, 3})}\n       // row_partition_tensors\n   );\n \n@@ -357,14 +361,15 @@ TEST_F(RaggedTensorToTensorOpTest, RaggedTensorToTensorContractExpanded) {\n // Adds a dense dimension.\n TEST_F(RaggedTensorToTensorOpTest, RaggedTensorToTensorContractExpandedDense) {\n   // params = [[.1, .2, .3], [], [.4, .5, .6, .7], [.8, .9]]\n-  BuildRaggedTensorToTensorGraph<float, int32>(\n+  BuildRaggedTensorToTensorGraph<float, int32_t>(\n       TensorShape({3, 5, 2}),              // shape\n       {\"FIRST_DIM_SIZE\", \"VALUE_ROWIDS\"},  // row_partition_types\n       ShapeAndValues<float>{TensorShape({9, 2}),\n                             {.1, 1.1, .2, 1.2, .3, 1.3, .4, 1.4, .5, 1.5, .6,\n                              1.6, .7, 1.7, .8, 1.8, .9, 1.9}},  // values\n       createScalar<float>(1.5),                                 // default_value\n-      {createScalar<int32>(4), createVector<int32>({0, 0, 0, 2, 2, 2, 2, 3, 3})}\n+      {createScalar<int32_t>(4),\n+       createVector<int32_t>({0, 0, 0, 2, 2, 2, 2, 3, 3})}\n       // row_partition_tensors\n   );\n \n@@ -386,12 +391,13 @@ TEST_F(RaggedTensorToTensorOpTest, RaggedTensorToTensorConstrained) {\n   //           [.4, .5, .6, .7],\n   //           [.8, .9]]\n   // constrained to (3, 3)\n-  BuildRaggedTensorToTensorGraph<float, int32>(\n+  BuildRaggedTensorToTensorGraph<float, int32_t>(\n       TensorShape({3, 3}),                 // shape\n       {\"FIRST_DIM_SIZE\", \"VALUE_ROWIDS\"},  // row_partition_types\n       createVector<float>({.1, .2, .3, .4, .5, .6, .7, .8, .9}),  // values\n       createScalar<float>(1.5),  // default_value\n-      {createScalar<int32>(4), createVector<int32>({0, 0, 0, 2, 2, 2, 2, 3, 3})}\n+      {createScalar<int32_t>(4),\n+       createVector<int32_t>({0, 0, 0, 2, 2, 2, 2, 3, 3})}\n       // row_partition_tensors\n   );\n \n@@ -418,16 +424,16 @@ TEST_F(RaggedTensorToTensorOpTest, RaggedTensorToTensor_3DParamsConstrained) {\n   //           [[.9]]\n   //          ]\n   // params.shape = [5, None, None]\n-  BuildRaggedTensorToTensorGraph<float, int32>(\n+  BuildRaggedTensorToTensorGraph<float, int32_t>(\n       TensorShape({4, 1, 2}),  // shape\n       {\"FIRST_DIM_SIZE\", \"VALUE_ROWIDS\",\n        \"VALUE_ROWIDS\"},  // row_partition_types\n       createVector<float>({.1, .2, .3, .4, .5, .6, .7, .8, .9}),  // values\n       createScalar<float>(1.5),  // default_value\n       {\n-          createScalar<int32>(5),\n-          createVector<int32>({0, 1, 1, 3, 3, 4}),\n-          createVector<int32>({1, 1, 2, 3, 3, 4, 4, 4, 5}),\n+          createScalar<int32_t>(5),\n+          createVector<int32_t>({0, 1, 1, 3, 3, 4}),\n+          createVector<int32_t>({1, 1, 2, 3, 3, 4, 4, 4, 5}),\n       }  // row_partition_tensors\n   );\n   TF_ASSERT_OK(RunOpKernel());\n@@ -457,15 +463,16 @@ TEST_F(RaggedTensorToTensorOpTest, RaggedTensorToTensor_4DParamsConstrained) {\n   //            []\n   // ]\n   // params.shape = [3, 2, 3, 2]\n-  BuildRaggedTensorToTensorGraph<int32, int32>(\n+  BuildRaggedTensorToTensorGraph<int32_t, int32_t>(\n       TensorShape({2, 2, 2, 2}),  // shape\n       {\"FIRST_DIM_SIZE\", \"VALUE_ROWIDS\", \"VALUE_ROWIDS\",\n-       \"VALUE_ROWIDS\"},                               // row_partition_types\n-      createVector<int32>({1, 2, 3, 4, 5, 6, 7, 8}),  // values\n-      createScalar<int32>(15),                        // default_value\n-      {createScalar<int32>(5), createVector<int32>({0, 1, 1}),\n-       createVector<int32>({1, 1, 1, 2}),\n-       createVector<int32>({0, 0, 1, 1, 2, 2, 3, 3})}  // row_partition_tensors\n+       \"VALUE_ROWIDS\"},                                 // row_partition_types\n+      createVector<int32_t>({1, 2, 3, 4, 5, 6, 7, 8}),  // values\n+      createScalar<int32_t>(15),                        // default_value\n+      {createScalar<int32_t>(5), createVector<int32_t>({0, 1, 1}),\n+       createVector<int32_t>({1, 1, 1, 2}),\n+       createVector<int32_t>({0, 0, 1, 1, 2, 2, 3, 3})}\n+      // row_partition_tensors\n   );\n \n   TF_ASSERT_OK(RunOpKernel());\n@@ -480,25 +487,38 @@ TEST_F(RaggedTensorToTensorOpTest, RaggedTensorToTensor_4DParamsConstrained) {\n   //           ],\n   //          ]\n   // params.shape = [3, 2, 3, 2]\n-  test::ExpectTensorEqual<int32>(*GetOutput(0), test::AsTensor<int32>(\n-                                                    {\n-                                                        15, 15, 15, 15,  //\n-                                                        15, 15, 15, 15,  //\n-                                                        1, 2, 3, 4,      //\n-                                                        7, 8, 15, 15,    //\n-                                                    },\n-                                                    TensorShape({2, 2, 2, 2})));\n+  test::ExpectTensorEqual<int32_t>(*GetOutput(0),\n+                                   test::AsTensor<int32_t>(\n+                                       {\n+                                           15,\n+                                           15,\n+                                           15,\n+                                           15,  //\n+                                           15,\n+                                           15,\n+                                           15,\n+                                           15,  //\n+                                           1,\n+                                           2,\n+                                           3,\n+                                           4,  //\n+                                           7,\n+                                           8,\n+                                           15,\n+                                           15,  //\n+                                       },\n+                                       TensorShape({2, 2, 2, 2})));\n }\n \n TEST_F(RaggedTensorToTensorOpTest, ShapeWrongDimensions) {\n-  BuildRaggedTensorToTensorGraph<int32, int32>(\n+  BuildRaggedTensorToTensorGraph<int32_t, int32_t>(\n       TensorShape({10, 7, 10, 20}),  // shape\n       {\"FIRST_DIM_SIZE\", \"VALUE_ROWIDS\",\n-       \"VALUE_ROWIDS\"},                   // row_partition_types\n-      createVector<int32>({1, 2, 3, 4}),  // values\n-      createScalar<int32>(15),            // default_value\n-      {createScalar<int32>(5), createVector<int32>({0, 1, 1}),\n-       createVector<int32>({1, 1, 1, 2})}  // row_partition_tensors\n+       \"VALUE_ROWIDS\"},                     // row_partition_types\n+      createVector<int32_t>({1, 2, 3, 4}),  // values\n+      createScalar<int32_t>(15),            // default_value\n+      {createScalar<int32_t>(5), createVector<int32_t>({0, 1, 1}),\n+       createVector<int32_t>({1, 1, 1, 2})}  // row_partition_tensors\n   );\n   // Fails with an invalid argument.\n   EXPECT_EQ(absl::IsInvalidArgument(RunOpKernel()), true);\n@@ -508,7 +528,7 @@ class RaggedTensorToTensorOpUnknownShapeTest\n     : public ::tensorflow::OpsTestBase {\n  protected:\n   std::unique_ptr<ShapeInferenceTestOp> op_;\n-  void SetAttributes(const absl::Span<const string> row_partition_types,\n+  void SetAttributes(const absl::Span<const std::string> row_partition_types,\n                      int num_row_partition_tensors) {\n     op_ = std::make_unique<ShapeInferenceTestOp>(\"RaggedTensorToTensor\");\n     SetAttrValue(row_partition_types,\n@@ -519,7 +539,8 @@ class RaggedTensorToTensorOpUnknownShapeTest\n };\n \n TEST_F(RaggedTensorToTensorOpUnknownShapeTest, ValueRowIDs) {\n-  SetAttributes(absl::Span<const string>{\"FIRST_DIM_SIZE\", \"VALUE_ROWIDS\"}, 2);\n+  SetAttributes(absl::Span<const std::string>{\"FIRST_DIM_SIZE\", \"VALUE_ROWIDS\"},\n+                2);\n \n   INFER_OK(*op_, \"?;?;?;?;?\", \"?\");\n   INFER_OK(*op_, \"?;[6];[];[];[6]\", \"[?,?]\");\n@@ -544,7 +565,7 @@ TEST_F(RaggedTensorToTensorOpUnknownShapeTest, ValueRowIDs) {\n TEST_F(RaggedTensorToTensorOpUnknownShapeTest, RowSplits) {\n   // RaggedTensorToTensor(param_splits+, param_values, indices) -> [splits+,\n   // values]\n-  SetAttributes(absl::Span<const string>{\"ROW_SPLITS\"}, 1);\n+  SetAttributes(absl::Span<const std::string>{\"ROW_SPLITS\"}, 1);\n \n   // value, default_value, ROW_SPLITS\n   INFER_OK(*op_, \"?;?;?;?\", \"?\");"
        },
        {
            "sha": "a46f40d177778cc7a47107607e50a5f5060e652c",
            "filename": "tensorflow/core/kernels/ragged_tensor_to_variant_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_variant_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_variant_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_variant_op.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -256,7 +256,7 @@ class RaggedTensorToVariantGradientOp : public OpKernel {\n     auto flat_row_splits = row_splits.flat<SPLIT_TYPE>();\n     TensorShape dense_values_shape;\n     OP_REQUIRES_OK(context,\n-                   TensorShapeUtils::MakeShape(context->input(2).vec<int32>(),\n+                   TensorShapeUtils::MakeShape(context->input(2).vec<int32_t>(),\n                                                &dense_values_shape));\n \n     // Validate row_splits."
        },
        {
            "sha": "f25f8b34198702ecf8d794196425be809fe56278",
            "filename": "tensorflow/core/kernels/ragged_tensor_to_variant_op_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_variant_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_variant_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_variant_op_test.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -232,20 +232,20 @@ TEST_F(RaggedTensorToVariantKernelTest, NonEmptyBatchedInputInt32Splits) {\n   const std::vector<int> batched_values = {0, 1, 1, 2, 2, 3, 4,\n                                            5, 6, 7, 8, 9, 8, 9};\n \n-  BuildEncodeRaggedTensorGraph<int, int32>(\n+  BuildEncodeRaggedTensorGraph<int, int32_t>(\n       {batched_splits_1, batched_splits_2, batched_splits_3}, TensorShape({14}),\n       batched_values, true);\n   TF_ASSERT_OK(RunOpKernel());\n \n   const auto& encoded_list = GetOutput(0)->vec<Variant>();\n   EXPECT_EQ(encoded_list.size(), 2);\n \n-  ExpectRaggedTensorVariantEqual<int, int32>(\n-      CreateVariantFromRagged<int, int32>(\n+  ExpectRaggedTensorVariantEqual<int, int32_t>(\n+      CreateVariantFromRagged<int, int32_t>(\n           {{0, 1, 3, 4, 5, 6}, {0, 2, 3, 4, 5, 6, 7}}, {0, 1, 1, 2, 2, 3, 4}),\n       *encoded_list(0).get<RaggedTensorVariant>());\n-  ExpectRaggedTensorVariantEqual<int, int32>(\n-      CreateVariantFromRagged<int, int32>(\n+  ExpectRaggedTensorVariantEqual<int, int32_t>(\n+      CreateVariantFromRagged<int, int32_t>(\n           {{0, 1, 2, 3, 4, 5}, {0, 1, 2, 5, 6, 7}}, {5, 6, 7, 8, 9, 8, 9}),\n       *encoded_list(1).get<RaggedTensorVariant>());\n }"
        },
        {
            "sha": "87cfc50f8a268ae3fab04ff50893edd57ea6f1a7",
            "filename": "tensorflow/core/kernels/ragged_tensor_to_variant_op_test.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_variant_op_test.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_variant_op_test.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_variant_op_test.h?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -140,7 +140,7 @@ class RaggedTensorToVariantGradientKernelTest\n   void BuildEncodeRaggedTensorGradientGraph(\n       const std::vector<Variant>& encoded_ragged_grad,\n       const std::vector<SPLIT_TYPE>& row_splits,\n-      const std::vector<int32>& dense_values_shape) {\n+      const std::vector<int32_t>& dense_values_shape) {\n     const auto values_dtype = DataTypeToEnum<VALUE_TYPE>::v();\n     const auto splits_dtype = DataTypeToEnum<SPLIT_TYPE>::v();\n \n@@ -161,8 +161,8 @@ class RaggedTensorToVariantGradientKernelTest\n     AddInputFromArray<SPLIT_TYPE>(TensorShape({splits_size}), row_splits);\n \n     int64_t dense_values_shape_size = dense_values_shape.size();\n-    AddInputFromArray<int32>(TensorShape({dense_values_shape_size}),\n-                             dense_values_shape);\n+    AddInputFromArray<int32_t>(TensorShape({dense_values_shape_size}),\n+                               dense_values_shape);\n   }\n \n   template <typename VALUE_TYPE, typename SPLIT_TYPE>"
        },
        {
            "sha": "5608888b5500d11ed421b2bbe5917148690ddc23",
            "filename": "tensorflow/core/kernels/ragged_tensor_variant.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_variant.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_variant.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_variant.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -22,9 +22,11 @@ limitations under the License.\n \n namespace tensorflow {\n \n-string RaggedTensorVariant::TypeName() const { return \"RaggedTensorVariant\"; }\n+std::string RaggedTensorVariant::TypeName() const {\n+  return \"RaggedTensorVariant\";\n+}\n \n-string RaggedTensorVariant::DebugString() const {\n+std::string RaggedTensorVariant::DebugString() const {\n   return absl::StrCat(\n       \"RaggedTensorVariant(dtype=\", DataTypeString(values_.dtype()),\n       \", ragged_rank=\", nested_splits_.size(), \", splits_dtype=\","
        },
        {
            "sha": "c75505a603c531ac817b25cff02aa4d124d6a011",
            "filename": "tensorflow/core/kernels/ragged_tensor_variant.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_variant.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_variant.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_variant.h?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -41,8 +41,8 @@ class RaggedTensorVariant {\n       : values_(std::move(values)), nested_splits_(nested_splits) {}\n \n   // Variant support methods.\n-  string TypeName() const;\n-  string DebugString() const;\n+  std::string TypeName() const;\n+  std::string DebugString() const;\n   void Encode(VariantTensorData* data) const;\n   bool Decode(const VariantTensorData& data);\n "
        },
        {
            "sha": "875744b86ecf47c4497a278b2bbd0f8c9e60a800",
            "filename": "tensorflow/core/kernels/random_binomial_op.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 13,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_binomial_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_binomial_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frandom_binomial_op.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -360,8 +360,8 @@ class RandomBinomialOp : public OpKernel {\n     TensorShape bcast_shape = BCast::ToShape(bcast.output_shape());\n     TensorShape output_shape;\n     if (shape_tensor.dtype() == DataType::DT_INT32) {\n-      OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(shape_tensor.vec<int32>(),\n-                                                      &output_shape));\n+      OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(\n+                              shape_tensor.vec<int32_t>(), &output_shape));\n     } else {\n       OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(\n                               shape_tensor.vec<int64_t>(), &output_shape));\n@@ -380,11 +380,11 @@ class RandomBinomialOp : public OpKernel {\n     const int64_t num_sample_dims =\n         (shape_tensor.dim_size(0) - bcast.output_shape().size());\n     for (int64_t i = 0; i < num_sample_dims; ++i) {\n-      samples_per_batch *= shape_tensor.flat<int32>()(i);\n+      samples_per_batch *= shape_tensor.flat<int32_t>()(i);\n     }\n     int64_t num_batches = 1;\n     for (int64_t i = num_sample_dims; i < shape_tensor.dim_size(0); ++i) {\n-      num_batches *= shape_tensor.flat<int32>()(i);\n+      num_batches *= shape_tensor.flat<int32_t>()(i);\n     }\n     const int64_t num_elements = num_batches * samples_per_batch;\n \n@@ -409,8 +409,9 @@ class RandomBinomialOp : public OpKernel {\n                 errors::InvalidArgument(\"Unsupported algorithm id: \", alg));\n     static_assert(std::is_same<StateElementType, int64_t>::value,\n                   \"StateElementType must be int64\");\n-    static_assert(std::is_same<PhiloxRandom::ResultElementType, uint32>::value,\n-                  \"PhiloxRandom::ResultElementType must be uint32\");\n+    static_assert(\n+        std::is_same<PhiloxRandom::ResultElementType, uint32_t>::value,\n+        \"PhiloxRandom::ResultElementType must be uint32\");\n     OP_REQUIRES(ctx, var_tensor_flat.size() >= PHILOX_MIN_STATE_SIZE,\n                 errors::InvalidArgument(\n                     \"For Philox algorithm, the size of state must be at least \",\n@@ -478,8 +479,8 @@ class StatelessRandomBinomialOp : public OpKernel {\n     TensorShape bcast_shape = BCast::ToShape(bcast.output_shape());\n     TensorShape output_shape;\n     if (shape_tensor.dtype() == DataType::DT_INT32) {\n-      OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(shape_tensor.vec<int32>(),\n-                                                      &output_shape));\n+      OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(\n+                              shape_tensor.vec<int32_t>(), &output_shape));\n     } else {\n       OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(\n                               shape_tensor.vec<int64_t>(), &output_shape));\n@@ -494,14 +495,14 @@ class StatelessRandomBinomialOp : public OpKernel {\n         (shape_tensor.dim_size(0) - bcast.output_shape().size());\n     for (int64_t i = 0; i < num_sample_dims; ++i) {\n       samples_per_batch *= shape_tensor.dtype() == DataType::DT_INT32\n-                               ? shape_tensor.flat<int32>()(i)\n-                               : shape_tensor.flat<int64>()(i);\n+                               ? shape_tensor.flat<int32_t>()(i)\n+                               : shape_tensor.flat<int64_t>()(i);\n     }\n     int64_t num_batches = 1;\n     for (int64_t i = num_sample_dims; i < shape_tensor.dim_size(0); ++i) {\n       num_batches *= shape_tensor.dtype() == DataType::DT_INT32\n-                         ? shape_tensor.flat<int32>()(i)\n-                         : shape_tensor.flat<int64>()(i);\n+                         ? shape_tensor.flat<int32_t>()(i)\n+                         : shape_tensor.flat<int64_t>()(i);\n     }\n     const int64_t num_elements = num_batches * samples_per_batch;\n \n@@ -557,7 +558,7 @@ class StatelessRandomBinomialOp : public OpKernel {\n REGISTER_ALL(Eigen::half);\n REGISTER_ALL(float);\n REGISTER_ALL(double);\n-REGISTER_ALL(int32);\n+REGISTER_ALL(int32_t);\n REGISTER_ALL(int64_t);\n \n #undef REGISTER"
        },
        {
            "sha": "9e715b5afccf9258c2522fc398f90ecd7e81674c",
            "filename": "tensorflow/core/kernels/random_binomial_op_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_binomial_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_binomial_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frandom_binomial_op_test.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -28,7 +28,7 @@ static Graph* RandomBinomialGraph(double count, double prob, int num_batches,\n                                   int samples_per_batch) {\n   Graph* g = new Graph(OpRegistry::Global());\n   Tensor shape_t(DT_INT32, TensorShape({2}));\n-  shape_t.flat<int32>().setValues({num_batches, samples_per_batch});\n+  shape_t.flat<int32_t>().setValues({num_batches, samples_per_batch});\n \n   Tensor counts_t(DT_FLOAT, TensorShape({num_batches}));\n   counts_t.flat<float>().setConstant(count);"
        },
        {
            "sha": "02458f4aa99f49a5cc58a9f11b43558fa18441ce",
            "filename": "tensorflow/core/kernels/random_index_shuffle_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_index_shuffle_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_index_shuffle_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frandom_index_shuffle_test.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -32,11 +32,11 @@ class RandomIndexShuffleTest : public ::testing::TestWithParam<uint64_t> {\n \n // Check that we do a correct bijection.\n TEST_P(RandomIndexShuffleTest, Bijection) {\n-  const std::array<uint32, 3>& key = {42, 73, 1991};\n+  const std::array<uint32_t, 3>& key = {42, 73, 1991};\n   const uint64_t max_value = GetMaxValue();\n   std::vector<bool> seen(max_value + 1, false);\n   for (uint64_t value = 0; value <= max_value; ++value) {\n-    const uint64 output_value =\n+    const uint64_t output_value =\n         index_shuffle(value, key, max_value, /* rounds= */ 4);\n     EXPECT_GE(output_value, 0);\n     EXPECT_LE(output_value, max_value);"
        },
        {
            "sha": "87179f9fef5e8f781b089193e217e7c7480c61e2",
            "filename": "tensorflow/core/kernels/random_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frandom_op.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -157,7 +157,7 @@ class RandomGammaOp : public OpKernel {\n                     shape_t.DebugString()));\n     TensorShape samples_shape;\n     if (shape_t.dtype() == DataType::DT_INT32) {\n-      auto vec = shape_t.flat<int32>();\n+      auto vec = shape_t.flat<int32_t>();\n       OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(vec.data(), vec.size(),\n                                                       &samples_shape));\n     } else if (shape_t.dtype() == DataType::DT_INT64) {"
        },
        {
            "sha": "1d6299802f21c7f988322f970241d7f85a9b7bac",
            "filename": "tensorflow/core/kernels/random_op.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_op.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_op.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frandom_op.h?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -40,8 +40,8 @@ typedef Eigen::ThreadPoolDevice CPUDevice;\n // nullptr, they provide the input; otherwise `gen` provides the input.\n template <class Distribution>\n struct FillPhiloxRandom<CPUDevice, Distribution> {\n-  void operator()(OpKernelContext* ctx, const CPUDevice& d, const uint64* key,\n-                  const uint64* counter, random::PhiloxRandom gen,\n+  void operator()(OpKernelContext* ctx, const CPUDevice& d, const uint64_t* key,\n+                  const uint64_t* counter, random::PhiloxRandom gen,\n                   typename Distribution::ResultElementType* data, int64_t size,\n                   Distribution dist);\n };"
        },
        {
            "sha": "7d7a16dcc6a3fcfd64a71143a533d33f812edfbb",
            "filename": "tensorflow/core/kernels/random_op_cpu.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_op_cpu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_op_cpu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frandom_op_cpu.h?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -60,8 +60,8 @@ using random::SingleSampleAdapter;\n template <typename Device, class Distribution>\n struct FillPhiloxRandom {\n   typedef typename Distribution::ResultElementType T;\n-  void operator()(OpKernelContext* ctx, const Device&, const uint64* key,\n-                  const uint64* counter, random::PhiloxRandom gen, T* data,\n+  void operator()(OpKernelContext* ctx, const Device&, const uint64_t* key,\n+                  const uint64_t* counter, random::PhiloxRandom gen, T* data,\n                   int64_t size, Distribution dist) {\n     OP_REQUIRES(\n         ctx, false,\n@@ -156,8 +156,8 @@ struct FillPhiloxRandomTask<Distribution, true> {\n // It splits the work into several tasks and run them in parallel\n template <class Distribution>\n void FillPhiloxRandom<CPUDevice, Distribution>::operator()(\n-    OpKernelContext* ctx, const CPUDevice&, const uint64* key,\n-    const uint64* counter, random::PhiloxRandom gen,\n+    OpKernelContext* ctx, const CPUDevice&, const uint64_t* key,\n+    const uint64_t* counter, random::PhiloxRandom gen,\n     typename Distribution::ResultElementType* data, int64_t size,\n     Distribution dist) {\n   if (key != nullptr && counter != nullptr) {"
        },
        {
            "sha": "5abe81f27f31e20f4f97df3c9d8908509321bf39",
            "filename": "tensorflow/core/kernels/random_op_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frandom_op_test.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -26,13 +26,13 @@ namespace tensorflow {\n namespace {\n \n Tensor VecShape(int64_t v) {\n-  if (v >= std::numeric_limits<int32>::max()) {\n+  if (v >= std::numeric_limits<int32_t>::max()) {\n     Tensor shape(DT_INT64, TensorShape({1}));\n     shape.vec<int64_t>()(0) = v;\n     return shape;\n   } else {\n     Tensor shape(DT_INT32, TensorShape({1}));\n-    shape.vec<int32>()(0) = v;\n+    shape.vec<int32_t>()(0) = v;\n     return shape;\n   }\n }"
        },
        {
            "sha": "c203181d5758187c35b039989c2cbe0f551f56bb",
            "filename": "tensorflow/core/kernels/random_ops_util.h",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_ops_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_ops_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frandom_ops_util.h?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -26,45 +26,46 @@ using random::PhiloxRandom;\n // The following 2 functions use the contract \"lower 32 bits for the first\n // uint32, higher 32 bits for the second\". Note that this is endian-neutral,\n // unlike a direct memory copy `memcpy(output, &input, 8)`.\n-PHILOX_DEVICE_INLINE void Uint64ToUint32s(uint64 input, uint32* output1,\n-                                          uint32* output2) {\n-  *output1 = static_cast<uint32>(input);\n-  *output2 = static_cast<uint32>(input >> 32);\n+PHILOX_DEVICE_INLINE void Uint64ToUint32s(uint64_t input, uint32_t* output1,\n+                                          uint32_t* output2) {\n+  *output1 = static_cast<uint32_t>(input);\n+  *output2 = static_cast<uint32_t>(input >> 32);\n }\n \n-PHILOX_DEVICE_INLINE uint64 Uint32sToUint64(uint32 input1, uint32 input2) {\n-  auto u64_1 = static_cast<uint64>(input1);\n-  auto u64_2 = static_cast<uint64>(input2);\n+PHILOX_DEVICE_INLINE uint64_t Uint32sToUint64(uint32_t input1,\n+                                              uint32_t input2) {\n+  auto u64_1 = static_cast<uint64_t>(input1);\n+  auto u64_2 = static_cast<uint64_t>(input2);\n   return u64_1 | (u64_2 << 32);\n }\n \n PHILOX_DEVICE_INLINE PhiloxRandom::ResultType GetCounterFromMem(\n-    uint64 const* ptr) {\n+    const uint64_t* ptr) {\n   PhiloxRandom::ResultType counter;\n   Uint64ToUint32s(ptr[0], &counter[0], &counter[1]);\n   Uint64ToUint32s(ptr[1], &counter[2], &counter[3]);\n   return counter;\n }\n \n PHILOX_DEVICE_INLINE void WriteCounterToMem(\n-    PhiloxRandom::ResultType const& counter, uint64* ptr) {\n+    PhiloxRandom::ResultType const& counter, uint64_t* ptr) {\n   ptr[0] = Uint32sToUint64(counter[0], counter[1]);\n   ptr[1] = Uint32sToUint64(counter[2], counter[3]);\n }\n \n-PHILOX_DEVICE_INLINE PhiloxRandom::Key GetKeyFromMem(uint64 const* ptr) {\n+PHILOX_DEVICE_INLINE PhiloxRandom::Key GetKeyFromMem(const uint64_t* ptr) {\n   PhiloxRandom::Key key;\n   Uint64ToUint32s(ptr[0], &key[0], &key[1]);\n   return key;\n }\n \n PHILOX_DEVICE_INLINE void WriteKeyToMem(PhiloxRandom::Key const& key,\n-                                        uint64* ptr) {\n+                                        uint64_t* ptr) {\n   *ptr = Uint32sToUint64(key[0], key[1]);\n }\n \n PHILOX_DEVICE_INLINE PhiloxRandom GetPhiloxRandomFromCounterKeyMem(\n-    uint64 const* counter_ptr, uint64 const* key_ptr) {\n+    const uint64_t* counter_ptr, const uint64_t* key_ptr) {\n   return PhiloxRandom(GetCounterFromMem(counter_ptr), GetKeyFromMem(key_ptr));\n }\n "
        },
        {
            "sha": "9b1f93584ad86b20c9f358babb77ebfdca7da82c",
            "filename": "tensorflow/core/kernels/random_poisson_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_poisson_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_poisson_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frandom_poisson_op.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -351,7 +351,7 @@ TF_CALL_double(REGISTER);\n REGISTER_ALL(Eigen::half);\n REGISTER_ALL(float);\n REGISTER_ALL(double);\n-REGISTER_ALL(int32);\n+REGISTER_ALL(int32_t);\n REGISTER_ALL(int64_t);\n \n #undef REGISTER_ALL"
        },
        {
            "sha": "4d8f62a2e142d87721ada42b625fa2c96e9df0e9",
            "filename": "tensorflow/core/kernels/random_poisson_op_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_poisson_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_poisson_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frandom_poisson_op_test.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -24,13 +24,13 @@ namespace tensorflow {\n namespace {\n \n Tensor VecShape(int64_t v) {\n-  if (v >= std::numeric_limits<int32>::max()) {\n+  if (v >= std::numeric_limits<int32_t>::max()) {\n     Tensor shape(DT_INT64, TensorShape({1}));\n     shape.vec<int64_t>()(0) = v;\n     return shape;\n   } else {\n     Tensor shape(DT_INT32, TensorShape({1}));\n-    shape.vec<int32>()(0) = v;\n+    shape.vec<int32_t>()(0) = v;\n     return shape;\n   }\n }"
        },
        {
            "sha": "c9c83d381e6ff9d3de2eef0d630855c3b65cfa1c",
            "filename": "tensorflow/core/kernels/random_shuffle_queue_op.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_shuffle_queue_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frandom_shuffle_queue_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frandom_shuffle_queue_op.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -45,7 +45,7 @@ class RandomShuffleQueue : public TypedQueue<std::vector<Tensor> > {\n   RandomShuffleQueue(int32_t capacity, int32_t min_after_dequeue, int64_t seed,\n                      int64_t seed2, const DataTypeVector& component_dtypes,\n                      const std::vector<TensorShape>& component_shapes,\n-                     const string& name);\n+                     const std::string& name);\n \n   absl::Status Initialize()\n       override;  // Must be called before any other method.\n@@ -61,7 +61,7 @@ class RandomShuffleQueue : public TypedQueue<std::vector<Tensor> > {\n                       CallbackWithTuple callback) override;\n   absl::Status MatchesNodeDef(const NodeDef& node_def) override;\n \n-  int32 size() const override {\n+  int32_t size() const override {\n     mutex_lock lock(mu_);\n     return queues_[0].size();\n   }\n@@ -78,7 +78,7 @@ class RandomShuffleQueue : public TypedQueue<std::vector<Tensor> > {\n                                                    OpKernelContext* ctx,\n                                                    Tensor* out_tensor);\n \n-  const int32 min_after_dequeue_;\n+  const int32_t min_after_dequeue_;\n   const int64_t original_seed_;\n   const int64_t original_seed2_;\n \n@@ -93,7 +93,7 @@ class RandomShuffleQueue : public TypedQueue<std::vector<Tensor> > {\n RandomShuffleQueue::RandomShuffleQueue(\n     int32_t capacity, int32_t min_after_dequeue, int64_t seed, int64_t seed2,\n     const DataTypeVector& component_dtypes,\n-    const std::vector<TensorShape>& component_shapes, const string& name)\n+    const std::vector<TensorShape>& component_shapes, const std::string& name)\n     : TypedQueue(capacity, component_dtypes, component_shapes, name),\n       min_after_dequeue_(min_after_dequeue),\n       original_seed_(seed),\n@@ -503,7 +503,7 @@ class RandomShuffleQueueOp : public TypedQueueOp {\n     return CreateTypedQueue(queue, ret);\n   }\n \n-  int32 min_after_dequeue_;\n+  int32_t min_after_dequeue_;\n   int64_t seed_;\n   int64_t seed2_;\n   std::vector<TensorShape> component_shapes_;"
        },
        {
            "sha": "2f8fb60c3b9f445ea8df56f1ea31bfa6741f135d",
            "filename": "tensorflow/core/kernels/range_sampler.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frange_sampler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frange_sampler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frange_sampler.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -248,7 +248,7 @@ FixedUnigramSampler::FixedUnigramSampler(int64_t range, float distortion,\n }\n \n absl::Status FixedUnigramSampler::SetDistributionSampler(\n-    Env* env, const string& vocab_file) {\n+    Env* env, const std::string& vocab_file) {\n   TF_RETURN_IF_ERROR(LoadFromFile(env, vocab_file, distortion_));\n   if (!TF_PREDICT_TRUE(FixedUnigramSampler::range() == weights_.size()))\n     return (errors::InvalidArgument(\"range is \", FixedUnigramSampler::range(),\n@@ -287,18 +287,18 @@ void FixedUnigramSampler::FillReservedIds(int32_t num_reserved_ids) {\n }\n \n absl::Status FixedUnigramSampler::LoadFromFile(Env* env,\n-                                               const string& vocab_file,\n+                                               const std::string& vocab_file,\n                                                float distortion) {\n   std::unique_ptr<RandomAccessFile> file;\n   TF_RETURN_IF_ERROR(env->NewRandomAccessFile(vocab_file, &file));\n \n   io::InputBuffer in(file.get(), 262144 /*bytes*/);\n-  string line;\n+  std::string line;\n   int32_t word_id = weights_.size();\n   while (in.ReadLine(&line).ok()) {\n     // The vocabulary file should be in csv like format, with the last\n     // field the weight associated with the word.\n-    std::vector<string> cols = str_util::Split(line, ',');\n+    std::vector<std::string> cols = str_util::Split(line, ',');\n     if (cols.empty()) continue;\n     // Skip entries that do not belong to this shard.\n     if (word_id % num_shards_ == shard_) {"
        },
        {
            "sha": "cecb681cd4e97323375522868eb4184f6fd13f7f",
            "filename": "tensorflow/core/kernels/range_sampler.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frange_sampler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frange_sampler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frange_sampler.h?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -208,7 +208,7 @@ class FixedUnigramSampler : public RangeSampler {\n                       int32_t num_shards, int32_t shard);\n   // The vocab_file is assumed to be a CSV, with the last entry of each row a\n   // value representing the counts or probabilities for the corresponding ID.\n-  absl::Status SetDistributionSampler(Env* env, const string& vocab_file);\n+  absl::Status SetDistributionSampler(Env* env, const std::string& vocab_file);\n   absl::Status SetDistributionSampler(const std::vector<float>& unigrams);\n   float Probability(int64_t value) const override;\n \n@@ -225,14 +225,14 @@ class FixedUnigramSampler : public RangeSampler {\n   // Sharding information of the sampler. The whole vocabulary is sharded\n   // into num_shards_ smaller ranges and each sampler is responsible for one\n   // such smaller range, identified by the shard number.\n-  int32 num_shards_;\n-  int32 shard_;\n+  int32_t num_shards_;\n+  int32_t shard_;\n   float distortion_;\n   // Fill the sampler with the appropriate number of reserved IDs.\n   void FillReservedIds(int32_t num_reserved_ids);\n   // Load IDs to sample from a CSV file. It is assumed that the last item of\n   // each row contains a count or probability for the corresponding ID.\n-  absl::Status LoadFromFile(Env* env, const string& vocab_file,\n+  absl::Status LoadFromFile(Env* env, const std::string& vocab_file,\n                             float distortion);\n   // Load from an in-memory array.\n   void LoadFromUnigrams(const std::vector<float>& unigrams, float distortion);"
        },
        {
            "sha": "93891f10446311953e15adbc92095e52d24951ba",
            "filename": "tensorflow/core/kernels/range_sampler_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frange_sampler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frange_sampler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frange_sampler_test.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -157,7 +157,7 @@ static const char kVocabContent[] =\n     \"w9,256\";\n TEST_F(RangeSamplerTest, FixedUnigramProbabilities) {\n   Env* env = Env::Default();\n-  string fname = io::JoinPath(testing::TmpDir(), \"vocab_file\");\n+  std::string fname = io::JoinPath(testing::TmpDir(), \"vocab_file\");\n   TF_CHECK_OK(WriteStringToFile(env, fname, kVocabContent));\n   FixedUnigramSampler* test_sampler = new FixedUnigramSampler(9, 0.8, 0, 1, 0);\n   TF_CHECK_OK(test_sampler->SetDistributionSampler(env, fname));\n@@ -169,15 +169,15 @@ TEST_F(RangeSamplerTest, FixedUnigramProbabilities) {\n }\n TEST_F(RangeSamplerTest, FixedUnigramNoExistingFilename) {\n   Env* env = Env::Default();\n-  string fname = \"NoExistingFile\";\n+  std::string fname = \"NoExistingFile\";\n   FixedUnigramSampler* test_sampler = new FixedUnigramSampler(9, 0.8, 0, 1, 0);\n   absl::Status s = test_sampler->SetDistributionSampler(env, fname);\n   sampler_.reset(test_sampler);\n   EXPECT_TRUE(absl::IsNotFound(s)) << s;\n }\n TEST_F(RangeSamplerTest, FixedUnigramNoMatchingRangeWeights) {\n   Env* env = Env::Default();\n-  string fname = io::JoinPath(testing::TmpDir(), \"vocab_file\");\n+  std::string fname = io::JoinPath(testing::TmpDir(), \"vocab_file\");\n   TF_CHECK_OK(WriteStringToFile(env, fname, kVocabContent));\n   FixedUnigramSampler* test_sampler = new FixedUnigramSampler(8, 0.8, 0, 1, 0);\n   absl::Status s = test_sampler->SetDistributionSampler(env, fname);\n@@ -186,7 +186,7 @@ TEST_F(RangeSamplerTest, FixedUnigramNoMatchingRangeWeights) {\n }\n TEST_F(RangeSamplerTest, FixedUnigramChecksum) {\n   Env* env = Env::Default();\n-  string fname = io::JoinPath(testing::TmpDir(), \"vocab_file\");\n+  std::string fname = io::JoinPath(testing::TmpDir(), \"vocab_file\");\n   TF_CHECK_OK(WriteStringToFile(env, fname, kVocabContent));\n   FixedUnigramSampler* test_sampler = new FixedUnigramSampler(9, 0.8, 0, 1, 0);\n   TF_CHECK_OK(test_sampler->SetDistributionSampler(env, fname));\n@@ -195,7 +195,7 @@ TEST_F(RangeSamplerTest, FixedUnigramChecksum) {\n }\n TEST_F(RangeSamplerTest, FixedUnigramHistogram) {\n   Env* env = Env::Default();\n-  string fname = io::JoinPath(testing::TmpDir(), \"vocab_file\");\n+  std::string fname = io::JoinPath(testing::TmpDir(), \"vocab_file\");\n   TF_CHECK_OK(WriteStringToFile(env, fname, kVocabContent));\n   FixedUnigramSampler* test_sampler = new FixedUnigramSampler(9, 0.8, 0, 1, 0);\n   TF_CHECK_OK(test_sampler->SetDistributionSampler(env, fname));\n@@ -204,7 +204,7 @@ TEST_F(RangeSamplerTest, FixedUnigramHistogram) {\n }\n TEST_F(RangeSamplerTest, FixedUnigramProbabilitiesReserve1) {\n   Env* env = Env::Default();\n-  string fname = io::JoinPath(testing::TmpDir(), \"vocab_file\");\n+  std::string fname = io::JoinPath(testing::TmpDir(), \"vocab_file\");\n   TF_CHECK_OK(WriteStringToFile(env, fname, kVocabContent));\n   FixedUnigramSampler* test_sampler = new FixedUnigramSampler(10, 0.8, 1, 1, 0);\n   TF_CHECK_OK(test_sampler->SetDistributionSampler(env, fname));\n@@ -217,7 +217,7 @@ TEST_F(RangeSamplerTest, FixedUnigramProbabilitiesReserve1) {\n }\n TEST_F(RangeSamplerTest, FixedUnigramProbabilitiesReserve2) {\n   Env* env = Env::Default();\n-  string fname = io::JoinPath(testing::TmpDir(), \"vocab_file\");\n+  std::string fname = io::JoinPath(testing::TmpDir(), \"vocab_file\");\n   TF_CHECK_OK(WriteStringToFile(env, fname, kVocabContent));\n   FixedUnigramSampler* test_sampler = new FixedUnigramSampler(11, 0.8, 2, 1, 0);\n   TF_CHECK_OK(test_sampler->SetDistributionSampler(env, fname));"
        },
        {
            "sha": "d1c3fbd1f70cb96825e2cb187983ccfbb514f3cb",
            "filename": "tensorflow/core/kernels/record_input_op.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frecord_input_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frecord_input_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frecord_input_op.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -30,13 +30,13 @@ class RecordInputOp : public OpKernel {\n   TYPE FIELD;                \\\n   OP_REQUIRES_OK(ctx, ctx->GetAttr(#FIELD, &FIELD));\n \n-    GETATTR(string, file_pattern);\n+    GETATTR(std::string, file_pattern);\n     GETATTR(int64_t, file_random_seed);\n     GETATTR(float, file_shuffle_shift_ratio);\n     GETATTR(int64_t, file_buffer_size);\n     GETATTR(int64_t, file_parallelism);\n     GETATTR(int64_t, batch_size);\n-    GETATTR(string, compression_type);\n+    GETATTR(std::string, compression_type);\n #undef GETATTR\n \n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"compression_type\", &compression_type));"
        },
        {
            "sha": "e186c92e7c3b3038799ab18d961619b7173b54d2",
            "filename": "tensorflow/core/kernels/record_yielder.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frecord_yielder.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frecord_yielder.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frecord_yielder.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -82,10 +82,10 @@ bool RecordYielder::ShouldFinish(const absl::Status& s) {\n   return stop_ || !status_.ok();\n }\n \n-static absl::Status MatchFiles(const string& patterns,\n-                               std::vector<string>* filenames) {\n+static absl::Status MatchFiles(const std::string& patterns,\n+                               std::vector<std::string>* filenames) {\n   for (const auto& file_pattern : str_util::Split(patterns, ',')) {\n-    std::vector<string> tmp_filenames;\n+    std::vector<std::string> tmp_filenames;\n     TF_RETURN_IF_ERROR(\n         Env::Default()->GetMatchingPaths(file_pattern, &tmp_filenames));\n     filenames->insert(filenames->end(),\n@@ -102,7 +102,7 @@ void RecordYielder::MainLoop() {\n     num_records_added_in_epoch_ = 0;\n \n     // Finds all files.\n-    std::vector<string> filenames;\n+    std::vector<std::string> filenames;\n     absl::Status s = MatchFiles(opts_.file_pattern, &filenames);\n \n     if (filenames.empty()) {\n@@ -121,7 +121,7 @@ void RecordYielder::MainLoop() {\n     std::shuffle(filenames.begin(), filenames.end(), shuffle_rnd);\n \n     // Left-shift the filename list.\n-    const std::vector<string>::size_type num = filenames.size();\n+    const std::vector<std::string>::size_type num = filenames.size();\n     int64_t shift;\n     if (0 <= opts_.file_shuffle_shift_ratio &&\n         opts_.file_shuffle_shift_ratio < 1) {\n@@ -136,7 +136,8 @@ void RecordYielder::MainLoop() {\n     for (int i = 0; i < N; ++i) {\n       Shard* shard = &shards[i];\n       shard->index = i;\n-      for (std::vector<string>::size_type j = i; j < filenames.size(); j += N) {\n+      for (std::vector<std::string>::size_type j = i; j < filenames.size();\n+           j += N) {\n         shard->filenames.push_back(filenames[j]);\n       }\n       thread_->Schedule([this, shard]() { ShardLoop(shard); });\n@@ -172,7 +173,7 @@ void RecordYielder::MainLoop() {\n   main_loop_done_.Notify();\n }\n \n-bool RecordYielder::Add(std::vector<string>* values) {\n+bool RecordYielder::Add(std::vector<std::string>* values) {\n   mutex_lock l(mu_);\n   while (!BufNotFull()) {\n     buf_not_full_.wait(l);\n@@ -197,9 +198,9 @@ bool RecordYielder::Add(std::vector<string>* values) {\n }\n \n void RecordYielder::ShardLoop(Shard* shard) {\n-  std::vector<string> values;\n+  std::vector<std::string> values;\n   const int64_t kRecords = 16;\n-  for (const string& filename : shard->filenames) {\n+  for (const std::string& filename : shard->filenames) {\n     std::unique_ptr<RandomAccessFile> file;\n     if (ShouldFinish(absl::OkStatus())) break;\n     absl::Status s = Env::Default()->NewRandomAccessFile(filename, &file);\n@@ -211,7 +212,7 @@ void RecordYielder::ShardLoop(Shard* shard) {\n         io::RecordReaderOptions::CreateRecordReaderOptions(\n             opts_.compression_type);\n     io::RecordReader rdr(file.get(), options);\n-    uint64 offset = 0;\n+    uint64_t offset = 0;\n     tstring record;\n     while (true) {\n       absl::Status s = rdr.ReadRecord(&offset, &record);"
        },
        {
            "sha": "8f201082eac5f4613cdfa2e512e924c8ffedb1fc",
            "filename": "tensorflow/core/kernels/record_yielder.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frecord_yielder.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Frecord_yielder.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Frecord_yielder.h?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -59,7 +59,7 @@ class RecordYielder {\n  public:\n   struct Options {\n     // Glob pattern for tfrecords.\n-    string file_pattern;\n+    std::string file_pattern;\n \n     // Random seed. It determines how data files are shuffled and how\n     // records are shuffled.\n@@ -73,13 +73,13 @@ class RecordYielder {\n     float file_shuffle_shift_ratio = 0;\n \n     // Randomization buffer keeps these many records.\n-    uint64 bufsize = 1;\n+    uint64_t bufsize = 1;\n \n     // Uses these many concurrent tfrecord iterators to iterate through\n     // tfrecords.\n-    int32 parallelism = 1;\n+    int32_t parallelism = 1;\n \n-    string compression_type;\n+    std::string compression_type;\n   };\n \n   explicit RecordYielder(OpKernelConstruction* context,\n@@ -116,7 +116,7 @@ class RecordYielder {\n   std::mt19937_64 rnd_ TF_GUARDED_BY(mu_);\n \n   // Randomization buffer.\n-  std::vector<string> buf_ TF_GUARDED_BY(mu_);\n+  std::vector<std::string> buf_ TF_GUARDED_BY(mu_);\n \n   // True iff we are draining an epoch.\n   bool epoch_end_ = false;\n@@ -145,14 +145,14 @@ class RecordYielder {\n     // any.\n     return stop_ || !status_.ok() || (epoch_end_ && !buf_.empty()) ||\n            (!epoch_end_ &&\n-            buf_.size() >= std::max<uint64>(1, opts_.bufsize / 2));\n+            buf_.size() >= std::max<uint64_t>(1, opts_.bufsize / 2));\n   }\n \n   void MainLoop();\n   struct Shard;\n   void ShardLoop(Shard* shard);\n   bool ShouldFinish(const absl::Status& s);\n-  bool Add(std::vector<string>* values);\n+  bool Add(std::vector<std::string>* values);\n };\n \n }  // namespace tensorflow"
        },
        {
            "sha": "e05e4c3b4d60302d942436563e57613080880ae6",
            "filename": "tensorflow/core/kernels/reduce_join_op.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freduce_join_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freduce_join_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Freduce_join_op.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -47,7 +47,7 @@ const absl::InlinedVector<int64_t, 8> GetStrides(const TensorShape& shape) {\n // nonspecified dimensions set to 0.  Dimensions must be ordered from outer-most\n // to inner-most with respect to the subset linear index.\n inline int64_t LinearSubIndexToFullIndex(\n-    int64_t output_index, const absl::InlinedVector<int32, 8>& dim_list,\n+    int64_t output_index, const absl::InlinedVector<int32_t, 8>& dim_list,\n     const TensorShape& input_shape,\n     const absl::InlinedVector<int64_t, 8>& strides) {\n   int64_t result = 0;\n@@ -63,7 +63,7 @@ inline int64_t LinearSubIndexToFullIndex(\n \n // Computes the number of input elements reduced per output element.\n int64_t GetReductionIterSize(\n-    const absl::InlinedVector<int32, 8>& reduced_indices,\n+    const absl::InlinedVector<int32_t, 8>& reduced_indices,\n     const TensorShape& input_shape) {\n   int64_t result = 1;\n   for (int32_t reduce_dim : reduced_indices) {\n@@ -74,12 +74,12 @@ int64_t GetReductionIterSize(\n \n // Computes a list of all true reduced indices, accounting for negative\n // indices.\n-absl::InlinedVector<int32, 8> GetReducedIndices(const Tensor& reduction_indices,\n-                                                int32_t input_dims) {\n-  const auto reduction_indices_flat = reduction_indices.flat<int32>();\n+absl::InlinedVector<int32_t, 8> GetReducedIndices(\n+    const Tensor& reduction_indices, int32_t input_dims) {\n+  const auto reduction_indices_flat = reduction_indices.flat<int32_t>();\n   const int32_t reduction_dims = reduction_indices_flat.size();\n \n-  absl::InlinedVector<int32, 8> reduced_indices(reduction_dims);\n+  absl::InlinedVector<int32_t, 8> reduced_indices(reduction_dims);\n   for (int32_t i = 0; i < reduction_dims; ++i) {\n     reduced_indices[i] = reduction_indices_flat(reduction_dims - i - 1);\n     reduced_indices[i] += reduced_indices[i] < 0 ? input_dims : 0;\n@@ -91,7 +91,7 @@ absl::InlinedVector<int32, 8> GetReducedIndices(const Tensor& reduction_indices,\n // Appends all unreduced dimensions to the given vector.\n void MakeUnreducedIndices(absl::InlinedVector<bool, 8> index_is_reduced,\n                           int32_t input_dims,\n-                          absl::InlinedVector<int32, 8>* unreduced_indices) {\n+                          absl::InlinedVector<int32_t, 8>* unreduced_indices) {\n   for (int32_t index = 0; index < input_dims; ++index) {\n     if (!index_is_reduced[index]) unreduced_indices->push_back(index);\n   }\n@@ -128,7 +128,7 @@ class ReduceJoinOp : public OpKernel {\n     const int32_t input_dims = input_shape.dims();\n \n     const Tensor& reduction_indices = context->input(1);\n-    const auto reduction_indices_flat = reduction_indices.flat<int32>();\n+    const auto reduction_indices_flat = reduction_indices.flat<int32_t>();\n     const int32_t reduction_dims = reduction_indices_flat.size();\n \n     absl::InlinedVector<bool, 8> index_is_reduced(input_dims, false);\n@@ -146,9 +146,9 @@ class ReduceJoinOp : public OpKernel {\n       index_is_reduced[true_reduce_index] = true;\n     }\n \n-    absl::InlinedVector<int32, 8> reduced_indices =\n+    absl::InlinedVector<int32_t, 8> reduced_indices =\n         GetReducedIndices(reduction_indices, input_dims);\n-    absl::InlinedVector<int32, 8> unreduced_indices;\n+    absl::InlinedVector<int32_t, 8> unreduced_indices;\n     MakeUnreducedIndices(index_is_reduced, input_dims, &unreduced_indices);\n     const auto strides = GetStrides(input_shape);\n \n@@ -179,7 +179,7 @@ class ReduceJoinOp : public OpKernel {\n \n  private:\n   bool keep_dims_;\n-  string separator_;\n+  std::string separator_;\n };\n \n REGISTER_KERNEL_BUILDER(Name(\"ReduceJoin\").Device(DEVICE_CPU), ReduceJoinOp);"
        },
        {
            "sha": "34f559704ed5215bd73ad48aab8be90961296783",
            "filename": "tensorflow/core/kernels/reduction_ops.h",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freduction_ops.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freduction_ops.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Freduction_ops.h?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -114,12 +114,12 @@ struct ReduceEigenImpl<Device, OUT_T, IN_T, ReductionAxes,\n     }                                                                         \\\n   }\n \n-CASTING_SPECIALIZATION(uint8, uint64);\n-CASTING_SPECIALIZATION(uint16, uint64);\n-CASTING_SPECIALIZATION(uint32, uint64);\n-CASTING_SPECIALIZATION(int8, int64_t);\n-CASTING_SPECIALIZATION(int16, int64_t);\n-CASTING_SPECIALIZATION(int32, int64_t);\n+CASTING_SPECIALIZATION(uint8_t, uint64_t);\n+CASTING_SPECIALIZATION(uint16_t, uint64_t);\n+CASTING_SPECIALIZATION(uint32_t, uint64_t);\n+CASTING_SPECIALIZATION(int8_t, int64_t);\n+CASTING_SPECIALIZATION(int16_t, int64_t);\n+CASTING_SPECIALIZATION(int32_t, int64_t);\n CASTING_SPECIALIZATION(bfloat16, float);\n #undef CASTING_SPECIALIZATION\n "
        },
        {
            "sha": "54c8e4969717c700832f49f92ab78d6c432d7c45",
            "filename": "tensorflow/core/kernels/reduction_ops_all.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freduction_ops_all.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freduction_ops_all.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Freduction_ops_all.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -19,16 +19,16 @@ namespace tensorflow {\n \n REGISTER_KERNEL_BUILDER(\n     Name(\"All\")\n-        .TypeConstraint<int32>(\"Tidx\")\n+        .TypeConstraint<int32_t>(\"Tidx\")\n         .Device(DEVICE_CPU)\n         .HostMemory(\"reduction_indices\"),\n-    ReductionOp<CPUDevice, bool, int32, Eigen::internal::AndReducer>);\n+    ReductionOp<CPUDevice, bool, int32_t, Eigen::internal::AndReducer>);\n REGISTER_KERNEL_BUILDER(\n     Name(\"All\")\n         .TypeConstraint<int64_t>(\"Tidx\")\n         .Device(DEVICE_CPU)\n         .HostMemory(\"reduction_indices\"),\n-    ReductionOp<CPUDevice, bool, int64, Eigen::internal::AndReducer>);\n+    ReductionOp<CPUDevice, bool, int64_t, Eigen::internal::AndReducer>);\n \n #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n REGISTER_KERNEL_BUILDER("
        },
        {
            "sha": "9675bbccc0f7e21aa216bab4c667c260f86c63c7",
            "filename": "tensorflow/core/kernels/reduction_ops_any.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freduction_ops_any.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freduction_ops_any.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Freduction_ops_any.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -19,16 +19,16 @@ namespace tensorflow {\n \n REGISTER_KERNEL_BUILDER(\n     Name(\"Any\")\n-        .TypeConstraint<int32>(\"Tidx\")\n+        .TypeConstraint<int32_t>(\"Tidx\")\n         .Device(DEVICE_CPU)\n         .HostMemory(\"reduction_indices\"),\n-    ReductionOp<CPUDevice, bool, int32, Eigen::internal::OrReducer>);\n+    ReductionOp<CPUDevice, bool, int32_t, Eigen::internal::OrReducer>);\n REGISTER_KERNEL_BUILDER(\n     Name(\"Any\")\n         .TypeConstraint<int64_t>(\"Tidx\")\n         .Device(DEVICE_CPU)\n         .HostMemory(\"reduction_indices\"),\n-    ReductionOp<CPUDevice, bool, int64, Eigen::internal::OrReducer>);\n+    ReductionOp<CPUDevice, bool, int64_t, Eigen::internal::OrReducer>);\n \n #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n REGISTER_KERNEL_BUILDER("
        },
        {
            "sha": "028743cf9c3d186be0eed388f2bdb5c4522263ab",
            "filename": "tensorflow/core/kernels/reduction_ops_common.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freduction_ops_common.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freduction_ops_common.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Freduction_ops_common.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -44,10 +44,10 @@ TensorShape ReductionHelper::shuffled_shape() {\n   return shape;\n }\n \n-absl::InlinedVector<int32, 8> ReductionHelper::permutation() {\n+absl::InlinedVector<int32_t, 8> ReductionHelper::permutation() {\n   const int dims = data_reshape_.size();\n   const int unreduced_dims = (dims + !reduce_first_axis_) / 2;\n-  absl::InlinedVector<int32, 8> perm(dims);\n+  absl::InlinedVector<int32_t, 8> perm(dims);\n   for (int i = 0; i < unreduced_dims; i++) {\n     perm[i] = 2 * i + reduce_first_axis_;\n   }\n@@ -84,7 +84,7 @@ absl::Status ReductionHelper::Simplify(const Tensor& data, const Tensor& axis,\n   // bitmap[i] indicates whether to reduce data along i-th axis.\n   absl::InlinedVector<bool, 4> bitmap(data.dims(), false);\n   if (axis.dtype() == DT_INT32) {\n-    TF_RETURN_IF_ERROR(SimplifyHelper<int32>(data, axis, bitmap));\n+    TF_RETURN_IF_ERROR(SimplifyHelper<int32_t>(data, axis, bitmap));\n   } else {\n     TF_RETURN_IF_ERROR(SimplifyHelper<int64_t>(data, axis, bitmap));\n   }"
        },
        {
            "sha": "daab208f725beca0cf3754816f9a2d23b9505e93",
            "filename": "tensorflow/core/kernels/reduction_ops_common.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freduction_ops_common.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freduction_ops_common.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Freduction_ops_common.h?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -114,7 +114,7 @@ class ReductionHelper {\n   TensorShape shuffled_shape();\n \n   // Permutation of reduced dims needed to put reduction dimensions at the end\n-  absl::InlinedVector<int32, 8> permutation();\n+  absl::InlinedVector<int32_t, 8> permutation();\n \n  private:\n   bool reduce_first_axis_;  // True if need to reduce the 0-th dimension."
        },
        {
            "sha": "54025c4e612fe2d74c57e29a1816920f335bb518",
            "filename": "tensorflow/core/kernels/reduction_ops_max.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freduction_ops_max.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freduction_ops_max.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Freduction_ops_max.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -67,23 +67,23 @@ REGISTER_GPU_KERNELS(int64_t);\n // A special DEVICE_DEFAULT kernel for int32.\n // TODO(b/25387198): Also enable int32 in device memory. This kernel\n // registration requires all int32 inputs and outputs to be in host memory.\n-REGISTER_KERNEL_BUILDER(\n-    Name(\"Max\")\n-        .Device(DEVICE_DEFAULT)\n-        .HostMemory(\"reduction_indices\")\n-        .HostMemory(\"input\")\n-        .HostMemory(\"output\")\n-        .TypeConstraint<int32>(\"T\")\n-        .TypeConstraint<int32>(\"Tidx\"),\n-    ReductionOp<CPUDevice, int32, int32, Eigen::internal::MaxReducer<int32>>);\n-REGISTER_KERNEL_BUILDER(\n-    Name(\"Max\")\n-        .Device(DEVICE_DEFAULT)\n-        .HostMemory(\"reduction_indices\")\n-        .HostMemory(\"input\")\n-        .HostMemory(\"output\")\n-        .TypeConstraint<int32>(\"T\")\n-        .TypeConstraint<int64_t>(\"Tidx\"),\n-    ReductionOp<CPUDevice, int32, int64, Eigen::internal::MaxReducer<int32>>);\n+REGISTER_KERNEL_BUILDER(Name(\"Max\")\n+                            .Device(DEVICE_DEFAULT)\n+                            .HostMemory(\"reduction_indices\")\n+                            .HostMemory(\"input\")\n+                            .HostMemory(\"output\")\n+                            .TypeConstraint<int32_t>(\"T\")\n+                            .TypeConstraint<int32_t>(\"Tidx\"),\n+                        ReductionOp<CPUDevice, int32_t, int32_t,\n+                                    Eigen::internal::MaxReducer<int32_t>>);\n+REGISTER_KERNEL_BUILDER(Name(\"Max\")\n+                            .Device(DEVICE_DEFAULT)\n+                            .HostMemory(\"reduction_indices\")\n+                            .HostMemory(\"input\")\n+                            .HostMemory(\"output\")\n+                            .TypeConstraint<int32_t>(\"T\")\n+                            .TypeConstraint<int64_t>(\"Tidx\"),\n+                        ReductionOp<CPUDevice, int32_t, int64_t,\n+                                    Eigen::internal::MaxReducer<int32_t>>);\n \n }  // namespace tensorflow"
        },
        {
            "sha": "b81cd549373d2effcf16d3c403442085eb22bab6",
            "filename": "tensorflow/core/kernels/reduction_ops_min.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 19,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freduction_ops_min.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freduction_ops_min.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Freduction_ops_min.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -65,24 +65,23 @@ REGISTER_GPU_KERNELS(double);\n // A special DEVICE_DEFAULT kernel for int32.\n // TODO(b/25387198): Also enable int32 in device memory. This kernel\n // registration requires all int32 inputs and outputs to be in host memory.\n-REGISTER_KERNEL_BUILDER(\n-    Name(\"Min\")\n-        .Device(DEVICE_DEFAULT)\n-        .HostMemory(\"reduction_indices\")\n-        .HostMemory(\"input\")\n-        .HostMemory(\"output\")\n-        .TypeConstraint<int32>(\"T\")\n-        .TypeConstraint<int32>(\"Tidx\"),\n-    ReductionOp<CPUDevice, int32, int32, Eigen::internal::MinReducer<int32>>);\n-REGISTER_KERNEL_BUILDER(\n-    Name(\"Min\")\n-        .Device(DEVICE_DEFAULT)\n-        .HostMemory(\"reduction_indices\")\n-        .HostMemory(\"input\")\n-        .HostMemory(\"output\")\n-        .TypeConstraint<int32>(\"T\")\n-        .TypeConstraint<int64_t>(\"Tidx\"),\n-    ReductionOp<CPUDevice, int32, int64, Eigen::internal::MinReducer<int32>>);\n-\n+REGISTER_KERNEL_BUILDER(Name(\"Min\")\n+                            .Device(DEVICE_DEFAULT)\n+                            .HostMemory(\"reduction_indices\")\n+                            .HostMemory(\"input\")\n+                            .HostMemory(\"output\")\n+                            .TypeConstraint<int32_t>(\"T\")\n+                            .TypeConstraint<int32_t>(\"Tidx\"),\n+                        ReductionOp<CPUDevice, int32_t, int32_t,\n+                                    Eigen::internal::MinReducer<int32_t>>);\n+REGISTER_KERNEL_BUILDER(Name(\"Min\")\n+                            .Device(DEVICE_DEFAULT)\n+                            .HostMemory(\"reduction_indices\")\n+                            .HostMemory(\"input\")\n+                            .HostMemory(\"output\")\n+                            .TypeConstraint<int32_t>(\"T\")\n+                            .TypeConstraint<int64_t>(\"Tidx\"),\n+                        ReductionOp<CPUDevice, int32_t, int64_t,\n+                                    Eigen::internal::MinReducer<int32_t>>);\n \n }  // namespace tensorflow"
        },
        {
            "sha": "4c77592f5dbf36be741f52545a5de3fc77a3a1f9",
            "filename": "tensorflow/core/kernels/reduction_ops_test.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freduction_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freduction_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Freduction_ops_test.cc?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -24,58 +24,58 @@ namespace tensorflow {\n // Creates a Graph which \"reduce\"s a 3D float tensor of \"num\" elements\n // into a scalar.\n template <typename T>\n-static Graph* ToScalar(const string& reduce, int num_x, int num_y) {\n+static Graph* ToScalar(const std::string& reduce, int num_x, int num_y) {\n   auto* g = new Graph(OpRegistry::Global());\n   Tensor data(DataTypeToEnum<T>::value, TensorShape({num_x, num_y}));\n   data.flat<T>().setRandom();\n   Tensor axes(DT_INT32, TensorShape({2}));\n-  axes.flat<int32>()(0) = 0;\n-  axes.flat<int32>()(1) = 1;\n+  axes.flat<int32_t>()(0) = 0;\n+  axes.flat<int32_t>()(1) = 1;\n   test::graph::Reduce(g, reduce, test::graph::Constant(g, data),\n                       test::graph::Constant(g, axes));\n   return g;\n }\n \n-static Graph* ColReduce(const string& reduce, int num_x, int num_y) {\n+static Graph* ColReduce(const std::string& reduce, int num_x, int num_y) {\n   auto* g = new Graph(OpRegistry::Global());\n   Tensor data(DT_FLOAT, TensorShape({num_x, num_y}));\n   data.flat<float>().setRandom();\n   Tensor axes(DT_INT32, TensorShape({1}));\n-  axes.flat<int32>()(0) = 0;\n+  axes.flat<int32_t>()(0) = 0;\n   test::graph::Reduce(g, reduce, test::graph::Constant(g, data),\n                       test::graph::Constant(g, axes));\n   return g;\n }\n \n-static Graph* RowReduce(const string& reduce, int num_x, int num_y) {\n+static Graph* RowReduce(const std::string& reduce, int num_x, int num_y) {\n   auto* g = new Graph(OpRegistry::Global());\n   Tensor data(DT_FLOAT, TensorShape({num_x, num_y}));\n   data.flat<float>().setRandom();\n   Tensor axes(DT_INT32, TensorShape({1}));\n-  axes.flat<int32>()(0) = 1;\n+  axes.flat<int32_t>()(0) = 1;\n   test::graph::Reduce(g, reduce, test::graph::Constant(g, data),\n                       test::graph::Constant(g, axes));\n   return g;\n }\n \n-static Graph* ThreeDYReduce(const string& reduce, int num_y, int num_z) {\n+static Graph* ThreeDYReduce(const std::string& reduce, int num_y, int num_z) {\n   auto* g = new Graph(OpRegistry::Global());\n   Tensor data(DT_FLOAT, TensorShape({4, num_y, num_z}));\n   data.flat<float>().setRandom();\n   Tensor axes(DT_INT32, TensorShape({1}));\n-  axes.flat<int32>()(0) = 1;\n+  axes.flat<int32_t>()(0) = 1;\n   test::graph::Reduce(g, reduce, test::graph::Constant(g, data),\n                       test::graph::Constant(g, axes));\n   return g;\n }\n \n-static Graph* ThreeDXZReduce(const string& reduce, int num_y, int num_z) {\n+static Graph* ThreeDXZReduce(const std::string& reduce, int num_y, int num_z) {\n   auto* g = new Graph(OpRegistry::Global());\n   Tensor data(DT_FLOAT, TensorShape({4, num_y, num_z}));\n   data.flat<float>().setRandom();\n   Tensor axes(DT_INT32, TensorShape({2}));\n-  axes.flat<int32>()(0) = 0;\n-  axes.flat<int32>()(1) = 2;\n+  axes.flat<int32_t>()(0) = 0;\n+  axes.flat<int32_t>()(1) = 2;\n   test::graph::Reduce(g, reduce, test::graph::Constant(g, data),\n                       test::graph::Constant(g, axes));\n   return g;\n@@ -85,7 +85,7 @@ static Graph* ThreeDXZReduce(const string& reduce, int num_y, int num_z) {\n // into a scalar on a \"device\". Runs the bench for \"iters\" times.\n template <typename T>\n static void ReduceToScalar(::testing::benchmark::State& state,\n-                           const string& device, const string& reduce,\n+                           const std::string& device, const std::string& reduce,\n                            int num_x, int num_y) {\n   test::Benchmark(device, ToScalar<T>(reduce, num_x, num_y),\n                   /*old_benchmark_api*/ false)\n@@ -97,8 +97,8 @@ static void ReduceToScalar(::testing::benchmark::State& state,\n }\n \n static void DoRowReduce(::testing::benchmark::State& state,\n-                        const string& device, const string& reduce, int num_x,\n-                        int num_y) {\n+                        const std::string& device, const std::string& reduce,\n+                        int num_x, int num_y) {\n   test::Benchmark(device, RowReduce(reduce, num_x, num_y),\n                   /*old_benchmark_api*/ false)\n       .Run(state);\n@@ -109,8 +109,8 @@ static void DoRowReduce(::testing::benchmark::State& state,\n }\n \n static void DoColReduce(::testing::benchmark::State& state,\n-                        const string& device, const string& reduce, int num_x,\n-                        int num_y) {\n+                        const std::string& device, const std::string& reduce,\n+                        int num_x, int num_y) {\n   test::Benchmark(device, ColReduce(reduce, num_x, num_y),\n                   /*old_benchmark_api*/ false)\n       .Run(state);\n@@ -121,8 +121,8 @@ static void DoColReduce(::testing::benchmark::State& state,\n }\n \n static void Do3DYReduce(::testing::benchmark::State& state,\n-                        const string& device, const string& reduce, int num_x,\n-                        int num_y) {\n+                        const std::string& device, const std::string& reduce,\n+                        int num_x, int num_y) {\n   test::Benchmark(device, ThreeDYReduce(reduce, num_x, num_y),\n                   /*old_benchmark_api*/ false)\n       .Run(state);\n@@ -133,8 +133,8 @@ static void Do3DYReduce(::testing::benchmark::State& state,\n }\n \n static void Do3DXZReduce(::testing::benchmark::State& state,\n-                         const string& device, const string& reduce, int num_x,\n-                         int num_y) {\n+                         const std::string& device, const std::string& reduce,\n+                         int num_x, int num_y) {\n   test::Benchmark(device, ThreeDXZReduce(reduce, num_x, num_y),\n                   /*old_benchmark_api*/ false)\n       .Run(state);"
        },
        {
            "sha": "e90656fd36b29850259e1007413135c8a556cfcb",
            "filename": "tensorflow/core/kernels/reference_gemm.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freference_gemm.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/960635397c7be3d84fd4afa9b876b5d824435769/tensorflow%2Fcore%2Fkernels%2Freference_gemm.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Freference_gemm.h?ref=960635397c7be3d84fd4afa9b876b5d824435769",
            "patch": "@@ -64,8 +64,8 @@ void ReferenceGemm(bool transpose_a, bool transpose_b, bool transpose_c,\n     c_j_stride = 1;\n   }\n \n-  const int32_t highest = static_cast<int32>(Eigen::NumTraits<T3>::highest());\n-  const int32_t lowest = static_cast<int32>(Eigen::NumTraits<T3>::lowest());\n+  const int32_t highest = static_cast<int32_t>(Eigen::NumTraits<T3>::highest());\n+  const int32_t lowest = static_cast<int32_t>(Eigen::NumTraits<T3>::lowest());\n   const int32_t rounding = (shift_c < 1) ? 0 : (1 << (shift_c - 1));\n \n   int i, j, l;\n@@ -74,9 +74,9 @@ void ReferenceGemm(bool transpose_a, bool transpose_b, bool transpose_c,\n       int32_t total = 0;\n       for (l = 0; l < k; l++) {\n         const size_t a_index = ((i * a_i_stride) + (l * a_l_stride));\n-        const int32_t a_value = static_cast<int32>(a[a_index]) - offset_a;\n+        const int32_t a_value = static_cast<int32_t>(a[a_index]) - offset_a;\n         const size_t b_index = ((j * b_j_stride) + (l * b_l_stride));\n-        const int32_t b_value = static_cast<int32>(b[b_index]) - offset_b;\n+        const int32_t b_value = static_cast<int32_t>(b[b_index]) - offset_b;\n         total += (a_value * b_value);\n       }\n       const size_t c_index = ((i * c_i_stride) + (j * c_j_stride));"
        }
    ],
    "stats": {
        "total": 696,
        "additions": 363,
        "deletions": 333
    }
}