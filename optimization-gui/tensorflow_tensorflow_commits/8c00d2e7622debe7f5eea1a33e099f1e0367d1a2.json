{
    "author": "aravindhbalaji1985",
    "message": "Merge branch 'master' into FixForCuda12.9.1Compilation",
    "sha": "8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
    "files": [
        {
            "sha": "2e3912041d9cf2ca1b566d6a6f1cbdc5365997cc",
            "filename": ".github/workflows/arm-cd.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Farm-cd.yml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Farm-cd.yml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/.github%2Fworkflows%2Farm-cd.yml?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -52,12 +52,12 @@ jobs:\n         run: find /home/ubuntu/actions-runner/_work/tensorflow/tensorflow/. -name . -o -prune -exec sudo rm -rf -- {} + || true\n       - name: Checkout repository for nightly (skipped for releases)\n         if: ${{ github.event_name == 'schedule' }}\n-        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n         with:\n           ref: 'nightly'\n       - name: Checkout repository for releases (skipped for nightly)\n         if: ${{ github.event_name == 'push' }}\n-        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n       - name: Build and test pip wheel\n         shell: bash\n         run: |"
        },
        {
            "sha": "54903a6998b090dbc56be95aa6cd46bf38e878d4",
            "filename": ".github/workflows/arm-ci-extended-cpp.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Farm-ci-extended-cpp.yml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Farm-ci-extended-cpp.yml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/.github%2Fworkflows%2Farm-ci-extended-cpp.yml?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -50,12 +50,12 @@ jobs:\n         run: find /home/ubuntu/actions-runner/_work/tensorflow/tensorflow/. -name . -o -prune -exec sudo rm -rf -- {} + || true\n       - name: Checkout repository for nightly (skipped for releases)\n         if: ${{ github.event_name == 'schedule' }}\n-        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n         with:\n           ref: 'nightly'\n       - name: Checkout repository\n         if: ${{ github.event_name == 'push' }}\n-        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n       - name: Build binary and run C++ tests\n         shell: bash\n         run: |"
        },
        {
            "sha": "2235cfc2d986dad4a14f79a2d71a7dcdf06d9933",
            "filename": ".github/workflows/arm-ci-extended.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Farm-ci-extended.yml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Farm-ci-extended.yml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/.github%2Fworkflows%2Farm-ci-extended.yml?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -51,12 +51,12 @@ jobs:\n         run: find /home/ubuntu/actions-runner/_work/tensorflow/tensorflow/. -name . -o -prune -exec sudo rm -rf -- {} + || true\n       - name: Checkout repository for nightly (skipped for releases)\n         if: ${{ github.event_name == 'schedule' }}\n-        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n         with:\n           ref: 'nightly'\n       - name: Checkout repository\n         if: ${{ github.event_name == 'push' }}\n-        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n       - name: Build binary and run python tests on nightly for all python versions\n         shell: bash\n         run: |"
        },
        {
            "sha": "a141bdd4676852f2f019149c805899fd701fdcf5",
            "filename": ".github/workflows/arm-ci.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Farm-ci.yml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Farm-ci.yml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/.github%2Fworkflows%2Farm-ci.yml?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -47,7 +47,7 @@ jobs:\n         shell: bash\n         run: find /home/ubuntu/actions-runner/_work/tensorflow/tensorflow/. -name . -o -prune -exec sudo rm -rf -- {} + || true\n       - name: Checkout repository\n-        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n       - name: Build binary and run python tests\n         shell: bash\n         run: |"
        },
        {
            "sha": "6421e08ccf0839be632eaa61d47d3f823e327f60",
            "filename": ".github/workflows/cffconvert.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Fcffconvert.yml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Fcffconvert.yml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/.github%2Fworkflows%2Fcffconvert.yml?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -30,7 +30,7 @@ jobs:\n     runs-on: ubuntu-latest\n     steps:\n       - name: Check out a copy of the repository\n-        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n \n       - name: Check whether the citation metadata from CITATION.cff is valid\n         uses: citation-file-format/cffconvert-github-action@4cf11baa70a673bfdf9dad0acc7ee33b3f4b6084 # v2.0.0"
        },
        {
            "sha": "28e610437757f16229865e821a2b20119403518b",
            "filename": ".github/workflows/issue-on-pr-rollback.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Fissue-on-pr-rollback.yml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Fissue-on-pr-rollback.yml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/.github%2Fworkflows%2Fissue-on-pr-rollback.yml?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -33,7 +33,7 @@ jobs:\n       startsWith(github.event.head_commit.message, 'Rollback of PR #')\n     steps:\n       - name: Checkout repo\n-        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n       - name: Create a new Github Issue\n         uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7.0.1\n         with:"
        },
        {
            "sha": "34d28df277c6eb01e00bf3ab43a6d8ff8e470f87",
            "filename": ".github/workflows/osv-scanner-scheduled.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Fosv-scanner-scheduled.yml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Fosv-scanner-scheduled.yml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/.github%2Fworkflows%2Fosv-scanner-scheduled.yml?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -28,7 +28,7 @@ permissions:\n jobs:\n   scan-scheduled:\n     if: github.repository == 'tensorflow/tensorflow'\n-    uses: \"google/osv-scanner-action/.github/workflows/osv-scanner-reusable.yml@v2.1.0\"\n+    uses: \"google/osv-scanner-action/.github/workflows/osv-scanner-reusable.yml@v2.2.2\"\n     with:\n       scan-args: |-\n         --lockfile=requirements.txt:./requirements_lock_3_9.txt"
        },
        {
            "sha": "26f1ac696996a881e60a5c822514ce088103decf",
            "filename": ".github/workflows/pylint-presubmit.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Fpylint-presubmit.yml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Fpylint-presubmit.yml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/.github%2Fworkflows%2Fpylint-presubmit.yml?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -28,7 +28,7 @@ jobs:\n     runs-on: ubuntu-latest\n     steps:\n     - name: Checkout code\n-      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+      uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n     - name: Get file changes\n       id: get_file_changes\n       uses: trilom/file-changes-action@a6ca26c14274c33b15e6499323aac178af06ad4b # v1.2.4"
        },
        {
            "sha": "69e03a040ae1a29c1b8e1ce8c2ff37036692368d",
            "filename": ".github/workflows/release-branch-cherrypick.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Frelease-branch-cherrypick.yml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Frelease-branch-cherrypick.yml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/.github%2Fworkflows%2Frelease-branch-cherrypick.yml?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -45,7 +45,7 @@ jobs:\n     if: github.repository == 'tensorflow/tensorflow' # Don't do this in forks\n     steps:\n     - name: Checkout code\n-      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+      uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n       with:\n         ref: ${{ github.event.inputs.release_branch }}\n     - name: Get some helpful info for formatting"
        },
        {
            "sha": "87393916383aa25c7e3dc7539a35af6728c8349f",
            "filename": ".github/workflows/scorecards-analysis.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Fscorecards-analysis.yml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Fscorecards-analysis.yml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/.github%2Fworkflows%2Fscorecards-analysis.yml?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -41,7 +41,7 @@ jobs:\n \n     steps:\n       - name: \"Checkout code\"\n-        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n         with:\n           persist-credentials: false\n \n@@ -64,6 +64,6 @@ jobs:\n       # Upload the results to GitHub's code scanning dashboard (optional).\n       # Commenting out will disable upload of results to your repo's Code Scanning dashboard\n       - name: \"Upload to code-scanning\"\n-        uses: github/codeql-action/upload-sarif@51f77329afa6477de8c49fc9c7046c15b9a4e79d # v3.29.5\n+        uses: github/codeql-action/upload-sarif@3c3833e0f8c1c83d449a7478aa59c036a9165498 # v3.29.5\n         with:\n           sarif_file: results.sarif"
        },
        {
            "sha": "a8dba883f5ff14e04cb770cbb69f2d7b02e8742c",
            "filename": ".github/workflows/update-rbe.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Fupdate-rbe.yml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/.github%2Fworkflows%2Fupdate-rbe.yml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/.github%2Fworkflows%2Fupdate-rbe.yml?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -30,7 +30,7 @@ jobs:\n     if: github.repository == 'tensorflow/tensorflow' # Don't do this in forks\n     steps:\n     - name: Checkout code\n-      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+      uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n     - name: Update the RBE Configs\n       run: |\n         function map() {"
        },
        {
            "sha": "a125bcc15b527f38ae4e00e06efaab7f1fd01648",
            "filename": "WORKSPACE",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/WORKSPACE",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/WORKSPACE",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/WORKSPACE?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -80,9 +80,15 @@ tf_workspace0()\n \n load(\n     \"@local_xla//third_party/py:python_wheel.bzl\",\n+    \"nvidia_wheel_versions_repository\",\n     \"python_wheel_version_suffix_repository\",\n )\n \n+nvidia_wheel_versions_repository(\n+    name = \"nvidia_wheel_versions\",\n+    versions_source = \"//ci/official/requirements_updater:nvidia-requirements.txt\",\n+)\n+\n python_wheel_version_suffix_repository(name = \"tf_wheel_version_suffix\")\n \n load("
        },
        {
            "sha": "0a29465aff05198d2ae722cc092633e87c5d9b22",
            "filename": "ci/official/requirements_updater/BUILD.bazel",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/ci%2Fofficial%2Frequirements_updater%2FBUILD.bazel",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/ci%2Fofficial%2Frequirements_updater%2FBUILD.bazel",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/ci%2Fofficial%2Frequirements_updater%2FBUILD.bazel?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -16,14 +16,27 @@\n load(\"@python//:defs.bzl\", \"compile_pip_requirements\")\n load(\"@python_version_repo//:py_version.bzl\", \"REQUIREMENTS\")\n \n+# TODO(ybaturina): Remove once TF is migrated to CUDA 12.9.\n+genrule(\n+    name = \"nvidia_constraints\",\n+    srcs = [\"nvidia-requirements.txt\"],\n+    outs = [\"nvidia-constraints.txt\"],\n+    cmd = \"\"\"sed -E \"s/>=/==/\" $(location nvidia-requirements.txt) > $@;\"\"\",\n+)\n+\n compile_pip_requirements(\n     name = \"requirements\",\n+    srcs = [\n+        \"nvidia-requirements.txt\",\n+        \"requirements.in\",\n+        \":nvidia_constraints\",\n+    ],\n     extra_args = [\n         \"--allow-unsafe\",\n         \"--build-isolation\",\n         \"--rebuild\",\n+        \"-c $(location :nvidia_constraints)\",\n     ],\n     generate_hashes = True,\n-    requirements_in = \"requirements.in\",\n     requirements_txt = REQUIREMENTS,\n )"
        },
        {
            "sha": "bad185ceac778deec62fb175817558a8cdf0dec0",
            "filename": "ci/official/requirements_updater/numpy1_requirements/requirements.in",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/ci%2Fofficial%2Frequirements_updater%2Fnumpy1_requirements%2Frequirements.in",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/ci%2Fofficial%2Frequirements_updater%2Fnumpy1_requirements%2Frequirements.in",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/ci%2Fofficial%2Frequirements_updater%2Fnumpy1_requirements%2Frequirements.in?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -34,21 +34,6 @@ packaging==23.2\n setuptools==78.1.1\n jax==0.4.7\n zstandard==0.23.0\n-# NVIDIA CUDA dependencies\n-# Note that the wheels are downloaded only when the targets in bazel command\n-# contain dependencies on these wheels.\n-nvidia-cublas-cu12 == 12.5.3.2\n-nvidia-cuda-cupti-cu12 == 12.5.82\n-nvidia-cuda-nvrtc-cu12 == 12.5.82\n-nvidia-cuda-runtime-cu12 == 12.5.82\n-nvidia-cudnn-cu12 == 9.3.0.75\n-nvidia-cufft-cu12 == 11.2.3.61\n-nvidia-curand-cu12 == 10.3.6.82\n-nvidia-cusolver-cu12 == 11.6.3.83\n-nvidia-cusparse-cu12 == 12.5.1.3\n-nvidia-nccl-cu12 == 2.27.7\n-nvidia-nvjitlink-cu12 == 12.5.82\n-nvidia-nvshmem-cu12>=3.2.5\n # The dependencies below are needed for TF wheel testing.\n tensorflow-io-gcs-filesystem==0.37.1 ; python_version <= \"3.12\"\n libclang >= 13.0.0"
        },
        {
            "sha": "1e4f85c2ab70c7276956fdc9b232f6f27f9f9c3d",
            "filename": "ci/official/requirements_updater/numpy1_requirements/requirements_lock_3_10.txt",
            "status": "modified",
            "additions": 56,
            "deletions": 15,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/ci%2Fofficial%2Frequirements_updater%2Fnumpy1_requirements%2Frequirements_lock_3_10.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/ci%2Fofficial%2Frequirements_updater%2Fnumpy1_requirements%2Frequirements_lock_3_10.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/ci%2Fofficial%2Frequirements_updater%2Fnumpy1_requirements%2Frequirements_lock_3_10.txt?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -407,68 +407,109 @@ nvidia-cublas-cu12==12.5.3.2 \\\n     --hash=sha256:7d0191251180de606023d396b94d66f66470a0ae96d1dbb906c7656ea0f71eda \\\n     --hash=sha256:ca070ad70e9fa6654084575d01bd001f30cc4665e33d4bb9fc8e0f321caa034b\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cudnn-cu12\n     #   nvidia-cusolver-cu12\n nvidia-cuda-cupti-cu12==12.5.82 \\\n     --hash=sha256:4f835281cf492e2bedd153f5c3de9da8f1d775a419468305e64ce73b3b0c6dc3 \\\n     --hash=sha256:bde77a5feb66752ec61db2adfe47f56b941842825b4c7e2068aff27c9d107953 \\\n     --hash=sha256:d32c06490c6ba35c4323730820c7d0c4c126c04ed58d2f57275adb8d54b138fe\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n+nvidia-cuda-nvcc-cu12==12.5.82 \\\n+    --hash=sha256:6eaa264da57a893ae7606dd80b169d9783444af941697822cb82c8379ffc4957 \\\n+    --hash=sha256:ab02fe922cee01235b7950f045042219fe83e15aceb4cd3c1d36db30b034dec7 \\\n+    --hash=sha256:b03e545b8e8c3ce7ebcd7fc44063180ff52ff01d064ece2127ed90a04ef12cd0\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cuda-nvrtc-cu12==12.5.82 \\\n     --hash=sha256:3dbd97b0104b4bfbc3c4f8c79cd2496307c89c43c29a9f83125f1d76296ff3fd \\\n     --hash=sha256:5bb6a0eb01d4974bb7ca3d48bd3859472debb3c3057a5e7de2b08fbdf35eed7e \\\n     --hash=sha256:e5db37e990056c70953b7772dd778336ef9da0a0b5bb28f9f2a61c2e42b51d78\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cuda-runtime-cu12==12.5.82 \\\n     --hash=sha256:0fd5fbca289bceb9f0690aa9858f06187b554fdeb7e2711dfd5bb3ce58900b46 \\\n     --hash=sha256:3e79a060e126df40fd3a068f3f787eb000fa51b251ec6cd97d09579632687115 \\\n     --hash=sha256:71f015dbf9df05dd71f7480132c6ebf47a6ceb2ab53d7db8e08e4b30ebb87e14\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cudnn-cu12==9.3.0.75 \\\n     --hash=sha256:9ad9c6929ebb5295eb4a1728024666d1c88283373e265a0c5c883e6f9d5cd76d \\\n     --hash=sha256:c5cf7ff3415e446adf195a5b7dd2ba56cd00c3ee78bfdc566e51698931aa4b7f \\\n     --hash=sha256:c819e82eed8cf564b9d37478ea4eab9e87194bb3b7f7f8098bc1f67c9b80f1b6\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cufft-cu12==11.2.3.61 \\\n     --hash=sha256:4a8f6f0ce93c52a50ee83422a80472b5f376054a63f38532d0eab4007e7ef28b \\\n     --hash=sha256:6d45b48a5ee7599e57131129cda2c58544d9b78b95064d3ec3e5c6b96e2b58cc \\\n     --hash=sha256:9a6e8df162585750f61983a638104a48c756aa13f9f48e19ab079b38e3c828b8\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-curand-cu12==10.3.6.82 \\\n     --hash=sha256:0631ba65231260ad832ce233ddda57e7b3b7158eabf000d78e46cbb5bd5b7aae \\\n     --hash=sha256:2823fb27de4e44dbb22394a6adf53aa6e1b013aca0f8c22867d1cfae58405536 \\\n     --hash=sha256:36aabeb5990297bbce3df324ea7c7c13c3aabb140c86d50ab3b23e4ec61672f1\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cusolver-cu12==11.6.3.83 \\\n     --hash=sha256:1b8b77d2fe8abe72bb722dafb708cceaeb81f1a03999477f20b33b34f46ab885 \\\n     --hash=sha256:6224732963cba312a84c78114b9a38c4ffabb2e2a6a120923ac99ba6f895c8cf \\\n     --hash=sha256:93cfafacde4428b71778eeb092ec615a02a3d05404da1bcf91c53e3fa1bce42b\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cusparse-cu12==12.5.1.3 \\\n     --hash=sha256:016df8e993c437e8301e62739f01775cba988fd5253cd4c64173f8e8d2f8e752 \\\n     --hash=sha256:33520db374e2f5ebc976d6faa1852b98c398a57e6f71150fe59705928596ffd1 \\\n     --hash=sha256:7b97fd01f0a61628af99d0efd52132fccc8c18fc5c509f13802dccf0574a19c2\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cusolver-cu12\n nvidia-nccl-cu12==2.27.7 \\\n     --hash=sha256:4617839f3bb730c3845bf9adf92dbe0e009bc53ca5022ed941f2e23fb76e6f17 \\\n     --hash=sha256:de5ba5562f08029a19cb1cd659404b18411ed0d6c90ac5f52f30bf99ad5809aa\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-nvjitlink-cu12==12.5.82 \\\n     --hash=sha256:98103729cc5226e13ca319a10bbf9433bbbd44ef64fe72f45f067cacc14b8d27 \\\n     --hash=sha256:e782564d705ff0bf61ac3e1bf730166da66dd2fe9012f111ede5fc49b64ae697 \\\n     --hash=sha256:f9b37bc5c8cf7509665cb6ada5aaa0ce65618f2332b7d3e78e9790511f111212\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cufft-cu12\n     #   nvidia-cusolver-cu12\n     #   nvidia-cusparse-cu12\n-nvidia-nvshmem-cu12==3.3.9 \\\n-    --hash=sha256:2de43cbfe559e16b8e3cb777b95f1fe2ddd5c2cfd79414b09cf9cf099feba2ba \\\n-    --hash=sha256:95ba1e98189c056eb5372bd355ab714b3741a03e6de1e32f167f5240fd967c5f\n-    # via -r ci/official/requirements_updater/requirements.in\n+nvidia-nvshmem-cu12==3.2.5 \\\n+    --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n+    --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549"
        },
        {
            "sha": "19da7ab86c09706def667adc6e8ea32f14b8dc26",
            "filename": "ci/official/requirements_updater/numpy1_requirements/requirements_lock_3_11.txt",
            "status": "modified",
            "additions": 52,
            "deletions": 12,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/ci%2Fofficial%2Frequirements_updater%2Fnumpy1_requirements%2Frequirements_lock_3_11.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/ci%2Fofficial%2Frequirements_updater%2Fnumpy1_requirements%2Frequirements_lock_3_11.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/ci%2Fofficial%2Frequirements_updater%2Fnumpy1_requirements%2Frequirements_lock_3_11.txt?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -407,69 +407,109 @@ nvidia-cublas-cu12==12.5.3.2 \\\n     --hash=sha256:7d0191251180de606023d396b94d66f66470a0ae96d1dbb906c7656ea0f71eda \\\n     --hash=sha256:ca070ad70e9fa6654084575d01bd001f30cc4665e33d4bb9fc8e0f321caa034b\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cudnn-cu12\n     #   nvidia-cusolver-cu12\n nvidia-cuda-cupti-cu12==12.5.82 \\\n     --hash=sha256:4f835281cf492e2bedd153f5c3de9da8f1d775a419468305e64ce73b3b0c6dc3 \\\n     --hash=sha256:bde77a5feb66752ec61db2adfe47f56b941842825b4c7e2068aff27c9d107953 \\\n     --hash=sha256:d32c06490c6ba35c4323730820c7d0c4c126c04ed58d2f57275adb8d54b138fe\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n+nvidia-cuda-nvcc-cu12==12.5.82 \\\n+    --hash=sha256:6eaa264da57a893ae7606dd80b169d9783444af941697822cb82c8379ffc4957 \\\n+    --hash=sha256:ab02fe922cee01235b7950f045042219fe83e15aceb4cd3c1d36db30b034dec7 \\\n+    --hash=sha256:b03e545b8e8c3ce7ebcd7fc44063180ff52ff01d064ece2127ed90a04ef12cd0\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cuda-nvrtc-cu12==12.5.82 \\\n     --hash=sha256:3dbd97b0104b4bfbc3c4f8c79cd2496307c89c43c29a9f83125f1d76296ff3fd \\\n     --hash=sha256:5bb6a0eb01d4974bb7ca3d48bd3859472debb3c3057a5e7de2b08fbdf35eed7e \\\n     --hash=sha256:e5db37e990056c70953b7772dd778336ef9da0a0b5bb28f9f2a61c2e42b51d78\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cuda-runtime-cu12==12.5.82 \\\n     --hash=sha256:0fd5fbca289bceb9f0690aa9858f06187b554fdeb7e2711dfd5bb3ce58900b46 \\\n     --hash=sha256:3e79a060e126df40fd3a068f3f787eb000fa51b251ec6cd97d09579632687115 \\\n     --hash=sha256:71f015dbf9df05dd71f7480132c6ebf47a6ceb2ab53d7db8e08e4b30ebb87e14\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cudnn-cu12==9.3.0.75 \\\n     --hash=sha256:9ad9c6929ebb5295eb4a1728024666d1c88283373e265a0c5c883e6f9d5cd76d \\\n     --hash=sha256:c5cf7ff3415e446adf195a5b7dd2ba56cd00c3ee78bfdc566e51698931aa4b7f \\\n     --hash=sha256:c819e82eed8cf564b9d37478ea4eab9e87194bb3b7f7f8098bc1f67c9b80f1b6\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cufft-cu12==11.2.3.61 \\\n     --hash=sha256:4a8f6f0ce93c52a50ee83422a80472b5f376054a63f38532d0eab4007e7ef28b \\\n     --hash=sha256:6d45b48a5ee7599e57131129cda2c58544d9b78b95064d3ec3e5c6b96e2b58cc \\\n     --hash=sha256:9a6e8df162585750f61983a638104a48c756aa13f9f48e19ab079b38e3c828b8\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-curand-cu12==10.3.6.82 \\\n     --hash=sha256:0631ba65231260ad832ce233ddda57e7b3b7158eabf000d78e46cbb5bd5b7aae \\\n     --hash=sha256:2823fb27de4e44dbb22394a6adf53aa6e1b013aca0f8c22867d1cfae58405536 \\\n     --hash=sha256:36aabeb5990297bbce3df324ea7c7c13c3aabb140c86d50ab3b23e4ec61672f1\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cusolver-cu12==11.6.3.83 \\\n     --hash=sha256:1b8b77d2fe8abe72bb722dafb708cceaeb81f1a03999477f20b33b34f46ab885 \\\n     --hash=sha256:6224732963cba312a84c78114b9a38c4ffabb2e2a6a120923ac99ba6f895c8cf \\\n     --hash=sha256:93cfafacde4428b71778eeb092ec615a02a3d05404da1bcf91c53e3fa1bce42b\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cusparse-cu12==12.5.1.3 \\\n     --hash=sha256:016df8e993c437e8301e62739f01775cba988fd5253cd4c64173f8e8d2f8e752 \\\n     --hash=sha256:33520db374e2f5ebc976d6faa1852b98c398a57e6f71150fe59705928596ffd1 \\\n     --hash=sha256:7b97fd01f0a61628af99d0efd52132fccc8c18fc5c509f13802dccf0574a19c2\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cusolver-cu12\n nvidia-nccl-cu12==2.27.7 \\\n     --hash=sha256:4617839f3bb730c3845bf9adf92dbe0e009bc53ca5022ed941f2e23fb76e6f17 \\\n     --hash=sha256:de5ba5562f08029a19cb1cd659404b18411ed0d6c90ac5f52f30bf99ad5809aa\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-nvjitlink-cu12==12.5.82 \\\n     --hash=sha256:98103729cc5226e13ca319a10bbf9433bbbd44ef64fe72f45f067cacc14b8d27 \\\n     --hash=sha256:e782564d705ff0bf61ac3e1bf730166da66dd2fe9012f111ede5fc49b64ae697 \\\n     --hash=sha256:f9b37bc5c8cf7509665cb6ada5aaa0ce65618f2332b7d3e78e9790511f111212\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cufft-cu12\n     #   nvidia-cusolver-cu12\n     #   nvidia-cusparse-cu12\n nvidia-nvshmem-cu12==3.2.5 \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549"
        },
        {
            "sha": "f0b93a1e52b3d5602acd3d9c13863ddb0f112478",
            "filename": "ci/official/requirements_updater/numpy1_requirements/requirements_lock_3_12.txt",
            "status": "modified",
            "additions": 52,
            "deletions": 12,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/ci%2Fofficial%2Frequirements_updater%2Fnumpy1_requirements%2Frequirements_lock_3_12.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/ci%2Fofficial%2Frequirements_updater%2Fnumpy1_requirements%2Frequirements_lock_3_12.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/ci%2Fofficial%2Frequirements_updater%2Fnumpy1_requirements%2Frequirements_lock_3_12.txt?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -407,69 +407,109 @@ nvidia-cublas-cu12==12.5.3.2 \\\n     --hash=sha256:7d0191251180de606023d396b94d66f66470a0ae96d1dbb906c7656ea0f71eda \\\n     --hash=sha256:ca070ad70e9fa6654084575d01bd001f30cc4665e33d4bb9fc8e0f321caa034b\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cudnn-cu12\n     #   nvidia-cusolver-cu12\n nvidia-cuda-cupti-cu12==12.5.82 \\\n     --hash=sha256:4f835281cf492e2bedd153f5c3de9da8f1d775a419468305e64ce73b3b0c6dc3 \\\n     --hash=sha256:bde77a5feb66752ec61db2adfe47f56b941842825b4c7e2068aff27c9d107953 \\\n     --hash=sha256:d32c06490c6ba35c4323730820c7d0c4c126c04ed58d2f57275adb8d54b138fe\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n+nvidia-cuda-nvcc-cu12==12.5.82 \\\n+    --hash=sha256:6eaa264da57a893ae7606dd80b169d9783444af941697822cb82c8379ffc4957 \\\n+    --hash=sha256:ab02fe922cee01235b7950f045042219fe83e15aceb4cd3c1d36db30b034dec7 \\\n+    --hash=sha256:b03e545b8e8c3ce7ebcd7fc44063180ff52ff01d064ece2127ed90a04ef12cd0\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cuda-nvrtc-cu12==12.5.82 \\\n     --hash=sha256:3dbd97b0104b4bfbc3c4f8c79cd2496307c89c43c29a9f83125f1d76296ff3fd \\\n     --hash=sha256:5bb6a0eb01d4974bb7ca3d48bd3859472debb3c3057a5e7de2b08fbdf35eed7e \\\n     --hash=sha256:e5db37e990056c70953b7772dd778336ef9da0a0b5bb28f9f2a61c2e42b51d78\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cuda-runtime-cu12==12.5.82 \\\n     --hash=sha256:0fd5fbca289bceb9f0690aa9858f06187b554fdeb7e2711dfd5bb3ce58900b46 \\\n     --hash=sha256:3e79a060e126df40fd3a068f3f787eb000fa51b251ec6cd97d09579632687115 \\\n     --hash=sha256:71f015dbf9df05dd71f7480132c6ebf47a6ceb2ab53d7db8e08e4b30ebb87e14\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cudnn-cu12==9.3.0.75 \\\n     --hash=sha256:9ad9c6929ebb5295eb4a1728024666d1c88283373e265a0c5c883e6f9d5cd76d \\\n     --hash=sha256:c5cf7ff3415e446adf195a5b7dd2ba56cd00c3ee78bfdc566e51698931aa4b7f \\\n     --hash=sha256:c819e82eed8cf564b9d37478ea4eab9e87194bb3b7f7f8098bc1f67c9b80f1b6\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cufft-cu12==11.2.3.61 \\\n     --hash=sha256:4a8f6f0ce93c52a50ee83422a80472b5f376054a63f38532d0eab4007e7ef28b \\\n     --hash=sha256:6d45b48a5ee7599e57131129cda2c58544d9b78b95064d3ec3e5c6b96e2b58cc \\\n     --hash=sha256:9a6e8df162585750f61983a638104a48c756aa13f9f48e19ab079b38e3c828b8\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-curand-cu12==10.3.6.82 \\\n     --hash=sha256:0631ba65231260ad832ce233ddda57e7b3b7158eabf000d78e46cbb5bd5b7aae \\\n     --hash=sha256:2823fb27de4e44dbb22394a6adf53aa6e1b013aca0f8c22867d1cfae58405536 \\\n     --hash=sha256:36aabeb5990297bbce3df324ea7c7c13c3aabb140c86d50ab3b23e4ec61672f1\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cusolver-cu12==11.6.3.83 \\\n     --hash=sha256:1b8b77d2fe8abe72bb722dafb708cceaeb81f1a03999477f20b33b34f46ab885 \\\n     --hash=sha256:6224732963cba312a84c78114b9a38c4ffabb2e2a6a120923ac99ba6f895c8cf \\\n     --hash=sha256:93cfafacde4428b71778eeb092ec615a02a3d05404da1bcf91c53e3fa1bce42b\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cusparse-cu12==12.5.1.3 \\\n     --hash=sha256:016df8e993c437e8301e62739f01775cba988fd5253cd4c64173f8e8d2f8e752 \\\n     --hash=sha256:33520db374e2f5ebc976d6faa1852b98c398a57e6f71150fe59705928596ffd1 \\\n     --hash=sha256:7b97fd01f0a61628af99d0efd52132fccc8c18fc5c509f13802dccf0574a19c2\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cusolver-cu12\n nvidia-nccl-cu12==2.27.7 \\\n     --hash=sha256:4617839f3bb730c3845bf9adf92dbe0e009bc53ca5022ed941f2e23fb76e6f17 \\\n     --hash=sha256:de5ba5562f08029a19cb1cd659404b18411ed0d6c90ac5f52f30bf99ad5809aa\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-nvjitlink-cu12==12.5.82 \\\n     --hash=sha256:98103729cc5226e13ca319a10bbf9433bbbd44ef64fe72f45f067cacc14b8d27 \\\n     --hash=sha256:e782564d705ff0bf61ac3e1bf730166da66dd2fe9012f111ede5fc49b64ae697 \\\n     --hash=sha256:f9b37bc5c8cf7509665cb6ada5aaa0ce65618f2332b7d3e78e9790511f111212\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cufft-cu12\n     #   nvidia-cusolver-cu12\n     #   nvidia-cusparse-cu12\n nvidia-nvshmem-cu12==3.2.5 \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549"
        },
        {
            "sha": "9a4144d10d9b315f5c99f8835782d8d1ba82bb74",
            "filename": "ci/official/requirements_updater/numpy1_requirements/requirements_lock_3_9.txt",
            "status": "modified",
            "additions": 56,
            "deletions": 15,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/ci%2Fofficial%2Frequirements_updater%2Fnumpy1_requirements%2Frequirements_lock_3_9.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/ci%2Fofficial%2Frequirements_updater%2Fnumpy1_requirements%2Frequirements_lock_3_9.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/ci%2Fofficial%2Frequirements_updater%2Fnumpy1_requirements%2Frequirements_lock_3_9.txt?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -411,68 +411,109 @@ nvidia-cublas-cu12==12.5.3.2 \\\n     --hash=sha256:7d0191251180de606023d396b94d66f66470a0ae96d1dbb906c7656ea0f71eda \\\n     --hash=sha256:ca070ad70e9fa6654084575d01bd001f30cc4665e33d4bb9fc8e0f321caa034b\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cudnn-cu12\n     #   nvidia-cusolver-cu12\n nvidia-cuda-cupti-cu12==12.5.82 \\\n     --hash=sha256:4f835281cf492e2bedd153f5c3de9da8f1d775a419468305e64ce73b3b0c6dc3 \\\n     --hash=sha256:bde77a5feb66752ec61db2adfe47f56b941842825b4c7e2068aff27c9d107953 \\\n     --hash=sha256:d32c06490c6ba35c4323730820c7d0c4c126c04ed58d2f57275adb8d54b138fe\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n+nvidia-cuda-nvcc-cu12==12.5.82 \\\n+    --hash=sha256:6eaa264da57a893ae7606dd80b169d9783444af941697822cb82c8379ffc4957 \\\n+    --hash=sha256:ab02fe922cee01235b7950f045042219fe83e15aceb4cd3c1d36db30b034dec7 \\\n+    --hash=sha256:b03e545b8e8c3ce7ebcd7fc44063180ff52ff01d064ece2127ed90a04ef12cd0\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cuda-nvrtc-cu12==12.5.82 \\\n     --hash=sha256:3dbd97b0104b4bfbc3c4f8c79cd2496307c89c43c29a9f83125f1d76296ff3fd \\\n     --hash=sha256:5bb6a0eb01d4974bb7ca3d48bd3859472debb3c3057a5e7de2b08fbdf35eed7e \\\n     --hash=sha256:e5db37e990056c70953b7772dd778336ef9da0a0b5bb28f9f2a61c2e42b51d78\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cuda-runtime-cu12==12.5.82 \\\n     --hash=sha256:0fd5fbca289bceb9f0690aa9858f06187b554fdeb7e2711dfd5bb3ce58900b46 \\\n     --hash=sha256:3e79a060e126df40fd3a068f3f787eb000fa51b251ec6cd97d09579632687115 \\\n     --hash=sha256:71f015dbf9df05dd71f7480132c6ebf47a6ceb2ab53d7db8e08e4b30ebb87e14\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cudnn-cu12==9.3.0.75 \\\n     --hash=sha256:9ad9c6929ebb5295eb4a1728024666d1c88283373e265a0c5c883e6f9d5cd76d \\\n     --hash=sha256:c5cf7ff3415e446adf195a5b7dd2ba56cd00c3ee78bfdc566e51698931aa4b7f \\\n     --hash=sha256:c819e82eed8cf564b9d37478ea4eab9e87194bb3b7f7f8098bc1f67c9b80f1b6\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cufft-cu12==11.2.3.61 \\\n     --hash=sha256:4a8f6f0ce93c52a50ee83422a80472b5f376054a63f38532d0eab4007e7ef28b \\\n     --hash=sha256:6d45b48a5ee7599e57131129cda2c58544d9b78b95064d3ec3e5c6b96e2b58cc \\\n     --hash=sha256:9a6e8df162585750f61983a638104a48c756aa13f9f48e19ab079b38e3c828b8\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-curand-cu12==10.3.6.82 \\\n     --hash=sha256:0631ba65231260ad832ce233ddda57e7b3b7158eabf000d78e46cbb5bd5b7aae \\\n     --hash=sha256:2823fb27de4e44dbb22394a6adf53aa6e1b013aca0f8c22867d1cfae58405536 \\\n     --hash=sha256:36aabeb5990297bbce3df324ea7c7c13c3aabb140c86d50ab3b23e4ec61672f1\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cusolver-cu12==11.6.3.83 \\\n     --hash=sha256:1b8b77d2fe8abe72bb722dafb708cceaeb81f1a03999477f20b33b34f46ab885 \\\n     --hash=sha256:6224732963cba312a84c78114b9a38c4ffabb2e2a6a120923ac99ba6f895c8cf \\\n     --hash=sha256:93cfafacde4428b71778eeb092ec615a02a3d05404da1bcf91c53e3fa1bce42b\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cusparse-cu12==12.5.1.3 \\\n     --hash=sha256:016df8e993c437e8301e62739f01775cba988fd5253cd4c64173f8e8d2f8e752 \\\n     --hash=sha256:33520db374e2f5ebc976d6faa1852b98c398a57e6f71150fe59705928596ffd1 \\\n     --hash=sha256:7b97fd01f0a61628af99d0efd52132fccc8c18fc5c509f13802dccf0574a19c2\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cusolver-cu12\n nvidia-nccl-cu12==2.27.7 \\\n     --hash=sha256:4617839f3bb730c3845bf9adf92dbe0e009bc53ca5022ed941f2e23fb76e6f17 \\\n     --hash=sha256:de5ba5562f08029a19cb1cd659404b18411ed0d6c90ac5f52f30bf99ad5809aa\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-nvjitlink-cu12==12.5.82 \\\n     --hash=sha256:98103729cc5226e13ca319a10bbf9433bbbd44ef64fe72f45f067cacc14b8d27 \\\n     --hash=sha256:e782564d705ff0bf61ac3e1bf730166da66dd2fe9012f111ede5fc49b64ae697 \\\n     --hash=sha256:f9b37bc5c8cf7509665cb6ada5aaa0ce65618f2332b7d3e78e9790511f111212\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cufft-cu12\n     #   nvidia-cusolver-cu12\n     #   nvidia-cusparse-cu12\n-nvidia-nvshmem-cu12==3.3.9 \\\n-    --hash=sha256:2de43cbfe559e16b8e3cb777b95f1fe2ddd5c2cfd79414b09cf9cf099feba2ba \\\n-    --hash=sha256:95ba1e98189c056eb5372bd355ab714b3741a03e6de1e32f167f5240fd967c5f\n-    # via -r ci/official/requirements_updater/requirements.in\n+nvidia-nvshmem-cu12==3.2.5 \\\n+    --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n+    --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549"
        },
        {
            "sha": "e494ea008f858e428f904d4809fe000fb06df48c",
            "filename": "ci/official/requirements_updater/nvidia-requirements.txt",
            "status": "added",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/ci%2Fofficial%2Frequirements_updater%2Fnvidia-requirements.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/ci%2Fofficial%2Frequirements_updater%2Fnvidia-requirements.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/ci%2Fofficial%2Frequirements_updater%2Fnvidia-requirements.txt?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -0,0 +1,16 @@\n+nvidia-cublas-cu12>=12.5.3.2,<13.0\n+nvidia-cuda-cupti-cu12>=12.5.82,<13.0\n+nvidia-cuda-nvcc-cu12>=12.5.82,<13.0\n+nvidia-cuda-nvrtc-cu12>=12.5.82,<13.0\n+nvidia-cuda-runtime-cu12>=12.5.82,<13.0\n+# The upper bound is set for the CUDNN API compatibility.\n+# See\n+# https://docs.nvidia.com/deeplearning/cudnn/backend/latest/developer/forward-compatibility.html#cudnn-api-compatibility\n+nvidia-cudnn-cu12>=9.3.0.75,<10.0\n+nvidia-cufft-cu12>=11.2.3.61,<12.0\n+nvidia-curand-cu12>=10.3.6.82,<11.0\n+nvidia-cusolver-cu12>=11.6.3.83,<12.0\n+nvidia-cusparse-cu12>=12.5.1.3,<13.0\n+nvidia-nccl-cu12>=2.27.7,<3.0\n+nvidia-nvjitlink-cu12>=12.5.82,<13.0\n+nvidia-nvshmem-cu12>=3.2.5\n\\ No newline at end of file"
        },
        {
            "sha": "ce5ecbd998c032ced808325cb7576a24136bf432",
            "filename": "ci/official/requirements_updater/requirements.in",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/ci%2Fofficial%2Frequirements_updater%2Frequirements.in",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/ci%2Fofficial%2Frequirements_updater%2Frequirements.in",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/ci%2Fofficial%2Frequirements_updater%2Frequirements.in?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -34,21 +34,6 @@ packaging==23.2\n setuptools==78.1.1\n jax==0.4.7\n zstandard==0.23.0\n-# NVIDIA CUDA dependencies\n-# Note that the wheels are downloaded only when the targets in bazel command\n-# contain dependencies on these wheels.\n-nvidia-cublas-cu12 == 12.5.3.2\n-nvidia-cuda-cupti-cu12 == 12.5.82\n-nvidia-cuda-nvrtc-cu12 == 12.5.82\n-nvidia-cuda-runtime-cu12 == 12.5.82\n-nvidia-cudnn-cu12 == 9.3.0.75\n-nvidia-cufft-cu12 == 11.2.3.61\n-nvidia-curand-cu12 == 10.3.6.82\n-nvidia-cusolver-cu12 == 11.6.3.83\n-nvidia-cusparse-cu12 == 12.5.1.3\n-nvidia-nccl-cu12 == 2.27.7\n-nvidia-nvjitlink-cu12 == 12.5.82\n-nvidia-nvshmem-cu12>=3.2.5\n # The dependencies below are needed for TF wheel testing.\n tensorflow-io-gcs-filesystem==0.37.1 ; python_version <= \"3.12\"\n libclang >= 13.0.0"
        },
        {
            "sha": "34b2ea8a386f1921f14f8897879e791d9c74dbb7",
            "filename": "requirements_lock_3_10.txt",
            "status": "modified",
            "additions": 56,
            "deletions": 15,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/requirements_lock_3_10.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/requirements_lock_3_10.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/requirements_lock_3_10.txt?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -426,68 +426,109 @@ nvidia-cublas-cu12==12.5.3.2 \\\n     --hash=sha256:7d0191251180de606023d396b94d66f66470a0ae96d1dbb906c7656ea0f71eda \\\n     --hash=sha256:ca070ad70e9fa6654084575d01bd001f30cc4665e33d4bb9fc8e0f321caa034b\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cudnn-cu12\n     #   nvidia-cusolver-cu12\n nvidia-cuda-cupti-cu12==12.5.82 \\\n     --hash=sha256:4f835281cf492e2bedd153f5c3de9da8f1d775a419468305e64ce73b3b0c6dc3 \\\n     --hash=sha256:bde77a5feb66752ec61db2adfe47f56b941842825b4c7e2068aff27c9d107953 \\\n     --hash=sha256:d32c06490c6ba35c4323730820c7d0c4c126c04ed58d2f57275adb8d54b138fe\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n+nvidia-cuda-nvcc-cu12==12.5.82 \\\n+    --hash=sha256:6eaa264da57a893ae7606dd80b169d9783444af941697822cb82c8379ffc4957 \\\n+    --hash=sha256:ab02fe922cee01235b7950f045042219fe83e15aceb4cd3c1d36db30b034dec7 \\\n+    --hash=sha256:b03e545b8e8c3ce7ebcd7fc44063180ff52ff01d064ece2127ed90a04ef12cd0\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cuda-nvrtc-cu12==12.5.82 \\\n     --hash=sha256:3dbd97b0104b4bfbc3c4f8c79cd2496307c89c43c29a9f83125f1d76296ff3fd \\\n     --hash=sha256:5bb6a0eb01d4974bb7ca3d48bd3859472debb3c3057a5e7de2b08fbdf35eed7e \\\n     --hash=sha256:e5db37e990056c70953b7772dd778336ef9da0a0b5bb28f9f2a61c2e42b51d78\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cuda-runtime-cu12==12.5.82 \\\n     --hash=sha256:0fd5fbca289bceb9f0690aa9858f06187b554fdeb7e2711dfd5bb3ce58900b46 \\\n     --hash=sha256:3e79a060e126df40fd3a068f3f787eb000fa51b251ec6cd97d09579632687115 \\\n     --hash=sha256:71f015dbf9df05dd71f7480132c6ebf47a6ceb2ab53d7db8e08e4b30ebb87e14\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cudnn-cu12==9.3.0.75 \\\n     --hash=sha256:9ad9c6929ebb5295eb4a1728024666d1c88283373e265a0c5c883e6f9d5cd76d \\\n     --hash=sha256:c5cf7ff3415e446adf195a5b7dd2ba56cd00c3ee78bfdc566e51698931aa4b7f \\\n     --hash=sha256:c819e82eed8cf564b9d37478ea4eab9e87194bb3b7f7f8098bc1f67c9b80f1b6\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cufft-cu12==11.2.3.61 \\\n     --hash=sha256:4a8f6f0ce93c52a50ee83422a80472b5f376054a63f38532d0eab4007e7ef28b \\\n     --hash=sha256:6d45b48a5ee7599e57131129cda2c58544d9b78b95064d3ec3e5c6b96e2b58cc \\\n     --hash=sha256:9a6e8df162585750f61983a638104a48c756aa13f9f48e19ab079b38e3c828b8\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-curand-cu12==10.3.6.82 \\\n     --hash=sha256:0631ba65231260ad832ce233ddda57e7b3b7158eabf000d78e46cbb5bd5b7aae \\\n     --hash=sha256:2823fb27de4e44dbb22394a6adf53aa6e1b013aca0f8c22867d1cfae58405536 \\\n     --hash=sha256:36aabeb5990297bbce3df324ea7c7c13c3aabb140c86d50ab3b23e4ec61672f1\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cusolver-cu12==11.6.3.83 \\\n     --hash=sha256:1b8b77d2fe8abe72bb722dafb708cceaeb81f1a03999477f20b33b34f46ab885 \\\n     --hash=sha256:6224732963cba312a84c78114b9a38c4ffabb2e2a6a120923ac99ba6f895c8cf \\\n     --hash=sha256:93cfafacde4428b71778eeb092ec615a02a3d05404da1bcf91c53e3fa1bce42b\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cusparse-cu12==12.5.1.3 \\\n     --hash=sha256:016df8e993c437e8301e62739f01775cba988fd5253cd4c64173f8e8d2f8e752 \\\n     --hash=sha256:33520db374e2f5ebc976d6faa1852b98c398a57e6f71150fe59705928596ffd1 \\\n     --hash=sha256:7b97fd01f0a61628af99d0efd52132fccc8c18fc5c509f13802dccf0574a19c2\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cusolver-cu12\n nvidia-nccl-cu12==2.27.7 \\\n     --hash=sha256:4617839f3bb730c3845bf9adf92dbe0e009bc53ca5022ed941f2e23fb76e6f17 \\\n     --hash=sha256:de5ba5562f08029a19cb1cd659404b18411ed0d6c90ac5f52f30bf99ad5809aa\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-nvjitlink-cu12==12.5.82 \\\n     --hash=sha256:98103729cc5226e13ca319a10bbf9433bbbd44ef64fe72f45f067cacc14b8d27 \\\n     --hash=sha256:e782564d705ff0bf61ac3e1bf730166da66dd2fe9012f111ede5fc49b64ae697 \\\n     --hash=sha256:f9b37bc5c8cf7509665cb6ada5aaa0ce65618f2332b7d3e78e9790511f111212\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cufft-cu12\n     #   nvidia-cusolver-cu12\n     #   nvidia-cusparse-cu12\n-nvidia-nvshmem-cu12==3.3.9 \\\n-    --hash=sha256:2de43cbfe559e16b8e3cb777b95f1fe2ddd5c2cfd79414b09cf9cf099feba2ba \\\n-    --hash=sha256:95ba1e98189c056eb5372bd355ab714b3741a03e6de1e32f167f5240fd967c5f\n-    # via -r ci/official/requirements_updater/requirements.in\n+nvidia-nvshmem-cu12==3.2.5 \\\n+    --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n+    --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549"
        },
        {
            "sha": "e7b4bbae6424e55a23a8e586e8da331a6f0c2c4a",
            "filename": "requirements_lock_3_11.txt",
            "status": "modified",
            "additions": 52,
            "deletions": 12,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/requirements_lock_3_11.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/requirements_lock_3_11.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/requirements_lock_3_11.txt?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -426,69 +426,109 @@ nvidia-cublas-cu12==12.5.3.2 \\\n     --hash=sha256:7d0191251180de606023d396b94d66f66470a0ae96d1dbb906c7656ea0f71eda \\\n     --hash=sha256:ca070ad70e9fa6654084575d01bd001f30cc4665e33d4bb9fc8e0f321caa034b\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cudnn-cu12\n     #   nvidia-cusolver-cu12\n nvidia-cuda-cupti-cu12==12.5.82 \\\n     --hash=sha256:4f835281cf492e2bedd153f5c3de9da8f1d775a419468305e64ce73b3b0c6dc3 \\\n     --hash=sha256:bde77a5feb66752ec61db2adfe47f56b941842825b4c7e2068aff27c9d107953 \\\n     --hash=sha256:d32c06490c6ba35c4323730820c7d0c4c126c04ed58d2f57275adb8d54b138fe\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n+nvidia-cuda-nvcc-cu12==12.5.82 \\\n+    --hash=sha256:6eaa264da57a893ae7606dd80b169d9783444af941697822cb82c8379ffc4957 \\\n+    --hash=sha256:ab02fe922cee01235b7950f045042219fe83e15aceb4cd3c1d36db30b034dec7 \\\n+    --hash=sha256:b03e545b8e8c3ce7ebcd7fc44063180ff52ff01d064ece2127ed90a04ef12cd0\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cuda-nvrtc-cu12==12.5.82 \\\n     --hash=sha256:3dbd97b0104b4bfbc3c4f8c79cd2496307c89c43c29a9f83125f1d76296ff3fd \\\n     --hash=sha256:5bb6a0eb01d4974bb7ca3d48bd3859472debb3c3057a5e7de2b08fbdf35eed7e \\\n     --hash=sha256:e5db37e990056c70953b7772dd778336ef9da0a0b5bb28f9f2a61c2e42b51d78\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cuda-runtime-cu12==12.5.82 \\\n     --hash=sha256:0fd5fbca289bceb9f0690aa9858f06187b554fdeb7e2711dfd5bb3ce58900b46 \\\n     --hash=sha256:3e79a060e126df40fd3a068f3f787eb000fa51b251ec6cd97d09579632687115 \\\n     --hash=sha256:71f015dbf9df05dd71f7480132c6ebf47a6ceb2ab53d7db8e08e4b30ebb87e14\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cudnn-cu12==9.3.0.75 \\\n     --hash=sha256:9ad9c6929ebb5295eb4a1728024666d1c88283373e265a0c5c883e6f9d5cd76d \\\n     --hash=sha256:c5cf7ff3415e446adf195a5b7dd2ba56cd00c3ee78bfdc566e51698931aa4b7f \\\n     --hash=sha256:c819e82eed8cf564b9d37478ea4eab9e87194bb3b7f7f8098bc1f67c9b80f1b6\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cufft-cu12==11.2.3.61 \\\n     --hash=sha256:4a8f6f0ce93c52a50ee83422a80472b5f376054a63f38532d0eab4007e7ef28b \\\n     --hash=sha256:6d45b48a5ee7599e57131129cda2c58544d9b78b95064d3ec3e5c6b96e2b58cc \\\n     --hash=sha256:9a6e8df162585750f61983a638104a48c756aa13f9f48e19ab079b38e3c828b8\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-curand-cu12==10.3.6.82 \\\n     --hash=sha256:0631ba65231260ad832ce233ddda57e7b3b7158eabf000d78e46cbb5bd5b7aae \\\n     --hash=sha256:2823fb27de4e44dbb22394a6adf53aa6e1b013aca0f8c22867d1cfae58405536 \\\n     --hash=sha256:36aabeb5990297bbce3df324ea7c7c13c3aabb140c86d50ab3b23e4ec61672f1\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cusolver-cu12==11.6.3.83 \\\n     --hash=sha256:1b8b77d2fe8abe72bb722dafb708cceaeb81f1a03999477f20b33b34f46ab885 \\\n     --hash=sha256:6224732963cba312a84c78114b9a38c4ffabb2e2a6a120923ac99ba6f895c8cf \\\n     --hash=sha256:93cfafacde4428b71778eeb092ec615a02a3d05404da1bcf91c53e3fa1bce42b\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cusparse-cu12==12.5.1.3 \\\n     --hash=sha256:016df8e993c437e8301e62739f01775cba988fd5253cd4c64173f8e8d2f8e752 \\\n     --hash=sha256:33520db374e2f5ebc976d6faa1852b98c398a57e6f71150fe59705928596ffd1 \\\n     --hash=sha256:7b97fd01f0a61628af99d0efd52132fccc8c18fc5c509f13802dccf0574a19c2\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cusolver-cu12\n nvidia-nccl-cu12==2.27.7 \\\n     --hash=sha256:4617839f3bb730c3845bf9adf92dbe0e009bc53ca5022ed941f2e23fb76e6f17 \\\n     --hash=sha256:de5ba5562f08029a19cb1cd659404b18411ed0d6c90ac5f52f30bf99ad5809aa\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-nvjitlink-cu12==12.5.82 \\\n     --hash=sha256:98103729cc5226e13ca319a10bbf9433bbbd44ef64fe72f45f067cacc14b8d27 \\\n     --hash=sha256:e782564d705ff0bf61ac3e1bf730166da66dd2fe9012f111ede5fc49b64ae697 \\\n     --hash=sha256:f9b37bc5c8cf7509665cb6ada5aaa0ce65618f2332b7d3e78e9790511f111212\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cufft-cu12\n     #   nvidia-cusolver-cu12\n     #   nvidia-cusparse-cu12\n nvidia-nvshmem-cu12==3.2.5 \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549"
        },
        {
            "sha": "6e821e61b1fd412f10a967d1b9e6cbb1fab1372f",
            "filename": "requirements_lock_3_12.txt",
            "status": "modified",
            "additions": 53,
            "deletions": 12,
            "changes": 65,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/requirements_lock_3_12.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/requirements_lock_3_12.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/requirements_lock_3_12.txt?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -426,68 +426,109 @@ nvidia-cublas-cu12==12.5.3.2 \\\n     --hash=sha256:7d0191251180de606023d396b94d66f66470a0ae96d1dbb906c7656ea0f71eda \\\n     --hash=sha256:ca070ad70e9fa6654084575d01bd001f30cc4665e33d4bb9fc8e0f321caa034b\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cudnn-cu12\n     #   nvidia-cusolver-cu12\n nvidia-cuda-cupti-cu12==12.5.82 \\\n     --hash=sha256:4f835281cf492e2bedd153f5c3de9da8f1d775a419468305e64ce73b3b0c6dc3 \\\n     --hash=sha256:bde77a5feb66752ec61db2adfe47f56b941842825b4c7e2068aff27c9d107953 \\\n     --hash=sha256:d32c06490c6ba35c4323730820c7d0c4c126c04ed58d2f57275adb8d54b138fe\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n+nvidia-cuda-nvcc-cu12==12.5.82 \\\n+    --hash=sha256:6eaa264da57a893ae7606dd80b169d9783444af941697822cb82c8379ffc4957 \\\n+    --hash=sha256:ab02fe922cee01235b7950f045042219fe83e15aceb4cd3c1d36db30b034dec7 \\\n+    --hash=sha256:b03e545b8e8c3ce7ebcd7fc44063180ff52ff01d064ece2127ed90a04ef12cd0\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cuda-nvrtc-cu12==12.5.82 \\\n     --hash=sha256:3dbd97b0104b4bfbc3c4f8c79cd2496307c89c43c29a9f83125f1d76296ff3fd \\\n     --hash=sha256:5bb6a0eb01d4974bb7ca3d48bd3859472debb3c3057a5e7de2b08fbdf35eed7e \\\n     --hash=sha256:e5db37e990056c70953b7772dd778336ef9da0a0b5bb28f9f2a61c2e42b51d78\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cuda-runtime-cu12==12.5.82 \\\n     --hash=sha256:0fd5fbca289bceb9f0690aa9858f06187b554fdeb7e2711dfd5bb3ce58900b46 \\\n     --hash=sha256:3e79a060e126df40fd3a068f3f787eb000fa51b251ec6cd97d09579632687115 \\\n     --hash=sha256:71f015dbf9df05dd71f7480132c6ebf47a6ceb2ab53d7db8e08e4b30ebb87e14\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cudnn-cu12==9.3.0.75 \\\n     --hash=sha256:9ad9c6929ebb5295eb4a1728024666d1c88283373e265a0c5c883e6f9d5cd76d \\\n     --hash=sha256:c5cf7ff3415e446adf195a5b7dd2ba56cd00c3ee78bfdc566e51698931aa4b7f \\\n     --hash=sha256:c819e82eed8cf564b9d37478ea4eab9e87194bb3b7f7f8098bc1f67c9b80f1b6\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cufft-cu12==11.2.3.61 \\\n     --hash=sha256:4a8f6f0ce93c52a50ee83422a80472b5f376054a63f38532d0eab4007e7ef28b \\\n     --hash=sha256:6d45b48a5ee7599e57131129cda2c58544d9b78b95064d3ec3e5c6b96e2b58cc \\\n     --hash=sha256:9a6e8df162585750f61983a638104a48c756aa13f9f48e19ab079b38e3c828b8\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-curand-cu12==10.3.6.82 \\\n     --hash=sha256:0631ba65231260ad832ce233ddda57e7b3b7158eabf000d78e46cbb5bd5b7aae \\\n     --hash=sha256:2823fb27de4e44dbb22394a6adf53aa6e1b013aca0f8c22867d1cfae58405536 \\\n     --hash=sha256:36aabeb5990297bbce3df324ea7c7c13c3aabb140c86d50ab3b23e4ec61672f1\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cusolver-cu12==11.6.3.83 \\\n     --hash=sha256:1b8b77d2fe8abe72bb722dafb708cceaeb81f1a03999477f20b33b34f46ab885 \\\n     --hash=sha256:6224732963cba312a84c78114b9a38c4ffabb2e2a6a120923ac99ba6f895c8cf \\\n     --hash=sha256:93cfafacde4428b71778eeb092ec615a02a3d05404da1bcf91c53e3fa1bce42b\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cusparse-cu12==12.5.1.3 \\\n     --hash=sha256:016df8e993c437e8301e62739f01775cba988fd5253cd4c64173f8e8d2f8e752 \\\n     --hash=sha256:33520db374e2f5ebc976d6faa1852b98c398a57e6f71150fe59705928596ffd1 \\\n     --hash=sha256:7b97fd01f0a61628af99d0efd52132fccc8c18fc5c509f13802dccf0574a19c2\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cusolver-cu12\n nvidia-nccl-cu12==2.27.7 \\\n     --hash=sha256:4617839f3bb730c3845bf9adf92dbe0e009bc53ca5022ed941f2e23fb76e6f17 \\\n     --hash=sha256:de5ba5562f08029a19cb1cd659404b18411ed0d6c90ac5f52f30bf99ad5809aa\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-nvjitlink-cu12==12.5.82 \\\n     --hash=sha256:98103729cc5226e13ca319a10bbf9433bbbd44ef64fe72f45f067cacc14b8d27 \\\n     --hash=sha256:e782564d705ff0bf61ac3e1bf730166da66dd2fe9012f111ede5fc49b64ae697 \\\n     --hash=sha256:f9b37bc5c8cf7509665cb6ada5aaa0ce65618f2332b7d3e78e9790511f111212\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cufft-cu12\n     #   nvidia-cusolver-cu12\n     #   nvidia-cusparse-cu12\n nvidia-nvshmem-cu12==3.2.5 \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549"
        },
        {
            "sha": "4974d7477a6c58363da7c76acb28a1574aa170e6",
            "filename": "requirements_lock_3_13.txt",
            "status": "modified",
            "additions": 52,
            "deletions": 12,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/requirements_lock_3_13.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/requirements_lock_3_13.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/requirements_lock_3_13.txt?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -426,69 +426,109 @@ nvidia-cublas-cu12==12.5.3.2 \\\n     --hash=sha256:7d0191251180de606023d396b94d66f66470a0ae96d1dbb906c7656ea0f71eda \\\n     --hash=sha256:ca070ad70e9fa6654084575d01bd001f30cc4665e33d4bb9fc8e0f321caa034b\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cudnn-cu12\n     #   nvidia-cusolver-cu12\n nvidia-cuda-cupti-cu12==12.5.82 \\\n     --hash=sha256:4f835281cf492e2bedd153f5c3de9da8f1d775a419468305e64ce73b3b0c6dc3 \\\n     --hash=sha256:bde77a5feb66752ec61db2adfe47f56b941842825b4c7e2068aff27c9d107953 \\\n     --hash=sha256:d32c06490c6ba35c4323730820c7d0c4c126c04ed58d2f57275adb8d54b138fe\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n+nvidia-cuda-nvcc-cu12==12.5.82 \\\n+    --hash=sha256:6eaa264da57a893ae7606dd80b169d9783444af941697822cb82c8379ffc4957 \\\n+    --hash=sha256:ab02fe922cee01235b7950f045042219fe83e15aceb4cd3c1d36db30b034dec7 \\\n+    --hash=sha256:b03e545b8e8c3ce7ebcd7fc44063180ff52ff01d064ece2127ed90a04ef12cd0\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cuda-nvrtc-cu12==12.5.82 \\\n     --hash=sha256:3dbd97b0104b4bfbc3c4f8c79cd2496307c89c43c29a9f83125f1d76296ff3fd \\\n     --hash=sha256:5bb6a0eb01d4974bb7ca3d48bd3859472debb3c3057a5e7de2b08fbdf35eed7e \\\n     --hash=sha256:e5db37e990056c70953b7772dd778336ef9da0a0b5bb28f9f2a61c2e42b51d78\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cuda-runtime-cu12==12.5.82 \\\n     --hash=sha256:0fd5fbca289bceb9f0690aa9858f06187b554fdeb7e2711dfd5bb3ce58900b46 \\\n     --hash=sha256:3e79a060e126df40fd3a068f3f787eb000fa51b251ec6cd97d09579632687115 \\\n     --hash=sha256:71f015dbf9df05dd71f7480132c6ebf47a6ceb2ab53d7db8e08e4b30ebb87e14\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cudnn-cu12==9.3.0.75 \\\n     --hash=sha256:9ad9c6929ebb5295eb4a1728024666d1c88283373e265a0c5c883e6f9d5cd76d \\\n     --hash=sha256:c5cf7ff3415e446adf195a5b7dd2ba56cd00c3ee78bfdc566e51698931aa4b7f \\\n     --hash=sha256:c819e82eed8cf564b9d37478ea4eab9e87194bb3b7f7f8098bc1f67c9b80f1b6\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cufft-cu12==11.2.3.61 \\\n     --hash=sha256:4a8f6f0ce93c52a50ee83422a80472b5f376054a63f38532d0eab4007e7ef28b \\\n     --hash=sha256:6d45b48a5ee7599e57131129cda2c58544d9b78b95064d3ec3e5c6b96e2b58cc \\\n     --hash=sha256:9a6e8df162585750f61983a638104a48c756aa13f9f48e19ab079b38e3c828b8\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-curand-cu12==10.3.6.82 \\\n     --hash=sha256:0631ba65231260ad832ce233ddda57e7b3b7158eabf000d78e46cbb5bd5b7aae \\\n     --hash=sha256:2823fb27de4e44dbb22394a6adf53aa6e1b013aca0f8c22867d1cfae58405536 \\\n     --hash=sha256:36aabeb5990297bbce3df324ea7c7c13c3aabb140c86d50ab3b23e4ec61672f1\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cusolver-cu12==11.6.3.83 \\\n     --hash=sha256:1b8b77d2fe8abe72bb722dafb708cceaeb81f1a03999477f20b33b34f46ab885 \\\n     --hash=sha256:6224732963cba312a84c78114b9a38c4ffabb2e2a6a120923ac99ba6f895c8cf \\\n     --hash=sha256:93cfafacde4428b71778eeb092ec615a02a3d05404da1bcf91c53e3fa1bce42b\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cusparse-cu12==12.5.1.3 \\\n     --hash=sha256:016df8e993c437e8301e62739f01775cba988fd5253cd4c64173f8e8d2f8e752 \\\n     --hash=sha256:33520db374e2f5ebc976d6faa1852b98c398a57e6f71150fe59705928596ffd1 \\\n     --hash=sha256:7b97fd01f0a61628af99d0efd52132fccc8c18fc5c509f13802dccf0574a19c2\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cusolver-cu12\n nvidia-nccl-cu12==2.27.7 \\\n     --hash=sha256:4617839f3bb730c3845bf9adf92dbe0e009bc53ca5022ed941f2e23fb76e6f17 \\\n     --hash=sha256:de5ba5562f08029a19cb1cd659404b18411ed0d6c90ac5f52f30bf99ad5809aa\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-nvjitlink-cu12==12.5.82 \\\n     --hash=sha256:98103729cc5226e13ca319a10bbf9433bbbd44ef64fe72f45f067cacc14b8d27 \\\n     --hash=sha256:e782564d705ff0bf61ac3e1bf730166da66dd2fe9012f111ede5fc49b64ae697 \\\n     --hash=sha256:f9b37bc5c8cf7509665cb6ada5aaa0ce65618f2332b7d3e78e9790511f111212\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cufft-cu12\n     #   nvidia-cusolver-cu12\n     #   nvidia-cusparse-cu12\n nvidia-nvshmem-cu12==3.2.5 \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549"
        },
        {
            "sha": "44318f02618592220d7721d6a30acbb6e93ff7ae",
            "filename": "requirements_lock_3_9.txt",
            "status": "modified",
            "additions": 56,
            "deletions": 15,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/requirements_lock_3_9.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/requirements_lock_3_9.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/requirements_lock_3_9.txt?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -420,68 +420,109 @@ nvidia-cublas-cu12==12.5.3.2 \\\n     --hash=sha256:7d0191251180de606023d396b94d66f66470a0ae96d1dbb906c7656ea0f71eda \\\n     --hash=sha256:ca070ad70e9fa6654084575d01bd001f30cc4665e33d4bb9fc8e0f321caa034b\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cudnn-cu12\n     #   nvidia-cusolver-cu12\n nvidia-cuda-cupti-cu12==12.5.82 \\\n     --hash=sha256:4f835281cf492e2bedd153f5c3de9da8f1d775a419468305e64ce73b3b0c6dc3 \\\n     --hash=sha256:bde77a5feb66752ec61db2adfe47f56b941842825b4c7e2068aff27c9d107953 \\\n     --hash=sha256:d32c06490c6ba35c4323730820c7d0c4c126c04ed58d2f57275adb8d54b138fe\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n+nvidia-cuda-nvcc-cu12==12.5.82 \\\n+    --hash=sha256:6eaa264da57a893ae7606dd80b169d9783444af941697822cb82c8379ffc4957 \\\n+    --hash=sha256:ab02fe922cee01235b7950f045042219fe83e15aceb4cd3c1d36db30b034dec7 \\\n+    --hash=sha256:b03e545b8e8c3ce7ebcd7fc44063180ff52ff01d064ece2127ed90a04ef12cd0\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cuda-nvrtc-cu12==12.5.82 \\\n     --hash=sha256:3dbd97b0104b4bfbc3c4f8c79cd2496307c89c43c29a9f83125f1d76296ff3fd \\\n     --hash=sha256:5bb6a0eb01d4974bb7ca3d48bd3859472debb3c3057a5e7de2b08fbdf35eed7e \\\n     --hash=sha256:e5db37e990056c70953b7772dd778336ef9da0a0b5bb28f9f2a61c2e42b51d78\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cuda-runtime-cu12==12.5.82 \\\n     --hash=sha256:0fd5fbca289bceb9f0690aa9858f06187b554fdeb7e2711dfd5bb3ce58900b46 \\\n     --hash=sha256:3e79a060e126df40fd3a068f3f787eb000fa51b251ec6cd97d09579632687115 \\\n     --hash=sha256:71f015dbf9df05dd71f7480132c6ebf47a6ceb2ab53d7db8e08e4b30ebb87e14\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cudnn-cu12==9.3.0.75 \\\n     --hash=sha256:9ad9c6929ebb5295eb4a1728024666d1c88283373e265a0c5c883e6f9d5cd76d \\\n     --hash=sha256:c5cf7ff3415e446adf195a5b7dd2ba56cd00c3ee78bfdc566e51698931aa4b7f \\\n     --hash=sha256:c819e82eed8cf564b9d37478ea4eab9e87194bb3b7f7f8098bc1f67c9b80f1b6\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cufft-cu12==11.2.3.61 \\\n     --hash=sha256:4a8f6f0ce93c52a50ee83422a80472b5f376054a63f38532d0eab4007e7ef28b \\\n     --hash=sha256:6d45b48a5ee7599e57131129cda2c58544d9b78b95064d3ec3e5c6b96e2b58cc \\\n     --hash=sha256:9a6e8df162585750f61983a638104a48c756aa13f9f48e19ab079b38e3c828b8\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-curand-cu12==10.3.6.82 \\\n     --hash=sha256:0631ba65231260ad832ce233ddda57e7b3b7158eabf000d78e46cbb5bd5b7aae \\\n     --hash=sha256:2823fb27de4e44dbb22394a6adf53aa6e1b013aca0f8c22867d1cfae58405536 \\\n     --hash=sha256:36aabeb5990297bbce3df324ea7c7c13c3aabb140c86d50ab3b23e4ec61672f1\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cusolver-cu12==11.6.3.83 \\\n     --hash=sha256:1b8b77d2fe8abe72bb722dafb708cceaeb81f1a03999477f20b33b34f46ab885 \\\n     --hash=sha256:6224732963cba312a84c78114b9a38c4ffabb2e2a6a120923ac99ba6f895c8cf \\\n     --hash=sha256:93cfafacde4428b71778eeb092ec615a02a3d05404da1bcf91c53e3fa1bce42b\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-cusparse-cu12==12.5.1.3 \\\n     --hash=sha256:016df8e993c437e8301e62739f01775cba988fd5253cd4c64173f8e8d2f8e752 \\\n     --hash=sha256:33520db374e2f5ebc976d6faa1852b98c398a57e6f71150fe59705928596ffd1 \\\n     --hash=sha256:7b97fd01f0a61628af99d0efd52132fccc8c18fc5c509f13802dccf0574a19c2\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cusolver-cu12\n nvidia-nccl-cu12==2.27.7 \\\n     --hash=sha256:4617839f3bb730c3845bf9adf92dbe0e009bc53ca5022ed941f2e23fb76e6f17 \\\n     --hash=sha256:de5ba5562f08029a19cb1cd659404b18411ed0d6c90ac5f52f30bf99ad5809aa\n-    # via -r ci/official/requirements_updater/requirements.in\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n nvidia-nvjitlink-cu12==12.5.82 \\\n     --hash=sha256:98103729cc5226e13ca319a10bbf9433bbbd44ef64fe72f45f067cacc14b8d27 \\\n     --hash=sha256:e782564d705ff0bf61ac3e1bf730166da66dd2fe9012f111ede5fc49b64ae697 \\\n     --hash=sha256:f9b37bc5c8cf7509665cb6ada5aaa0ce65618f2332b7d3e78e9790511f111212\n     # via\n-    #   -r ci/official/requirements_updater/requirements.in\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n     #   nvidia-cufft-cu12\n     #   nvidia-cusolver-cu12\n     #   nvidia-cusparse-cu12\n-nvidia-nvshmem-cu12==3.3.9 \\\n-    --hash=sha256:2de43cbfe559e16b8e3cb777b95f1fe2ddd5c2cfd79414b09cf9cf099feba2ba \\\n-    --hash=sha256:95ba1e98189c056eb5372bd355ab714b3741a03e6de1e32f167f5240fd967c5f\n-    # via -r ci/official/requirements_updater/requirements.in\n+nvidia-nvshmem-cu12==3.2.5 \\\n+    --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n+    --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n+    # via\n+    #   -c ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-constraints.txt\n+    #   -r ci/official/requirements_updater/nvidia-requirements.txt\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549"
        },
        {
            "sha": "f1a136d549d39b925649764d94ffe7a18643f0b0",
            "filename": "tensorflow/BUILD",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1084,20 +1084,22 @@ bzl_library(\n     visibility = [\"//visibility:public\"],\n     deps = [\n         \":tf_version_bzl\",\n+        # copybara:uncomment \"//devtools/build_cleaner/skylark:build_defs_lib\",\n+        \"@rules_cc//cc/common\",\n+        \"@rules_java//java:rules\",\n+        \"@local_xla//xla/tsl:tsl_bzl\",\n+        \"@local_xla//xla/tsl:tsl_default_bzl\",\n+        \"@local_xla//xla/tsl/mkl:build_defs_bzl\",\n         \"//tensorflow/core/platform:build_config_root_bzl\",\n         \"//tensorflow/core/platform:rules_cc_bzl\",\n+        \"@local_xla//third_party/py/rules_pywrap:pywrap_bzl\",\n+        \"@local_xla//third_party/compute_library:build_defs_bzl\",\n+        \"@local_xla//third_party/llvm_openmp:openmp_bzl\",\n         \"@bazel_skylib//lib:new_sets\",\n         \"@bazel_skylib//rules:common_settings\",\n         \"@local_config_cuda//cuda:build_defs_bzl\",\n         \"@local_config_rocm//rocm:build_defs_bzl\",\n         \"@local_config_tensorrt//:build_defs_bzl\",\n-        \"@local_xla//third_party/compute_library:build_defs_bzl\",\n-        \"@local_xla//third_party/llvm_openmp:openmp_bzl\",\n-        \"@local_xla//third_party/py/rules_pywrap:pywrap_bzl\",\n-        \"@local_xla//xla/tsl:tsl_bzl\",\n-        \"@local_xla//xla/tsl:tsl_default_bzl\",\n-        \"@local_xla//xla/tsl/mkl:build_defs_bzl\",\n-        \"@rules_java//java:rules\",\n     ],\n )\n "
        },
        {
            "sha": "6073899502250b36cbb6422b58b435df91647749",
            "filename": "tensorflow/compiler/mlir/lite/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Flite%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Flite%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1886,6 +1886,7 @@ tf_cc_binary(\n     srcs = [\n         \":tf_tfl_translate_main\",\n     ],\n+    # visibility = [\"//visibility:public\"], # copybara:uncomment\n     deps = [\n         \":common\",\n         \":converter_flags_proto_cc\","
        },
        {
            "sha": "342acb49aabe0ee3f97cc5974f06a69b4b2fddc0",
            "filename": "tensorflow/compiler/mlir/lite/ir/tfl_ops.cc",
            "status": "modified",
            "additions": 78,
            "deletions": 47,
            "changes": 125,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fir%2Ftfl_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fir%2Ftfl_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fir%2Ftfl_ops.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -251,7 +251,7 @@ bool ShouldFoldOperation(Operation* inst) {\n   int64_t operands_size = get_size(inst->getOperandTypes());\n \n   constexpr int kSizeFactor = 2;\n-  constexpr int64_t kResultsSizeThreshold = (1 << 16);  // 64 Kib =   8 KiB\n+  constexpr int64_t kResultsSizeThreshold = (1 << 19);                // 64 KiB\n   constexpr int64_t kOperandsSizeThreshold = 200L * 1024 * 1024 * 8;  // 200 MiB\n \n   return (operands_size <= kOperandsSizeThreshold) &&\n@@ -989,6 +989,26 @@ int64_t AddOp::GetArithmeticCount(Operation* op) {\n   return -1;\n }\n \n+//===----------------------------------------------------------------------===//\n+// CeilOp\n+//===----------------------------------------------------------------------===//\n+\n+OpFoldResult CeilOp::fold(FoldAdaptor adaptor) {\n+  if (!ShouldFoldOperation(this->getOperation())) return {};\n+\n+  auto operands = adaptor.getOperands();\n+  auto result_type = getType();\n+  if (!IsF32ShapedType(result_type)) return {};\n+\n+  auto compute = [](APFloat value) -> APFloat {\n+    float f = value.convertToFloat();\n+    float result = std::ceil(f);\n+    return APFloat(result);\n+  };\n+\n+  return ConstFoldUnaryOp(result_type, operands[0], compute);\n+}\n+\n //===----------------------------------------------------------------------===//\n // FloorOp\n //===----------------------------------------------------------------------===//\n@@ -1703,13 +1723,22 @@ LogicalResult FullyConnectedOp::fold(FoldAdaptor adaptor,\n       !(!getBias() || mlir::isa<NoneType>(getBias().getType()));\n \n   // Get the tensors.\n-  DenseElementsAttr input_tensor, weights_tensor, bias_tensor;\n-  if (!matchPattern(getInput(), m_Constant(&input_tensor)) ||\n-      !matchPattern(getFilter(), m_Constant(&weights_tensor)) ||\n-      (has_bias && !matchPattern(getBias(), m_Constant(&bias_tensor)))) {\n+  auto operands = adaptor.getOperands();\n+  DenseElementsAttr input_tensor =\n+      dyn_cast_or_null<DenseElementsAttr>(operands[0]);\n+  DenseElementsAttr weights_tensor =\n+      dyn_cast_or_null<DenseElementsAttr>(operands[1]);\n+  DenseElementsAttr bias_tensor;\n+\n+  if (!input_tensor || !weights_tensor) {\n     return failure();\n   }\n \n+  if (has_bias) {\n+    bias_tensor = dyn_cast_or_null<DenseElementsAttr>(operands[2]);\n+    if (!bias_tensor) return failure();\n+  }\n+\n   // Get the tensor types.\n   const auto input_type = mlir::cast<ShapedType>(input_tensor.getType());\n   const auto weights_type = mlir::cast<ShapedType>(weights_tensor.getType());\n@@ -1732,58 +1761,60 @@ LogicalResult FullyConnectedOp::fold(FoldAdaptor adaptor,\n     return failure();\n   }\n \n-  auto is_foldable = [](llvm::ArrayRef<int64_t> shape) {\n-    return shape.size() == 1 || (shape.size() == 2 && shape.front() == 1);\n-  };\n+  if (weights_type.getRank() != 2) {\n+    return failure();\n+  }\n \n-  const bool weights_foldable = weights_type.getShape().size() == 2;\n-  const bool bias_foldable = !has_bias || is_foldable(bias_type.getShape());\n+  const int64_t in_dim = weights_type.getDimSize(1);\n+  const int64_t out_dim = weights_type.getDimSize(0);\n \n-  // Folding only implemented for 1D input, 2D weights and 1D bias\n-  if (!is_foldable(input_type.getShape()) || !bias_foldable ||\n-      !weights_foldable) {\n-    return failure();\n+  if (has_bias) {\n+    if (bias_type.getRank() > 2 ||\n+        (bias_type.getRank() == 2 && bias_type.getDimSize(0) != 1) ||\n+        bias_type.getNumElements() != out_dim) {\n+      return failure();\n+    }\n   }\n \n-  // Get the sizes\n-  const auto input_size = input_type.getNumElements();\n-  const auto output_size = output_type.getNumElements();\n+  const int64_t batch_size = input_type.getNumElements() / in_dim;\n+\n+  if (output_type.getNumElements() != batch_size * out_dim) {\n+    return failure();\n+  }\n \n-  // Get iterators to the tensors.\n-  const auto input_values_it = input_tensor.getValues<float>().begin();\n-  const auto weights_values_ptr = weights_tensor.getValues<float>().begin();\n-  auto weights_row_it = weights_values_ptr;\n-  // The 'else' case could be nullptr, but the types don't match.\n-  auto bias_values_it =\n-      has_bias ? bias_tensor.getValues<float>().begin() : input_values_it;\n+  auto input_values_range = input_tensor.getValues<float>();\n+  auto weights_values_range = weights_tensor.getValues<float>();\n+  std::optional<decltype(input_values_range)> bias_values_range;\n+  if (has_bias) bias_values_range = bias_tensor.getValues<float>();\n \n   // Do the actual folding, one output at a time.\n   std::vector<float> result_values;\n-  result_values.reserve(output_size);\n-\n-  for (int i = 0; i < output_size; ++i) {\n-    // Dot product with Kahan/Neumaier summation to minimize numeric errors.\n-    float sum = has_bias ? *bias_values_it : 0.0f;\n-    float compensation = 0.0f;\n-    for (int j = 0; j < input_size; ++j) {\n-      const float addend = input_values_it[j] * weights_row_it[j];\n-      const float new_sum = sum + addend;\n-      // DO NOT enable -funsafe-math-optimizations here.\n-      // There is a test detecting unsafe optimizations.\n-      // Unsafe math optimizations can reorder float formulas, and set the\n-      // compensation to constant 0. The formula must be evaluated as written\n-      // for the algorithm to work.\n-      // (Note: -ffast-math is a superset of -funsafe-math-optimizations.)\n-      if (std::abs(sum) >= std::abs(addend)) {\n-        compensation += (sum - new_sum) + addend;\n-      } else {\n-        compensation += (addend - new_sum) + sum;\n+  result_values.reserve(batch_size * out_dim);\n+\n+  for (int b = 0; b < batch_size; ++b) {\n+    for (int o = 0; o < out_dim; ++o) {\n+      // Dot product with Kahan/Neumaier summation to minimize numeric errors.\n+      float sum = has_bias ? (*bias_values_range)[o] : 0.0f;\n+      float compensation = 0.0f;\n+      for (int i = 0; i < in_dim; ++i) {\n+        const float addend = input_values_range[b * in_dim + i] *\n+                             weights_values_range[o * in_dim + i];\n+        const float new_sum = sum + addend;\n+        // DO NOT enable -funsafe-math-optimizations here.\n+        // There is a test detecting unsafe optimizations.\n+        // Unsafe math optimizations can reorder float formulas, and set the\n+        // compensation to constant 0. The formula must be evaluated as written\n+        // for the algorithm to work.\n+        // (Note: -ffast-math is a superset of -funsafe-math-optimizations.)\n+        if (std::abs(sum) >= std::abs(addend)) {\n+          compensation += (sum - new_sum) + addend;\n+        } else {\n+          compensation += (addend - new_sum) + sum;\n+        }\n+        sum = new_sum;\n       }\n-      sum = new_sum;\n+      result_values.push_back(sum + compensation);\n     }\n-    result_values.push_back(sum + compensation);\n-    weights_row_it += input_size;\n-    bias_values_it++;\n   }\n \n   // Set result tensor"
        },
        {
            "sha": "c842d0db9edb46a80fc1ee19ee1aa01870b290a4",
            "filename": "tensorflow/compiler/mlir/lite/ir/tfl_ops.td",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fir%2Ftfl_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fir%2Ftfl_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fir%2Ftfl_ops.td?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -828,6 +828,8 @@ def TFL_CeilOp: TFL_Op<\"ceil\", [\n \n   let results = (outs TFL_FpTensor:$y);\n \n+  let hasFolder = 1;\n+\n   let extraClassDeclaration = [{\n     // Returns whether the return types are compatible.\n     static bool isCompatibleReturnTypes(TypeRange l, TypeRange r) {\n@@ -1618,7 +1620,7 @@ def TFL_DivOp : TFL_Op<\"div\", [\n     // TODO(fengliuai): NoQuantizableResult is only correct for int8\n     // quantization. update to handle Uint8 quantization.\n     BinaryOpSameElementTypeConstraint,\n-    TFL_OperandsHaveSameShapesOrBroadcastableShape<[0, 1], 5>,\n+    TFL_OperandsHaveSameShapesOrBroadcastableShape<[0, 1], 6>,\n     ResultsBroadcastableShape,\n     Pure,\n     QuantizableResult,"
        },
        {
            "sha": "39cf68d22e743e1f646583a4e91250f9a2220599",
            "filename": "tensorflow/compiler/mlir/lite/stablehlo/transforms/composite_lowering_patterns.td",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Fcomposite_lowering_patterns.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Fcomposite_lowering_patterns.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fstablehlo%2Ftransforms%2Fcomposite_lowering_patterns.td?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -244,3 +244,30 @@ def LegalizeCompositePack4Elements1 : Pat<\n         (TFL_PackOp (variadic $i0, $i1, $i2, $i3),\n           (GetCompositeAttributeAs<\"values_count\", \"IntegerAttr\"> $attrs),\n           (GetCompositeAttributeAs<\"axis\", \"IntegerAttr\"> $attrs))>;\n+\n+def LegalizeDynamicSlice : Pat<\n+        (MHLO_CompositeOp:$composite\n+          (variadic $input, $begin, $size),\n+          ConstantStrAttr<StrAttr, \"tfl.slice\">, $attrs, $_, $_),\n+        (TFL_SliceOp $input, $begin, $size)>;\n+\n+def LegalizeDynamicSlice_1 : Pat<\n+        (MHLO_CompositeOp:$composite\n+          (variadic $_, $input, $begin, $size),\n+          ConstantStrAttr<StrAttr, \"tfl.slice\">, $attrs, $_, $_),\n+        (TFL_SliceOp $input, $begin, $size)>;\n+\n+def LegalizeBMM4dAdjY : Pat<\n+        (MHLO_CompositeOp:$composite\n+          (variadic $lhs, $rhs),\n+          ConstantStrAttr<StrAttr, \"tfl.bmm_4d_adj_y\">, $attrs, $_, $_),\n+        (TFL_BatchMatMulOp $lhs, $rhs, ConstBoolAttrFalse, ConstBoolAttrTrue,\n+                     ConstBoolAttrFalse)>;\n+\n+// For dynamic shaped lhs and rhs.\n+def LegalizeBMM4dAdjY_1 : Pat<\n+        (MHLO_CompositeOp:$composite\n+          (variadic $_, $lhs, $rhs),\n+          ConstantStrAttr<StrAttr, \"tfl.bmm_4d_adj_y\">, $attrs, $_, $_),\n+        (TFL_BatchMatMulOp $lhs, $rhs, ConstBoolAttrFalse, ConstBoolAttrTrue,\n+                     ConstBoolAttrFalse)>;"
        },
        {
            "sha": "6043e26cb757d8f55293081d09132e14c220695a",
            "filename": "tensorflow/compiler/mlir/lite/tests/const-fold.mlir",
            "status": "modified",
            "additions": 35,
            "deletions": 10,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Fconst-fold.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Fconst-fold.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Fconst-fold.mlir?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1186,20 +1186,18 @@ func.func @NoFoldFullyConnectedNonFloat() -> tensor<1024xf32> {\n   // CHECK: return %[[VAL]] : tensor<1024xf32>\n }\n \n-// CHECK-LABEL: @NoFoldFullyConnectedHighRank\n-func.func @NoFoldFullyConnectedHighRank() -> tensor<2x1024xf32> {\n+// CHECK-LABEL: @ConstantFoldFullyConnectedHighRank\n+func.func @ConstantFoldFullyConnectedHighRank() -> tensor<2x1024xf32> {\n   %cst_input = arith.constant dense<1.0> : tensor<2x512xf32>\n   %cst_weights = arith.constant dense<2.0> : tensor<1024x512xf32>\n   %cst_bias = arith.constant dense<4.0> : tensor<1024xf32>\n \n   %0 = \"tfl.fully_connected\" (%cst_input, %cst_weights, %cst_bias) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<2x512xf32>, tensor<1024x512xf32>, tensor<1024xf32>) -> tensor<2x1024xf32>\n \n   func.return %0 : tensor<2x1024xf32>\n-  // CHECK-DAG: %[[CST:.*]] = arith.constant dense<1.000000e+00> : tensor<2x512xf32>\n-  // CHECK-DAG: %[[CST_0:.*]] = arith.constant dense<2.000000e+00> : tensor<1024x512xf32>\n-  // CHECK-DAG: %[[CST_1:.*]] = arith.constant dense<4.000000e+00> : tensor<1024xf32>\n-  // CHECK: %[[VAL:.*]] = \"tfl.fully_connected\"(%[[CST]], %[[CST_0]], %[[CST_1]]) <{fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"}> : (tensor<2x512xf32>, tensor<1024x512xf32>, tensor<1024xf32>) -> tensor<2x1024xf32>\n-  // CHECK: return %[[VAL]] : tensor<2x1024xf32>\n+  // 1.0 * 2.0 * 512 + 4.0 = 1028.0\n+  // CHECK: %[[CST:.*]] = arith.constant dense<1.028000e+03> : tensor<2x1024xf32>\n+  // CHECK:  return %[[CST]]\n }\n \n // CHECK-LABEL: @ConstantFoldFullyConnectedCheckPrecision\n@@ -1227,6 +1225,20 @@ func.func @fully_connected_with_unit_dim() -> tensor<1x5xf32> {\n // CHECK:     %cst = arith.constant dense<6.000000e+00> : tensor<1x5xf32>\n // CHECK-NOT: fully_connected\n \n+// CHECK-LABEL: @ConstantFoldFullyConnectedBatched\n+func.func @ConstantFoldFullyConnectedBatched() -> tensor<13x1536xf32> {\n+  %cst_input = arith.constant dense<1.0> : tensor<13x1536xf32>\n+  %cst_weights = arith.constant dense<1.0> : tensor<1536x1536xf32>\n+  %cst_bias = \"tfl.no_value\"() {value = unit} : () -> none\n+\n+  %0 = \"tfl.fully_connected\" (%cst_input, %cst_weights, %cst_bias) {fused_activation_function = \"NONE\", keep_num_dims = true, weights_format = \"DEFAULT\"} : (tensor<13x1536xf32>, tensor<1536x1536xf32>, none) -> tensor<13x1536xf32>\n+  func.return %0 : tensor<13x1536xf32>\n+\n+  // 1.0 * 1.0 * 1536 = 1536.0\n+  // CHECK: %[[CST:.*]] = arith.constant dense<1.536000e+03> : tensor<13x1536xf32>\n+  // CHECK:  return %[[CST]]\n+}\n+\n // CHECK-LABEL: @ShapeOpI32\n func.func @ShapeOpI32(%arg0 : tensor<576x72xf32>) -> tensor<2xi32> {\n   %0 = \"tfl.shape\"(%arg0) : (tensor<576x72xf32>) -> tensor<2xi32>\n@@ -1602,16 +1614,30 @@ func.func @select_float() -> tensor<4xf32> {\n \n   func.return %2 : tensor<4xf32>\n }\n-\n // CHECK: %cst = arith.constant dense<[1.000000e+00, 2.000000e+00, -3.000000e+00, -4.000000e+00]> : tensor<4xf32\n \n+// CHECK-LABEL: ceil\n+func.func @ceil() -> tensor<3xf32> {\n+  %cst = arith.constant dense<[-1.0, 0.0, 0.99]> : tensor<3xf32>\n+  %0 = \"tfl.ceil\"(%cst) : (tensor<3xf32>) -> tensor<3xf32>\n+  func.return %0 : tensor<3xf32>\n+}\n+// CHECK: %cst = arith.constant dense<[-1.000000e+00, 0.000000e+00, 1.000000e+00]> : tensor<3xf32>\n+\n+// CHECK-LABEL: ceil_f64\n+func.func @ceil_f64() -> tensor<3xf64> {\n+  %cst = arith.constant dense<[-1.0, 0.0, 0.99]> : tensor<3xf64>\n+  %0 = \"tfl.ceil\"(%cst) : (tensor<3xf64>) -> tensor<3xf64>\n+  func.return %0 : tensor<3xf64>\n+}\n+// CHECK: tfl.ceil\n+\n // CHECK-LABEL: floor\n func.func @floor() -> tensor<3xf32> {\n   %cst = arith.constant dense<[-1.0, 0.0, 0.99]> : tensor<3xf32>\n   %0 = \"tfl.floor\"(%cst) : (tensor<3xf32>) -> tensor<3xf32>\n   func.return %0 : tensor<3xf32>\n }\n-\n // CHECK: %cst = arith.constant dense<[-1.000000e+00, 0.000000e+00, 0.000000e+00]> : tensor<3xf32>\n \n // CHECK-LABEL: floor_f64\n@@ -1620,7 +1646,6 @@ func.func @floor_f64() -> tensor<3xf64> {\n   %0 = \"tfl.floor\"(%cst) : (tensor<3xf64>) -> tensor<3xf64>\n   func.return %0 : tensor<3xf64>\n }\n-\n // CHECK: tfl.floor\n \n // CHECK-LABEL: exp"
        },
        {
            "sha": "31081e65fea0a722eb03a0a1bc0bf10613dd3840",
            "filename": "tensorflow/compiler/mlir/lite/tests/legalize-tf.mlir",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Flegalize-tf.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Flegalize-tf.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Flegalize-tf.mlir?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1996,14 +1996,21 @@ func.func @mul_with_int32_7d_inputs(%arg0: tensor<1x1x1x1x1x3x1xi32>, %arg1 : te\n \n // CHECK-LABEL: testDivWithBroadcastToOps\n func.func @testDivWithBroadcastToOps(%arg0: tensor<1x2x1x4x5x6xi32>, %arg1: tensor<1x2x3x4x5x1xi32>) -> tensor<1x2x3x4x5x6xi32> {\n-  // CHECK: [[CST:%.*]] = arith.constant dense<[1, 2, 3, 4, 5, 6]> : tensor<6xi64>\n-  // CHECK: [[BCAST:%.*]] = \"tfl.broadcast_to\"(%arg0, [[CST]])\n-  // CHECK: [[BCAST_1:%.*]] = \"tfl.broadcast_to\"(%arg1, [[CST]])\n-  // CHECK: tfl.div [[BCAST]], [[BCAST_1]] {fused_activation_function = \"NONE\"} : tensor<1x2x3x4x5x6xi32>\n+  // CHECK: tfl.div(%arg0, %arg1) <{fused_activation_function = \"NONE\"}> : (tensor<1x2x1x4x5x6xi32>, tensor<1x2x3x4x5x1xi32>) -> tensor<1x2x3x4x5x6xi32>\n   %0 = \"tf.Div\"(%arg0, %arg1) : (tensor<1x2x1x4x5x6xi32>, tensor<1x2x3x4x5x1xi32>) -> tensor<1x2x3x4x5x6xi32>\n   func.return %0 : tensor<1x2x3x4x5x6xi32>\n }\n \n+// CHECK-LABEL: testDivWithBroadcastToOps7D\n+func.func @testDivWithBroadcastToOps7D(%arg0: tensor<1x2x1x4x5x6x7xi32>, %arg1: tensor<1x2x3x4x5x1x7xi32>) -> tensor<1x2x3x4x5x6x7xi32> {\n+  // CHECK: [[CST:%.*]] = arith.constant dense<[1, 2, 3, 4, 5, 6, 7]> : tensor<7xi64>\n+  // CHECK: [[BCAST:%.*]] = \"tfl.broadcast_to\"(%arg0, [[CST]])\n+  // CHECK: [[BCAST_1:%.*]] = \"tfl.broadcast_to\"(%arg1, [[CST]])\n+  // CHECK: tfl.div [[BCAST]], [[BCAST_1]] {fused_activation_function = \"NONE\"} : tensor<1x2x3x4x5x6x7xi32>\n+  %0 = \"tf.Div\"(%arg0, %arg1) : (tensor<1x2x1x4x5x6x7xi32>, tensor<1x2x3x4x5x1x7xi32>) -> tensor<1x2x3x4x5x6x7xi32>\n+  func.return %0 : tensor<1x2x3x4x5x6x7xi32>\n+}\n+\n // CHECK-LABEL: testFloorDivWithBroadcastToOps\n func.func @testFloorDivWithBroadcastToOps(%arg0: tensor<1x2x1x4x5x6xi32>, %arg1: tensor<1x2x3x4x5x1xi32>) -> tensor<1x2x3x4x5x6xi32> {\n   // CHECK: [[CST:%.*]] = arith.constant dense<[1, 2, 3, 4, 5, 6]> : tensor<6xi64>"
        },
        {
            "sha": "7b5370c90242b46e2855077d607a6b45c567f9e6",
            "filename": "tensorflow/compiler/mlir/lite/tests/optimize.mlir",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Foptimize.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Foptimize.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftests%2Foptimize.mlir?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -677,6 +677,21 @@ func.func @FuseFullyConnectedAddWithNoBias(%arg0: tensor<40x37xf32>, %arg1: tens\n   // CHECK: return %[[fc]]\n }\n \n+// CHECK-LABEL: @FuseFullyConnectedAddWithNoBias\n+func.func @FuseFullyConnectedAddWithNoBiasAndDifferentInputFilterType(%arg0: tensor<40x37xbf16>, %arg1: tensor<40x37xbf16>) -> tensor<40x40xf32> {\n+  %cst = \"tfl.no_value\"() {value} : () -> none\n+  %cst2 = arith.constant dense<2.0> : tensor<40xf32>\n+\n+  %0 = \"tfl.fully_connected\" (%arg0, %arg1, %cst) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<40x37xbf16>, tensor<40x37xbf16>, none) -> (tensor<40x40xf32>)\n+  %1 = \"tfl.add\"(%0, %cst2) {fused_activation_function = \"NONE\"} : (tensor<40x40xf32>, tensor<40xf32>) -> tensor<40x40xf32>\n+\n+  func.return %1 : tensor<40x40xf32>\n+\n+  // CHECK-DAG: %cst = arith.constant dense<2.000000e+00> : tensor<40xf32>\n+  // CHECK: %[[fc:.*]] = \"tfl.fully_connected\"(%arg0, %arg1, %cst)\n+  // CHECK: return %[[fc]]\n+}\n+\n // CHECK-LABEL: @FuseFullyConnectedAddWithNoBiasWithQDQs\n func.func @FuseFullyConnectedAddWithNoBiasWithQDQs(%arg0: tensor<40x37xf32>, %arg1: tensor<40x37xf32>) -> tensor<40x40xf32> {\n   %cst = \"tfl.no_value\"() {value} : () -> none"
        },
        {
            "sha": "6a116197e67043b388488e0716d2c2a218b3b2d4",
            "filename": "tensorflow/compiler/mlir/lite/transforms/optimize_pass.cc",
            "status": "modified",
            "additions": 29,
            "deletions": 51,
            "changes": 80,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Foptimize_pass.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1247,16 +1247,17 @@ static std::optional<Value> GetAs1DValue(PatternRewriter& rewriter, Value value,\n }\n \n // Tries to get the given `bias` as a 1D tensor of `num_channels` elements.\n-// If `bias` is a `NoneType`, a 1D tensor of zeros is created.\n+// If `bias` is a `NoneType`, a 1D tensor of zeros is created with the given\n+// `fallback_element_type`.\n // Otherwise, it uses `GetAs1DValue` to handle scalar constants and other\n // broadcastable shapes.\n static std::optional<Value> GetBiasIn1D(PatternRewriter& rewriter, Value bias,\n                                         int num_channels,\n-                                        Type filter_element_type) {\n+                                        Type fallback_element_type) {\n   // If it's none, create a zero tensor with shape {num_channels}.\n   if (mlir::isa<NoneType>(bias.getType())) {\n     RankedTensorType type =\n-        RankedTensorType::get({num_channels}, filter_element_type);\n+        RankedTensorType::get({num_channels}, fallback_element_type);\n     auto attr = rewriter.getZeroAttr(type);\n     return rewriter.create<arith::ConstantOp>(bias.getLoc(), type, attr);\n   }\n@@ -1294,38 +1295,6 @@ static RankedTensorType GetRankedTensorType(Value value) {\n   return nullptr;\n }\n \n-// Gets the number of channels and filter element type for a FullyConnected op.\n-// This is used to determine the shape of the bias tensor when fusing an Add op.\n-// It first tries to get this information from the filter tensor. If the filter\n-// is unranked, it falls back to using the output tensor of the FullyConnected\n-// op.\n-static std::optional<std::pair<int, Type>> GetFcNumChannelsAndFilterType(\n-    TFL::FullyConnectedOp fc_op) {\n-  Value filter = fc_op.getFilter();\n-  if (auto filter_type = GetRankedTensorType(filter);\n-      filter_type && filter_type.getRank() == 2 &&\n-      !mlir::isa<quant::QuantizedType>(filter_type.getElementType())) {\n-    // Get the number of channels from the filter's shape if it's a ranked\n-    // 2D tensor. Filter must be a `2D` tensor with `{num_channels,\n-    // num_features}` shape.\n-    int num_channels = filter_type.getShape()[0];\n-    Type filter_element_type = filter_type.getElementType();\n-    return {{num_channels, filter_element_type}};\n-  }\n-\n-  // Fallback to using the FC op's output shape to determine the number of\n-  // channels. This is useful when the filter is unranked.\n-  auto fc_output_type =\n-      mlir::dyn_cast<RankedTensorType>(fc_op.getOutput()[0].getType());\n-  if (!fc_output_type || !fc_output_type.hasStaticShape() ||\n-      fc_output_type.getRank() == 0) {\n-    return std::nullopt;\n-  }\n-  int num_channels = fc_output_type.getShape().back();\n-  Type filter_element_type = fc_output_type.getElementType();\n-  return {{num_channels, filter_element_type}};\n-}\n-\n // Fuse Add with proceeding FullyConnected.\n // TODO(b/136285429): Move to tablegen when variadic is supported\n struct FuseFullyConnectedAndAdd : public OpRewritePattern<TFL::AddOp> {\n@@ -1369,22 +1338,6 @@ struct FuseFullyConnectedAndAdd : public OpRewritePattern<TFL::AddOp> {\n     ElementsAttr bias_value;\n     if (fc_op.getFusedActivationFunction() != \"NONE\") return failure();\n \n-    // Get the number of channels if possible.\n-    auto fc_info = GetFcNumChannelsAndFilterType(fc_op);\n-    if (!fc_info) {\n-      return failure();\n-    }\n-    const auto& [num_channels, filter_element_type] = *fc_info;\n-\n-    auto bias_1d =\n-        GetBiasIn1D(rewriter, bias, num_channels, filter_element_type);\n-    // Get the added value as a 1D tensor.\n-    auto add_rhs_1d = GetAs1DValue(rewriter, add_rhs, num_channels);\n-\n-    if (!bias_1d.has_value() || !add_rhs_1d.has_value()) {\n-      return failure();\n-    }\n-\n     auto fc_output_type =\n         mlir::dyn_cast<RankedTensorType>(fc_op.getOutput()[0].getType());\n     auto add_output_type =\n@@ -1398,6 +1351,31 @@ struct FuseFullyConnectedAndAdd : public OpRewritePattern<TFL::AddOp> {\n       return failure();\n     }\n \n+    // Get the number of output channels.\n+    if (fc_output_type.getShape().size() == 0) {\n+      return failure();\n+    }\n+    const int64_t num_channels = fc_output_type.getShape().back();\n+    if (::mlir::ShapedType::isDynamic(num_channels)) {\n+      return failure();\n+    }\n+\n+    auto bias_1d = GetBiasIn1D(rewriter, bias, num_channels,\n+                               add_output_type.getElementType());\n+    // Get the added value as a 1D tensor.\n+    auto add_rhs_1d = GetAs1DValue(rewriter, add_rhs, num_channels);\n+\n+    if (!bias_1d.has_value() || !add_rhs_1d.has_value()) {\n+      return failure();\n+    }\n+    // Sanity check that bias and add_rhs can be broadcasted together (shapes\n+    // should be broadcastable and element types must match).\n+    if (!IsBroadcastableElementsAttrAndType(bias_1d->getType(),\n+                                            add_rhs_1d->getType())) {\n+      return rewriter.notifyMatchFailure(\n+          add_op, \"Bias and add_rhs are not broadcastable\");\n+    }\n+\n     auto new_bias =\n         rewriter\n             .create<AddOp>(add_op.getLoc(), bias_1d.value(), add_rhs_1d.value(),"
        },
        {
            "sha": "8d2015bd8615324669472c07c4e60b78a6064470",
            "filename": "tensorflow/compiler/mlir/quantization/stablehlo/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1,4 +1,5 @@\n load(\"@llvm-project//mlir:tblgen.bzl\", \"gentbl_cc_library\", \"td_library\")\n+load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n \n # Placeholder: load py_proto_library\n load(\"//tensorflow:tensorflow.bzl\", \"tf_cc_binary\", \"tf_cc_test\")"
        },
        {
            "sha": "095390c0e2c28f9a2b8d389adba5c9e253d523b0",
            "filename": "tensorflow/compiler/mlir/quantization/stablehlo/cc/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fcc%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fcc%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fcc%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1,3 +1,4 @@\n+load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n load(\"//tensorflow:tensorflow.bzl\", \"tf_cc_test\")\n load(\n     \"//tensorflow:tensorflow.default.bzl\","
        },
        {
            "sha": "4cd0353f49b638442f90bee2aa4de368090aee92",
            "filename": "tensorflow/compiler/mlir/quantization/stablehlo/cc/calibration/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fcc%2Fcalibration%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fcc%2Fcalibration%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fcc%2Fcalibration%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1,3 +1,4 @@\n+load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n load(\"//tensorflow:tensorflow.bzl\", \"tf_cc_test\")\n load(\"//tensorflow:tensorflow.default.bzl\", \"get_compatible_with_portable\")\n "
        },
        {
            "sha": "955945b848a6346e9abe6e4bb6ac5b3eb6ba8d91",
            "filename": "tensorflow/compiler/mlir/quantization/stablehlo/instrumentations/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Finstrumentations%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Finstrumentations%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Finstrumentations%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1,3 +1,4 @@\n+load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n load(\"//tensorflow:tensorflow.bzl\", \"tf_cc_test\")\n load(\"//tensorflow:tensorflow.default.bzl\", \"get_compatible_with_portable\")\n "
        },
        {
            "sha": "2465aa61e4a80af0d2b1117550238e44fd35fda0",
            "filename": "tensorflow/compiler/mlir/quantization/stablehlo/ops/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fops%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fops%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fops%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1,3 +1,4 @@\n+load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n load(\"//tensorflow:tensorflow.bzl\", \"tf_cc_test\")\n load(\"//tensorflow:tensorflow.default.bzl\", \"get_compatible_with_portable\")\n "
        },
        {
            "sha": "64ee67b90420e9991d34fc2514808489292334ca",
            "filename": "tensorflow/compiler/mlir/quantization/stablehlo/python/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpython%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpython%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Fstablehlo%2Fpython%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1,4 +1,5 @@\n load(\"@local_xla//xla/tsl/platform:build_config_root.bzl\", \"if_pywrap\")\n+load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n load(\"//tensorflow:pytype.default.bzl\", \"pytype_strict_library\")\n load(\n     \"//tensorflow:tensorflow.default.bzl\","
        },
        {
            "sha": "2b64cc07c7c30bc52ad557d8c0c808185c586bc8",
            "filename": "tensorflow/compiler/mlir/quantization/tensorflow/calibrator/calibration_statistics_saver_op_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Ftensorflow%2Fcalibrator%2Fcalibration_statistics_saver_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Ftensorflow%2Fcalibrator%2Fcalibration_statistics_saver_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Fquantization%2Ftensorflow%2Fcalibrator%2Fcalibration_statistics_saver_op_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -43,7 +43,6 @@ using ::testing::ElementsAre;\n using ::testing::HasSubstr;\n using ::testing::Key;\n using ::testing::SizeIs;\n-using ::tsl::testing::StatusIs;\n \n class CalibrationStatisticsSaverTest : public OpsTestBase {};\n "
        },
        {
            "sha": "23b6565c1c77688b8aef0538048db0d81fea6e3a",
            "filename": "tensorflow/compiler/mlir/tensorflow/c/c_api_unified_experimental_mlir.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Fc%2Fc_api_unified_experimental_mlir.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Fc%2Fc_api_unified_experimental_mlir.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Fc%2Fc_api_unified_experimental_mlir.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -269,9 +269,8 @@ class MlirFunctionContext : public TracingContext {\n     RegisterDialects(*context_);\n     // TODO(aminim) figure out the location story here\n     module_ = ModuleOp::create(builder_.getUnknownLoc());\n-    func_ = func::FuncOp::create(\n-        builder_.getUnknownLoc(), name,\n-        builder_.getFunctionType(std::nullopt, std::nullopt));\n+    func_ = func::FuncOp::create(builder_.getUnknownLoc(), name,\n+                                 builder_.getFunctionType({}, {}));\n     module_->push_back(func_);\n     builder_ = OpBuilder::atBlockBegin(func_.addEntryBlock());\n   }"
        },
        {
            "sha": "4d30d43c0ccdf6932ea18f785f4226d83c09f1bc",
            "filename": "tensorflow/compiler/mlir/tensorflow/ir/host_runtime/tfrt_ops.td",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Fir%2Fhost_runtime%2Ftfrt_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Fir%2Fhost_runtime%2Ftfrt_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Fir%2Fhost_runtime%2Ftfrt_ops.td?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -188,4 +188,19 @@ def TF_PwStreamResultsOp : TF_Op<\"PwStreamResults\"> {\n   let hasVerifier = 1;\n }\n \n+ def TF_IfrtResourceDeserializeOp : TF_Op<\"IfrtResourceDeserialize\", []> {\n+   let summary = \"Deserialize resource vars.\";\n+   let description = [{\n+     This Op is a variant of IfrtResourceDeserialize for use with the\n+     TFRT/IFRT runtime. It is not a stable interface.\n+   }];\n+\n+   let arguments = (ins\n+     Arg<TF_ResourceTensor, \"The variable to change.\">:$resource_var,\n+     Arg<TF_StrTensor, \"The directory to read from.\">:$input_dir,\n+     DefaultValuedOptionalAttr<BoolAttr, \"true\">:$require_matching_crc,\n+     DefaultValuedOptionalAttr<StrAttr, \"\\\"\\\"\">:$tensor_name\n+   );\n+ }\n+\n #endif // TFRT_OPS"
        },
        {
            "sha": "e23f510182259f9076f935d88395c510a09b5b2b",
            "filename": "tensorflow/compiler/mlir/tensorflow/ir/tf_ops.td",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Fir%2Ftf_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Fir%2Ftf_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Fir%2Ftf_ops.td?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -3212,12 +3212,12 @@ def TF_XlaSparseActivationsUnstackOp : TF_Op<\"XlaSparseActivationsUnstack\", [Pur\n     conversion, stacking and optionally interleaving of the embedding\n     activations, while also offloading this work to SparseCore.\n \n-    The op assumes its operand is in SparseCore layout, while also being F32.\n-    The output is a tuple of tensors in TensorCore layout, in any data type.\n+    The op assumes its operand is in SparseCore layout.\n+    The output is a tuple of tensors in TensorCore layout.\n   }];\n \n   let arguments = (ins\n-    TF_Float32Tensor:$stacked_activations,\n+    TF_Tensor:$stacked_activations,\n \n     ConfinedAttr<I64Attr, [IntMinValue<1>]>:$num_tables,\n     ConfinedAttr<I64ArrayAttr, [ArrayMinCount<1>]>:$sample_counts,\n@@ -3236,8 +3236,8 @@ def TF_XlaSparseGradientsStackOp : TF_Op<\"XlaSparseGradientsStack\", [Pure]> {\n     unstacking and optionally interleaving of the embedding gradients, while\n     also offloading this work to SparseCore.\n \n-    The op assumes its operands are in TensoreCore layout and in any data type.\n-    The output is in SparseCore layout and is expected to be F32.\n+    The op assumes its operands are in TensoreCore layout.\n+    The output is in SparseCore layout.\n   }];\n \n   let arguments = (ins\n@@ -3248,7 +3248,7 @@ def TF_XlaSparseGradientsStackOp : TF_Op<\"XlaSparseGradientsStack\", [Pure]> {\n   );\n \n   let results = (outs\n-    TF_Float32Tensor:$stacked_gradients\n+    TF_Tensor:$stacked_gradients\n   );\n }\n #endif // TF_OPS"
        },
        {
            "sha": "9ec71c36687c65b31849970c75d12e688a17fcef",
            "filename": "tensorflow/compiler/mlir/tensorflow/tests/cannonicalize_ops_outside_compilation.mlir",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftests%2Fcannonicalize_ops_outside_compilation.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftests%2Fcannonicalize_ops_outside_compilation.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftests%2Fcannonicalize_ops_outside_compilation.mlir?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -6,10 +6,7 @@\n \n // Reshape should not be executed on TPU as all are marked by outside\n // compilation. And there should be no host-device communication.\n-// CHECK: tf._TPUCompile\n-// CHECK-NOT: tf.Reshape\n-// CHECK: launch{{.*}}CPU\n-// CHECK: tf.TPUCompileSucceeded\n+// CHECK: tf._TPUCompileMlir\n // CHECK-NOT: tf.Reshape\n // CHECK-NOT: tf._XlaHostComputeMlir\n "
        },
        {
            "sha": "8c1920efd9432d2edd4a3605bd28214464fbc82b",
            "filename": "tensorflow/compiler/mlir/tensorflow/tests/ici_weight_distribution_spmd_mlir_end_to_end.mlir",
            "status": "modified",
            "additions": 19,
            "deletions": 20,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftests%2Fici_weight_distribution_spmd_mlir_end_to_end.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftests%2Fici_weight_distribution_spmd_mlir_end_to_end.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftests%2Fici_weight_distribution_spmd_mlir_end_to_end.mlir?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1,32 +1,31 @@\n // RUN: tf-opt %s -tf-replicated-clustering-bridge-v2 -tfrt-lower-cluster-to-runtime-ops-tpu -tf-dialect-to-executor-v2 | FileCheck %s\n \n // CHECK-LABEL: func.func @main\n-// CHECK: %outputs:5, %control = tf_executor.island wraps \"tf._TPUCompileMlir\"()\n-// CHECK: %outputs_0, %control_1 = tf_executor.island wraps \"tf.ReadVariableOp\"(%arg3) : (tensor<*x!tf_type.resource<tensor<128x1024xf32>>>) -> tensor<128x1024xf32>\n-// CHECK: %outputs_2, %control_3 = tf_executor.island wraps \"tf.ReadVariableOp\"(%arg4) : (tensor<*x!tf_type.resource<tensor<1024xf32>>>) -> tensor<1024xf32>\n-// CHECK: %outputs_4, %control_5 = tf_executor.island wraps \"tf.TPUDummyInput\"() <{shape = #tf_type.shape<128x1024>}> {_ici_weight_distribution_mlir_bridge_marker = true} : () -> tensor<128x1024xf32>\n-// CHECK: %outputs_6, %control_7 = tf_executor.island wraps \"tf.TPUDummyInput\"() <{shape = #tf_type.shape<1024>}> {_ici_weight_distribution_mlir_bridge_marker = true} : () -> tensor<1024xf32>\n-// CHECK: %outputs_8, %control_9 = tf_executor.island wraps \"tf.Identity\"(%outputs#0) {device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"} : (tensor<!tf_type.string>) -> tensor<!tf_type.string>\n+// CHECK: %outputs, %control = tf_executor.island wraps \"tf.ReadVariableOp\"(%arg3) : (tensor<*x!tf_type.resource<tensor<128x1024xf32>>>) -> tensor<128x1024xf32>\n+// CHECK: %outputs_0, %control_1 = tf_executor.island wraps \"tf.ReadVariableOp\"(%arg4) : (tensor<*x!tf_type.resource<tensor<1024xf32>>>) -> tensor<1024xf32>\n+// CHECK: %outputs_2, %control_3 = tf_executor.island wraps \"tf.TPUDummyInput\"() <{shape = #tf_type.shape<128x1024>}> {_ici_weight_distribution_mlir_bridge_marker = true} : () -> tensor<128x1024xf32>\n+// CHECK: %outputs_4, %control_5 = tf_executor.island wraps \"tf.TPUDummyInput\"() <{shape = #tf_type.shape<1024>}> {_ici_weight_distribution_mlir_bridge_marker = true} : () -> tensor<1024xf32>\n+// CHECK: %outputs_6:5, %control_7 = tf_executor.island wraps \"tf._TPUCompileMlir\"()\n+// CHECK: %outputs_8, %control_9 = tf_executor.island wraps \"tf.Identity\"(%outputs_6#0) {device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"} : (tensor<!tf_type.string>) -> tensor<!tf_type.string>\n // CHECK: %control_10 = tf_executor.island wraps \"tf.TPUCompileSucceededAssert\"(%outputs_8) {device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"} : (tensor<!tf_type.string>) -> ()\n // CHECK: %outputs_11, %control_12 = tf_executor.island wraps \"tf.Const\"() <{value = dense<0> : tensor<i32>}> {_ici_weight_distribution_mlir_bridge_marker = true} : () -> tensor<i32>\n-// CHECK: %outputs_13, %control_14 = tf_executor.island wraps \"tf.Identity\"(%outputs_0) {_ici_weight_distribution_mlir_bridge_marker = true, _parallel_execution_ids = \"r0:0\", device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"} : (tensor<128x1024xf32>) -> tensor<128x1024xf32>\n-// CHECK: %outputs_15, %control_16 = tf_executor.island wraps \"tf.Identity\"(%outputs_2) {_ici_weight_distribution_mlir_bridge_marker = true, _parallel_execution_ids = \"r0:0\", device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"} : (tensor<1024xf32>) -> tensor<1024xf32>\n+// CHECK: %outputs_13, %control_14 = tf_executor.island wraps \"tf.Identity\"(%outputs) {_ici_weight_distribution_mlir_bridge_marker = true, _parallel_execution_ids = \"r0:0\", device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"} : (tensor<128x1024xf32>) -> tensor<128x1024xf32>\n+// CHECK: %outputs_15, %control_16 = tf_executor.island wraps \"tf.Identity\"(%outputs_0) {_ici_weight_distribution_mlir_bridge_marker = true, _parallel_execution_ids = \"r0:0\", device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"} : (tensor<1024xf32>) -> tensor<1024xf32>\n // CHECK: %outputs_17:4, %control_18 = tf_executor.island wraps \"tf.Split\"(%outputs_11, %outputs_13) {_ici_weight_distribution_mlir_bridge_marker = true, _parallel_execution_ids = \"r0:0\", num_split = 4 : i32} : (tensor<i32>, tensor<128x1024xf32>) -> (tensor<32x1024xf32>, tensor<32x1024xf32>, tensor<32x1024xf32>, tensor<32x1024xf32>)\n-// CHECK: %outputs_19, %control_20 = tf_executor.island wraps \"tf.TPUExecute\"(%outputs_17#0, %outputs_15, %outputs#1) {_parallel_execution_ids = \"r0:0,p0:0\", device = \"/job:tpu_host_worker/replica:0/task:0/device:TPU:0\"} : (tensor<32x1024xf32>, tensor<1024xf32>, tensor<3x!tf_type.string>) -> tensor<*xf32>\n-// CHECK: %outputs_21, %control_22 = tf_executor.island wraps \"tf.TPUExecute\"(%outputs_17#1, %outputs_15, %outputs#2) {_parallel_execution_ids = \"r0:0,p0:1\", device = \"/job:tpu_host_worker/replica:0/task:0/device:TPU:1\"} : (tensor<32x1024xf32>, tensor<1024xf32>, tensor<3x!tf_type.string>) -> tensor<*xf32>\n-// CHECK: %outputs_23, %control_24 = tf_executor.island wraps \"tf.TPUExecute\"(%outputs_17#2, %outputs_15, %outputs#3) {_parallel_execution_ids = \"r0:0,p0:2\", device = \"/job:tpu_host_worker/replica:0/task:1/device:TPU:0\"} : (tensor<32x1024xf32>, tensor<1024xf32>, tensor<3x!tf_type.string>) -> tensor<*xf32>\n-// CHECK: %outputs_25, %control_26 = tf_executor.island wraps \"tf.TPUExecute\"(%outputs_17#3, %outputs_15, %outputs#4) {_parallel_execution_ids = \"r0:0,p0:3\", device = \"/job:tpu_host_worker/replica:0/task:1/device:TPU:1\"} : (tensor<32x1024xf32>, tensor<1024xf32>, tensor<3x!tf_type.string>) -> tensor<*xf32>\n-// CHECK: %outputs_27, %control_28 = tf_executor.island wraps \"tf.Identity\"(%outputs_4) {_ici_weight_distribution_mlir_bridge_marker = true, _parallel_execution_ids = \"r0:1\", device = \"/job:tpu_host_worker/replica:0/task:2/device:CPU:0\"} : (tensor<128x1024xf32>) -> tensor<128x1024xf32>\n-// CHECK: %outputs_29, %control_30 = tf_executor.island wraps \"tf.Identity\"(%outputs_6) {_ici_weight_distribution_mlir_bridge_marker = true, _parallel_execution_ids = \"r0:1\", device = \"/job:tpu_host_worker/replica:0/task:2/device:CPU:0\"} : (tensor<1024xf32>) -> tensor<1024xf32>\n+// CHECK: %outputs_19, %control_20 = tf_executor.island wraps \"tf.TPUExecute\"(%outputs_17#0, %outputs_15, %outputs_6#1) {_parallel_execution_ids = \"r0:0,p0:0\", device = \"/job:tpu_host_worker/replica:0/task:0/device:TPU:0\"} : (tensor<32x1024xf32>, tensor<1024xf32>, tensor<3x!tf_type.string>) -> tensor<*xf32>\n+// CHECK: %outputs_21, %control_22 = tf_executor.island wraps \"tf.TPUExecute\"(%outputs_17#1, %outputs_15, %outputs_6#2) {_parallel_execution_ids = \"r0:0,p0:1\", device = \"/job:tpu_host_worker/replica:0/task:0/device:TPU:1\"} : (tensor<32x1024xf32>, tensor<1024xf32>, tensor<3x!tf_type.string>) -> tensor<*xf32>\n+// CHECK: %outputs_23, %control_24 = tf_executor.island wraps \"tf.TPUExecute\"(%outputs_17#2, %outputs_15, %outputs_6#3) {_parallel_execution_ids = \"r0:0,p0:2\", device = \"/job:tpu_host_worker/replica:0/task:1/device:TPU:0\"} : (tensor<32x1024xf32>, tensor<1024xf32>, tensor<3x!tf_type.string>) -> tensor<*xf32>\n+// CHECK: %outputs_25, %control_26 = tf_executor.island wraps \"tf.TPUExecute\"(%outputs_17#3, %outputs_15, %outputs_6#4) {_parallel_execution_ids = \"r0:0,p0:3\", device = \"/job:tpu_host_worker/replica:0/task:1/device:TPU:1\"} : (tensor<32x1024xf32>, tensor<1024xf32>, tensor<3x!tf_type.string>) -> tensor<*xf32>\n+// CHECK: %outputs_27, %control_28 = tf_executor.island wraps \"tf.Identity\"(%outputs_2) {_ici_weight_distribution_mlir_bridge_marker = true, _parallel_execution_ids = \"r0:1\", device = \"/job:tpu_host_worker/replica:0/task:2/device:CPU:0\"} : (tensor<128x1024xf32>) -> tensor<128x1024xf32>\n+// CHECK: %outputs_29, %control_30 = tf_executor.island wraps \"tf.Identity\"(%outputs_4) {_ici_weight_distribution_mlir_bridge_marker = true, _parallel_execution_ids = \"r0:1\", device = \"/job:tpu_host_worker/replica:0/task:2/device:CPU:0\"} : (tensor<1024xf32>) -> tensor<1024xf32>\n // CHECK: %outputs_31:4, %control_32 = tf_executor.island wraps \"tf.Split\"(%outputs_11, %outputs_27) {_ici_weight_distribution_mlir_bridge_marker = true, _parallel_execution_ids = \"r0:1\", num_split = 4 : i32} : (tensor<i32>, tensor<128x1024xf32>) -> (tensor<32x1024xf32>, tensor<32x1024xf32>, tensor<32x1024xf32>, tensor<32x1024xf32>)\n-// CHECK: %outputs_33, %control_34 = tf_executor.island wraps \"tf.TPUExecute\"(%outputs_31#0, %outputs_29, %outputs#1) {_parallel_execution_ids = \"r0:1,p0:0\", device = \"/job:tpu_host_worker/replica:0/task:2/device:TPU:0\"} : (tensor<32x1024xf32>, tensor<1024xf32>, tensor<3x!tf_type.string>) -> tensor<*xf32>\n-// CHECK: %outputs_35, %control_36 = tf_executor.island wraps \"tf.TPUExecute\"(%outputs_31#1, %outputs_29, %outputs#2) {_parallel_execution_ids = \"r0:1,p0:1\", device = \"/job:tpu_host_worker/replica:0/task:2/device:TPU:1\"} : (tensor<32x1024xf32>, tensor<1024xf32>, tensor<3x!tf_type.string>) -> tensor<*xf32>\n-// CHECK: %outputs_37, %control_38 = tf_executor.island wraps \"tf.TPUExecute\"(%outputs_31#2, %outputs_29, %outputs#3) {_parallel_execution_ids = \"r0:1,p0:2\", device = \"/job:tpu_host_worker/replica:0/task:3/device:TPU:0\"} : (tensor<32x1024xf32>, tensor<1024xf32>, tensor<3x!tf_type.string>) -> tensor<*xf32>\n-// CHECK: %outputs_39, %control_40 = tf_executor.island wraps \"tf.TPUExecute\"(%outputs_31#3, %outputs_29, %outputs#4) {_parallel_execution_ids = \"r0:1,p0:3\", device = \"/job:tpu_host_worker/replica:0/task:3/device:TPU:1\"} : (tensor<32x1024xf32>, tensor<1024xf32>, tensor<3x!tf_type.string>) -> tensor<*xf32>\n+// CHECK: %outputs_33, %control_34 = tf_executor.island wraps \"tf.TPUExecute\"(%outputs_31#0, %outputs_29, %outputs_6#1) {_parallel_execution_ids = \"r0:1,p0:0\", device = \"/job:tpu_host_worker/replica:0/task:2/device:TPU:0\"} : (tensor<32x1024xf32>, tensor<1024xf32>, tensor<3x!tf_type.string>) -> tensor<*xf32>\n+// CHECK: %outputs_35, %control_36 = tf_executor.island wraps \"tf.TPUExecute\"(%outputs_31#1, %outputs_29, %outputs_6#2) {_parallel_execution_ids = \"r0:1,p0:1\", device = \"/job:tpu_host_worker/replica:0/task:2/device:TPU:1\"} : (tensor<32x1024xf32>, tensor<1024xf32>, tensor<3x!tf_type.string>) -> tensor<*xf32>\n+// CHECK: %outputs_37, %control_38 = tf_executor.island wraps \"tf.TPUExecute\"(%outputs_31#2, %outputs_29, %outputs_6#3) {_parallel_execution_ids = \"r0:1,p0:2\", device = \"/job:tpu_host_worker/replica:0/task:3/device:TPU:0\"} : (tensor<32x1024xf32>, tensor<1024xf32>, tensor<3x!tf_type.string>) -> tensor<*xf32>\n+// CHECK: %outputs_39, %control_40 = tf_executor.island wraps \"tf.TPUExecute\"(%outputs_31#3, %outputs_29, %outputs_6#4) {_parallel_execution_ids = \"r0:1,p0:3\", device = \"/job:tpu_host_worker/replica:0/task:3/device:TPU:1\"} : (tensor<32x1024xf32>, tensor<1024xf32>, tensor<3x!tf_type.string>) -> tensor<*xf32>\n // CHECK: %outputs_41, %control_42 = tf_executor.island wraps \"tf.ReadVariableOp\"(%arg2) {device = \"\"} : (tensor<*x!tf_type.resource<tensor<i64>>>) -> tensor<i64>\n // CHECK: %outputs_43, %control_44 = tf_executor.island wraps \"tf.Identity\"(%outputs_41) {device = \"\"} : (tensor<i64>) -> tensor<i64>\n-// CHECK: tf_executor.fetch %outputs_43, %control_1, %control_3, %control_20, %control_22, %control_24, %control_26, %control_34, %control_36, %control_38, %control_40, %control_42 : tensor<i64>, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control\n-\n+// CHECK: tf_executor.fetch %outputs_43, %control, %control_1, %control_20, %control_22, %control_24, %control_26, %control_34, %control_36, %control_38, %control_40, %control_42 : tensor<i64>, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control\n \n module attributes {tf.devices = {\"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\", \"/job:tpu_host_worker/replica:0/task:0/device:TPU:0\", \"/job:tpu_host_worker/replica:0/task:0/device:TPU:1\", \"/job:tpu_host_worker/replica:0/task:0/device:TPU_SYSTEM:0\", \"/job:tpu_host_worker/replica:0/task:1/device:CPU:0\", \"/job:tpu_host_worker/replica:0/task:1/device:TPU:0\", \"/job:tpu_host_worker/replica:0/task:1/device:TPU:1\", \"/job:tpu_host_worker/replica:0/task:1/device:TPU_SYSTEM:0\", \"/job:tpu_host_worker/replica:0/task:2/device:CPU:0\", \"/job:tpu_host_worker/replica:0/task:2/device:TPU:0\", \"/job:tpu_host_worker/replica:0/task:2/device:TPU:1\", \"/job:tpu_host_worker/replica:0/task:2/device:TPU_SYSTEM:0\", \"/job:tpu_host_worker/replica:0/task:3/device:CPU:0\", \"/job:tpu_host_worker/replica:0/task:3/device:TPU:0\", \"/job:tpu_host_worker/replica:0/task:3/device:TPU:1\", \"/job:tpu_host_worker/replica:0/task:3/device:TPU_SYSTEM:0\"}, tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 1857 : i32}} {\n   func.func @main(%arg0: tensor<i32> {tf._user_specified_name = \"steps\", tf.device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"}, %arg1: tensor<*x!tf_type.resource<tensor<i64>>> {tf._user_specified_name = \"899\", tf.device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"}, %arg2: tensor<*x!tf_type.resource<tensor<i64>>> {tf._user_specified_name = \"901\", tf.device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"}, %arg3: tensor<*x!tf_type.resource<tensor<128x1024xf32>>> {tf._user_specified_name = \"903\", tf.device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"}, %arg4: tensor<*x!tf_type.resource<tensor<1024xf32>>> {tf._user_specified_name = \"905\", tf.device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"}, %arg5: tensor<*x!tf_type.resource<tensor<1024x1xf32>>> {tf._user_specified_name = \"907\", tf.device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"}, %arg6: tensor<*x!tf_type.resource<tensor<i64>>> {tf._user_specified_name = \"909\", tf.device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"}, %arg7: tensor<*x!tf_type.resource<tensor<25001x64xf32>>> {tf._user_specified_name = \"911\", tf.device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"}, %arg8: tensor<*x!tf_type.resource<tensor<25001x64xf32>>> {tf._user_specified_name = \"913\", tf.device = \"/job:tpu_host_worker/replica:0/task:1/device:CPU:0\"}, %arg9: tensor<*x!tf_type.resource<tensor<25001x64xf32>>> {tf._user_specified_name = \"915\", tf.device = \"/job:tpu_host_worker/replica:0/task:2/device:CPU:0\"}, %arg10: tensor<*x!tf_type.resource<tensor<25001x64xf32>>> {tf._user_specified_name = \"917\", tf.device = \"/job:tpu_host_worker/replica:0/task:3/device:CPU:0\"}, %arg11: tensor<*x!tf_type.resource<tensor<25001x32xf32>>> {tf._user_specified_name = \"919\", tf.device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"}, %arg12: tensor<*x!tf_type.resource<tensor<25001x32xf32>>> {tf._user_specified_name = \"921\", tf.device = \"/job:tpu_host_worker/replica:0/task:1/device:CPU:0\"}, %arg13: tensor<*x!tf_type.resource<tensor<25001x32xf32>>> {tf._user_specified_name = \"923\", tf.device = \"/job:tpu_host_worker/replica:0/task:2/device:CPU:0\"}, %arg14: tensor<*x!tf_type.resource<tensor<25001x32xf32>>> {tf._user_specified_name = \"925\", tf.device = \"/job:tpu_host_worker/replica:0/task:3/device:CPU:0\"}, %arg15: tensor<*x!tf_type.resource<tensor<6x32xf32>>> {tf._user_specified_name = \"927\", tf.device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"}, %arg16: tensor<*x!tf_type.resource<tensor<6x32xf32>>> {tf._user_specified_name = \"929\", tf.device = \"/job:tpu_host_worker/replica:0/task:1/device:CPU:0\"}, %arg17: tensor<*x!tf_type.resource<tensor<6x32xf32>>> {tf._user_specified_name = \"931\", tf.device = \"/job:tpu_host_worker/replica:0/task:2/device:CPU:0\"}, %arg18: tensor<*x!tf_type.resource<tensor<6x32xf32>>> {tf._user_specified_name = \"933\", tf.device = \"/job:tpu_host_worker/replica:0/task:3/device:CPU:0\"}, %arg19: tensor<*x!tf_type.resource<tensor<128x1024xf32>>> {tf._user_specified_name = \"935\", tf.device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"}, %arg20: tensor<*x!tf_type.resource<tensor<1024xf32>>> {tf._user_specified_name = \"937\", tf.device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"}, %arg21: tensor<*x!tf_type.resource<tensor<1024x1xf32>>> {tf._user_specified_name = \"939\", tf.device = \"/job:tpu_host_worker/replica:0/task:0/device:CPU:0\"}) -> tensor<*xi64> attributes {allow_soft_placement = false, tf.entry_function = {control_outputs = \"\", inputs = \"steps,unknown,unknown_0,unknown_1,unknown_2,unknown_3,unknown_4,unknown_5,unknown_6,unknown_7,unknown_8,unknown_9,unknown_10,unknown_11,unknown_12,unknown_13,unknown_14,unknown_15,unknown_16,unknown_17,unknown_18,unknown_19\", outputs = \"statefulpartitionedcall_RetVal\"}} {"
        },
        {
            "sha": "266e95e74e8e80ba947ac3669888296d1e1fa65e",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/host_runtime/lower_cluster_to_runtime_ops.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fhost_runtime%2Flower_cluster_to_runtime_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fhost_runtime%2Flower_cluster_to_runtime_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fhost_runtime%2Flower_cluster_to_runtime_ops.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -79,7 +79,6 @@ void AddTPULowerClusterToRuntimeOpsPassPipeline(OpPassManager& pm,\n   pm.addPass(mlir::createSymbolDCEPass());\n   pm.addNestedPass<FuncOp>(\n       mlir::TFDevice::CreateReplicateInvariantOpHoistingPass());\n-  pm.addPass(mlir::TF::CreateOrderForProgramKeyPass());\n   pm.addNestedPass<FuncOp>(mlir::TFDevice::CreateEmbeddingProgramKeyPass());\n   pm.addPass(mlir::TFTPU::CreateTPUMergeVariablesWithExecutePass());\n   pm.addNestedPass<FuncOp>("
        },
        {
            "sha": "0130e7a63c70bc3368e317e07e656bf75f9d43d8",
            "filename": "tensorflow/compiler/mlir/tensorflow/transforms/host_runtime/lower_cluster_to_runtime_ops_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fhost_runtime%2Flower_cluster_to_runtime_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fhost_runtime%2Flower_cluster_to_runtime_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftensorflow%2Ftransforms%2Fhost_runtime%2Flower_cluster_to_runtime_ops_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -191,7 +191,7 @@ TEST_F(LowerClusterToRuntimeOpsTest, DumpsPipelinePasses) {\n       *mlir_module_, DeviceType(DEVICE_TPU_XLA_JIT)));\n \n   TF_ASSERT_OK(env_->GetChildren(test_dir_, &files));\n-  EXPECT_THAT(files, ::testing::SizeIs(16));\n+  EXPECT_THAT(files, ::testing::SizeIs(15));\n }\n \n }  // namespace"
        },
        {
            "sha": "d9b30a3661473981b4eb3d6e484e9dc90a1793f2",
            "filename": "tensorflow/compiler/mlir/tf2xla/transforms/legalization_op_config_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Flegalization_op_config_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Flegalization_op_config_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Flegalization_op_config_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -84,7 +84,7 @@ TEST(LegalizationOpConfigTest, CountLoweringsSet) {\n   // a new op, we should expect these to change too.\n   EXPECT_EQ(mlir_lowering_count, 67);\n   EXPECT_EQ(tf2xla_fallback_count, 333);\n-  EXPECT_EQ(non_categorized_count, 432);\n+  EXPECT_EQ(non_categorized_count, 433);\n }\n \n // Just a counter test to see which ops have duplicate lowerings. This isn't a"
        },
        {
            "sha": "441793d13bff7561b666d96dea7898a8ec0aa3ea",
            "filename": "tensorflow/compiler/mlir/tfrt/ir/mlrt/tf_mlrt_ops.td",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Fir%2Fmlrt%2Ftf_mlrt_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Fir%2Fmlrt%2Ftf_mlrt_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Fir%2Fmlrt%2Ftf_mlrt_ops.td?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -533,4 +533,17 @@ def IfrtRestoreVariableOp: TensorflowMlrt_Op<\"ifrt_restore_variable\", []> {\n   );\n }\n \n+def MlrtIfrtResourceDeserializeOp: TensorflowMlrt_Op<\"ifrt_resource_deserialize\", []> {\n+  let summary = \"Deserialize resource vars.\";\n+  let description = [{\n+    This is the MLRT version of the IfrtResourceDeserialize op.\n+  }];\n+\n+  let arguments = (ins\n+    TFTensorType:$resource_var,\n+    TFTensorType:$input_dir,\n+    StrAttr:$tensor_name\n+  );\n+}\n+\n #endif"
        },
        {
            "sha": "66d210e4df96c8e92bcd54a5b95475ccc503058c",
            "filename": "tensorflow/compiler/mlir/tfrt/tests/mlrt/tf_to_mlrt.mlir",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftests%2Fmlrt%2Ftf_to_mlrt.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftests%2Fmlrt%2Ftf_to_mlrt.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftests%2Fmlrt%2Ftf_to_mlrt.mlir?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -495,4 +495,15 @@ func.func @ifrt_restore_variable_test() -> () {\n   func.return\n }\n \n+// -----\n+\n+// Test lowering of tf.IfrtResourceDeserializeOp to tf_mlrt.ifrt_resource_deserialize\n+\n+// CHECK-LABEL: func @ifrt_resource_deserialize_test\n+func.func @ifrt_resource_deserialize_test(%arg0: tensor<!tf_type.resource<tensor<f32>>>) {\n+  %input_dir = \"tf.Const\"() { value = dense<\"some/path\"> : tensor<!tf_type.string> } : () -> tensor<!tf_type.string>\n+  // CHECK: \"tf_mlrt.ifrt_resource_deserialize\"(%arg0, %{{.*}}) <{tensor_name = \"my_tensor\"}>\n+  \"tf.IfrtResourceDeserialize\"(%arg0, %input_dir) {require_matching_crc = false, tensor_name = \"my_tensor\"} : (tensor<!tf_type.resource<tensor<f32>>>, tensor<!tf_type.string>) -> ()\n+  func.return\n+}\n "
        },
        {
            "sha": "278b7da74ac75dba60ded78b7ef66b2e6095edbe",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/ifrt/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -212,18 +212,17 @@ tf_cc_test(\n         \"@llvm-project//mlir:AllPassesAndDialects\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Parser\",\n-        \"@llvm-project//mlir:RegisterAllDialects\",\n+        \"@llvm-project//mlir:RegisterAllDialects\",  # buildcleaner: keep\n         \"@local_tsl//tsl/platform:protobuf\",\n-        \"@local_xla//xla/pjrt:pjrt_client\",\n         \"@local_xla//xla/pjrt:pjrt_compiler\",\n-        \"@local_xla//xla/pjrt/gpu:se_gpu_pjrt_client\",\n         \"@local_xla//xla/pjrt/plugin/xla_cpu:cpu_topology_description\",\n         \"@local_xla//xla/python/ifrt\",\n         \"@local_xla//xla/python/ifrt:mock\",\n         \"@local_xla//xla/python/ifrt:test_util\",\n         \"@local_xla//xla/python/pjrt_ifrt\",\n         \"@local_xla//xla/python/pjrt_ifrt:tfrt_cpu_client_test_lib\",\n         \"@local_xla//xla/service:computation_placer_hdr\",\n+        \"@local_xla//xla/tsl/platform:statusor\",\n     ],\n )\n "
        },
        {
            "sha": "32898953f8973e5d227361ed72e361ae92cf9d1e",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/ifrt/ifrt_backend_compiler_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Fifrt_backend_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Fifrt_backend_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Fifrt_backend_compiler_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -111,7 +111,6 @@ class IfrtBackendCompilerTest : public ::testing::Test {\n \n namespace {\n using ::testing::HasSubstr;\n-using ::tsl::testing::StatusIs;\n \n struct IfrtBackendCompilerTestParams {\n   std::string mlir_file_name;"
        },
        {
            "sha": "22acb4d136e6c5be5f118a1cf698de01adb3124c",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/ifrt/tf2hlo_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Ftf2hlo_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Ftf2hlo_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Ftf2hlo_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -37,8 +37,6 @@ limitations under the License.\n #include \"tensorflow/compiler/mlir/tensorflow/dialect_registration.h\"\n #include \"tensorflow/compiler/mlir/tfrt/transforms/ifrt/ifrt_types.h\"\n #include \"tensorflow/compiler/tf2xla/xla_helpers.h\"\n-#include \"xla/pjrt/gpu/se_gpu_pjrt_client.h\"\n-#include \"xla/pjrt/pjrt_client.h\"\n #include \"xla/pjrt/pjrt_compiler.h\"\n #include \"xla/pjrt/plugin/xla_cpu/cpu_topology_description.h\"\n #include \"xla/python/ifrt/client.h\"\n@@ -47,6 +45,7 @@ limitations under the License.\n #include \"xla/python/pjrt_ifrt/pjrt_topology.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"tensorflow/core/platform/resource_loader.h\"\n #include \"tensorflow/core/platform/test.h\"\n #include \"tsl/platform/protobuf.h\"\n@@ -57,7 +56,6 @@ namespace {\n using ::testing::Eq;\n using ::testing::HasSubstr;\n using ::testing::Ne;\n-using tsl::testing::StatusIs;\n \n // TODO(b/229726259): Make EqualsProto available in OSS\n class ProtoStringMatcher {\n@@ -533,9 +531,7 @@ TEST_F(Tf2HloTest, GpuCompile) {\n       .entry_function_name = \"main\",\n       .compile_metadata = compile_metadata,\n       .shape_representation_fn = tensorflow::IdentityShapeRepresentationFn(),\n-      .topology = std::make_shared<xla::ifrt::PjRtTopology>(\n-          std::make_shared<xla::StreamExecutorGpuTopologyDescription>(\n-              xla::CudaId(), xla::CudaName(), /*gpu_topology=*/nullptr)),\n+      .topology = nullptr,\n       .platform_name = xla::CudaName(),\n   };\n "
        },
        {
            "sha": "379a91c9080edd33098e6a72bd4ea6f4daac573c",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/mlrt/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fmlrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fmlrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fmlrt%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -174,6 +174,7 @@ cc_library(\n         \":assign_op_key\",\n         \":passes\",\n         \":while_to_map_fn\",\n+        \"//tensorflow/compiler/mlir/tensorflow:bridge_logger\",\n         \"//tensorflow/compiler/mlir/tensorflow:dump_mlir_util\",\n         \"//tensorflow/compiler/mlir/tensorflow:error_util\",\n         \"//tensorflow/compiler/mlir/tf2xla/api/v2:tf_executor_to_graph\",\n@@ -183,6 +184,7 @@ cc_library(\n         \"//tensorflow/compiler/mlir/tfrt:tfrt_compile_options\",\n         \"//tensorflow/compiler/mlir/tfrt:tfrt_pipeline_options\",\n         \"//tensorflow/compiler/mlir/tfrt/translate/mlrt:mlir_to_bytecode\",\n+        \"//tensorflow/core:framework\",\n         \"//tensorflow/core:protos_all_cc\",\n         \"//tensorflow/core/platform:status\",\n         \"//tensorflow/core/platform:statusor\",\n@@ -194,6 +196,7 @@ cc_library(\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/strings\",\n+        \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Pass\","
        },
        {
            "sha": "7b4d35501251be3a0a708421f76f45705776a218",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/mlrt/import_model.cc",
            "status": "modified",
            "additions": 31,
            "deletions": 2,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fmlrt%2Fimport_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fmlrt%2Fimport_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fmlrt%2Fimport_model.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -14,19 +14,22 @@ limitations under the License.\n ==============================================================================*/\n #include \"tensorflow/compiler/mlir/tfrt/transforms/mlrt/import_model.h\"\n \n+#include <memory>\n #include <string>\n #include <utility>\n #include <vector>\n \n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n+#include \"llvm/ADT/StringRef.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"  // from @llvm-project\n #include \"mlir/IR/BuiltinOps.h\"  // from @llvm-project\n #include \"mlir/IR/OwningOpRef.h\"  // from @llvm-project\n #include \"mlir/Pass/PassManager.h\"  // from @llvm-project\n #include \"mlir/Support/LogicalResult.h\"  // from @llvm-project\n #include \"mlir/Transforms/Passes.h\"  // from @llvm-project\n+#include \"tensorflow/compiler/mlir/tensorflow/utils/data_dumper_logger_config.h\"\n #include \"tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.h\"\n #include \"tensorflow/compiler/mlir/tensorflow/utils/error_util.h\"\n #include \"tensorflow/compiler/mlir/tfrt/transforms/mlrt/assign_op_key.h\"\n@@ -47,9 +50,30 @@ limitations under the License.\n #include \"tensorflow/core/tfrt/mlrt/attribute/attribute.h\"\n #include \"tensorflow/core/tfrt/mlrt/bytecode/bytecode.h\"\n #include \"tensorflow/core/tfrt/runtime/runtime.h\"\n+#include \"tensorflow/core/util/debug_data_dumper.h\"\n \n namespace tensorflow {\n namespace mlrt_compiler {\n+namespace {\n+// Setup the input pass manager to enable IR dumping after each pass.\n+// Note a side effect of this method is that multi threading will be disabled.\n+void EnablePassIRPrinting(mlir::PassManager& pm,\n+                          const std::string& dump_group_name,\n+                          llvm::StringRef module_name) {\n+  // Print the whole module after each pass, which requires disabling\n+  // multi-threading as well.\n+  pm.getContext()->disableMultithreading();\n+  pm.enableIRPrinting(std::make_unique<::tensorflow::DataDumperLoggerConfig>(\n+      [module_name, dump_group_name](const std::string& pass_tag_name,\n+                                     mlir::Operation* op) {\n+        return DEBUG_DATA_DUMPER()->GetDumpFilename(\n+            module_name.str(), dump_group_name, pass_tag_name);\n+      },\n+      /*pass_prefix=*/\"\",\n+      /*print_module_scope=*/true));\n+  pm.enableTiming();\n+}\n+}  // namespace\n \n absl::StatusOr<mlrt::bc::Buffer> ConvertTfMlirToBytecode(\n     const TfrtCompileOptions& options, tfrt_stub::FallbackState& fallback_state,\n@@ -100,13 +124,18 @@ absl::StatusOr<mlrt::bc::Buffer> ConvertTfMlirToBytecode(\n         // Remove unreachable private functions after map_fn conversion.\n         pm.addPass(mlir::createSymbolDCEPass());\n \n-        tensorflow::CreateTFExecutorToTFInvariantOptimizationPipelineHelper(\n-            pm, options);\n+        tensorflow::CreateTFInvariantOptimizationPipelineHelper(pm, options);\n         // TODO(b/283481729): Add test to cover unused constants that do not\n         // cause op_key discontinuity\n         pm.addNestedPass<mlir::func::FuncOp>(mlir::createCanonicalizerPass());\n         pm.addPass(mlrt_compiler::CreateAssignOpKeyPass());\n+\n         // Run passes until (including) AssignOpKeyPass.\n+        if (VLOG_IS_ON(4)) {\n+          EnablePassIRPrinting(pm, \"mlrt_runtime_lowering_tf\",\n+                               \"mlrt_runtime_lowering_tf\");\n+        }\n+\n         if (mlir::failed(pm.run(module))) {\n           return diag_handler.Combine(absl::InternalError(\n               \"failed to finish passes before (including) assign op keys.\"));"
        },
        {
            "sha": "cc59c9150da7697497d94b682b68cb3d89a0646a",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/mlrt/tf_to_mlrt.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 1,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fmlrt%2Ftf_to_mlrt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fmlrt%2Ftf_to_mlrt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fmlrt%2Ftf_to_mlrt.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -382,6 +382,29 @@ class IfrtRestoreVariableOpConversion\n   }\n };\n \n+class IfrtResourceDeserializeOpConversion\n+    : public mlir::OpConversionPattern<mlir::TF::IfrtResourceDeserializeOp> {\n+ public:\n+  using OpConversionPattern::OpConversionPattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      mlir::TF::IfrtResourceDeserializeOp op, OpAdaptor adaptor,\n+      mlir::ConversionPatternRewriter& rewriter) const override {\n+    // Transfer the tensor_name attribute; drop the unused require_matching_crc.\n+    auto tensor_name_attr = op->getAttr(\"tensor_name\");\n+    if (!tensor_name_attr) {\n+      return op.emitError(\"tensor_name attribute not found\");\n+    }\n+\n+    auto new_op = rewriter.create<tf_mlrt::MlrtIfrtResourceDeserializeOp>(\n+        op.getLoc(), adaptor.getResourceVar(), adaptor.getInputDir(),\n+        llvm::cast<mlir::StringAttr>(tensor_name_attr));\n+    rewriter.replaceOp(op, new_op);\n+\n+    return mlir::success();\n+  }\n+};\n+\n std::optional<std::string> DecodeLongName(mlir::Location loc) {\n   if (auto name_loc = mlir::dyn_cast<mlir::NameLoc>(loc)) {\n     return name_loc.getName().str();\n@@ -1278,7 +1301,8 @@ class TfToMlrtConversionPass\n     patterns.add<WhileOpConversion>(&context, &type_converter_, &symbol_table);\n     patterns.add<AsyncOpConversion, GetResourceOpConversion,\n                  SetResourceOpConversion, IfrtRestoreVariableOpConversion,\n-                 TFAwaitOpConversion, TFPromiseOpConversion>(&context);\n+                 TFAwaitOpConversion, TFPromiseOpConversion,\n+                 IfrtResourceDeserializeOpConversion>(&context);\n     patterns.add<BatchFunctionOpConversion, CaseOpConversion, CondOpConversion,\n                  TFAsyncWhileOpConversion, TFMapFnOpConversion>(type_converter_,\n                                                                 &context);"
        },
        {
            "sha": "ddff1b2bde43f981027df0a45ea211961871d742",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/passes.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fpasses.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fpasses.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fpasses.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -94,6 +94,9 @@ void CreateTFExecutorToTFPreInvariantOptimizationPipelineHelper(\n   // as TFRT-specific optimization may create more opportunities.\n   pm.addNestedPass<mlir::func::FuncOp>(\n       tfrt_compiler::CreateOptimizeTfForTfrtPass());\n+\n+  pm.addNestedPass<mlir::func::FuncOp>(\n+      mlir::CreateExecutorDialectToFunctionalConversionPass());\n   pm.addNestedPass<mlir::func::FuncOp>(mlir::createCanonicalizerPass());\n   // Guarantee all functions have one use, which enables more exact shape\n   // inference.\n@@ -219,8 +222,8 @@ void CreateTFExecutorToTFPreInvariantOptimizationPipelineHelper(\n   AddTfDeviceAssignmentPasses(pm, options);\n }\n \n-void CreateTFExecutorToTFInvariantOptimizationPipelineHelper(\n-    mlir::OpPassManager &pm, const TfrtPipelineOptions &options) {\n+void CreateTFInvariantOptimizationPipelineHelper(\n+    mlir::OpPassManager& pm, const TfrtPipelineOptions& options) {\n   if (options.sink_in_invariant_ops) {\n     pm.addPass(CreateSinkInInvariantOpsPass());\n   }"
        },
        {
            "sha": "1285c8df7147b1f04ca29f77555cd7def80f6627",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/passes.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fpasses.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -160,7 +160,7 @@ absl::Status CreateTFExecutorToTFPipeline(mlir::PassManager& pm,\n // TODO(deqiangc): refactor below helpers once mlrt is OSSed.\n void CreateTFExecutorToTFPreInvariantOptimizationPipelineHelper(\n     mlir::OpPassManager& pm, const TfrtPipelineOptions& options);\n-void CreateTFExecutorToTFInvariantOptimizationPipelineHelper(\n+void CreateTFInvariantOptimizationPipelineHelper(\n     mlir::OpPassManager& pm, const TfrtPipelineOptions& options);\n \n absl::Status CreateTFExecutorToTFPreInvariantOptimizationPipeline("
        },
        {
            "sha": "de9ed33392544a395fdc3d2a65d5c82c29a51914",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/tf_to_tfrt.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Ftf_to_tfrt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Ftf_to_tfrt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Ftf_to_tfrt.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1878,7 +1878,7 @@ void CreateTfToTfrtPipeline(mlir::OpPassManager &pm,\n static void CreateTfExecutorToTfrtPipelineHelper(\n     mlir::OpPassManager &pm, const TfrtPipelineOptions &options) {\n   CreateTFExecutorToTFPreInvariantOptimizationPipelineHelper(pm, options);\n-  CreateTFExecutorToTFInvariantOptimizationPipelineHelper(pm, options);\n+  CreateTFInvariantOptimizationPipelineHelper(pm, options);\n   CreateTfToTfrtPipeline(pm, options);\n }\n \n@@ -1889,7 +1889,7 @@ absl::Status CreateTfExecutorToTfrtPipeline(\n     mlir::PassManager &pm, const TfrtPipelineOptions &options) {\n   TF_RETURN_IF_ERROR(\n       CreateTFExecutorToTFPreInvariantOptimizationPipeline(pm, options));\n-  CreateTFExecutorToTFInvariantOptimizationPipelineHelper(pm, options);\n+  CreateTFInvariantOptimizationPipelineHelper(pm, options);\n   CreateTfToTfrtPipeline(pm, options);\n   return absl::OkStatus();\n }\n@@ -1898,7 +1898,7 @@ absl::Status CreateTFExecutorToTFPipeline(mlir::PassManager &pm,\n                                           const TfrtPipelineOptions &options) {\n   TF_RETURN_IF_ERROR(\n       CreateTFExecutorToTFPreInvariantOptimizationPipeline(pm, options));\n-  CreateTFExecutorToTFInvariantOptimizationPipelineHelper(pm, options);\n+  CreateTFInvariantOptimizationPipelineHelper(pm, options);\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "e8004f17a24b474072b1c4e6610cc9f8be7a6a05",
            "filename": "tensorflow/compiler/mlir/tfrt/translate/import_model.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fimport_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fimport_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fimport_model.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -280,8 +280,7 @@ absl::Status ConvertTfMlirToBef(\n       [bef_buffer](mlir::PassManager& pm, mlir::ModuleOp module,\n                    const tensorflow::TfrtPipelineOptions& options) {\n         mlir::StatusScopedDiagnosticHandler diag_handler(module.getContext());\n-        tensorflow::CreateTFExecutorToTFInvariantOptimizationPipelineHelper(\n-            pm, options);\n+        tensorflow::CreateTFInvariantOptimizationPipelineHelper(pm, options);\n         tensorflow::CreateTfToTfrtPipeline(pm, options);\n \n         if (mlir::failed(pm.run(module))) {"
        },
        {
            "sha": "3750d1abde96bb22936eca8d21a4cb26536b3f2c",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -99,6 +99,7 @@ cc_library(\n         \"@local_xla//xla/mlir_hlo:transforms_gpu_passes\",\n         \"@local_xla//xla/mlir_hlo:transforms_passes\",\n         \"@stablehlo//:chlo_ops\",\n+        \"@stablehlo//:stablehlo_passes\",\n     ],\n )\n "
        },
        {
            "sha": "ed9d46df4eaaf7deda8349f185e2ab3e51662a0f",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/kernel_creator.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Fkernel_creator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Fkernel_creator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Fkernel_creator.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -62,6 +62,7 @@ limitations under the License.\n #include \"mlir/Transforms/DialectConversion.h\"  // from @llvm-project\n #include \"mlir/Transforms/Passes.h\"  // from @llvm-project\n #include \"stablehlo/dialect/ChloOps.h\"  // from @stablehlo\n+#include \"stablehlo/transforms/Passes.h\"  // from @stablehlo\n #include \"tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.h\"\n #include \"tensorflow/compiler/mlir/tools/kernel_gen/transforms/passes.h\"\n #include \"tensorflow/compiler/mlir/tools/kernel_gen/transforms/rewriters.h\"\n@@ -165,7 +166,9 @@ absl::Status LowerHlotoLoops(mlir::ModuleOp module,\n             /*jit_i64_indexed_for_large_tensors=*/true));\n   }\n \n-  pm.addNestedPass<FuncOp>(mlir::mhlo::createChloLegalizeToHloPass());\n+  pm.addNestedPass<mlir::func::FuncOp>(\n+      mlir::stablehlo::createChloLegalizeToStablehloPass());\n+  pm.addPass(mlir::mhlo::createStablehloLegalizeToHloPass());\n \n   pm.addNestedPass<FuncOp>(mlir::createCanonicalizerPass());\n   pm.addNestedPass<FuncOp>(mlir::createCSEPass());"
        },
        {
            "sha": "89d946516f6b9b40a97d58fc97a42f77cfe202de",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/transforms/func_to_jit_invocations.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Ffunc_to_jit_invocations.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Ffunc_to_jit_invocations.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Ffunc_to_jit_invocations.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -67,7 +67,7 @@ LogicalResult RewriteToFullJit(func::FuncOp op) {\n   // Create the JIT compile op.\n   auto jit_compile_op = rewriter.create<tf_framework::JITCompileOp>(\n       loc, rewriter.getType<tf_framework::JITCallableType>(),\n-      /*ctx=*/std::nullopt);\n+      /*ctx=*/mlir::Value());\n \n   // Move the original functions operations into the body.\n   {"
        },
        {
            "sha": "ceda47565bf999031f3415f6ed6687033b0a4d7e",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/transforms/rewrite_tf_framework_assert.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Frewrite_tf_framework_assert.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Frewrite_tf_framework_assert.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Frewrite_tf_framework_assert.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -60,8 +60,8 @@ class TFAssertOpConverter : public OpConversionPattern<TFAssertOp> {\n \n     rewriter.restoreInsertionPoint(ip);\n     rewriter.replaceOpWithNewOp<cf::CondBranchOp>(\n-        op, adaptor.getArg(), split_block, std::nullopt, error_reporting_block,\n-        std::nullopt);\n+        op, adaptor.getArg(), split_block, mlir::ValueRange(),\n+        error_reporting_block, mlir::ValueRange());\n     return success();\n   }\n };"
        },
        {
            "sha": "4cbe21b73f62c3d4b38ef7b4e26bfa472ac04504",
            "filename": "tensorflow/compiler/mlir/tools/kernel_gen/transforms/tf_framework_legalize_to_llvm.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Ftf_framework_legalize_to_llvm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Ftf_framework_legalize_to_llvm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftools%2Fkernel_gen%2Ftransforms%2Ftf_framework_legalize_to_llvm.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -237,7 +237,7 @@ class TFDeallocOpConverter : public ConvertToLLVMCallOpPattern<TFDeallocOp> {\n     FlatSymbolRefAttr tf_func_ref =\n         GetOrInsertLLVMFunction(GetFuncName(), GetFuncType(), op, &rewriter);\n     rewriter.replaceOpWithNewOp<LLVM::CallOp>(\n-        op, std::nullopt, tf_func_ref,\n+        op, mlir::TypeRange(), tf_func_ref,\n         llvm::ArrayRef({adaptor.getCtx(), allocated_bytes_ptr}));\n     return success();\n   }\n@@ -353,7 +353,7 @@ class JITExecuteOpConverter : public ConvertToLLVMCallOpPattern<JITExecuteOp> {\n     FlatSymbolRefAttr tf_func_ref =\n         GetOrInsertLLVMFunction(GetFuncName(), GetFuncType(), op, &rewriter);\n     rewriter.create<LLVM::CallOp>(\n-        loc, std::nullopt, tf_func_ref,\n+        loc, mlir::TypeRange(), tf_func_ref,\n         ValueRange{adaptor.getCtx(), adaptor.getCallable(), result_ptr,\n                    num_args, args_ptr});\n \n@@ -406,7 +406,7 @@ class ReportErrorOpConverter\n         loc, typeConverter->convertType(rewriter.getI32Type()),\n         adaptor.getErrorCodeAttr());\n     rewriter.replaceOpWithNewOp<LLVM::CallOp>(\n-        op, std::nullopt, tf_func_ref,\n+        op, mlir::TypeRange(), tf_func_ref,\n         llvm::ArrayRef({adaptor.getCtx(), error_code, message_constant}));\n     return success();\n   }\n@@ -516,12 +516,10 @@ class NullMemRefOpConverter : public ConvertOpToLLVMPattern<NullMemRefOp> {\n     // Due to the current way of handling unranked memref results escaping, we\n     // have to actually construct a ranked underlying descriptor instead of just\n     // setting its pointer to NULL.\n-    SmallVector<Value, 4> sizes;\n-    UnrankedMemRefDescriptor::computeSizes(rewriter, loc, *getTypeConverter(),\n-                                           desc, addressSpace, sizes);\n+    Value alloca_size = UnrankedMemRefDescriptor::computeSize(\n+        rewriter, loc, *getTypeConverter(), desc, addressSpace);\n     Value underlying_desc_ptr = rewriter.create<LLVM::AllocaOp>(\n-        loc, getVoidPtrType(), IntegerType::get(getContext(), 8),\n-        sizes.front());\n+        loc, getVoidPtrType(), IntegerType::get(getContext(), 8), alloca_size);\n \n     // Populate underlying ranked descriptor.\n     Value null = rewriter.create<LLVM::ZeroOp>(loc, llvm_ptr_type);"
        },
        {
            "sha": "3b37da874b585be9f91c164857fff7ac72d95f7c",
            "filename": "tensorflow/compiler/tf2xla/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Ftf2xla%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcompiler%2Ftf2xla%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -369,12 +369,10 @@ cc_library(\n #         \"@local_xla//xla/tsl/platform/default:context\",\n #         \"@local_xla//xla/tsl/platform/default:cord\",\n #         \"@local_xla//xla/tsl/platform/default:env_time\",\n-#         \"@local_xla//xla/tsl/platform/default:logging\",\n #         \"@local_xla//xla/tsl/platform/default:types\",\n #         \"@local_xla//xla/tsl/platform/google:context\",\n #         \"@local_xla//xla/tsl/platform/google:cord\",\n #         \"@local_xla//xla/tsl/platform/google:env_time\",\n-#         \"@local_xla//xla/tsl/platform/google:logging\",\n #         \"@local_xla//xla/tsl/platform/google:types\",\n #         \"@local_xla//xla/tsl/platform/windows:env_time\",\n #         \"//tensorflow/core/platform:bfloat16\","
        },
        {
            "sha": "8fde99e62ff656217a167b922d579c03842edcbc",
            "filename": "tensorflow/core/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -495,7 +495,6 @@ cc_library(\n         \"@local_xla//xla/tsl/framework:numeric_types.h\",\n         \"@local_xla//xla/tsl/framework:type_traits.h\",\n         \"@local_xla//xla/tsl/platform/default:integral_types.h\",\n-        \"@local_xla//xla/tsl/platform/default:logging.h\",\n     ],\n     visibility = [\"//visibility:public\"],\n     deps = [\n@@ -1539,7 +1538,6 @@ cc_library(\n         \"//tensorflow/core/platform:tflite_portable_logging_hdrs\",\n         \"@local_tsl//tsl/platform:tflite_portable_logging_hdrs\",\n         \"@local_xla//xla/tsl/platform/default:integral_types.h\",\n-        \"@local_xla//xla/tsl/platform/default:logging.h\",\n     ],\n     compatible_with = get_compatible_with_portable(),\n     copts = tf_copts(),"
        },
        {
            "sha": "c281800f8ab4a58306af1a4bbec889d49b45bc99",
            "filename": "tensorflow/core/common_runtime/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fcommon_runtime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fcommon_runtime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -2755,6 +2755,7 @@ tf_cuda_cc_test(\n         \"//tensorflow/core:test_main\",\n         \"//tensorflow/core:testlib\",\n         \"//tensorflow/core/kernels:cast_op\",\n+        \"@com_google_absl//absl/status\",\n     ],\n )\n \n@@ -3619,6 +3620,7 @@ tf_cc_test(\n tf_cc_fuzz_test(\n     name = \"graph_constructor_fuzz\",\n     srcs = [\"graph_constructor_fuzz.cc\"],\n+    shard_count = 10,\n     tags = [\"no_oss\"],\n     deps = [\n         \":core\","
        },
        {
            "sha": "23c8d54200c458264c98834f24c45f789f56b7fd",
            "filename": "tensorflow/core/common_runtime/gpu/gpu_device.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_device.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_device.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_device.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1385,7 +1385,10 @@ Status BaseGPUDeviceFactory::GetDeviceDetails(\n   auto desc = std::move(desc_status).value();\n   (*details)[\"device_name\"] = desc->name();\n #if GOOGLE_CUDA\n-  (*details)[\"compute_capability\"] = desc->cuda_compute_capability().ToString();\n+  // Some users of this API expect the compute capability to be in the format\n+  // X.Y. Therefore we don't expose the feature extension here.\n+  (*details)[\"compute_capability\"] =\n+      desc->cuda_compute_capability().WithoutAnyFeatureExtension().ToString();\n #endif  // GOOGLE_CUDA\n   return OkStatus();\n }"
        },
        {
            "sha": "26f414c14204cee1650b94b405468c688a712c11",
            "filename": "tensorflow/core/common_runtime/memory_types_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fcommon_runtime%2Fmemory_types_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fcommon_runtime%2Fmemory_types_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fmemory_types_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -52,7 +52,7 @@ TEST(MemoryTypeChecker, Int32NotOk) {\n #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n   // There is no kernel for casting int32/host memory to float/device\n   // memory.\n-  EXPECT_TRUE(errors::IsInternal(ValidateMemoryTypes(DEVICE_GPU, g)));\n+  EXPECT_TRUE(absl::IsInternal(ValidateMemoryTypes(DEVICE_GPU, g)));\n \n   // But we can insert _HostSend/_HostRecv to ensure the invariant.\n   TF_EXPECT_OK(EnsureMemoryTypes(DEVICE_GPU, \"/device:GPU:0\", g));"
        },
        {
            "sha": "e23d6445fadaabc52bf9f427ce41941c761ccff1",
            "filename": "tensorflow/core/common_runtime/next_pluggable_device/c_plugin_coordination_service_agent_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fcommon_runtime%2Fnext_pluggable_device%2Fc_plugin_coordination_service_agent_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fcommon_runtime%2Fnext_pluggable_device%2Fc_plugin_coordination_service_agent_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fnext_pluggable_device%2Fc_plugin_coordination_service_agent_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -102,6 +102,10 @@ class TestCoordinationClient : public CoordinationClient {\n               (const TryGetKeyValueRequest*, TryGetKeyValueResponse*,\n                StatusCallback),\n               (override));\n+  MOCK_METHOD(void, IncrementKeyValueAsync,\n+              (const IncrementKeyValueRequest*, IncrementKeyValueResponse*,\n+               StatusCallback),\n+              (override));\n   MOCK_METHOD(void, InsertKeyValueAsync,\n               (const InsertKeyValueRequest*, InsertKeyValueResponse*,\n                StatusCallback),"
        },
        {
            "sha": "0f495b17a69544ee28d5380d4af9445bf6899b1d",
            "filename": "tensorflow/core/framework/common_shape_fns.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fframework%2Fcommon_shape_fns.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fframework%2Fcommon_shape_fns.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fcommon_shape_fns.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -710,7 +710,7 @@ absl::Status Conv2DShapeImpl(shape_inference::InferenceContext* c,\n     absl::Status s = c->GetAttr(\"explicit_paddings\", &explicit_paddings);\n     // Use the default value, which is an empty list, if the attribute is not\n     // found. Otherwise return the error to the caller.\n-    if (!s.ok() && !errors::IsNotFound(s)) {\n+    if (!s.ok() && !absl::IsNotFound(s)) {\n       return s;\n     }\n     TF_RETURN_IF_ERROR(CheckValidPadding(padding, explicit_paddings,\n@@ -724,7 +724,7 @@ absl::Status Conv2DShapeImpl(shape_inference::InferenceContext* c,\n     // `padding_list` attribute is used by Fused int8 convolutions to support\n     // explicit paddings.\n     absl::Status s_p_list = c->GetAttr(\"padding_list\", &p_list);\n-    if (!s_p_list.ok() && !errors::IsNotFound(s_p_list)) {\n+    if (!s_p_list.ok() && !absl::IsNotFound(s_p_list)) {\n       return s_p_list;\n     }\n     if (s_p_list.ok() && !p_list.empty()) {\n@@ -1872,7 +1872,7 @@ absl::Status MaxPoolShapeImpl(shape_inference::InferenceContext* c,\n     absl::Status status = c->GetAttr(\"explicit_paddings\", &explicit_paddings);\n     // Use the default value, which is an empty list, if the attribute is not\n     // found. Otherwise return the error to the caller.\n-    if (!status.ok() && !errors::IsNotFound(status)) {\n+    if (!status.ok() && !absl::IsNotFound(status)) {\n       return status;\n     }\n     TF_RETURN_IF_ERROR(CheckValidPadding(padding, explicit_paddings,"
        },
        {
            "sha": "e4b8688f8ece1ff83c34d698ddaa751030794ad5",
            "filename": "tensorflow/core/framework/run_handler.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 21,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fframework%2Frun_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fframework%2Frun_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Frun_handler.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include <list>\n #include <memory>\n \n+#include \"absl/strings/str_cat.h\"\n #include \"unsupported/Eigen/CXX11/Tensor\"  // from @eigen_archive\n #include \"tensorflow/core/framework/run_handler_util.h\"\n #include \"tensorflow/core/lib/core/threadpool_interface.h\"\n@@ -303,11 +304,11 @@ unsigned ThreadWorkSource::NonBlockingWorkShardingFactor() {\n }\n \n std::string ThreadWorkSource::ToString() {\n-  return strings::StrCat(\"traceme_id = \", GetTracemeId(),\n-                         \", inter queue size = \", TaskQueueSize(true),\n-                         \", inter inflight = \", GetInflightTaskCount(true),\n-                         \", intra queue size = \", TaskQueueSize(false),\n-                         \", intra inflight = \", GetInflightTaskCount(false));\n+  return absl::StrCat(\"traceme_id = \", GetTracemeId(),\n+                      \", inter queue size = \", TaskQueueSize(true),\n+                      \", inter inflight = \", GetInflightTaskCount(true),\n+                      \", intra queue size = \", TaskQueueSize(false),\n+                      \", intra inflight = \", GetInflightTaskCount(false));\n }\n \n RunHandlerThreadPool::RunHandlerThreadPool(\n@@ -376,8 +377,8 @@ void RunHandlerThreadPool::Start() {\n           WorkerLoop(i, is_blocking_thread);\n         },\n         is_blocking_thread\n-            ? strings::StrCat(name_, \"_blocking_thread_\", sub_thread_pool_id)\n-            : strings::StrCat(name_, \"_non_blocking_thread\")));\n+            ? absl::StrCat(name_, \"_blocking_thread_\", sub_thread_pool_id)\n+            : absl::StrCat(name_, \"_non_blocking_thread\")));\n   }\n }\n \n@@ -625,9 +626,9 @@ void RunHandlerThreadPool::WorkerLoop(int thread_id,\n     if (t.f) {\n       tsl::profiler::TraceMe activity(\n           [=] {\n-            return strings::StrCat(task_from_blocking_queue ? \"inter\" : \"intra\",\n-                                   \" #id = \", tws->GetTracemeId(), \" \",\n-                                   thread_id, \"#\");\n+            return absl::StrCat(task_from_blocking_queue ? \"inter\" : \"intra\",\n+                                \" #id = \", tws->GetTracemeId(), \" \", thread_id,\n+                                \"#\");\n           },\n           tsl::profiler::TraceMeLevel::kInfo);\n       VLOG(2) << \"Running \" << (task_from_blocking_queue ? \"inter\" : \"intra\")\n@@ -637,9 +638,7 @@ void RunHandlerThreadPool::WorkerLoop(int thread_id,\n       tws->DecrementInflightTaskCount(task_from_blocking_queue);\n     } else {\n       tsl::profiler::TraceMe activity(\n-          [=] {\n-            return strings::StrCat(\"Sleeping#thread_id=\", thread_id, \"#\");\n-          },\n+          [=] { return absl::StrCat(\"Sleeping#thread_id=\", thread_id, \"#\"); },\n           tsl::profiler::TraceMeLevel::kInfo);\n       if (VLOG_IS_ON(4)) {\n         for (int i = 0; i < thread_work_sources->size(); ++i) {\n@@ -850,14 +849,13 @@ class RunHandlerPool::Impl {\n       if (!has_free_handler()) {\n         tsl::profiler::TraceMe activity(\n             [&] {\n-              return strings::StrCat(\"WaitingForHandler#step_id=\", step_id,\n-                                     \"#\");\n+              return absl::StrCat(\"WaitingForHandler#step_id=\", step_id, \"#\");\n             },\n             tsl::profiler::TraceMeLevel::kInfo);\n         TRACESTRING(\n-            strings::StrCat(\"RunHandlerPool::Impl::Get waiting for a handler \"\n-                            \"with timeout in millisecond\",\n-                            timeout_in_ms));\n+            absl::StrCat(\"RunHandlerPool::Impl::Get waiting for a handler \"\n+                         \"with timeout in millisecond\",\n+                         timeout_in_ms));\n         if (timeout_in_ms == 0) {\n           mu_.Await(Condition(this, &Impl::has_free_handler));\n         } else if (!mu_.AwaitWithDeadline(\n@@ -1031,9 +1029,9 @@ void RunHandlerPool::Impl::LogInfo() {\n         ids_str += \" \";\n       }\n \n-      times_str +=\n-          strings::StrCat((now - (*it)->start_time_us()) / 1000.0, \" ms.\");\n-      ids_str += strings::StrCat((*it)->tws()->GetTracemeId());\n+      absl::StrAppend(&times_str, (now - (*it)->start_time_us()) / 1000.0,\n+                      \" ms.\");\n+      absl::StrAppend(&ids_str, (*it)->tws()->GetTracemeId());\n       ++it;\n     }\n     VLOG(1) << \"Elapsed times are: \" << times_str;"
        },
        {
            "sha": "c91df85e3da365100c44dc6e5e6701120289f039",
            "filename": "tensorflow/core/framework/tensor.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fframework%2Ftensor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fframework%2Ftensor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftensor.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -650,6 +650,12 @@ TensorBuffer* Int4OrInt2FromProtoField(Allocator* a, const TensorProto& in,\n   const int64_t in_n = in.int_val().size();\n   auto begin = in.int_val().begin();\n   if (n <= in_n) {\n+// swapping bits of the data pointer for big endian systems\n+#if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\n+    for (int64_t i = 0; i < n; ++i) {\n+      data[i] = ((data[i] & 0xF0) >> 4) | ((data[i] & 0x0F) << 4);\n+    }\n+#endif\n     std::copy_n(begin, n, data);\n   } else if (in_n > 0) {\n     std::copy_n(begin, in_n, data);"
        },
        {
            "sha": "3c6cc215e95bc532ace7d267447aba917b483f23",
            "filename": "tensorflow/core/graph/algorithm_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Falgorithm_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Falgorithm_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Falgorithm_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -56,7 +56,7 @@ bool ExpectBefore(const std::vector<std::pair<string, string>>& ordered_pairs,\n     bool seen_both = false;\n     for (const Node* node : inputs) {\n       if (!seen_before && after_node == node->name()) {\n-        *error = strings::StrCat(\"Saw \", after_node, \" before \", before_node);\n+        *error = absl::StrCat(\"Saw \", after_node, \" before \", before_node);\n         return false;\n       }\n \n@@ -68,8 +68,8 @@ bool ExpectBefore(const std::vector<std::pair<string, string>>& ordered_pairs,\n       }\n     }\n     if (!seen_both) {\n-      *error = strings::StrCat(\"didn't see either \", before_node, \" or \",\n-                               after_node);\n+      *error =\n+          absl::StrCat(\"didn't see either \", before_node, \" or \", after_node);\n       return false;\n     }\n   }\n@@ -142,7 +142,7 @@ TEST(AlgorithmTest, ReversePostOrderStable) {\n     // implemented correctly.\n     for (int64_t j = 0; j < i; ++j) {\n       BinaryOp(\"TestMul\", w1, {input, 1},\n-               b.opts().WithName(strings::StrCat(\"internal\", j)));\n+               b.opts().WithName(absl::StrCat(\"internal\", j)));\n     }\n \n     BinaryOp(\"TestMul\", w1, {input, 1}, b.opts().WithName(\"t3\"));"
        },
        {
            "sha": "0e5c2273f53b20c1d55a6eead5047310b9750391",
            "filename": "tensorflow/core/graph/costmodel_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Fcostmodel_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Fcostmodel_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fcostmodel_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -101,7 +101,7 @@ Node* AddNode(Graph& graph, const string& name, const string& node_type,\n               int num_inputs) {\n   auto builder = NodeDefBuilder(name, node_type);\n   for (int i = 0; i < num_inputs; ++i) {\n-    builder = builder.Input(strings::StrCat(\"node_\", i), i, DT_FLOAT);\n+    builder = builder.Input(absl::StrCat(\"node_\", i), i, DT_FLOAT);\n   }\n \n   NodeDef node_def;"
        },
        {
            "sha": "a3e14eac396859654069d4555585502c9aee6ed4",
            "filename": "tensorflow/core/graph/graph.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Fgraph.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Fgraph.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fgraph.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -115,11 +115,11 @@ Node::NodeClass Node::GetNodeClassForOp(const std::string& ts) {\n }\n \n std::string Node::DebugString() const {\n-  std::string ret = strings::StrCat(\"{name:'\", name(), \"' id:\", id_);\n+  std::string ret = absl::StrCat(\"{name:'\", name(), \"' id:\", id_);\n   if (IsSource()) {\n-    strings::StrAppend(&ret, \" source}\");\n+    absl::StrAppend(&ret, \" source}\");\n   } else if (IsSink()) {\n-    strings::StrAppend(&ret, \" sink}\");\n+    absl::StrAppend(&ret, \" sink}\");\n   } else {\n     strings::StrAppend(&ret, \" op device:\", \"{requested: '\", requested_device(),\n                        \"', assigned: '\", assigned_device_name(), \"'}\", \" def:{\",\n@@ -693,7 +693,7 @@ const Edge* Graph::AddControlEdge(Node* source, Node* dest,\n   // Modify dest's NodeDef if necessary.\n   if (!source->IsSource() && !dest->IsSink() && !allow_duplicates) {\n     // Check if this input is already in dest's NodeDef.\n-    const std::string new_input = strings::StrCat(\"^\", source->name());\n+    const std::string new_input = absl::StrCat(\"^\", source->name());\n     bool input_exists = false;\n     for (const std::string& input : dest->props_->node_def.input()) {\n       if (input == new_input) {\n@@ -712,7 +712,7 @@ const Edge* Graph::AddControlEdge(Node* source, Node* dest,\n void Graph::RemoveControlEdge(const Edge* e) {\n   if (!e->src_->IsSource() && !e->dst_->IsSink()) {\n     e->dst_->MaybeCopyOnWrite();\n-    std::string e_src_name = strings::StrCat(\"^\", e->src_->name());\n+    std::string e_src_name = absl::StrCat(\"^\", e->src_->name());\n     auto* inputs = e->dst_->props_->node_def.mutable_input();\n     for (auto it = inputs->begin(); it != inputs->end(); ++it) {\n       if (*it == e_src_name) {\n@@ -746,17 +746,17 @@ absl::Status Graph::UpdateEdge(Node* new_src, int new_src_index, Node* dst,\n   AddEdge(new_src, new_src_index, dst, dst_index);\n   dst->MaybeCopyOnWrite();\n   (*dst->props_->node_def.mutable_input())[dst_index] =\n-      strings::StrCat(new_src->name(), \":\", new_src_index);\n+      absl::StrCat(new_src->name(), \":\", new_src_index);\n   return absl::OkStatus();\n }\n \n void Graph::AddInput(NodeDef* dst, absl::string_view src_name, int src_slot) {\n   if (src_slot == Graph::kControlSlot) {\n-    dst->add_input(strings::StrCat(\"^\", src_name));\n+    dst->add_input(absl::StrCat(\"^\", src_name));\n   } else if (src_slot == 0) {\n     dst->add_input(src_name.data(), src_name.size());\n   } else {\n-    dst->add_input(strings::StrCat(src_name, \":\", src_slot));\n+    dst->add_input(absl::StrCat(src_name, \":\", src_slot));\n   }\n }\n \n@@ -779,7 +779,7 @@ absl::Status Graph::AddWhileInputHack(Node* new_src, int new_src_index,\n   AddEdge(new_src, new_src_index, dst, dst_index);\n   dst->MaybeCopyOnWrite();\n   dst->props_->node_def.add_input(\n-      strings::StrCat(new_src->name(), \":\", new_src_index));\n+      absl::StrCat(new_src->name(), \":\", new_src_index));\n   return absl::OkStatus();\n }\n \n@@ -912,7 +912,7 @@ void Graph::ToGraphDefSubRange(GraphDef* graph_def, int from_node_id,\n }\n \n std::string Graph::NewName(absl::string_view prefix) {\n-  return strings::StrCat(prefix, \"/_\", name_counter_++);\n+  return absl::StrCat(prefix, \"/_\", name_counter_++);\n }\n \n absl::Status Graph::IsValidNode(const Node* node) const {"
        },
        {
            "sha": "be5a5423ae57c670ec9d8734702015808942cc95",
            "filename": "tensorflow/core/graph/graph_partition.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 15,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Fgraph_partition.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Fgraph_partition.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fgraph_partition.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -163,7 +163,7 @@ void AddReadControl(const std::vector<NodeDef*>& recvs,\n                     const std::vector<string>& inputs) {\n   for (NodeDef* recv : recvs) {\n     for (const string& input : inputs) {\n-      recv->add_input(strings::StrCat(\"^\", input));\n+      recv->add_input(absl::StrCat(\"^\", input));\n     }\n   }\n }\n@@ -328,12 +328,12 @@ NodeDef* AddDummyConst(const PartitionOptions& opts, GraphDef* gdef,\n   const Node* src = edge->src();\n   Tensor tensor(DT_FLOAT, TensorShape({0}));\n   NodeDef* result = gdef->add_node();\n-  *status = NodeDefBuilder(opts.new_name(strings::StrCat(src->name(), \"/ctrl\")),\n-                           \"Const\")\n-                .Device(src->assigned_device_name())\n-                .Attr(\"dtype\", DT_FLOAT)\n-                .Attr(\"value\", tensor)\n-                .Finalize(result, /*consume=*/true);\n+  *status =\n+      NodeDefBuilder(opts.new_name(absl::StrCat(src->name(), \"/ctrl\")), \"Const\")\n+          .Device(src->assigned_device_name())\n+          .Attr(\"dtype\", DT_FLOAT)\n+          .Attr(\"value\", tensor)\n+          .Finalize(result, /*consume=*/true);\n   return result;\n }\n \n@@ -342,7 +342,7 @@ NodeDef* AddControlTrigger(const PartitionOptions& opts, GraphDef* gdef,\n                            const string& assigned_device_name, int64_t epoch,\n                            int64_t starttime, absl::Status* status) {\n   NodeDef* result = gdef->add_node();\n-  *status = NodeDefBuilder(opts.new_name(strings::StrCat(\"synch_\", epoch)),\n+  *status = NodeDefBuilder(opts.new_name(absl::StrCat(\"synch_\", epoch)),\n                            \"ControlTrigger\")\n                 .Device(assigned_device_name)\n                 .Attr(\"_start_time\", starttime)\n@@ -399,7 +399,7 @@ void OptimizeControlFlowColocation(Graph* graph) {\n }\n \n string ControlLoopName(const string& name) {\n-  return strings::StrCat(\"_cloop\", name);\n+  return absl::StrCat(\"_cloop\", name);\n }\n \n bool IsControlLoop(const Node* node) {\n@@ -700,7 +700,7 @@ absl::Status AddControlFlow(const PartitionOptions& opts, Graph* g,\n         break;\n       }\n \n-      const string& cl_key = strings::StrCat(curr_frame_name, \"$$\", dst_device);\n+      const string& cl_key = absl::StrCat(curr_frame_name, \"$$\", dst_device);\n       auto it = control_loops.find(cl_key);\n       if (it != control_loops.end()) {\n         if (child_loop.enter != nullptr) {\n@@ -756,8 +756,7 @@ absl::Status AddControlFlow(const PartitionOptions& opts, Graph* g,\n       const string& src_frame_name = cf_info[src_frame->id()].frame_name;\n       const string& dst_frame_name = cf_info[dst_frame->id()].frame_name;\n       if (!src_frame_name.empty() && src_frame_name == dst_frame_name) {\n-        const string& cl_key =\n-            strings::StrCat(dst_frame_name, \"$$\", dst_device);\n+        const string& cl_key = absl::StrCat(dst_frame_name, \"$$\", dst_device);\n         ControlLoop loop = control_loops[cl_key];\n         DCHECK(loop.enter != nullptr);\n         // Note that we'll create multiple duplicate edges if dst has multiple\n@@ -1163,7 +1162,7 @@ absl::Status Partition(const PartitionOptions& opts, Graph* g,\n         tensor_name_attr = opts.get_tensor_name_attr(edge);\n       } else {\n         tensor_name_attr =\n-            strings::StrCat(\"edge_\", edge->id(), \"_\", edge->src()->name());\n+            absl::StrCat(\"edge_\", edge->id(), \"_\", edge->src()->name());\n       }\n \n       if (VLOG_IS_ON(1) && IsConstant(edge->src())) {\n@@ -1297,8 +1296,8 @@ absl::Status Partition(const PartitionOptions& opts, Graph* g,\n   if (VLOG_IS_ON(2)) {\n     for (auto& it : *partitions) {\n       GraphDef* gdef = &it.second;\n-      DumpGraphDefToFile(strings::StrCat(\"partition_\", it.first, \"_\",\n-                                         reinterpret_cast<uintptr_t>(gdef)),\n+      DumpGraphDefToFile(absl::StrCat(\"partition_\", it.first, \"_\",\n+                                      reinterpret_cast<uintptr_t>(gdef)),\n                          *gdef);\n     }\n   }"
        },
        {
            "sha": "5f3d0a1b4117f21c20a255776686347b3a6dd249",
            "filename": "tensorflow/core/graph/graph_partition_test.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 16,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Fgraph_partition_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Fgraph_partition_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fgraph_partition_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -77,7 +77,7 @@ string DeviceName(const Node* node) {\n   } else {\n     const string cpu_prefix = \"/job:a/replica:0/task:0/cpu:\";\n     int index = first - 'A';\n-    return strings::StrCat(cpu_prefix, index);\n+    return absl::StrCat(cpu_prefix, index);\n   }\n }\n \n@@ -634,8 +634,7 @@ TEST(TopologicalSortNodesWithTimePriorityTest, NoDependencies) {\n   }\n   std::vector<ops::Placeholder> placeholders;\n   for (int i : indexes) {\n-    placeholders.emplace_back(root.WithOpName(strings::StrCat(\"p\", i)),\n-                              DT_FLOAT);\n+    placeholders.emplace_back(root.WithOpName(absl::StrCat(\"p\", i)), DT_FLOAT);\n     placeholders.back().node()->AddAttr(\"_start_time\", i + 1);\n   }\n \n@@ -648,7 +647,7 @@ TEST(TopologicalSortNodesWithTimePriorityTest, NoDependencies) {\n       TopologicalSortNodesWithTimePriority(&gdef, &nodes, &node_to_start_time));\n   ASSERT_EQ(nodes.size(), 20);\n   for (int i = 0; i < nodes.size(); ++i) {\n-    EXPECT_EQ(strings::StrCat(\"p\", i), nodes[i].first->name());\n+    EXPECT_EQ(absl::StrCat(\"p\", i), nodes[i].first->name());\n     EXPECT_EQ(i + 1, nodes[i].second);\n   }\n }\n@@ -662,7 +661,7 @@ TEST(TopologicalSortNodesWithTimePriority, Dependencies) {\n   const int num_leaves = 20;\n   for (int i = 0; i < num_leaves; ++i) {\n     indexes.push_back((i + 2001) % num_leaves);\n-    placeholders_in_order.emplace_back(root.WithOpName(strings::StrCat(\"p\", i)),\n+    placeholders_in_order.emplace_back(root.WithOpName(absl::StrCat(\"p\", i)),\n                                        DT_FLOAT);\n     placeholders_in_order.back().node()->AddAttr(\"_start_time\", i + 1);\n   }\n@@ -676,7 +675,7 @@ TEST(TopologicalSortNodesWithTimePriority, Dependencies) {\n   // placeholder runs last).\n   std::vector<ops::Square> squares;\n   for (int i : indexes) {\n-    squares.emplace_back(root.WithOpName(strings::StrCat(\"s\", i)),\n+    squares.emplace_back(root.WithOpName(absl::StrCat(\"s\", i)),\n                          placeholders[i]);\n     squares.back().node()->AddAttr(\"_start_time\", 50 - (i + 1));\n   }\n@@ -700,15 +699,15 @@ TEST(TopologicalSortNodesWithTimePriority, Dependencies) {\n   ASSERT_EQ(1 + squares.size() + placeholders.size(), nodes.size());\n   for (int i = 0; i < placeholders.size(); ++i) {\n     const NodeDef* node = nodes[i].first;\n-    EXPECT_EQ(strings::StrCat(\"p\", i), node->name());\n+    EXPECT_EQ(absl::StrCat(\"p\", i), node->name());\n     EXPECT_EQ(i + 1, nodes[i].second);\n     EXPECT_EQ(i + 1, node_to_start_time[node]);\n   }\n   for (int i = 0; i < squares.size(); ++i) {\n     int node_index = placeholders.size() + i;\n     int square_index = num_leaves - 1 - i;\n     const NodeDef* node = nodes[node_index].first;\n-    EXPECT_EQ(strings::StrCat(\"s\", square_index), node->name());\n+    EXPECT_EQ(absl::StrCat(\"s\", square_index), node->name());\n     EXPECT_EQ(50 - (square_index + 1), nodes[node_index].second);\n     EXPECT_EQ(50 - (square_index + 1), node_to_start_time[node]);\n   }\n@@ -728,7 +727,7 @@ TEST(TopologicalSortNodesWithTimePriority, WhileLoop) {\n   const int num_leaves = 20;\n   for (int i = 0; i < num_leaves; ++i) {\n     indexes.push_back((i + 2001) % num_leaves);\n-    placeholders_in_order.emplace_back(root.WithOpName(strings::StrCat(\"p\", i)),\n+    placeholders_in_order.emplace_back(root.WithOpName(absl::StrCat(\"p\", i)),\n                                        DT_FLOAT);\n     placeholders_in_order.back().node()->AddAttr(\"_start_time\", i + 1);\n   }\n@@ -742,10 +741,10 @@ TEST(TopologicalSortNodesWithTimePriority, WhileLoop) {\n   std::vector<Exit> while_exits;\n   const int nodes_per_loop = 8;\n   for (int i : indexes) {\n-    Scope scope = root.NewSubScope(strings::StrCat(\"while\", i));\n+    Scope scope = root.NewSubScope(absl::StrCat(\"while\", i));\n     auto dummy = Placeholder(scope, DT_FLOAT);\n \n-    Enter enter(scope, placeholders[i], strings::StrCat(\"frame\", i));\n+    Enter enter(scope, placeholders[i], absl::StrCat(\"frame\", i));\n     Merge merge(scope, std::initializer_list<Input>{enter, dummy});\n     auto cv = Const(scope.WithControlDependencies({merge.output}), false);\n     LoopCond loop_cond(scope, cv);\n@@ -772,8 +771,7 @@ TEST(TopologicalSortNodesWithTimePriority, WhileLoop) {\n   std::vector<Square> squares;\n   squares.reserve(indexes.size());\n   for (int i : indexes) {\n-    squares.emplace_back(root.WithOpName(strings::StrCat(\"s\", i)),\n-                         while_exits[i]);\n+    squares.emplace_back(root.WithOpName(absl::StrCat(\"s\", i)), while_exits[i]);\n     squares.back().node()->AddAttr(\"_start_time\", 500 - (i + 1));\n   }\n \n@@ -790,20 +788,20 @@ TEST(TopologicalSortNodesWithTimePriority, WhileLoop) {\n   int node_index = 0;\n   for (int i = 0; i < placeholders.size(); ++i, ++node_index) {\n     const NodeDef* node = nodes[i].first;\n-    EXPECT_EQ(strings::StrCat(\"p\", i), node->name());\n+    EXPECT_EQ(absl::StrCat(\"p\", i), node->name());\n     EXPECT_EQ(i + 1, nodes[i].second);\n     EXPECT_EQ(i + 1, node_to_start_time[node]);\n   }\n   for (int i = 0; i < while_exits.size(); ++i, node_index += nodes_per_loop) {\n     const NodeDef* node = nodes[node_index].first;\n-    EXPECT_EQ(strings::StrCat(\"while\", i, \"/Enter\"), node->name());\n+    EXPECT_EQ(absl::StrCat(\"while\", i, \"/Enter\"), node->name());\n     EXPECT_EQ(100 + i * 10, nodes[node_index].second);\n     EXPECT_EQ(100 + i * 10, node_to_start_time[node]);\n   }\n   for (int i = 0; i < squares.size(); ++i, ++node_index) {\n     int square_index = num_leaves - 1 - i;\n     const NodeDef* node = nodes[node_index].first;\n-    EXPECT_EQ(strings::StrCat(\"s\", square_index), node->name());\n+    EXPECT_EQ(absl::StrCat(\"s\", square_index), node->name());\n     EXPECT_EQ(500 - (square_index + 1), nodes[node_index].second);\n     EXPECT_EQ(500 - (square_index + 1), node_to_start_time[node]);\n   }"
        },
        {
            "sha": "a5b519365034f25ba466fc6f06325f0f27bfd46f",
            "filename": "tensorflow/core/graph/graph_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Fgraph_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Fgraph_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fgraph_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -110,7 +110,7 @@ class GraphTest : public ::testing::Test {\n                     int num_inputs) {\n     auto builder = NodeDefBuilder(name, node_type);\n     for (int i = 0; i < num_inputs; ++i) {\n-      builder = builder.Input(strings::StrCat(\"node_\", i), i, DT_FLOAT);\n+      builder = builder.Input(absl::StrCat(\"node_\", i), i, DT_FLOAT);\n     }\n \n     NodeDef node_def;\n@@ -144,7 +144,7 @@ class GraphTest : public ::testing::Test {\n         return true;\n       }\n     }\n-    std::string control_edge_name = strings::StrCat(\"^\", src->name());\n+    std::string control_edge_name = absl::StrCat(\"^\", src->name());\n     for (int i = 0; i < dst->def().input_size(); ++i) {\n       if (dst->def().input(i) == control_edge_name) {\n         return true;\n@@ -397,7 +397,7 @@ static string EdgeIter(const Graph& g) {\n   std::sort(edges.begin(), edges.end());\n   string result;\n   for (auto& p : edges) {\n-    strings::StrAppend(&result, p.first, \"->\", p.second, \";\");\n+    absl::StrAppend(&result, p.first, \"->\", p.second, \";\");\n   }\n   return result;\n }"
        },
        {
            "sha": "e2fe533ce4b2385bc9e8f3c93ba62e16f39b60ab",
            "filename": "tensorflow/core/graph/node_builder.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Fnode_builder.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Fnode_builder.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fnode_builder.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -160,8 +160,8 @@ absl::Status NodeBuilder::Finalize(Graph* graph, Node** created_node,\n void NodeBuilder::AddIndexError(const Node* node, int i) {\n   if (node == nullptr) {\n     errors_.emplace_back(\n-        strings::StrCat(\"Attempt to add nullptr Node to node with type \",\n-                        def_builder_.op_def().name()));\n+        absl::StrCat(\"Attempt to add nullptr Node to node with type \",\n+                     def_builder_.op_def().name()));\n   } else {\n     errors_.emplace_back(strings::StrCat(\n         \"Attempt to add output \", i, \" of \", node->name(), \" not in range [0, \","
        },
        {
            "sha": "94b4cabb2fd884b60c45225884d72ec4bca98941",
            "filename": "tensorflow/core/graph/optimizer_cse_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Foptimizer_cse_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Foptimizer_cse_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Foptimizer_cse_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -61,9 +61,9 @@ class OptimizerCSETest : public ::testing::Test {\n     if (index == 0) {\n       return n->name();\n     } else if (index == Graph::kControlSlot) {\n-      return strings::StrCat(n->name(), \":control\");\n+      return absl::StrCat(n->name(), \":control\");\n     } else {\n-      return strings::StrCat(n->name(), \":\", index);\n+      return absl::StrCat(n->name(), \":\", index);\n     }\n   }\n \n@@ -72,20 +72,20 @@ class OptimizerCSETest : public ::testing::Test {\n     std::vector<string> edges;\n     for (const Node* n : g->nodes()) {\n       if (IncludeNode(n)) {\n-        nodes.push_back(strings::StrCat(n->name(), \"(\", n->type_string(), \")\"));\n+        nodes.push_back(absl::StrCat(n->name(), \"(\", n->type_string(), \")\"));\n       }\n     }\n     for (const Edge* e : g->edges()) {\n       if (IncludeNode(e->src()) && IncludeNode(e->dst())) {\n-        edges.push_back(strings::StrCat(EdgeId(e->src(), e->src_output()), \"->\",\n-                                        EdgeId(e->dst(), e->dst_input())));\n+        edges.push_back(absl::StrCat(EdgeId(e->src(), e->src_output()), \"->\",\n+                                     EdgeId(e->dst(), e->dst_input())));\n       }\n     }\n     // Canonicalize\n     std::sort(nodes.begin(), nodes.end());\n     std::sort(edges.begin(), edges.end());\n-    return strings::StrCat(absl::StrJoin(nodes, \";\"), \"|\",\n-                           absl::StrJoin(edges, \";\"));\n+    return absl::StrCat(absl::StrJoin(nodes, \";\"), \"|\",\n+                        absl::StrJoin(edges, \";\"));\n   }\n \n   string DoCSE(const std::function<bool(const Node*)>& consider_fn = nullptr) {\n@@ -370,7 +370,7 @@ void BM_CSE(::testing::benchmark::State& state) {\n     InitGraph(s, graph);\n     int N = graph->num_node_ids();\n     if (first) {\n-      state.SetLabel(strings::StrCat(\"Per graph node.  Nodes: \", N));\n+      state.SetLabel(absl::StrCat(\"Per graph node.  Nodes: \", N));\n       first = false;\n     }\n     {"
        },
        {
            "sha": "1d03877c02583c728d6853c6a2201b9eb2dcb1aa",
            "filename": "tensorflow/core/graph/subgraph.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Fsubgraph.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Fsubgraph.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fsubgraph.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -195,12 +195,12 @@ absl::Status PruneForTargets(Graph* g, const NameIndex& name_index,\n   std::unordered_set<const Node*> targets;\n   for (Node* n : fetch_nodes) {\n     if (!AddNodeToTargets(n->name(), name_index, &targets)) {\n-      strings::StrAppend(&not_found, n->name(), \" \");\n+      absl::StrAppend(&not_found, n->name(), \" \");\n     }\n   }\n   for (const string& s : target_nodes) {\n     if (!AddNodeToTargets(s, name_index, &targets)) {\n-      strings::StrAppend(&not_found, s, \" \");\n+      absl::StrAppend(&not_found, s, \" \");\n     }\n   }\n   if (!not_found.empty()) {\n@@ -238,8 +238,8 @@ absl::Status RecvFeedRewrite::AddNode(Graph* g,\n                                       NodeBuilder::NodeOut feed_tensor,\n                                       Node** out_node) {\n   TF_RETURN_IF_ERROR(\n-      NodeBuilder(strings::StrCat(\"_recv_\", feed_tensor.node->name(), \"_\",\n-                                  feed_tensor.index),\n+      NodeBuilder(absl::StrCat(\"_recv_\", feed_tensor.node->name(), \"_\",\n+                               feed_tensor.index),\n                   \"_Recv\")\n           .Attr(\"tensor_type\",\n                 BaseType(feed_tensor.node->output_type(feed_tensor.index)))\n@@ -279,8 +279,8 @@ absl::Status SendFetchRewrite::AddNode(Graph* g,\n                                        NodeBuilder::NodeOut fetch_tensor,\n                                        Node** out_node) {\n   TF_RETURN_IF_ERROR(\n-      NodeBuilder(strings::StrCat(\"_send_\", fetch_tensor.node->name(), \"_\",\n-                                  fetch_tensor.index),\n+      NodeBuilder(absl::StrCat(\"_send_\", fetch_tensor.node->name(), \"_\",\n+                               fetch_tensor.index),\n                   \"_Send\")\n           .Input(fetch_tensor.node, fetch_tensor.index)\n           .Attr(\"tensor_name\", endpoint_name())"
        },
        {
            "sha": "31c5cf8a3bb444a3a374159d6db70ab5058ac2a0",
            "filename": "tensorflow/core/graph/subgraph_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Fsubgraph_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Fsubgraph_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fsubgraph_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -355,7 +355,7 @@ void BM_SubgraphHelper(::testing::benchmark::State& state,\n     GraphDefBuilder b(GraphDefBuilder::kFailImmediately);\n     Node* last_node = nullptr;\n     for (int i = 0; i < num_nodes; i++) {\n-      string name = strings::StrCat(\"N\", i);\n+      string name = absl::StrCat(\"N\", i);\n       if (i > 0) {\n         last_node = ops::UnaryOp(\"Op\", last_node, b.opts().WithName(name));\n       } else {\n@@ -367,10 +367,10 @@ void BM_SubgraphHelper(::testing::benchmark::State& state,\n \n   std::vector<string> fed;\n   if (num_nodes > 1000) {\n-    fed.push_back(strings::StrCat(\"N\", num_nodes - 1000));\n+    fed.push_back(absl::StrCat(\"N\", num_nodes - 1000));\n   }\n   std::vector<string> fetch;\n-  std::vector<string> targets = {strings::StrCat(\"N\", num_nodes - 1)};\n+  std::vector<string> targets = {absl::StrCat(\"N\", num_nodes - 1)};\n \n   for (auto s : state) {\n     Graph* subgraph = new Graph(OpRegistry::Global());"
        },
        {
            "sha": "cabd7968b2827418a8371cc33d445a0b4e508cd0",
            "filename": "tensorflow/core/graph/tensor_id.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Ftensor_id.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Ftensor_id.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Ftensor_id.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -45,8 +45,8 @@ struct TensorId : public std::pair<absl::string_view, int> {\n   int index() const { return second; }\n \n   string ToString() const {\n-    if (second == Graph::kControlSlot) return strings::StrCat(\"^\", first);\n-    return strings::StrCat(first, \":\", second);\n+    if (second == Graph::kControlSlot) return absl::StrCat(\"^\", first);\n+    return absl::StrCat(first, \":\", second);\n   }\n \n   struct Hasher {\n@@ -77,8 +77,8 @@ struct SafeTensorId : public std::pair<string, int> {\n   int index() const { return second; }\n \n   string ToString() const {\n-    if (second == Graph::kControlSlot) return strings::StrCat(\"^\", first);\n-    return strings::StrCat(first, \":\", second);\n+    if (second == Graph::kControlSlot) return absl::StrCat(\"^\", first);\n+    return absl::StrCat(first, \":\", second);\n   }\n \n   struct Hasher {"
        },
        {
            "sha": "15bffd170642c8523421429946d52da6d67130f6",
            "filename": "tensorflow/core/graph/tensor_id_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Ftensor_id_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Ftensor_id_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Ftensor_id_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -54,7 +54,7 @@ void BM_ParseTensorName(::testing::benchmark::State& state) {\n           name += rnd.OneIn(4) ? '0' : 'a';\n         }\n         if (rnd.OneIn(3)) {\n-          strings::StrAppend(&name, \":\", rnd.Uniform(12));\n+          absl::StrAppend(&name, \":\", rnd.Uniform(12));\n         }\n         break;\n       }"
        },
        {
            "sha": "b593a2c9b63c7eb3472e624f14f403e1a4760837",
            "filename": "tensorflow/core/graph/validate_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Fvalidate_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgraph%2Fvalidate_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fvalidate_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -225,7 +225,7 @@ Node* AddNodeFromNodeDef(Graph& graph, const string& name,\n                          const string& node_type, int num_inputs) {\n   auto builder = NodeDefBuilder(name, node_type);\n   for (int i = 0; i < num_inputs; ++i) {\n-    builder = builder.Input(strings::StrCat(\"node_\", i), i, DT_FLOAT);\n+    builder = builder.Input(absl::StrCat(\"node_\", i), i, DT_FLOAT);\n   }\n \n   NodeDef node_def;"
        },
        {
            "sha": "87576cdb9aec816da15c2f9bc5492c35c7c10e97",
            "filename": "tensorflow/core/grappler/optimizers/meta_optimizer.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 23,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fmeta_optimizer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fmeta_optimizer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fmeta_optimizer.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -91,10 +91,10 @@ int64_t NumEdges(const GraphDef& graph) {\n }\n \n string PrintSizesBeforeAfter(const GraphDef& before, const GraphDef& after) {\n-  return strings::StrCat(\"Graph size after: \", after.node_size(), \" nodes (\",\n-                         after.node_size() - before.node_size(), \"), \",\n-                         NumEdges(after), \" edges (\",\n-                         NumEdges(after) - NumEdges(before), \")\");\n+  return absl::StrCat(\"Graph size after: \", after.node_size(), \" nodes (\",\n+                      after.node_size() - before.node_size(), \"), \",\n+                      NumEdges(after), \" edges (\",\n+                      NumEdges(after) - NumEdges(before), \")\");\n }\n \n int NumIterations(const RewriterConfig& cfg) {\n@@ -720,10 +720,10 @@ void MetaOptimizer::PrintUserAndPluginConfigs(\n   string logs =\n       \"\\nConfig of optimizers\\t\\tUser's config\\tPlugin's config\\tFinal \"\n       \"config(User & Plugin)\\n\";\n-  strings::StrAppend(&logs, \"disable_model_pruning\\t\\t\",\n-                     user_cfg.disable_model_pruning, \"\\t\\t\",\n-                     plugin_cfg.disable_model_pruning, \"\\t\\t\",\n-                     final_cfg.disable_model_pruning, \"\\n\");\n+  absl::StrAppend(&logs, \"disable_model_pruning\\t\\t\",\n+                  user_cfg.disable_model_pruning, \"\\t\\t\",\n+                  plugin_cfg.disable_model_pruning, \"\\t\\t\",\n+                  final_cfg.disable_model_pruning, \"\\n\");\n   for (auto& pair : user_cfg.toggle_config) {\n     if (pair.first == \"debug_stripper\" ||\n         pair.first == \"auto_mixed_precision\" ||\n@@ -735,7 +735,7 @@ void MetaOptimizer::PrintUserAndPluginConfigs(\n       // These optimizers are turned off by default.\n       // TODO(penporn): Remove the hard-coded length and change it to max length\n       // of all option strings.\n-      strings::StrAppend(\n+      absl::StrAppend(\n           &logs, pair.first, string(40 - pair.first.size(), ' '),\n           (pair.second == RewriterConfig::ON), \"\\t\\t\",\n           (plugin_cfg.toggle_config[pair.first] == RewriterConfig::ON), \"\\t\\t\",\n@@ -744,7 +744,7 @@ void MetaOptimizer::PrintUserAndPluginConfigs(\n       // These optimizers are turned on by default.\n       // TODO(penporn): Remove the hard-coded length and change it to max length\n       // of all option strings.\n-      strings::StrAppend(\n+      absl::StrAppend(\n           &logs, pair.first, string(40 - pair.first.size(), ' '),\n           (pair.second != RewriterConfig::OFF), \"\\t\\t\",\n           (plugin_cfg.toggle_config[pair.first] != RewriterConfig::OFF), \"\\t\\t\",\n@@ -826,8 +826,8 @@ absl::Status MetaOptimizer::OptimizeGraph(\n     VLOG(4) << \"Starting optimization iteration \" << iteration;\n     if (VLOG_IS_ON(4)) {\n       DumpGraphDefToFile(\n-          strings::StrCat(\"before_MetaOptimizer_iteration_\", iteration, \"_\",\n-                          reinterpret_cast<uintptr_t>(optimized_graph)),\n+          absl::StrCat(\"before_MetaOptimizer_iteration_\", iteration, \"_\",\n+                       reinterpret_cast<uintptr_t>(optimized_graph)),\n           *optimized_graph);\n     }\n \n@@ -852,9 +852,9 @@ absl::Status MetaOptimizer::OptimizeGraph(\n \n       if (VLOG_IS_ON(4)) {\n         DumpGraphDefToFile(\n-            strings::StrCat(\"after_MetaOptimizer_iteration_\", iteration, \"_\",\n-                            optimizer->name(), \"_\",\n-                            reinterpret_cast<uintptr_t>(optimized_graph)),\n+            absl::StrCat(\"after_MetaOptimizer_iteration_\", iteration, \"_\",\n+                         optimizer->name(), \"_\",\n+                         reinterpret_cast<uintptr_t>(optimized_graph)),\n             *optimized_graph);\n       }\n       for (const auto& verifier : inter_optimizer_verifiers) {\n@@ -864,8 +864,8 @@ absl::Status MetaOptimizer::OptimizeGraph(\n     }\n     if (VLOG_IS_ON(4)) {\n       DumpGraphDefToFile(\n-          strings::StrCat(\"after_MetaOptimizer_iteration_\", iteration, \"_\",\n-                          reinterpret_cast<uintptr_t>(optimized_graph)),\n+          absl::StrCat(\"after_MetaOptimizer_iteration_\", iteration, \"_\",\n+                       reinterpret_cast<uintptr_t>(optimized_graph)),\n           *optimized_graph);\n     }\n     // TODO(ashwinm): Need to enforce verification_deadline.\n@@ -953,21 +953,21 @@ absl::Status MetaOptimizer::RunOptimizer(\n     if (absl::IsAborted(status)) {\n       // By convention we (ab-)use the Aborted error code to signal that the\n       // optimizer returned without performing any changes to the graph.\n-      message = strings::StrCat(optimizer->name(),\n-                                \" did nothing. time = \", duration_ms, \"ms.\");\n+      message = absl::StrCat(optimizer->name(),\n+                             \" did nothing. time = \", duration_ms, \"ms.\");\n       // Swallow the non-critical error.\n       status = absl::OkStatus();\n     } else if (absl::IsDeadlineExceeded(status)) {\n       message =\n-          strings::StrCat(status.ToString(), \", time = \", duration_ms, \"ms.\");\n+          absl::StrCat(status.ToString(), \", time = \", duration_ms, \"ms.\");\n       LOG_EVERY_N_SEC(WARNING, 60)\n           << optimizer->name() << \" failed: \" << message;\n     } else {\n       message = status.ToString();\n       LOG_EVERY_N_SEC(ERROR, 60) << optimizer->name() << \" failed: \" << message;\n     }\n   } else {\n-    message = strings::StrCat(\n+    message = absl::StrCat(\n         PrintSizesBeforeAfter(optimized_item->graph, *optimized_graph),\n         \", time = \", duration_ms, \"ms.\");\n     VLOG(1) << optimizer->name() << \": \" << message;\n@@ -1313,8 +1313,8 @@ absl::Status MetaOptimizer::OptimizeConsumeItem(Cluster* cluster,\n   VLOG(3) << \"Optimized graph =\\n\" << optimized_graph->DebugString();\n   if (VLOG_IS_ON(1)) {\n     DumpGraphDefToFile(\n-        strings::StrCat(\"after_MetaOptimizer_\",\n-                        reinterpret_cast<uintptr_t>(optimized_graph)),\n+        absl::StrCat(\"after_MetaOptimizer_\",\n+                     reinterpret_cast<uintptr_t>(optimized_graph)),\n         *optimized_graph);\n   }\n "
        },
        {
            "sha": "2b9117dc53f44ecf4d68d6eef60a91ad97c43de7",
            "filename": "tensorflow/core/kernels/batching_util/batch_resource_base.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 7,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -136,18 +136,28 @@ void RecordInputBatchSize(int32_t batch_size, const string& model_name,\n   cell->GetCell(model_name, op_name)->Add(static_cast<double>(batch_size));\n }\n \n-void RecordInputBatchSizeV2(int32_t batch_size, const string& model_name,\n-                            const string& op_name) {\n-  static auto* cell = tensorflow::monitoring::Sampler<2>::New(\n+void RecordInputStatsV2(int32_t batch_size, const string& model_name,\n+                        const string& op_name,\n+                        const tsl::criticality::Criticality& criticality) {\n+  static auto* cell = tensorflow::monitoring::Sampler<3>::New(\n       {\"/tensorflow/serving/batching/input_batch_size_v2\",\n        \"Tracks the batch size distribution on the inputs by model_name (if \"\n        \"available).\",\n-       \"model_name\", \"op_name\"},\n+       \"model_name\", \"op_name\", \"criticality\"},\n       // Buckets centered at powers of 2, and have bounds:\n       // [(2/3) * 2^i, (4/3) * 2^i] for i = 0, ..., 13.\n       // Largest bucket has range: [(2/3) *  2^14, DBL_MAX]\n       monitoring::Buckets::Exponential(2.0 / 3.0, 2, 15));\n-  cell->GetCell(model_name, op_name)->Add(static_cast<double>(batch_size));\n+  const std::string criticality_str = absl::StrCat(criticality);\n+  cell->GetCell(model_name, op_name, criticality_str)\n+      ->Add(static_cast<double>(batch_size));\n+\n+  static auto* num_tasks_counter = tensorflow::monitoring::Counter<3>::New(\n+      \"/tensorflow/serving/batching/input_num_tasks\",\n+      \"Tracks the number of batches submitted to the batching scheduler.\",\n+      \"model_name\", \"op_name\", \"criticality\");\n+  num_tasks_counter->GetCell(model_name, op_name, criticality_str)\n+      ->IncrementBy(1);\n }\n \n // Record the actual batch size without padding.\n@@ -415,8 +425,9 @@ absl::Status BatchResourceBase::RegisterInput(\n   }\n   RecordInputBatchSize(tensors[0].shape().dim_size(0), GetModelName(context),\n                        context->op_kernel().name());\n-  RecordInputBatchSizeV2(tensors[0].shape().dim_size(0), GetModelName(context),\n-                         context->op_kernel().name());\n+  RecordInputStatsV2(tensors[0].shape().dim_size(0), GetModelName(context),\n+                     context->op_kernel().name(),\n+                     batch_components->criticality());\n   if (batcher_) {\n     RecordBatchParamBatchTimeoutMicros(\n         batcher_queue_options_.batch_timeout_micros, GetModelName(context),"
        },
        {
            "sha": "a6cece16d20ddf91536ba82ce8b03af4c2c5b5d6",
            "filename": "tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Fconcat_lib_gpu_impl.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Fconcat_lib_gpu_impl.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fconcat_lib_gpu_impl.cu.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -70,7 +70,10 @@ __global__ void concat_variable_kernel(\n   IntType num_inputs = input_ptr_data.size;\n \n   // verbose declaration needed due to template\n-  GPU_DYNAMIC_SHARED_MEM_DECL(8, unsigned char, smem);\n+  constexpr size_t kAlignTI =\n+      (alignof(T) > alignof(IntType)) ? alignof(T) : alignof(IntType);\n+  constexpr size_t kAlign = (kAlignTI < 16) ? 16 : kAlignTI;\n+  GPU_DYNAMIC_SHARED_MEM_DECL(kAlign, unsigned char, smem);\n   IntType* smem_col_scan = reinterpret_cast<IntType*>(smem);\n \n   if (useSmem) {"
        },
        {
            "sha": "af17a62947d66774564c1bdc674dfb4064390009",
            "filename": "tensorflow/core/kernels/linalg/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Flinalg%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Flinalg%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Flinalg%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -279,6 +279,7 @@ tf_kernel_library(\n         \"//tensorflow/core:framework\",\n         \"//tensorflow/core:lib\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/status\",\n         \"@eigen_archive//:eigen3\",\n     ],\n )"
        },
        {
            "sha": "bd9582d9a5cc10e183de0057a98e8b33ff9500a3",
            "filename": "tensorflow/core/kernels/linalg/cholesky_op_gpu.cu.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fcholesky_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fcholesky_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fcholesky_op_gpu.cu.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -195,7 +195,7 @@ class CholeskyOpGpu : public AsyncOpKernel {\n     auto info_checker = [context, done, n](\n                             const Status& status,\n                             const std::vector<HostLapackInfo>& host_infos) {\n-      if (!status.ok() && errors::IsInvalidArgument(status) &&\n+      if (!status.ok() && absl::IsInvalidArgument(status) &&\n           !host_infos.empty()) {\n         Tensor* output = context->mutable_output(0);\n         auto output_reshaped = output->template flat_inner_dims<Scalar, 3>();"
        },
        {
            "sha": "e4590f8def21dec001b738199bab4456c587c65e",
            "filename": "tensorflow/core/kernels/linalg/determinant_op.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fdeterminant_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fdeterminant_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fdeterminant_op.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -237,7 +237,7 @@ class DeterminantOpGpu : public AsyncOpKernel {\n     auto info_checker = [context, done](\n                             const Status& status,\n                             const std::vector<HostLapackInfo>& host_infos) {\n-      if (!status.ok() && errors::IsInvalidArgument(status) &&\n+      if (!status.ok() && absl::IsInvalidArgument(status) &&\n           !host_infos.empty()) {\n         for (int i = 0; i < host_infos[0].size(); ++i) {\n           // It is OK for a matrix to be singular (signaled by info > 0),\n@@ -383,7 +383,7 @@ class LogDeterminantOpGpu : public AsyncOpKernel {\n     auto info_checker = [context, done](\n                             const Status& status,\n                             const std::vector<HostLapackInfo>& host_infos) {\n-      if (!status.ok() && errors::IsInvalidArgument(status) &&\n+      if (!status.ok() && absl::IsInvalidArgument(status) &&\n           !host_infos.empty()) {\n         for (int i = 0; i < host_infos[0].size(); ++i) {\n           // It is OK for a matrix to be singular (signaled by info > 0),"
        },
        {
            "sha": "74004c7147a96902d75284e336f3e69b790dae93",
            "filename": "tensorflow/core/kernels/linalg/lu_op_gpu.cu.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Flinalg%2Flu_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Flinalg%2Flu_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Flinalg%2Flu_op_gpu.cu.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <algorithm>\n #include <vector>\n \n+#include \"absl/status/status.h\"\n #include \"unsupported/Eigen/CXX11/Tensor\"  // from @eigen_archive\n #include \"tensorflow/core/framework/kernel_def_builder.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n@@ -238,7 +239,7 @@ class LuOpGpu : public AsyncOpKernel {\n     auto info_checker = [context, done, dev_info](\n                             const Status& status,\n                             const std::vector<HostLapackInfo>& host_infos) {\n-      if (!status.ok() && errors::IsInvalidArgument(status) &&\n+      if (!status.ok() && absl::IsInvalidArgument(status) &&\n           !host_infos.empty()) {\n         for (int i = 0; i < host_infos[0].size(); ++i) {\n           // Match the CPU error message for singular matrices. Otherwise"
        },
        {
            "sha": "e542b838d3754c5f9a34d6b8543701e6ef0cb461",
            "filename": "tensorflow/core/kernels/linalg/matrix_inverse_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fmatrix_inverse_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fmatrix_inverse_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fmatrix_inverse_op.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -292,7 +292,7 @@ class MatrixInverseOpGpu : public AsyncOpKernel {\n     auto info_checker = [context, done](\n                             const Status& status,\n                             const std::vector<HostLapackInfo>& host_infos) {\n-      if (!status.ok() && errors::IsInvalidArgument(status)) {\n+      if (!status.ok() && absl::IsInvalidArgument(status)) {\n         for (const auto& host_info : host_infos) {\n           for (int i = 0; i < host_info.size(); ++i) {\n             // Match the CPU error message for singular matrices. Otherwise"
        },
        {
            "sha": "5b88e5d89a05b51aba30bb03dc4580b7a8f558f9",
            "filename": "tensorflow/core/kernels/linalg/matrix_solve_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fmatrix_solve_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fmatrix_solve_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fmatrix_solve_op.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -367,7 +367,7 @@ class MatrixSolveOpGpu : public AsyncOpKernel {\n     auto info_checker = [context, done, dev_info](\n                             const Status& status,\n                             const std::vector<HostLapackInfo>& host_infos) {\n-      if (!status.ok() && errors::IsInvalidArgument(status) &&\n+      if (!status.ok() && absl::IsInvalidArgument(status) &&\n           !host_infos.empty()) {\n         for (int i = 0; i < host_infos[0].size(); ++i) {\n           // Match the CPU error message for singular matrices. Otherwise"
        },
        {
            "sha": "a7aabe8f9fec1af5397f0aa6c6fb21103c76ad1f",
            "filename": "tensorflow/core/kernels/mlir_generated/build_defs.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fbuild_defs.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fbuild_defs.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fmlir_generated%2Fbuild_defs.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -11,6 +11,8 @@ load(\n     \"if_gpu_is_configured\",\n )\n load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n+load(\"@rules_cc//cc/common:cc_info.bzl\", \"CcInfo\")\n load(\"@rules_shell//shell:sh_test.bzl\", \"sh_test\")\n \n def _lookup_file(filegroup, path):"
        },
        {
            "sha": "b55845bb4e9f6f31a0aa1e81b5524d8ef39ce801",
            "filename": "tensorflow/core/kernels/split_lib_gpu.cu.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Fsplit_lib_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fkernels%2Fsplit_lib_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fsplit_lib_gpu.cu.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -120,7 +120,10 @@ __global__ void split_v_kernel(const T* __restrict__ input_ptr,\n   int num_outputs = output_ptr_data.size;\n \n   // verbose declaration needed due to template\n-  GPU_DYNAMIC_SHARED_MEM_DECL(2, unsigned char, smem);\n+  constexpr size_t kAlignTI =\n+      (alignof(T) > alignof(IntType)) ? alignof(T) : alignof(IntType);\n+  constexpr size_t kAlign = (kAlignTI < 16) ? 16 : kAlignTI;\n+  GPU_DYNAMIC_SHARED_MEM_DECL(kAlign, unsigned char, smem);\n   IntType* smem_col_scan = reinterpret_cast<IntType*>(smem);\n \n   if (useSmem) {"
        },
        {
            "sha": "8c27edc30dcc7c097bbb1a28649d60c093070b0e",
            "filename": "tensorflow/core/lib/gif/BUILD",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Flib%2Fgif%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Flib%2Fgif%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Flib%2Fgif%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -53,18 +53,26 @@ cc_library(\n         \"//tensorflow/core/platform:gif_internal_hdrs\",\n         \"@local_tsl//tsl/platform:gif_internal_hdrs\",\n         \"@local_xla//xla/tsl/platform/default:integral_types.h\",\n-        \"@local_xla//xla/tsl/platform/default:logging.h\",\n     ],\n     copts = tf_copts(),\n     features = [\"-layering_check\"],\n     linkopts = if_android([\"-ldl\"]),\n     deps = [\n+        \"//tensorflow/core/platform:byte_order\",\n+        \"//tensorflow/core/platform:cord\",\n         \"//tensorflow/core/platform:dynamic_annotations\",\n         \"//tensorflow/core/platform:gif\",\n         \"//tensorflow/core/platform:logging\",\n         \"//tensorflow/core/platform:stringpiece\",\n         \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/base:dynamic_annotations\",\n         \"@com_google_absl//absl/strings\",\n+        \"@eigen_archive//:eigen3\",\n+        \"@gif\",\n+        \"@local_tsl//tsl/platform:logging\",\n+        \"@local_tsl//tsl/platform:tstring\",\n+        \"@local_tsl//tsl/platform:types\",\n+        \"@local_xla//xla/tsl/platform:types\",\n     ],\n )\n "
        },
        {
            "sha": "628b027ad9f6581b57106d2c5808f4ec62166110",
            "filename": "tensorflow/core/lib/jpeg/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Flib%2Fjpeg%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Flib%2Fjpeg%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Flib%2Fjpeg%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -62,7 +62,6 @@ cc_library(\n         \"//tensorflow/core/platform:jpeg_internal_hdrs\",\n         \"@local_tsl//tsl/platform:jpeg_internal_hdrs\",\n         \"@local_xla//xla/tsl/platform/default:integral_types.h\",\n-        \"@local_xla//xla/tsl/platform/default:logging.h\",\n     ],\n     copts = tf_copts(),\n     linkopts = if_android([\"-ldl\"]),"
        },
        {
            "sha": "21e07095b6ba80462dfb671cffd7119fe06814ac",
            "filename": "tensorflow/core/platform/build_config.bzl",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fplatform%2Fbuild_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fplatform%2Fbuild_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fplatform%2Fbuild_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -17,7 +17,6 @@ load(\n     _tf_google_mobile_srcs_only_runtime = \"tf_google_mobile_srcs_only_runtime\",\n     _tf_jspb_proto_library = \"tf_jspb_proto_library\",\n     _tf_lib_proto_parsing_deps = \"tf_lib_proto_parsing_deps\",\n-    _tf_logging_deps = \"tf_logging_deps\",\n     _tf_platform_alias = \"tf_platform_alias\",\n     _tf_platform_deps = \"tf_platform_deps\",\n     _tf_portable_deps_no_runtime = \"tf_portable_deps_no_runtime\",\n@@ -58,7 +57,6 @@ tf_google_mobile_srcs_no_runtime = _tf_google_mobile_srcs_no_runtime\n tf_google_mobile_srcs_only_runtime = _tf_google_mobile_srcs_only_runtime\n tf_jspb_proto_library = _tf_jspb_proto_library\n tf_lib_proto_parsing_deps = _tf_lib_proto_parsing_deps\n-tf_logging_deps = _tf_logging_deps\n tf_platform_alias = _tf_platform_alias\n tf_platform_deps = _tf_platform_deps\n tf_portable_proto_lib = _tf_portable_proto_lib"
        },
        {
            "sha": "c677af2376ae35cd38c56f35b3f72d45c18e7524",
            "filename": "tensorflow/core/public/version.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fpublic%2Fversion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Fpublic%2Fversion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fpublic%2Fversion.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -93,7 +93,7 @@ limitations under the License.\n \n #define TF_GRAPH_DEF_VERSION_MIN_PRODUCER 0\n #define TF_GRAPH_DEF_VERSION_MIN_CONSUMER 0\n-#define TF_GRAPH_DEF_VERSION 2330  // Updated: 2025/8/25\n+#define TF_GRAPH_DEF_VERSION 2345  // Updated: 2025/9/9\n \n // Checkpoint compatibility versions (the versions field in SavedSliceMeta).\n //"
        },
        {
            "sha": "adcf527f186750cf2507066a28abed51c4e58984",
            "filename": "tensorflow/core/tfrt/common/create_pjrt_client_util_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fcommon%2Fcreate_pjrt_client_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fcommon%2Fcreate_pjrt_client_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Fcommon%2Fcreate_pjrt_client_util_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -26,7 +26,6 @@ namespace tensorflow {\n namespace {\n \n using ::testing::HasSubstr;\n-using ::tsl::testing::StatusIs;\n \n TEST(CreatePjRtClientTest, GetNotExistPjRtClientNotImplemented) {\n   EXPECT_THAT(GetOrCreatePjRtClient(DEVICE_CPU),"
        },
        {
            "sha": "9f71bde2e9e410e2c766e7d867bea4d4ed12330e",
            "filename": "tensorflow/core/tfrt/fallback/fallback_state_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Ffallback%2Ffallback_state_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Ffallback%2Ffallback_state_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Ffallback%2Ffallback_state_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -36,7 +36,6 @@ limitations under the License.\n namespace tensorflow {\n namespace {\n \n-using ::tensorflow::testing::StatusIs;\n using ::testing::HasSubstr;\n using ::testing::Not;\n "
        },
        {
            "sha": "f8ebd91f053f72e3e23ef78dac4fcd89b9470748",
            "filename": "tensorflow/core/tfrt/fallback/op_kernel_runner.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Ffallback%2Fop_kernel_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Ffallback%2Fop_kernel_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Ffallback%2Fop_kernel_runner.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -148,17 +148,14 @@ absl::StatusOr<OpKernelRunner> OpKernelRunner::Create(\n     return absl::InternalError(\n         absl::StrCat(\"Failed to create OpKernel for op: \", op_name));\n   }\n-  return OpKernelRunner(op_name, device, function_library_runtime,\n-                        std::move(op_kernel));\n+  return OpKernelRunner(device, function_library_runtime, std::move(op_kernel));\n }\n \n OpKernelRunner::OpKernelRunner(\n-    absl::string_view op_name, tensorflow::Device* device,\n+    tensorflow::Device* device,\n     tensorflow::FunctionLibraryRuntime* function_library_runtime,\n     std::unique_ptr<tensorflow::OpKernel> op_kernel)\n-    : op_kernel_(std::move(op_kernel)),\n-      info_(std::make_unique<Info>()),\n-      op_name_(op_name) {\n+    : op_kernel_(std::move(op_kernel)), info_(std::make_unique<Info>()) {\n   DCHECK(device);\n   DCHECK(function_library_runtime);\n "
        },
        {
            "sha": "44d94db09f48756a7d8291b0ef290b9aba21b9c7",
            "filename": "tensorflow/core/tfrt/fallback/op_kernel_runner.h",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Ffallback%2Fop_kernel_runner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Ffallback%2Fop_kernel_runner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Ffallback%2Fop_kernel_runner.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -129,7 +129,7 @@ class OpKernelRunner {\n \n  private:\n   explicit OpKernelRunner(\n-      absl::string_view op_name, tensorflow::Device* device,\n+      tensorflow::Device* device,\n       tensorflow::FunctionLibraryRuntime* function_library_runtime,\n       std::unique_ptr<OpKernel> op_kernel);\n \n@@ -146,8 +146,6 @@ class OpKernelRunner {\n   absl::Span<const AllocatorAttributes> input_alloc_attrs_;\n   std::unique_ptr<Info> info_;\n   absl::Span<const AllocatorAttributes> output_alloc_attrs_;\n-\n-  std::string op_name_;\n };\n \n // OpKernelRunState keeps the states needed for per-kernel execution."
        },
        {
            "sha": "dd56183438284e9ceab91d931d849eb79eeb1194",
            "filename": "tensorflow/core/tfrt/graph_executor/graph_executor.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fgraph_executor%2Fgraph_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fgraph_executor%2Fgraph_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Fgraph_executor%2Fgraph_executor.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -252,8 +252,9 @@ absl::StatusOr<std::unique_ptr<RequestInfo>> CreateRequestInfo(\n   } else {\n     request_id = GetNextStepId().id;\n     // Otherwise we use the global queue in `runtime`.\n-    TF_ASSIGN_OR_RETURN(request_info->request_queue_owner,\n-                        runtime.CreateRequestQueue(request_id));\n+    TF_ASSIGN_OR_RETURN(\n+        request_info->request_queue_owner,\n+        runtime.CreateRequestQueue(request_id, run_options.priority));\n     request_info->request_queue = request_info->request_queue_owner.get();\n   }\n   auto* request_queue = request_info->request_queue;"
        },
        {
            "sha": "cb7925b88e6de10b76058054526a7982642c10ae",
            "filename": "tensorflow/core/tfrt/ifrt/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fifrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fifrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Fifrt%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -6,6 +6,7 @@ package(\n     # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n     default_visibility = [\n         # copybara:uncomment \"//learning/brain/tfrt:__subpackages__\",\n+        # copybara:uncomment \"//learning/infra/mira/experimental/orbax_model:__subpackages__\",\n         \"//tensorflow/compiler/mlir/tfrt:__subpackages__\",\n         \"//tensorflow/core/tfrt:__subpackages__\",\n         \"//tensorflow/core/tfrt/mlrt:__subpackages__\","
        },
        {
            "sha": "1c1b4cbc21d65782f5e957689b880ad0c85406a9",
            "filename": "tensorflow/core/tfrt/run_handler_thread_pool/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Frun_handler_thread_pool%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Frun_handler_thread_pool%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Frun_handler_thread_pool%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -62,8 +62,8 @@ cc_library(\n         \"//tensorflow/core/tfrt/runtime:work_queue_interface\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/strings\",\n         \"@eigen_archive//:eigen3\",\n-        \"@local_xla//xla/tsl/platform:env\",\n         \"@tf_runtime//:hostcontext\",\n     ],\n )"
        },
        {
            "sha": "11bca25cb28891300a6921dfd4605a6a06901746",
            "filename": "tensorflow/core/tfrt/run_handler_thread_pool/run_handler.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Frun_handler_thread_pool%2Frun_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Frun_handler_thread_pool%2Frun_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Frun_handler_thread_pool%2Frun_handler.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n \n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n+#include \"absl/strings/str_cat.h\"\n #define EIGEN_USE_THREADS\n \n #include <optional>\n@@ -339,12 +340,11 @@ unsigned ThreadWorkSource::NonBlockingWorkShardingFactor() {\n }\n \n std::string ThreadWorkSource::ToString() {\n-  return tensorflow::strings::StrCat(\n-      \"traceme_id = \", GetTracemeId(),\n-      \", inter queue size = \", TaskQueueSize(true),\n-      \", inter inflight = \", GetInflightTaskCount(true),\n-      \", intra queue size = \", TaskQueueSize(false),\n-      \", intra inflight = \", GetInflightTaskCount(false));\n+  return absl::StrCat(\"traceme_id = \", GetTracemeId(),\n+                      \", inter queue size = \", TaskQueueSize(true),\n+                      \", inter inflight = \", GetInflightTaskCount(true),\n+                      \", intra queue size = \", TaskQueueSize(false),\n+                      \", intra inflight = \", GetInflightTaskCount(false));\n }\n \n RunHandlerThreadPool::RunHandlerThreadPool(\n@@ -990,9 +990,9 @@ void RunHandlerPool::Impl::LogInfo() {\n         ids_str += \" \";\n       }\n \n-      times_str += tensorflow::strings::StrCat(\n-          (now - (*it)->start_time_us()) / 1000.0, \" ms.\");\n-      ids_str += tensorflow::strings::StrCat((*it)->tws()->GetTracemeId());\n+      absl::StrAppend(&times_str, (now - (*it)->start_time_us()) / 1000.0,\n+                      \" ms.\");\n+      absl::StrAppend(&ids_str, (*it)->tws()->GetTracemeId());\n       ++it;\n     }\n     VLOG(1) << \"Elapsed times are: \" << times_str;"
        },
        {
            "sha": "83ca5b1aa7e11253843b1bdc2c0d245b8d852688",
            "filename": "tensorflow/core/tfrt/run_handler_thread_pool/run_handler_concurrent_work_queue.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Frun_handler_thread_pool%2Frun_handler_concurrent_work_queue.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Frun_handler_thread_pool%2Frun_handler_concurrent_work_queue.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Frun_handler_thread_pool%2Frun_handler_concurrent_work_queue.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -65,8 +65,12 @@ RunHandlerThreadWorkQueue::RunHandlerThreadWorkQueue(const Options& options)\n }\n \n absl::StatusOr<std::unique_ptr<tensorflow::tfrt_stub::WorkQueueInterface>>\n-RunHandlerThreadWorkQueue::InitializeRequest(int64_t request_id) const {\n+RunHandlerThreadWorkQueue::InitializeRequest(int64_t request_id,\n+                                             int priority) const {\n   RunHandlerOptions options;\n+  if (options_.enable_priority_based_queuing) {\n+    options.priority = priority;\n+  }\n   std::unique_ptr<RunHandler> handler =\n       handler_pool_->Get(request_id, options_.init_timeout_ms, options);\n   if (!handler) {\n@@ -131,7 +135,9 @@ std::ostream& operator<<(std::ostream& strm,\n               << options.use_adaptive_waiting_time\n               << \", wait_if_no_active_request = \"\n               << options.wait_if_no_active_request\n-              << \", enable_wake_up = \" << options.enable_wake_up << \"}\";\n+              << \", enable_wake_up = \" << options.enable_wake_up\n+              << \", enable_priority_based_queuing = \"\n+              << options.enable_priority_based_queuing << \" }\";\n }\n \n }  // namespace tf"
        },
        {
            "sha": "bfef743d110635251d43a3f00620d178400162df",
            "filename": "tensorflow/core/tfrt/run_handler_thread_pool/run_handler_concurrent_work_queue.h",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Frun_handler_thread_pool%2Frun_handler_concurrent_work_queue.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Frun_handler_thread_pool%2Frun_handler_concurrent_work_queue.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Frun_handler_thread_pool%2Frun_handler_concurrent_work_queue.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -81,6 +81,9 @@ class RunHandlerThreadWorkQueue\n \n     // If true, threads will be waken up by new tasks.\n     bool enable_wake_up = true;\n+\n+    // If true, enables priority based queuing of requests.\n+    bool enable_priority_based_queuing = false;\n   };\n \n   explicit RunHandlerThreadWorkQueue(const Options& options);\n@@ -94,7 +97,7 @@ class RunHandlerThreadWorkQueue\n   }\n \n   absl::StatusOr<std::unique_ptr<tensorflow::tfrt_stub::WorkQueueInterface>>\n-  InitializeRequest(int64_t request_id) const override;\n+  InitializeRequest(int64_t request_id, int priority) const override;\n \n   int GetParallelismLevel() const override {\n     return options_.num_main_threads + options_.num_complementary_threads;"
        },
        {
            "sha": "383a229f984cf0cd3e4e66fefaa6a602a444626b",
            "filename": "tensorflow/core/tfrt/run_handler_thread_pool/run_handler_concurrent_work_queue_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Frun_handler_thread_pool%2Frun_handler_concurrent_work_queue_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Frun_handler_thread_pool%2Frun_handler_concurrent_work_queue_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Frun_handler_thread_pool%2Frun_handler_concurrent_work_queue_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -61,7 +61,7 @@ class RunHandlerThreadWorkQueueTest : public ::testing::Test {\n                                           std::move(work_queue));\n     RequestContextBuilder req_ctx_builder{host_.get(),\n                                           /*resource_context=*/nullptr};\n-    auto queue = pool_->InitializeRequest(/*request_id=*/100);\n+    auto queue = pool_->InitializeRequest(/*request_id=*/100, /*priority=*/0);\n     TF_CHECK_OK(queue.status());\n     queue_ = std::move(*queue);\n     auto req_ctx = std::move(req_ctx_builder).build();\n@@ -183,7 +183,7 @@ TEST_F(RunHandlerThreadWorkQueueTest, NoHandlerReturnsError) {\n   auto queue = std::make_unique<RunHandlerThreadWorkQueue>(options);\n   tfrt::RequestContextBuilder ctx_builder(nullptr, nullptr);\n   EXPECT_THAT(\n-      queue->InitializeRequest(/*request_id=*/100),\n+      queue->InitializeRequest(/*request_id=*/100, /*priority=*/0),\n       absl_testing::StatusIs(\n           absl::StatusCode::kDeadlineExceeded,\n           \"Could not obtain RunHandler for request after waiting for 1 ms.\"));"
        },
        {
            "sha": "208363a315f6ce42a8cfd3c228ec7179250f49e7",
            "filename": "tensorflow/core/tfrt/runtime/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Fruntime%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -115,7 +115,6 @@ cc_library(\n         \"//tensorflow/core/tfrt/utils:thread_pool\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/status:statusor\",\n-        \"@llvm-project//llvm:Support\",\n         \"@tf_runtime//:hostcontext\",\n         \"@tf_runtime//:support\",\n     ],\n@@ -189,14 +188,11 @@ tf_cc_test(\n     deps = [\n         \":tf_threadpool_concurrent_work_queue\",\n         \"//tensorflow/core:framework_internal\",\n-        \"//tensorflow/core:lib\",\n         \"//tensorflow/core:test\",\n         \"//tensorflow/core:test_main\",\n         \"//tensorflow/core/platform:errors\",\n         \"//tensorflow/core/platform:status_matchers\",\n         \"//tensorflow/core/tfrt/utils:thread_pool\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/time\",\n         \"@com_google_googletest//:gtest\",\n         \"@tf_runtime//:hostcontext\",\n         \"@tf_runtime//:support\","
        },
        {
            "sha": "19c8ecbd31e58b7277b7830cf5bfdb6907984d2b",
            "filename": "tensorflow/core/tfrt/runtime/runtime.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Fruntime.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Fruntime.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Fruntime.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -218,12 +218,12 @@ class Runtime {\n \n   // Creates a work queue for a request.\n   absl::StatusOr<std::unique_ptr<WorkQueueInterface>> CreateRequestQueue(\n-      int64_t request_id) const {\n+      int64_t request_id, int priority) const {\n     if (create_request_queue_fn_) {\n       return create_request_queue_fn_(request_id);\n     }\n \n-    return work_queue_->InitializeRequest(request_id);\n+    return work_queue_->InitializeRequest(request_id, priority);\n   }\n \n  private:"
        },
        {
            "sha": "4659006f649fa9782776eacba0d93da4315941f3",
            "filename": "tensorflow/core/tfrt/runtime/tf_threadpool_concurrent_work_queue.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Ftf_threadpool_concurrent_work_queue.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Ftf_threadpool_concurrent_work_queue.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Ftf_threadpool_concurrent_work_queue.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -37,7 +37,8 @@ namespace tfrt_stub {\n using ::tensorflow::thread::ThreadPoolInterface;\n \n absl::StatusOr<std::unique_ptr<WorkQueueInterface>>\n-TfThreadPoolWorkQueue::InitializeRequest(int64_t request_id) const {\n+TfThreadPoolWorkQueue::InitializeRequest(int64_t request_id,\n+                                         int priority) const {\n   return {std::make_unique<TfThreadPoolWorkQueue>(\n       request_id, intra_op_threadpool_, inter_op_threadpool_)};\n }"
        },
        {
            "sha": "6d04169aea8cfe530461017222c55614bfe9e82b",
            "filename": "tensorflow/core/tfrt/runtime/tf_threadpool_concurrent_work_queue.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Ftf_threadpool_concurrent_work_queue.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Ftf_threadpool_concurrent_work_queue.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Ftf_threadpool_concurrent_work_queue.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -53,7 +53,7 @@ class TfThreadPoolWorkQueue : public WorkQueueInterface {\n         inter_op_threadpool_(inter_op_threadpool) {}\n \n   absl::StatusOr<std::unique_ptr<WorkQueueInterface>> InitializeRequest(\n-      int64_t request_id) const override;\n+      int64_t request_id, int priority) const override;\n \n   int GetParallelismLevel() const override {\n     return inter_op_threadpool_->NumThreads();"
        },
        {
            "sha": "bc7f37654ea1a94830ef567e99057bd406f42581",
            "filename": "tensorflow/core/tfrt/runtime/tf_threadpool_concurrent_work_queue_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Ftf_threadpool_concurrent_work_queue_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Ftf_threadpool_concurrent_work_queue_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Ftf_threadpool_concurrent_work_queue_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -52,7 +52,8 @@ TEST_F(TfThreadpoolWorkQueueTest, GetNameOk) {\n TEST_F(TfThreadpoolWorkQueueTest, InitializeRequestOk) {\n   tfrt::RequestContextBuilder ctx_builder(/*host=*/nullptr,\n                                           /*resource_context=*/nullptr);\n-  auto queue = tf_threadpool_cwq_->InitializeRequest(/*request_id=*/0);\n+  auto queue =\n+      tf_threadpool_cwq_->InitializeRequest(/*request_id=*/0, /*priority=*/0);\n   TF_ASSERT_OK(queue.status());\n   EXPECT_NE(*queue, nullptr);\n   EXPECT_NE((*queue)->GetIntraOpThreadPool(), nullptr);"
        },
        {
            "sha": "40f15e4e97ffd0b43fb6c9cc949ce26e755ca459",
            "filename": "tensorflow/core/tfrt/runtime/work_queue_interface.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Fwork_queue_interface.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Fwork_queue_interface.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Fwork_queue_interface.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -78,7 +78,7 @@ class DefaultWorkQueueWrapper : public WorkQueueInterface {\n   }\n \n   absl::StatusOr<std::unique_ptr<WorkQueueInterface>> InitializeRequest(\n-      int64_t request_id) const override {\n+      int64_t request_id, int priority) const override {\n     return {std::make_unique<DefaultWorkQueueWrapper>(request_id, work_queue_,\n                                                       GetIntraOpThreadPool())};\n   }"
        },
        {
            "sha": "11e09d622c3354122f01bc17262c4ac689d13a36",
            "filename": "tensorflow/core/tfrt/runtime/work_queue_interface.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Fwork_queue_interface.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Fwork_queue_interface.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Fwork_queue_interface.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -61,7 +61,7 @@ class WorkQueueInterface : public tfrt::ConcurrentWorkQueue {\n   // should be handled separately.\n   ABSL_DEPRECATED(\"Create the instance directly instead.\")\n   virtual absl::StatusOr<std::unique_ptr<WorkQueueInterface>> InitializeRequest(\n-      int64_t request_id) const {\n+      int64_t request_id, int priority) const {\n     return {nullptr};\n   }\n "
        },
        {
            "sha": "408475eb054bf6197f011ad04fb40852ba025e20",
            "filename": "tensorflow/core/tfrt/runtime/work_queue_interface_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Fwork_queue_interface_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Fwork_queue_interface_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Fruntime%2Fwork_queue_interface_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -86,7 +86,7 @@ TEST(DefaultWorkQueueWrapperTest, IntraOpThreadPool) {\n       WrapDefaultWorkQueue(std::move(work_queue), &intra_op_thread_pool);\n \n   TF_ASSERT_OK_AND_ASSIGN(auto queue, work_queue_wrapper->InitializeRequest(\n-                                          /*request_id=*/0));\n+                                          /*request_id=*/0, /*priority=*/0));\n   EXPECT_NE(queue, nullptr);\n   EXPECT_EQ(queue->GetIntraOpThreadPool(), &intra_op_thread_pool);\n }"
        },
        {
            "sha": "35e1d22295f26193bde085e9133a439a822a7953",
            "filename": "tensorflow/core/tfrt/saved_model/saved_model.cc",
            "status": "modified",
            "additions": 59,
            "deletions": 36,
            "changes": 95,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fsaved_model%2Fsaved_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftfrt%2Fsaved_model%2Fsaved_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Fsaved_model%2Fsaved_model.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -287,20 +287,24 @@ absl::Status RunBefInitializers(\n absl::Status IsInputSpecsCorrect(absl::string_view name,\n                                  const internal::Signature& signature,\n                                  absl::Span<const tensorflow::Tensor> inputs) {\n-  TF_RET_CHECK(signature.input_specs.size() == inputs.size())\n-      << \"signature \" << name\n-      << \" input size is wrong, expected: \" << signature.input_specs.size()\n-      << \", actual: \" << inputs.size();\n+  if (signature.input_specs.size() != inputs.size()) {\n+    return absl::InvalidArgumentError(absl::StrCat(\n+        \"signature \", name, \" input size is wrong, expected: \",\n+        signature.input_specs.size(), \", actual: \", inputs.size()));\n+  }\n   for (size_t i = 0; i < inputs.size(); ++i) {\n     const auto& expected_input_spec = signature.input_specs[i];\n-    TF_RET_CHECK(expected_input_spec.dtype == inputs[i].dtype())\n-        << \"signature \" << name\n-        << \" input dtype is wrong, expected: \" << expected_input_spec.dtype\n-        << \", actual: \" << inputs[i].dtype();\n-    TF_RET_CHECK(expected_input_spec.shape.IsCompatibleWith(inputs[i].shape()))\n-        << \"signature \" << name\n-        << \" input shape is wrong, expected : \" << expected_input_spec.shape\n-        << \", actual: \" << inputs[i].shape();\n+    if (expected_input_spec.dtype != inputs[i].dtype()) {\n+      return absl::InvalidArgumentError(absl::StrCat(\n+          \"signature \", name, \" input dtype is wrong, expected: \",\n+          expected_input_spec.dtype, \", actual: \", inputs[i].dtype()));\n+    }\n+    if (!expected_input_spec.shape.IsCompatibleWith(inputs[i].shape())) {\n+      return absl::InvalidArgumentError(\n+          absl::StrCat(\"signature \", name, \" input shape is wrong, expected : \",\n+                       expected_input_spec.shape.DebugString(),\n+                       \", actual: \", inputs[i].shape().DebugString()));\n+    }\n   }\n   return absl::OkStatus();\n }\n@@ -348,10 +352,11 @@ absl::Status PreprocessSignature(\n   TF_RETURN_IF_ERROR(CheckInputSpecs(model_metadata, run_options,\n                                      signature_name, signature, input_tensors));\n \n-  TF_RET_CHECK(input_tensors.size() == signature_def.inputs().size())\n-      << \"Incorrect input size for signature: \" << signature_name\n-      << \": expected \" << signature_def.inputs().size() << \", but got \"\n-      << input_tensors.size();\n+  if (input_tensors.size() != signature_def.inputs().size()) {\n+    return absl::InvalidArgumentError(absl::StrCat(\n+        \"Incorrect input size for signature: \", signature_name, \": expected \",\n+        signature_def.inputs().size(), \", but got \", input_tensors.size()));\n+  }\n   DCHECK_EQ(input_names.size(), signature_def.inputs().size());\n \n   // Then we find out the corresponding tensor names (ie.\n@@ -365,9 +370,11 @@ absl::Status PreprocessSignature(\n     // TODO(b/184675681): Support other encoding cases.\n     //\n     // TODO(b/184679394): Add unit test for this check.\n-    TF_RET_CHECK(tensor_info.encoding_case() == tensorflow::TensorInfo::kName)\n-        << \"Only dense tensor is supported, but got encoding case \"\n-        << tensor_info.encoding_case();\n+    if (tensor_info.encoding_case() != tensorflow::TensorInfo::kName) {\n+      return absl::UnimplementedError(\n+          absl::StrCat(\"Only dense tensor is supported, but got encoding case \",\n+                       tensor_info.encoding_case()));\n+    }\n \n     const auto& tensor_name = tensor_info.name();\n \n@@ -387,9 +394,11 @@ absl::Status PreprocessSignature(\n     VLOG(1) << \"Importing Signature Output: output_key = \" << output_key\n             << \", tensor_info = \" << tensor_info.DebugString();\n \n-    TF_RET_CHECK(tensor_info.encoding_case() == tensorflow::TensorInfo::kName)\n-        << \"Only dense tensor is supported, but got encoding case \"\n-        << tensor_info.encoding_case();\n+    if (tensor_info.encoding_case() != tensorflow::TensorInfo::kName) {\n+      return absl::UnimplementedError(\n+          absl::StrCat(\"Only dense tensor is supported, but got encoding case \",\n+                       tensor_info.encoding_case()));\n+    }\n \n     output_tensor_names.push_back(tensor_info.name());\n   }\n@@ -927,12 +936,16 @@ absl::Status SavedModelImpl::Run(const RunOptions& run_options,\n                                  absl::string_view name,\n                                  absl::Span<const tensorflow::Tensor> inputs,\n                                  std::vector<tensorflow::Tensor>* outputs) {\n-  TF_RET_CHECK(outputs) << \"outputs must be provided\";\n+  if (!outputs) {\n+    return absl::InvalidArgumentError(\"outputs must be provided\");\n+  }\n   outputs->clear();\n \n   auto sig_iter = signatures_.find(name);\n-  TF_RET_CHECK(sig_iter != signatures_.end())\n-      << \"failed to find signature \" << name << \" in the graph\";\n+  if (sig_iter == signatures_.end()) {\n+    return absl::NotFoundError(\n+        absl::StrCat(\"failed to find signature \", name, \" in the graph\"));\n+  }\n   const auto& signature = sig_iter->second;\n   const auto& signature_def = meta_graph_def_.signature_def().at(name);\n   const tensorflow::SessionMetadata& model_metadata =\n@@ -1035,9 +1048,13 @@ absl::Status SavedModelImpl::RunMultipleSignatures(\n     const RunOptions& run_options, absl::Span<const std::string> names,\n     absl::Span<const std::vector<tensorflow::Tensor>> multi_inputs,\n     std::vector<std::vector<tensorflow::Tensor>>* multi_outputs) {\n-  TF_RET_CHECK(names.size() == multi_inputs.size())\n-      << \"the sizes of names and inputs should be the same\";\n-  TF_RET_CHECK(multi_outputs) << \"outputs must be provided\";\n+  if (names.size() != multi_inputs.size()) {\n+    return absl::InvalidArgumentError(\n+        \"the sizes of names and inputs should be the same\");\n+  }\n+  if (!multi_outputs) {\n+    return absl::InvalidArgumentError(\"outputs must be provided\");\n+  }\n   multi_outputs->clear();\n \n   // Due to possible overlapping of feed nodes among user-specified inputs, We\n@@ -1059,8 +1076,10 @@ absl::Status SavedModelImpl::RunMultipleSignatures(\n     auto sig_iter = signature_defs.find(signature_name);\n \n     // Early out if any signature can't be found.\n-    TF_RET_CHECK(sig_iter != signature_defs.end())\n-        << \"failed to find signature in the graph\";\n+    if (sig_iter == signature_defs.end()) {\n+      return absl::NotFoundError(absl::StrCat(\"failed to find signature \",\n+                                              signature_name, \" in the graph\"));\n+    }\n     const auto& signature_def = sig_iter->second;\n \n     // `signatures_` keeps the user-specified input names that is in the same\n@@ -1173,9 +1192,11 @@ absl::StatusOr<JoinedSignature> JoinSignatures(\n       // TODO(b/184675681): Support other encoding cases.\n       //\n       // TODO(b/184679394): Add unit test for this check.\n-      TF_RET_CHECK(tensor_info.encoding_case() == tensorflow::TensorInfo::kName)\n-          << \"Only dense tensor is supported, but got encoding case \"\n-          << tensor_info.encoding_case();\n+      if (tensor_info.encoding_case() != tensorflow::TensorInfo::kName) {\n+        return absl::UnimplementedError(absl::StrCat(\n+            \"Only dense tensor is supported, but got encoding case \",\n+            tensor_info.encoding_case()));\n+      }\n \n       VLOG(1) << \"Importing Signature Input: input_key = \" << iter.first\n               << \", tensor_info = \" << tensor_info.DebugString();\n@@ -1205,9 +1226,11 @@ absl::StatusOr<JoinedSignature> JoinSignatures(\n       VLOG(1) << \"Importing Signature Output: output_key = \" << output_key\n               << \", tensor_info = \" << tensor_info.DebugString();\n \n-      TF_RET_CHECK(tensor_info.encoding_case() == tensorflow::TensorInfo::kName)\n-          << \"Only dense tensor is supported, but got encoding case \"\n-          << tensor_info.encoding_case();\n+      if (tensor_info.encoding_case() != tensorflow::TensorInfo::kName) {\n+        return absl::UnimplementedError(absl::StrCat(\n+            \"Only dense tensor is supported, but got encoding case \",\n+            tensor_info.encoding_case()));\n+      }\n \n       joined_signature.output_nodes.push_back(tensor_info.name());\n     }"
        },
        {
            "sha": "f4fcbb8f54605704384e2991a7dbacbe2f136996",
            "filename": "tensorflow/core/tpu/kernels/host_compute_ops.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fhost_compute_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fhost_compute_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fhost_compute_ops.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -142,7 +142,7 @@ class RecvAtHostOp : public AsyncOpKernel {\n       rendezvous_key[i] = Rendezvous::CreateKey(\n           device_ordinal_is_attr ? remote_device_ : remote_device,\n           /*src_incarnation=*/1, cpu_device_,\n-          strings::StrCat(rendezvous_key_base, key_, \"_dtoh_\", i),\n+          absl::StrCat(rendezvous_key_base, key_, \"_dtoh_\", i),\n           FrameAndIter(0, 0));\n \n       OP_REQUIRES_OK_ASYNC(\n@@ -301,7 +301,7 @@ class SendFromHostOp : public OpKernel {\n       const string& rendezvous_key = Rendezvous::CreateKey(\n           cpu_device_, /*src_incarnation=*/1,\n           device_ordinal_is_attr ? remote_device_ : remote_device,\n-          strings::StrCat(rendezvous_key_base, key_, \"_htod_\", i),\n+          absl::StrCat(rendezvous_key_base, key_, \"_htod_\", i),\n           FrameAndIter(0, 0));\n \n       Rendezvous::ParsedKey parsed_key;"
        },
        {
            "sha": "fb288f967bc6ed8da960d3949e6dda69923a4555",
            "filename": "tensorflow/core/tpu/ops/sparse_core_ops.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 7,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftpu%2Fops%2Fsparse_core_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fcore%2Ftpu%2Fops%2Fsparse_core_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fops%2Fsparse_core_ops.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -963,12 +963,21 @@ REGISTER_OP(\"XlaSparseActivationsUnstack\")\n             absl::StrFormat(\"Invalid number of features. Expected: %d, got: %d\",\n                             num_tables, features.size()));\n       }\n+      DataType input_dtype;\n+      TF_RETURN_IF_ERROR(c->GetAttr(\"input_dtype\", &input_dtype));\n+      if (input_dtype == DT_BFLOAT16) {\n+        return absl::InvalidArgumentError(absl::StrFormat(\n+            \"Unsupported input dtype for stacked activations: %s\",\n+            DataType_Name(input_dtype)));\n+      }\n       DataType dtype;\n-      TF_RETURN_IF_ERROR(c->GetAttr(\"input_dtype\", &dtype));\n-      if (dtype != DT_FLOAT) {\n-        return absl::InvalidArgumentError(\n-            absl::StrFormat(\"Unsupported dtype for stacked activations: %s\",\n-                            DataType_Name(dtype)));\n+      TF_RETURN_IF_ERROR(c->GetAttr(\"dtype\", &dtype));\n+      // If conversion is requested, only allow f32 -> bf16.\n+      if (input_dtype != dtype &&\n+          !(input_dtype == DT_FLOAT && dtype == DT_BFLOAT16)) {\n+        return absl::InvalidArgumentError(absl::StrFormat(\n+            \"Unsupported dtype conversion for stacked activations: %s -> %s\",\n+            DataType_Name(input_dtype), DataType_Name(dtype)));\n       }\n       for (int i = 0; i < num_tables; ++i) {\n         shape_inference::ShapeHandle unstacked_activation_shape =\n@@ -999,12 +1008,21 @@ REGISTER_OP(\"XlaSparseGradientsStack\")\n         features[i] = c->Value(c->Dim(c->input(i), 1));\n         total_sample_count += c->Value(c->Dim(c->input(i), 0));\n       }\n+      DataType input_dtype;\n+      TF_RETURN_IF_ERROR(c->GetAttr(\"input_dtype\", &input_dtype));\n       DataType dtype;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"dtype\", &dtype));\n-      if (dtype != DT_FLOAT) {\n+      if (dtype == DT_BFLOAT16) {\n         return absl::InvalidArgumentError(\n             absl::StrFormat(\"Unsupported dtype for stacked gradients: %s\",\n-                            DataType_Name(dtype)));\n+                            DataType_Name(input_dtype)));\n+      }\n+      // If conversion is requested, only allow bf16 -> f32.\n+      if (input_dtype != dtype &&\n+          !(input_dtype == DT_BFLOAT16 && dtype == DT_FLOAT)) {\n+        return absl::InvalidArgumentError(absl::StrFormat(\n+            \"Unsupported dtype conversion for stacked gradients: %s -> %s\",\n+            DataType_Name(input_dtype), DataType_Name(dtype)));\n       }\n       int padded_feature = xla::RoundUpTo(*absl::c_max_element(features), 8);\n       shape_inference::ShapeHandle stacked_gradients_shape ="
        },
        {
            "sha": "20c51c5f4d8da3bd969f86df4c91a8395a03ec91",
            "filename": "tensorflow/distribute/experimental/rpc/kernels/oss/grpc_credentials.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fdistribute%2Fexperimental%2Frpc%2Fkernels%2Foss%2Fgrpc_credentials.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fdistribute%2Fexperimental%2Frpc%2Fkernels%2Foss%2Fgrpc_credentials.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdistribute%2Fexperimental%2Frpc%2Fkernels%2Foss%2Fgrpc_credentials.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -21,12 +21,12 @@ limitations under the License.\n namespace tensorflow {\n namespace rpc {\n \n-// Returns LOAS credentials for use when creating a gRPC server inside Google.\n+// Returns insecure credentials for use when creating a gRPC server.\n std::shared_ptr<::grpc::ServerCredentials> GetDefaultServerCredentials() {\n   return ::grpc::InsecureServerCredentials();\n }\n \n-// Returns LOAS credentials for use when creating a gRPC channel.\n+// Returns insecure credentials for use when creating a gRPC channel.\n std::shared_ptr<::grpc::ChannelCredentials> GetDefaultChannelCredentials() {\n   return ::grpc::InsecureChannelCredentials();\n }"
        },
        {
            "sha": "c8dd29135e96cabfc69a5fd1289928f197c9f0cf",
            "filename": "tensorflow/dtensor/mlir/layout_propagation_v2.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fdtensor%2Fmlir%2Flayout_propagation_v2.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fdtensor%2Fmlir%2Flayout_propagation_v2.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Fmlir%2Flayout_propagation_v2.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1022,7 +1022,7 @@ class LayoutPrinter : public mlir::OpAsmPrinter {\n     os_ << symbolRef;\n   };\n \n-  void printNamedAttribute(mlir::NamedAttribute attr) {\n+  void printNamedAttribute(mlir::NamedAttribute attr) override {\n     os_ << attr.getName().strref() << \" = \";\n     printAttribute(attr.getValue());\n   }"
        },
        {
            "sha": "ada98d7af5cf47cdd730e7e88aa6ec56557f1a9d",
            "filename": "tensorflow/dtensor/tests/slice_util_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fdtensor%2Ftests%2Fslice_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fdtensor%2Ftests%2Fslice_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fdtensor%2Ftests%2Fslice_util_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -31,7 +31,6 @@ namespace slice_util {\n namespace {\n \n using ::testing::SizeIs;\n-using ::tsl::testing::IsOk;\n \n TEST(TokenTest, NormalizeDynamic) {\n   auto spec = Token(Token::REGULAR, /*begin=*/0, /*end=*/0, /*stride=*/1,"
        },
        {
            "sha": "f37b7e1384863e128856067a35955d0a5ff03215",
            "filename": "tensorflow/examples/custom_ops_doc/simple_hash_table/README.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fexamples%2Fcustom_ops_doc%2Fsimple_hash_table%2FREADME.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fexamples%2Fcustom_ops_doc%2Fsimple_hash_table%2FREADME.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fexamples%2Fcustom_ops_doc%2Fsimple_hash_table%2FREADME.md?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -277,16 +277,16 @@ class SimpleHashTableResource : public ::tensorflow::ResourceBase {\n     mutex_lock l(mu_);\n     for (const auto& pair : table_) {\n       if (count >= num_pairs) {\n-        strings::StrAppend(&rval, \"...\");\n+        absl::StrAppend(&rval, \"...\");\n         break;\n       }\n-      std::string kv_str = strings::StrCat(pair.first, \": \", pair.second);\n-      strings::StrAppend(&rval, kv_str.substr(0, max_kv_str_len));\n-      if (kv_str.length() > max_kv_str_len) strings::StrAppend(&rval, \" ...\");\n-      strings::StrAppend(&rval, \", \");\n+      std::string kv_str = absl::StrCat(pair.first, \": \", pair.second);\n+      absl::StrAppend(&rval, kv_str.substr(0, max_kv_str_len));\n+      if (kv_str.length() > max_kv_str_len) absl::StrAppend(&rval, \" ...\");\n+      absl::StrAppend(&rval, \", \");\n       count += 1;\n     }\n-    strings::StrAppend(&rval, \"}\");\n+    absl::StrAppend(&rval, \"}\");\n     return rval;\n   }\n "
        },
        {
            "sha": "5a7aa3079b3b27c967b9c64ec1c396dafdc0afc7",
            "filename": "tensorflow/examples/custom_ops_doc/simple_hash_table/simple_hash_table_kernel.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fexamples%2Fcustom_ops_doc%2Fsimple_hash_table%2Fsimple_hash_table_kernel.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fexamples%2Fcustom_ops_doc%2Fsimple_hash_table%2Fsimple_hash_table_kernel.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fexamples%2Fcustom_ops_doc%2Fsimple_hash_table%2Fsimple_hash_table_kernel.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n #include <string>\n \n #include \"absl/container/flat_hash_map.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"tensorflow/core/framework/allocator.h\"\n@@ -129,16 +130,16 @@ class SimpleHashTableResource : public ::tensorflow::ResourceBase {\n     mutex_lock l(mu_);\n     for (const auto& pair : table_) {\n       if (count >= num_pairs) {\n-        strings::StrAppend(&rval, \"...\");\n+        absl::StrAppend(&rval, \"...\");\n         break;\n       }\n-      std::string kv_str = strings::StrCat(pair.first, \": \", pair.second);\n-      strings::StrAppend(&rval, kv_str.substr(0, max_kv_str_len));\n-      if (kv_str.length() > max_kv_str_len) strings::StrAppend(&rval, \" ...\");\n-      strings::StrAppend(&rval, \", \");\n+      std::string kv_str = absl::StrCat(pair.first, \": \", pair.second);\n+      absl::StrAppend(&rval, kv_str.substr(0, max_kv_str_len));\n+      if (kv_str.length() > max_kv_str_len) absl::StrAppend(&rval, \" ...\");\n+      absl::StrAppend(&rval, \", \");\n       count += 1;\n     }\n-    strings::StrAppend(&rval, \"}\");\n+    absl::StrAppend(&rval, \"}\");\n     return rval;\n   }\n "
        },
        {
            "sha": "019855a88810acbd6e3eb5a24f0e82c74308d20c",
            "filename": "tensorflow/lite/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1,6 +1,8 @@\n load(\"@bazel_skylib//:bzl_library.bzl\", \"bzl_library\")\n load(\"@bazel_skylib//lib:selects.bzl\", \"selects\")\n load(\"@bazel_skylib//rules:common_settings.bzl\", \"bool_flag\")\n+load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n+load(\"@rules_cc//cc:cc_test.bzl\", \"cc_test\")\n load(\"//tensorflow:tensorflow.bzl\", \"if_google\", \"if_not_windows\", \"if_oss\", \"tf_cc_test\")\n load(\"//tensorflow:tensorflow.default.bzl\", \"get_compatible_with_portable\")\n load(\"//tensorflow/lite:build_def.bzl\", \"tflite_cc_shared_object\", \"tflite_copts\", \"tflite_copts_warnings\", \"tflite_linkopts_no_undefined\", \"tflite_self_contained_libs_test_suite\")"
        },
        {
            "sha": "6aa3cfa19ef675e84a9d8a3460e492b835c8d3fe",
            "filename": "tensorflow/lite/core/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fcore%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fcore%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fcore%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -180,9 +180,10 @@ cc_library(\n     ],\n     compatible_with = get_compatible_with_portable(),\n     visibility = [\n-        \"//tensorflow/lite:__subpackages__\",\n         \"//third_party/deepmind/lyria_live/internal/odml:__subpackages__\",\n-        \"//third_party/odml/litert/litert:__subpackages__\",\n+        # copybara:uncomment \"//third_party/odml/litert/litert:__subpackages__\",\n+        # \"//third_party/odml/litert:__subpackages__\",  # copybara:uncomment\n+        \"//tensorflow/lite:__subpackages__\",\n     ] + core_cc_api_stable_visibility_allowlist(),\n     deps = [\n         \":model_builder\","
        },
        {
            "sha": "9f97ec049bc01a1d8aa3eb86b66512db8a9605a2",
            "filename": "tensorflow/lite/core/c/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fcore%2Fc%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fcore%2Fc%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fcore%2Fc%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -16,8 +16,9 @@ load(\n package(\n     # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n     default_visibility = [\n+        # \"//litert:__subpackages__\",  # copybara:uncomment\n+        # copybara:uncomment \"//third_party/odml/litert:__subpackages__\",\n         \"//tensorflow/lite:__subpackages__\",\n-        \"//third_party/odml/litert:__subpackages__\",\n     ],\n     licenses = [\"notice\"],\n )\n@@ -299,9 +300,10 @@ tflite_cc_library_with_c_headers_test(\n     compatible_with = get_compatible_with_portable(),\n     copts = tflite_copts(),\n     visibility = [\n+        # \"//litert/litert:__subpackages__\",  # copybara:uncomment\n+        # copybara:uncomment \"//third_party/odml/litert/litert:__subpackages__\",\n         \"//tensorflow/compiler/mlir/lite/experimental/lrt:__subpackages__\",\n         \"//tensorflow/lite:__subpackages__\",\n-        \"//third_party/odml/litert/litert:__subpackages__\",\n     ] + c_api_visibility_allowlist(),\n     deps = [\n         \"//tensorflow/compiler/mlir/lite/core/c:tflite_common\","
        },
        {
            "sha": "67ba43b54e8ca8efc7d34d8f877375b49a105791",
            "filename": "tensorflow/lite/core/interpreter_builder.cc",
            "status": "modified",
            "additions": 37,
            "deletions": 39,
            "changes": 76,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fcore%2Finterpreter_builder.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fcore%2Finterpreter_builder.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fcore%2Finterpreter_builder.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -778,7 +778,9 @@ TfLiteStatus InterpreterBuilder::operator()(\n     std::unique_ptr<Interpreter>* interpreter, int num_threads) {\n   TfLiteStatus status = SetNumThreads(num_threads);\n   if (status != kTfLiteOk) {\n-    interpreter->reset();\n+    if (interpreter) {\n+      interpreter->reset();\n+    }\n     return status;\n   }\n   return (*this)(interpreter);\n@@ -791,30 +793,24 @@ TfLiteStatus InterpreterBuilder::operator()(\n                          \"Null output pointer passed to InterpreterBuilder.\");\n     return kTfLiteError;\n   }\n-\n-  // Safe exit by deleting partially created interpreter, to reduce verbosity\n-  // on error conditions. Use by return cleanup_on_error();\n-  auto cleanup_and_error = [&interpreter]() {\n-    interpreter->reset();\n-    return kTfLiteError;\n-  };\n+  interpreter->reset();\n \n   if (!model_) {\n     TF_LITE_REPORT_ERROR(error_reporter_, \"Null pointer passed in as model.\");\n-    return cleanup_and_error();\n+    return kTfLiteError;\n   }\n \n   if (model_->version() != TFLITE_SCHEMA_VERSION) {\n     TF_LITE_REPORT_ERROR(error_reporter_,\n                          \"Model provided is schema version %d not equal \"\n                          \"to supported version %d.\\n\",\n                          model_->version(), TFLITE_SCHEMA_VERSION);\n-    return cleanup_and_error();\n+    return kTfLiteError;\n   }\n \n   if (BuildLocalIndexToRegistrationMapping() != kTfLiteOk) {\n     TF_LITE_REPORT_ERROR(error_reporter_, \"Registration failed.\\n\");\n-    return cleanup_and_error();\n+    return kTfLiteError;\n   }\n \n   // Flatbuffer model schemas define a list of opcodes independent of the\n@@ -827,32 +823,32 @@ TfLiteStatus InterpreterBuilder::operator()(\n \n   if (subgraphs->size() == 0) {\n     TF_LITE_REPORT_ERROR(error_reporter_, \"No subgraph in the model.\\n\");\n-    return cleanup_and_error();\n+    return kTfLiteError;\n   }\n \n   if (!buffers) {\n     TF_LITE_REPORT_ERROR(error_reporter_, \"No buffers in the model.\\n\");\n-    return cleanup_and_error();\n+    return kTfLiteError;\n   }\n \n-  *interpreter = std::make_unique<Interpreter>(error_reporter_);\n+  auto tmp_interpreter = std::make_unique<Interpreter>(error_reporter_);\n   if (subgraphs->size() > 1) {\n-    (*interpreter)->AddSubgraphs(subgraphs->size() - 1);\n+    tmp_interpreter->AddSubgraphs(subgraphs->size() - 1);\n   }\n \n   // Set num threads after all the subgraphs are added.\n-  (*interpreter)->SetNumThreads(num_threads_);\n+  tmp_interpreter->SetNumThreads(num_threads_);\n \n   // Set Interpreter options\n-  (*interpreter)->ApplyOptionsImpl(&options_);\n+  tmp_interpreter->ApplyOptionsImpl(&options_);\n \n-  (*interpreter)\n-      ->SetProfilerImpl(tflite::profiling::MaybeCreatePlatformProfiler());\n+  tmp_interpreter->SetProfilerImpl(\n+      tflite::profiling::MaybeCreatePlatformProfiler());\n \n   bool telemetry_registered = telemetry_profiler_ != nullptr;\n   std::unique_ptr<TfLiteTelemetryInterpreterSettings> telemetry_settings;\n   if (telemetry_registered) {\n-    (*interpreter)->AddProfiler(std::move(telemetry_profiler_));\n+    tmp_interpreter->AddProfiler(std::move(telemetry_profiler_));\n     telemetry_settings = std::make_unique<TfLiteTelemetryInterpreterSettings>();\n     telemetry_settings->subgraph_infos.resize(subgraphs->size());\n   }\n@@ -861,7 +857,7 @@ TfLiteStatus InterpreterBuilder::operator()(\n        ++subgraph_index) {\n     const tflite::SubGraph* subgraph = (*subgraphs)[subgraph_index];\n     tflite::Subgraph* modified_subgraph =\n-        (*interpreter)->subgraph(subgraph_index);\n+        tmp_interpreter->subgraph(subgraph_index);\n     modified_subgraph->allocation_ = allocation_;\n     auto* subgraph_info =\n         telemetry_registered\n@@ -873,10 +869,10 @@ TfLiteStatus InterpreterBuilder::operator()(\n       TF_LITE_REPORT_ERROR(error_reporter_,\n                            \"Did not get tensors in subgraph %d.\\n\",\n                            subgraph_index);\n-      return cleanup_and_error();\n+      return kTfLiteError;\n     }\n     if (modified_subgraph->AddTensors(tensors->size()) != kTfLiteOk) {\n-      return cleanup_and_error();\n+      return kTfLiteError;\n     }\n     // Parse inputs/outputs\n     modified_subgraph->SetInputs(\n@@ -889,9 +885,9 @@ TfLiteStatus InterpreterBuilder::operator()(\n     // nodes.\n     if (ParseTensors(buffers, tensors, modified_subgraph, subgraph_info) !=\n         kTfLiteOk)\n-      return cleanup_and_error();\n+      return kTfLiteError;\n     if (operators && ParseNodes(operators, modified_subgraph) != kTfLiteOk)\n-      return cleanup_and_error();\n+      return kTfLiteError;\n \n     std::vector<int> variables;\n     for (int i = 0; i < modified_subgraph->tensors_size(); ++i) {\n@@ -906,14 +902,14 @@ TfLiteStatus InterpreterBuilder::operator()(\n     }\n   }\n \n-  if (ParseSignatureDefs(model_->signature_defs(), interpreter->get()) !=\n+  if (ParseSignatureDefs(model_->signature_defs(), tmp_interpreter.get()) !=\n       kTfLiteOk) {\n-    return cleanup_and_error();\n+    return kTfLiteError;\n   }\n \n   if (options_.GetUseSignatureTensorNames()) {\n-    for (auto& signature_def : (*interpreter)->signature_defs_) {\n-      auto* subgraph = (*interpreter)->subgraph(signature_def.subgraph_index);\n+    for (auto& signature_def : tmp_interpreter->signature_defs_) {\n+      auto* subgraph = tmp_interpreter->subgraph(signature_def.subgraph_index);\n       for (auto& [name, tensor_index] : signature_def.inputs) {\n         auto tensor = subgraph->tensor(tensor_index);\n         tensor->name = name.c_str();\n@@ -925,33 +921,35 @@ TfLiteStatus InterpreterBuilder::operator()(\n     }\n   }\n \n-  if ((*interpreter)->SetMetadata(metadata_) != kTfLiteOk) {\n-    return cleanup_and_error();\n+  if (tmp_interpreter->SetMetadata(metadata_) != kTfLiteOk) {\n+    return kTfLiteError;\n   }\n \n   if (ShouldCreateLazyDelegateProviders(num_fp32_tensors_)) {\n-    (*interpreter)->lazy_delegate_providers_ =\n+    tmp_interpreter->lazy_delegate_providers_ =\n         op_resolver_.GetDelegateCreators();\n   }\n \n   if (telemetry_registered) {\n     ParseConversionMetadata(telemetry_settings.get());\n-    (*interpreter)->SetTelemetrySettings(std::move(telemetry_settings));\n+    tmp_interpreter->SetTelemetrySettings(std::move(telemetry_settings));\n     // Reports model and interpreter settings if telemetry is applied.\n-    (*interpreter)->ReportTelemetrySettings(kTelemetryBuilderEventName);\n+    tmp_interpreter->ReportTelemetrySettings(kTelemetryBuilderEventName);\n   }\n \n-  TfLiteStatus status = ApplyDelegates(interpreter->get());\n-  if (status != kTfLiteOk) {\n-    interpreter->reset();\n+  if (TfLiteStatus status = ApplyDelegates(tmp_interpreter.get());\n+      status != kTfLiteOk) {\n+    TF_LITE_REPORT_ERROR(error_reporter_, \"Failed to apply delegates.\\n\");\n+    return status;\n   }\n \n   // Apply Interpreter options again for dynamic allocation.\n   if (options_.GetDynamicAllocationForLargeTensors()) {\n-    (*interpreter)->ApplyOptionsImpl(&options_);\n+    tmp_interpreter->ApplyOptionsImpl(&options_);\n   }\n \n-  return status;\n+  *interpreter = std::move(tmp_interpreter);\n+  return kTfLiteOk;\n }\n \n void InterpreterBuilder::AddDelegate(TfLiteDelegate* delegate) {"
        },
        {
            "sha": "b8cdedee2f9316ceaa089c1485630c532a4d2c94",
            "filename": "tensorflow/lite/delegates/hexagon/builders/tests/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fdelegates%2Fhexagon%2Fbuilders%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fdelegates%2Fhexagon%2Fbuilders%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fdelegates%2Fhexagon%2Fbuilders%2Ftests%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1,3 +1,5 @@\n+load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n+load(\"@rules_cc//cc:cc_test.bzl\", \"cc_test\")\n load(\":tests.bzl\", \"hexagon_op_tests\")\n \n package("
        },
        {
            "sha": "0a6b542674e0a717590b3ff222bf7c0ee6e89419",
            "filename": "tensorflow/lite/delegates/hexagon/builders/tests/tests.bzl",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fdelegates%2Fhexagon%2Fbuilders%2Ftests%2Ftests.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fdelegates%2Fhexagon%2Fbuilders%2Ftests%2Ftests.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fdelegates%2Fhexagon%2Fbuilders%2Ftests%2Ftests.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1,5 +1,6 @@\n \"\"\"Rules for generating unit-tests using hexagon delegates.\"\"\"\n \n+load(\"@rules_cc//cc:cc_test.bzl\", \"cc_test\")\n load(\"//tensorflow/lite:special_rules.bzl\", \"tflite_hexagon_mobile_test\")  #'@unused'\n \n def hexagon_op_tests(\n@@ -14,7 +15,7 @@ def hexagon_op_tests(\n \n     for src in srcs:\n         parts = src.split(\".cc\")\n-        native.cc_test(\n+        cc_test(\n             name = \"hexagon_\" + parts[0],\n             srcs = [src],\n             deps = deps,\n@@ -27,7 +28,7 @@ def hexagon_op_tests(\n         )\n \n     all_ops_test_name = \"hexagon_op_tests_all\"\n-    native.cc_test(\n+    cc_test(\n         name = all_ops_test_name,\n         srcs = srcs,\n         deps = deps,"
        },
        {
            "sha": "274bdb42b0e1c5a60e598b0834618b16e5fd62ad",
            "filename": "tensorflow/lite/delegates/xnnpack/weight_cache.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fdelegates%2Fxnnpack%2Fweight_cache.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fdelegates%2Fxnnpack%2Fweight_cache.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fdelegates%2Fxnnpack%2Fweight_cache.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -155,6 +155,11 @@ bool WeightCacheBuilder::StartBuildStep() {\n     XNNPACK_RETURN_CHECK(buffer_list_data.Map(fd_, header.buffer_list_offset,\n                                               file_path_.c_str()),\n                          \"could not map buffer list mapping\");\n+    flatbuffers::Verifier verifier(\n+        reinterpret_cast<const uint8_t*>(buffer_list_data.data()),\n+        header.buffer_list_size);\n+    XNNPACK_RETURN_CHECK(cache::schema::VerifyBufferListBuffer(verifier),\n+                         \"could not verify buffer list mapping\");\n     cache::schema::GetBufferList(buffer_list_data.data())->UnPackTo(&schema_);\n   }\n "
        },
        {
            "sha": "9e97e5265867d107f82efebe4ed0f711bd079565",
            "filename": "tensorflow/lite/delegates/xnnpack/weight_cache_test.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fdelegates%2Fxnnpack%2Fweight_cache_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fdelegates%2Fxnnpack%2Fweight_cache_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fdelegates%2Fxnnpack%2Fweight_cache_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -186,6 +186,38 @@ TEST(WeightCacheBuilderTest, AppendWithoutReserveWriteWorks) {\n   EXPECT_THAT(cache_data, ElementsAreArray(payload));\n }\n \n+TEST(WeightCacheBuilderTest, CorruptBufferListFailsGracefully) {\n+  const std::string cache_path = testing::TempDir() + \"/cache\";\n+  const std::string payload = \"This is some data in the file.\";\n+  const PackIdentifier dummy_id{1, 2, 3};\n+\n+  FileDescriptor file_descriptor = FileDescriptor::Open(\n+      cache_path.c_str(), O_CREAT | O_TRUNC | O_RDWR, 0644);\n+  WeightCacheBuilder builder;\n+  ASSERT_TRUE(builder.Start(cache_path.c_str(), file_descriptor));\n+  ASSERT_TRUE(builder.StartBuildStep());\n+\n+  const size_t payload_size = size(payload);\n+  auto loc = builder.Append(dummy_id, payload.c_str(), payload_size);\n+  EXPECT_EQ(loc.size, payload_size);\n+  ASSERT_TRUE(builder.StopBuildStep());\n+\n+  // corrupt the buffer list data.\n+  {\n+    FileDescriptor file_descriptor =\n+        FileDescriptor::Open(cache_path.c_str(), O_RDWR, 0644);\n+    ASSERT_TRUE(file_descriptor.IsValid());\n+    XNNPackCacheHeader header;\n+    file_descriptor.SetPos(0);\n+    ASSERT_TRUE(file_descriptor.Read(&header, sizeof(header)));\n+    file_descriptor.SetPos(header.buffer_list_offset + 1);\n+    std::string data(8, 'a');\n+    ASSERT_TRUE(file_descriptor.Write(data.data(), data.size()));\n+  }\n+\n+  EXPECT_FALSE(builder.StartBuildStep());\n+}\n+\n TEST(WeightCacheBuilderTest, InvalidFileDescriptorFails) {\n   WeightCacheBuilder builder;\n   EXPECT_FALSE(builder.Start(\"\", FileDescriptor()));"
        },
        {
            "sha": "c8debb37899f834019808d574031f2ec9be1a70f",
            "filename": "tensorflow/lite/experimental/acceleration/mini_benchmark/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fexperimental%2Facceleration%2Fmini_benchmark%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fexperimental%2Facceleration%2Fmini_benchmark%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fexperimental%2Facceleration%2Fmini_benchmark%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -13,6 +13,9 @@\n # limitations under the License.\n # ==============================================================================\n \n+load(\"@rules_cc//cc:cc_binary.bzl\", \"cc_binary\")\n+load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n+load(\"@rules_cc//cc:cc_test.bzl\", \"cc_test\")\n load(\"//tensorflow:strict.default.bzl\", \"py_strict_binary\")\n load(\"//tensorflow:tensorflow.bzl\", \"clean_dep\")\n "
        },
        {
            "sha": "8a33a3d0bf03936c120c56ea06f6ea6dab3a5e86",
            "filename": "tensorflow/lite/experimental/acceleration/mini_benchmark/build_defs.bzl",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fexperimental%2Facceleration%2Fmini_benchmark%2Fbuild_defs.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fexperimental%2Facceleration%2Fmini_benchmark%2Fbuild_defs.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fexperimental%2Facceleration%2Fmini_benchmark%2Fbuild_defs.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -14,6 +14,8 @@\n # ==============================================================================\n \"\"\"Helpers for mini-benchmark build rules.\"\"\"\n \n+load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n+load(\"@rules_cc//cc:cc_test.bzl\", \"cc_test\")\n load(\n     \"//tensorflow:tensorflow.bzl\",\n     \"clean_dep\",\n@@ -59,8 +61,7 @@ def embedded_binary(name, binary, array_variable_name, testonly = False, exec_pr\n         tools = [\"//tensorflow/lite/experimental/acceleration/compatibility:convert_binary_to_cc_source\"],\n         testonly = testonly,\n     )\n-\n-    native.cc_library(\n+    cc_library(\n         name = name,\n         srcs = [cc_name],\n         hdrs = [h_name],\n@@ -161,7 +162,7 @@ def validation_test(name, validation_model, tags = [], copts = [], deps = []):\n         binary = validation_model,\n         array_variable_name = \"g_tflite_acceleration_\" + name + \"_model\",\n     )\n-    native.cc_test(\n+    cc_test(\n         name = name,\n         srcs = [\"//tensorflow/lite/experimental/acceleration/mini_benchmark:model_validation_test.cc\"],\n         tags = tags + [\"no_mac\", \"no_windows\", \"tflite_not_portable_ios\"],\n@@ -243,7 +244,7 @@ def cc_library_with_forced_in_process_benchmark_variant(\n       **kwargs:\n         Additional cc_library parameters.\n     \"\"\"\n-    native.cc_library(\n+    cc_library(\n         name = name,\n         deps = deps + in_process_deps + _concat([select(map) for map in non_in_process_deps_selects]) + [\n             \"//tensorflow/lite/experimental/acceleration/mini_benchmark:tflite_acceleration_in_process_default\",\n@@ -252,7 +253,7 @@ def cc_library_with_forced_in_process_benchmark_variant(\n     )\n \n     in_process_deps_renamed = [add_suffix(in_process_dep, \"_in_process\") for in_process_dep in in_process_deps]\n-    native.cc_library(\n+    cc_library(\n         name = name + \"_in_process\",\n         deps = deps + in_process_deps_renamed + forced_in_process_deps + [\n             \"//tensorflow/lite/experimental/acceleration/mini_benchmark:tflite_acceleration_in_process_enable\","
        },
        {
            "sha": "c103bb3839ea2773439f179f8a7e9a7472948381",
            "filename": "tensorflow/lite/kernels/hadamard_rotation.cc",
            "status": "modified",
            "additions": 53,
            "deletions": 48,
            "changes": 101,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fkernels%2Fhadamard_rotation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fkernels%2Fhadamard_rotation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Fhadamard_rotation.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -39,57 +39,66 @@ struct OpData {\n   std::vector<int> random_binary_vector;\n };\n \n-// Fast Walsh Hadamard Transform. Updates `data` in place.\n-void FWHTGeneral(float* data, int n, bool normalize) {\n-  if ((n & (n - 1)) != 0) {\n-    std::cerr << \"Error: Input size must be a power of 2.\" << std::endl;\n-    return;\n-  }\n-\n-  int h = 1;\n-  while (h < n) {\n-    for (int i = 0; i < n; i += h * 2) {\n-      for (int j = i; j < i + h; ++j) {\n-        float x = data[j];\n-        float y = data[j + h];\n+// FWHT implementation for fixed bounds.\n+//\n+// The compiler is capable of fully unrolling this, which gives much better\n+// performance.\n+template <size_t N, size_t H>\n+void FWHTStaticSize(float* data) {\n+  for (size_t h = H; h < N; h *= 2) {\n+    for (size_t i = 0; i < N; i += 2 * h) {\n+      for (size_t j = i; j < i + h; ++j) {\n+        const float x = data[j];\n+        const float y = data[j + h];\n         data[j] = x + y;\n         data[j + h] = x - y;\n       }\n     }\n-    h *= 2;\n-  }\n-  if (normalize) {\n-    // Calculate the inverse square root once.\n-    const float norm_factor = 1.0f / std::sqrt(static_cast<float>(n));\n-    for (int k = 0; k < n; ++k) {\n-      data[k] *= norm_factor;\n-    }\n   }\n }\n \n-// Same FWHT algorithm, with loops explicitly unrolled for sizes >= 16.\n+// Fast Walsh Hadamard Transform. Updates `data` in place.\n+template <size_t kUnrollThreshold = 64>\n void FWHTFast(float* data, int hadamard_size) {\n-  std::vector<float> output(hadamard_size);\n-  int num_chunks = hadamard_size / 16;\n-\n-  float* in = data;\n-  // Use general, iterative loops algorithm for sizes up to 16.\n-  for (int chunk = 0; chunk < num_chunks; ++chunk, in += 16) {\n-    FWHTGeneral(in, 16, false);\n+  if ((hadamard_size & (hadamard_size - 1)) != 0) {\n+    std::cerr << \"hadamard_size needs to be a power of 2\\n\";\n+    return;\n   }\n-  // Finish the bigger butterflies with explicit unrolling.\n-  for (int chunk_size = 16; chunk_size < hadamard_size; chunk_size *= 2) {\n-    float* in1 = &data[0];\n-    float* in2 = &data[chunk_size];\n-    for (int i = 0; i < hadamard_size;\n-         i += chunk_size * 2, in1 += chunk_size, in2 += chunk_size) {\n-      for (int j = i; j < i + chunk_size; j += 16) {\n-        // Compiler will unroll this fixed size loop easily.\n-        for (int k = 0; k < 16; k++) {\n-          float x = *in1;\n-          float y = *in2;\n-          *in1++ = x + y;\n-          *in2++ = x - y;\n+  if (hadamard_size < kUnrollThreshold) {\n+    // For small sizes, we run an \"unoptimized\" loop. This avoids unrolling the\n+    // loops for every valid size under kUnrollSize.\n+    for (size_t h = 1; h < hadamard_size; h *= 2) {\n+      for (size_t i = 0; i < hadamard_size; i += 2 * h) {\n+        for (size_t j = i; j < i + h; ++j) {\n+          const float x = data[j];\n+          const float y = data[j + h];\n+          data[j] = x + y;\n+          data[j + h] = x - y;\n+        }\n+      }\n+    }\n+  } else {\n+    const int num_chunks = hadamard_size / kUnrollThreshold;\n+    float* in = data;\n+    // Use general, iterative loops algorithm for sizes up to kUnrollLimit.\n+    for (int chunk = 0; chunk < num_chunks; ++chunk, in += kUnrollThreshold) {\n+      FWHTStaticSize<kUnrollThreshold, 1>(in);\n+    }\n+    // Finish the bigger butterflies manually.\n+    for (int chunk_size = kUnrollThreshold; chunk_size < hadamard_size;\n+         chunk_size *= 2) {\n+      float* in1 = &data[0];\n+      float* in2 = &data[chunk_size];\n+      for (int i = 0; i < hadamard_size;\n+           i += chunk_size * 2, in1 += chunk_size, in2 += chunk_size) {\n+        for (int j = i; j < i + chunk_size; j += kUnrollThreshold) {\n+          // Compiler will unroll this fixed size loop easily.\n+          for (int k = 0; k < kUnrollThreshold; k++) {\n+            float x = *in1;\n+            float y = *in2;\n+            *in1++ = x + y;\n+            *in2++ = x - y;\n+          }\n         }\n       }\n     }\n@@ -167,17 +176,13 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n \n   memcpy(output->data.f, input_tensor->data.f, input_tensor->bytes);\n \n-  int num_hadamards_per_feature = input_feature_size / hadamard_size;\n+  const int num_hadamards_per_feature = input_feature_size / hadamard_size;\n   const int total_transforms =\n       input_batch * input_features * num_hadamards_per_feature;\n   for (int i = 0; i < total_transforms; ++i) {\n     int chunk_start = i * hadamard_size;\n     // Update output->data.f in place.\n-    if (hadamard_size < 16) {\n-      FWHTGeneral(&output->data.f[chunk_start], hadamard_size, true);\n-    } else {\n-      FWHTFast(&output->data.f[chunk_start], hadamard_size);\n-    }\n+    FWHTFast(&output->data.f[chunk_start], hadamard_size);\n   }\n \n   return kTfLiteOk;"
        },
        {
            "sha": "e4e82eaf8418fd6937ac9ce7087be6ff78995885",
            "filename": "tensorflow/lite/kernels/register_ref.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fkernels%2Fregister_ref.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Fkernels%2Fregister_ref.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Fregister_ref.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -164,6 +164,8 @@ TfLiteRegistration* Register_IMAG();\n TfLiteRegistration* Register_REAL();\n TfLiteRegistration* Register_COMPLEX_ABS();\n TfLiteRegistration* Register_CONV_3D_TRANSPOSE_REF();\n+TfLiteRegistration* Register_RFFT2D();\n+TfLiteRegistration* Register_CUMSUM();\n TfLiteRegistration* Register_BROADCAST_ARGS();\n TfLiteRegistration* Register_RANDOM_STANDARD_NORMAL();\n TfLiteRegistration* Register_BUCKETIZE();\n@@ -406,6 +408,7 @@ BuiltinRefOpResolver::BuiltinRefOpResolver() {\n              /* max_version = */ 2);\n   AddBuiltin(BuiltinOperator_FLOOR, Register_FLOOR_REF());\n   AddBuiltin(BuiltinOperator_NEG, Register_NEG());\n+  AddBuiltin(BuiltinOperator_CUMSUM, Register_CUMSUM());\n   AddBuiltin(BuiltinOperator_SELECT, Register_SELECT(),\n              /* min_version = */ 1,\n              /* max_version = */ 2);\n@@ -471,6 +474,7 @@ BuiltinRefOpResolver::BuiltinRefOpResolver() {\n              /* min_version = */ 1,\n              /* max_version = */ 3);\n   AddBuiltin(BuiltinOperator_SQUARE, Register_SQUARE());\n+  AddBuiltin(BuiltinOperator_RFFT2D, Register_RFFT2D());\n   AddBuiltin(BuiltinOperator_ZEROS_LIKE, Register_ZEROS_LIKE());\n   AddBuiltin(BuiltinOperator_FLOOR_MOD, Register_FLOOR_MOD(),\n              /* min_version = */ 1,"
        },
        {
            "sha": "0dea571cb442f4f734ac31b2e6e23097e0d48355",
            "filename": "tensorflow/lite/tools/cmake/modules/eigen.cmake",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Ftools%2Fcmake%2Fmodules%2Feigen.cmake",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Ftools%2Fcmake%2Fmodules%2Feigen.cmake",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Ftools%2Fcmake%2Fmodules%2Feigen.cmake?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -23,7 +23,7 @@ OverridableFetchContent_Declare(\n   eigen\n   GIT_REPOSITORY https://gitlab.com/libeigen/eigen.git\n   # Sync with tensorflow/third_party/eigen3/workspace.bzl\n-  GIT_TAG 4c38131a16803130b66266a912029504f2cf23cd\n+  GIT_TAG 70d8d99d0df9fd967b135efd8d12ed20fc48d007\n   # It's not currently (cmake 3.17) possible to shallow clone with a GIT TAG\n   # as cmake attempts to git checkout the commit hash after the clone\n   # which doesn't work as it's a shallow clone hence a different commit hash."
        },
        {
            "sha": "94bfea8541c2672def291ecbba9f553ece8a3b87",
            "filename": "tensorflow/lite/tools/cmake/modules/xnnpack.cmake",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Ftools%2Fcmake%2Fmodules%2Fxnnpack.cmake",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Flite%2Ftools%2Fcmake%2Fmodules%2Fxnnpack.cmake",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Ftools%2Fcmake%2Fmodules%2Fxnnpack.cmake?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -23,7 +23,7 @@ OverridableFetchContent_Declare(\n   xnnpack\n   GIT_REPOSITORY https://github.com/google/XNNPACK\n   # Sync with tensorflow/workspace2.bzl\n-  GIT_TAG 45bf06030727ce049793ce6749e943cc2ea896fe\n+  GIT_TAG e757940dbdcf465fd9eb7901ce73f4ff21387663\n   GIT_PROGRESS TRUE\n   PREFIX \"${CMAKE_BINARY_DIR}\"\n   SOURCE_DIR \"${CMAKE_BINARY_DIR}/xnnpack\""
        },
        {
            "sha": "f47d73a584555807c128419cc26ea6cbfb8ce85a",
            "filename": "tensorflow/opensource_only.files",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fopensource_only.files",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fopensource_only.files",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fopensource_only.files?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -168,6 +168,7 @@ tf_staging/tensorflow/tools/pip_package/BUILD:\n tf_staging/tensorflow/tools/pip_package/MANIFEST.in:\n tf_staging/tensorflow/tools/pip_package/README:\n tf_staging/tensorflow/tools/pip_package/check_load_py_test:.py\n+tf_staging/tensorflow/tools/pip_package/modify_setup_py:.py\n tf_staging/tensorflow/tools/pip_package/pip_smoke_test:.py\n tf_staging/tensorflow/tools/pip_package/setup:.py.tpl\n tf_staging/tensorflow/tools/pip_package/simple_console:.py"
        },
        {
            "sha": "62b2557290f7805e6d7c9fe1a31f29dbed690b6c",
            "filename": "tensorflow/python/_pywrap_tensorflow.def",
            "status": "modified",
            "additions": 7,
            "deletions": 15,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2F_pywrap_tensorflow.def",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2F_pywrap_tensorflow.def",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2F_pywrap_tensorflow.def?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -44,7 +44,6 @@ EXPORTS\n   ??0CalibrationOptions@quantization@stablehlo@@IEAA@PEAVArena@protobuf@google@@@Z\n   ??0CalibrationStatistics@calibrator@tensorflow@@IEAA@PEAVArena@protobuf@google@@@Z\n   ??0CancellationManager@tsl@@QEAA@XZ\n-  ??0CheckOpMessageBuilder@internal@tsl@@QEAA@PEBD@Z\n   ??0CheckpointReader@checkpoint@tensorflow@@QEAA@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAUTSL_Status@@@Z\n   ??0ConfigProto@tensorflow@@IEAA@PEAVArena@protobuf@google@@@Z\n   ??0ConfigProto@tensorflow@@IEAA@PEAVArena@protobuf@google@@AEBV01@@Z\n@@ -73,9 +72,7 @@ EXPORTS\n   ??0Impl@MakeErrorStream@status_macros@xla@@QEAA@PEBDHW4Code@error@tensorflow@@PEAV123@_N@Z\n   ??0LayoutProto@dtensor@tensorflow@@IEAA@PEAVArena@protobuf@google@@@Z\n   ??0LayoutProto@dtensor@tensorflow@@IEAA@PEAVArena@protobuf@google@@AEBV012@@Z\n-  ??0LogMessage@internal@tsl@@QEAA@PEBDHW4LogSeverity@lts_20250512@absl@@@Z\n   ??0LogMessage@log_internal@lts_20250512@absl@@QEAA@PEBDHUInfoTag@0123@@Z\n-  ??0LogMessageFatal@internal@tsl@@QEAA@PEBDH@Z\n   ??0LogMessageFatal@log_internal@lts_20250512@absl@@QEAA@PEBDH0@Z\n   ??0LogMessageFatal@log_internal@lts_20250512@absl@@QEAA@PEBDH@Z\n   ??0MLIRContext@mlir@@QEAA@AEBVDialectRegistry@1@W4Threading@01@@Z\n@@ -95,6 +92,7 @@ EXPORTS\n   ??0OpKernel@tensorflow@@QEAA@PEAVOpKernelConstruction@1@@Z\n   ??0OpLevelCostEstimator@grappler@tensorflow@@QEAA@XZ\n   ??0OpPerformanceList@tensorflow@@IEAA@PEAVArena@protobuf@google@@@Z\n+  ??0OstreamView@LogMessage@log_internal@lts_20250512@absl@@QEAA@AEAULogMessageData@1234@@Z\n   ??0PyInstanceChecker@py_dispatch@tensorflow@@QEAA@AEBV?$vector@PEAU_object@@V?$allocator@PEAU_object@@@std@@@std@@@Z\n   ??0PySignatureChecker@py_dispatch@tensorflow@@QEAA@V?$vector@U?$pair@HV?$shared_ptr@VPyTypeChecker@py_dispatch@tensorflow@@@std@@@std@@V?$allocator@U?$pair@HV?$shared_ptr@VPyTypeChecker@py_dispatch@tensorflow@@@std@@@std@@@2@@std@@@Z\n   ??0PythonAPIDispatcher@py_dispatch@tensorflow@@QEAA@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$Span@PEBD@lts_20250512@absl@@V?$Span@PEAU_object@@@67@@Z\n@@ -133,7 +131,6 @@ EXPORTS\n   ??1CalibrationOptions@quantization@stablehlo@@UEAA@XZ\n   ??1CalibrationStatistics@calibrator@tensorflow@@UEAA@XZ\n   ??1CancellationManager@tsl@@QEAA@XZ\n-  ??1CheckOpMessageBuilder@internal@tsl@@QEAA@XZ\n   ??1ConfigProto@tensorflow@@UEAA@XZ\n   ??1CoordinatedTask@tensorflow@@UEAA@XZ\n   ??1DataServiceMetadata@data@tensorflow@@UEAA@XZ\n@@ -151,9 +148,7 @@ EXPORTS\n   ??1Impl@MakeErrorStream@status_macros@xla@@QEAA@XZ\n   ??1KernelDefBuilder@tensorflow@@QEAA@XZ\n   ??1LayoutProto@dtensor@tensorflow@@UEAA@XZ\n-  ??1LogMessage@internal@tsl@@UEAA@XZ\n   ??1LogMessage@log_internal@lts_20250512@absl@@QEAA@XZ\n-  ??1LogMessageFatal@internal@tsl@@UEAA@XZ\n   ??1LogMessageFatal@log_internal@lts_20250512@absl@@QEAA@XZ\n   ??1MLIRContext@mlir@@QEAA@XZ\n   ??1MeshProto@dtensor@tensorflow@@UEAA@XZ\n@@ -165,6 +160,7 @@ EXPORTS\n   ??1OpInfo_TensorProperties@tensorflow@@UEAA@XZ\n   ??1OpKernel@tensorflow@@UEAA@XZ\n   ??1OpPerformanceList@tensorflow@@UEAA@XZ\n+  ??1OstreamView@LogMessage@log_internal@lts_20250512@absl@@UEAA@XZ\n   ??1ProfilerServer@profiler@tsl@@QEAA@XZ\n   ??1ProfilerSession@tsl@@QEAA@XZ\n   ??1PyContextManager@tensorflow@@QEAA@XZ\n@@ -221,7 +217,6 @@ EXPORTS\n   ?Build@KernelDefBuilder@tensorflow@@QEAAPEBVKernelDef@2@XZ\n   ?Canonicalize@FunctionParameterCanonicalizer@tensorflow@@QEAA_NPEAU_object@@0V?$Span@PEAU_object@@@lts_20250512@absl@@@Z\n   ?Capture@StackTrace@tensorflow@@SA?AV?$shared_ptr@VStackTrace@tensorflow@@@std@@H@Z\n-  ?CatPieces@internal@strings@tsl@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$initializer_list@V?$basic_string_view@DU?$char_traits@D@std@@@std@@@5@@Z\n   ?CatPieces@strings_internal@lts_20250512@absl@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$initializer_list@V?$basic_string_view@DU?$char_traits@D@std@@@std@@@5@@Z\n   ?Check@PyListChecker@py_dispatch@tensorflow@@UEAA?AW4MatchType@PyTypeChecker@23@PEAU_object@@@Z\n   ?Check@PyUnionChecker@py_dispatch@tensorflow@@UEAA?AW4MatchType@PyTypeChecker@23@PEAU_object@@@Z\n@@ -270,7 +265,6 @@ EXPORTS\n   ?CreateMesh@Mesh@dtensor@tensorflow@@SA?AV123@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@5@AEBV?$vector@_JV?$allocator@_J@std@@@5@2121_N@Z\n   ?CreateRecordReaderOptions@RecordReaderOptions@io@tsl@@SA?AU123@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z\n   ?CreateRecordWriterOptions@RecordWriterOptions@io@tsl@@SA?AU123@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z\n-  ?CreateTypeInfoAndReturnTypeIdImpl@AsyncValue@tsl@@CAGAEBUTypeInfo@12@@Z\n   ?CtxFailure@OpKernelConstruction@tensorflow@@QEAAXPEBDHAEBVStatus@lts_20250512@absl@@@Z\n   ?CtxFailure@OpKernelContext@tensorflow@@QEAAXPEBDHAEBVStatus@lts_20250512@absl@@@Z\n   ?CtxFailureWithWarning@OpKernelConstruction@tensorflow@@QEAAXPEBDHAEBVStatus@lts_20250512@absl@@@Z\n@@ -352,7 +346,6 @@ EXPORTS\n   ?FlushExecutionFiles@DebugEventsWriter@tfdbg@tensorflow@@QEAA?AVStatus@lts_20250512@absl@@XZ\n   ?FlushNonExecutionFiles@DebugEventsWriter@tfdbg@tensorflow@@QEAA?AVStatus@lts_20250512@absl@@XZ\n   ?ForEachPayload@StatusRep@status_internal@lts_20250512@absl@@QEBAXV?$FunctionRef@$$A6AXV?$basic_string_view@DU?$char_traits@D@std@@@std@@AEBVCord@lts_20250512@absl@@@Z@34@@Z\n-  ?ForVar2@CheckOpMessageBuilder@internal@tsl@@QEAAPEAV?$basic_ostream@DU?$char_traits@D@std@@@std@@XZ\n   ?FormatConvertImpl@str_format_internal@lts_20250512@absl@@YA?AU?$ArgConvertResult@$0IAAAE@@123@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VFormatConversionSpecImpl@123@PEAVFormatSinkImpl@123@@Z\n   ?FormatPack@str_format_internal@lts_20250512@absl@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VUntypedFormatSpecImpl@123@V?$Span@$$CBVFormatArgImpl@str_format_internal@lts_20250512@absl@@@23@@Z\n   ?FromProto@Layout@dtensor@tensorflow@@SA?AV?$StatusOr@VLayout@dtensor@tensorflow@@@lts_20250512@absl@@AEBVLayoutProto@23@@Z\n@@ -535,7 +528,6 @@ EXPORTS\n   ?NewDispatchServer@data@tensorflow@@YA?AVStatus@lts_20250512@absl@@AEBVDispatcherConfig@experimental@12@AEAV?$unique_ptr@VDispatchGrpcDataServer@data@tensorflow@@U?$default_delete@VDispatchGrpcDataServer@data@tensorflow@@@std@@@std@@@Z\n   ?NewProfiler@tfprof@tensorflow@@YA_NPEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@0@Z\n   ?NewRandomAccessFile@Env@tsl@@QEAA?AVStatus@lts_20250512@absl@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VRandomAccessFile@tsl@@U?$default_delete@VRandomAccessFile@tsl@@@std@@@7@@Z\n-  ?NewString@CheckOpMessageBuilder@internal@tsl@@QEAAPEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ\n   ?NewWorkerServer@data@tensorflow@@YA?AVStatus@lts_20250512@absl@@AEBVWorkerConfig@experimental@12@AEAV?$unique_ptr@VWorkerGrpcDataServer@data@tensorflow@@U?$default_delete@VWorkerGrpcDataServer@data@tensorflow@@@std@@@std@@@Z\n   ?NewWritableFile@Env@tsl@@QEAA?AVStatus@lts_20250512@absl@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VWritableFile@tsl@@U?$default_delete@VWritableFile@tsl@@@std@@@7@@Z\n   ?NotFoundError@lts_20250512@absl@@YA?AVStatus@12@V?$basic_string_view@DU?$char_traits@D@std@@@std@@@Z\n@@ -664,10 +656,10 @@ EXPORTS\n   ?StrCat@lts_20250512@absl@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVAlphaNum@12@000@Z\n   ?StrCat@lts_20250512@absl@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVAlphaNum@12@00@Z\n   ?StrCat@lts_20250512@absl@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVAlphaNum@12@0@Z\n-  ?StrCat@strings@tsl@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVAlphaNum@12@000@Z\n-  ?StrCat@strings@tsl@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVAlphaNum@12@00@Z\n-  ?StrCat@strings@tsl@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVAlphaNum@12@0@Z\n-  ?StrCat@strings@tsl@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVAlphaNum@12@@Z\n+  ?StrCat@strings@tsl@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVAlphaNum@lts_20250512@absl@@000@Z\n+  ?StrCat@strings@tsl@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVAlphaNum@lts_20250512@absl@@00@Z\n+  ?StrCat@strings@tsl@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVAlphaNum@lts_20250512@absl@@0@Z\n+  ?StrCat@strings@tsl@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVAlphaNum@lts_20250512@absl@@@Z\n   ?StripDefaultAttributes@tensorflow@@YAXAEBVOpRegistryInterface@1@PEAV?$RepeatedPtrField@VNodeDef@tensorflow@@@protobuf@google@@@Z\n   ?Sub@ops@tensorflow@@YA?AVStatus@lts_20250512@absl@@PEAVAbstractContext@2@QEAVAbstractTensorHandle@2@1PEAPEAV72@PEBD3@Z\n   ?SubRegisterer@gradients@tensorflow@@YAPEAVGradientFunction@12@AEBUForwardOperation@12@@Z\n@@ -776,7 +768,6 @@ EXPORTS\n   ?UpdateEdge@tensorflow@@YAXPEAUTF_Graph@@UTF_Output@@UTF_Input@@PEAUTSL_Status@@@Z\n   ?ValidateType@ResourceHandle@tensorflow@@QEBA?AVStatus@lts_20250512@absl@@AEBVTypeIndex@2@@Z\n   ?Vector@InferenceContext@shape_inference@tensorflow@@QEAA?AVShapeHandle@23@UDimensionOrConstant@23@@Z\n-  ?VmoduleActivated@LogMessage@internal@tsl@@SA_NPEBDH@Z\n   ?WaitCommon@CondVar@lts_20250512@absl@@AEAA_NPEAVMutex@23@VKernelTimeout@synchronization_internal@23@@Z\n   ?Watch@Tape@gradients@tensorflow@@QEAAXPEBVAbstractTensorHandle@3@@Z\n   ?WithRank@InferenceContext@shape_inference@tensorflow@@QEAA?AVStatus@lts_20250512@absl@@VShapeHandle@23@_JPEAV723@@Z\n@@ -846,6 +837,7 @@ EXPORTS\n   ?set_requested_device@Node@tensorflow@@QEAAXAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z\n   ?set_tf2_execution@tensorflow@@YAX_N@Z\n   ?sharding_spec_strs@Layout@dtensor@tensorflow@@QEBA?AV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@XZ\n+  ?stream@OstreamView@LogMessage@log_internal@lts_20250512@absl@@QEAAAEAV?$basic_ostream@DU?$char_traits@D@std@@@std@@XZ\n   ?tensor_data@Tensor@tensorflow@@QEBA?AV?$basic_string_view@DU?$char_traits@D@std@@@std@@XZ\n   ?tensor_float_32_execution_enabled@tsl@@YA_NXZ\n   ?tf2_execution_enabled@tensorflow@@YA_NXZ"
        },
        {
            "sha": "4dd122543bac232ba2ec8ed9c3b2b73545139ce4",
            "filename": "tensorflow/python/compat/compat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2Fcompat%2Fcompat.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2Fcompat%2Fcompat.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Fcompat%2Fcompat.py?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -29,7 +29,7 @@\n # This value changes every day with an automatic CL. It can be modified in code\n # via `forward_compatibility_horizon()` or with the environment variable\n # TF_FORWARD_COMPATIBILITY_DELTA_DAYS, which is added to the compatibility date.\n-_FORWARD_COMPATIBILITY_HORIZON = datetime.date(2025, 8, 25)\n+_FORWARD_COMPATIBILITY_HORIZON = datetime.date(2025, 9, 9)\n _FORWARD_COMPATIBILITY_DELTA_DAYS_VAR_NAME = \"TF_FORWARD_COMPATIBILITY_DELTA_DAYS\"\n _FORWARD_COMPATIBILITY_DATE_NUMBER = None\n "
        },
        {
            "sha": "2018c782c74d27a952460b51b9f2cbc8bff8cc83",
            "filename": "tensorflow/python/eager/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2Feager%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2Feager%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Feager%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1003,7 +1003,10 @@ tf_py_strict_test(\n py_strict_library(\n     name = \"wrap_function\",\n     srcs = [\"wrap_function.py\"],\n-    visibility = [\"//tensorflow:internal\"],\n+    visibility = [\n+        \"//tensorflow:internal\",\n+        \"//third_party/py/orbax/export:__subpackages__\",\n+    ],\n     deps = [\n         \":context\",\n         \":function\","
        },
        {
            "sha": "21a8d478e3a2275fa12d6e1ed41497449c3e8e93",
            "filename": "tensorflow/python/kernel_tests/nn_ops/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2Fkernel_tests%2Fnn_ops%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2Fkernel_tests%2Fnn_ops%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Fkernel_tests%2Fnn_ops%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -77,6 +77,9 @@ cuda_py_strict_test(\n     name = \"betainc_op_test\",\n     size = \"small\",\n     srcs = [\"betainc_op_test.py\"],\n+    tags = [\n+        \"notap\",  # TODO(delhibabu): Re-enable once the test is fixed.\n+    ],\n     xla_tags = [\n         \"no_cuda_asan\",  # times out\n     ],"
        },
        {
            "sha": "4519f1aef2a6cae2e1ec1282312a70c75bfcbe42",
            "filename": "tensorflow/python/ops/nn_ops.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2Fops%2Fnn_ops.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2Fops%2Fnn_ops.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Fops%2Fnn_ops.py?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1737,10 +1737,10 @@ def pool_v2(\n       with \"NC\".  Pooling happens over the spatial dimensions only.\n     window_shape: Sequence of N ints >= 1.\n     pooling_type: Specifies pooling operation, must be \"AVG\" or \"MAX\".\n-    strides: Optional. Sequence of N ints >= 1.  Defaults to `[1]*N`. If any value of\n-      strides is > 1, then all values of dilation_rate must be 1.\n-    padding: The padding algorithm, must be \"SAME\" or \"VALID\". Defaults to \"SAME\".\n-      See\n+    strides: Optional. Sequence of N ints >= 1.  Defaults to `[1]*N`. If any\n+      value of strides is > 1, then all values of dilation_rate must be 1.\n+    padding: The padding algorithm, must be \"SAME\" or \"VALID\". Defaults to\n+      \"SAME\". See\n       [here](https://www.tensorflow.org/api_docs/python/tf/nn#notes_on_padding_2)\n       for more information.\n     data_format: A string or None.  Specifies whether the channel dimension of\n@@ -1751,7 +1751,7 @@ def pool_v2(\n       N=3, the valid values are \"NDHWC\" (default) and \"NCDHW\".\n     dilations: Optional.  Dilation rate.  List of N ints >= 1. Defaults to\n       `[1]*N`.  If any value of dilation_rate is > 1, then all values of strides\n-      must be 1.\n+      must be 1, and padding should not be set to \"SAME\".\n     name: Optional. Name of the op.\n \n   Returns:"
        },
        {
            "sha": "d0211e0b9caeb8d464e04ff20190603695819f72",
            "filename": "tensorflow/python/ops/sparse_ops.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2Fops%2Fsparse_ops.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2Fops%2Fsparse_ops.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Fops%2Fsparse_ops.py?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -3687,6 +3687,12 @@ def handle(self, args, kwargs):\n     # TODO(b/120307967) Add dispatchers for additional TensorFlow ops.\n     math_ops.abs,\n     math_ops.negative,\n+    math_ops.asinh,\n+    math_ops.sin,\n+    math_ops.tan,\n+    math_ops.atan,\n+    math_ops.asin,\n+    math_ops.atanh,\n     math_ops.sign,\n     math_ops.square,\n     math_ops.sqrt,"
        },
        {
            "sha": "0b3e6d9f974984a3cf9920525c1fe869d6cd7551",
            "filename": "tensorflow/python/ops/sparse_ops_test.py",
            "status": "modified",
            "additions": 31,
            "deletions": 1,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2Fops%2Fsparse_ops_test.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2Fops%2Fsparse_ops_test.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Fops%2Fsparse_ops_test.py?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -108,6 +108,36 @@ def testSparseExpandDims(self):\n       (math_ops.negative, [1.0, -1.0, 3.0, -4.0], [-1.0, 1.0, -3.0, 4.0]),\n       (math_ops.sign, [3.0, -2.0, 0.0, -4.0], [1.0, -1.0, 0.0, -1.0]),\n       (math_ops.square, [1.0, -1.0, 3.0, -4.0], [1.0, 1.0, 9.0, 16.0]),\n+      (\n+          math_ops.asinh,\n+          [1.0, -1.0, 3.0, -4.0],\n+          [0.8813736, -0.8813736, 1.8184465, -2.0947125],\n+      ),\n+      (\n+          math_ops.sin,\n+          [1.0, -1.0, 3.0, -4.0],\n+          [0.84147096, -0.84147096, 0.14112, 0.7568025],\n+      ),\n+      (\n+          math_ops.asin,\n+          [1.0, -1.0, 0.4, -0.5],\n+          [1.5707964, -1.5707964, 0.41151685, -0.5235988],\n+      ),\n+      (\n+          math_ops.tan,\n+          [1.0, -1.0, 0.4, -0.5],\n+          [1.5574077, -1.5574077, 0.42279324, -0.5463025],\n+      ),\n+      (\n+          math_ops.atan,\n+          [0.4, -0.4, 1.0, 0.5],\n+          [0.3805064, -0.3805064, 0.7853982, 0.4636476],\n+      ),\n+      (\n+          math_ops.atanh,\n+          [0.4, -0.4, -0.5, 0.5],\n+          [0.42364895, -0.42364895, -0.54930615, 0.54930615],\n+      ),\n   ])\n   def testUnarySparseDispatch(self, op, values, expected):\n     st = sparse_tensor.SparseTensor(\n@@ -117,7 +147,7 @@ def testUnarySparseDispatch(self, op, values, expected):\n     result = op(st)\n     result_value = self.evaluate(result)\n     self.assertAllEqual(result_value.indices, st.indices)\n-    self.assertAllEqual(result_value.values, expected)\n+    self.assertAllClose(result_value.values, expected)\n     self.assertAllEqual(result_value.dense_shape, st.dense_shape)\n \n   def testSparseToDenseGradient(self):"
        },
        {
            "sha": "1af261667bbc1f54754c46f168ef0aa919ea8a84",
            "filename": "tensorflow/python/tpu/tpu_embedding_v3_checkpoint_adapter.py",
            "status": "modified",
            "additions": 92,
            "deletions": 50,
            "changes": 142,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3_checkpoint_adapter.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3_checkpoint_adapter.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3_checkpoint_adapter.py?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -150,13 +150,13 @@ def _unshard_from_sc_to_cpu(\n   logging.vlog(\n       1,\n       \"To unshuffle_from_sc_to_cpu on stacked_table.shape: %s\",\n-      stacked_table[0].shape,\n+      stacked_table.shape,\n   )\n   ret_tensors = []\n \n   for layout in from_shard_layouts:\n     padded_table = tpu_embedding_v3_utils.unshuffle_from_sc_to_cpu(\n-        stacked_table[0],\n+        stacked_table,\n         num_sparse_cores=layout.num_sparse_cores,\n         offset_in_shard=layout.sparse_core_shard_row_offset,\n         size_in_shard=layout.unsharded_padded_shape[0]\n@@ -279,20 +279,19 @@ class EmbeddingReshardCallback(checkpoint_adapter.ReshardCallback):\n   def __init__(\n       self,\n       object_local_name: str,\n-      from_shard_layouts: Sequence[\n-          sparse_core_layout_pb2.SparseCoreTableLayout\n-      ],  # table name to layout\n-      to_shard_layouts: Sequence[\n-          sparse_core_layout_pb2.SparseCoreTableLayout\n-      ],  # table name to layout\n+      from_shard_layouts: Mapping[\n+          str, Sequence[sparse_core_layout_pb2.SparseCoreTableLayout]\n+      ],\n+      to_shard_layouts: Sequence[sparse_core_layout_pb2.SparseCoreTableLayout],\n   ):\n     \"\"\"Initializes  Reshard callback.\n \n     Args:\n       object_local_name:  The local name of the object being restored.\n-      from_shard_layouts: layouts as in checkpoint being restored from.\n-      to_shard_layouts: target layouts as specified in the embedding being\n-        restored.\n+      from_shard_layouts: A dictionary in stacked table name to a list of its\n+        consituent table layouts.  The layouts are coming from the checkpoint\n+        being restored.\n+      to_shard_layouts: a list of target layouts that will be resharded to.\n     \"\"\"\n     logging.info(\"Creating EmbeddingReshardCallback for %s\", object_local_name)\n     self._object_local_name = object_local_name\n@@ -322,55 +321,76 @@ def update_restore_inputs(\n       restore_v2 op will usually be passed to reshard method of this class to\n       get the final resharded value.\n     \"\"\"\n-    logging.vlog(\n-        1,\n-        \"Updating restore v2 inputs for %s[%s]: %s\",\n-        checkpoint_key,\n-        self._object_local_name,\n-        shape_and_slice_spec,\n-    )\n-\n+    keys = []\n     slices = []\n+    for stacked_name, table_layouts in self._from_shard_layouts.items():\n+      key = checkpoint_key.replace(self._object_local_name, stacked_name)\n+      keys.append(key)\n+\n+      # use the first layout get the full shape of the stacked table\n+      first_layout = table_layouts[0]\n+      full_vocab_size = (\n+          first_layout.total_rows_per_sparse_core_shard\n+          * first_layout.num_sparse_cores\n+      )\n+      stack_dim = first_layout.unsharded_padded_shape[1]\n+      full_shape = [full_vocab_size, stack_dim]\n+      slices.append(\n+          _shard_info_str(\n+              full_shape,\n+              trackable_base.ShardInfo(offset=[0, 0], shape=full_shape),\n+          )\n+      )\n \n-    # use the first layout get the full shape of the stacked table\n-    first_layout = self._from_shard_layouts[0]\n-    full_vocab_size = (\n-        first_layout.total_rows_per_sparse_core_shard\n-        * first_layout.num_sparse_cores\n-    )\n-    stack_dim = first_layout.unsharded_padded_shape[1]\n-    full_shape = [full_vocab_size, stack_dim]\n-    logging.vlog(\n-        1,\n-        \"Read checkpoint_key %s: %s\",\n+    logging.info(\n+        \"Updating restore v2 inputs for %s[%s]:%s to stacked_tables: [%s],\"\n+        \" slices: [%s]\",\n         checkpoint_key,\n-        full_shape,\n+        self._object_local_name,\n+        shape_and_slice_spec,\n+        \", \".join(keys),\n+        \", \".join(slices),\n     )\n \n-    slices.append(\n-        _shard_info_str(\n-            full_shape,\n-            trackable_base.ShardInfo(offset=[0, 0], shape=full_shape),\n-        )\n-    )\n-    return ([checkpoint_key], slices)\n+    return (keys, slices)\n \n   def reshard(\n-      self, checkpoint_values: tensor.Tensor, shape_and_slice: str\n+      self,\n+      checkpoint_values: Sequence[tensor.Tensor],\n+      shape_and_slice: str,\n   ) -> tensor.Tensor:\n     # unshard\n     stime = time.time()\n-    logging.vlog(\n-        1,\n-        \"EmbeddingReshardCallback: starting to reshard [%s]\",\n+    logging.info(\n+        \"EmbeddingReshardCallback: starting to reshard [%s],\"\n+        \" from checkpoint_value with shapes: %s\",\n         self._object_local_name,\n+        \", \".join([str(t.shape) for t in checkpoint_values]),\n     )\n-    unsharded_tensors = _unshard_from_sc_to_cpu(\n-        checkpoint_values, self._from_shard_layouts\n-    )\n \n+    unsharded_tables = dict()\n+\n+    for stacked_table, layouts in zip(\n+        checkpoint_values,\n+        list(self._from_shard_layouts.values()),\n+    ):\n+      logging.info(\n+          \"Unshard sc_to_cpu stacked_table: %s, shape: %s, no. of constituent\"\n+          \" tables: %d\",\n+          layouts[0].stacked_table_name,\n+          stacked_table.shape,\n+          len(layouts),\n+      )\n+\n+      unsharded_tensors = _unshard_from_sc_to_cpu(stacked_table, layouts)\n+      for unshared_tensor, layout in zip(unsharded_tensors, layouts):\n+        unsharded_tables[layout.table_name] = unshared_tensor\n+\n+    required_tables = [\n+        unsharded_tables[layout.table_name] for layout in self._to_shard_layouts\n+    ]\n     ret = _shard_from_cpu_to_sc(\n-        unsharded_tensors, shape_and_slice, self._to_shard_layouts\n+        required_tables, shape_and_slice, self._to_shard_layouts\n     )\n \n     etime = time.time()\n@@ -385,7 +405,16 @@ def reshard(\n def _reorg_layouts(\n     layouts: Sequence[sparse_core_layout_pb2.SparseCoreTableLayout],\n ) -> Mapping[str, Sequence[sparse_core_layout_pb2.SparseCoreTableLayout]]:\n-  \"\"\"Reorg the layouts to be in the order of the logical table.\"\"\"\n+  \"\"\"Reorg the layouts to be in the order of the logical table.\n+\n+    Building a Dict[StackedTableName, SortedList[TableLayout]]\n+\n+  Args:\n+    layouts: The layouts to be reorged.\n+\n+  Returns:\n+    A dict of stacked table name to sorted list of table layouts.\n+  \"\"\"\n   stacked_name_to_table_names = collections.defaultdict(list)\n   for layout in layouts:\n     stacked_name_to_table_names[layout.stacked_table_name].append(layout)\n@@ -470,12 +499,25 @@ def initialize_reshard_callbacks(\n     # Reshard to different SC Layout\n     from_layouts = _reorg_layouts(list(self._checkpoint_layouts.values()))\n     to_layouts = _reorg_layouts(list(embedding_layouts.values()))\n-    for stacked_name in from_layouts.keys():\n-      logging.info(\"Creating resharding plan for %s\", stacked_name)\n+    for stacked_name, table_layouts in to_layouts.items():\n+      # look for required stacked tables\n+      required_stacked_tables = dict()\n+      for table_layout in table_layouts:\n+        for from_stacked_name, from_table_layouts in from_layouts.items():\n+          if table_layout.table_name in {\n+              layout.table_name for layout in from_table_layouts\n+          }:\n+            required_stacked_tables[from_stacked_name] = from_table_layouts\n+\n+      logging.info(\n+          \"Creating resharding plan for %s, required stacked_tables: %s\",\n+          stacked_name,\n+          \", \".join(required_stacked_tables.keys()),\n+      )\n       self._checkpoint_to_reshard_callback[stacked_name] = (\n           EmbeddingReshardCallback(\n               object_local_name=stacked_name,\n-              from_shard_layouts=from_layouts[stacked_name],\n+              from_shard_layouts=required_stacked_tables,\n               to_shard_layouts=to_layouts[stacked_name],\n           )\n       )"
        },
        {
            "sha": "7f9252550efcff83e696107ca84f08fca3d87e37",
            "filename": "tensorflow/python/tpu/tpu_embedding_v3_checkpoint_adapter_test.py",
            "status": "modified",
            "additions": 217,
            "deletions": 2,
            "changes": 219,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3_checkpoint_adapter_test.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3_checkpoint_adapter_test.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3_checkpoint_adapter_test.py?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -376,12 +376,12 @@ def test_adapt_to_different_sharded_stacked(self):\n     callback = sc_to_sc_adapter.get_reshard_callback(\"one_two_three\")\n     self.assertEqual(callback.object_name(), \"one_two_three\")\n     updated_keys, updated_slices = callback.update_restore_inputs(\n-        \"path/to/embedding/one_two/in/checkpoint\", \"24 8 6,12:0,8\"\n+        \"path/to/embedding/one_two_three/in/checkpoint\", \"24 8 6,12:0,8\"\n     )\n     self.assertAllEqual(\n         updated_keys,\n         [\n-            \"path/to/embedding/one_two/in/checkpoint\",\n+            \"path/to/embedding/one_two_three/in/checkpoint\",\n         ],\n     )\n     self.assertAllEqual(\n@@ -454,6 +454,221 @@ def test_adapt_to_different_sharded_stacked(self):\n         callback.reshard([one_two_three], \"32 8 8,16:0,8\"),\n     )\n \n+  def test_adapt_to_different_sc_table_stacking(self):\n+\n+    source_layouts = {\n+        \"one\": create_layout(\n+            tables_name=\"one\",\n+            stacked_table_name=\"one_two\",\n+            num_sparse_cores=4,\n+            num_partitions=2,\n+            unsharded_shape=(6, 5),\n+            unsharded_padded_shape=(8, 8),\n+            row_offset=0,\n+            shard_rotation=0,\n+            total_rows_per_sparse_core_shard=4,\n+        ),\n+        \"two\": create_layout(\n+            tables_name=\"two\",\n+            stacked_table_name=\"one_two\",\n+            num_sparse_cores=4,\n+            num_partitions=2,\n+            unsharded_shape=(7, 4),\n+            unsharded_padded_shape=(8, 8),\n+            row_offset=2,\n+            shard_rotation=1,\n+            total_rows_per_sparse_core_shard=4,\n+        ),\n+        \"three\": create_layout(\n+            tables_name=\"three\",\n+            stacked_table_name=\"three\",\n+            num_sparse_cores=4,\n+            num_partitions=2,\n+            unsharded_shape=(15, 3),\n+            unsharded_padded_shape=(16, 8),\n+            row_offset=0,\n+            shard_rotation=0,\n+            total_rows_per_sparse_core_shard=4,\n+        ),\n+    }\n+    src_layouts_pb = sparse_core_layout_pb2.SparseCoreTableLayouts()\n+    src_layouts_pb.tables.extend(source_layouts.values())\n+\n+    sc_to_sc_adapter = (\n+        tpu_embedding_v3_checkpoint_adapter.TpuEmbeddingV3CheckpointAdapter(\n+            layouts=src_layouts_pb\n+        )\n+    )\n+\n+    target_layouts = {\n+        \"one\": create_layout(\n+            tables_name=\"one\",\n+            stacked_table_name=\"one\",\n+            num_sparse_cores=8,\n+            num_partitions=4,\n+            unsharded_shape=(6, 5),\n+            unsharded_padded_shape=(8, 8),\n+            row_offset=0,\n+            shard_rotation=0,\n+            total_rows_per_sparse_core_shard=1,\n+        ),\n+        \"two\": create_layout(\n+            tables_name=\"two\",\n+            stacked_table_name=\"two_three\",\n+            num_sparse_cores=8,\n+            num_partitions=4,\n+            unsharded_shape=(7, 4),\n+            unsharded_padded_shape=(8, 8),\n+            row_offset=0,\n+            shard_rotation=0,\n+            total_rows_per_sparse_core_shard=3,\n+        ),\n+        \"three\": create_layout(\n+            tables_name=\"three\",\n+            stacked_table_name=\"two_three\",\n+            num_sparse_cores=8,\n+            num_partitions=4,\n+            unsharded_shape=(15, 3),\n+            unsharded_padded_shape=(16, 8),\n+            row_offset=1,\n+            shard_rotation=1,\n+            total_rows_per_sparse_core_shard=3,\n+        ),\n+    }\n+\n+    # this take a mapping[str, sparse_core_layout_pb2.SparseCoreTableLayout]\n+    sc_to_sc_adapter.initialize_reshard_callbacks(target_layouts)\n+\n+    src_one_two = tf_constant([\n+        # shard 0\n+        [0, 0, 0, 0, 0, 0, 0, 0],\n+        [4, 4, 4, 4, 4, 0, 0, 0],\n+        [13, 13, 13, 13, 0, 0, 0, 0],\n+        [0, 0, 0, 0, 0, 0, 0, 0],\n+        # shard 1\n+        [1, 1, 1, 1, 1, 0, 0, 0],\n+        [5, 5, 5, 5, 5, 0, 0, 0],\n+        [10, 10, 10, 10, 0, 0, 0, 0],\n+        [14, 14, 14, 14, 0, 0, 0, 0],\n+        # shard 2\n+        [2, 2, 2, 2, 2, 0, 0, 0],\n+        [6, 6, 6, 6, 6, 0, 0, 0],\n+        [11, 11, 11, 11, 0, 0, 0, 0],\n+        [15, 15, 15, 15, 0, 0, 0, 0],\n+        # shard 3\n+        [3, 3, 3, 3, 3, 0, 0, 0],\n+        [7, 7, 7, 7, 7, 0, 0, 0],\n+        [12, 12, 12, 12, 0, 0, 0, 0],\n+        [16, 16, 16, 16, 0, 0, 0, 0],\n+    ])\n+\n+    src_three = tf_constant([\n+        # shard 0\n+        [100, 100, 100, 0, 0, 0, 0, 0],\n+        [104, 104, 104, 0, 0, 0, 0, 0],\n+        [108, 108, 108, 0, 0, 0, 0, 0],\n+        [112, 112, 112, 0, 0, 0, 0, 0],\n+        # shard 1\n+        [101, 101, 101, 0, 0, 0, 0, 0],\n+        [105, 105, 105, 0, 0, 0, 0, 0],\n+        [109, 109, 109, 0, 0, 0, 0, 0],\n+        [113, 113, 113, 0, 0, 0, 0, 0],\n+        # shard 2\n+        [102, 102, 102, 0, 0, 0, 0, 0],\n+        [106, 106, 106, 0, 0, 0, 0, 0],\n+        [110, 110, 110, 0, 0, 0, 0, 0],\n+        [114, 114, 114, 0, 0, 0, 0, 0],\n+        # shard 3\n+        [103, 103, 103, 0, 0, 0, 0, 0],\n+        [107, 107, 107, 0, 0, 0, 0, 0],\n+        [111, 111, 111, 0, 0, 0, 0, 0],\n+        [0, 0, 0, 0, 0, 0, 0, 0],\n+    ])\n+\n+    with self.subTest(\"one\"):\n+      callback = sc_to_sc_adapter.get_reshard_callback(\"one\")\n+      self.assertEqual(callback.object_name(), \"one\")\n+      updated_keys, updated_slices = callback.update_restore_inputs(\n+          \"path/to/embedding/one/in/checkpoint\", \"8 8 2,3:0,8\"\n+      )\n+\n+      self.assertAllEqual(\n+          updated_keys,\n+          [\n+              \"path/to/embedding/one_two/in/checkpoint\",\n+          ],\n+      )\n+      self.assertAllEqual(\n+          updated_slices,\n+          [\"16 8 0,16:0,8\"],\n+      )\n+\n+      self.assertAllEqual(\n+          tf_constant([\n+              [2, 2, 2, 2, 2, 0, 0, 0],\n+              [3, 3, 3, 3, 3, 0, 0, 0],\n+              [4, 4, 4, 4, 4, 0, 0, 0],\n+              [5, 5, 5, 5, 5, 0, 0, 0],\n+          ]),\n+          callback.reshard([src_one_two], \"8 8 2,4:0,8\"),\n+      )\n+\n+    self.assertAllEqual(\n+        tf_constant([\n+            [4, 4, 4, 4, 4, 0, 0, 0],\n+            [5, 5, 5, 5, 5, 0, 0, 0],\n+            [0, 0, 0, 0, 0, 0, 0, 0],\n+            [0, 0, 0, 0, 0, 0, 0, 0],\n+        ]),\n+        callback.reshard([src_one_two], \"8 8 4,4:0,8\"),\n+    )\n+\n+    with self.subTest(\"two_three\"):\n+      callback = sc_to_sc_adapter.get_reshard_callback(\"two_three\")\n+      self.assertEqual(callback.object_name(), \"two_three\")\n+      updated_keys, updated_slices = callback.update_restore_inputs(\n+          \"path/to/embedding/two_three/in/checkpoint\", \"24 8 8,6:0,8\"\n+      )\n+      self.assertAllEqual(\n+          updated_keys,\n+          [\n+              \"path/to/embedding/one_two/in/checkpoint\",\n+              \"path/to/embedding/three/in/checkpoint\",\n+          ],\n+      )\n+      self.assertAllEqual(\n+          updated_slices,\n+          [\"16 8 0,16:0,8\", \"16 8 0,16:0,8\"],\n+      )\n+\n+    self.assertAllEqual(\n+        tf_constant([\n+            #  shard 2\n+            [12, 12, 12, 12, 0, 0, 0, 0],\n+            [101, 101, 101, 0, 0, 0, 0, 0],\n+            [109, 109, 109, 0, 0, 0, 0, 0],\n+            #  shard 3\n+            [13, 13, 13, 13, 0, 0, 0, 0],\n+            [102, 102, 102, 0, 0, 0, 0, 0],\n+            [110, 110, 110, 0, 0, 0, 0, 0],\n+        ]),\n+        callback.reshard([src_one_two, src_three], \"24 8 6,6:0,8\"),\n+    )\n+\n+    self.assertAllEqual(\n+        tf_constant([\n+            #  shard 6\n+            [16, 16, 16, 16, 0, 0, 0, 0],\n+            [105, 105, 105, 0, 0, 0, 0, 0],\n+            [113, 113, 113, 0, 0, 0, 0, 0],\n+            #  shard 7\n+            [0, 0, 0, 0, 0, 0, 0, 0],\n+            [106, 106, 106, 0, 0, 0, 0, 0],\n+            [114, 114, 114, 0, 0, 0, 0, 0],\n+        ]),\n+        callback.reshard([src_one_two, src_three], \"24 8 18,6:0,8\"),\n+    )\n+\n \n if __name__ == \"__main__\":\n   v2_compat.enable_v2_behavior()"
        },
        {
            "sha": "1a7b0e6a55d236c0469d935b575d7bf175ca7ae5",
            "filename": "tensorflow/python/tpu/tpu_embedding_v3_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3_utils.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3_utils.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fpython%2Ftpu%2Ftpu_embedding_v3_utils.py?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -58,8 +58,8 @@ def unshuffle_from_sc_to_cpu(\n   # checkpoints value to meet this requirement.\n   if t.shape[0] % num_sparse_cores != 0:\n     raise ValueError(\n-        \"The dim of table ({}) should be multiple of number of sparse cores\"\n-        \" ({})\".format(t.shape[1], num_sparse_cores)\n+        \"The first dim of the table ({}) should be multiple of number of sparse\"\n+        \" cores ({})\".format(t.shape[0], num_sparse_cores)\n     )\n   # get shards in the input t\n   shards_t = array_ops.reshape("
        },
        {
            "sha": "d6b2fae58e1dcb3533bb344f28a86c2010c54181",
            "filename": "tensorflow/tensorflow.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftensorflow.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftensorflow.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftensorflow.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1,5 +1,6 @@\n \"\"\"Provides build configuration for TensorFlow.\"\"\"\n \n+load(\"@rules_cc//cc/common:cc_info.bzl\", \"CcInfo\")\n load(\"@rules_java//java:defs.bzl\", \"java_test\")\n load(\n     \"//tensorflow:py.default.bzl\","
        },
        {
            "sha": "0872eca2d9b21dff4d1b3ea60727e7e396473dd6",
            "filename": "tensorflow/tools/benchmark/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Fbenchmark%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Fbenchmark%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Fbenchmark%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -45,6 +45,7 @@ cc_library(\n         \"//tensorflow/core/platform:numbers\",\n         \"//tensorflow/core/util:stats_calculator_portable\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n         \"@local_xla//xla/tsl/platform:status\",\n     ],\n )"
        },
        {
            "sha": "fad14ad8eb647d26d36d4cf0a69578eb57d6cb01",
            "filename": "tensorflow/tools/benchmark/benchmark_model.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Fbenchmark%2Fbenchmark_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Fbenchmark%2Fbenchmark_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Fbenchmark%2Fbenchmark_model.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/status.h\"\n #include \"tensorflow/core/framework/graph.pb.h\"\n@@ -649,17 +650,17 @@ int Main(int argc, char** argv) {\n     }\n     string pretty_flops;\n     if (total_flops < 1000) {\n-      pretty_flops = strings::StrCat(total_flops, \" FLOPs\");\n+      pretty_flops = absl::StrCat(total_flops, \" FLOPs\");\n     } else if (total_flops < (1000 * 1000)) {\n       const float rounded_flops = (total_flops / 1000.0f);\n-      pretty_flops = strings::StrCat(rounded_flops, \"k FLOPs\");\n+      pretty_flops = absl::StrCat(rounded_flops, \"k FLOPs\");\n     } else if (total_flops < (1000 * 1000 * 1000)) {\n       const float rounded_flops = round(total_flops / 1000.0f) / 1000.0f;\n-      pretty_flops = strings::StrCat(rounded_flops, \" million FLOPs\");\n+      pretty_flops = absl::StrCat(rounded_flops, \" million FLOPs\");\n     } else {\n       const float rounded_flops =\n           round(total_flops / (1000.0f * 1000.0f)) / 1000.0f;\n-      pretty_flops = strings::StrCat(rounded_flops, \" billion FLOPs\");\n+      pretty_flops = absl::StrCat(rounded_flops, \" billion FLOPs\");\n     }\n     LOG(INFO) << \"FLOPs estimate: \" << strings::HumanReadableNum(total_flops);\n     const double mean_run_time = no_stat_wall_time / no_stat_num_runs;"
        },
        {
            "sha": "74776d2680eb67c899562e443b32cfee24485007",
            "filename": "tensorflow/tools/pip_package/BUILD",
            "status": "modified",
            "additions": 23,
            "deletions": 2,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Fpip_package%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Fpip_package%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Fpip_package%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1,6 +1,7 @@\n # Description:\n #  Tools for building the TensorFlow pip package.\n \n+load(\"@cuda_cudart//:version.bzl\", _cudart_version = \"VERSION\")\n load(\"@local_config_cuda//cuda:build_defs.bzl\", \"if_cuda\")\n load(\"@local_config_syslibs//:build_defs.bzl\", \"if_not_system_lib\")\n load(\n@@ -13,6 +14,7 @@ load(\n )\n load(\"@local_xla//third_party/py:python_wheel.bzl\", \"collect_data_files\", \"transitive_py_deps\")\n load(\"@local_xla//xla/tsl/mkl:build_defs.bzl\", \"if_enable_mkl\", \"if_mkl\", \"if_mkl_ml\")\n+load(\"@nvidia_wheel_versions//:versions.bzl\", \"NVIDIA_WHEEL_VERSIONS\")\n load(\"//tensorflow:tensorflow.bzl\", \"if_wheel_dependency\", \"if_with_tpu_support\", \"transitive_hdrs\")\n load(\"//tensorflow:tf_version.bzl\", \"TF_SEMANTIC_VERSION_SUFFIX\", \"TF_VERSION\")\n load(\n@@ -268,15 +270,33 @@ transitive_py_deps(\n     deps = COMMON_PIP_DEPS,\n )\n \n+py_binary(\n+    name = \"modify_setup_py_binary\",\n+    srcs = [\n+        \"modify_setup_py.py\",\n+    ],\n+    main = \"modify_setup_py.py\",\n+    deps = [\"@local_xla//third_party/py:setup_py_nvidia_dependencies_util\"],\n+)\n+\n genrule(\n     name = \"setup_py\",\n     srcs = [\"setup.py.tpl\"],\n     outs = [\"setup.py\"],\n-    cmd = \"\"\"sed -E \"s/_VERSION = '0.0.0'/_VERSION = '{wheel_version}{wheel_version_suffix}'/\" \\\n-$(location setup.py.tpl) > $@;\"\"\".format(\n+    cmd = \"\"\"\n+      $(location :modify_setup_py_binary) \\\\\n+          --template_file $(location setup.py.tpl) \\\\\n+          --output_file $(OUTS) \\\\\n+          --nvidia_wheel_versions_data '{nvidia_wheel_versions}' \\\\\n+          --tf_version \"{wheel_version}{wheel_version_suffix}\" \\\\\n+          --cuda_version {cuda_version}\n+    \"\"\".format(\n+        cuda_version = _cudart_version or \"12\",\n+        nvidia_wheel_versions = NVIDIA_WHEEL_VERSIONS,\n         wheel_version = TF_VERSION,\n         wheel_version_suffix = TF_SEMANTIC_VERSION_SUFFIX,\n     ),\n+    tools = [\":modify_setup_py_binary\"],\n )\n \n py_binary(\n@@ -435,6 +455,7 @@ py_import(\n     wheel_deps = if_cuda([\n         \"@pypi_nvidia_cublas_cu12//:pkg\",\n         \"@pypi_nvidia_cuda_cupti_cu12//:pkg\",\n+        \"@pypi_nvidia_cuda_nvcc_cu12//:pkg\",\n         \"@pypi_nvidia_cuda_nvrtc_cu12//:pkg\",\n         \"@pypi_nvidia_cuda_runtime_cu12//:pkg\",\n         \"@pypi_nvidia_cudnn_cu12//:pkg\","
        },
        {
            "sha": "7f78a437060471379c7548128b01d3782e0bdd20",
            "filename": "tensorflow/tools/pip_package/modify_setup_py.py",
            "status": "added",
            "additions": 97,
            "deletions": 0,
            "changes": 97,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Fpip_package%2Fmodify_setup_py.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Fpip_package%2Fmodify_setup_py.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Fpip_package%2Fmodify_setup_py.py?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -0,0 +1,97 @@\n+# Copyright 2025 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License..\n+# ==============================================================================\n+\"\"\"Modify setup.py with TensorFlow and NVIDIA wheel versions.\"\"\"\n+\n+import argparse\n+import pathlib\n+import third_party.py.setup_py_nvidia_dependencies_util as util\n+\n+\n+def _update_setup_with_tf_and_nvidia_wheel_versions(\n+    template_path: pathlib.Path,\n+    output_path: pathlib.Path,\n+    nvidia_wheel_versions_data: str,\n+    tf_version: str,\n+    cuda_version: str,\n+):\n+  \"\"\"Updates a setup.py template with TensorFlow and NVIDIA wheel versions.\n+\n+  This function reads a setup.py template file, replaces placeholder versions\n+  for TensorFlow and various NVIDIA-related wheels based on the provided\n+  data, and writes the result to an output file.\n+\n+  Args:\n+    template_path: Path to the input setup.py.tpl template file.\n+    output_path: Path where the modified setup.py file will be written.\n+    nvidia_wheel_versions_data: A string containing NVIDIA wheel version data,\n+      with each line in the format \"wheel_name version_spec\".\n+    tf_version: The version string for the TensorFlow package.\n+    cuda_version: The CUDA version string.\n+  \"\"\"\n+\n+  with open(template_path) as f:\n+    content = f.read()\n+\n+  content = content.replace(\"_VERSION = '0.0.0'\", f\"_VERSION = '{tf_version}'\")\n+  content = util.get_setup_py_content_with_nvidia_wheel_versions(\n+      content, cuda_version, nvidia_wheel_versions_data\n+  )\n+\n+  with open(output_path, \"w\") as f:\n+    f.write(content)\n+\n+\n+if __name__ == \"__main__\":\n+  parser = argparse.ArgumentParser()\n+  parser.add_argument(\n+      \"--template_file\",\n+      type=pathlib.Path,\n+      required=True,\n+      help=\"Path to the setup.py.tpl template file\",\n+  )\n+  parser.add_argument(\n+      \"--output_file\",\n+      type=pathlib.Path,\n+      required=True,\n+      help=\"Path to write the generated setup.py file\",\n+  )\n+  parser.add_argument(\n+      \"--nvidia_wheel_versions_data\",\n+      default=None,\n+      required=True,\n+      help=\"NVIDIA wheel versions data\",\n+  )\n+  parser.add_argument(\n+      \"--cuda_version\",\n+      type=str,\n+      required=True,\n+      help=\"The CUDA version string\",\n+      default=\"12\",\n+  )\n+  parser.add_argument(\n+      \"--tf_version\",\n+      type=str,\n+      required=True,\n+      help=\"The TensorFlow package version string\",\n+  )\n+  args = parser.parse_args()\n+\n+  _update_setup_with_tf_and_nvidia_wheel_versions(\n+      args.template_file,\n+      args.output_file,\n+      args.nvidia_wheel_versions_data,\n+      args.tf_version,\n+      args.cuda_version,\n+  )"
        },
        {
            "sha": "3ee660591813b08c36a33075c39d88212682dbd5",
            "filename": "tensorflow/tools/pip_package/setup.py.tpl",
            "status": "modified",
            "additions": 28,
            "deletions": 12,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Fpip_package%2Fsetup.py.tpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Fpip_package%2Fsetup.py.tpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Fpip_package%2Fsetup.py.tpl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -54,6 +54,22 @@ from setuptools.dist import Distribution\n # result for pip.\n _VERSION = '0.0.0'\n \n+cuda_version = 0  # placeholder\n+cuda_wheel_suffix = ''  # placeholder\n+\n+nvidia_cublas_version = ''  # placeholder\n+nvidia_cuda_cupti_version = ''  # placeholder\n+nvidia_cuda_nvcc_version = ''  # placeholder\n+nvidia_cuda_runtime_version = ''  # placeholder\n+nvidia_cudnn_version = ''  # placeholder\n+nvidia_cufft_version = ''  # placeholder\n+nvidia_cusolver_version = ''  # placeholder\n+nvidia_cusparse_version = ''  # placeholder\n+nvidia_nccl_version = ''  # placeholder\n+nvidia_nvjitlink_version = ''  # placeholder\n+nvidia_cuda_nvrtc_version = ''  # placeholder\n+nvidia_curand_version = ''  # placeholder\n+\n # We use the same setup.py for all tensorflow_* packages and for the nightly\n # equivalents (tf_nightly_*). The package is controlled from the argument line\n # when building the pip package.\n@@ -145,18 +161,18 @@ if collaborator_build:\n EXTRA_PACKAGES = {\n     'and-cuda': [\n         # TODO(nluehr): set nvidia-* versions based on build components.\n-        'nvidia-cublas-cu12 >= 12.5.3.2, < 13.0',\n-        'nvidia-cuda-cupti-cu12 >= 12.5.82, < 13.0',\n-        'nvidia-cuda-nvcc-cu12 >= 12.5.82, < 13.0',\n-        'nvidia-cuda-nvrtc-cu12 >= 12.5.82, < 13.0',\n-        'nvidia-cuda-runtime-cu12 >= 12.5.82, < 13.0',\n-        'nvidia-cudnn-cu12 >= 9.3.0.75, < 10.0',\n-        'nvidia-cufft-cu12 >= 11.2.3.61, < 12.0',\n-        'nvidia-curand-cu12 >= 10.3.6.82, < 11.0',\n-        'nvidia-cusolver-cu12 >= 11.6.3.83, < 12.0',\n-        'nvidia-cusparse-cu12 >= 12.5.1.3, < 13.0',\n-        'nvidia-nccl-cu12 >= 2.27.7, < 3.0',\n-        'nvidia-nvjitlink-cu12 >= 12.5.82, < 13.0',\n+        f'nvidia-cublas{cuda_wheel_suffix}{nvidia_cublas_version}',\n+        f'nvidia-cuda-cupti{cuda_wheel_suffix}{nvidia_cuda_cupti_version}',\n+        f'nvidia-cuda-nvcc{cuda_wheel_suffix}{nvidia_cuda_nvcc_version}',\n+        f'nvidia-cuda-nvrtc{cuda_wheel_suffix}{nvidia_cuda_nvrtc_version}',\n+        f'nvidia-cuda-runtime{cuda_wheel_suffix}{nvidia_cuda_runtime_version}',\n+        f'nvidia-cudnn-cu{cuda_version}{nvidia_cudnn_version}',\n+        f'nvidia-cufft{cuda_wheel_suffix}{nvidia_cufft_version}',\n+        f'nvidia-curand{cuda_wheel_suffix}{nvidia_curand_version}',\n+        f'nvidia-cusolver{cuda_wheel_suffix}{nvidia_cusolver_version}',\n+        f'nvidia-cusparse{cuda_wheel_suffix}{nvidia_cusparse_version}',\n+        f'nvidia-nccl-cu{cuda_version}{nvidia_nccl_version}',\n+        f'nvidia-nvjitlink{cuda_wheel_suffix}{nvidia_nvjitlink_version}',\n     ],\n     'gcs-filesystem': [\n         ('tensorflow-io-gcs-filesystem>=0.23.1; '"
        },
        {
            "sha": "cfda7a3f211ecd91d476563d6ac0bd30fa874f18",
            "filename": "tensorflow/tools/toolchains/cross_compile/cc/cc_toolchain_config.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fcross_compile%2Fcc%2Fcc_toolchain_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fcross_compile%2Fcc%2Fcc_toolchain_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Ftoolchains%2Fcross_compile%2Fcc%2Fcc_toolchain_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -40,6 +40,7 @@ load(\n     \"variable_with_value\",\n     \"with_feature_set\",\n )\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n \n def layering_check_features(compiler):\n     if compiler != \"clang\":"
        },
        {
            "sha": "56131cf026c50b898fe99b52ff8b31c7a3acbe75",
            "filename": "tensorflow/tools/toolchains/remote_config/configs.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fremote_config%2Fconfigs.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fremote_config%2Fconfigs.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Ftoolchains%2Fremote_config%2Fconfigs.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -50,7 +50,7 @@ def initialize_rbe_configs():\n     # The `ml-build-rbe` image is identical to the `ml-build` image except for the base image.\n     # The `ml-build`'s base image is a standard `ubuntu22.04` image.\n     # The `ml-build-rbe`'s base image is `nvidia/cuda:12.3.2-base-ubuntu22.04` which has nvidia driver installed.\n-    ml_build_rbe_config(\"docker://us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-rbe@sha256:aaeb29799463729092c05f5ac8393113b3bb5d1ecf085f9f1f2016e3a1ece11c\")\n+    ml_build_rbe_config(\"docker://us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build-rbe@sha256:7bae9f7604645cbad40b11a22294f5058db16022315d52a130b832d07e54c9ef\")\n \n     # TF-Version-Specific SIG Build RBE Configs. The crosstool generated from these\n     # configs are python-version-independent because they only care about the"
        },
        {
            "sha": "ae0527efe74bbfe97f959be4ed2f58702e95450c",
            "filename": "tensorflow/tools/toolchains/win/20240424/armeabi_cc_toolchain_config.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fwin%2F20240424%2Farmeabi_cc_toolchain_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fwin%2F20240424%2Farmeabi_cc_toolchain_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Ftoolchains%2Fwin%2F20240424%2Farmeabi_cc_toolchain_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -19,6 +19,7 @@ load(\n     \"feature\",\n     \"tool_path\",\n )\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n \n def _impl(ctx):\n     toolchain_identifier = \"stub_armeabi-v7a\""
        },
        {
            "sha": "aebbf497f57c5f4236d8a5fbacccbd03a941b19a",
            "filename": "tensorflow/tools/toolchains/win/20240424/windows_cc_toolchain_config.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fwin%2F20240424%2Fwindows_cc_toolchain_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fwin%2F20240424%2Fwindows_cc_toolchain_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Ftoolchains%2Fwin%2F20240424%2Fwindows_cc_toolchain_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -29,6 +29,7 @@ load(\n     \"variable_with_value\",\n     \"with_feature_set\",\n )\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n \n all_compile_actions = [\n     ACTION_NAMES.c_compile,"
        },
        {
            "sha": "f3a4fd42dc80c895189ab741dbc3483a54a46e04",
            "filename": "tensorflow/tools/toolchains/win/bazel_211/armeabi_cc_toolchain_config.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fwin%2Fbazel_211%2Farmeabi_cc_toolchain_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fwin%2Fbazel_211%2Farmeabi_cc_toolchain_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Ftoolchains%2Fwin%2Fbazel_211%2Farmeabi_cc_toolchain_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -19,6 +19,7 @@ load(\n     \"feature\",\n     \"tool_path\",\n )\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n \n def _impl(ctx):\n     toolchain_identifier = \"stub_armeabi-v7a\""
        },
        {
            "sha": "e0f8b70d82287aef8fecde5acd0327bcaea81a35",
            "filename": "tensorflow/tools/toolchains/win/bazel_211/windows_cc_toolchain_config.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fwin%2Fbazel_211%2Fwindows_cc_toolchain_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fwin%2Fbazel_211%2Fwindows_cc_toolchain_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Ftoolchains%2Fwin%2Fbazel_211%2Fwindows_cc_toolchain_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -30,6 +30,7 @@ load(\n     \"variable_with_value\",\n     \"with_feature_set\",\n )\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n \n all_compile_actions = [\n     ACTION_NAMES.c_compile,"
        },
        {
            "sha": "ae0527efe74bbfe97f959be4ed2f58702e95450c",
            "filename": "tensorflow/tools/toolchains/win/tf_win_05022023/armeabi_cc_toolchain_config.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fwin%2Ftf_win_05022023%2Farmeabi_cc_toolchain_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fwin%2Ftf_win_05022023%2Farmeabi_cc_toolchain_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Ftoolchains%2Fwin%2Ftf_win_05022023%2Farmeabi_cc_toolchain_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -19,6 +19,7 @@ load(\n     \"feature\",\n     \"tool_path\",\n )\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n \n def _impl(ctx):\n     toolchain_identifier = \"stub_armeabi-v7a\""
        },
        {
            "sha": "e8af866670eefcba9c279df96068342cbeeb3ea9",
            "filename": "tensorflow/tools/toolchains/win/tf_win_05022023/windows_cc_toolchain_config.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fwin%2Ftf_win_05022023%2Fwindows_cc_toolchain_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fwin%2Ftf_win_05022023%2Fwindows_cc_toolchain_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Ftoolchains%2Fwin%2Ftf_win_05022023%2Fwindows_cc_toolchain_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -29,6 +29,7 @@ load(\n     \"variable_with_value\",\n     \"with_feature_set\",\n )\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n \n all_compile_actions = [\n     ACTION_NAMES.c_compile,"
        },
        {
            "sha": "ae0527efe74bbfe97f959be4ed2f58702e95450c",
            "filename": "tensorflow/tools/toolchains/win2022/20241118/armeabi_cc_toolchain_config.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fwin2022%2F20241118%2Farmeabi_cc_toolchain_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fwin2022%2F20241118%2Farmeabi_cc_toolchain_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Ftoolchains%2Fwin2022%2F20241118%2Farmeabi_cc_toolchain_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -19,6 +19,7 @@ load(\n     \"feature\",\n     \"tool_path\",\n )\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n \n def _impl(ctx):\n     toolchain_identifier = \"stub_armeabi-v7a\""
        },
        {
            "sha": "aebbf497f57c5f4236d8a5fbacccbd03a941b19a",
            "filename": "tensorflow/tools/toolchains/win2022/20241118/windows_cc_toolchain_config.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fwin2022%2F20241118%2Fwindows_cc_toolchain_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Ftools%2Ftoolchains%2Fwin2022%2F20241118%2Fwindows_cc_toolchain_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Ftools%2Ftoolchains%2Fwin2022%2F20241118%2Fwindows_cc_toolchain_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -29,6 +29,7 @@ load(\n     \"variable_with_value\",\n     \"with_feature_set\",\n )\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n \n all_compile_actions = [\n     ACTION_NAMES.c_compile,"
        },
        {
            "sha": "c30ce9be39e9009f19e4a328b76fb192dc80e723",
            "filename": "tensorflow/workspace2.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fworkspace2.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/tensorflow%2Fworkspace2.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fworkspace2.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -391,7 +391,7 @@ def _tf_repositories():\n \n     tf_http_archive(\n         name = \"com_google_protobuf\",\n-        patch_file = [\"@local_xla//third_party/protobuf:protobuf-6.31.1.patch\"],\n+        patch_file = [\"@local_xla//third_party/protobuf:protobuf.patch\"],\n         sha256 = \"6e09bbc950ba60c3a7b30280210cd285af8d7d8ed5e0a6ed101c72aff22e8d88\",\n         strip_prefix = \"protobuf-6.31.1\",\n         urls = tf_mirror_urls(\"https://github.com/protocolbuffers/protobuf/archive/refs/tags/v6.31.1.zip\"),"
        },
        {
            "sha": "5afdeffee932391267a2e40bdc4509db5b14179b",
            "filename": "third_party/py/BUILD.bazel",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fpy%2FBUILD.bazel",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fpy%2FBUILD.bazel",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fpy%2FBUILD.bazel?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -65,3 +65,9 @@ py_binary(\n     main = \"unpack_wheel_and_unzip_archive_files.py\",\n     visibility = [\"//visibility:public\"],\n )\n+\n+py_library(\n+    name = \"setup_py_nvidia_dependencies_util\",\n+    srcs = [\"setup_py_nvidia_dependencies_util.py\"],\n+    visibility = [\"//visibility:public\"],\n+)"
        },
        {
            "sha": "6177f2eb1a52255280c41ece27e53dfc852e7625",
            "filename": "third_party/tf_runtime/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Ftf_runtime%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Ftf_runtime%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Ftf_runtime%2Fworkspace.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -6,8 +6,8 @@ def repo():\n     \"\"\"Imports TFRT.\"\"\"\n \n     # Attention: tools parse and update these lines.\n-    TFRT_COMMIT = \"db460e7893e4accc0971876aff9b29b3382ade80\"\n-    TFRT_SHA256 = \"7065c8f508f61e8e2a579829694febe564d93dccf09f22dcc012762d5e20bc88\"\n+    TFRT_COMMIT = \"4ecc3a44a32c832b748328bed3f9a599f795ca8d\"\n+    TFRT_SHA256 = \"5e81d70f9534340f7ef8e63ec43bdd5971135e48183079be50ecb3f74b1fed66\"\n \n     tf_http_archive(\n         name = \"tf_runtime\","
        },
        {
            "sha": "635a0bffc39ef7b0249bafd47068c48cd8f8b7aa",
            "filename": "third_party/xla/.github/workflows/benchmarks/compare_with_baseline.py",
            "status": "modified",
            "additions": 54,
            "deletions": 24,
            "changes": 78,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2F.github%2Fworkflows%2Fbenchmarks%2Fcompare_with_baseline.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2F.github%2Fworkflows%2Fbenchmarks%2Fcompare_with_baseline.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2F.github%2Fworkflows%2Fbenchmarks%2Fcompare_with_baseline.py?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -95,14 +95,23 @@ def compare_metrics(\n   # {\n   #   ...,\n   #   \"metrics\": {\n-  #     \"GPU_DEVICE_TIME\": { \"value_ms\": 150.0, \"unit\": \"ms\", ... },\n-  #     \"GPU_DEVICE_MEMCPY_TIME\": { \"value_ms\": 1.2, \"unit\": \"ms\", ... },\n+  #     \"GPU_DEVICE_TIME\": { \"value\": 150.0, \"unit\": \"ms\", ... },\n+  #     \"GPU_DEVICE_MEMCPY_TIME\": { \"value\": 1.2, \"unit\": \"ms\", ... },\n+  #     \"PEAK_GPU_MEMORY\": { \"value\": 12.45, \"unit\": \"GB\", ... },\n   #     ...\n   #   }\n   # }\n   # If your actual results.json structure is different, you MUST adapt this\n   # section.\n   actual_metrics_container = results_data.get(\"metrics\")\n+  metric_to_baseline_key = {\n+      \"WALL_TIME\": \"baseline_ms\",\n+      \"GPU_DEVICE_TIME\": \"baseline_ms\",\n+      \"GPU_DEVICE_MEMCPY_TIME\": \"baseline_ms\",\n+      \"CPU_TIME\": \"baseline_ms\",\n+      \"PEAK_CPU_MEMORY\": \"baseline_gb\",\n+      \"PEAK_GPU_MEMORY\": \"baseline_gb\",\n+  }\n \n   if not actual_metrics_container or not isinstance(\n       actual_metrics_container, dict\n@@ -119,33 +128,44 @@ def compare_metrics(\n     sys.exit(0)  # Exit cleanly if no metrics to compare\n \n   for metric_name, baseline_info in config_baselines.items():\n-    if not isinstance(baseline_info, dict) or not all(\n-        key in baseline_info for key in [\"baseline_ms\", \"threshold\"]\n+    if (\n+        not isinstance(baseline_info, dict)\n+        or \"threshold\" not in baseline_info\n+        or not {\"baseline_ms\", \"baseline_gb\"}.intersection(baseline_info.keys())\n     ):\n       summary_messages.append(\n           f\"::warning title=Malformed Baseline::Metric '{metric_name}' in\"\n-          f\" baseline for '{config_id}' is missing 'baseline_ms' or\"\n-          \" 'threshold', or is not structured as a dictionary. Skipping.\"\n+          f\" baseline for '{config_id}' is missing 'threshold', or is missing\"\n+          \" both 'baseline_ms' and 'baseline_gb', or is not structured as a\"\n+          \" dictionary. Skipping.\"\n+      )\n+      continue\n+\n+    baseline_key = metric_to_baseline_key.get(metric_name)\n+    if not baseline_key:\n+      summary_messages.append(\n+          f\"::warning title=Unsupported Metric::Metric '{metric_name}' is not\"\n+          \" supported by this script. Skipping.\"\n       )\n       continue\n \n     try:\n-      baseline_value_ms = float(baseline_info[\"baseline_ms\"])\n+      baseline_value = float(baseline_info[baseline_key])\n       threshold_percentage = float(baseline_info[\"threshold\"])\n     except ValueError:\n       summary_messages.append(\n           f\"::warning title=Invalid Baseline Value::Metric '{metric_name}' in\"\n-          f\" baseline for '{config_id}' has non-numeric 'baseline_ms' or\"\n+          f\" baseline for '{config_id}' has non-numeric '{baseline_key}' or\"\n           \" 'threshold'. Skipping.\"\n       )\n       continue\n \n     # Extract the actual metric value from results.json\n     actual_metric_entry = actual_metrics_container.get(metric_name)\n \n-    if not actual_metric_entry or \"value_ms\" not in actual_metric_entry:\n+    if not actual_metric_entry or \"value\" not in actual_metric_entry:\n       summary_messages.append(\n-          f\"Metric '{metric_name}': Actual value or 'value_ms' key not found in\"\n+          f\"Metric '{metric_name}': Actual value or 'value' key not found in\"\n           \" results, or not a dictionary. Skipping.\"\n       )\n       # For debugging:\n@@ -161,50 +181,60 @@ def compare_metrics(\n       continue\n \n     try:\n-      actual_value_ms = float(actual_metric_entry[\"value_ms\"])\n+      actual_value = float(actual_metric_entry[\"value\"])\n     except (ValueError, TypeError):\n       summary_messages.append(\n           f\"Metric '{metric_name}': Actual value\"\n-          f\" '{actual_metric_entry['value_ms']}' is not a valid number.\"\n+          f\" '{actual_metric_entry['value']}' is not a valid number.\"\n+          \" Skipping.\"\n+      )\n+      continue\n+\n+    actual_unit = actual_metric_entry.get(\"unit\")\n+    if not actual_unit:\n+      summary_messages.append(\n+          f\"Metric '{metric_name}': Actual value unit is not specified.\"\n           \" Skipping.\"\n       )\n       continue\n \n     summary_messages.append(f\"\\nComparing metric: {metric_name}\")\n-    summary_messages.append(f\"  Actual Value: {actual_value_ms:.3f} ms\")\n-    summary_messages.append(f\"  Baseline Value: {baseline_value_ms:.3f} ms\")\n+    summary_messages.append(f\"  Actual Value: {actual_value:.3f} {actual_unit}\")\n+    summary_messages.append(\n+        f\"  Baseline Value: {baseline_value:.3f} {actual_unit}\"\n+    )\n     summary_messages.append(\n         f\"  Allowed Threshold: {threshold_percentage*100:.1f}%\"\n     )\n \n     # Higher value is worse for time-based metrics\n-    allowed_upper_bound = baseline_value_ms * (1.0 + threshold_percentage)\n+    allowed_upper_bound = baseline_value * (1.0 + threshold_percentage)\n     summary_messages.append(\n         \"  Allowed Upper Bound (Baseline * (1 + Threshold)):\"\n-        f\" {allowed_upper_bound:.3f} ms\"\n+        f\" {allowed_upper_bound:.3f} {actual_unit}\"\n     )\n \n-    if actual_value_ms > allowed_upper_bound:\n+    if actual_value > allowed_upper_bound:\n       percentage_diff = 0.0\n       if (\n-          abs(baseline_value_ms) > 1e-9\n+          abs(baseline_value) > 1e-9\n       ):  # Avoid division by zero for very small baselines\n         percentage_diff = (\n-            (actual_value_ms - baseline_value_ms) / baseline_value_ms\n+            (actual_value - baseline_value) / baseline_value\n         ) * 100.0\n       elif (\n-          actual_value_ms > 0\n+          actual_value > 0\n       ):  # If baseline is effectively zero, any positive value is infinitely\n         # worse.\n         percentage_diff = float(\"inf\")\n \n       # Use GitHub Actions error annotation for better visibility\n       error_title = f\"REGRESSION: {metric_name}\"\n       error_details = (\n-          f\"Value {actual_value_ms:.3f} ms is {percentage_diff:.2f}% worse than\"\n-          f\" baseline {baseline_value_ms:.3f} ms. Exceeds threshold of\"\n-          f\" {threshold_percentage*100:.1f}% (max allowed:\"\n-          f\" {allowed_upper_bound:.3f} ms).\"\n+          f\"Value {actual_value:.3f} {actual_unit} is {percentage_diff:.2f}%\"\n+          f\" worse than baseline {baseline_value:.3f} {actual_unit}. Exceeds\"\n+          f\" threshold of {threshold_percentage*100:.1f}% (max allowed:\"\n+          f\" {allowed_upper_bound:.3f} {actual_unit}).\"\n       )\n       summary_messages.append(\n           \"  ::error\""
        },
        {
            "sha": "f579831c0c41b86823c5816c93a37f8676ee279e",
            "filename": "third_party/xla/.github/workflows/benchmarks/run_benchmark.sh",
            "status": "modified",
            "additions": 47,
            "deletions": 39,
            "changes": 86,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2F.github%2Fworkflows%2Fbenchmarks%2Frun_benchmark.sh",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2F.github%2Fworkflows%2Fbenchmarks%2Frun_benchmark.sh",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2F.github%2Fworkflows%2Fbenchmarks%2Frun_benchmark.sh?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -128,49 +128,57 @@ if [ -f \"$XSPACE_FILE_PATH\" ] && [ $RUNNER_EXIT_CODE -eq 0 ]; then\n         key=$(echo \"$key\" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')\n         value=$(echo \"$value\" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')\n \n-        # Expecting lines like \"Metric Name: 123.45 us\"\n-        if [[ \"$value\" == *us ]]; then\n-            num_value=$(echo \"$value\" | sed 's/ us$//')\n-            # Convert microseconds to milliseconds using awk\n-            ms_value=$(LC_ALL=C awk -v num=\"$num_value\" 'BEGIN { printf \"%.3f\", num / 1000 }')\n-\n-            # Sanitize base metric key (e.g., \"Device Time\" -> \"DEVICE_TIME\")\n-            base_metric_key=$(echo \"$key\" | tr ' ' '_' | tr '[:lower:]' '[:upper:]')\n-            final_metric_key=\"\"\n-\n-            # Determine the final metric key based on HARDWARE_CATEGORY for baseline matching\n-            if [[ \"$HARDWARE_CATEGORY\" == GPU* ]]; then\n-                if [[ \"$base_metric_key\" == \"DEVICE_TIME\" ]]; then\n-                    final_metric_key=\"GPU_DEVICE_TIME\"\n-                elif [[ \"$base_metric_key\" == \"DEVICE_MEMCPY_TIME\" ]]; then\n-                    final_metric_key=\"GPU_DEVICE_MEMCPY_TIME\"\n-                # Add other specific GPU mappings here if needed\n-                # else\n-                #    final_metric_key=\"GPU_${base_metric_key}\" # Generic prefix\n-                else\n-                    final_metric_key=\"$base_metric_key\" # If no specific GPU mapping, use base\n-                fi\n-            elif [[ \"$HARDWARE_CATEGORY\" == CPU* ]]; then\n-                if [[ \"$base_metric_key\" == \"CPU_TIME\" ]] || [[ \"$base_metric_key\" == \"TIME\" ]]; then # Handle \"CPU Time\" or just \"Time\"\n-                    final_metric_key=\"CPU_TIME\"\n-                elif [[ \"$base_metric_key\" == \"WALL_TIME\" ]]; then\n-                    final_metric_key=\"WALL_TIME\" # Wall time is generic\n-                # Add other specific CPU mappings here if needed\n-                # else\n-                #    final_metric_key=\"CPU_${base_metric_key}\" # Generic prefix\n-                else\n-                    final_metric_key=\"$base_metric_key\" # If no specific CPU mapping, use base\n-                fi\n+        # Sanitize base metric key (e.g., \"Device Time\" -> \"DEVICE_TIME\")\n+        base_metric_key=$(echo \"$key\" | tr ' ' '_' | tr '[:lower:]' '[:upper:]')\n+        final_metric_key=\"\"\n+\n+        # Determine the final metric key based on HARDWARE_CATEGORY for baseline matching\n+        if [[ \"$HARDWARE_CATEGORY\" == GPU* ]]; then\n+            if [[ \"$base_metric_key\" == \"DEVICE_TIME\" ]]; then\n+                final_metric_key=\"GPU_DEVICE_TIME\"\n+            elif [[ \"$base_metric_key\" == \"DEVICE_MEMCPY_TIME\" ]]; then\n+                final_metric_key=\"GPU_DEVICE_MEMCPY_TIME\"\n+            elif  [[ \"$base_metric_key\" == \"PEAK_MEMORY\" ]]; then\n+                final_metric_key=\"PEAK_GPU_MEMORY\"\n+            # Add other specific GPU mappings here if needed\n+            # else\n+            #    final_metric_key=\"GPU_${base_metric_key}\" # Generic prefix\n             else\n-                final_metric_key=\"$base_metric_key\" # For unknown/other categories\n+                final_metric_key=\"$base_metric_key\" # If no specific GPU mapping, use base\n             fi\n+        elif [[ \"$HARDWARE_CATEGORY\" == CPU* ]]; then\n+            if [[ \"$base_metric_key\" == \"CPU_TIME\" ]] || [[ \"$base_metric_key\" == \"TIME\" ]]; then # Handle \"CPU Time\" or just \"Time\"\n+                final_metric_key=\"CPU_TIME\"\n+            elif [[ \"$base_metric_key\" == \"WALL_TIME\" ]]; then\n+                final_metric_key=\"WALL_TIME\" # Wall time is generic\n+            # Add other specific CPU mappings here if needed\n+            # else\n+            #    final_metric_key=\"CPU_${base_metric_key}\" # Generic prefix\n+            else\n+                final_metric_key=\"$base_metric_key\" # If no specific CPU mapping, use base\n+            fi\n+        else\n+            final_metric_key=\"$base_metric_key\" # For unknown/other categories\n+        fi\n \n-            echo \"INFO: Parsed metric: OriginalKey='$key', BaseKey='$base_metric_key', FinalKey='$final_metric_key', ValueMs='$ms_value'\"\n-\n-            if ! $first_metric; then metrics_obj_str+=\",\"; fi\n-            metrics_obj_str+=\"\\\"$final_metric_key\\\": {\\\"value_ms\\\": $ms_value, \\\"unit\\\": \\\"ms\\\"}\"\n-            first_metric=false\n+        # Expecting lines like \"Metric Name: 123.45 us\" or \"Metric Name: 12345 bytes\"\n+        read number unit <<< $(echo \"$value\" | sed -E 's/([0-9]+\\.?[0-9]*)\\s*([a-zA-Z]+).*/\\1 \\2/')\n+        # Convert microseconds to milliseconds\n+        if [[ \"$unit\" == \"us\" ]]; then\n+            final_metric_value=$(LC_ALL=C awk -v num=\"$number\" 'BEGIN { printf \"%.3f\", num / 1000 }')\n+            final_unit=\"ms\"\n+        elif [[ \"$unit\" == \"bytes\" ]]; then\n+            # Convert bytes to GB\n+            final_metric_value=$(echo \"$number\" | awk '{printf \"%.2f\", $1/1024^3}')\n+            final_unit=\"GB\"\n+        else\n+          echo \"::warning::Skipping unsupported unit: $unit\"\n+          continue\n         fi\n+        if ! $first_metric; then metrics_obj_str+=\",\"; fi\n+        metrics_obj_str+=\"\\\"$final_metric_key\\\": {\\\"value\\\": $final_metric_value, \\\"unit\\\": \\\"$final_unit\\\"}\"\n+        first_metric=false\n+        echo \"INFO: Parsed metric: OriginalKey='$key', BaseKey='$base_metric_key', FinalKey='$final_metric_key', Value='$final_metric_value $final_unit'\"\n     done <<< \"$STATS_OUTPUT\"\n     metrics_obj_str+=\"}\"\n "
        },
        {
            "sha": "72a9ee851d1cb28c306b32742a0eae8f91f9005e",
            "filename": "third_party/xla/.github/workflows/scorecards-analysis.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2F.github%2Fworkflows%2Fscorecards-analysis.yml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2F.github%2Fworkflows%2Fscorecards-analysis.yml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2F.github%2Fworkflows%2Fscorecards-analysis.yml?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -67,6 +67,6 @@ jobs:\n       # Upload the results to GitHub's code scanning dashboard (optional).\n       # Commenting out will disable upload of results to your repo's Code Scanning dashboard\n       - name: \"Upload to code-scanning\"\n-        uses: github/codeql-action/upload-sarif@96f518a34f7a870018057716cc4d7a5c014bd61c # v3.29.5\n+        uses: github/codeql-action/upload-sarif@3c3833e0f8c1c83d449a7478aa59c036a9165498 # v3.29.5\n         with:\n           sarif_file: results.sarif"
        },
        {
            "sha": "bc871108513d5ab41336e1df6e71afdd630dd65f",
            "filename": "third_party/xla/WORKSPACE",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2FWORKSPACE",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2FWORKSPACE",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2FWORKSPACE?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -9,10 +9,10 @@ load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\")\n # Details: https://github.com/google-ml-infra/rules_ml_toolchain\n http_archive(\n     name = \"rules_ml_toolchain\",\n-    sha256 = \"e7e44c4e349a1c1f31398bd2257c51432e73ea0e7e24cce67090b68b0b50007e\",\n-    strip_prefix = \"rules_ml_toolchain-55dcd0a52c7e0f9eec9927a32512229c09ac3b3e\",\n+    sha256 = \"1a855dd94eebedae69d1804e8837ad70b8018358a0a03eea0bec71d7dc2b096a\",\n+    strip_prefix = \"rules_ml_toolchain-d321763a84c900bc29b4f5459a4f81fad19b2356\",\n     urls = [\n-        \"https://github.com/google-ml-infra/rules_ml_toolchain/archive/55dcd0a52c7e0f9eec9927a32512229c09ac3b3e.tar.gz\",\n+        \"https://github.com/google-ml-infra/rules_ml_toolchain/archive/d321763a84c900bc29b4f5459a4f81fad19b2356.tar.gz\",\n     ],\n )\n \n@@ -27,6 +27,10 @@ register_toolchains(\"@rules_ml_toolchain//cc:linux_x86_64_linux_x86_64\")\n \n register_toolchains(\"@rules_ml_toolchain//cc:linux_x86_64_linux_x86_64_cuda\")\n \n+register_toolchains(\"@rules_ml_toolchain//cc:linux_aarch64_linux_aarch64\")\n+\n+register_toolchains(\"@rules_ml_toolchain//cc:linux_aarch64_linux_aarch64_cuda\")\n+\n # Initialize the XLA repository and all dependencies.\n #\n # The cascade of load() statements and xla_workspace?() calls works around the"
        },
        {
            "sha": "af40ce5521bfa875712d68388003392f7a4be405",
            "filename": "third_party/xla/build_tools/ci/build.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fbuild_tools%2Fci%2Fbuild.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fbuild_tools%2Fci%2Fbuild.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fci%2Fbuild.py?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -52,7 +52,10 @@\n     \"//build_tools/...\",\n     \"@local_tsl//tsl/...\",\n )\n-_XLA_ONEAPI_TARGET_PATTERNS = (\"//xla/stream_executor/sycl/...\",)\n+_XLA_ONEAPI_TARGET_PATTERNS = (\n+    \"//xla/stream_executor/sycl/...\",\n+    \"//xla/service/gpu/...\",\n+)\n _XLA_CPU_PRESUBMIT_BENCHMARKS_DEFAULT_TARGET_PATTERNS = (\n     \"//xla/tools/multihost_hlo_runner:hlo_runner_main\",\n     \"//xla/tools:compute_xspace_stats_main\","
        },
        {
            "sha": "ae305ed6b551ce114eebf1e48ffb808317b329a8",
            "filename": "third_party/xla/build_tools/ci/golden_commands.txt",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fbuild_tools%2Fci%2Fgolden_commands.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fbuild_tools%2Fci%2Fgolden_commands.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fci%2Fgolden_commands.txt?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -65,8 +65,8 @@ bazel test --build_tag_filters=-no_oss,requires-gpu-nvidia,gpu,-rocm-only,-oneap\n bazel analyze-profile profile.json.gz\n # END BuildType.XLA_LINUX_X86_GPU_L4_GITHUB_ACTIONS\n # BEGIN BuildType.XLA_LINUX_X86_GPU_ONEAPI_GITHUB_ACTIONS\n-parallel --ungroup --retries 3 --delay 15 --nonall -- bazel build --build_tag_filters=oneapi-only,requires-gpu-intel,-requires-gpu-amd,-requires-gpu-nvidia,-no_oss,-cuda-only,-rocm-only,-no-oneapi --test_tag_filters=oneapi-only,-requires-gpu-intel,-requires-gpu-amd,-requires-gpu-nvidia,-no_oss,-cuda-only,-rocm-only,-no-oneapi --config=sycl --config=sycl_hermetic --config=icpx_clang --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --//xla/tsl:ci_build --nobuild -- //xla/stream_executor/sycl/...\n-bazel build --build_tag_filters=oneapi-only,requires-gpu-intel,-requires-gpu-amd,-requires-gpu-nvidia,-no_oss,-cuda-only,-rocm-only,-no-oneapi --test_tag_filters=oneapi-only,-requires-gpu-intel,-requires-gpu-amd,-requires-gpu-nvidia,-no_oss,-cuda-only,-rocm-only,-no-oneapi --config=sycl --config=sycl_hermetic --config=icpx_clang --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --//xla/tsl:ci_build -- //xla/stream_executor/sycl/...\n+parallel --ungroup --retries 3 --delay 15 --nonall -- bazel build --build_tag_filters=oneapi-only,requires-gpu-intel,-requires-gpu-amd,-requires-gpu-nvidia,-no_oss,-cuda-only,-rocm-only,-no-oneapi --test_tag_filters=oneapi-only,-requires-gpu-intel,-requires-gpu-amd,-requires-gpu-nvidia,-no_oss,-cuda-only,-rocm-only,-no-oneapi --config=sycl --config=sycl_hermetic --config=icpx_clang --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --//xla/tsl:ci_build --nobuild -- //xla/stream_executor/sycl/... //xla/service/gpu/...\n+bazel build --build_tag_filters=oneapi-only,requires-gpu-intel,-requires-gpu-amd,-requires-gpu-nvidia,-no_oss,-cuda-only,-rocm-only,-no-oneapi --test_tag_filters=oneapi-only,-requires-gpu-intel,-requires-gpu-amd,-requires-gpu-nvidia,-no_oss,-cuda-only,-rocm-only,-no-oneapi --config=sycl --config=sycl_hermetic --config=icpx_clang --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --//xla/tsl:ci_build -- //xla/stream_executor/sycl/... //xla/service/gpu/...\n bazel analyze-profile profile.json.gz\n # END BuildType.XLA_LINUX_X86_GPU_ONEAPI_GITHUB_ACTIONS\n # BEGIN BuildType.XLA_MACOS_ARM64_CPU_KOKORO"
        },
        {
            "sha": "6129c137166b6c236f58234790d6b5244af02c00",
            "filename": "third_party/xla/build_tools/pjrt_wheels/BUILD.bazel",
            "status": "added",
            "additions": 105,
            "deletions": 0,
            "changes": 105,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fbuild_tools%2Fpjrt_wheels%2FBUILD.bazel",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fbuild_tools%2Fpjrt_wheels%2FBUILD.bazel",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fpjrt_wheels%2FBUILD.bazel?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -0,0 +1,105 @@\n+load(\"@rules_python//python:packaging.bzl\", \"py_wheel\")\n+\n+cuda_wheels = [\n+    \"cuda12\",\n+    \"cuda13\",\n+]\n+\n+cpu_wheel = [\"cpu\"]\n+\n+# Aliases and filegroups don't move files within bazel-out, so we need to use genrules to place them\n+# in the correct directory structure for the wheel.\n+[\n+    genrule(\n+        name = \"init_file_\" + arch,\n+        srcs = [\"__init__.py\"],\n+        outs = [\"xla_plugins/xla_\" + arch + \"_pjrt/__init__.py\"],\n+        cmd = \"cp $< $@\",\n+    )\n+    for arch in cuda_wheels + cpu_wheel\n+]\n+\n+# GPU-specific files\n+[\n+    cc_binary(\n+        name = \"xla_plugins/xla_\" + arch + \"_pjrt/xla_gpu_pjrt.so\",\n+        linkopts = [\n+            \"-Wl,--version-script,$(location :pjrt_symbols.lds)\",\n+            \"-Wl,--no-undefined\",\n+        ],\n+        linkshared = True,\n+        deps = [\n+            \":pjrt_symbols.lds\",\n+            \"//xla/pjrt/c:pjrt_c_api_gpu\",\n+        ],\n+    )\n+    for arch in cuda_wheels\n+]\n+\n+# GPU wheels\n+[\n+    py_wheel(\n+        name = \"xla_\" + arch + \"_pjrt\",\n+        author = \"The OpenXLA Authors\",\n+        classifiers = [\"Development Status :: 1 - Planning\"],\n+        distribution = \"xla_\" + arch + \"_pjrt\",\n+        entry_points = {\n+            \"xla_plugins\": [\n+                \"xla_\" + arch + \"_pjrt = xla_plugins.xla_\" + arch + \"_pjrt\\n\",\n+            ],\n+        },\n+        platform = \"manylinux2014_x86_64\",\n+        python_tag = \"py3\",\n+        strip_path_prefixes = [\"build_tools/pjrt_wheels\"],\n+        summary = \"XLA PJRT Plugin\",\n+        version = \"0.0.0.dev0\",\n+        deps = [\n+            \":xla_plugins/xla_\" + arch + \"_pjrt/xla_gpu_pjrt.so\",\n+            \":init_file_\" + arch,\n+        ],\n+    )\n+    for arch in cuda_wheels\n+]\n+\n+# CPU-specific files\n+[\n+    # TODO: Replace this with alias to .so found in xla/pjrt/c/BUILD\n+    cc_binary(\n+        name = \"xla_plugins/xla_\" + arch + \"_pjrt/xla_cpu_pjrt.so\",\n+        linkopts = [\n+            \"-Wl,--version-script,$(location :pjrt_symbols.lds)\",\n+            \"-Wl,--no-undefined\",\n+        ],\n+        linkshared = True,\n+        deps = [\n+            \":pjrt_symbols.lds\",\n+            \"//xla/pjrt/c:pjrt_c_api_cpu\",\n+        ],\n+    )\n+    for arch in cpu_wheel\n+]\n+\n+# CPU wheel\n+[\n+    py_wheel(\n+        name = \"xla_\" + arch + \"_pjrt\",\n+        author = \"The OpenXLA Authors\",\n+        classifiers = [\"Development Status :: 1 - Planning\"],\n+        distribution = \"xla_\" + arch + \"_pjrt\",\n+        entry_points = {\n+            \"xla_plugins\": [\n+                \"xla_\" + arch + \"_pjrt = xla_plugins.xla_\" + arch + \"_pjrt\\n\",\n+            ],\n+        },\n+        platform = \"manylinux2014_x86_64\",\n+        python_tag = \"py3\",\n+        strip_path_prefixes = [\"build_tools/pjrt_wheels\"],\n+        summary = \"XLA PJRT Plugin\",\n+        version = \"0.0.0.dev0\",\n+        deps = [\n+            \":xla_plugins/xla_\" + arch + \"_pjrt/xla_cpu_pjrt.so\",\n+            \":init_file_\" + arch,\n+        ],\n+    )\n+    for arch in cpu_wheel\n+]"
        },
        {
            "sha": "331a2f38efe7a329d667b82dd85752658ec5b260",
            "filename": "third_party/xla/build_tools/pjrt_wheels/__init__.py",
            "status": "added",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fbuild_tools%2Fpjrt_wheels%2F__init__.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fbuild_tools%2Fpjrt_wheels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fpjrt_wheels%2F__init__.py?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -0,0 +1 @@\n+# This is currently just here to mark the directory as a module."
        },
        {
            "sha": "cbac4549bde38d04f2083f98de5cf511811e21c1",
            "filename": "third_party/xla/build_tools/pjrt_wheels/pjrt_symbols.lds",
            "status": "added",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fbuild_tools%2Fpjrt_wheels%2Fpjrt_symbols.lds",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fbuild_tools%2Fpjrt_wheels%2Fpjrt_symbols.lds",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fpjrt_wheels%2Fpjrt_symbols.lds?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -0,0 +1,9 @@\n+VERS_1.0 {\n+  global:\n+    extern \"C\" {\n+      GetPjrtApi;\n+    };\n+\n+  local:\n+    *;\n+};"
        },
        {
            "sha": "a6f7c652795c78af2b02c3e7173f457dbfe9cd29",
            "filename": "third_party/xla/build_tools/sycl/ci_test_xla.sh",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fbuild_tools%2Fsycl%2Fci_test_xla.sh",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fbuild_tools%2Fsycl%2Fci_test_xla.sh",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fsycl%2Fci_test_xla.sh?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n # ==============================================================================\n \n-# This script builds and executes tests. It can be run only on a system that \n+# This script builds and executes tests. It can be run only on a system that\n # has an Intel GPU with the appropriate driver and oneAPI tools installed.\n # Hermetic build is not currently fully supported for executing tests.\n ./configure.py --backend=SYCL --host_compiler=CLANG --sycl_compiler=ICPX\n@@ -23,4 +23,5 @@ bazel test \\\n       --build_tag_filters=gpu,oneapi-only,requires-gpu-intel,-requires-gpu-amd,-requires-gpu-nvidia,-no_oss,-cuda-only,-rocm-only,-no-oneapi \\\n       --test_tag_filters=gpu,oneapi-only,requires-gpu-intel,-requires-gpu-amd,-requires-gpu-nvidia,-no_oss,-cuda-only,-rocm-only,-no-oneapi \\\n       //xla/stream_executor/sycl:sycl_status_test \\\n+      //xla/stream_executor/sycl:sycl_event_test_intelgpu_any \\\n       //xla/stream_executor/sycl:sycl_kernel_test_intelgpu_any"
        },
        {
            "sha": "3f18028cf6c9ea709946892693405801886e7f04",
            "filename": "third_party/xla/docs/_toc.yaml",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fdocs%2F_toc.yaml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fdocs%2F_toc.yaml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2F_toc.yaml?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -51,6 +51,11 @@ toc:\n     path: /xla/flags_guidance\n   - title: XLA Tooling\n     path: /xla/tools\n+- title: Debugging\n+  section:\n+  # This is the default tab for the Debugging section.\n+  - title: Dump HLO Computations\n+    path: /xla/hlo_dumps\n - title: Contributing\n   # These should be in alphabetical order unless otherwise noted.\n   section:"
        },
        {
            "sha": "d834b437b4ae78feac29b6baaa24d05f6438ed8b",
            "filename": "third_party/xla/docs/hlo_dumps.md",
            "status": "added",
            "additions": 260,
            "deletions": 0,
            "changes": 260,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fdocs%2Fhlo_dumps.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fdocs%2Fhlo_dumps.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Fhlo_dumps.md?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -0,0 +1,260 @@\n+# Dump HLO Computations\n+\n+An HLO dump is a textual representation of the HLO modules at different stages\n+of the computation. It is useful for debugging, and you often need to include it\n+in bug reports. This is typically a human-readable **text file** that lists the\n+HLO instructions and their properties. Sometimes, HLO modules are dumped as:\n+\n+-   **HloProto:** Protocol buffer files, which are a more structured,\n+    machine-readable format.\n+-   **HloSnapshot**: HLO module plus its inputs. For replaying HLOs, you\n+    sometimes require the actual inputs fed to a given computation rather than\n+    random data.\n+\n+You can use XLA flags to specify and get dumps. In most cases, you can set it\n+with an environment variable. JAX also offers a programmatic way to print the\n+HLO dump.\n+\n+## Local Execution\n+\n+### Using Environment Variables\n+\n+You can set the `XLA_FLAGS` environment variable with the necessary flags to get\n+dumps. This works for JAX, TensorFlow, and PyTorch/XLA.\n+\n+To dump HLO modules and other debugging information to a specific directory, run\n+your program with the `--xla_dump_to` flag:\n+\n+```shell\n+XLA_FLAGS=\"--xla_dump_to=DIRECTORY_PATH\"\n+```\n+\n+For example, you can use `/tmp` or `/tmp/xladump` as the paths.\n+\n+By default, this dumps HLO modules as text, at the very beginning and end of the\n+optimization pipeline.\n+\n+You can also explicitly specify the format:\n+\n+1.  Text dumps\n+\n+```shell\n+XLA_FLAGS=\"--xla_dump_hlo_as_text --xla_dump_to=DIRECTORY_PATH\"\n+```\n+\n+1.  HLO protos\n+\n+```shell\n+XLA_FLAGS=\"--xla_dump_hlo_as_proto --xla_dump_to=DIRECTORY_PATH\"\n+```\n+\n+1.  HLO Snapshots\n+\n+```shell\n+XLA_FLAGS=\"--xla_dump_hlo_snapshots --xla_dump_to=DIRECTORY_PATH\"\n+```\n+\n+1.  Graph render with graphviz server (only works well for small graphs)\n+\n+```shell\n+XLA_FLAGS=\"--xla_dump_hlo_as_url --xla_dump_to=DIRECTORY_PATH\"\n+```\n+\n+1.  Graph render to HTML file (only works well for small graphs)\n+\n+```shell\n+XLA_FLAGS=\"--xla_dump_hlo_as_html --xla_dump_to=DIRECTORY_PATH\"\n+```\n+\n+For larger graphs, you can use `interactive_graphviz` to visualize parts of the\n+graph.\n+\n+**Note:** If `--xla_dump_to` is not specified but another dumping flag is\n+specified, it will dump to stdout. But the dump will not include binary data,\n+e.g., proto files, to stdout.\n+\n+## Dump Specific Intermediate Passes\n+\n+In addition to the standard pre-optimized / final-optimized HLOs, you can also\n+dump the state of HLOs after a particular compiler pass.\n+\n+```shell\n+XLA_FLAGS=\"--xla_dump_hlo_pass_re=regex --xla_dump_to=DIRECTORY_PATH\"\n+```\n+\n+HLO modules will be dumped for the passes whose names match the regular\n+expression (regex). For example, you can observe the HLOs resulting from passes\n+related to SPMD partitioning with:\n+\n+```shell\n+XLA_FLAGS=\"--xla_dump_to=DIRECTORY_PATH --xla_dump_hlo_pass_re=spmd|propagation\"\n+```\n+\n+To dump the result after every XLA pass (this will result in a lot of files),\n+you can set:\n+\n+```shell\n+XLA_FLAGS=\"--xla_dump_to=DIRECTORY_PATH --xla_dump_hlo_pass_re=.*\"\n+```\n+\n+### JAX-specific Options\n+\n+#### Programmatically in JAX\n+\n+Instead of passing flags or environment variables, you can also programmatically\n+dump HLO using JAXs `lower` and `compile` APIs.\n+\n+Locally fetch the unoptimized original lowered HLO with:\n+\n+```python\n+jax.jit(f).lower(*args).as_text('hlo')\n+```\n+\n+For dumping to files during HLO compilation passes, specify:\n+\n+```python\n+compilation_args = {\n+    'xla_dump_to': DIRECTORY_PATH,\n+    'xla_dump_hlo_pass_re': 'spmd|propagation', # or some other pass filter\n+    ...\n+    }\n+\n+jax.jit(f).lower(*args).compile(compilation_args)\n+```\n+\n+#### Dump jaxprs\n+\n+[`jaxpr`s](https://docs.jax.dev/en/latest/jaxpr.html) are JAX's intermediate\n+representation for program traces. To dump this, set the environment variables:\n+\n+```shell\n+JAX_DUMP_IR_TO=\"DIRECTORY_PATH\" JAX_DUMP_IR_MODES=jaxpr\n+```\n+\n+Learn more in JAX documentation on\n+[Exporting and serializing staged-out computations: Debugging](https://docs.jax.dev/en/latest/export/export.html#debugging).\n+\n+## Google Colab\n+\n+### Environment variables\n+\n+In the first executed cell of your notebook (because environment variables and\n+command-line flags are usually only processed once, e.g., at module-import time\n+or XLA backend initialization time), add the `XLA_FLAGS` detailed above with\n+`os.environ`, for example:\n+\n+```python\n+import os\n+os.environ['XLA_FLAGS'] = \"--xla_dump_to=DIRECTORY_PATH\"\n+```\n+\n+This will dump the computation to `DIRECTORY_PATH`, for example `/tmp`. On\n+Colab, navigate to the \"Files\" browser in the left sidebar, to view and access\n+this directory.\n+\n+You can use all the flags mentioned in the Local Execution section.\n+\n+### JAX-specific options\n+\n+Similar to local execution; for live, interactive introspection you can directly\n+print a computations pre-optimized HLO:\n+\n+```python\n+def f(x):\n+    return jax.numpy.sin(jax.numpy.cos(x))\n+\n+c = jax.jit(f).lower(3.).compiler_ir('hlo')\n+\n+print(c.as_hlo_text())\n+```\n+\n+You can also directly print a computations optimized HLO:\n+\n+```python\n+def optimized_HLO(f, *args, platform=None):\n+    print(jax.jit(f).lower(*args).compile().as_text())\n+\n+def f(x):\n+    return jax.numpy.sin(jax.numpy.cos(x))\n+\n+optimized_HLO(f, 1.0)\n+```\n+\n+#### Dumping All/Small Computations\n+\n+If you want to see everything in a dump including all small compilations, set\n+the JAX environment variable:\n+\n+```shell\n+JAX_COMPILER_DETAILED_LOGGING_MIN_OPS=0\n+```\n+\n+#### Mosaic\n+\n+Mosaic is a compiler for the Pallas TPU backend, and the experimental Pallas GPU\n+backend. To dump mosaic computation, set the following flag:\n+\n+```shell\n+--xla_mosaic_dump_to=/tmp/mosaic_dumps\n+```\n+\n+Or, set TPU init arguments as an environment variable:\n+\n+```shell\n+export LIBTPU_INIT_ARGS=\"--xla_mosaic_dump_to=/tmp/mosaic_dumps\"\n+```\n+\n+Check out the\n+[JAX documentation on Pallas and Mosaic](https://docs.jax.dev/en/latest/pallas/index.html)\n+to learn more.\n+\n+## More with HLO Dumps\n+\n+### Finding the right computation\n+\n+Usually, many computations get dumped. The dumped files are explicitly named\n+with the JAX, Tensorflow, or PyTorch/XLA \"computation name that are called out\n+in the logs, making it easy to identify the relevant HLO files. For example:\n+\n+```\n+1624325116260738.module_0065.pmap__unnamed_wrapped_function_.186875.before_optimizations.txt\n+```\n+\n+Otherwise, you can use `ripgrep` to quickly identify which module holds\n+particular symbols or computations.\n+\n+**Tip:** Include the 3 dumped before/after/buffer-assignment files of interest\n+in your bug reports.\n+\n+### HLO Conversion\n+\n+A tool called `hlo-opt` that can translate between HLOProto and text formats.\n+It's useful in cases where you have one format, but need the other for\n+debugging.\n+\n+Learn to use it:\n+[XLA Tooling documentation: hlo-opt](tools.md#hlo-opt-convert-hlo-module-formats).\n+\n+### Replay\n+\n+You can run (replay) the dumped computations on a specified XLA backend with\n+fake data or input snapshots. This is a convenient way to reproduce, iterate,\n+and debug issues in XLA.\n+\n+The following commands use fake data. If you have saved HLO Snapshots, you can\n+pass those in instead, and the data from the snapshot will be used. To still use\n+fake data while running the snapshot, pass the flag `--force_fake_data`.\n+\n+CPU backend:\n+\n+```shell\n+bazel run -c opt //xla/hlo/tools:run_hlo_module -- --platform=cpu\n+ /tmp/xladump/module_4561.before_optimizations.txt\n+```\n+\n+GPU backend:\n+\n+```shell\n+bazel run -c opt //xla/hlo/tools:run_hlo_module -- --platform=CUDA\n+ /tmp/xladump/module_4561.before_optimizations.txt\n+```"
        },
        {
            "sha": "41f74de21edbaac64d3a2b97e95f0d54be375d74",
            "filename": "third_party/xla/docs/operation_semantics.md",
            "status": "modified",
            "additions": 334,
            "deletions": 56,
            "changes": 390,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fdocs%2Foperation_semantics.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fdocs%2Foperation_semantics.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Foperation_semantics.md?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -194,15 +194,15 @@ for a detailed description of the algorithm.\n \n Calculates gradients of batch norm.\n \n-**`BatchNormGrad(operand, scale, mean, variance, grad_output, epsilon,\n-                 feature_index)`**\n+**`BatchNormGrad(operand, scale, batch_mean, batch_var, grad_output, epsilon,\n+feature_index)`**\n \n Arguments       | Type    | Semantics\n --------------- | ------- | ----------------------------------------------------\n `operand`       | `XlaOp` | n dimensional array to be normalized (x)\n `scale`         | `XlaOp` | 1 dimensional array ($\\gamma$)\n-`mean`          | `XlaOp` | 1 dimensional array ($\\mu$)\n-`variance`      | `XlaOp` | 1 dimensional array ($\\sigma^2$)\n+`batch_mean`    | `XlaOp` | 1 dimensional array ($\\mu$)\n+`batch_var`     | `XlaOp` | 1 dimensional array ($\\sigma^2$)\n `grad_output`   | `XlaOp` | Gradients passed to `BatchNormTraining` ($\\nabla y$)\n `epsilon`       | `float` | Epsilon value ($\\epsilon$)\n `feature_index` | `int64` | Index to feature dimension in `operand`\n@@ -235,19 +235,19 @@ d_l&=\n \\end{split}\n $$\n \n-The inputs `mean` and `variance` represent moments values across batch and\n-spatial dimensions.\n+The inputs `batch_mean` and `batch_var` represent moments values across batch\n+and spatial dimensions.\n \n The output type is a tuple of three handles:\n \n | Outputs        | Type    | Semantics                                         |\n | -------------- | ------- | ------------------------------------------------- |\n | `grad_operand` | `XlaOp` | gradient with respect to input `operand` ($\\nabla |\n :                :         : x$)                                               :\n-| `grad_scale`   | `XlaOp` | gradient with respect to input `scale` ($\\nabla   |\n-:                :         : \\gamma$)                                          :\n-| `grad_offset`  | `XlaOp` | gradient with respect to input `offset`($\\nabla   |\n-:                :         : \\beta$)                                           :\n+| `grad_scale`   | `XlaOp` | gradient with respect to input `scale`            |\n+:                :         : ($\\nabla\\gamma$)                                  :\n+| `grad_offset`  | `XlaOp` | gradient with respect to input                    |\n+:                :         : `offset`($\\nabla\\beta$)                           :\n \n ## BatchNormInference\n \n@@ -508,6 +508,13 @@ frontend_attributes = {\n | `decomposition`             | `XlaComputation`       | computation of type `T_0, T_1, ..., T_{N-1} -> S` with N parameters of arbitrary type |\n | `version`                   | `int64`.               | number to version updates to semantics of the composite op                            |\n \n+An ops `decomposition` isnt a field called, but instead appears as a to_apply\n+attribute that points to the function which contains the lower-level\n+implementation, i.e. `to_apply=%funcname`\n+\n+More information on composite and decomposition can be found on\n+[StableHLO Specification](https://openxla.org/stablehlo/spec#composite)\n+\n ## Cholesky\n \n See also\n@@ -657,7 +664,7 @@ Note that there are the following restrictions on the `source_target_pair`:\n -   If a replica id is not a target in any pair, then the output on that replica\n     is a tensor consisting of 0(s) with the same shape as the input.\n \n-## Concatenate\n+## ConcatInDim (Concatenate)\n \n See also\n [`XlaBuilder::ConcatInDim`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n@@ -765,35 +772,46 @@ type of the returned value of each `branch_computations[b]` must be the same.\n Note that only one of the `branch_computations` will be executed depending on\n the value of `branch_index`.\n \n-## Conv (convolution)\n+## Conv (Convolution)\n \n See also\n [`XlaBuilder::Conv`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n \n-As ConvWithGeneralPadding, but the padding is specified in a short-hand way as\n-either SAME or VALID. SAME padding pads the input (`lhs`) with zeroes so that\n-the output has the same shape as the input when not taking striding into\n-account. VALID padding simply means no padding.\n-\n-## ConvWithGeneralPadding (convolution)\n-\n-See also\n-[`XlaBuilder::ConvWithGeneralPadding`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n-\n Computes a convolution of the kind used in neural networks. Here, a convolution\n can be thought of as a n-dimensional window moving across a n-dimensional base\n area and a computation is performed for each possible position of the window.\n \n-| Arguments             | Type                     | Semantics                |\n-| --------------------- | ------------------------ | ------------------------ |\n-| `lhs`                 | `XlaOp`                  | (n+2)-dimensional array of inputs |\n-| `rhs`                 | `XlaOp`                  | (n+2)-dimensional array of kernel weights |\n-| `window_strides`      | `ArraySlice<int64>`      | n-d array of kernel strides |\n-| `padding`             | `ArraySlice< pair<int64,int64>>` | n-d array of (low, high) padding |\n-| `lhs_dilation`        | `ArraySlice<int64>`      | n-d lhs dilation factor array |\n-| `rhs_dilation`        | `ArraySlice<int64>`      | n-d rhs dilation factor array |\n-| `feature_group_count` | int64                    | the number of feature groups |\n-| `batch_group_count`   | int64                    | the number of batch groups |\n+`Conv` Enqueues a convolution instruction onto the computation, which uses the\n+default convolution dimension numbers with no dilation.\n+\n+The padding is specified in a short-hand way as either SAME or VALID. SAME\n+padding pads the input (`lhs`) with zeroes so that the output has the same shape\n+as the input when not taking striding into account. VALID padding simply means\n+no padding.\n+\n+**`Conv(lhs, rhs, window_strides, padding, feature_group_count,\n+batch_group_count, precision_config, preferred_element_type)`**\n+\n+| Arguments                | Type                | Semantics                   |\n+| ------------------------ | ------------------- | --------------------------- |\n+| `lhs`                    | `XlaOp`             | (n+2)-dimensional array of  |\n+:                          :                     : inputs                      :\n+| `rhs`                    | `XlaOp`             | (n+2)-dimensional array of  |\n+:                          :                     : kernel weights              :\n+| `window_strides`         | `ArraySlice<int64>` | n-d array of kernel strides |\n+| `padding`                | `Padding`           | enum of padding             |\n+| `feature_group_count`    | int64               | the number of feature       |\n+:                          :                     : groups                      :\n+| `batch_group_count`      | int64               | the number of batch groups  |\n+| `precision_config`       | optional            | enum for level of precision |\n+:                          : `PrecisionConfig`   :                             :\n+| `preferred_element_type` | optional            | enum of scalar element type |\n+:                          : `PrimitiveType`     :                             :\n+\n+Increasing levels of controls are available for `Conv`: -\n+[ConvWithGeneralPadding](#ConvWithGeneralPadding) -\n+[ConvWithGeneralDimensions](#ConvWithGeneralDimensions) -\n+[ConvGeneral](#ConvGeneral) - [ConvGeneralDilated](#convgeneraldilated)\n \n Let n be the number of spatial dimensions. The `lhs` argument is an\n (n+2)-dimensional array describing the base area. This is called the input,\n@@ -921,6 +939,158 @@ for (b, oz, oy, ox) {  // output coordinates\n }\n ```\n \n+`precision_config` is used to indicate the precision configuration. The level\n+dictates whether hardware should attempt to generate more machine code\n+instructions to provide more accurate dtype emulation when needed (i.e.\n+emulating f32 on a TPU that only supports bf16 matmuls). Values may be\n+`DEFAULT`, `HIGH`, `HIGHEST`. Additional details\n+[in the MXU sections](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus).\n+\n+`preferred_element_type` is a scalar element of higher/lower precision output\n+types used for accumulation. `preferred_element_type` recommends the\n+accumulation type for the given operaiton, however it is not guaranteed. This\n+allows for some hardware backends to instead accumulate in a different type and\n+convert to the preferred output type.\n+\n+### ConvWithGeneralPadding\n+\n+**`ConvWithGeneralPadding(lhs, rhs, window_strides, padding,\n+feature_group_count, batch_group_count, precision_config,\n+preferred_element_type)`**\n+\n+See also\n+[`XlaBuilder::ConvWithGeneralPadding`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n+\n+Same as [`Conv`](#conv-convolution) where padding configuration is explicit.\n+\n+| Arguments                | Type                | Semantics                   |\n+| ------------------------ | ------------------- | --------------------------- |\n+| `lhs`                    | `XlaOp`             | (n+2)-dimensional array of  |\n+:                          :                     : inputs                      :\n+| `rhs`                    | `XlaOp`             | (n+2)-dimensional array of  |\n+:                          :                     : kernel weights              :\n+| `window_strides`         | `ArraySlice<int64>` | n-d array of kernel strides |\n+| `padding`                | `ArraySlice<        | n-d array of (low, high)    |\n+:                          : pair<int64,int64>>` : padding                     :\n+| `feature_group_count`    | int64               | the number of feature       |\n+:                          :                     : groups                      :\n+| `batch_group_count`      | int64               | the number of batch groups  |\n+| `precision_config`       | optional            | enum for level of precision |\n+:                          : `PrecisionConfig`   :                             :\n+| `preferred_element_type` | optional            | enum of scalar element type |\n+:                          : `PrimitiveType`     :                             :\n+\n+### ConvWithGeneralDimensions\n+\n+**`ConvWithGeneralDimensions(lhs, rhs, window_strides, padding,\n+dimension_numbers, feature_group_count, batch_group_count, precision_config,\n+preferred_element_type)`**\n+\n+See also\n+[`XlaBuilder::ConvWithGeneralDimensions`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n+\n+Same as [`Conv`](#conv-convolution) where dimension numbers are explicit.\n+\n+| Arguments                | Type                          | Semantics         |\n+| ------------------------ | ----------------------------- | ----------------- |\n+| `lhs`                    | `XlaOp`                       | (n+2)-dimensional |\n+:                          :                               : array of inputs   :\n+| `rhs`                    | `XlaOp`                       | (n+2)-dimensional |\n+:                          :                               : array of kernel   :\n+:                          :                               : weights           :\n+| `window_strides`         | `ArraySlice<int64>`           | n-d array of      |\n+:                          :                               : kernel strides    :\n+| `padding`                | `Padding`                     | enum of padding   |\n+| `dimension_numbers`      | `ConvolutionDimensionNumbers` | the number of     |\n+:                          :                               : dimensions        :\n+| `feature_group_count`    | int64                         | the number of     |\n+:                          :                               : feature groups    :\n+| `batch_group_count`      | int64                         | the number of     |\n+:                          :                               : batch groups      :\n+| `precision_config`       | optional `PrecisionConfig`    | enum for level of |\n+:                          :                               : precision         :\n+| `preferred_element_type` | optional `PrimitiveType`      | enum of scalar    |\n+:                          :                               : element type      :\n+\n+### ConvGeneral\n+\n+**`ConvGeneral(lhs, rhs, window_strides, padding, dimension_numbers,\n+feature_group_count, batch_group_count, precision_config,\n+preferred_element_type)`**\n+\n+See also\n+[`XlaBuilder::ConvGeneral`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n+\n+Same as [`Conv`](#conv-convolution) where dimension numbers and padding\n+configuration is explicit\n+\n+| Arguments                | Type                          | Semantics         |\n+| ------------------------ | ----------------------------- | ----------------- |\n+| `lhs`                    | `XlaOp`                       | (n+2)-dimensional |\n+:                          :                               : array of inputs   :\n+| `rhs`                    | `XlaOp`                       | (n+2)-dimensional |\n+:                          :                               : array of kernel   :\n+:                          :                               : weights           :\n+| `window_strides`         | `ArraySlice<int64>`           | n-d array of      |\n+:                          :                               : kernel strides    :\n+| `padding`                | `ArraySlice<                  | n-d array of      |\n+:                          : pair<int64,int64>>`           : (low, high)       :\n+:                          :                               : padding           :\n+| `dimension_numbers`      | `ConvolutionDimensionNumbers` | the number of     |\n+:                          :                               : dimensions        :\n+| `feature_group_count`    | int64                         | the number of     |\n+:                          :                               : feature groups    :\n+| `batch_group_count`      | int64                         | the number of     |\n+:                          :                               : batch groups      :\n+| `precision_config`       | optional `PrecisionConfig`    | enum for level of |\n+:                          :                               : precision         :\n+| `preferred_element_type` | optional `PrimitiveType`      | enum of scalar    |\n+:                          :                               : element type      :\n+\n+### ConvGeneralDilated\n+\n+**`ConvGeneralDilated(lhs, rhs, window_strides, padding, lhs_dilation,\n+rhs_dilation, dimension_numbers, feature_group_count, batch_group_count,\n+precision_config, preferred_element_type, window_reversal)`**\n+\n+See also\n+[`XlaBuilder::ConvGeneralDilated`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n+\n+Same as [`Conv`](#conv-convolution) where padding configuration, dilation\n+factors, and dimension numbers are explicit.\n+\n+| Arguments                | Type                          | Semantics         |\n+| ------------------------ | ----------------------------- | ----------------- |\n+| `lhs`                    | `XlaOp`                       | (n+2)-dimensional |\n+:                          :                               : array of inputs   :\n+| `rhs`                    | `XlaOp`                       | (n+2)-dimensional |\n+:                          :                               : array of kernel   :\n+:                          :                               : weights           :\n+| `window_strides`         | `ArraySlice<int64>`           | n-d array of      |\n+:                          :                               : kernel strides    :\n+| `padding`                | `ArraySlice<                  | n-d array of      |\n+:                          : pair<int64,int64>>`           : (low, high)       :\n+:                          :                               : padding           :\n+| `lhs_dilation`           | `ArraySlice<int64>`           | n-d lhs dilation  |\n+:                          :                               : factor array      :\n+| `rhs_dilation`           | `ArraySlice<int64>`           | n-d rhs dilation  |\n+:                          :                               : factor array      :\n+| `dimension_numbers`      | `ConvolutionDimensionNumbers` | the number of     |\n+:                          :                               : dimensions        :\n+| `feature_group_count`    | int64                         | the number of     |\n+:                          :                               : feature groups    :\n+| `batch_group_count`      | int64                         | the number of     |\n+:                          :                               : batch groups      :\n+| `precision_config`       | optional `PrecisionConfig`    | enum for level of |\n+:                          :                               : precision         :\n+| `preferred_element_type` | optional `PrimitiveType`      | enum of scalar    |\n+:                          :                               : element type      :\n+| `window_reversal`        | optional `vector<bool>`       | flag used to      |\n+:                          :                               : logically reverse :\n+:                          :                               : dimension before  :\n+:                          :                               : applying the      :\n+:                          :                               : convolution       :\n+\n ## ConvertElementType\n \n See also\n@@ -1021,39 +1191,64 @@ idempotent.\n See also\n [`XlaBuilder::Dot`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n \n-**`Dot(lhs, rhs)`**\n+**`Dot(lhs, rhs, precision_config, preferred_element_type)`**\n \n-Arguments | Type    | Semantics\n---------- | ------- | ---------------\n-`lhs`     | `XlaOp` | array of type T\n-`rhs`     | `XlaOp` | array of type T\n+| Arguments                | Type              | Semantics                   |\n+| ------------------------ | ----------------- | --------------------------- |\n+| `lhs`                    | `XlaOp`           | array of type T             |\n+| `rhs`                    | `XlaOp`           | array of type T             |\n+| `precision_config`       | optional          | enum for level of precision |\n+:                          : `PrecisionConfig` :                             :\n+| `preferred_element_type` | optional          | enum of scalar element type |\n+:                          : `PrimitiveType`   :                             :\n \n The exact semantics of this operation depend on the ranks of the operands:\n \n-| Input                               | Output          | Semantics               |\n-| ----------------------------------- | --------------- | ----------------------- |\n-| vector [n] `dot` vector [n]         | scalar          | vector dot product      |\n-| matrix [m x k] `dot` vector [k]     | vector [m]      | matrix-vector multiplication |\n-| matrix [m x k] `dot` matrix [k x n] | matrix [m x n]  | matrix-matrix multiplication |\n+| Input                       | Output         | Semantics                    |\n+| --------------------------- | -------------- | ---------------------------- |\n+| vector [n] `dot` vector [n] | scalar         | vector dot product           |\n+| matrix [m x k] `dot` vector | vector [m]     | matrix-vector multiplication |\n+: [k]                         :                :                              :\n+| matrix [m x k] `dot` matrix | matrix [m x n] | matrix-matrix multiplication |\n+: [k x n]                     :                :                              :\n \n The operation performs sum of products over the second dimension of `lhs` (or\n the first if it has 1 dimension) and the first dimension of `rhs`. These are the\n \"contracted\" dimensions. The contracted dimensions of `lhs` and `rhs` must be of\n the same size. In practice, it can be used to perform dot products between\n vectors, vector/matrix multiplications or matrix/matrix multiplications.\n \n+`precision_config` is used to indicate the precision configuration. The level\n+dictates whether hardware should attempt to generate more machine code\n+instructions to provide more accurate dtype emulation when needed (i.e.\n+emulating f32 on a TPU that only supports bf16 matmuls). Values may be\n+`DEFAULT`, `HIGH`, `HIGHEST`. Additional details\n+[in the MXU sections](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus).\n+\n+`preferred_element_type` is a scalar element of higher/lower precision output\n+types used for accumulation. `preferred_element_type` recommends the\n+accumulation type for the given operaiton, however it is not guaranteed. This\n+allows for some hardware backends to instead accumulate in a different type and\n+convert to the preferred output type.\n+\n ## DotGeneral\n \n See also\n [`XlaBuilder::DotGeneral`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n \n-**`DotGeneral(lhs, rhs, dimension_numbers)`**\n+**`DotGeneral(lhs, rhs, dimension_numbers, precision_config,\n+preferred_element_type)`**\n \n-Arguments           | Type                  | Semantics\n-------------------- | --------------------- | ---------------\n-`lhs`               | `XlaOp`               | array of type T\n-`rhs`               | `XlaOp`               | array of type T\n-`dimension_numbers` | `DotDimensionNumbers` | contracting and batch dimension numbers\n+| Arguments                | Type                  | Semantics              |\n+| ------------------------ | --------------------- | ---------------------- |\n+| `lhs`                    | `XlaOp`               | array of type T        |\n+| `rhs`                    | `XlaOp`               | array of type T        |\n+| `dimension_numbers`      | `DotDimensionNumbers` | contracting and batch  |\n+:                          :                       : dimension numbers      :\n+| `precision_config`       | optional              | enum for level of      |\n+:                          : `PrecisionConfig`     : precision              :\n+| `preferred_element_type` | optional              | enum of scalar element |\n+:                          : `PrimitiveType`       : type                   :\n \n Similar to Dot, but allows contracting and batch dimension numbers to be\n specified for both the `lhs` and `rhs`.\n@@ -1129,6 +1324,19 @@ It follows that the resulting dimension number starts with the batch dimension,\n then the `lhs` non-contracting/non-batch dimension, and finally the `rhs`\n non-contracting/non-batch dimension.\n \n+`precision_config` is used to indicate the precision configuration. The level\n+dictates whether hardware should attempt to generate more machine code\n+instructions to provide more accurate dtype emulation when needed (i.e.\n+emulating f32 on a TPU that only supports bf16 matmuls). Values may be\n+`DEFAULT`, `HIGH`, `HIGHEST`. Additional details\n+[can be found in the MXU sections](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus).\n+\n+`preferred_element_type` is a scalar element of higher/lower precision output\n+types used for accumulation. `preferred_element_type` recommends the\n+accumulation type for the given operaiton, however it is not guaranteed. This\n+allows for some hardware backends to instead accumulate in a different type and\n+convert to the preferred output type.\n+\n ## DynamicSlice\n \n See also\n@@ -1426,6 +1634,60 @@ The function is applied to each element in the `operand` array, resulting in an\n array with the same shape. It is allowed for `operand` to be a scalar\n (0-dimensional).\n \n+### Optional Result Accuracy\n+\n+XlaBuilder supports these element-wise unary functions with the optional\n+`result_accuracy` argument:\n+\n+<b>`Cbrt(operand, result_accuracy)`</b> Element-wise cubic root operation `x ->\n+cbrt(x)`.\n+\n+<b>`Cos(operand, result_accuracy)`</b> Element-wise cosine `x -> cos(x)`.\n+\n+<b>`Erf(operand, result_accuracy)`</b> Element-wise error function `x -> erf(x)`\n+where\n+\n+$$\\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}}\\int_0^x e^{-t^2} \\, dt$$.\n+\n+<b>`Exp(operand, result_accuracy)`</b> Element-wise natural exponential `x ->\n+e^x`.\n+\n+<b>`Expm1(operand, result_accuracy)`</b> Element-wise natural exponential minus\n+one `x -> e^x - 1`.\n+\n+<b>`Log(operand, result_accuracy)`</b> Element-wise natural logarithm `x ->\n+ln(x)`.\n+\n+<b>`Log1p(operand, result_accuracy)`</b> Element-wise shifted natural logarithm\n+`x -> ln(1+x)`.\n+\n+<b>`Logistic(operand, result_accuracy)`</b> Element-wise logistic function\n+computation `x -> logistic(x)`.\n+\n+<b>`Rsqrt(operand, result_accuracy)`</b> Element-wise reciprocal of square root\n+operation `x -> 1.0 / sqrt(x)`.\n+\n+<b>`Sin(operand, result_accuracy)`</b> Element-wise sine `x -> sin(x)`.\n+\n+<b>`Sqrt(operand, result_accuracy)`</b> Element-wise square root operation `x ->\n+sqrt(x)`.\n+\n+<b>`Tan(operand, result_accuracy)`</b> Element-wise tangent `x -> tan(x)`.\n+\n+<b>`Tanh(operand, result_accuracy)`</b> Element-wise hyperbolic tangent `x ->\n+tanh(x)`.\n+\n+| Arguments         | Type                      | Semantics                   |\n+| ----------------- | ------------------------- | --------------------------- |\n+| `operand`         | `XlaOp`                   | The operand to the function |\n+| `result_accuracy` | optional `ResultAccuracy` | The types of accuracy the   |\n+:                   :                           : user can request for unary  :\n+:                   :                           : ops with multiple           :\n+:                   :                           : implementations             :\n+\n+For more information on `result_accuracy` see\n+[Result Accuracy](https://github.com/openxla/stablehlo/blob/main/rfcs/20241015-result-accuracy.md)\n+\n ## Fft\n \n The XLA FFT operation implements the forward and inverse Fourier Transforms for\n@@ -1835,6 +2097,9 @@ Blocks any optimization pass from moving computations across the barrier.\n Ensures that all inputs are evaluated before any operators that depend on the\n barrier's outputs.\n \n+See also\n+[StableHLO optimization_barrier](https://openxla.org/stablehlo/spec#optimization_barrier)\n+\n ## Pad\n \n See also\n@@ -1915,14 +2180,20 @@ See also\n \n Applies a reduction function to one or more arrays in parallel.\n \n-**`Reduce(operands..., init_values..., computation, dimensions)`**\n-\n-| Arguments     | Type                  | Semantics                        |\n-| ------------- | --------------------- | -------------------------------- |\n-| `operands`    | Sequence of N `XlaOp` | N arrays of types `T_0, ..., T_{N-1}`. |\n-| `init_values` | Sequence of N `XlaOp` | N scalars of types `T_0, ..., T_{N-1}`. |\n-| `computation` | `XlaComputation`      | computation of type `T_0, ..., T_{N-1}, T_0, ..., T_{N-1} ->` `Collate(T_0, ..., T_{N-1})`. |\n-| `dimensions`  | `int64` array         | unordered array of dimensions to reduce. |\n+**`Reduce(operands..., init_values..., computation, dimensions_to_reduce)`**\n+\n+| Arguments              | Type                  | Semantics                 |\n+| ---------------------- | --------------------- | ------------------------- |\n+| `operands`             | Sequence of N `XlaOp` | N arrays of types `T_0,   |\n+:                        :                       : ..., T_{N-1}`.            :\n+| `init_values`          | Sequence of N `XlaOp` | N scalars of types `T_0,  |\n+:                        :                       : ..., T_{N-1}`.            :\n+| `computation`          | `XlaComputation`      | computation of type `T_0, |\n+:                        :                       : ..., T_{N-1}, T_0, ...,   :\n+:                        :                       : T_{N-1} ->` `Collate(T_0, :\n+:                        :                       : ..., T_{N-1})`.           :\n+| `dimensions_to_reduce` | `int64` array         | unordered array of        |\n+:                        :                       : dimensions to reduce.     :\n \n Where:\n \n@@ -2901,6 +3172,13 @@ let t: (f32[10], s32) = tuple(v, s);\n Tuples can be deconstructed (accessed) via the [`GetTupleElement`]\n (#gettupleelement) operation.\n \n+For more information see\n+[StableHLO Tuple](https://openxla.org/stablehlo/spec#tuple)\n+\n+> **Note:** In HLO, tuples are needed for most ops that return >1 result. While\n+> in StableHLO/MLIR, variadic results can be expressed and tuples are not used,\n+> except in custom_calls/get_tuple_element.\n+\n ## While\n \n See also"
        },
        {
            "sha": "065983e8608cff2c1ad00fb039f0a7d5015aada4",
            "filename": "third_party/xla/opensource_only.files",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fopensource_only.files",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fopensource_only.files",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fopensource_only.files?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -25,6 +25,7 @@ tensorflow/third_party/py/python_repo.bzl:\n tensorflow/third_party/py/python_wheel.bzl:\n tensorflow/third_party/py/rules_pywrap/def_file_filter_tool.py:\n tensorflow/third_party/py/rules_pywrap/wrapped_py_init.cc:\n+tensorflow/third_party/py/setup_py_nvidia_dependencies_util.py:\n tensorflow/third_party/py/unpack_wheel_and_unzip_archive_files.py:\n tensorflow/tools/def_file_filter/BUILD.tpl:\n tensorflow/tools/def_file_filter/BUILD:"
        },
        {
            "sha": "41069982fd80e88b588a6445e5b40e554b56f9a8",
            "filename": "third_party/xla/tensorflow.bazelrc",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftensorflow.bazelrc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftensorflow.bazelrc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Ftensorflow.bazelrc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -229,12 +229,6 @@ build:cuda_nvcc --@local_config_cuda//:cuda_compiler=nvcc\n # Old config for backward compatibility\n build:nvcc_clang --config=cuda_nvcc\n \n-# TODO(yuriit): Remove this once Linux aarch64 hermetic build is supported.\n-# This is a temporary solution to build for Linux aarch64 by using NVCC.\n-build:cuda_nvcc_linux_aarch64 --config=cuda_nvcc\n-build:cuda_nvcc_linux_aarch64 --config=clang_local\n-build:cuda_nvcc_linux_aarch64 --crosstool_top=@local_config_cuda//crosstool:toolchain\n-\n # Debug config. Enables Bazel's 'dbg' compilation mode, build with debugging enabled\n build:dbg -c dbg\n # Compiling all dependencies with debug info can cause linker failures"
        },
        {
            "sha": "ee8f420dd30c2d878172a701f93338b09df2e28c",
            "filename": "third_party/xla/third_party/eigen3/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Feigen3%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Feigen3%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Feigen3%2Fworkspace.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -7,8 +7,8 @@ def repo():\n \n     # Attention: tools parse and update these lines.\n     # LINT.IfChange\n-    EIGEN_COMMIT = \"4c38131a16803130b66266a912029504f2cf23cd\"\n-    EIGEN_SHA256 = \"1a432ccbd597ea7b9faa1557b1752328d6adc1a3db8969f6fe793ff704be3bf0\"\n+    EIGEN_COMMIT = \"70d8d99d0df9fd967b135efd8d12ed20fc48d007\"\n+    EIGEN_SHA256 = \"78d1158871b8d3663cead3fb3c482721155df9a331d94cfcc60bcdf5cdbf18e1\"\n     # LINT.ThenChange(//tensorflow/lite/tools/cmake/modules/eigen.cmake)\n \n     tf_http_archive("
        },
        {
            "sha": "36f2a93f023a9902d2c1d501f19b37f79001dc48",
            "filename": "third_party/xla/third_party/gpus/cuda/build_defs.bzl.tpl",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fgpus%2Fcuda%2Fbuild_defs.bzl.tpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fgpus%2Fcuda%2Fbuild_defs.bzl.tpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fgpus%2Fcuda%2Fbuild_defs.bzl.tpl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -11,7 +11,7 @@ def if_cuda(if_true, if_false = []):\n     })\n \n def if_cuda_clang(if_true, if_false = []):\n-   \"\"\"Shorthand for select()'ing on wheteher we're building with cuda-clang.\n+   \"\"\"Shorthand for select()'ing on whether we're building with cuda-clang.\n \n     Returns a select statement which evaluates to if_true if we're building\n     with cuda-clang.  Otherwise, the select statement evaluates to if_false.\n@@ -31,7 +31,7 @@ def if_cuda_exec(if_true, if_false = []):\n     return if_cuda(if_true, if_false)\n \n def cuda_compiler(if_cuda_clang, if_nvcc, neither = []):\n-    \"\"\"Shorthand for select()'ing on wheteher we're building with cuda-clang or nvcc.\n+    \"\"\"Shorthand for select()'ing on whether we're building with cuda-clang or nvcc.\n \n      Returns a select statement which evaluates to if_cuda_clang if we're building\n      with cuda-clang, if_nvcc if we're building with NVCC.\n@@ -48,7 +48,7 @@ def cuda_compiler(if_cuda_clang, if_nvcc, neither = []):\n         return neither\n \n def if_cuda_clang_opt(if_true, if_false = []):\n-   \"\"\"Shorthand for select()'ing on wheteher we're building with cuda-clang\n+   \"\"\"Shorthand for select()'ing on whether we're building with cuda-clang\n    in opt mode.\n \n     Returns a select statement which evaluates to if_true if we're building"
        },
        {
            "sha": "8b4324dcc8c9daff37549a9316b0af612ee68f99",
            "filename": "third_party/xla/third_party/gpus/sycl/build_defs.bzl.tpl",
            "status": "modified",
            "additions": 14,
            "deletions": 3,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fgpus%2Fsycl%2Fbuild_defs.bzl.tpl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fgpus%2Fsycl%2Fbuild_defs.bzl.tpl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fgpus%2Fsycl%2Fbuild_defs.bzl.tpl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -13,7 +13,11 @@ def if_sycl(if_true, if_false = []):\n \n def sycl_default_copts():\n     \"\"\"Default options for all SYCL compilations.\"\"\"\n-    return if_sycl([\"-x\", \"sycl\"])\n+    return if_sycl([\"-sycl_compile\"])\n+\n+def sycl_default_linkopts():\n+    \"\"\"Default options for all SYCL compilations.\"\"\"\n+    return if_sycl([\"-link_stage\", \"-lirc\"])\n \n def sycl_build_is_configured():\n     \"\"\"Returns true if SYCL compiler was enabled during the configure process.\"\"\"\n@@ -34,6 +38,13 @@ def if_sycl_build_is_configured(x, y):\n       return x\n     return y\n \n-def sycl_library(copts = [], **kwargs):\n+def sycl_library(copts = [], linkopts = [], tags = [], deps = [], **kwargs):\n     \"\"\"Wrapper over cc_library which adds default SYCL options.\"\"\"\n-    native.cc_library(copts = sycl_default_copts() + copts, **kwargs)\n+    native.cc_library(copts = sycl_default_copts() + copts,\n+                      linkopts = sycl_default_linkopts() + linkopts,\n+                      tags = tags + [\"gpu\"],\n+                      deps = deps + if_sycl_is_configured([\n+                        \"@local_config_sycl//sycl:sycl_headers\",\n+                        \"@local_config_sycl//sycl:level_zero\",\n+                      ]),\n+                      **kwargs)"
        },
        {
            "sha": "09d0f8288c45c25bc1b830be4f96371c89989948",
            "filename": "third_party/xla/third_party/llvm/generated.patch",
            "status": "modified",
            "additions": 954,
            "deletions": 244,
            "changes": 1198,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fllvm%2Fgenerated.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fllvm%2Fgenerated.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fllvm%2Fgenerated.patch?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1,254 +1,964 @@\n Auto generated patch. Do not edit or delete it, even if empty.\n-diff -ruN --strip-trailing-cr a/llvm/include/llvm/Analysis/LoopInfo.h b/llvm/include/llvm/Analysis/LoopInfo.h\n---- a/llvm/include/llvm/Analysis/LoopInfo.h\n-+++ b/llvm/include/llvm/Analysis/LoopInfo.h\n-@@ -59,11 +59,12 @@\n-   };\n- \n-   /// Return true if the specified value is loop invariant.\n--  bool isLoopInvariant(const Value *V) const;\n-+  bool isLoopInvariant(const Value *V, bool HasCoroSuspendInst = false) const;\n- \n-   /// Return true if all the operands of the specified instruction are loop\n-   /// invariant.\n--  bool hasLoopInvariantOperands(const Instruction *I) const;\n-+  bool hasLoopInvariantOperands(const Instruction *I,\n-+                                bool HasCoroSuspendInst = false) const;\n- \n-   /// If the given value is an instruction inside of the loop and it can be\n-   /// hoisted, do so to make it trivially loop-invariant.\n-diff -ruN --strip-trailing-cr a/llvm/include/llvm/Transforms/Utils/LoopUtils.h b/llvm/include/llvm/Transforms/Utils/LoopUtils.h\n---- a/llvm/include/llvm/Transforms/Utils/LoopUtils.h\n-+++ b/llvm/include/llvm/Transforms/Utils/LoopUtils.h\n-@@ -185,7 +185,8 @@\n-                           TargetLibraryInfo *, Loop *, MemorySSAUpdater &,\n-                           ScalarEvolution *, ICFLoopSafetyInfo *,\n-                           SinkAndHoistLICMFlags &, OptimizationRemarkEmitter *,\n--                          bool, bool AllowSpeculation);\n-+                          bool, bool AllowSpeculation,\n-+                          bool HasCoroSuspendInst = false);\n- \n- /// Return true if the induction variable \\p IV in a Loop whose latch is\n- /// \\p LatchBlock would become dead if the exit test \\p Cond were removed.\n-diff -ruN --strip-trailing-cr a/llvm/lib/Analysis/LoopInfo.cpp b/llvm/lib/Analysis/LoopInfo.cpp\n---- a/llvm/lib/Analysis/LoopInfo.cpp\n-+++ b/llvm/lib/Analysis/LoopInfo.cpp\n-@@ -58,14 +58,26 @@\n- // Loop implementation\n- //\n- \n--bool Loop::isLoopInvariant(const Value *V) const {\n--  if (const Instruction *I = dyn_cast<Instruction>(V))\n--    return !contains(I);\n-+bool Loop::isLoopInvariant(const Value *V, bool HasCoroSuspendInst) const {\n-+  if (const Instruction *I = dyn_cast<Instruction>(V)) {\n-+    // FIXME: this is semantically inconsistent. We're tracking a proper fix in\n-+    // issue #149604.\n-+    // If V is a pointer to stack object and L contains a coro.suspend function\n-+    // call, then V may not be loop invariant because the ramp function and\n-+    // resume function have different stack frames.\n-+    if (HasCoroSuspendInst && isa<AllocaInst>(I))\n-+      return false;\n-+    else\n-+      return !contains(I);\n+diff -ruN --strip-trailing-cr a/clang/include/clang/Analysis/FlowSensitive/StorageLocation.h b/clang/include/clang/Analysis/FlowSensitive/StorageLocation.h\n+--- a/clang/include/clang/Analysis/FlowSensitive/StorageLocation.h\n++++ b/clang/include/clang/Analysis/FlowSensitive/StorageLocation.h\n+@@ -17,7 +17,6 @@\n+ #include \"clang/AST/Decl.h\"\n+ #include \"clang/AST/Type.h\"\n+ #include \"llvm/ADT/DenseMap.h\"\n+-#include \"llvm/ADT/StringRef.h\"\n+ #include \"llvm/Support/Debug.h\"\n+ #include <cassert>\n+ \n+@@ -153,11 +152,6 @@\n+     return {SyntheticFields.begin(), SyntheticFields.end()};\n+   }\n+ \n+-  /// Add a synthetic field, if none by that name is already present.\n+-  void addSyntheticField(llvm::StringRef Name, StorageLocation &Loc) {\n+-    SyntheticFields.insert({Name, &Loc});\n+-  }\n+-\n+   /// Changes the child storage location for a field `D` of reference type.\n+   /// All other fields cannot change their storage location and always retain\n+   /// the storage location passed to the `RecordStorageLocation` constructor.\n+@@ -170,11 +164,6 @@\n+     Children[&D] = Loc;\n+   }\n+ \n+-  /// Add a child storage location for a field `D`, if not already present.\n+-  void addChild(const ValueDecl &D, StorageLocation *Loc) {\n+-    Children.insert({&D, Loc});\n+-  }\n+-\n+   llvm::iterator_range<FieldToLoc::const_iterator> children() const {\n+     return {Children.begin(), Children.end()};\n+   }\n+diff -ruN --strip-trailing-cr a/clang/include/clang/AST/DeclCXX.h b/clang/include/clang/AST/DeclCXX.h\n+--- a/clang/include/clang/AST/DeclCXX.h\n++++ b/clang/include/clang/AST/DeclCXX.h\n+@@ -3826,7 +3826,7 @@\n+ \n+ public:\n+   EnumDecl *getEnumDecl() const {\n+-    return cast<clang::EnumType>(EnumType->getType())->getOriginalDecl();\n++    return EnumType->getType()->castAs<clang::EnumType>()->getOriginalDecl();\n+   }\n+ \n+   static UsingEnumDecl *Create(ASTContext &C, DeclContext *DC,\n+diff -ruN --strip-trailing-cr a/clang/lib/Analysis/FlowSensitive/Transfer.cpp b/clang/lib/Analysis/FlowSensitive/Transfer.cpp\n+--- a/clang/lib/Analysis/FlowSensitive/Transfer.cpp\n++++ b/clang/lib/Analysis/FlowSensitive/Transfer.cpp\n+@@ -20,17 +20,14 @@\n+ #include \"clang/AST/OperationKinds.h\"\n+ #include \"clang/AST/Stmt.h\"\n+ #include \"clang/AST/StmtVisitor.h\"\n+-#include \"clang/AST/Type.h\"\n+ #include \"clang/Analysis/FlowSensitive/ASTOps.h\"\n+ #include \"clang/Analysis/FlowSensitive/AdornedCFG.h\"\n+ #include \"clang/Analysis/FlowSensitive/DataflowAnalysisContext.h\"\n+ #include \"clang/Analysis/FlowSensitive/DataflowEnvironment.h\"\n+ #include \"clang/Analysis/FlowSensitive/NoopAnalysis.h\"\n+ #include \"clang/Analysis/FlowSensitive/RecordOps.h\"\n+-#include \"clang/Analysis/FlowSensitive/StorageLocation.h\"\n+ #include \"clang/Analysis/FlowSensitive/Value.h\"\n+ #include \"clang/Basic/Builtins.h\"\n+-#include \"clang/Basic/LLVM.h\"\n+ #include \"clang/Basic/OperatorKinds.h\"\n+ #include \"llvm/Support/Casting.h\"\n+ #include <assert.h>\n+@@ -290,7 +287,7 @@\n+     }\n+   }\n+ \n+-  void VisitCastExpr(const CastExpr *S) {\n++  void VisitImplicitCastExpr(const ImplicitCastExpr *S) {\n+     const Expr *SubExpr = S->getSubExpr();\n+     assert(SubExpr != nullptr);\n+ \n+@@ -320,60 +317,6 @@\n+       break;\n+     }\n+ \n+-    case CK_BaseToDerived: {\n+-      // This is a cast of (single-layer) pointer or reference to a record type.\n+-      // We should now model the fields for the derived type.\n+-\n+-      // Get the RecordStorageLocation for the record object underneath.\n+-      RecordStorageLocation *Loc = nullptr;\n+-      if (S->getType()->isPointerType()) {\n+-        auto *PV = Env.get<PointerValue>(*SubExpr);\n+-        assert(PV != nullptr);\n+-        if (PV == nullptr)\n+-          break;\n+-        Loc = cast<RecordStorageLocation>(&PV->getPointeeLoc());\n+-      } else {\n+-        assert(S->getType()->isRecordType());\n+-        if (SubExpr->isGLValue()) {\n+-          Loc = Env.get<RecordStorageLocation>(*SubExpr);\n+-        } else {\n+-          Loc = &Env.getResultObjectLocation(*SubExpr);\n+-        }\n+-      }\n+-      if (!Loc) {\n+-        // Nowhere to add children or propagate from, so we're done.\n+-        break;\n+-      }\n+-\n+-      // Get the derived record type underneath the reference or pointer.\n+-      QualType Derived = S->getType().getNonReferenceType();\n+-      if (Derived->isPointerType()) {\n+-        Derived = Derived->getPointeeType();\n+-      }\n+-\n+-      // Add children to the storage location for fields (including synthetic\n+-      // fields) of the derived type and initialize their values.\n+-      for (const FieldDecl *Field :\n+-           Env.getDataflowAnalysisContext().getModeledFields(Derived)) {\n+-        assert(Field != nullptr);\n+-        QualType FieldType = Field->getType();\n+-        if (FieldType->isReferenceType()) {\n+-          Loc->addChild(*Field, nullptr);\n+-        } else {\n+-          Loc->addChild(*Field, &Env.createStorageLocation(FieldType));\n+-        }\n+-\n+-        for (const auto &Entry :\n+-             Env.getDataflowAnalysisContext().getSyntheticFields(Derived)) {\n+-          Loc->addSyntheticField(Entry.getKey(),\n+-                                 Env.createStorageLocation(Entry.getValue()));\n+-        }\n+-      }\n+-      Env.initializeFieldsWithValues(*Loc, Derived);\n+-\n+-      // Fall through to propagate SubExpr's StorageLocation to the CastExpr.\n+-      [[fallthrough]];\n+-    }\n+     case CK_IntegralCast:\n+       // FIXME: This cast creates a new integral value from the\n+       // subexpression. But, because we don't model integers, we don't\n+@@ -381,9 +324,10 @@\n+       // modeling is added, then update this code to create a fresh location and\n+       // value.\n+     case CK_UncheckedDerivedToBase:\n+-    case CK_DerivedToBase:\n+     case CK_ConstructorConversion:\n+     case CK_UserDefinedConversion:\n++      // FIXME: Add tests that excercise CK_UncheckedDerivedToBase,\n++      // CK_ConstructorConversion, and CK_UserDefinedConversion.\n+     case CK_NoOp: {\n+       // FIXME: Consider making `Environment::getStorageLocation` skip noop\n+       // expressions (this and other similar expressions in the file) instead\n+@@ -740,6 +684,15 @@\n+     propagateValue(*SubExpr, *S, Env);\n+   }\n+ \n++  void VisitCXXStaticCastExpr(const CXXStaticCastExpr *S) {\n++    if (S->getCastKind() == CK_NoOp) {\n++      const Expr *SubExpr = S->getSubExpr();\n++      assert(SubExpr != nullptr);\n++\n++      propagateValueOrStorageLocation(*SubExpr, *S, Env);\n++    }\n +  }\n-   return true; // All non-instructions are loop invariant\n++\n+   void VisitConditionalOperator(const ConditionalOperator *S) {\n+     const Environment *TrueEnv = StmtToEnv.getEnvironment(*S->getTrueExpr());\n+     const Environment *FalseEnv = StmtToEnv.getEnvironment(*S->getFalseExpr());\n+diff -ruN --strip-trailing-cr a/clang/lib/AST/ASTImporter.cpp b/clang/lib/AST/ASTImporter.cpp\n+--- a/clang/lib/AST/ASTImporter.cpp\n++++ b/clang/lib/AST/ASTImporter.cpp\n+@@ -1740,10 +1740,21 @@\n  }\n  \n--bool Loop::hasLoopInvariantOperands(const Instruction *I) const {\n--  return all_of(I->operands(), [this](Value *V) { return isLoopInvariant(V); });\n-+bool Loop::hasLoopInvariantOperands(const Instruction *I,\n-+                                    bool HasCoroSuspendInst) const {\n-+  return all_of(I->operands(), [&](Value *V) {\n-+    return isLoopInvariant(V, HasCoroSuspendInst);\n-+  });\n- }\n+ ExpectedType ASTNodeImporter::VisitTagType(const TagType *T) {\n+-  Expected<TagDecl *> ToDeclOrErr = import(T->getOriginalDecl());\n++  TagDecl *DeclForType = T->getOriginalDecl();\n++  Expected<TagDecl *> ToDeclOrErr = import(DeclForType);\n+   if (!ToDeclOrErr)\n+     return ToDeclOrErr.takeError();\n  \n- bool Loop::makeLoopInvariant(Value *V, bool &Changed, Instruction *InsertPt,\n-diff -ruN --strip-trailing-cr a/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp b/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp\n---- a/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp\n-+++ b/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp\n-@@ -680,6 +680,8 @@\n-   // No support for these operations with v2f32.\n-   setOperationAction(ISD::INSERT_VECTOR_ELT, MVT::v2f32, Expand);\n-   setOperationAction(ISD::VECTOR_SHUFFLE, MVT::v2f32, Expand);\n-+  // Need custom lowering in case the index is dynamic.\n-+  setOperationAction(ISD::EXTRACT_VECTOR_ELT, MVT::v2f32, Custom);\n- \n-   // Custom conversions to/from v2i8.\n-   setOperationAction(ISD::BITCAST, MVT::v2i8, Custom);\n-diff -ruN --strip-trailing-cr a/llvm/lib/Transforms/Scalar/LICM.cpp b/llvm/lib/Transforms/Scalar/LICM.cpp\n---- a/llvm/lib/Transforms/Scalar/LICM.cpp\n-+++ b/llvm/lib/Transforms/Scalar/LICM.cpp\n-@@ -472,7 +472,7 @@\n-   if (Preheader)\n-     Changed |= hoistRegion(DT->getNode(L->getHeader()), AA, LI, DT, AC, TLI, L,\n-                            MSSAU, SE, &SafetyInfo, Flags, ORE, LoopNestMode,\n--                           LicmAllowSpeculation);\n-+                           LicmAllowSpeculation, HasCoroSuspendInst);\n- \n-   // Now that all loop invariants have been removed from the loop, promote any\n-   // memory references to scalars that we can.\n-@@ -881,7 +881,7 @@\n-                        ICFLoopSafetyInfo *SafetyInfo,\n-                        SinkAndHoistLICMFlags &Flags,\n-                        OptimizationRemarkEmitter *ORE, bool LoopNestMode,\n--                       bool AllowSpeculation) {\n-+                       bool AllowSpeculation, bool HasCoroSuspendInst) {\n-   // Verify inputs.\n-   assert(N != nullptr && AA != nullptr && LI != nullptr && DT != nullptr &&\n-          CurLoop != nullptr && SafetyInfo != nullptr &&\n-@@ -914,11 +914,11 @@\n-       // TODO: It may be safe to hoist if we are hoisting to a conditional block\n-       // and we have accurately duplicated the control flow from the loop header\n-       // to that block.\n--      if (CurLoop->hasLoopInvariantOperands(&I) &&\n-+      if (CurLoop->hasLoopInvariantOperands(&I, HasCoroSuspendInst) &&\n-           canSinkOrHoistInst(I, AA, DT, CurLoop, MSSAU, true, Flags, ORE) &&\n--          isSafeToExecuteUnconditionally(\n--              I, DT, TLI, CurLoop, SafetyInfo, ORE,\n--              Preheader->getTerminator(), AC, AllowSpeculation)) {\n-+          isSafeToExecuteUnconditionally(I, DT, TLI, CurLoop, SafetyInfo, ORE,\n-+                                         Preheader->getTerminator(), AC,\n-+                                         AllowSpeculation)) {\n-         hoist(I, DT, CurLoop, CFH.getOrCreateHoistedBlock(BB), SafetyInfo,\n-               MSSAU, SE, ORE);\n-         HoistedInstructions.push_back(&I);\n-@@ -964,7 +964,7 @@\n-                SafetyInfo->doesNotWriteMemoryBefore(I, CurLoop);\n-       };\n-       if ((IsInvariantStart(I) || isGuard(&I)) &&\n--          CurLoop->hasLoopInvariantOperands(&I) &&\n-+          CurLoop->hasLoopInvariantOperands(&I, HasCoroSuspendInst) &&\n-           MustExecuteWithoutWritesBefore(I)) {\n-         hoist(I, DT, CurLoop, CFH.getOrCreateHoistedBlock(BB), SafetyInfo,\n-               MSSAU, SE, ORE);\n-diff -ruN --strip-trailing-cr a/llvm/test/CodeGen/NVPTX/f32x2-instructions.ll b/llvm/test/CodeGen/NVPTX/f32x2-instructions.ll\n---- a/llvm/test/CodeGen/NVPTX/f32x2-instructions.ll\n-+++ b/llvm/test/CodeGen/NVPTX/f32x2-instructions.ll\n-@@ -79,13 +79,24 @@\n-   ret float %e\n++  if (DeclForType->isUsed()) {\n++    // If there is a definition of the 'OriginalDecl', it should be imported to\n++    // have all information for the type in the \"To\" AST. (In some cases no\n++    // other reference may exist to the definition decl and it would not be\n++    // imported otherwise.)\n++    Expected<TagDecl *> ToDefDeclOrErr = import(DeclForType->getDefinition());\n++    if (!ToDefDeclOrErr)\n++      return ToDefDeclOrErr.takeError();\n++  }\n++\n+   if (T->isCanonicalUnqualified())\n+     return Importer.getToContext().getCanonicalTagType(*ToDeclOrErr);\n+ \n+diff -ruN --strip-trailing-cr a/clang/lib/Sema/SemaDecl.cpp b/clang/lib/Sema/SemaDecl.cpp\n+--- a/clang/lib/Sema/SemaDecl.cpp\n++++ b/clang/lib/Sema/SemaDecl.cpp\n+@@ -5291,10 +5291,8 @@\n+     //   UNION_TYPE;   <- where UNION_TYPE is a typedef union.\n+     if ((Tag && Tag->getDeclName()) ||\n+         DS.getTypeSpecType() == DeclSpec::TST_typename) {\n+-      RecordDecl *Record = dyn_cast_or_null<RecordDecl>(Tag);\n+-      if (!Record)\n+-        Record = DS.getRepAsType().get()->getAsRecordDecl();\n+-\n++      RecordDecl *Record = Tag ? dyn_cast<RecordDecl>(Tag)\n++                               : DS.getRepAsType().get()->getAsRecordDecl();\n+       if (Record && getLangOpts().MicrosoftExt) {\n+         Diag(DS.getBeginLoc(), diag::ext_ms_anonymous_record)\n+             << Record->isUnion() << DS.getSourceRange();\n+@@ -18052,7 +18050,8 @@\n+           }\n+         }\n+       } else if (auto *RD = dyn_cast<CXXRecordDecl>(PrevDecl);\n+-                 RD && RD->isInjectedClassName()) {\n++                 TUK == TagUseKind::Reference && RD &&\n++                 RD->isInjectedClassName()) {\n+         // If lookup found the injected class name, the previous declaration is\n+         // the class being injected into.\n+         PrevDecl = cast<TagDecl>(RD->getDeclContext());\n+@@ -18544,8 +18543,14 @@\n+   if (PrevDecl)\n+     CheckRedeclarationInModule(New, PrevDecl);\n+ \n+-  if (TUK == TagUseKind::Definition && (!SkipBody || !SkipBody->ShouldSkip))\n+-    New->startDefinition();\n++  if (TUK == TagUseKind::Definition) {\n++    if (!SkipBody || !SkipBody->ShouldSkip) {\n++      New->startDefinition();\n++    } else {\n++      New->setCompleteDefinition();\n++      New->demoteThisDefinitionToDeclaration();\n++    }\n++  }\n+ \n+   ProcessDeclAttributeList(S, New, Attrs);\n+   AddPragmaAttributes(S, New);\n+diff -ruN --strip-trailing-cr a/clang/lib/Sema/SemaType.cpp b/clang/lib/Sema/SemaType.cpp\n+--- a/clang/lib/Sema/SemaType.cpp\n++++ b/clang/lib/Sema/SemaType.cpp\n+@@ -9878,7 +9878,14 @@\n+   S.DiagnoseUseOfDecl(ED, Loc);\n+ \n+   QualType Underlying = ED->getIntegerType();\n+-  assert(!Underlying.isNull());\n++  if (Underlying.isNull()) {\n++    // This is an enum without a fixed underlying type which we skipped parsing\n++    // the body because we saw its definition previously in another module.\n++    // Use the definition's integer type in that case.\n++    assert(ED->isThisDeclarationADemotedDefinition());\n++    Underlying = ED->getDefinition()->getIntegerType();\n++    assert(!Underlying.isNull());\n++  }\n+ \n+   return Underlying;\n  }\n+diff -ruN --strip-trailing-cr a/clang/lib/Serialization/ASTReaderDecl.cpp b/clang/lib/Serialization/ASTReaderDecl.cpp\n+--- a/clang/lib/Serialization/ASTReaderDecl.cpp\n++++ b/clang/lib/Serialization/ASTReaderDecl.cpp\n+@@ -2107,6 +2107,8 @@\n+     auto *Def = DD.Definition;\n+     DD = std::move(MergeDD);\n+     DD.Definition = Def;\n++    while ((Def = Def->getPreviousDecl()))\n++      cast<CXXRecordDecl>(Def)->DefinitionData = &DD;\n+     return;\n+   }\n  \n--; NOTE: disabled as -O3 miscompiles this into pointer arithmetic on\n--; test_extract_i_param_0 where the symbol's address is not taken first (that\n--; is, moved to a temporary)\n--; define float @test_extract_i(<2 x float> %a, i64 %idx) #0 {\n--;   %e = extractelement <2 x float> %a, i64 %idx\n--;   ret float %e\n--; }\n-+define float @test_extract_i(<2 x float> %a, i64 %idx) #0 {\n-+; CHECK-LABEL: test_extract_i(\n-+; CHECK:       {\n-+; CHECK-NEXT:    .reg .pred %p<2>;\n-+; CHECK-NEXT:    .reg .b32 %r<4>;\n-+; CHECK-NEXT:    .reg .b64 %rd<3>;\n-+; CHECK-EMPTY:\n-+; CHECK-NEXT:  // %bb.0:\n-+; CHECK-NEXT:    ld.param.b64 %rd2, [test_extract_i_param_1];\n-+; CHECK-NEXT:    ld.param.b64 %rd1, [test_extract_i_param_0];\n-+; CHECK-NEXT:    setp.eq.b64 %p1, %rd2, 0;\n-+; CHECK-NEXT:    mov.b64 {%r1, %r2}, %rd1;\n-+; CHECK-NEXT:    selp.f32 %r3, %r1, %r2, %p1;\n-+; CHECK-NEXT:    st.param.b32 [func_retval0], %r3;\n-+; CHECK-NEXT:    ret;\n-+  %e = extractelement <2 x float> %a, i64 %idx\n-+  ret float %e\n+diff -ruN --strip-trailing-cr a/clang/test/Analysis/ctu-import-type-decl-definition.c b/clang/test/Analysis/ctu-import-type-decl-definition.c\n+--- a/clang/test/Analysis/ctu-import-type-decl-definition.c\n++++ b/clang/test/Analysis/ctu-import-type-decl-definition.c\n+@@ -0,0 +1,43 @@\n++// RUN: rm -rf %t\n++// RUN: mkdir -p %t\n++// RUN: split-file %s %t\n++\n++// RUN: %clang_cc1 -emit-pch -o %t/import.c.ast %t/import.c\n++\n++// RUN: %clang_extdef_map -- -x c %t/import.c >> %t/externalDefMap.txt\n++// RUN: sed -i 's/$/.ast/' %t/externalDefMap.txt\n++\n++// RUN: %clang_cc1 -analyze \\\n++// RUN:   -analyzer-checker=core \\\n++// RUN:   -analyzer-config experimental-enable-naive-ctu-analysis=true \\\n++// RUN:   -analyzer-config display-ctu-progress=true \\\n++// RUN:   -analyzer-config ctu-dir=%t \\\n++// RUN:   -verify %t/main.c\n++\n++//--- main.c\n++\n++// expected-no-diagnostics\n++\n++typedef struct X_s X_t;\n++unsigned long f_import(struct X_s *xPtr);\n++\n++static void freeWriteFileResources(struct X_s *xPtr) {\n++  f_import(xPtr);\n +}\n- \n- define <2 x float> @test_fadd(<2 x float> %a, <2 x float> %b) #0 {\n- ; CHECK-NOF32X2-LABEL: test_fadd(\n-diff -ruN --strip-trailing-cr a/llvm/test/Transforms/LICM/licm-coroutine.ll b/llvm/test/Transforms/LICM/licm-coroutine.ll\n---- a/llvm/test/Transforms/LICM/licm-coroutine.ll\n-+++ b/llvm/test/Transforms/LICM/licm-coroutine.ll\n-@@ -0,0 +1,78 @@\n-+; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5\n-+; RUN: opt < %s -passes=licm -S | FileCheck %s\n-+\n-+; %fca.0 and %fca.1 should not be hoisted out of the loop because the ramp\n-+; function and resume function have different stack frames, so %pointer1 and\n-+; %pointer2 have different values before and after @llvm.coro.suspend.\n-+\n-+define ptr @f(i32 %n) presplitcoroutine {\n-+; CHECK-LABEL: define ptr @f(\n-+; CHECK-SAME: i32 [[N:%.*]]) #[[ATTR0:[0-9]+]] {\n-+; CHECK-NEXT:  [[ENTRY:.*]]:\n-+; CHECK-NEXT:    [[POINTER1:%.*]] = alloca ptr, align 8\n-+; CHECK-NEXT:    [[POINTER2:%.*]] = alloca ptr, align 8\n-+; CHECK-NEXT:    [[ID:%.*]] = call token @llvm.coro.id(i32 0, ptr null, ptr null, ptr null)\n-+; CHECK-NEXT:    [[SIZE:%.*]] = call i32 @llvm.coro.size.i32()\n-+; CHECK-NEXT:    [[ALLOC:%.*]] = call ptr @malloc(i32 [[SIZE]])\n-+; CHECK-NEXT:    [[HDL:%.*]] = call noalias ptr @llvm.coro.begin(token [[ID]], ptr [[ALLOC]])\n-+; CHECK-NEXT:    br label %[[LOOP:.*]]\n-+; CHECK:       [[LOOP]]:\n-+; CHECK-NEXT:    [[N_VAL:%.*]] = phi i32 [ [[N]], %[[ENTRY]] ], [ [[INC:%.*]], %[[RESUME:.*]] ]\n-+; CHECK-NEXT:    [[INC]] = add nsw i32 [[N_VAL]], 1\n-+; CHECK-NEXT:    call void @print(i32 [[N_VAL]])\n-+; CHECK-NEXT:    [[TMP0:%.*]] = call i8 @llvm.coro.suspend(token none, i1 false)\n-+; CHECK-NEXT:    switch i8 [[TMP0]], label %[[SUSPEND_LOOPEXIT:.*]] [\n-+; CHECK-NEXT:      i8 0, label %[[RESUME]]\n-+; CHECK-NEXT:      i8 1, label %[[CLEANUP:.*]]\n-+; CHECK-NEXT:    ]\n-+; CHECK:       [[RESUME]]:\n-+; CHECK-NEXT:    [[FCA_0:%.*]] = insertvalue [2 x ptr] poison, ptr [[POINTER1]], 0\n-+; CHECK-NEXT:    [[FCA_1:%.*]] = insertvalue [2 x ptr] [[FCA_0]], ptr [[POINTER2]], 1\n-+; CHECK-NEXT:    call void @foo([2 x ptr] [[FCA_1]])\n-+; CHECK-NEXT:    br label %[[LOOP]]\n-+; CHECK:       [[CLEANUP]]:\n-+; CHECK-NEXT:    [[MEM:%.*]] = call ptr @llvm.coro.free(token [[ID]], ptr [[HDL]])\n-+; CHECK-NEXT:    call void @free(ptr [[MEM]])\n-+; CHECK-NEXT:    br label %[[SUSPEND:.*]]\n-+; CHECK:       [[SUSPEND_LOOPEXIT]]:\n-+; CHECK-NEXT:    br label %[[SUSPEND]]\n-+; CHECK:       [[SUSPEND]]:\n-+; CHECK-NEXT:    [[UNUSED:%.*]] = call i1 @llvm.coro.end(ptr [[HDL]], i1 false, token none)\n-+; CHECK-NEXT:    ret ptr [[HDL]]\n-+;\n-+entry:\n-+  %pointer1 = alloca ptr\n-+  %pointer2 = alloca ptr\n-+  %id = call token @llvm.coro.id(i32 0, ptr null, ptr null, ptr null)\n-+  %size = call i32 @llvm.coro.size.i32()\n-+  %alloc = call ptr @malloc(i32 %size)\n-+  %hdl = call noalias ptr @llvm.coro.begin(token %id, ptr %alloc)\n-+  br label %loop\n-+\n-+loop:\n-+  %n.val = phi i32 [ %n, %entry ], [ %inc, %resume ]\n-+  %inc = add nsw i32 %n.val, 1\n-+  call void @print(i32 %n.val)\n-+  %0 = call i8 @llvm.coro.suspend(token none, i1 false)\n-+  switch i8 %0, label %suspend [i8 0, label %resume\n-+  i8 1, label %cleanup]\n-+\n-+resume:\n-+  %fca.0 = insertvalue [2 x ptr] poison, ptr %pointer1, 0\n-+  %fca.1 = insertvalue [2 x ptr] %fca.0, ptr %pointer2, 1\n-+  call void @foo([2 x ptr] %fca.1)\n-+  br label %loop\n-+\n-+cleanup:\n-+  %mem = call ptr @llvm.coro.free(token %id, ptr %hdl)\n-+  call void @free(ptr %mem)\n-+  br label %suspend\n-+suspend:\n-+  %unused = call i1 @llvm.coro.end(ptr %hdl, i1 false, token none)\n-+  ret ptr %hdl\n++\n++//--- import.c\n++\n++typedef struct Y_s Y_t;\n++\n++struct Y_s {\n++};\n++\n++struct X_s {\n++  Y_t y;\n++};\n++\n++unsigned long f_import(struct X_s *xPtr) {\n++  if (xPtr != 0) {\n++  }\n++  return 0;\n +}\n+diff -ruN --strip-trailing-cr a/clang/test/AST/ast-dump-decl.cpp b/clang/test/AST/ast-dump-decl.cpp\n+--- a/clang/test/AST/ast-dump-decl.cpp\n++++ b/clang/test/AST/ast-dump-decl.cpp\n+@@ -990,3 +990,18 @@\n+   // CHECK-NEXT:    `-RecordType [[TestInjectedClassName_RT]] 'A' injected\n+   // CHECK-NEXT:      `-CXXRecord [[TestInjectedClassName_RD]] 'A'\n+ } // namespace InjectedClassName\n++\n++namespace TestGH155936 {\n++  struct Foo {\n++    struct A {\n++      struct Foo {};\n++    };\n++  };\n++  // CHECK-LABEL: Dumping TestGH155936:\n++  // CHECK: CXXRecordDecl 0x{{.+}} <{{.+}}> line:[[@LINE-6]]:10 struct Foo definition\n++  // CHECK: CXXRecordDecl 0x{{.+}} <col:3, col:10> col:10 implicit struct Foo\n++  // CHECK: CXXRecordDecl 0x{{.+}} <{{.+}}> line:[[@LINE-7]]:12 struct A definition\n++  // CHECK: CXXRecordDecl 0x{{.+}} <col:5, col:12> col:12 implicit struct A\n++  // CHECK: CXXRecordDecl 0x{{.+}} <line:[[@LINE-8]]:7, col:19> col:14 struct Foo definition\n++  // CHECH: CXXRecordDecl 0x{{.+}} <col:9, col:16> col:16 implicit struct Foo\n++} // namspace GH155936\n+diff -ruN --strip-trailing-cr a/clang/test/Modules/GH154840.cpp b/clang/test/Modules/GH154840.cpp\n+--- a/clang/test/Modules/GH154840.cpp\n++++ b/clang/test/Modules/GH154840.cpp\n+@@ -0,0 +1,97 @@\n++// RUN: rm -rf %t\n++// RUN: mkdir -p %t\n++// RUN: split-file %s %t\n++// RUN: cd %t\n++//\n++// RUN: %clang_cc1 -fmodule-name=A -fno-cxx-modules -emit-module -fmodules -xc++ A.cppmap -o A.pcm\n++// RUN: %clang_cc1 -fmodule-name=B -fno-cxx-modules -emit-module -fmodules -xc++ B.cppmap -o B.pcm -fmodule-file=A.pcm\n++// RUN: %clang_cc1 -fmodule-name=C -fno-cxx-modules -emit-module -fmodules -xc++ C.cppmap -o C.pcm -fmodule-file=A.pcm\n++// RUN: %clang_cc1 -fmodule-name=D -fno-cxx-modules -emit-module -fmodules -xc++ D.cppmap -o D.pcm -fmodule-file=A.pcm\n++// RUN: %clang_cc1 -fmodule-name=E -fno-cxx-modules -emit-module -fmodules -xc++ E.cppmap -o E.pcm -fmodule-file=D.pcm -fmodule-file=B.pcm -fmodule-file=C.pcm\n++// RUN: %clang_cc1 -fno-cxx-modules -fmodules -fmodule-file=B.pcm -fmodule-file=E.pcm -emit-llvm -o /dev/null S.cpp\n++\n++//--- A.h\n++namespace std {\n++\n++template <class T> void zz(T);\n++\n++template <class> struct vec {\n++  struct w {};\n++  struct xx {};\n++\n++  vec(vec &) { init(); }\n++  constexpr vec &operator=(const vec &);\n++  template <class U> constexpr void pb(U);\n++  constexpr void init();\n++\n++  w s;\n++};\n++\n++template <class T> constexpr void vec<T>::init() {\n++  xx yy;\n++  zz(yy);\n++}\n++\n++template <class T> constexpr vec<T> &vec<T>::operator=(const vec &) {\n++  pb(s);\n++  return *this;\n++}\n++\n++template <class T> template <class U> constexpr void vec<T>::pb(U) { init(); }\n++} // namespace std\n++\n++//--- A.cppmap\n++module \"A\" {\n++  header \"A.h\"\n++}\n++\n++//--- X.h\n++#pragma clang module import A\n++\n++namespace project {\n++  class thing : std::vec<thing> {};\n++} // namespace project\n++\n++//--- B.h\n++#include \"X.h\"\n++\n++//--- B.cppmap\n++module \"B\" {\n++  header \"B.h\"\n++}\n++\n++//--- C.h\n++#include \"X.h\"\n++\n++//--- C.cppmap\n++module \"C\" {\n++  header \"C.h\"\n++}\n++\n++//--- D.h\n++#include \"X.h\"\n++\n++//--- D.cppmap\n++module \"D\" {\n++  header \"D.h\"\n++}\n++\n++//--- Y.h\n++#include \"X.h\"\n++struct other {\n++  other() : data(data) {}\n++  std::vec<project::thing> data;\n++};\n++\n++//--- E.h\n++#include \"Y.h\"\n++\n++//--- E.cppmap\n++module \"E\" {\n++  header \"E.h\"\n++}\n++\n++//--- S.cpp\n++#pragma clang module import A\n++#pragma clang module import E\n++void func(std::vec<project::thing> *a, std::vec<project::thing> *b) { *a = *b; }\n+diff -ruN --strip-trailing-cr a/clang/test/Modules/GH155028-1.cpp b/clang/test/Modules/GH155028-1.cpp\n+--- a/clang/test/Modules/GH155028-1.cpp\n++++ b/clang/test/Modules/GH155028-1.cpp\n+@@ -0,0 +1,17 @@\n++// RUN: %clang_cc1 -std=c++20 -verify %s\n++// expected-no-diagnostics\n++\n++#pragma clang module build M\n++module \"M\" {\n++  module \"A\" {}\n++  module \"B\" {}\n++}\n++#pragma clang module contents\n++#pragma clang module begin M.A\n++enum E1 {};\n++#pragma clang module end\n++#pragma clang module begin M.B\n++enum E1 {};\n++using T = __underlying_type(E1);\n++#pragma clang module end\n++#pragma clang module endbuild\n+diff -ruN --strip-trailing-cr a/clang/test/Sema/GH155794.c b/clang/test/Sema/GH155794.c\n+--- a/clang/test/Sema/GH155794.c\n++++ b/clang/test/Sema/GH155794.c\n+@@ -0,0 +1,6 @@\n++// RUN: %clang_cc1 -fsyntax-only -verify -Wno-everything %s\n++\n++struct S {\n++  enum e1 {} // expected-error {{use of empty enum}} expected-error {{expected ';' after enum}}\n++  enum e2 {} // expected-error {{use of empty enum}}\n++}; // expected-error {{expected member name or ';' after declaration specifiers}}\n+diff -ruN --strip-trailing-cr a/clang/test/SemaTemplate/using-decl.cpp b/clang/test/SemaTemplate/using-decl.cpp\n+--- a/clang/test/SemaTemplate/using-decl.cpp\n++++ b/clang/test/SemaTemplate/using-decl.cpp\n+@@ -14,3 +14,15 @@\n+   }\n+   void e() { c<int>(); }\n+ }\n++\n++namespace UsingUsingEnum {\n++  namespace foo {\n++    enum class EnumOne {};\n++  }\n++  using foo::EnumOne;\n++\n++  template <class> void t() {\n++    using enum EnumOne;\n++  }\n++  template void t<void>();\n++} // namespace UsingUsingEnum\n+diff -ruN --strip-trailing-cr a/clang/unittests/Analysis/FlowSensitive/TransferTest.cpp b/clang/unittests/Analysis/FlowSensitive/TransferTest.cpp\n+--- a/clang/unittests/Analysis/FlowSensitive/TransferTest.cpp\n++++ b/clang/unittests/Analysis/FlowSensitive/TransferTest.cpp\n+@@ -9,25 +9,17 @@\n+ #include \"TestingSupport.h\"\n+ #include \"clang/AST/ASTContext.h\"\n+ #include \"clang/AST/Decl.h\"\n+-#include \"clang/AST/Expr.h\"\n+-#include \"clang/AST/ExprCXX.h\"\n+-#include \"clang/AST/OperationKinds.h\"\n+-#include \"clang/ASTMatchers/ASTMatchFinder.h\"\n+ #include \"clang/ASTMatchers/ASTMatchers.h\"\n+-#include \"clang/Analysis/FlowSensitive/DataflowAnalysis.h\"\n+ #include \"clang/Analysis/FlowSensitive/DataflowAnalysisContext.h\"\n+ #include \"clang/Analysis/FlowSensitive/DataflowEnvironment.h\"\n+ #include \"clang/Analysis/FlowSensitive/NoopAnalysis.h\"\n+-#include \"clang/Analysis/FlowSensitive/NoopLattice.h\"\n+ #include \"clang/Analysis/FlowSensitive/RecordOps.h\"\n+ #include \"clang/Analysis/FlowSensitive/StorageLocation.h\"\n+ #include \"clang/Analysis/FlowSensitive/Value.h\"\n+ #include \"clang/Basic/LangStandard.h\"\n+ #include \"clang/Testing/TestAST.h\"\n+ #include \"llvm/ADT/SmallVector.h\"\n+-#include \"llvm/ADT/StringMap.h\"\n+ #include \"llvm/ADT/StringRef.h\"\n+-#include \"llvm/Support/Casting.h\"\n+ #include \"llvm/Testing/Support/Error.h\"\n+ #include \"gmock/gmock.h\"\n+ #include \"gtest/gtest.h\"\n+@@ -35,7 +27,6 @@\n+ #include <string>\n+ #include <string_view>\n+ #include <utility>\n+-#include <vector>\n+ \n+ namespace clang {\n+ namespace dataflow {\n+@@ -3550,7 +3541,7 @@\n+   testFunction(Code, \"noexceptTarget\");\n+ }\n+ \n+-TEST(TransferTest, StaticCastNoOp) {\n++TEST(TransferTest, StaticCast) {\n+   std::string Code = R\"(\n+     void target(int Foo) {\n+       int Bar = static_cast<int>(Foo);\n+@@ -3570,13 +3561,6 @@\n+         const ValueDecl *BarDecl = findValueDecl(ASTCtx, \"Bar\");\n+         ASSERT_THAT(BarDecl, NotNull());\n+ \n+-        const auto *Cast = ast_matchers::selectFirst<CXXStaticCastExpr>(\n+-            \"cast\",\n+-            ast_matchers::match(ast_matchers::cxxStaticCastExpr().bind(\"cast\"),\n+-                                ASTCtx));\n+-        ASSERT_THAT(Cast, NotNull());\n+-        ASSERT_EQ(Cast->getCastKind(), CK_NoOp);\n+-\n+         const auto *FooVal = Env.getValue(*FooDecl);\n+         const auto *BarVal = Env.getValue(*BarDecl);\n+         EXPECT_TRUE(isa<IntegerValue>(FooVal));\n+@@ -3585,268 +3569,6 @@\n+       });\n+ }\n+ \n+-TEST(TransferTest, StaticCastBaseToDerived) {\n+-  std::string Code = R\"cc(\n+-    struct Base {\n+-      char C;\n+-    };\n+-    struct Intermediate : public Base {\n+-      bool B;\n+-    };\n+-    struct Derived : public Intermediate {\n+-      int I;\n+-    };\n+-    Base& getBaseRef();\n+-    void target(Base* BPtr) {\n+-      Derived* DPtr = static_cast<Derived*>(BPtr);\n+-      DPtr->C;\n+-      DPtr->B;\n+-      DPtr->I;\n+-      Derived& DRef = static_cast<Derived&>(*BPtr);\n+-      DRef.C;\n+-      DRef.B;\n+-      DRef.I;\n+-      Derived& DRefFromFunc = static_cast<Derived&>(getBaseRef());\n+-      DRefFromFunc.C;\n+-      DRefFromFunc.B;\n+-      DRefFromFunc.I;\n+-      // [[p]]\n+-    }\n+-  )cc\";\n+-  runDataflow(\n+-      Code,\n+-      [](const llvm::StringMap<DataflowAnalysisState<NoopLattice>> &Results,\n+-         ASTContext &ASTCtx) {\n+-        ASSERT_THAT(Results.keys(), UnorderedElementsAre(\"p\"));\n+-        const Environment &Env = getEnvironmentAtAnnotation(Results, \"p\");\n+-\n+-        const ValueDecl *BPtrDecl = findValueDecl(ASTCtx, \"BPtr\");\n+-        ASSERT_THAT(BPtrDecl, NotNull());\n+-\n+-        const ValueDecl *DPtrDecl = findValueDecl(ASTCtx, \"DPtr\");\n+-        ASSERT_THAT(DPtrDecl, NotNull());\n+-\n+-        const ValueDecl *DRefDecl = findValueDecl(ASTCtx, \"DRef\");\n+-        ASSERT_THAT(DRefDecl, NotNull());\n+-\n+-        const ValueDecl *DRefFromFuncDecl =\n+-            findValueDecl(ASTCtx, \"DRefFromFunc\");\n+-        ASSERT_THAT(DRefFromFuncDecl, NotNull());\n+-\n+-        const auto *Cast = ast_matchers::selectFirst<CXXStaticCastExpr>(\n+-            \"cast\",\n+-            ast_matchers::match(ast_matchers::cxxStaticCastExpr().bind(\"cast\"),\n+-                                ASTCtx));\n+-        ASSERT_THAT(Cast, NotNull());\n+-        ASSERT_EQ(Cast->getCastKind(), CK_BaseToDerived);\n+-\n+-        EXPECT_EQ(Env.getValue(*BPtrDecl), Env.getValue(*DPtrDecl));\n+-        EXPECT_EQ(&Env.get<PointerValue>(*BPtrDecl)->getPointeeLoc(),\n+-                  Env.getStorageLocation(*DRefDecl));\n+-        // For DRefFromFunc, not crashing when analyzing the field accesses is\n+-        // enough.\n+-      });\n+-}\n+-\n+-TEST(TransferTest, ExplicitDerivedToBaseCast) {\n+-  std::string Code = R\"cc(\n+-    struct Base {};\n+-    struct Derived : public Base {};\n+-    void target(Derived D) {\n+-      (Base*)&D;\n+-      // [[p]]\n+-    }\n+-)cc\";\n+-  runDataflow(\n+-      Code,\n+-      [](const llvm::StringMap<DataflowAnalysisState<NoopLattice>> &Results,\n+-         ASTContext &ASTCtx) {\n+-        ASSERT_THAT(Results.keys(), UnorderedElementsAre(\"p\"));\n+-        const Environment &Env = getEnvironmentAtAnnotation(Results, \"p\");\n+-\n+-        auto *Cast = ast_matchers::selectFirst<ImplicitCastExpr>(\n+-            \"cast\", ast_matchers::match(\n+-                        ast_matchers::implicitCastExpr().bind(\"cast\"), ASTCtx));\n+-        ASSERT_THAT(Cast, NotNull());\n+-        ASSERT_EQ(Cast->getCastKind(), CK_DerivedToBase);\n+-\n+-        auto *AddressOf = ast_matchers::selectFirst<UnaryOperator>(\n+-            \"addressof\",\n+-            ast_matchers::match(ast_matchers::unaryOperator().bind(\"addressof\"),\n+-                                ASTCtx));\n+-        ASSERT_THAT(AddressOf, NotNull());\n+-        ASSERT_EQ(AddressOf->getOpcode(), UO_AddrOf);\n+-\n+-        EXPECT_EQ(Env.getValue(*Cast), Env.getValue(*AddressOf));\n+-      });\n+-}\n+-\n+-TEST(TransferTest, ConstructorConversion) {\n+-  std::string Code = R\"cc(\n+-    struct Base {};\n+-    struct Derived : public Base {};\n+-    void target(Derived D) {\n+-      Base B = (Base)D;\n+-      // [[p]]\n+-    }\n+-)cc\";\n+-  runDataflow(\n+-      Code,\n+-      [](const llvm::StringMap<DataflowAnalysisState<NoopLattice>> &Results,\n+-         ASTContext &ASTCtx) {\n+-        ASSERT_THAT(Results.keys(), UnorderedElementsAre(\"p\"));\n+-        const Environment &Env = getEnvironmentAtAnnotation(Results, \"p\");\n+-\n+-        auto *Cast = ast_matchers::selectFirst<CStyleCastExpr>(\n+-            \"cast\", ast_matchers::match(\n+-                        ast_matchers::cStyleCastExpr().bind(\"cast\"), ASTCtx));\n+-        ASSERT_THAT(Cast, NotNull());\n+-        ASSERT_EQ(Cast->getCastKind(), CK_ConstructorConversion);\n+-\n+-        auto &DLoc = getLocForDecl<StorageLocation>(ASTCtx, Env, \"D\");\n+-        auto &BLoc = getLocForDecl<StorageLocation>(ASTCtx, Env, \"B\");\n+-        EXPECT_NE(&BLoc, &DLoc);\n+-      });\n+-}\n+-\n+-TEST(TransferTest, UserDefinedConversion) {\n+-  std::string Code = R\"cc(\n+-    struct To {};\n+-    struct From {\n+-        operator To();\n+-    };\n+-    void target(From F) {\n+-        To T = (To)F;\n+-        // [[p]]\n+-    }\n+-)cc\";\n+-  runDataflow(\n+-      Code,\n+-      [](const llvm::StringMap<DataflowAnalysisState<NoopLattice>> &Results,\n+-         ASTContext &ASTCtx) {\n+-        ASSERT_THAT(Results.keys(), UnorderedElementsAre(\"p\"));\n+-        const Environment &Env = getEnvironmentAtAnnotation(Results, \"p\");\n+-\n+-        auto *Cast = ast_matchers::selectFirst<ImplicitCastExpr>(\n+-            \"cast\", ast_matchers::match(\n+-                        ast_matchers::implicitCastExpr().bind(\"cast\"), ASTCtx));\n+-        ASSERT_THAT(Cast, NotNull());\n+-        ASSERT_EQ(Cast->getCastKind(), CK_UserDefinedConversion);\n+-\n+-        auto &FLoc = getLocForDecl<StorageLocation>(ASTCtx, Env, \"F\");\n+-        auto &TLoc = getLocForDecl<StorageLocation>(ASTCtx, Env, \"T\");\n+-        EXPECT_NE(&TLoc, &FLoc);\n+-      });\n+-}\n+-\n+-TEST(TransferTest, ImplicitUncheckedDerivedToBaseCast) {\n+-  std::string Code = R\"cc(\n+-    struct Base {\n+-      void method();\n+-    };\n+-    struct Derived : public Base {};\n+-    void target(Derived D) {\n+-      D.method();\n+-      // [[p]]\n+-    }\n+-)cc\";\n+-  runDataflow(\n+-      Code,\n+-      [](const llvm::StringMap<DataflowAnalysisState<NoopLattice>> &Results,\n+-         ASTContext &ASTCtx) {\n+-        ASSERT_THAT(Results.keys(), UnorderedElementsAre(\"p\"));\n+-        const Environment &Env = getEnvironmentAtAnnotation(Results, \"p\");\n+-\n+-        auto *Cast = ast_matchers::selectFirst<ImplicitCastExpr>(\n+-            \"cast\", ast_matchers::match(\n+-                        ast_matchers::implicitCastExpr().bind(\"cast\"), ASTCtx));\n+-        ASSERT_THAT(Cast, NotNull());\n+-        ASSERT_EQ(Cast->getCastKind(), CK_UncheckedDerivedToBase);\n+-\n+-        auto &DLoc = getLocForDecl<StorageLocation>(ASTCtx, Env, \"D\");\n+-        EXPECT_EQ(Env.getStorageLocation(*Cast), &DLoc);\n+-      });\n+-}\n+-\n+-TEST(TransferTest, ImplicitDerivedToBaseCast) {\n+-  std::string Code = R\"cc(\n+-    struct Base {};\n+-    struct Derived : public Base {};\n+-    void target() {\n+-      Base* B = new Derived();\n+-      // [[p]]\n+-    }\n+-)cc\";\n+-  runDataflow(\n+-      Code,\n+-      [](const llvm::StringMap<DataflowAnalysisState<NoopLattice>> &Results,\n+-         ASTContext &ASTCtx) {\n+-        ASSERT_THAT(Results.keys(), UnorderedElementsAre(\"p\"));\n+-        const Environment &Env = getEnvironmentAtAnnotation(Results, \"p\");\n+-\n+-        auto *Cast = ast_matchers::selectFirst<ImplicitCastExpr>(\n+-            \"cast\", ast_matchers::match(\n+-                        ast_matchers::implicitCastExpr().bind(\"cast\"), ASTCtx));\n+-        ASSERT_THAT(Cast, NotNull());\n+-        ASSERT_EQ(Cast->getCastKind(), CK_DerivedToBase);\n+-\n+-        auto *New = ast_matchers::selectFirst<CXXNewExpr>(\n+-            \"new\", ast_matchers::match(ast_matchers::cxxNewExpr().bind(\"new\"),\n+-                                       ASTCtx));\n+-        ASSERT_THAT(New, NotNull());\n+-\n+-        EXPECT_EQ(Env.getValue(*Cast), Env.getValue(*New));\n+-      });\n+-}\n+-\n+-TEST(TransferTest, ReinterpretCast) {\n+-  std::string Code = R\"cc(\n+-    struct S {\n+-        int I;\n+-    };\n+-\n+-    void target(unsigned char* Bytes) {\n+-        S& SRef = reinterpret_cast<S&>(Bytes);\n+-        SRef.I;\n+-        S* SPtr = reinterpret_cast<S*>(Bytes);\n+-        SPtr->I;\n+-        // [[p]]\n+-    }\n+-  )cc\";\n+-  runDataflow(Code, [](const llvm::StringMap<DataflowAnalysisState<NoopLattice>>\n+-                           &Results,\n+-                       ASTContext &ASTCtx) {\n+-    ASSERT_THAT(Results.keys(), UnorderedElementsAre(\"p\"));\n+-    const Environment &Env = getEnvironmentAtAnnotation(Results, \"p\");\n+-    const ValueDecl *I = findValueDecl(ASTCtx, \"I\");\n+-    ASSERT_THAT(I, NotNull());\n+-\n+-    // No particular knowledge of I's value is modeled, but for both casts,\n+-    // the fields of S are modeled.\n+-\n+-    {\n+-      auto &Loc = getLocForDecl<RecordStorageLocation>(ASTCtx, Env, \"SRef\");\n+-      std::vector<const ValueDecl *> Children;\n+-      for (const auto &Entry : Loc.children()) {\n+-        Children.push_back(Entry.getFirst());\n+-      }\n+-\n+-      EXPECT_THAT(Children, UnorderedElementsAre(I));\n+-    }\n+-\n+-    {\n+-      auto &Loc = cast<RecordStorageLocation>(\n+-          getValueForDecl<PointerValue>(ASTCtx, Env, \"SPtr\").getPointeeLoc());\n+-      std::vector<const ValueDecl *> Children;\n+-      for (const auto &Entry : Loc.children()) {\n+-        Children.push_back(Entry.getFirst());\n+-      }\n+-\n+-      EXPECT_THAT(Children, UnorderedElementsAre(I));\n+-    }\n+-  });\n+-}\n+-\n+ TEST(TransferTest, IntegralCast) {\n+   std::string Code = R\"(\n+     void target(int Foo) {\n+diff -ruN --strip-trailing-cr a/clang-tools-extra/test/clang-tidy/check_clang_tidy.py b/clang-tools-extra/test/clang-tidy/check_clang_tidy.py\n+--- a/clang-tools-extra/test/clang-tidy/check_clang_tidy.py\n++++ b/clang-tools-extra/test/clang-tidy/check_clang_tidy.py\n+@@ -391,9 +391,7 @@\n+     args, extra_args = parser.parse_known_args()\n+     if args.std is None:\n+         _, extension = os.path.splitext(args.assume_filename or args.input_file_name)\n+-        args.std = [\n+-            \"c++11-or-later\" if extension in [\".cpp\", \".hpp\", \".mm\"] else \"c99-or-later\"\n+-        ]\n++        args.std = [\"c99-or-later\" if extension in [\".c\", \".m\"] else \"c++11-or-later\"]\n+ \n+     return (args, extra_args)\n+ \n+diff -ruN --strip-trailing-cr a/lldb/source/Plugins/SymbolFile/NativePDB/SymbolFileNativePDB.cpp b/lldb/source/Plugins/SymbolFile/NativePDB/SymbolFileNativePDB.cpp\n+--- a/lldb/source/Plugins/SymbolFile/NativePDB/SymbolFileNativePDB.cpp\n++++ b/lldb/source/Plugins/SymbolFile/NativePDB/SymbolFileNativePDB.cpp\n+@@ -1735,11 +1735,11 @@\n+   }\n+ \n+   // Sort them before value searching is working properly.\n+-  m_func_full_names.Sort();\n++  m_func_full_names.Sort(std::less<uint32_t>());\n+   m_func_full_names.SizeToFit();\n+-  m_func_method_names.Sort();\n++  m_func_method_names.Sort(std::less<uint32_t>());\n+   m_func_method_names.SizeToFit();\n+-  m_func_base_names.Sort();\n++  m_func_base_names.Sort(std::less<uint32_t>());\n+   m_func_base_names.SizeToFit();\n+ }\n+ \n+@@ -2426,7 +2426,7 @@\n+ \n+   // After calling Append(), the type-name map needs to be sorted again to be\n+   // able to look up a type by its name.\n+-  m_type_base_names.Sort();\n++  m_type_base_names.Sort(std::less<uint32_t>());\n+ \n+   // Now that we know the forward -> full mapping of all type indices, we can\n+   // re-write all the indices.  At the end of this process, we want a mapping\n+diff -ruN --strip-trailing-cr a/lldb/tools/lldb-dap/Handler/ModuleSymbolsRequestHandler.cpp b/lldb/tools/lldb-dap/Handler/ModuleSymbolsRequestHandler.cpp\n+--- a/lldb/tools/lldb-dap/Handler/ModuleSymbolsRequestHandler.cpp\n++++ b/lldb/tools/lldb-dap/Handler/ModuleSymbolsRequestHandler.cpp\n+@@ -60,7 +60,7 @@\n+     if (!symbol.IsValid())\n+       continue;\n+ \n+-    Symbol dap_symbol;\n++    Symbol dap_symbol = {};\n+     dap_symbol.id = symbol.GetID();\n+     dap_symbol.type = symbol.GetType();\n+     dap_symbol.isDebug = symbol.IsDebug();\n+diff -ruN --strip-trailing-cr a/lldb/tools/lldb-dap/src-ts/ui/symbols-provider.ts b/lldb/tools/lldb-dap/src-ts/ui/symbols-provider.ts\n+--- a/lldb/tools/lldb-dap/src-ts/ui/symbols-provider.ts\n++++ b/lldb/tools/lldb-dap/src-ts/ui/symbols-provider.ts\n+@@ -61,18 +61,18 @@\n+       return;\n+     }\n+ \n+-    this.showSymbolsForModule(session, selectedModule.module);\n++    await this.showSymbolsForModule(session, selectedModule.module);\n+   }\n+ \n+   private async showSymbolsForModule(session: vscode.DebugSession, module: DebugProtocol.Module) {\n+     try {\n+       const symbols = await this.getSymbolsForModule(session, module.id.toString());\n+-      this.showSymbolsInNewTab(module.name.toString(), symbols);\n++      await this.showSymbolsInNewTab(module.name.toString(), symbols);\n+     } catch (error) {\n+       if (error instanceof Error) {\n+-        vscode.window.showErrorMessage(\"Failed to retrieve symbols: \" + error.message);\n++        await vscode.window.showErrorMessage(\"Failed to retrieve symbols: \" + error.message);\n+       } else {\n+-        vscode.window.showErrorMessage(\"Failed to retrieve symbols due to an unknown error.\");\n++        await vscode.window.showErrorMessage(\"Failed to retrieve symbols due to an unknown error.\");\n+       }\n+       \n+       return;\n+@@ -106,7 +106,7 @@\n+     const symbolsTableScriptPath = panel.webview.asWebviewUri(vscode.Uri.joinPath(this.getExtensionResourcePath(), \"symbols-table-view.js\"));\n+ \n+     panel.webview.html = getSymbolsTableHTMLContent(tabulatorJsPath, tabulatorCssPath, symbolsTableScriptPath);\n+-    panel.webview.postMessage({ command: \"updateSymbols\", symbols: symbols });\n++    await panel.webview.postMessage({ command: \"updateSymbols\", symbols: symbols });\n+   }\n+ \n+   private getExtensionResourcePath(): vscode.Uri {\n+diff -ruN --strip-trailing-cr a/utils/bazel/llvm-project-overlay/clang/BUILD.bazel b/utils/bazel/llvm-project-overlay/clang/BUILD.bazel\n+--- a/utils/bazel/llvm-project-overlay/clang/BUILD.bazel\n++++ b/utils/bazel/llvm-project-overlay/clang/BUILD.bazel\n+@@ -58,6 +58,7 @@\n+         \"Refactoring\",\n+         \"Sema\",\n+         \"Serialization\",\n++        \"Trap\",\n+     ] for out in [\n+         (\n+             \"include/clang/Basic/Diagnostic%sKinds.inc\" % c,\n+diff -ruN --strip-trailing-cr a/utils/bazel/llvm-project-overlay/mlir/BUILD.bazel b/utils/bazel/llvm-project-overlay/mlir/BUILD.bazel\n+--- a/utils/bazel/llvm-project-overlay/mlir/BUILD.bazel\n++++ b/utils/bazel/llvm-project-overlay/mlir/BUILD.bazel\n+@@ -4167,6 +4167,7 @@\n+         \":VectorToSCF\",\n+         \":VectorToSPIRV\",\n+         \":VectorToXeGPU\",\n++        \":XeGPUToXeVM\",\n+         \":XeVMToLLVM\",\n+     ],\n+ )\n+@@ -13945,6 +13946,37 @@\n+ )\n+ \n+ cc_library(\n++    name = \"XeGPUToXeVM\",\n++    srcs = glob([\n++        \"lib/Conversion/XeGPUToXeVM/*.cpp\",\n++    ]),\n++    hdrs = glob([\n++        \"include/mlir/Conversion/XeGPUToXeVM/*.h\",\n++    ]),\n++    includes = [\"include\"],\n++    deps = [\n++        \":ArithDialect\",\n++        \":ConversionPassIncGen\",\n++        \":ConvertToLLVMInterface\",\n++        \":GPUDialect\",\n++        \":IR\",\n++        \":IndexDialect\",\n++        \":LLVMCommonConversion\",\n++        \":LLVMDialect\",\n++        \":MemRefDialect\",\n++        \":Pass\",\n++        \":SCFDialect\",\n++        \":SCFTransforms\",\n++        \":Support\",\n++        \":TransformUtils\",\n++        \":VectorDialect\",\n++        \":XeGPUDialect\",\n++        \":XeVMDialect\",\n++        \"//llvm:Support\",\n++    ],\n++)\n +\n-+declare void @free(ptr)\n-+declare ptr @malloc(i32)\n-+declare void @print(i32)\n-+declare void @foo([2 x ptr])\n-diff -ruN --strip-trailing-cr a/mlir/test/Dialect/Linalg/linalg-morph-category-ops.mlir b/mlir/test/Dialect/Linalg/linalg-morph-category-ops.mlir\n---- a/mlir/test/Dialect/Linalg/linalg-morph-category-ops.mlir\n-+++ b/mlir/test/Dialect/Linalg/linalg-morph-category-ops.mlir\n-@@ -2,7 +2,7 @@\n- // RUN: mlir-opt %s -linalg-morph-ops=named-to-category | FileCheck %s  --check-prefix=NAMED_TO_CATEGORY\n- \n- // RUN: mlir-opt %s -linalg-morph-ops=named-to-category |  \\\n--// RUN:   mlir-opt %s -linalg-morph-ops=category-to-generic | FileCheck %s  --check-prefix=CATEGORY_TO_GENERIC\n-+// RUN:   mlir-opt -linalg-morph-ops=category-to-generic | FileCheck %s  --check-prefix=CATEGORY_TO_GENERIC\n- \n- func.func @exp(%A : tensor<16x8xf32>, %B : tensor<16x8xf32>) ->  tensor<16x8xf32> {\n-   %exp = linalg.exp ins(%A : tensor<16x8xf32>) outs(%B :  tensor<16x8xf32>) -> tensor<16x8xf32>\n++cc_library(\n+     name = \"XeVMToLLVM\",\n+     srcs = glob([\n+         \"lib/Conversion/XeVMToLLVM/*.cpp\","
        },
        {
            "sha": "983f65d8524c359281ea17cf82652533babe0b5d",
            "filename": "third_party/xla/third_party/llvm/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fllvm%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fllvm%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fllvm%2Fworkspace.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -4,8 +4,8 @@ load(\"//third_party:repo.bzl\", \"tf_http_archive\")\n \n def repo(name):\n     \"\"\"Imports LLVM.\"\"\"\n-    LLVM_COMMIT = \"fc44a4fcd3c54be927c15ddd9211aca1501633e7\"\n-    LLVM_SHA256 = \"d228aebe5583c69c4e48fd7a8e149e3d22ee6dafaeae94009467143d32d9bfc4\"\n+    LLVM_COMMIT = \"5bca8f2f97d23c3562544e959702826eb20696af\"\n+    LLVM_SHA256 = \"d0e5d52ce939c396f3fa8533d7a1f911ed059e072d4797e3f9cb15043a6fd113\"\n \n     tf_http_archive(\n         name = name,"
        },
        {
            "sha": "5100d91c25ad164a583cf508e11d7e784e650114",
            "filename": "third_party/xla/third_party/protobuf/protobuf-6.31.1.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 201,
            "changes": 201,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Fprotobuf%2Fprotobuf-6.31.1.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Fprotobuf%2Fprotobuf-6.31.1.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fprotobuf%2Fprotobuf-6.31.1.patch?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -1,201 +0,0 @@\n-diff --git a/BUILD.bazel b/BUILD.bazel\n---- a/BUILD.bazel\n-+++ b/BUILD.bazel\n-@@ -555,7 +555,8 @@ proto_lang_toolchain(\n-         \"//:cpp_features_proto\",\n-         \"//:descriptor_proto\",\n-     ],\n--    command_line = \"--cpp_out=$(OUT)\",\n-+    command_line = \"--cpp_out=dllexport_decl=PROTOBUF_EXPORT:$(OUT)\",\n-+#    command_line = \"--cpp_out=$(OUT)\",\n-     runtime = \"//src/google/protobuf\",\n-     visibility = [\"//visibility:public\"],\n- )\n-diff --git a/build_defs/BUILD.bazel b/build_defs/BUILD.bazel\n---- a/build_defs/BUILD.bazel\n-+++ b/build_defs/BUILD.bazel\n-@@ -1,6 +1,7 @@\n- # Internal Starlark definitions for Protobuf.\n- \n- load(\"@bazel_skylib//lib:selects.bzl\", \"selects\")\n-+load(\"@bazel_skylib//rules:common_settings.bzl\", \"bool_flag\")\n- load(\"@rules_pkg//pkg:mappings.bzl\", \"pkg_files\", \"strip_prefix\")\n- load(\"//bazel:cc_proto_library.bzl\", starlark_cc_proto_library = \"cc_proto_library\")\n- load(\":cc_proto_blacklist_test.bzl\", \"cc_proto_blacklist_test\")\n-@@ -13,6 +14,20 @@ package(\n-     ],\n- )\n- \n-+bool_flag(\n-+    name = \"use_dlls\",\n-+    build_setting_default = False,\n-+    visibility = [\"//visibility:public\"],\n-+)\n-+\n-+config_setting(\n-+    name = \"config_use_dlls\",\n-+    flag_values = {\n-+        \":use_dlls\": \"True\",\n-+    },\n-+    visibility = [\"//visibility:public\"],\n-+)\n-+\n- create_compiler_config_setting(\n-     name = \"config_msvc_cl\",\n-     value = \"msvc-cl\",\n-diff --git a/python/dist/system_python.bzl b/python/dist/system_python.bzl\n---- a/python/dist/system_python.bzl\n-+++ b/python/dist/system_python.bzl\n-@@ -73,11 +73,10 @@ load(\"@bazel_skylib//lib:selects.bzl\", \"selects\")\n- load(\"@bazel_skylib//rules:common_settings.bzl\", \"string_flag\")\n- load(\"@bazel_tools//tools/python:toolchain.bzl\", \"py_runtime_pair\")\n- \n--cc_library(\n--   name = \"python_headers\",\n--   hdrs = glob([\"python/**/*.h\"], allow_empty = True),\n--   includes = [\"python\"],\n--   visibility = [\"//visibility:public\"],\n-+alias(\n-+    name = \"python_headers\",\n-+    actual = \"@python//:python_headers\",\n-+    visibility = [\"//visibility:public\"],\n- )\n- \n- string_flag(\n-@@ -219,7 +218,7 @@ def _system_python_impl(repository_ctx):\n-     python3 = repository_ctx.which(\"python3\")\n-     python_version = _get_python_version(repository_ctx)\n- \n--    if path and python_version[0] == \"3\":\n-+    if False:\n-         _populate_package(repository_ctx, path, python3, python_version)\n-     else:\n-         # buildifier: disable=print\n-diff --git a/python/google/protobuf/__init__.py b/python/google/protobuf/__init__.py\n---- a/python/google/protobuf/__init__.py\n-+++ b/python/google/protobuf/__init__.py\n-@@ -8,3 +8,9 @@\n- # Copyright 2007 Google Inc. All Rights Reserved.\n- \n- __version__ = '6.31.1'\n-+\n-+if __name__ != '__main__':\n-+  try:\n-+    __import__('pkg_resources').declare_namespace(__name__)\n-+  except ImportError:\n-+    __path__ = __import__('pkgutil').extend_path(__path__, __name__)\n-diff --git a/src/google/protobuf/BUILD.bazel b/src/google/protobuf/BUILD.bazel\n---- a/src/google/protobuf/BUILD.bazel\n-+++ b/src/google/protobuf/BUILD.bazel\n-@@ -525,6 +525,13 @@ cc_library(\n-         \"serial_arena.h\",\n-         \"thread_safe_arena.h\",\n-     ],\n-+    local_defines = select({\n-+        \"//build_defs:config_use_dlls\": [\n-+            \"PROTOBUF_USE_DLLS\",\n-+            \"LIBPROTOBUF_EXPORTS\",\n-+        ],\n-+        \"//conditions:default\": [],\n-+    }),\n-     strip_include_prefix = \"/src\",\n-     visibility = [\n-         \"//:__subpackages__\",\n-@@ -657,7 +664,15 @@ cc_library(\n-         \"serial_arena.h\",\n-         \"thread_safe_arena.h\",\n-         \"wire_format_lite.h\",\n-+        \"port.h\",\n-     ],\n-+    local_defines = select({\n-+        \"//build_defs:config_use_dlls\": [\n-+            \"PROTOBUF_USE_DLLS\",\n-+            \"LIBPROTOBUF_EXPORTS\",\n-+        ],\n-+        \"//conditions:default\": [],\n-+    }),\n-     copts = COPTS + select({\n-         \"//build_defs:config_msvc\": [],\n-         \"//conditions:default\": [\n-@@ -767,6 +782,13 @@ cc_library(\n-     ],\n-     hdrs = PROTOBUF_HEADERS,\n-     copts = COPTS,\n-+    local_defines = select({\n-+        \"//build_defs:config_use_dlls\": [\n-+            \"PROTOBUF_USE_DLLS\",\n-+            \"LIBPROTOBUF_EXPORTS\",\n-+        ],\n-+        \"//conditions:default\": [],\n-+    }),\n-     linkopts = LINK_OPTS,\n-     strip_include_prefix = \"/src\",\n-     visibility = [\n-diff --git a/src/google/protobuf/arena.cc b/src/google/protobuf/arena.cc\n---- a/src/google/protobuf/arena.cc\n-+++ b/src/google/protobuf/arena.cc\n-@@ -547,7 +547,7 @@ ThreadSafeArena::ThreadCache& ThreadSafeArena::thread_cache() {\n-       new internal::ThreadLocalStorage<ThreadCache>();\n-   return *thread_cache_->Get();\n- }\n--#elif defined(PROTOBUF_USE_DLLS) && defined(_WIN32)\n-+#elif defined(_WIN32)\n- ThreadSafeArena::ThreadCache& ThreadSafeArena::thread_cache() {\n-   static PROTOBUF_THREAD_LOCAL ThreadCache thread_cache;\n-   return thread_cache;\n-diff --git a/src/google/protobuf/io/BUILD.bazel b/src/google/protobuf/io/BUILD.bazel\n---- a/src/google/protobuf/io/BUILD.bazel\n-+++ b/src/google/protobuf/io/BUILD.bazel\n-@@ -22,6 +22,13 @@ cc_library(\n-         \"zero_copy_stream_impl.h\",\n-         \"zero_copy_stream_impl_lite.h\",\n-     ],\n-+    local_defines = select({\n-+        \"//build_defs:config_use_dlls\": [\n-+            \"PROTOBUF_USE_DLLS\",\n-+            \"LIBPROTOBUF_EXPORTS\",\n-+        ],\n-+        \"//conditions:default\": [],\n-+    }),\n-     copts = COPTS,\n-     strip_include_prefix = \"/src\",\n-     deps = [\n-diff --git a/src/google/protobuf/message_lite.h b/src/google/protobuf/message_lite.h\n---- a/src/google/protobuf/message_lite.h\n-+++ b/src/google/protobuf/message_lite.h\n-@@ -282,7 +282,11 @@ template <typename T>\n- using MessageTraits = decltype(MessageTraitsImpl::value<T>);\n- \n- struct EnumTraitsImpl {\n-+#ifdef __CUDACC__\n-+  struct Undefined{};\n-+#else\n-   struct Undefined;\n-+#endif\n-   template <typename T>\n-   static Undefined value;\n- };\n-diff --git a/src/google/protobuf/port_def.inc b/src/google/protobuf/port_def.inc\n---- a/src/google/protobuf/port_def.inc\n-+++ b/src/google/protobuf/port_def.inc\n-@@ -421,7 +421,7 @@ static_assert(PROTOBUF_ABSL_MIN(20230125, 3),\n- #endif\n- \n- // Lexan sets both MSV_VER and clang, so handle it with the clang path.\n--#if defined(_MSC_VER) && !defined(__clang__)\n-+#if defined(_MSC_VER)\n- // MSVC 17 currently seems to raise an error about constant-initialized pointers.\n- # if PROTOBUF_MSC_VER_MIN(1930)\n- #  define PROTOBUF_CONSTINIT\n-diff --git a/src/google/protobuf/thread_safe_arena.h b/src/google/protobuf/thread_safe_arena.h\n---- a/src/google/protobuf/thread_safe_arena.h\n-+++ b/src/google/protobuf/thread_safe_arena.h\n-@@ -249,7 +249,7 @@ class PROTOBUF_EXPORT ThreadSafeArena {\n-   // iOS does not support __thread keyword so we use a custom thread local\n-   // storage class we implemented.\n-   static ThreadCache& thread_cache();\n--#elif defined(PROTOBUF_USE_DLLS) && defined(_WIN32)\n-+#elif defined(_WIN32)\n-   // Thread local variables cannot be exposed through MSVC DLL interface but we\n-   // can wrap them in static functions.\n-   static ThreadCache& thread_cache();"
        },
        {
            "sha": "d02ba25380563601a1511bf80fd9ae69cc3f36cc",
            "filename": "third_party/xla/third_party/protobuf/protobuf.patch",
            "status": "modified",
            "additions": 104,
            "deletions": 101,
            "changes": 205,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fprotobuf%2Fprotobuf.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fprotobuf%2Fprotobuf.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fprotobuf%2Fprotobuf.patch?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1,18 +1,19 @@\n-diff --git a/src/google/protobuf/map_field.h b/src/google/protobuf/map_field.h\n---- a/src/google/protobuf/map_field.h\t(revision 5fda5abda3dee5f7a102c85860594bff8d8610bd)\n-+++ b/src/google/protobuf/map_field.h\t(date 1748033655723)\n-@@ -710,7 +710,7 @@\n-   typedef MapField<T, Key, Value, kKeyFieldType, kValueFieldType> MapFieldType;\n- };\n-\n--class PROTOBUF_EXPORT DynamicMapField final\n-+class DynamicMapField final\n-     : public TypeDefinedMapFieldBase<MapKey, MapValueRef> {\n-  public:\n-   explicit DynamicMapField(const Message* default_entry);\n+diff --git a/BUILD.bazel b/BUILD.bazel\n+--- a/BUILD.bazel\n++++ b/BUILD.bazel\n+@@ -555,7 +555,8 @@ proto_lang_toolchain(\n+         \"//:cpp_features_proto\",\n+         \"//:descriptor_proto\",\n+     ],\n+-    command_line = \"--cpp_out=$(OUT)\",\n++    command_line = \"--cpp_out=dllexport_decl=PROTOBUF_EXPORT:$(OUT)\",\n++#    command_line = \"--cpp_out=$(OUT)\",\n+     runtime = \"//src/google/protobuf\",\n+     visibility = [\"//visibility:public\"],\n+ )\n diff --git a/build_defs/BUILD.bazel b/build_defs/BUILD.bazel\n---- a/build_defs/BUILD.bazel\t(revision 5fda5abda3dee5f7a102c85860594bff8d8610bd)\n-+++ b/build_defs/BUILD.bazel\t(date 1748034990268)\n+--- a/build_defs/BUILD.bazel\n++++ b/build_defs/BUILD.bazel\n @@ -1,6 +1,7 @@\n  # Internal Starlark definitions for Protobuf.\n \n@@ -21,7 +22,7 @@ diff --git a/build_defs/BUILD.bazel b/build_defs/BUILD.bazel\n  load(\"@rules_pkg//pkg:mappings.bzl\", \"pkg_files\", \"strip_prefix\")\n  load(\"//bazel:cc_proto_library.bzl\", starlark_cc_proto_library = \"cc_proto_library\")\n  load(\":cc_proto_blacklist_test.bzl\", \"cc_proto_blacklist_test\")\n-@@ -13,6 +14,20 @@\n+@@ -13,6 +14,20 @@ package(\n      ],\n  )\n \n@@ -43,9 +44,9 @@ diff --git a/build_defs/BUILD.bazel b/build_defs/BUILD.bazel\n      name = \"config_msvc_cl\",\n      value = \"msvc-cl\",\n diff --git a/python/dist/system_python.bzl b/python/dist/system_python.bzl\n---- a/python/dist/system_python.bzl\t(revision 5fda5abda3dee5f7a102c85860594bff8d8610bd)\n-+++ b/python/dist/system_python.bzl\t(date 1746939979885)\n-@@ -73,11 +73,10 @@\n+--- a/python/dist/system_python.bzl\n++++ b/python/dist/system_python.bzl\n+@@ -73,11 +73,10 @@ load(\"@bazel_skylib//lib:selects.bzl\", \"selects\")\n  load(\"@bazel_skylib//rules:common_settings.bzl\", \"string_flag\")\n  load(\"@bazel_tools//tools/python:toolchain.bzl\", \"py_runtime_pair\")\n \n@@ -61,7 +62,7 @@ diff --git a/python/dist/system_python.bzl b/python/dist/system_python.bzl\n  )\n \n  string_flag(\n-@@ -219,7 +218,7 @@\n+@@ -219,7 +218,7 @@ def _system_python_impl(repository_ctx):\n      python3 = repository_ctx.which(\"python3\")\n      python_version = _get_python_version(repository_ctx)\n \n@@ -70,76 +71,23 @@ diff --git a/python/dist/system_python.bzl b/python/dist/system_python.bzl\n          _populate_package(repository_ctx, path, python3, python_version)\n      else:\n          # buildifier: disable=print\n-diff --git a/src/google/protobuf/arena.cc b/src/google/protobuf/arena.cc\n---- a/src/google/protobuf/arena.cc\t(revision 5fda5abda3dee5f7a102c85860594bff8d8610bd)\n-+++ b/src/google/protobuf/arena.cc\t(date 1748042877047)\n-@@ -554,7 +554,7 @@\n-       new internal::ThreadLocalStorage<ThreadCache>();\n-   return *thread_cache_->Get();\n- }\n--#elif defined(PROTOBUF_USE_DLLS) && defined(_WIN32)\n-+#elif defined(_WIN32)\n- ThreadSafeArena::ThreadCache& ThreadSafeArena::thread_cache() {\n-   static PROTOBUF_THREAD_LOCAL ThreadCache thread_cache;\n-   return thread_cache;\n-diff --git a/BUILD.bazel b/BUILD.bazel\n---- a/BUILD.bazel\t(revision 5fda5abda3dee5f7a102c85860594bff8d8610bd)\n-+++ b/BUILD.bazel\t(date 1746940994484)\n-@@ -424,7 +424,8 @@\n-         \"//:cpp_features_proto\",\n-         \"//:descriptor_proto\",\n-     ],\n--    command_line = \"--cpp_out=$(OUT)\",\n-+    command_line = \"--cpp_out=dllexport_decl=PROTOBUF_EXPORT:$(OUT)\",\n-+#    command_line = \"--cpp_out=$(OUT)\",\n-     runtime = \"//src/google/protobuf\",\n-     visibility = [\"//visibility:public\"],\n- )\n-diff --git a/src/google/protobuf/io/BUILD.bazel b/src/google/protobuf/io/BUILD.bazel\n---- a/src/google/protobuf/io/BUILD.bazel\t(revision 5fda5abda3dee5f7a102c85860594bff8d8610bd)\n-+++ b/src/google/protobuf/io/BUILD.bazel\t(date 1748038681727)\n-@@ -22,6 +22,13 @@\n-         \"zero_copy_stream_impl.h\",\n-         \"zero_copy_stream_impl_lite.h\",\n-     ],\n-+    local_defines = select({\n-+        \"//build_defs:config_use_dlls\": [\n-+            \"PROTOBUF_USE_DLLS\",\n-+            \"LIBPROTOBUF_EXPORTS\",\n-+        ],\n-+        \"//conditions:default\": [],\n-+    }),\n-     copts = COPTS,\n-     strip_include_prefix = \"/src\",\n-     deps = [\n-diff --git a/src/google/protobuf/port_def.inc b/src/google/protobuf/port_def.inc\n---- a/src/google/protobuf/port_def.inc\t(revision 5fda5abda3dee5f7a102c85860594bff8d8610bd)\n-+++ b/src/google/protobuf/port_def.inc\t(date 1748019340115)\n-@@ -456,7 +456,7 @@\n- #endif\n+diff --git a/python/google/protobuf/__init__.py b/python/google/protobuf/__init__.py\n+--- a/python/google/protobuf/__init__.py\n++++ b/python/google/protobuf/__init__.py\n+@@ -8,3 +8,9 @@\n+ # Copyright 2007 Google Inc. All Rights Reserved.\n \n- // Lexan sets both MSV_VER and clang, so handle it with the clang path.\n--#if defined(_MSC_VER) && !defined(__clang__)\n-+#if defined(_MSC_VER)\n- // MSVC 17 currently seems to raise an error about constant-initialized pointers.\n- # if PROTOBUF_MSC_VER_MIN(1930)\n- #  define PROTOBUF_CONSTINIT\n-diff --git a/src/google/protobuf/thread_safe_arena.h b/src/google/protobuf/thread_safe_arena.h\n---- a/src/google/protobuf/thread_safe_arena.h\t(revision 5fda5abda3dee5f7a102c85860594bff8d8610bd)\n-+++ b/src/google/protobuf/thread_safe_arena.h\t(date 1748042886641)\n-@@ -248,7 +248,7 @@\n-   // iOS does not support __thread keyword so we use a custom thread local\n-   // storage class we implemented.\n-   static ThreadCache& thread_cache();\n--#elif defined(PROTOBUF_USE_DLLS) && defined(_WIN32)\n-+#elif defined(_WIN32)\n-   // Thread local variables cannot be exposed through MSVC DLL interface but we\n-   // can wrap them in static functions.\n-   static ThreadCache& thread_cache();\n+ __version__ = '6.31.1'\n++\n++if __name__ != '__main__':\n++  try:\n++    __import__('pkg_resources').declare_namespace(__name__)\n++  except ImportError:\n++    __path__ = __import__('pkgutil').extend_path(__path__, __name__)\n diff --git a/src/google/protobuf/BUILD.bazel b/src/google/protobuf/BUILD.bazel\n---- a/src/google/protobuf/BUILD.bazel\t(revision 5fda5abda3dee5f7a102c85860594bff8d8610bd)\n-+++ b/src/google/protobuf/BUILD.bazel\t(date 1748038617126)\n-@@ -411,6 +411,13 @@\n+--- a/src/google/protobuf/BUILD.bazel\n++++ b/src/google/protobuf/BUILD.bazel\n+@@ -525,6 +525,13 @@ cc_library(\n          \"serial_arena.h\",\n          \"thread_safe_arena.h\",\n      ],\n@@ -153,7 +101,7 @@ diff --git a/src/google/protobuf/BUILD.bazel b/src/google/protobuf/BUILD.bazel\n      strip_include_prefix = \"/src\",\n      visibility = [\n          \"//:__subpackages__\",\n-@@ -509,7 +516,15 @@\n+@@ -657,7 +664,15 @@ cc_library(\n          \"serial_arena.h\",\n          \"thread_safe_arena.h\",\n          \"wire_format_lite.h\",\n@@ -169,7 +117,7 @@ diff --git a/src/google/protobuf/BUILD.bazel b/src/google/protobuf/BUILD.bazel\n      copts = COPTS + select({\n          \"//build_defs:config_msvc\": [],\n          \"//conditions:default\": [\n-@@ -615,6 +630,13 @@\n+@@ -767,6 +782,13 @@ cc_library(\n      ],\n      hdrs = PROTOBUF_HEADERS,\n      copts = COPTS,\n@@ -183,16 +131,71 @@ diff --git a/src/google/protobuf/BUILD.bazel b/src/google/protobuf/BUILD.bazel\n      linkopts = LINK_OPTS,\n      strip_include_prefix = \"/src\",\n      visibility = [\n-diff --git a/python/google/protobuf/__init__.py b/python/google/protobuf/__init__.py\n---- a/python/google/protobuf/__init__.py\t(revision 5fda5abda3dee5f7a102c85860594bff8d8610bd)\n-+++ b/python/google/protobuf/__init__.py\t(date 1746939979902)\n-@@ -8,3 +8,9 @@\n- # Copyright 2007 Google Inc. All Rights Reserved.\n+diff --git a/src/google/protobuf/arena.cc b/src/google/protobuf/arena.cc\n+--- a/src/google/protobuf/arena.cc\n++++ b/src/google/protobuf/arena.cc\n+@@ -547,7 +547,7 @@ ThreadSafeArena::ThreadCache& ThreadSafeArena::thread_cache() {\n+       new internal::ThreadLocalStorage<ThreadCache>();\n+   return *thread_cache_->Get();\n+ }\n+-#elif defined(PROTOBUF_USE_DLLS) && defined(_WIN32)\n++#elif defined(_WIN32)\n+ ThreadSafeArena::ThreadCache& ThreadSafeArena::thread_cache() {\n+   static PROTOBUF_THREAD_LOCAL ThreadCache thread_cache;\n+   return thread_cache;\n+diff --git a/src/google/protobuf/io/BUILD.bazel b/src/google/protobuf/io/BUILD.bazel\n+--- a/src/google/protobuf/io/BUILD.bazel\n++++ b/src/google/protobuf/io/BUILD.bazel\n+@@ -22,6 +22,13 @@ cc_library(\n+         \"zero_copy_stream_impl.h\",\n+         \"zero_copy_stream_impl_lite.h\",\n+     ],\n++    local_defines = select({\n++        \"//build_defs:config_use_dlls\": [\n++            \"PROTOBUF_USE_DLLS\",\n++            \"LIBPROTOBUF_EXPORTS\",\n++        ],\n++        \"//conditions:default\": [],\n++    }),\n+     copts = COPTS,\n+     strip_include_prefix = \"/src\",\n+     deps = [\n+diff --git a/src/google/protobuf/message_lite.h b/src/google/protobuf/message_lite.h\n+--- a/src/google/protobuf/message_lite.h\n++++ b/src/google/protobuf/message_lite.h\n+@@ -282,7 +282,11 @@ template <typename T>\n+ using MessageTraits = decltype(MessageTraitsImpl::value<T>);\n \n- __version__ = '5.28.3'\n-+\n-+if __name__ != '__main__':\n-+  try:\n-+    __import__('pkg_resources').declare_namespace(__name__)\n-+  except ImportError:\n-+    __path__ = __import__('pkgutil').extend_path(__path__, __name__)\n+ struct EnumTraitsImpl {\n++#ifdef __CUDACC__\n++  struct Undefined{};\n++#else\n+   struct Undefined;\n++#endif\n+   template <typename T>\n+   static Undefined value;\n+ };\n+diff --git a/src/google/protobuf/port_def.inc b/src/google/protobuf/port_def.inc\n+--- a/src/google/protobuf/port_def.inc\n++++ b/src/google/protobuf/port_def.inc\n+@@ -421,7 +421,7 @@ static_assert(PROTOBUF_ABSL_MIN(20230125, 3),\n+ #endif\n+\n+ // Lexan sets both MSV_VER and clang, so handle it with the clang path.\n+-#if defined(_MSC_VER) && !defined(__clang__)\n++#if defined(_MSC_VER)\n+ // MSVC 17 currently seems to raise an error about constant-initialized pointers.\n+ # if PROTOBUF_MSC_VER_MIN(1930)\n+ #  define PROTOBUF_CONSTINIT\n+diff --git a/src/google/protobuf/thread_safe_arena.h b/src/google/protobuf/thread_safe_arena.h\n+--- a/src/google/protobuf/thread_safe_arena.h\n++++ b/src/google/protobuf/thread_safe_arena.h\n+@@ -249,7 +249,7 @@ class PROTOBUF_EXPORT ThreadSafeArena {\n+   // iOS does not support __thread keyword so we use a custom thread local\n+   // storage class we implemented.\n+   static ThreadCache& thread_cache();\n+-#elif defined(PROTOBUF_USE_DLLS) && defined(_WIN32)\n++#elif defined(_WIN32)\n+   // Thread local variables cannot be exposed through MSVC DLL interface but we\n+   // can wrap them in static functions.\n+   static ThreadCache& thread_cache();"
        },
        {
            "sha": "5afdeffee932391267a2e40bdc4509db5b14179b",
            "filename": "third_party/xla/third_party/py/BUILD.bazel",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fpy%2FBUILD.bazel",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fpy%2FBUILD.bazel",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fpy%2FBUILD.bazel?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -65,3 +65,9 @@ py_binary(\n     main = \"unpack_wheel_and_unzip_archive_files.py\",\n     visibility = [\"//visibility:public\"],\n )\n+\n+py_library(\n+    name = \"setup_py_nvidia_dependencies_util\",\n+    srcs = [\"setup_py_nvidia_dependencies_util.py\"],\n+    visibility = [\"//visibility:public\"],\n+)"
        },
        {
            "sha": "231e667b2134f8d1dd93d11089e9890d8c01b65c",
            "filename": "third_party/xla/third_party/py/README.md",
            "status": "added",
            "additions": 105,
            "deletions": 0,
            "changes": 105,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fpy%2FREADME.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fpy%2FREADME.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fpy%2FREADME.md?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -0,0 +1,105 @@\n+# ML wheels\n+\n+Provides a standardized and efficient system for packaging and verifying ML\n+software.\n+\n+## ML wheels features\n+\n+- Standardized creation, validation (`auditwheel`) and testing of wheel\n+  artifacts.\n+\n+- Availability of the final wheel artifacts in the Bazel Build phase, which\n+  enables testing of generated wheels by regular `py_test` targets\n+  together with the rest of the existing tests.\n+\n+- Ability to use Bazel RBE for wheel creation and testing.\n+\n+- Reproducible and unified steps for generating testing of the wheels on\n+  different platforms.\n+\n+## Getting started\n+\n+1. Integrate hermetic Python, C++ and CUDA (if needed) toolchains in the\n+   project.\n+\n+   Examples:\n+\n+   [JAX hermetic Python and C++ integration](https://github.com/jax-ml/jax/blob/006b2904720bf029cb4298ab963f8f50438e79df/WORKSPACE#L16-L84)\n+\n+   [JAX hermetic CUDA integration](https://github.com/jax-ml/jax/blob/006b2904720bf029cb4298ab963f8f50438e79df/WORKSPACE#L131-L196)\n+\n+   [TensorFlow hermetic Python integration](https://github.com/tensorflow/tensorflow/blob/5feca557408c3552494c4db03a02b36f9817bd37/WORKSPACE#L32-L67)\n+\n+   [TensorFlow hermetic C++ and CUDA integration](https://github.com/tensorflow/tensorflow/blob/5feca557408c3552494c4db03a02b36f9817bd37/WORKSPACE#L94-L170)\n+\n+2. Create python script that produces a wheel, and declare it as `py_binary`\n+   build rule.\n+\n+   A common case scenario: a python script should take wheel sources provided in\n+   the arguments list, then do the required transformations and run command like\n+   `python -m build` in the folder with the collected resources.\n+\n+   [JAX py_binary declaration](https://github.com/jax-ml/jax/blob/006b2904720bf029cb4298ab963f8f50438e79df/jaxlib/tools/BUILD.bazel#L230-L241)\n+\n+   [TensorFlow py_binary declaration](https://github.com/tensorflow/tensorflow/blob/5feca557408c3552494c4db03a02b36f9817bd37/tensorflow/tools/pip_package/BUILD#L242-L253)\n+\n+3. Create Bazel build rule that returns python wheel in the output.\n+\n+   In a common case scenario, this Bazel rule runs `py_binary` (created in\n+   step 1) passed in the rule attributes.\n+\n+   [JAX rule definition](https://github.com/tensorflow/tensorflow/blob/5feca557408c3552494c4db03a02b36f9817bd37/tensorflow/tools/pip_package/BUILD#L242-L253)\n+\n+   [TensorFlow rule definition](https://github.com/tensorflow/tensorflow/blob/5feca557408c3552494c4db03a02b36f9817bd37/tensorflow/tools/pip_package/utils/tf_wheel.bzl#L137-L154)\n+\n+   - The wheel sources should be provided in the wheel build rule attributes.\n+\n+     To collect the wheel sources that are suitable for all types of Bazel\n+     builds, including cross-compile builds, the following build rules should be\n+     used: `collect_data_files`, `transitive_py_deps` from\n+     `@local_xla//third_party/py:python_wheel.bzl`, and `transitive_hdrs` from\n+     `@local_xla//xla/tsl:tsl.bzl`.\n+\n+     [jaxlib wheel sources](https://github.com/jax-ml/jax/blob/006b2904720bf029cb4298ab963f8f50438e79df/jaxlib/tools/BUILD.bazel#L243-L265)\n+\n+     [TensorFlow wheel sources](https://github.com/tensorflow/tensorflow/blob/5feca557408c3552494c4db03a02b36f9817bd37/tensorflow/tools/pip_package/BUILD#L312-L367)\n+\n+   - the wheel name should conform to\n+     [PEP-491 naming convention](https://peps.python.org/pep-0491/#file-name-convention).\n+\n+     [JAX example](https://github.com/jax-ml/jax/blob/006b2904720bf029cb4298ab963f8f50438e79df/jaxlib/jax.bzl#L326-L348)\n+\n+     [TensorFlow example](https://github.com/tensorflow/tensorflow/blob/5feca557408c3552494c4db03a02b36f9817bd37/tensorflow/tools/pip_package/utils/tf_wheel.bzl#L56-L69)\n+\n+   - Storing of the wheel version is custom, and should be implemented per\n+     project. It can be additional repository rule, or a constant in .bzl file.\n+\n+     [JAX example](https://github.com/jax-ml/jax/blob/006b2904720bf029cb4298ab963f8f50438e79df/WORKSPACE#L108-L114)\n+\n+     [Tensorflow example](https://github.com/tensorflow/tensorflow/blob/5feca557408c3552494c4db03a02b36f9817bd37/tensorflow/tf_version.bzl#L11)\n+\n+   - The wheel suffix is controlled by a common repository rule\n+     `python_wheel_version_suffix_repository`, that should be called in\n+     `WORKSPACE` file.\n+\n+     [JAX rule call](https://github.com/jax-ml/jax/blob/006b2904720bf029cb4298ab963f8f50438e79df/WORKSPACE#L127-L129)\n+\n+     [Tensorflow rule call](https://github.com/tensorflow/tensorflow/blob/5feca557408c3552494c4db03a02b36f9817bd37/WORKSPACE#L92)\n+\n+4. To verify manylinux tag compliance, use common py_binary\n+  `verify_manylinux_compliance_test`.\n+\n+  [JAX tests](https://github.com/jax-ml/jax/blob/006b2904720bf029cb4298ab963f8f50438e79df/jaxlib/tools/BUILD.bazel#L626-L668)\n+\n+  [Tensorflow test](https://github.com/tensorflow/tensorflow/blob/5feca557408c3552494c4db03a02b36f9817bd37/tensorflow/tools/pip_package/BUILD#L441-L450)\n+\n+5. With the wheel build rule defined, one can run Bazel test targets dependent\n+  on the wheel instead of individual Bazel targets. To implement it, define\n+  `py_import` call. `py_import` target can be used in other python targets in\n+  the same way as `py_library`.\n+\n+  [JAX example](https://github.com/jax-ml/jax/blob/006b2904720bf029cb4298ab963f8f50438e79df/jaxlib/tools/BUILD.bazel#L542-L570)\n+\n+  [Tensorflow example](https://github.com/tensorflow/tensorflow/blob/5feca557408c3552494c4db03a02b36f9817bd37/tensorflow/tools/pip_package/BUILD#L452-L485)\n+\n+  [TensorFlow tests dependent on `py_import`](https://github.com/tensorflow/tensorflow/blob/5feca557408c3552494c4db03a02b36f9817bd37/tensorflow/tools/pip_package/BUILD#L411-L439)"
        },
        {
            "sha": "6b80837d9a7b55e2c8bff383e9949e931b6af97d",
            "filename": "third_party/xla/third_party/py/python_wheel.bzl",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fpy%2Fpython_wheel.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fpy%2Fpython_wheel.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fpy%2Fpython_wheel.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -257,3 +257,23 @@ It recursively traverses `deps` attribute of the target and collects paths to\n files that are in `data` attribute. Then it filters all files that do not match\n the provided extensions.\n \"\"\"  # buildifier: disable=no-effect\n+\n+def _nvidia_wheel_versions_repository_impl(repository_ctx):\n+    \"\"\"Repository rule for storing NVIDIA wheel versions.\"\"\"\n+    versions_source = repository_ctx.attr.versions_source\n+\n+    versions_file_content = repository_ctx.read(\n+        repository_ctx.path(versions_source),\n+    )\n+    repository_ctx.file(\n+        \"versions.bzl\",\n+        \"NVIDIA_WHEEL_VERSIONS = '''%s'''\" % versions_file_content,\n+    )\n+    repository_ctx.file(\"BUILD\", \"\")\n+\n+nvidia_wheel_versions_repository = repository_rule(\n+    implementation = _nvidia_wheel_versions_repository_impl,\n+    attrs = {\n+        \"versions_source\": attr.label(mandatory = True, allow_single_file = True),\n+    },\n+)"
        },
        {
            "sha": "fd2c585b666f92ad3f52422d024e28bc6f49df66",
            "filename": "third_party/xla/third_party/py/setup_py_nvidia_dependencies_util.py",
            "status": "added",
            "additions": 76,
            "deletions": 0,
            "changes": 76,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fpy%2Fsetup_py_nvidia_dependencies_util.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fpy%2Fsetup_py_nvidia_dependencies_util.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fpy%2Fsetup_py_nvidia_dependencies_util.py?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -0,0 +1,76 @@\n+# Copyright 2025 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Utility function for updating setup.py with NVIDIA wheel versions.\n+\n+The content of the setup.py file is updated with the NVIDIA wheel versions\n+provided in the nvidia_wheel_versions_data string.\n+\n+The setup.py file is expected to have the following lines:\n+\n+```\n+# Mandatory placeholders\n+cuda_version = 0  # placeholder\n+cuda_wheel_suffix = ''  # placeholder\n+\n+# Optional placeholders (add only those that are needed)\n+nvidia_cublas_version = ''  # placeholder\n+\n+EXTRA_PACKAGES = {\n+    'and-cuda': [\n+        f'nvidia-cublas{cuda_wheel_suffix}{nvidia_cublas_version}',\n+        # add more wheels here\n+    ],\n+}\n+```\n+\"\"\"\n+\n+import re\n+\n+# Regex to capture wheel name and its version constraint\n+# Example: \"nvidia-cublas-cu12>=12.1.3.1 ; sys_platform == 'linux'\"\n+NVIDIA_WHEEL_VERSIONS_PATTERN = re.compile(r\"^([a-z0-9_-]+)(\\W*[0-9\\.]*.*)$\")\n+\n+\n+def get_setup_py_content_with_nvidia_wheel_versions(\n+    setup_py_content: str, cuda_version: str, nvidia_wheel_versions_data: str\n+) -> str:\n+  nvidia_wheel_versions = {\"12\": {}, \"13\": {}}\n+  for line in nvidia_wheel_versions_data.splitlines():\n+    match = NVIDIA_WHEEL_VERSIONS_PATTERN.match(line)\n+    if match:\n+      wheel_name = match.group(1).replace(\"-\", \"_\")\n+      for suffix, version in {\"_cu12\": \"12\", \"_cu13\": \"13\", \"\": \"13\"}.items():\n+        if not wheel_name.endswith(suffix):\n+          continue\n+        wheel_name = wheel_name.replace(suffix, \"\") + \"_version\"\n+        nvidia_wheel_versions[version][wheel_name] = match.group(2).strip()\n+        break\n+\n+  setup_py_content = setup_py_content.replace(\n+      \"cuda_version = 0  # placeholder\", f\"cuda_version = {cuda_version}\"\n+  )\n+  setup_py_content = setup_py_content.replace(\n+      \"cuda_wheel_suffix = ''  # placeholder\",\n+      \"cuda_wheel_suffix = '-cu12'\" if cuda_version == \"12\" else \"cuda_wheel_suffix = ''\",\n+  )\n+  for version_name, version_value in nvidia_wheel_versions[\n+      str(cuda_version)\n+  ].items():\n+    setup_py_content = setup_py_content.replace(\n+        f\"{version_name} = ''  # placeholder\",\n+        f\"{version_name} = '{version_value}'\",\n+    )\n+\n+  return setup_py_content"
        },
        {
            "sha": "e3a5e229e79df1c3329c602356b7693409e70f58",
            "filename": "third_party/xla/third_party/raft/cudart_utils.hpp.patch",
            "status": "added",
            "additions": 39,
            "deletions": 0,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fraft%2Fcudart_utils.hpp.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fraft%2Fcudart_utils.hpp.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fraft%2Fcudart_utils.hpp.patch?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -0,0 +1,39 @@\n+diff --git a/cpp/include/raft/util/cudart_utils.hpp b/cpp/include/raft/util/cudart_utils.hpp\n+--- a/cpp/include/raft/util/cudart_utils.hpp\n++++ b/cpp/include/raft/util/cudart_utils.hpp\n+@@ -21,6 +21,7 @@\n+ \n+ #include <rmm/cuda_stream_view.hpp>\n+ \n++#include <cuda_bf16.h>\n+ #include <cuda_fp16.h>\n+ #include <cuda_runtime_api.h>\n+ \n+@@ -456,4 +457,27 @@ constexpr inline auto upper_bound<half>(\n+   return static_cast<half>(__half_constexpr{0x7c00u});\n+ }\n+ \n++/**\n++ * This is a hack to allow constexpr definition of `bfloat16` constants.\n++ *\n++ * Same reasoning as for `half`: CUDAs `__nv_bfloat16` has no constexpr constructor.\n++ */\n++struct __bfloat16_constexpr : __nv_bfloat16 {  // NOLINT\n++  constexpr explicit inline __bfloat16_constexpr(uint16_t u) : __nv_bfloat16() { __x = u; }\n++};\n++\n++template <>\n++constexpr inline auto lower_bound<__nv_bfloat16>() -> __nv_bfloat16\n++{\n++  // Negative infinity in bfloat16 (sign=1, exp=all ones, mantissa=0)\n++  return static_cast<__nv_bfloat16>(__bfloat16_constexpr{0xff80u});\n++}\n++\n++template <>\n++constexpr inline auto upper_bound<__nv_bfloat16>() -> __nv_bfloat16\n++{\n++  // Positive infinity in bfloat16 (sign=0, exp=all ones, mantissa=0)\n++  return static_cast<__nv_bfloat16>(__bfloat16_constexpr{0x7f80u});\n++}\n++\n+ }  // namespace raft"
        },
        {
            "sha": "614d94dc657d1028d5008d9c4c50bd0521c0f949",
            "filename": "third_party/xla/third_party/raft/vectorized.cuh.patch",
            "status": "added",
            "additions": 45,
            "deletions": 0,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fraft%2Fvectorized.cuh.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fraft%2Fvectorized.cuh.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fraft%2Fvectorized.cuh.patch?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -0,0 +1,45 @@\n+diff --git a/cpp/include/raft/util/vectorized.cuh b/cpp/include/raft/util/vectorized.cuh\n+--- a/cpp/include/raft/util/vectorized.cuh\n++++ b/cpp/include/raft/util/vectorized.cuh\n+@@ -134,6 +134,22 @@ struct IOType<__half, 8> {\n+   typedef uint4 Type;\n+ };\n+ template <>\n++struct IOType<__nv_bfloat16, 1> {\n++  typedef __nv_bfloat16 Type;\n++};\n++template <>\n++struct IOType<__nv_bfloat16, 2> {\n++  typedef __nv_bfloat162 Type;\n++};\n++template <>\n++struct IOType<__nv_bfloat16, 4> {\n++  typedef uint2 Type;\n++};\n++template <>\n++struct IOType<__nv_bfloat16, 8> {\n++  typedef uint4 Type;\n++};\n++template <>\n+ struct IOType<__half2, 1> {\n+   typedef __half2 Type;\n+ };\n+@@ -146,6 +162,18 @@ struct IOType<__half2, 4> {\n+   typedef uint4 Type;\n+ };\n+ template <>\n++struct IOType<__nv_bfloat162, 1> {\n++  typedef __nv_bfloat162 Type;\n++};\n++template <>\n++struct IOType<__nv_bfloat162, 2> {\n++  typedef uint2 Type;\n++};\n++template <>\n++struct IOType<__nv_bfloat162, 4> {\n++  typedef uint4 Type;\n++};\n++template <>\n+ struct IOType<int32_t, 1> {\n+   typedef int32_t Type;\n+ };"
        },
        {
            "sha": "674c0a57083d57908b7f7bcf850b5999243b26bf",
            "filename": "third_party/xla/third_party/raft/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fraft%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fraft%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fraft%2Fworkspace.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -15,6 +15,8 @@ def repo():\n         urls = tf_mirror_urls(\"https://github.com/rapidsai/raft/archive/refs/tags/v{version}.tar.gz\".format(version = RAFT_VERSION)),\n         build_file = \"//third_party/raft:raft.BUILD\",\n         patch_file = [\n+            \"//third_party/raft:cudart_utils.hpp.patch\",\n+            \"//third_party/raft:vectorized.cuh.patch\",\n             \"//third_party/raft:logger_macros.hpp.patch\",\n             \"//third_party/raft:select_k_runner.hpp.patch\",\n             \"//third_party/raft:select_k_runner.cu.cc.patch\","
        },
        {
            "sha": "cb20a8090781d051ebeebe6fe549b5042885a9ba",
            "filename": "third_party/xla/third_party/shardy/temporary.patch",
            "status": "modified",
            "additions": 2368,
            "deletions": 4609,
            "changes": 6977,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fshardy%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fshardy%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fshardy%2Ftemporary.patch?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2"
        },
        {
            "sha": "7deed73ec55daf2dec95848944740c8c60daeee4",
            "filename": "third_party/xla/third_party/shardy/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fshardy%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fshardy%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fshardy%2Fworkspace.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -3,8 +3,8 @@\n load(\"//third_party:repo.bzl\", \"tf_http_archive\", \"tf_mirror_urls\")\n \n def repo():\n-    SHARDY_COMMIT = \"e99cfa73916a8758e35015d61767ba7e986fc79d\"\n-    SHARDY_SHA256 = \"973ba2b5c77337157e37ff331a111873ac6eddf9831555b3c18391e910673a6d\"\n+    SHARDY_COMMIT = \"0b8873d121008abc3edf7db2281f2b48cc647978\"\n+    SHARDY_SHA256 = \"cd1d0ebe479387adc4206257b8fe2853d14129ee294e7d9bebe4a3fc7670d7ca\"\n \n     tf_http_archive(\n         name = \"shardy\","
        },
        {
            "sha": "1fcd9fdceec0cc8264124681813d1f589f7d47e2",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 1496,
            "deletions": 0,
            "changes": 1496,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -4706,4 +4706,1500 @@ diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.c\n -\n -}  // namespace stablehlo\n -}  // namespace mlir\n+diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n+--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n++++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n+@@ -150,8 +150,8 @@\n+ ////////\n+ // CompareOp\n+ \n+-// CHECK-LABEL: func.func @compare_folds\n+-func.func @compare_folds()\n++// CHECK-LABEL: func.func @compare_fold_int\n++func.func @compare_fold_int()\n+   -> (tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>) {\n+   %cn1 = stablehlo.constant dense<-1> : tensor<i32>\n+   %c0 = stablehlo.constant dense<0> : tensor<i32>\n+@@ -176,6 +176,270 @@\n+          tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>\n+ }\n+ \n++// -----\n++\n++// CHECK-LABEL: func.func @compare_fold_float\n++func.func @compare_fold_float()\n++  -> (tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>) {\n++  %c0 = stablehlo.constant dense<0.0> : tensor<f32>\n++  %c1 = stablehlo.constant dense<0.01> : tensor<f32>\n++  %c2 = stablehlo.constant dense<-0.01> : tensor<f32>\n++  %c3 = stablehlo.constant dense<42.1> : tensor<f32>\n++  %c4 = stablehlo.constant dense<-50.0> : tensor<f32>\n++\n++  %0 = stablehlo.compare EQ, %c0, %c0, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %1 = stablehlo.compare EQ, %c1, %c2, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %2 = stablehlo.compare NE, %c0, %c0, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %3 = stablehlo.compare NE, %c1, %c2, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %4 = stablehlo.compare GT, %c3, %c3, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %5 = stablehlo.compare GT, %c3, %c4, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %6 = stablehlo.compare GE, %c3, %c3, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %7 = stablehlo.compare GE, %c3, %c4, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %8 = stablehlo.compare LT, %c2, %c2, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %9 = stablehlo.compare LT, %c2, %c4, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %10 = stablehlo.compare LE, %c2, %c2, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %11 = stablehlo.compare LE, %c2, %c4, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++\n++  // CHECK-DAG:  [[FALSE:%.+]] = stablehlo.constant dense<false> : tensor<i1>\n++  // CHECK-DAG:  [[TRUE:%.+]] = stablehlo.constant dense<true> : tensor<i1>\n++\n++  // CHECK-NEXT: return [[TRUE]], [[FALSE]], [[FALSE]], [[TRUE]], [[FALSE]], [[TRUE]], [[TRUE]], [[TRUE]], [[FALSE]], [[FALSE]], [[TRUE]], [[FALSE]]\n++  return %0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11 :\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>\n++}\n++\n++// -----\n++\n++// CHECK-LABEL: func.func @compare_fold_float_edge_cases\n++func.func @compare_fold_float_edge_cases()\n++  -> (tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>) {\n++  %zero = stablehlo.constant dense<0.0> : tensor<f32>\n++  %pos_inf = stablehlo.constant dense<0x7F800000> : tensor<f32>\n++  %neg_inf = stablehlo.constant dense<0xFF800000> : tensor<f32>\n++  %nan = stablehlo.constant dense<0x7FC00000> : tensor<f32>\n++\n++  // CHECK-DAG:  [[FALSE:%.+]] = stablehlo.constant dense<false> : tensor<i1>\n++  // CHECK-DAG:  [[TRUE:%.+]] = stablehlo.constant dense<true> : tensor<i1>\n++\n++  %0 = stablehlo.compare EQ, %zero, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %1 = stablehlo.compare EQ, %zero, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %2 = stablehlo.compare EQ, %zero, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %3 = stablehlo.compare EQ, %zero, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %4 = stablehlo.compare EQ, %pos_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %5 = stablehlo.compare EQ, %pos_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %6 = stablehlo.compare EQ, %pos_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %7 = stablehlo.compare EQ, %pos_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %8 = stablehlo.compare EQ, %neg_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %9 = stablehlo.compare EQ, %neg_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %10 = stablehlo.compare EQ, %neg_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %11 = stablehlo.compare EQ, %neg_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %12 = stablehlo.compare EQ, %nan, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %13 = stablehlo.compare EQ, %nan, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %14 = stablehlo.compare EQ, %nan, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %15 = stablehlo.compare EQ, %nan, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++\n++  %16 = stablehlo.compare NE, %zero, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %17 = stablehlo.compare NE, %zero, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %18 = stablehlo.compare NE, %zero, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %19 = stablehlo.compare NE, %zero, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %20 = stablehlo.compare NE, %pos_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %21 = stablehlo.compare NE, %pos_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %22 = stablehlo.compare NE, %pos_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %23 = stablehlo.compare NE, %pos_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %24 = stablehlo.compare NE, %neg_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %25 = stablehlo.compare NE, %neg_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %26 = stablehlo.compare NE, %neg_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %27 = stablehlo.compare NE, %neg_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %28 = stablehlo.compare NE, %nan, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %29 = stablehlo.compare NE, %nan, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %30 = stablehlo.compare NE, %nan, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %31 = stablehlo.compare NE, %nan, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++\n++  %32 = stablehlo.compare GT, %zero, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %33 = stablehlo.compare GT, %zero, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %34 = stablehlo.compare GT, %zero, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %35 = stablehlo.compare GT, %zero, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %36 = stablehlo.compare GT, %pos_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %37 = stablehlo.compare GT, %pos_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %38 = stablehlo.compare GT, %pos_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %39 = stablehlo.compare GT, %pos_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %40 = stablehlo.compare GT, %neg_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %41 = stablehlo.compare GT, %neg_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %42 = stablehlo.compare GT, %neg_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %43 = stablehlo.compare GT, %neg_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %44 = stablehlo.compare GT, %nan, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %45 = stablehlo.compare GT, %nan, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %46 = stablehlo.compare GT, %nan, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %47 = stablehlo.compare GT, %nan, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++\n++  %48 = stablehlo.compare GE, %zero, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %49 = stablehlo.compare GE, %zero, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %50 = stablehlo.compare GE, %zero, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %51 = stablehlo.compare GE, %zero, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %52 = stablehlo.compare GE, %pos_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %53 = stablehlo.compare GE, %pos_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %54 = stablehlo.compare GE, %pos_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %55 = stablehlo.compare GE, %pos_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %56 = stablehlo.compare GE, %neg_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %57 = stablehlo.compare GE, %neg_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %58 = stablehlo.compare GE, %neg_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %59 = stablehlo.compare GE, %neg_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %60 = stablehlo.compare GE, %nan, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %61 = stablehlo.compare GE, %nan, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %62 = stablehlo.compare GE, %nan, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %63 = stablehlo.compare GE, %nan, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++\n++  %64 = stablehlo.compare LT, %zero, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %65 = stablehlo.compare LT, %zero, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %66 = stablehlo.compare LT, %zero, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %67 = stablehlo.compare LT, %zero, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %68 = stablehlo.compare LT, %pos_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %69 = stablehlo.compare LT, %pos_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %70 = stablehlo.compare LT, %pos_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %71 = stablehlo.compare LT, %pos_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %72 = stablehlo.compare LT, %neg_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %73 = stablehlo.compare LT, %neg_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %74 = stablehlo.compare LT, %neg_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %75 = stablehlo.compare LT, %neg_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %76 = stablehlo.compare LT, %nan, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %77 = stablehlo.compare LT, %nan, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %78 = stablehlo.compare LT, %nan, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %79 = stablehlo.compare LT, %nan, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++\n++  %80 = stablehlo.compare LE, %zero, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %81 = stablehlo.compare LE, %zero, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %82 = stablehlo.compare LE, %zero, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %83 = stablehlo.compare LE, %zero, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %84 = stablehlo.compare LE, %pos_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %85 = stablehlo.compare LE, %pos_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %86 = stablehlo.compare LE, %pos_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %87 = stablehlo.compare LE, %pos_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %88 = stablehlo.compare LE, %neg_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %89 = stablehlo.compare LE, %neg_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %90 = stablehlo.compare LE, %neg_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %91 = stablehlo.compare LE, %neg_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %92 = stablehlo.compare LE, %nan, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %93 = stablehlo.compare LE, %nan, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %94 = stablehlo.compare LE, %nan, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %95 = stablehlo.compare LE, %nan, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++\n++  // CHECK: return [[TRUE]],  [[FALSE]], [[FALSE]], [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[TRUE]],  [[FALSE]], [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[TRUE]],  [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]],\n++\n++  // CHECK-SAME:   [[FALSE]], [[TRUE]],  [[TRUE]],  [[TRUE]],\n++  // CHECK-SAME:   [[TRUE]],  [[FALSE]], [[TRUE]],  [[TRUE]],\n++  // CHECK-SAME:   [[TRUE]],  [[TRUE]],  [[FALSE]], [[TRUE]],\n++  // CHECK-SAME:   [[TRUE]],  [[TRUE]],  [[TRUE]],  [[TRUE]],\n++\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[TRUE]],  [[FALSE]],\n++  // CHECK-SAME:   [[TRUE]],  [[FALSE]], [[TRUE]],  [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]],\n++\n++  // CHECK-SAME:   [[TRUE]],  [[FALSE]], [[TRUE]],  [[FALSE]],\n++  // CHECK-SAME:   [[TRUE]],  [[TRUE]],  [[TRUE]],  [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[TRUE]],  [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]],\n++\n++  // CHECK-SAME:   [[FALSE]], [[TRUE]],  [[FALSE]], [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]],\n++  // CHECK-SAME:   [[TRUE]],  [[TRUE]],  [[FALSE]], [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]],\n++\n++  // CHECK-SAME:   [[TRUE]],  [[TRUE]],  [[FALSE]], [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[TRUE]],  [[FALSE]], [[FALSE]],\n++  // CHECK-SAME:   [[TRUE]],  [[TRUE]],  [[TRUE]],  [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]]\n++\n++  return  %0,  %1,  %2,  %3,\n++          %4,  %5,  %6,  %7,\n++          %8,  %9, %10, %11,\n++         %12, %13, %14, %15,\n++\n++         %16, %17, %18, %19,\n++         %20, %21, %22, %23,\n++         %24, %25, %26, %27,\n++         %28, %29, %30, %31,\n++\n++         %32, %33, %34, %35,\n++         %36, %37, %38, %39,\n++         %40, %41, %42, %43,\n++         %44, %45, %46, %47,\n++\n++         %48, %49, %50, %51,\n++         %52, %53, %54, %55,\n++         %56, %57, %58, %59,\n++         %60, %61, %62, %63,\n++\n++         %64, %65, %66, %67,\n++         %68, %69, %70, %71,\n++         %72, %73, %74, %75,\n++         %76, %77, %78, %79,\n++\n++         %80, %81, %82, %83,\n++         %84, %85, %86, %87,\n++         %88, %89, %90, %91,\n++         %92, %93, %94, %95 :\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>\n++}\n+ \n+ // -----\n+ \n+@@ -218,8 +482,7 @@\n+   %cst_2 = stablehlo.constant dense<2.0> : tensor<f32>\n+   // CHECK: stablehlo.constant dense<1> : tensor<i32>\n+   // CHECK: stablehlo.constant dense<1> : tensor<ui32>\n+-  // CHECK: stablehlo.divide{{.*}} : tensor<f32>\n+-  // DISABLED-CHECK: stablehlo.constant dense<1.0{{.*}}> : tensor<f32>\n++  // CHECK: stablehlo.constant dense<1.0{{.*}}> : tensor<f32>\n+   %0 = stablehlo.divide %cst, %cst : tensor<i32>\n+   %1 = stablehlo.divide %cst_1, %cst_1 : tensor<ui32>\n+   %2 = stablehlo.divide %cst_2, %cst_2 : tensor<f32>\n+diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n+--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n++++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n+@@ -1633,17 +1633,17 @@\n+   %s4 = stablehlo.select %4, %arg1, %arg2 : (tensor<2xi1>, tensor<2xi32>, tensor<2xi32>) -> tensor<2xi32>\n+   %s5 = stablehlo.select %5, %arg1, %arg3 : (tensor<2xi1>, tensor<2xi32>, tensor<2xi32>) -> tensor<2xi32>\n+ \n+-  // CHECK-DAG:  [[C0:%.+]] = stablehlo.compare EQ, [[ARG0]], [[ARG1]], SIGNED\n+-  // CHECK-DAG:  [[C1:%.+]] = stablehlo.compare NE, [[ARG0]], [[ARG1]], SIGNED\n+-\n+-  // CHECK-DAG:  [[S0:%.+]] = stablehlo.select [[C0]], [[ARG0]], [[ARG1]]\n+-  // CHECK-DAG:  [[S1:%.+]] = stablehlo.select [[C1]], [[ARG0]], [[ARG1]]\n+-  // CHECK-DAG:  [[S2:%.+]] = stablehlo.maximum [[ARG0]], [[ARG1]]\n+-  // CHECK-DAG:  [[S3:%.+]] = stablehlo.maximum [[ARG0]], [[ARG2]]\n+-  // CHECK-DAG:  [[S4:%.+]] = stablehlo.minimum [[ARG1]], [[ARG2]]\n+-  // CHECK-DAG:  [[S5:%.+]] = stablehlo.minimum [[ARG1]], [[ARG3]]\n+-\n+-  // CHECK-NEXT: return [[S0]], [[S1]], [[S2]], [[S3]], [[S4]], [[S5]]\n++  // DISABLED-CHECK-DAG:  [[C0:%.+]] = stablehlo.compare EQ, [[ARG0]], [[ARG1]], SIGNED\n++  // DISABLED-CHECK-DAG:  [[C1:%.+]] = stablehlo.compare NE, [[ARG0]], [[ARG1]], SIGNED\n++\n++  // DISABLED-CHECK-DAG:  [[S0:%.+]] = stablehlo.select [[C0]], [[ARG0]], [[ARG1]]\n++  // DISABLED-CHECK-DAG:  [[S1:%.+]] = stablehlo.select [[C1]], [[ARG0]], [[ARG1]]\n++  // DISABLED-CHECK-DAG:  [[S2:%.+]] = stablehlo.maximum [[ARG0]], [[ARG1]]\n++  // DISABLED-CHECK-DAG:  [[S3:%.+]] = stablehlo.maximum [[ARG0]], [[ARG2]]\n++  // DISABLED-CHECK-DAG:  [[S4:%.+]] = stablehlo.minimum [[ARG1]], [[ARG2]]\n++  // DISABLED-CHECK-DAG:  [[S5:%.+]] = stablehlo.minimum [[ARG1]], [[ARG3]]\n++\n++  // DISABLED-CHECK-NEXT: return [[S0]], [[S1]], [[S2]], [[S3]], [[S4]], [[S5]]\n+   return %s0, %s1, %s2, %s3, %s4, %s5 :\n+          tensor<2xi32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>\n+ }\n+@@ -1674,23 +1674,23 @@\n+   %s6 = stablehlo.select %6, %arg3, %arg2 : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>\n+   %s7 = stablehlo.select %7, %arg2, %arg3 : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>\n+ \n+-  // CHECK-DAG:  [[C1:%.+]] = stablehlo.compare GT, [[ARG1]], [[ARG2]], SIGNED\n+-  // CHECK-DAG:  [[C3:%.+]] = stablehlo.compare GE, [[ARG1]], [[ARG2]], SIGNED\n+-\n+-  // CHECK-DAG:  [[S0:%.+]] = stablehlo.minimum [[ARG0]], [[ARG1]]\n+-  // CHECK-DAG:  [[S1:%.+]] = stablehlo.select [[C1]], [[ARG0]], [[ARG1]]\n+-  // CHECK-DAG:  [[S2:%.+]] = stablehlo.minimum [[ARG3]], [[ARG1]]\n+-  // CHECK-DAG:  [[S3:%.+]] = stablehlo.select [[C3]], [[ARG0]], [[ARG2]]\n+-\n+-  // CHECK-DAG:  [[C5:%.+]] = stablehlo.compare LT, [[ARG0]], [[ARG2]], SIGNED\n+-  // CHECK-DAG:  [[C7:%.+]] = stablehlo.compare LE, [[ARG0]], [[ARG2]], SIGNED\n+-\n+-  // CHECK-DAG:  [[S4:%.+]] = stablehlo.maximum [[ARG2]], [[ARG1]]\n+-  // CHECK-DAG:  [[S5:%.+]] = stablehlo.select [[C5]], [[ARG1]], [[ARG2]]\n+-  // CHECK-DAG:  [[S6:%.+]] = stablehlo.maximum [[ARG3]], [[ARG2]]\n+-  // CHECK-DAG:  [[S7:%.+]] = stablehlo.select [[C7]], [[ARG2]], [[ARG3]]\n+-\n+-  // CHECK-NEXT: return [[S0]], [[S1]], [[S2]], [[S3]], [[S4]], [[S5]], [[S6]], [[S7]]\n++  // DISABLED-CHECK-DAG:  [[C1:%.+]] = stablehlo.compare GT, [[ARG1]], [[ARG2]], SIGNED\n++  // DISABLED-CHECK-DAG:  [[C3:%.+]] = stablehlo.compare GE, [[ARG1]], [[ARG2]], SIGNED\n++\n++  // DISABLED-CHECK-DAG:  [[S0:%.+]] = stablehlo.minimum [[ARG0]], [[ARG1]]\n++  // DISABLED-CHECK-DAG:  [[S1:%.+]] = stablehlo.select [[C1]], [[ARG0]], [[ARG1]]\n++  // DISABLED-CHECK-DAG:  [[S2:%.+]] = stablehlo.minimum [[ARG3]], [[ARG1]]\n++  // DISABLED-CHECK-DAG:  [[S3:%.+]] = stablehlo.select [[C3]], [[ARG0]], [[ARG2]]\n++\n++  // DISABLED-CHECK-DAG:  [[C5:%.+]] = stablehlo.compare LT, [[ARG0]], [[ARG2]], SIGNED\n++  // DISABLED-CHECK-DAG:  [[C7:%.+]] = stablehlo.compare LE, [[ARG0]], [[ARG2]], SIGNED\n++\n++  // DISABLED-CHECK-DAG:  [[S4:%.+]] = stablehlo.maximum [[ARG2]], [[ARG1]]\n++  // DISABLED-CHECK-DAG:  [[S5:%.+]] = stablehlo.select [[C5]], [[ARG1]], [[ARG2]]\n++  // DISABLED-CHECK-DAG:  [[S6:%.+]] = stablehlo.maximum [[ARG3]], [[ARG2]]\n++  // DISABLED-CHECK-DAG:  [[S7:%.+]] = stablehlo.select [[C7]], [[ARG2]], [[ARG3]]\n++\n++  // DISABLED-CHECK-NEXT: return [[S0]], [[S1]], [[S2]], [[S3]], [[S4]], [[S5]], [[S6]], [[S7]]\n+   return %s0, %s1, %s2, %s3, %s4, %s5, %s6, %s7 : tensor<i32>, tensor<i32>, tensor<i32>, tensor<i32>,\n+                                                   tensor<i32>, tensor<i32>, tensor<i32>, tensor<i32>\n+ }\n+@@ -2040,12 +2040,12 @@\n+ \n+ // CHECK-LABEL: @push_shape_ops_to_end\n+ func.func @push_shape_ops_to_end(%arg0 : tensor<12xf32>) -> tensor<3x4x2x1xf32> {\n+-  // CHECK: %[[COS:.+]] = stablehlo.cosine %arg0 : tensor<12xf32>\n+-  // CHECK: %[[ABS:.+]] = stablehlo.abs %[[COS]] : tensor<12xf32>\n+-  // CHECK: %[[RESHAPE:.+]] = stablehlo.reshape %[[ABS]] : (tensor<12xf32>) -> tensor<3x4xf32>\n+-  // CHECK: %[[BROADCAST:.+]] = stablehlo.broadcast %[[RESHAPE]], sizes = [1, 2] : (tensor<3x4xf32>) -> tensor<1x2x3x4xf32>\n+-  // CHECK: %[[TRANSPOSE:.+]] = stablehlo.transpose %[[BROADCAST]], dims = [2, 3, 1, 0] : (tensor<1x2x3x4xf32>) -> tensor<3x4x2x1xf32>\n+-  // CHECK: return %[[TRANSPOSE]]\n++  // DISABLED-CHECK: %[[COS:.+]] = stablehlo.cosine %arg0 : tensor<12xf32>\n++  // DISABLED-CHECK: %[[ABS:.+]] = stablehlo.abs %[[COS]] : tensor<12xf32>\n++  // DISABLED-CHECK: %[[RESHAPE:.+]] = stablehlo.reshape %[[ABS]] : (tensor<12xf32>) -> tensor<3x4xf32>\n++  // DISABLED-CHECK: %[[BROADCAST:.+]] = stablehlo.broadcast %[[RESHAPE]], sizes = [1, 2] : (tensor<3x4xf32>) -> tensor<1x2x3x4xf32>\n++  // DISABLED-CHECK: %[[TRANSPOSE:.+]] = stablehlo.transpose %[[BROADCAST]], dims = [2, 3, 1, 0] : (tensor<1x2x3x4xf32>) -> tensor<3x4x2x1xf32>\n++  // DISABLED-CHECK: return %[[TRANSPOSE]]\n+   %0 = stablehlo.reshape %arg0 : (tensor<12xf32>) -> tensor<3x4xf32>\n+   %1 = stablehlo.broadcast %0, sizes = [1, 2] : (tensor<3x4xf32>) -> tensor<1x2x3x4xf32>\n+   %2 = stablehlo.cosine %1 : (tensor<1x2x3x4xf32>) -> tensor<1x2x3x4xf32>\n+@@ -2059,9 +2059,9 @@\n+ \n+ // CHECK-LABEL: @reorder_with_type_change\n+ func.func @reorder_with_type_change(%arg0 : tensor<3x4xi32>) -> tensor<12xi64> {\n+-  // CHECK: %[[CONVERT:.+]] = stablehlo.convert %arg0 : (tensor<3x4xi32>) -> tensor<3x4xi64>\n+-  // CHECK: %[[RESHAPE:.+]] = stablehlo.reshape %[[CONVERT]] : (tensor<3x4xi64>) -> tensor<12xi64>\n+-  // CHECK: return %[[RESHAPE]]\n++  // DISABLED-CHECK: %[[CONVERT:.+]] = stablehlo.convert %arg0 : (tensor<3x4xi32>) -> tensor<3x4xi64>\n++  // DISABLED-CHECK: %[[RESHAPE:.+]] = stablehlo.reshape %[[CONVERT]] : (tensor<3x4xi64>) -> tensor<12xi64>\n++  // DISABLED-CHECK: return %[[RESHAPE]]\n+   %0 = stablehlo.reshape %arg0 : (tensor<3x4xi32>) -> tensor<12xi32>\n+   %1 = stablehlo.convert %0 : (tensor<12xi32>) -> tensor<12xi64>\n+   return %1 : tensor<12xi64>\n+diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir\n+--- stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir\n++++ stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir\n+@@ -59,3 +59,34 @@\n+   }\n+   return %0 : tensor<i64>\n+ }\n++\n++// -----\n++\n++// Check that we properly handle expressions involving NaN terms or variables\n++// that could potentially be NaN.\n++\n++// CHECK-LABEL: @fold_constant_nan_to_nan\n++func.func @fold_constant_nan_to_nan() -> tensor<f32> {\n++  // CHECK: [[NAN:%.*]] = stablehlo.constant dense<0x7FC00000> : tensor<f32>\n++  // CHECK: return [[NAN]] : tensor<f32>\n++  %zero = stablehlo.constant dense<0.0> : tensor<f32>\n++  %one = stablehlo.constant dense<1.0> : tensor<f32>\n++  %nan = stablehlo.constant dense<0x7FC00000> : tensor<f32>\n++  %nan_times_zero = stablehlo.multiply %nan, %zero : tensor<f32>\n++  %result = stablehlo.add %one, %nan_times_zero : tensor<f32>\n++  return %result : tensor<f32>\n++}\n++\n++// TODO: Consider adding an `--assume-non-nan` pass option to override this.\n++// CHECK-LABEL: @do_not_assume_non_nan\n++func.func @do_not_assume_non_nan(%arg0: tensor<f32>) -> tensor<f32> {\n++  // Note: These two checks are out of order on purpose: [[RESULT]] binds to the\n++  // `return` op first and then looks backward for the corresponding assignment.\n++  // CHECK-DAG: return [[RESULT:.*]] : tensor<f32>\n++  // CHECK-DAG: [[RESULT]] = stablehlo.{{(add|multiply).*}} : tensor<f32>\n++  %zero = stablehlo.constant dense<0.0> : tensor<f32>\n++  %one = stablehlo.constant dense<1.0> : tensor<f32>\n++  %arg_times_zero = stablehlo.multiply %arg0, %zero : tensor<f32>\n++  %result = stablehlo.add %one, %arg_times_zero : tensor<f32>\n++  return %result : tensor<f32>\n++}\n+diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n+--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n++++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n+@@ -23,6 +23,7 @@\n+ #include <string>\n+ #include <utility>\n+ \n++#include \"llvm/ADT/APFloat.h\"\n+ #include \"llvm/ADT/APInt.h\"\n+ #include \"llvm/ADT/APSInt.h\"\n+ #include \"llvm/ADT/FloatingPointMode.h\"\n+@@ -86,6 +87,18 @@\n+       /*isUnsigned=*/isUnsigned);\n+ }\n+ \n++APFloat getAPFloat(\n++    Type type, double value,\n++    llvm::RoundingMode roundingMode = llvm::RoundingMode::NearestTiesToEven) {\n++  auto floatType = dyn_cast<FloatType>(type);\n++  if (!floatType) llvm::report_fatal_error(\"expected float type\");\n++\n++  APFloat result(value);\n++  bool losesInfo = false;\n++  result.convert(floatType.getFloatSemantics(), roundingMode, &losesInfo);\n++  return result;\n++}\n++\n+ LogicalResult validateStaticShapeResult(PatternRewriter& rewriter,\n+                                         Operation* op, ShapedType resultType) {\n+   if (!resultType.hasStaticShape())\n+@@ -94,26 +107,30 @@\n+   return success();\n+ }\n+ \n+-template <typename Fn>\n+-static TypedAttr foldUnaryOpIntOrFloat(Type resultType, TypedAttr operand,\n+-                                       Fn&& folder) {\n++/// Unary constant folder that uses a generic folder function to handle both\n++/// ints and floats.\n++template <typename Fn, typename IntResultType = IntegerAttr,\n++          typename FloatResultType = FloatAttr>\n++TypedAttr foldUnaryOpIntOrFloat(Type resultType, TypedAttr operand,\n++                                Fn&& folder) {\n+   Type elemTy = getElementTypeOrSelf(operand);\n+ \n+   Attribute res;\n+   if (isa<IntegerType>(elemTy))\n+-    res = constFoldUnaryOp<IntegerAttr, IntegerAttr::ValueType, void>(operand,\n+-                                                                      folder);\n++    res = constFoldUnaryOp<IntegerAttr, IntegerAttr::ValueType, void,\n++                           IntResultType>(operand, folder);\n+   if (isa<FloatType>(elemTy))\n+-    res = constFoldUnaryOp<FloatAttr, FloatAttr::ValueType, void>(operand,\n+-                                                                  folder);\n++    res = constFoldUnaryOp<FloatAttr, FloatAttr::ValueType, void,\n++                           FloatResultType>(operand, folder);\n+   if (res) return cast<TypedAttr>(res);\n+ \n+   return nullptr;\n+ }\n+ \n+-/// Binary constant folder that used a generic folder function to handle both\n++/// Unary constant folder that uses a generic folder function to handle both\n+ /// ints and floats.\n+-template <typename Fn>\n++template <typename Fn, typename IntResultType = IntegerAttr,\n++          typename FloatResultType = FloatAttr>\n+ FailureOr<TypedAttr> foldUnaryOpIntOrFloat(PatternRewriter& rewriter,\n+                                            Operation* op, Fn&& folder) {\n+   if (op->getNumOperands() != 1 || op->getNumResults() != 1)\n+@@ -124,35 +141,38 @@\n+ \n+   if (!attr) return rewriter.notifyMatchFailure(op, \"operand not constants\");\n+ \n+-  TypedAttr res = foldUnaryOpIntOrFloat(op->getResultTypes()[0], attr, folder);\n++  TypedAttr res = foldUnaryOpIntOrFloat<Fn, IntResultType, FloatResultType>(\n++      op->getResultTypes()[0], attr, std::forward<Fn>(folder));\n+   if (!res) return rewriter.notifyMatchFailure(op, \"folding failed\");\n+ \n+   return res;\n+ }\n+ \n+-/// Binary constant folder that used a generic folder function to handle both\n++/// Binary constant folder that uses a generic folder function to handle both\n+ /// ints and floats.\n+-template <typename Fn>\n+-static TypedAttr foldBinaryOpIntOrFloat(Type resultType, TypedAttr lhs,\n+-                                        TypedAttr rhs, Fn&& folder) {\n++template <typename Fn, typename IntResultType = IntegerAttr,\n++          typename FloatResultType = FloatAttr>\n++TypedAttr foldBinaryOpIntOrFloat(Type resultType, TypedAttr lhs, TypedAttr rhs,\n++                                 Fn&& folder) {\n+   Attribute operands[2] = {lhs, rhs};\n+   Type elemTy = getElementTypeOrSelf(lhs);\n+ \n+   Attribute res;\n+   if (isa<IntegerType>(elemTy))\n+-    res = constFoldBinaryOp<IntegerAttr, IntegerAttr::ValueType, void>(\n+-        operands, resultType, folder);\n++    res = constFoldBinaryOp<IntegerAttr, IntegerAttr::ValueType, void,\n++                            IntResultType>(operands, resultType, folder);\n+   if (isa<FloatType>(elemTy))\n+-    res = constFoldBinaryOp<FloatAttr, FloatAttr::ValueType, void>(\n+-        operands, resultType, folder);\n++    res = constFoldBinaryOp<FloatAttr, FloatAttr::ValueType, void,\n++                            FloatResultType>(operands, resultType, folder);\n+   if (res) return cast<TypedAttr>(res);\n+ \n+   return nullptr;\n+ }\n+ \n+-/// Binary constant folder that used a generic folder function to handle both\n++/// Binary constant folder that uses a generic folder function to handle both\n+ /// ints and floats.\n+-template <typename Fn>\n++template <typename Fn, typename IntResultType = IntegerAttr,\n++          typename FloatResultType = FloatAttr>\n+ FailureOr<TypedAttr> foldBinaryOpIntOrFloat(PatternRewriter& rewriter,\n+                                             Operation* op, Fn&& folder) {\n+   if (op->getNumOperands() != 2 || op->getNumResults() != 1)\n+@@ -165,8 +185,8 @@\n+   if (!lhsAttr || !rhsAttr)\n+     return rewriter.notifyMatchFailure(op, \"lhs & rhs operands not constants\");\n+ \n+-  TypedAttr res =\n+-      foldBinaryOpIntOrFloat(op->getResultTypes()[0], lhsAttr, rhsAttr, folder);\n++  TypedAttr res = foldBinaryOpIntOrFloat<Fn, IntResultType, FloatResultType>(\n++      op->getResultTypes()[0], lhsAttr, rhsAttr, std::forward<Fn>(folder));\n+   if (!res) return rewriter.notifyMatchFailure(op, \"folding failed\");\n+ \n+   return res;\n+@@ -371,23 +391,38 @@\n+ struct FoldAndOpPattern : public ShapeOpRewritePattern<AndOp> {\n+   using ShapeOpRewritePattern::ShapeOpRewritePattern;\n+ \n+-  LogicalResult matchAndRewrite(mlir::stablehlo::AndOp op,\n+-                                PatternRewriter& rewriter) const override {\n+-    // TODO: Support more int types\n++  LogicalResult matchAndRewrite(AndOp op,\n++                                PatternRewriter& rewriter) const override {\n+     auto resultType = op.getType();\n+-    if (!resultType.getElementType().isInteger(1))\n+-      return rewriter.notifyMatchFailure(op, \"expected boolean element type\");\n+-\n+-    auto res = foldBinaryOpIntOrFloat(rewriter, op, FoldAnd{});\n+-    if (failed(res)) return failure();\n+-    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());\n+-    return success();\n+-  }\n+-\n+-  struct FoldAnd {\n++    auto resultElementType = resultType.getElementType();\n++    FailureOr<TypedAttr> result;\n++\n++    if (resultElementType.isInteger(/*width=*/1)) {\n++      result = foldBinaryOpIntOrFloat(rewriter, op, FoldLogicalAnd{});\n++    } else if (resultElementType.isInteger()) {\n++      result = foldBinaryOpIntOrFloat(rewriter, op, FoldBitwiseAnd{});\n++    } else {\n++      return rewriter.notifyMatchFailure(op, \"Expected integral element type.\");\n++    }\n++\n++    if (failed(result)) return failure();\n++    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op,\n++                                                             result.value());\n++    return success();\n++  }\n++\n++  struct FoldLogicalAnd {\n+     APInt operator()(APInt lhs, APInt rhs) const {\n+       return APInt(lhs.getBitWidth(), !lhs.isZero() && !rhs.isZero());\n+     }\n++    std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) const {\n++      return std::nullopt;\n++    }\n++  };\n++\n++  struct FoldBitwiseAnd {\n++    APInt operator()(APInt lhs, APInt rhs) const { return lhs & rhs; }\n++\n+     std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) const {\n+       return std::nullopt;\n+     }\n+@@ -426,7 +461,7 @@\n+     if (failed(validateShapeFoldDtype(rewriter, op, resultType)))\n+       return failure();\n+ \n+-    auto res = foldBinaryOpIntOrFloat(\n++    auto res = foldBinaryOpIntOrFloat<FoldCompare, IntegerAttr, IntegerAttr>(\n+         rewriter, op,\n+         FoldCompare(op.getComparisonDirection(), op.getCompareType()));\n+     if (failed(res)) return failure();\n+@@ -441,9 +476,29 @@\n+     ComparisonDirection direction;\n+     std::optional<ComparisonType> kind;\n+ \n+-    // TODO: Enable float folding.\n+-    std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {\n+-      return std::nullopt;\n++    APInt operator()(APFloat lhs, APFloat rhs) {\n++      bool result = false;\n++      switch (direction) {\n++        case ComparisonDirection::EQ:\n++          result = lhs == rhs;\n++          break;\n++        case ComparisonDirection::NE:\n++          result = lhs != rhs;\n++          break;\n++        case ComparisonDirection::GE:\n++          result = lhs >= rhs;\n++          break;\n++        case ComparisonDirection::GT:\n++          result = lhs > rhs;\n++          break;\n++        case ComparisonDirection::LE:\n++          result = lhs <= rhs;\n++          break;\n++        case ComparisonDirection::LT:\n++          result = lhs < rhs;\n++          break;\n++      }\n++      return APInt(/*bitwidth=*/1, result);\n+     }\n+     APInt operator()(APInt lhs, APInt rhs) {\n+       bool result = false;\n+@@ -509,6 +564,20 @@\n+     Operation* terminator = blockToInline->getTerminator();\n+     ValueRange results = terminator->getOperands();\n+ \n++    // TODO: Add support for complex, quantized, and token return types.\n++    // Currently, this pattern only supports int and float return types. We'll\n++    // need a more general equivalent of `getZeroAttr` to support other types.\n++    SmallVector<TypedAttr> placeholderAttrs;\n++    for (auto result : op.getResults()) {\n++      TypedAttr placeholderAttr = rewriter.getZeroAttr(result.getType());\n++      if (!placeholderAttr)\n++        return rewriter.notifyMatchFailure(\n++            op,\n++            \"The case op's return type isn't currently supported by this \"\n++            \"optimization pattern.\");\n++      placeholderAttrs.push_back(placeholderAttr);\n++    }\n++\n+     // Inline the active branch of the `case` op.\n+     rewriter.inlineBlockBefore(blockToInline, op, blockArgs);\n+     rewriter.replaceAllOpUsesWith(op, results);\n+@@ -521,9 +590,9 @@\n+     Block& noopBlock = region.emplaceBlock();\n+     SmallVector<Value> placeholderResults;\n+     rewriter.setInsertionPointToEnd(&noopBlock);\n+-    for (auto result : op.getResults()) {\n+-      placeholderResults.push_back(rewriter.create<ConstantOp>(\n+-          region.getLoc(), rewriter.getZeroAttr(result.getType())));\n++    for (auto placeholderAttr : placeholderAttrs) {\n++      placeholderResults.push_back(\n++          rewriter.create<ConstantOp>(region.getLoc(), placeholderAttr));\n+     }\n+     rewriter.create<stablehlo::ReturnOp>(region.getLoc(), placeholderResults);\n+ \n+@@ -628,10 +697,7 @@\n+         : foldIntFn(isUnsignedInt ? foldUint : foldSint) {}\n+     std::function<APInt(APInt, APInt)> foldIntFn;\n+ \n+-    // TODO: Enable float folding.\n+-    std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {\n+-      return std::nullopt;  // return lhs / rhs;\n+-    }\n++    APFloat operator()(APFloat lhs, APFloat rhs) { return lhs / rhs; }\n+     APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }\n+     static APInt foldUint(APInt lhs, APInt rhs) { return lhs.udiv(rhs); }\n+     static APInt foldSint(APInt lhs, APInt rhs) { return lhs.sdiv(rhs); }\n+@@ -669,9 +735,8 @@\n+       : foldIntFn(isUnsignedInt ? foldUint : foldSint) {}\n+   std::function<APInt(APInt, APInt)> foldIntFn;\n+ \n+-  // TODO: Enable float folding.\n+-  std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {\n+-    return std::nullopt;  // return lhs >= rhs ? lhs : rhs;\n++  APFloat operator()(APFloat lhs, APFloat rhs) {\n++    return lhs >= rhs ? lhs : rhs;\n+   }\n+   APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }\n+   static APInt foldUint(APInt lhs, APInt rhs) {\n+@@ -687,9 +752,8 @@\n+       : foldIntFn(isUnsignedInt ? foldUint : foldSint) {}\n+   std::function<APInt(APInt, APInt)> foldIntFn;\n+ \n+-  // TODO: Enable float folding.\n+-  std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {\n+-    return std::nullopt;  // return lhs <= rhs ? lhs : rhs;\n++  APFloat operator()(APFloat lhs, APFloat rhs) {\n++    return lhs <= rhs ? lhs : rhs;\n+   }\n+   APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }\n+   static APInt foldUint(APInt lhs, APInt rhs) {\n+@@ -706,11 +770,14 @@\n+   LogicalResult matchAndRewrite(MaxOp op,\n+                                 PatternRewriter& rewriter) const override {\n+     auto resultType = op.getType();\n++    auto resultElementType = resultType.getElementType();\n+     if (failed(validateShapeFoldDtype(rewriter, op, resultType)))\n+       return failure();\n+ \n+-    bool isUnsignedInt = resultType.getElementType().isUnsignedInteger();\n+-    auto res = foldBinaryOpIntOrFloat(rewriter, op, FoldMax(isUnsignedInt));\n++    bool isUnsignedIntOrBool = resultElementType.isUnsignedInteger() ||\n++                               resultElementType.isInteger(/*width=*/1);\n++    auto res =\n++        foldBinaryOpIntOrFloat(rewriter, op, FoldMax(isUnsignedIntOrBool));\n+     if (failed(res)) return failure();\n+     rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());\n+     return success();\n+@@ -723,11 +790,14 @@\n+   LogicalResult matchAndRewrite(MinOp op,\n+                                 PatternRewriter& rewriter) const override {\n+     auto resultType = op.getType();\n++    auto resultElementType = resultType.getElementType();\n+     if (failed(validateShapeFoldDtype(rewriter, op, resultType)))\n+       return failure();\n+ \n+-    bool isUnsignedInt = resultType.getElementType().isUnsignedInteger();\n+-    auto res = foldBinaryOpIntOrFloat(rewriter, op, FoldMin(isUnsignedInt));\n++    bool isUnsignedIntOrBool = resultElementType.isUnsignedInteger() ||\n++                               resultElementType.isInteger(/*width=*/1);\n++    auto res =\n++        foldBinaryOpIntOrFloat(rewriter, op, FoldMin(isUnsignedIntOrBool));\n+     if (failed(res)) return failure();\n+     rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());\n+     return success();\n+@@ -786,21 +856,36 @@\n+ \n+   LogicalResult matchAndRewrite(OrOp op,\n+                                 PatternRewriter& rewriter) const override {\n+-    // TODO: Support more int types\n+     auto resultType = op.getType();\n+-    if (!resultType.getElementType().isInteger(1))\n+-      return rewriter.notifyMatchFailure(op, \"expected boolean element type\");\n+-\n+-    auto res = foldBinaryOpIntOrFloat(rewriter, op, FoldOr{});\n+-    if (failed(res)) return failure();\n+-    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());\n+-    return success();\n+-  }\n+-\n+-  struct FoldOr {\n++    auto resultElementType = resultType.getElementType();\n++    FailureOr<TypedAttr> result;\n++\n++    if (resultElementType.isInteger(/*width=*/1)) {\n++      result = foldBinaryOpIntOrFloat(rewriter, op, FoldLogicalOr{});\n++    } else if (resultElementType.isInteger()) {\n++      result = foldBinaryOpIntOrFloat(rewriter, op, FoldBitwiseOr{});\n++    } else {\n++      return rewriter.notifyMatchFailure(op, \"Expected integral element type.\");\n++    }\n++\n++    if (failed(result)) return failure();\n++    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op,\n++                                                             result.value());\n++    return success();\n++  }\n++\n++  struct FoldLogicalOr {\n+     APInt operator()(APInt lhs, APInt rhs) const {\n+       return APInt(lhs.getBitWidth(), !lhs.isZero() || !rhs.isZero());\n+     }\n++    std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) const {\n++      return std::nullopt;\n++    }\n++  };\n++\n++  struct FoldBitwiseOr {\n++    APInt operator()(APInt lhs, APInt rhs) const { return lhs | rhs; }\n++\n+     std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) const {\n+       return std::nullopt;\n+     }\n+@@ -828,9 +913,12 @@\n+         : foldIntFn(isUnsignedInt ? foldUint : foldSint) {}\n+     std::function<APInt(APInt, APInt)> foldIntFn;\n+ \n+-    // TODO: Enable float folding.\n+     std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {\n+-      return std::nullopt;  // return lhs.remainder(rhs);\n++      // `APFloat::mod` requires both operands to have identical semantics.\n++      if (&lhs.getSemantics() != &rhs.getSemantics()) return std::nullopt;\n++\n++      lhs.mod(rhs);  // This modifies `lhs` in place.\n++      return lhs;    // `lhs` now holds the result.\n+     }\n+     APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }\n+     static APInt foldUint(APInt lhs, APInt rhs) { return lhs.urem(rhs); }\n+@@ -963,8 +1051,16 @@\n+   struct FoldSign {\n+     FoldSign(Type elementType) : elementType(elementType) {}\n+     Type elementType;\n+-    // TODO: Enable float folding.\n+-    std::optional<APFloat> operator()(APFloat operand) { return std::nullopt; }\n++    double result;\n++    APFloat operator()(APFloat operand) {\n++      if (operand.isNegative())\n++        result = -1.0;\n++      else if (operand.isZero())\n++        result = 0.0;\n++      else\n++        result = 1.0;\n++      return getAPFloat(elementType, result);\n++    }\n+ \n+     APInt operator()(APInt operand) {\n+       // SignOp only supports signed integers.\n+@@ -1220,13 +1316,9 @@\n+ \n+     for (auto [inputValue, bodyArg] :\n+          llvm::zip_equal(op.getOperands(), body.getArguments())) {\n+-      auto inputConstantOp = inputValue.getDefiningOp<ConstantOp>();\n+-      if (!inputConstantOp)\n+-        return rewriter.notifyMatchFailure(op, \"Input must be a constant.\");\n+-\n+-      auto inputConstantAttr =\n+-          dyn_cast_or_null<DenseElementsAttr>(inputConstantOp.getValue());\n+-      if (!inputConstantAttr)\n++      SplatElementsAttr constantSplatAttr;\n++      if (!matchPattern(inputValue, m_Constant(&constantSplatAttr)) ||\n++          !constantSplatAttr)\n+         return rewriter.notifyMatchFailure(op,\n+                                            \"Input must be a splat constant.\");\n+ \n+@@ -1236,7 +1328,7 @@\n+             op, \"Could not get the shape of the body argument.\");\n+ \n+       bodyArgConstantAttrs.push_back(DenseElementsAttr::get(\n+-          bodyArgShapedType, inputConstantAttr.getSplatValue<Attribute>()));\n++          bodyArgShapedType, constantSplatAttr.getSplatValue<Attribute>()));\n+     }\n+ \n+     for (BlockArgument bodyArg : body.getArguments()) {\n+diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n+--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n++++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n+@@ -64,7 +64,7 @@\n+ static constexpr StablehloAggressiveSimplificationPassOptions kDefaultOptions;\n+ \n+ static bool isIotaRange(ArrayRef<int64_t> dims) {\n+-  return llvm::all_of(llvm::enumerate(dims), [](const auto &it) {\n++  return llvm::all_of(llvm::enumerate(dims), [](const auto& it) {\n+     return static_cast<int64_t>(it.index()) == it.value();\n+   });\n+ }\n+@@ -72,20 +72,20 @@\n+ template <typename OpType>\n+ struct SimplifyOpRewritePattern : OpRewritePattern<OpType> {\n+   SimplifyOpRewritePattern(\n+-      MLIRContext *context,\n+-      const StablehloAggressiveSimplificationPassOptions &options,\n++      MLIRContext* context,\n++      const StablehloAggressiveSimplificationPassOptions& options,\n+       PatternBenefit benefit = 1, ArrayRef<StringRef> generatedNames = {})\n+       : OpRewritePattern<OpType>(context, benefit, generatedNames),\n+         options(options) {}\n+ \n+   // Prevent `options` from binding to a temporary.\n+   SimplifyOpRewritePattern(\n+-      MLIRContext *context,\n+-      StablehloAggressiveSimplificationPassOptions &&options,\n++      MLIRContext* context,\n++      StablehloAggressiveSimplificationPassOptions&& options,\n+       PatternBenefit benefit = 1,\n+       ArrayRef<StringRef> generatedNames = {}) = delete;\n+ \n+-  const StablehloAggressiveSimplificationPassOptions &options;\n++  const StablehloAggressiveSimplificationPassOptions& options;\n+ };\n+ \n+ /// Matches when either of the submatchers match.\n+@@ -93,7 +93,7 @@\n+ struct m_AnyOf {\n+   m_AnyOf(MatcherA a, MatcherB b) : matcherA(a), matcherB(b) {}\n+ \n+-  bool match(Operation *op) { return matcherA.match(op) || matcherB.match(op); }\n++  bool match(Operation* op) { return matcherA.match(op) || matcherB.match(op); }\n+ \n+   MatcherA matcherA;\n+   MatcherB matcherB;\n+@@ -146,7 +146,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(CompareOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     RankedTensorType type = op.getType();\n+ \n+     // Bail out on non-integer comparison.\n+@@ -211,7 +211,7 @@\n+  public:\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+   LogicalResult matchAndRewrite(ConcatenateOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     if (op.getInputs().size() != 1 ||\n+         op.getInputs().front().getType() != op.getType())\n+       return rewriter.notifyMatchFailure(op, \"not single operand noop-concat\");\n+@@ -227,7 +227,7 @@\n+  public:\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+   LogicalResult matchAndRewrite(ConcatenateOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     auto axis = op.getDimension();\n+     llvm::SmallVector<Value> newOperands = llvm::to_vector(\n+         llvm::make_filter_range(op.getOperands(), [&](Value operand) {\n+@@ -249,8 +249,8 @@\n+ class ConcatenateOpFlatten : public SimplifyOpRewritePattern<ConcatenateOp> {\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+   LogicalResult matchAndRewrite(ConcatenateOp op,\n+-                                PatternRewriter &rewriter) const override {\n+-    auto getFlattenedOperands = [&](const Value &val) -> ValueRange {\n++                                PatternRewriter& rewriter) const override {\n++    auto getFlattenedOperands = [&](const Value& val) -> ValueRange {\n+       auto definingOp = dyn_cast_or_null<ConcatenateOp>(val.getDefiningOp());\n+       // To avoid inflate the memory footprint, only flatten the\n+       // ConcatenateOp when it has only one use.\n+@@ -293,7 +293,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(CustomCallOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     constexpr StringRef kMhloBackendConfigAttrName = \"mhlo.backend_config\";\n+ \n+     if (op.getApiVersion() != CustomCallApiVersion::API_VERSION_ORIGINAL)\n+@@ -327,7 +327,7 @@\n+ /////////////////////////////////\n+ \n+ // Used in DRR file.\n+-DenseI64ArrayAttr getMergedBroadcastDimensions(OpBuilder &b,\n++DenseI64ArrayAttr getMergedBroadcastDimensions(OpBuilder& b,\n+                                                ArrayRef<int64_t> dims,\n+                                                ArrayRef<int64_t> dimsParent) {\n+   auto mergedDims = llvm::map_to_vector(\n+@@ -350,8 +350,8 @@\n+ /// the op is used outside of the HLO dialect (e.g. in func.return). In these\n+ /// cases, we insert a stablehlo.convert to smooth things out.\n+ template <typename OpTy, typename... Args>\n+-static OpTy refineOpWithNewOp(PatternRewriter &rewriter, Operation *op,\n+-                              Args &&...args) {\n++static OpTy refineOpWithNewOp(PatternRewriter& rewriter, Operation* op,\n++                              Args&&... args) {\n+   auto newOp = rewriter.create<OpTy>(op->getLoc(), std::forward<Args>(args)...);\n+ \n+   llvm::SmallVector<Value> replacementResults;\n+@@ -360,7 +360,7 @@\n+   for (auto [opResult, newOpResult] :\n+        llvm::zip(op->getResults(), newOp->getResults())) {\n+     Value replacementResult = newOpResult;\n+-    if (llvm::any_of(opResult.getUsers(), [&](Operation *user) {\n++    if (llvm::any_of(opResult.getUsers(), [&](Operation* user) {\n+           return user->getDialect() != op->getDialect();\n+         }))\n+       replacementResult = rewriter.create<ConvertOp>(\n+@@ -379,7 +379,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(DynamicBroadcastInDimOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     RankedTensorType operandType = op.getOperand().getType();\n+     if (!operandType.hasStaticShape())\n+       return rewriter.notifyMatchFailure(op, \"requires operand static shape\");\n+@@ -410,7 +410,7 @@\n+ // DynamicGatherOp\n+ /////////////////////////////////\n+ \n+-DenseI64ArrayAttr convertToI64Array(OpBuilder &b, Attribute attr) {\n++DenseI64ArrayAttr convertToI64Array(OpBuilder& b, Attribute attr) {\n+   auto denseAttr = cast<ElementsAttr>(attr);\n+   SmallVector<int64_t> result;\n+   result.reserve(denseAttr.getNumElements());\n+@@ -427,7 +427,7 @@\n+   using SimplifyOpRewritePattern<DynamicIotaOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(DynamicIotaOp iota,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     // Result type has static shape, replace with iota.\n+     auto resultTy = cast<ShapedType>(iota.getType());\n+     if (!resultTy.hasStaticShape())\n+@@ -447,7 +447,7 @@\n+   using SimplifyOpRewritePattern<DynamicIotaOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(DynamicIotaOp iota,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     auto resultType = cast<ShapedType>(iota.getType());\n+     if (resultType.getRank() < 2)\n+       return rewriter.notifyMatchFailure(iota, \"requires rank >= 2\");\n+@@ -496,7 +496,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(DynamicReshapeOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     // This is a noop when the output type is already a static shape.\n+     RankedTensorType type = op.getType();\n+     if (!type.hasStaticShape())\n+@@ -516,8 +516,8 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(DynamicReshapeOp op,\n+-                                PatternRewriter &rewriter) const override {\n+-    Operation *defOp = op.getOperand().getDefiningOp();\n++                                PatternRewriter& rewriter) const override {\n++    Operation* defOp = op.getOperand().getDefiningOp();\n+     if (!defOp ||\n+         !defOp->hasTrait<mlir::OpTrait::SameOperandsAndResultShape>()) {\n+       return rewriter.notifyMatchFailure(\n+@@ -549,7 +549,7 @@\n+   using SimplifyOpRewritePattern<DynamicSliceOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(DynamicSliceOp dynamicSlice,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     Value input = dynamicSlice.getOperand();\n+     auto inputType = cast<ShapedType>(input.getType());\n+     if (!inputType.hasStaticShape())\n+@@ -558,7 +558,7 @@\n+ \n+     auto sliceSizes = dynamicSlice.getSliceSizes();\n+     SmallVector<int64_t, 4> tempStartIndices;\n+-    for (const auto &indexAndSliceStart :\n++    for (const auto& indexAndSliceStart :\n+          llvm::enumerate(dynamicSlice.getStartIndices())) {\n+       APInt val;\n+       Value start = indexAndSliceStart.value();\n+@@ -579,7 +579,7 @@\n+     // pack them into a single tensor.\n+     auto sliceStartIndices = rewriter.getDenseI64ArrayAttr(tempStartIndices);\n+     SmallVector<int64_t, 4> tempSliceLimits;\n+-    for (const auto &[start, size] : llvm::zip(tempStartIndices, sliceSizes)) {\n++    for (const auto& [start, size] : llvm::zip(tempStartIndices, sliceSizes)) {\n+       tempSliceLimits.push_back(start + size);\n+     }\n+     auto sliceLimits = rewriter.getDenseI64ArrayAttr(tempSliceLimits);\n+@@ -605,7 +605,7 @@\n+   using SimplifyOpRewritePattern<RealDynamicSliceOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(RealDynamicSliceOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     // This rewrite only works for unit strides because DynamicSliceOp\n+     // doesn't support strides (i.e. it implicitly has unit strides).\n+     DenseIntElementsAttr stridesAttr;\n+@@ -670,11 +670,11 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(ReduceOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     // If all returned values in the ReduceOp region exists outside the\n+     // region, replace the ReduceOp with those values.\n+     if (auto retOp = dyn_cast<ReturnOp>(op.getBody().front().getTerminator())) {\n+-      Region *retRegion = retOp->getParentRegion();\n++      Region* retRegion = retOp->getParentRegion();\n+       if (llvm::any_of(retOp.getResults(), [retRegion](Value result) {\n+             return result.getParentRegion() == retRegion;\n+           }))\n+@@ -693,7 +693,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(ReduceOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     // We require all reduce shapes to be the same, up to the element types, so\n+     // we can just use the first operand and the first result as\n+     // representatives.\n+@@ -733,7 +733,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(ReduceOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     SmallVector<OpResult, 4> usedResults;\n+     llvm::copy_if(op.getResults(), std::back_inserter(usedResults),\n+                   [](OpResult result) { return !result.use_empty(); });\n+@@ -745,7 +745,7 @@\n+     const auto numOperands = op.getNumOperands();\n+     const auto numOperandPairs = numOperands / pairSize;\n+ \n+-    Block &reducerBlock = op.getBody().front();\n++    Block& reducerBlock = op.getBody().front();\n+     auto retOp = cast<ReturnOp>(reducerBlock.getTerminator());\n+ \n+     assert(numOperandPairs == op.getNumResults() &&\n+@@ -757,10 +757,10 @@\n+       if (v.getParentRegion() == reducerBody) workList.push_back(v);\n+     };\n+ \n+-    SmallPtrSet<Operation *, 16> usedOps;\n++    SmallPtrSet<Operation*, 16> usedOps;\n+     SmallBitVector usedArgs(numOperands);\n+     SmallBitVector usedReturnOperands(numOperandPairs);\n+-    for (const auto &usedResult : usedResults) {\n++    for (const auto& usedResult : usedResults) {\n+       auto resultNo = usedResult.getResultNumber();\n+       usedReturnOperands.set(resultNo);\n+ \n+@@ -774,9 +774,9 @@\n+           const auto pairNo = blockArg.getArgNumber() % numOperandPairs;\n+           usedArgs.set(pairNo);\n+           usedArgs.set(pairNo + numOperandPairs);\n+-        } else if (auto *defOp = definition.getDefiningOp()) {\n++        } else if (auto* defOp = definition.getDefiningOp()) {\n+           usedOps.insert(defOp);\n+-          for (const auto &operand : defOp->getOperands())\n++          for (const auto& operand : defOp->getOperands())\n+             addToWorkList(operand);\n+         }\n+       }\n+@@ -785,7 +785,7 @@\n+     const auto newNumOperandPairs = usedResults.size();\n+     const auto newNumOperands = newNumOperandPairs * pairSize;\n+     if (newNumOperands != usedArgs.count()) {\n+-      return rewriter.notifyMatchFailure(op, [&](Diagnostic &diag) {\n++      return rewriter.notifyMatchFailure(op, [&](Diagnostic& diag) {\n+         diag << \"non-conservative case: \" << newNumOperandPairs\n+              << \" return results should be matched with \" << newNumOperands\n+              << \" operands, but got \" << usedArgs.count();\n+@@ -809,7 +809,7 @@\n+     auto newOp =\n+         rewriter.create<ReduceOp>(op.getLoc(), newInputs, newInitVals,\n+                                   op.getDimensionsAttr(), newElementTypes);\n+-    Block *newReducerBlock = rewriter.createBlock(&newOp.getBody());\n++    Block* newReducerBlock = rewriter.createBlock(&newOp.getBody());\n+ \n+     IRMapping mapper;\n+     for (auto arg : reducerBlock.getArguments())\n+@@ -818,11 +818,11 @@\n+                    newReducerBlock->addArgument(arg.getType(), arg.getLoc()));\n+ \n+     rewriter.setInsertionPointToStart(newReducerBlock);\n+-    for (Operation &op : reducerBlock.getOperations())\n++    for (Operation& op : reducerBlock.getOperations())\n+       if (usedOps.contains(&op)) rewriter.clone(op, mapper);\n+ \n+     SmallVector<Value> newReturnOperands;\n+-    for (const auto &en : llvm::enumerate(retOp.getOperands()))\n++    for (const auto& en : llvm::enumerate(retOp.getOperands()))\n+       if (usedReturnOperands[en.index()])\n+         newReturnOperands.push_back(mapper.lookup(en.value()));\n+ \n+@@ -830,7 +830,7 @@\n+ \n+     // Build new results list (unused entries will be null).\n+     SmallVector<Value> newResults(op.getNumResults());\n+-    for (const auto &[i, result] : llvm::enumerate(usedResults)) {\n++    for (const auto& [i, result] : llvm::enumerate(usedResults)) {\n+       newResults[result.getResultNumber()] = newOp.getResult(i);\n+     }\n+ \n+@@ -851,7 +851,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(GetDimensionSizeOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     // Fold get_dimension_size when the queried dim is statically known.\n+     RankedTensorType operandTy = op.getOperand().getType();\n+ \n+@@ -877,7 +877,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(GatherOp gather,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     DenseIntElementsAttr index;\n+     if (!matchPattern(gather.getStartIndices(), m_Constant(&index)))\n+       return failure();\n+@@ -952,7 +952,7 @@\n+   using SimplifyOpRewritePattern<IotaOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(IotaOp iota,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     auto resultTy = cast<ShapedType>(iota.getType());\n+     if (resultTy.getRank() < 2)\n+       return rewriter.notifyMatchFailure(iota, \"itoa not broadcastable\");\n+@@ -989,7 +989,7 @@\n+   using SimplifyOpRewritePattern<PadOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(PadOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     auto operand = op.getOperand();\n+     auto padVal = op.getPaddingValue();\n+ \n+@@ -1028,7 +1028,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(SelectOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     RankedTensorType type = op.getType();\n+ \n+     Value trueVal = op.getOnTrue();\n+@@ -1079,7 +1079,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(SelectOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     Value pred = op.getPred();\n+     Value trueVal = op.getOnTrue();\n+     Value falseVal = op.getOnFalse();\n+@@ -1133,7 +1133,7 @@\n+   using SimplifyOpRewritePattern<SliceOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(SliceOp slice,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     auto resultTy = cast<ShapedType>(slice.getType());\n+     if (!resultTy.hasStaticShape())\n+       return rewriter.notifyMatchFailure(slice, \"result shape not static\");\n+@@ -1228,12 +1228,12 @@\n+   using SimplifyOpRewritePattern<SortOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(SortOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     DenseSet<unsigned> erasedArgs;\n+     unsigned numOperands = op.getNumOperands();\n+     for (unsigned i = 0; i < numOperands; ++i) {\n+       if (!op.getResult(i).use_empty()) continue;\n+-      Block &block = op.getComparator().front();\n++      Block& block = op.getComparator().front();\n+       if (!block.getArgument(i * 2).use_empty()) continue;\n+       if (!block.getArgument(i * 2 + 1).use_empty()) continue;\n+       erasedArgs.insert(i);\n+@@ -1242,7 +1242,7 @@\n+ \n+     SmallVector<Value> newOperands;\n+     BitVector erasedBlockArgs(op.getNumOperands() * 2);\n+-    for (const auto &en : llvm::enumerate(op.getInputs())) {\n++    for (const auto& en : llvm::enumerate(op.getInputs())) {\n+       if (erasedArgs.contains(en.index())) {\n+         erasedBlockArgs.set(en.index() * 2);\n+         erasedBlockArgs.set(en.index() * 2 + 1);\n+@@ -1253,7 +1253,7 @@\n+ \n+     auto newOp = rewriter.create<SortOp>(op.getLoc(), newOperands,\n+                                          op.getDimension(), op.getIsStable());\n+-    Region &region = newOp.getComparator();\n++    Region& region = newOp.getComparator();\n+     rewriter.inlineRegionBefore(op.getComparator(), region, region.end());\n+     region.front().eraseArguments(erasedBlockArgs);\n+ \n+@@ -1278,7 +1278,7 @@\n+   using SimplifyOpRewritePattern<SortOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(SortOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     if (op.getResults().empty() ||\n+         static_cast<int64_t>(op.getDimension()) != -1)\n+       return rewriter.notifyMatchFailure(op,\n+@@ -1304,7 +1304,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(TransposeOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     auto input = op.getOperand();\n+     auto permutation = op.getPermutation();\n+ \n+@@ -1340,7 +1340,7 @@\n+   using SimplifyOpRewritePattern<TupleOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(TupleOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     if (op.getVal().empty())\n+       return rewriter.notifyMatchFailure(op, \"empty tuple\");\n+ \n+@@ -1356,7 +1356,7 @@\n+           op, \"tuple predecessor type does not match\");\n+ \n+     // Check that this is a repacking of the parent tuple.\n+-    for (const auto &elementAndIdx : llvm::enumerate(op.getVal())) {\n++    for (const auto& elementAndIdx : llvm::enumerate(op.getVal())) {\n+       auto elementOp = elementAndIdx.value().getDefiningOp<GetTupleElementOp>();\n+       if (!elementOp ||\n+           elementOp.getIndexAttr().getInt() !=\n+@@ -1385,9 +1385,9 @@\n+   using SimplifyOpRewritePattern<WhileOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(WhileOp whileOp,\n+-                                PatternRewriter &rewriter) const override {\n+-    Block *cond = whileOp.SingleBlock::getBody(0);\n+-    Block *body = whileOp.SingleBlock::getBody(1);\n++                                PatternRewriter& rewriter) const override {\n++    Block* cond = whileOp.SingleBlock::getBody(0);\n++    Block* body = whileOp.SingleBlock::getBody(1);\n+     auto bodyReturnOp = cast<ReturnOp>(body->getTerminator());\n+     if (!llvm::any_of(llvm::zip(whileOp->getOperands(), body->getArguments(),\n+                                 bodyReturnOp->getOperands()),\n+@@ -1400,10 +1400,10 @@\n+     SmallVector<Value> newOperands, resultsToReplace;\n+     SmallVector<unsigned> invariantArgIdxs;\n+     BitVector invariantArgIdxBitVector(cond->getNumArguments());\n+-    for (const auto &enumeratedOperands : llvm::enumerate(llvm::zip(\n++    for (const auto& enumeratedOperands : llvm::enumerate(llvm::zip(\n+              whileOp.getOperands(), cond->getArguments(), body->getArguments(),\n+              bodyReturnOp->getOperands(), whileOp->getResults()))) {\n+-      const auto &operands = enumeratedOperands.value();\n++      const auto& operands = enumeratedOperands.value();\n+       Value whileOperand = std::get<0>(operands);\n+       BlockArgument condBlockArg = std::get<1>(operands);\n+       BlockArgument bodyBlockArg = std::get<2>(operands);\n+@@ -1455,14 +1455,14 @@\n+ // Pattern: op(X : zero_extent_tensor) -> constant([])\n+ struct ZeroExtentToEmptyConstant final : RewritePattern {\n+   explicit ZeroExtentToEmptyConstant(\n+-      MLIRContext *context,\n++      MLIRContext* context,\n+       StablehloAggressiveSimplificationPassOptions options,\n+       PatternBenefit benefit = 1)\n+       : RewritePattern(MatchAnyOpTypeTag(), benefit, context),\n+         options(options) {}\n+ \n+-  LogicalResult matchAndRewrite(Operation *op,\n+-                                PatternRewriter &rewriter) const override {\n++  LogicalResult matchAndRewrite(Operation* op,\n++                                PatternRewriter& rewriter) const override {\n+     auto loc = op->getLoc();\n+ \n+     if (!isa_and_present<StablehloDialect>(op->getDialect()))\n+@@ -1492,10 +1492,10 @@\n+ \n+     // If one of the operands is a zero-extent tensor, replace the operand with\n+     // an empty tensor.\n+-    for (OpOperand &operand : op->getOpOperands()) {\n++    for (OpOperand& operand : op->getOpOperands()) {\n+       auto operandType = getMaybeZeroExtentType(operand.get().getType());\n+       if (!operandType || operand.get().getDefiningOp<ConstantOp>()) continue;\n+-      Operation *owner = operand.getOwner();\n++      Operation* owner = operand.getOwner();\n+       int operandNum = operand.getOperandNumber();\n+       auto emptyConstantOp = rewriter.create<ConstantOp>(\n+           loc, operandType.value(),\n+@@ -1514,13 +1514,13 @@\n+ struct ReorderElementwiseAndShapeOp final\n+     : OpTraitRewritePattern<OpTrait::Elementwise> {\n+   explicit ReorderElementwiseAndShapeOp(\n+-      MLIRContext *context,\n++      MLIRContext* context,\n+       StablehloAggressiveSimplificationPassOptions options,\n+       PatternBenefit benefit = 1)\n+       : OpTraitRewritePattern(context, benefit), options(options) {}\n+ \n+-  LogicalResult matchAndRewrite(Operation *op,\n+-                                PatternRewriter &rewriter) const override {\n++  LogicalResult matchAndRewrite(Operation* op,\n++                                PatternRewriter& rewriter) const override {\n+     if (op->getOperands().size() != 1)\n+       return rewriter.notifyMatchFailure(op, \"expected to be unary\");\n+ \n+@@ -1575,7 +1575,7 @@\n+       : StablehloAggressiveSimplificationPassBase() {}\n+ \n+   void runOnOperation() override {\n+-    MLIRContext *context = &getContext();\n++    MLIRContext* context = &getContext();\n+     RewritePatternSet patterns(context);\n+ \n+     StablehloAggressiveSimplificationPassOptions options{\n+@@ -1597,12 +1597,13 @@\n+ }  // namespace\n+ \n+ void populateStablehloCanonicalizationPatterns(\n+-    MLIRContext *context, RewritePatternSet *patterns,\n+-    const StablehloAggressiveSimplificationPassOptions &options,\n++    MLIRContext* context, RewritePatternSet* patterns,\n++    const StablehloAggressiveSimplificationPassOptions& options,\n+     PatternBenefit benefit) {\n+   populateWithGenerated(*patterns);\n++  // TODO: Re-enable `CompareSelectIntoMinMax` after fixing legalization issue.\n+   patterns->add<\n+-      CompareOpCanon, CompareSelectIntoMinMax, ConcatenateOpFlatten,\n++      CompareOpCanon, /*CompareSelectIntoMinMax,*/ ConcatenateOpFlatten,\n+       ConcatenateOpNoop, ConcatenateOpRemoveEmpty,\n+       CustomCallUnregisteredBackendConfigToFfi, DynamicIotaOpToBroadcast,\n+       DynamicReshapeOpSameOperandAndResultShape, DynamicSliceOpToSlice,\n+@@ -1614,8 +1615,11 @@\n+       context, options, benefit);\n+ \n+   // Generic patterns\n+-  patterns->add<ReorderElementwiseAndShapeOp, ZeroExtentToEmptyConstant>(\n+-      context, options, benefit);\n++  // TODO: Re-enable `ReorderElementwiseAndShapeOp` after fixing BF16 precision\n++  // emulation issue in XLA-CPU.\n++  patterns->add<\n++      /*ReorderElementwiseAndShapeOp,*/\n++      ZeroExtentToEmptyConstant>(context, options, benefit);\n+ \n+   // TODO: Dynamism Refinements, consider merging with canonicalize dynamism\n+   patterns\n+@@ -1623,22 +1627,22 @@\n+             DynamicReshapeOpIsStatic, DynamicIotaIsStatic>(context, options);\n+ }\n+ \n+-void populateStablehloCanonicalizationPatterns(MLIRContext *context,\n+-                                               RewritePatternSet *patterns,\n++void populateStablehloCanonicalizationPatterns(MLIRContext* context,\n++                                               RewritePatternSet* patterns,\n+                                                PatternBenefit benefit) {\n+   populateStablehloCanonicalizationPatterns(context, patterns, kDefaultOptions,\n+                                             benefit);\n+ }\n+ \n+ void populateStablehloHloImportCanonicalizationPatterns(\n+-    MLIRContext *context, RewritePatternSet *patterns,\n+-    const StablehloAggressiveSimplificationPassOptions &options) {\n++    MLIRContext* context, RewritePatternSet* patterns,\n++    const StablehloAggressiveSimplificationPassOptions& options) {\n+   patterns->add<ReshapeOp_RemoveNoop, GetTupleElementOp_UnpackTuple>(context);\n+   patterns->add<TupleIsRepacking, WhileOpImplicitCapture>(context, options);\n+ }\n+ \n+ void populateStablehloHloImportCanonicalizationPatterns(\n+-    MLIRContext *context, RewritePatternSet *patterns) {\n++    MLIRContext* context, RewritePatternSet* patterns) {\n+   populateStablehloHloImportCanonicalizationPatterns(context, patterns,\n+                                                      kDefaultOptions);\n+ }\n+diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n+--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n++++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n+@@ -234,7 +234,7 @@\n+ //\n+ // No-op, but wrap in ConvertOp to preserve dynamic output shape. This can be\n+ // important if the result is returned, in which case refining the type would\n+-// require also updating the funciton signature.\n++// require also updating the function signature.\n+ def DynamicBroadcastInDimOp_ReplaceNoopWithConvert\n+   : Pat<(StableHLO_DynamicBroadcastInDimOp:$op\n+             $operand, $shape, IotaDims:$dims, $expanding, $nonexpanding),\n+@@ -387,9 +387,10 @@\n+ \n+ // Pattern: multiply(X, 0i) -> 0i\n+ //\n+-// Multiplication by 0. This fold is not trivial for floats in presence of NaNs.\n++// Multiplication by 0. This fold is not trivial for floats in presence of NaNs,\n++// so we currently only enable it for ints.\n+ def MulOp_FoldToZero\n+-  : Pat<(StableHLO_MulOp $lhs, (StableHLO_ConstantOp:$zero AnyZero:$value)),\n++  : Pat<(StableHLO_MulOp $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n+         (replaceWithValue $zero)>;\n+ \n+ // Pattern: multiply(X, 1i) -> X\n "
        },
        {
            "sha": "6863f76f3a931e91bcae2648a907e7c4a9a04786",
            "filename": "third_party/xla/third_party/triton/llvm_integration/cl776164071.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 101,
            "changes": 101,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl776164071.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl776164071.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl776164071.patch?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -1,101 +0,0 @@\n-\n---- a/include/triton/Conversion/TritonGPUToLLVM/PatternTritonGPUOpToLLVM.h\t2025-03-25 07:48:50.000000000 -0700\n-+++ b/include/triton/Conversion/TritonGPUToLLVM/PatternTritonGPUOpToLLVM.h\t2025-06-26 09:20:47.000000000 -0700\n-@@ -95,7 +95,8 @@\n- void populateFuncOpConversionPattern(LLVMTypeConverter &typeConverter,\n-                                      RewritePatternSet &patterns,\n-                                      const TargetInfoBase &targetInfo,\n--                                     PatternBenefit benefit);\n-+                                     PatternBenefit benefit,\n-+                                     SymbolTableCollection *symbolTables);\n- \n- void populatePrintOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n-                                   RewritePatternSet &patterns,\n-\n---- a/lib/Conversion/TritonGPUToLLVM/FuncOpToLLVM.cpp\t2025-04-25 05:19:43.000000000 -0700\n-+++ b/lib/Conversion/TritonGPUToLLVM/FuncOpToLLVM.cpp\t2025-06-26 09:20:48.000000000 -0700\n-@@ -7,7 +7,8 @@\n- FailureOr<LLVM::LLVMFuncOp>\n- convertFuncOpToLLVMFuncOp(FunctionOpInterface funcOp,\n-                           ConversionPatternRewriter &rewriter,\n--                          const LLVMTypeConverter &converter);\n-+                          const LLVMTypeConverter &converter,\n-+                          SymbolTableCollection *symbolTables);\n- }\n- \n- namespace {\n-@@ -33,8 +34,10 @@\n- /// information.\n- struct FuncOpConversion : public ConvertOpToLLVMPattern<triton::FuncOp> {\n-   FuncOpConversion(LLVMTypeConverter &converter,\n--                   const TargetInfoBase &targetInfo, PatternBenefit benefit)\n--      : ConvertOpToLLVMPattern(converter, benefit), targetInfo(targetInfo) {}\n-+                   const TargetInfoBase &targetInfo, PatternBenefit benefit,\n-+                   SymbolTableCollection *symbolTables)\n-+      : ConvertOpToLLVMPattern(converter, benefit), targetInfo(targetInfo),\n-+        symbolTables(symbolTables) {}\n- \n-   /// Only retain those attributes that are not constructed by\n-   /// `LLVMFuncOp::build`. If `filterArgAttrs` is set, also filter out argument\n-@@ -152,7 +155,7 @@\n- \n-     FailureOr<LLVM::LLVMFuncOp> maybeNewFuncOp =\n-         mlir::convertFuncOpToLLVMFuncOp(amendedFuncOp, rewriter,\n--                                        *getTypeConverter());\n-+                                        *getTypeConverter(), symbolTables);\n-     if (failed(maybeNewFuncOp)) {\n-       return failure();\n-     }\n-@@ -202,12 +205,16 @@\n- \n- private:\n-   const TargetInfoBase &targetInfo;\n-+  // Store a pointer to the single, pass-wide symbol table\n-+  SymbolTableCollection *symbolTables;\n- };\n- \n- } // namespace\n- \n- void mlir::triton::populateFuncOpConversionPattern(\n-     LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n--    const TargetInfoBase &targetInfo, PatternBenefit benefit) {\n--  patterns.add<FuncOpConversion>(typeConverter, targetInfo, benefit);\n-+    const TargetInfoBase &targetInfo, PatternBenefit benefit,\n-+    SymbolTableCollection *symbolTables) {\n-+  patterns.add<FuncOpConversion>(typeConverter, targetInfo, benefit,\n-+                                 symbolTables);\n- }\n-\n---- a/third_party/amd/lib/TritonAMDGPUToLLVM/TritonGPUToLLVM.cpp\t2025-06-02 05:51:09.000000000 -0700\n-+++ b/third_party/amd/lib/TritonAMDGPUToLLVM/TritonGPUToLLVM.cpp\t2025-06-26 09:20:48.000000000 -0700\n-@@ -110,7 +110,8 @@\n-       TritonLLVMFunctionConversionTarget funcTarget(*context);\n-       RewritePatternSet funcPatterns(context);\n-       mlir::triton::populateFuncOpConversionPattern(\n--          typeConverter, funcPatterns, targetInfo, patternBenefitDefault);\n-+          typeConverter, funcPatterns, targetInfo, patternBenefitDefault,\n-+          /*symTable=*/nullptr);\n-       mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,\n-                                                             funcPatterns);\n-       if (failed(\n-\n---- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TritonGPUToLLVM.cpp\t2025-04-11 01:29:32.000000000 -0700\n-+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TritonGPUToLLVM.cpp\t2025-06-26 09:20:48.000000000 -0700\n-@@ -79,6 +79,7 @@\n-   void runOnOperation() override {\n-     MLIRContext *context = &getContext();\n-     ModuleOp mod = getOperation();\n-+\n-     TargetInfo targetInfo(computeCapability, ptxVersion);\n- \n-     // Allocate shared memory and set barrier\n-@@ -94,7 +95,8 @@\n-     TritonLLVMFunctionConversionTarget funcTarget(*context);\n-     RewritePatternSet funcPatterns(context);\n-     mlir::triton::populateFuncOpConversionPattern(\n--        typeConverter, funcPatterns, targetInfo, patternBenefitDefault);\n-+        typeConverter, funcPatterns, targetInfo, patternBenefitDefault,\n-+        /*symTable=*/nullptr);\n-     mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,\n-                                                           funcPatterns);\n-     if (failed("
        },
        {
            "sha": "5d6498d902ffb325445409382be204a60b3a4de2",
            "filename": "third_party/xla/third_party/triton/llvm_integration/cl789494309.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl789494309.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl789494309.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl789494309.patch?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -1,20 +0,0 @@\n-\n---- a/third_party/amd/include/TritonAMDGPUToLLVM/Passes.h\t2025-06-02 05:51:09.000000000 -0700\n-+++ b/third_party/amd/include/TritonAMDGPUToLLVM/Passes.h\t2025-07-31 15:53:03.000000000 -0700\n-@@ -1,13 +1,14 @@\n- #ifndef TRITON_THIRD_PARTY_AMD_INCLUDE_TRITONAMDGPUTOLLVM_PASSES_H_\n- #define TRITON_THIRD_PARTY_AMD_INCLUDE_TRITONAMDGPUTOLLVM_PASSES_H_\n- \n-+#include <memory>\n-+\n-+#include \"llvm/IR/Function.h\"\n- #include \"mlir/Conversion/LLVMCommon/TypeConverter.h\"\n- #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n- #include \"mlir/Pass/Pass.h\"\n- #include \"mlir/Transforms/DialectConversion.h\"\n- \n--#include <memory>\n--\n- namespace mlir {\n- \n- class ModuleOp;"
        },
        {
            "sha": "409aeb6a0ae232a9ae87ea7edd2f82c023df5ca6",
            "filename": "third_party/xla/third_party/triton/llvm_integration/cl791659411.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl791659411.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl791659411.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl791659411.patch?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -1,12 +0,0 @@\n-diff --git a/BUILD b/BUILD\n-index 246fe7e5f..6dba44973 100644\n---- a/BUILD\n-+++ b/BUILD\n-@@ -951,6 +951,7 @@ cc_library(\n-         \":triton_conversion_triton_to_triton_gpu_passes_inc_gen\",\n-         \":triton_nvidia_gpu_transforms_inc_gen\",\n-         \"@llvm-project//mlir:AllPassesAndDialects\",\n-+        \"@llvm-project//mlir:RegisterAllPasses\",\n-         \"@triton//test:TritonTestAnalysis\",\n-         \"@triton//test:TritonTestDialect\",\n-         \"@triton//third_party/amd:TritonAMDGPU\","
        },
        {
            "sha": "560286b054bafaf64b45ad158beecc15a61a5c34",
            "filename": "third_party/xla/third_party/triton/llvm_integration/cl793679540.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 95,
            "changes": 95,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl793679540.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl793679540.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl793679540.patch?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -1,95 +0,0 @@\n-\n---- a/test/Conversion/tritongpu_to_llvm_hopper.mlir\t2025-07-31 00:13:23.000000000 -0700\n-+++ b/test/Conversion/tritongpu_to_llvm_hopper.mlir\t2025-08-11 09:50:34.000000000 -0700\n-@@ -285,7 +285,7 @@\n- // CHECK-LABEL: distribute_to_shared_st_matrix_local_store\n- module attributes {\"ttg.target\" = \"cuda:90\", \"ttg.num-ctas\" = 1 : i32, \"ttg.num-warps\" = 4 : i32, \"ttg.threads-per-warp\" = 32 : i32} {\n-   tt.func @distribute_to_shared_st_matrix_local_store(%a: tensor<64x128xf16, #linear>) {\n--    // CHECK-COUNT-8: nvvm.stmatrix %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}} {layout = #nvvm.mma_layout<col>}\n-+    // CHECK-COUNT-8: nvvm.stmatrix %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}} {eltType = #nvvm.ld_st_matrix_elt_type<b16>, layout = #nvvm.mma_layout<col>, shape = #nvvm.ld_st_matrix_shape<m = 8, n = 8>}\n-     //          CHECK: llvm.return\n-     %b = ttg.local_alloc {allocation.offset = 0 : i32} : () -> !ttg.memdesc<64x128xf16, #shared, #smem, mutable>\n-     ttg.local_store %a, %b : tensor<64x128xf16, #linear> -> !ttg.memdesc<64x128xf16, #shared, #smem, mutable>\n-@@ -317,7 +317,7 @@\n- // CHECK-LABEL: linear_to_swizzled_st_matrix_local_store\n- module attributes {\"ttg.target\" = \"cuda:90\", \"ttg.num-ctas\" = 1 : i32, \"ttg.num-warps\" = 4 : i32, \"ttg.threads-per-warp\" = 32 : i32} {\n-   tt.func @linear_to_swizzled_st_matrix_local_store(%a: tensor<64x32xf16, #linear>) {\n--    // CHECK-COUNT-2: nvvm.stmatrix %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}} {layout = #nvvm.mma_layout<row>}\n-+    // CHECK-COUNT-2: nvvm.stmatrix %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}} {eltType = #nvvm.ld_st_matrix_elt_type<b16>, layout = #nvvm.mma_layout<row>, shape = #nvvm.ld_st_matrix_shape<m = 8, n = 8>}\n-     //          CHECK: llvm.return\n-     %b = ttg.local_alloc {allocation.offset = 0 : i32} : () -> !ttg.memdesc<64x32xf16, #shared, #smem, mutable>\n-     ttg.local_store %a, %b : tensor<64x32xf16, #linear> -> !ttg.memdesc<64x32xf16, #shared, #smem, mutable>\n-@@ -339,7 +339,7 @@\n- // CHECK-LABEL: linear_to_swizzled_st_matrix_local_store\n- module attributes {\"ttg.target\" = \"cuda:90\", \"ttg.num-ctas\" = 1 : i32, \"ttg.num-warps\" = 4 : i32, \"ttg.threads-per-warp\" = 32 : i32} {\n-   tt.func @linear_to_swizzled_st_matrix_local_store(%a: tensor<32x32xf16, #linear>) {\n--    // CHECK-COUNT-2: nvvm.stmatrix %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}} {layout = #nvvm.mma_layout<row>}\n-+    // CHECK-COUNT-2: nvvm.stmatrix %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}} {eltType = #nvvm.ld_st_matrix_elt_type<b16>, layout = #nvvm.mma_layout<row>, shape = #nvvm.ld_st_matrix_shape<m = 8, n = 8>}\n-     //          CHECK: llvm.return\n-     %b = ttg.local_alloc {allocation.offset = 0 : i32} : () -> !ttg.memdesc<32x32xf16, #shared, #smem, mutable>\n-     ttg.local_store %a, %b : tensor<32x32xf16, #linear> -> !ttg.memdesc<32x32xf16, #shared, #smem, mutable>\n-@@ -355,7 +355,7 @@\n- // CHECK-LABEL: linear_to_swizzled_st_matrix_x2_local_store_fp8\n- module attributes {\"ttg.target\" = \"cuda:90\", \"ttg.num-ctas\" = 1 : i32, \"ttg.num-warps\" = 4 : i32, \"ttg.threads-per-warp\" = 32 : i32} {\n-   tt.func @linear_to_swizzled_st_matrix_x2_local_store_fp8(%a: tensor<64x16xf8E4M3FNUZ, #linear>) {\n--    // CHECK-COUNT-1: nvvm.stmatrix %{{.*}}, %{{.*}}, %{{.*}} {layout = #nvvm.mma_layout<row>} :\n-+    // CHECK-COUNT-1: nvvm.stmatrix %{{.*}}, %{{.*}}, %{{.*}} {eltType = #nvvm.ld_st_matrix_elt_type<b16>, layout = #nvvm.mma_layout<row>, shape = #nvvm.ld_st_matrix_shape<m = 8, n = 8>} :\n-     //          CHECK: llvm.return\n-     %b = ttg.local_alloc {allocation.offset = 0 : i32} : () -> !ttg.memdesc<64x16xf8E4M3FNUZ, #shared, #smem, mutable>\n-     ttg.local_store %a, %b : tensor<64x16xf8E4M3FNUZ, #linear> -> !ttg.memdesc<64x16xf8E4M3FNUZ, #shared, #smem, mutable>\n-@@ -371,7 +371,7 @@\n- // CHECK-LABEL: linear_to_swizzled_st_matrix_local_store_fp32\n- module attributes {\"ttg.target\" = \"cuda:90\", \"ttg.num-ctas\" = 1 : i32, \"ttg.num-warps\" = 4 : i32, \"ttg.threads-per-warp\" = 32 : i32} {\n-   tt.func @linear_to_swizzled_st_matrix_local_store_fp32(%a: tensor<64x16xf32, #linear>) {\n--    // CHECK-COUNT-2: nvvm.stmatrix %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}} {layout = #nvvm.mma_layout<row>}\n-+    // CHECK-COUNT-2: nvvm.stmatrix %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}} {eltType = #nvvm.ld_st_matrix_elt_type<b16>, layout = #nvvm.mma_layout<row>, shape = #nvvm.ld_st_matrix_shape<m = 8, n = 8>}\n-     //          CHECK: llvm.return\n-     %b = ttg.local_alloc {allocation.offset = 0 : i32} : () -> !ttg.memdesc<64x16xf32, #shared, #smem, mutable>\n-     ttg.local_store %a, %b : tensor<64x16xf32, #linear> -> !ttg.memdesc<64x16xf32, #shared, #smem, mutable>\n-@@ -388,7 +388,7 @@\n- // CHECK-LABEL: linear_to_swizzled_st_matrix_trans_local_store\n- module attributes {\"ttg.target\" = \"cuda:90\", \"ttg.num-ctas\" = 1 : i32, \"ttg.num-warps\" = 4 : i32, \"ttg.threads-per-warp\" = 32 : i32} {\n-   tt.func @linear_to_swizzled_st_matrix_trans_local_store(%a: tensor<64x32xf16, #linear>) {\n--    // CHECK-COUNT-2: nvvm.stmatrix %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}} {layout = #nvvm.mma_layout<col>}\n-+    // CHECK-COUNT-2: nvvm.stmatrix %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}} {eltType = #nvvm.ld_st_matrix_elt_type<b16>, layout = #nvvm.mma_layout<col>, shape = #nvvm.ld_st_matrix_shape<m = 8, n = 8>}\n-     //          CHECK: llvm.return\n-     %b = ttg.local_alloc {allocation.offset = 0 : i32} : () -> !ttg.memdesc<64x32xf16, #shared, #smem, mutable>\n-     ttg.local_store %a, %b : tensor<64x32xf16, #linear> -> !ttg.memdesc<64x32xf16, #shared, #smem, mutable>\n-@@ -410,7 +410,7 @@\n- // CHECK-LABEL: linear_to_swizzled_st_matrix_trans_local_store\n- module attributes {\"ttg.target\" = \"cuda:90\", \"ttg.num-ctas\" = 1 : i32, \"ttg.num-warps\" = 4 : i32, \"ttg.threads-per-warp\" = 32 : i32} {\n-   tt.func @linear_to_swizzled_st_matrix_trans_local_store(%a: tensor<16x32xf16, #linear>) {\n--    // CHECK-COUNT-2: nvvm.stmatrix %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}} {layout = #nvvm.mma_layout<col>}\n-+    // CHECK-COUNT-2: nvvm.stmatrix %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}}, %{{.*}} {eltType = #nvvm.ld_st_matrix_elt_type<b16>, layout = #nvvm.mma_layout<col>, shape = #nvvm.ld_st_matrix_shape<m = 8, n = 8>}\n-     //          CHECK: llvm.return\n-     %b = ttg.local_alloc {allocation.offset = 0 : i32} : () -> !ttg.memdesc<16x32xf16, #shared, #smem, mutable>\n-     ttg.local_store %a, %b : tensor<16x32xf16, #linear> -> !ttg.memdesc<16x32xf16, #shared, #smem, mutable>\n-\n---- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/MemoryOpToLLVM.cpp\t2025-07-31 00:13:23.000000000 -0700\n-+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/MemoryOpToLLVM.cpp\t2025-08-11 09:50:35.000000000 -0700\n-@@ -220,7 +220,9 @@\n-         }\n-         inputs.push_back(b.bitcast(input, i32_ty));\n-       }\n--      rewriter.create<NVVM::StMatrixOp>(loc, vecAddr, inputs, layout);\n-+      auto shapeAttr = NVVM::LdStMatrixShapeAttr::get(ctx, /*m=*/8, /*n=*/8);\n-+      rewriter.create<NVVM::StMatrixOp>(loc, vecAddr, inputs, layout, shapeAttr,\n-+                                        NVVM::LdStMatrixEltType::B16);\n-     } else {\n-       Type matTy = nVecs == 1\n-                        ? i32_ty\n-\n---- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TargetInfo.cpp\t2025-07-31 00:13:23.000000000 -0700\n-+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TargetInfo.cpp\t2025-08-11 09:50:35.000000000 -0700\n-@@ -550,7 +550,10 @@\n-     }\n-     inputs.push_back(b.bitcast(input, i32_ty));\n-   }\n--  rewriter.create<NVVM::StMatrixOp>(loc, ptr, inputs, NVVM::MMALayout::row);\n-+  auto shapeAttr =\n-+      NVVM::LdStMatrixShapeAttr::get(rewriter.getContext(), /*m=*/8, /*n=*/8);\n-+  rewriter.create<NVVM::StMatrixOp>(loc, ptr, inputs, NVVM::MMALayout::row,\n-+                                    shapeAttr, NVVM::LdStMatrixEltType::B16);\n- }\n- \n- std::string TargetInfo::getMulhiFuncName(Type resultElementTy) const {"
        },
        {
            "sha": "f94d85a1689c3200de9888f5afe987cc5e0df1f9",
            "filename": "third_party/xla/third_party/triton/llvm_integration/cl801607173.patch",
            "status": "added",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl801607173.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl801607173.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl801607173.patch?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -0,0 +1,41 @@\n+\n+--- a/third_party/amd/lib/TritonAMDGPUToLLVM/AtomicRMWOpsEmitter.cpp\t2025-07-31 00:13:23.000000000 -0700\n++++ b/third_party/amd/lib/TritonAMDGPUToLLVM/AtomicRMWOpsEmitter.cpp\t2025-08-31 17:53:42.000000000 -0700\n+@@ -405,12 +405,31 @@\n+   Value mask = targetInfo.ballot(rewriter, loc, i64_ty, done);\n+   Value start = loopBody->getArgument(0);\n+   Value cnt = b.trunc(i32_ty, generatePopcount64(rewriter, mask));\n+-  Value mbcntLoRes = rewriter\n+-                         .create<ROCDL::MbcntLoOp>(\n+-                             loc, i32_ty, b.trunc(i32_ty, mask), b.i32_val(0))\n+-                         ->getResult(0);\n+-  Value idx = rewriter.create<ROCDL::MbcntHiOp>(\n+-      loc, i32_ty, b.trunc(i32_ty, b.lshr(mask, b.i64_val(32))), mbcntLoRes);\n++\n++  NamedAttribute noundef = rewriter.getNamedAttr(\n++      LLVM::LLVMDialect::getNoUndefAttrName(), rewriter.getUnitAttr());\n++  NamedAttribute lowRange = rewriter.getNamedAttr(\n++      LLVM::LLVMDialect::getRangeAttrName(),\n++      LLVM::ConstantRangeAttr::get(rewriter.getContext(), APInt::getZero(32),\n++                                   APInt(32, 32)));\n++  NamedAttribute highRange = rewriter.getNamedAttr(\n++      LLVM::LLVMDialect::getRangeAttrName(),\n++      LLVM::ConstantRangeAttr::get(rewriter.getContext(), APInt::getZero(32),\n++                                   APInt(32, 64)));\n++\n++  Value mbcntLoRes =\n++      ROCDL::MbcntLoOp::create(\n++          rewriter, loc, i32_ty, b.trunc(i32_ty, mask), b.i32_val(0),\n++          /*arg_attrs=*/{},\n++          /*res_attrs=*/\n++          rewriter.getArrayAttr(\n++              rewriter.getDictionaryAttr({noundef, lowRange})))\n++          ->getResult(0);\n++  Value idx = ROCDL::MbcntHiOp::create(\n++      rewriter, loc, i32_ty, b.trunc(i32_ty, b.lshr(mask, b.i64_val(32))),\n++      mbcntLoRes,\n++      /*arg_attrs=*/{},\n++      rewriter.getArrayAttr(rewriter.getDictionaryAttr({noundef, highRange})));\n+   Value base = b.add(start, cnt);\n+   Value leader = b.icmp_eq(idx, b.i32_val(0));\n+   cnt = b.sub(cnt, idx);"
        },
        {
            "sha": "bef272c563b2ada8e669cd7b3ed82b6c78e0bd6a",
            "filename": "third_party/xla/third_party/triton/llvm_integration/mem_sync_scope_agent_to_device.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fmem_sync_scope_agent_to_device.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fmem_sync_scope_agent_to_device.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fmem_sync_scope_agent_to_device.patch?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -1,24 +0,0 @@\n-b/433429549: Fix the issue where AtomicRMWOp with 2 bf16 elements was not being\n-translated correctly. The sync scope for NV should be device, not agent.\n-\n-diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/LoadStoreOpToLLVM.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/LoadStoreOpToLLVM.cpp\n---- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/LoadStoreOpToLLVM.cpp\n-+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/LoadStoreOpToLLVM.cpp\n-@@ -923,7 +923,7 @@ struct AtomicRMWOpConversion\n-         Value atom = rewriter\n-                          .create<LLVM::AtomicRMWOp>(\n-                              loc, *llvmAtomicBinOp, rmwPtr, valElements[i],\n--                             *llvmAtomicMemOrdering, StringRef(\"agent\"))\n-+                             *llvmAtomicMemOrdering, StringRef(\"device\"))\n-                          .getResult();\n-         // Handle the 2 bf16 case\n-         if (packed == 2 && valueElemNBits == 16) {\n-@@ -931,7 +931,7 @@ struct AtomicRMWOpConversion\n-                             .create<LLVM::AtomicRMWOp>(\n-                                 loc, *llvmAtomicBinOp, ptrElements[i + 1],\n-                                 valElements[i + 1], *llvmAtomicMemOrdering,\n--                                StringRef(\"agent\"))\n-+                                StringRef(\"device\"))\n-                             .getResult();\n-           auto vecTy = vec_ty(valueElemTy, vec);\n-           auto tmp ="
        },
        {
            "sha": "fecb1f1dfac00a0f277c363e7a1e9615b4cf0809",
            "filename": "third_party/xla/third_party/triton/llvm_integration/series.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fseries.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fseries.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fseries.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -8,5 +8,6 @@ LLVM nor MLIR integrator, please do not add any patches to this list.\n \"\"\"\n \n llvm_patch_list = [\n+    \"//third_party/triton:llvm_integration/cl801607173.patch\",\n     # Add new patches just above this line\n ]"
        },
        {
            "sha": "64504cb7208283181079c09b6ae1b68843094a6f",
            "filename": "third_party/xla/third_party/triton/llvm_integration/tritongpu-to-ptx-mmav3.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Ftritongpu-to-ptx-mmav3.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Ftritongpu-to-ptx-mmav3.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Ftritongpu-to-ptx-mmav3.patch?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -1,33 +0,0 @@\n-The PTX generated for this test is sensitive to the LLVM version. Newer versions\n-of the NVPTX backend may use 'prmt.b32' instead of 'bfe.u32' for byte extraction\n-and 'setp.eq.b32' instead of 'setp.eq.s32' for equality comparisons. A later\n-update (probably https://github.com/llvm/llvm-project/commit/f480e1b8258eac3565b3ffaf3f8ed0f77eb87fee)\n-optimized the number of prmt instructions generated for this code, so the\n-number of 'prmt.b32' instructions went from 64 to 48.\n-  The checks below have been updated to match the internal codegen as we think\n-  that they are just optimizations.\n-\n---- a/test/Conversion/tritongpu_to_ptx_mmav3.mlir\t2025-07-31 05:01:16.000000000 -0700\n-+++ b/test/Conversion/tritongpu_to_ptx_mmav3.mlir\t2025-08-06 05:43:00.000000000 -0700\n-@@ -57,7 +57,7 @@\n- \n-     // CHECK: mov.u32       [[TID:%.*]], %tid.x;\n-     // CHECK: and.b32       [[L0_VAL:%.*]], [[TID]], 1;\n--    // CHECK: setp.eq.s32   [[L0_OFF:%.*]], [[L0_VAL]], 0;\n-+    // CHECK: setp.eq.b32   [[L0_OFF:%.*]], [[L0_VAL]], 0;\n- \n-     // This is used to perform 16 independent selects in stage 1.\n- \n-@@ -106,10 +106,10 @@\n-     // the predicate (step 3).\n- \n-     // CHECK-DAG: and.b32           [[L1_VAL:%.*]], [[TID]], 2;\n--    // CHECK-DAG: setp.eq.s32       [[L1_OFF:%.*]], [[L1_VAL]], 0;\n-+    // CHECK-DAG: setp.eq.b32       [[L1_OFF:%.*]], [[L1_VAL]], 0;\n-     // CHECK-COUNT-16: selp.b32     {{.*}}, {{.*}}, [[L1_OFF]];\n- \n--    // CHECK-COUNT-64: bfe.u32\n-+    // CHECK-COUNT-48: prmt.b32\n-     // CHECK-COUNT-64: st.volatile.global.b8\n- \n-     %0 = ttg.convert_layout %arg0 : tensor<128x64xf8E5M2, #mma> -> tensor<128x64xf8E5M2, #dot_op>"
        },
        {
            "sha": "6ce6392617228c7e9f46ebd95e315fb13970b209",
            "filename": "third_party/xla/third_party/triton/temporary/add_set_insertion_point.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fadd_set_insertion_point.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fadd_set_insertion_point.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fadd_set_insertion_point.patch?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -1,29 +0,0 @@\n-// Remove once it is upstreamed. Tracking bug: b/440003867\n---- a/lib/Dialect/TritonNvidiaGPU/Transforms/InterleaveTMem.cpp\t2025-07-31 05:01:16.000000000 -0700\n-+++ b/lib/Dialect/TritonNvidiaGPU/Transforms/InterleaveTMem.cpp\t2025-08-19 22:42:39.000000000 -0700\n-@@ -63,6 +63,7 @@\n- std::pair<Value, AccessRange>\n- findBufferAccessMemdescSubview(Operation *subview) {\n-   OpBuilder builder(subview->getContext());\n-+  builder.setInsertionPoint(subview);\n-   Location loc = subview->getLoc();\n-   TypedValue<ttg::MemDescType> src;\n-   SmallVector<int64_t> shape;\n-\n---- a/test/TritonNvidiaGPU/interleave_tmem.mlir\t2025-07-31 05:01:16.000000000 -0700\n-+++ b/test/TritonNvidiaGPU/interleave_tmem.mlir\t2025-08-19 22:57:53.000000000 -0700\n-@@ -124,12 +124,12 @@\n-   %subview0 = ttg.memdesc_index %alloc0, %c0 : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>\n-   // CHECK: [[ALLOC1:%.+]] = ttng.tmem_alloc\n-   %alloc1 = ttng.tmem_alloc : () -> !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable>\n--  // CHECK-NEXT: [[SUBVIEW1:%.+]] = ttg.memdesc_index [[ALLOC1]]\n-+  // CHECK: [[SUBVIEW1:%.+]] = ttg.memdesc_index [[ALLOC1]]\n-   %subview1 = ttg.memdesc_index %alloc1, %c0 : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>\n-   // CHECK-NEXT: tmem_store %arg0, [[SUBVIEW1]]\n-   ttng.tmem_store %arg0, %subview1, %true : tensor<128x128xf32, #blocked> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>\n-   // CHECK-NEXT: [[ALLOC0:%.+]] = ttng.tmem_alloc\n--  // CHECK-NEXT: [[SUBVIEW0:%.+]] = ttg.memdesc_index [[ALLOC0]]\n-+  // CHECK: [[SUBVIEW0:%.+]] = ttg.memdesc_index [[ALLOC0]]\n-   // CHECK-NEXT: tmem_store %arg0, [[SUBVIEW0]]\n-   ttng.tmem_store %arg0, %subview0, %true : tensor<128x128xf32, #blocked> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>\n-   tt.return"
        },
        {
            "sha": "157d9621fbd36ac777f8f816aebf40ef57cdf529",
            "filename": "third_party/xla/third_party/triton/temporary/allocate-shared-memory-nv.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 39,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fallocate-shared-memory-nv.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fallocate-shared-memory-nv.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fallocate-shared-memory-nv.patch?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -1,39 +0,0 @@\n-// This patch should be upstreamed. It is exactly what this is already in\n-// upstream for createConvertTritonGPUToLLVMPass, but not for the new\n-// createAllocateSharedMemoryNvPass. Other option is to pass 84 as ptx version\n-// in the pipeline, but we should be consistent with the other passes.\n-\n---- a/third_party/nvidia/include/TritonNVIDIAGPUToLLVM/Passes.h\t2025-07-31 05:01:16.000000000 -0700\n-+++ b/third_party/nvidia/include/TritonNVIDIAGPUToLLVM/Passes.h\t2025-08-04 08:45:47.000000000 -0700\n-@@ -23,6 +23,8 @@\n- std::unique_ptr<OperationPass<ModuleOp>>\n- createConvertTritonGPUToLLVMPass(int32_t computeCapability, int32_t ptxVersion);\n- std::unique_ptr<OperationPass<ModuleOp>>\n-+createAllocateSharedMemoryNvPass(int32_t computeCapability);\n-+std::unique_ptr<OperationPass<ModuleOp>>\n- createAllocateSharedMemoryNvPass(int32_t computeCapability, int32_t ptxVersion);\n- \n- #define GEN_PASS_REGISTRATION\n-\n---- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/Allocation.cpp\t2025-07-31 05:01:16.000000000 -0700\n-+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/Allocation.cpp\t2025-08-04 08:58:21.000000000 -0700\n-@@ -26,6 +26,8 @@\n-           AllocateSharedMemoryNv> {\n-   using AllocateSharedMemoryNvBase::AllocateSharedMemoryNvBase;\n- \n-+  AllocateSharedMemoryNv(int32_t computeCapability)\n-+      : AllocateSharedMemoryNvBase({computeCapability}) {}\n-   AllocateSharedMemoryNv(int32_t computeCapability, int32_t ptxVersion)\n-       : AllocateSharedMemoryNvBase({computeCapability, ptxVersion}) {}\n- \n-@@ -77,6 +79,10 @@\n- } // namespace mlir::triton::nvidia_gpu\n- \n- namespace mlir::triton {\n-+std::unique_ptr<OperationPass<ModuleOp>>\n-+createAllocateSharedMemoryNvPass(int32_t computeCapability) {\n-+  return std::make_unique<AllocateSharedMemoryNv>(computeCapability);\n-+}\n- std::unique_ptr<OperationPass<ModuleOp>>\n- createAllocateSharedMemoryNvPass(int32_t computeCapability,\n-                                  int32_t ptxVersion) {"
        },
        {
            "sha": "cf8ce47c585acabcb3225db32856f271df15a9e8",
            "filename": "third_party/xla/third_party/triton/temporary/series.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fseries.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fseries.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fseries.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -14,5 +14,7 @@ those to this list.\n \"\"\"\n \n temporary_patch_list = [\n+    \"//third_party/triton:temporary/verify_nvmma_encoding.patch\",\n+    \"//third_party/triton:temporary/triton-tensor-layout-init-fiasco.patch\",\n     # Add new patches just above this line\n ]"
        },
        {
            "sha": "3bc314aebd4b53f5d97a8e5e4a460a25cde9d048",
            "filename": "third_party/xla/third_party/triton/temporary/triton-tensor-layout-init-fiasco.patch",
            "status": "added",
            "additions": 67,
            "deletions": 0,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ftriton-tensor-layout-init-fiasco.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ftriton-tensor-layout-init-fiasco.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ftriton-tensor-layout-init-fiasco.patch?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -0,0 +1,67 @@\n+Upsteam Pull Request: https://github.com/triton-lang/triton/pull/8117.\n+\n+diff --git a/bin/triton-tensor-layout.cpp b/bin/triton-tensor-layout.cpp\n+--- a/bin/triton-tensor-layout.cpp\n++++ b/bin/triton-tensor-layout.cpp\n+@@ -41,29 +41,32 @@ using namespace mlir;\n+ // CLI options\n+ //===--------------------------------------------------------------------===//\n+ \n+-cl::OptionCategory PrinterCategory(\"Available Print Options\",\n+-                                   \"Options for the tensor layout printing.\");\n++static cl::OptionCategory &getPrinterCategory() {\n++  static cl::OptionCategory PrinterCategory(\n++      \"Available Print Options\", \"Options for the tensor layout printing.\");\n++  return PrinterCategory;\n++}\n+ \n+ static cl::opt<std::string> InputFile(\n+     \"i\", cl::desc(\"File that contains the tensor data layout attributes\"),\n+-    cl::init(\"\"), cl::value_desc(\"filename\"), cl::cat(PrinterCategory));\n++    cl::init(\"\"), cl::value_desc(\"filename\"), cl::cat(getPrinterCategory()));\n+ \n+ static cl::opt<std::string>\n+     OutputFile(\"o\", cl::desc(\"Output file to write the layout into\"),\n+                cl::init(\"\"), cl::value_desc(\"filename\"),\n+-               cl::cat(PrinterCategory));\n++               cl::cat(getPrinterCategory()));\n+ \n+ static cl::opt<std::string>\n+     DataLayoutStr(\"l\", cl::desc(\"Tensor data layout attribute in string\"),\n+                   cl::value_desc(\"layout-string\"), cl::init(\"\"),\n+-                  cl::cat(PrinterCategory));\n++                  cl::cat(getPrinterCategory()));\n+ \n+ static cl::list<std::string>\n+     AliasName(\"alias-names\",\n+               cl::desc(\"A list of alias names (separated by comma) of the \"\n+                        \"layout attributes in the input file\"),\n+               cl::value_desc(\"name1,name2,name3,...\"), cl::CommaSeparated,\n+-              cl::ZeroOrMore, cl::cat(PrinterCategory));\n++              cl::ZeroOrMore, cl::cat(getPrinterCategory()));\n+ \n+ static cl::opt<bool> UseHWPointOfView(\n+     \"use-hw-view\",\n+@@ -71,11 +74,11 @@ static cl::opt<bool> UseHWPointOfView(\n+         \"Print the layout in hardware point of view. This means the output is \"\n+         \"from the warp's perspective. Otherwise, the output is from the \"\n+         \"tensor's perspective (e.g., each element maps to xxx thread).\"),\n+-    cl::init(false), cl::cat(PrinterCategory));\n++    cl::init(false), cl::cat(getPrinterCategory()));\n+ \n+ static cl::opt<std::string> TensorStr(\n+     \"t\", cl::desc(\"Tensor shape and element type (e.g., tensor<2x2xf32>)\"),\n+-    cl::init(\"\"), cl::value_desc(\"tensor-type\"), cl::cat(PrinterCategory));\n++    cl::init(\"\"), cl::value_desc(\"tensor-type\"), cl::cat(getPrinterCategory()));\n+ \n+ //===--------------------------------------------------------------------===//\n+ // Helper functions\n+@@ -182,7 +185,7 @@ static LogicalResult printLayoutFromStri\n+ //===--------------------------------------------------------------------===//\n+ \n+ int main(int argc, char **argv) {\n+-  cl::HideUnrelatedOptions(PrinterCategory);\n++  cl::HideUnrelatedOptions(getPrinterCategory());\n+   cl::ParseCommandLineOptions(argc, argv, \"tensor layout printer\\n\");\n+ \n+   DialectRegistry registry;\n\\ No newline at end of file"
        },
        {
            "sha": "02a1bc0612b0b60fb1b6d8184d090200020a9150",
            "filename": "third_party/xla/third_party/triton/temporary/verify_nvmma_encoding.patch",
            "status": "added",
            "additions": 93,
            "deletions": 0,
            "changes": 93,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fverify_nvmma_encoding.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fverify_nvmma_encoding.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fverify_nvmma_encoding.patch?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -0,0 +1,93 @@\n+\n+--- a/lib/Dialect/TritonGPU/IR/Ops.cpp\t2025-08-22 04:02:56.000000000 -0700\n++++ b/lib/Dialect/TritonGPU/IR/Ops.cpp\t2025-09-08 07:22:55.000000000 -0700\n+@@ -1,3 +1,5 @@\n++#include \"llvm/Support/Casting.h\"\n++#include \"llvm/Support/LogicalResult.h\"\n+ #include \"mlir/IR/BuiltinTypes.h\"\n+ #include \"mlir/IR/Diagnostics.h\"\n+ #include \"mlir/Support/DebugStringHelper.h\"\n+@@ -9,9 +11,8 @@\n+ #include \"triton/Dialect/TritonGPU/IR/Types.h\"\n+ #include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n+ #include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n++#include \"triton/Dialect/TritonNvidiaGPU/Transforms/TMAUtilities.h\"\n+ #include \"triton/Tools/LayoutUtils.h\"\n+-#include \"llvm/Support/Casting.h\"\n+-#include \"llvm/Support/LogicalResult.h\"\n+ \n+ // Provide custom directive handlers for declarative assemblyFormat.\n+ // They must be visible before including the generated op classes.\n+@@ -517,10 +518,47 @@\n+   return success();\n+ }\n+ \n+-static LogicalResult inferMemDescReshapeOpEncoding(ArrayRef<int64_t> srcShape,\n++// Verification copied from nvmmaSharedToLinearLayout().\n++LogicalResult verifyNVMMASharedEncoding(std::optional<Location> loc,\n++                                        NVMMASharedEncodingAttr attr,\n++                                        ArrayRef<int64_t> shape,\n++                                        int elementBitWidth) {\n++  if (attr.getSwizzlingByteWidth() == 0) return success();\n++  if (shape.size() < 2)\n++    return emitOptionalError(loc, \"nvmma_shared encoding requires rank >= 2\");\n++\n++  auto shapePerCTA = getShapePerCTA(attr, shape);\n++  auto tmaShape = triton::nvidia_gpu::getTMABlockShape(attr, shapePerCTA,\n++                                                       /*packedSize=*/true);\n++  std::array<int64_t, 2> collapsedTmaShape{1, tmaShape.back()};\n++  for (int i = 0; i + 1 < shape.size(); i++)\n++    collapsedTmaShape[0] *= tmaShape[i];\n++  if (attr.getTransposed()) {\n++    std::swap(collapsedTmaShape[0], collapsedTmaShape[1]);\n++  }\n++\n++  int tileRows = 8;\n++  int tileCols = 8 * attr.getSwizzlingByteWidth() / elementBitWidth;\n++  if (attr.getFp4Padded()) tileCols /= 2;\n++\n++  int packingFactor = attr.getFp4Padded() ? 2 : 1;\n++  if (collapsedTmaShape[1] * packingFactor < tileCols ||\n++      collapsedTmaShape[0] < tileRows) {\n++    return emitOptionalError(\n++        loc,\n++        \"Illegal shared layout; expected collapsed shapePerCTA to \"\n++        \"be at least [\",\n++        tileRows, \", \", (tileCols / packingFactor), \"], collapsedTmaShape: [\",\n++        collapsedTmaShape[0], \", \", collapsedTmaShape[1], \"]\");\n++  }\n++  return success();\n++}\n++\n++static LogicalResult inferMemDescReshapeOpEncoding(std::optional<Location> loc,\n++                                                   ArrayRef<int64_t> srcShape,\n+                                                    Attribute srcEnc,\n+                                                    ArrayRef<int64_t> dstShape,\n+-                                                   Attribute &dstEnc) {\n++                                                   Attribute& dstEnc) {\n+   if (auto mmaEncoding = dyn_cast<NVMMASharedEncodingAttr>(srcEnc)) {\n+     // TODO: supporting reshape of CTA layouts is non-trivial.\n+     if (getNumCTAs(mmaEncoding) > 1)\n+@@ -544,6 +582,11 @@\n+         ctx, mmaEncoding.getSwizzlingByteWidth(), mmaEncoding.getTransposed(),\n+         mmaEncoding.getElementBitWidth(), mmaEncoding.getFp4Padded(),\n+         CTALayout);\n++    if (failed(verifyNVMMASharedEncoding(\n++            loc, cast<NVMMASharedEncodingAttr>(dstEnc), dstShape,\n++            mmaEncoding.getElementBitWidth()))) {\n++      return failure();\n++    }\n+     // Big guns, check linear layouts are equivalent\n+     // We disallow reshaping memdesc_subslice in the verifier\n+     auto srcLL = toLinearLayout(srcShape, srcEnc, srcShape);\n+@@ -565,8 +608,8 @@\n+ \n+   Attribute dstEncoding;\n+   if (Attribute srcEnc = srcTy.getEncoding()) {\n+-    if (failed(inferMemDescReshapeOpEncoding(srcTy.getShape(), srcEnc, dstShape,\n+-                                             dstEncoding)))\n++    if (failed(inferMemDescReshapeOpEncoding(loc, srcTy.getShape(), srcEnc,\n++                                             dstShape, dstEncoding)))\n+       return failure();\n+   }\n+ "
        },
        {
            "sha": "3c00850fcae0e07f75abf69315bcffed05506b62",
            "filename": "third_party/xla/third_party/triton/temporary/ws_fix.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fws_fix.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fws_fix.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fws_fix.patch?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -1,22 +0,0 @@\n-Upstreamed in https://github.com/triton-lang/triton/pull/7796\n-\n-diff --git a/third_party/nvidia/hopper/lib/Transforms/WarpSpecialization/WSLowerToken.cpp b/third_party/nvidia/hopper/lib/Transforms/WarpSpecialization/WSLowerToken.cpp\n---- a/third_party/nvidia/hopper/lib/Transforms/WarpSpecialization/WSLowerToken.cpp\n-+++ b/third_party/nvidia/hopper/lib/Transforms/WarpSpecialization/WSLowerToken.cpp\n-@@ -13,6 +13,7 @@\n- #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n- #include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n- #include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n-+#include \"llvm/ADT/STLExtras.h\"\n- \n- namespace tt = mlir::triton;\n- namespace ttg = mlir::triton::gpu;\n-@@ -266,7 +267,7 @@ void lowerTokenOperations(Operation *par\n-     if (auto tokenOp = dyn_cast<ttnvws::CreateTokenOp>(op)) {\n-       // Check to see if it is used by warpSpec. If yes, eraseOperand and\n-       // eraseArgument.\n--      for (OpOperand &use : tokenOp->getUses()) {\n-+      for (OpOperand &use : llvm::make_early_inc_range(tokenOp->getUses())) {\n-         Operation *user = use.getOwner();\n-         if (auto wsOp = dyn_cast<ttg::WarpSpecializeOp>(user)) {\n-           unsigned opndNum = use.getOperandNumber();"
        },
        {
            "sha": "7fb3c6eb8daed564e8860ed522df6820d3e9b8cd",
            "filename": "third_party/xla/third_party/triton/temporary/ws_ub_fix.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 59,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fws_ub_fix.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fws_ub_fix.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fws_ub_fix.patch?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -1,59 +0,0 @@\n-Being upstreamed in https://github.com/triton-lang/triton/pull/7828\n-\n-diff --git a/third_party/nvidia/hopper/lib/Transforms/WarpSpecialization/WSCodePartition.cpp b/third_party/nvidia/hopper/lib/Transforms/WarpSpecialization/WSCodePartition.cpp\n---- a/third_party/nvidia/hopper/lib/Transforms/WarpSpecialization/WSCodePartition.cpp\n-+++ b/third_party/nvidia/hopper/lib/Transforms/WarpSpecialization/WSCodePartition.cpp\n-@@ -359,25 +359,16 @@ void groupChannels(\n- \n-   // Reorder channels associated with one entry based on program order of the\n-   // producers.\n--  for (auto &kv : consumerChannels) {\n--    if (kv.second.size() > 1) {\n--      auto &allOps = kv.second.front()->getSrcOp()->getBlock()->getOperations();\n--      std::sort(\n--          kv.second.begin(), kv.second.end(), [&](Channel *a, Channel *b) {\n--            auto itrA =\n--                std::find_if(allOps.begin(), allOps.end(), [&](Operation &op) {\n--                  Operation *opPointer = &op;\n--                  return opPointer == a->getSrcOp();\n--                });\n--            auto itrB =\n--                std::find_if(allOps.begin(), allOps.end(), [&](Operation &op) {\n--                  Operation *opPointer = &op;\n--                  return opPointer == b->getSrcOp();\n--                });\n--            assert(itrA != allOps.end() && itrB != allOps.end());\n--            return std::distance(itrA, itrB) < 0;\n--          });\n-+  for (auto &group : make_second_range(consumerChannels)) {\n-+    auto &allOps = group.front()->getSrcOp()->getBlock()->getOperations();\n-+    DenseMap<Operation *, size_t> opIdx;\n-+    opIdx.reserve(allOps.size());\n-+    for (auto [idx, op] : enumerate(allOps)) {\n-+      opIdx[&op] = idx;\n-     }\n-+    sort(group, [&](Channel *a, Channel *b) {\n-+      return opIdx[a->getSrcOp()] < opIdx[b->getSrcOp()];\n-+    });\n-   }\n- \n-   // Switch to using channel as the key instead of ops as ops can be volatile.\n-@@ -587,6 +578,18 @@ void createToken(\n-   DenseMap<ttng::TCGen5MMAOp, Channel *> gen5Barriers;\n-   for (auto *key : orderedChannels) {\n-     auto it = channelsGroupedByConsumers.find(key);\n-+    LLVM_DEBUG({\n-+      LDBG(\"createToken key:\");\n-+      LDBG(\"consumer: \");\n-+      key->getDstOp()->dump();\n-+\n-+      LDBG(\"createToken channelsGroupedByConsumers:\");\n-+      for (auto map_key : make_first_range(channelsGroupedByConsumers)) {\n-+        LDBG(\"representative consumer: \");\n-+        map_key->getDstOp()->dump();\n-+      }\n-+    });\n-+    assert(it != channelsGroupedByConsumers.end());\n-     Channel *channel = it->second.front();\n- \n-     CommChannel commChannel;"
        },
        {
            "sha": "42379d8283772b591b560fdacfaa7d6c5fcf1ad4",
            "filename": "third_party/xla/third_party/triton/workspace.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Ftriton%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Ftriton%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fworkspace.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -3,7 +3,6 @@\n load(\"//third_party:repo.bzl\", \"tf_http_archive\", \"tf_mirror_urls\")\n load(\"//third_party/triton:llvm_integration/series.bzl\", \"llvm_patch_list\")\n load(\"//third_party/triton:temporary/series.bzl\", \"temporary_patch_list\")\n-load(\"//third_party/triton:xla_extensions/series.bzl\", \"extensions_files_patch_list\")\n \n def repo():\n     \"\"\"Imports Triton.\"\"\"\n@@ -15,5 +14,5 @@ def repo():\n         sha256 = TRITON_SHA256,\n         strip_prefix = \"triton-\" + TRITON_COMMIT,\n         urls = tf_mirror_urls(\"https://github.com/openxla/triton/archive/{}.tar.gz\".format(TRITON_COMMIT)),\n-        patch_file = extensions_files_patch_list + llvm_patch_list + temporary_patch_list,\n+        patch_file = llvm_patch_list + temporary_patch_list,\n     )"
        },
        {
            "sha": "6076783281a740de19b228a82914feacdf75f701",
            "filename": "third_party/xla/third_party/triton/xla_extensions/series.bzl",
            "status": "removed",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Fxla_extensions%2Fseries.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftriton%2Fxla_extensions%2Fseries.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fxla_extensions%2Fseries.bzl?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -1,11 +0,0 @@\n-\"\"\"\n-Provides the list of long-term patches applied to openxla/xla that are not possible to be\n-applied in the previous copybara workflow.\n-\n-IMPORTANT: This is a temporary hack while we are figuring out the proper way to handle extensions\n-(b/335420963). Please do not add any patches to this list before confirming it with gflegar@.\n-\"\"\"\n-\n-extensions_files_patch_list = [\n-    # Add new patches just above this line\n-]"
        },
        {
            "sha": "2511d1ac41acdf12238217b2858917ad492753b1",
            "filename": "third_party/xla/third_party/tsl/tsl/platform/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -458,7 +458,6 @@ filegroup(\n         \"statusor.h\",\n         \"str_util.cc\",\n         \"str_util.h\",\n-        \"strcat.cc\",\n         \"strcat.h\",\n         \"stringpiece.h\",\n         \"stringprintf.cc\",\n@@ -774,7 +773,6 @@ cc_library(\n \n cc_library(\n     name = \"strcat\",\n-    srcs = [\"strcat.cc\"],\n     hdrs = [\"strcat.h\"],\n     deps = [\n         \":numbers\",\n@@ -1368,23 +1366,6 @@ tsl_cc_test(\n     ],\n )\n \n-tsl_cc_test(\n-    name = \"strcat_test\",\n-    size = \"small\",\n-    srcs = [\n-        \"strcat_test.cc\",\n-    ],\n-    deps = [\n-        \":bfloat16\",\n-        \":strcat\",\n-        \":stringprintf\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@local_xla//xla/tsl/platform:test\",\n-        \"@local_xla//xla/tsl/platform:types\",\n-    ],\n-)\n-\n tsl_cc_test(\n     name = \"stringpiece_test\",\n     size = \"small\","
        },
        {
            "sha": "202af7ae609c00793cd4d09cbc8c4ab447b0109e",
            "filename": "third_party/xla/third_party/tsl/tsl/platform/strcat.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 272,
            "changes": 272,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstrcat.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstrcat.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstrcat.cc?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -1,272 +0,0 @@\n-/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"tsl/platform/strcat.h\"\n-\n-#include <stdarg.h>\n-#include <stdint.h>\n-#include <stdio.h>\n-#include <string.h>\n-\n-#include <algorithm>\n-\n-#include \"absl/meta/type_traits.h\"\n-#include \"xla/tsl/platform/logging.h\"\n-\n-namespace tsl {\n-namespace strings {\n-\n-AlphaNum::AlphaNum(absl::Hex hex) {\n-  char *const end = &digits_[kFastToBufferSize];\n-  char *writer = end;\n-  uint64 value = hex.value;\n-  uint64 width = hex.width;\n-  // We accomplish minimum width by OR'ing in 0x10000 to the user's value,\n-  // where 0x10000 is the smallest hex number that is as wide as the user\n-  // asked for.\n-  uint64 mask = (static_cast<uint64>(1) << (width - 1) * 4) | value;\n-  static const char hexdigits[] = \"0123456789abcdef\";\n-  do {\n-    *--writer = hexdigits[value & 0xF];\n-    value >>= 4;\n-    mask >>= 4;\n-  } while (mask != 0);\n-  piece_ = absl::string_view(writer, end - writer);\n-}\n-\n-// ----------------------------------------------------------------------\n-// StrCat()\n-//    This merges the given strings or integers, with no delimiter.  This\n-//    is designed to be the fastest possible way to construct a string out\n-//    of a mix of raw C strings, StringPieces, strings, and integer values.\n-// ----------------------------------------------------------------------\n-\n-// Append is merely a version of memcpy that returns the address of the byte\n-// after the area just overwritten.  It comes in multiple flavors to minimize\n-// call overhead.\n-static char *Append1(char *out, const AlphaNum &x) {\n-  if (x.data() == nullptr) return out;\n-\n-  memcpy(out, x.data(), x.size());\n-  return out + x.size();\n-}\n-\n-static char *Append2(char *out, const AlphaNum &x1, const AlphaNum &x2) {\n-  if (x1.data() != nullptr) {\n-    memcpy(out, x1.data(), x1.size());\n-    out += x1.size();\n-  }\n-\n-  if (x2.data() == nullptr) return out;\n-\n-  memcpy(out, x2.data(), x2.size());\n-  return out + x2.size();\n-}\n-\n-static char *Append4(char *out, const AlphaNum &x1, const AlphaNum &x2,\n-                     const AlphaNum &x3, const AlphaNum &x4) {\n-  if (x1.data() != nullptr) {\n-    memcpy(out, x1.data(), x1.size());\n-    out += x1.size();\n-  }\n-\n-  if (x2.data() != nullptr) {\n-    memcpy(out, x2.data(), x2.size());\n-    out += x2.size();\n-  }\n-\n-  if (x3.data() != nullptr) {\n-    memcpy(out, x3.data(), x3.size());\n-    out += x3.size();\n-  }\n-\n-  if (x4.data() == nullptr) return out;\n-\n-  memcpy(out, x4.data(), x4.size());\n-  return out + x4.size();\n-}\n-\n-string StrCat(const AlphaNum &a) { return string(a.data(), a.size()); }\n-\n-string StrCat(const AlphaNum &a, const AlphaNum &b) {\n-  string result(a.size() + b.size(), '\\0');\n-  char *const begin = &*result.begin();\n-  char *out = Append2(begin, a, b);\n-  DCHECK_EQ(out, begin + result.size());\n-  return result;\n-}\n-\n-string StrCat(const AlphaNum &a, const AlphaNum &b, const AlphaNum &c) {\n-  string result(a.size() + b.size() + c.size(), '\\0');\n-  char *const begin = &*result.begin();\n-  char *out = Append2(begin, a, b);\n-  out = Append1(out, c);\n-  DCHECK_EQ(out, begin + result.size());\n-  return result;\n-}\n-\n-string StrCat(const AlphaNum &a, const AlphaNum &b, const AlphaNum &c,\n-              const AlphaNum &d) {\n-  string result(a.size() + b.size() + c.size() + d.size(), '\\0');\n-  char *const begin = &*result.begin();\n-  char *out = Append4(begin, a, b, c, d);\n-  DCHECK_EQ(out, begin + result.size());\n-  return result;\n-}\n-\n-namespace {\n-// HasMember is true_type or false_type, depending on whether or not\n-// T has a __resize_default_init member. Resize will call the\n-// __resize_default_init member if it exists, and will call the resize\n-// member otherwise.\n-template <typename string_type, typename = void>\n-struct ResizeUninitializedTraits {\n-  using HasMember = std::false_type;\n-  static void Resize(string_type *s, size_t new_size) { s->resize(new_size); }\n-};\n-\n-// __resize_default_init is provided by libc++ >= 8.0.\n-template <typename string_type>\n-struct ResizeUninitializedTraits<\n-    string_type, absl::void_t<decltype(std::declval<string_type &>()\n-                                           .__resize_default_init(237))> > {\n-  using HasMember = std::true_type;\n-  static void Resize(string_type *s, size_t new_size) {\n-    s->__resize_default_init(new_size);\n-  }\n-};\n-\n-static inline void STLStringResizeUninitialized(string *s, size_t new_size) {\n-  ResizeUninitializedTraits<string>::Resize(s, new_size);\n-}\n-\n-// Used to ensure exponential growth so that the amortized complexity of\n-// increasing the string size by a small amount is O(1), in contrast to\n-// O(str->size()) in the case of precise growth.\n-// TODO(b/217943845): Would be better to use absl::strings so we don't need to\n-// keep cherry-picking performance fixes.\n-template <typename string_type>\n-void STLStringReserveAmortized(string_type *s, size_t new_size) {\n-  const size_t cap = s->capacity();\n-  if (new_size > cap) {\n-    // Make sure to always grow by at least a factor of 2x.\n-    s->reserve((std::max)(new_size, 2 * cap));\n-  }\n-}\n-\n-// Like STLStringResizeUninitialized(str, new_size), except guaranteed to use\n-// exponential growth so that the amortized complexity of increasing the string\n-// size by a small amount is O(1), in contrast to O(str->size()) in the case of\n-// precise growth.\n-template <typename string_type>\n-void STLStringResizeUninitializedAmortized(string_type *s, size_t new_size) {\n-  STLStringReserveAmortized(s, new_size);\n-  STLStringResizeUninitialized(s, new_size);\n-}\n-\n-}  // namespace\n-namespace internal {\n-\n-// Do not call directly - these are not part of the public API.\n-string CatPieces(std::initializer_list<absl::string_view> pieces) {\n-  size_t total_size = 0;\n-  for (const absl::string_view piece : pieces) total_size += piece.size();\n-  string result(total_size, '\\0');\n-\n-  char *const begin = &*result.begin();\n-  char *out = begin;\n-  for (const absl::string_view piece : pieces) {\n-    const size_t this_size = piece.size();\n-    memcpy(out, piece.data(), this_size);\n-    out += this_size;\n-  }\n-  DCHECK_EQ(out, begin + result.size());\n-  return result;\n-}\n-\n-// It's possible to call StrAppend with a StringPiece that is itself a fragment\n-// of the string we're appending to.  However the results of this are random.\n-// Therefore, check for this in debug mode.  Use unsigned math so we only have\n-// to do one comparison.\n-#define DCHECK_NO_OVERLAP(dest, src) \\\n-  DCHECK_GE(uintptr_t((src).data() - (dest).data()), uintptr_t((dest).size()))\n-\n-void AppendPieces(string *result,\n-                  std::initializer_list<absl::string_view> pieces) {\n-  size_t old_size = result->size();\n-  size_t total_size = old_size;\n-  for (const absl::string_view piece : pieces) {\n-    DCHECK_NO_OVERLAP(*result, piece);\n-    total_size += piece.size();\n-  }\n-  STLStringResizeUninitializedAmortized(result, total_size);\n-\n-  char *const begin = &*result->begin();\n-  char *out = begin + old_size;\n-  for (const absl::string_view piece : pieces) {\n-    const size_t this_size = piece.size();\n-    memcpy(out, piece.data(), this_size);\n-    out += this_size;\n-  }\n-  DCHECK_EQ(out, begin + result->size());\n-}\n-\n-}  // namespace internal\n-\n-void StrAppend(string *result, const AlphaNum &a) {\n-  DCHECK_NO_OVERLAP(*result, a);\n-  result->append(a.data(), a.size());\n-}\n-\n-void StrAppend(string *result, const AlphaNum &a, const AlphaNum &b) {\n-  DCHECK_NO_OVERLAP(*result, a);\n-  DCHECK_NO_OVERLAP(*result, b);\n-  string::size_type old_size = result->size();\n-  STLStringResizeUninitializedAmortized(result, old_size + a.size() + b.size());\n-  char *const begin = &*result->begin();\n-  char *out = Append2(begin + old_size, a, b);\n-  DCHECK_EQ(out, begin + result->size());\n-}\n-\n-void StrAppend(string *result, const AlphaNum &a, const AlphaNum &b,\n-               const AlphaNum &c) {\n-  DCHECK_NO_OVERLAP(*result, a);\n-  DCHECK_NO_OVERLAP(*result, b);\n-  DCHECK_NO_OVERLAP(*result, c);\n-  string::size_type old_size = result->size();\n-  STLStringResizeUninitializedAmortized(\n-      result, old_size + a.size() + b.size() + c.size());\n-  char *const begin = &*result->begin();\n-  char *out = Append2(begin + old_size, a, b);\n-  out = Append1(out, c);\n-  DCHECK_EQ(out, begin + result->size());\n-}\n-\n-void StrAppend(string *result, const AlphaNum &a, const AlphaNum &b,\n-               const AlphaNum &c, const AlphaNum &d) {\n-  DCHECK_NO_OVERLAP(*result, a);\n-  DCHECK_NO_OVERLAP(*result, b);\n-  DCHECK_NO_OVERLAP(*result, c);\n-  DCHECK_NO_OVERLAP(*result, d);\n-  string::size_type old_size = result->size();\n-  STLStringResizeUninitializedAmortized(\n-      result, old_size + a.size() + b.size() + c.size() + d.size());\n-  char *const begin = &*result->begin();\n-  char *out = Append4(begin + old_size, a, b, c, d);\n-  DCHECK_EQ(out, begin + result->size());\n-}\n-\n-}  // namespace strings\n-}  // namespace tsl"
        },
        {
            "sha": "f82190957c1b0a3b6b1305cb2fdb2a94ffca4776",
            "filename": "third_party/xla/third_party/tsl/tsl/platform/strcat.h",
            "status": "modified",
            "additions": 47,
            "deletions": 101,
            "changes": 148,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstrcat.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstrcat.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstrcat.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -32,7 +32,7 @@ limitations under the License.\n \n // The AlphaNum type was designed to be used as the parameter type for StrCat().\n // Any routine accepting either a string or a number may accept it.\n-// The basic idea is that by accepting a \"const AlphaNum &\" as an argument\n+// The basic idea is that by accepting a \"const absl::AlphaNum& \" as an argument\n // to your function, your callers will automatically convert bools, integers,\n // and floating point values to strings for you.\n //\n@@ -79,60 +79,7 @@ using absl::kZeroPad7;\n using absl::kZeroPad8;\n using absl::kZeroPad9;\n using Hex ABSL_DEPRECATE_AND_INLINE() = absl::Hex;\n-\n-class AlphaNum {\n-  // NOLINTBEGIN(google-explicit-constructor)\n- public:\n-  // No bool ctor -- bools convert to an integral type.\n-  // A bool ctor would also convert incoming pointers (bletch).\n-  AlphaNum(int i32)  // NOLINT(runtime/explicit)\n-      : piece_(digits_, FastInt32ToBufferLeft(i32, digits_)) {}\n-  AlphaNum(unsigned int u32)  // NOLINT(runtime/explicit)\n-      : piece_(digits_, FastUInt32ToBufferLeft(u32, digits_)) {}\n-  AlphaNum(long x)  // NOLINT(runtime/explicit)\n-      : piece_(digits_, FastInt64ToBufferLeft(x, digits_)) {}\n-  AlphaNum(unsigned long x)  // NOLINT(runtime/explicit)\n-      : piece_(digits_, FastUInt64ToBufferLeft(x, digits_)) {}\n-  AlphaNum(long long int i64)  // NOLINT(runtime/explicit)\n-      : piece_(digits_, FastInt64ToBufferLeft(i64, digits_)) {}\n-  AlphaNum(unsigned long long int u64)  // NOLINT(runtime/explicit)\n-      : piece_(digits_, FastUInt64ToBufferLeft(u64, digits_)) {}\n-\n-  AlphaNum(float f)  // NOLINT(runtime/explicit)\n-      : piece_(digits_, FloatToBuffer(f, digits_)) {}\n-  AlphaNum(double f)  // NOLINT(runtime/explicit)\n-      : piece_(digits_, DoubleToBuffer(f, digits_)) {}\n-  AlphaNum(bfloat16 bf)  // NOLINT(runtime/explicit)\n-      : piece_(digits_, FloatToBuffer(static_cast<float>(bf), digits_)) {}\n-\n-  AlphaNum(absl::Hex hex);  // NOLINT(runtime/explicit)\n-\n-  AlphaNum(const char *c_str) : piece_(c_str) {}  // NOLINT(runtime/explicit)\n-  AlphaNum(const absl::string_view &pc)\n-      : piece_(pc) {}               // NOLINT(runtime/explicit)\n-  AlphaNum(const std::string &str)  // NOLINT(runtime/explicit)\n-      : piece_(str) {}\n-  AlphaNum(const tstring &str)  // NOLINT(runtime/explicit)\n-      : piece_(str) {}\n-  template <typename A>\n-  AlphaNum(const std::basic_string<char, std::char_traits<char>, A> &str)\n-      : piece_(str) {}  // NOLINT(runtime/explicit)\n-\n-  absl::string_view::size_type size() const { return piece_.size(); }\n-  const char *data() const { return piece_.data(); }\n-  absl::string_view Piece() const { return piece_; }\n-\n- private:\n-  absl::string_view piece_;\n-  char digits_[kFastToBufferSize];\n-\n-  // Use \":\" not ':'\n-  AlphaNum(char c);  // NOLINT(runtime/explicit)\n-\n-  // NOLINTEND(google-explicit-constructor)\n-  AlphaNum(const AlphaNum &) = delete;\n-  void operator=(const AlphaNum &) = delete;\n-};\n+using AlphaNum ABSL_DEPRECATE_AND_INLINE() = absl::AlphaNum;\n \n // ----------------------------------------------------------------------\n // StrCat()\n@@ -158,40 +105,30 @@ class AlphaNum {\n // ----------------------------------------------------------------------\n \n // For performance reasons, we have specializations for <= 4 args.\n-ABSL_DEPRECATED(\"Use absl::StrCat() instead.\")\n-std::string StrCat(const AlphaNum &a) TF_MUST_USE_RESULT;\n-ABSL_DEPRECATED(\"Use absl::StrCat() instead.\")\n-std::string StrCat(const AlphaNum &a, const AlphaNum &b) TF_MUST_USE_RESULT;\n-ABSL_DEPRECATED(\"Use absl::StrCat() instead.\")\n-std::string StrCat(const AlphaNum &a, const AlphaNum &b,\n-                   const AlphaNum &c) TF_MUST_USE_RESULT;\n-ABSL_DEPRECATED(\"Use absl::StrCat() instead.\")\n-std::string StrCat(const AlphaNum &a, const AlphaNum &b, const AlphaNum &c,\n-                   const AlphaNum &d) TF_MUST_USE_RESULT;\n-\n-namespace internal {\n-\n-// Do not call directly - this is not part of the public API.\n-std::string CatPieces(std::initializer_list<absl::string_view> pieces);\n-void AppendPieces(std::string *dest,\n-                  std::initializer_list<absl::string_view> pieces);\n-\n-}  // namespace internal\n+ABSL_DEPRECATE_AND_INLINE()\n+inline std::string StrCat(const absl::AlphaNum& a) { return absl::StrCat(a); }\n+ABSL_DEPRECATE_AND_INLINE()\n+inline std::string StrCat(const absl::AlphaNum& a, const absl::AlphaNum& b) {\n+  return absl::StrCat(a, b);\n+}\n+ABSL_DEPRECATE_AND_INLINE()\n+inline std::string StrCat(const absl::AlphaNum& a, const absl::AlphaNum& b,\n+                          const absl::AlphaNum& c) {\n+  return absl::StrCat(a, b, c);\n+}\n+ABSL_DEPRECATE_AND_INLINE()\n+inline std::string StrCat(const absl::AlphaNum& a, const absl::AlphaNum& b,\n+                          const absl::AlphaNum& c, const absl::AlphaNum& d) {\n+  return absl::StrCat(a, b, c, d);\n+}\n \n // Support 5 or more arguments\n template <typename... AV>\n ABSL_DEPRECATED(\"Use absl::StrCat() instead.\")\n-std::string StrCat(const AlphaNum &a, const AlphaNum &b, const AlphaNum &c,\n-                   const AlphaNum &d, const AlphaNum &e,\n-                   const AV &...args) TF_MUST_USE_RESULT;\n-\n-template <typename... AV>\n-ABSL_DEPRECATED(\"Use absl::StrCat() instead.\")\n-std::string StrCat(const AlphaNum &a, const AlphaNum &b, const AlphaNum &c,\n-                   const AlphaNum &d, const AlphaNum &e, const AV &...args) {\n-  return internal::CatPieces({a.Piece(), b.Piece(), c.Piece(), d.Piece(),\n-                              e.Piece(),\n-                              static_cast<const AlphaNum &>(args).Piece()...});\n+std::string StrCat(const absl::AlphaNum& a, const absl::AlphaNum& b,\n+                   const absl::AlphaNum& c, const absl::AlphaNum& d,\n+                   const absl::AlphaNum& e, const AV&... args) {\n+  return absl::StrCat(a, b, c, d, e, args...);\n }\n \n // ----------------------------------------------------------------------\n@@ -215,26 +152,35 @@ std::string StrCat(const AlphaNum &a, const AlphaNum &b, const AlphaNum &c,\n //    worked around as consecutive calls to StrAppend are quite efficient.\n // ----------------------------------------------------------------------\n \n-ABSL_DEPRECATED(\"Use absl::StrAppend() instead.\")\n-void StrAppend(std::string *dest, const AlphaNum &a);\n-ABSL_DEPRECATED(\"Use absl::StrAppend() instead.\")\n-void StrAppend(std::string *dest, const AlphaNum &a, const AlphaNum &b);\n-ABSL_DEPRECATED(\"Use absl::StrAppend() instead.\")\n-void StrAppend(std::string *dest, const AlphaNum &a, const AlphaNum &b,\n-               const AlphaNum &c);\n-ABSL_DEPRECATED(\"Use absl::StrAppend() instead.\")\n-void StrAppend(std::string *dest, const AlphaNum &a, const AlphaNum &b,\n-               const AlphaNum &c, const AlphaNum &d);\n+ABSL_DEPRECATE_AND_INLINE()\n+inline void StrAppend(std::string* dest, const absl::AlphaNum& a) {\n+  absl::StrAppend(dest, a);\n+}\n+ABSL_DEPRECATE_AND_INLINE()\n+inline void StrAppend(std::string* dest, const absl::AlphaNum& a,\n+                      const absl::AlphaNum& b) {\n+  absl::StrAppend(dest, a, b);\n+}\n+ABSL_DEPRECATE_AND_INLINE()\n+inline void StrAppend(std::string* dest, const absl::AlphaNum& a,\n+                      const absl::AlphaNum& b, const absl::AlphaNum& c) {\n+  absl::StrAppend(dest, a, b, c);\n+}\n+ABSL_DEPRECATE_AND_INLINE()\n+inline void StrAppend(std::string* dest, const absl::AlphaNum& a,\n+                      const absl::AlphaNum& b, const absl::AlphaNum& c,\n+                      const absl::AlphaNum& d) {\n+  absl::StrAppend(dest, a, b, c, d);\n+}\n \n // Support 5 or more arguments\n template <typename... AV>\n ABSL_DEPRECATED(\"Use absl::StrAppend() instead.\")\n-inline void StrAppend(std::string *dest, const AlphaNum &a, const AlphaNum &b,\n-                      const AlphaNum &c, const AlphaNum &d, const AlphaNum &e,\n-                      const AV &...args) {\n-  internal::AppendPieces(dest,\n-                         {a.Piece(), b.Piece(), c.Piece(), d.Piece(), e.Piece(),\n-                          static_cast<const AlphaNum &>(args).Piece()...});\n+inline void StrAppend(std::string* dest, const absl::AlphaNum& a,\n+                      const absl::AlphaNum& b, const absl::AlphaNum& c,\n+                      const absl::AlphaNum& d, const absl::AlphaNum& e,\n+                      const AV&... args) {\n+  absl::StrAppend(dest, a, b, c, d, e, args...);\n }\n \n }  // namespace strings"
        },
        {
            "sha": "9ff95912c77cd24fd9461d4c0c5f9f3fd3f7ff6a",
            "filename": "third_party/xla/third_party/tsl/tsl/platform/strcat_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 397,
            "changes": 397,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstrcat_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7a85e500337c0b93d939ffbf01fc116e91c7fb09/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstrcat_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftsl%2Ftsl%2Fplatform%2Fstrcat_test.cc?ref=7a85e500337c0b93d939ffbf01fc116e91c7fb09",
            "patch": "@@ -1,397 +0,0 @@\n-/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"tsl/platform/strcat.h\"\n-\n-#include <cstddef>\n-#include <cstdint>\n-#include <string>\n-\n-#include \"absl/strings/str_cat.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"xla/tsl/platform/test.h\"\n-#include \"xla/tsl/platform/types.h\"\n-#include \"tsl/platform/bfloat16.h\"\n-#include \"tsl/platform/stringprintf.h\"\n-\n-#ifdef _MSC_VER\n-// ssize_t is not a standard C++ type.\n-typedef ptrdiff_t ssize_t;\n-#endif\n-\n-namespace tsl {\n-namespace strings {\n-\n-// Test StrCat of ints and longs of various sizes and signdedness.\n-TEST(StrCat, Ints) {\n-  const int16_t s = -1;\n-  const uint16 us = 2;\n-  const int i = -3;\n-  const unsigned int ui = 4;\n-  const int32_t l = -5;\n-  const uint32 ul = 6;\n-  const int64_t ll = -7;\n-  const uint64 ull = 8;\n-  const ptrdiff_t ptrdiff = -9;\n-  const size_t size = 10;\n-  const ssize_t ssize = -11;\n-  const intptr_t intptr = -12;\n-  const uintptr_t uintptr = 13;\n-  string answer;\n-  answer = StrCat(s, us);\n-  EXPECT_EQ(answer, \"-12\");\n-  answer = StrCat(i, ui);\n-  EXPECT_EQ(answer, \"-34\");\n-  answer = StrCat(l, ul);\n-  EXPECT_EQ(answer, \"-56\");\n-  answer = StrCat(ll, ull);\n-  EXPECT_EQ(answer, \"-78\");\n-  answer = StrCat(ptrdiff, size);\n-  EXPECT_EQ(answer, \"-910\");\n-  answer = StrCat(ssize, intptr);\n-  EXPECT_EQ(answer, \"-11-12\");\n-  answer = StrCat(uintptr, 0);\n-  EXPECT_EQ(answer, \"130\");\n-}\n-\n-TEST(StrCat, Floats) {\n-  const int s = 0;\n-  const float f = 1.5f;\n-  const double d = 1.5;\n-  const bfloat16 bf(1.5f);\n-\n-  string answer;\n-  answer = StrCat(s, f);\n-  EXPECT_EQ(answer, \"01.5\");\n-  answer = StrCat(s, d);\n-  EXPECT_EQ(answer, \"01.5\");\n-  answer = StrCat(s, bf);\n-  EXPECT_EQ(answer, \"01.5\");\n-}\n-\n-TEST(StrCat, Nulls) {\n-  string result;\n-  // When passed to StrCat the below will produce a NULL data pointer\n-  absl::string_view v;\n-\n-  string strs[] = {\"Hello\", \"Cruel\", \"World\"};\n-\n-  result = StrCat(v);\n-  EXPECT_EQ(result, \"\");\n-\n-  result = StrCat(strs[0], v);\n-  EXPECT_EQ(result, \"Hello\");\n-\n-  result = StrCat(v, strs[0]);\n-  EXPECT_EQ(result, \"Hello\");\n-\n-  result = StrCat(v, strs[0], strs[1]);\n-  EXPECT_EQ(result, \"HelloCruel\");\n-\n-  result = StrCat(strs[0], v, strs[1]);\n-  EXPECT_EQ(result, \"HelloCruel\");\n-\n-  result = StrCat(strs[0], strs[1], v);\n-  EXPECT_EQ(result, \"HelloCruel\");\n-\n-  result = StrCat(v, strs[0], strs[1], strs[2]);\n-  EXPECT_EQ(result, \"HelloCruelWorld\");\n-\n-  result = StrCat(strs[0], v, strs[1], strs[2]);\n-  EXPECT_EQ(result, \"HelloCruelWorld\");\n-\n-  result = StrCat(strs[0], strs[1], v, strs[2]);\n-  EXPECT_EQ(result, \"HelloCruelWorld\");\n-\n-  result = StrCat(strs[0], strs[1], strs[2], v);\n-  EXPECT_EQ(result, \"HelloCruelWorld\");\n-}\n-\n-TEST(StrCat, Basics) {\n-  string result;\n-\n-  string strs[] = {\"Hello\", \"Cruel\", \"World\"};\n-\n-  absl::string_view pieces[] = {\"Hello\", \"Cruel\", \"World\"};\n-\n-  const char *c_strs[] = {\"Hello\", \"Cruel\", \"World\"};\n-\n-  int32 i32s[] = {'H', 'C', 'W'};\n-  uint64 ui64s[] = {12345678910LL, 10987654321LL};\n-\n-  result = StrCat(false, true, 2, 3);\n-  EXPECT_EQ(result, \"0123\");\n-\n-  result = StrCat(-1);\n-  EXPECT_EQ(result, \"-1\");\n-\n-  result = StrCat(0.5);\n-  EXPECT_EQ(result, \"0.5\");\n-\n-  result = StrCat(strs[1], pieces[2]);\n-  EXPECT_EQ(result, \"CruelWorld\");\n-\n-  result = StrCat(strs[0], \", \", pieces[2]);\n-  EXPECT_EQ(result, \"Hello, World\");\n-\n-  result = StrCat(strs[0], \", \", strs[1], \" \", strs[2], \"!\");\n-  EXPECT_EQ(result, \"Hello, Cruel World!\");\n-\n-  result = StrCat(pieces[0], \", \", pieces[1], \" \", pieces[2]);\n-  EXPECT_EQ(result, \"Hello, Cruel World\");\n-\n-  result = StrCat(c_strs[0], \", \", c_strs[1], \" \", c_strs[2]);\n-  EXPECT_EQ(result, \"Hello, Cruel World\");\n-\n-  result = StrCat(\"ASCII \", i32s[0], \", \", i32s[1], \" \", i32s[2], \"!\");\n-  EXPECT_EQ(result, \"ASCII 72, 67 87!\");\n-\n-  result = StrCat(ui64s[0], \", \", ui64s[1], \"!\");\n-  EXPECT_EQ(result, \"12345678910, 10987654321!\");\n-\n-  string one = \"1\";  // Actually, it's the size of this string that we want; a\n-                     // 64-bit build distinguishes between size_t and uint64,\n-                     // even though they're both unsigned 64-bit values.\n-  result = StrCat(\"And a \", one.size(), \" and a \", &result[2] - &result[0],\n-                  \" and a \", one, \" 2 3 4\", \"!\");\n-  EXPECT_EQ(result, \"And a 1 and a 2 and a 1 2 3 4!\");\n-\n-  // result = StrCat(\"Single chars won't compile\", '!');\n-  // result = StrCat(\"Neither will NULLs\", NULL);\n-  result = StrCat(\"To output a char by ASCII/numeric value, use +: \", '!' + 0);\n-  EXPECT_EQ(result, \"To output a char by ASCII/numeric value, use +: 33\");\n-\n-  float f = 100000.5;\n-  result = StrCat(\"A hundred K and a half is \", f);\n-  EXPECT_EQ(result, \"A hundred K and a half is 100000.5\");\n-\n-  double d = f;\n-  d *= d;\n-  result = StrCat(\"A hundred K and a half squared is \", d);\n-  EXPECT_EQ(result, \"A hundred K and a half squared is 10000100000.25\");\n-\n-  result = StrCat(1, 2, 333, 4444, 55555, 666666, 7777777, 88888888, 999999999);\n-  EXPECT_EQ(result, \"12333444455555666666777777788888888999999999\");\n-}\n-\n-TEST(StrCat, MaxArgs) {\n-  string result;\n-  // Test 10 up to 26 arguments, the current maximum\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\");\n-  EXPECT_EQ(result, \"123456789a\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\");\n-  EXPECT_EQ(result, \"123456789ab\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\");\n-  EXPECT_EQ(result, \"123456789abc\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\");\n-  EXPECT_EQ(result, \"123456789abcd\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\");\n-  EXPECT_EQ(result, \"123456789abcde\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\");\n-  EXPECT_EQ(result, \"123456789abcdef\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\");\n-  EXPECT_EQ(result, \"123456789abcdefg\");\n-  result =\n-      StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\");\n-  EXPECT_EQ(result, \"123456789abcdefgh\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n-                  \"h\", \"i\");\n-  EXPECT_EQ(result, \"123456789abcdefghi\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n-                  \"h\", \"i\", \"j\");\n-  EXPECT_EQ(result, \"123456789abcdefghij\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n-                  \"h\", \"i\", \"j\", \"k\");\n-  EXPECT_EQ(result, \"123456789abcdefghijk\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n-                  \"h\", \"i\", \"j\", \"k\", \"l\");\n-  EXPECT_EQ(result, \"123456789abcdefghijkl\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n-                  \"h\", \"i\", \"j\", \"k\", \"l\", \"m\");\n-  EXPECT_EQ(result, \"123456789abcdefghijklm\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n-                  \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\");\n-  EXPECT_EQ(result, \"123456789abcdefghijklmn\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n-                  \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\");\n-  EXPECT_EQ(result, \"123456789abcdefghijklmno\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n-                  \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\");\n-  EXPECT_EQ(result, \"123456789abcdefghijklmnop\");\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n-                  \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\");\n-  EXPECT_EQ(result, \"123456789abcdefghijklmnopq\");\n-  // No limit thanks to C++11's variadic templates\n-  result = StrCat(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, \"a\", \"b\", \"c\", \"d\", \"e\", \"f\",\n-                  \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\",\n-                  \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\", \"D\",\n-                  \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\",\n-                  \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\");\n-  EXPECT_EQ(result,\n-            \"12345678910abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\");\n-}\n-\n-TEST(StrAppend, Basics) {\n-  string result = \"existing text\";\n-\n-  string strs[] = {\"Hello\", \"Cruel\", \"World\"};\n-\n-  absl::string_view pieces[] = {\"Hello\", \"Cruel\", \"World\"};\n-\n-  const char *c_strs[] = {\"Hello\", \"Cruel\", \"World\"};\n-\n-  int32 i32s[] = {'H', 'C', 'W'};\n-  uint64 ui64s[] = {12345678910LL, 10987654321LL};\n-\n-  string::size_type old_size = result.size();\n-  StrAppend(&result, strs[0]);\n-  EXPECT_EQ(result.substr(old_size), \"Hello\");\n-\n-  old_size = result.size();\n-  StrAppend(&result, strs[1], pieces[2]);\n-  EXPECT_EQ(result.substr(old_size), \"CruelWorld\");\n-\n-  old_size = result.size();\n-  StrAppend(&result, strs[0], \", \", pieces[2]);\n-  EXPECT_EQ(result.substr(old_size), \"Hello, World\");\n-\n-  old_size = result.size();\n-  StrAppend(&result, strs[0], \", \", strs[1], \" \", strs[2], \"!\");\n-  EXPECT_EQ(result.substr(old_size), \"Hello, Cruel World!\");\n-\n-  old_size = result.size();\n-  StrAppend(&result, pieces[0], \", \", pieces[1], \" \", pieces[2]);\n-  EXPECT_EQ(result.substr(old_size), \"Hello, Cruel World\");\n-\n-  old_size = result.size();\n-  StrAppend(&result, c_strs[0], \", \", c_strs[1], \" \", c_strs[2]);\n-  EXPECT_EQ(result.substr(old_size), \"Hello, Cruel World\");\n-\n-  old_size = result.size();\n-  StrAppend(&result, \"ASCII \", i32s[0], \", \", i32s[1], \" \", i32s[2], \"!\");\n-  EXPECT_EQ(result.substr(old_size), \"ASCII 72, 67 87!\");\n-\n-  old_size = result.size();\n-  StrAppend(&result, ui64s[0], \", \", ui64s[1], \"!\");\n-  EXPECT_EQ(result.substr(old_size), \"12345678910, 10987654321!\");\n-\n-  string one = \"1\";  // Actually, it's the size of this string that we want; a\n-                     // 64-bit build distinguishes between size_t and uint64,\n-                     // even though they're both unsigned 64-bit values.\n-  old_size = result.size();\n-  StrAppend(&result, \"And a \", one.size(), \" and a \", &result[2] - &result[0],\n-            \" and a \", one, \" 2 3 4\", \"!\");\n-  EXPECT_EQ(result.substr(old_size), \"And a 1 and a 2 and a 1 2 3 4!\");\n-\n-  // result = StrCat(\"Single chars won't compile\", '!');\n-  // result = StrCat(\"Neither will NULLs\", NULL);\n-  old_size = result.size();\n-  StrAppend(&result,\n-            \"To output a char by ASCII/numeric value, use +: \", '!' + 0);\n-  EXPECT_EQ(result.substr(old_size),\n-            \"To output a char by ASCII/numeric value, use +: 33\");\n-\n-  float f = 100000.5;\n-  old_size = result.size();\n-  StrAppend(&result, \"A hundred K and a half is \", f);\n-  EXPECT_EQ(result.substr(old_size), \"A hundred K and a half is 100000.5\");\n-\n-  double d = f;\n-  d *= d;\n-  old_size = result.size();\n-  StrAppend(&result, \"A hundred K and a half squared is \", d);\n-  EXPECT_EQ(result.substr(old_size),\n-            \"A hundred K and a half squared is 10000100000.25\");\n-\n-  // Test 9 arguments, the old maximum\n-  old_size = result.size();\n-  StrAppend(&result, 1, 22, 333, 4444, 55555, 666666, 7777777, 88888888, 9);\n-  EXPECT_EQ(result.substr(old_size), \"1223334444555556666667777777888888889\");\n-\n-  // No limit thanks to C++11's variadic templates\n-  old_size = result.size();\n-  StrAppend(&result, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, \"a\", \"b\", \"c\", \"d\", \"e\",\n-            \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\",\n-            \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\", \"D\", \"E\",\n-            \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\",\n-            \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\",\n-            \"No limit thanks to C++11's variadic templates\");\n-  EXPECT_EQ(result.substr(old_size),\n-            \"12345678910abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n-            \"No limit thanks to C++11's variadic templates\");\n-}\n-\n-TEST(StrAppend, Death) {\n-  string s = \"self\";\n-  EXPECT_DEBUG_DEATH(StrAppend(&s, s.c_str() + 1), \"Check failed:\");\n-  EXPECT_DEBUG_DEATH(StrAppend(&s, s), \"Check failed:\");\n-}\n-\n-static void CheckHex64(uint64 v) {\n-  string actual = strings::StrCat(absl::Hex(v, absl::kZeroPad16));\n-  string expected = Printf(\"%016llx\", static_cast<unsigned long long>(v));\n-  EXPECT_EQ(expected, actual) << \" decimal value \" << v;\n-\n-  actual = strings::StrCat(absl::Hex(v, absl::kZeroPad8));\n-  expected = Printf(\"%08llx\", static_cast<unsigned long long>(v));\n-  EXPECT_EQ(expected, actual) << \" decimal value \" << v;\n-\n-  actual = strings::StrCat(absl::Hex(v));\n-  expected = Printf(\"%llx\", static_cast<unsigned long long>(v));\n-  EXPECT_EQ(expected, actual) << \" decimal value \" << v;\n-}\n-\n-static void CheckHex32(uint32 v) {\n-  string actual = strings::StrCat(absl::Hex(v, absl::kZeroPad8));\n-  string expected = Printf(\"%08x\", v);\n-  EXPECT_EQ(expected, actual) << \" decimal value \" << v;\n-\n-  actual = strings::StrCat(absl::Hex(v));\n-  expected = Printf(\"%x\", v);\n-  EXPECT_EQ(expected, actual) << \" decimal value \" << v;\n-}\n-\n-static void CheckHexSigned32(int32_t v) {\n-  string actual = strings::StrCat(absl::Hex(v, absl::kZeroPad8));\n-  string expected = Printf(\"%08x\", v);\n-  EXPECT_EQ(expected, actual) << \" decimal value \" << v;\n-\n-  actual = strings::StrCat(absl::Hex(v));\n-  expected = Printf(\"%x\", v);\n-  EXPECT_EQ(expected, actual) << \" decimal value \" << v;\n-}\n-\n-static void TestFastPrints() {\n-  // Test min int to make sure that works\n-  for (int i = 0; i < 10000; i++) {\n-    CheckHex64(i);\n-    CheckHex32(i);\n-    CheckHexSigned32(i);\n-    CheckHexSigned32(-i);\n-  }\n-  CheckHex64(0x123456789abcdef0ull);\n-  CheckHex32(0x12345678);\n-\n-  int8_t minus_one_8bit = -1;\n-  EXPECT_EQ(\"ff\", strings::StrCat(absl::Hex(minus_one_8bit)));\n-\n-  int16_t minus_one_16bit = -1;\n-  EXPECT_EQ(\"ffff\", strings::StrCat(absl::Hex(minus_one_16bit)));\n-}\n-\n-TEST(Numbers, TestFunctionsMovedOverFromNumbersMain) { TestFastPrints(); }\n-\n-}  // namespace strings\n-}  // namespace tsl"
        },
        {
            "sha": "cfda7a3f211ecd91d476563d6ac0bd30fa874f18",
            "filename": "third_party/xla/tools/toolchains/cross_compile/cc/cc_toolchain_config.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fcross_compile%2Fcc%2Fcc_toolchain_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fcross_compile%2Fcc%2Fcc_toolchain_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Ftools%2Ftoolchains%2Fcross_compile%2Fcc%2Fcc_toolchain_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -40,6 +40,7 @@ load(\n     \"variable_with_value\",\n     \"with_feature_set\",\n )\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n \n def layering_check_features(compiler):\n     if compiler != \"clang\":"
        },
        {
            "sha": "716fc3cadf3388097886cac1e981102c88eb380e",
            "filename": "third_party/xla/tools/toolchains/remote_config/configs.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fremote_config%2Fconfigs.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fremote_config%2Fconfigs.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Ftools%2Ftoolchains%2Fremote_config%2Fconfigs.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -50,7 +50,7 @@ def initialize_rbe_configs():\n     # The `ml-build-rbe` image is identical to the `ml-build` image except for the base image.\n     # The `ml-build`'s base image is a standard `ubuntu22.04` image.\n     # The `ml-build-rbe`'s base image is `nvidia/cuda:12.3.2-base-ubuntu22.04` which has nvidia driver installed.\n-    ml_build_rbe_config(\"docker://us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-rbe@sha256:aaeb29799463729092c05f5ac8393113b3bb5d1ecf085f9f1f2016e3a1ece11c\")\n+    ml_build_rbe_config(\"docker://us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build-rbe@sha256:7bae9f7604645cbad40b11a22294f5058db16022315d52a130b832d07e54c9ef\")\n \n     # TF-Version-Specific SIG Build RBE Configs. The crosstool generated from these\n     # configs are python-version-independent because they only care about the"
        },
        {
            "sha": "ae0527efe74bbfe97f959be4ed2f58702e95450c",
            "filename": "third_party/xla/tools/toolchains/win/20240424/armeabi_cc_toolchain_config.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin%2F20240424%2Farmeabi_cc_toolchain_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin%2F20240424%2Farmeabi_cc_toolchain_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin%2F20240424%2Farmeabi_cc_toolchain_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -19,6 +19,7 @@ load(\n     \"feature\",\n     \"tool_path\",\n )\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n \n def _impl(ctx):\n     toolchain_identifier = \"stub_armeabi-v7a\""
        },
        {
            "sha": "aebbf497f57c5f4236d8a5fbacccbd03a941b19a",
            "filename": "third_party/xla/tools/toolchains/win/20240424/windows_cc_toolchain_config.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin%2F20240424%2Fwindows_cc_toolchain_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin%2F20240424%2Fwindows_cc_toolchain_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin%2F20240424%2Fwindows_cc_toolchain_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -29,6 +29,7 @@ load(\n     \"variable_with_value\",\n     \"with_feature_set\",\n )\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n \n all_compile_actions = [\n     ACTION_NAMES.c_compile,"
        },
        {
            "sha": "f3a4fd42dc80c895189ab741dbc3483a54a46e04",
            "filename": "third_party/xla/tools/toolchains/win/bazel_211/armeabi_cc_toolchain_config.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin%2Fbazel_211%2Farmeabi_cc_toolchain_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin%2Fbazel_211%2Farmeabi_cc_toolchain_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin%2Fbazel_211%2Farmeabi_cc_toolchain_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -19,6 +19,7 @@ load(\n     \"feature\",\n     \"tool_path\",\n )\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n \n def _impl(ctx):\n     toolchain_identifier = \"stub_armeabi-v7a\""
        },
        {
            "sha": "e0f8b70d82287aef8fecde5acd0327bcaea81a35",
            "filename": "third_party/xla/tools/toolchains/win/bazel_211/windows_cc_toolchain_config.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin%2Fbazel_211%2Fwindows_cc_toolchain_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin%2Fbazel_211%2Fwindows_cc_toolchain_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin%2Fbazel_211%2Fwindows_cc_toolchain_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -30,6 +30,7 @@ load(\n     \"variable_with_value\",\n     \"with_feature_set\",\n )\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n \n all_compile_actions = [\n     ACTION_NAMES.c_compile,"
        },
        {
            "sha": "ae0527efe74bbfe97f959be4ed2f58702e95450c",
            "filename": "third_party/xla/tools/toolchains/win/tf_win_05022023/armeabi_cc_toolchain_config.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin%2Ftf_win_05022023%2Farmeabi_cc_toolchain_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin%2Ftf_win_05022023%2Farmeabi_cc_toolchain_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin%2Ftf_win_05022023%2Farmeabi_cc_toolchain_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -19,6 +19,7 @@ load(\n     \"feature\",\n     \"tool_path\",\n )\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n \n def _impl(ctx):\n     toolchain_identifier = \"stub_armeabi-v7a\""
        },
        {
            "sha": "e8af866670eefcba9c279df96068342cbeeb3ea9",
            "filename": "third_party/xla/tools/toolchains/win/tf_win_05022023/windows_cc_toolchain_config.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin%2Ftf_win_05022023%2Fwindows_cc_toolchain_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin%2Ftf_win_05022023%2Fwindows_cc_toolchain_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin%2Ftf_win_05022023%2Fwindows_cc_toolchain_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -29,6 +29,7 @@ load(\n     \"variable_with_value\",\n     \"with_feature_set\",\n )\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n \n all_compile_actions = [\n     ACTION_NAMES.c_compile,"
        },
        {
            "sha": "ae0527efe74bbfe97f959be4ed2f58702e95450c",
            "filename": "third_party/xla/tools/toolchains/win2022/20241118/armeabi_cc_toolchain_config.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin2022%2F20241118%2Farmeabi_cc_toolchain_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin2022%2F20241118%2Farmeabi_cc_toolchain_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin2022%2F20241118%2Farmeabi_cc_toolchain_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -19,6 +19,7 @@ load(\n     \"feature\",\n     \"tool_path\",\n )\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n \n def _impl(ctx):\n     toolchain_identifier = \"stub_armeabi-v7a\""
        },
        {
            "sha": "aebbf497f57c5f4236d8a5fbacccbd03a941b19a",
            "filename": "third_party/xla/tools/toolchains/win2022/20241118/windows_cc_toolchain_config.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin2022%2F20241118%2Fwindows_cc_toolchain_config.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin2022%2F20241118%2Fwindows_cc_toolchain_config.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Ftools%2Ftoolchains%2Fwin2022%2F20241118%2Fwindows_cc_toolchain_config.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -29,6 +29,7 @@ load(\n     \"variable_with_value\",\n     \"with_feature_set\",\n )\n+load(\"@rules_cc//cc/common:cc_common.bzl\", \"cc_common\")\n \n all_compile_actions = [\n     ACTION_NAMES.c_compile,"
        },
        {
            "sha": "4d7dd8743074501eeaf0f133163a2d7a3a8bb272",
            "filename": "third_party/xla/workspace0.bzl",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fworkspace0.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fworkspace0.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fworkspace0.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -140,10 +140,10 @@ def workspace():\n     if \"rules_ml_toolchain\" not in native.existing_rules():\n         http_archive(\n             name = \"rules_ml_toolchain\",\n-            sha256 = \"e7e44c4e349a1c1f31398bd2257c51432e73ea0e7e24cce67090b68b0b50007e\",\n-            strip_prefix = \"rules_ml_toolchain-55dcd0a52c7e0f9eec9927a32512229c09ac3b3e\",\n+            sha256 = \"1a855dd94eebedae69d1804e8837ad70b8018358a0a03eea0bec71d7dc2b096a\",\n+            strip_prefix = \"rules_ml_toolchain-d321763a84c900bc29b4f5459a4f81fad19b2356\",\n             urls = [\n-                \"https://github.com/google-ml-infra/rules_ml_toolchain/archive/55dcd0a52c7e0f9eec9927a32512229c09ac3b3e.tar.gz\",\n+                \"https://github.com/google-ml-infra/rules_ml_toolchain/archive/d321763a84c900bc29b4f5459a4f81fad19b2356.tar.gz\",\n             ],\n         )\n "
        },
        {
            "sha": "5897e325e4ed737e2ab866da454f382914d76dca",
            "filename": "third_party/xla/workspace2.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fworkspace2.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fworkspace2.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fworkspace2.bzl?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -317,7 +317,7 @@ def _tf_repositories():\n \n     tf_http_archive(\n         name = \"com_google_protobuf\",\n-        patch_file = [\"//third_party/protobuf:protobuf-6.31.1.patch\"],\n+        patch_file = [\"//third_party/protobuf:protobuf.patch\"],\n         sha256 = \"6e09bbc950ba60c3a7b30280210cd285af8d7d8ed5e0a6ed101c72aff22e8d88\",\n         strip_prefix = \"protobuf-6.31.1\",\n         urls = tf_mirror_urls(\"https://github.com/protocolbuffers/protobuf/archive/refs/tags/v6.31.1.zip\"),"
        },
        {
            "sha": "7fbe4f1edbb266de0e2c7ac0b1f8299a65bc3804",
            "filename": "third_party/xla/xla/BUILD",
            "status": "modified",
            "additions": 31,
            "deletions": 3,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -264,7 +264,6 @@ xla_cc_test(\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test_main\",\n         \"@com_google_absl//absl/base:log_severity\",\n-        \"@com_google_absl//absl/log:log_sink\",\n         \"@com_google_absl//absl/log:scoped_mock_log\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n@@ -477,6 +476,35 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"shape_pool\",\n+    srcs = [\"shape_pool.cc\"],\n+    hdrs = [\"shape_pool.h\"],\n+    visibility = [\"//visibility:public\"],\n+    deps = [\n+        \":shape_util\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/base:no_destructor\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/synchronization\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"shape_pool_test\",\n+    srcs = [\"shape_pool_test.cc\"],\n+    deps = [\n+        \":literal_util\",\n+        \":shape_pool\",\n+        \"//xla:shape_util\",\n+        \"//xla/tsl/platform:test\",\n+        \"//xla/tsl/platform:test_benchmark\",\n+        \"//xla/tsl/platform:test_main\",\n+        \"@com_google_googletest//:gtest\",\n+    ],\n+)\n+\n tf_proto_library(\n     name = \"shape_util_proto\",\n     srcs = [\"shape_util.proto\"],\n@@ -867,8 +895,7 @@ cc_library(\n     visibility = internal_visibility([\":friends\"]),\n     deps = [\n         \":array\",\n-        \":types\",\n-        \"//xla/tsl/platform:logging\",\n+        \":util\",\n     ],\n )\n \n@@ -1477,6 +1504,7 @@ bzl_library(\n     name = \"lit_bzl\",\n     srcs = [\"lit.bzl\"],\n     deps = [\n+        \"@rules_cc//cc/common\",\n         # copybara:uncomment \"@rules_python//python:defs_bzl\",\n         \"@bazel_skylib//lib:paths\",\n         \"//xla/tsl:package_groups_bzl\","
        },
        {
            "sha": "f75dbd38ed65a30c4a7e580056f800d672dd35c9",
            "filename": "third_party/xla/xla/array2d.h",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Farray2d.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Farray2d.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Farray2d.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -18,17 +18,12 @@ limitations under the License.\n \n #include <algorithm>\n #include <cstdint>\n-#include <functional>\n #include <initializer_list>\n-#include <iterator>\n #include <memory>\n-#include <random>\n #include <vector>\n \n #include \"absl/functional/function_ref.h\"\n-#include \"absl/strings/str_cat.h\"\n #include \"xla/array.h\"\n-#include \"xla/types.h\"\n #include \"xla/util.h\"\n \n namespace xla {"
        },
        {
            "sha": "e5b5eac65c4897a932371aece5bf5cb3c2a303b4",
            "filename": "third_party/xla/xla/array3d.h",
            "status": "modified",
            "additions": 16,
            "deletions": 8,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Farray3d.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Farray3d.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Farray3d.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -16,17 +16,12 @@ limitations under the License.\n #ifndef XLA_ARRAY3D_H_\n #define XLA_ARRAY3D_H_\n \n-#include <algorithm>\n-#include <functional>\n+#include <cstdint>\n #include <initializer_list>\n-#include <iterator>\n-#include <memory>\n-#include <numeric>\n-#include <random>\n+#include <vector>\n \n #include \"xla/array.h\"\n-#include \"xla/tsl/platform/logging.h\"\n-#include \"xla/types.h\"\n+#include \"xla/util.h\"\n \n namespace xla {\n \n@@ -66,6 +61,19 @@ class Array3D : public Array<T> {\n   int64_t n1() const { return this->dim(0); }\n   int64_t n2() const { return this->dim(1); }\n   int64_t n3() const { return this->dim(2); }\n+\n+  void FillUnique(T start_value = 0) {\n+    int shift2 = Log2Ceiling<uint64_t>(n2());\n+    int shift3 = Log2Ceiling<uint64_t>(n3());\n+    for (int64_t i0 = 0; i0 < n1(); ++i0) {\n+      for (int64_t i1 = 0; i1 < n2(); ++i1) {\n+        for (int64_t i2 = 0; i2 < n3(); ++i2) {\n+          (*this)(i0, i1, i2) =\n+              ((i0 << (shift3 + shift2)) | (i1 << shift2) | i2) + start_value;\n+        }\n+      }\n+    }\n+  }\n };\n \n }  // namespace xla"
        },
        {
            "sha": "78185cf9e2169b6038d47e96de96bb49fe658ff7",
            "filename": "third_party/xla/xla/backends/autotuner/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -113,6 +113,7 @@ cc_library(\n         \":autotuner_cache_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings:string_view\",\n     ],\n )\n \n@@ -149,6 +150,7 @@ xla_cc_test(\n     name = \"file_based_autotuner_cache_test\",\n     srcs = [\"file_based_autotuner_cache_test.cc\"],\n     deps = [\n+        \":autotuner_cache_interface\",\n         \":autotuner_cache_proto_cc\",\n         \":file_based_autotuner_cache\",\n         \"//xla:literal_util\","
        },
        {
            "sha": "bb064e1a4694a594d6dce211c10848f6961927cb",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner.cc",
            "status": "modified",
            "additions": 71,
            "deletions": 42,
            "changes": 113,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -67,30 +67,57 @@ absl::StatusOr<std::unique_ptr<Autotuner>> Autotuner::Create(\n                     std::move(autotune_config), std::move(cache), thread_pool));\n }\n \n+absl::Status Autotuner::Autotune(HloModule* module,\n+                                 const InstructionFilterFn& should_autotune) {\n+  InstructionsByFingerprint instrunctions_by_fingerprint =\n+      GetAutotuningCandidates(module, should_autotune);\n+  if (instrunctions_by_fingerprint.empty()) {\n+    VLOG(1) << \"No instructions to autotune.\";\n+    return absl::OkStatus();\n+  }\n+\n+  VLOG(1) << \"Autotuning \" << instrunctions_by_fingerprint.size()\n+          << \" unique instructions.\";\n+  for (auto& [_, instructions] : instrunctions_by_fingerprint) {\n+    CHECK(!instructions.empty());\n+    VLOG(1) << \"Autotuning instruction:\" << instructions[0]->ToString();\n+    TF_ASSIGN_OR_RETURN(Config best_config,\n+                        GetCachedOrTuneBestConfig(instructions[0]));\n+    CodegenBackend* best_codegen_backend = best_config.codegen_backend;\n+    for (auto* instr : instructions) {\n+      TF_RETURN_IF_ERROR(best_codegen_backend->ApplyConfig(\n+          *instr, *best_config.backend_config));\n+    }\n+  }\n+  return absl::OkStatus();\n+}\n+\n absl::Status Autotuner::Autotune(HloInstruction* instr) {\n   VLOG(1) << \"Autotuning HLO: \" << instr->ToString();\n-  TF_ASSIGN_OR_RETURN(auto best_config, GetBestConfig(instr));\n+  TF_ASSIGN_OR_RETURN(Config best_config, GetCachedOrTuneBestConfig(instr));\n   CodegenBackend* best_codegen_backend = best_config.codegen_backend;\n   return best_codegen_backend->ApplyConfig(*instr, *best_config.backend_config);\n }\n \n-absl::StatusOr<Autotuner::Config> Autotuner::GetBestConfig(\n+absl::StatusOr<Autotuner::Config> Autotuner::GetCachedOrTuneBestConfig(\n     HloInstruction* instr) {\n-  if (cache_) {\n-    auto cached_entry = cache_->Lookup(instr);\n-    if (cached_entry.has_value()) {\n-      VLOG(1) << \"Found cached entry for HLO: \" << instr->ToString();\n-      for (auto& codegen_backend : codegen_backends_) {\n-        if (codegen_backend->name() == cached_entry->codegen_backend()) {\n-          auto backend_config = std::make_unique<google::protobuf::Any>(\n-              cached_entry->backend_config());\n-          return Config{codegen_backend.get(), std::move(backend_config)};\n-        }\n-      }\n-      return absl::InternalError(\"Cached backend not found!\");\n+  std::optional<Config> cached_config = LookUp(instr);\n+  Config best_config;\n+  if (cached_config.has_value()) {\n+    best_config = std::move(*cached_config);\n+  } else {\n+    if (autotune_config_.expect_all_instructions_in_cache) {\n+      return absl::InternalError(\"No cached config found for HLO instr: \" +\n+                                 instr->ToString());\n     }\n+    TF_ASSIGN_OR_RETURN(best_config, TuneBestConfig(instr));\n+    Insert(instr, best_config);\n   }\n+  return best_config;\n+}\n \n+absl::StatusOr<Autotuner::Config> Autotuner::TuneBestConfig(\n+    HloInstruction* instr) {\n   TF_ASSIGN_OR_RETURN(std::vector<Config> supported_configs,\n                       GetSupportedConfigs(instr));\n   if (supported_configs.empty()) {\n@@ -114,7 +141,7 @@ absl::StatusOr<Autotuner::Config> Autotuner::GetBestConfig(\n   VLOG(1) << \"Successfully compiled \" << executable_candidates.size()\n           << \" configs out of \" << supported_configs.size() << \" configs.\";\n \n-  return ProfileAndPickBest(instr, executable_candidates);\n+  return ProfileAndPickBest(executable_candidates);\n }\n \n Autotuner::InstructionsByFingerprint Autotuner::GetAutotuningCandidates(\n@@ -130,28 +157,34 @@ Autotuner::InstructionsByFingerprint Autotuner::GetAutotuningCandidates(\n   return instrunctions_by_fingerprint;\n }\n \n-absl::Status Autotuner::Autotune(HloModule* module,\n-                                 const InstructionFilterFn& should_autotune) {\n-  InstructionsByFingerprint instrunctions_by_fingerprint =\n-      GetAutotuningCandidates(module, should_autotune);\n-  if (instrunctions_by_fingerprint.empty()) {\n-    VLOG(1) << \"No instructions to autotune.\";\n-    return absl::OkStatus();\n+std::optional<Autotuner::Config> Autotuner::LookUp(\n+    const HloInstruction* instr) {\n+  if (cache_) {\n+    auto cached_config = cache_->Lookup(instr);\n+    if (cached_config.has_value()) {\n+      VLOG(1) << \"Found cached config for HLO: \" << instr->ToString();\n+      for (auto& codegen_backend : codegen_backends_) {\n+        if (codegen_backend->name() == cached_config->codegen_backend_name) {\n+          auto backend_config = std::make_unique<google::protobuf::Any>(\n+              cached_config->backend_config);\n+          return Config{codegen_backend.get(), std::move(backend_config)};\n+        }\n+      }\n+      LOG(WARNING) << \"Cached config for HLO: \" << instr->ToString()\n+                   << \" has unsupported backend \"\n+                   << cached_config->codegen_backend_name;\n+    }\n   }\n+  return std::nullopt;\n+}\n \n-  VLOG(1) << \"Autotuning \" << instrunctions_by_fingerprint.size()\n-          << \" unique instructions.\";\n-  for (auto& [_, instructions] : instrunctions_by_fingerprint) {\n-    CHECK(!instructions.empty());\n-    VLOG(1) << \"Autotuning instruction:\" << instructions[0]->ToString();\n-    TF_ASSIGN_OR_RETURN(Config best_config, GetBestConfig(instructions[0]));\n-    CodegenBackend* best_codegen_backend = best_config.codegen_backend;\n-    for (auto* instr : instructions) {\n-      TF_RETURN_IF_ERROR(best_codegen_backend->ApplyConfig(\n-          *instr, *best_config.backend_config));\n-    }\n+void Autotuner::Insert(const HloInstruction* instr, Autotuner::Config& config) {\n+  if (cache_) {\n+    AutotunerCacheInterface::Config cached_config;\n+    cached_config.codegen_backend_name = config.codegen_backend->name();\n+    cached_config.backend_config = *config.backend_config;\n+    CHECK_OK(cache_->Insert(instr, cached_config));\n   }\n-  return absl::OkStatus();\n }\n \n absl::StatusOr<std::vector<Autotuner::Config>> Autotuner::GetSupportedConfigs(\n@@ -196,7 +229,7 @@ std::vector<absl::StatusOr<std::unique_ptr<Executable>>> Autotuner::CompileAll(\n }\n \n absl::StatusOr<Autotuner::Config> Autotuner::ProfileAndPickBest(\n-    HloInstruction* instr, std::vector<ExecutableCandidate>& candidates) {\n+    std::vector<ExecutableCandidate>& candidates) {\n   if (candidates.empty()) {\n     return absl::InternalError(\"No executables to profile!\");\n   }\n@@ -223,7 +256,7 @@ absl::StatusOr<Autotuner::Config> Autotuner::ProfileAndPickBest(\n     absl::StatusOr<ProfileResult> profile_result =\n         profiler_->Profile(candidates[i].executable.get(), *input_buffers);\n     if (!profile_result.ok()) {\n-      VLOG(2) << \"Failed to profile config \" << i << \": \"\n+      VLOG(4) << \"Failed to profile config \" << i << \": \"\n               << profile_result.status();\n       continue;\n     }\n@@ -275,12 +308,8 @@ absl::StatusOr<Autotuner::Config> Autotuner::ProfileAndPickBest(\n     }\n   }\n \n-  AutotunerCacheEntry cache_entry;\n-  cache_entry.set_codegen_backend(min_duration_config->codegen_backend->name());\n-  *cache_entry.mutable_backend_config() = *best_config->backend_config;\n-  if (cache_) {\n-    TF_RETURN_IF_ERROR(cache_->Insert(instr, cache_entry));\n-  }\n+  VLOG(1) << \"Picked config: \" << best_config->codegen_backend->name() << \" \"\n+          << best_config->backend_config->ShortDebugString();\n   return std::move(*best_config);\n }\n "
        },
        {
            "sha": "55f92edc2ef2511e12b91897079915596913eb6c",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner.h",
            "status": "modified",
            "additions": 22,
            "deletions": 5,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n #define XLA_BACKENDS_AUTOTUNER_AUTOTUNER_H_\n \n #include <memory>\n+#include <optional>\n #include <utility>\n #include <vector>\n \n@@ -42,7 +43,7 @@ struct AutotuneConfig {\n   bool skip_failing_configs = true;\n   // Whether to check the correctness of the output buffers and OOM reads on\n   // Input Buffers.\n-  bool check_buffers = false;\n+  bool check_buffers = true;\n   // Relative tolerance for correctness check.\n   float relative_tolerance = 1e-6;\n   // Whether to crash the process on check failure.\n@@ -55,6 +56,9 @@ struct AutotuneConfig {\n   bool optimize_scratch_bytes = false;\n   // Window size in microseconds to consider for scratch bytes optimization.\n   int scratch_bytes_window_size_us = 4;\n+  // If true, the autotuner will return an error if the best config for a\n+  // certain instruction is not in the cache.\n+  bool expect_all_instructions_in_cache = false;\n };\n \n class Autotuner {\n@@ -65,8 +69,10 @@ class Autotuner {\n       std::unique_ptr<AutotunerCacheInterface> cache,\n       tsl::thread::ThreadPool* thread_pool = nullptr);\n \n-  // Try all supported configs from the registered codegen backends for the\n-  // given HLO instruction and apply the best one.\n+  // Autotune the given HLO instruction. If a cache is provided, the cached\n+  // config will be used if the instruction is in the cache. Otherwise, the\n+  // autotuner will try all supported configs from the registered codegen\n+  // backends for the given HLO instruction and apply the best one.\n   absl::Status Autotune(HloInstruction* instr);\n \n   // Autotune all instructions in the module for which the filter function\n@@ -102,15 +108,26 @@ class Autotuner {\n   InstructionsByFingerprint GetAutotuningCandidates(\n       const HloModule* module, const InstructionFilterFn& should_autotune);\n \n-  absl::StatusOr<Config> GetBestConfig(HloInstruction* instr);\n+  // Gets the best config for the given instruction either from cache or by\n+  // tuning all supported configs if the instruction is not in the cache.\n+  absl::StatusOr<Config> GetCachedOrTuneBestConfig(HloInstruction* instr);\n+  // Gets the best config for the given instruction by compiling and profiling\n+  // all supported configs.\n+  absl::StatusOr<Config> TuneBestConfig(HloInstruction* instr);\n+\n+  // TODO: b/407494653 - Directly use cache api when the configs are unified.\n+  // Translates from Autotuner::Config to AutotunerCacheInterface::Config and\n+  // the other way around.\n+  std::optional<Autotuner::Config> LookUp(const HloInstruction* instr);\n+  void Insert(const HloInstruction* instr, Autotuner::Config& config);\n \n   absl::StatusOr<std::vector<Config>> GetSupportedConfigs(\n       HloInstruction* instr);\n   std::vector<absl::StatusOr<std::unique_ptr<Executable>>> CompileAll(\n       HloInstruction* instr, std::vector<Config>& configs);\n \n   absl::StatusOr<Config> ProfileAndPickBest(\n-      HloInstruction* instr, std::vector<ExecutableCandidate>& candidates);\n+      std::vector<ExecutableCandidate>& candidates);\n \n   absl::StatusOr<ScopedShapedBuffer> GetReferenceOutput(\n       std::vector<ExecutableCandidate>& candidates,"
        },
        {
            "sha": "763c8a3acc130b3f6a82221448b52bfcd5cab6d9",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner_cache_interface.h",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_cache_interface.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_cache_interface.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_cache_interface.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <optional>\n \n #include \"absl/status/status.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"xla/backends/autotuner/autotuner_cache.pb.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n \n@@ -29,13 +30,18 @@ namespace xla {\n // AutotunerCacheInterface implementations may have different cache keys.\n class AutotunerCacheInterface {\n  public:\n+  // Serializable config. Will be changed to a proto in the future.\n+  struct Config {\n+    absl::string_view codegen_backend_name;\n+    google::protobuf::Any backend_config;\n+  };\n+\n   virtual ~AutotunerCacheInterface() = default;\n \n-  virtual std::optional<AutotunerCacheEntry> Lookup(\n-      const HloInstruction* instr) = 0;\n+  virtual std::optional<Config> Lookup(const HloInstruction* instr) = 0;\n \n   virtual absl::Status Insert(const HloInstruction* instr,\n-                              AutotunerCacheEntry& entry) = 0;\n+                              Config& best_config) = 0;\n };\n \n }  // namespace xla"
        },
        {
            "sha": "3b8750909092c33cffc8dc563e04b23a52335a33",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner_test.cc",
            "status": "modified",
            "additions": 39,
            "deletions": 35,
            "changes": 74,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -68,6 +68,12 @@ std::unique_ptr<google::protobuf::Any> GetTestConfig(std::string name) {\n   return any;\n }\n \n+AutotuneConfig GetTestAutotuneConfig() {\n+  AutotuneConfig config;\n+  config.check_buffers = false;\n+  return config;\n+}\n+\n class MockCodegenBackend : public CodegenBackend {\n  public:\n   MOCK_METHOD(absl::string_view, name, (), (const, override));\n@@ -91,10 +97,6 @@ class MockCodegenBackendWithWrongResults : public MockCodegenBackend {\n \n class MockProfiler : public Profiler {\n  public:\n-  MOCK_METHOD(absl::StatusOr<std::vector<absl::StatusOr<ProfileResult>>>,\n-              ProfileWithSharedBuffers,\n-              (std::vector<std::unique_ptr<Executable>> executables),\n-              (override));\n   MOCK_METHOD(absl::StatusOr<ProfileResult>, Profile,\n               (Executable * executable, const InputBuffers& buffers),\n               (override));\n@@ -110,10 +112,11 @@ class MockProfiler : public Profiler {\n \n class MockAutotunerCache : public AutotunerCacheInterface {\n  public:\n-  MOCK_METHOD(std::optional<AutotunerCacheEntry>, Lookup,\n+  MOCK_METHOD(std::optional<AutotunerCacheInterface::Config>, Lookup,\n               (const HloInstruction* instr), (override));\n   MOCK_METHOD(absl::Status, Insert,\n-              (const HloInstruction* instr, AutotunerCacheEntry& entry),\n+              (const HloInstruction* instr,\n+               AutotunerCacheInterface::Config& best_config),\n               (override));\n };\n \n@@ -165,7 +168,7 @@ absl::StatusOr<std::unique_ptr<Autotuner>> SetupAutotunerWithExpectations(\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::move(backend));\n   return Autotuner::Create(std::move(backends), std::move(profiler),\n-                           AutotuneConfig(), std::move(cache_manager));\n+                           GetTestAutotuneConfig(), std::move(cache_manager));\n }\n \n constexpr absl::string_view kHlo = R\"(\n@@ -179,11 +182,15 @@ constexpr absl::string_view kHlo = R\"(\n   }\n   )\";\n \n-class AutotunerTest : public HloHardwareIndependentTestBase {};\n+class AutotunerTest : public HloHardwareIndependentTestBase {\n+ public:\n+  AutotunerTest() { config_ = GetTestAutotuneConfig(); }\n+  AutotuneConfig config_;\n+};\n \n TEST_F(AutotunerTest, NoCodegenBackend) {\n   auto device_description = CreateDummyDeviceDescription();\n-  auto autotuner = Autotuner::Create({}, nullptr, AutotuneConfig(),\n+  auto autotuner = Autotuner::Create({}, nullptr, config_,\n                                      std::make_unique<MockAutotunerCache>());\n   EXPECT_THAT(autotuner, StatusIs(absl::StatusCode::kInvalidArgument));\n }\n@@ -192,8 +199,8 @@ TEST_F(AutotunerTest, NoCacheManager) {\n   auto device_description = CreateDummyDeviceDescription();\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<MockCodegenBackend>());\n-  auto autotuner = Autotuner::Create(std::move(backends), nullptr,\n-                                     AutotuneConfig(), nullptr);\n+  auto autotuner =\n+      Autotuner::Create(std::move(backends), nullptr, config_, nullptr);\n   EXPECT_THAT(autotuner, IsOk());\n }\n \n@@ -213,8 +220,8 @@ TEST_F(AutotunerTest, AutotuneButNoSupportedConfigs) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto autotuner,\n-      Autotuner::Create(std::move(backends), std::move(profiler),\n-                        AutotuneConfig(), std::move(cache_manager)));\n+      Autotuner::Create(std::move(backends), std::move(profiler), config_,\n+                        std::move(cache_manager)));\n   auto dummy_instr = HloInstruction::CreateConstant(LiteralUtil::CreateR0(1));\n   EXPECT_THAT(autotuner->Autotune(dummy_instr.get()),\n               absl_testing::StatusIs(absl::StatusCode::kInternal));\n@@ -241,8 +248,8 @@ TEST_F(AutotunerTest, AutotuneButNoCompiledConfigs) {\n   backends.push_back(std::move(backend));\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto autotuner,\n-      Autotuner::Create(std::move(backends), std::move(profiler),\n-                        AutotuneConfig(), std::move(cache_manager)));\n+      Autotuner::Create(std::move(backends), std::move(profiler), config_,\n+                        std::move(cache_manager)));\n   auto dummy_instr = HloInstruction::CreateConstant(LiteralUtil::CreateR0(1));\n   EXPECT_THAT(autotuner->Autotune(dummy_instr.get()),\n               absl_testing::StatusIs(absl::StatusCode::kInternal));\n@@ -280,8 +287,8 @@ TEST_F(AutotunerTest, AutotuneAppliesBestConfigAndSkipsNonCompilableConfig) {\n   backends.push_back(std::move(backend));\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto autotuner,\n-      Autotuner::Create(std::move(backends), std::move(profiler),\n-                        AutotuneConfig(), std::move(cache_manager)));\n+      Autotuner::Create(std::move(backends), std::move(profiler), config_,\n+                        std::move(cache_manager)));\n   auto dummy_instr = HloInstruction::CreateConstant(LiteralUtil::CreateR0(1));\n   EXPECT_THAT(autotuner->Autotune(dummy_instr.get()), absl_testing::IsOk());\n }\n@@ -318,9 +325,8 @@ TEST_F(AutotunerTest, AutotuneAppliesBestConfigUsingThreadPool) {\n   tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test\", 2);\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto autotuner,\n-      Autotuner::Create(std::move(backends), std::move(profiler),\n-                        AutotuneConfig(), std::move(cache_manager),\n-                        &thread_pool));\n+      Autotuner::Create(std::move(backends), std::move(profiler), config_,\n+                        std::move(cache_manager), &thread_pool));\n   auto dummy_instr = HloInstruction::CreateConstant(LiteralUtil::CreateR0(1));\n   EXPECT_THAT(autotuner->Autotune(dummy_instr.get()), absl_testing::IsOk());\n }\n@@ -333,7 +339,7 @@ TEST_F(AutotunerTest, AutotuneModuleFindsNoInstructionsToAutotune) {\n   auto device_description = CreateDummyDeviceDescription();\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto autotuner,\n-      Autotuner::Create(std::move(backends), nullptr, AutotuneConfig(),\n+      Autotuner::Create(std::move(backends), nullptr, config_,\n                         std::make_unique<MockAutotunerCache>()));\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n@@ -379,13 +385,13 @@ TEST_F(AutotunerTest, AutotuneModuleWithDuplicateInstructions) {\n \n TEST_F(AutotunerTest, CacheHit) {\n   auto cache_manager = std::make_unique<MockAutotunerCache>();\n-  AutotunerCacheEntry entry;\n-  entry.set_codegen_backend(\"mock_backend\");\n+  AutotunerCacheInterface::Config config;\n+  config.codegen_backend_name = \"mock_backend\";\n   TestConfig test_config;\n   GetTestConfig(\"test_config_2\")->UnpackTo(&test_config);\n-  entry.mutable_backend_config()->PackFrom(test_config);\n+  config.backend_config.PackFrom(test_config);\n \n-  EXPECT_CALL(*cache_manager, Lookup(_)).WillOnce(Return(entry));\n+  EXPECT_CALL(*cache_manager, Lookup(_)).WillOnce(Return(config));\n \n   auto backend = std::make_unique<MockCodegenBackend>();\n   EXPECT_CALL(*backend, name()).WillRepeatedly(Return(\"mock_backend\"));\n@@ -395,19 +401,20 @@ TEST_F(AutotunerTest, CacheHit) {\n   EXPECT_CALL(*backend, name()).WillRepeatedly(Return(\"mock_backend\"));\n \n   auto profiler = std::make_unique<MockProfiler>();\n-  EXPECT_CALL(*profiler, ProfileWithSharedBuffers).Times(0);\n \n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::move(backend));\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto autotuner,\n-      Autotuner::Create(std::move(backends), std::move(profiler),\n-                        AutotuneConfig(), std::move(cache_manager)));\n+      Autotuner::Create(std::move(backends), std::move(profiler), config_,\n+                        std::move(cache_manager)));\n   auto dummy_instr = HloInstruction::CreateConstant(LiteralUtil::CreateR0(1));\n   EXPECT_THAT(autotuner->Autotune(dummy_instr.get()), IsOk());\n }\n \n TEST_F(AutotunerTest, AutotuneWithBufferCheck) {\n+  config_.check_buffers = true;\n+\n   std::vector<std::unique_ptr<BackendConfig>> configs_1;\n   configs_1.push_back(GetTestConfig(\"test_config_1\"));\n   auto backend_1 = std::make_unique<MockCodegenBackend>();\n@@ -444,11 +451,9 @@ TEST_F(AutotunerTest, AutotuneWithBufferCheck) {\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::move(backend_1));\n   backends.push_back(std::move(backend_2));\n-  AutotuneConfig config;\n-  config.check_buffers = true;\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto autotuner,\n-      Autotuner::Create(std::move(backends), std::move(profiler), config,\n+      Autotuner::Create(std::move(backends), std::move(profiler), config_,\n                         std::make_unique<MockAutotunerCache>()));\n   auto dummy_instr = HloInstruction::CreateConstant(LiteralUtil::CreateR0(1));\n   EXPECT_THAT(autotuner->Autotune(dummy_instr.get()), IsOk());\n@@ -487,12 +492,11 @@ TEST_F(AutotunerTest, AutotuneWithScratchBytesOptimization) {\n \n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::move(backend_1));\n-  AutotuneConfig config;\n-  config.optimize_scratch_bytes = true;\n-  config.scratch_bytes_window_size_us = 2;\n+  config_.optimize_scratch_bytes = true;\n+  config_.scratch_bytes_window_size_us = 2;\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto autotuner,\n-      Autotuner::Create(std::move(backends), std::move(profiler), config,\n+      Autotuner::Create(std::move(backends), std::move(profiler), config_,\n                         std::make_unique<MockAutotunerCache>()));\n   auto dummy_instr = HloInstruction::CreateConstant(LiteralUtil::CreateR0(1));\n   EXPECT_THAT(autotuner->Autotune(dummy_instr.get()), IsOk());"
        },
        {
            "sha": "1c6f1923871e34bc5561ab38488cf01378cc92e9",
            "filename": "third_party/xla/xla/backends/autotuner/file_based_autotuner_cache.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -128,7 +128,7 @@ absl::StatusOr<AutotunerCacheKey> FileBasedAutotunerCache::GetProtoKey(\n   return key;\n }\n \n-std::optional<AutotunerCacheEntry> FileBasedAutotunerCache::Lookup(\n+std::optional<AutotunerCacheInterface::Config> FileBasedAutotunerCache::Lookup(\n     const HloInstruction* instr) {\n   absl::StatusOr<std::string> map_key = GetMapKey(instr);\n   if (!map_key.ok()) {\n@@ -140,19 +140,25 @@ std::optional<AutotunerCacheEntry> FileBasedAutotunerCache::Lookup(\n   if (it == in_memory_cache_.end()) {\n     return std::nullopt;\n   }\n-  return it->second;\n+  AutotunerCacheInterface::Config config;\n+  config.codegen_backend_name = it->second.codegen_backend();\n+  config.backend_config = it->second.backend_config();\n+  return config;\n }\n \n-absl::Status FileBasedAutotunerCache::Insert(const HloInstruction* instr,\n-                                             AutotunerCacheEntry& entry) {\n+absl::Status FileBasedAutotunerCache::Insert(\n+    const HloInstruction* instr, AutotunerCacheInterface::Config& best_config) {\n   if (cache_config_.autotune_cache_mode ==\n       FileBasedCacheConfig::CacheMode::READ) {\n     return absl::OkStatus();\n   }\n   TF_ASSIGN_OR_RETURN(const std::string map_key, GetMapKey(instr));\n   TF_ASSIGN_OR_RETURN(AutotunerCacheKey proto_key, GetProtoKey(instr));\n   absl::MutexLock lock(&mutex_);\n+  AutotunerCacheEntry entry;\n   *entry.mutable_key() = proto_key;\n+  entry.set_codegen_backend(best_config.codegen_backend_name);\n+  *entry.mutable_backend_config() = best_config.backend_config;\n   in_memory_cache_[map_key] = entry;\n   if (!cache_config_.autotune_cache_dir.empty()) {\n     return Save(map_key, entry);"
        },
        {
            "sha": "42066e9c584750442da63a57b532c409a8d7b260",
            "filename": "third_party/xla/xla/backends/autotuner/file_based_autotuner_cache.h",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -79,11 +79,10 @@ class FileBasedAutotunerCache : public AutotunerCacheInterface {\n   static absl::StatusOr<std::unique_ptr<AutotunerCacheInterface>> Create(\n       const FileBasedCacheConfig& cache_config);\n \n-  std::optional<AutotunerCacheEntry> Lookup(\n-      const HloInstruction* instr) override ABSL_LOCKS_EXCLUDED(mutex_);\n+  std::optional<Config> Lookup(const HloInstruction* instr) override\n+      ABSL_LOCKS_EXCLUDED(mutex_);\n \n-  absl::Status Insert(const HloInstruction* instr,\n-                      AutotunerCacheEntry& entry) override\n+  absl::Status Insert(const HloInstruction* instr, Config& best_config) override\n       ABSL_LOCKS_EXCLUDED(mutex_);\n \n  private:"
        },
        {
            "sha": "4f3b09544c28b7ccc07b1344708d1cca20959a58",
            "filename": "third_party/xla/xla/backends/autotuner/file_based_autotuner_cache_test.cc",
            "status": "modified",
            "additions": 71,
            "deletions": 45,
            "changes": 116,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Ffile_based_autotuner_cache_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -26,22 +26,22 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n #include \"xla/backends/autotuner/autotuner_cache.pb.h\"\n+#include \"xla/backends/autotuner/autotuner_cache_interface.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/tsl/util/proto/proto_matchers.h\"\n #include \"tsl/platform/path.h\"\n \n namespace xla {\n namespace {\n \n+using Config = ::xla::AutotunerCacheInterface::Config;\n using ::testing::Eq;\n using ::testing::Optional;\n-using ::tsl::proto_testing::EqualsProto;\n \n // Helper to create a dummy DeviceDescription.\n se::DeviceDescription CreateDummyDeviceDescription(\n@@ -103,6 +103,32 @@ class FileBasedAutotunerCacheTest : public ::testing::Test {\n   }\n };\n \n+// Matcher for Config.\n+MATCHER_P(ConfigEq, expected_config, \"\") {\n+  const Config& actual_config = arg;\n+  if (actual_config.codegen_backend_name !=\n+      expected_config.codegen_backend_name) {\n+    *result_listener << \"codegen_backend mismatch: expected \"\n+                     << expected_config.codegen_backend_name << \", got \"\n+                     << actual_config.codegen_backend_name;\n+    return false;\n+  }\n+  // Compare backend_config (google::protobuf::Any)\n+  if (actual_config.backend_config.type_url() !=\n+      expected_config.backend_config.type_url()) {\n+    *result_listener << \"backend_config type_url mismatch: expected \"\n+                     << expected_config.backend_config.type_url() << \", got \"\n+                     << actual_config.backend_config.type_url();\n+    return false;\n+  }\n+  if (actual_config.backend_config.value() !=\n+      expected_config.backend_config.value()) {\n+    *result_listener << \"backend_config value mismatch\";\n+    return false;\n+  }\n+  return true;\n+}\n+\n TEST_F(FileBasedAutotunerCacheTest, CreateEmpty) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto cache, FileBasedAutotunerCache::Create(\n@@ -118,27 +144,27 @@ TEST_F(FileBasedAutotunerCacheTest, InsertAndLookup) {\n                       GetConfig(CreateDummyDeviceDescription(),\n                                 FileBasedCacheConfig::CacheMode::READ_WRITE)));\n   auto instr = CreateDummyInstr(\"hlo1\");\n-  AutotunerCacheEntry entry;\n-  entry.set_codegen_backend(\"TestBackend\");\n-  *entry.mutable_backend_config() = CreateDummyBackendConfig();\n+  Config config;\n+  config.codegen_backend_name = \"TestBackend\";\n+  config.backend_config = CreateDummyBackendConfig();\n \n-  TF_ASSERT_OK(cache->Insert(instr.get(), entry));\n-  EXPECT_THAT(cache->Lookup(instr.get()), Optional(EqualsProto(entry)));\n+  TF_ASSERT_OK(cache->Insert(instr.get(), config));\n+  EXPECT_THAT(cache->Lookup(instr.get()), Optional(ConfigEq(config)));\n }\n \n TEST_F(FileBasedAutotunerCacheTest, SaveAndLoad) {\n   auto instr = CreateDummyInstr(\"hlo2\");\n-  AutotunerCacheEntry entry;\n-  entry.set_codegen_backend(\"TestBackend\");\n-  *entry.mutable_backend_config() = CreateDummyBackendConfig();\n+  Config config;\n+  config.codegen_backend_name = \"TestBackend\";\n+  config.backend_config = CreateDummyBackendConfig();\n \n   // Create cache, insert, and let it save.\n   {\n     TF_ASSERT_OK_AND_ASSIGN(auto cache,\n                             FileBasedAutotunerCache::Create(GetConfig(\n                                 CreateDummyDeviceDescription(),\n                                 FileBasedCacheConfig::CacheMode::READ_WRITE)));\n-    TF_ASSERT_OK(cache->Insert(instr.get(), entry));\n+    TF_ASSERT_OK(cache->Insert(instr.get(), config));\n   }\n \n   // Create a new cache, which should load from disk.\n@@ -147,23 +173,23 @@ TEST_F(FileBasedAutotunerCacheTest, SaveAndLoad) {\n                             FileBasedAutotunerCache::Create(GetConfig(\n                                 CreateDummyDeviceDescription(),\n                                 FileBasedCacheConfig::CacheMode::READ_WRITE)));\n-    EXPECT_THAT(cache->Lookup(instr.get()), Optional(EqualsProto(entry)));\n+    EXPECT_THAT(cache->Lookup(instr.get()), Optional(ConfigEq(config)));\n   }\n }\n \n TEST_F(FileBasedAutotunerCacheTest, LoadWithDifferentDevice) {\n   auto instr = CreateDummyInstr(\"hlo2\");\n-  AutotunerCacheEntry entry;\n-  entry.set_codegen_backend(\"TestBackend\");\n-  *entry.mutable_backend_config() = CreateDummyBackendConfig();\n+  Config config;\n+  config.codegen_backend_name = \"TestBackend\";\n+  config.backend_config = CreateDummyBackendConfig();\n \n   // Create cache, insert, and let it save.\n   {\n     TF_ASSERT_OK_AND_ASSIGN(auto cache,\n                             FileBasedAutotunerCache::Create(GetConfig(\n                                 CreateDummyDeviceDescription(),\n                                 FileBasedCacheConfig::CacheMode::READ_WRITE)));\n-    TF_ASSERT_OK(cache->Insert(instr.get(), entry));\n+    TF_ASSERT_OK(cache->Insert(instr.get(), config));\n   }\n \n   // Create a new cache with different device, should not load the entry.\n@@ -178,43 +204,43 @@ TEST_F(FileBasedAutotunerCacheTest, LoadWithDifferentDevice) {\n \n TEST_F(FileBasedAutotunerCacheTest, LoadWithDifferentVersion) {\n   auto instr = CreateDummyInstr(\"hlo2\");\n-  AutotunerCacheEntry entry;\n-  entry.set_codegen_backend(\"TestBackend\");\n-  *entry.mutable_backend_config() = CreateDummyBackendConfig();\n+  Config config;\n+  config.codegen_backend_name = \"TestBackend\";\n+  config.backend_config = CreateDummyBackendConfig();\n \n   // Create cache, insert, and let it save.\n   {\n     TF_ASSERT_OK_AND_ASSIGN(auto cache,\n                             FileBasedAutotunerCache::Create(GetConfig(\n                                 CreateDummyDeviceDescription(),\n                                 FileBasedCacheConfig::CacheMode::READ_WRITE)));\n-    TF_ASSERT_OK(cache->Insert(instr.get(), entry));\n+    TF_ASSERT_OK(cache->Insert(instr.get(), config));\n   }\n \n   // Create a new cache with different version, should not load the entry.\n   {\n-    auto config = GetConfig(CreateDummyDeviceDescription(),\n-                            FileBasedCacheConfig::CacheMode::READ_WRITE);\n-    config.cache_version = \"2\";\n+    auto cache_config = GetConfig(CreateDummyDeviceDescription(),\n+                                  FileBasedCacheConfig::CacheMode::READ_WRITE);\n+    cache_config.cache_version = \"2\";\n     TF_ASSERT_OK_AND_ASSIGN(auto cache,\n-                            FileBasedAutotunerCache::Create(config));\n+                            FileBasedAutotunerCache::Create(cache_config));\n     EXPECT_THAT(cache->Lookup(instr.get()), Eq(std::nullopt));\n   }\n }\n \n TEST_F(FileBasedAutotunerCacheTest, ReadOnlyMode) {\n   auto instr = CreateDummyInstr(\"hlo3\");\n-  AutotunerCacheEntry entry;\n-  entry.set_codegen_backend(\"TestBackend\");\n-  *entry.mutable_backend_config() = CreateDummyBackendConfig();\n+  Config config;\n+  config.codegen_backend_name = \"TestBackend\";\n+  config.backend_config = CreateDummyBackendConfig();\n \n   // Create in READ_WRITE mode to pre-populate the cache file.\n   {\n     TF_ASSERT_OK_AND_ASSIGN(auto cache,\n                             FileBasedAutotunerCache::Create(GetConfig(\n                                 CreateDummyDeviceDescription(),\n                                 FileBasedCacheConfig::CacheMode::READ_WRITE)));\n-    TF_ASSERT_OK(cache->Insert(instr.get(), entry));\n+    TF_ASSERT_OK(cache->Insert(instr.get(), config));\n   }\n \n   // Create in READ mode.\n@@ -223,14 +249,14 @@ TEST_F(FileBasedAutotunerCacheTest, ReadOnlyMode) {\n                       GetConfig(CreateDummyDeviceDescription(),\n                                 FileBasedCacheConfig::CacheMode::READ)));\n   // Lookup should work.\n-  EXPECT_THAT(cache->Lookup(instr.get()), Optional(EqualsProto(entry)));\n+  EXPECT_THAT(cache->Lookup(instr.get()), Optional(ConfigEq(config)));\n \n   // Insert a new entry.\n   auto instr2 = CreateDummyInstr(\"hlo4\");\n-  AutotunerCacheEntry entry2;\n-  entry2.set_codegen_backend(\"AnotherBackend\");\n-  *entry2.mutable_backend_config() = CreateDummyBackendConfig();\n-  TF_ASSERT_OK(cache->Insert(instr2.get(), entry2));\n+  Config config2;\n+  config2.codegen_backend_name = \"AnotherBackend\";\n+  config2.backend_config = CreateDummyBackendConfig();\n+  TF_ASSERT_OK(cache->Insert(instr2.get(), config2));\n   EXPECT_THAT(cache->Lookup(instr2.get()), Eq(std::nullopt));\n \n   // Create a new cache, key2 should not be present as it wasn't saved.\n@@ -239,7 +265,7 @@ TEST_F(FileBasedAutotunerCacheTest, ReadOnlyMode) {\n                             FileBasedAutotunerCache::Create(GetConfig(\n                                 CreateDummyDeviceDescription(),\n                                 FileBasedCacheConfig::CacheMode::READ_WRITE)));\n-    EXPECT_THAT(cache2->Lookup(instr.get()), Optional(EqualsProto(entry)));\n+    EXPECT_THAT(cache2->Lookup(instr.get()), Optional(ConfigEq(config)));\n     EXPECT_THAT(cache2->Lookup(instr2.get()), Eq(std::nullopt));\n   }\n }\n@@ -251,17 +277,17 @@ TEST_F(FileBasedAutotunerCacheTest, OverwriteEntry) {\n                                 FileBasedCacheConfig::CacheMode::READ_WRITE)));\n   auto instr = CreateDummyInstr(\"hlo5\");\n \n-  AutotunerCacheEntry entry1;\n-  entry1.set_codegen_backend(\"BackendV1\");\n-  *entry1.mutable_backend_config() = CreateDummyBackendConfig();\n-  TF_ASSERT_OK(cache->Insert(instr.get(), entry1));\n-  EXPECT_THAT(cache->Lookup(instr.get()), Optional(EqualsProto(entry1)));\n-\n-  AutotunerCacheEntry entry2;\n-  entry2.set_codegen_backend(\"BackendV2\");\n-  *entry2.mutable_backend_config() = CreateDummyBackendConfig();\n-  TF_ASSERT_OK(cache->Insert(instr.get(), entry2));\n-  EXPECT_THAT(cache->Lookup(instr.get()), Optional(EqualsProto(entry2)));\n+  Config config1;\n+  config1.codegen_backend_name = \"BackendV1\";\n+  config1.backend_config = CreateDummyBackendConfig();\n+  TF_ASSERT_OK(cache->Insert(instr.get(), config1));\n+  EXPECT_THAT(cache->Lookup(instr.get()), Optional(ConfigEq(config1)));\n+\n+  Config config2;\n+  config2.codegen_backend_name = \"BackendV2\";\n+  config2.backend_config = CreateDummyBackendConfig();\n+  TF_ASSERT_OK(cache->Insert(instr.get(), config2));\n+  EXPECT_THAT(cache->Lookup(instr.get()), Optional(ConfigEq(config2)));\n }\n \n }  // namespace"
        },
        {
            "sha": "4944c296ac82e0f02e9266a8010c01c078ce110d",
            "filename": "third_party/xla/xla/backends/autotuner/profiler.h",
            "status": "modified",
            "additions": 4,
            "deletions": 25,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fprofiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fprofiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fprofiler.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -18,7 +18,6 @@ limitations under the License.\n \n #include <memory>\n #include <optional>\n-#include <vector>\n \n #include \"absl/log/check.h\"\n #include \"absl/status/statusor.h\"\n@@ -35,15 +34,15 @@ struct ProfileOptions {\n   // Whether to initialize the buffers with random data or leave them\n   // uninitialized.\n   bool should_init_buffers = false;\n-  // Whether to populate the output_buffer in the ProfileResult with the result\n-  // of the execution. This is to avoid data copies if the caller doesn't need\n-  // the output buffer.\n-  bool should_populate_output_buffer = true;\n };\n \n struct ProfileResult {\n+  // The duration of the executable run.\n   absl::Duration duration = absl::ZeroDuration();\n+  // The output buffer of the executable., only captures the first buffer if\n+  // the output is a tuple.\n   std::optional<ScopedShapedBuffer> output_buffer = std::nullopt;\n+  // The scratch bytes used by the executable, if any.\n   int scratch_bytes = 0;\n };\n \n@@ -65,26 +64,6 @@ class Profiler {\n     return Profile(executable.get(), *buffers);\n   }\n \n-  // Profiles multiple executables with shared buffers. This guarantees that\n-  // the provided executables have same arguments. This is important for\n-  // autotuning as we run same instruction with different configs.\n-  // Note that an executable can still fail during runtime even if it compiled\n-  // successfully, which is why the return type is a vector of StatusOr.\n-  virtual absl::StatusOr<std::vector<absl::StatusOr<ProfileResult>>>\n-  ProfileWithSharedBuffers(\n-      std::vector<std::unique_ptr<Executable>> executables) {\n-    std::vector<absl::StatusOr<ProfileResult>> results;\n-    if (executables.empty()) {\n-      return results;\n-    }\n-    TF_ASSIGN_OR_RETURN(std::unique_ptr<InputBuffers> buffers,\n-                        CreateInputBuffers(executables[0].get()));\n-    for (auto& executable : executables) {\n-      results.push_back(Profile(executable.get(), *buffers));\n-    }\n-    return results;\n-  }\n-\n   // Creates Input buffers for a given executable on the device. The buffers\n   // are created with the same shape as the input parameters of the executable.\n   virtual absl::StatusOr<std::unique_ptr<InputBuffers>> CreateInputBuffers("
        },
        {
            "sha": "bbcafbe7ec217952c4a79ff20fadf6327eb7b995",
            "filename": "third_party/xla/xla/backends/cpu/BUILD",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -45,6 +45,7 @@ onednn_graph_cc_library(\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/backends/cpu/runtime:dot_lib\",\n         \"//xla/backends/cpu/runtime/onednn:onednn_interop\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/tsl/mkl:onednn\",\n@@ -65,17 +66,21 @@ onednn_graph_cc_library(\n     deps = [\"//xla/tsl/mkl:onednn\"],\n )\n \n-cc_library(\n+onednn_graph_cc_library(\n     name = \"onednn_support\",\n     srcs = [\"onednn_support.cc\"],\n     hdrs = [\"onednn_support.h\"],\n     compatible_with = get_compatible_with_portable(),\n     deps = [\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/backends/cpu/codegen:target_machine_features\",\n         \"//xla/backends/cpu/runtime:dot_lib\",\n+        \"//xla/tsl/mkl:onednn\",\n+        \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n+        \"@local_tsl//tsl/platform:platform_port\",\n     ],\n )\n "
        },
        {
            "sha": "3753691a8a7ecbe374c89f69747f1faad81c2b84",
            "filename": "third_party/xla/xla/backends/cpu/autotuner/cpu_profiler.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fcpu_profiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fcpu_profiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fcpu_profiler.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -71,8 +71,6 @@ absl::StatusOr<std::unique_ptr<InputBuffers>> CpuProfiler::CreateInputBuffers(\n }\n \n std::unique_ptr<Profiler> CpuProfiler::Create(ProfileOptions options) {\n-  CHECK(options.should_populate_output_buffer == false)\n-      << \"Output buffer is not supported on CPU.\";\n   return absl::WrapUnique(new CpuProfiler(options));\n }\n "
        },
        {
            "sha": "21b9158558a1ef470e4505d79783caf2d349310e",
            "filename": "third_party/xla/xla/backends/cpu/autotuner/cpu_profiler_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 34,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fcpu_profiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fcpu_profiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fcpu_profiler_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -50,43 +50,10 @@ absl::StatusOr<std::unique_ptr<Executable>> CompileHloModule(\n \n class CpuProfilerTest : public HloHardwareIndependentTestBase {\n  public:\n-  CpuProfilerTest() { profile_options_.should_populate_output_buffer = false; }\n+  CpuProfilerTest() = default;\n   ProfileOptions profile_options_;\n };\n \n-TEST_F(CpuProfilerTest, ProfileWithSharedBuffers) {\n-  constexpr absl::string_view kHloModule = R\"(\n-        HloModule module\n-        ENTRY main {\n-          ROOT c = s32[] constant(1)\n-        }\n-      )\";\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> hlo_module,\n-                          ParseAndReturnVerifiedModule(kHloModule));\n-\n-  std::vector<std::unique_ptr<Executable>> executables;\n-\n-  TF_ASSERT_OK_AND_ASSIGN(executables.emplace_back(),\n-                          CompileHloModule(std::move(hlo_module)));\n-\n-  auto profiler = CpuProfiler::Create(profile_options_);\n-  TF_ASSERT_OK_AND_ASSIGN(auto profiles, profiler->ProfileWithSharedBuffers(\n-                                             std::move(executables)));\n-\n-  // We expect only one profile because we only have one executable.\n-  EXPECT_EQ(profiles.size(), 1);\n-  TF_EXPECT_OK(profiles[0].status());\n-}\n-\n-TEST_F(CpuProfilerTest, ProfileWithSharedBuffersWithoutExecutable) {\n-  auto profiler = CpuProfiler::Create(profile_options_);\n-  TF_ASSERT_OK_AND_ASSIGN(auto profiles,\n-                          profiler->ProfileWithSharedBuffers({}));\n-\n-  // No executable means no profiles.\n-  EXPECT_EQ(profiles.size(), 0);\n-}\n-\n TEST_F(CpuProfilerTest, CreateInputBuffersAndProfile) {\n   constexpr absl::string_view kHloModule = R\"(\n         HloModule module"
        },
        {
            "sha": "d8984847035da02173bc9022e525834543f6afdd",
            "filename": "third_party/xla/xla/backends/cpu/autotuner/llvm_kernel_autotuner.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fllvm_kernel_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fllvm_kernel_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fllvm_kernel_autotuner.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -45,16 +45,16 @@ absl::StatusOr<bool> LlvmKernelAutotuner::Run(\n   TF_ASSIGN_OR_RETURN(auto compiler,\n                       CpuCodegenBackend::CreateBackendCompiler());\n   TF_ASSIGN_OR_RETURN(auto backend, LlvmKernelBackend::Create(compiler.get()));\n-  ProfileOptions profile_options;\n-  profile_options.should_populate_output_buffer = false;\n-  std::unique_ptr<Profiler> profiler = CpuProfiler::Create(profile_options);\n+  std::unique_ptr<Profiler> profiler = CpuProfiler::Create(ProfileOptions());\n \n   std::vector<std::unique_ptr<CodegenBackend>> codegen_backends;\n   codegen_backends.push_back(std::move(backend));\n \n+  AutotuneConfig autotune_config;\n+  autotune_config.check_buffers = false;\n   TF_ASSIGN_OR_RETURN(std::unique_ptr<Autotuner> autotuner,\n                       Autotuner::Create(std::move(codegen_backends),\n-                                        std::move(profiler), AutotuneConfig(),\n+                                        std::move(profiler), autotune_config,\n                                         /*cache=*/nullptr));\n \n   bool hlo_changed = false;"
        },
        {
            "sha": "8bebf90d6e63384f3fb0e72a41c99dbf7ac5e2ba",
            "filename": "third_party/xla/xla/backends/cpu/benchmarks/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -514,7 +514,6 @@ xla_cc_test(\n         \"//xla/tsl/platform:test_main\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n-        \"@com_google_absl//absl/types:span\",\n     ],\n )\n "
        },
        {
            "sha": "61a7b8fd4d77dafb204af7bf378859e7cdca54ed",
            "filename": "third_party/xla/xla/backends/cpu/benchmarks/hlo_benchmark_runner.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Fhlo_benchmark_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Fhlo_benchmark_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Fhlo_benchmark_runner.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -326,12 +326,6 @@ absl::Status CompileHloBenchmark(benchmark::State& state,\n     compile_options.executable_build_options.mutable_debug_options()\n         ->add_xla_disable_hlo_passes(\"cpu-parallel-task-assigner\");\n   }\n-  // TODO(intel-tf): Remove this if-block once oneDNN custom calls are enabled\n-  // with thunk runtime\n-  if (!benchmark_options.use_thunk_runtime) {\n-    compile_options.executable_build_options.mutable_debug_options()\n-        ->set_xla_cpu_use_thunk_runtime(false);\n-  }\n \n   for (auto _ : state) {\n     TF_ASSIGN_OR_RETURN(std::unique_ptr<PjRtLoadedExecutable> executable,"
        },
        {
            "sha": "87fac7b2cbb1cbeff1dc0b2d00a054b83afe6fbd",
            "filename": "third_party/xla/xla/backends/cpu/benchmarks/hlo_benchmark_runner.h",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Fhlo_benchmark_runner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Fhlo_benchmark_runner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Fhlo_benchmark_runner.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -40,7 +40,6 @@ using StrToStrMapping =\n struct HloBenchmarkOptions {\n   int32_t num_executions = 1;\n   bool disable_parallel_task_assigner = false;\n-  bool use_thunk_runtime = true;\n   // If not null, AOT compilation will be used.\n   std::unique_ptr<AotCompilationOptions> aot_options;\n };"
        },
        {
            "sha": "c354818455c6569e7f987008a5cccd96beb55813",
            "filename": "third_party/xla/xla/backends/cpu/benchmarks/onednn_matmul_benchmark_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Fonednn_matmul_benchmark_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Fonednn_matmul_benchmark_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Fonednn_matmul_benchmark_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -74,7 +74,6 @@ static void BM_oneDNN_MM(benchmark::State& state) {\n \n   std::vector<const Literal*> args = {&p0, &p1};\n   HloBenchmarkOptions benchmark_options;\n-  benchmark_options.use_thunk_runtime = false;\n   CHECK_OK(RunHloBenchmark(\n       state, hlo, args,\n       {{\"$dtype\", primitive_util::LowercasePrimitiveTypeName(dtype)},"
        },
        {
            "sha": "9f04c66b2d474bcd984592910c1f065f323b0fe2",
            "filename": "third_party/xla/xla/backends/cpu/benchmarks/tanh_benchmark_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Ftanh_benchmark_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Ftanh_benchmark_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Ftanh_benchmark_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -19,7 +19,6 @@ limitations under the License.\n \n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n #include \"xla/backends/cpu/benchmarks/hlo_benchmark_runner.h\"\n #include \"xla/backends/cpu/benchmarks/multi_benchmark_config.h\"\n #include \"xla/literal.h\"\n@@ -75,7 +74,7 @@ static void BM_TanhF16(benchmark::State& state) {\n }\n \n static void BM_TanhF64(benchmark::State& state, HloBenchmarkOptions options) {\n-  int64_t d0 = state.range(0);\n+  const int64_t d0 = state.range(0);\n \n   absl::string_view hlo = R\"(\n     HloModule tanh_f64_$d0\n@@ -94,6 +93,9 @@ static void BM_TanhF64(benchmark::State& state, HloBenchmarkOptions options) {\n   std::vector<const Literal*> args = {&p0};\n   CHECK_OK(\n       RunHloBenchmark(state, hlo, args, {{\"$d0\", absl::StrCat(d0)}}, options));\n+\n+  state.SetItemsProcessed(state.iterations() * d0);\n+  state.SetBytesProcessed(state.iterations() * d0 * sizeof(double));\n }\n \n #define REGISTER_TANH_BENCHMARK(NAME) \\"
        },
        {
            "sha": "fec9bbc2755b6261cdd92bed0905c0a7cbb7cd57",
            "filename": "third_party/xla/xla/backends/cpu/codegen/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -65,6 +65,7 @@ cc_library(\n         \"//xla:util\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/codegen:intrinsic_lib\",\n+        \"//xla/codegen/intrinsic\",\n         \"//xla/codegen/intrinsic:intrinsic_compiler_lib\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service/cpu:backend_config_proto_cc\",\n@@ -623,8 +624,10 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/backends/cpu:alignment\",\n         \"//xla/codegen:hlo_fusion_spec\",\n+        \"//xla/codegen:ir_emission_utils\",\n         \"//xla/codegen:mlir_kernel_definition\",\n         \"//xla/codegen/emitters:concatenate_kernel_emitter\",\n+        \"//xla/codegen/emitters:dynamic_update_slice_kernel_emitter\",\n         \"//xla/codegen/emitters:kernel_arguments\",\n         \"//xla/codegen/emitters:loop_kernel_emitter\",\n         \"//xla/codegen/emitters/ir:xla\","
        },
        {
            "sha": "6d7d125035d158f42dfe052ae4d3bc75d7d9256c",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/cpu_fusion_emitter_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -124,7 +124,6 @@ TEST_F(CpuFusionEmitterTest, ScatterMlir) {\n   TF_ASSERT_OK_AND_ASSIGN(auto hlo_module,\n                           ParseAndReturnVerifiedModule(kScatterHlo));\n   auto& debug_options = hlo_module->mutable_config().mutable_debug_options();\n-  debug_options.set_xla_cpu_use_thunk_runtime(true);\n   debug_options.set_xla_cpu_use_fusion_emitters(true);\n   TF_ASSERT_OK_AND_ASSIGN(auto buffer_assignment,\n                           RunBufferAssignment(*hlo_module));\n@@ -152,7 +151,6 @@ TEST_F(CpuFusionEmitterTest, ScatterLlvm) {\n   TF_ASSERT_OK_AND_ASSIGN(auto hlo_module,\n                           ParseAndReturnVerifiedModule(kScatterHlo));\n   auto& debug_options = hlo_module->mutable_config().mutable_debug_options();\n-  debug_options.set_xla_cpu_use_thunk_runtime(true);\n   debug_options.set_xla_cpu_use_fusion_emitters(true);\n   debug_options.set_xla_cpu_prefer_vector_width(512);\n   TF_ASSERT_OK_AND_ASSIGN(auto buffer_assignment,"
        },
        {
            "sha": "7aaeb57cc5c9b4aede6cea8ccc0fea0a3f39b01f",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/transforms/add_loop_unroll_flags.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 41,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fadd_loop_unroll_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fadd_loop_unroll_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fadd_loop_unroll_flags.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -55,9 +55,9 @@ class AddLoopUnrollFlagsPass\n     mlir::func::FuncOp func_op = getOperation();\n     mlir::MLIRContext* context = func_op.getContext();\n \n-    llvm::DenseMap<mlir::scf::ForOp, int64_t> nested_iteration_bits;\n+    llvm::DenseMap<mlir::scf::ForOp, int64_t> nested_iteration_map;\n     func_op->walk<mlir::WalkOrder::PreOrder>([&](mlir::scf::ForOp for_op) {\n-      RecursiveWalk(for_op, nested_iteration_bits);\n+      RecursiveWalk(for_op, nested_iteration_map);\n       return mlir::WalkResult::skip();\n     });\n \n@@ -80,48 +80,19 @@ class AddLoopUnrollFlagsPass\n         /*endLoc=*/nullptr,\n         /*parallelAccesses=*/{});\n \n-    for (auto& [for_op, bits] : nested_iteration_bits) {\n-      if (bits >= max_nested_bits_) {\n+    for (auto& [for_op, nested_iterations] : nested_iteration_map) {\n+      if (nested_iterations > max_nested_iterations_) {\n         for_op->setAttr(mlir::LLVM::LoopAnnotationAttr::getMnemonic(),\n                         loop_annotation);\n       }\n     }\n   }\n \n  private:\n-  // Get the minimum element size in bits of any tensor extract/insert that use\n-  // the loops induction variable.\n-  static int64_t MinElementBits(mlir::scf::ForOp& for_op) {\n-    mlir::DataLayout data_layout = mlir::DataLayout::closest(for_op);\n-    std::optional<int64_t> min_element_bits;\n-\n-    auto update_min_element_bits = [&](mlir::Type type) {\n-      llvm::TypeSize size = data_layout.getTypeSizeInBits(type);\n-      int64_t element_bits = size.getFixedValue();\n-      if (!min_element_bits.has_value()) {\n-        min_element_bits = element_bits;\n-      } else if (element_bits < min_element_bits.value()) {\n-        min_element_bits = element_bits;\n-      }\n-    };\n-\n-    for_op.walk([&](mlir::Operation* op) {\n-      if (auto extract_op = mlir::dyn_cast<mlir::tensor::ExtractOp>(op)) {\n-        update_min_element_bits(extract_op.getResult().getType());\n-      }\n-\n-      if (auto insert_op = mlir::dyn_cast<mlir::tensor::InsertOp>(op)) {\n-        update_min_element_bits(insert_op.getScalar().getType());\n-      }\n-    });\n-\n-    return min_element_bits ? *min_element_bits : 0;\n-  }\n-\n   // Recursively insert the number of nested accessed bits for each loop.\n   static int64_t RecursiveWalk(\n       mlir::scf::ForOp for_op,\n-      llvm::DenseMap<mlir::scf::ForOp, int64_t>& nested_iteration_bits) {\n+      llvm::DenseMap<mlir::scf::ForOp, int64_t>& nested_iteration_map) {\n     auto lb = for_op.getLowerBound();\n     auto ub = for_op.getUpperBound();\n     auto step = for_op.getStep();\n@@ -133,17 +104,14 @@ class AddLoopUnrollFlagsPass\n       return 0;\n     }\n \n-    int64_t min_element_bits = MinElementBits(for_op);\n-\n     int64_t nested_iterations = 0;\n     for_op.getBody()->walk<mlir::WalkOrder::PreOrder>(\n         [&](mlir::scf::ForOp for_op) {\n-          nested_iterations += RecursiveWalk(for_op, nested_iteration_bits);\n+          nested_iterations += RecursiveWalk(for_op, nested_iteration_map);\n           return mlir::WalkResult::skip();\n         });\n \n-    nested_iteration_bits.insert(\n-        {for_op, nested_iterations * min_element_bits});\n+    nested_iteration_map.insert({for_op, nested_iterations});\n \n     if (nested_iterations == 0) {\n       return *this_trip_count;\n@@ -156,9 +124,9 @@ class AddLoopUnrollFlagsPass\n }  // namespace\n \n std::unique_ptr<mlir::Pass> CreateAddLoopUnrollFlagsPass(\n-    int32_t max_nested_bits) {\n+    int32_t max_nested_iterations) {\n   AddLoopUnrollFlagsPassOptions options;\n-  options.max_nested_bits_ = max_nested_bits;\n+  options.max_nested_iterations_ = max_nested_iterations;\n   return std::make_unique<AddLoopUnrollFlagsPass>(options);\n }\n "
        },
        {
            "sha": "e2cfc8188089cc6a10d6440b772e1f2b690e2771",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/transforms/lower_xla_shared.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Flower_xla_shared.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Flower_xla_shared.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Flower_xla_shared.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -117,29 +117,6 @@ struct LowerForall : mlir::OpRewritePattern<mlir::scf::ForallOp> {\n           return new_results;\n         });\n \n-    // Disable unrolling for all loops except the innermost.\n-    auto loop_unroll = rewriter.getAttr<mlir::LLVM::LoopUnrollAttr>(\n-        /*disable=*/rewriter.getBoolAttr(true), /*count=*/nullptr,\n-        /*runtimeDisable=*/nullptr,\n-        /*full=*/nullptr,\n-        /*followupUnrolled=*/nullptr, /*followupRemainder=*/nullptr,\n-        /*followupAll=*/nullptr);\n-    auto loop_annotation = rewriter.getAttr<mlir::LLVM::LoopAnnotationAttr>(\n-        /*disableNonforced=*/nullptr, /*vectorize=*/nullptr,\n-        /*interleave=*/nullptr, loop_unroll,\n-        /*unrollAndJam=*/nullptr, /*licm=*/nullptr, /*distribute=*/nullptr,\n-        /*pipeline=*/nullptr, /*peeled=*/nullptr, /*unswitch=*/nullptr,\n-        /*mustProgress=*/nullptr,\n-        /*isVectorized=*/nullptr,\n-        /*startLoc=*/nullptr,\n-        /*endLoc=*/nullptr,\n-        /*parallelAccesses=*/llvm::ArrayRef<mlir::LLVM::AccessGroupAttr>());\n-\n-    for (auto& for_op : absl::MakeSpan(loop_nest.loops).first(num_dims - 1)) {\n-      for_op->setAttr(mlir::LLVM::LoopAnnotationAttr::getMnemonic(),\n-                      loop_annotation);\n-    }\n-\n     rewriter.replaceOp(op, loop_nest.results);\n     return mlir::success();\n   }"
        },
        {
            "sha": "da7ce28b20608feeb00c4f376acf2084881f60b5",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/transforms/passes.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fpasses.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -31,7 +31,7 @@ std::unique_ptr<mlir::Pass> CreateLowerXlaSharedPass();\n std::unique_ptr<mlir::Pass> CreateExpandFloatOpsPass();\n std::unique_ptr<mlir::Pass> CreateAddReductionFastMathFlagsPass();\n std::unique_ptr<mlir::Pass> CreateAddLoopUnrollFlagsPass(\n-    int32_t max_nested_bits = 256);\n+    int32_t max_nested_iterations = 1);\n std::unique_ptr<mlir::Pass> CreatePeelWorkgroupLoopPass();\n \n #define GEN_PASS_REGISTRATION"
        },
        {
            "sha": "15a01ec923b039c05c5dddcee77e375ac0e3e6e6",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/transforms/passes.td",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Fpasses.td?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -102,8 +102,8 @@ def AddLoopUnrollFlagsPass : Pass<\"xla-cpu-add-loop-unroll-flags\", \"mlir::func::\n   ];\n \n   let options = [\n-    Option<\"max_nested_bits_\", \"max_nested_bits\", \"int32_t\", \"256\",\n-           \"The maximum number of bits accessed in a nested loop before disabling unrolling\">,\n+    Option<\"max_nested_iterations_\", \"max_nested_iterations\", \"int32_t\", \"1\",\n+           \"The maximum number of iterations accessed in a nested loop before disabling unrolling\">,\n   ];\n \n   let constructor = \"CreateAddLoopUnrollFlagsPass()\";"
        },
        {
            "sha": "d3367b86b33e89aa64abb76194267769490be3f3",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/transforms/tests/add_loop_unroll_flags.mlir",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Fadd_loop_unroll_flags.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Fadd_loop_unroll_flags.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Fadd_loop_unroll_flags.mlir?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -9,10 +9,7 @@ func.func @nested_for(%arg : tensor<16x16x8xf32>) -> () {\n   scf.for %iter0 = %c0 to %c16 step %c1 iter_args(%res0 = %arg) -> tensor<16x16x8xf32> {\n     scf.for %iter1 = %c0 to %c16 step %c1 iter_args(%res1 = %res0) -> tensor<16x16x8xf32> {\n       scf.for %iter2 = %c0 to %c8 step %c1 iter_args(%res2 = %res1) -> tensor<16x16x8xf32> {\n-        // Ensure this still works when IV is used indirectly.\n-        %c_0 = arith.constant 0 : index\n-        %iter1_plus_zero = arith.addi %iter1, %c_0 : index\n-        %extracted = tensor.extract %res2[%iter0, %iter1_plus_zero, %iter2] : tensor<16x16x8xf32>\n+        %extracted = tensor.extract %res2[%iter0, %iter1, %iter2] : tensor<16x16x8xf32>\n         scf.yield %res2 : tensor<16x16x8xf32>\n       }\n       scf.yield %res1 : tensor<16x16x8xf32>"
        },
        {
            "sha": "6abdda5ad78ccfa4eea1710fa44408556ec421cd",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/transforms/tests/lower_xla_shared.mlir",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Flower_xla_shared.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Flower_xla_shared.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Flower_xla_shared.mlir?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -14,8 +14,6 @@ func.func @forall_op(%input: tensor<1024x32x2xf32>) -> (tensor<1024x32x2xf32>) {\n   }\n   func.return %double : tensor<1024x32x2xf32>\n }\n-// CHECK: #[[LOOP_UNROLL:.*]] = #llvm.loop_unroll<disable = true>\n-// CHECK: #[[LOOP_ANNOTATION:.*]] = #llvm.loop_annotation<unroll = #[[LOOP_UNROLL]]>\n // CHECK-DAG: [[CONST_0:%.*]] = arith.constant 0 : index\n // CHECK-DAG: [[CONST_1:%.*]] = arith.constant 1 : index\n // CHECK-DAG: [[CONST_2:%.*]] = arith.constant 2 : index\n@@ -29,7 +27,7 @@ func.func @forall_op(%input: tensor<1024x32x2xf32>) -> (tensor<1024x32x2xf32>) {\n // CHECK-NEXT: }\n // CHECK-NOT: loop_annotation\n // CHECK: scf.yield\n-// CHECK-NEXT: } {loop_annotation = #[[LOOP_ANNOTATION]]}\n+// CHECK-NEXT: }\n // CHECK: scf.yield\n-// CHECK-NEXT: } {loop_annotation = #[[LOOP_ANNOTATION]]}\n+// CHECK-NEXT: }\n "
        },
        {
            "sha": "b51dc944b7840d3c22dd4bfdea6dc07443c0dc50",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_compiler.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 1,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -28,8 +28,13 @@ limitations under the License.\n #include \"absl/strings/str_join.h\"\n #include \"absl/strings/string_view.h\"\n #include \"llvm/ADT/StringRef.h\"\n+#include \"llvm/IR/BasicBlock.h\"\n+#include \"llvm/IR/FMF.h\"\n+#include \"llvm/IR/Function.h\"\n+#include \"llvm/IR/Instruction.h\"\n #include \"llvm/IR/Metadata.h\"\n #include \"llvm/IR/Module.h\"\n+#include \"llvm/Support/Casting.h\"\n #include \"mlir/Conversion/AffineToStandard/AffineToStandard.h\"\n #include \"mlir/Conversion/ComplexToStandard/ComplexToStandard.h\"\n #include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n@@ -56,6 +61,7 @@ limitations under the License.\n #include \"mlir/IR/Visitors.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Support/WalkResult.h\"\n #include \"mlir/Target/LLVMIR/Dialect/Builtin/BuiltinToLLVMIRTranslation.h\"\n #include \"mlir/Target/LLVMIR/Dialect/LLVMIR/LLVMToLLVMIRTranslation.h\"\n #include \"mlir/Target/LLVMIR/Export.h\"\n@@ -132,7 +138,6 @@ static void AddLoopTransformationPasses(mlir::OpPassManager& pm,\n   pm.addPass(mlir::mhlo::createConvertToSignlessPass());\n   pm.addPass(emitters::CreatePropagateSliceIndicesPass());\n   pm.addPass(emitters::CreateFlattenTensorsPass());\n-  pm.addPass(emitters::createPropagateAliasScopesPass());\n   // We need LICM before unswitching loops, because our loop unswitcher only\n   // detects for loops with a single if inside them.\n   pm.addPass(mlir::createLoopInvariantCodeMotionPass());\n@@ -146,6 +151,7 @@ static void AddLoopTransformationPasses(mlir::OpPassManager& pm,\n   //     emitters::CreateVectorizeLoadsAndStoresPass(/*target_type=*/\"cpu\"));\n   pm.addPass(mlir::createCanonicalizerPass());\n   pm.addPass(mlir::createCSEPass());\n+  pm.addNestedPass<mlir::func::FuncOp>(CreateAddLoopUnrollFlagsPass());\n }\n \n static void AddLoweringPasses(mlir::OpPassManager& pm, int32_t vector_width,\n@@ -200,6 +206,19 @@ static int GetLlvmFunctionDefCount(mlir::ModuleOp m) {\n   return count;\n };\n \n+static void ApplyFastMathFlags(llvm::Module& llvm_module,\n+                               const llvm::FastMathFlags& fast_math_flags) {\n+  for (llvm::Function& function : llvm_module) {\n+    for (llvm::BasicBlock& basic_block : function) {\n+      for (llvm::Instruction& instruction : basic_block) {\n+        if (llvm::isa<llvm::FPMathOperator>(instruction)) {\n+          instruction.setFastMathFlags(fast_math_flags);\n+        }\n+      }\n+    }\n+  }\n+}\n+\n FusionCompiler::FusionCompiler(mlir::MLIRContext* context, Options options,\n                                CompilationHooks hooks)\n     : options_(std::move(options)),\n@@ -298,6 +317,10 @@ absl::StatusOr<std::unique_ptr<llvm::Module>> FusionCompiler::Compile(\n \n   llvm_module->setDataLayout(llvm_module->getDataLayout());\n \n+  if (options_.fast_math_flags.any()) {\n+    ApplyFastMathFlags(*llvm_module, options_.fast_math_flags);\n+  }\n+\n   return llvm_module;\n }\n "
        },
        {
            "sha": "434a25bb634ed4ec549343d641e18458e897917f",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_compiler.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n \n #include \"absl/functional/any_invocable.h\"\n #include \"absl/status/statusor.h\"\n+#include \"llvm/IR/FMF.h\"\n #include \"llvm/IR/LLVMContext.h\"\n #include \"llvm/IR/Module.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n@@ -46,6 +47,7 @@ class FusionCompiler {\n     int32_t vector_width;\n     int32_t verification_level;\n     bool fast_min_max;\n+    llvm::FastMathFlags fast_math_flags;\n   };\n \n   FusionCompiler(mlir::MLIRContext* context, Options options,"
        },
        {
            "sha": "00d5867b8dfdf60c9a8eddac0ca0234fdebc543b",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_emitter.cc",
            "status": "modified",
            "additions": 48,
            "deletions": 0,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -37,10 +37,12 @@ limitations under the License.\n #include \"xla/backends/cpu/codegen/kernel_api_ir_builder.h\"\n #include \"xla/backends/cpu/codegen/symbol_name_util.h\"\n #include \"xla/codegen/emitters/concatenate_kernel_emitter.h\"\n+#include \"xla/codegen/emitters/dynamic_update_slice_kernel_emitter.h\"\n #include \"xla/codegen/emitters/ir/xla_ops.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n #include \"xla/codegen/emitters/loop_kernel_emitter.h\"\n #include \"xla/codegen/hlo_fusion_spec.h\"\n+#include \"xla/codegen/ir_emission_utils.h\"\n #include \"xla/codegen/mlir_kernel_definition.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -179,6 +181,14 @@ static WorkDimensions GetConcatenateEmitterWorkDims(\n   return GetWorkDimensions(indexing_shape, fusion);\n }\n \n+static WorkDimensions GetDynamicUpdateSliceEmitterWorkDims(\n+    const HloFusionInstruction& fusion, const HloFusionSpec& fusion_spec) {\n+  Shape indexing_shape =\n+      emitters::DynamicUpdateSliceKernelEmitter::GetIndexingShape(fusion_spec);\n+\n+  return GetWorkDimensions(indexing_shape, fusion);\n+}\n+\n static HloFusionSpec GetLoopFusionSpec(const HloFusionInstruction& fusion) {\n   // Crash OK, this is checked in the caller.\n   CHECK(fusion.fusion_kind() == HloFusionInstruction::FusionKind::kLoop);\n@@ -249,6 +259,33 @@ static absl::StatusOr<MlirKernelDefinition> EmitConcatenateFusionKernel(\n   return MlirKernelDefinition(std::move(kernel_spec), std::move(kernel_source));\n }\n \n+static absl::StatusOr<MlirKernelDefinition> EmitDynamicUpdateSliceFusionKernel(\n+    mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n+    const BufferAssignment* buffer_assignment, absl::string_view name) {\n+  VLOG(2) << \"Emitting dynamic update slice fusion kernel: \" << name;\n+  HloFusionSpec fusion_spec = GetLoopFusionSpec(fusion);\n+  auto work_dimensions =\n+      GetDynamicUpdateSliceEmitterWorkDims(fusion, fusion_spec);\n+\n+  emitters::DynamicUpdateSliceKernelEmitter emitter(\n+      context, fusion, std::move(fusion_spec), buffer_assignment,\n+      GetDefaultBufferAlignment(), work_dimensions, name, BackendKind::kCpu);\n+  TF_ASSIGN_OR_RETURN(auto mlir_kernel_definition,\n+                      emitter.EmitKernelDefinition());\n+\n+  // We have to release otherwise the source wouldn't be mutable, and we\n+  // wouldn't be able to set the CpuMemoryRegionNameAttr.\n+  auto [kernel_spec, kernel_source] =\n+      std::move(mlir_kernel_definition).ReleaseStorage();\n+\n+  mlir::OpBuilder builder(&context);\n+  kernel_source.module().getOperation()->setAttr(\n+      xla::CpuMemoryRegionNameAttr::name,\n+      builder.getStringAttr(\n+          BuildModuleMemoryRegionName(emitter.name(), &fusion)));\n+  return MlirKernelDefinition(std::move(kernel_spec), std::move(kernel_source));\n+}\n+\n absl::StatusOr<MlirKernelDefinition> EmitFusionKernel(\n     mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n     const BufferAssignment* buffer_assignment, bool use_unique_c_name) {\n@@ -260,6 +297,17 @@ absl::StatusOr<MlirKernelDefinition> EmitFusionKernel(\n       return EmitConcatenateFusionKernel(context, fusion, buffer_assignment,\n                                          name);\n     }\n+    auto fusion_spec = GetLoopFusionSpec(fusion);\n+    if (IsDynamicUpdateSliceFusion(fusion_spec)) {\n+      TF_ASSIGN_OR_RETURN(\n+          bool dus_inplace,\n+          CanEmitFusedDynamicUpdateSliceInPlace(fusion_spec.fusion(),\n+                                                buffer_assignment, &fusion));\n+      if (dus_inplace) {\n+        return EmitDynamicUpdateSliceFusionKernel(context, fusion,\n+                                                  buffer_assignment, name);\n+      }\n+    }\n     return EmitLoopFusionKernel(context, fusion, buffer_assignment, name);\n   }\n "
        },
        {
            "sha": "288d5e6ce7bff9ad358b24cb6b9088c166635627",
            "filename": "third_party/xla/xla/backends/cpu/codegen/ir_compiler.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 3,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fir_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fir_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fir_compiler.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -66,6 +66,7 @@ limitations under the License.\n #include \"xla/backends/cpu/codegen/cpu_features.h\"\n #include \"xla/backends/cpu/codegen/kernel_api_ir_builder.h\"\n #include \"xla/backends/cpu/codegen/polynomial_approximations.h\"\n+#include \"xla/codegen/intrinsic/intrinsic.h\"\n #include \"xla/codegen/intrinsic/intrinsic_compiler_lib.h\"\n #include \"xla/codegen/intrinsic_lib.h\"\n #include \"xla/service/cpu/backend_config.pb.h\"\n@@ -345,10 +346,30 @@ llvm::Error IrCompiler::RunIrPasses(llvm::Module& module,\n       std::make_unique<llvm::TargetLibraryInfoImpl>(target_triple);\n   target_library_info_impl->addVectorizableFunctions(\n       PolynomialApproximationsVectorization());\n+\n+  xla::codegen::intrinsics::DeviceType device_type;\n+  if (target_triple.isX86()) {\n+    // As a heuristic, we check for SSE4a to determine if we are on AMD.\n+    // This feature was added in 2007 and is set on all AMD CPUs since then, and\n+    // no intel cpus. This is a bit of a hack though, as there is no strict link\n+    // between increased precision and SSE4a; Intel could decide to add it in\n+    // the future but they are very unlikely to do so as they haven't in the\n+    // past 18 years.\n+    if (target_machine->getTargetFeatureString().contains(\"+sse4a\")) {\n+      device_type = xla::codegen::intrinsics::DeviceType::kAmdCpu;\n+    } else {\n+      device_type = xla::codegen::intrinsics::DeviceType::kIntelCpu;\n+    }\n+  } else if (target_triple.isAArch64() || target_triple.isARM()) {\n+    device_type = xla::codegen::intrinsics::DeviceType::kArmCpu;\n+  } else {\n+    LOG(FATAL) << \"Unsupported CPU type: \" << target_triple.str();\n+  }\n+\n   codegen::IntrinsicFunctionLib intrinsic_lib(\n-      {target_machine->getTargetFeatureString().str(),\n-       /*disable_platform_dependent_math=*/options_\n-           .disable_platform_dependent_math});\n+      {target_machine->getTargetFeatureString().str(), device_type,\n+       /*disable_platform_dependent_math=*/\n+       options_.disable_platform_dependent_math});\n   target_library_info_impl->addVectorizableFunctions(\n       intrinsic_lib.Vectorizations());\n "
        },
        {
            "sha": "4c426a18c12cd6cc147da7613ae79c9a610be97e",
            "filename": "third_party/xla/xla/backends/cpu/codegen/target_machine_features.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftarget_machine_features.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftarget_machine_features.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftarget_machine_features.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -39,6 +39,8 @@ TargetMachineFeatures::TargetMachineFeatures(\n   if (target_machine_) {\n     has_avx512bf16_ = absl::StrContains(\n         target_machine_->getTargetFeatureString().str(), \"+avx512bf16\");\n+    has_avx512fp16_ = absl::StrContains(\n+        target_machine_->getTargetFeatureString().str(), \"+avx512fp16\");\n   }\n }\n "
        },
        {
            "sha": "8725d22d4fddb990c3c03e1f6c045bf1e82a41c3",
            "filename": "third_party/xla/xla/backends/cpu/codegen/target_machine_features.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftarget_machine_features.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftarget_machine_features.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftarget_machine_features.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -72,6 +72,7 @@ class TargetMachineFeatures {\n   virtual std::string get_target_feature_string() const;\n \n   virtual bool has_avx512bf16() const { return has_avx512bf16_; }\n+  virtual bool has_avx512fp16() const { return has_avx512fp16_; }\n \n  private:\n   llvm::TargetTransformInfo* GetTargetTransformInfoFor(\n@@ -85,6 +86,7 @@ class TargetMachineFeatures {\n \n   // Store availability of popular features here for efficient checks.\n   bool has_avx512bf16_ = false;\n+  bool has_avx512fp16_ = false;\n };\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "54757c444b77d4eaad62b47bffb60e9555b5066c",
            "filename": "third_party/xla/xla/backends/cpu/codegen/target_machine_features_test.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 16,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftarget_machine_features_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftarget_machine_features_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftarget_machine_features_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -26,42 +26,44 @@ limitations under the License.\n namespace xla::cpu {\n namespace {\n \n-struct Avx512Bf16TestSpec {\n+struct Avx512Bf16Fp16TestSpec {\n   std::string cpu_name;\n   std::string features;\n   bool has_avx512bf16;\n+  bool has_avx512fp16;\n };\n \n-class Avx512Bf16Test\n+class Avx512Bf16Fp16Test\n     : public TargetMachineTestBase,\n-      public ::testing::WithParamInterface<Avx512Bf16TestSpec> {\n+      public ::testing::WithParamInterface<Avx512Bf16Fp16TestSpec> {\n  public:\n   static std::string Name(\n-      const ::testing::TestParamInfo<Avx512Bf16TestSpec>& info) {\n+      const ::testing::TestParamInfo<Avx512Bf16Fp16TestSpec>& info) {\n     return info.param.cpu_name;\n   }\n };\n \n-TEST_P(Avx512Bf16Test, CheckAvailability) {\n-  Avx512Bf16TestSpec spec = GetParam();\n+TEST_P(Avx512Bf16Fp16Test, CheckAvailability) {\n+  Avx512Bf16Fp16TestSpec spec = GetParam();\n   const char* triple_string = \"x86_64-unknown-linux-gnu\";\n   std::unique_ptr<TargetMachineFeatures> features =\n       CreateTargetMachineFeatures(triple_string, spec.cpu_name, spec.features);\n   EXPECT_EQ(features->has_avx512bf16(), spec.has_avx512bf16);\n+  EXPECT_EQ(features->has_avx512fp16(), spec.has_avx512fp16);\n }\n \n-std::vector<Avx512Bf16TestSpec> GetAvx512Bf16TestSpecs() {\n-  return std::vector<Avx512Bf16TestSpec>{\n-      Avx512Bf16TestSpec{\"znver3\", \"+avx,+avx2\", false},\n-      Avx512Bf16TestSpec{\"sapphirerapids\",\n-                         \"+avx512vnni,+avx512bf16,+amx-bf16,+amx-int8,\"\n-                         \"+amx-tile,+amx-transpose\",\n-                         true}};\n+std::vector<Avx512Bf16Fp16TestSpec> GetAvx512Bf16Fp16TestSpecs() {\n+  return std::vector<Avx512Bf16Fp16TestSpec>{\n+      Avx512Bf16Fp16TestSpec{\"znver3\", \"+avx,+avx2\", false, false},\n+      Avx512Bf16Fp16TestSpec{\"sapphirerapids\",\n+                             \"+avx512vnni,+avx512bf16,+avx512fp16,+amx-bf16,\"\n+                             \"+amx-int8,+amx-tile,+amx-transpose\",\n+                             true, true}};\n }\n \n-INSTANTIATE_TEST_SUITE_P(Avx512Bf16Suite, Avx512Bf16Test,\n-                         ::testing::ValuesIn(GetAvx512Bf16TestSpecs()),\n-                         Avx512Bf16Test::Name);\n+INSTANTIATE_TEST_SUITE_P(Avx512Bf16Fp16Suite, Avx512Bf16Fp16Test,\n+                         ::testing::ValuesIn(GetAvx512Bf16Fp16TestSpecs()),\n+                         Avx512Bf16Fp16Test::Name);\n \n }  // namespace\n }  // namespace xla::cpu"
        },
        {
            "sha": "a3dccc67632c6f09f0bf9fb7135075916a82ce85",
            "filename": "third_party/xla/xla/backends/cpu/nanort/ifrt_client.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 3,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -51,6 +51,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n #include \"xla/layout.h\"\n+#include \"xla/layout_util.h\"\n #include \"xla/pjrt/mlir_to_hlo.h\"\n #include \"xla/pjrt/pjrt_compiler.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n@@ -412,7 +413,8 @@ class NanoArray final : public NanoValue<NanoArray, ifrt::Array> {\n   absl::StatusOr<std::shared_ptr<const PjRtLayout>> pjrt_layout()\n       const override {\n     TF_RETURN_IF_ERROR(ValidateNotDeleted());\n-    return std::make_shared<PjRtLayout>(Layout(shape().dims()));\n+    return std::make_shared<PjRtLayout>(\n+        LayoutUtil::MakeDescendingLayout(shape().dims().size()));\n   }\n \n   absl::StatusOr<std::vector<ifrt::ArrayRef>> DisassembleIntoSingleDeviceArrays(\n@@ -1382,6 +1384,13 @@ absl::StatusOr<std::vector<ifrt::ArrayRef>> NanoIfrtClient::RemapArrays(\n   return absl::UnimplementedError(\"RemapArrays is not implemented.\");\n }\n \n+absl::StatusOr<std::vector<xla::ifrt::ArrayRef>> NanoIfrtClient::ReshardArrays(\n+    absl::Span<xla::ifrt::ArrayRef> arrays,\n+    absl::Span<const xla::ifrt::ArraySpec> specs,\n+    xla::ifrt::ArrayCopySemantics semantics) {\n+  return absl::UnimplementedError(\"ReshardArrays is not implemented.\");\n+}\n+\n ifrt::Future<> NanoIfrtClient::GetReadyFuture(\n     absl::Span<const ifrt::ValueRef> values) {\n   return Ready();\n@@ -1403,7 +1412,10 @@ ifrt::PlatformId NanoIfrtClient::platform_id() const {\n }\n \n const ifrt::AttributeMap& NanoIfrtClient::Attributes() const {\n-  static auto attributes = new ifrt::AttributeMap({});\n+  static auto attributes = new ifrt::AttributeMap({\n+      {\"supports_executable_serialization\",\n+       ifrt::AttributeMap::BoolValue(false)},\n+  });\n   return *attributes;\n }\n \n@@ -1482,7 +1494,8 @@ NanoIfrtClient::GetDefaultPjRtLayout(ifrt::DType dtype,\n                                      absl::Span<const int64_t> dims,\n                                      ifrt::Device* device,\n                                      ifrt::MemoryKind memory_kind) const {\n-  return std::make_shared<PjRtLayout>(Layout(dims));\n+  return std::make_shared<PjRtLayout>(\n+      LayoutUtil::MakeDescendingLayout(dims.size()));\n }\n \n NanoIfrtClient::NanoIfrtClient(int32_t num_devices,"
        },
        {
            "sha": "a34e4c9b9c459a1e7cd0f5d45d5307b1adc1590a",
            "filename": "third_party/xla/xla/backends/cpu/nanort/ifrt_client.h",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -147,6 +147,11 @@ class NanoIfrtClient : public llvm::RTTIExtends<NanoIfrtClient, ifrt::Client> {\n       const ifrt::RemapPlan& plan, absl::Span<ifrt::ArrayRef> arrays,\n       ifrt::ArrayCopySemantics semantics) override;\n \n+  absl::StatusOr<std::vector<xla::ifrt::ArrayRef>> ReshardArrays(\n+      absl::Span<xla::ifrt::ArrayRef> arrays,\n+      absl::Span<const xla::ifrt::ArraySpec> specs,\n+      xla::ifrt::ArrayCopySemantics semantics) override;\n+\n   ifrt::Future<> GetReadyFuture(\n       absl::Span<const ifrt::ValueRef> values) override;\n "
        },
        {
            "sha": "19e275a410c24f23600c89fdea1ad7c65737083b",
            "filename": "third_party/xla/xla/backends/cpu/nanort/ifrt_client_test.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 1,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fnanort%2Fifrt_client_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -320,7 +320,21 @@ int main(int argc, char** argv) {\n       \"ArrayImplTest.\"\n       \"MakeArraysFromHostBufferShardsAndCopyToHostBufferWithString:\"\n       // `MakeErrorArrays` is not supported in NanoIfrtClient.\n-      \"ArrayImplTest.MakeErrorArrays\";\n+      \"ArrayImplTest.MakeErrorArrays:\"\n+      \"ArrayImplTest.CopyPoisonedArray:\"\n+      // Sub-byte types are not supported in NanoIfrtClient.\n+      \"ArrayImplTest.HostBufferInt4:\"\n+      \"ArrayImplTest.CopyArraysSubByteDType:\"\n+      // NanoRT does not handle zero-sized buffers correctly.\n+      \"ArrayImplTest.MakeAndCopyZeroSizedBuffers:\"\n+      // Executable returns a wrong number of devices.\n+      \"*LoadedExecutableImplTest.Properties*:\"\n+      // Incorrect deleted state of donated inputs.\n+      \"*LoadedExecutableImplTest.Donation*:\"\n+      // Analysis methods are not implemented.\n+      \"*LoadedExecutableImplTest.Analysis*:\"\n+      // Serialization is not implemented.\n+      \"*SerializeAndLoad*\";\n   xla::ifrt::test_util::SetTestFilterIfNotUserSpecified(kFilter);\n \n   for (int i = 1; i < argc; i++) {"
        },
        {
            "sha": "3789386749862b99011c16ee38405fb02186f051",
            "filename": "third_party/xla/xla/backends/cpu/onednn_emitter.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 5,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fonednn_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fonednn_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fonednn_emitter.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"xla/backends/cpu/onednn_fusion.h\"\n #include \"xla/backends/cpu/onednn_support.h\"\n+#include \"xla/backends/cpu/runtime/dot_lib.h\"\n #include \"xla/backends/cpu/runtime/onednn/onednn_interop.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -51,6 +52,12 @@ static absl::StatusOr<dnnl::graph::logical_tensor::data_type> OneDnnDatatype(\n   switch (type) {\n     case F32:\n       return dnnl::graph::logical_tensor::data_type::f32;\n+    case F16:\n+      return dnnl::graph::logical_tensor::data_type::f16;\n+    case BF16:\n+      return dnnl::graph::logical_tensor::data_type::bf16;\n+    case PRED:\n+      return dnnl::graph::logical_tensor::data_type::boolean;\n     default:\n       return InvalidArgument(\"Unsupported oneDNN data type: %s\",\n                              primitive_util::LowercasePrimitiveTypeName(type));\n@@ -125,11 +132,11 @@ static absl::StatusOr<dnnl::graph::logical_tensor> CreateLogicalTensor(\n }\n \n static absl::StatusOr<dnnl::graph::logical_tensor> DefineParameter(\n-    const HloInstruction* param) {\n+    LogicalTensorMap& logical_tensors, const HloInstruction* param) {\n   VLOG(3) << absl::StreamFormat(\"Define logical tensor for parameter: %s\",\n                                 param->ToString());\n-\n-  return CreateLogicalTensor(param->parameter_number(), param->shape());\n+  size_t id = logical_tensors.size();\n+  return CreateLogicalTensor(id, param->shape());\n }\n \n static absl::StatusOr<dnnl::graph::logical_tensor> DefineUnaryOp(\n@@ -185,7 +192,7 @@ static absl::StatusOr<dnnl::graph::logical_tensor> DefineBinaryOp(\n static absl::StatusOr<dnnl::graph::logical_tensor> DefineMatMul(\n     dnnl::graph::graph& graph, size_t op_id, LogicalTensorMap& logical_tensors,\n     const HloInstruction* instr) {\n-  // Verify that this Dot is supported by XNNPACK.\n+  // Verify that this Dot is supported by oneDNN.\n   const DotDimensionNumbers& dnums = instr->dot_dimension_numbers();\n   const Shape& lhs_shape = instr->operand(0)->shape();\n   const Shape& rhs_shape = instr->operand(1)->shape();\n@@ -215,6 +222,19 @@ static absl::StatusOr<dnnl::graph::logical_tensor> DefineMatMul(\n                                 lhs.get_id(), rhs.get_id(), output.get_id());\n \n   dnnl::graph::op op(op_id, matmul_op, {lhs, rhs}, {output});\n+\n+  TF_ASSIGN_OR_RETURN(DotShape dot_shape,\n+                      GetDotShape(dnums, lhs_shape, rhs_shape, instr->shape()));\n+  TF_ASSIGN_OR_RETURN(DotCanonicalDims dot_canonical_dims,\n+                      GetDotCanonicalDims(dnums, dot_shape));\n+\n+  if (!dot_canonical_dims.lhs_canonical) {\n+    op.set_attr<bool>(dnnl::graph::op::attr::transpose_a, true);\n+  }\n+  if (!dot_canonical_dims.rhs_canonical) {\n+    op.set_attr<bool>(dnnl::graph::op::attr::transpose_b, true);\n+  }\n+\n   ONEDNN_RETURN_IF_ERROR(graph.add_op(op));\n \n   return output;\n@@ -240,7 +260,8 @@ static absl::StatusOr<OneDnnFusion> EmitOneDnnFusion(\n   for (const HloInstruction* instr : instructions) {\n     switch (instr->opcode()) {\n       case HloOpcode::kParameter: {\n-        TF_ASSIGN_OR_RETURN(logical_tensors[instr], DefineParameter(instr));\n+        TF_ASSIGN_OR_RETURN(logical_tensors[instr],\n+                            DefineParameter(logical_tensors, instr));\n       } break;\n \n       // Unary elementwise ops."
        },
        {
            "sha": "396e681bcc700ddafd863948236747c91cf85e3d",
            "filename": "third_party/xla/xla/backends/cpu/onednn_support.cc",
            "status": "modified",
            "additions": 78,
            "deletions": 11,
            "changes": 89,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fonednn_support.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fonednn_support.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fonednn_support.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -15,31 +15,98 @@ limitations under the License.\n \n #include \"xla/backends/cpu/onednn_support.h\"\n \n+#include \"absl/log/log.h\"\n #include \"absl/status/statusor.h\"\n+#include \"dnnl.hpp\"  // NOLINT: for DNNL_MAX_NDIMS\n+#include \"xla/backends/cpu/codegen/target_machine_features.h\"\n #include \"xla/backends/cpu/runtime/dot_lib.h\"\n #include \"xla/shape.h\"\n #include \"xla/xla_data.pb.h\"\n+#include \"tsl/platform/cpu_info.h\"\n \n namespace xla::cpu {\n \n+bool IsOneDnnSupportedDType(PrimitiveType dtype) {\n+  using tsl::port::CPUFeature;\n+  switch (dtype) {\n+    case F32:\n+      return true;\n+    case BF16:\n+      return TestCPUFeature(CPUFeature::AVX512F) ||\n+             TestCPUFeature(CPUFeature::AVX_NE_CONVERT) ||\n+             TestCPUFeature(CPUFeature::AMX_BF16);\n+    case F16:\n+      return (TestCPUFeature(CPUFeature::AVX512BW) &&\n+              (TestCPUFeature(CPUFeature::AVX512_FP16) ||\n+               TestCPUFeature(CPUFeature::AMX_FP16))) ||\n+             TestCPUFeature(CPUFeature::AVX_NE_CONVERT);\n+    default:\n+      return false;\n+  }\n+}\n+\n+bool IsOneDnnSupportedDType(PrimitiveType dtype,\n+                            const TargetMachineFeatures* cpu_features) {\n+  if (dtype == F32) {\n+    return true;\n+  }\n+\n+  if (cpu_features == nullptr) {\n+    return IsOneDnnSupportedDType(dtype);\n+  }\n+\n+  if (dtype == BF16) {\n+    return cpu_features->has_avx512bf16();\n+  }\n+  if (dtype == F16) {\n+    return cpu_features->has_avx512fp16();\n+  }\n+\n+  return false;\n+}\n+\n absl::StatusOr<bool> IsOneDnnDotSupported(\n     const DotDimensionNumbers& dot_dimensions, const Shape& lhs_shape,\n-    const Shape& rhs_shape, const Shape& out_shape) {\n-  // TODO(penporn): Support other element types.\n-  if (lhs_shape.element_type() != F32 || rhs_shape.element_type() != F32 ||\n-      out_shape.element_type() != F32) {\n+    const Shape& rhs_shape, const Shape& out_shape,\n+    const TargetMachineFeatures* cpu_features) {\n+  if (lhs_shape.element_type() != rhs_shape.element_type() ||\n+      lhs_shape.element_type() != out_shape.element_type()) {\n+    return false;\n+  }\n+  if (!IsOneDnnSupportedDType(out_shape.element_type(), cpu_features)) {\n+    return false;\n+  }\n+\n+  if (ShapeUtil::IsZeroElementArray(lhs_shape) ||\n+      ShapeUtil::IsZeroElementArray(rhs_shape) ||\n+      ShapeUtil::IsZeroElementArray(out_shape)) {\n+    return false;\n+  }\n+\n+  // NOLINTNEXTLINE: Use dnnl.hpp for DNNL_MAX_NDIMS for now.\n+  if (lhs_shape.dimensions_size() > DNNL_MAX_NDIMS ||\n+      rhs_shape.dimensions_size() > DNNL_MAX_NDIMS ||\n+      lhs_shape.dimensions_size() != rhs_shape.dimensions_size()) {\n     return false;\n   }\n \n-  TF_ASSIGN_OR_RETURN(DotShape dot_shape, GetDotShape(dot_dimensions, lhs_shape,\n-                                                      rhs_shape, out_shape));\n+  auto dot_shape_result =\n+      GetDotShape(dot_dimensions, lhs_shape, rhs_shape, out_shape);\n+  if (!dot_shape_result.ok()) {\n+    VLOG(2) << \"GetDotShape Error: \" << dot_shape_result.status();\n+    return false;\n+  }\n+  DotShape dot_shape = dot_shape_result.value();\n \n-  TF_ASSIGN_OR_RETURN(DotCanonicalDims dot_canonical_dims,\n-                      GetDotCanonicalDims(dot_dimensions, dot_shape));\n+  auto dot_canonical_result = GetDotCanonicalDims(dot_dimensions, dot_shape);\n+  if (!dot_canonical_result.ok()) {\n+    VLOG(2) << \"GetDotCanonicalDims Error: \" << dot_canonical_result.status();\n+    return false;\n+  }\n+  DotCanonicalDims dot_canonical_dims = dot_canonical_result.value();\n \n-  // Restrict support to no transposes and row-major layouts for now.\n-  return dot_canonical_dims.lhs_canonical && dot_canonical_dims.rhs_canonical &&\n-         !dot_canonical_dims.lhs_column_major &&\n+  // Restrict support to row-major layouts.\n+  return !dot_canonical_dims.lhs_column_major &&\n          !dot_canonical_dims.rhs_column_major;\n }\n "
        },
        {
            "sha": "3903950609085562ab8a621e072cf9d86de25a53",
            "filename": "third_party/xla/xla/backends/cpu/onednn_support.h",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fonednn_support.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fonednn_support.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fonednn_support.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -21,18 +21,24 @@ limitations under the License.\n \n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"xla/backends/cpu/codegen/target_machine_features.h\"\n #include \"xla/shape.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla::cpu {\n \n inline constexpr absl::string_view kOneDnnFusionKind = \"__onednn_fusion\";\n \n+bool IsOneDnnSupportedDType(PrimitiveType dtype);\n+bool IsOneDnnSupportedDType(PrimitiveType dtype,\n+                            const TargetMachineFeatures* cpu_features);\n+\n // Returns true if the dot operation is supported by oneDNN. Returns an error\n // if the dot operation shape is invalid.\n absl::StatusOr<bool> IsOneDnnDotSupported(\n     const DotDimensionNumbers& dot_dimensions, const Shape& lhs_shape,\n-    const Shape& rhs_shape, const Shape& out_shape);\n+    const Shape& rhs_shape, const Shape& out_shape,\n+    const TargetMachineFeatures* cpu_features = nullptr);\n \n }  // namespace xla::cpu\n "
        },
        {
            "sha": "8fc02e0fbb75eb9f5b254e7f318e53f83e8c6223",
            "filename": "third_party/xla/xla/backends/cpu/runtime/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1213,7 +1213,6 @@ cc_library(\n     hdrs = [\"work_queue.h\"],\n     deps = [\n         \"//xla/tsl/concurrency:async_value\",\n-        \"//xla/tsl/lib/math:math_util\",\n         \"//xla/tsl/platform:logging\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/container:fixed_array\","
        },
        {
            "sha": "feba693e0bdd39cb2da825a82e9fd65cd6df8832",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -90,6 +90,7 @@ onednn_graph_cc_library(\n onednn_graph_cc_test(\n     name = \"onednn_threadpool_test\",\n     srcs = [\"onednn_threadpool_test.cc\"],\n+    copts = tsl_copts(),\n     deps = [\n         \":onednn_interop\",\n         \":onednn_threadpool\","
        },
        {
            "sha": "f2a4faf2c9a4c236bcde5ce2e63e5844941296b7",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_threadpool.h",
            "status": "modified",
            "additions": 62,
            "deletions": 13,
            "changes": 75,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_threadpool.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_threadpool.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_threadpool.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <cstdint>\n #include <functional>\n \n+#include \"dnnl_threadpool.hpp\"\n #include \"oneapi/dnnl/dnnl_threadpool_iface.hpp\"\n #include \"xla/backends/cpu/runtime/work_queue.h\"\n \n@@ -28,34 +29,72 @@ limitations under the License.\n \n namespace xla::cpu {\n \n+static tsl::AsyncValueRef<tsl::Chain> OkDoneEventSingleton() {\n+  static tsl::AsyncValueOwningRef<tsl::Chain>* singleton = [] {\n+    auto* storage = new tsl::internal::AsyncValueStorage<tsl::Chain>();\n+    return new tsl::AsyncValueOwningRef<tsl::Chain>(\n+        tsl::MakeAvailableAsyncValueRef<tsl::Chain>(*storage));\n+  }();\n+  return singleton->AsRef();\n+}\n+\n class OneDnnThreadPool final\n     : public dnnl::threadpool_interop::threadpool_iface {\n  public:\n-  explicit OneDnnThreadPool(Eigen::ThreadPoolInterface* thread_pool)\n-      : thread_pool_(thread_pool) {}\n+  explicit OneDnnThreadPool(Eigen::ThreadPoolInterface* thread_pool,\n+                            bool is_async = false)\n+      : thread_pool_(thread_pool), is_async_(is_async) {\n+    if (is_async_) {\n+      done_event_ = OkDoneEventSingleton();\n+      dnnl_threadpool_interop_set_max_concurrency(thread_pool_->NumThreads());\n+    }\n+  }\n \n   int get_num_threads() const final { return thread_pool_->NumThreads(); }\n \n   bool get_in_parallel() const final {\n+    if (is_async_) {\n+      // TODO(intel-tf): this is a temporary fix without which oneDNN runs\n+      // single-threaded.\n+      return false;\n+    }\n     return thread_pool_->CurrentThreadId() >= 0;\n   }\n \n-  uint64_t get_flags() const final { return 0; }\n+  uint64_t get_flags() const final { return is_async_ ? ASYNCHRONOUS : 0; }\n \n #ifdef ENABLE_ONEDNN_ASYNC\n-  // This is a placeholder implementation for the wait method, as we\n-  // need to satisfy the interface requirements of the\n-  // dnnl::threadpool_interop::threadpool_iface with the experimental\n-  // asynchronous runtime support in oneDNN.\n-  // TODO(intel-tf): Implement proper wait logic when thunk runtime\n-  // with oneDNN is enabled.\n-  void wait() final {}\n+  // The wait() method only exists with oneDNN's experimental support for\n+  // asynchronous execution determined by the ENABLE_ONEDNN_ASYNC.\n+  void wait() override {\n+    if (is_async_) {\n+      // While performing asynchronous execution, wait() method is needed to\n+      // notify the user that the output is ready. oneDNN will not call wait()\n+      // inside the library to avoid deadlock.\n+      tsl::BlockUntilReady(done_event_);\n+    }\n+  }\n #endif  // ENABLE_ONEDNN_ASYNC\n \n   void parallel_for(int n, const std::function<void(int, int)>& fn) final {\n-    // It is perfectly safe to block here as Worker implements work stealing\n-    // that guarantees forward progress and deadlock freedom, even if we are\n-    // running in the same thread pool as the Eigen thread_pool.\n+    if (is_async_) {\n+      // If we are using oneDNN with async support, we need to schedule the\n+      // parallel loop using the done_event_. This allows us to return\n+      // immediately and not block the caller thread.\n+      auto parallelize = [this, n, fn](tsl::Chain) {\n+        return Worker::Parallelize(\n+            thread_pool_, thread_pool_->NumThreads(), n,\n+            [fn, n](size_t i) { fn(static_cast<int>(i), n); });\n+      };\n+\n+      done_event_ = done_event_.FlatMap(parallelize);\n+      return;\n+    }\n+\n+    // If we are not using oneDNN with async support, it is perfectly safe to\n+    // block here as Worker implements work stealing that guarantees forward\n+    // progress and deadlock freedom, even if we are running in the same thread\n+    // pool as the Eigen thread_pool.\n     tsl::BlockUntilReady(Worker::Parallelize(thread_pool_,\n                                              thread_pool_->NumThreads(), n,\n                                              [fn, n](size_t i) { fn(i, n); }));\n@@ -65,8 +104,18 @@ class OneDnnThreadPool final\n     thread_pool_ = thread_pool;\n   }\n \n+  tsl::AsyncValueRef<tsl::Chain> done_event() const { return done_event_; }\n+\n  private:\n   Eigen::ThreadPoolInterface* thread_pool_;\n+\n+  // Indicates if we are using oneDNN with async support. TODO(intel-tf): Remove\n+  // this flag when oneDNN supports asynchronous execution by default.\n+  bool is_async_ = false;\n+\n+  // Async value that signals completion of the last scheduled parallel loop.\n+  // This is used only when is_async_ is true.\n+  tsl::AsyncValueRef<tsl::Chain> done_event_;\n };\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "8a204377b5fd987e0d840a313bb93fe9e98999e6",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_threadpool_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_threadpool_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_threadpool_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_threadpool_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -53,7 +53,12 @@ static absl::StatusOr<dnnl::graph::graph> CreateExpGraph(\n \n TEST(OneDnnThreadPoolTest, Binary) {\n   tsl::thread::ThreadPool threads(tsl::Env::Default(), \"test\", 32);\n+\n+#ifdef ENABLE_ONEDNN_ASYNC\n+  OneDnnThreadPool threadpool(threads.AsEigenThreadPool(), /*is_async=*/true);\n+#else\n   OneDnnThreadPool threadpool(threads.AsEigenThreadPool());\n+#endif  // ENABLE_ONEDNN_ASYNC\n \n   int64_t d0 = 100;\n   int64_t d1 = 1000;\n@@ -100,6 +105,10 @@ TEST(OneDnnThreadPoolTest, Binary) {\n   // Execute compiled oneDNN graph on the CPU stream.\n   compiled_partitions[0].execute(stream, {src}, {dst});\n \n+#ifdef ENABLE_ONEDNN_ASYNC\n+  stream.wait();\n+#endif\n+\n   for (int i = 0; i < num_elements; ++i) {\n     EXPECT_NEAR(dst_data[i], std::exp(1.0f), 1e-5);\n   }"
        },
        {
            "sha": "4801d69499e782586cc7dd87324098bc144b6ee5",
            "filename": "third_party/xla/xla/backends/cpu/runtime/thunk.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -175,11 +175,13 @@ Thunk::ExecuteSession::ExecuteSession(int64_t max_workers,\n       split_threshold_(split_threshold) {}\n \n // Encodes thunk info into the TraceMe compatible format.\n-std::string Thunk::TraceMeEncode() const {\n+std::string Thunk::TraceMeEncode(int64_t run_id, int64_t device_ordinal) const {\n   return tsl::profiler::TraceMeEncode(info_.op_name,\n                                       {{\"hlo_op\", info_.op_name},\n                                        {\"hlo_module\", info_.module_name},\n-                                       {\"program_id\", info_.module_id}});\n+                                       {\"program_id\", info_.module_id},\n+                                       {\"run_id\", run_id},\n+                                       {\"device_ordinal\", device_ordinal}});\n }\n \n std::ostream& operator<<(std::ostream& os, Thunk::Kind kind) {"
        },
        {
            "sha": "df4e276b19663ee52d24515bbb771ba56af985f4",
            "filename": "third_party/xla/xla/backends/cpu/runtime/thunk.h",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -277,6 +277,8 @@ class Thunk {\n     CollectiveExecuteParams* collective_params = nullptr;\n     CustomCallExecuteParams* custom_call_params = nullptr;\n     XnnParams* xnn_params = nullptr;\n+    int64_t run_id = -1;          // -1 means no run id is set.\n+    int64_t device_ordinal = -1;  // -1 means no device ordinal is set.\n     ExecuteSession session = ExecuteSession(ExecuteSession::kMaxWorkers,\n                                             ExecuteSession::kSplitThreshold);\n   };\n@@ -347,7 +349,7 @@ class Thunk {\n \n   // Encodes thunk info into the TraceMe compatible format. Used by\n   // ThunkExecutor to create TraceMe annotations for profiler.\n-  std::string TraceMeEncode() const;\n+  std::string TraceMeEncode(int64_t run_id, int64_t device_ordinal) const;\n \n   Kind kind_;\n   Info info_;"
        },
        {
            "sha": "00f8555ff4ea7600ac4fc778cfbf040a241dd77e",
            "filename": "third_party/xla/xla/backends/cpu/runtime/thunk_executor.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_executor.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -225,8 +225,9 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> ThunkExecutor::TracedExecute(\n   }\n \n   // Create a producer traceme to capture the start event.\n-  tsl::profiler::TraceMeProducer producer([&] { return thunk.TraceMeEncode(); },\n-                                          tsl::profiler::ContextType::kGeneric);\n+  tsl::profiler::TraceMeProducer producer(\n+      [&] { return thunk.TraceMeEncode(params.run_id, params.device_ordinal); },\n+      tsl::profiler::ContextType::kGeneric);\n \n   auto execute_event = thunk.Execute(params);\n "
        },
        {
            "sha": "93f6fe557fc5e14aedc4728d6503d55116a57d7f",
            "filename": "third_party/xla/xla/backends/cpu/runtime/work_queue.h",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwork_queue.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwork_queue.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fwork_queue.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -33,7 +33,6 @@ limitations under the License.\n #include \"Eigen/ThreadPool\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/concurrency/chain.h\"\n-#include \"xla/tsl/lib/math/math_util.h\"\n #include \"xla/tsl/platform/logging.h\"\n \n #define EIGEN_USE_THREADS\n@@ -130,8 +129,7 @@ inline WorkQueue::WorkQueue(size_t num_tasks, size_t num_partitions)\n     : partitions_(num_partitions),\n       empty_(num_tasks == 0),\n       num_work_stealing_workers_(0) {\n-  size_t partition_size =\n-      tsl::MathUtil::FloorOfRatio(num_tasks, num_partitions);\n+  size_t partition_size = num_tasks / num_partitions;\n   size_t rem_tasks = num_tasks % num_partitions;\n   for (size_t i = 0, begin = 0, end = 0; i < num_partitions; ++i, begin = end) {\n     end = begin + partition_size + ((i < rem_tasks) ? 1 : 0);"
        },
        {
            "sha": "0bb5ab3108d3478358d464fa79bb630be2647d57",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -107,7 +107,7 @@ XnnFusionThunk::XnnExecutable::Invoke(\n     external_values.push_back(value);\n   }\n \n-  DCHECK_NE(runtime, nullptr) << \"XNNPACK runtime is not initialized\";\n+  DCHECK_NE(runtime.get(), nullptr) << \"XNNPACK runtime is not initialized\";\n   XNN_RETURN_IF_ERROR(xnn_setup_runtime_v2(\n       runtime.get(), external_values.size(), external_values.data()));\n "
        },
        {
            "sha": "eb56ece3679eb923a2908515289bb37ae2ac8d50",
            "filename": "third_party/xla/xla/backends/cpu/transforms/BUILD",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2FBUILD?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -1,5 +1,7 @@\n load(\"//xla:xla.default.bzl\", \"xla_cc_test\")\n load(\"//xla/tsl:tsl.bzl\", \"internal_visibility\")\n+load(\"//xla/tsl/mkl:build_defs.bzl\", \"if_graph_api\")\n+load(\"//xla/tsl/mkl:graph.bzl\", \"onednn_graph_cc_library\")\n load(\"//xla/tsl/platform:rules_cc.bzl\", \"cc_library\")\n \n package(\n@@ -19,6 +21,7 @@ cc_library(\n     name = \"dot_library_rewriter\",\n     srcs = [\"dot_library_rewriter.cc\"],\n     hdrs = [\"dot_library_rewriter.h\"],\n+    local_defines = if_graph_api([\"XLA_ONEDNN_USE_GRAPH_API=1\"]),\n     deps = [\n         \":library_matcher\",\n         \":onednn_matcher\",\n@@ -45,6 +48,7 @@ cc_library(\n xla_cc_test(\n     name = \"dot_library_rewriter_test\",\n     srcs = [\"dot_library_rewriter_test.cc\"],\n+    local_defines = if_graph_api([\"XLA_ONEDNN_USE_GRAPH_API=1\"]),\n     deps = [\n         \":dot_library_rewriter\",\n         \"//xla:xla_data_proto_cc\",\n@@ -56,6 +60,7 @@ xla_cc_test(\n         \"//xla/hlo/utils:hlo_query\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/base:no_destructor\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/strings\",\n@@ -78,9 +83,10 @@ cc_library(\n     ],\n )\n \n-cc_library(\n+onednn_graph_cc_library(\n     name = \"onednn_matcher\",\n     hdrs = [\"onednn_matcher.h\"],\n+    # copybara:uncomment compatible_with = [\"//buildenv/target:non_prod\"],\n     deps = [\n         \":library_matcher\",\n         \"//xla/backends/cpu:onednn_support\",\n@@ -101,6 +107,7 @@ cc_library(\n         \"//xla/backends/cpu:xnn_support\",\n         \"//xla/backends/cpu/codegen:target_machine_features\",\n         \"//xla/hlo/ir:hlo\",\n+        \"@com_google_absl//absl/base:no_destructor\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\","
        },
        {
            "sha": "f433053d478022ad239e35b31a57a699d27570ba",
            "filename": "third_party/xla/xla/backends/cpu/transforms/dot_library_rewriter.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 9,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fdot_library_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fdot_library_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fdot_library_rewriter.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -199,19 +199,24 @@ void DotLibraryRewriter::AddFusionCandidates(\n   }\n }\n \n-absl::Status DotLibraryRewriter::MergeFusionInstructions(\n-    HloFusionInstruction* main, HloFusionInstruction* neighbor,\n-    FusionDirection dir) {\n+absl::StatusOr<HloFusionInstruction*>\n+DotLibraryRewriter::MergeFusionInstructions(HloFusionInstruction* main,\n+                                            HloFusionInstruction* neighbor,\n+                                            FusionDirection dir) {\n   VLOG(3) << \"  \" << FusionDirectionToString(dir)\n           << \": Fusing with: \" << neighbor->ToString();\n   if (dir == FusionDirection::kUp) {\n     main->MergeFusionInstruction(neighbor);\n     TF_RETURN_IF_ERROR(main->parent()->RemoveInstruction(neighbor));\n-  } else if (dir == FusionDirection::kDown) {\n+    return main;\n+  }\n+  if (dir == FusionDirection::kDown) {\n     neighbor->MergeFusionInstruction(main);\n     TF_RETURN_IF_ERROR(neighbor->parent()->RemoveInstruction(main));\n+    return neighbor;\n   }\n-  return absl::OkStatus();\n+  return InvalidArgument(\"Invalid fusion direction: %s\",\n+                         FusionDirectionToString(dir));\n }\n \n absl::StatusOr<HloInstruction*> DotLibraryRewriter::GrowFusion(\n@@ -246,15 +251,17 @@ absl::Status DotLibraryRewriter::FuseNeighbors(HloFusionInstruction* fusion,\n     auto [instr, dir] = frontier.front();\n     frontier.pop();\n     if (dir != FusionDirection::kUp && dir != FusionDirection::kDown) {\n-      return InvalidArgument(\"Invalid travel direction: %c\", dir);\n+      return InvalidArgument(\"Invalid travel direction: %s\",\n+                             FusionDirectionToString(dir));\n     }\n \n     // If `instr` is another fusion of the same library type, fuse it.\n     // We don't need to add its neighbors to the frontier because anything that\n     // can be fused would have already been fused into `instr`.\n     if (IsCustomFusionWithKind(instr, lib->fusion_kind())) {\n-      TF_RETURN_IF_ERROR(MergeFusionInstructions(\n-          fusion, Cast<HloFusionInstruction>(instr), dir));\n+      TF_ASSIGN_OR_RETURN(fusion,\n+                          MergeFusionInstructions(\n+                              fusion, Cast<HloFusionInstruction>(instr), dir));\n       continue;\n     }\n \n@@ -281,14 +288,17 @@ absl::Status DotLibraryRewriter::FuseNeighbors(HloFusionInstruction* fusion,\n absl::StatusOr<bool> DotLibraryRewriter::ProcessComputation(\n     HloComputation* computation) {\n   // Construct a list of instructions that can start a library fusion, starting\n-  // from the root up to the top. Prioritize dot ops over element-wise ops.\n+  // from the root up to the top. Prioritize dot and reduce ops over\n+  // element-wise ops.\n   // TODO(penporn): Use priority queue when we have a cost model.\n   std::vector<HloInstruction*> fusion_starters;\n   std::vector<HloInstruction*> eltwise_ops;\n   auto instructions = computation->MakeInstructionPostOrder();\n   for (auto it = instructions.rbegin(); it != instructions.rend(); ++it) {\n     if (fuse_dot_ && (*it)->opcode() == HloOpcode::kDot) {\n       fusion_starters.push_back(*it);\n+    } else if (fuse_reduce_ && (*it)->opcode() == HloOpcode::kReduce) {\n+      fusion_starters.push_back(*it);\n     } else if (fuse_eltwise_ && (*it)->IsElementwise()) {\n       eltwise_ops.push_back(*it);\n     }"
        },
        {
            "sha": "8880f3c928c478ac031566b1b95670a7981cc737",
            "filename": "third_party/xla/xla/backends/cpu/transforms/dot_library_rewriter.h",
            "status": "modified",
            "additions": 12,
            "deletions": 4,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fdot_library_rewriter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fdot_library_rewriter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fdot_library_rewriter.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -28,14 +28,17 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"xla/backends/cpu/codegen/target_machine_features.h\"\n #include \"xla/backends/cpu/transforms/library_matcher.h\"\n-#include \"xla/backends/cpu/transforms/onednn_matcher.h\"\n #include \"xla/backends/cpu/transforms/xnn_matcher.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n #include \"tsl/platform/protobuf.h\"\n \n+#if XLA_ONEDNN_USE_GRAPH_API\n+#include \"xla/backends/cpu/transforms/onednn_matcher.h\"\n+#endif  // XLA_ONEDNN_USE_GRAPH_API\n+\n namespace xla::cpu {\n \n enum class FusionDirection {\n@@ -60,11 +63,13 @@ class DotLibraryRewriter : public HloModulePass {\n       : target_machine_features_(target_machine_features),\n         options_(std::move(options)) {\n     // Initialize library matchers.\n+#if XLA_ONEDNN_USE_GRAPH_API\n     if (options_.use_onednn && options_.onednn_fusion_types != nullptr &&\n         !options_.onednn_fusion_types->empty()) {\n       libs_.push_back(std::make_unique<OneDnnMatcher>(\n           target_machine_features_, options_.onednn_fusion_types));\n     }\n+#endif  // XLA_ONEDNN_USE_GRAPH_API\n     if (options_.use_xnnpack && options_.xnn_fusion_types != nullptr &&\n         !options_.xnn_fusion_types->empty()) {\n       libs_.push_back(std::make_unique<XnnMatcher>(target_machine_features_,\n@@ -77,6 +82,8 @@ class DotLibraryRewriter : public HloModulePass {\n     // Check if any library supports each of the fusion types.\n     fuse_dot_ =\n         absl::c_any_of(libs_, [](const auto& lib) { return lib->fuse_dot(); });\n+    fuse_reduce_ = absl::c_any_of(\n+        libs_, [](const auto& lib) { return lib->fuse_reduce(); });\n     fuse_eltwise_ = absl::c_any_of(\n         libs_, [](const auto& lib) { return lib->fuse_eltwise(); });\n   }\n@@ -94,9 +101,9 @@ class DotLibraryRewriter : public HloModulePass {\n   // Merges two fusions `main` and `neighbor` together. `main` is the current\n   // fusion instruction we are growing. `neighbor` is a neighboring fusion node\n   // found through BFS from `main`.\n-  absl::Status MergeFusionInstructions(HloFusionInstruction* main,\n-                                       HloFusionInstruction* neighbor,\n-                                       FusionDirection dir);\n+  absl::StatusOr<HloFusionInstruction*> MergeFusionInstructions(\n+      HloFusionInstruction* main, HloFusionInstruction* neighbor,\n+      FusionDirection dir);\n \n   // Fuses `to_fuse` into the fusion `fusion` based on the specified direction.\n   // Returns the pointer to the new `to_fuse` node in the fusion region.\n@@ -125,6 +132,7 @@ class DotLibraryRewriter : public HloModulePass {\n   absl::flat_hash_set<HloOpcode> supported_ops_;\n   absl::flat_hash_set<HloInstruction*> fused_;\n   bool fuse_dot_ = false;\n+  bool fuse_reduce_ = false;\n   bool fuse_eltwise_ = false;\n };\n "
        },
        {
            "sha": "94bc86212e9d71c3df42593db7478be36dda9052",
            "filename": "third_party/xla/xla/backends/cpu/transforms/dot_library_rewriter_test.cc",
            "status": "modified",
            "additions": 152,
            "deletions": 19,
            "changes": 171,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fdot_library_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fdot_library_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fdot_library_rewriter_test.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <vector>\n \n #include <gtest/gtest.h>\n+#include \"absl/base/no_destructor.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/log.h\"\n #include \"absl/strings/match.h\"\n@@ -61,6 +62,12 @@ class CpuLibraryTest : public TargetMachineTestBase {\n     bool changed;\n   };\n \n+  static const DotRewriteTestSpec& GetDefaultTestSpec() {\n+    static const absl::NoDestructor<DotRewriteTestSpec> kDefaultTestSpec(\n+        {\"xnn\", \"f32\", \"f32\", \"znver3\", \"+avx,+avx2\", \"dot\"});\n+    return *kDefaultTestSpec;\n+  }\n+\n   virtual void RunTest(absl::string_view hlo_template,\n                        FusionProperties expected) {}\n \n@@ -88,6 +95,9 @@ class CpuLibraryTest : public TargetMachineTestBase {\n     if (spec.fusion_mode == \"greedy\") {\n       fusion_types.Add(DebugOptions::LIBRARY_FUSION_TYPE_ELTWISE);\n     }\n+    if (spec.fusion_mode == \"reduce\") {\n+      fusion_types.Add(DebugOptions::LIBRARY_FUSION_TYPE_REDUCE);\n+    }\n     tsl::protobuf::RepeatedField<int> empty_fusion_types;\n     bool use_onednn = spec.lib == \"onednn\";\n     bool use_xnnpack = spec.lib == \"xnn\";\n@@ -110,7 +120,7 @@ class CpuLibraryTest : public TargetMachineTestBase {\n     EXPECT_EQ(fusion->fusion_kind(), HloInstruction::FusionKind::kCustom);\n \n     // Adjust the expected values if a convert is auto-inserted.\n-    if (spec.out_dtype == \"bf16\" &&\n+    if (!use_onednn && spec.out_dtype == \"bf16\" &&\n         hlo_query::FindInstruction(fusion->fused_instructions_computation(),\n                                    HloOpcode::kDot)) {\n       ++expected.num_instructions_in_fused_computation;\n@@ -146,7 +156,14 @@ class CpuLibraryFullParamTest\n   bool IsDotEnabledOnCPU() {\n     DotRewriteTestSpec spec = GetParam();\n     bool bf16_dot_supported = absl::StrContains(spec.features, \"+avx512bf16\");\n-    return spec.in_dtype != \"bf16\" || bf16_dot_supported;\n+    bool fp16_dot_supported = absl::StrContains(spec.features, \"+avx512fp16\");\n+    if (spec.in_dtype == \"bf16\") {\n+      return bf16_dot_supported;\n+    }\n+    if (spec.in_dtype == \"f16\") {\n+      return fp16_dot_supported;\n+    }\n+    return true;\n   }\n };\n \n@@ -188,6 +205,61 @@ TEST_P(CpuLibraryFullParamTest, MatMul) {\n   RunTest(hlo_template, {HloOpcode::kDot, 2, 3, IsDotEnabledOnCPU()});\n }\n \n+TEST_P(CpuLibraryFullParamTest, MatMulTransposeRHS) {\n+  const absl::string_view hlo_template = R\"(\n+    HloModule matmul\n+\n+    ENTRY %main {\n+      %input = $in_dtype[32,8,128,64]{3,2,1,0} parameter(0)\n+      %weight = $in_dtype[32,8,128,64]{3,2,1,0} parameter(1)\n+      ROOT %dot = $out_dtype[32,8,128,128]{3,2,1,0} dot(%input, %weight),\n+                  lhs_batch_dims={0,1}, lhs_contracting_dims={3},\n+                  rhs_batch_dims={0,1}, rhs_contracting_dims={3}\n+    })\";\n+\n+  RunTest(hlo_template, {HloOpcode::kDot, 2, 3, IsDotEnabledOnCPU()});\n+}\n+\n+TEST_P(CpuLibraryFullParamTest, MatMulTransposeLHS) {\n+  const absl::string_view hlo_template = R\"(\n+    HloModule matmul\n+\n+    ENTRY %main {\n+      %input = $in_dtype[32,8,128,64]{3,2,1,0} parameter(0)\n+      %weight = $in_dtype[32,8,128,64]{3,2,1,0} parameter(1)\n+      ROOT %dot = $out_dtype[32,8,64,64]{3,2,1,0} dot(%input, %weight),\n+                  lhs_batch_dims={0,1}, lhs_contracting_dims={2},\n+                  rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n+    })\";\n+\n+  DotRewriteTestSpec spec = GetParam();\n+  FusionProperties expected = {HloOpcode::kDot, 0, 0, false};\n+  if (spec.lib == \"onednn\" && IsDotEnabledOnCPU()) {\n+    expected = FusionProperties{HloOpcode::kDot, 2, 3, true};\n+  }\n+  RunTest(hlo_template, expected);\n+}\n+\n+TEST_P(CpuLibraryFullParamTest, MatMulDimSizeUnqual) {\n+  const absl::string_view hlo_template = R\"(\n+    HloModule matmul\n+\n+    ENTRY %main {\n+      %input = $in_dtype[1,16,256,256]{3,2,1,0} parameter(0)\n+      %weight = $in_dtype[1,16,256]{2,1,0} parameter(1)\n+      ROOT %dot = $out_dtype[1,16,256]{2,1,0} dot(%input, %weight),\n+                  lhs_batch_dims={0,1}, lhs_contracting_dims={3},\n+                  rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n+    })\";\n+\n+  DotRewriteTestSpec spec = GetParam();\n+  FusionProperties expected = {HloOpcode::kDot, 0, 0, false};\n+  if (spec.lib == \"xnn\" && IsDotEnabledOnCPU()) {\n+    expected = FusionProperties{HloOpcode::kDot, 2, 3, true};\n+  }\n+  RunTest(hlo_template, expected);\n+}\n+\n TEST_P(CpuLibraryFullParamTest, MatMulAndAdd) {\n   const absl::string_view hlo_template = R\"(\n     HloModule matmul\n@@ -353,7 +425,8 @@ std::vector<DotRewriteTestSpec> GetDotRewriteTestSpecs() {\n   absl::flat_hash_map<std::string, std::string> cpu_to_features = {\n       {\"znver3\", \"+avx,+avx2\"},\n       {\"sapphirerapids\",\n-       \"+avx512vnni,+avx512bf16,+amx-bf16,+amx-int8,+amx-tile,+amx-transpose\"},\n+       \"+avx512vnni,+avx512bf16,+amx-bf16,+avx512fp16,+amx-int8,+amx-tile,+amx-\"\n+       \"transpose\"},\n   };\n \n   // Input and output data types to test per each library + CPU combination.\n@@ -362,16 +435,20 @@ std::vector<DotRewriteTestSpec> GetDotRewriteTestSpecs() {\n       {{\"xnn\", \"znver3\"}, {{\"f32\", \"f32\"}, {\"bf16\", \"f32\"}}},\n       {{\"xnn\", \"sapphirerapids\"},\n        {{\"f32\", \"f32\"}, {\"bf16\", \"f32\"}, {\"bf16\", \"bf16\"}}},\n-      {{\"onednn\", \"sapphirerapids\"}, {{\"f32\", \"f32\"}}},\n   };\n \n   // Fusion modes to test for each library.\n   // We temporarily use XNN_GRAPH_FUSION_MODE_DISABLED to denote the dot fusion\n   // mode (starting fusion nodes with dots).\n   absl::flat_hash_map<std::string, std::vector<std::string>> fusion_modes = {\n-      {\"xnn\", {\"dot\", \"greedy\"}},\n-      {\"onednn\", {\"dot\"}},\n-  };\n+      {\"xnn\", {\"dot\", \"greedy\"}}};\n+\n+#if XLA_ONEDNN_USE_GRAPH_API\n+  // Don't test oneDNN if we don't build with it.\n+  dtype_map[{\"onednn\", \"sapphirerapids\"}] = {\n+      {\"f32\", \"f32\"}, {\"bf16\", \"bf16\"}, {\"f16\", \"f16\"}};\n+  fusion_modes[\"onednn\"] = {\"dot\"};\n+#endif  // XLA_ONEDNN_USE_GRAPH_API\n \n   std::vector<DotRewriteTestSpec> specs;\n   for (auto& [lib_cpu, dtype_pairs] : dtype_map) {\n@@ -399,21 +476,16 @@ class CpuLibraryFusionTypeTest\n       public ::testing::WithParamInterface<std::string> {\n  public:\n   static std::string Name(const ::testing::TestParamInfo<std::string>& info) {\n-    return absl::StrCat(kLib, \"_\", info.param, \"_\", kDType, \"_\", kCpuName);\n+    return info.param;\n   }\n \n  protected:\n   void RunTest(absl::string_view hlo_template,\n                FusionProperties expected) override {\n-    DotRewriteTestSpec spec = {std::string(kLib),      std::string(kDType),\n-                               std::string(kDType),    std::string(kCpuName),\n-                               std::string(kFeatures), GetParam()};\n+    DotRewriteTestSpec spec = GetDefaultTestSpec();\n+    spec.fusion_mode = GetParam();\n     RunTestInternal(spec, hlo_template, expected);\n   }\n-  static constexpr absl::string_view kLib = \"xnn\";\n-  static constexpr absl::string_view kDType = \"f32\";\n-  static constexpr absl::string_view kCpuName = \"znver3\";\n-  static constexpr absl::string_view kFeatures = \"+avx,+avx2\";\n };\n \n TEST_P(CpuLibraryFusionTypeTest, AllEltwiseFusion) {\n@@ -510,16 +582,77 @@ TEST_P(CpuLibraryFusionTypeTest, JoiningFusions) {\n       ROOT %mul = $in_dtype[64,64] multiply(%exp, %add2)\n     })\";\n \n-  RunTest(hlo_template, GetParam() == \"greedy\"\n-                            ? FusionProperties{HloOpcode::kMultiply, 3, 8, true}\n-                            : FusionProperties{HloOpcode::kDot, 3, 5, true});\n+  if (GetParam() == \"greedy\") {\n+    RunTest(hlo_template, FusionProperties{HloOpcode::kMultiply, 3, 8, true});\n+  }\n+  if (GetParam() == \"dot\") {\n+    RunTest(hlo_template, FusionProperties{HloOpcode::kDot, 3, 5, true});\n+  }\n+}\n+\n+TEST_P(CpuLibraryFusionTypeTest, Reduce) {\n+  const absl::string_view hlo_template = R\"(\n+    HloModule reduce\n+\n+    reducer_add {\n+      lhs = $in_dtype[] parameter(0)\n+      rhs = $in_dtype[] parameter(1)\n+      ROOT sum = $in_dtype[] add(lhs, rhs)\n+    }\n+\n+    ENTRY main {\n+      input = $in_dtype[64,64]{1,0} parameter(0)\n+      c = $in_dtype[] constant(0)\n+      ROOT output = $in_dtype[64]{0} reduce(input, c), dimensions={1}, to_apply=reducer_add\n+    }\n+    )\";\n+  if (GetParam() == \"reduce\") {\n+    RunTest(hlo_template, {HloOpcode::kReduce, 1, 3, true});\n+  }\n }\n \n INSTANTIATE_TEST_SUITE_P(CpuLibraryFusionTypeTestSuite,\n                          CpuLibraryFusionTypeTest,\n                          ::testing::ValuesIn({std::string(\"dot\"),\n-                                              std::string(\"greedy\")}),\n+                                              std::string(\"greedy\"),\n+                                              std::string(\"reduce\")}),\n                          CpuLibraryFusionTypeTest::Name);\n \n+TEST_F(CpuLibraryTest, UpdateFusion) {\n+  //                      c\n+  //                       \\\n+  //   b ------------------ dot2\n+  //    \\                       \\\n+  // a -- sub1 -- add1 -- dot1 -- add2\n+  //\n+  // In \"dot\" mode, `dot2` + `add2` get fused first (call this `fusion2`). Then\n+  // `dot1` will create a new fusion (`fusion1`), fuse with `add1, and try to\n+  // fuse with `fusion2`. Since fusions are merged by updating the consumer\n+  // fusion, `fusion1` will get absorbed into `fusion2`. When we continue\n+  // growing the fusion around `dot1`, we need to use `fusion2` instead of\n+  // `fusion1`. This test will fail if the old `fusion1` is used (`sub1` will\n+  // not recognize `fusion1` as its user).\n+  const absl::string_view hlo_template = R\"(\n+    HloModule matmul\n+\n+    ENTRY %main {\n+      %a = $in_dtype[64,64] parameter(0)\n+      %b = $in_dtype[64,64] parameter(1)\n+      %c = $in_dtype[64,64] parameter(2)\n+      %sub1 = $in_dtype[64,64] subtract(%a, %b)\n+      %add1 = $in_dtype[64,64] add(%a, %sub1)\n+      %dot1 = $in_dtype[64,64] dot(%add1, %b), lhs_contracting_dims={1},\n+                                            rhs_contracting_dims={0}\n+      %dot2 = $in_dtype[64,64] dot(%b, %c), lhs_contracting_dims={1},\n+                                            rhs_contracting_dims={0}\n+      ROOT %add2 = $in_dtype[64,64] add(%dot1, %dot2)\n+    })\";\n+\n+  DotRewriteTestSpec spec = GetDefaultTestSpec();\n+  spec.fusion_mode = \"dot\";\n+  RunTestInternal(spec, hlo_template,\n+                  FusionProperties{HloOpcode::kAdd, 3, 8, true});\n+}\n+\n }  // namespace\n }  // namespace xla::cpu"
        },
        {
            "sha": "bc1c2bf9ba9f88698f4b3d17028743e4f2ea860f",
            "filename": "third_party/xla/xla/backends/cpu/transforms/library_matcher.h",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Flibrary_matcher.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Flibrary_matcher.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Flibrary_matcher.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -43,6 +43,9 @@ class LibraryMatcher {\n         case DebugOptions::LIBRARY_FUSION_TYPE_ELTWISE:\n           fuse_eltwise_ = true;\n           break;\n+        case DebugOptions::LIBRARY_FUSION_TYPE_REDUCE:\n+          fuse_reduce_ = true;\n+          break;\n         default:\n           LOG(ERROR) << \"Unsupported fusion type: \" << *it;\n       }\n@@ -80,9 +83,13 @@ class LibraryMatcher {\n   // Returns whether elementwise ops can start a fusion.\n   bool fuse_eltwise() const { return fuse_eltwise_; }\n \n+  // Returns whether reduce ops can start a fusion.\n+  bool fuse_reduce() const { return fuse_reduce_; }\n+\n  protected:\n   bool fuse_dot_ = false;\n   bool fuse_eltwise_ = false;\n+  bool fuse_reduce_ = false;\n   const TargetMachineFeatures* target_machine_features_;\n };\n "
        },
        {
            "sha": "7b033f7ca946ec7153b3656cadf9c296e56afcf7",
            "filename": "third_party/xla/xla/backends/cpu/transforms/onednn_matcher.h",
            "status": "modified",
            "additions": 12,
            "deletions": 4,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fonednn_matcher.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fonednn_matcher.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fonednn_matcher.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -51,11 +51,19 @@ class OneDnnMatcher : public LibraryMatcher {\n     if (!SupportedOps().contains(instr->opcode())) {\n       return false;\n     }\n-    // Assume all ops are supported as long as all inputs and output are F32.\n-    return instr->shape().element_type() == F32 &&\n+    if (instr->opcode() == HloOpcode::kDot) {\n+      return IsOneDnnDotSupported(\n+          instr->dot_dimension_numbers(), instr->operand(0)->shape(),\n+          instr->operand(1)->shape(), instr->shape(), target_machine_features_);\n+    }\n+\n+    return IsOneDnnSupportedDType(instr->shape().element_type(),\n+                                  target_machine_features_) &&\n            std::all_of(instr->operands().begin(), instr->operands().end(),\n-                       [](const HloInstruction* operand) {\n-                         return operand->shape().element_type() == F32;\n+                       [this](const HloInstruction* operand) {\n+                         return IsOneDnnSupportedDType(\n+                             operand->shape().element_type(),\n+                             target_machine_features_);\n                        });\n   }\n "
        },
        {
            "sha": "faa943fa4ce929d1253e2a9ab2c6a06640779aeb",
            "filename": "third_party/xla/xla/backends/cpu/transforms/xnn_matcher.h",
            "status": "modified",
            "additions": 25,
            "deletions": 11,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fxnn_matcher.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fxnn_matcher.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fxnn_matcher.h?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n \n #include <string>\n \n+#include \"absl/base/no_destructor.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n@@ -39,17 +40,18 @@ class XnnMatcher : public LibraryMatcher {\n \n   // Returns the set of supported HLO instructions.\n   absl::flat_hash_set<HloOpcode> SupportedOps() const override {\n-    static const auto* kSupportedOps = []() {\n-      static auto* supported_ops =\n-          new absl::flat_hash_set<HloOpcode>{HloOpcode::kDot};\n-      for (const auto& [op, _] : GetXnnUnaryOpMap()) {\n-        supported_ops->insert(op);\n-      }\n-      for (const auto& [op, _] : GetXnnBinaryOpMap()) {\n-        supported_ops->insert(op);\n-      }\n-      return supported_ops;\n-    }();\n+    static const absl::NoDestructor<absl::flat_hash_set<HloOpcode>>\n+        kSupportedOps{[]() {\n+          absl::flat_hash_set<HloOpcode> supported_ops{\n+              HloOpcode::kDot, HloOpcode::kReduce, HloOpcode::kConstant};\n+          for (const auto& [op, _] : GetXnnUnaryOpMap()) {\n+            supported_ops.insert(op);\n+          }\n+          for (const auto& [op, _] : GetXnnBinaryOpMap()) {\n+            supported_ops.insert(op);\n+          }\n+          return supported_ops;\n+        }()};\n     return *kSupportedOps;\n   }\n \n@@ -60,9 +62,18 @@ class XnnMatcher : public LibraryMatcher {\n           instr->dot_dimension_numbers(), instr->operand(0)->shape(),\n           instr->operand(1)->shape(), instr->shape(), target_machine_features_);\n     }\n+    if (instr->opcode() == HloOpcode::kReduce) {\n+      return IsReduceOpSupportedByXnn(instr);\n+    }\n     if (instr->IsConstant()) {\n       return IsConstantSupportedByXnn(instr);\n     }\n+    // TODO(b/441837668): Need to get the reduction performance/cost model\n+    // right before enabling fusions. Fusions make performance analysis quite\n+    // challenging.\n+    if (fuse_reduce_) {\n+      return false;\n+    }\n     if (instr->IsElementwise()) {\n       return IsElementwiseOpSupportedByXnn(instr);\n     }\n@@ -76,6 +87,9 @@ class XnnMatcher : public LibraryMatcher {\n     if (fuse_dot_ && instr->opcode() == HloOpcode::kDot) {\n       return true;\n     }\n+    if (fuse_reduce_ && instr->opcode() == HloOpcode::kReduce) {\n+      return true;\n+    }\n     return fuse_eltwise_ && instr->IsElementwise();\n   }\n "
        },
        {
            "sha": "30473a0bc2ed9a0616ff3b00044aaaa7e6d21d0f",
            "filename": "third_party/xla/xla/backends/cpu/xnn_emitter.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_emitter.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -456,11 +456,11 @@ static absl::StatusOr<XnnSubgraph> EmitXnnSubgraph(\n       } break;\n \n       case HloOpcode::kReduce: {\n-        if (!IsReduceOpSupportedByXnn(instr)) {\n-          return InvalidArgument(\n-              \"Unsupported reduce instruction in XNN fusion: %s\",\n-              instr->ToString());\n-        }\n+        // FIXME: Validate the reduce instruction.\n+        // One cannot directly use IsReduceOpSupportedByXnn since the invariant\n+        // value is not necessarily included into the same fusion. This might\n+        // happen if the original instruction has multiple users or was rejected\n+        // by the fusion compiler pass.\n         TF_ASSIGN_OR_RETURN(tensor_ids[instr],\n                             DefineReduceOp(subgraph.get(), tensor_ids, instr));\n       } break;"
        },
        {
            "sha": "6750a8495247413f368b9fa63d859085cadcdfb7",
            "filename": "third_party/xla/xla/backends/cpu/xnn_gemm_config.cc",
            "status": "modified",
            "additions": 47,
            "deletions": 47,
            "changes": 94,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_gemm_config.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_gemm_config.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_gemm_config.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -149,7 +149,7 @@ static constexpr GemmFilter BF16BF16F32GemmFilter{\n };\n \n static constexpr GemmFilter AMDRomeGemmFilter{\n-  /*input_range=*/{16, 2048},\n+  /*input_range=*/{16, 4096},\n   /*lhs_dtype=*/PrimitiveType::F32,\n   /*rhs_dtype=*/PrimitiveType::F32,\n   /*out_dtype=*/PrimitiveType::F32,\n@@ -158,53 +158,53 @@ static constexpr GemmFilter AMDRomeGemmFilter{\n static constexpr Net AMDRomeNet{\n   /*scaler=*/{\n     /*mean=*/\n-    {{ 1018.0043402777778, 1023.5008680555555, 1025.861328125, 6.616354874682953, 6.620083771919308, 6.63456382646144 }},\n+    {{ 2031.4479060265578, 2036.3171603677222, 2062.2170582226763, 7.29227087924762, 7.308301476602625, 7.331674465299577 }},\n     /*scale=*/\n-    {{ 590.632963260763, 593.7150826714844, 588.1463320809286, 0.9965492348670616, 1.0053835765793744, 0.9754062267657724 }}\n+    {{ 1188.2177375470617, 1178.7350461452038, 1179.7790996965598, 1.0416890873676914, 1.0053399234375506, 0.9757991392501179 }},\n   },\n   /*hidden_layer_1=*/{\n     /*weights=*/{{\n-      {{ 1.005891244892035, -0.11072959330426384, -0.968046373485514, -4.921521725424851, -0.13899276332759042, -1.226293159684938 }},\n-      {{ -1.2502714170922153, -1.1765486776310423, 0.4888716998554182, 0.5172335453177302, -0.593110373636639, 0.1209811095977735 }},\n-      {{ -0.18095204827877465, -0.06060134530184746, -0.3750576441489519, 0.8188111073502161, 0.3576805251431741, 1.4968701417615413 }},\n-      {{ -0.05111798751008719, 0.07769076545705815, 0.6207635489778949, 3.6101378942470967, -0.05282332398428117, 1.7854763674893495 }},\n-      {{ 0.747350403236783, 0.04930450878767915, 0.8777099456443157, 1.4990255665580505, 0.08212232867228593, -1.9843375290758656 }},\n-      {{ 0.5541966277545818, -0.33159380874523126, -0.3194666063636952, 1.6816423644238223, 0.1017686813844035, 3.1265875450394813 }},\n-      {{ 0.7345608720339312, 1.5974970323865725, -2.2148504671311953, 0.5363830361034387, 0.3700332861259481, 0.9331234440564571 }},\n-      {{ 1.4423494345704555, 0.289382200477869, 1.5110082992053955, 1.418682530523278, -0.5749656156984766, 2.684031259217246 }},\n+      {{ 0.5255922128957278, -0.8065013670906714, -0.5264014380189966, -1.2772498330118651, 1.3840216299823802, 0.7322759674330881 }},\n+      {{ -0.7597171548555842, -1.2571169773685882, -0.32518437620636936, 1.0212806356673838, 0.9165371224616725, -0.19250317971610814 }},\n+      {{ -2.3497882574965994, 0.23878289300722322, -2.5867259166595944, 0.8432052252434499, -0.7374592701571068, 0.6061228206232958 }},\n+      {{ 0.3412638507438349, 0.009127030753615727, -0.43271581733053577, 0.3058216852138156, 0.4132978840654225, 0.08892908864656021 }},\n+      {{ -0.3843556431761765, -0.5398088470059381, -2.0478454682095735, -1.9041927205327738, -1.0368295384919808, -0.1653666006655781 }},\n+      {{ 0.9415170642828504, -0.4671602009419241, -2.594401365132767, 0.5011818371933664, 2.6743454901058725, 1.090931094328555 }},\n+      {{ -2.030867525769208, 0.9360281369657524, -2.179490537456837, 0.6315631977398317, -0.2797813498393135, 1.1780045163240112 }},\n+      {{ 2.026780502536945, 1.1382782700184098, 0.7076892737809293, -0.5003242829913847, 1.7337823655903326, 0.676979521067241 }},\n     }},\n     /*biases=*/{\n-      { 1.0309774603375907, 0.6465236073322296, 3.553393762532912, -0.8185749009865861, -0.12955426436728715, 0.8031554507587597, -1.5785991424170187, -0.39445212677049063 }\n+      { 2.827760670625431, -0.9347274494671962, 1.7748650815163647, -0.5102747570142624, 1.1443725632238269, 2.0573020231014616, 0.33721201132380757, 2.7437956980307643 },\n     }\n   },\n   /*hidden_layer_2=*/{\n     /*weights=*/{{\n-      {{ 0.5283786603352238, -0.056889406800860146, 0.6606078794503449, 0.48144968239995534, 0.010554846273878095, -0.14799959162846965, 0.20457525406298369, -0.12661568456264205 }},\n-      {{ -0.9534585055098438, -0.10012840023501332, -0.5795955688342004, 1.9348789870642515, 0.21271731229957186, 0.1557077642737526, 1.5288709351139655, -0.7002696129400411 }},\n-      {{ 2.1906088364770784, 0.07703067791952263, 2.1646753484073655, -0.32251840895547773, 0.6850197180505169, 0.30061421444806946, 1.0728025841881765, -0.8100244450669523 }},\n-      {{ 0.8106365294738354, -0.3410735969241413, 0.7910924608271775, -0.07017938451436888, 0.16051916138347214, 0.3004275708609215, 0.7729045870717262, -0.2332237341201925 }},\n-      {{ 0.07246756191918226, -0.05758991153686244, 0.911745169839482, 1.2510377533921035, -1.182537901423303, -1.295182969102456, -1.904956642808503, 0.007010431897136803 }},\n-      {{ -0.8733376736355871, 0.157671979745821, -1.0372041545921873, 3.3000069112365584, 0.25551941086911717, 0.9589328110123956, -0.23856740081287128, -2.0315351809352586 }},\n-      {{ -1.9337567589656532, 1.4676259894002257, 1.4886721579905993, -1.5705845737356183, 0.48937401463732866, 1.083620050144208, -1.0031665521883135, -1.2660789079048749 }},\n-      {{ 2.4310441535271687, 0.49189784223311395, 0.31483156395428413, -0.0865355927145238, 0.7631527157107736, -2.7077958375575055, -0.9228446079924654, -0.7391110100336947 }}\n+      {{ 2.571821311709108, 0.16869445337763503, 0.3541411973512104, 0.31040383433531593, -1.9138308971941267, 1.577267326066108, 1.0358680188904088, -0.48597239908310547 }},\n+      {{ -0.3168524372865204, -0.8109707535168992, -0.6883758912881943, 0.20041683878416458, 0.29562419861502953, 2.9699371941875183, -0.06378706528945598, -1.2627270412739198 }},\n+      {{ 1.2121865841893051, 0.4324679330555888, 0.5756742637802713, -0.3965637421226802, -0.8316876650525071, 1.4267737797853521, 0.6590628275882154, 1.0969896994507335 }},\n+      {{ 0.08152092107879703, 0.987281670566132, 2.711801967605775, 0.03262333498333622, -0.24851434369301018, 0.5857580261361529, -0.14172228489696118, 1.0096244465236095 }},\n+      {{ -1.099617291565094, -0.96182176932886, 1.1198642662894356, 0.09569259551658717, 0.9865508260397995, -1.7073686127591108, 0.8545686868857858, 1.276785903326864 }},\n+      {{ 0.6284115174399925, -0.5692706408214737, -0.3776497427936689, 0.2850473804130665, 0.5611912673866001, 0.7074167980672433, 1.3602397130866593, -2.4641849404042104 }},\n+      {{ -0.2235255127724266, -0.6066818030776572, 2.098453748102861, -0.551860833640914, -0.6607678541967575, -1.0968858307838945, -3.097129404864497, 1.22936241411423 }},\n+      {{ -0.35359032516179434, 0.16659401401800453, 0.7409562527506246, 0.12880569714035928, 1.6235584538175323, 0.35055754805485, -0.5085408039033421, 0.03832167245213557 }},\n     }},\n     /*biases=*/{\n-      { 0.3486705866196229, 2.2776343748673153, 0.10764831721796844, 0.09166185120840216, -1.0034214854612917, -0.927160996221299, 1.5172112381212808, 0.4772212967805247 }\n+      { -0.9650088973529635, 0.18404512445819377, -1.1301082618712814, -0.4114680200097482, -2.16829227705252, -0.792693003568079, 2.0186809343196432, 0.6651750830570318 },\n     }\n   },\n   /*output_layer=*/{\n     /*weights=*/{{\n-      {{ -0.5896335640757622, 2.3283855361577914, -1.9754605319484158, -0.6632271049751296, 1.9086390756642784, 3.7433099466616238, 1.824432545010804, 2.09625742741301 }}\n+      {{ -3.4950798141841886, 3.052869401349734, -1.9332425183341917, -2.4468455334890375, 3.1182134156177734, 2.662143418701658, 3.609609051057281, -1.6114776062537006 }},\n     }},\n     /*biases=*/{\n-      { -0.04977871506692414 }\n+      { -0.8627209596023582 },\n     }\n   },\n   /*threshold=*/0.03,\n };\n \n static constexpr GemmFilter AMDGenoaGemmFilter{\n-  /*input_range=*/{16, 2048},\n+  /*input_range=*/{16, 4096},\n   /*lhs_dtype=*/PrimitiveType::F32,\n   /*rhs_dtype=*/PrimitiveType::F32,\n   /*out_dtype=*/PrimitiveType::F32,\n@@ -213,49 +213,49 @@ static constexpr GemmFilter AMDGenoaGemmFilter{\n static constexpr Net AMDGenoaNet {\n   /*scaler=*/{\n     /*mean=*/\n-    {{ 1002.1257625527921, 1040.082590333177, 1029.2266541529798, 6.598251685888309, 6.6404292804559635, 6.654739928938151 }},\n+    {{ 2048.487742594484, 2032.4805924412667, 2042.0275791624106, 7.311636506981553, 7.331182177414692, 7.324348610024091 }},\n     /*scale=*/\n-    {{ 594.7876923848536, 596.4073362076931, 579.6412706089599, 0.9806421276088567, 1.0047695259963145, 0.9426741875163773 }}\n+    {{ 1191.317145630777, 1166.4230415375375, 1162.7572402044934, 1.0130577584567735, 0.9372130582909888, 0.9819331632142719 }},\n   },\n   /*hidden_layer_1=*/{\n     /*weights=*/{{\n-      {{ -1.8525957121690557, -0.12363449157789586, -0.8386369273170659, -3.3954448946951414, 0.33886983245776847, -0.9317201523227778 }},\n-      {{ 0.5120871227810689, -1.5214338320394882, -0.7274651337778577, 1.3200335397974383, 0.5923648903998096, 1.7241300558638806 }},\n-      {{ -1.8962086199924455, -1.7249686491398133, -0.15047446639707035, 1.1356266853003538, 2.70970817913586, 0.7154911861570797 }},\n-      {{ -0.0020642101422613684, 0.14097136562712495, -0.09163191463561046, -0.02820803725731568, 0.03955304538877561, 0.16661173026752624 }},\n-      {{ -2.001943294276495, 1.4970193262821043, 0.7777143265827485, 2.4639584696544814, -2.5991183905189827, -1.7850169579413313 }},\n-      {{ 1.447549192320386, 1.361173224199936, -0.43481695242532376, 0.8671054211799711, -0.25280176689036743, 2.5275631466098756 }},\n-      {{ -0.1441737229514477, -0.3347815066193075, -0.4611605626954958, 2.181799166007114, 1.6323192872442907, 0.15928222502988382 }},\n-      {{ -2.4693645353916667, 2.0428513209522623, 1.749924045625967, 1.7019213519700969, 1.142024009890298, 0.8079292507334053 }},\n+      {{ -0.3975566315544443, 0.5914998393825349, 0.6099048505253704, -2.2657754130482575, 0.36614796953745665, -0.9019941522654611 }},\n+      {{ -1.634528631004246, -1.0247790097319367, 0.7441596497436759, 1.1627072134985457, 0.05409335988074912, -0.12091065051829138 }},\n+      {{ 0.38395072299848293, 0.6541884828037803, 0.417837898603066, -0.9405446354332785, 2.184810649384631, -0.36876630139170674 }},\n+      {{ 1.4311717327837925, 0.9019482519954495, 0.010222966815173684, 0.3734603575926762, -0.48722286699557477, 0.6097423536728197 }},\n+      {{ -0.7136793187709407, -1.9428210404652928, 0.4274609198312262, 0.7241649472475438, 0.7127139917668667, -0.17169269406677637 }},\n+      {{ 0.7274093691413374, 1.5619764328746881, 0.3132760663502329, 0.1150444561729908, 0.2015964262316955, -1.6488397218364703 }},\n+      {{ -0.2753144111803734, 0.851664634951511, -0.7668837132534746, 0.8536953128922471, 0.5346385907475031, -0.3903852123459044 }},\n+      {{ -0.33049518181245935, -0.1445885038395346, 0.33671360297244707, 0.19923558301288513, 0.47714692266995923, 2.673625950077934 }},\n     }},\n     /*biases=*/{\n-      { 1.5113512311183828, 1.1809617131571573, 0.4478042727248335, -0.7947383308647096, 0.9825820925820611, 2.3808221280278645, -0.4758496641489877, -0.17036794796484162 }\n+      { 1.8781920773242509, 0.6510580145727756, 1.3641835181490685, -1.237083419397511, 0.09563962519162661, 1.0633713668067988, -0.2750294272946441, 0.4082406241441991 },\n     }\n   },\n   /*hidden_layer_2=*/{\n     /*weights=*/{{\n-      {{ 0.35014293742424724, -0.5476873131211425, 2.120121365055135, 0.2526392749218728, -0.9407987661758053, -0.9846910595845514, 0.48052568401528156, 0.3180570478836074 }},\n-      {{ -0.7150926363558526, -0.8982987199655207, 1.5360045616379934, 0.1803870098694977, -3.472436216409119, 1.0044744229912244, 0.4335209413598261, 1.041064125260285 }},\n-      {{ 0.8966091979738551, 2.227321493016081, -1.4494415364024744, 0.0254402271585605, 1.537814361721512, 1.3889169541326774, -0.16388027659485285, 1.7176584253327756 }},\n-      {{ -0.0597193070244571, -2.5061384854500623, 2.124958864280429, 0.020011322193699232, 0.48193219767756923, 1.323504600240459, 1.9316400738347115, 0.9369893381605147 }},\n-      {{ 0.032699385072007786, -0.9107111450265581, 0.5333081096223299, -0.03148698112648736, -0.16377114190322742, -1.6085833510134837, 0.7968443974614786, 2.238778726588351 }},\n-      {{ -1.5463140026380815, 1.5303232342633923, 1.3050928202993113, 0.17310173303944465, -0.9857258240351916, -0.9550401224056498, -0.0071639117187034555, -0.7976265480773017 }},\n-      {{ -2.2035284133385966, -0.9376946654220445, 1.3260159557423135, -0.406540234329094, 0.9533782897195586, 2.4910588396009277, -1.3676852361286786, 0.6322307655713885 }},\n-      {{ 0.32147827378305643, 0.26619936190034904, -1.200452781975383, 0.00920972156405369, 2.8156283745109, -1.229898728424551, 1.3389598114381762, 1.7166384524807266 }},\n+      {{ 1.482788775138106, -0.5911919348052194, -0.35265948412831416, 0.5693173975201452, 0.08299331485534553, -1.0926309595949408, 0.334160671733911, -0.8259113265483281 }},\n+      {{ -0.7244072332431708, 1.7167578358580047, -0.4425799291591407, 0.38193961610444616, -0.3131049026459214, 0.7057668457879581, -0.8977670579096759, -1.1564071580034785 }},\n+      {{ 0.2358887563481682, 0.845047198622242, 0.3965633248481624, -0.9292260319808021, 0.38780851270938177, 0.9073719197977955, 0.8942857890487362, 2.2078844573893486 }},\n+      {{ 0.7588397006376895, 0.39649528525833017, 1.1922103753418032, -0.2623025347145879, -1.8688404509544276, 0.23950836230216038, 0.15018196046213705, 1.1091046070474726 }},\n+      {{ -0.06639877236719088, 0.09408482409872725, 0.08853697547037886, -0.027191640785169502, -0.025050403848262424, -0.14821218627938373, -0.05119778874800481, -0.003846457076482196 }},\n+      {{ -1.3626737341753659, -0.509211567650967, -1.3709529389911908, 0.8181695565961004, -0.9154056938786789, 1.6786394527771, -0.38910973671573107, 0.6109302318778375 }},\n+      {{ -0.9490250745418807, -0.22890259271729135, -0.7669763564967859, -1.2378100390537607, 0.9325554827865082, -0.7707072257516585, -0.6101643395959798, 0.6438447441624673 }},\n+      {{ 1.1581876959277013, 1.4439015663052703, -1.4659507082977212, 1.0425420146162472, -0.20891484120663645, 0.3292514803046433, 0.38947771607697135, 0.06588859566944062 }},\n     }},\n     /*biases=*/{\n-      { 0.9699621387917686, 0.5909615635903002, 1.427690501080743, 0.8892984037070786, -0.8768819880157831, 3.2935191809063777, 1.8570755726618182, -1.0853011858156631 }\n+      { 2.0991435035679293, 0.9220598032166089, 0.001237522670163396, -0.2035381110666839, -0.7214610628375114, -2.275782698263265, 3.2572710355363337, -1.309956720253099 },\n     }\n   },\n   /*output_layer=*/{\n     /*weights=*/{{\n-      {{ 2.253756860038231, 2.6258281376128605, -1.6079113669729557, -4.006565437256236, -2.8600538503590087, 2.808280959332861, 2.8146068804622777, 2.604358926679522 }},\n+      {{ -2.214950317234679, 2.3173207097966624, -2.4148863077632057, 2.440952250974181, 0.016504153668811035, 3.00219780922754, 2.454200734592688, 2.444832006369846 }},\n     }},\n     /*biases=*/{\n-      { 0.26142347527936727 }\n+      { -0.2538826384470055 },\n     }\n   },\n-  /*threshold=*/0.03,\n+  /*threshold=*/0.05,\n };\n \n // clang-format on"
        },
        {
            "sha": "a27df279a24a293b331d44e94c002d4c4397282e",
            "filename": "third_party/xla/xla/backends/cpu/xnn_support.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_support.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8c00d2e7622debe7f5eea1a33e099f1e0367d1a2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_support.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_support.cc?ref=8c00d2e7622debe7f5eea1a33e099f1e0367d1a2",
            "patch": "@@ -81,6 +81,11 @@ absl::StatusOr<bool> IsDotSupportedByXnn(\n   if (!AreDtypesSupported(lhs_shape, rhs_shape, out_shape, cpu_features)) {\n     return false;\n   }\n+  if (!IsLayoutSupportedByXnn(lhs_shape) ||\n+      !IsLayoutSupportedByXnn(rhs_shape) ||\n+      !IsLayoutSupportedByXnn(out_shape)) {\n+    return false;\n+  }\n \n   // Check shapes.\n   TF_ASSIGN_OR_RETURN(DotShape dot_shape, GetDotShape(dot_dimensions, lhs_shape,"
        }
    ],
    "stats": {
        "total": 91913,
        "additions": 65542,
        "deletions": 26371
    }
}