{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Derive replica and partition counts from HloModule in ExecuteReplicated.\n\nPiperOrigin-RevId: 838135801",
    "sha": "95ade166bde35b002a5516633a69c21a9c755443",
    "files": [
        {
            "sha": "5996bd49a3ca23287ece0134a7189111c3d7060c",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test_base.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/95ade166bde35b002a5516633a69c21a9c755443/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/95ade166bde35b002a5516633a69c21a9c755443/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.cc?ref=95ade166bde35b002a5516633a69c21a9c755443",
            "patch": "@@ -180,8 +180,10 @@ CollectiveOpsE2ETestBase::ExecuteReplicated(\n absl::StatusOr<CollectiveOpsE2ETestBase::ExecutionResult>\n CollectiveOpsE2ETestBase::ExecuteReplicated(\n     std::unique_ptr<HloModule> module,\n-    const std::vector<std::vector<Literal*>> arguments, int64_t num_replicas,\n-    int64_t num_partitions, bool run_hlo_passes) {\n+    const std::vector<std::vector<Literal*>> arguments, bool run_hlo_passes) {\n+  int64_t num_replicas = module->config().replica_count();\n+  int64_t num_partitions = module->config().num_partitions();\n+\n   CHECK(num_replicas > 0 && \"expect at least one replica\");\n   CHECK(num_partitions > 0 && \"expect at least one partition\");\n   CHECK(num_replicas == arguments.size() &&"
        },
        {
            "sha": "b89c75c3f71a1abbb24c2af423c7a1ce43b7ffa2",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test_base.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/95ade166bde35b002a5516633a69c21a9c755443/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/95ade166bde35b002a5516633a69c21a9c755443/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.h?ref=95ade166bde35b002a5516633a69c21a9c755443",
            "patch": "@@ -77,8 +77,7 @@ class CollectiveOpsE2ETestBase : public HloHardwareIndependentTestBase {\n \n   absl::StatusOr<ExecutionResult> ExecuteReplicated(\n       std::unique_ptr<HloModule> module,\n-      std::vector<std::vector<Literal*>> arguments, int64_t num_replicas,\n-      int64_t num_partitions, bool run_hlo_passes = true);\n+      std::vector<std::vector<Literal*>> arguments, bool run_hlo_passes = true);\n \n   const se::GpuComputeCapability& Capability() {\n     return hlo_runner_->backend()"
        },
        {
            "sha": "e0183b86f5173d01acbec6ebcc6dd978f5565c75",
            "filename": "third_party/xla/xla/tests/ragged_all_to_all_e2e_test.cc",
            "status": "modified",
            "additions": 85,
            "deletions": 167,
            "changes": 252,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/95ade166bde35b002a5516633a69c21a9c755443/third_party%2Fxla%2Fxla%2Ftests%2Fragged_all_to_all_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/95ade166bde35b002a5516633a69c21a9c755443/third_party%2Fxla%2Fxla%2Ftests%2Fragged_all_to_all_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fragged_all_to_all_e2e_test.cc?ref=95ade166bde35b002a5516633a69c21a9c755443",
            "patch": "@@ -337,26 +337,20 @@ TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs) {\n   })\";\n \n   const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n   ASSERT_GE(hlo_runner_->device_count(), kNumReplicas)\n       << \"Test requires at least \" << kNumReplicas << \" devices (\"\n       << hlo_runner_->device_count() << \" available)\";\n \n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(\n+                                           kModuleReplicatedStr, kNumReplicas));\n \n   TF_ASSERT_OK(CreateRandomTestData(module.get(),\n                                     /*input_sizes=*/{/*replica_0=*/{1, 1},\n                                                      /*replica_1=*/{3, 1}}));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       ExecutionResult execution_result,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*num_partitions=*/kNumPartitions));\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs()));\n \n   const std::vector<Literal>& results = execution_result.results;\n \n@@ -382,16 +376,12 @@ TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_S4) {\n   })\";\n \n   const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n   ASSERT_GE(hlo_runner_->device_count(), kNumReplicas)\n       << \"Test requires at least \" << kNumReplicas << \" devices (\"\n       << hlo_runner_->device_count() << \" available)\";\n \n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(\n+                                           kModuleReplicatedStr, kNumReplicas));\n \n   TF_ASSERT_OK(CreateRandomTestData(module.get(),\n                                     /*input_sizes=*/{/*replica_0=*/{1, 1},\n@@ -412,9 +402,7 @@ TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_S4) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       ExecutionResult execution_result,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*num_partitions=*/kNumPartitions));\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs()));\n \n   // Check that ragged-all-to-all on S4 was converted to S8.\n   // Skip this check for decomposer test, because there ragged-all-to-all was\n@@ -448,26 +436,20 @@ TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_InputBufferLargerThanOutput) {\n   })\";\n \n   const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n   ASSERT_GE(hlo_runner_->device_count(), kNumReplicas)\n       << \"Test requires at least \" << kNumReplicas << \" devices (\"\n       << hlo_runner_->device_count() << \" available)\";\n \n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(\n+                                           kModuleReplicatedStr, kNumReplicas));\n \n   TF_ASSERT_OK(CreateRandomTestData(module.get(),\n                                     /*input_sizes=*/{/*replica_0=*/{8, 5},\n                                                      /*replica_1=*/{4, 3}}));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       ExecutionResult execution_result,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*num_partitions=*/kNumPartitions));\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs()));\n \n   const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n@@ -491,26 +473,20 @@ TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_OutputBufferLargerThanInput) {\n   })\";\n \n   const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n-  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n-      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas)\n+      << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+      << hlo_runner_->device_count() << \" available)\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(\n+                                           kModuleReplicatedStr, kNumReplicas));\n \n   TF_ASSERT_OK(CreateRandomTestData(module.get(),\n                                     /*input_sizes=*/{/*replica_0=*/{4, 12},\n                                                      /*replica_1=*/{5, 11}}));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       ExecutionResult execution_result,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*num_partitions=*/kNumPartitions));\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs()));\n \n   const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n@@ -534,13 +510,11 @@ TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_MultipleUpdates) {\n   })\";\n \n   const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n-  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n-      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas)\n+      << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+      << hlo_runner_->device_count() << \" available)\";\n \n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+  HloModuleConfig config = GetModuleConfigForTest(kNumReplicas);\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n@@ -551,9 +525,7 @@ TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_MultipleUpdates) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       ExecutionResult execution_result,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*num_partitions=*/kNumPartitions));\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs()));\n \n   const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n@@ -578,26 +550,20 @@ TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_MultiDimData) {\n   })\";\n \n   const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n-  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n-      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas)\n+      << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+      << hlo_runner_->device_count() << \" available)\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(\n+                                           kModuleReplicatedStr, kNumReplicas));\n \n   TF_ASSERT_OK(CreateRandomTestData(module.get(),\n                                     /*input_sizes=*/{/*replica_0=*/{4, 7},\n                                                      /*replica_1=*/{2, 5}}));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       ExecutionResult execution_result,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*num_partitions=*/kNumPartitions));\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs()));\n \n   const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n@@ -622,26 +588,20 @@ TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_Degenerate) {\n   })\";\n \n   const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n-  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n-      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas)\n+      << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+      << hlo_runner_->device_count() << \" available)\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(\n+                                           kModuleReplicatedStr, kNumReplicas));\n \n   TF_ASSERT_OK(CreateRandomTestData(module.get(),\n                                     /*input_sizes=*/{/*replica_0=*/{1},\n                                                      /*replica_1=*/{3}}));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       ExecutionResult execution_result,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*num_partitions=*/kNumPartitions));\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs()));\n \n   const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n@@ -666,16 +626,12 @@ TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_NonDefaultLayout) {\n   })\";\n \n   const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n-  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n-      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas)\n+      << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+      << hlo_runner_->device_count() << \" available)\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(\n+                                           kModuleReplicatedStr, kNumReplicas));\n \n   auto ragged_all_to_all =\n       FindInstruction(module.get(), HloOpcode::kRaggedAllToAll);\n@@ -687,9 +643,7 @@ TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_NonDefaultLayout) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       ExecutionResult execution_result,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*num_partitions=*/kNumPartitions));\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs()));\n \n   const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n@@ -715,26 +669,20 @@ TEST_P(RaggedAllToAllTest,\n   })\";\n \n   const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n-  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n-      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas)\n+      << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+      << hlo_runner_->device_count() << \" available)\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(\n+                                           kModuleReplicatedStr, kNumReplicas));\n \n   TF_ASSERT_OK(CreateRandomTestData(module.get(),\n                                     /*input_sizes=*/{/*replica_0=*/{1, 1},\n                                                      /*replica_1=*/{3, 1}}));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       ExecutionResult execution_result,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*num_partitions=*/kNumPartitions));\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs()));\n \n   const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n@@ -759,19 +707,14 @@ TEST_P(RaggedAllToAllTest, RaggedAllToAll_8GPUs) {\n   })\";\n \n   const int64_t kNumReplicas = 8;\n-  const int64_t kNumPartitions = 1;\n   const int64_t kNumUpdatesPerReplica = 4;\n-  if (hlo_runner_->device_count() < kNumReplicas * kNumPartitions) {\n-    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-                 << \" devices (\" << hlo_runner_->device_count()\n-                 << \" available)\";\n+  if (hlo_runner_->device_count() < kNumReplicas) {\n+    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+                 << hlo_runner_->device_count() << \" available)\";\n   }\n \n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(\n+                                           kModuleReplicatedStr, kNumReplicas));\n \n   Array<int64_t> input_sizes(\n       {kNumReplicas, kNumReplicas, kNumUpdatesPerReplica});\n@@ -781,9 +724,7 @@ TEST_P(RaggedAllToAllTest, RaggedAllToAll_8GPUs) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       ExecutionResult execution_result,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*num_partitions=*/kNumPartitions));\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs()));\n \n   const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n@@ -811,19 +752,17 @@ TEST_P(RaggedAllToAllTest, RaggedAllToAll_8GPUs_2ReplicasPerGroups) {\n \n   const int64_t kNumReplicas = 8;\n   const int64_t kNumReplicasPerGroup = 2;\n-  const int64_t kNumPartitions = 1;\n   const int64_t kNumUpdatesPerReplica = 16;\n-  if (hlo_runner_->device_count() < kNumReplicas * kNumPartitions) {\n-    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-                 << \" devices (\" << hlo_runner_->device_count()\n-                 << \" available)\";\n+  if (hlo_runner_->device_count() < kNumReplicas) {\n+    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+                 << hlo_runner_->device_count() << \" available)\";\n   }\n \n   HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas);\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(\n+                                           kModuleReplicatedStr, kNumReplicas));\n \n   Array<int64_t> input_sizes(\n       {kNumReplicas, kNumReplicasPerGroup, kNumUpdatesPerReplica});\n@@ -833,9 +772,7 @@ TEST_P(RaggedAllToAllTest, RaggedAllToAll_8GPUs_2ReplicasPerGroups) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       ExecutionResult execution_result,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*num_partitions=*/kNumPartitions));\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs()));\n \n   const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n@@ -863,19 +800,17 @@ TEST_P(RaggedAllToAllTest, RaggedAllToAll_8GPUs_4ReplicasPerGroups) {\n \n   const int64_t kNumReplicas = 8;\n   const int64_t kNumReplicasPerGroup = 4;\n-  const int64_t kNumPartitions = 1;\n   const int64_t kNumUpdatesPerReplica = 8;\n-  if (hlo_runner_->device_count() < kNumReplicas * kNumPartitions) {\n-    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-                 << \" devices (\" << hlo_runner_->device_count()\n-                 << \" available)\";\n+  if (hlo_runner_->device_count() < kNumReplicas) {\n+    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+                 << hlo_runner_->device_count() << \" available)\";\n   }\n \n   HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas);\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(\n+                                           kModuleReplicatedStr, kNumReplicas));\n \n   Array<int64_t> input_sizes(\n       {kNumReplicas, kNumReplicasPerGroup, kNumUpdatesPerReplica});\n@@ -885,9 +820,7 @@ TEST_P(RaggedAllToAllTest, RaggedAllToAll_8GPUs_4ReplicasPerGroups) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       ExecutionResult execution_result,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*num_partitions=*/kNumPartitions));\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs()));\n \n   const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n@@ -964,14 +897,13 @@ TEST_P(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_2GPUs_SliceSize1) {\n                        num_input_rows, num_output_rows);\n \n   const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n   const int64_t kNumUpdatesPerReplica = 16;\n-  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n-      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas)\n+      << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+      << hlo_runner_->device_count() << \" available)\";\n \n   HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas);\n \n   config.mutable_debug_options()\n       .set_xla_gpu_unsupported_override_fast_interconnect_slice_size(1);\n@@ -987,9 +919,7 @@ TEST_P(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_2GPUs_SliceSize1) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       ExecutionResult execution_result,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*num_partitions=*/kNumPartitions));\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs()));\n \n   const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n@@ -1020,23 +950,19 @@ TEST_P(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_8GPUs_SliceSize4) {\n                        num_input_rows, num_output_rows);\n \n   const int64_t kNumReplicas = 8;\n-  const int64_t kNumPartitions = 1;\n   const int64_t kNumUpdatesPerReplica = 4;\n-  if (hlo_runner_->device_count() < kNumReplicas * kNumPartitions) {\n-    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-                 << \" devices (\" << hlo_runner_->device_count()\n-                 << \" available)\";\n+  if (hlo_runner_->device_count() < kNumReplicas) {\n+    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+                 << hlo_runner_->device_count() << \" available)\";\n   }\n \n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(\n+                                           kModuleReplicatedStr, kNumReplicas));\n \n-  config.mutable_debug_options()\n+  module->mutable_config()\n+      .mutable_debug_options()\n       .set_xla_gpu_unsupported_override_fast_interconnect_slice_size(4);\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n-\n   Array<int64_t> input_sizes(\n       {kNumReplicas, kNumReplicas, kNumUpdatesPerReplica});\n   input_sizes.FillRandomUniform(0, 16);\n@@ -1045,9 +971,7 @@ TEST_P(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_8GPUs_SliceSize4) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       ExecutionResult execution_result,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*num_partitions=*/kNumPartitions));\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs()));\n \n   const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n@@ -1080,23 +1004,19 @@ TEST_P(RaggedAllToAllMultiHostDecomposerTest,\n \n   const int64_t kNumReplicas = 8;\n   const int64_t kNumReplicasPerGroup = 4;\n-  const int64_t kNumPartitions = 1;\n   const int64_t kNumUpdatesPerReplica = 8;\n-  if (hlo_runner_->device_count() < kNumReplicas * kNumPartitions) {\n-    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-                 << \" devices (\" << hlo_runner_->device_count()\n-                 << \" available)\";\n+  if (hlo_runner_->device_count() < kNumReplicas) {\n+    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+                 << hlo_runner_->device_count() << \" available)\";\n   }\n \n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(\n+                                           kModuleReplicatedStr, kNumReplicas));\n \n-  config.mutable_debug_options()\n+  module->mutable_config()\n+      .mutable_debug_options()\n       .set_xla_gpu_unsupported_override_fast_interconnect_slice_size(4);\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n-\n   Array<int64_t> input_sizes(\n       {kNumReplicas, kNumReplicasPerGroup, kNumUpdatesPerReplica});\n   input_sizes.FillRandomUniform(0, 10);\n@@ -1105,9 +1025,7 @@ TEST_P(RaggedAllToAllMultiHostDecomposerTest,\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       ExecutionResult execution_result,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*num_partitions=*/kNumPartitions));\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs()));\n \n   const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);"
        }
    ],
    "stats": {
        "total": 261,
        "additions": 90,
        "deletions": 171
    }
}