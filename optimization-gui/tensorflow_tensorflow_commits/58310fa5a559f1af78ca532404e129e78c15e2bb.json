{
    "author": "junwhanahn",
    "message": "Add a test suite for `Client::ReshardArrays`\n\nThese tests are located under `pjrt_ifrt` even though the API is not specific to PjRt-IFRT because they rely on IFRT implementations being backed by PjRt, e.g., support for `device`, `pinned_host`, and `unpinned_host` memory kinds, the use of `HloSharding`, etc. Once we find a better home for \"XLA\"-like implementations, we can move these tests there.\n\nPiperOrigin-RevId: 799876943",
    "sha": "58310fa5a559f1af78ca532404e129e78c15e2bb",
    "files": [
        {
            "sha": "9a6ba0286b28b264b8ac1f2967c6e02f658525b5",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/BUILD",
            "status": "modified",
            "additions": 42,
            "deletions": 0,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/58310fa5a559f1af78ca532404e129e78c15e2bb/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/58310fa5a559f1af78ca532404e129e78c15e2bb/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2FBUILD?ref=58310fa5a559f1af78ca532404e129e78c15e2bb",
            "patch": "@@ -195,6 +195,48 @@ xla_cc_test(\n     ],\n )\n \n+# TODO(hyeontaek): Move this target out of pjrt_ifrt.\n+cc_library(\n+    name = \"reshard_impl_test_lib\",\n+    testonly = True,\n+    srcs = [\"reshard_impl_test_lib.cc\"],\n+    visibility = internal_visibility([\n+        \":friends\",\n+        \":internal\",\n+    ]),\n+    deps = [\n+        \"//xla:literal\",\n+        \"//xla:shape_util\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/hlo/ir:hlo_sharding\",\n+        \"//xla/pjrt:pjrt_layout\",\n+        \"//xla/python/ifrt\",\n+        \"//xla/python/ifrt:test_util\",\n+        \"//xla/python/pjrt_ifrt:pjrt_dtype\",\n+        \"//xla/python/pjrt_ifrt:xla_ifrt\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:status_matchers\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/base:nullability\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@com_google_googletest//:gtest_for_library\",\n+    ],\n+    alwayslink = True,\n+)\n+\n+# TODO(hyeontaek): Move this target out of pjrt_ifrt.\n+build_test(\n+    name = \"reshard_impl_test_no_impl\",\n+    targets = [\":reshard_impl_test_lib\"],\n+)\n+\n cc_library(\n     name = \"pjrt_ifrt\",\n     srcs = ["
        },
        {
            "sha": "450c59245278d23167256dc08765cebd161e7814",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/reshard_impl_test_lib.cc",
            "status": "added",
            "additions": 687,
            "deletions": 0,
            "changes": 687,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/58310fa5a559f1af78ca532404e129e78c15e2bb/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Freshard_impl_test_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/58310fa5a559f1af78ca532404e129e78c15e2bb/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Freshard_impl_test_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Freshard_impl_test_lib.cc?ref=58310fa5a559f1af78ca532404e129e78c15e2bb",
            "patch": "@@ -0,0 +1,687 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstdint>\n+#include <functional>\n+#include <memory>\n+#include <optional>\n+#include <string>\n+#include <tuple>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/base/nullability.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/container/inlined_vector.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/hlo/ir/hlo_sharding.h\"\n+#include \"xla/layout_util.h\"\n+#include \"xla/literal.h\"\n+#include \"xla/pjrt/pjrt_layout.h\"\n+#include \"xla/primitive_util.h\"\n+#include \"xla/python/ifrt/array.h\"\n+#include \"xla/python/ifrt/array_spec.h\"\n+#include \"xla/python/ifrt/client.h\"\n+#include \"xla/python/ifrt/device.h\"\n+#include \"xla/python/ifrt/device_list.h\"\n+#include \"xla/python/ifrt/dtype.h\"\n+#include \"xla/python/ifrt/future.h\"\n+#include \"xla/python/ifrt/index.h\"\n+#include \"xla/python/ifrt/index_domain.h\"\n+#include \"xla/python/ifrt/memory.h\"\n+#include \"xla/python/ifrt/shape.h\"\n+#include \"xla/python/ifrt/sharding.h\"\n+#include \"xla/python/ifrt/test_util.h\"\n+#include \"xla/python/pjrt_ifrt/pjrt_dtype.h\"\n+#include \"xla/python/pjrt_ifrt/xla_sharding.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/status_matchers.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla {\n+namespace ifrt {\n+namespace {\n+\n+using ::testing::Eq;\n+using ::testing::HasSubstr;\n+using ::tsl::testing::IsOkAndHolds;\n+using ::tsl::testing::StatusIs;\n+\n+absl::StatusOr<ArrayRef> MakeArrayFromLiteral(Client* absl_nonnull client,\n+                                              const xla::LiteralBase& literal,\n+                                              const ShardingRef& sharding) {\n+  TF_ASSIGN_OR_RETURN(const DType dtype,\n+                      ToDType(literal.shape().element_type()));\n+  const Shape shape(literal.shape().dimensions());\n+\n+  TF_ASSIGN_OR_RETURN(const std::vector<IndexDomain> index_domains,\n+                      sharding->IndexDomains(shape));\n+\n+  Client::MakeArraysFromHostBufferShardsSpec spec = {\n+      /*buffers=*/{},\n+      /*array_spec=*/\n+      {\n+          /*dtype=*/dtype,\n+          /*shape=*/shape,\n+          /*sharding=*/sharding,\n+      },\n+  };\n+  for (int i = 0; i < index_domains.size(); ++i) {\n+    const Index& offset = index_domains[i].origin();\n+    const Shape& shard_shape = index_domains[i].shape();\n+\n+    const Index limit = offset + Index(shard_shape.dims());\n+    auto sliced = std::make_shared<xla::Literal>(\n+        literal.Slice(offset.elements(), limit.elements()));\n+    VLOG(2) << \"Slice #\" << i << \"(\" << index_domains[i]\n+            << \"): \" << sliced->ToString();\n+\n+    Client::HostBuffer host_buffer = {\n+        /*data=*/sliced->untyped_data(),\n+        /*dtype=*/dtype,\n+        /*shape=*/shard_shape,\n+        /*byte_strides=*/std::nullopt,\n+        /*on_done=*/[sliced]() {},\n+    };\n+    spec.buffers.push_back({{i}, std::move(host_buffer)});\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(\n+      std::vector<ArrayRef> arrays,\n+      client->MakeArraysFromHostBufferShards(\n+          absl::MakeSpan(&spec, 1),\n+          Client::HostBufferSemantics::kImmutableUntilTransferCompletes));\n+  return arrays[0];\n+}\n+\n+absl::StatusOr<xla::Literal> CopyArrayToLiteral(ArrayRef array) {\n+  TF_ASSIGN_OR_RETURN(const xla::PrimitiveType element_type,\n+                      ToPrimitiveType(array->dtype()));\n+  const auto xla_shape =\n+      xla::ShapeUtil::MakeShape(element_type, array->shape().dims());\n+\n+  TF_ASSIGN_OR_RETURN(const std::vector<IndexDomain> index_domains,\n+                      array->sharding().IndexDomains(array->shape()));\n+  TF_ASSIGN_OR_RETURN(std::vector<ArrayRef> shards,\n+                      array->DisassembleIntoSingleDeviceArrays(\n+                          ArrayCopySemantics::kReuseInput,\n+                          SingleDeviceShardSemantics::kAddressableShards));\n+\n+  TF_ASSIGN_OR_RETURN(xla::Literal literal, xla::Literal::Make(xla_shape));\n+  absl::flat_hash_set<IndexDomain> seen;\n+\n+  for (int i = 0; i < shards.size(); ++i) {\n+    const Index& offset = index_domains[i].origin();\n+    const Shape& shard_shape = index_domains[i].shape();\n+\n+    TF_ASSIGN_OR_RETURN(xla::Literal slice,\n+                        xla::Literal::Make(xla::ShapeUtil::MakeShape(\n+                            element_type, shard_shape.dims())));\n+    Future<> future = shards[i]->CopyToHostBuffer(\n+        slice.untyped_data(), std::nullopt, ArrayCopySemantics::kAlwaysCopy);\n+    TF_RETURN_IF_ERROR(future.Await());\n+    VLOG(2) << \"Slice #\" << i << \" (\" << index_domains[i]\n+            << \"): \" << slice.ToString();\n+\n+    if (seen.insert(index_domains[i]).second) {\n+      TF_RETURN_IF_ERROR(literal.CopySliceFrom(\n+          slice, Index::Zeros(shard_shape.dims().size()).elements(),\n+          offset.elements(), shard_shape.dims()));\n+    } else {\n+      Index limits = offset + Index(shard_shape.dims());\n+      const xla::Literal expected =\n+          literal.Slice(offset.elements(), limits.elements());\n+      if (slice != expected) {\n+        return absl::InternalError(\n+            absl::StrCat(\"Inconsistent replication in \", index_domains[i], \": \",\n+                         slice.ToString(), \" vs. \", expected.ToString()));\n+      }\n+    }\n+  }\n+  return literal;\n+}\n+\n+absl::StatusOr<xla::Literal> CreateIotaLiteral(xla::PrimitiveType element_type,\n+                                               absl::Span<const int64_t> dims) {\n+  TF_ASSIGN_OR_RETURN(\n+      xla::Literal literal,\n+      xla::Literal::Make(xla::ShapeUtil::MakeShape(element_type, dims)));\n+  TF_RETURN_IF_ERROR(xla::primitive_util::IntegralTypeSwitch(\n+      [&](auto primitive_type_constant) -> absl::Status {\n+        using T = xla::primitive_util::NativeTypeOf<primitive_type_constant>;\n+        T value(0);\n+        return literal.Populate<T>(\n+            [&](absl::Span<const int64_t> indices) { return value++; });\n+      },\n+      literal.shape().element_type()));\n+  return literal;\n+}\n+\n+class ReshardTest : public testing::Test {\n+ protected:\n+  void SetUp() override {\n+    TF_ASSERT_OK_AND_ASSIGN(client_, test_util::GetClient());\n+  }\n+\n+  std::shared_ptr<Client> client_;\n+};\n+\n+TEST_F(ReshardTest, BatchedWithDifferentSharding) {\n+  TF_ASSERT_OK_AND_ASSIGN(const xla::Literal literal,\n+                          CreateIotaLiteral(xla::PrimitiveType::S32, {4, 8}));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(const DeviceListRef src_device_list,\n+                          client_->MakeDeviceList(client_->devices()));\n+  std::vector<ArrayRef> src_arrays;\n+  for (int i = 0; i < 2; ++i) {\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        src_arrays.emplace_back(),\n+        MakeArrayFromLiteral(\n+            client_.get(), literal,\n+            HloSharding::Create(src_device_list, MemoryKind(),\n+                                xla::HloSharding::IotaTile({4, 2}))));\n+  }\n+\n+  TF_ASSERT_OK_AND_ASSIGN(const DeviceListRef dst_device_list,\n+                          client_->MakeDeviceList(client_->devices()));\n+  std::vector<ArraySpec> array_specs = {\n+      {\n+          /*dtype=*/src_arrays[0]->dtype(),\n+          /*shape=*/src_arrays[0]->shape(),\n+          /*sharding=*/\n+          HloSharding::Create(dst_device_list, MemoryKind(),\n+                              xla::HloSharding::Replicate()),\n+      },\n+      {\n+          /*dtype=*/src_arrays[1]->dtype(),\n+          /*shape=*/src_arrays[1]->shape(),\n+          /*sharding=*/\n+          HloSharding::Create(dst_device_list, MemoryKind(),\n+                              xla::HloSharding::IotaTile({2, 4})),\n+      },\n+  };\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<ArrayRef> dst_arrays,\n+      client_->ReshardArrays(absl::MakeSpan(src_arrays), array_specs,\n+                             ArrayCopySemantics::kDonateInput));\n+  ASSERT_EQ(dst_arrays.size(), 2);\n+\n+  EXPECT_EQ(dst_arrays[0]->sharding(), *array_specs[0].sharding);\n+  EXPECT_THAT(CopyArrayToLiteral(dst_arrays[0]),\n+              IsOkAndHolds(Eq(std::cref(literal))));\n+\n+  EXPECT_EQ(dst_arrays[1]->sharding(), *array_specs[1].sharding);\n+  EXPECT_THAT(CopyArrayToLiteral(dst_arrays[1]),\n+              IsOkAndHolds(Eq(std::cref(literal))));\n+}\n+\n+TEST_F(ReshardTest, BatchedWithDifferentDeviceLists) {\n+  TF_ASSERT_OK_AND_ASSIGN(const xla::Literal literal,\n+                          CreateIotaLiteral(xla::PrimitiveType::S32, {4, 8}));\n+\n+  std::vector<ArrayRef> src_arrays;\n+  {\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        const DeviceListRef src_device_list,\n+        client_->MakeDeviceList(client_->devices().subspan(0, 4)));\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        src_arrays.emplace_back(),\n+        MakeArrayFromLiteral(\n+            client_.get(), literal,\n+            HloSharding::Create(src_device_list, MemoryKind(),\n+                                xla::HloSharding::IotaTile({2, 2}))));\n+  }\n+  {\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        const DeviceListRef src_device_list,\n+        client_->MakeDeviceList(client_->devices().subspan(4, 4)));\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        src_arrays.emplace_back(),\n+        MakeArrayFromLiteral(\n+            client_.get(), literal,\n+            HloSharding::Create(src_device_list, MemoryKind(),\n+                                xla::HloSharding::IotaTile({2, 2}))));\n+  }\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      DeviceListRef device_list_0_4,\n+      client_->MakeDeviceList(client_->devices().subspan(0, 4)));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      DeviceListRef device_list_4_4,\n+      client_->MakeDeviceList(client_->devices().subspan(4, 4)));\n+  std::vector<ArraySpec> array_specs = {\n+      {\n+          /*dtype=*/src_arrays[0]->dtype(),\n+          /*shape=*/src_arrays[0]->shape(),\n+          /*sharding=*/\n+          HloSharding::Create(std::move(device_list_0_4), MemoryKind(),\n+                              xla::HloSharding::Replicate()),\n+      },\n+      {\n+          /*dtype=*/src_arrays[1]->dtype(),\n+          /*shape=*/src_arrays[1]->shape(),\n+          /*sharding=*/\n+          HloSharding::Create(std::move(device_list_4_4), MemoryKind(),\n+                              xla::HloSharding::IotaTile({2, 2})),\n+      },\n+  };\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<ArrayRef> dst_arrays,\n+      client_->ReshardArrays(absl::MakeSpan(src_arrays), array_specs,\n+                             ArrayCopySemantics::kDonateInput));\n+  ASSERT_EQ(dst_arrays.size(), 2);\n+\n+  EXPECT_EQ(dst_arrays[0]->sharding(), *array_specs[0].sharding);\n+  EXPECT_THAT(CopyArrayToLiteral(dst_arrays[0]),\n+              IsOkAndHolds(Eq(std::cref(literal))));\n+\n+  EXPECT_EQ(dst_arrays[1]->sharding(), *array_specs[1].sharding);\n+  EXPECT_THAT(CopyArrayToLiteral(dst_arrays[1]),\n+              IsOkAndHolds(Eq(std::cref(literal))));\n+}\n+\n+TEST_F(ReshardTest, PoisonedInput) {\n+  TF_ASSERT_OK_AND_ASSIGN(const xla::Literal literal,\n+                          CreateIotaLiteral(xla::PrimitiveType::S32, {4, 8}));\n+  const absl::Status error = absl::InternalError(\"injected error\");\n+\n+  std::vector<ArrayRef> src_arrays;\n+  {\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        const DeviceListRef src_device_list,\n+        client_->MakeDeviceList(client_->devices().subspan(0, 4)));\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        src_arrays.emplace_back(),\n+        MakeArrayFromLiteral(\n+            client_.get(), literal,\n+            HloSharding::Create(src_device_list, MemoryKind(),\n+                                xla::HloSharding::IotaTile({2, 2}))));\n+  }\n+  {\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        const DeviceListRef src_device_list,\n+        client_->MakeDeviceList(client_->devices().subspan(4, 4)));\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        auto arrays,\n+        client_->MakeErrorArrays(\n+            error, {{\n+                       /*dtype=*/DType(DType::kS32),\n+                       /*shape=*/Shape({4, 8}),\n+                       /*sharding=*/\n+                       HloSharding::Create(src_device_list, MemoryKind(),\n+                                           xla::HloSharding::IotaTile({2, 2})),\n+                   }}));\n+    src_arrays.push_back(std::move(arrays[0]));\n+  }\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      DeviceListRef device_list_0_4,\n+      client_->MakeDeviceList(client_->devices().subspan(0, 4)));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      DeviceListRef device_list_4_4,\n+      client_->MakeDeviceList(client_->devices().subspan(4, 4)));\n+  std::vector<ArraySpec> array_specs = {\n+      {\n+          /*dtype=*/src_arrays[0]->dtype(),\n+          /*shape=*/src_arrays[0]->shape(),\n+          /*sharding=*/\n+          HloSharding::Create(std::move(device_list_0_4), MemoryKind(),\n+                              xla::HloSharding::Replicate()),\n+      },\n+      {\n+          /*dtype=*/src_arrays[1]->dtype(),\n+          /*shape=*/src_arrays[1]->shape(),\n+          /*sharding=*/\n+          HloSharding::Create(std::move(device_list_4_4), MemoryKind(),\n+                              xla::HloSharding::IotaTile({2, 2})),\n+      },\n+  };\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<ArrayRef> dst_arrays,\n+      client_->ReshardArrays(absl::MakeSpan(src_arrays), array_specs,\n+                             ArrayCopySemantics::kDonateInput));\n+  ASSERT_EQ(dst_arrays.size(), 2);\n+\n+  EXPECT_EQ(dst_arrays[0]->sharding(), *array_specs[0].sharding);\n+  EXPECT_THAT(CopyArrayToLiteral(dst_arrays[0]),\n+              IsOkAndHolds(Eq(std::cref(literal))));\n+\n+  EXPECT_EQ(dst_arrays[1]->sharding(), *array_specs[1].sharding);\n+  EXPECT_THAT(dst_arrays[1]->GetReadyFuture().Await(),\n+              StatusIs(error.code(), HasSubstr(error.message())));\n+}\n+\n+TEST_F(ReshardTest, DifferentDestinationLayout) {\n+  TF_ASSERT_OK_AND_ASSIGN(const xla::Literal literal,\n+                          CreateIotaLiteral(xla::PrimitiveType::S32, {4, 8}));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(const DeviceListRef src_device_list,\n+                          client_->MakeDeviceList(client_->devices()));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      ArrayRef src_array,\n+      MakeArrayFromLiteral(\n+          client_.get(), literal,\n+          HloSharding::Create(src_device_list, MemoryKind(),\n+                              xla::HloSharding::IotaTile({4, 2}))));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(const DeviceListRef dst_device_list,\n+                          client_->MakeDeviceList(client_->devices()));\n+  ArraySpec dst_array_spec = {\n+      /*dtype=*/src_array->dtype(),\n+      /*shape=*/src_array->shape(),\n+      /*sharding=*/\n+      HloSharding::Create(dst_device_list, MemoryKind(),\n+                          xla::HloSharding::Replicate()),\n+      /*layout=*/\n+      std::make_shared<xla::PjRtLayout>(\n+          xla::LayoutUtil::MakeAscendingLayout(2)),\n+  };\n+\n+  // Make sure that the destination layout is actually different from the source\n+  // layout in order to ensure the test coverage.\n+  TF_ASSERT_OK_AND_ASSIGN(const auto src_layout, src_array->layout());\n+  ASSERT_NE(src_layout->xla_layout(), dst_array_spec.layout->xla_layout());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<ArrayRef> dst_arrays,\n+      client_->ReshardArrays(absl::MakeSpan(&src_array, 1), {dst_array_spec},\n+                             ArrayCopySemantics::kDonateInput));\n+  ASSERT_EQ(dst_arrays.size(), 1);\n+\n+  const ArrayRef& dst_array = dst_arrays[0];\n+  EXPECT_EQ(dst_array->sharding(), *dst_array_spec.sharding);\n+\n+  // Verify that the destination array is created with the user-provided layout.\n+  TF_ASSERT_OK_AND_ASSIGN(const auto dst_layout, dst_array->layout());\n+  EXPECT_EQ(dst_layout->xla_layout(), dst_array_spec.layout->xla_layout());\n+\n+  EXPECT_THAT(CopyArrayToLiteral(dst_array),\n+              IsOkAndHolds(Eq(std::cref(literal))));\n+}\n+\n+class ReshardMemoryKindTest : public ReshardTest,\n+                              public testing::WithParamInterface<MemoryKind> {};\n+\n+TEST_P(ReshardMemoryKindTest, Int4) {\n+  const MemoryKind memory_kind = GetParam();\n+  TF_ASSERT_OK_AND_ASSIGN(const xla::Literal literal,\n+                          CreateIotaLiteral(xla::PrimitiveType::S4, {4, 8}));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(const DeviceListRef src_device_list,\n+                          client_->MakeDeviceList(client_->devices()));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      ArrayRef src_array,\n+      MakeArrayFromLiteral(\n+          client_.get(), literal,\n+          HloSharding::Create(src_device_list, memory_kind,\n+                              xla::HloSharding::IotaTile({4, 2}))));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(const DeviceListRef dst_device_list,\n+                          client_->MakeDeviceList(client_->devices()));\n+  ArraySpec dst_array_spec = {\n+      /*dtype=*/src_array->dtype(),\n+      /*shape=*/src_array->shape(),\n+      /*sharding=*/\n+      HloSharding::Create(dst_device_list, memory_kind,\n+                          xla::HloSharding::IotaTile({2, 4})),\n+  };\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<ArrayRef> dst_arrays,\n+      client_->ReshardArrays(absl::MakeSpan(&src_array, 1), {dst_array_spec},\n+                             ArrayCopySemantics::kDonateInput));\n+  ASSERT_EQ(dst_arrays.size(), 1);\n+\n+  const ArrayRef& dst_array = dst_arrays[0];\n+  EXPECT_EQ(dst_array->sharding(), *dst_array_spec.sharding);\n+  EXPECT_THAT(CopyArrayToLiteral(dst_array),\n+              IsOkAndHolds(Eq(std::cref(literal))));\n+}\n+\n+auto AllMemoryKinds() {\n+  return testing::Values(MemoryKind(\"device\"), MemoryKind(\"pinned_host\"),\n+                         MemoryKind(\"unpinned_host\"));\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    AllMemoryKinds, ReshardMemoryKindTest, AllMemoryKinds(),\n+    [](const testing::TestParamInfo<ReshardMemoryKindTest::ParamType>& info)\n+        -> std::string { return absl::StrCat(info.param); });\n+\n+struct ReshardTestParam {\n+  absl::string_view name;\n+\n+  Shape shape;\n+\n+  xla::HloSharding src_sharding;\n+  std::vector<int> src_device_indices;\n+\n+  xla::HloSharding dst_sharding;\n+  std::vector<int> dst_device_indices;\n+};\n+\n+class ReshardParameterizedTest\n+    : public ReshardTest,\n+      public testing::WithParamInterface<\n+          std::tuple<ReshardTestParam, MemoryKind, MemoryKind>> {};\n+\n+TEST_P(ReshardParameterizedTest, RoundTrip) {\n+  const auto& [param, src_memory_kind, dst_memory_kind] = GetParam();\n+\n+  absl::InlinedVector<Device*, 1> src_devices;\n+  src_devices.reserve(param.src_device_indices.size());\n+  for (const int index : param.src_device_indices) {\n+    src_devices.push_back(client_->devices()[index]);\n+  }\n+  TF_ASSERT_OK_AND_ASSIGN(const DeviceListRef src_device_list,\n+                          client_->MakeDeviceList(src_devices));\n+  const ShardingRef src_sharding = HloSharding::Create(\n+      std::move(src_device_list), src_memory_kind, param.src_sharding);\n+\n+  absl::InlinedVector<Device*, 1> dst_devices;\n+  dst_devices.reserve(param.dst_device_indices.size());\n+  for (const int index : param.dst_device_indices) {\n+    dst_devices.push_back(client_->devices()[index]);\n+  }\n+  TF_ASSERT_OK_AND_ASSIGN(const DeviceListRef dst_device_list,\n+                          client_->MakeDeviceList(dst_devices));\n+  const ShardingRef dst_sharding = HloSharding::Create(\n+      std::move(dst_device_list), dst_memory_kind, param.dst_sharding);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      const xla::Literal literal,\n+      CreateIotaLiteral(xla::PrimitiveType::S32, param.shape.dims()));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      ArrayRef src_array,\n+      MakeArrayFromLiteral(client_.get(), literal, src_sharding));\n+\n+  // Reshard from source to destination.\n+  ArrayRef dst_array;\n+  {\n+    SCOPED_TRACE(absl::StrCat(*src_sharding, \" -> \", *dst_sharding));\n+\n+    ArraySpec array_spec = {\n+        /*dtype=*/src_array->dtype(),\n+        /*shape=*/src_array->shape(),\n+        /*sharding=*/dst_sharding,\n+    };\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        std::vector<ArrayRef> dst_arrays,\n+        client_->ReshardArrays(absl::MakeSpan(&src_array, 1), {array_spec},\n+                               ArrayCopySemantics::kDonateInput));\n+    ASSERT_EQ(dst_arrays.size(), 1);\n+    dst_array = std::move(dst_arrays[0]);\n+\n+    EXPECT_EQ(dst_array->sharding(), *array_spec.sharding);\n+    EXPECT_THAT(CopyArrayToLiteral(dst_array),\n+                IsOkAndHolds(Eq(std::cref(literal))));\n+  }\n+\n+  // Reshard from destination back to source.\n+  {\n+    SCOPED_TRACE(absl::StrCat(*dst_sharding, \" -> \", *src_sharding));\n+\n+    ArraySpec array_spec = {\n+        /*dtype=*/dst_array->dtype(),\n+        /*shape=*/dst_array->shape(),\n+        /*sharding=*/src_sharding,\n+    };\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        std::vector<ArrayRef> src_arrays,\n+        client_->ReshardArrays(absl::MakeSpan(&dst_array, 1), {array_spec},\n+                               ArrayCopySemantics::kDonateInput));\n+    ASSERT_EQ(src_arrays.size(), 1);\n+    src_array = std::move(src_arrays[0]);\n+\n+    EXPECT_EQ(src_array->sharding(), *array_spec.sharding);\n+    EXPECT_THAT(CopyArrayToLiteral(src_array),\n+                IsOkAndHolds(Eq(std::cref(literal))));\n+  }\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    SameDeviceCount, ReshardParameterizedTest,\n+    testing::Combine(     //\n+        testing::Values(  //\n+            ReshardTestParam{\n+                /*name=*/\"ReplicateToReplicate\",\n+                /*shape=*/Shape({4, 8}),\n+                /*src_sharding=*/xla::HloSharding::Replicate(),\n+                /*src_device_indices=*/{0, 1, 2, 3, 4, 5, 6, 7},\n+                /*dst_sharding=*/xla::HloSharding::Replicate(),\n+                /*dst_device_indices=*/{0, 1, 2, 3, 4, 5, 6, 7},\n+            },\n+            ReshardTestParam{\n+                /*name=*/\"ReplicateToReplicateDeviceLayout\",\n+                /*shape=*/Shape({4, 8}),\n+                /*src_sharding=*/xla::HloSharding::Replicate(),\n+                /*src_device_indices=*/{0, 1, 2, 3, 4, 5, 6, 7},\n+                /*dst_sharding=*/xla::HloSharding::Replicate(),\n+                /*dst_device_indices=*/{0, 1, 2, 3, 4, 5, 6, 7},\n+            },\n+            ReshardTestParam{\n+                /*name=*/\"ReplicateToTile\",\n+                /*shape=*/Shape({4, 8}),\n+                /*src_sharding=*/xla::HloSharding::Replicate(),\n+                /*src_device_indices=*/{0, 1, 2, 3, 4, 5, 6, 7},\n+                /*dst_sharding=*/xla::HloSharding::IotaTile({4, 2}),\n+                /*dst_device_indices=*/{0, 1, 2, 3, 4, 5, 6, 7},\n+            },\n+            ReshardTestParam{\n+                /*name=*/\"TileToTile\",\n+                /*shape=*/Shape({4, 8}),\n+                /*src_sharding=*/xla::HloSharding::IotaTile({2, 4}),\n+                /*src_device_indices=*/{0, 1, 2, 3, 4, 5, 6, 7},\n+                /*dst_sharding=*/xla::HloSharding::IotaTile({4, 2}),\n+                /*dst_device_indices=*/{0, 1, 2, 3, 4, 5, 6, 7},\n+            },\n+            ReshardTestParam{\n+                /*name=*/\"TileToPartialTile\",\n+                /*shape=*/Shape({4, 8}),\n+                /*src_sharding=*/xla::HloSharding::IotaTile({4, 2}),\n+                /*src_device_indices=*/{0, 1, 2, 3, 4, 5, 6, 7},\n+                /*dst_sharding=*/\n+                xla::HloSharding::PartialTile(xla::TileAssignment({1, 4, 2})),\n+                /*dst_device_indices=*/{0, 1, 2, 3, 4, 5, 6, 7},\n+            },\n+            ReshardTestParam{\n+                /*name=*/\"ZeroSized\",\n+                /*shape=*/Shape({0, 4}),\n+                /*src_sharding=*/xla::HloSharding::IotaTile({4, 2}),\n+                /*src_device_indices=*/{0, 1, 2, 3, 4, 5, 6, 7},\n+                /*dst_sharding=*/xla::HloSharding::IotaTile({2, 4}),\n+                /*dst_device_indices=*/{0, 1, 2, 3, 4, 5, 6, 7},\n+            }),\n+        AllMemoryKinds(), AllMemoryKinds()),\n+    ([](const testing::TestParamInfo<ReshardParameterizedTest::ParamType>&\n+            info) {\n+      const auto& [param, src_memory_kind, dst_memory_kind] = info.param;\n+      return absl::StrCat(param.name, \"_\", src_memory_kind, \"_to_\",\n+                          dst_memory_kind);\n+    }));\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    DifferentDeviceCount, ReshardParameterizedTest,\n+    testing::Combine(     //\n+        testing::Values(  //\n+            ReshardTestParam{\n+                /*name=*/\"ReplicateToReplicate\",\n+                /*shape=*/Shape({4, 8}),\n+                /*src_sharding=*/xla::HloSharding::Replicate(),\n+                /*src_device_indices=*/{0, 1},\n+                /*dst_sharding=*/xla::HloSharding::Replicate(),\n+                /*dst_device_indices=*/{0, 1, 2, 3, 4, 5, 6, 7},\n+            },\n+            ReshardTestParam{\n+                /*name=*/\"ReplicateToReplicateDeviceLayout\",\n+                /*shape=*/Shape({4, 8}),\n+                /*src_sharding=*/xla::HloSharding::Replicate(),\n+                /*src_device_indices=*/{0, 1},\n+                /*dst_sharding=*/xla::HloSharding::Replicate(),\n+                /*dst_device_indices=*/{0, 1, 2, 3, 4, 5, 6, 7},\n+            },\n+            ReshardTestParam{\n+                /*name=*/\"ReplicateToTile\",\n+                /*shape=*/Shape({4, 8}),\n+                /*src_sharding=*/xla::HloSharding::Replicate(),\n+                /*src_device_indices=*/{0, 1},\n+                /*dst_sharding=*/xla::HloSharding::IotaTile({4, 2}),\n+                /*dst_device_indices=*/{0, 1, 2, 3, 4, 5, 6, 7},\n+            },\n+            ReshardTestParam{\n+                /*name=*/\"TileToTile\",\n+                /*shape=*/Shape({4, 4, 4}),\n+                /*src_sharding=*/xla::HloSharding::IotaTile({2, 2, 2}),\n+                /*src_device_indices=*/{0, 1, 2, 3, 4, 5, 6, 7},\n+                /*dst_sharding=*/xla::HloSharding::IotaTile({1, 2, 1}),\n+                /*dst_device_indices=*/{4, 0},\n+            },\n+            ReshardTestParam{\n+                /*name=*/\"TileToPartialTile\",\n+                /*shape=*/Shape({4, 8}),\n+                /*src_sharding=*/xla::HloSharding::IotaTile({2, 1}),\n+                /*src_device_indices=*/{1, 0},\n+                /*dst_sharding=*/\n+                xla::HloSharding::PartialTile(xla::TileAssignment({1, 4, 2})),\n+                /*dst_device_indices=*/{0, 1, 2, 3, 4, 5, 6, 7},\n+            },\n+            ReshardTestParam{\n+                /*name=*/\"ZeroSized\",\n+                /*shape=*/Shape({0, 4}),\n+                /*src_sharding=*/xla::HloSharding::IotaTile({2, 1}),\n+                /*src_device_indices=*/{0, 1},\n+                /*dst_sharding=*/xla::HloSharding::IotaTile({2, 4}),\n+                /*dst_device_indices=*/{0, 1, 2, 3, 4, 5, 6, 7},\n+            }),\n+        AllMemoryKinds(), AllMemoryKinds()),\n+    ([](const testing::TestParamInfo<ReshardParameterizedTest::ParamType>&\n+            info) {\n+      const auto& [param, src_memory_kind, dst_memory_kind] = info.param;\n+      return absl::StrCat(param.name, \"_\", src_memory_kind, \"_to_\",\n+                          dst_memory_kind);\n+    }));\n+\n+}  // namespace\n+}  // namespace ifrt\n+}  // namespace xla"
        }
    ],
    "stats": {
        "total": 729,
        "additions": 729,
        "deletions": 0
    }
}