{
    "author": "tensorflower-gardener",
    "message": "[Autotuner] Use DebugOptions to create autouner and profiler config.\n\n1. Always set cache even if per_fusion_cache_dir is not set as there are other ways to initialize the legacy cache in XLA ex: LoadAutotuneResultsFromFile\n2. Some recent submits changed the instructions in autotuner cache but did not break gpu_compiler_test as xla_gpu_require_complete_aot_autotune_results is a no-op in the new autotuner before this change. So updating the cache entries to match latest instructions.\n\nPiperOrigin-RevId: 806248837",
    "sha": "aa3c1fd281c82bd60e793d2888be8d184d9653b3",
    "files": [
        {
            "sha": "3cf8540777b8e13ed20e7186bbfba2fe56ccb28f",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa3c1fd281c82bd60e793d2888be8d184d9653b3/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa3c1fd281c82bd60e793d2888be8d184d9653b3/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc?ref=aa3c1fd281c82bd60e793d2888be8d184d9653b3",
            "patch": "@@ -107,7 +107,7 @@ absl::StatusOr<Autotuner::Config> Autotuner::GetCachedOrTuneBestConfig(\n     best_config = std::move(*cached_config);\n   } else {\n     if (autotune_config_.expect_all_instructions_in_cache) {\n-      return absl::InternalError(\"No cached config found for HLO instr: \" +\n+      return absl::NotFoundError(\"No cached config found for HLO instr: \" +\n                                  instr->ToString());\n     }\n     TF_ASSIGN_OR_RETURN(best_config, TuneBestConfig(instr));"
        },
        {
            "sha": "eb12fd40eb8d4d6c50c96049cf177547507e1aa5",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner_test.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 4,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa3c1fd281c82bd60e793d2888be8d184d9653b3/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa3c1fd281c82bd60e793d2888be8d184d9653b3/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc?ref=aa3c1fd281c82bd60e793d2888be8d184d9653b3",
            "patch": "@@ -120,10 +120,10 @@ class MockAutotunerCache : public AutotunerCacheInterface {\n               (override));\n };\n \n+using absl_testing::IsOk;\n+using absl_testing::StatusIs;\n using ::testing::_;\n using ::testing::Return;\n-using tsl::testing::IsOk;\n-using tsl::testing::StatusIs;\n \n se::DeviceDescription CreateDummyDeviceDescription() {\n   se::DeviceDescription desc;\n@@ -224,7 +224,7 @@ TEST_F(AutotunerTest, AutotuneButNoSupportedConfigs) {\n                         std::move(cache_manager)));\n   auto dummy_instr = HloInstruction::CreateConstant(LiteralUtil::CreateR0(1));\n   EXPECT_THAT(autotuner->Autotune(dummy_instr.get()),\n-              absl_testing::StatusIs(absl::StatusCode::kInternal));\n+              StatusIs(absl::StatusCode::kInternal));\n }\n \n TEST_F(AutotunerTest, AutotuneButNoCompiledConfigs) {\n@@ -252,7 +252,7 @@ TEST_F(AutotunerTest, AutotuneButNoCompiledConfigs) {\n                         std::move(cache_manager)));\n   auto dummy_instr = HloInstruction::CreateConstant(LiteralUtil::CreateR0(1));\n   EXPECT_THAT(autotuner->Autotune(dummy_instr.get()),\n-              absl_testing::StatusIs(absl::StatusCode::kInternal));\n+              StatusIs(absl::StatusCode::kInternal));\n }\n \n TEST_F(AutotunerTest, AutotuneAppliesBestConfigAndSkipsNonCompilableConfig) {\n@@ -502,5 +502,25 @@ TEST_F(AutotunerTest, AutotuneWithScratchBytesOptimization) {\n   EXPECT_THAT(autotuner->Autotune(dummy_instr.get()), IsOk());\n }\n \n+TEST_F(AutotunerTest, ExpectAllInstructionsInCache) {\n+  auto cache_manager = std::make_unique<MockAutotunerCache>();\n+  EXPECT_CALL(*cache_manager, Lookup(_)).WillOnce(Return(std::nullopt));\n+  EXPECT_CALL(*cache_manager, Insert(_, _)).Times(0);\n+\n+  config_.expect_all_instructions_in_cache = true;\n+\n+  auto backend = std::make_unique<MockCodegenBackend>();\n+  EXPECT_CALL(*backend, GetSupportedConfigs).Times(0);\n+  std::vector<std::unique_ptr<CodegenBackend>> backends;\n+  backends.push_back(std::move(backend));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto autotuner, Autotuner::Create(std::move(backends), nullptr, config_,\n+                                        std::move(cache_manager)));\n+  auto dummy_instr = HloInstruction::CreateConstant(LiteralUtil::CreateR0(1));\n+  EXPECT_THAT(autotuner->Autotune(dummy_instr.get()),\n+              StatusIs(absl::StatusCode::kNotFound));\n+}\n+\n }  // namespace\n }  // namespace xla"
        },
        {
            "sha": "0b34d850c7fed2dab27cc2b738ba20f403d91731",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 18,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa3c1fd281c82bd60e793d2888be8d184d9653b3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa3c1fd281c82bd60e793d2888be8d184d9653b3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc?ref=aa3c1fd281c82bd60e793d2888be8d184d9653b3",
            "patch": "@@ -44,6 +44,29 @@ limitations under the License.\n namespace xla {\n namespace gpu {\n \n+namespace {\n+\n+AutotuneConfig GetAutotuneConfig(const DebugOptions& debug_options) {\n+  AutotuneConfig autotune_config;\n+  autotune_config.check_buffers = debug_options.xla_gpu_autotune_level() >= 4;\n+  autotune_config.relative_tolerance =\n+      debug_options.xla_gpu_autotune_gemm_rtol();\n+  autotune_config.crash_on_check_failure =\n+      debug_options.xla_gpu_crash_on_verification_failures();\n+  autotune_config.expect_all_instructions_in_cache =\n+      debug_options.xla_gpu_require_complete_aot_autotune_results();\n+  return autotune_config;\n+}\n+\n+ProfileOptions GetProfileOptions(const DebugOptions& debug_options) {\n+  ProfileOptions profile_options;\n+  profile_options.redzone_padding_bytes =\n+      debug_options.xla_gpu_redzone_padding_bytes();\n+  return profile_options;\n+}\n+\n+}  // namespace\n+\n absl::StatusOr<std::unique_ptr<AutotunerPass>> AutotunerPass::Create(\n     std::vector<std::unique_ptr<CodegenBackend>> backends,\n     const DebugOptions& debug_options,\n@@ -53,29 +76,20 @@ absl::StatusOr<std::unique_ptr<AutotunerPass>> AutotunerPass::Create(\n   // At least one of stream_executor or allocator must be provided.\n   CHECK(stream_executor != nullptr || allocator != nullptr);\n \n-  std::unique_ptr<GpuProfiler> profiler =\n-      GpuProfiler::Create(stream_executor, ProfileOptions(), allocator);\n-\n-  std::unique_ptr<AutotunerCacheInterface> cache = nullptr;\n-  const std::string& cache_dir =\n-      debug_options.xla_gpu_experimental_autotuner_cache_dir();\n-  if (!cache_dir.empty()) {\n-    cache = std::make_unique<LegacyCache>(\n-        cache_dir, debug_options.xla_gpu_experimental_autotune_cache_mode(),\n-        stream_executor->GetDeviceDescription());\n-  }\n+  std::unique_ptr<GpuProfiler> profiler = GpuProfiler::Create(\n+      stream_executor, GetProfileOptions(debug_options), allocator);\n \n-  AutotuneConfig autotune_config;\n-  autotune_config.check_buffers = debug_options.xla_gpu_autotune_level() >= 4;\n-  autotune_config.relative_tolerance =\n-      debug_options.xla_gpu_autotune_gemm_rtol();\n-  autotune_config.crash_on_check_failure =\n-      debug_options.xla_gpu_crash_on_verification_failures();\n+  std::unique_ptr<AutotunerCacheInterface> cache =\n+      std::make_unique<LegacyCache>(\n+          debug_options.xla_gpu_experimental_autotuner_cache_dir(),\n+          debug_options.xla_gpu_experimental_autotune_cache_mode(),\n+          stream_executor->GetDeviceDescription());\n \n   TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<Autotuner> autotuner,\n       Autotuner::Create(std::move(backends), std::move(profiler),\n-                        autotune_config, std::move(cache), thread_pool));\n+                        GetAutotuneConfig(debug_options), std::move(cache),\n+                        thread_pool));\n   return absl::WrapUnique(\n       new AutotunerPass(std::move(autotuner), should_autotune));\n }"
        },
        {
            "sha": "6a7fe1e3467af5ffefc52cd35ba81ff5ae896edf",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 30,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa3c1fd281c82bd60e793d2888be8d184d9653b3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa3c1fd281c82bd60e793d2888be8d184d9653b3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc?ref=aa3c1fd281c82bd60e793d2888be8d184d9653b3",
            "patch": "@@ -197,41 +197,23 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotunedAndCached) {\n   ASSERT_TRUE(gpu_backend_config_after_first_run.gemm_backend_config()\n                   .has_selected_algorithm());\n \n-  // Find the cache file and make sure it's not empty.\n-  std::vector<std::string> children;\n-  TF_ASSERT_OK(tsl::Env::Default()->GetChildren(cache_dir, &children));\n-  std::string cache_file;\n-  for (const auto& child : children) {\n-    std::string filename = tsl::io::JoinPath(cache_dir, child);\n-    if (!tsl::Env::Default()->IsDirectory(filename).ok()) {\n-      uint64_t file_size;\n-      TF_ASSERT_OK(tsl::Env::Default()->GetFileSize(filename, &file_size));\n-      if (file_size > 0) {\n-        cache_file = filename;\n-        break;\n-      }\n-    }\n-  }\n-  ASSERT_FALSE(cache_file.empty());\n+  // Run the pass on the same original HLO reusing the cache\n+  // Make sure it hits the cache by setting\n+  // xla_gpu_require_complete_aot_autotune_results to true.\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module_2,\n+                          ParseAndReturnVerifiedModule(kCublasCustomCallHlo));\n \n-  // Clear the selected algorithm to simulate a pre-autotuning state.\n-  HloInstruction* custom_call =\n-      module->entry_computation()->GetInstructionWithName(\"custom-call.1\");\n-  TF_ASSERT_OK_AND_ASSIGN(auto gpu_backend_config_before_second_run,\n-                          custom_call->backend_config<GpuBackendConfig>());\n-  GemmBackendConfig gemm_config =\n-      gpu_backend_config_before_second_run.gemm_backend_config();\n-  gemm_config.clear_selected_algorithm();\n-  *gpu_backend_config_before_second_run.mutable_gemm_backend_config() =\n-      gemm_config;\n-  TF_ASSERT_OK(\n-      custom_call->set_backend_config(gpu_backend_config_before_second_run));\n-\n-  // Run the pass for the second time, this should hit the cache.\n+  module_2->mutable_config()\n+      .mutable_debug_options()\n+      .set_xla_gpu_experimental_autotuner_cache_dir(cache_dir);\n+  module_2->mutable_config()\n+      .mutable_debug_options()\n+      .set_xla_gpu_require_complete_aot_autotune_results(true);\n   {\n     std::vector<std::unique_ptr<CodegenBackend>> backends2;\n     backends2.push_back(std::make_unique<CublasBackend>(\n         stream_executor_, &module->config().debug_options(), &compiler_));\n+\n     TF_ASSERT_OK_AND_ASSIGN(\n         std::unique_ptr<AutotunerPass> pass2,\n         AutotunerPass::Create(std::move(backends2),"
        },
        {
            "sha": "cae3c97108cffc4a405f2a0109df5eb56f4e9899",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test_autotune_db.textproto",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa3c1fd281c82bd60e793d2888be8d184d9653b3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa3c1fd281c82bd60e793d2888be8d184d9653b3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto?ref=aa3c1fd281c82bd60e793d2888be8d184d9653b3",
            "patch": "@@ -75,7 +75,7 @@ results {\n }\n results {\n   device: \"CUDA: 9.0, Cores: 132, GPU clock: 1.98 GHz, Memory bandwidth: 3352 GB/s, L2 cache: 50 MB, DNN version: 1.2.3\"\n-  hlo: \"(bf16[128,1024,1024]{2,1,0}, s8[33554432]{0}) custom-call(bf16[128,1024,1024]{2,1,0}, bf16[128,1024,1024]{2,1,0}), custom_call_target=\\\"__cublas$gemm\\\", backend_config={\\\"force_earliest_schedule\\\":false,\\\"gemm_backend_config\\\":{\\\"alpha_imag\\\":0,\\\"alpha_real\\\":1,\\\"beta\\\":0,\\\"damax_output\\\":false,\\\"dot_dimension_numbers\\\":{\\\"lhs_batch_dimensions\\\":[\\\"0\\\"],\\\"lhs_contracting_dimensions\\\":[\\\"2\\\"],\\\"rhs_batch_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\",\\\"grad_x\\\":false,\\\"grad_y\\\":false,\\\"lhs_stride\\\":\\\"1048576\\\",\\\"precision_config\\\":{\\\"algorithm\\\":\\\"ALG_UNSET\\\",\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"rhs_stride\\\":\\\"1048576\\\"},\\\"operation_queue_id\\\":\\\"0\\\",\\\"wait_on_operation_queues\\\":[]}\"\n+  hlo: \"(bf16[128,1024,1024]{2,1,0}, s8[33554432]{0}) custom-call(bf16[128,1024,1024]{2,1,0}, bf16[128,1024,1024]{2,1,0}), custom_call_target=\\\"__cublas$gemm\\\", backend_config={\\\"device_type\\\":\\\"DEVICE_TYPE_INVALID\\\",\\\"force_earliest_schedule\\\":false,\\\"gemm_backend_config\\\":{\\\"alpha_imag\\\":0,\\\"alpha_real\\\":1,\\\"beta\\\":0,\\\"damax_output\\\":false,\\\"dot_dimension_numbers\\\":{\\\"lhs_batch_dimensions\\\":[\\\"0\\\"],\\\"lhs_contracting_dimensions\\\":[\\\"2\\\"],\\\"rhs_batch_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\",\\\"grad_x\\\":false,\\\"grad_y\\\":false,\\\"lhs_stride\\\":\\\"1048576\\\",\\\"precision_config\\\":{\\\"algorithm\\\":\\\"ALG_UNSET\\\",\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"rhs_stride\\\":\\\"1048576\\\"},\\\"operation_queue_id\\\":\\\"0\\\",\\\"reification_cost\\\":[],\\\"wait_on_operation_queues\\\":[]}\"\n   result {\n     gemm {\n       algorithm: -1\n@@ -87,7 +87,7 @@ results {\n }\n results {\n   device: \"CUDA: 9.0, Cores: 132, GPU clock: 1.98 GHz, Memory bandwidth: 3352 GB/s, L2 cache: 50 MB, DNN version: 1.2.3\"\n-  hlo: \"(bf16[12288,16384]{1,0}, s8[33554432]{0}) custom-call(f8e4m3fn[4096,12288]{0,1}, f8e4m3fn[4096,16384]{0,1}, f32[], f32[], f32[], f32[]), custom_call_target=\\\"__cublas$lt$matmul$f8\\\", backend_config={\\\"force_earliest_schedule\\\":false,\\\"gemm_backend_config\\\":{\\\"alpha_imag\\\":0,\\\"alpha_real\\\":0.95703125,\\\"beta\\\":0,\\\"damax_output\\\":false,\\\"dot_dimension_numbers\\\":{\\\"lhs_batch_dimensions\\\":[],\\\"lhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"rhs_batch_dimensions\\\":[],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\",\\\"grad_x\\\":false,\\\"grad_y\\\":false,\\\"lhs_stride\\\":\\\"50331648\\\",\\\"precision_config\\\":{\\\"algorithm\\\":\\\"ALG_UNSET\\\",\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"rhs_stride\\\":\\\"67108864\\\"},\\\"operation_queue_id\\\":\\\"0\\\",\\\"wait_on_operation_queues\\\":[]}\"\n+  hlo: \"(bf16[12288,16384]{1,0}, s8[33554432]{0}) custom-call(f8e4m3fn[12288,4096]{1,0}, f8e4m3fn[4096,16384]{0,1}, f32[], f32[]), custom_call_target=\\\"__cublas$lt$matmul$f8\\\", backend_config={\\\"device_type\\\":\\\"DEVICE_TYPE_INVALID\\\",\\\"force_earliest_schedule\\\":false,\\\"gemm_backend_config\\\":{\\\"alpha_imag\\\":0,\\\"alpha_real\\\":0.95703125,\\\"beta\\\":0,\\\"damax_output\\\":false,\\\"dot_dimension_numbers\\\":{\\\"lhs_batch_dimensions\\\":[],\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_batch_dimensions\\\":[],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\",\\\"grad_x\\\":false,\\\"grad_y\\\":false,\\\"lhs_stride\\\":\\\"50331648\\\",\\\"precision_config\\\":{\\\"algorithm\\\":\\\"ALG_UNSET\\\",\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"rhs_stride\\\":\\\"67108864\\\"},\\\"operation_queue_id\\\":\\\"0\\\",\\\"reification_cost\\\":[],\\\"wait_on_operation_queues\\\":[]}\"\n   result {\n     gemm {\n     }"
        }
    ],
    "stats": {
        "total": 126,
        "additions": 71,
        "deletions": 55
    }
}