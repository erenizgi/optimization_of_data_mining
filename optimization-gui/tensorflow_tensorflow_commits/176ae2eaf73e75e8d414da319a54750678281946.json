{
    "author": "tensorflower-gardener",
    "message": "[Autotuner] Propagate dumping from debug options to autotuner.\n\nPiperOrigin-RevId: 827438081",
    "sha": "176ae2eaf73e75e8d414da319a54750678281946",
    "files": [
        {
            "sha": "1b25331b71095e71b10c1d8d993e1a438455aa8c",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/176ae2eaf73e75e8d414da319a54750678281946/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/176ae2eaf73e75e8d414da319a54750678281946/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc?ref=176ae2eaf73e75e8d414da319a54750678281946",
            "patch": "@@ -73,6 +73,8 @@ AutotuneConfig GetAutotuneConfig(const DebugOptions& debug_options,\n \n   autotune_config.expect_all_instructions_in_cache =\n       debug_options.xla_gpu_require_complete_aot_autotune_results();\n+  autotune_config.dump_hlos =\n+      debug_options.xla_gpu_dump_autotuned_gemm_fusions();\n \n   return autotune_config;\n }"
        },
        {
            "sha": "74d7ec40b22b3b77c93fd51d3c35a09fd208a5e7",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/176ae2eaf73e75e8d414da319a54750678281946/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/176ae2eaf73e75e8d414da319a54750678281946/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=176ae2eaf73e75e8d414da319a54750678281946",
            "patch": "@@ -485,6 +485,8 @@ message DebugOptions {\n   // compilation, possibly multiple times per process. This only works on CUDA.\n   optional string xla_gpu_dump_autotune_results_to = 222;\n \n+  // The flag is being generalized to dump all autotuned instructions as we\n+  // combine the autotuner passes into a single pass.\n   optional bool xla_gpu_dump_autotuned_gemm_fusions = 232;\n \n   // Whether to dump llvm ir when compiling to ptx."
        }
    ],
    "stats": {
        "total": 4,
        "additions": 4,
        "deletions": 0
    }
}