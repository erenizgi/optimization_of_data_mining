{
    "author": "beckerhe",
    "message": "Skip FP8 fused attention tests on cuDNN 9.10.0.\n\ncuDNN 9.10.0 does not support FP8 Scaled Dot Product Attention (SDPA) or Flash Attention, so tests relying on this functionality are skipped for this specific cuDNN version.\n\nSupport has been removed with the release of cudnn_frontend 1.12. There is no mention in the release notes why. I suspect something is broken.\n\nPiperOrigin-RevId: 802898130",
    "sha": "dbde2d6995cc3e21bf0530762433ce2f448d1463",
    "files": [
        {
            "sha": "dc56f417e3397be0d70525a9cfcf3633967da0df",
            "filename": "third_party/xla/xla/service/gpu/tests/gpu_fused_mha_test.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 2,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dbde2d6995cc3e21bf0530762433ce2f448d1463/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_fused_mha_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dbde2d6995cc3e21bf0530762433ce2f448d1463/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_fused_mha_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_fused_mha_test.cc?ref=dbde2d6995cc3e21bf0530762433ce2f448d1463",
            "patch": "@@ -1695,11 +1695,17 @@ absl::string_view GetModuleFlashAttentionBMMScaleSoftmaxBMMCommonF8() {\n // BMM1 - Scale - Softmax - BMM2 fp8\n TEST_F(FlashAttentionBMMScaleSoftmaxBMMF8,\n        Flash_Attention_Inference_BMM1_NoMask_Softmax_BMM2_BNTH_F8) {\n-  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) {\n+    GTEST_SKIP() << *skip_reason_;\n+  }\n   if (GetDnnVersionInfoOrDefault(backend().default_stream_executor()) <\n       se::dnn::VersionInfo(9, 1, 0)) {\n     GTEST_SKIP() << \"Flash Attention requires cuDNN >= 9.1.0.\";\n   }\n+  if (GetDnnVersionInfoOrDefault(backend().default_stream_executor()) ==\n+      se::dnn::VersionInfo(9, 10, 0)) {\n+    GTEST_SKIP() << \"Flash Attention is not supported in cuDNN 9.10.0.\";\n+  }\n   auto cc = GetCudaComputeCapability();\n   if (!cc.IsAtLeastHopper()) {\n     GTEST_SKIP() << \"Flash Attention fp8 requires at least Hopper.\";\n@@ -1872,11 +1878,17 @@ TEST_F(FlashAttentionBMMScaleSoftmaxBMMF8,\n \n TEST_F(FlashAttentionBMMScaleSoftmaxBMMF8,\n        Flash_Attention_Inference_BMM1_NoMask_Softmax_BMM2_BTNH_F8) {\n-  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) {\n+    GTEST_SKIP() << *skip_reason_;\n+  }\n   if (GetDnnVersionInfoOrDefault(backend().default_stream_executor()) <\n       se::dnn::VersionInfo(9, 1, 0)) {\n     GTEST_SKIP() << \"Flash Attention requires cuDNN >= 9.1.0.\";\n   }\n+  if (GetDnnVersionInfoOrDefault(backend().default_stream_executor()) ==\n+      se::dnn::VersionInfo(9, 10, 0)) {\n+    GTEST_SKIP() << \"Flash Attention is not supported in cuDNN 9.10.0.\";\n+  }\n   auto cc = GetCudaComputeCapability();\n   if (!cc.IsAtLeastHopper()) {\n     GTEST_SKIP() << \"Flash Attention fp8 requires at least Hopper.\";"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 14,
        "deletions": 2
    }
}