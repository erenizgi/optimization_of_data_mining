{
    "author": "jinliangwei",
    "message": "[XLA] Make the memory dump a little more readable for large tuples: print the subshape instead of the full shape.\n\nPiperOrigin-RevId: 797445036",
    "sha": "9c75e950c0bfbec78a99d93e980d04ec92e2c88f",
    "files": [
        {
            "sha": "c16dcce8105f4ee9024ac14758e5edc5ace4135d",
            "filename": "third_party/xla/xla/service/buffer_assignment.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9c75e950c0bfbec78a99d93e980d04ec92e2c88f/third_party%2Fxla%2Fxla%2Fservice%2Fbuffer_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9c75e950c0bfbec78a99d93e980d04ec92e2c88f/third_party%2Fxla%2Fxla%2Fservice%2Fbuffer_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fbuffer_assignment.cc?ref=9c75e950c0bfbec78a99d93e980d04ec92e2c88f",
            "patch": "@@ -404,8 +404,10 @@ std::string BufferAllocation::ToShortString(bool human_readable_size) const {\n   if (is_entry_computation_parameter()) {\n     const HloInstruction* param = GetEntryParameterInstruction(*this);\n     StrAppend(&output, \", parameter \", parameter_number(), \", shape |\",\n-              param ? param->shape().ToString(/*print_layout=*/false)\n-                    : \"<unknown shape>\",\n+              param\n+                  ? ShapeUtil::GetSubshape(param->shape(), param_shape_index())\n+                        .ToString(/*print_layout=*/false)\n+                  : \"<unknown shape>\",\n               \"| at ShapeIndex \", param_shape_index().ToString());\n   }\n   if (const HloInstruction* instr = GetOutputInstruction(*this)) {"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 4,
        "deletions": 2
    }
}