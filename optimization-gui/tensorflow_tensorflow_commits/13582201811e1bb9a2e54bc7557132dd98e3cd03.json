{
    "author": "othakkar",
    "message": "PR #31511: [XLA:CPU][oneDNN] Enable oneDNN Convolution Custom Calls in Thunk Runtime\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31511\n\nThis PR enables support for oneDNN Convolution operations in the XLA:CPU Thunk runtime, building upon the foundational implementation of `OneDnnOpThunk`.\n\nKey changes:\n- Updated thunk emitter to emit `OneDnnOpThunk` for oneDNN convolution during compilation.\n- Enabled oneDNN custom call rewrite for Convolution in `onednn_contraction_rewriter.cc`.\n- Added support for oneDNN Convolution op via `ExecuteOneDnnConvolution(...)` in `onednn_convolution.cc`.\nCopybara import of the project:\n\n--\nae78caeb97d61ee94b9ddf3111abb157ce2d4241 by Om Thakkar <om.thakkar@intel.com>:\n\nenable oneDNN convolution in thunk runtime\n\n--\n0e4bac41a8daf88c878333d6645ab93bbcd541a1 by Om Thakkar <om.thakkar@intel.com>:\n\nadd a test for oneDNN conv thunk\n\n--\nc103dd964d6a59319c30c9aabb4345dfa4a882c7 by Om Thakkar <om.thakkar@intel.com>:\n\naddressing review comments\n\n--\n7f32dadefc39c53ca1421dac051e8ba8ae9bb6f0 by Om Thakkar <om.thakkar@intel.com>:\n\nclang-format checks\n\n--\n008699fb65fb79059f5eb26d69f9ae913bd74c3c by Om Thakkar <om.thakkar@intel.com>:\n\nrestore changes lost while merging with main\n\n--\n1bc5c7364e81e4e867d4763ae57eb1c15907d01e by Om Thakkar <om.thakkar@intel.com>:\n\nremove dead-code related to oneDNN convolution for legacy runtime\n\n--\n9a4ba20c1624fb3d51e3a43cb805e1f4ce7dede7 by Om Thakkar <om.thakkar@intel.com>:\n\nfixup: onednn_contraction_rewriter - lint related\n\n--\n21bf9f7e4dd11f360c38056ff1a262a8d790f80b by Om Thakkar <om.thakkar@intel.com>:\n\nreplace auto with corresponding type\n\nMerging this change closes #31511\n\nPiperOrigin-RevId: 814610643",
    "sha": "13582201811e1bb9a2e54bc7557132dd98e3cd03",
    "files": [
        {
            "sha": "298b434317cfb6bb19990578068dc89a58de8933",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD?ref=13582201811e1bb9a2e54bc7557132dd98e3cd03",
            "patch": "@@ -128,6 +128,7 @@ cc_library(\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service/cpu:onednn_config_proto_cc\",\n+        \"//xla/service/cpu:onednn_convolution\",\n         \"//xla/service/cpu:onednn_matmul\",\n         \"//xla/service/cpu:onednn_memory_util\",\n         \"//xla/stream_executor:device_memory\","
        },
        {
            "sha": "d8c29713b6fcd6c7ede5108ce084d6202f9c08cd",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_op_thunk.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.cc?ref=13582201811e1bb9a2e54bc7557132dd98e3cd03",
            "patch": "@@ -36,6 +36,7 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/onednn/onednn_threadpool.h\"\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/runtime/buffer_use.h\"\n+#include \"xla/service/cpu/onednn_convolution.h\"\n #include \"xla/service/cpu/onednn_matmul.h\"\n #include \"xla/service/cpu/onednn_memory_util.h\"\n #include \"xla/stream_executor/device_memory.h\"\n@@ -100,6 +101,10 @@ OneDnnOpThunk::OneDnnRuntime::Invoke(\n     const auto& matmul_config = std::get<OneDnnMatMulConfig>(config);\n     ExecuteOneDnnMatMul(arguments, results, matmul_config, cpu_engine,\n                         onednn_stream, resources);\n+  } else if (target == \"__onednn$convolution\") {\n+    const auto& conv_config = std::get<OneDnnConvolutionConfig>(config);\n+    ExecuteOneDnnConvolution(arguments, results, conv_config, cpu_engine,\n+                             onednn_stream, resources);\n   } else {\n     return absl::InvalidArgumentError(\n         absl::StrFormat(\"Unsupported oneDNN operation target: `%s`\", target));"
        },
        {
            "sha": "7b6f721f03e317098bd2d9150076c472d8475901",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_op_thunk.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.h?ref=13582201811e1bb9a2e54bc7557132dd98e3cd03",
            "patch": "@@ -46,7 +46,8 @@ class OneDnnOpThunk : public Thunk {\n \n   // Variant config for supported oneDNN ops.\n   // TODO(intel-tf): Add more oneDNN operation configs as needed.\n-  using OneDnnOpConfig = std::variant<OneDnnMatMulConfig>;\n+  using OneDnnOpConfig =\n+      std::variant<OneDnnMatMulConfig, OneDnnConvolutionConfig>;\n \n   static absl::StatusOr<std::unique_ptr<OneDnnOpThunk>> Create(\n       const std::string& custom_call_target, Info info, OpBuffers buffers,"
        },
        {
            "sha": "cc7536e34efbd4cd6657b73a70dab48cab246d57",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_op_thunk_test.cc",
            "status": "modified",
            "additions": 136,
            "deletions": 7,
            "changes": 143,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk_test.cc?ref=13582201811e1bb9a2e54bc7557132dd98e3cd03",
            "patch": "@@ -71,13 +71,11 @@ TEST(OneDnnOpThunkTest, SimpleOneDnnMatMulThunk) {\n       Array2D<float>({{0.f, 0.f}, {0.f, 0.f}}));\n \n   // Create buffer allocations and slices\n-  auto lhs_alloc = CreateBufferAllocation(0, lhs_literal);\n-  auto rhs_alloc = CreateBufferAllocation(1, rhs_literal);\n-  auto out_alloc = CreateBufferAllocation(2, out_literal);\n+  auto [lhs_alloc, rhs_alloc, out_alloc] =\n+      CreateBufferAllocation(lhs_literal, rhs_literal, out_literal);\n \n-  auto lhs_slice = CreateBufferAllocationSlice(lhs_alloc);\n-  auto rhs_slice = CreateBufferAllocationSlice(rhs_alloc);\n-  auto out_slice = CreateBufferAllocationSlice(out_alloc);\n+  auto [lhs_slice, rhs_slice, out_slice] =\n+      CreateBufferAllocationSlice(lhs_alloc, rhs_alloc, out_alloc);\n \n   BufferAllocations allocations =\n       CreateBufferAllocations(lhs_literal, rhs_literal, out_literal);\n@@ -100,7 +98,7 @@ TEST(OneDnnOpThunkTest, SimpleOneDnnMatMulThunk) {\n   params.intra_op_threadpool = &device;\n \n   // Execute the thunk\n-  auto exec_event = thunk->Execute(params);\n+  tsl::AsyncValueRef<Thunk::ExecuteEvent> exec_event = thunk->Execute(params);\n   tsl::BlockUntilReady(exec_event);\n   ASSERT_FALSE(exec_event.IsError()) << \"OneDnnOpThunk execution failed\";\n \n@@ -112,5 +110,136 @@ TEST(OneDnnOpThunkTest, SimpleOneDnnMatMulThunk) {\n   EXPECT_EQ(out_literal, expected);\n }\n \n+// Small 2D NHWC convolution test for OneDnnOpThunk.\n+// Input: 1x3x3x1, Kernel: 2x2x1x1 with values [[1,0],[0,1]] (HWIO).\n+// Output (valid conv): each element = top-left + bottom-right of 2x2 patch:\n+// [[1+5, 2+6], [4+8, 5+9]] = [[6, 8], [12, 14]].\n+// Layout metadata uses one-based spatial dim indices.\n+// Window parameter encoding (matches runtime expectations defined in\n+// onednn_contraction_rewriter.cc):\n+//   strides stored as (actual + 1)\n+//   pads stored as (actual + 1)\n+//   dilations stored as (actual + 1).\n+TEST(OneDnnOpThunkTest, SimpleOneDnnConvolutionThunk) {\n+  tsl::thread::ThreadPool threads(tsl::Env::Default(), \"test\", 4);\n+  Eigen::ThreadPoolDevice device(threads.AsEigenThreadPool(),\n+                                 threads.NumThreads());\n+\n+  // Input: N=1,H=3,W=3,C=1 (NHWC)\n+  // Weights: KH=2, KW=2, IC=1, OC=1 (HWIO)\n+  // Output: N=1,H=2,W=2,C=1 (stride=1, no pad)\n+  Shape input_shape = ShapeUtil::MakeShape(F32, {1, 3, 3, 1});\n+  Shape weight_shape = ShapeUtil::MakeShape(F32, {2, 2, 1, 1});\n+  Shape output_shape = ShapeUtil::MakeShape(F32, {1, 2, 2, 1});\n+\n+  // Input data (H x W):\n+  // 1 2 3\n+  // 4 5 6\n+  // 7 8 9\n+  Literal input_literal =\n+      LiteralUtil::CreateR4FromArray4D<float>(Array4D<float>(\n+          1, 3, 3, 1, {1.f, 2.f, 3.f, 4.f, 5.f, 6.f, 7.f, 8.f, 9.f}));\n+\n+  // 2x2 kernel:\n+  // 1 0\n+  // 0 1\n+  Literal weight_literal = LiteralUtil::CreateR4FromArray4D<float>(\n+      Array4D<float>(2, 2, 1, 1, {1.f, 0.f, 0.f, 1.f}));\n+\n+  // Output buffer init zeros\n+  Literal output_literal = LiteralUtil::CreateR4FromArray4D<float>(\n+      Array4D<float>(1, 2, 2, 1, {0.f, 0.f, 0.f, 0.f}));\n+\n+  // Expected convolution (valid):\n+  // [[ (1*1 +2*0 +4*0 +5*1)=6,  (2*1 +3*0 +5*0 +6*1)=8 ],\n+  //  [ (4*1 +5*0 +7*0 +8*1)=12, (5*1 +6*0 +8*0 +9*1)=14 ]]\n+  Literal expected_literal = LiteralUtil::CreateR4FromArray4D<float>(\n+      Array4D<float>(1, 2, 2, 1, {6.f, 8.f, 12.f, 14.f}));\n+\n+  // Buffer allocations\n+  auto [input_alloc, weight_alloc, output_alloc] =\n+      CreateBufferAllocation(input_literal, weight_literal, output_literal);\n+\n+  auto [input_slice, weight_slice, output_slice] =\n+      CreateBufferAllocationSlice(input_alloc, weight_alloc, output_alloc);\n+\n+  BufferAllocations allocations =\n+      CreateBufferAllocations(input_literal, weight_literal, output_literal);\n+\n+  // Build a minimal OneDnnConvolutionConfig proto.\n+  OneDnnConvolutionConfig conv_config;\n+  conv_config.set_dims(4);\n+  conv_config.set_feature_groups(1);\n+\n+  OneDnnTensorLayoutProto* inp = conv_config.mutable_input();\n+  inp->set_dims(4);\n+  OneDnnDataLayoutProto* inp_data = inp->mutable_data();\n+  inp_data->set_batch_dim(0);\n+  inp_data->set_feature_dim(3);\n+  // Spatial dims stored as one-based (so 1->2, 2->3).\n+  inp_data->add_spatial_dims(2);\n+  inp_data->add_spatial_dims(3);\n+\n+  // Kernel layout assumed HWIO (H,W,In,Out):\n+  OneDnnTensorLayoutProto* ker = conv_config.mutable_kernel();\n+  ker->set_dims(4);\n+  OneDnnFilterLayoutProto* filter = ker->mutable_filter();\n+  filter->set_input_feature_dim(2);   // zero-based index of IC\n+  filter->set_output_feature_dim(3);  // zero-based index of OC\n+  // Spatial dims (H,W) one-based: (0->1,1->2) => 1,2\n+  filter->add_spatial_dims(1);\n+  filter->add_spatial_dims(2);\n+\n+  // Output layout NHWC\n+  OneDnnTensorLayoutProto* out = conv_config.mutable_output();\n+  out->set_dims(4);\n+  OneDnnDataLayoutProto* out_data = out->mutable_data();\n+  out_data->set_batch_dim(0);\n+  out_data->set_feature_dim(3);\n+  out_data->add_spatial_dims(2);\n+  out_data->add_spatial_dims(3);\n+\n+  conv_config.set_feature_groups(1);\n+\n+  // Window parameters: stride=1, pad=0, dilation=1 encoded with offsets.\n+  OneDnnWindowProto* win = conv_config.mutable_window();\n+  // Store (actual + 1) for strides so 2 -> (2 - 1 = 1 real stride).\n+  win->add_strides(2);\n+  win->add_strides(2);\n+  // Pads store (actual +1) so 1 -> 0 actual pad.\n+  win->add_pad_left(1);\n+  win->add_pad_left(1);\n+  win->add_pad_right(1);\n+  win->add_pad_right(1);\n+  // Dilations store (actual +1) so 2 -> 1 actual dilation.\n+  win->add_window_dilations(2);\n+  win->add_window_dilations(2);\n+\n+  // Set up op buffers\n+  OneDnnOpThunk::OpBuffers op_buffers;\n+  op_buffers.arguments_buffers = {input_slice, weight_slice};\n+  op_buffers.arguments_shapes = {input_shape, weight_shape};\n+  op_buffers.results_buffers = {output_slice};\n+  op_buffers.results_shapes = {output_shape};\n+\n+  // Wrap config in variant\n+  OneDnnOpThunk::OneDnnOpConfig config_variant = conv_config;\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto thunk, OneDnnOpThunk::Create(\"__onednn$convolution\", Thunk::Info(),\n+                                        op_buffers, config_variant));\n+\n+  Thunk::ExecuteParams params;\n+  params.buffer_allocations = &allocations;\n+  params.intra_op_threadpool = &device;\n+\n+  tsl::AsyncValueRef<Thunk::ExecuteEvent> exec_event = thunk->Execute(params);\n+  tsl::BlockUntilReady(exec_event);\n+  ASSERT_FALSE(exec_event.IsError())\n+      << \"OneDnnOpThunk convolution execution failed\";\n+\n+  EXPECT_EQ(output_literal, expected_literal);\n+}\n+\n }  // namespace\n }  // namespace xla::cpu"
        },
        {
            "sha": "fe2d0dbbbbc0e13ea949f8f4fd4ca912defb35fe",
            "filename": "third_party/xla/xla/service/cpu/cpu_runtime.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.cc?ref=13582201811e1bb9a2e54bc7557132dd98e3cd03",
            "patch": "@@ -173,8 +173,6 @@ extern const char* const kOneDnnSoftmaxSymbolName =\n     \"__xla_cpu_runtime_OneDnnSoftmax\";\n extern const char* const kOneDnnLayerNormSymbolName =\n     \"__xla_cpu_runtime_OneDnnLayerNorm\";\n-extern const char* const kOneDnnConvolutionSymbolName =\n-    \"__xla_cpu_runtime_OneDnnConvolution\";\n extern const char* const kOneDnnMatMulReorderSymbolName =\n     \"__xla_cpu_runtime_OneDnnMatMulReorder\";\n extern const char* const kHandleFfiCallSymbolName ="
        },
        {
            "sha": "de453bbb8ec12dd62f77a9c433fc8e8a7e71a663",
            "filename": "third_party/xla/xla/service/cpu/cpu_runtime.h",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.h?ref=13582201811e1bb9a2e54bc7557132dd98e3cd03",
            "patch": "@@ -90,7 +90,6 @@ extern const char* const kReduceScatterSymbolName;\n extern const char* const kOneDnnMatMulSymbolName;\n extern const char* const kOneDnnSoftmaxSymbolName;\n extern const char* const kOneDnnLayerNormSymbolName;\n-extern const char* const kOneDnnConvolutionSymbolName;\n extern const char* const kOneDnnMatMulReorderSymbolName;\n extern const char* const kHandleFfiCallSymbolName;\n "
        },
        {
            "sha": "f59ddbda82c9dafea36afebd576be412ba528ef1",
            "filename": "third_party/xla/xla/service/cpu/ir_emitter.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 103,
            "changes": 103,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.cc?ref=13582201811e1bb9a2e54bc7557132dd98e3cd03",
            "patch": "@@ -2563,106 +2563,6 @@ absl::Status IrEmitter::HandleOneDnnMatMulCalls(\n   return absl::OkStatus();\n }\n \n-absl::Status IrEmitter::HandleOneDnnConvolution(HloInstruction* custom_call) {\n-  //      args[0]: ptr to nargs\n-  //      args[1]: ptr to ExecutableRunOptions\n-  //      args[2]: ptr to OneDnnConvolutionConfig\n-  //      args[3...]: ptrs to operands\n-\n-  // First three arguments: nargs, ExecutableRunOptions, and\n-  // OneDnnConvolutionConfig.\n-  const int nargs_offset = 3;\n-  const int num_operands = custom_call->operand_count();\n-  const int nargs = nargs_offset + num_operands;\n-  int arg_indx = 0;\n-\n-  llvm::Type* i64_type = b()->getInt64Ty();\n-  llvm::Type* ptr_type = b()->getPtrTy();\n-  llvm::ArrayType* ptr_array_type = llvm::ArrayType::get(ptr_type, nargs);\n-  llvm::Value* args_val = llvm::UndefValue::get(ptr_array_type);\n-\n-  llvm::Value* nargs_val = b()->getInt64(nargs);\n-  llvm::Value* nargs_ptr =\n-      llvm_ir::EmitAllocaAtFunctionEntry(i64_type, \"nargs\", b());\n-  b()->CreateLifetimeStart(nargs_ptr);\n-  b()->CreateStore(nargs_val, nargs_ptr);\n-  args_val = b()->CreateInsertValue(args_val, nargs_ptr, arg_indx++);\n-\n-  llvm::Value* run_opts_val = GetExecutableRunOptionsArgument();\n-  args_val = b()->CreateInsertValue(args_val, run_opts_val, arg_indx++);\n-\n-  auto typed_custom_call = Cast<HloCustomCallInstruction>(custom_call);\n-  auto backend_config = typed_custom_call->backend_config<BackendConfig>();\n-  OneDnnConvolutionConfig conv_config;\n-  conv_config.CopyFrom(backend_config->onednn_conv_config());\n-  std::string str_config;\n-  conv_config.SerializeToString(&str_config);\n-  llvm::Value* conv_config_val =\n-      b()->CreateGlobalStringPtr(llvm_ir::AsStringRef(str_config));\n-  args_val = b()->CreateInsertValue(args_val, conv_config_val, arg_indx++);\n-\n-  auto operands_stack_alloca =\n-      EmitOneDnnOperandsAlloca(custom_call, args_val, arg_indx);\n-  TF_RET_CHECK(nargs == arg_indx)\n-      << \"Number of arguments don't equal the last argument index.\";\n-\n-  llvm::Value* args_ptr = llvm_ir::EmitAllocaAtFunctionEntry(\n-      ptr_array_type, \"convolution.args\", b());\n-  b()->CreateLifetimeStart(args_ptr);\n-  b()->CreateStore(args_val, args_ptr);\n-\n-  TF_RETURN_IF_ERROR(EmitTargetAddressForOp(custom_call));\n-\n-  StackAlloca result_stack_alloca;\n-  StackAlloca scratch_stack_alloca;\n-  std::vector<llvm::Value*> fn_call_args;\n-  fn_call_args.reserve(3);\n-  // Add the scratch buffer to the output, so that oneDNN can use it as a\n-  // user-provided scratchpad\n-  const bool use_scratchpad = custom_call->shape().IsTuple();\n-  if (use_scratchpad) {\n-    llvm::Value* result_slice_ptr;\n-    llvm::Value* scratch_slice_ptr;\n-    TF_ASSIGN_OR_RETURN(const BufferAllocation::Slice result_slice,\n-                        assignment_.GetUniqueSlice(custom_call, {0}));\n-    const Shape& result_shape = custom_call->shape().tuple_shapes(0);\n-    std::tie(result_slice_ptr, result_stack_alloca) =\n-        GetPtrAndAllocaFromBufferSlice(result_slice, result_shape);\n-    fn_call_args.push_back(result_stack_alloca.value);\n-\n-    TF_ASSIGN_OR_RETURN(const BufferAllocation::Slice scratch_slice,\n-                        assignment_.GetUniqueSlice(custom_call, {1}));\n-    const Shape& scratch_shape = custom_call->shape().tuple_shapes(1);\n-    std::tie(scratch_slice_ptr, scratch_stack_alloca) =\n-        GetPtrAndAllocaFromBufferSlice(scratch_slice, scratch_shape);\n-    fn_call_args.push_back(scratch_stack_alloca.value);\n-    llvm_ir::EmitTuple(GetIrArrayFor(custom_call),\n-                       {result_slice_ptr, scratch_slice_ptr}, b());\n-  } else {\n-    llvm_ir::IrArray result_array;\n-    result_array = GetIrArrayFor(custom_call);\n-    result_stack_alloca = GetAllocaAndEmitMemrefInfo(*b(), result_array);\n-    fn_call_args.push_back(result_stack_alloca.value);\n-    fn_call_args.push_back(llvm::ConstantPointerNull::get(b()->getPtrTy()));\n-  }\n-  fn_call_args.push_back(args_ptr);\n-  EmitCallToFunc(runtime::kOneDnnConvolutionSymbolName, fn_call_args,\n-                 b()->getVoidTy());\n-\n-  // Lifetime ends for all stack allocations.\n-  b()->CreateLifetimeEnd(nargs_ptr);\n-  b()->CreateLifetimeEnd(args_ptr);\n-  for (StackAlloca& alloca : operands_stack_alloca) {\n-    alloca.EmitLifetimeEnd();\n-  }\n-  result_stack_alloca.EmitLifetimeEnd();\n-  if (use_scratchpad) {\n-    scratch_stack_alloca.EmitLifetimeEnd();\n-  }\n-\n-  return absl::OkStatus();\n-}\n-\n absl::Status IrEmitter::HandleOneDnnLayerNorm(HloInstruction* custom_call) {\n   //      args[0]: ptr to nargs\n   //      args[1]: ptr to ExecutableRunOptions\n@@ -2785,9 +2685,6 @@ absl::Status IrEmitter::HandleCustomCall(HloInstruction* custom_call) {\n   if (custom_call->custom_call_target() == \"__onednn$layernorm\") {\n     return HandleOneDnnLayerNorm(custom_call);\n   }\n-  if (custom_call->custom_call_target() == \"__onednn$convolution\") {\n-    return HandleOneDnnConvolution(custom_call);\n-  }\n   if (custom_call->custom_call_target() == \"__onednn$matmul_reorder\") {\n     return HandleOneDnnMatMulCalls(custom_call,\n                                    runtime::kOneDnnMatMulReorderSymbolName);"
        },
        {
            "sha": "2fa416a94d16334332463dde69db134162bc5015",
            "filename": "third_party/xla/xla/service/cpu/ir_emitter.h",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.h?ref=13582201811e1bb9a2e54bc7557132dd98e3cd03",
            "patch": "@@ -346,7 +346,6 @@ class IrEmitter : public DfsHloVisitorWithDefault,\n                                        std::string runtime_symbol_name);\n   absl::Status HandleOneDnnSoftmax(HloInstruction* hlo);\n   absl::Status HandleOneDnnLayerNorm(HloInstruction* hlo);\n-  absl::Status HandleOneDnnConvolution(HloInstruction* hlo);\n #endif  // XLA_ONEDNN\n   // Private helper to initialize an IR function for the computation.\n   void InitializeIrFunction(const std::string& function_name);"
        },
        {
            "sha": "d050657d52f9998536b508f051314baa39be4a7f",
            "filename": "third_party/xla/xla/service/cpu/onednn_contraction_rewriter.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_contraction_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_contraction_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_contraction_rewriter.cc?ref=13582201811e1bb9a2e54bc7557132dd98e3cd03",
            "patch": "@@ -554,11 +554,6 @@ bool OneDnnContractionRewriter::ShouldRewriteDot(\n \n bool OneDnnContractionRewriter::ShouldRewriteConv(\n     const HloInstruction* conv_instr) {\n-  // TODO(intel-tf): remove this restriction after enabling oneDNN convolution\n-  // support in thunk runtime.\n-  return false;\n-\n-  // NOLINTBEGIN(clang-diagnostic-unreachable-code)\n   if (conv_instr->opcode() != HloOpcode::kConvolution ||\n       conv_instr->HasControlDependencies() ||\n       !IsSupportedType(conv_instr->shape().element_type()) ||\n@@ -592,7 +587,6 @@ bool OneDnnContractionRewriter::ShouldRewriteConv(\n   }\n \n   return true;\n-  // NOLINTEND(clang-diagnostic-unreachable-code)\n }\n \n class OneDnnContractionRewriteVisitor : public DfsHloRewriteVisitor {"
        },
        {
            "sha": "9af97ca03e048b00cfb00f575857f5586391fc3a",
            "filename": "third_party/xla/xla/service/cpu/onednn_convolution.cc",
            "status": "modified",
            "additions": 45,
            "deletions": 65,
            "changes": 110,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_convolution.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_convolution.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_convolution.cc?ref=13582201811e1bb9a2e54bc7557132dd98e3cd03",
            "patch": "@@ -51,6 +51,7 @@ namespace {\n using dnnl::algorithm;\n using dnnl::convolution_forward;\n using dnnl::memory;\n+using dnnl::primitive;\n using dnnl::prop_kind;\n using dnnl::stream;\n \n@@ -213,36 +214,15 @@ CreateOneDnnPrimDesc<dnnl::convolution_forward::primitive_desc>(\n       any_res_md, strides, rhs_dilations, pad_left, pad_right, attrs);\n }\n \n-ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_OneDnnConvolution(\n-    void* result, void* scratch, void** args) {\n-  // args[0]: ptr to nargs\n-  // args[1]: ptr to ExecutableRunOptions\n-  // args[2]: ptr to OneDnnConvolutionConfig\n-  // args[3...]: ptrs to operands\n-  int arg_indx = 0;\n-  const int64_t num_args = *(static_cast<int64_t*>(args[arg_indx++]));\n-\n-  const xla::ExecutableRunOptions* run_options =\n-      static_cast<const xla::ExecutableRunOptions*>(args[arg_indx++]);\n-  XLA_LIGHTWEIGHT_CHECK(run_options != nullptr);\n-  XLA_LIGHTWEIGHT_CHECK(run_options->intra_op_thread_pool() != nullptr);\n-  tsl::OneDnnThreadPool thread_pool(\n-      run_options->intra_op_thread_pool()->getPool(), false);\n-  dnnl::engine cpu_engine(dnnl::engine::kind::cpu, 0);\n-#ifndef ENABLE_ONEDNN_OPENMP\n-  auto onednn_stream =\n-      stream(dnnl::threadpool_interop::make_stream(cpu_engine, &thread_pool));\n-#else\n-  auto onednn_stream = stream(cpu_engine);\n-#endif  // ENABLE_ONEDNN_OPENMP\n-\n-  std::string config_str(static_cast<const char*>(args[arg_indx++]));\n-  OneDnnConvolutionConfig conv_config;\n-  conv_config.ParseFromString(config_str);\n-\n-  MemrefInfo inp_minfo(args[arg_indx++]);\n-  MemrefInfo ker_minfo(args[arg_indx++]);\n-  MemrefInfo res_minfo(result);\n+void ExecuteOneDnnConvolution(absl::Span<MemrefInfoHandler> arguments,\n+                              absl::Span<MemrefInfoHandler> results,\n+                              OneDnnConvolutionConfig conv_config,\n+                              const dnnl::engine& cpu_engine,\n+                              dnnl::stream& onednn_stream,\n+                              OneDnnResources& resources) {\n+  MemrefInfo inp_minfo(arguments[0].get());\n+  MemrefInfo ker_minfo(arguments[1].get());\n+  MemrefInfo res_minfo(results[0].get());\n \n   memory::desc inp_md = inp_minfo.GetOneDnnMemDesc();\n   memory::desc ker_md = ker_minfo.GetOneDnnMemDesc();\n@@ -279,11 +259,11 @@ ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_OneDnnConvolution(\n     new_ker_md = new_ker_md.reshape(corr_dims);\n   }\n \n-  const int64_t num_fused_operands = num_args - arg_indx;\n+  const int64_t num_fused_operands = arguments.size() - 2;\n   std::vector<memory::desc> fused_mds;\n   std::vector<void*> fused_bufs;\n   for (int64_t i = 0; i < num_fused_operands; ++i) {\n-    MemrefInfo operand_minfo(args[arg_indx++]);\n+    MemrefInfo operand_minfo(arguments[i + 2].get());\n     memory::desc mem_desc = operand_minfo.GetOneDnnMemDesc();\n     if (mem_desc.get_ndims() == new_res_md.get_ndims()) {\n       mem_desc = mem_desc.permute_axes(ComputePermutations(\n@@ -295,8 +275,7 @@ ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_OneDnnConvolution(\n     fused_bufs.push_back(operand_minfo.Data());\n   }\n \n-  std::vector<std::pair<int, dnnl::memory>> postop_args;\n-  FusedOperandsRef fused_operands_ref{fused_bufs, postop_args};\n+  FusedOperandsRef fused_operands_ref{fused_bufs, resources.postop_args};\n \n   memory::desc bias_md = memory::desc();\n \n@@ -314,8 +293,6 @@ ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_OneDnnConvolution(\n       memory::desc(new_res_md.get_dims(), new_res_md.get_data_type(),\n                    GetFormatTag(new_res_md.get_ndims()));\n \n-  XLA_LIGHTWEIGHT_CHECK(num_args == arg_indx);\n-\n   dnnl::primitive_attr attrs;\n \n   if (conv_config.optimization_config().user_scratchpad()) {\n@@ -335,43 +312,46 @@ ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_OneDnnConvolution(\n   auto ker_mem = memory(new_ker_md, cpu_engine, ker_minfo.Data());\n   auto res_mem = memory(new_res_md, cpu_engine, res_minfo.Data());\n \n-  auto new_inp_mem = (conv_pd->src_desc() == inp_mem.get_desc())\n-                         ? inp_mem\n-                         : ReorderMemory(cpu_engine, conv_pd->src_desc(),\n-                                         inp_mem, onednn_stream);\n-  auto new_ker_mem = (conv_pd->weights_desc() == ker_mem.get_desc())\n-                         ? ker_mem\n-                         : ReorderMemory(cpu_engine, conv_pd->weights_desc(),\n-                                         ker_mem, onednn_stream);\n-  auto new_res_mem = (conv_pd->dst_desc() == res_mem.get_desc())\n-                         ? res_mem\n-                         : memory(conv_pd->dst_desc(), cpu_engine);\n+  resources.src_mem = (conv_pd->src_desc() == inp_mem.get_desc())\n+                          ? inp_mem\n+                          : ReorderMemory(cpu_engine, conv_pd->src_desc(),\n+                                          inp_mem, onednn_stream);\n+  resources.wei_mem = (conv_pd->weights_desc() == ker_mem.get_desc())\n+                          ? ker_mem\n+                          : ReorderMemory(cpu_engine, conv_pd->weights_desc(),\n+                                          ker_mem, onednn_stream);\n+  resources.dst_mem = (conv_pd->dst_desc() == res_mem.get_desc())\n+                          ? res_mem\n+                          : memory(conv_pd->dst_desc(), cpu_engine);\n+\n+  resources.primitive = primitive(*conv_pd);\n+\n+  std::unordered_map<int, memory> conv_args{\n+      {DNNL_ARG_SRC, resources.src_mem},\n+      {DNNL_ARG_WEIGHTS, resources.wei_mem},\n+      {DNNL_ARG_DST, resources.dst_mem}};\n \n-  auto conv_prim = convolution_forward(*conv_pd);\n+  if (conv_config.optimization_config().user_scratchpad()) {\n+    XLA_LIGHTWEIGHT_CHECK(results.size() > 1);\n+    MemrefInfo scratch_minfo(results[1].get());\n \n-  std::unordered_map<int, memory> conv_args{{DNNL_ARG_SRC, new_inp_mem},\n-                                            {DNNL_ARG_WEIGHTS, new_ker_mem},\n-                                            {DNNL_ARG_DST, new_res_mem}};\n+    size_t required_size = conv_pd->scratchpad_desc().get_size();\n+    size_t provided_size = scratch_minfo.GetOneDnnDims()[0];  // bytes (u8)\n+    XLA_LIGHTWEIGHT_CHECK(required_size <= provided_size);\n \n-  if (conv_config.optimization_config().user_scratchpad()) {\n-    XLA_LIGHTWEIGHT_CHECK(scratch != nullptr);\n-    MemrefInfo scratch_minfo(scratch);\n-    memory::desc scratchpad_md = conv_pd->scratchpad_desc();\n-    XLA_LIGHTWEIGHT_CHECK(scratchpad_md.get_size() <=\n-                          scratch_minfo.GetOneDnnDims()[0]);\n-    memory scratch_mem =\n-        memory(scratchpad_md, cpu_engine, scratch_minfo.Data());\n-    conv_args.insert({DNNL_ARG_SCRATCHPAD, scratch_mem});\n+    resources.scratch_mem =\n+        memory(conv_pd->scratchpad_desc(), cpu_engine, scratch_minfo.Data());\n+    conv_args.insert({DNNL_ARG_SCRATCHPAD, resources.scratch_mem});\n   }\n \n-  conv_args.insert(postop_args.begin(), postop_args.end());\n-  conv_prim.execute(onednn_stream, conv_args);\n+  conv_args.insert(resources.postop_args.begin(), resources.postop_args.end());\n+  resources.primitive.execute(onednn_stream, conv_args);\n \n   if (conv_pd->dst_desc() == res_mem.get_desc()) {\n-    res_mem = new_res_mem;\n+    res_mem = resources.dst_mem;\n   } else {\n-    dnnl::reorder(new_res_mem, res_mem)\n-        .execute(onednn_stream, new_res_mem, res_mem);\n+    dnnl::reorder(resources.dst_mem, res_mem)\n+        .execute(onednn_stream, resources.dst_mem, res_mem);\n   }\n }\n "
        },
        {
            "sha": "be3f23349364458bd12af518ccd0325a58d1b2bd",
            "filename": "third_party/xla/xla/service/cpu/onednn_convolution.h",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_convolution.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_convolution.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_convolution.h?ref=13582201811e1bb9a2e54bc7557132dd98e3cd03",
            "patch": "@@ -16,17 +16,20 @@ limitations under the License.\n #ifndef XLA_SERVICE_CPU_ONEDNN_CONVOLUTION_H_\n #define XLA_SERVICE_CPU_ONEDNN_CONVOLUTION_H_\n \n+#include \"xla/service/cpu/onednn_memory_util.h\"\n #include \"xla/service/cpu/onednn_util.h\"\n \n namespace xla {\n namespace cpu {\n \n constexpr auto kOnednnConvConfig = BackendConfigOneofCase::kOnednnConvConfig;\n \n-extern \"C\" {\n-extern void __xla_cpu_runtime_OneDnnConvolution(void* result, void* scratch,\n-                                                void** args);\n-}  // extern \"C\"\n+void ExecuteOneDnnConvolution(absl::Span<MemrefInfoHandler> arguments,\n+                              absl::Span<MemrefInfoHandler> results,\n+                              OneDnnConvolutionConfig conv_config,\n+                              const dnnl::engine& cpu_engine,\n+                              dnnl::stream& onednn_stream,\n+                              OneDnnResources& resources);\n \n template <>\n struct PrimitiveTrait<kOnednnConvConfig> {"
        },
        {
            "sha": "b4377597a245abcbc970536deb86e75dc94845a7",
            "filename": "third_party/xla/xla/service/cpu/runtime_symbol_generator.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_symbol_generator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_symbol_generator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_symbol_generator.cc?ref=13582201811e1bb9a2e54bc7557132dd98e3cd03",
            "patch": "@@ -191,7 +191,6 @@ static bool RegisterKnownJITSymbols() {\n   REGISTER_CPU_RUNTIME_SYMBOL(OneDnnMatMul);\n   REGISTER_CPU_RUNTIME_SYMBOL(OneDnnSoftmax);\n   REGISTER_CPU_RUNTIME_SYMBOL(OneDnnLayerNorm);\n-  REGISTER_CPU_RUNTIME_SYMBOL(OneDnnConvolution);\n   REGISTER_CPU_RUNTIME_SYMBOL(OneDnnMatMulReorder);\n #endif  // XLA_ONEDNN\n "
        },
        {
            "sha": "e119db445053177b6ef168b684fcca3f2419dba5",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/13582201811e1bb9a2e54bc7557132dd98e3cd03/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc?ref=13582201811e1bb9a2e54bc7557132dd98e3cd03",
            "patch": "@@ -1215,6 +1215,8 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitOneDnnOpThunk(\n   OneDnnOpThunk::OneDnnOpConfig config;\n   if (custom_call_target == \"__onednn$matmul\") {\n     config = backend_config->onednn_matmul_config();\n+  } else if (custom_call_target == \"__onednn$convolution\") {\n+    config = backend_config->onednn_conv_config();\n   } else {\n     return Unimplemented(\n         \"Custom call target %s is not supported in thunk runtime\",\n@@ -1247,7 +1249,6 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCustomCallThunk(\n   // TODO(penporn): Support these existing targets.\n   auto custom_call_target = custom_call->custom_call_target();\n   if (custom_call_target == \"PadToStatic\" ||\n-      custom_call_target == \"__onednn$convolution\" ||\n       custom_call_target == \"__onednn$softmax\" ||\n       custom_call_target == \"__onednn$layernorm\") {\n     return Unimplemented(\"Custom call target %s is not implemented.\","
        }
    ],
    "stats": {
        "total": 390,
        "additions": 198,
        "deletions": 192
    }
}