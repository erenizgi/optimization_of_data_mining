{
    "author": "basioli-k",
    "message": "[XLA:GPU][host offloading] Add compute host offloading compiler passes.\n\nPiperOrigin-RevId: 798152030",
    "sha": "0f90c777f1af7cce3e0fca4fa921d29300b79264",
    "files": [
        {
            "sha": "b5fe5b3313f8852f1a5358f4ccc8acacfb2390e3",
            "filename": "third_party/xla/xla/hlo/ir/backend_config_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f90c777f1af7cce3e0fca4fa921d29300b79264/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fbackend_config_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f90c777f1af7cce3e0fca4fa921d29300b79264/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fbackend_config_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fbackend_config_test.cc?ref=0f90c777f1af7cce3e0fca4fa921d29300b79264",
            "patch": "@@ -37,7 +37,7 @@ const int kNumRepetitions = 100;\n // since the == operator does not canonicalize the raw strings before comparing\n // them.\n constexpr absl::string_view kRawString =\n-    R\"({\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\"triton_gemm_config\":{\"block_m\":\"256\",\"block_n\":\"256\",\"block_k\":\"32\",\"split_k\":\"1\",\"num_stages\":\"1\",\"num_warps\":\"16\",\"num_ctas\":\"1\",\"is_tma_allowed\":false}},\"force_earliest_schedule\":false,\"reification_cost\":[]})\";\n+    R\"({\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\"triton_gemm_config\":{\"block_m\":\"256\",\"block_n\":\"256\",\"block_k\":\"32\",\"split_k\":\"1\",\"num_stages\":\"1\",\"num_warps\":\"16\",\"num_ctas\":\"1\",\"is_tma_allowed\":false}},\"force_earliest_schedule\":false,\"reification_cost\":[],\"device_type\":\"DEVICE_TYPE_INVALID\"})\";\n \n template <typename Input, typename CheckFn>\n void RunThreaded(Input input, CheckFn check_fn) {"
        },
        {
            "sha": "cf8a979b6582903583eb6d30bc61986b1a75a0a7",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f90c777f1af7cce3e0fca4fa921d29300b79264/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f90c777f1af7cce3e0fca4fa921d29300b79264/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=0f90c777f1af7cce3e0fca4fa921d29300b79264",
            "patch": "@@ -1454,6 +1454,8 @@ cc_library(\n         \"//xla/backends/gpu/codegen/triton:support\",\n         \"//xla/backends/gpu/runtime:sequential_thunk\",\n         \"//xla/backends/gpu/runtime:thunk\",\n+        \"//xla/core/host_offloading:hlo_host_device_type_call_wrapper\",\n+        \"//xla/core/host_offloading:host_compute_asyncifier\",\n         \"//xla/hlo/analysis:alias_info\",\n         \"//xla/hlo/analysis:hlo_dataflow_analysis\",\n         \"//xla/hlo/analysis:hlo_ordering\",\n@@ -1464,6 +1466,7 @@ cc_library(\n         \"//xla/hlo/transforms:convert_memory_placement_to_internal_annotations\",\n         \"//xla/hlo/transforms:host_offload_legalize\",\n         \"//xla/hlo/transforms:host_offloader\",\n+        \"//xla/hlo/transforms:host_offloading_prepare\",\n         \"//xla/hlo/transforms:operand_upcaster\",\n         \"//xla/hlo/transforms:while_loop_trip_count_annotator\",\n         \"//xla/hlo/transforms/collectives:all_gather_broadcast_reorder\","
        },
        {
            "sha": "64e682b37d067912f48452a7ad8440b1934f9eb8",
            "filename": "third_party/xla/xla/service/gpu/backend_configs.proto",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f90c777f1af7cce3e0fca4fa921d29300b79264/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbackend_configs.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f90c777f1af7cce3e0fca4fa921d29300b79264/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbackend_configs.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbackend_configs.proto?ref=0f90c777f1af7cce3e0fca4fa921d29300b79264",
            "patch": "@@ -341,8 +341,14 @@ message BlockScaledDotBackendConfig {\n   int32 block_size = 1;\n }\n \n+enum DeviceType {\n+  DEVICE_TYPE_INVALID = 0;\n+  DEVICE_TYPE_DEVICE = 1;\n+  DEVICE_TYPE_HOST = 2;\n+}\n+\n // Generic backend config for XLA:GPU\n-// Next-Id: 14\n+// Next-Id: 15\n message GpuBackendConfig {\n   // Specifies which operation queue the current instruction will run on.\n   // A backend may have multiple operation queues to run instructions\n@@ -391,4 +397,7 @@ message GpuBackendConfig {\n   // Reification cost for the instruction which attaches this\n   // `GpuBackendConfig`.\n   repeated ReificationCost reification_cost = 12;\n+\n+  // Specifies the device type for the instruction.\n+  DeviceType device_type = 14;\n }"
        },
        {
            "sha": "db56d46bc148c0341b0875152d9f0592573ef927",
            "filename": "third_party/xla/xla/service/gpu/data/hlo_algorithm_denylist.pbtxt",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f90c777f1af7cce3e0fca4fa921d29300b79264/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdata%2Fhlo_algorithm_denylist.pbtxt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f90c777f1af7cce3e0fca4fa921d29300b79264/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdata%2Fhlo_algorithm_denylist.pbtxt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdata%2Fhlo_algorithm_denylist.pbtxt?ref=0f90c777f1af7cce3e0fca4fa921d29300b79264",
            "patch": "@@ -5,6 +5,7 @@ entries {\n     wait_on_operation_queues: []\n     cudnn_conv_backend_config: { activation_mode: kNone conv_result_scale: 1 side_input_scale: 0 leakyrelu_alpha: 0 }\n     force_earliest_schedule: false\n+    device_type: DEVICE_TYPE_DEVICE\n   }\n   cc: { major: 7 minor: 0 }\n   cudnn_version: { major: 7 minor: 6 patch: 0 }\n@@ -19,6 +20,7 @@ entries {\n     wait_on_operation_queues: []\n     cudnn_conv_backend_config: { activation_mode: kNone conv_result_scale: 1 side_input_scale: 0 leakyrelu_alpha: 0 }\n     force_earliest_schedule: false\n+    device_type: DEVICE_TYPE_DEVICE\n   }\n   cc: { major: 7 minor: 0 }\n   cudnn_version: { major: 7 minor: 6 patch: 2 }\n@@ -33,6 +35,7 @@ entries {\n     wait_on_operation_queues: []\n     cudnn_conv_backend_config: { activation_mode: kNone conv_result_scale: 1 side_input_scale: 0 leakyrelu_alpha: 0 }\n     force_earliest_schedule: false\n+    device_type: DEVICE_TYPE_DEVICE\n   }\n   cc: { major: 7 minor: 0 }\n   cudnn_version: { major: 7 minor: 6 patch: 2 }"
        },
        {
            "sha": "571e4eb593a1c95813706065898270dcf511fb59",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f90c777f1af7cce3e0fca4fa921d29300b79264/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f90c777f1af7cce3e0fca4fa921d29300b79264/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=0f90c777f1af7cce3e0fca4fa921d29300b79264",
            "patch": "@@ -63,6 +63,8 @@ limitations under the License.\n #include \"mlir/Support/LLVM.h\"\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n+#include \"xla/core/host_offloading/hlo_host_device_type_call_wrapper.h\"\n+#include \"xla/core/host_offloading/host_compute_asyncifier.h\"\n #include \"xla/hlo/analysis/alias_info.h\"\n #include \"xla/hlo/analysis/hlo_dataflow_analysis.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n@@ -102,6 +104,7 @@ limitations under the License.\n #include \"xla/hlo/transforms/expanders/stochastic_convert_decomposer.h\"\n #include \"xla/hlo/transforms/host_offload_legalize.h\"\n #include \"xla/hlo/transforms/host_offloader.h\"\n+#include \"xla/hlo/transforms/host_offloading_prepare.h\"\n #include \"xla/hlo/transforms/operand_upcaster.h\"\n #include \"xla/hlo/transforms/simplifiers/algebraic_simplifier.h\"\n #include \"xla/hlo/transforms/simplifiers/all_gather_pad_ds_simplifier.h\"\n@@ -684,6 +687,36 @@ absl::Status RunSPMDPasses(\n   }\n }\n \n+namespace {\n+\n+absl::Status SetHostDeviceType(HloInstruction* instr) {\n+  TF_ASSIGN_OR_RETURN(auto backend_config,\n+                      instr->backend_config<GpuBackendConfig>());\n+  backend_config.set_device_type(DEVICE_TYPE_HOST);\n+  TF_RETURN_IF_ERROR(instr->set_backend_config(backend_config));\n+  return absl::OkStatus();\n+}\n+\n+absl::Status ClearBackendConfigDeviceType(HloInstruction* instr) {\n+  TF_ASSIGN_OR_RETURN(auto backend_config,\n+                      instr->backend_config<GpuBackendConfig>());\n+  backend_config.clear_device_type();\n+  return instr->set_backend_config(backend_config);\n+}\n+\n+bool BackendConfigDeviceTypeIsHost(HloInstruction* instr) {\n+  if (!instr->has_backend_config()) {\n+    return false;\n+  }\n+  auto backend_config = instr->backend_config<GpuBackendConfig>();\n+  if (!backend_config.ok()) {\n+    return false;\n+  }\n+  return backend_config->device_type() == DEVICE_TYPE_HOST;\n+}\n+\n+}  // namespace\n+\n absl::Status RunOptimizationPasses(\n     HloModule* hlo_module, stream_executor::StreamExecutor* stream_exec,\n     const Compiler::TargetConfig& gpu_target_config,\n@@ -1466,6 +1499,19 @@ absl::Status GpuCompiler::OptimizeHloModule(\n                                     hlo_module->config().debug_options(),\n                                     gpu_target_config.platform_name == \"ROCM\");\n \n+  {\n+    HloPassPipeline pipeline(\"annotate-host-compute\");\n+    HloHostDeviceTypeCallWrapper::Options\n+        hlo_host_device_type_call_wrapper_options;\n+    hlo_host_device_type_call_wrapper_options.set_backend_config_fn =\n+        SetHostDeviceType;\n+    hlo_host_device_type_call_wrapper_options.clear_backend_config_device_type =\n+        ClearBackendConfigDeviceType;\n+    pipeline.AddPass<HloHostDeviceTypeCallWrapper>(\n+        hlo_host_device_type_call_wrapper_options);\n+    TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());\n+  }\n+\n   TF_RETURN_IF_ERROR(RunPreSPMDPartitionerPasses(hlo_module));\n   // Set max_windowed_einsum_iteration to slice_size, as there will be\n   // significant overhead when scaled beyond the maximum size of the\n@@ -1475,6 +1521,14 @@ absl::Status GpuCompiler::OptimizeHloModule(\n                     layout_insensitive_algsimp_opts,\n                     /*max_windowed_einsum_iteration=*/options.slice_size));\n \n+  {\n+    HloPassPipeline pipeline(\"host-compute\");\n+    pipeline.AddPass<HostComputeAsyncifier>(BackendConfigDeviceTypeIsHost);\n+    pipeline.AddPass<HostOffloadingPrepare>(\n+        HostOffloadingPrepare::Rewrite::kConvertToCustomCall);\n+    TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());\n+  }\n+\n   // Dump the HLO module after SPMD partitioning. There should be no more Python\n   // callbacks at this point.\n   DumpHloModuleIfEnabled(*hlo_module, \"after_spmd_partitioner\");"
        },
        {
            "sha": "5a60ee0892dc652f536330b72fad94b8175a9ed4",
            "filename": "third_party/xla/xla/service/gpu/hlo_algorithm_denylist.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f90c777f1af7cce3e0fca4fa921d29300b79264/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_algorithm_denylist.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f90c777f1af7cce3e0fca4fa921d29300b79264/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_algorithm_denylist.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_algorithm_denylist.cc?ref=0f90c777f1af7cce3e0fca4fa921d29300b79264",
            "patch": "@@ -55,6 +55,7 @@ constexpr char kDefaultDenylist[] = R\"pb(\n         leakyrelu_alpha: 0\n       },\n       force_earliest_schedule: false\n+      device_type: DEVICE_TYPE_DEVICE\n     }\n     cc { major: 7 }\n     cudnn_version { major: 9 }"
        },
        {
            "sha": "202f29837c9dfeb845e97f2110fe040c7aa0af99",
            "filename": "third_party/xla/xla/service/gpu/hlo_algorithm_denylist_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f90c777f1af7cce3e0fca4fa921d29300b79264/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_algorithm_denylist_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f90c777f1af7cce3e0fca4fa921d29300b79264/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_algorithm_denylist_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_algorithm_denylist_test.cc?ref=0f90c777f1af7cce3e0fca4fa921d29300b79264",
            "patch": "@@ -80,7 +80,7 @@ TEST_F(DenylistTest, DefaultTest) {\n       ENTRY main {\n           arg1 = f16[256,224,224,4]{3,2,1,0} parameter(0)\n           arg2 = f16[7,7,4,64]{2,1,0,3} parameter(1)\n-          ROOT root = (f16[256,112,112,64]{3,2,1,0}, u8[0]{0}) custom-call(arg1, arg2), window={size=7x7 stride=2x2 pad=3_3x3_3}, dim_labels=b01f_01io->b01f, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]}\n+          ROOT root = (f16[256,112,112,64]{3,2,1,0}, u8[0]{0}) custom-call(arg1, arg2), window={size=7x7 stride=2x2 pad=3_3x3_3}, dim_labels=b01f_01io->b01f, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[],\"device_type\":\"DEVICE_TYPE_DEVICE\"}\n       }\n   )hlo\"));\n \n@@ -119,7 +119,7 @@ TEST_F(DenylistTest, NoBlasVersionSet) {\n       ENTRY main {\n           arg1 = f16[256,224,224,4]{3,2,1,0} parameter(0)\n           arg2 = f16[7,7,4,64]{2,1,0,3} parameter(1)\n-          ROOT root = (f16[256,112,112,64]{3,2,1,0}, u8[0]{0}) custom-call(arg1, arg2), window={size=7x7 stride=2x2 pad=3_3x3_3}, dim_labels=b01f_01io->b01f, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]}\n+          ROOT root = (f16[256,112,112,64]{3,2,1,0}, u8[0]{0}) custom-call(arg1, arg2), window={size=7x7 stride=2x2 pad=3_3x3_3}, dim_labels=b01f_01io->b01f, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[],\"device_type\":\"DEVICE_TYPE_DEVICE\"}\n       }\n   )hlo\"));\n \n@@ -150,7 +150,7 @@ TEST_F(DenylistTest, EntryFromHardcodedList) {\n          arg1 = f32[512,512,7,7]{3,2,1,0} parameter(0)\n          arg2 = f32[512,512,3,3]{3,2,1,0} parameter(1)\n          arg3 = f32[512]{0} parameter(2)\n-         ROOT root = (f32[512,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(arg1, arg2, arg3), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]}\n+         ROOT root = (f32[512,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(arg1, arg2, arg3), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[],\"device_type\":\"DEVICE_TYPE_DEVICE\"}\n       }\n   )hlo\"));\n \n@@ -180,7 +180,7 @@ TEST_F(DenylistTest, GenerateDenyListEntry) {\n          arg1 = f32[512,512,7,7]{3,2,1,0} parameter(0)\n          arg2 = f32[512,512,3,3]{3,2,1,0} parameter(1)\n          arg3 = f32[512]{0} parameter(2)\n-         ROOT root = (f32[512,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(arg1, arg2, arg3), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]}\n+         ROOT root = (f32[512,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(arg1, arg2, arg3), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[],\"device_type\":\"DEVICE_TYPE_DEVICE\"}\n       }\n   )hlo\"));\n "
        },
        {
            "sha": "37093bb41054edd30bf45ce328da346977963d33",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment.cc",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0f90c777f1af7cce3e0fca4fa921d29300b79264/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0f90c777f1af7cce3e0fca4fa921d29300b79264/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc?ref=0f90c777f1af7cce3e0fca4fa921d29300b79264",
            "patch": "@@ -629,6 +629,37 @@ absl::Status GpuLayoutAssignment::AddBackendConstraints(\n       LayoutUtil::SetToDefaultLayout(&operand_shape);\n       TF_RETURN_IF_ERROR(SetOperandLayout(operand_shape, instruction, 0));\n       TF_RETURN_IF_ERROR(SetInstructionLayout(operand_shape, instruction));\n+    } else if (instruction->opcode() == HloOpcode::kAsyncStart) {\n+      HloComputation* called_computation =\n+          instruction->async_wrapped_computation();\n+\n+      if (called_computation->execution_thread() !=\n+          HloInstruction::kHostThread) {\n+        continue;\n+      }\n+\n+      Shape new_shape = instruction->shape();\n+      *new_shape.mutable_tuple_shapes(0) = ShapeUtil::MakeTupleShape(\n+          called_computation->ComputeProgramShape().parameters());\n+      *new_shape.mutable_tuple_shapes(1) =\n+          called_computation->ComputeProgramShape().result();\n+      TF_RETURN_IF_ERROR(SetInstructionLayout(new_shape, instruction,\n+                                              /*mandatory=*/true, /*dfs=*/true,\n+                                              /*allow_alias=*/true));\n+    } else if (instruction->opcode() == HloOpcode::kAsyncDone) {\n+      HloComputation* called_computation =\n+          instruction->async_wrapped_computation();\n+\n+      if (called_computation->execution_thread() !=\n+          HloInstruction::kHostThread) {\n+        continue;\n+      }\n+\n+      Shape new_shape = called_computation->root_instruction()->shape();\n+\n+      TF_RETURN_IF_ERROR(SetInstructionLayout(new_shape, instruction,\n+                                              /*mandatory=*/true, /*dfs=*/true,\n+                                              /*allow_alias=*/true));\n     }\n   }\n   return absl::OkStatus();"
        }
    ],
    "stats": {
        "total": 113,
        "additions": 107,
        "deletions": 6
    }
}