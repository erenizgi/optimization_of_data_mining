{
    "author": "KanishAnand",
    "message": "Introduce `NamedSharding` to `OpSharding` proto and `HloSharding`.\n\n#hloshardingv3\n\nPiperOrigin-RevId: 817614842",
    "sha": "7bf89c1611f18c15712e1bd7d0dce0978fef4a0a",
    "files": [
        {
            "sha": "071435dfe80841d310444a233f25a1d920ac1ef5",
            "filename": "third_party/xla/xla/hlo/ir/hlo_sharding.h",
            "status": "modified",
            "additions": 56,
            "deletions": 0,
            "changes": 56,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7bf89c1611f18c15712e1bd7d0dce0978fef4a0a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7bf89c1611f18c15712e1bd7d0dce0978fef4a0a/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h?ref=7bf89c1611f18c15712e1bd7d0dce0978fef4a0a",
            "patch": "@@ -53,6 +53,52 @@ class HloSharding {\n   static inline constexpr absl::string_view kShardingFrontendAttrName =\n       \"xla.sdy.sharding\";\n \n+  // C++ representation for corresponding proto types in `xla_data.proto` so\n+  // same documentation applies, except AxisRef elements are pointers to\n+  // `MeshAxis` elements instead of indices.\n+  //\n+  // TODO(b/449783607): Move mesh, axis to mesh_and_axis.h and move\n+  // NamedSharding out of HloSharding to match proto after change to using\n+  // mesh_and_axis.h. Currently simply moving this out will cause name\n+  // clashes with proto as they both use same xla namespace.\n+  struct MeshAxis {\n+    std::string name;\n+    int64_t size;\n+  };\n+\n+  struct Mesh {\n+    std::vector<MeshAxis> axes;\n+    std::vector<int64_t> device_ids;\n+  };\n+\n+  struct AxisRef {\n+    struct SubAxis {\n+      int64_t pre_size;\n+      int64_t size;\n+    };\n+\n+    const MeshAxis* axis;\n+    std::optional<SubAxis> sub_axis_info;\n+  };\n+\n+  // C++ representation for corresponding `OpSharding::NamedSharding` proto.\n+  //\n+  // TODO(b/450770542): Add corresponding IFTTT in attrs.td\n+  class NamedSharding {\n+    struct DimensionSharding {\n+      std::vector<AxisRef> axes;\n+      bool is_closed;\n+    };\n+\n+    std::vector<NamedSharding> tuple_shardings_;\n+\n+    Mesh mesh_;\n+    std::vector<DimensionSharding> dim_shardings_;\n+    std::vector<AxisRef> replicated_axes_;\n+    std::vector<AxisRef> unreduced_axes_;\n+    std::vector<OpMetadata> metadata_;\n+  };\n+\n   // Creates a trivial sharding that replicates a maximal tile across all\n   // devices.\n   static HloSharding Replicate(absl::Span<const OpMetadata> metadata = {}) {\n@@ -767,6 +813,16 @@ class HloSharding {\n   // within the same shard group (i.e. under the same shard_group_id) will be\n   // sharded alike or exactly the same as each other.\n   ShardGroup shard_group_ = NotShardGroup();\n+\n+  // Optional field to migrate HloSharding to new NamedSharding representation.\n+  // If this field is populated, all other fields in HloSharding should be empty\n+  // or else are ignored. This is to facilitate migration from the old sharding\n+  // format.\n+  //\n+  // Note that instead of reusing HloSharding's fields like metadata, we have\n+  // separate fields in NamedSharding to treat it as a standalone message which\n+  // is more clear and will help in future cleanup.\n+  std::optional<NamedSharding> named_sharding_;\n };\n \n std::ostream& operator<<(std::ostream& out, const HloSharding& sharding);"
        },
        {
            "sha": "e3b52b5506a6b868faba16a79b76b1f8c899726d",
            "filename": "third_party/xla/xla/service/spmd/shardy/stablehlo_round_trip/export_shardings.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7bf89c1611f18c15712e1bd7d0dce0978fef4a0a/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_shardings.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7bf89c1611f18c15712e1bd7d0dce0978fef4a0a/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_shardings.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_shardings.cc?ref=7bf89c1611f18c15712e1bd7d0dce0978fef4a0a",
            "patch": "@@ -262,7 +262,7 @@ HloSharding getHloShardingForOp(\n     return convertToHloSharding(shardings.front(), getMeshAttr, manualAxes);\n   }\n \n-  SmallVector<HloSharding> newShardings;\n+  std::vector<HloSharding> newShardings;\n   llvm::transform(shardings, std::back_inserter(newShardings),\n                   [&](TensorShardingAttr sdySharding) {\n                     return convertToHloSharding(sdySharding, getMeshAttr,"
        },
        {
            "sha": "4a9a8e49aa90737c21f0520977d2dd7deb8517ae",
            "filename": "third_party/xla/xla/xla_data.proto",
            "status": "modified",
            "additions": 126,
            "deletions": 0,
            "changes": 126,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7bf89c1611f18c15712e1bd7d0dce0978fef4a0a/third_party%2Fxla%2Fxla%2Fxla_data.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7bf89c1611f18c15712e1bd7d0dce0978fef4a0a/third_party%2Fxla%2Fxla%2Fxla_data.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla_data.proto?ref=7bf89c1611f18c15712e1bd7d0dce0978fef4a0a",
            "patch": "@@ -902,6 +902,122 @@ message StatisticsViz {\n   repeated Statistic statistics = 2;\n }\n \n+// A mesh is a list of axes and an optional list of device IDs specifying\n+// the device ordering.\n+//\n+// If the list of axes is empty, the mesh has an implicit unnamed axis of\n+// size 1. In this case, if a device ID list is not provided, the implicit\n+// device ID list is [0]; if a device ID list is provided, it must\n+// contains a single integer of any non-negative value. We call this\n+// maximal-sharding case.\n+//\n+// For all non-maximal-sharding cases, if a device ID list is specified,\n+// the product of the axis sizes should match the number of devices. If a\n+// device ID list is not specified, the implicit device ID list is\n+// iota(product(axes)). For simplicity, we also disallow specifying a\n+// device ID list that is the same as iota(product(axes)); in this case, a\n+// device ID list shouldn't be specified.\n+//\n+// Here are some examples of meshes:\n+// - An empty mesh represents a placeholder mesh that can be replaced during\n+// propagation: <[]>\n+// - A mesh with an unnamed axis and an explicit device ID, which is\n+// typically used to represent maximal sharding: <[], device_ids=[3]>\n+// - A mesh with two axes and implicit device IDs iota(6): <[\"a\"=2, \"b\"=3]>\n+// - A mesh with two axes and explicit device IDs specifying the device\n+// ordering: <[\"a\"=3, \"b\"=2], device_ids=[0, 2, 4, 1, 3, 5]>\n+//\n+// Constraints:\n+// - Elements in `axes` must not have duplicate names.\n+// - If `device_ids` is specified:\n+//   * The product of axis sizes must match the number of devices.\n+//   * All of its elements must be non-negative.\n+//   * `device_ids` should not be equal to `iota(product(axis_sizes))`.\n+//   * Sorted `device_ids` must be `iota(product(axis_sizes))`.\n+message Mesh {\n+  message MeshAxis {\n+    string name = 1;\n+    int64 size = 2;\n+  }\n+\n+  repeated MeshAxis axes = 1;\n+  repeated int64 device_ids = 2;\n+}\n+\n+// Reference to either a full axis via index into Mesh axes or a split\n+// sub-axis. Axis index is used to reference the axis in the mesh instead of\n+// axis name to reduce proto size.\n+//\n+// Constraints:\n+// - `axis_index` is a valid index into `mesh.axes`.\n+message AxisRef {\n+  // When splitting a full axis into n sub-axes, the axis is reshaped into\n+  // [k_1,...,k_n], and the ith sub-axis can be expressed by the product of\n+  // all axis sizes to its left `m=prod(k_1,...,k_(i-1))` (aka pre-size) and\n+  // size k_i. Therefore, the sub-axis-info attribute holds those two numbers\n+  // and is denoted as follows: `(m)k` for pre-size m and size k.\n+  //\n+  // Constraints:\n+  // - `pre-size` is at least 1.\n+  // - `size` is greater than 1.\n+  // - `pre-size` must divide the size of the full axis, i.e., both `pre-size`\n+  //    and `size` divide the size of the full axis, and the sub-axis doesn't\n+  //    go beyond the full axis.\n+  // - The size of the sub-axis isn't equal to the size of the corresponding\n+  //   full axis, in which case the full axis should be used instead.\n+  message SubAxis {\n+    int64 pre_size = 1;\n+    int64 size = 2;\n+  }\n+\n+  int64 axis_index = 1;\n+  optional SubAxis sub_axis_info = 2;\n+}\n+\n+// A NamedSharding is bound to a specific mesh, and can only reference axis\n+// names from that mesh. The dimension shardings tell us for each dimension of\n+// the tensor, along which axes it is sharded from major to minor. All other\n+// axes that donâ€™t shard a dimension are either implicitly or explicitly (if\n+// they appear in the list of replicated axes) replicated. A sharding can have\n+// unreduced axes (specified by `unreduced_axes`), meaning the tensor is\n+// unreduced along these axes. Note that no sharding attribute on a tensor is\n+// equivalent to a fully open sharding.\n+// This is corresponding to mlir sharding representation `Sdy_TensorSharding`\n+// (https://github.com/openxla/shardy/blob/main/shardy/dialect/sdy/ir/attrs.td)\n+// used in Shardy.\n+message NamedSharding {\n+  // Describes how a tensor dimension is sharded using indices into\n+  // mesh.axes, from major to minor, and a boolean indicating whether\n+  // the dimension can be further sharded.\n+  message DimensionSharding {\n+    // List of axes to shard a tensor dimension on from major to minor. The\n+    // dimension should have at least one axis if it is closed.\n+    repeated AxisRef axes = 1;\n+    // If true, this dimension is \"closed\" and can't be further sharded.\n+    bool is_closed = 2;\n+  }\n+\n+  // If this is non-empty, then all the other fields are ignored and this\n+  // represents a tuple sharding.\n+  repeated NamedSharding tuple_shardings = 1;\n+\n+  Mesh mesh = 2;\n+  // The dimension shardings tell us for each dimension of the tensor, along\n+  // which axes it is sharded from major to minor.\n+  repeated DimensionSharding dim_shardings = 3;\n+  // All axes in this list are explicitly replicated.\n+  repeated AxisRef replicated_axes = 4;\n+  // A sharding can have unreduced axes, meaning the tensor is unreduced\n+  // along these axes.\n+  repeated AxisRef unreduced_axes = 5;\n+\n+  // This field is used to track the source of this sharding, usually derived\n+  // from instructions. Multple metadata may be populated if sharding is\n+  // combined with other shardings. Metadata should be set on individual tuple\n+  // elements and not tuple_sharding.\n+  repeated OpMetadata metadata = 6;\n+}\n+\n // LINT.IfChange\n message OpSharding {\n   enum Type {\n@@ -989,6 +1105,16 @@ message OpSharding {\n   }\n \n   ShardGroupType shard_group_type = 13;\n+\n+  // Optional field to migrate OpSharding to new NamedSharding representation.\n+  // If this field is populated, all other fields in OpSharding should be empty\n+  // or else are ignored. This is to facilitate migration from the old sharding\n+  // format.\n+  //\n+  // Note that instead of reusing OpSharding's fields like metadata, we have\n+  // separate fields in NamedSharding to treat it as a standalone message which\n+  // is more clear and will help in future cleanup.\n+  optional NamedSharding named_sharding = 14;\n }\n // LINT.ThenChange()\n "
        }
    ],
    "stats": {
        "total": 184,
        "additions": 183,
        "deletions": 1
    }
}