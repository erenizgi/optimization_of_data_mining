{
    "author": "Moerafaat",
    "message": "[XLA:GPU/TMA] Deprecate TMA flag since it is enabled by default and is stable. Also explicitly state TMA configurations in the default set for the current GEMM autotuner. This makes it easier to maintain an explicit list with a known size.\n\nPiperOrigin-RevId: 842666877",
    "sha": "b29ba0638e3d994956981379630a9a88e423ff36",
    "files": [
        {
            "sha": "253c2373c087a7fe23f4abf6f35f66566b86d156",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.cc?ref=b29ba0638e3d994956981379630a9a88e423ff36",
            "patch": "@@ -290,12 +290,9 @@ BlockLevelEmitterBackend::GetSupportedConfigs(const HloInstruction& instr) {\n     configs.push_back(std::move(any));\n   }\n \n-  // Allow TMA tuning for Hopper+ devices when TMA flag is passed.\n-  bool autotune_tma =\n-      debug_options().xla_gpu_experimental_enable_triton_tma() &&\n-      stream_executor::gpu::IsTmaAvailableForDevice(\n-          target_config().device_description);\n-  if (autotune_tma) {\n+  // Allow TMA tuning for Hopper+ devices.\n+  if (stream_executor::gpu::IsTmaAvailableForDevice(\n+          target_config().device_description)) {\n     ExtendConfigsWithTma(configs);\n   }\n "
        },
        {
            "sha": "603a22cbb991cc3f6c3e312442a6f8af6c0bf206",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc?ref=b29ba0638e3d994956981379630a9a88e423ff36",
            "patch": "@@ -72,9 +72,7 @@ class TritonBlockLevelFusionEmitterBackendTest\n                              .value()),\n         target_config_(stream_executor_),\n         backend_(&debug_options_, &compiler_,\n-                 compiler_.ShapeSizeBytesFunction(), &target_config_) {\n-    debug_options_.set_xla_gpu_experimental_enable_triton_tma(true);\n-  }\n+                 compiler_.ShapeSizeBytesFunction(), &target_config_) {}\n \n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;"
        },
        {
            "sha": "0ee4e09dc605720876adfff30525b898dfe3e3f9",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 29,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc?ref=b29ba0638e3d994956981379630a9a88e423ff36",
            "patch": "@@ -58,7 +58,7 @@ namespace gpu {\n \n namespace {\n std::vector<TritonGemmConfig> GetDefaultTritonConfigs(\n-    se::GpuComputeCapability compute_capability, bool autotune_tma) {\n+    se::GpuComputeCapability compute_capability) {\n   if (compute_capability.IsRocm()) {\n     return GetTritonConfigsForPlatform(TritonConfigsPlatform::kDefaultRocm);\n   }\n@@ -69,29 +69,15 @@ std::vector<TritonGemmConfig> GetDefaultTritonConfigs(\n \n   if (cuda_compute_capability->IsAtLeastBlackwell()) {\n     configs = GetTritonConfigsForPlatform(TritonConfigsPlatform::kBlackwell);\n-  } else if (cuda_compute_capability->IsHopper() ||\n-             cuda_compute_capability->IsAmpere()) {\n-    configs = GetTritonConfigsForPlatform(TritonConfigsPlatform::kHopperAmpere);\n+  } else if (cuda_compute_capability->IsHopper()) {\n+    configs = GetTritonConfigsForPlatform(TritonConfigsPlatform::kHopper);\n+  } else if (cuda_compute_capability->IsAmpere()) {\n+    configs = GetTritonConfigsForPlatform(TritonConfigsPlatform::kAmpere);\n   } else {\n     configs = GetTritonConfigsForPlatform(TritonConfigsPlatform::kDefaultCuda);\n   }\n \n-  if (!autotune_tma) {\n-    return configs;\n-  }\n-\n-  // Hopper+ devices support TMA. Add TMA parameterized configs.\n-  std::vector<TritonGemmConfig> tma_parameterized_configs;\n-  for (auto& config : configs) {\n-    config.is_tma_allowed = false;\n-    tma_parameterized_configs.push_back(config);\n-\n-    if (IsTmaRecommended(config)) {\n-      config.is_tma_allowed = true;\n-      tma_parameterized_configs.push_back(config);\n-    }\n-  }\n-  return tma_parameterized_configs;\n+  return configs;\n }\n \n }  // namespace\n@@ -128,11 +114,6 @@ TritonBackend::GetSupportedConfigsForDot(const HloInstruction* instr) {\n       supports_contracting_split &&\n       debug_options().xla_gpu_enable_split_k_autotuning();\n \n-  // Allow TMA tuning for Hopper+ devices when TMA flag is passed.\n-  bool autotune_tma =\n-      debug_options().xla_gpu_experimental_enable_triton_tma() &&\n-      stream_executor::gpu::IsTmaAvailableForDevice(\n-          target_config().device_description);\n   std::vector<std::unique_ptr<BackendConfig>> configs;\n   VLOG(1) << \"Generating configs from search space: \"\n           << search_space.ToString();\n@@ -141,15 +122,13 @@ TritonBackend::GetSupportedConfigsForDot(const HloInstruction* instr) {\n   std::vector<TritonGemmConfig> gemm_configs = search_space.GenerateConfigs(\n       /*force_contracting_split=*/autotune_contracting_split\n           ? std::nullopt\n-          : std::make_optional(1),\n-      /*autotune_tma=*/autotune_tma);\n+          : std::make_optional(1));\n \n   if (!debug_options().xla_gpu_exhaustive_tiling_search()) {\n     VLOG(1) << \"Restricting configs to the default set.\";\n     gemm_configs = search_space.OptimizeConfigSet(\n         gemm_configs, /*hints=*/GetDefaultTritonConfigs(\n-            target_config().device_description.gpu_compute_capability(),\n-            autotune_tma));\n+            target_config().device_description.gpu_compute_capability()));\n   }\n   configs.reserve(gemm_configs.size());\n   for (const auto& config : gemm_configs) {"
        },
        {
            "sha": "0c029caa2a5b88174cf3022d9ee1fdc20f04031a",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc?ref=b29ba0638e3d994956981379630a9a88e423ff36",
            "patch": "@@ -96,7 +96,6 @@ class TritonBackendTest : public HloHardwareIndependentTestBase {\n                              .value()),\n         target_config_(stream_executor_),\n         backend_(&debug_options_, &compiler_, &target_config_, &mlir_context_) {\n-    debug_options_.set_xla_gpu_experimental_enable_triton_tma(true);\n   }\n \n   DebugOptions debug_options_;"
        },
        {
            "sha": "39c8a8afe129eeaa868290aee3b67e011a14b60e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 18,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=b29ba0638e3d994956981379630a9a88e423ff36",
            "patch": "@@ -104,14 +104,7 @@ class TritonEmitterTest : public GpuCodegenTest {\n \n class TmaParameterizedTritonEmitterTest\n     : public TritonEmitterTest,\n-      public ::testing::WithParamInterface<bool> {\n- public:\n-  DebugOptions GetDebugOptionsForTest() const override {\n-    DebugOptions debug_options = TritonEmitterTest::GetDebugOptionsForTest();\n-    debug_options.set_xla_gpu_experimental_enable_triton_tma(GetParam());\n-    return debug_options;\n-  }\n-};\n+      public ::testing::WithParamInterface<bool> {};\n \n INSTANTIATE_TEST_SUITE_P(TmaParameterizedTritonEmitterTestSuite,\n                          TmaParameterizedTritonEmitterTest, ::testing::Bool(),\n@@ -123,7 +116,6 @@ class WarpSpecializationTritonEmitterTest : public TritonEmitterTest {\n  public:\n   DebugOptions GetDebugOptionsForTest() const override {\n     DebugOptions debug_options = TritonEmitterTest::GetDebugOptionsForTest();\n-    debug_options.set_xla_gpu_experimental_enable_triton_tma(true);\n     debug_options.set_xla_gpu_experimental_enable_triton_warp_specialization(\n         true);\n     return debug_options;\n@@ -139,15 +131,7 @@ struct TmaAndDotLayoutTestParams {\n \n class TmaAndLayoutParameterizedTritonEmitterTest\n     : public TritonEmitterTest,\n-      public ::testing::WithParamInterface<TmaAndDotLayoutTestParams> {\n- public:\n-  DebugOptions GetDebugOptionsForTest() const override {\n-    DebugOptions debug_options = TritonEmitterTest::GetDebugOptionsForTest();\n-    debug_options.set_xla_gpu_experimental_enable_triton_tma(\n-        GetParam().enable_tma);\n-    return debug_options;\n-  }\n-};\n+      public ::testing::WithParamInterface<TmaAndDotLayoutTestParams> {};\n \n std::string TmaAndDotLayoutTestParamsToString(\n     const ::testing::TestParamInfo<TmaAndDotLayoutTestParams>& data) {"
        },
        {
            "sha": "fa77b022d841a41038a73df57ef6f5c97d066954",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_deviceless_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc?ref=b29ba0638e3d994956981379630a9a88e423ff36",
            "patch": "@@ -48,7 +48,6 @@ class WarpSpecializationTritonEmitterTest : public TritonEmitterDevicelessTest {\n   DebugOptions GetDebugOptionsForTest() const override {\n     DebugOptions debug_options =\n         TritonEmitterDevicelessTest::GetDebugOptionsForTest();\n-    debug_options.set_xla_gpu_experimental_enable_triton_tma(true);\n     debug_options.set_xla_gpu_experimental_enable_triton_warp_specialization(\n         true);\n     return debug_options;"
        },
        {
            "sha": "0f53dd18a323c8894e7ba905a8cd1cc0f8e5c059",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/tma_utils.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftma_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftma_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftma_utils.cc?ref=b29ba0638e3d994956981379630a9a88e423ff36",
            "patch": "@@ -139,10 +139,10 @@ absl::StatusOr<TmaDescriptor> CreateTmaDescriptor(\n                              GetTmaSwizzleMode(swizzle_mode));\n }\n \n+// The current recommendation is based on analyzing the E2E \"Nucleo\" group\n+// data. It might make sense to re-evaluate this recommendation later if we\n+// believe there are missed opportunities.\n bool IsTmaRecommended(const TritonGemmConfig& config) {\n-  // The current recommendation is based on analyzing the E2E \"Nucleo\" group\n-  // data. It might make sense to re-evaluate this recommendation later if we\n-  // believe there are missed opportunities.\n   return (config.split_k == 1 || config.split_k == 16) &&\n          config.num_warps <= 8 &&\n          (config.num_stages == 1 || config.num_stages == 3 ||"
        },
        {
            "sha": "abfb81977ccdb0ac1b61ff0a590d18113e39fb2b",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=b29ba0638e3d994956981379630a9a88e423ff36",
            "patch": "@@ -460,7 +460,6 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_enable_scoped_logging_timers(true);\n   opts.set_xla_unsupported_crash_on_hlo_pass_noop_change(false);\n   opts.set_xla_gpu_experimental_enable_split_k_rewrite(false);\n-  opts.set_xla_gpu_experimental_enable_triton_tma(true);\n   opts.set_xla_gpu_experimental_enable_triton_warp_specialization(false);\n   opts.set_xla_detect_unstable_reductions(DebugOptions::DETECTION_MODE_NONE);\n   opts.set_xla_detect_unstable_reductions_post_optimizations(\n@@ -2623,12 +2622,6 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n       debug_options->xla_gpu_experimental_enable_split_k_rewrite(),\n       \"Enable the pass that splits GEMMs that underutilize the GPU load by \"\n       \"splitting the K dimension using a heuristic.\"));\n-  flag_list->push_back(tsl::Flag(\n-      \"xla_gpu_experimental_enable_triton_tma\",\n-      bool_setter_for(\n-          &DebugOptions::set_xla_gpu_experimental_enable_triton_tma),\n-      debug_options->xla_gpu_experimental_enable_triton_tma(),\n-      \"Enable Triton's TMA loads/stores for arguments where applicable.\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_gpu_experimental_enable_triton_warp_specialization\",\n       bool_setter_for("
        },
        {
            "sha": "77b0d2daf56629a5f003f79b58089fa7111941b3",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=b29ba0638e3d994956981379630a9a88e423ff36",
            "patch": "@@ -50,7 +50,6 @@ cc_library(\n         \"//xla:xla_proto_cc\",\n         \"//xla/backends/autotuner:codegen_backend\",\n         \"//xla/backends/gpu/autotuner:cudnn\",\n-        \"//xla/backends/gpu/codegen/triton:tma_utils\",\n         \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n@@ -70,7 +69,6 @@ cc_library(\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n-        \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/platform:env\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n@@ -317,6 +315,7 @@ cc_library(\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/lib/core:bits\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\","
        },
        {
            "sha": "4446900f5569a6aaf0839e442e5a3c5d238b5f35",
            "filename": "third_party/xla/xla/service/gpu/autotuning/dot_search_space.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 16,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space.cc?ref=b29ba0638e3d994956981379630a9a88e423ff36",
            "patch": "@@ -40,6 +40,7 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/tsl/lib/core/bits.h\"\n #include \"xla/util.h\"\n #include \"tsl/platform/protobuf.h\"\n@@ -124,7 +125,7 @@ TritonDotFusionSearchSpace::TritonDotFusionSearchSpace(\n }\n \n std::vector<TritonGemmConfig> TritonDotFusionSearchSpace::GenerateConfigs(\n-    std::optional<int64_t> force_contracting_split, bool autotune_tma,\n+    std::optional<int64_t> force_contracting_split,\n     bool autotune_warp_specialization) const {\n   std::vector<ConfigWithNotes> configs;\n   if (force_contracting_split.has_value()) {\n@@ -153,22 +154,12 @@ std::vector<TritonGemmConfig> TritonDotFusionSearchSpace::GenerateConfigs(\n   ExtendConfigs(configs, &TritonDotFusionSearchSpace::AddCtaSizeParameter);\n   ExtendConfigs(configs, &TritonDotFusionSearchSpace::AddContractingTiling);\n   ExtendConfigs(configs, &TritonDotFusionSearchSpace::AddPipeliningParameter);\n-\n-  if (autotune_warp_specialization && !autotune_tma) {\n-    LOG(WARNING)\n-        << \"Warp specialization is requested, but TMA is not enabled, hence \"\n-           \"warp specialization will be ignored. Set both \"\n-           \"`is_warp_specialization_allowed` and `is_tma_allowed` \"\n-           \"to true on the configuration to enable warp specialization.\";\n-  }\n-  if (autotune_tma) {\n-    VLOG(10) << \"Parameterizing all currently constructed configs with \"\n-                \"TMA.\";\n+  if (stream_executor::gpu::IsTmaAvailableForDevice(device_description_)) {\n     ExtendConfigs(configs, &TritonDotFusionSearchSpace::AddTmaParameter);\n-    if (autotune_warp_specialization) {\n-      ExtendConfigs(\n-          configs, &TritonDotFusionSearchSpace::AddWarpSpecializationParameter);\n-    }\n+  }\n+  if (autotune_warp_specialization) {\n+    ExtendConfigs(configs,\n+                  &TritonDotFusionSearchSpace::AddWarpSpecializationParameter);\n   }\n \n   std::vector<TritonGemmConfig> result;"
        },
        {
            "sha": "7d6e68ccb0bd8c07f76edf4df8beb1d069394fc8",
            "filename": "third_party/xla/xla/service/gpu/autotuning/dot_search_space.h",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space.h?ref=b29ba0638e3d994956981379630a9a88e423ff36",
            "patch": "@@ -46,14 +46,10 @@ class TritonDotFusionSearchSpace {\n   // If `force_contracting_split` is set, the search space\n   // will be restricted to only include configs with the given split_k factor.\n   //\n-  // If true, `autotune_tma` and `autotune_warp_specialization` extend the\n-  // search space with TMA parameterization and warp specialization\n-  // respectively. Setting 'autotune_warp_specialization' to true also requires\n-  // `autotune_tma` to be true, given that warp specialization is probably not\n-  // useful without TMA.\n+  // If true, `autotune_warp_specialization` extends the search space with warp\n+  // specialization support.\n   std::vector<TritonGemmConfig> GenerateConfigs(\n       std::optional<int64_t> force_contracting_split = std::nullopt,\n-      bool autotune_tma = false,\n       bool autotune_warp_specialization = false) const;\n \n   // Restrict the set of configs to the ones compatible with the hints list."
        },
        {
            "sha": "17c836bf78839b59b98b321e35c714b9be37b840",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=b29ba0638e3d994956981379630a9a88e423ff36",
            "patch": "@@ -1003,20 +1003,9 @@ GemmFusionAutotunerImpl::GenerateTritonConfigs(const HloDotInstruction& dot) {\n       supports_contracting_split &&\n       debug_options_.xla_gpu_enable_split_k_autotuning();\n \n-  // Allow TMA tuning for Hopper+ devices when TMA flag is passed.\n-  bool autotune_tma = debug_options_.xla_gpu_experimental_enable_triton_tma() &&\n-                      stream_executor::gpu::IsTmaAvailableForDevice(\n-                          config_.GetDeviceDescription());\n   bool autotune_warp_specialization =\n       debug_options_.xla_gpu_experimental_enable_triton_warp_specialization() &&\n       IsWarpSpecializationAvailable();\n-  if (autotune_warp_specialization && !autotune_tma) {\n-    return absl::InvalidArgumentError(\n-        \"Warp specialization is requested, but TMA is not enabled. If you wish \"\n-        \"to enable warp specialization, set both \"\n-        \"`xla_gpu_experimental_enable_triton_tma` and \"\n-        \"`xla_gpu_experimental_enable_triton_warp_specialization` to true.\");\n-  }\n   TritonDotFusionSearchSpace search_space(config_.GetDeviceDescription(), &dot);\n   VLOG(1) << \"Generating configs from search space: \"\n           << search_space.ToString();\n@@ -1026,7 +1015,6 @@ GemmFusionAutotunerImpl::GenerateTritonConfigs(const HloDotInstruction& dot) {\n       /*force_contracting_split=*/autotune_contracting_split\n           ? std::nullopt\n           : std::make_optional(1),\n-      /*autotune_tma=*/autotune_tma,\n       /*autotune_warp_specialization=*/autotune_warp_specialization);\n \n   if (auto overrides = config_.gemm_config_overrides(); overrides.has_value()) {"
        },
        {
            "sha": "f086503f3ec7956415b34dbb404d9d6020e64262",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_cuda.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 22,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc?ref=b29ba0638e3d994956981379630a9a88e423ff36",
            "patch": "@@ -21,7 +21,6 @@ limitations under the License.\n #include \"third_party/gpus/cuda/include/cublas_v2.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/cudnn.h\"\n-#include \"xla/backends/gpu/codegen/triton/tma_utils.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -39,7 +38,6 @@ limitations under the License.\n #include \"xla/service/gpu/transforms/block_scaling_rewriter.h\"\n #include \"xla/service/gpu/transforms/cudnn_fusion_compiler.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n-#include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n \n namespace xla {\n@@ -119,37 +117,23 @@ std::vector<TritonGemmConfig> GemmFusionAutotunerImpl::GetDefaultTritonConfigs()\n \n   if (compute_capability.IsAtLeastBlackwell()) {\n     configs = GetTritonConfigsForPlatform(TritonConfigsPlatform::kBlackwell);\n-  } else if (compute_capability.IsHopper() || compute_capability.IsAmpere()) {\n-    configs = GetTritonConfigsForPlatform(TritonConfigsPlatform::kHopperAmpere);\n+  } else if (compute_capability.IsHopper()) {\n+    configs = GetTritonConfigsForPlatform(TritonConfigsPlatform::kHopper);\n+  } else if (compute_capability.IsAmpere()) {\n+    configs = GetTritonConfigsForPlatform(TritonConfigsPlatform::kAmpere);\n   } else {\n     configs = GetTritonConfigsForPlatform(TritonConfigsPlatform::kDefaultCuda);\n   }\n \n-  if (!debug_options_.xla_gpu_experimental_enable_triton_tma() ||\n-      !stream_executor::gpu::IsTmaAvailableForDevice(\n-          config_.GetDeviceDescription())) {\n-    return configs;\n-  }\n-  std::vector<TritonGemmConfig> tma_parameterized_configs;\n-  for (auto& config : configs) {\n-    config.is_tma_allowed = false;\n-    tma_parameterized_configs.push_back(config);\n-\n-    if (IsTmaRecommended(config)) {\n-      config.is_tma_allowed = true;\n-      tma_parameterized_configs.push_back(config);\n-    }\n-  }\n-\n   // TODO(b/449668102): Currently only supporting warp specialization on\n   // Blackwell+. Potentially extend support to Hopper.\n   if (!debug_options_\n            .xla_gpu_experimental_enable_triton_warp_specialization() ||\n       !compute_capability.IsAtLeastBlackwell()) {\n-    return tma_parameterized_configs;\n+    return configs;\n   }\n   std::vector<TritonGemmConfig> warp_specialized_configs;\n-  for (auto& config : tma_parameterized_configs) {\n+  for (auto& config : configs) {\n     config.is_warp_specialization_allowed = false;\n     warp_specialized_configs.push_back(config);\n "
        },
        {
            "sha": "a540e6a2ede81dea714c9b1f7252f12b4c8f8522",
            "filename": "third_party/xla/xla/service/gpu/autotuning/triton_configs.cc",
            "status": "modified",
            "additions": 91,
            "deletions": 2,
            "changes": 93,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Ftriton_configs.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Ftriton_configs.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Ftriton_configs.cc?ref=b29ba0638e3d994956981379630a9a88e423ff36",
            "patch": "@@ -72,6 +72,28 @@ config { block_m: 64 block_n: 32 block_k: 64 split_k: 64 num_stages: 3 num_warps\n config { block_m: 64 block_n: 64 block_k: 128 split_k: 8 num_stages: 1 num_warps: 8 num_ctas: 1 }\n config { block_m: 64 block_n: 64 block_k: 16 split_k: 1 num_stages: 1 num_warps: 2 num_ctas: 1 }\n config { block_m: 64 block_n: 64 block_k: 16 split_k: 1 num_stages: 3 num_warps: 2 num_ctas: 1 }\n+config { block_m: 128 block_n: 128 block_k: 32 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 128 block_n: 128 block_k: 64 split_k: 1 num_stages: 1 num_warps: 8 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 128 block_n: 16 block_k: 32 split_k: 16 num_stages: 3 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 128 block_n: 16 block_k: 64 split_k: 16 num_stages: 3 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 128 block_n: 256 block_k: 64 split_k: 1 num_stages: 4 num_warps: 8 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 128 block_n: 64 block_k: 64 split_k: 1 num_stages: 3 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 128 block_n: 64 block_k: 64 split_k: 16 num_stages: 4 num_warps: 8 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 16 block_n: 16 block_k: 128 split_k: 1 num_stages: 3 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 16 block_n: 16 block_k: 16 split_k: 1 num_stages: 1 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 16 block_n: 32 block_k: 64 split_k: 1 num_stages: 3 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 256 block_n: 128 block_k: 64 split_k: 1 num_stages: 3 num_warps: 8 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 256 block_n: 16 block_k: 16 split_k: 1 num_stages: 1 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 256 block_n: 32 block_k: 32 split_k: 16 num_stages: 3 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 32 block_n: 16 block_k: 32 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 32 block_n: 16 block_k: 64 split_k: 1 num_stages: 1 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 32 block_n: 16 block_k: 64 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 128 block_k: 16 split_k: 1 num_stages: 3 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 128 block_k: 64 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 32 block_k: 128 split_k: 1 num_stages: 3 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 32 block_k: 32 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 64 block_k: 16 split_k: 1 num_stages: 1 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 64 block_k: 16 split_k: 1 num_stages: 3 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n )\";\n \n constexpr absl::string_view kDefaultCudaTritonConfigs = R\"(\n@@ -118,7 +140,7 @@ config { block_m: 16 block_n: 16 block_k: 256 split_k: 1 num_stages: 1 num_warps\n config { block_m: 16 block_n: 128 block_k: 32 split_k: 16 num_stages: 1 num_warps: 4 num_ctas: 1 }\n )\";\n \n-constexpr absl::string_view kHopperAmpereTritonConfigs = R\"(\n+constexpr absl::string_view kAmpereTritonConfigs = R\"(\n config { block_m: 16 block_n: 16 block_k: 64 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 }\n config { block_m: 16 block_n: 16 block_k: 128 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n config { block_m: 16 block_n: 16 block_k: 128 split_k: 128 num_stages: 4 num_warps: 2 num_ctas: 1 }\n@@ -157,6 +179,72 @@ config { block_m: 128 block_n: 256 block_k: 64 split_k: 1 num_stages: 4 num_warp\n config { block_m: 64 block_n: 8 block_k: 128 split_k: 2 num_stages: 3 num_warps: 4 num_ctas: 1 }\n )\";\n \n+constexpr absl::string_view kHopperTritonConfigs = R\"(\n+config { block_m: 16 block_n: 16 block_k: 64 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 16 block_n: 16 block_k: 128 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 16 block_n: 16 block_k: 128 split_k: 128 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 16 block_n: 16 block_k: 128 split_k: 16 num_stages: 1 num_warps: 2 num_ctas: 1 }\n+config { block_m: 16 block_n: 256 block_k: 16 split_k: 1 num_stages: 1 num_warps: 2 num_ctas: 1 }\n+config { block_m: 32 block_n: 32 block_k: 128 split_k: 16 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 32 block_n: 256 block_k: 32 split_k: 1 num_stages: 3 num_warps: 4 num_ctas: 1 }\n+config { block_m: 32 block_n: 256 block_k: 32 split_k: 16 num_stages: 3 num_warps: 8 num_ctas: 1 }\n+config { block_m: 64 block_n: 16 block_k: 32 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 16 block_k: 32 split_k: 16 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 16 block_k: 64 split_k: 1 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 16 block_k: 64 split_k: 4 num_stages: 3 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 16 block_k: 64 split_k: 16 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 16 block_k: 128 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 16 block_k: 128 split_k: 16 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 32 block_k: 32 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 32 block_k: 64 split_k: 16 num_stages: 3 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 32 block_k: 128 split_k: 1 num_stages: 3 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 32 block_k: 128 split_k: 128 num_stages: 2 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 64 block_k: 32 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 64 block_k: 64 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 64 block_k: 64 split_k: 4 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 64 block_k: 128 split_k: 16 num_stages: 3 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 64 block_k: 256 split_k: 16 num_stages: 4 num_warps: 8 num_ctas: 1 }\n+config { block_m: 64 block_n: 128 block_k: 16 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 64 block_n: 128 block_k: 64 split_k: 1 num_stages: 3 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 128 block_k: 128 split_k: 8 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 64 block_n: 256 block_k: 32 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 }\n+config { block_m: 128 block_n: 16 block_k: 32 split_k: 8 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 128 block_n: 16 block_k: 64 split_k: 16 num_stages: 3 num_warps: 2 num_ctas: 1 }\n+config { block_m: 128 block_n: 16 block_k: 64 split_k: 16 num_stages: 1 num_warps: 4 num_ctas: 1 }\n+config { block_m: 128 block_n: 32 block_k: 32 split_k: 8 num_stages: 4 num_warps: 2 num_ctas: 1 }\n+config { block_m: 128 block_n: 128 block_k: 32 split_k: 8 num_stages: 4 num_warps: 8 num_ctas: 1 }\n+config { block_m: 128 block_n: 256 block_k: 32 split_k: 1 num_stages: 4 num_warps: 8 num_ctas: 1 }\n+config { block_m: 128 block_n: 256 block_k: 64 split_k: 1 num_stages: 4 num_warps: 8 num_ctas: 1 }\n+config { block_m: 64 block_n: 8 block_k: 128 split_k: 2 num_stages: 3 num_warps: 4 num_ctas: 1 }\n+config { block_m: 16 block_n: 16 block_k: 64 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 16 block_n: 16 block_k: 128 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 16 block_n: 16 block_k: 128 split_k: 16 num_stages: 1 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 16 block_n: 256 block_k: 16 split_k: 1 num_stages: 1 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 32 block_n: 32 block_k: 128 split_k: 16 num_stages: 1 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 32 block_n: 256 block_k: 32 split_k: 1 num_stages: 3 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 32 block_n: 256 block_k: 32 split_k: 16 num_stages: 3 num_warps: 8 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 16 block_k: 32 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 16 block_k: 32 split_k: 16 num_stages: 4 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 16 block_k: 64 split_k: 1 num_stages: 1 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 16 block_k: 64 split_k: 16 num_stages: 4 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 16 block_k: 128 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 16 block_k: 128 split_k: 16 num_stages: 4 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 32 block_k: 32 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 32 block_k: 64 split_k: 16 num_stages: 3 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 32 block_k: 128 split_k: 1 num_stages: 3 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 64 block_k: 32 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 64 block_k: 64 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 64 block_k: 128 split_k: 16 num_stages: 3 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 64 block_k: 256 split_k: 16 num_stages: 4 num_warps: 8 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 128 block_k: 16 split_k: 1 num_stages: 4 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 128 block_k: 64 split_k: 1 num_stages: 3 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 64 block_n: 256 block_k: 32 split_k: 1 num_stages: 4 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 128 block_n: 16 block_k: 64 split_k: 16 num_stages: 3 num_warps: 2 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 128 block_n: 16 block_k: 64 split_k: 16 num_stages: 1 num_warps: 4 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 128 block_n: 256 block_k: 32 split_k: 1 num_stages: 4 num_warps: 8 num_ctas: 1 is_tma_allowed: true }\n+config { block_m: 128 block_n: 256 block_k: 64 split_k: 1 num_stages: 4 num_warps: 8 num_ctas: 1 is_tma_allowed: true }\n+)\";\n+\n absl::flat_hash_map<TritonConfigsPlatform, std::vector<TritonGemmConfig>>\n LoadTritonConfigs() {\n   absl::flat_hash_map<TritonConfigsPlatform, std::vector<TritonGemmConfig>>\n@@ -181,10 +269,11 @@ LoadTritonConfigs() {\n   const std::initializer_list<\n       std::pair<TritonConfigsPlatform, absl::string_view>>\n       kConfigsMap = {\n+          {TritonConfigsPlatform::kAmpere, kAmpereTritonConfigs},\n           {TritonConfigsPlatform::kBlackwell, kBlackwellTritonConfigs},\n           {TritonConfigsPlatform::kDefaultCuda, kDefaultCudaTritonConfigs},\n           {TritonConfigsPlatform::kDefaultRocm, kDefaultRocmTritonConfigs},\n-          {TritonConfigsPlatform::kHopperAmpere, kHopperAmpereTritonConfigs},\n+          {TritonConfigsPlatform::kHopper, kHopperTritonConfigs},\n       };\n   for (const auto& [platform, config_str] : kConfigsMap) {\n     result[platform] = parse_config(config_str);"
        },
        {
            "sha": "b37950a6e1d7b009f342dfc462f1a0850e1457d1",
            "filename": "third_party/xla/xla/service/gpu/autotuning/triton_configs.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Ftriton_configs.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Ftriton_configs.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Ftriton_configs.h?ref=b29ba0638e3d994956981379630a9a88e423ff36",
            "patch": "@@ -24,10 +24,11 @@ namespace xla {\n namespace gpu {\n \n enum class TritonConfigsPlatform {\n+  kAmpere,\n   kBlackwell,\n   kDefaultCuda,\n   kDefaultRocm,\n-  kHopperAmpere,\n+  kHopper,\n };\n \n const std::vector<TritonGemmConfig>& GetTritonConfigsForPlatform("
        },
        {
            "sha": "f3b61c30ae07a62add03cc6d4da82c93604b14f2",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b29ba0638e3d994956981379630a9a88e423ff36/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=b29ba0638e3d994956981379630a9a88e423ff36",
            "patch": "@@ -697,9 +697,6 @@ message DebugOptions {\n   optional bool xla_gpu_experimental_enable_triton_heroless_priority_fusion =\n       340;\n \n-  // When possible, XLA will use Triton's TMA loads/stores.\n-  optional bool xla_gpu_experimental_enable_triton_tma = 355;\n-\n   // When possible, XLA will use Triton's auto warp specialization feature.\n   optional bool xla_gpu_experimental_enable_triton_warp_specialization = 421;\n \n@@ -1426,14 +1423,15 @@ message DebugOptions {\n   reserved \"xla_use_shardy\";\n   reserved \"xla_gpu_unsupported_annotate_with_emitter_loc\";\n   reserved \"xla_gpu_experimental_enable_command_buffer_on_thunks\";\n+  reserved \"xla_gpu_experimental_enable_triton_tma\";\n \n   reserved 5, 63, 80, 93, 94, 98, 117, 130, 133, 134, 139, 141, 143, 152, 158,\n       160, 161, 162, 167, 168, 169, 171, 172, 173, 176, 177, 178, 179, 180, 183,\n       184, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 204, 206,\n       207, 211, 214, 218, 220, 221, 226, 229, 230, 233, 234, 238, 242, 249, 263,\n       264, 266, 270, 271, 275, 276, 278, 279, 281, 282, 286, 298, 299, 302, 303,\n-      309, 313, 314, 319, 320, 325, 326, 332, 346, 352, 358, 361, 367, 369, 371,\n-      385, 394, 398, 402, 423;\n+      309, 313, 314, 319, 320, 325, 326, 332, 346, 352, 355, 358, 361, 367, 369,\n+      371, 385, 394, 398, 402, 423;\n }\n \n // Contains flags which affects the GPU compilation result."
        }
    ],
    "stats": {
        "total": 263,
        "additions": 129,
        "deletions": 134
    }
}