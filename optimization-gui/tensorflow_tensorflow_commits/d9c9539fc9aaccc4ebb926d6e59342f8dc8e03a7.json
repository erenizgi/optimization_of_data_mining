{
    "author": "basioli-k",
    "message": "[XLA][host offloading] Open source HostComputeAsyncifier.\n\nPiperOrigin-RevId: 797741246",
    "sha": "d9c9539fc9aaccc4ebb926d6e59342f8dc8e03a7",
    "files": [
        {
            "sha": "53ed7015ee5c0e747eee4ae4825d8797ca0b9b9e",
            "filename": "third_party/xla/xla/core/host_offloading/BUILD",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d9c9539fc9aaccc4ebb926d6e59342f8dc8e03a7/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d9c9539fc9aaccc4ebb926d6e59342f8dc8e03a7/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2FBUILD?ref=d9c9539fc9aaccc4ebb926d6e59342f8dc8e03a7",
            "patch": "@@ -342,3 +342,34 @@ cc_library(\n         \"@local_tsl//tsl/platform:casts\",\n     ],\n )\n+\n+cc_library(\n+    name = \"host_compute_asyncifier\",\n+    srcs = [\n+        \"host_compute_asyncifier.cc\",\n+    ],\n+    hdrs = [\n+        \"host_compute_asyncifier.h\",\n+    ],\n+    compatible_with = get_compatible_with_libtpu_portable(),\n+    deps = [\n+        \"//xla:shape_util\",\n+        \"//xla:status_macros\",\n+        \"//xla/core/host_offloading:hlo_host_device_type_call_wrapper\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/pass:hlo_pass\",\n+        \"//xla/hlo/transforms/simplifiers:hlo_dce\",\n+        \"//xla/hlo/transforms/simplifiers:tuple_simplifier\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@local_tsl//tsl/platform:casts\",\n+        \"@local_tsl//tsl/platform:errors\",\n+        \"@local_tsl//tsl/platform:statusor\",\n+    ],\n+)"
        },
        {
            "sha": "21d6dece65980c300fc63378ad92f5277c41000f",
            "filename": "third_party/xla/xla/core/host_offloading/host_compute_asyncifier.cc",
            "status": "added",
            "additions": 137,
            "deletions": 0,
            "changes": 137,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d9c9539fc9aaccc4ebb926d6e59342f8dc8e03a7/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2Fhost_compute_asyncifier.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d9c9539fc9aaccc4ebb926d6e59342f8dc8e03a7/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2Fhost_compute_asyncifier.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2Fhost_compute_asyncifier.cc?ref=d9c9539fc9aaccc4ebb926d6e59342f8dc8e03a7",
            "patch": "@@ -0,0 +1,137 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/core/host_offloading/host_compute_asyncifier.h\"\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/core/host_offloading/hlo_host_device_type_call_wrapper.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/transforms/simplifiers/hlo_dce.h\"\n+#include \"xla/hlo/transforms/simplifiers/tuple_simplifier.h\"\n+#include \"xla/layout.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/status_macros.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"tsl/platform/casts.h\"\n+\n+namespace xla {\n+\n+namespace {\n+\n+// Removes tiles and memory spaces from all instructions inside `computation`.\n+void RemoveTilesAndMemorySpaces(HloComputation* computation) {\n+  for (HloInstruction* instruction : computation->instructions()) {\n+    VLOG(1) << absl::StreamFormat(\n+        \"Removing tiles and memory spaces from \\\"%s\\\".\",\n+        instruction->ToString());\n+    ShapeUtil::ForEachMutableSubshape(\n+        instruction->mutable_shape(),\n+        [](Shape* subshape, const ShapeIndex& subshape_index) {\n+          if (!subshape->has_layout()) {\n+            return;\n+          }\n+          Layout* layout = subshape->mutable_layout();\n+          layout->clear_tiles();\n+          layout->set_memory_space(Layout::kDefaultMemorySpace);\n+        });\n+  }\n+}\n+}  // namespace\n+\n+absl::StatusOr<bool> HostComputeAsyncifier::Run(\n+    HloModule* module,\n+    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n+  bool modified = false;\n+  for (HloComputation* computation :\n+       module->MakeNonfusionComputations(execution_threads)) {\n+    for (HloInstruction* call : computation->instructions()) {\n+      if (call->opcode() != HloOpcode::kCall) {\n+        continue;\n+      }\n+      if (!backend_config_device_type_is_host_fn_(call)) {\n+        continue;\n+      }\n+      VLOG(1) << \"Host Call: \" << call->name();\n+\n+      HloComputation* parent_computation = call->parent();\n+      HloComputation* host_computation = call->called_computations().front();\n+      HloCallInstruction* call_instr =\n+          tsl::down_cast<HloCallInstruction*>(call);\n+      CHECK_NE(call_instr, nullptr);\n+\n+      TF_ASSIGN_OR_RETURN(\n+          HloCallInstruction * call_instr_no_tuple_operands,\n+          HloHostDeviceTypeCallWrapper::RemoveTupleParameters(call_instr));\n+      TF_RETURN_IF_ERROR(TupleSimplifier().Run(module).status());\n+      TF_ASSIGN_OR_RETURN(\n+          HloInstruction * call_instr_no_constants,\n+          HloHostDeviceTypeCallWrapper::MaterializeConstantsOnHostComputation(\n+              call_instr_no_tuple_operands));\n+\n+      VLOG(1) << \"Call instruction without constants: \"\n+              << call_instr_no_constants->name();\n+\n+      TF_RET_CHECK(call_instr_no_constants->operands().size() ==\n+                   host_computation->num_parameters())\n+          << \"Expected the number of operands to match the number of \"\n+             \"parameters \"\n+             \"of the host called computation.\";\n+      TF_RET_CHECK(\n+          ShapeUtil::Equal(host_computation->root_instruction()->shape(),\n+                           call_instr_no_constants->shape()))\n+          << \"Shape mismatch between the host computation and the \"\n+             \"corresponding \"\n+             \"host call.\";\n+\n+      TF_ASSIGN_OR_RETURN(\n+          HloInstruction * async_done,\n+          parent_computation->CreateAsyncInstructions(\n+              call_instr_no_constants, {ShapeUtil::MakeScalarShape(U32)},\n+              HloInstruction::kHostThread,\n+              /*replace=*/true, /*override_names=*/true));\n+      VLOG(1) << \"Turning \" << call_instr_no_constants->name()\n+              << \" into an async instruction \" << async_done->name();\n+\n+      VLOG(1) << \"Replacing\" << call_instr_no_constants->name() << \" with \"\n+              << async_done->name();\n+      TF_RETURN_IF_ERROR(\n+          call_instr_no_constants->ReplaceAllUsesWith(async_done));\n+\n+      RemoveTilesAndMemorySpaces(host_computation);\n+\n+      modified = true;\n+    }\n+  }\n+\n+  if (modified) {\n+    TF_RETURN_IF_ERROR(HloDCE().Run(module).status());\n+\n+    if (module->has_schedule()) {\n+      TF_RETURN_IF_ERROR(module->schedule().Update());\n+    }\n+  }\n+  return modified;\n+}\n+\n+}  // namespace xla"
        },
        {
            "sha": "d8d4776bc692d3386865bc644cad5886d5b4a71b",
            "filename": "third_party/xla/xla/core/host_offloading/host_compute_asyncifier.h",
            "status": "added",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d9c9539fc9aaccc4ebb926d6e59342f8dc8e03a7/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2Fhost_compute_asyncifier.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d9c9539fc9aaccc4ebb926d6e59342f8dc8e03a7/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2Fhost_compute_asyncifier.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2Fhost_compute_asyncifier.h?ref=d9c9539fc9aaccc4ebb926d6e59342f8dc8e03a7",
            "patch": "@@ -0,0 +1,54 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_CORE_HOST_OFFLOADING_HOST_COMPUTE_ASYNCIFIER_H_\n+#define XLA_CORE_HOST_OFFLOADING_HOST_COMPUTE_ASYNCIFIER_H_\n+\n+#include <functional>\n+#include <utility>\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/pass/hlo_pass_interface.h\"\n+\n+namespace xla {\n+\n+// Converts call instructions that execute on the host into async start/done\n+// instructions.\n+class HostComputeAsyncifier : public HloModulePass {\n+ public:\n+  explicit HostComputeAsyncifier(std::function<bool(HloInstruction*)>\n+                                     backend_config_device_type_is_host_fn)\n+      : backend_config_device_type_is_host_fn_(\n+            std::move(backend_config_device_type_is_host_fn)) {}\n+\n+  absl::string_view name() const override { return \"host_compute_asyncifier\"; }\n+\n+  absl::StatusOr<bool> Run(\n+      HloModule* module,\n+      const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n+\n+ private:\n+  // Function that returns true if the instruction should be executed on the\n+  // host. Implementation of the function is device specific.\n+  std::function<bool(HloInstruction*)> backend_config_device_type_is_host_fn_;\n+};\n+\n+}  // namespace xla\n+\n+#endif  // XLA_CORE_HOST_OFFLOADING_HOST_COMPUTE_ASYNCIFIER_H_"
        }
    ],
    "stats": {
        "total": 222,
        "additions": 222,
        "deletions": 0
    }
}