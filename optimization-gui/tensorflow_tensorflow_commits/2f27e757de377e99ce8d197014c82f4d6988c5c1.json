{
    "author": "tensorflower-gardener",
    "message": "Refactor: Use XLA_LOG_DEVICE and XLA_VLOG_DEVICE macros for logging with the device number prefix message.\n\nPiperOrigin-RevId: 839116078",
    "sha": "2f27e757de377e99ce8d197014c82f4d6988c5c1",
    "files": [
        {
            "sha": "735e1492509923589cae9f89d934eadbb8c69de6",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.cc",
            "status": "modified",
            "additions": 167,
            "deletions": 160,
            "changes": 327,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2f27e757de377e99ce8d197014c82f4d6988c5c1/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2f27e757de377e99ce8d197014c82f4d6988c5c1/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc?ref=2f27e757de377e99ce8d197014c82f4d6988c5c1",
            "patch": "@@ -175,15 +175,15 @@ absl::StatusOr<CUmodule> LoadPtx(Context* context, const char* ptx_contents) {\n         CHECK_LE(info_log_buffer_bytes, kLogBufferBytesLimit);\n \n         if (!status.ok()) {\n-          LOG(ERROR) << \"[\" << context->device_ordinal()\n-                     << \"] failed to load PTX text as a module: \" << status;\n+          XLA_LOG_DEVICE(ERROR, context->device_ordinal())\n+              << \"failed to load PTX text as a module: \" << status;\n           // As a precaution for null termination of the API-provided value,\n           // ensure that at least the last byte is null.\n           error_log_buffer[error_log_buffer_bytes ? error_log_buffer_bytes - 1\n                                                   : 0] = '\\0';\n-          LOG(ERROR) << \"[\" << context->device_ordinal()\n-                     << \"] error log buffer (\" << error_log_buffer_bytes\n-                     << \" bytes): \" << error_log_buffer.data();\n+          XLA_LOG_DEVICE(ERROR, context->device_ordinal())\n+              << \"error log buffer (\" << error_log_buffer_bytes\n+              << \" bytes): \" << error_log_buffer.data();\n           if (absl::StrContains(error_log_buffer.data(),\n                                 \"Register allocation failed\")) {\n             returned_status = absl::ResourceExhaustedError(absl::StrFormat(\n@@ -197,12 +197,12 @@ absl::StatusOr<CUmodule> LoadPtx(Context* context, const char* ptx_contents) {\n           return;\n         }\n \n-        VLOG(3) << \"[\" << context->device_ordinal()\n-                << \"] PTX compilation info log (\" << info_log_buffer_bytes\n-                << \" bytes): \" << info_log_buffer.data();\n-        VLOG(3) << \"[\" << context->device_ordinal()\n-                << \"] PTX compilation error log (\" << error_log_buffer_bytes\n-                << \" bytes): \" << error_log_buffer.data();\n+        XLA_VLOG_DEVICE(3, context->device_ordinal())\n+            << \"PTX compilation info log (\" << info_log_buffer_bytes\n+            << \" bytes): \" << info_log_buffer.data();\n+        XLA_VLOG_DEVICE(3, context->device_ordinal())\n+            << \"PTX compilation error log (\" << error_log_buffer_bytes\n+            << \" bytes): \" << error_log_buffer.data();\n         CHECK(module != nullptr);\n         notification.Notify();\n       });\n@@ -217,11 +217,11 @@ absl::StatusOr<CUmodule> LoadPtx(Context* context, const char* ptx_contents) {\n absl::StatusOr<CUmodule> LoadCubin(Context* context, const char* cubin_bytes) {\n   ScopedActivateContext activation(context);\n   CUmodule module;\n-  TF_RETURN_IF_ERROR(\n-      cuda::ToStatus(cuModuleLoadFatBinary(&module, cubin_bytes),\n-                     absl::StrFormat(\"[%d] Failed to load in-memory CUBIN \"\n-                                     \"(compiled for a different GPU?).\",\n-                                     context->device_ordinal())));\n+  TF_RETURN_IF_ERROR(cuda::ToStatus(\n+      cuModuleLoadFatBinary(&module, cubin_bytes),\n+      absl::StrCat(xla::XlaFormatDevice(context->device_ordinal()),\n+                   \"Failed to load in-memory CUBIN \"\n+                   \"(compiled for a different GPU?).\")));\n   return module;\n }\n \n@@ -234,17 +234,17 @@ absl::StatusOr<CUfunction> GetModuleFunction(Context* context, CUmodule module,\n   CHECK(module != nullptr && kernel_name != nullptr);\n   cudaError_t cuda_error = cudaPeekAtLastError();\n   if (cuda_error != cudaSuccess) {\n-    return absl::InternalError(absl::StrCat(\n-        \"[\", context->device_ordinal(),\n-        \"] There was an error before calling cuModuleGetFunction (\", cuda_error,\n-        \"): \", cudaGetErrorName(cuda_error), \" : \",\n-        cudaGetErrorString(cuda_error)));\n+    return absl::InternalError(\n+        absl::StrCat(xla::XlaFormatDevice(context->device_ordinal()),\n+                     \"There was an error before calling cuModuleGetFunction (\",\n+                     cuda_error, \"): \", cudaGetErrorName(cuda_error), \" : \",\n+                     cudaGetErrorString(cuda_error)));\n   }\n   CUfunction function;\n-  TF_RETURN_IF_ERROR(\n-      cuda::ToStatus(cuModuleGetFunction(&function, module, kernel_name),\n-                     absl::StrFormat(\"[%d] Failed to get module function\",\n-                                     context->device_ordinal())));\n+  TF_RETURN_IF_ERROR(cuda::ToStatus(\n+      cuModuleGetFunction(&function, module, kernel_name),\n+      absl::StrCat(xla::XlaFormatDevice(context->device_ordinal()),\n+                   \"Failed to get module function\")));\n   return function;\n }\n \n@@ -268,8 +268,8 @@ void UnloadCudaModule(Context* context, CUmodule module) {\n   ScopedActivateContext activated{context};\n   auto status = cuda::ToStatus(cuModuleUnload(module));\n   if (!status.ok()) {\n-    LOG(ERROR) << \"failed to unload module \" << module\n-               << \"; leaking: \" << status;\n+    XLA_LOG_DEVICE(ERROR, context->device_ordinal())\n+        << \"failed to unload module \" << module << \"; leaking: \" << status;\n   }\n }\n \n@@ -454,7 +454,8 @@ bool GetDeviceTotalMemory(CUdevice device, uint64_t* result) {\n   size_t value{};\n   auto status = cuda::ToStatus(cuDeviceTotalMem(&value, device));\n   if (!status.ok()) {\n-    LOG(ERROR) << \"failed to query total available memory: \" << status;\n+    XLA_LOG_DEVICE(ERROR, device)\n+        << \"failed to query total available memory: \" << status;\n     return false;\n   }\n \n@@ -467,7 +468,7 @@ bool IsEccEnabled(CUdevice device, bool* result) {\n   auto status = cuda::ToStatus(\n       cuDeviceGetAttribute(&value, CU_DEVICE_ATTRIBUTE_ECC_ENABLED, device));\n   if (!status.ok()) {\n-    LOG(ERROR) << \"failed to query ECC status: \" << status;\n+    XLA_LOG_DEVICE(ERROR, device) << \"failed to query ECC status: \" << status;\n     return false;\n   }\n \n@@ -483,11 +484,12 @@ std::string GetPCIBusID(CUdevice device) {\n   absl::Status status = cuda::ToStatus(\n       cuDeviceGetPCIBusId(raw_pci_bus_id.data(), kBufferSize, device));\n   if (!status.ok()) {\n-    LOG(ERROR) << \"failed to query PCI bus id for device: \" << status;\n+    XLA_LOG_DEVICE(ERROR, device)\n+        << \"failed to query PCI bus id for device: \" << status;\n     return \"\";\n   }\n   if (!absl::c_linear_search(raw_pci_bus_id, '\\0')) {\n-    LOG(ERROR) << \"PCI bus id is not null terminated.\";\n+    XLA_LOG_DEVICE(ERROR, device) << \"PCI bus id is not null terminated.\";\n     return \"\";\n   }\n   // Lower the hex characters to match sysfs.\n@@ -536,8 +538,9 @@ void* DeviceAllocate(Context* context, uint64_t bytes) {\n     return nullptr;\n   }\n   void* ptr = reinterpret_cast<void*>(result);\n-  VLOG(2) << \"[\" << context->device_ordinal() << \"] allocated \" << ptr\n-          << \" for context \" << context << \" of \" << bytes << \" bytes\";\n+  XLA_VLOG_DEVICE(2, context->device_ordinal())\n+      << \"allocated \" << ptr << \" for context \" << context << \" of \" << bytes\n+      << \" bytes\";\n   return ptr;\n }\n \n@@ -548,12 +551,12 @@ void DeviceDeallocate(Context* context, void* location) {\n   CUdeviceptr pointer = absl::bit_cast<CUdeviceptr>(location);\n   auto status = cuda::ToStatus(cuMemFree(pointer));\n   if (!status.ok()) {\n-    LOG(ERROR) << \"[\" << context->device_ordinal()\n-               << \"] failed to free device memory at \" << location\n-               << \"; result: \" << status;\n+    XLA_LOG_DEVICE(ERROR, context->device_ordinal())\n+        << \"failed to free device memory at \" << location\n+        << \"; result: \" << status;\n   } else {\n-    VLOG(2) << \"[\" << context->device_ordinal() << \"] deallocated \" << location\n-            << \" for context \" << context;\n+    XLA_VLOG_DEVICE(2, context->device_ordinal())\n+        << \"deallocated \" << location << \" for context \" << context;\n   }\n }\n \n@@ -567,17 +570,17 @@ absl::StatusOr<void*> HostAllocate(Context* context, int numa_node,\n     auto* buffer =\n         tsl::port::NUMAMalloc(numa_node, size, /* minimum_alignment=*/256);\n     if (buffer == nullptr && size > 0) {\n-      return absl::InternalError(\n-          absl::StrFormat(\"[%d] Failed to allocate host memory of size %d \"\n-                          \"pinned to NUMA node %d\",\n-                          context->device_ordinal(), size, numa_node));\n+      return absl::InternalError(absl::StrFormat(\n+          \"%sFailed to allocate host memory of size %d \"\n+          \"pinned to NUMA node %d\",\n+          xla::XlaFormatDevice(context->device_ordinal()), size, numa_node));\n     }\n     if (size > 0 && !HostRegister(context, buffer, size)) {\n       tsl::port::NUMAFree(buffer, size);\n       return absl::InternalError(absl::StrFormat(\n-          \"[%d] Failed to register host memory of size %d pinned to \"\n+          \"%sFailed to register host memory of size %d pinned to \"\n           \"NUMA node %d with the GPU driver\",\n-          context->device_ordinal(), size, numa_node));\n+          xla::XlaFormatDevice(context->device_ordinal()), size, numa_node));\n     }\n     return buffer;\n   } else {\n@@ -589,8 +592,8 @@ absl::StatusOr<void*> HostAllocate(Context* context, int numa_node,\n         cuMemHostAlloc(&buffer, size, CU_MEMHOSTALLOC_PORTABLE)));\n     if (!buffer && size > 0) {\n       return absl::InternalError(absl::StrFormat(\n-          \"[%d] Failed to allocate pinned host memory of size %d\",\n-          context->device_ordinal(), size));\n+          \"%sFailed to allocate pinned host memory of size %d\",\n+          xla::XlaFormatDevice(context->device_ordinal()), size));\n     }\n     return buffer;\n   }\n@@ -608,9 +611,8 @@ void HostDeallocate(Context* context, int numa_node, void* location,\n     ScopedActivateContext activation(context);\n     auto status = cuda::ToStatus(cuMemFreeHost(location));\n     if (!status.ok()) {\n-      LOG(ERROR) << \"[\" << context->device_ordinal()\n-                 << \"] error deallocating host memory at \" << location << \": \"\n-                 << status;\n+      XLA_LOG_DEVICE(ERROR, context->device_ordinal())\n+          << \"error deallocating host memory at \" << location << \": \" << status;\n     }\n   }\n }\n@@ -619,15 +621,15 @@ void HostDeallocate(Context* context, int numa_node, void* location,\n absl::StatusOr<std::unique_ptr<MemoryAllocation>> AllocateHostMemory(\n     CudaContext* cuda_context, int numa_node, uint64_t size) {\n   TF_ASSIGN_OR_RETURN(void* ptr, HostAllocate(cuda_context, numa_node, size));\n-  VLOG(2) << \"[\" << cuda_context->device_ordinal() << \"] allocated \" << ptr\n-          << \" for context \" << cuda_context << \" of \" << size\n-          << \" bytes of host memory\";\n+  XLA_VLOG_DEVICE(2, cuda_context->device_ordinal())\n+      << \"allocated \" << ptr << \" for context \" << cuda_context << \" of \"\n+      << size << \" bytes of host memory\";\n   return std::make_unique<GenericMemoryAllocation>(\n       ptr, size, [cuda_context, numa_node](void* location, uint64_t size) {\n         HostDeallocate(cuda_context, numa_node, location, size);\n-        VLOG(2) << \"[\" << cuda_context->device_ordinal()\n-                << \"] deallocated collective memory at \" << location\n-                << \" for context \" << cuda_context;\n+        XLA_VLOG_DEVICE(2, cuda_context->device_ordinal())\n+            << \"deallocated collective memory at \" << location\n+            << \" for context \" << cuda_context;\n       });\n }\n \n@@ -933,9 +935,10 @@ absl::StatusOr<void*> CudaExecutor::VmmAllocateMemory(uint64_t bytes) {\n   TF_RETURN_IF_ERROR(cuda::ToStatus(\n       cuMemAddressReserve(&ptr, padded_size, granularity, 0, 0)));\n   TF_RETURN_IF_ERROR(cuda::ToStatus(cuMemMap(ptr, padded_size, 0, handle, 0)));\n-  VLOG(3) << \"[\" << device_ordinal() << \"] VMM allocated \" << ptr\n-          << \" requested size: \" << bytes << \" padded size: \" << padded_size\n-          << \" granularity: \" << granularity;\n+\n+  XLA_VLOG_DEVICE(3, device_ordinal())\n+      << \"VMM allocated \" << ptr << \" requested size: \" << bytes\n+      << \" padded size: \" << padded_size << \" granularity: \" << granularity;\n \n   int device_count = 0;\n   TF_RETURN_IF_ERROR(cuda::ToStatus(cudaGetDeviceCount(&device_count)));\n@@ -1021,22 +1024,22 @@ CudaExecutor::CreateMemoryAllocator(MemoryType type) {\n           TF_RETURN_IF_ERROR(cuda::ToStatus(\n               cuMemAllocManaged(&result, size, CU_MEM_ATTACH_GLOBAL)));\n           void* ptr = reinterpret_cast<void*>(result);\n-          VLOG(2) << \"[\" << device_ordinal() << \"] allocated \" << ptr\n-                  << \" for context \" << cuda_context_ << \" of \" << size\n-                  << \" bytes in unified memory\";\n+          XLA_VLOG_DEVICE(2, device_ordinal())\n+              << \"allocated \" << ptr << \" for context \" << cuda_context_\n+              << \" of \" << size << \" bytes in unified memory\";\n           return std::make_unique<GenericMemoryAllocation>(\n               ptr, size, [this](void* location, uint64_t size) {\n                 std::unique_ptr<ActivateContext> activation = Activate();\n                 CUdeviceptr pointer = absl::bit_cast<CUdeviceptr>(location);\n                 auto status = cuda::ToStatus(cuMemFree(pointer));\n                 if (!status.ok()) {\n-                  LOG(ERROR) << \"[\" << device_ordinal()\n-                             << \"] failed to free unified memory at \"\n-                             << location << \"; result: \" << status;\n+                  XLA_LOG_DEVICE(ERROR, device_ordinal())\n+                      << \"failed to free unified memory at \" << location\n+                      << \"; result: \" << status;\n                 } else {\n-                  VLOG(2) << \"[\" << device_ordinal()\n-                          << \"] deallocated unified memory at \" << location\n-                          << \" for context \" << cuda_context_;\n+                  XLA_VLOG_DEVICE(2, device_ordinal())\n+                      << \"deallocated unified memory at \" << location\n+                      << \" for context \" << cuda_context_;\n                 }\n               });\n         });\n@@ -1047,20 +1050,20 @@ CudaExecutor::CreateMemoryAllocator(MemoryType type) {\n         [this](uint64_t size)\n             -> absl::StatusOr<std::unique_ptr<MemoryAllocation>> {\n           TF_ASSIGN_OR_RETURN(void* ptr, CollectiveMemoryAllocate(this, size));\n-          VLOG(2) << \"[\" << device_ordinal() << \"] allocated \" << ptr\n-                  << \" for context \" << cuda_context_ << \" of \" << size\n-                  << \" bytes of collective memory\";\n+          XLA_VLOG_DEVICE(2, device_ordinal())\n+              << \"allocated \" << ptr << \" for context \" << cuda_context_\n+              << \" of \" << size << \" bytes of collective memory\";\n           return std::make_unique<GenericMemoryAllocation>(\n               ptr, size, [this](void* location, uint64_t size) {\n                 auto status = CollectiveMemoryDeallocate(this, location);\n                 if (!status.ok()) {\n-                  LOG(ERROR) << \"[\" << device_ordinal()\n-                             << \"] failed to free collective memory at \"\n-                             << location << \"; result: \" << status;\n+                  XLA_LOG_DEVICE(ERROR, device_ordinal())\n+                      << \"failed to free collective memory at \" << location\n+                      << \"; result: \" << status;\n                 } else {\n-                  VLOG(2) << \"[\" << device_ordinal()\n-                          << \"] deallocated collective memory at \" << location\n-                          << \" for context \" << cuda_context_;\n+                  XLA_VLOG_DEVICE(2, device_ordinal())\n+                      << \"deallocated collective memory at \" << location\n+                      << \" for context \" << cuda_context_;\n                 }\n               });\n         });\n@@ -1088,7 +1091,7 @@ absl::Status CudaExecutor::Init() {\n   numa_node_ = ReadNumaNode(GetPCIBusID(device_), device_ordinal())\n                    .value_or(tsl::port::kNUMANoAffinity);\n   if (numa_node_ == tsl::port::kNUMANoAffinity) {\n-    VLOG(2) << \"[\" << device_ordinal() << \"] Could not determine NUMA node\";\n+    XLA_VLOG_DEVICE(2, device_ordinal()) << \"Could not determine NUMA node\";\n   }\n   return absl::OkStatus();\n }\n@@ -1113,13 +1116,14 @@ absl::StatusOr<ModuleHandle> CudaExecutor::LoadModuleFromCuBin(\n   if (module == nullptr) {\n     TF_ASSIGN_OR_RETURN(module, LoadCubin(cuda_context_, cubin));\n     module_refcount = 1;\n-    VLOG(3) << \"[\" << device_ordinal() << \"] Loaded CUBIN \"\n-            << static_cast<const void*>(cubin) << \" as module \" << module;\n+    XLA_VLOG_DEVICE(3, device_ordinal())\n+        << \"Loaded CUBIN \" << static_cast<const void*>(cubin) << \" as module \"\n+        << module;\n   } else {\n     ++module_refcount;\n-    VLOG(3) << \"[\" << device_ordinal() << \"] CUBIN \"\n-            << static_cast<const void*>(cubin)\n-            << \" is already loaded as module \" << module;\n+    XLA_VLOG_DEVICE(3, device_ordinal())\n+        << \"CUBIN \" << static_cast<const void*>(cubin)\n+        << \" is already loaded as module \" << module;\n   }\n   gpu_binary_to_module_[module_handle] = {module, module_refcount};\n   return module_handle;\n@@ -1133,14 +1137,15 @@ absl::StatusOr<ModuleHandle> CudaExecutor::LoadModuleFromPtx(const char* ptx) {\n \n   if (module == nullptr) {\n     TF_ASSIGN_OR_RETURN(module, LoadPtx(cuda_context_, ptx));\n-    VLOG(3) << \"[\" << device_ordinal() << \"] Loaded PTX \"\n-            << static_cast<const void*>(ptx) << \" as module \" << module;\n+    XLA_VLOG_DEVICE(3, device_ordinal())\n+        << \"Loaded PTX \" << static_cast<const void*>(ptx) << \" as module \"\n+        << module;\n     module_refcount = 1;\n   } else {\n     ++module_refcount;\n-    VLOG(3) << \"[\" << device_ordinal() << \"] PTX \"\n-            << static_cast<const void*>(ptx) << \" is already loaded as module \"\n-            << module;\n+    XLA_VLOG_DEVICE(3, device_ordinal())\n+        << \"PTX \" << static_cast<const void*>(ptx)\n+        << \" is already loaded as module \" << module;\n   }\n   gpu_binary_to_module_[module_handle] = {module, module_refcount};\n   return module_handle;\n@@ -1159,8 +1164,8 @@ absl::StatusOr<std::unique_ptr<Kernel>> CudaExecutor::LoadKernel(\n     kernel_to_gpu_binary_[cuda_kernel.get()] = module_handle;\n \n     CUmodule module = gpu_binary_to_module_.at(module_handle).first;\n-    VLOG(2) << \"[\" << device_ordinal() << \"] getting function \" << kernel_name\n-            << \" from module \" << module;\n+    XLA_VLOG_DEVICE(2, device_ordinal())\n+        << \"getting function \" << kernel_name << \" from module \" << module;\n     TF_ASSIGN_OR_RETURN(\n         CUfunction function,\n         GetModuleFunction(cuda_context_, module, kernel_name.c_str()));\n@@ -1169,17 +1174,17 @@ absl::StatusOr<std::unique_ptr<Kernel>> CudaExecutor::LoadKernel(\n   } else if (spec.has_cuda_ptx_in_memory()) {\n     const char* ptx = spec.cuda_ptx_in_memory()->ptx.data();\n     if (ptx == nullptr) {\n-      LOG(FATAL) << \"[\" << device_ordinal()\n-                 << \"] Loader spec has no ptx for kernel \" << kernel_name;\n+      XLA_LOG_DEVICE(FATAL, device_ordinal())\n+          << \"Loader spec has no ptx for kernel \" << kernel_name;\n     }\n \n     absl::MutexLock lock{in_memory_modules_mu_};\n     TF_ASSIGN_OR_RETURN(ModuleHandle module_handle, LoadModuleFromPtx(ptx));\n     kernel_to_gpu_binary_[cuda_kernel.get()] = module_handle;\n \n     CUmodule module = gpu_binary_to_module_.at(module_handle).first;\n-    VLOG(2) << \"[\" << device_ordinal() << \"] getting function \" << kernel_name\n-            << \" from module \" << module;\n+    XLA_VLOG_DEVICE(2, device_ordinal())\n+        << \"getting function \" << kernel_name << \" from module \" << module;\n     TF_ASSIGN_OR_RETURN(\n         CUfunction function,\n         GetModuleFunction(cuda_context_, module, kernel_name.c_str()));\n@@ -1188,8 +1193,9 @@ absl::StatusOr<std::unique_ptr<Kernel>> CudaExecutor::LoadKernel(\n   } else if (spec.has_in_process_symbol()) {\n     void* symbol = spec.in_process_symbol()->symbol;\n \n-    VLOG(2) << \"[\" << device_ordinal() << \"] Resolve CUDA kernel \"\n-            << kernel_name << \" from symbol pointer: \" << symbol;\n+    XLA_VLOG_DEVICE(2, device_ordinal())\n+        << \"Resolve CUDA kernel \" << kernel_name\n+        << \" from symbol pointer: \" << symbol;\n     cudaFunction_t func;\n     std::unique_ptr<ActivateContext> scoped_activation = Activate();\n     TF_RETURN_IF_ERROR(cuda::ToStatus(\n@@ -1201,8 +1207,8 @@ absl::StatusOr<std::unique_ptr<Kernel>> CudaExecutor::LoadKernel(\n   } else {\n     return absl::InternalError(\"No method of loading CUDA kernel provided\");\n   }\n-  VLOG(3) << \"[\" << device_ordinal()\n-          << \"] LoadKernel on kernel : \" << kernel_name;\n+  XLA_VLOG_DEVICE(3, device_ordinal())\n+      << \"LoadKernel on kernel : \" << kernel_name;\n \n   {\n     // Keep track of loaded kernels.\n@@ -1256,25 +1262,25 @@ CudaExecutor::CreateEventBasedTimer(Stream* stream, bool use_delay_kernel) {\n bool CudaExecutor::UnloadGpuBinary(ModuleHandle gpu_binary) {\n   auto module_it = gpu_binary_to_module_.find(gpu_binary);\n   if (gpu_binary_to_module_.end() == module_it) {\n-    VLOG(3) << \"[\" << device_ordinal() << \"] No loaded CUDA module for \"\n-            << gpu_binary;\n+    XLA_VLOG_DEVICE(3, device_ordinal())\n+        << \"No loaded CUDA module for \" << gpu_binary;\n     return false;\n   }\n   auto& module = module_it->second.first;\n   auto& refcount = module_it->second.second;\n-  VLOG(3) << \"[\" << device_ordinal() << \"] Found CUDA module \" << module\n-          << \" with refcount \" << refcount;\n+  XLA_VLOG_DEVICE(3, device_ordinal())\n+      << \"Found CUDA module \" << module << \" with refcount \" << refcount;\n   if (--refcount == 0) {\n-    VLOG(3) << \"[\" << device_ordinal() << \"] Unloading CUDA module \" << module;\n+    XLA_VLOG_DEVICE(3, device_ordinal()) << \"Unloading CUDA module \" << module;\n     UnloadCudaModule(cuda_context_, module);\n     gpu_binary_to_module_.erase(module_it);\n   }\n   return true;\n }\n \n void CudaExecutor::UnloadKernel(const Kernel* kernel) {\n-  VLOG(3) << \"[\" << device_ordinal() << \"] Unloading kernel \" << kernel << \" : \"\n-          << kernel->name();\n+  XLA_VLOG_DEVICE(3, device_ordinal())\n+      << \"Unloading kernel \" << kernel << \" : \" << kernel->name();\n \n   absl::MutexLock lock{in_memory_modules_mu_};\n   loaded_kernels_.erase(kernel);\n@@ -1283,12 +1289,14 @@ void CudaExecutor::UnloadKernel(const Kernel* kernel) {\n   if (kernel_to_gpu_binary_.end() == gpu_binary_it) {\n     // We might never see kernel being explicitly loaded if it was resolved from\n     // in process symbol pointer (CUDA C++ device function pointer).\n-    VLOG(3) << \"[\" << device_ordinal() << \"] Kernel \" << kernel << \" : \"\n-            << kernel->name() << \" has never been loaded.\";\n+    XLA_VLOG_DEVICE(3, device_ordinal())\n+        << \"Kernel \" << kernel << \" : \" << kernel->name()\n+        << \" has never been loaded.\";\n     return;\n   }\n-  VLOG(3) << \"[\" << device_ordinal() << \"] Kernel \" << kernel << \" : \"\n-          << kernel->name() << \" has loaded GPU code \" << gpu_binary_it->second;\n+  XLA_VLOG_DEVICE(3, device_ordinal())\n+      << \"Kernel \" << kernel << \" : \" << kernel->name()\n+      << \" has loaded GPU code \" << gpu_binary_it->second;\n   UnloadGpuBinary(gpu_binary_it->second);\n   kernel_to_gpu_binary_.erase(gpu_binary_it);\n }\n@@ -1393,31 +1401,31 @@ CudaExecutor::CreateOrShareConstant(Stream* stream,\n }\n \n DeviceMemoryBase CudaExecutor::Allocate(uint64_t size, int64_t memory_space) {\n-  VLOG(1) << \"[\" << device_ordinal()\n-          << \"] CudaExecutor::Allocate size: \" << size\n-          << \" memory_space: \" << memory_space;\n+  XLA_VLOG_DEVICE(1, device_ordinal())\n+      << \"CudaExecutor::Allocate size: \" << size\n+      << \" memory_space: \" << memory_space;\n \n   if (memory_space == static_cast<int64_t>(MemoryType::kCollective)) {\n     auto result = CollectiveMemoryAllocate(this, size);\n     if (!result.ok()) {\n-      LOG(ERROR) << \"Failed to allocate collective memory: \" << result.status();\n-      return DeviceMemoryBase(nullptr, 0);\n+      XLA_LOG_DEVICE(ERROR, device_ordinal())\n+          << \"CudaExecutor::Allocate returns \" << result.value();\n     }\n-    VLOG(1) << \"[\" << device_ordinal() << \"] CudaExecutor::Allocate returns \"\n-            << result.value();\n+    XLA_VLOG_DEVICE(1, device_ordinal())\n+        << \"CudaExecutor::Allocate returns \" << result.value();\n     return DeviceMemoryBase(result.value(), size);\n   }\n \n   if (memory_space ==\n       static_cast<int64_t>(stream_executor::MemoryType::kHost)) {\n     auto result = HostAllocate(cuda_context_, numa_node_, size);\n     if (!result.ok()) {\n-      LOG(ERROR) << \"[\" << device_ordinal()\n-                 << \"] Failed to allocate host memory: \" << result.status();\n+      XLA_LOG_DEVICE(ERROR, device_ordinal())\n+          << \"Failed to allocate host memory: \" << result.status();\n       return DeviceMemoryBase(nullptr, 0);\n     }\n-    VLOG(1) << \"[\" << device_ordinal() << \"] CudaExecutor::Allocate returns \"\n-            << result.value();\n+    XLA_VLOG_DEVICE(1, device_ordinal())\n+        << \"CudaExecutor::Allocate returns \" << result.value();\n     return DeviceMemoryBase(result.value(), size);\n   }\n \n@@ -1429,8 +1437,8 @@ DeviceMemoryBase CudaExecutor::Allocate(uint64_t size, int64_t memory_space) {\n       return DeviceMemoryBase(device_buf_base.value(), size);\n     }\n \n-    LOG(ERROR) << \"Failed to allocate memory with VMM: \"\n-               << device_buf_base.status();\n+    XLA_LOG_DEVICE(ERROR, device_ordinal())\n+        << \"Failed to allocate memory with VMM: \" << device_buf_base.status();\n \n     return DeviceMemoryBase(nullptr, 0);\n   }\n@@ -1439,8 +1447,8 @@ DeviceMemoryBase CudaExecutor::Allocate(uint64_t size, int64_t memory_space) {\n         memory_space == static_cast<int64_t>(MemoryType::kP2P));\n \n   auto device_buf_base = DeviceAllocate(cuda_context_, size);\n-  VLOG(1) << \"[\" << device_ordinal() << \"] CudaExecutor::Allocate returns \"\n-          << device_buf_base;\n+  XLA_VLOG_DEVICE(1, device_ordinal())\n+      << \"CudaExecutor::Allocate returns \" << device_buf_base;\n   return DeviceMemoryBase(device_buf_base, size);\n }\n \n@@ -1450,8 +1458,8 @@ CudaExecutor::HostMemoryAllocate(uint64_t size) {\n }\n \n void CudaExecutor::Deallocate(DeviceMemoryBase* mem) {\n-  VLOG(1) << \"[\" << device_ordinal()\n-          << \"] CudaExecutor::Deallocate mem: \" << mem->opaque();\n+  XLA_VLOG_DEVICE(1, device_ordinal())\n+      << \"CudaExecutor::Deallocate mem: \" << mem->opaque();\n \n   auto status_or_memory_space = GetPointerMemorySpace(mem->opaque());\n   if (!status_or_memory_space.ok()) {\n@@ -1480,15 +1488,14 @@ bool CudaExecutor::SynchronizeAllActivity() {\n }\n \n bool CudaExecutor::HostMemoryRegister(void* location, uint64_t size) {\n-  VLOG(1) << \"[\" << device_ordinal()\n-          << \"] Called StreamExecutor::HostMemoryRegister(data=\" << location\n-          << \")\";\n+  XLA_VLOG_DEVICE(1, device_ordinal())\n+      << \"Called StreamExecutor::HostMemoryRegister(data=\" << location << \")\";\n   return HostRegister(cuda_context_, location, size);\n }\n \n bool CudaExecutor::HostMemoryUnregister(void* location) {\n-  VLOG(1) << \"[\" << device_ordinal()\n-          << \"] Called StreamExecutor::HostUnregister(data=\" << location << \")\";\n+  XLA_VLOG_DEVICE(1, device_ordinal())\n+      << \"Called StreamExecutor::HostUnregister(data=\" << location << \")\";\n   return HostUnregister(cuda_context_, location);\n }\n \n@@ -1510,15 +1517,15 @@ absl::Status CudaExecutor::SynchronousMemcpy(DeviceMemoryBase* gpu_dst,\n                                              const void* host_src,\n                                              uint64_t size) {\n   std::unique_ptr<ActivateContext> activation = Activate();\n-  TF_RETURN_IF_ERROR(\n-      cuda::ToStatus(cuMemcpyHtoD(AsCudaDevicePtr(gpu_dst), host_src, size),\n-                     absl::StrFormat(\"[%d] failed to synchronous memcpy from \"\n-                                     \"host to device: GPU dst: %llx;\"\n-                                     \" host src: %p; size: %u=0x%x\",\n-                                     device_ordinal(), AsCudaDevicePtr(gpu_dst),\n-                                     host_src, size, size)));\n-  VLOG(2) << \"[\" << device_ordinal()\n-          << \"] successfully enqueued sync memcpy h2d of \" << size << \" bytes\";\n+  TF_RETURN_IF_ERROR(cuda::ToStatus(\n+      cuMemcpyHtoD(AsCudaDevicePtr(gpu_dst), host_src, size),\n+      absl::StrFormat(\"%sfailed to synchronous memcpy from \"\n+                      \"host to device: GPU dst: %llx;\"\n+                      \" host src: %p; size: %u=0x%x\",\n+                      xla::XlaFormatDevice(device_ordinal()),\n+                      AsCudaDevicePtr(gpu_dst), host_src, size, size)));\n+  XLA_VLOG_DEVICE(2, device_ordinal())\n+      << \"successfully enqueued sync memcpy h2d of \" << size << \" bytes\";\n   return absl::OkStatus();\n }\n \n@@ -1528,12 +1535,12 @@ absl::Status CudaExecutor::SynchronousMemcpy(void* host_dst,\n   std::unique_ptr<ActivateContext> activation = Activate();\n   TF_RETURN_IF_ERROR(cuda::ToStatus(\n       cuMemcpyDtoH(host_dst, AsCudaDevicePtr(gpu_src), size),\n-      absl::StrFormat(\"[%d] failed to synchronous memcpy from device to host \"\n+      absl::StrFormat(\"%sfailed to synchronous memcpy from device to host \"\n                       \"host dst: %p; GPU src: %llx; size: %u=0x%x\",\n-                      device_ordinal(), host_dst, AsCudaDevicePtr(gpu_src),\n-                      size, size)));\n-  VLOG(2) << \"[\" << device_ordinal() << \"] successfully sync memcpy'd d2h of \"\n-          << size << \" bytes to \" << host_dst;\n+                      xla::XlaFormatDevice(device_ordinal()), host_dst,\n+                      AsCudaDevicePtr(gpu_src), size, size)));\n+  XLA_VLOG_DEVICE(2, device_ordinal()) << \"successfully sync memcpy'd d2h of \"\n+                                       << size << \" bytes to \" << host_dst;\n   return absl::OkStatus();\n }\n \n@@ -1690,8 +1697,8 @@ absl::StatusOr<std::unique_ptr<Stream>> CudaExecutor::CreateStream(\n \n absl::StatusOr<std::unique_ptr<CommandBuffer>>\n CudaExecutor::CreateCommandBuffer(CommandBuffer::Mode mode) {\n-  VLOG(2) << \"[\" << device_ordinal()\n-          << \"] Create CUDA command buffer (CUDA graph)\";\n+  XLA_VLOG_DEVICE(2, device_ordinal())\n+      << \"Create CUDA command buffer (CUDA graph)\";\n   return CudaCommandBuffer::Create(mode, this, cuda_context_);\n }\n \n@@ -2004,15 +2011,15 @@ CudaExecutor::CreateMulticastMemory(uint64_t size, int num_devices) const {\n CudaExecutor::CudaMulticastMemory::~CudaMulticastMemory() {\n   if (handle_ != 0) {\n     for (auto const& [device_ordinal, mapped_memory_ptr] : mapped_devices_) {\n-      VLOG(3) << \"[\" << device_ordinal << \"] Unbind multicast: \" << handle_;\n+      XLA_VLOG_DEVICE(3, device_ordinal) << \"Unbind multicast: \" << handle_;\n       CHECK_OK(stream_executor::cuda::ToStatus(cuMulticastUnbind(\n           handle_, device_ordinal, /*mcOffset=*/0, padded_size_)));\n \n-      VLOG(3) << \"[\" << device_ordinal << \"] Unmap ptr: \" << mapped_memory_ptr;\n+      XLA_VLOG_DEVICE(3, device_ordinal) << \"Unmap ptr: \" << mapped_memory_ptr;\n       CHECK_OK(stream_executor::cuda::ToStatus(\n           cuMemUnmap(mapped_memory_ptr, padded_size_)));\n-      VLOG(3) << \"[\" << device_ordinal\n-              << \"] Release address space: \" << mapped_memory_ptr;\n+      XLA_VLOG_DEVICE(3, device_ordinal)\n+          << \"Release address space: \" << mapped_memory_ptr;\n       CHECK_OK(stream_executor::cuda::ToStatus(\n           cuMemAddressFree(mapped_memory_ptr, padded_size_)));\n     }\n@@ -2053,10 +2060,10 @@ absl::Status CudaExecutor::CudaMulticastMemory::Initialize(\n \n   TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(\n       cuMulticastCreate(&handle_, &multicast_properties)));\n-  VLOG(3) << \"[\" << static_cast<int>(cuda_executor->device_)\n-          << \"] Created multicast memory: \" << static_cast<uint64_t>(handle_)\n-          << \" size: \" << padded_size_ << \" with granularity: \" << granularity_\n-          << \" for \" << num_devices_ << \" devices.\";\n+  XLA_VLOG_DEVICE(3, cuda_executor->device_ordinal())\n+      << \"Created multicast memory: \" << static_cast<uint64_t>(handle_)\n+      << \" size: \" << padded_size_ << \" with granularity: \" << granularity_\n+      << \" for \" << num_devices_ << \" devices.\";\n   return absl::OkStatus();\n }\n \n@@ -2071,7 +2078,7 @@ absl::Status CudaExecutor::CudaMulticastMemory::SubscribeDevice(\n     return absl::InvalidArgumentError(\"All devices are already subscribed.\");\n   }\n \n-  VLOG(3) << \"[\" << device_number << \"] Subscribe to multicast: \" << handle_;\n+  XLA_VLOG_DEVICE(3, device_number) << \"Subscribe to multicast: \" << handle_;\n   TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(\n       cuMulticastAddDevice(handle_, device_number)));\n   subscribed_devices_++;\n@@ -2116,11 +2123,11 @@ absl::StatusOr<void*> CudaExecutor::CudaMulticastMemory::MapMemory(\n       cuMulticastBindMem(handle_, /*mcOffset=*/0, retained_memory_handle,\n                          /*memOffset=*/offset, padded_size_, /*flags=*/0)));\n \n-  VLOG(3) << \"[\" << static_cast<int>(cuda_executor->device_)\n-          << \"] Mapped multicast memory: \" << static_cast<uint64_t>(handle_)\n-          << \" size: \" << padded_size_ << \" with granularity: \" << granularity_\n-          << \" to address: \" << location.opaque()\n-          << \" offset from base range: \" << offset;\n+  XLA_VLOG_DEVICE(3, cuda_executor->device_ordinal())\n+      << \"Mapped multicast memory: \" << static_cast<uint64_t>(handle_)\n+      << \" size: \" << padded_size_ << \" with granularity: \" << granularity_\n+      << \" to address: \" << location.opaque()\n+      << \" offset from base range: \" << offset;\n \n   // Map a virtual address range for the multicast memory. Multicast\n   // memory is used to reduce the data stored in the multicast object."
        },
        {
            "sha": "3c5dabcdbaf4ae3f33059694e892782fc28cedf4",
            "filename": "third_party/xla/xla/util.h",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2f27e757de377e99ce8d197014c82f4d6988c5c1/third_party%2Fxla%2Fxla%2Futil.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2f27e757de377e99ce8d197014c82f4d6988c5c1/third_party%2Fxla%2Fxla%2Futil.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Futil.h?ref=2f27e757de377e99ce8d197014c82f4d6988c5c1",
            "patch": "@@ -129,6 +129,16 @@ struct TimerStats {\n   uint64_t times_called ABSL_GUARDED_BY(stats_mutex) = 0;\n };\n \n+inline std::string XlaFormatDevice(int device_ordinal) {\n+  return absl::StrFormat(\"device=[%d] \", device_ordinal);\n+}\n+\n+#define XLA_VLOG_DEVICE(level, device_ordinal) \\\n+  VLOG(level) << xla::XlaFormatDevice(device_ordinal)\n+\n+#define XLA_LOG_DEVICE(level, device_ordinal) \\\n+  LOG(level) << xla::XlaFormatDevice(device_ordinal)\n+\n // RAII timer for XLA_SCOPED_LOGGING_TIMER and XLA_SCOPED_LOGGING_TIMER_LEVEL\n // macros above.  Recommended usage is via the macros so you don't have to give\n // the timer a name or worry about calling VLOG_IS_ON yourself."
        }
    ],
    "stats": {
        "total": 337,
        "additions": 177,
        "deletions": 160
    }
}