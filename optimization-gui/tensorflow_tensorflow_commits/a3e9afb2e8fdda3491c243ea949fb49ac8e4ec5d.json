{
    "author": "beckerhe",
    "message": "Move custom call handler resolution into CustomCallThunk\n\nFor a custom call we need to resolve a target name (string) to a function pointer. So far this happens in `IrEmitterUnnested` (at the end of the compilation pipeline). But for thunk serialization we need this to happen at runtime (when the thunks are getting reconstructed from the proto representation). Therefore I'm moving this resolving step into the `CustomCallThunk` factory function.\n\nNote that there remains a way to construct a `CustomCallThunk` from just a function pointer. These thunks will not be serializable and that's okay. The logic handles these cases and returns an error. It is important for tests to be able to quickly create a CustomCallThunk from a closure. If we had to register these calls in the registry first it would complicate our tests significantly.\n\nIn detail this change entails:\n- Move resolver logic in new overloads of the `CustomCallThunk::Create` factory function.\n- Call these overloads from `IrEmitterUnnested` and the custom kernel fusion emitter.\n- Add tests for the new overloads\n- Migrate some tests to FFI (the new custom call registry and API)\n- Adjust some error codes in custom call tests. (If a custom call was not found now `kNotFound` is returned instead of `kUnimplemented`).\n\nPiperOrigin-RevId: 818655300",
    "sha": "a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d",
    "files": [
        {
            "sha": "4e0ba31238f9804f21644fda5876eb11f2bcdf58",
            "filename": "third_party/xla/xla/backends/gpu/codegen/custom.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc?ref=a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d",
            "patch": "@@ -941,9 +941,10 @@ absl::StatusOr<FusionEmissionResult> EmitCustomCall(\n       TF_ASSIGN_OR_RETURN(attributes, xla::ffi::BuildAttributesMap(dict));\n     }\n     return CustomCallThunk::Create(\n-        thunk_info, call_target_name, registration->bundle, std::move(ops),\n-        std::move(res), std::move(attributes),\n-        called_computations.empty() ? nullptr : called_computations[0]);\n+        thunk_info, call_target_name, std::move(ops), std::move(res),\n+        std::move(attributes),\n+        called_computations.empty() ? nullptr : called_computations[0],\n+        ir_emitter_context.platform_name());\n   };\n \n   auto legacy_thunk =\n@@ -953,9 +954,10 @@ absl::StatusOr<FusionEmissionResult> EmitCustomCall(\n         backend_config.ok()\n             ? backend_config->custom_call_backend_config().opaque()\n             : custom_call.raw_backend_config_string();\n-    return CustomCallThunk::Create(\n-        thunk_info, call_target_name, std::move(custom_call_target),\n-        std::move(ops), std::move(res), std::move(opaque));\n+    return CustomCallThunk::Create(thunk_info, call_target_name, std::move(ops),\n+                                   std::move(res), std::move(opaque),\n+                                   custom_call.api_version(),\n+                                   ir_emitter_context.platform_name());\n   };\n \n   std::vector<std::unique_ptr<BufferAllocation>> fake_allocations(num_args);"
        },
        {
            "sha": "471d735d5025042efe9a1577b921f9b81b447195",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 15,
            "deletions": 3,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d",
            "patch": "@@ -646,6 +646,7 @@ cc_library(\n     srcs = [\"custom_call_thunk.cc\"],\n     hdrs = [\"custom_call_thunk.h\"],\n     deps = [\n+        \":custom_call_target\",\n         \":thunk\",\n         \"//xla:executable_run_options\",\n         \"//xla:shape_util\",\n@@ -660,19 +661,24 @@ cc_library(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:custom_call_status\",\n         \"//xla/service:custom_call_status_internal\",\n+        \"//xla/service:custom_call_target_registry\",\n         \"//xla/service/gpu:buffer_allocations\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/memory\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\",\n+        \"@local_tsl//tsl/platform\",\n     ],\n )\n \n@@ -684,19 +690,25 @@ xla_test(\n         \":custom_call_thunk\",\n         \":thunk\",\n         \"//xla:executable_run_options\",\n+        \"//xla/ffi\",\n+        \"//xla/ffi:ffi_api\",\n         \"//xla/service:custom_call_status_public_headers\",\n+        \"//xla/service:custom_call_target_registry\",\n         \"//xla/service:executable\",\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:buffer_allocations\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n-        \"@local_tsl//tsl/platform:status_matchers\",\n-        \"@local_tsl//tsl/platform:statusor\",\n-        \"@local_tsl//tsl/platform:test\",\n     ],\n )\n "
        },
        {
            "sha": "90a29ab8f7a45edbb9633d282248a118a80e2066",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk.cc",
            "status": "modified",
            "additions": 114,
            "deletions": 19,
            "changes": 133,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc?ref=a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n \n+#include <cstddef>\n #include <cstdint>\n #include <functional>\n #include <memory>\n@@ -25,11 +26,15 @@ limitations under the License.\n \n #include \"absl/algorithm/container.h\"\n #include \"absl/container/inlined_vector.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n #include \"absl/memory/memory.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/runtime/custom_call_target.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/ffi/api/c_api.h\"\n@@ -41,12 +46,15 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/custom_call_status.h\"\n #include \"xla/service/custom_call_status_internal.h\"\n+#include \"xla/service/custom_call_target_registry.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n+#include \"tsl/platform/platform.h\"\n \n namespace xla {\n namespace gpu {\n@@ -112,13 +120,98 @@ static absl::StatusOr<ffi::CallFrame> BuildCallFramePrototype(\n   return builder.Build();\n }\n \n+static absl::StatusOr<CustomCallThunk::CustomCallTarget>\n+ResolveLegacyCustomCall(const CustomCallTargetRegistry& registry,\n+                        absl::string_view target_name,\n+                        absl::string_view platform_name,\n+                        CustomCallApiVersion api_version) {\n+  void* call_target = CustomCallTargetRegistry::Global()->Lookup(\n+      std::string(target_name), std::string(platform_name));\n+\n+  // For information about this calling convention, see\n+  // xla/g3doc/custom_call.md.\n+  switch (api_version) {\n+    case CustomCallApiVersion::API_VERSION_ORIGINAL: {\n+      constexpr absl::string_view kErrorMessage =\n+          \"Custom call API version `API_VERSION_ORIGINAL` is not supported by \"\n+          \"XLA:GPU. Prefer https://docs.jax.dev/en/latest/ffi.html. It will be \"\n+          \"fully removed in November 2025.\";\n+      if constexpr (tsl::kIsOpenSource) {\n+        LOG(ERROR) << kErrorMessage;\n+      } else {\n+        LOG(FATAL) << kErrorMessage;\n+      }\n+\n+      return [call_target](stream_executor::Stream* stream, void** buffers,\n+                           const char* opaque, size_t opaque_len,\n+                           XlaCustomCallStatus*) {\n+        reinterpret_cast<CustomCallWithOpaqueStreamHandle>(call_target)(\n+            stream->platform_specific_handle().stream, buffers, opaque,\n+            opaque_len);\n+      };\n+      break;\n+    }\n+    case CustomCallApiVersion::API_VERSION_STATUS_RETURNING:\n+    case CustomCallApiVersion::API_VERSION_STATUS_RETURNING_UNIFIED:\n+      return [call_target](stream_executor::Stream* stream, void** buffers,\n+                           const char* opaque, size_t opaque_len,\n+                           XlaCustomCallStatus* status) {\n+        reinterpret_cast<CustomCallWithStatusAndOpaqueStreamHandle>(\n+            call_target)(stream->platform_specific_handle().stream, buffers,\n+                         opaque, opaque_len, status);\n+      };\n+      break;\n+    case CustomCallApiVersion::API_VERSION_TYPED_FFI:\n+      return absl::InvalidArgumentError(\n+          \"Called ResolveLegacyCustomCall with API_VERSION_TYPED_FFI\");\n+    default:\n+      return Internal(\"Unknown custom-call API version enum value: %d\",\n+                      api_version);\n+  }\n+}\n+\n absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n     ThunkInfo thunk_info, std::string target_name, CustomCallTarget call_target,\n     std::vector<std::optional<Slice>> operands,\n-    std::vector<std::optional<Slice>> results, const std::string& opaque) {\n+    std::vector<std::optional<Slice>> results, std::string opaque) {\n   return absl::WrapUnique(new CustomCallThunk(\n-      thunk_info, std::move(target_name), std::move(call_target),\n-      std::move(operands), std::move(results), opaque));\n+      thunk_info, std::move(target_name), std::move(operands),\n+      std::move(results), std::move(opaque), std::move(call_target),\n+      /*api_version=*/std::nullopt));\n+}\n+\n+absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n+    ThunkInfo thunk_info, std::string target_name,\n+    std::vector<std::optional<Slice>> operands,\n+    std::vector<std::optional<Slice>> results, std::string opaque,\n+    CustomCallApiVersion api_version, absl::string_view platform_name) {\n+  if (api_version == CustomCallApiVersion::API_VERSION_TYPED_FFI) {\n+    return absl::InvalidArgumentError(\n+        \"Called overload of CustomCallThunk::Create that is intended for \"\n+        \"legacy custom calls with api_version=API_VERSION_TYPED_FFI\");\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(\n+      CustomCallTarget call_target,\n+      ResolveLegacyCustomCall(*CustomCallTargetRegistry::Global(), target_name,\n+                              platform_name, api_version));\n+\n+  return absl::WrapUnique(new CustomCallThunk(\n+      thunk_info, std::move(target_name), std::move(operands),\n+      std::move(results), std::move(opaque), call_target, api_version));\n+}\n+\n+absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n+    ThunkInfo thunk_info, std::string target_name,\n+    std::vector<std::optional<Slice>> operands,\n+    std::vector<std::optional<Slice>> results, AttributesMap attributes,\n+    const HloComputation* called_computation, absl::string_view platform_name) {\n+  TF_ASSIGN_OR_RETURN(ffi::HandlerRegistration registration,\n+                      ffi::FindHandler(target_name, platform_name));\n+\n+  return Create(thunk_info, std::move(target_name),\n+                std::move(registration.bundle), std::move(operands),\n+                std::move(results), std::move(attributes), called_computation);\n }\n \n absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n@@ -146,39 +239,42 @@ absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n                             XLA_FFI_ExecutionStage_INSTANTIATE));\n   }\n \n-  TF_ASSIGN_OR_RETURN(\n-      CallFrame call_frame,\n-      BuildCallFramePrototype(operands, results, std::move(attributes)));\n-\n+  TF_ASSIGN_OR_RETURN(CallFrame call_frame,\n+                      BuildCallFramePrototype(operands, results, attributes));\n   return absl::WrapUnique(new CustomCallThunk(\n-      thunk_info, std::move(target_name), bundle, std::move(operands),\n-      std::move(results), std::move(call_frame), std::move(execution_state),\n-      called_computation));\n+      thunk_info, std::move(target_name), std::move(bundle),\n+      std::move(operands), std::move(results), std::move(call_frame),\n+      std::move(attributes), std::move(execution_state), called_computation));\n }\n \n-CustomCallThunk::CustomCallThunk(ThunkInfo thunk_info, std::string target_name,\n-                                 CustomCallTarget call_target,\n-                                 std::vector<std::optional<Slice>> operands,\n-                                 std::vector<std::optional<Slice>> results,\n-                                 const std::string& opaque)\n+CustomCallThunk::CustomCallThunk(\n+    ThunkInfo thunk_info, std::string target_name,\n+    std::vector<std::optional<Slice>> operands,\n+    std::vector<std::optional<Slice>> results, std::string opaque,\n+    CustomCallTarget call_target,\n+    const std::optional<CustomCallApiVersion>& api_version)\n     : Thunk(Thunk::kCustomCall, thunk_info),\n+      api_version_(api_version),\n       target_name_(std::move(target_name)),\n       operands_(std::move(operands)),\n       results_(std::move(results)),\n       call_target_(std::move(call_target)),\n-      opaque_(opaque) {}\n+      opaque_(std::move(opaque)) {}\n \n CustomCallThunk::CustomCallThunk(\n     ThunkInfo thunk_info, std::string target_name,\n     XLA_FFI_Handler_Bundle bundle, std::vector<std::optional<Slice>> operands,\n     std::vector<std::optional<Slice>> results, CallFrame call_frame,\n+    AttributesMap attributes,\n     std::unique_ptr<ffi::ExecutionState> execution_state,\n     const HloComputation* called_computation)\n     : Thunk(Thunk::kCustomCall, thunk_info),\n+      api_version_(CustomCallApiVersion::API_VERSION_TYPED_FFI),\n       target_name_(std::move(target_name)),\n       operands_(std::move(operands)),\n       results_(std::move(results)),\n-      bundle_(bundle),\n+      bundle_(std::move(bundle)),\n+      attributes_(std::move(attributes)),\n       call_frame_(std::move(call_frame)),\n       call_frames_([this] { return call_frame_->Copy(); }),\n       execution_state_(std::move(execution_state)),\n@@ -213,9 +309,8 @@ absl::Status CustomCallThunk::ExecuteCustomCall(const ExecuteParams& params) {\n   auto message = CustomCallStatusGetMessage(&custom_call_status);\n   if (message) {\n     return Internal(\"CustomCall failed: %s\", *message);\n-  } else {\n-    return absl::OkStatus();\n   }\n+  return absl::OkStatus();\n }\n \n absl::Status CustomCallThunk::ExecuteFfiHandler("
        },
        {
            "sha": "1cc30efe47b1f038a95530804d853760ca31df28",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk.h",
            "status": "modified",
            "additions": 33,
            "deletions": 5,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h?ref=a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d",
            "patch": "@@ -71,11 +71,34 @@ class CustomCallThunk : public Thunk {\n   using Attribute = ffi::CallFrameBuilder::Attribute;\n   using AttributesMap = ffi::CallFrameBuilder::AttributesMap;\n \n+  // Creates a serializable custom call thunk. The callback is resolved using\n+  // the legacy CustomCall registry. For new code please use XLA FFI instead.\n+  static absl::StatusOr<std::unique_ptr<CustomCallThunk>> Create(\n+      ThunkInfo thunk_info, std::string target_name,\n+      std::vector<std::optional<Slice>> operands,\n+      std::vector<std::optional<Slice>> results, std::string opaque,\n+      CustomCallApiVersion api_version, absl::string_view platform_name);\n+\n+  // Creates a custom call thunk from the given legacy custom call target.\n+  // Note that a thunk created this way can't be serialized to a proto.\n+  // This function is only permitted for unit testing code.\n   static absl::StatusOr<std::unique_ptr<CustomCallThunk>> Create(\n       ThunkInfo thunk_info, std::string target_name,\n       CustomCallTarget call_target, std::vector<std::optional<Slice>> operands,\n-      std::vector<std::optional<Slice>> results, const std::string& opaque);\n+      std::vector<std::optional<Slice>> results, std::string opaque);\n+\n+  // Creates a serializable custom call thunk. The callback is resolved using\n+  // XLA FFI.\n+  static absl::StatusOr<std::unique_ptr<CustomCallThunk>> Create(\n+      ThunkInfo thunk_info, std::string target_name,\n+      std::vector<std::optional<Slice>> operands,\n+      std::vector<std::optional<Slice>> results, AttributesMap attributes,\n+      const HloComputation* called_computation,\n+      absl::string_view platform_name);\n \n+  // Creates a serializable custom call thunk from the given XLA FFI handler\n+  // bundle. Note that `target_name` needs to refer to a registered XLA FFI\n+  // handler which matches the given bundle.\n   static absl::StatusOr<std::unique_ptr<CustomCallThunk>> Create(\n       ThunkInfo thunk_info, std::string target_name,\n       XLA_FFI_Handler_Bundle bundle, std::vector<std::optional<Slice>> operands,\n@@ -103,16 +126,16 @@ class CustomCallThunk : public Thunk {\n \n  private:\n   CustomCallThunk(ThunkInfo thunk_info, std::string target_name,\n-                  CustomCallTarget call_target,\n                   std::vector<std::optional<Slice>> operands,\n-                  std::vector<std::optional<Slice>> results,\n-                  const std::string& opaque);\n+                  std::vector<std::optional<Slice>> results, std::string opaque,\n+                  CustomCallTarget call_target,\n+                  const std::optional<CustomCallApiVersion>& api_version);\n \n   CustomCallThunk(ThunkInfo thunk_info, std::string target_name,\n                   XLA_FFI_Handler_Bundle bundle,\n                   std::vector<std::optional<Slice>> operands,\n                   std::vector<std::optional<Slice>> results,\n-                  ffi::CallFrame call_frame,\n+                  ffi::CallFrame call_frame, AttributesMap attributes,\n                   std::unique_ptr<ffi::ExecutionState> execution_state,\n                   const HloComputation* called_computation);\n \n@@ -124,6 +147,10 @@ class CustomCallThunk : public Thunk {\n                                  const ffi::ExecutionContext* execution_context,\n                                  const BufferAllocations* buffer_allocations);\n \n+  // API version of the custom call. If not set, it means the custom call thunk\n+  // was initialized from a non-registered function pointer and can't be\n+  // serialized to a proto.\n+  std::optional<CustomCallApiVersion> api_version_;\n   std::string target_name_;\n \n   std::vector<std::optional<Slice>> operands_;\n@@ -138,6 +165,7 @@ class CustomCallThunk : public Thunk {\n   // functions with XLA runtime. It's under construction, and still misses\n   // a lot of features. Long term it will replace legacy custom calls.\n   std::optional<XLA_FFI_Handler_Bundle> bundle_;\n+  std::optional<AttributesMap> attributes_;\n \n   // Reference call frame pre-initialized at construction time.\n   std::optional<ffi::CallFrame> call_frame_;"
        },
        {
            "sha": "abd3d5394a0bc52db2705b0e5aa52796b5c44401",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk_test.cc",
            "status": "modified",
            "additions": 98,
            "deletions": 3,
            "changes": 101,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk_test.cc?ref=a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d",
            "patch": "@@ -17,26 +17,35 @@ limitations under the License.\n \n #include <cstddef>\n #include <memory>\n+#include <string>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/executable_run_options.h\"\n+#include \"xla/ffi/ffi.h\"\n+#include \"xla/ffi/ffi_api.h\"\n #include \"xla/service/custom_call_status.h\"\n+#include \"xla/service/custom_call_target_registry.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n-#include \"tsl/platform/status_matchers.h\"\n-#include \"tsl/platform/statusor.h\"\n-#include \"tsl/platform/test.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n \n namespace xla::gpu {\n namespace {\n+using absl_testing::StatusIs;\n+using ::testing::HasSubstr;\n \n static absl::StatusOr<se::StreamExecutor*> GpuExecutor() {\n   TF_ASSIGN_OR_RETURN(auto name, PlatformUtil::CanonicalPlatformName(\"gpu\"));\n@@ -103,5 +112,91 @@ TEST(CustomCallThunkTest, CustomCallOnCustomStream) {\n               absl_testing::IsOk());\n }\n \n+// A simple callback function that always returns an error.\n+absl::Status ReturnError() {\n+  return absl::UnknownError(\"Custom call was executed!\");\n+}\n+\n+XLA_FFI_DEFINE_HANDLER(kReturnError, ReturnError, ffi::Ffi::Bind(),\n+                       {ffi::Traits::kCmdBufferCompatible});\n+\n+constexpr absl::string_view kReturnErrorCustomCallName =\n+    \"__xla_test$$return_error\";\n+\n+XLA_FFI_REGISTER_HANDLER(ffi::GetXlaFfiApi(), kReturnErrorCustomCallName,\n+                         \"CUDA\", kReturnError);\n+XLA_FFI_REGISTER_HANDLER(ffi::GetXlaFfiApi(), kReturnErrorCustomCallName,\n+                         \"ROCM\", kReturnError);\n+\n+TEST(CustomCallThunkTest, ResolvesFFICustomCall) {\n+  TF_ASSERT_OK_AND_ASSIGN(se::StreamExecutor * executor, GpuExecutor());\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<se::Stream> stream,\n+                          executor->CreateStream());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<CustomCallThunk> thunk,\n+      CustomCallThunk::Create(\n+          Thunk::ThunkInfo(),\n+          /*target_name=*/std::string(kReturnErrorCustomCallName),\n+          /*operands=*/{},\n+          /*results=*/{}, /*attributes=*/{},\n+          /*called_computation=*/nullptr,\n+          /*platform_name=*/executor->GetPlatform()->Name()));\n+\n+  se::StreamExecutorMemoryAllocator allocator(executor);\n+  BufferAllocations empty_unused_allocations({}, 0, &allocator);\n+  Thunk::ExecuteParams params = Thunk::ExecuteParams::Create(\n+      ServiceExecutableRunOptions(), empty_unused_allocations,\n+      /*stream=*/stream.get(),\n+      /*command_buffer_trace_stream=*/stream.get(),\n+      /*collective_params=*/nullptr,\n+      /*collective_cliques=*/nullptr);\n+  EXPECT_THAT(thunk->ExecuteOnStream(params),\n+              StatusIs(absl::StatusCode::kUnknown,\n+                       HasSubstr(\"Custom call was executed!\")));\n+}\n+\n+// A simple callback function that always returns an error and has the function\n+// signature for a legacy custom call.\n+void Callback_WithStatusFailed(void* /*stream*/, void** /*buffers*/,\n+                               const char* /*opaque*/, size_t /*opaque_len*/,\n+                               XlaCustomCallStatus* status) {\n+  constexpr absl::string_view kErrorMessage =\n+      \"Legacy Custom call was executed!\";\n+  XlaCustomCallStatusSetFailure(status, kErrorMessage.data(),\n+                                kErrorMessage.size());\n+}\n+\n+XLA_REGISTER_CUSTOM_CALL_TARGET(Callback_WithStatusFailed, \"CUDA\");\n+XLA_REGISTER_CUSTOM_CALL_TARGET(Callback_WithStatusFailed, \"ROCM\");\n+\n+TEST(CustomCallThunkTest, ResolvesLegacyCustomCall) {\n+  TF_ASSERT_OK_AND_ASSIGN(se::StreamExecutor * executor, GpuExecutor());\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<se::Stream> stream,\n+                          executor->CreateStream());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<CustomCallThunk> thunk,\n+      CustomCallThunk::Create(\n+          Thunk::ThunkInfo(),\n+          /*target_name=*/\"Callback_WithStatusFailed\",\n+          /*operands=*/{},\n+          /*results=*/{}, /*opaque=*/\"\",\n+          CustomCallApiVersion::API_VERSION_STATUS_RETURNING,\n+          /*platform_name=*/executor->GetPlatform()->Name()));\n+\n+  se::StreamExecutorMemoryAllocator allocator(executor);\n+  BufferAllocations empty_unused_allocations({}, 0, &allocator);\n+  Thunk::ExecuteParams params = Thunk::ExecuteParams::Create(\n+      ServiceExecutableRunOptions(), empty_unused_allocations,\n+      /*stream=*/stream.get(),\n+      /*command_buffer_trace_stream=*/stream.get(),\n+      /*collective_params=*/nullptr,\n+      /*collective_cliques=*/nullptr);\n+  EXPECT_THAT(thunk->ExecuteOnStream(params),\n+              StatusIs(absl::StatusCode::kInternal,\n+                       HasSubstr(\"Legacy Custom call was executed!\")));\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        },
        {
            "sha": "47c87c7fc8f4f49e90bf2c89e9f6a491ad142ca3",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d",
            "patch": "@@ -195,6 +195,7 @@ xla_test(\n     tags = [\"no-oneapi\"],  # TODO(intel-tf): Remove it when macro substitutions for SYCL are available in xla/stream_executor/sycl/*.\n     deps = [\n         \"//xla:debug_options_flags\",\n+        \"//xla:literal\",\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n         \"//xla:xla_data_proto_cc\",\n@@ -223,6 +224,7 @@ xla_test(\n         \"//xla/tsl/platform:test\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\","
        },
        {
            "sha": "8e58a5e67a64a9ed05df8f0fc447ac7d81506de5",
            "filename": "third_party/xla/xla/service/gpu/custom_call_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 5,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_call_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_call_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_call_test.cc?ref=a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d",
            "patch": "@@ -21,8 +21,6 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n-#include \"xla/shape.h\"\n-\n #if GOOGLE_CUDA\n #include \"third_party/gpus/cuda/include/cuda.h\"  // IWYU pragma: keep\n #include \"third_party/gpus/cuda/include/cuda_runtime_api.h\"\n@@ -34,6 +32,7 @@ limitations under the License.\n #endif\n \n #include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n@@ -48,9 +47,11 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/parser/hlo_parser.h\"\n+#include \"xla/literal.h\"\n #include \"xla/service/custom_call_status.h\"\n #include \"xla/service/custom_call_target_registry.h\"\n #include \"xla/service/hlo_module_config.h\"\n+#include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/gpu_types.h\"\n@@ -95,6 +96,8 @@ XLA_FFI_REGISTER_STRUCT_ATTR_DECODING(::xla::Range, StructMember<int64_t>(\"lo\"),\n \n namespace xla {\n namespace {\n+using ::absl_testing::StatusIs;\n+using ::testing::HasSubstr;\n \n using CustomCallTest = ClientLibraryTestRunnerMixin<HloTestBase>;\n \n@@ -364,9 +367,12 @@ TEST_F(CustomCallTest, ExportedFfiUnknownTarget) {\n              /*schedule=*/CustomCallSchedule::SCHEDULE_NONE,\n              /*api_version=*/CustomCallApiVersion::API_VERSION_TYPED_FFI);\n   auto status = ExecuteAndTransfer(&b, {}).status();\n-  EXPECT_EQ(status.code(), absl::StatusCode::kUnimplemented);\n-  EXPECT_THAT(status.message(),\n-              ::testing::HasSubstr(\"No registered implementation\"));\n+  EXPECT_THAT(\n+      status,\n+      StatusIs(\n+          absl::StatusCode::kNotFound,\n+          HasSubstr(\n+              \"No FFI handler registered for __xla_test$$unknown_target\")));\n }\n \n // Memcpy and SubBuffers tests are already ported in"
        },
        {
            "sha": "a18866ff8dcbf64824745cfe2e8d9666d06afb61",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 86,
            "changes": 108,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d",
            "patch": "@@ -1169,36 +1169,10 @@ absl::Status IrEmitterUnnested::EmitCustomCallThunk(\n   const std::string& call_target_name = instr->custom_call_target();\n \n   // Typed FFI custom calls is a replacement for legacy custom calls\n-  // with a rich type safe API. It's under construction and not\n-  // fully supported.\n+  // with a rich type safe API.\n   bool is_ffi_custom_call =\n       instr->api_version() == CustomCallApiVersion::API_VERSION_TYPED_FFI;\n \n-  void* call_target = CustomCallTargetRegistry::Global()->Lookup(\n-      call_target_name, std::string(platform_name()));\n-\n-  absl::StatusOr<ffi::HandlerRegistration> registration =\n-      ffi::FindHandler(call_target_name, platform_name());\n-\n-  // At least one implementation should be available at run time.\n-  bool found_custom_call = !is_ffi_custom_call && call_target != nullptr;\n-  bool found_ffi_handler = is_ffi_custom_call && registration.ok();\n-\n-  if (!found_custom_call && !found_ffi_handler) {\n-    auto& debug_options = ir_emitter_context_->debug_options();\n-\n-    // If true, then all custom calls that are not found in custom\n-    // call or FFI registries will become no-op (we don't emit any\n-    // thunks for them).\n-    if (debug_options.xla_gpu_mock_custom_calls()) {\n-      return absl::OkStatus();\n-    }\n-\n-    return absl::UnimplementedError(\n-        absl::StrCat(\"No registered implementation for custom call to \",\n-                     call_target_name, \" for platform \", platform_name()));\n-  }\n-\n   using Slices = std::vector<std::optional<CustomCallThunk::Slice>>;\n \n   Slices operands;\n@@ -1234,62 +1208,12 @@ absl::Status IrEmitterUnnested::EmitCustomCallThunk(\n         return absl::OkStatus();\n       }));\n \n-  // For legacy custom calls we convert all API versions into the\n-  // latest status-returning one and pass backend config as an\n-  // opaque string.\n-  CustomCallThunk::CustomCallTarget custom_call_target;\n-\n   // For XLA FFI handlers we decode opaque backend config into\n   // attributes map at IR emission time, so that we do not need to\n   // parse MLIR at run time. For FFI handlers backend config must be\n   // a compatible MLIR dictionary.\n   CustomCallThunk::AttributesMap attributes;\n \n-  // For information about this calling convention, see\n-  // xla/g3doc/custom_call.md.\n-  switch (instr->api_version()) {\n-    case CustomCallApiVersion::API_VERSION_ORIGINAL: {\n-      constexpr absl::string_view kErrorMessage =\n-          \"Custom call API version `API_VERSION_ORIGINAL` is \"\n-          \"not supported \"\n-          \"by XLA:GPU. Prefer \"\n-          \"https://docs.jax.dev/en/latest/ffi.html. It \"\n-          \"will be fully removed in November 2025.\";\n-      if constexpr (tsl::kIsOpenSource) {\n-        LOG(ERROR) << kErrorMessage;\n-      } else {\n-        LOG(FATAL) << kErrorMessage;\n-      }\n-\n-      custom_call_target = [call_target](stream_executor::Stream* stream,\n-                                         void** buffers, const char* opaque,\n-                                         size_t opaque_len,\n-                                         XlaCustomCallStatus*) {\n-        reinterpret_cast<CustomCallWithOpaqueStreamHandle>(call_target)(\n-            stream->platform_specific_handle().stream, buffers, opaque,\n-            opaque_len);\n-      };\n-      break;\n-    }\n-    case CustomCallApiVersion::API_VERSION_STATUS_RETURNING:\n-    case CustomCallApiVersion::API_VERSION_STATUS_RETURNING_UNIFIED:\n-      custom_call_target = [call_target](stream_executor::Stream* stream,\n-                                         void** buffers, const char* opaque,\n-                                         size_t opaque_len,\n-                                         XlaCustomCallStatus* status) {\n-        reinterpret_cast<CustomCallWithStatusAndOpaqueStreamHandle>(\n-            call_target)(stream->platform_specific_handle().stream, buffers,\n-                         opaque, opaque_len, status);\n-      };\n-      break;\n-    case CustomCallApiVersion::API_VERSION_TYPED_FFI:\n-      // We already checked `handler` above.\n-      break;\n-    default:\n-      return Internal(\"Unknown custom-call API version enum value: %d\",\n-                      instr->api_version());\n-  }\n-\n   auto backend_config = instr->backend_config<GpuBackendConfig>();\n   if (!backend_config.ok()) {\n     VLOG(3) << \"Unable to parse backend config for custom call: \"\n@@ -1318,9 +1242,10 @@ absl::Status IrEmitterUnnested::EmitCustomCallThunk(\n     return CustomCallThunk::Create(\n         Thunk::ThunkInfo::WithProfileAnnotation(\n             instr, ir_emitter_context_->GetNextThunkId()),\n-        call_target_name, registration->bundle, std::move(operands),\n-        std::move(results), std::move(attributes),\n-        called_computations.empty() ? nullptr : called_computations[0]);\n+        call_target_name, std::move(operands), std::move(results),\n+        std::move(attributes),\n+        called_computations.empty() ? nullptr : called_computations[0],\n+        ir_emitter_context_->platform_name());\n   };\n \n   auto legacy_thunk =\n@@ -1332,15 +1257,26 @@ absl::Status IrEmitterUnnested::EmitCustomCallThunk(\n     return CustomCallThunk::Create(\n         Thunk::ThunkInfo::WithProfileAnnotation(\n             instr, ir_emitter_context_->GetNextThunkId()),\n-        call_target_name, std::move(custom_call_target), std::move(operands),\n-        std::move(results), std::move(opaque));\n+        call_target_name, std::move(operands), std::move(results),\n+        std::move(opaque), instr->api_version(),\n+        ir_emitter_context_->platform_name());\n   };\n \n-  TF_ASSIGN_OR_RETURN(std::unique_ptr<CustomCallThunk> custom_call_thunk,\n-                      found_ffi_handler ? ffi_thunk() : legacy_thunk());\n-  AddThunkToThunkSequence(std::move(custom_call_thunk));\n+  absl::StatusOr<std::unique_ptr<CustomCallThunk>> custom_call_thunk =\n+      is_ffi_custom_call ? ffi_thunk() : legacy_thunk();\n \n-  return absl::OkStatus();\n+  if (custom_call_thunk.ok()) {\n+    AddThunkToThunkSequence(std::move(custom_call_thunk.value()));\n+    return absl::OkStatus();\n+  }\n+\n+  if (ir_emitter_context_->debug_options().xla_gpu_mock_custom_calls()) {\n+    // xla_gpu_mock_custom_calls=true means we won't emit thunks for all custom\n+    // call targets that couldn't be found.\n+    return absl::OkStatus();\n+  }\n+\n+  return custom_call_thunk.status();\n }\n \n absl::Status IrEmitterUnnested::EmitFftThunk(const HloFftInstruction* instr) {"
        },
        {
            "sha": "398edbb8cb36436334f79907928b04d269dd7b48",
            "filename": "third_party/xla/xla/service/gpu/tests/mock_custom_call_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fmock_custom_call_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fmock_custom_call_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fmock_custom_call_test.cc?ref=a3e9afb2e8fdda3491c243ea949fb49ac8e4ec5d",
            "patch": "@@ -29,7 +29,7 @@ TEST_F(UnknownCustomCallFails, UnknownCustomCallFails) {\n \n     ENTRY Test1 {\n       a = f32[128] parameter(0)\n-      ROOT r1 = f32[128] custom-call(a), custom_call_target=\"my_custom_call\"\n+      ROOT r1 = f32[128] custom-call(a), custom_call_target=\"my_custom_call\", api_version=API_VERSION_TYPED_FFI\n     }\n   )\";\n \n@@ -50,7 +50,7 @@ TEST_F(MockedCustomCall, CustomCallIgnored) {\n \n     ENTRY Test1 {\n       a = f32[128] parameter(0)\n-      ROOT r1 = f32[128] custom-call(a), custom_call_target=\"my_custom_call\"\n+      ROOT r1 = f32[128] custom-call(a), custom_call_target=\"my_custom_call\", api_version=API_VERSION_TYPED_FFI\n     }\n   )\";\n "
        }
    ],
    "stats": {
        "total": 434,
        "additions": 305,
        "deletions": 129
    }
}