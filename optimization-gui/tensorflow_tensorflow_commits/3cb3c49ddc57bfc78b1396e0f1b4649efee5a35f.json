{
    "author": "chsigg",
    "message": "Integrate Triton up to [ff05bd2f](https://github.com/openai/triton/commits/ff05bd2f0b908c07bb920cefbe724aaa5a78bb5e)\n\nhttps://github.com/openxla/triton/tree/triton_integrate_branch-1.14\n\nPiperOrigin-RevId: 831295997",
    "sha": "3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f",
    "files": [
        {
            "sha": "656b9c894904d8511d2c54523fd3883f09f6e876",
            "filename": "third_party/xla/third_party/triton/llvm_integration/series.bzl",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fseries.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fseries.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fseries.bzl?ref=3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f",
            "patch": "@@ -8,8 +8,5 @@ LLVM nor MLIR integrator, please do not add any patches to this list.\n \"\"\"\n \n llvm_patch_list = [\n-    \"//third_party/triton:llvm_integration/cl823109577.patch\",\n-    \"//third_party/triton:llvm_integration/cl825373861.patch\",\n-    \"//third_party/triton:llvm_integration/cl828494580.patch\",\n     # Add new patches just above this line\n ]"
        },
        {
            "sha": "f933f2a8946e9c413d6ac137a7433c01feb4f842",
            "filename": "third_party/xla/third_party/triton/temporary/construction_order.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bea62b9acc239bbc5981a97841c262457b5838ae/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconstruction_order.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bea62b9acc239bbc5981a97841c262457b5838ae/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconstruction_order.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconstruction_order.patch?ref=bea62b9acc239bbc5981a97841c262457b5838ae",
            "patch": "@@ -1,23 +0,0 @@\n-\n---- a/third_party/nvidia/lib/Dialect/NVWS/Transforms/InsertTmemAref.cpp\t2025-09-25 06:36:50.000000000 -0700\n-+++ b/third_party/nvidia/lib/Dialect/NVWS/Transforms/InsertTmemAref.cpp\t2025-09-29 04:55:20.000000000 -0700\n-@@ -54,8 +54,8 @@\n-     SmallVector<std::unique_ptr<Node>> subDags;\n-     Node(Operation *op, OpOperand *tokOperand,\n-          std::optional<PartitionId> partitionId, Node *parent)\n--        : op(op), tokOperand(tokOperand), partitionId(partitionId),\n--          parent(parent), parentDag(nullptr) {}\n-+        : op(op), tokOperand(tokOperand), parent(parent), parentDag(nullptr),\n-+          partitionId(partitionId) {}\n- \n-     // ------------------------------------------------------------------------\n- \n-@@ -364,7 +364,7 @@\n-   enum Kind { PUT, GET };\n- \n-   TMEMAref(Value aref, Value origBuffer, Value replToken)\n--      : aref(aref), origBuffer(origBuffer), replToken(replToken), kind(PUT) {}\n-+      : origBuffer(origBuffer), aref(aref), replToken(replToken), kind(PUT) {}\n- \n-   void acquire(OpBuilder &b, Location loc,\n-                std::pair<std::optional<PartitionId>, StageCluster>"
        },
        {
            "sha": "4fa55269e3323cf15808181679605105d6e2a224",
            "filename": "third_party/xla/third_party/triton/temporary/series.bzl",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fseries.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fseries.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fseries.bzl?ref=3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f",
            "patch": "@@ -14,7 +14,5 @@ those to this list.\n \"\"\"\n \n temporary_patch_list = [\n-    \"//third_party/triton:temporary/utility-fix.patch\",\n-    \"//third_party/triton:temporary/launcher_tma_desc_fix.patch\",\n     # Add new patches just above this line\n ]"
        },
        {
            "sha": "599be88da7b1fd8971bb106faafeb56c4ddbf22a",
            "filename": "third_party/xla/third_party/triton/temporary/triton-tensor-layout-init-fiasco.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 67,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bea62b9acc239bbc5981a97841c262457b5838ae/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ftriton-tensor-layout-init-fiasco.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bea62b9acc239bbc5981a97841c262457b5838ae/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ftriton-tensor-layout-init-fiasco.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ftriton-tensor-layout-init-fiasco.patch?ref=bea62b9acc239bbc5981a97841c262457b5838ae",
            "patch": "@@ -1,67 +0,0 @@\n-Upsteam Pull Request: https://github.com/triton-lang/triton/pull/8117.\n-\n-diff --git a/bin/triton-tensor-layout.cpp b/bin/triton-tensor-layout.cpp\n---- a/bin/triton-tensor-layout.cpp\n-+++ b/bin/triton-tensor-layout.cpp\n-@@ -39,29 +39,32 @@ using namespace mlir;\n- // CLI options\n- //===--------------------------------------------------------------------===//\n-\n--cl::OptionCategory PrinterCategory(\"Available Print Options\",\n--                                   \"Options for the tensor layout printing.\");\n-+static cl::OptionCategory &getPrinterCategory() {\n-+  static cl::OptionCategory PrinterCategory(\n-+      \"Available Print Options\", \"Options for the tensor layout printing.\");\n-+  return PrinterCategory;\n-+}\n-\n- static cl::opt<std::string> InputFile(\n-     \"i\", cl::desc(\"File that contains the tensor data layout attributes\"),\n--    cl::init(\"\"), cl::value_desc(\"filename\"), cl::cat(PrinterCategory));\n-+    cl::init(\"\"), cl::value_desc(\"filename\"), cl::cat(getPrinterCategory()));\n-\n- static cl::opt<std::string>\n-     OutputFile(\"o\", cl::desc(\"Output file to write the layout into\"),\n-                cl::init(\"\"), cl::value_desc(\"filename\"),\n--               cl::cat(PrinterCategory));\n-+               cl::cat(getPrinterCategory()));\n-\n- static cl::opt<std::string>\n-     DataLayoutStr(\"l\", cl::desc(\"Tensor data layout attribute in string\"),\n-                   cl::value_desc(\"layout-string\"), cl::init(\"\"),\n--                  cl::cat(PrinterCategory));\n-+                  cl::cat(getPrinterCategory()));\n-\n- static cl::list<std::string>\n-     AliasName(\"alias-names\",\n-               cl::desc(\"A list of alias names (separated by comma) of the \"\n-                        \"layout attributes in the input file\"),\n-               cl::value_desc(\"name1,name2,name3,...\"), cl::CommaSeparated,\n--              cl::ZeroOrMore, cl::cat(PrinterCategory));\n-+              cl::ZeroOrMore, cl::cat(getPrinterCategory()));\n-\n- static cl::opt<bool> UseHWPointOfView(\n-     \"use-hw-view\",\n-@@ -69,11 +72,11 @@ static cl::opt<bool> UseHWPointOfView(\n-         \"Print the layout in hardware point of view. This means the output is \"\n-         \"from the warp's perspective. Otherwise, the output is from the \"\n-         \"tensor's perspective (e.g., each element maps to xxx thread).\"),\n--    cl::init(false), cl::cat(PrinterCategory));\n-+    cl::init(false), cl::cat(getPrinterCategory()));\n-\n- static cl::opt<std::string> TensorStr(\n-     \"t\", cl::desc(\"Tensor shape and element type (e.g., tensor<2x2xf32>)\"),\n--    cl::init(\"\"), cl::value_desc(\"tensor-type\"), cl::cat(PrinterCategory));\n-+    cl::init(\"\"), cl::value_desc(\"tensor-type\"), cl::cat(getPrinterCategory()));\n-\n- //===--------------------------------------------------------------------===//\n- // Helper functions\n-@@ -180,7 +183,7 @@ static LogicalResult printLayoutFromString(MLIRContext *context,\n- //===--------------------------------------------------------------------===//\n-\n- int main(int argc, char **argv) {\n--  cl::HideUnrelatedOptions(PrinterCategory);\n-+  cl::HideUnrelatedOptions(getPrinterCategory());\n-   cl::ParseCommandLineOptions(argc, argv, \"tensor layout printer\\n\");\n-\n-   DialectRegistry registry;"
        },
        {
            "sha": "53dcecea088c03989f4e4dead4a2808bdbc3ea16",
            "filename": "third_party/xla/third_party/triton/temporary/type-fix-in-test.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bea62b9acc239bbc5981a97841c262457b5838ae/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ftype-fix-in-test.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bea62b9acc239bbc5981a97841c262457b5838ae/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ftype-fix-in-test.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ftype-fix-in-test.patch?ref=bea62b9acc239bbc5981a97841c262457b5838ae",
            "patch": "@@ -1,28 +0,0 @@\n-This test was broken on it's first integration already. This can be upstreamed.\n---- a/test/LLVMIR/convert-to-llvmir-with-dbg-info.mlir\t2025-09-25 06:36:50.000000000 -0700\n-+++ b/test/LLVMIR/convert-to-llvmir-with-dbg-info.mlir\t2025-10-02 02:23:37.000000000 -0700\n-@@ -29,6 +29,7 @@\n-                         %arg2: !llvm.ptr<1>, %arg3: i32, %arg4: !llvm.ptr<1>) {\n-     %constant_i32 = llvm.mlir.constant(9 : i32) : i32\n-     %constant_i16 = llvm.mlir.constant(0 : i16) : i16\n-+    %constant_i64 = llvm.mlir.constant(0 : i64) : i64\n- \n-     // CHECK: !DILocalVariable(name: \"pid\", scope:\n-     %pid = rocdl.workgroup.id.x : i32 loc(#loc14)\n-@@ -49,14 +50,14 @@\n- \n-     // CHECK: !DILocalVariable(name: \"x\", scope:\n-     %x_ptr = llvm.getelementptr %arg0[%block_start] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32\n--    %x_buffer_ptr = rocdl.make.buffer.rsrc %x_ptr, %constant_i16, %constant_i32, %constant_i32 : <1> to <8> loc(#loc18)\n-+    %x_buffer_ptr = rocdl.make.buffer.rsrc %x_ptr, %constant_i16, %constant_i64, %constant_i32 : <1> to <8> loc(#loc18)\n-     llvm.intr.dbg.value #di_local_variable4 = %x_buffer_ptr : !llvm.ptr<8> loc(#loc8)\n-     %x_val = rocdl.raw.ptr.buffer.load %x_buffer_ptr, %mask_i1, %constant_i32, %constant_i32 : vector<4xf32> loc(#loc18)\n-     %x_scalar = llvm.extractelement %x_val[%constant_i32 : i32] : vector<4xf32> loc(#loc18)\n- \n-     // CHECK: !DILocalVariable(name: \"y\", scope:\n-     %y_ptr = llvm.getelementptr %arg1[%block_start] : (!llvm.ptr<1>, i32) -> !llvm.ptr<1>, f32\n--    %y_buffer_ptr = rocdl.make.buffer.rsrc %y_ptr, %constant_i16, %constant_i32, %constant_i32 : <1> to <8> loc(#loc19)\n-+    %y_buffer_ptr = rocdl.make.buffer.rsrc %y_ptr, %constant_i16, %constant_i64, %constant_i32 : <1> to <8> loc(#loc19)\n-     llvm.intr.dbg.value #di_local_variable5 = %y_buffer_ptr : !llvm.ptr<8> loc(#loc10)\n-     %y_val = rocdl.raw.ptr.buffer.load %y_buffer_ptr, %mask_i1, %constant_i32, %constant_i32 : vector<4xf32> loc(#loc19)\n-     %y_scalar = llvm.extractelement %y_val[%constant_i32 : i32] : vector<4xf32> loc(#loc19)"
        },
        {
            "sha": "e9e66e03defb4a6592fadd1f1ffcf234aa36351c",
            "filename": "third_party/xla/third_party/triton/temporary/verify_nvmma_encoding.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 93,
            "changes": 93,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bea62b9acc239bbc5981a97841c262457b5838ae/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fverify_nvmma_encoding.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bea62b9acc239bbc5981a97841c262457b5838ae/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fverify_nvmma_encoding.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fverify_nvmma_encoding.patch?ref=bea62b9acc239bbc5981a97841c262457b5838ae",
            "patch": "@@ -1,93 +0,0 @@\n-\n---- a/lib/Dialect/TritonGPU/IR/Ops.cpp\t2025-08-22 04:02:56.000000000 -0700\n-+++ b/lib/Dialect/TritonGPU/IR/Ops.cpp\t2025-09-08 07:22:55.000000000 -0700\n-@@ -1,3 +1,5 @@\n-+#include \"llvm/Support/Casting.h\"\n-+#include \"llvm/Support/LogicalResult.h\"\n- #include \"mlir/IR/BuiltinTypes.h\"\n- #include \"mlir/IR/Diagnostics.h\"\n- #include \"mlir/Support/DebugStringHelper.h\"\n-@@ -9,9 +11,8 @@\n- #include \"triton/Dialect/TritonGPU/IR/Types.h\"\n- #include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n- #include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n-+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/TMAUtilities.h\"\n- #include \"triton/Tools/LayoutUtils.h\"\n--#include \"llvm/Support/Casting.h\"\n--#include \"llvm/Support/LogicalResult.h\"\n-\n- // Provide custom directive handlers for declarative assemblyFormat.\n- // They must be visible before including the generated op classes.\n-@@ -517,10 +518,47 @@ LogicalResult MemDescReshapeOp::verify() {\n-   return success();\n- }\n-\n--static LogicalResult inferMemDescReshapeOpEncoding(ArrayRef<int64_t> srcShape,\n-+// Verification copied from nvmmaSharedToLinearLayout().\n-+LogicalResult verifyNVMMASharedEncoding(std::optional<Location> loc,\n-+                                        NVMMASharedEncodingAttr attr,\n-+                                        ArrayRef<int64_t> shape,\n-+                                        int elementBitWidth) {\n-+  if (attr.getSwizzlingByteWidth() == 0) return success();\n-+  if (shape.size() < 2)\n-+    return emitOptionalError(loc, \"nvmma_shared encoding requires rank >= 2\");\n-+\n-+  auto shapePerCTA = getShapePerCTA(attr, shape);\n-+  auto tmaShape = triton::nvidia_gpu::getTMABlockShape(attr, shapePerCTA,\n-+                                                       /*packedSize=*/true);\n-+  std::array<int64_t, 2> collapsedTmaShape{1, tmaShape.back()};\n-+  for (int i = 0; i + 1 < shape.size(); i++)\n-+    collapsedTmaShape[0] *= tmaShape[i];\n-+  if (attr.getTransposed()) {\n-+    std::swap(collapsedTmaShape[0], collapsedTmaShape[1]);\n-+  }\n-+\n-+  int tileRows = 8;\n-+  int tileCols = 8 * attr.getSwizzlingByteWidth() / elementBitWidth;\n-+  if (attr.getFp4Padded()) tileCols /= 2;\n-+\n-+  int packingFactor = attr.getFp4Padded() ? 2 : 1;\n-+  if (collapsedTmaShape[1] * packingFactor < tileCols ||\n-+      collapsedTmaShape[0] < tileRows) {\n-+    return emitOptionalError(\n-+        loc,\n-+        \"Illegal shared layout; expected collapsed shapePerCTA to \"\n-+        \"be at least [\",\n-+        tileRows, \", \", (tileCols / packingFactor), \"], collapsedTmaShape: [\",\n-+        collapsedTmaShape[0], \", \", collapsedTmaShape[1], \"]\");\n-+  }\n-+  return success();\n-+}\n-+\n-+static LogicalResult inferMemDescReshapeOpEncoding(std::optional<Location> loc,\n-+                                                   ArrayRef<int64_t> srcShape,\n-                                                    Attribute srcEnc,\n-                                                    ArrayRef<int64_t> dstShape,\n--                                                   Attribute &dstEnc) {\n-+                                                   Attribute& dstEnc) {\n-   if (auto mmaEncoding = dyn_cast<NVMMASharedEncodingAttr>(srcEnc)) {\n-     // TODO: supporting reshape of CTA layouts is non-trivial.\n-     if (getNumCTAs(mmaEncoding) > 1)\n-@@ -544,6 +582,11 @@ static LogicalResult inferMemDescReshapeOpEncoding(ArrayRef<int64_t> srcShape,\n-         ctx, mmaEncoding.getSwizzlingByteWidth(), mmaEncoding.getTransposed(),\n-         mmaEncoding.getElementBitWidth(), mmaEncoding.getFp4Padded(),\n-         CTALayout);\n-+    if (failed(verifyNVMMASharedEncoding(\n-+            loc, cast<NVMMASharedEncodingAttr>(dstEnc), dstShape,\n-+            mmaEncoding.getElementBitWidth()))) {\n-+      return failure();\n-+    }\n-     // Big guns, check linear layouts are equivalent\n-     // We disallow reshaping memdesc_subslice in the verifier\n-     // so allocShape == shape\n-@@ -566,8 +609,8 @@ LogicalResult MemDescReshapeOp::inferReturnTypes(\n-\n-   Attribute dstEncoding;\n-   if (Attribute srcEnc = srcTy.getEncoding()) {\n--    if (failed(inferMemDescReshapeOpEncoding(srcTy.getShape(), srcEnc, dstShape,\n--                                             dstEncoding)))\n-+    if (failed(inferMemDescReshapeOpEncoding(loc, srcTy.getShape(), srcEnc,\n-+                                             dstShape, dstEncoding)))\n-       return failure();\n-   }\n-"
        },
        {
            "sha": "6316fc91a1a9fa65292a7bb19709d45598f21e0f",
            "filename": "third_party/xla/third_party/triton/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f/third_party%2Fxla%2Fthird_party%2Ftriton%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f/third_party%2Fxla%2Fthird_party%2Ftriton%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fworkspace.bzl?ref=3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f",
            "patch": "@@ -7,8 +7,8 @@ load(\"//third_party/triton:temporary/series.bzl\", \"temporary_patch_list\")\n def repo():\n     \"\"\"Imports Triton.\"\"\"\n \n-    TRITON_COMMIT = \"triton_integrate_branch-1.13\"\n-    TRITON_SHA256 = \"390ce756b3e0ce7be0a69633897f11bfd3227682ad90bd720fe4860bfedc4849\"\n+    TRITON_COMMIT = \"triton_integrate_branch-1.14\"\n+    TRITON_SHA256 = \"b684cff8d07e839f8a1ea6cc7d331f370615b4c5530489db76f619aa7aa66608\"\n     tf_http_archive(\n         name = \"triton\",\n         sha256 = TRITON_SHA256,"
        },
        {
            "sha": "133de281e50e85fd74dc298da5bd964c352b10ea",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline_cuda.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc?ref=3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f",
            "patch": "@@ -102,6 +102,7 @@ static void MakeTTGIR(mlir::OpPassManager* pm,\n     pm->addPass(\n         mt::gpu::createTritonGPUAutomaticWarpSpecialization({num_stages}));\n     pm->addPass(mt::gpu::createTritonGPUPipeline({num_stages}));\n+    pm->addPass(mt::gpu::createTritonGPUOptimizePartitionWarps());\n     pm->addPass(mt::gpu::createTritonGPUCombineTensorSelectAndIf());\n     pm->addPass(mt::gpu::createTritonGPUHoistTMEMAlloc({true}));\n     pm->addPass(ttng::createTritonNvidiaGPURemoveTMEMTokensPass());"
        },
        {
            "sha": "761a45c1356db9642200641d875472be2a0ca88d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline_rocm.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_rocm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_rocm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_rocm.cc?ref=3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f",
            "patch": "@@ -81,10 +81,10 @@ static void MakeTTGIR(mlir::OpPassManager* pm,\n   pm->addPass(mlir::createCanonicalizerPass());\n \n   if (rocm_cc.has_amd_matrix_instr()) {\n-    pm->addPass(mlir::createTritonAMDGPUStreamPipeline(\n-        {num_stages, /*global_prefetch=*/0, /*local_prefetch=*/0,\n-         /*use_async_copy=*/false, /*use_block_pingpong=*/false}));\n     // TODO(ROCm) Modify when corresponding run time flags are introduced.\n+    pm->addPass(mlir::createTritonAMDGPUScheduleLoops({num_stages}));\n+    pm->addPass(mlir::createTritonAMDGPUPipeline(\n+        {/*useAsyncCopy=*/false, /*usePingpong=*/false}));\n     if (/*use_async_copy=*/false) {  // Not enabled by default.\n       pm->addPass(mlir::createTritonAMDGPUCoalesceAsyncCopy());\n     }"
        },
        {
            "sha": "819332c4a848266f61e6acad0202952903d1b4a1",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotune_cache_key.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.h?ref=3cb3c49ddc57bfc78b1396e0f1b4649efee5a35f",
            "patch": "@@ -32,7 +32,7 @@ class AutotuneCacheKey {\n   // Tie a version to the cache key in order to invalidate the cache when\n   // necessary. This should be incremented on triton upgrades or any other\n   // changes that may affect the autotuning results.\n-  static constexpr int kCurrentVersion = 15;\n+  static constexpr int kCurrentVersion = 16;\n \n   AutotuneCacheKey(const se::DeviceDescription& device_description,\n                    const HloInstruction& instruction,"
        }
    ],
    "stats": {
        "total": 229,
        "additions": 7,
        "deletions": 222
    }
}