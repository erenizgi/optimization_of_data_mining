{
    "author": "felixwqp",
    "message": "[XLA:GPU] Restrict NVLink-based collective optimizations to Ampere, Hopper, and Blackwell GPUs.\n\nThe `IsNVLinkConnected` utility function now checks the GPU's compute capability and returns false if the device is not Ampere, Hopper, or Blackwell, as NVLink is only available on these architectures. This ensures that collective combining heuristics are only applied when NVLink is actually present.\n\nPiperOrigin-RevId: 801246376",
    "sha": "3bde37102665aec84fe55b5e853af60757077676",
    "files": [
        {
            "sha": "2910324d2109e9fa5d434ec5214a76fa1c9035e4",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3bde37102665aec84fe55b5e853af60757077676/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3bde37102665aec84fe55b5e853af60757077676/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=3bde37102665aec84fe55b5e853af60757077676",
            "patch": "@@ -1231,7 +1231,8 @@ void AddCollectiveCombinerPasses(\n \n   bool enable_heuristic_collective_combining =\n       opts.xla_gpu_experimental_enable_heuristic_collective_combining() &&\n-      !IsNVLinkConnected(module.config(), options.slice_size);\n+      !IsNVLinkConnected(module.config(), device_description,\n+                         options.slice_size);\n \n   if (enable_heuristic_collective_combining) {\n     pipeline.AddPass<CollectiveCombinerAnnotator>(device_description,"
        },
        {
            "sha": "f99125e107c66db0609b62e0fc8dd1ae9dab39e2",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3bde37102665aec84fe55b5e853af60757077676/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3bde37102665aec84fe55b5e853af60757077676/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc?ref=3bde37102665aec84fe55b5e853af60757077676",
            "patch": "@@ -164,7 +164,13 @@ absl::StatusOr<GPUCommunicationType> CommunicationType(\n }\n \n bool IsNVLinkConnected(const HloModuleConfig& config,\n+                       const se::DeviceDescription& device_description,\n                        int64_t nvlink_slice_size) {\n+  se::CudaComputeCapability cc = device_description.cuda_compute_capability();\n+  // NVLink is only available on Ampere/Hopper/Blackwell GPUs.\n+  if (!(cc.IsHopper() || cc.IsAmpere() || cc.IsBlackwell())) {\n+    return false;\n+  }\n   int hlo_device_count = config.num_partitions() * config.replica_count();\n   if (hlo_device_count <= nvlink_slice_size) {\n     VLOG(1) << \"NVLink connected: HLO device count \" << hlo_device_count"
        },
        {
            "sha": "4fe51860e1e8cd410582d9409de402a623c3d679",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3bde37102665aec84fe55b5e853af60757077676/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3bde37102665aec84fe55b5e853af60757077676/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h?ref=3bde37102665aec84fe55b5e853af60757077676",
            "patch": "@@ -49,6 +49,7 @@ enum class GPUTopologyType {\n };\n \n bool IsNVLinkConnected(const HloModuleConfig& config,\n+                       const se::DeviceDescription& device_description,\n                        int64_t nvlink_slice_size);\n \n }  // namespace gpu"
        },
        {
            "sha": "3e5342769c03452823143331067bf8103ae220c1",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils_test.cc",
            "status": "modified",
            "additions": 57,
            "deletions": 22,
            "changes": 79,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3bde37102665aec84fe55b5e853af60757077676/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3bde37102665aec84fe55b5e853af60757077676/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc?ref=3bde37102665aec84fe55b5e853af60757077676",
            "patch": "@@ -37,49 +37,84 @@ namespace {\n using ::absl_testing::IsOkAndHolds;\n using ::testing::Test;\n \n-bool IsNVLinkConnected(int num_partitions, int replica_count,\n+bool IsNVLinkConnected(se::CudaComputeCapability compute_capability,\n+                       int num_partitions, int replica_count,\n                        int64_t nvlink_slice_size) {\n   HloModuleConfig config;\n   config.set_num_partitions(num_partitions);\n   config.set_replica_count(replica_count);\n-  return xla::gpu::IsNVLinkConnected(config, nvlink_slice_size);\n+  se::DeviceDescription device_description =\n+      TestGpuDeviceInfo::RTXA6000DeviceInfo(compute_capability);\n+  return xla::gpu::IsNVLinkConnected(config, device_description,\n+                                     nvlink_slice_size);\n }\n \n TEST(IsNVLinkConnectedTest, SingleHostSingleDevice) {\n   // H100/B200\n-  EXPECT_TRUE(IsNVLinkConnected(\n-      /*num_partitions=*/1, /*replica_count=*/1, /*nvlink_slice_size=*/8));\n+  EXPECT_TRUE(IsNVLinkConnected(se::CudaComputeCapability::Hopper(),\n+                                /*num_partitions=*/1, /*replica_count=*/1,\n+                                /*nvlink_slice_size=*/8));\n+  EXPECT_TRUE(IsNVLinkConnected(se::CudaComputeCapability::Blackwell(),\n+                                /*num_partitions=*/1, /*replica_count=*/1,\n+                                /*nvlink_slice_size=*/8));\n   // A100\n-  EXPECT_TRUE(IsNVLinkConnected(\n-      /*num_partitions=*/1, /*replica_count=*/1, /*nvlink_slice_size=*/16));\n+  EXPECT_TRUE(IsNVLinkConnected(se::CudaComputeCapability::Ampere(),\n+                                /*num_partitions=*/1, /*replica_count=*/1,\n+                                /*nvlink_slice_size=*/16));\n }\n \n TEST(IsNVLinkConnectedTest, SingleHostMultiDevices) {\n   // H100/B200\n-  EXPECT_TRUE(IsNVLinkConnected(\n-      /*num_partitions=*/8, /*replica_count=*/1, /*nvlink_slice_size=*/8));\n-  EXPECT_TRUE(IsNVLinkConnected(\n-      /*num_partitions=*/1, /*replica_count=*/8, /*nvlink_slice_size=*/8));\n+  EXPECT_TRUE(IsNVLinkConnected(se::CudaComputeCapability::Hopper(),\n+                                /*num_partitions=*/8, /*replica_count=*/1,\n+                                /*nvlink_slice_size=*/8));\n+  EXPECT_TRUE(IsNVLinkConnected(se::CudaComputeCapability::Hopper(),\n+                                /*num_partitions=*/1, /*replica_count=*/8,\n+                                /*nvlink_slice_size=*/8));\n+  EXPECT_TRUE(IsNVLinkConnected(se::CudaComputeCapability::Blackwell(),\n+                                /*num_partitions=*/8, /*replica_count=*/1,\n+                                /*nvlink_slice_size=*/8));\n+  EXPECT_TRUE(IsNVLinkConnected(se::CudaComputeCapability::Blackwell(),\n+                                /*num_partitions=*/1, /*replica_count=*/8,\n+                                /*nvlink_slice_size=*/8));\n \n   // A100\n-  EXPECT_TRUE(IsNVLinkConnected(\n-      /*num_partitions=*/1, /*replica_count=*/16, /*nvlink_slice_size=*/16));\n-  EXPECT_TRUE(IsNVLinkConnected(\n-      /*num_partitions=*/16, /*replica_count=*/1, /*nvlink_slice_size=*/16));\n+  EXPECT_TRUE(IsNVLinkConnected(se::CudaComputeCapability::Ampere(),\n+                                /*num_partitions=*/1, /*replica_count=*/16,\n+                                /*nvlink_slice_size=*/16));\n+  EXPECT_TRUE(IsNVLinkConnected(se::CudaComputeCapability::Ampere(),\n+                                /*num_partitions=*/16, /*replica_count=*/1,\n+                                /*nvlink_slice_size=*/16));\n }\n \n TEST(IsNVLinkConnectedTest, MultiHosts) {\n   // H100/B200\n-  EXPECT_FALSE(IsNVLinkConnected(\n-      /*num_partitions=*/16, /*replica_count=*/1, /*nvlink_slice_size=*/8));\n-  EXPECT_FALSE(IsNVLinkConnected(\n-      /*num_partitions=*/1, /*replica_count=*/16, /*nvlink_slice_size=*/8));\n+  EXPECT_FALSE(IsNVLinkConnected(se::CudaComputeCapability::Hopper(),\n+                                 /*num_partitions=*/16, /*replica_count=*/1,\n+                                 /*nvlink_slice_size=*/8));\n+  EXPECT_FALSE(IsNVLinkConnected(se::CudaComputeCapability::Hopper(),\n+                                 /*num_partitions=*/1, /*replica_count=*/16,\n+                                 /*nvlink_slice_size=*/8));\n+  EXPECT_FALSE(IsNVLinkConnected(se::CudaComputeCapability::Blackwell(),\n+                                 /*num_partitions=*/16, /*replica_count=*/1,\n+                                 /*nvlink_slice_size=*/8));\n+  EXPECT_FALSE(IsNVLinkConnected(se::CudaComputeCapability::Blackwell(),\n+                                 /*num_partitions=*/1, /*replica_count=*/16,\n+                                 /*nvlink_slice_size=*/8));\n \n   // A100\n-  EXPECT_FALSE(IsNVLinkConnected(\n-      /*num_partitions=*/1, /*replica_count=*/32, /*nvlink_slice_size=*/16));\n-  EXPECT_FALSE(IsNVLinkConnected(\n-      /*num_partitions=*/32, /*replica_count=*/1, /*nvlink_slice_size=*/16));\n+  EXPECT_FALSE(IsNVLinkConnected(se::CudaComputeCapability::Ampere(),\n+                                 /*num_partitions=*/1, /*replica_count=*/32,\n+                                 /*nvlink_slice_size=*/16));\n+  EXPECT_FALSE(IsNVLinkConnected(se::CudaComputeCapability::Ampere(),\n+                                 /*num_partitions=*/32, /*replica_count=*/1,\n+                                 /*nvlink_slice_size=*/16));\n+}\n+\n+TEST(IsNVLinkConnectedTest, UnsupportedGPU) {\n+  EXPECT_FALSE(IsNVLinkConnected(se::CudaComputeCapability::Volta(),\n+                                 /*num_partitions=*/1, /*replica_count=*/1,\n+                                 /*nvlink_slice_size=*/8));\n }\n \n class CommunicationTypeTest : public Test {"
        }
    ],
    "stats": {
        "total": 89,
        "additions": 66,
        "deletions": 23
    }
}