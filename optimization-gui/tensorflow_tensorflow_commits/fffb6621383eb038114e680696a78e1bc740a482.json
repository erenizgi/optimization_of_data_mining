{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 809666383",
    "sha": "fffb6621383eb038114e680696a78e1bc740a482",
    "files": [
        {
            "sha": "ae315940b73233ef2cb8d0ec1e3ab6942cd5be1b",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fffb6621383eb038114e680696a78e1bc740a482/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fffb6621383eb038114e680696a78e1bc740a482/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=fffb6621383eb038114e680696a78e1bc740a482",
            "patch": "@@ -511,7 +511,7 @@ absl::StatusOr<const GpuExecutable::BufferAllocToDeviceMemoryMap*>\n GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {\n   se::StreamExecutor* executor = stream->parent();\n \n-  absl::MutexLock lock(&module_handle_mutex_);\n+  absl::MutexLock lock(module_handle_mutex_);\n   auto it = module_globals_.find(executor);\n   if (it != module_globals_.end()) {\n     return it->second.get();\n@@ -900,7 +900,7 @@ absl::Status GpuExecutable::ExecuteThunks(\n     // address, and report the allocation info if memory addressed changed.\n     // Useful for identify in user's model if it is command buffer perf friendly\n     // (no command buffer update cost).\n-    absl::MutexLock lock(&module_handle_mutex_);\n+    absl::MutexLock lock(module_handle_mutex_);\n     if (module_allocations_.find(executor) == module_allocations_.end()) {\n       std::vector<se::DeviceMemoryBase> allocs_addr;\n       allocs_addr.reserve(buffer_allocations.size());"
        },
        {
            "sha": "c0f508f21c8c83f1a7b38abdb96dcaedc45562be",
            "filename": "third_party/xla/xla/service/gpu/gpu_fusible.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fffb6621383eb038114e680696a78e1bc740a482/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fffb6621383eb038114e680696a78e1bc740a482/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc?ref=fffb6621383eb038114e680696a78e1bc740a482",
            "patch": "@@ -549,7 +549,7 @@ static int64_t SharedMemoryUsageNoCache(\n \n int64_t FusionInfoCache::GetSharedMemoryUsage(const HloInstruction& instr) {\n   {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     auto it = shared_memory_usage_.find(&instr);\n     if (it != shared_memory_usage_.end()) {\n       return it->second;\n@@ -562,7 +562,7 @@ int64_t FusionInfoCache::GetSharedMemoryUsage(const HloInstruction& instr) {\n   // SharedMemoryUsageNoCache and use the cache *within* the fusion.\n   int64_t shared_memory_usage = SharedMemoryUsageNoCache(instr, device_info_);\n \n-  absl::MutexLock lock(&mutex_);\n+  absl::MutexLock lock(mutex_);\n   shared_memory_usage_.emplace(&instr, shared_memory_usage);\n   return shared_memory_usage;\n }\n@@ -599,7 +599,7 @@ static int64_t NumUnnestedReductionsNoCache(\n \n int64_t FusionInfoCache::GetNumUnnestedReductions(const HloInstruction& instr) {\n   {\n-    absl::MutexLock lock(&mutex_);\n+    absl::MutexLock lock(mutex_);\n     auto it = num_unnested_reductions_.find(&instr);\n     if (it != num_unnested_reductions_.end()) {\n       return it->second;\n@@ -613,7 +613,7 @@ int64_t FusionInfoCache::GetNumUnnestedReductions(const HloInstruction& instr) {\n   int64_t num_unnested_reductions =\n       NumUnnestedReductionsNoCache(instr, device_info_);\n \n-  absl::MutexLock lock(&mutex_);\n+  absl::MutexLock lock(mutex_);\n   num_unnested_reductions_.emplace(&instr, num_unnested_reductions);\n   return num_unnested_reductions;\n }"
        },
        {
            "sha": "c3194edef0368e3f4187aac5ee077054f38ffc1c",
            "filename": "third_party/xla/xla/service/gpu/gpu_transfer_manager.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fffb6621383eb038114e680696a78e1bc740a482/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_transfer_manager.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fffb6621383eb038114e680696a78e1bc740a482/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_transfer_manager.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_transfer_manager.cc?ref=fffb6621383eb038114e680696a78e1bc740a482",
            "patch": "@@ -71,7 +71,7 @@ InfeedManager* GpuTransferManager::GetOrCreateInfeedManager(\n   static auto* const infeed_managers =\n       new absl::flat_hash_map<se::StreamExecutor*,\n                               std::unique_ptr<InfeedManager>>();\n-  absl::MutexLock lock(mutex);\n+  absl::MutexLock lock(*mutex);\n   if (!infeed_managers->contains(executor)) {\n     infeed_managers->emplace(executor,\n                              std::make_unique<InfeedManager>(executor));\n@@ -85,7 +85,7 @@ OutfeedManager* GpuTransferManager::GetOrCreateOutfeedManager(\n   static auto* const outfeed_managers =\n       new absl::flat_hash_map<se::StreamExecutor*,\n                               std::unique_ptr<OutfeedManager>>();\n-  absl::MutexLock lock(mutex);\n+  absl::MutexLock lock(*mutex);\n   if (!outfeed_managers->contains(executor)) {\n     outfeed_managers->emplace(executor, std::make_unique<OutfeedManager>());\n   }\n@@ -178,13 +178,13 @@ absl::Status GpuTransferManager::ReadDynamicShapes(\n \n   // Return checked-out buffers at the end of this function.\n   absl::Cleanup cleanup = [&] {\n-    absl::MutexLock lock(&mu_);\n+    absl::MutexLock lock(mu_);\n     pinned_buffers_.insert(pinned_buffers_.end(), checked_out_buffers.begin(),\n                            checked_out_buffers.end());\n   };\n \n   {\n-    absl::MutexLock lock(&mu_);\n+    absl::MutexLock lock(mu_);\n     TF_RETURN_IF_ERROR(EnsurePinnedBuffersAllocated(stream->parent()));\n \n     for (const auto& src_dst : copies) {\n@@ -261,7 +261,7 @@ absl::Status GpuTransferManager::TransferBufferFromDevice(\n   TF_ASSIGN_OR_RETURN(auto staging_buffer,\n                       GetOrCreateStagingBuffer(stream->parent()));\n \n-  absl::MutexLock lock(&staging_buffer->mutex);\n+  absl::MutexLock lock(staging_buffer->mutex);\n   void* staging = staging_buffer->allocation->opaque();\n \n   // Transfer chunk of data from device to destination via staging buffer.\n@@ -302,7 +302,7 @@ absl::Status GpuTransferManager::TransferBufferToDevice(\n   TF_ASSIGN_OR_RETURN(auto staging_buffer,\n                       GetOrCreateStagingBuffer(stream->parent()));\n \n-  absl::MutexLock lock(&staging_buffer->mutex);\n+  absl::MutexLock lock(staging_buffer->mutex);\n   void* staging = staging_buffer->allocation->opaque();\n \n   // Transfer chunk of data from device to destination.\n@@ -334,7 +334,7 @@ GpuTransferManager::StagingBuffer::StagingBuffer(\n \n absl::StatusOr<GpuTransferManager::StagingBuffer*>\n GpuTransferManager::GetOrCreateStagingBuffer(se::StreamExecutor* executor) {\n-  absl::MutexLock lock(&mutex_);\n+  absl::MutexLock lock(mutex_);\n   if (auto it = staging_buffers_.find(executor); it != staging_buffers_.end()) {\n     return &it->second;\n   }"
        }
    ],
    "stats": {
        "total": 26,
        "additions": 13,
        "deletions": 13
    }
}