{
    "author": "khasanovaa",
    "message": "Integrate Triton up to [d82cfd32](https://github.com/openai/triton/commits/d82cfd3211bc7730063fadd9c5ef28670db7c83e)\n\nhttps://github.com/openxla/triton/tree/triton_integrate_branch-1.11\n\nPiperOrigin-RevId: 798151215",
    "sha": "4cd1b10213e05df5787d1e59a9f756a0c4dce50d",
    "files": [
        {
            "sha": "527848449e54ad9ac73a939e95d9aff00b005466",
            "filename": "third_party/xla/third_party/triton/llvm_integration/cl778630982.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 162,
            "changes": 162,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ebc7b1b14a8096bfec775bad30c2a7b0bd240eff/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl778630982.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ebc7b1b14a8096bfec775bad30c2a7b0bd240eff/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl778630982.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl778630982.patch?ref=ebc7b1b14a8096bfec775bad30c2a7b0bd240eff",
            "patch": "@@ -1,162 +0,0 @@\n-\n---- a/lib/Dialect/Triton/Transforms/RewriteTensorDescriptorToPointer.cpp\t2025-06-02 05:51:09.000000000 -0700\n-+++ b/lib/Dialect/Triton/Transforms/RewriteTensorDescriptorToPointer.cpp\t2025-07-02 14:03:15.000000000 -0700\n-@@ -165,7 +165,7 @@\n- \n-     // Compare with lower bound\n-     Value lowerBound = builder.create<mlir::arith::ConstantIntOp>(\n--        loc, 0, builder.getI64Type());\n-+        loc, builder.getI64Type(), 0);\n-     Value splatLowerBound = builder.create<triton::SplatOp>(\n-         loc, offsetRanges[i].getType(), lowerBound);\n-     Value cmpLower = builder.create<arith::CmpIOp>(\n-\n---- a/lib/Dialect/Triton/Transforms/RewriteTensorPointer.cpp\t2025-06-02 05:51:09.000000000 -0700\n-+++ b/lib/Dialect/Triton/Transforms/RewriteTensorPointer.cpp\t2025-07-02 14:03:15.000000000 -0700\n-@@ -133,7 +133,7 @@\n- \n-       // Compare with lower bound\n-       Value lowerBound = builder.create<mlir::arith::ConstantIntOp>(\n--          loc, 0, builder.getI64Type());\n-+          loc, builder.getI64Type(), 0);\n-       Value splatLowerBound = builder.create<triton::SplatOp>(\n-           loc, offsetWithRange.getType(), lowerBound);\n-       Value cmpLower = builder.create<arith::CmpIOp>(\n-\n---- a/lib/Dialect/TritonGPU/Transforms/Pipeliner/SoftwarePipeliner.cpp\t2025-06-26 08:27:04.000000000 -0700\n-+++ b/lib/Dialect/TritonGPU/Transforms/Pipeliner/SoftwarePipeliner.cpp\t2025-07-02 14:03:15.000000000 -0700\n-@@ -128,11 +128,11 @@\n-       if (isEpilogue) {\n-         // Return false for the predicate of the peeled iteration\n-         return rewriter.create<mlir::arith::ConstantIntOp>(\n--            predOp.getLoc(), 0, predOp.getResult().getType());\n-+            predOp.getLoc(), predOp.getResult().getType(), 0);\n-       } else {\n-         if (predOp.getStage() == predOp.getMaxStage() - 1) {\n-           return rewriter.create<mlir::arith::ConstantIntOp>(\n--              predOp.getLoc(), 1, predOp.getResult().getType());\n-+              predOp.getLoc(), predOp.getResult().getType(), 1);\n-         } else {\n-           OpBuilder::InsertionGuard guard(rewriter);\n-           rewriter.setInsertionPoint(op);\n-\n---- a/python/src/ir.cc\t2025-06-26 08:27:04.000000000 -0700\n-+++ b/python/src/ir.cc\t2025-07-02 14:03:15.000000000 -0700\n-@@ -761,53 +761,53 @@\n-       .def(\"get_int1\",\n-            [](TritonOpBuilder &self, bool v) -> Value {\n-              return Value(self.create<arith::ConstantIntOp>(\n--                 v, self.getBuilder().getI1Type()));\n-+                 self.getBuilder().getI1Type(), v));\n-            })\n-       .def(\"get_int8\",\n-            [](TritonOpBuilder &self, int64_t v) -> Value {\n-              return Value(self.create<arith::ConstantIntOp>(\n--                 v, self.getBuilder().getI8Type()));\n-+                 self.getBuilder().getI8Type(), v));\n-            })\n-       .def(\"get_int16\",\n-            [](TritonOpBuilder &self, int64_t v) -> Value {\n-              return Value(self.create<arith::ConstantIntOp>(\n--                 v, self.getBuilder().getI16Type()));\n-+                 self.getBuilder().getI16Type(), v));\n-            })\n-       .def(\"get_int32\",\n-            [](TritonOpBuilder &self, int64_t v) -> Value {\n-              return Value(self.create<arith::ConstantIntOp>(\n--                 v, self.getBuilder().getI32Type()));\n-+                 self.getBuilder().getI32Type(), v));\n-            })\n-       .def(\"get_int64\",\n-            [](TritonOpBuilder &self, int64_t v) -> Value {\n-              return Value(self.create<arith::ConstantIntOp>(\n--                 v, self.getBuilder().getI64Type()));\n-+                 self.getBuilder().getI64Type(), v));\n-            })\n-       .def(\"get_uint8\",\n-            [](TritonOpBuilder &self, uint64_t v) -> Value {\n-              return Value(self.create<arith::ConstantIntOp>(\n--                 v, self.getBuilder().getI8Type()));\n-+                 self.getBuilder().getI8Type(), v));\n-            })\n-       .def(\"get_uint16\",\n-            [](TritonOpBuilder &self, uint64_t v) -> Value {\n-              return Value(self.create<arith::ConstantIntOp>(\n--                 v, self.getBuilder().getI16Type()));\n-+                 self.getBuilder().getI16Type(), v));\n-            })\n-       .def(\"get_uint32\",\n-            [](TritonOpBuilder &self, uint64_t v) -> Value {\n-              return Value(self.create<arith::ConstantIntOp>(\n--                 v, self.getBuilder().getI32Type()));\n-+                 self.getBuilder().getI32Type(), v));\n-            })\n-       .def(\"get_uint64\",\n-            [](TritonOpBuilder &self, uint64_t v) -> Value {\n-              return Value(self.create<arith::ConstantIntOp>(\n--                 v, self.getBuilder().getI64Type()));\n-+                 self.getBuilder().getI64Type(), v));\n-            })\n-       .def(\"get_bf16\",\n-            [](TritonOpBuilder &self, float v) -> Value {\n-              auto type = self.getBuilder().getBF16Type();\n-              return self.create<arith::ConstantFloatOp>(\n--                 APFloat(type.getFloatSemantics(), std::to_string(v)), type);\n-+                 type, APFloat(type.getFloatSemantics(), std::to_string(v)));\n-            })\n-       .def(\"get_fp16\",\n-            [](TritonOpBuilder &self, float v) -> Value {\n-@@ -828,9 +828,9 @@\n-            [](TritonOpBuilder &self, Type type) -> Value {\n-              if (auto floatTy = dyn_cast<FloatType>(type))\n-                return self.create<arith::ConstantFloatOp>(\n--                   APFloat(floatTy.getFloatSemantics(), 0), floatTy);\n-+                   floatTy, APFloat(floatTy.getFloatSemantics(), 0));\n-              else if (auto intTy = dyn_cast<IntegerType>(type))\n--               return self.create<arith::ConstantIntOp>(0, intTy);\n-+               return self.create<arith::ConstantIntOp>(intTy, 0);\n-              else\n-                throw std::runtime_error(\"Not implemented\");\n-            })\n-@@ -838,7 +838,7 @@\n-            [](TritonOpBuilder &self, Type type) -> Value {\n-              uint64_t val = 0xFFFFFFFFFFFFFFFF;\n-              if (auto intTy = dyn_cast<IntegerType>(type))\n--               return self.create<arith::ConstantIntOp>(val, intTy);\n-+               return self.create<arith::ConstantIntOp>(intTy, val);\n-              else\n-                throw std::runtime_error(\"Not implemented\");\n-            })\n-@@ -851,7 +851,7 @@\n-       .def(\"get_int1_ty\",\n-            [](TritonOpBuilder &self) -> Type {\n-              return self.getBuilder().getI1Type();\n--           }) // or ret::copy?\n-+           })  // or ret::copy?\n-       .def(\"get_int8_ty\",\n-            [](TritonOpBuilder &self) -> Type {\n-              return self.getBuilder().getI8Type();\n-\n---- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/Utility.cpp\t2025-03-25 07:48:50.000000000 -0700\n-+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/Utility.cpp\t2025-07-02 14:03:15.000000000 -0700\n-@@ -114,7 +114,8 @@\n- \n- /// Create a predicate with just single active thread.\n- Value createElectPredicate(Location loc, RewriterBase &rewriter) {\n--  return rewriter.create<NVVM::ElectSyncOp>(loc, i1_ty);\n-+  Value memberMask; // default-constructed = null Value\n-+  return rewriter.create<NVVM::ElectSyncOp>(loc, i1_ty, memberMask);\n- }\n- \n- void createSyncWarp(Location loc, OpBuilder &rewriter) {\n---- a/python/src/llvm.cc\t2025-07-02 05:29:46.000000000 -0700\n-+++ b/python/src/llvm.cc\t2025-07-04 00:15:45.000000000 -0700\n-@@ -318,7 +318,7 @@\n-         ModuleAnalysisManager mam;\n- \n-         if (arch.empty()) {\n--          llvm::TargetLibraryInfoImpl TLII;\n-+          llvm::TargetLibraryInfoImpl TLII(mod->getTargetTriple());\n-           TLII.disableAllFunctions();\n-           fam.registerPass([TLII = std::move(TLII)] {\n-             return llvm::TargetLibraryAnalysis(TLII);"
        },
        {
            "sha": "a1af14d16b85c63e08385f14a025305f37db5d16",
            "filename": "third_party/xla/third_party/triton/llvm_integration/cl784348345.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ebc7b1b14a8096bfec775bad30c2a7b0bd240eff/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl784348345.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ebc7b1b14a8096bfec775bad30c2a7b0bd240eff/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl784348345.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl784348345.patch?ref=ebc7b1b14a8096bfec775bad30c2a7b0bd240eff",
            "patch": "@@ -1,21 +0,0 @@\n-\n---- a/third_party/amd/lib/TritonAMDGPUToLLVM/SchedInstructions.cpp\t2025-04-11 01:29:32.000000000 -0700\n-+++ b/third_party/amd/lib/TritonAMDGPUToLLVM/SchedInstructions.cpp\t2025-07-17 16:41:52.000000000 -0700\n-@@ -471,7 +471,7 @@\n- \n-   explicit TritonAMDGPULowerInstructionSchedHints(StringRef arch,\n-                                                   int32_t numStages) {\n--    this->arch = std::move(arch.str());\n-+    this->arch = arch.str();\n-     this->numStages = numStages;\n-   }\n- \n-@@ -504,7 +504,7 @@\n-           TritonAMDGPUInsertInstructionSchedHints> {\n- \n-   explicit TritonAMDGPUInsertInstructionSchedHints(StringRef variant) {\n--    this->variant = std::move(variant.str());\n-+    this->variant = variant.str();\n-   }\n- \n-   void runOnOperation() override {"
        },
        {
            "sha": "900c1c99b146b16dcf87417721248cfb33ac4921",
            "filename": "third_party/xla/third_party/triton/llvm_integration/cl787144572.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 55,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ebc7b1b14a8096bfec775bad30c2a7b0bd240eff/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl787144572.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ebc7b1b14a8096bfec775bad30c2a7b0bd240eff/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl787144572.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl787144572.patch?ref=ebc7b1b14a8096bfec775bad30c2a7b0bd240eff",
            "patch": "@@ -1,55 +0,0 @@\n-\n---- a/include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td\t2025-06-26 08:27:04.000000000 -0700\n-+++ b/include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td\t2025-07-25 10:12:59.000000000 -0700\n-@@ -102,12 +102,6 @@\n-     DefaultValuedAttr<BoolAttr, \"false\">:$isVolatile\n-   );\n- \n--  let builders = [\n--      OpBuilder<(ins \"Value\":$src, \"Value\":$result,\n--                     \"triton::CacheModifier\":$cache,\n--                     \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>,\n--  ];\n--\n-   let results = (outs TTG_AsyncToken:$token);\n- \n-   let extraClassDeclaration = [{\n-@@ -361,9 +355,6 @@\n-     let arguments = (ins I1:$pred);\n-     let results = (outs Variadic<AnyType>:$result);\n-     let regions = (region SizedRegion<1>:$region);\n--    let builders = [\n--        OpBuilder<(ins \"Value\":$pred)>,\n--    ];\n- }\n- \n- def TTG_MaskReturnOp: TTG_Op<\"mask.return\",\n-\n---- a/python/test/unit/language/test_core.py\t2025-06-26 08:27:04.000000000 -0700\n-+++ b/python/test/unit/language/test_core.py\t2025-07-25 10:13:00.000000000 -0700\n-@@ -1607,7 +1607,7 @@\n-     # atom.add.bf16 is unsupported prior to Hopper so instead we generate an\n-     # atom.cas add loop on Ampere and prior\n-     if dst_type == 'bfloat16' and torch.cuda.get_device_capability()[0] < 9:\n--        assert f\"atom.{sem_str}.global.cas\" in h.asm[\"ptx\"]\n-+        assert f\"atom.{sem_str}.gpu.global.cas\" in h.asm[\"ptx\"]\n-         return\n- \n-     assert f\"atom.global.gpu.{sem_str}\" in h.asm[\"ptx\"]\n-\n---- a/third_party/amd/include/Dialect/TritonAMDGPU/IR/TritonAMDGPUOps.td\t2025-06-02 05:51:09.000000000 -0700\n-+++ b/third_party/amd/include/Dialect/TritonAMDGPU/IR/TritonAMDGPUOps.td\t2025-07-25 10:13:00.000000000 -0700\n-@@ -98,13 +98,6 @@\n-   );\n-   let results = (outs AnyRankedTensor:$result);\n- \n--  let builders = [\n--      // Build a ExtractSliceOp with static offsets and the same result type\n--      OpBuilder<(ins \"RankedTensorType\":$resultType,\n--          \"Value\":$source,\n--          \"ArrayRef<int64_t>\": $static_offsets)>,\n--  ];\n--\n-   let extraClassDeclaration = [{\n-     std::array<unsigned, 3> getArrayAttrMaxRanks() {\n-       unsigned rank = getSource().getType().getRank();"
        },
        {
            "sha": "bef272c563b2ada8e669cd7b3ed82b6c78e0bd6a",
            "filename": "third_party/xla/third_party/triton/llvm_integration/mem_sync_scope_agent_to_device.patch",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fmem_sync_scope_agent_to_device.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fmem_sync_scope_agent_to_device.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fmem_sync_scope_agent_to_device.patch?ref=4cd1b10213e05df5787d1e59a9f756a0c4dce50d",
            "previous_filename": "third_party/xla/third_party/triton/temporary/mem_sync_scope_agent_to_device.patch"
        },
        {
            "sha": "656b9c894904d8511d2c54523fd3883f09f6e876",
            "filename": "third_party/xla/third_party/triton/llvm_integration/series.bzl",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fseries.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fseries.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fseries.bzl?ref=4cd1b10213e05df5787d1e59a9f756a0c4dce50d",
            "patch": "@@ -8,9 +8,5 @@ LLVM nor MLIR integrator, please do not add any patches to this list.\n \"\"\"\n \n llvm_patch_list = [\n-    \"//third_party/triton:llvm_integration/cl787144572.patch\",\n-    \"//third_party/triton:llvm_integration/cl789494309.patch\",\n-    \"//third_party/triton:llvm_integration/cl791659411.patch\",\n-    \"//third_party/triton:llvm_integration/cl793679540.patch\",\n     # Add new patches just above this line\n ]"
        },
        {
            "sha": "64504cb7208283181079c09b6ae1b68843094a6f",
            "filename": "third_party/xla/third_party/triton/llvm_integration/tritongpu-to-ptx-mmav3.patch",
            "status": "added",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Ftritongpu-to-ptx-mmav3.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Ftritongpu-to-ptx-mmav3.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Ftritongpu-to-ptx-mmav3.patch?ref=4cd1b10213e05df5787d1e59a9f756a0c4dce50d",
            "patch": "@@ -0,0 +1,33 @@\n+The PTX generated for this test is sensitive to the LLVM version. Newer versions\n+of the NVPTX backend may use 'prmt.b32' instead of 'bfe.u32' for byte extraction\n+and 'setp.eq.b32' instead of 'setp.eq.s32' for equality comparisons. A later\n+update (probably https://github.com/llvm/llvm-project/commit/f480e1b8258eac3565b3ffaf3f8ed0f77eb87fee)\n+optimized the number of prmt instructions generated for this code, so the\n+number of 'prmt.b32' instructions went from 64 to 48.\n+  The checks below have been updated to match the internal codegen as we think\n+  that they are just optimizations.\n+\n+--- a/test/Conversion/tritongpu_to_ptx_mmav3.mlir\t2025-07-31 05:01:16.000000000 -0700\n++++ b/test/Conversion/tritongpu_to_ptx_mmav3.mlir\t2025-08-06 05:43:00.000000000 -0700\n+@@ -57,7 +57,7 @@\n+ \n+     // CHECK: mov.u32       [[TID:%.*]], %tid.x;\n+     // CHECK: and.b32       [[L0_VAL:%.*]], [[TID]], 1;\n+-    // CHECK: setp.eq.s32   [[L0_OFF:%.*]], [[L0_VAL]], 0;\n++    // CHECK: setp.eq.b32   [[L0_OFF:%.*]], [[L0_VAL]], 0;\n+ \n+     // This is used to perform 16 independent selects in stage 1.\n+ \n+@@ -106,10 +106,10 @@\n+     // the predicate (step 3).\n+ \n+     // CHECK-DAG: and.b32           [[L1_VAL:%.*]], [[TID]], 2;\n+-    // CHECK-DAG: setp.eq.s32       [[L1_OFF:%.*]], [[L1_VAL]], 0;\n++    // CHECK-DAG: setp.eq.b32       [[L1_OFF:%.*]], [[L1_VAL]], 0;\n+     // CHECK-COUNT-16: selp.b32     {{.*}}, {{.*}}, [[L1_OFF]];\n+ \n+-    // CHECK-COUNT-64: bfe.u32\n++    // CHECK-COUNT-48: prmt.b32\n+     // CHECK-COUNT-64: st.volatile.global.b8\n+ \n+     %0 = ttg.convert_layout %arg0 : tensor<128x64xf8E5M2, #mma> -> tensor<128x64xf8E5M2, #dot_op>"
        },
        {
            "sha": "6ce6392617228c7e9f46ebd95e315fb13970b209",
            "filename": "third_party/xla/third_party/triton/temporary/add_set_insertion_point.patch",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fadd_set_insertion_point.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fadd_set_insertion_point.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fadd_set_insertion_point.patch?ref=4cd1b10213e05df5787d1e59a9f756a0c4dce50d",
            "patch": "@@ -0,0 +1,29 @@\n+// Remove once it is upstreamed. Tracking bug: b/440003867\n+--- a/lib/Dialect/TritonNvidiaGPU/Transforms/InterleaveTMem.cpp\t2025-07-31 05:01:16.000000000 -0700\n++++ b/lib/Dialect/TritonNvidiaGPU/Transforms/InterleaveTMem.cpp\t2025-08-19 22:42:39.000000000 -0700\n+@@ -63,6 +63,7 @@\n+ std::pair<Value, AccessRange>\n+ findBufferAccessMemdescSubview(Operation *subview) {\n+   OpBuilder builder(subview->getContext());\n++  builder.setInsertionPoint(subview);\n+   Location loc = subview->getLoc();\n+   TypedValue<ttg::MemDescType> src;\n+   SmallVector<int64_t> shape;\n+\n+--- a/test/TritonNvidiaGPU/interleave_tmem.mlir\t2025-07-31 05:01:16.000000000 -0700\n++++ b/test/TritonNvidiaGPU/interleave_tmem.mlir\t2025-08-19 22:57:53.000000000 -0700\n+@@ -124,12 +124,12 @@\n+   %subview0 = ttg.memdesc_index %alloc0, %c0 : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>\n+   // CHECK: [[ALLOC1:%.+]] = ttng.tmem_alloc\n+   %alloc1 = ttng.tmem_alloc : () -> !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable>\n+-  // CHECK-NEXT: [[SUBVIEW1:%.+]] = ttg.memdesc_index [[ALLOC1]]\n++  // CHECK: [[SUBVIEW1:%.+]] = ttg.memdesc_index [[ALLOC1]]\n+   %subview1 = ttg.memdesc_index %alloc1, %c0 : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>\n+   // CHECK-NEXT: tmem_store %arg0, [[SUBVIEW1]]\n+   ttng.tmem_store %arg0, %subview1, %true : tensor<128x128xf32, #blocked> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>\n+   // CHECK-NEXT: [[ALLOC0:%.+]] = ttng.tmem_alloc\n+-  // CHECK-NEXT: [[SUBVIEW0:%.+]] = ttg.memdesc_index [[ALLOC0]]\n++  // CHECK: [[SUBVIEW0:%.+]] = ttg.memdesc_index [[ALLOC0]]\n+   // CHECK-NEXT: tmem_store %arg0, [[SUBVIEW0]]\n+   ttng.tmem_store %arg0, %subview0, %true : tensor<128x128xf32, #blocked> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>\n+   tt.return"
        },
        {
            "sha": "157d9621fbd36ac777f8f816aebf40ef57cdf529",
            "filename": "third_party/xla/third_party/triton/temporary/allocate-shared-memory-nv.patch",
            "status": "added",
            "additions": 39,
            "deletions": 0,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fallocate-shared-memory-nv.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fallocate-shared-memory-nv.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fallocate-shared-memory-nv.patch?ref=4cd1b10213e05df5787d1e59a9f756a0c4dce50d",
            "patch": "@@ -0,0 +1,39 @@\n+// This patch should be upstreamed. It is exactly what this is already in\n+// upstream for createConvertTritonGPUToLLVMPass, but not for the new\n+// createAllocateSharedMemoryNvPass. Other option is to pass 84 as ptx version\n+// in the pipeline, but we should be consistent with the other passes.\n+\n+--- a/third_party/nvidia/include/TritonNVIDIAGPUToLLVM/Passes.h\t2025-07-31 05:01:16.000000000 -0700\n++++ b/third_party/nvidia/include/TritonNVIDIAGPUToLLVM/Passes.h\t2025-08-04 08:45:47.000000000 -0700\n+@@ -23,6 +23,8 @@\n+ std::unique_ptr<OperationPass<ModuleOp>>\n+ createConvertTritonGPUToLLVMPass(int32_t computeCapability, int32_t ptxVersion);\n+ std::unique_ptr<OperationPass<ModuleOp>>\n++createAllocateSharedMemoryNvPass(int32_t computeCapability);\n++std::unique_ptr<OperationPass<ModuleOp>>\n+ createAllocateSharedMemoryNvPass(int32_t computeCapability, int32_t ptxVersion);\n+ \n+ #define GEN_PASS_REGISTRATION\n+\n+--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/Allocation.cpp\t2025-07-31 05:01:16.000000000 -0700\n++++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/Allocation.cpp\t2025-08-04 08:58:21.000000000 -0700\n+@@ -26,6 +26,8 @@\n+           AllocateSharedMemoryNv> {\n+   using AllocateSharedMemoryNvBase::AllocateSharedMemoryNvBase;\n+ \n++  AllocateSharedMemoryNv(int32_t computeCapability)\n++      : AllocateSharedMemoryNvBase({computeCapability}) {}\n+   AllocateSharedMemoryNv(int32_t computeCapability, int32_t ptxVersion)\n+       : AllocateSharedMemoryNvBase({computeCapability, ptxVersion}) {}\n+ \n+@@ -77,6 +79,10 @@\n+ } // namespace mlir::triton::nvidia_gpu\n+ \n+ namespace mlir::triton {\n++std::unique_ptr<OperationPass<ModuleOp>>\n++createAllocateSharedMemoryNvPass(int32_t computeCapability) {\n++  return std::make_unique<AllocateSharedMemoryNv>(computeCapability);\n++}\n+ std::unique_ptr<OperationPass<ModuleOp>>\n+ createAllocateSharedMemoryNvPass(int32_t computeCapability,\n+                                  int32_t ptxVersion) {"
        },
        {
            "sha": "0031d9f748c428bae5ca5a4fc77058601bc41896",
            "filename": "third_party/xla/third_party/triton/temporary/convert_layout_heuristic.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ebc7b1b14a8096bfec775bad30c2a7b0bd240eff/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconvert_layout_heuristic.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ebc7b1b14a8096bfec775bad30c2a7b0bd240eff/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconvert_layout_heuristic.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconvert_layout_heuristic.patch?ref=ebc7b1b14a8096bfec775bad30c2a7b0bd240eff",
            "patch": "@@ -1,28 +0,0 @@\n-This chanage prevents convert_layout propagation of slice layouts across\n-broadcast/expand_dims.\n-\n-It is generally a good idea, but it is especially important when running\n-triton-xla-squeeze-dims, which removes expand_dims ops. The expand_dims ops\n-remove the slice layouts, so we need to prevent them from propagating\n-further across broadcast ops which can cause large tensor materialization.\n-\n-See also b/422133176.\n-\n---- a/lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp\t2025-05-20 08:08:14.000000000 -0700\n-+++ b/lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp\t2025-07-22 04:36:43.000000000 -0700\n-@@ -300,6 +300,15 @@\n-         continue;\n-       }\n-     }\n-+    // Heuristic: don't propagate slice layouts across broadcasts.\n-+    // This can cause massive register pressure. It's better to convert to\n-+    // blocked before the broadcast.\n-+    if (isa<BroadcastOp>(user) &&\n-+        llvm::any_of(info.encodings, [](Attribute encoding) {\n-+          return llvm::isa_and_nonnull<SliceEncodingAttr>(encoding);\n-+        })) {\n-+      continue;\n-+    }\n-     if (user->hasTrait<OpTrait::SameOperandsAndResultEncoding>() ||\n-         user->hasTrait<OpTrait::Elementwise>() ||\n-         isa<ReduceOp, ExpandDimsOp, ReshapeOp, TransOp, JoinOp, SplitOp,"
        },
        {
            "sha": "3668753e439b921e2925717728a16e07737273a7",
            "filename": "third_party/xla/third_party/triton/temporary/disable-filecheck.patch",
            "status": "added",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fdisable-filecheck.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fdisable-filecheck.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fdisable-filecheck.patch?ref=4cd1b10213e05df5787d1e59a9f756a0c4dce50d",
            "patch": "@@ -0,0 +1,17 @@\n+TODO(b/436582531): The filecheck python package is not available in g3. We\n+should find a replacement for it or have this as a private patch.\n+\n+--- a/python/test/unit/language/test_line_info.py\t2025-07-31 05:01:16.000000000 -0700\n++++ b/python/test/unit/language/test_line_info.py\t2025-08-06 01:12:05.000000000 -0700\n+@@ -255,7 +255,10 @@\n+ \n+ def test_use_name_loc_as_prefix(fresh_triton_cache):\n+     import inspect\n+-    from triton._filecheck import run_filecheck\n++    # TODO(b/436582531): The filecheck python package is not available in g3.\n++    # from triton._filecheck import run_filecheck\n++    def run_filecheck(name, text, template):\n++        pass\n+ \n+     @triton.jit\n+     def kernel_basic(src, N, BLOCK_SIZE: tl.constexpr):"
        },
        {
            "sha": "bf718b017eedb30808b6f2eb948649bdc63e9483",
            "filename": "third_party/xla/third_party/triton/temporary/disable_cublas.patch",
            "status": "added",
            "additions": 49,
            "deletions": 0,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fdisable_cublas.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fdisable_cublas.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fdisable_cublas.patch?ref=4cd1b10213e05df5787d1e59a9f756a0c4dce50d",
            "patch": "@@ -0,0 +1,49 @@\n+Remove after fixing b/436154455. In my opinion, this should be a private patch,\n+since it appliest to a google-internal issue. We might consider merging with\n+xla/third_party/triton/temporary/tutorial_fixes.patch,\n+because we have a similar issue there. Also related to b/346755023.\n+\n+--- a/python/test/unit/language/test_warp_specialization.py\t2025-07-31 05:01:16.000000000 -0700\n++++ b/python/test/unit/language/test_warp_specialization.py\t2025-08-05 04:15:09.000000000 -0700\n+@@ -7,7 +7,10 @@\n+ from triton._internal_testing import is_hip, is_hopper, is_blackwell\n+ from triton.tools.tensor_descriptor import TensorDescriptor\n+ \n+-if not is_hip() and torch.cuda.is_available() and torch.cuda.get_device_capability()[0] in [9, 10]:\n++# Attempts to dlopen cuBLAS, prevent this path\n++# TODO: b/436154455 - Re-enable once we can link in cuBLAS properly\n++# if not is_hip() and torch.cuda.is_available() and torch.cuda.get_device_capability()[0] in [9, 10]:\n++if False:\n+     from triton._C.libtriton import nvidia\n+     cublas_workspace = torch.empty(32 * 1024 * 1024, device=\"cuda\", dtype=torch.uint8)\n+     cublas = nvidia.cublas.CublasLt(cublas_workspace)\n+@@ -285,9 +288,11 @@\n+     else:\n+         assert \"ttg.warp_specialize\" in ttgir\n+ \n+-    ref_out = torch.empty((M, N), dtype=dtype, device=device)\n+-    cublas.matmul(A, B, ref_out)\n+-    torch.testing.assert_close(ref_out.to(torch.float16), C.to(torch.float16), atol=0.03, rtol=0.03)\n++    # TODO: b/436154455 - Re-enable once we can link in cuBLAS properly\n++    if cublas is not None:\n++        ref_out = torch.empty((M, N), dtype=dtype, device=device)\n++        cublas.matmul(A, B, ref_out)\n++        torch.testing.assert_close(ref_out.to(torch.float16), C.to(torch.float16), atol=0.03, rtol=0.03)\n+ \n+ \n+ @triton.jit\n+@@ -386,9 +391,11 @@\n+     else:\n+         assert \"ttg.warp_specialize\" in ttgir\n+ \n+-    ref_out = torch.empty((M, N), dtype=dtype, device=device)\n+-    cublas.matmul(A, B, ref_out)\n+-    torch.testing.assert_close(ref_out.to(torch.float16), C.to(torch.float16), atol=0.03, rtol=0.03)\n++    # TODO: b/436154455 - Re-enable once we can link in cuBLAS properly\n++    if cublas is not None:\n++        ref_out = torch.empty((M, N), dtype=dtype, device=device)\n++        cublas.matmul(A, B, ref_out)\n++        torch.testing.assert_close(ref_out.to(torch.float16), C.to(torch.float16), atol=0.03, rtol=0.03)\n+ \n+ \n+ @triton.jit"
        },
        {
            "sha": "b6f17a917f6ff126e106a0ff8d0850e629a7aa93",
            "filename": "third_party/xla/third_party/triton/temporary/fix_test_core_h100_target.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ebc7b1b14a8096bfec775bad30c2a7b0bd240eff/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ffix_test_core_h100_target.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ebc7b1b14a8096bfec775bad30c2a7b0bd240eff/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ffix_test_core_h100_target.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ffix_test_core_h100_target.patch?ref=ebc7b1b14a8096bfec775bad30c2a7b0bd240eff",
            "patch": "@@ -1,20 +0,0 @@\n-b/433429549: Fixes the wrong behavior of the\n-test_load_scope_sem_coop_grid_cta_not_one in test_core.py test for H100.\n-\n-diff --git a/third_party/nvidia/backend/cuda_utils.cc b/third_party/nvidia/backend/cuda_utils.cc\n---- a/third_party/nvidia/backend/cuda_utils.cc\n-+++ b/third_party/nvidia/backend/cuda_utils.cc\n-@@ -544,13 +544,6 @@ PyObject* launch(PyObject* self, PyObjec\n-                         &kernel_args)) {\n-     return nullptr;\n-   }\n--  if (num_ctas != cluster.size()) {\n--    PyErr_Format(\n--        PyExc_ValueError,\n--        \"Expected cluster dimensions (%d, %d, %d) to have a total size of %d\",\n--        cluster.x, cluster.y, cluster.z, num_ctas);\n--    return nullptr;\n--  }\n-   llvm::ArrayRef<char> signature_metadata(\n-       PyBytes_AS_STRING(signature_metadata_bytes),\n-       PyBytes_GET_SIZE(signature_metadata_bytes));"
        },
        {
            "sha": "a030585d6dc15a204cfcccdb712fd96231587d5a",
            "filename": "third_party/xla/third_party/triton/temporary/no_type_annotation_for_args.patch",
            "status": "added",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fno_type_annotation_for_args.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fno_type_annotation_for_args.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fno_type_annotation_for_args.patch?ref=4cd1b10213e05df5787d1e59a9f756a0c4dce50d",
            "patch": "@@ -0,0 +1,15 @@\n+Adding the type annotation for the args caused pytype to complain that triton.language.core.tuple is not a container.\n+That's because pytype expands the variadic argument type to tuple[], and this file also defines own `tuple` type, so wrong type is assumed.\n+This patch has to be removed once b/438115892 is fixed.\n+\n+--- a/python/triton/language/core.py\t2025-07-31 05:01:16.000000000 -0700\n++++ b/python/triton/language/core.py\t2025-08-12 00:25:59.000000000 -0700\n+@@ -2784,7 +2784,7 @@\n+ @builtin\n+ def map_elementwise(\n+     scalar_fn: Callable[..., Tuple[tensor, ...]],\n+-    *args: tensor,\n++    *args,\n+     pack=1,\n+     _semantic=None,\n+     _generator=None,"
        },
        {
            "sha": "4fa55269e3323cf15808181679605105d6e2a224",
            "filename": "third_party/xla/third_party/triton/temporary/series.bzl",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fseries.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fseries.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fseries.bzl?ref=4cd1b10213e05df5787d1e59a9f756a0c4dce50d",
            "patch": "@@ -14,10 +14,5 @@ those to this list.\n \"\"\"\n \n temporary_patch_list = [\n-    \"//third_party/triton:temporary/fix_test_core_h100_target.patch\",\n-    \"//third_party/triton:temporary/speed_up_int4_unpacking.patch\",\n-    \"//third_party/triton:temporary/tutorial_fixes.patch\",\n-    \"//third_party/triton:temporary/ws_fix.patch\",\n-    \"//third_party/triton:temporary/ws_ub_fix.patch\",\n     # Add new patches just above this line\n ]"
        },
        {
            "sha": "c77d99e13a429902d21310ae829324ee2dafcf0a",
            "filename": "third_party/xla/third_party/triton/temporary/speed_up_int4_unpacking.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 88,
            "changes": 88,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ebc7b1b14a8096bfec775bad30c2a7b0bd240eff/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fspeed_up_int4_unpacking.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ebc7b1b14a8096bfec775bad30c2a7b0bd240eff/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fspeed_up_int4_unpacking.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fspeed_up_int4_unpacking.patch?ref=ebc7b1b14a8096bfec775bad30c2a7b0bd240eff",
            "patch": "@@ -1,88 +0,0 @@\n-b/376639863: Speed up int4 unpacking by using inline_asm with packed elements at\n-an early stage on the XLA side with further optimizations from nVidia #24 and\n-#25 on the Triton side.\n-\n-diff --git a/lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp b/lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp\n---- a/lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp\n-+++ b/lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp\n-@@ -220,6 +220,19 @@ struct JoinOpConversion : public ConvertOpToLLVMPattern<JoinOp> {\n-     assert(lhsVals.size() == rhsVals.size());\n-     SmallVector<Value> joinedVals;\n-     joinedVals.resize(lhsVals.size() * 2);\n-+\n-+    // Specifically for packed upcasting from 4b to 16b dtypes\n-+    // numContiguousValues cannot be too large, since the two outputs of\n-+    // inline_asm contain interleaved values OTOH, if numContiguousValues * 16b\n-+    // < 32b, then we'll need to rearrange 16b values in 32b registers. Hence we\n-+    // set numContiguousValues to 2\n-+    auto inlineOp =\n-+        dyn_cast<ElementwiseInlineAsmOp>(op.getLhs().getDefiningOp());\n-+    if (inlineOp && inlineOp.getPackedElement() == 4 &&\n-+        dstTy.getElementTypeBitWidth() == 16 && joinedVals.size() > 2) {\n-+      numContiguousValues = 2;\n-+    }\n-+\n-     for (int i = 0; i < lhsVals.size(); i += numContiguousValues) {\n-       for (int j = 0; j < numContiguousValues; j++) {\n-         joinedVals[2 * i + j] = lhsVals[i + j];\n-diff --git a/lib/Dialect/TritonGPU/Transforms/Utility.cpp b/lib/Dialect/TritonGPU/Transforms/Utility.cpp\n---- a/lib/Dialect/TritonGPU/Transforms/Utility.cpp\n-+++ b/lib/Dialect/TritonGPU/Transforms/Utility.cpp\n-@@ -1109,6 +1109,35 @@ swizzleDotOperandLike(RankedTensorType type, ttg::CTALayoutAttr ctaLayout) {\n-       type.getElementTypeBitWidth(), false);\n- }\n- \n-+// Rough utility for obtaining a SharedEnc for a LinearEncoding,\n-+// as we've replaced DotOpEnc with Linear in some cases\n-+// (specifically, fp4ToFp and similar unpack-upcast thru join)\n-+std::optional<ttg::SwizzledSharedEncodingAttr> getSharedForLinear(\n-+    ttg::LinearEncodingAttr enc, ArrayRef<unsigned int> globalOrder,\n-+    ArrayRef<int64_t> shape, unsigned elemBitWidth,\n-+    ttg::CTALayoutAttr ctaLayout) {\n-+  auto ctx = enc.getContext();\n-+  auto ll = enc.getLinearLayout();\n-+  auto rank = shape.size();\n-+  if (rank != 2) return std::nullopt;\n-+  auto order = enc.getOrder();\n-+  assert(globalOrder.size() == rank);\n-+  // TODO add memdesc_trans support for dot(trans(cvt(src) #linear) #dot_op)\n-+  if (order != globalOrder) return std::nullopt;\n-+  auto innerDim = order[0];\n-+  auto outerDim = order[1];\n-+  auto contigPerWarp = enc.getContigPerWarp();\n-+  constexpr unsigned BANK_SIZE{128};\n-+  auto elemBytes = elemBitWidth / 8;\n-+  auto vec = contigPerWarp[innerDim];\n-+  auto rowSize = elemBytes * (unsigned)shape[innerDim];\n-+  auto perPhase = std::max(BANK_SIZE / rowSize, 1u);\n-+  auto maxPhase = std::max(contigPerWarp[outerDim] / perPhase, 1u);\n-+  // cp.async does not support transfer size < 4B\n-+  if (vec * elemBytes < 4 && perPhase < maxPhase) return std::nullopt;\n-+  return ttg::SwizzledSharedEncodingAttr::get(ctx, vec, perPhase, maxPhase,\n-+                                              order, ctaLayout);\n-+}\n- // If all the transitive uses of the given value have are used by a convert to\n- // the same dot operand encoding, return the shared encoding that needs to be\n- // used to be compatible with users' layouts. If there are incompatible shared\n-@@ -1143,14 +1172,19 @@ getSharedEncIfAllUsersAreDotEnc(Value val, bool &incompatible) {\n-       auto CTALayout = isa<ttg::LinearEncodingAttr>(dstTy.getEncoding())\n-                            ? ttg::getCTALayout(srcTy.getEncoding())\n-                            : ttg::getCTALayout(dstTy.getEncoding());\n-+      auto order = getOrderForMemory(srcTy);\n-+      unsigned bitWidth = srcTy.getElementTypeBitWidth();\n- \n-       if (auto dot =\n-               dyn_cast<ttg::DotOperandEncodingAttr>(dstTy.getEncoding())) {\n--        auto order = getOrderForMemory(srcTy);\n--        unsigned bitWidth = srcTy.getElementTypeBitWidth();\n-         tempAttr = ttg::SwizzledSharedEncodingAttr::get(\n-             val.getContext(), dot, srcTy.getShape(), order, CTALayout, bitWidth,\n-             /*needTrans=*/false);\n-+      } else if (auto linearEnc = dyn_cast<ttg::LinearEncodingAttr>(dstTy.getEncoding())) {\n-+        auto attrOpt = getSharedForLinear(linearEnc, order, srcTy.getShape(),\n-+                                          bitWidth, CTALayout);\n-+        if (!attrOpt) return std::nullopt;\n-+        tempAttr = *attrOpt;\n-       } else {\n-         // Try to see if the layout is like an mma microtile\n-         tempAttr = swizzleDotOperandLike(dstTy, CTALayout);"
        },
        {
            "sha": "b9f8785cbc4d4888f6b9e3e4f0d0a7d2ddfb2f58",
            "filename": "third_party/xla/third_party/triton/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fthird_party%2Ftriton%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fworkspace.bzl?ref=4cd1b10213e05df5787d1e59a9f756a0c4dce50d",
            "patch": "@@ -8,8 +8,8 @@ load(\"//third_party/triton:xla_extensions/series.bzl\", \"extensions_files_patch_l\n def repo():\n     \"\"\"Imports Triton.\"\"\"\n \n-    TRITON_COMMIT = \"triton_integrate_branch-1.10\"\n-    TRITON_SHA256 = \"a7aef2fdc4355c8fb135c75bc81b91d06c909e7d6f903b36813a2dd486f4fcb9\"\n+    TRITON_COMMIT = \"triton_integrate_branch-1.11\"\n+    TRITON_SHA256 = \"1125fd9e344de2cb4041e4a9ec2cf02c307082833e421d87f91ffcf9983f9a90\"\n     tf_http_archive(\n         name = \"triton\",\n         sha256 = TRITON_SHA256,"
        },
        {
            "sha": "159d5f1292b6eaa7f916a271ba6955f355fe5780",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline_cuda.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc?ref=4cd1b10213e05df5787d1e59a9f756a0c4dce50d",
            "patch": "@@ -102,14 +102,15 @@ absl::Status CreateTritonPipeline(\n     pm->addPass(mlir::createCanonicalizerPass());\n     pm->addPass(mlir::createLoopInvariantCodeMotionPass());\n     pm->addPass(mt::gpu::createTritonGPUOptimizeAccumulatorInit());\n-    pm->addPass(mt::gpu::createTritonGPUHoistTMEMAlloc());\n+    pm->addPass(mt::gpu::createTritonGPUHoistTMEMAlloc({false}));\n     pm->addPass(ttng::createTritonNvidiaGPUPromoteLHSToTMemPass());\n     pm->addPass(mt::gpu::createTritonGPUAssignLatencies({num_stages}));\n     pm->addPass(mt::gpu::createTritonGPUScheduleLoops());\n     pm->addPass(\n         mt::gpu::createTritonGPUAutomaticWarpSpecialization({num_stages}));\n     pm->addPass(mt::gpu::createTritonGPUPipeline({num_stages}));\n     pm->addPass(mt::gpu::createTritonGPUCombineTensorSelectAndIf());\n+    pm->addPass(mt::gpu::createTritonGPUHoistTMEMAlloc({true}));\n     pm->addPass(ttng::createTritonNvidiaGPURemoveTMEMTokensPass());\n   } else {\n     pm->addPass(mlir::createLoopInvariantCodeMotionPass());\n@@ -144,7 +145,7 @@ absl::Status CreateTritonPipeline(\n   pm->addPass(mt::gpu::createTritonGPUCombineTensorSelectAndIf());\n   pm->addPass(mt::gpu::createTritonGPUAllocateWarpGroups());\n   pm->addPass(mlir::createSCFToControlFlowPass());\n-  pm->addPass(mt::gpu::createAllocateSharedMemory());\n+  pm->addPass(mt::createAllocateSharedMemoryNvPass(ccAsInt));\n   pm->addPass(ttng::createTritonTensorMemoryAllocationPass());\n   // We could add a flag to XLA to optionally enable the following pass:\n   // pm->addPass(mt::instrument::createTritonInstrumentConcurrencySanitizer());"
        },
        {
            "sha": "2d1bd69f4da243adf34b8b00634632824903da4d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline_rocm.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_rocm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_rocm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_rocm.cc?ref=4cd1b10213e05df5787d1e59a9f756a0c4dce50d",
            "patch": "@@ -95,8 +95,9 @@ absl::Status CreateTritonPipeline(\n   pm->addPass(mlir::createCanonicalizerPass());\n \n   if (cc.has_amd_matrix_core()) {\n-    pm->addPass(\n-        mlir::createTritonAMDGPUStreamPipeline({num_stages, 0, 0, false}));\n+    pm->addPass(mlir::createTritonAMDGPUStreamPipeline(\n+        {num_stages, /*global_prefetch=*/0, /*local_prefetch=*/0,\n+         /*use_async_copy=*/false, /*use_block_pingpong=*/false}));\n     // TODO(ROCm) Modify when corresponding run time flags are introduced.\n     if (/*use_async_copy=*/false) {  // Not enabled by default.\n       pm->addPass(mlir::createTritonAMDGPUCoalesceAsyncCopy());\n@@ -116,7 +117,7 @@ absl::Status CreateTritonPipeline(\n   if (cc.has_amd_matrix_core()) {\n     pm->addPass(mt::gpu::createTritonGPUReorderInstructions());\n   }\n-  if (/*(use_block_pingpong == \"none\") ==*/false) {\n+  if (/*use_block_pingpong=*/false) {\n     pm->addPass(mlir::createTritonAMDGPUBlockPingpong({num_stages}));\n   }\n   if (/*use_buffer_ops=*/false) {  // Not enabled by default."
        },
        {
            "sha": "7ccc404c0a8b9d2a5001f07392ac0149d5bb2ce1",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_util.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4cd1b10213e05df5787d1e59a9f756a0c4dce50d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.h?ref=4cd1b10213e05df5787d1e59a9f756a0c4dce50d",
            "patch": "@@ -112,7 +112,7 @@ class AutotuneCacheKey {\n   // Tie a version to the cache key in order to invalidate the cache when\n   // necessary. This should be incremented on triton upgrades or any other\n   // changes that may affect the autotuning results.\n-  static constexpr int kCurrentVersion = 10;\n+  static constexpr int kCurrentVersion = 12;\n \n   AutotuneCacheKey(const se::DeviceDescription& device_description,\n                    const HloInstruction& instruction,"
        }
    ],
    "stats": {
        "total": 583,
        "additions": 192,
        "deletions": 391
    }
}