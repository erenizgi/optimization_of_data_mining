{
    "author": "Moerafaat",
    "message": "[NFC] Moving extraction utility out of fusion_emitter to emitter_helpers. Also added a test for coverage as I realize this function wasn't tested.\n\nMore utilities will follow as part of an upcoming change, so this refactor makes sense to land first.\n\nPiperOrigin-RevId: 819716328",
    "sha": "aa3cb5c5d814e3bb6c2469ce6f2d413f36c6adca",
    "files": [
        {
            "sha": "85690d8ab274f8303836acd5f9b2124911be61f2",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa3cb5c5d814e3bb6c2469ce6f2d413f36c6adca/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa3cb5c5d814e3bb6c2469ce6f2d413f36c6adca/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=aa3cb5c5d814e3bb6c2469ce6f2d413f36c6adca",
            "patch": "@@ -102,10 +102,12 @@ cc_library(\n         \"emitter_helpers.h\",\n     ],\n     deps = [\n+        \":tma_utils\",\n         \"//xla:literal\",\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n+        \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n         \"//xla/codegen:emitter_loc_op_builder\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/mlir_hlo\",\n@@ -114,6 +116,7 @@ cc_library(\n         \"//xla/service/gpu:target_util\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/platform:status\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n@@ -124,13 +127,31 @@ cc_library(\n         \"@llvm-project//llvm:TargetParser\",\n         \"@llvm-project//mlir:ArithDialect\",\n         \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:LLVMDialect\",\n         \"@llvm-project//mlir:MathDialect\",\n         \"@llvm-project//mlir:Support\",\n         \"@local_tsl//tsl/platform:statusor\",\n         \"@triton//:TritonDialects\",\n     ],\n )\n \n+xla_cc_test(\n+    name = \"emitter_helpers_test\",\n+    srcs = [\"emitter_helpers_test.cc\"],\n+    deps = [\n+        \":emitter_helpers\",\n+        \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n+        \"//xla/stream_executor/gpu:tma_metadata\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:LLVMDialect\",\n+        \"@llvm-project//mlir:Parser\",\n+        \"@triton//:TritonDialects\",\n+    ],\n+)\n+\n cc_library(\n     name = \"compilation_pipeline\",\n     srcs = ["
        },
        {
            "sha": "403a94741b2c3735747af8c188f308dd8244115f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa3cb5c5d814e3bb6c2469ce6f2d413f36c6adca/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa3cb5c5d814e3bb6c2469ce6f2d413f36c6adca/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=aa3cb5c5d814e3bb6c2469ce6f2d413f36c6adca",
            "patch": "@@ -24,19 +24,24 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/Support/MathExtras.h\"\n #include \"llvm/TargetParser/Triple.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Dialect/Math/IR/Math.h\"\n #include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"mlir/IR/Types.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/IR/ValueRange.h\"\n #include \"mlir/Support/LLVM.h\"\n+#include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n+#include \"xla/backends/gpu/codegen/triton/tma_utils.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n@@ -47,6 +52,7 @@ limitations under the License.\n #include \"xla/service/gpu/target_util.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/statusor.h\"\n@@ -520,4 +526,34 @@ Value Bitcast(EmitterLocOpBuilder& b, Value value, Type type) {\n   return b.create<mlir::arith::BitcastOp>(value_type, value);\n }\n \n+absl::StatusOr<stream_executor::gpu::TmaMetadata> ExtractTmaMetadata(\n+    mlir::ModuleOp triton_module, absl::string_view kernel_name) {\n+  stream_executor::gpu::TmaMetadata tma_metadata;\n+  SmallVector<mlir::LLVM::LLVMFuncOp> func_ops;\n+  for (auto func : triton_module.getOps<mlir::LLVM::LLVMFuncOp>()) {\n+    // Custom calls will also match to LLVMFuncOp, so we are only interested in\n+    // the entry function.\n+    if (func.getName().str() == kernel_name) {\n+      func_ops.push_back(func);\n+    }\n+  }\n+  CHECK_EQ(func_ops.size(), 1)\n+      << \"Expected a single LLVMFuncOp in the module for the entry function.\";\n+\n+  for (auto [idx, arg] : llvm::enumerate(func_ops[0].getArguments())) {\n+    if (auto attr =\n+            func_ops[0].getArgAttrOfType<mlir::triton::xla::TmaDescriptorAttr>(\n+                idx, \"tt.tma_descriptor\")) {\n+      TF_ASSIGN_OR_RETURN(\n+          auto tma_desc,\n+          CreateTmaDescriptor(attr.getGlobalShape(), attr.getTileShape(),\n+                              attr.getTileStrides(), attr.getLayout(),\n+                              attr.getElementByteSize(),\n+                              attr.getSwizzleMode().getValue()));\n+      tma_metadata.arg_index_to_tma_info.insert({idx, tma_desc});\n+    }\n+  }\n+  return tma_metadata;\n+}\n+\n }  // namespace xla::gpu::triton"
        },
        {
            "sha": "6f5fa687efe5769dc1d238e7a9b898bd52b33938",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.h",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa3cb5c5d814e3bb6c2469ce6f2d413f36c6adca/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa3cb5c5d814e3bb6c2469ce6f2d413f36c6adca/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h?ref=aa3cb5c5d814e3bb6c2469ce6f2d413f36c6adca",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/Support/raw_ostream.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n@@ -40,6 +41,7 @@ limitations under the License.\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/tsl/platform/status.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -213,6 +215,11 @@ absl::StatusOr<mlir::Value> EmitElementwise(\n \n mlir::Value Bitcast(EmitterLocOpBuilder& b, mlir::Value value, mlir::Type type);\n \n+// Extracts TMA metadata information from LLVM generated by the Triton\n+// compilation. This will be empty if TMA is not used.\n+absl::StatusOr<stream_executor::gpu::TmaMetadata> ExtractTmaMetadata(\n+    mlir::ModuleOp triton_module, absl::string_view kernel_name);\n+\n }  // namespace xla::gpu::triton\n \n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_EMITTER_HELPERS_H_"
        },
        {
            "sha": "697b71ff5f65123380b3bfbf48e50f21ce692710",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers_test.cc",
            "status": "added",
            "additions": 84,
            "deletions": 0,
            "changes": 84,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa3cb5c5d814e3bb6c2469ce6f2d413f36c6adca/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa3cb5c5d814e3bb6c2469ce6f2d413f36c6adca/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers_test.cc?ref=aa3cb5c5d814e3bb6c2469ce6f2d413f36c6adca",
            "patch": "@@ -0,0 +1,84 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n+\n+#include <string>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/log/check.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/OwningOpRef.h\"\n+#include \"mlir/Parser/Parser.h\"\n+#include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n+#include \"xla/stream_executor/gpu/tma_metadata.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+\n+using ::testing::ElementsAre;\n+\n+namespace xgt = ::xla::gpu::triton;\n+namespace xla::gpu {\n+namespace {\n+\n+TEST(EmitterHelpersTest, ExtractTmaMetadataWorksCorrectlyWhenTmaIsUsed) {\n+  const std::string kMlirModule = R\"(\n+module {\n+  llvm.func @fusion_impl(%arg0: !llvm.ptr<1> {tt.divisibility = 16 : i32},\n+                        %arg1: !llvm.ptr {llvm.align = 64 : i32, llvm.byval = !llvm.array<128 x i8>, nvvm.grid_constant, tt.nv_tma_desc = 1 : i32, tt.tma_descriptor = #triton_xla.tma_descriptor<global_shape = [123, 512], tile_shape = [32, 64], tile_strides = [1, 1], layout = [1, 0], element_byte_size = 4, swizzle_mode = \"128b\">},\n+                        %arg2: !llvm.ptr {llvm.align = 64 : i32, llvm.byval = !llvm.array<128 x i8>, nvvm.grid_constant, tt.nv_tma_desc = 1 : i32, tt.tma_descriptor = #triton_xla.tma_descriptor<global_shape = [32, 512], tile_shape = [16, 64], tile_strides = [1, 1], layout = [1, 0], element_byte_size = 4, swizzle_mode = \"128b\">},\n+                        %arg3: !llvm.ptr<1>, %arg4: !llvm.ptr<1>)\n+                        attributes {nvvm.kernel = 1 : ui1, nvvm.reqntid = array<i32: 32>, ttg.global_scratch_memory_alignment = 1 : i32, ttg.global_scratch_memory_size = 0 : i32} {\n+    llvm.return\n+  }\n+})\";\n+\n+  mlir::MLIRContext context;\n+  context.loadDialect<\n+      mlir::triton::TritonDialect, mlir::triton::gpu::TritonGPUDialect,\n+      mlir::triton::xla::XlaTritonDialect, mlir::LLVM::LLVMDialect>();\n+  mlir::OwningOpRef<mlir::ModuleOp> module =\n+      mlir::parseSourceString<mlir::ModuleOp>(kMlirModule, &context);\n+  CHECK(module);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(stream_executor::gpu::TmaMetadata tma_metadata,\n+                          xgt::ExtractTmaMetadata(module.get(), \"fusion_impl\"));\n+\n+  EXPECT_EQ(tma_metadata.arg_index_to_tma_info.size(), 2);\n+  EXPECT_TRUE(tma_metadata.arg_index_to_tma_info.contains(1));\n+  EXPECT_TRUE(tma_metadata.arg_index_to_tma_info.contains(2));\n+\n+  auto tma_arg_1 = tma_metadata.arg_index_to_tma_info.at(1);\n+  auto tma_arg_2 = tma_metadata.arg_index_to_tma_info.at(2);\n+\n+  EXPECT_THAT(tma_arg_1.global_dims(), ElementsAre(512, 123));\n+  EXPECT_THAT(tma_arg_1.box_dims(), ElementsAre(32, 32));\n+  EXPECT_THAT(tma_arg_1.element_strides(), ElementsAre(1, 1));\n+  EXPECT_THAT(tma_arg_1.element_size(), 4);\n+  EXPECT_EQ(tma_arg_1.swizzle(),\n+            stream_executor::gpu::TmaDescriptor::TmaSwizzle::k128B);\n+\n+  EXPECT_THAT(tma_arg_2.global_dims(), ElementsAre(512, 32));\n+  EXPECT_THAT(tma_arg_2.box_dims(), ElementsAre(32, 16));\n+  EXPECT_THAT(tma_arg_2.element_strides(), ElementsAre(1, 1));\n+  EXPECT_THAT(tma_arg_2.element_size(), 4);\n+  EXPECT_EQ(tma_arg_2.swizzle(),\n+            stream_executor::gpu::TmaDescriptor::TmaSwizzle::k128B);\n+}\n+}  // namespace\n+}  // namespace xla::gpu"
        },
        {
            "sha": "33d4a1ad27f6ddbd1ffe6602176692a992112a2b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 30,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa3cb5c5d814e3bb6c2469ce6f2d413f36c6adca/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa3cb5c5d814e3bb6c2469ce6f2d413f36c6adca/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=aa3cb5c5d814e3bb6c2469ce6f2d413f36c6adca",
            "patch": "@@ -157,6 +157,7 @@ namespace arith = ::mlir::arith;\n namespace ttir = ::mlir::triton;\n namespace mtx = ::mlir::triton::xla;\n namespace stablehlo = ::mlir::stablehlo;\n+namespace xgt = ::xla::gpu::triton;\n \n using ::llvm::SmallVector;\n using ::mlir::AffineMap;\n@@ -1921,35 +1922,6 @@ void EmitReturnOp(EmitterLocOpBuilder b, absl::string_view fusion_kind) {\n   }\n }\n \n-absl::StatusOr<stream_executor::gpu::TmaMetadata> ExtractTmaMetadata(\n-    mlir::ModuleOp triton_module, absl::string_view kernel_name) {\n-  stream_executor::gpu::TmaMetadata tma_metadata;\n-  SmallVector<mlir::LLVM::LLVMFuncOp> func_ops;\n-  for (auto func : triton_module.getOps<mlir::LLVM::LLVMFuncOp>()) {\n-    // Custom calls will also match to LLVMFuncOp, so we are only interested in\n-    // the entry function.\n-    if (func.getName().str() == kernel_name) {\n-      func_ops.push_back(func);\n-    }\n-  }\n-  CHECK_EQ(func_ops.size(), 1)\n-      << \"Expected a single LLVMFuncOp in the module for the entry function.\";\n-\n-  for (auto [idx, arg] : llvm::enumerate(func_ops[0].getArguments())) {\n-    if (auto attr = func_ops[0].getArgAttrOfType<mtx::TmaDescriptorAttr>(\n-            idx, \"tt.tma_descriptor\")) {\n-      TF_ASSIGN_OR_RETURN(\n-          auto tma_desc,\n-          CreateTmaDescriptor(attr.getGlobalShape(), attr.getTileShape(),\n-                              attr.getTileStrides(), attr.getLayout(),\n-                              attr.getElementByteSize(),\n-                              attr.getSwizzleMode().getValue()));\n-      tma_metadata.arg_index_to_tma_info.insert({idx, tma_desc});\n-    }\n-  }\n-  return tma_metadata;\n-}\n-\n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n     absl::string_view fn_name, const HloFusionInstruction* fusion,\n     const se::DeviceDescription& device_info,\n@@ -2213,7 +2185,7 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n   // It's okay for tma_metadata to be empty; it's only populated when used\n   // explicitly.\n   TF_ASSIGN_OR_RETURN(stream_executor::gpu::TmaMetadata tma_metadata,\n-                      ExtractTmaMetadata(triton_module, kernel_name));\n+                      xgt::ExtractTmaMetadata(triton_module, kernel_name));\n \n   return {\n       {shared_mem_bytes, cluster_dim, tma_metadata, captured_nvvm_annotations}};"
        }
    ],
    "stats": {
        "total": 180,
        "additions": 150,
        "deletions": 30
    }
}