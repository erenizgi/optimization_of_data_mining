{
    "author": "mkuperst",
    "message": "[XLA] Move parameter sharding conflict resolution earlier.\n\nFix up the sharding on called computation parameters for kCall/kWhile/kConditional during preprocessing, instead of doing it on the fly while sharding the caller instruction. This should have no effect on its own, and is just preparation for supporting non-flat graphs.\n\nReverts 2816b067b5471ba5bf6de7154b9ff11e1865db02\n\nPiperOrigin-RevId: 797933862",
    "sha": "7133b5fe2b98108cb1feb52e9ad8b79450a25286",
    "files": [
        {
            "sha": "e6ea3a5b99073433d4bf41a973e182ad954330bb",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 48,
            "deletions": 31,
            "changes": 79,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7133b5fe2b98108cb1feb52e9ad8b79450a25286/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7133b5fe2b98108cb1feb52e9ad8b79450a25286/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=7133b5fe2b98108cb1feb52e9ad8b79450a25286",
            "patch": "@@ -2545,24 +2545,20 @@ std::vector<ReplicaGroup> SpmdPartitioningVisitor::CreateReplicaGroups(\n \n absl::Status SpmdPartitioningVisitor::HandleCall(HloInstruction* hlo) {\n   std::vector<HloInstruction*> call_args;\n-  HloComputation* computation = hlo->called_computations()[0];\n+  call_args.reserve(hlo->operand_count());\n   for (int64_t i = 0; i < hlo->operand_count(); ++i) {\n-    // Shardings of the computation parameter and its argument must be\n-    // the same.\n-    computation->parameter_instruction(i)->set_sharding(\n-        hlo->operand(i)->sharding());\n     call_args.push_back(GetPartitionedHlo(hlo->operand(i)).hlo());\n   }\n \n-  TF_RETURN_IF_ERROR(partitioner_\n-                         ->PartitionComputation(computation, hlo->sharding(),\n-                                                next_channel_id_, logger_,\n-                                                call_graph_)\n-                         .status());\n+  TF_RETURN_IF_ERROR(\n+      partitioner_\n+          ->PartitionComputation(hlo->to_apply(), hlo->sharding(),\n+                                 next_channel_id_, logger_, call_graph_)\n+          .status());\n   SetPartitionedHlo(hlo, [&] {\n     auto* call = b_.AddInstruction(HloInstruction::CreateCall(\n         MakePartitionedShape(hlo->shape(), hlo->sharding()), call_args,\n-        hlo->called_computations()[0]));\n+        hlo->to_apply()));\n     call->set_raw_backend_config_string(hlo->raw_backend_config_string());\n     return call;\n   });\n@@ -4280,18 +4276,8 @@ absl::Status SpmdPartitioningVisitor::HandleReverse(HloInstruction* hlo) {\n \n absl::Status SpmdPartitioningVisitor::HandleWhile(HloInstruction* hlo) {\n   const HloSharding& sharding = hlo->sharding();\n-\n-  // Shardings for the body parameter, body root, and cond parameter must be\n-  // the same.\n-  hlo->while_condition()->parameter_instruction(0)->set_sharding(sharding);\n-  hlo->while_body()->parameter_instruction(0)->set_sharding(sharding);\n-\n-  // The condition root must be replicated so that all partitions follow the\n-  // same control flow.\n   HloInstruction* cond_root = hlo->while_condition()->root_instruction();\n-  const HloSharding cond_root_sharding =\n-      hlo_sharding_util::ReplicateAllDataDims(cond_root->sharding());\n-  cond_root->set_sharding(cond_root_sharding);\n+  const HloSharding cond_root_sharding = cond_root->sharding();\n   TF_RETURN_IF_ERROR(\n       partitioner_\n           ->PartitionComputation(hlo->while_condition(), cond_root_sharding,\n@@ -4315,12 +4301,6 @@ absl::Status SpmdPartitioningVisitor::HandleWhile(HloInstruction* hlo) {\n absl::Status SpmdPartitioningVisitor::HandleConditional(HloInstruction* hlo) {\n   std::vector<HloInstruction*> branch_args;\n   for (int64_t i = 0; i < hlo->branch_count(); ++i) {\n-    HloComputation* computation = hlo->branch_computation(i);\n-\n-    // Shardings of the branch computation parameter and its argument must be\n-    // the same.\n-    computation->parameter_instruction(0)->set_sharding(\n-        hlo->operand(i + 1)->sharding());\n     branch_args.push_back(GetPartitionedHlo(hlo->operand(i + 1)).hlo());\n   }\n \n@@ -4927,7 +4907,7 @@ absl::StatusOr<bool> SpmdPartitioningVisitor::DoPartition(\n     const SpmdPartitionerOptions& options) {\n   VLOG(2) << \"Partitioning computation \" << computation->name() << \" for \"\n           << num_replicas_ << \" replicas and \" << num_partitions_\n-          << \" partitions\";\n+          << \" partitions\" << \" with root sharding \" << root_sharding;\n   TF_RETURN_IF_ERROR(computation->Accept(this));\n \n   HloModule* module = computation->parent();\n@@ -5615,8 +5595,9 @@ absl::StatusOr<bool> SpmdPartitioner::Run(\n absl::Status SpmdPartitioner::PreprocessSharding(\n     HloModule* module,\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n-  for (HloComputation* computation : module->computations(execution_threads)) {\n-    for (HloInstruction* hlo : computation->instructions()) {\n+  for (HloComputation* computation :\n+       module->MakeComputationPostOrder(execution_threads)) {\n+    for (HloInstruction* hlo : computation->MakeInstructionPostOrder()) {\n       if (hlo->HasSideEffectNoRecurse() && hlo->opcode() != HloOpcode::kRng &&\n           (hlo->opcode() != HloOpcode::kCustomCall ||\n            GetCustomCallPartitioner(hlo->custom_call_target()) == nullptr)) {\n@@ -5654,6 +5635,42 @@ absl::Status SpmdPartitioner::PreprocessSharding(\n               HloSharding::Single(hlo->shape(), HloSharding::Replicate()));\n         }\n       }\n+\n+      // For control-flow constructs, we must make sure that the inputs and\n+      // outputs of the called computation have the same sharding as the\n+      // arguments being passed in.\n+      switch (hlo->opcode()) {\n+        case HloOpcode::kWhile: {\n+          hlo->while_condition()->parameter_instruction(0)->set_sharding(\n+              hlo->sharding());\n+          hlo->while_body()->parameter_instruction(0)->set_sharding(\n+              hlo->sharding());\n+          // The condition root must be replicated so that all partitions follow\n+          // the same control flow.\n+          HloInstruction* cond_root =\n+              hlo->while_condition()->root_instruction();\n+          const HloSharding cond_root_sharding =\n+              hlo_sharding_util::ReplicateAllDataDims(cond_root->sharding());\n+          cond_root->set_sharding(cond_root_sharding);\n+          break;\n+        }\n+        case HloOpcode::kConditional: {\n+          for (int64_t i = 0; i < hlo->branch_count(); ++i) {\n+            hlo->branch_computation(i)->parameter_instruction(0)->set_sharding(\n+                hlo->operand(i + 1)->sharding());\n+          }\n+          break;\n+        }\n+        case HloOpcode::kCall: {\n+          for (int64_t i = 0; i < hlo->operand_count(); ++i) {\n+            hlo->to_apply()->parameter_instruction(i)->set_sharding(\n+                hlo->operand(i)->sharding());\n+          }\n+          break;\n+        }\n+        default:\n+          break;\n+      }\n     }\n   }\n "
        },
        {
            "sha": "7ca2ee7c6c184d6872a1d2b0b2770e3bf4d787fe",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_test.cc",
            "status": "modified",
            "additions": 76,
            "deletions": 0,
            "changes": 76,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7133b5fe2b98108cb1feb52e9ad8b79450a25286/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7133b5fe2b98108cb1feb52e9ad8b79450a25286/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc?ref=7133b5fe2b98108cb1feb52e9ad8b79450a25286",
            "patch": "@@ -16497,6 +16497,82 @@ ENTRY entry {\n               ::testing::ElementsAre(8, 10, 12, 14, 9, 11, 13, 15))));\n }\n \n+TEST_P(SpmdPartitioningTest, ShardingPreprocessOrderWhile) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+%body (body_param: (s32[], token[], token[])) -> (s32[], token[], token[]) {\n+  ROOT %body_param = (s32[], token[], token[]) parameter(0)\n+}\n+\n+%cond (cond_param: (s32[], token[], token[])) -> pred[] {\n+  %cond_param = (s32[], token[], token[]) parameter(0)\n+  %get-tuple-element = s32[] get-tuple-element(%cond_param), index=0\n+  %val = s32[] constant(4)\n+  ROOT %less = pred[] compare(%get-tuple-element, %val), direction=LT\n+}\n+\n+ENTRY entry {\n+  %param = s32[] parameter(0), sharding={replicated}\n+  %after-all.0 = token[] after-all()\n+  %after-all.1 = token[] after-all()\n+  %tuple = (s32[], token[], token[]) tuple(%param, %after-all.0, %after-all.1)\n+  ROOT %while = (s32[], token[], token[]) while(%tuple), condition=%cond, body=%body\n+}\n+)\";\n+\n+  HloModuleConfig config = GetModuleConfigForTest();\n+  config.set_use_spmd_partitioning(true);\n+  config.set_num_partitions(2);\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(hlo_string, config));\n+\n+  HloInstruction* while_op = module->entry_computation()->root_instruction();\n+  HloComputation* new_cond =\n+      module->AddEmbeddedComputation(while_op->while_condition()->Clone());\n+  while_op->set_while_condition(new_cond);\n+  SpmdPartitioner partitioner(/*num_partitions=*/2, /*num_replicas=*/1,\n+                              /*options=*/{});\n+  TF_EXPECT_OK(partitioner.Run(module.get()).status());\n+}\n+\n+TEST_P(SpmdPartitioningTest, ShardingPreprocessOrderConditional) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+%branch.then (param: ()) -> s32[] {\n+  %param = () parameter(0)\n+  ROOT %const.0 = s32[] constant(0)\n+}\n+\n+%branch.else (param: ()) -> s32[] {\n+  %param = () parameter(0)\n+  ROOT %const.1 = s32[] constant(1)\n+}\n+\n+ENTRY entry {\n+  %param = pred[] parameter(0), sharding={replicated}\n+  %tuple.0 = () tuple()\n+  %tuple.1 = () tuple()\n+  ROOT %conditional = s32[] conditional(%param, %tuple.0, %tuple.1), true_computation=%branch.then, false_computation=%branch.else\n+}\n+)\";\n+\n+  HloModuleConfig config = GetModuleConfigForTest();\n+  config.set_use_spmd_partitioning(true);\n+  config.set_num_partitions(2);\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(hlo_string, config));\n+  HloInstruction* conditional_op =\n+      module->entry_computation()->root_instruction();\n+  HloInstruction* new_true_param = module->entry_computation()->AddInstruction(\n+      conditional_op->mutable_operand(1)->Clone());\n+  TF_ASSERT_OK(conditional_op->ReplaceOperandWith(1, new_true_param));\n+  SpmdPartitioner partitioner(/*num_partitions=*/2, /*num_replicas=*/1,\n+                              /*options=*/{});\n+  TF_EXPECT_OK(partitioner.Run(module.get()).status());\n+}\n+\n }  // namespace\n }  // namespace spmd\n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 155,
        "additions": 124,
        "deletions": 31
    }
}