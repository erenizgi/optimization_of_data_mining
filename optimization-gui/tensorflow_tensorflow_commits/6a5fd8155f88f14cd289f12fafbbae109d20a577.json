{
    "author": "basioli-k",
    "message": "[XLA][codegen] Move emitter helpers and dot algorithms to xtile namespace\n\nNo longer triton specific, shared between GPU and CPU.\n\nPiperOrigin-RevId: 837820736",
    "sha": "6a5fd8155f88f14cd289f12fafbbae109d20a577",
    "files": [
        {
            "sha": "9010c73df4dcbe8785dddd7dcd0a5450acd0f29b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc?ref=6a5fd8155f88f14cd289f12fafbbae109d20a577",
            "patch": "@@ -68,9 +68,9 @@ namespace {\n \n using ::mlir::ShapedType;\n using ::mlir::Value;\n-using ::xla::gpu::triton::TensorValue;\n-using ::xla::gpu::triton::TileInfo;\n using ::xla::se::gpu::AllReduceStrategy;\n+using ::xla::xtile::TensorValue;\n+using ::xla::xtile::TileInfo;\n \n namespace ttir = ::mlir::triton;\n namespace mtx = ::mlir::triton::xla;\n@@ -262,7 +262,7 @@ absl::StatusOr<TensorValue> EmitAllReduce(\n   mlir::Value accumulator_zero =\n       arith::ConstantOp::create(b, elem_type, b.getZeroAttr(elem_type));\n   TensorValue accumulator =\n-      triton::Splat(b, accumulator_zero, input_tile.getType().getShape());\n+      xtile::Splat(b, accumulator_zero, input_tile.getType().getShape());\n   for (int rank = 0; rank < world_size; ++rank) {\n     Value rank_idx =\n         arith::ConstantOp::create(b, b.getI64Type(), b.getI64IntegerAttr(rank));\n@@ -285,9 +285,9 @@ absl::StatusOr<TensorValue> EmitAllReduce(\n         accumulator;\n     region_values[reduction_computation->parameter_instruction(1)] = next_tile;\n     TF_ASSIGN_OR_RETURN(accumulator,\n-                        triton::EmitScope(b,\n-                                          /*instructions=*/to_emit,\n-                                          /*values=*/region_values));\n+                        xtile::EmitScope(b,\n+                                         /*instructions=*/to_emit,\n+                                         /*values=*/region_values));\n   }\n   return accumulator;\n }\n@@ -390,7 +390,7 @@ absl::StatusOr<int32_t> AddCollectiveMetadataArguments(\n     } else if (type == S4) {\n       ir_type = b.getI4Type();\n     } else {\n-      TF_ASSIGN_OR_RETURN(ir_type, triton::TritonType(b, type));\n+      TF_ASSIGN_OR_RETURN(ir_type, xtile::PrimitiveTypeToMlirType(b, type));\n     }\n     // Also add the remote/scratch buffers for collectives.\n     // !tt.ptr<!tt.ptr<type>>"
        },
        {
            "sha": "9d955beb8a4b21080c0baec6f2632aaf80f0c915",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.h?ref=6a5fd8155f88f14cd289f12fafbbae109d20a577",
            "patch": "@@ -74,12 +74,12 @@ absl::StatusOr<std::vector<Shape>> GetCollectiveUnmanagedKernelArguments(\n // Emits tiled XTile/Triton IR for a collective op.\n // See [EmitTiledHloInstruction] for an overview of how this fits into the\n // emitter.\n-absl::StatusOr<triton::TensorValue> EmitCollective(\n+absl::StatusOr<xtile::TensorValue> EmitCollective(\n     mlir::ImplicitLocOpBuilder& b, const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_hlo_reduce,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, mlir::Value pid,\n-    absl::flat_hash_map<const TiledHloInstruction*, triton::TensorValue>&\n+    absl::flat_hash_map<const TiledHloInstruction*, xtile::TensorValue>&\n         values);\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "6e819ccbdb2dcffcffbc74fd646396b8030127fc",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc?ref=6a5fd8155f88f14cd289f12fafbbae109d20a577",
            "patch": "@@ -50,8 +50,7 @@ limitations under the License.\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n \n namespace xla {\n-namespace gpu {\n-namespace triton {\n+namespace xtile {\n \n namespace {\n \n@@ -174,12 +173,14 @@ Value EmitStableHloDotAndAdd(mlir::ImplicitLocOpBuilder& b, Value lhs,\n \n absl::StatusOr<Type> GetAlgUnsetAccumulatorType(mlir::ImplicitLocOpBuilder& b,\n                                                 const HloDotInstruction& dot) {\n-  TF_ASSIGN_OR_RETURN(Type lhs_type,\n-                      TritonType(b, dot.operand(0)->shape().element_type()));\n-  TF_ASSIGN_OR_RETURN(Type rhs_type,\n-                      TritonType(b, dot.operand(1)->shape().element_type()));\n+  TF_ASSIGN_OR_RETURN(\n+      Type lhs_type,\n+      PrimitiveTypeToMlirType(b, dot.operand(0)->shape().element_type()));\n+  TF_ASSIGN_OR_RETURN(\n+      Type rhs_type,\n+      PrimitiveTypeToMlirType(b, dot.operand(1)->shape().element_type()));\n   TF_ASSIGN_OR_RETURN(Type accumulator_type,\n-                      TritonType(b, dot.shape().element_type()));\n+                      PrimitiveTypeToMlirType(b, dot.shape().element_type()));\n \n   // The code below assumes that lhs and rhs have the same type. However\n   // this may not always be the case with f8 matmuls, e.g. e4m3×e5m2 is\n@@ -219,7 +220,7 @@ absl::StatusOr<std::optional<Type>> GetForceOperandsType(\n   std::vector<Type> allowed_operands_types;\n   allowed_operands_types.reserve(allowed_operands_primitive_types.size());\n   for (PrimitiveType primitive_type : allowed_operands_primitive_types) {\n-    TF_ASSIGN_OR_RETURN(Type type, TritonType(b, primitive_type));\n+    TF_ASSIGN_OR_RETURN(Type type, PrimitiveTypeToMlirType(b, primitive_type));\n     allowed_operands_types.push_back(type);\n   }\n \n@@ -261,7 +262,7 @@ absl::StatusOr<Type> GetDotAccumulatorType(mlir::ImplicitLocOpBuilder& b,\n \n   TF_ASSIGN_OR_RETURN(PrimitiveType accumulator_type,\n                       algorithm_util::GetDotAccumulatorType(algorithm));\n-  return TritonType(b, accumulator_type);\n+  return PrimitiveTypeToMlirType(b, accumulator_type);\n }\n \n absl::StatusOr<Value> EmitSingleTileDot(mlir::ImplicitLocOpBuilder& b,\n@@ -316,6 +317,5 @@ absl::StatusOr<Value> EmitSingleTileScaledDot(\n   return ScaledDot(b, dot_operands);\n }\n \n-}  // namespace triton\n-}  // namespace gpu\n+}  // namespace xtile\n }  // namespace xla"
        },
        {
            "sha": "bf0d228ef87f9ab14a3dacf181663cea7f170b92",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms.h",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.h?ref=6a5fd8155f88f14cd289f12fafbbae109d20a577",
            "patch": "@@ -25,8 +25,7 @@ limitations under the License.\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n \n namespace xla {\n-namespace gpu {\n-namespace triton {\n+namespace xtile {\n \n // Precision-relevant configuration bits for `dot`s.\n struct PrecisionSpec {\n@@ -80,8 +79,7 @@ absl::StatusOr<mlir::triton::ScaleDotElemType> GetScaleDotElemType(\n \n }  // namespace internal\n \n-}  // namespace triton\n-}  // namespace gpu\n+}  // namespace xtile\n }  // namespace xla\n \n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_DOT_ALGORITHMS_H_"
        },
        {
            "sha": "c81bfdb35696c506037ff81534d88ac24723163f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=6a5fd8155f88f14cd289f12fafbbae109d20a577",
            "patch": "@@ -66,7 +66,7 @@ limitations under the License.\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n \n-namespace xla::gpu::triton {\n+namespace xla::xtile {\n \n using ::llvm::SmallVector;\n using ::mlir::ArrayRef;\n@@ -116,7 +116,7 @@ absl::StatusOr<SmallVector<Value>> ComputeOffsetsForTile(\n   for (const auto& [rt_var, value] : llvm::zip(rt_vars, runtime_values)) {\n     Value clamped_index =\n         EmitClampedIndex(b, value, rt_var.bounds.lower, rt_var.bounds.upper);\n-    dims.push_back(triton::Cast(b, clamped_index, pid.getType()));\n+    dims.push_back(Cast(b, clamped_index, pid.getType()));\n   }\n   return emitters::ApplyIndexing(dim_only_tiling, /*dims=*/dims,\n                                  /*symbols=*/{}, b);\n@@ -167,8 +167,9 @@ SmallVector<int64_t> GetPaddedTileSizes(ArrayRef<int64_t> tile_sizes) {\n   return result;\n }\n \n-absl::StatusOr<Type> TritonType(mlir::ImplicitLocOpBuilder& b,\n-                                PrimitiveType t) {\n+absl::StatusOr<Type> PrimitiveTypeToMlirType(mlir::ImplicitLocOpBuilder& b,\n+\n+                                             PrimitiveType t) {\n   switch (t) {\n     case F64:\n       return b.getF64Type();\n@@ -429,8 +430,8 @@ absl::StatusOr<Value> EmitElementwise(mlir::ImplicitLocOpBuilder& b,\n       // NegFOp is not supported by Triton.\n       return Subtract(b, {ZerosLike(b, inputs[0]), inputs[0]});\n     case HloOpcode::kConvert: {\n-      TF_ASSIGN_OR_RETURN(Type dst_ty,\n-                          TritonType(b, hlo.shape().element_type()));\n+      TF_ASSIGN_OR_RETURN(\n+          Type dst_ty, PrimitiveTypeToMlirType(b, hlo.shape().element_type()));\n       return Cast(b, inputs[0], dst_ty);\n     }\n     case HloOpcode::kAdd:\n@@ -548,7 +549,8 @@ absl::StatusOr<Value> EmitElementwise(mlir::ImplicitLocOpBuilder& b,\n \n absl::StatusOr<mlir::TypedValue<mlir::RankedTensorType>> EmitConstant(\n     mlir::ImplicitLocOpBuilder& b, const HloInstruction& constant) {\n-  TF_ASSIGN_OR_RETURN(Type ty, TritonType(b, constant.shape().element_type()));\n+  TF_ASSIGN_OR_RETURN(\n+      Type ty, PrimitiveTypeToMlirType(b, constant.shape().element_type()));\n   llvm::SmallVector<int64_t> shape{constant.shape().dimensions().begin(),\n                                    constant.shape().dimensions().end()};\n \n@@ -583,7 +585,7 @@ Value Bitcast(mlir::ImplicitLocOpBuilder& b, Value value, Type type) {\n \n   const Shape& shape = tiled_hlo.hlo()->shape();\n   TF_ASSIGN_OR_RETURN(Type expected_element_type,\n-                      TritonType(b, shape.element_type()));\n+                      PrimitiveTypeToMlirType(b, shape.element_type()));\n   auto storage_type = StorageType(expected_element_type);\n \n   auto tile_strides = tiled_hlo.tile_strides();\n@@ -684,4 +686,4 @@ TensorValue Splat(mlir::ImplicitLocOpBuilder& b, Value value,\n   return BroadcastInDims(b, tensor_value, output_shape, /*dims=*/{});\n }\n \n-}  // namespace xla::gpu::triton\n+}  // namespace xla::xtile"
        },
        {
            "sha": "afb2f87d2fe016d78632a15d20fe180580d7cf29",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h?ref=6a5fd8155f88f14cd289f12fafbbae109d20a577",
            "patch": "@@ -44,7 +44,7 @@ limitations under the License.\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n \n-namespace xla::gpu::triton {\n+namespace xla::xtile {\n \n using TensorValue = mlir::TypedValue<mlir::RankedTensorType>;\n \n@@ -119,16 +119,16 @@ class TileInfo {\n llvm::SmallVector<int64_t> GetPaddedTileSizes(\n     llvm::ArrayRef<int64_t> tile_sizes);\n \n-// XLA -> Triton type conversions.\n-absl::StatusOr<mlir::Type> TritonType(mlir::ImplicitLocOpBuilder& b,\n-                                      PrimitiveType t);\n+// XLA -> MLIR type conversions.\n+absl::StatusOr<mlir::Type> PrimitiveTypeToMlirType(\n+    mlir::ImplicitLocOpBuilder& b, PrimitiveType t);\n \n-// Triton type -> XLA type conversions.\n+// MLIR type -> XLA type conversions.\n absl::StatusOr<PrimitiveType> GetPrimitiveType(mlir::Type t);\n \n mlir::Type StorageType(mlir::Type t);\n \n-// Get the value of the scalar constant's literal in a C++ type.\n+// Get the value of the scalar constant's literal in a C++ ty˝pe.\n template <typename T>\n T ScalarConstantValue(const HloInstruction& instr, PrimitiveType dst_type) {\n   CHECK_EQ(instr.opcode(), HloOpcode::kConstant);\n@@ -244,6 +244,6 @@ TensorValue BroadcastInDims(mlir::ImplicitLocOpBuilder& b, TensorValue value,\n TensorValue Splat(mlir::ImplicitLocOpBuilder& b, ::mlir::Value value,\n                   ::mlir::ArrayRef<int64_t> output_shape);\n \n-}  // namespace xla::gpu::triton\n+}  // namespace xla::xtile\n \n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_EMITTER_HELPERS_H_"
        },
        {
            "sha": "9d0426a22481b18e812896f1ead323a4d4a39ec8",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 51,
            "deletions": 46,
            "changes": 97,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=6a5fd8155f88f14cd289f12fafbbae109d20a577",
            "patch": "@@ -126,16 +126,16 @@ using ::mlir::Type;\n using ::mlir::Value;\n using ::mlir::ValueRange;\n \n-using ::xla::gpu::triton::Cast;\n-using ::xla::gpu::triton::CreateConst;\n-using ::xla::gpu::triton::EmitConstant;\n-using ::xla::gpu::triton::EmitElementwise;\n-using ::xla::gpu::triton::EmitScope;\n-using ::xla::gpu::triton::GetPaddedTileSizes;\n-using ::xla::gpu::triton::StorageType;\n-using ::xla::gpu::triton::TensorValue;\n-using ::xla::gpu::triton::TileInfo;\n-using ::xla::gpu::triton::TritonType;\n+using ::xla::xtile::Cast;\n+using ::xla::xtile::CreateConst;\n+using ::xla::xtile::EmitConstant;\n+using ::xla::xtile::EmitElementwise;\n+using ::xla::xtile::EmitScope;\n+using ::xla::xtile::GetPaddedTileSizes;\n+using ::xla::xtile::PrimitiveTypeToMlirType;\n+using ::xla::xtile::StorageType;\n+using ::xla::xtile::TensorValue;\n+using ::xla::xtile::TileInfo;\n \n namespace {\n \n@@ -190,8 +190,9 @@ absl::StatusOr<TensorValue> EmitReduce(\n   stablehlo::ReduceOp reduction = stablehlo::ReduceOp::create(\n       b, input, init_value, hlo_reduce.dimensions());\n   {\n-    TF_ASSIGN_OR_RETURN(Type result_ty,\n-                        TritonType(b, hlo_reduce.shape().element_type()));\n+    TF_ASSIGN_OR_RETURN(\n+        Type result_ty,\n+        PrimitiveTypeToMlirType(b, hlo_reduce.shape().element_type()));\n     result_ty = mlir::RankedTensorType::get({}, result_ty);\n \n     mlir::Location loc = b.getLoc();\n@@ -253,7 +254,7 @@ TensorValue EmitTiledBroadcast(\n       GetPaddedTileSizes(output_tile_shape);\n \n   TensorValue input = values[tiled_broadcast.operand(0)];\n-  return triton::BroadcastInDims(\n+  return xtile::BroadcastInDims(\n       b, input, padded_output_tile_shape,\n       MakeArrayRef(tiled_broadcast.hlo()->dimensions()));\n }\n@@ -282,26 +283,27 @@ absl::StatusOr<TensorValue> EmitTiledIota(\n   // First, stride as needed between the iota components.\n   Value range = arith::MulIOp::create(\n       b, Iota(b, padded_tile_sizes[iota_dim]),\n-      triton::Splat(\n+      xtile::Splat(\n           b,\n           CreateConst(b, b.getI32Type(), tiled_iota.tile_strides()[iota_dim]),\n           padded_tile_sizes[iota_dim]));\n \n   // Then, add the base offset to the iota components.\n   range = arith::AddIOp::create(\n-      b, range, triton::Splat(b, iota_dim_offset, padded_tile_sizes[iota_dim]));\n+      b, range, xtile::Splat(b, iota_dim_offset, padded_tile_sizes[iota_dim]));\n \n   // Cast the result to the targeted type.\n-  TF_ASSIGN_OR_RETURN(Type iota_element_type,\n-                      TritonType(b, hlo_iota->shape().element_type()));\n+  TF_ASSIGN_OR_RETURN(\n+      Type iota_element_type,\n+      PrimitiveTypeToMlirType(b, hlo_iota->shape().element_type()));\n \n   range = Cast(b, range, iota_element_type);\n \n   // And finally, produce a broadcast along the non-iota dimensions in order to\n   // produce the whole iota tile.\n-  return triton::BroadcastInDims(b, mlir::cast<TensorValue>(range),\n-                                 padded_tile_sizes,\n-                                 /*dims=*/{iota_dim});\n+  return xtile::BroadcastInDims(b, mlir::cast<TensorValue>(range),\n+                                padded_tile_sizes,\n+                                /*dims=*/{iota_dim});\n }\n \n SmallVector<Value> GetRuntimeValues(\n@@ -372,8 +374,9 @@ absl::StatusOr<TensorValue> EmitTiledBitcast(\n           \"Bitcast with different bitwidth for operand and output shape \"\n           \"element type is not yet supported.\");\n     }\n-    TF_ASSIGN_OR_RETURN(Type output_element_type,\n-                        TritonType(b, output_shape.element_type()));\n+    TF_ASSIGN_OR_RETURN(\n+        Type output_element_type,\n+        PrimitiveTypeToMlirType(b, output_shape.element_type()));\n     auto output_type = mlir::RankedTensorType::get(\n         GetPaddedTileSizes(tiled_bitcast.operand(0)->tile_sizes()),\n         output_element_type);\n@@ -507,7 +510,7 @@ absl::StatusOr<TensorValue> MaskDotOperand(\n           b, contracting_dimension_tile_index, tile_size_value);\n       TensorValue range = Iota(b, tile_size);\n       TensorValue broadcasted_tile_offset =\n-          triton::Splat(b, tile_offset, {tile_size});\n+          xtile::Splat(b, tile_offset, {tile_size});\n       Value indices = arith::AddIOp::create(b, range, broadcasted_tile_offset);\n \n       Value boundary = CreateConst(b, b.getI32Type(),\n@@ -516,11 +519,11 @@ absl::StatusOr<TensorValue> MaskDotOperand(\n       Value mask = arith::CmpIOp::create(b, arith::CmpIPredicate::slt, indices,\n                                          boundary);\n \n-      mask = triton::BroadcastInDims(b, mlir::cast<TensorValue>(mask),\n-                                     tile_shape, {contraction_dimension_index});\n-      TF_ASSIGN_OR_RETURN(\n-          auto element_type,\n-          TritonType(b, dot_operand.hlo()->shape().element_type()));\n+      mask = xtile::BroadcastInDims(b, mlir::cast<TensorValue>(mask),\n+                                    tile_shape, {contraction_dimension_index});\n+      TF_ASSIGN_OR_RETURN(auto element_type,\n+                          PrimitiveTypeToMlirType(\n+                              b, dot_operand.hlo()->shape().element_type()));\n \n       TensorValue zero = CreateConst(b, element_type, 0.0f, tile_shape);\n \n@@ -673,7 +676,7 @@ absl::StatusOr<TensorValue> EmitDot(\n   // of the dot. In particular, that is the case when an algorithm is specified\n   // and the dot's output type does not match its expectations.\n   TF_ASSIGN_OR_RETURN(Type accumulator_type,\n-                      triton::GetDotAccumulatorType(b, dot));\n+                      xtile::GetDotAccumulatorType(b, dot));\n   TensorValue accumulator =\n       CreateConst(b, accumulator_type, 0.0f, padded_tile_sizes_no_unit_dims);\n \n@@ -753,14 +756,14 @@ absl::StatusOr<TensorValue> EmitDot(\n \n     TF_ASSIGN_OR_RETURN(\n         Value acc_next,\n-        triton::EmitSingleTileDot(b, dot, triton::DotOperands{lhs, rhs, acc}));\n+        xtile::EmitSingleTileDot(b, dot, xtile::DotOperands{lhs, rhs, acc}));\n     mlir::scf::YieldOp::create(b, acc_next);\n   }\n \n   // The output of the loop may not match the expected output type of the dot.\n   // We make sure to issue a conversion if necessary.\n   TF_ASSIGN_OR_RETURN(Type dot_output_type,\n-                      TritonType(b, dot.shape().element_type()));\n+                      PrimitiveTypeToMlirType(b, dot.shape().element_type()));\n \n   Value result = for_op.getResult(0);\n   if (dot_output_type != accumulator_type) {\n@@ -904,16 +907,17 @@ absl::StatusOr<TensorValue> EmitScaledDot(\n \n     TF_ASSIGN_OR_RETURN(\n         Value acc_next,\n-        triton::EmitSingleTileScaledDot(\n+        xtile::EmitSingleTileScaledDot(\n             b, scaled_dot,\n-            triton::ScaledDotOperands{lhs, rhs, lhs_scale, rhs_scale, acc}));\n+            xtile::ScaledDotOperands{lhs, rhs, lhs_scale, rhs_scale, acc}));\n     mlir::scf::YieldOp::create(b, acc_next);\n   }\n \n   // The output of the loop may not match the expected output type of the dot.\n   // We make sure to issue a conversion if necessary.\n-  TF_ASSIGN_OR_RETURN(Type dot_output_type,\n-                      TritonType(b, scaled_dot.shape().element_type()));\n+  TF_ASSIGN_OR_RETURN(\n+      Type dot_output_type,\n+      PrimitiveTypeToMlirType(b, scaled_dot.shape().element_type()));\n \n   Value result = for_op.getResult(0);\n   if (dot_output_type != accumulator_type) {\n@@ -971,9 +975,9 @@ absl::StatusOr<TensorValue> EmitConcatenate(\n           operand_concat_dim_size, \" % \", concat_dim_tile_size, \" != 0\"));\n     }\n   }\n-  TF_ASSIGN_OR_RETURN(\n-      Type element_type,\n-      TritonType(b, tiled_concatenate.hlo()->shape().element_type()));\n+  TF_ASSIGN_OR_RETURN(Type element_type,\n+                      PrimitiveTypeToMlirType(\n+                          b, tiled_concatenate.hlo()->shape().element_type()));\n   Type result_type =\n       mlir::RankedTensorType::get(padded_tile_sizes, element_type);\n \n@@ -1070,15 +1074,14 @@ absl::StatusOr<TensorValue> EmitPad(\n \n     // LHS for the compare is an iota broadcasted to the output shape.\n     TensorValue range = Iota(b, pad_output_dim_size);\n-    TensorValue bcast = triton::BroadcastInDims(\n+    TensorValue bcast = xtile::BroadcastInDims(\n         b, range, padded_tile_sizes, {static_cast<int64_t>(dim_index)});\n \n     // RHS for the compare is splat(pad_input_dim_size - tile_offset).\n     Value tile_offset_i32 = Cast(b, tile_offset, i32_type);\n     Value threshold = arith::SubIOp::create(\n         b, CreateConst(b, i32_type, pad_input_dim_size), tile_offset_i32);\n-    TensorValue threshold_splat =\n-        triton::Splat(b, threshold, padded_tile_sizes);\n+    TensorValue threshold_splat = xtile::Splat(b, threshold, padded_tile_sizes);\n     Value cmp = arith::CmpIOp::create(b, arith::CmpIPredicate::slt, bcast,\n                                       threshold_splat);\n     mask = mask ? arith::AndIOp::create(b, mask, cmp) : cmp;\n@@ -1089,7 +1092,7 @@ absl::StatusOr<TensorValue> EmitPad(\n   const TiledHloInstruction* padding_value = tiled_pad.operand(1);\n \n   TensorValue pad_value_splat =\n-      triton::Splat(b, values[padding_value], padded_tile_sizes);\n+      xtile::Splat(b, values[padding_value], padded_tile_sizes);\n   return mlir::cast<TensorValue>(\n       arith::SelectOp::create(b, mask, values[tiled_operand], pad_value_splat)\n           .getResult());\n@@ -1127,8 +1130,9 @@ absl::StatusOr<TensorValue> EmitTiledHloInstruction(\n     // loading if the type of the loaded parameter does not match what is\n     // expected.\n     Type loaded_element_type = getElementTypeOrSelf(parameter.getType());\n-    TF_ASSIGN_OR_RETURN(Type expected_element_type,\n-                        TritonType(b, hlo->shape().element_type()));\n+    TF_ASSIGN_OR_RETURN(\n+        Type expected_element_type,\n+        PrimitiveTypeToMlirType(b, hlo->shape().element_type()));\n \n     if (expected_element_type != loaded_element_type) {\n       // Ensure that we didn't mess up somewhere else by checking that we\n@@ -1532,13 +1536,14 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n     } else if (type == S4) {\n       ir_type = b.getI4Type();\n     } else {\n-      TF_ASSIGN_OR_RETURN(ir_type, TritonType(b, type));\n+      TF_ASSIGN_OR_RETURN(ir_type, PrimitiveTypeToMlirType(b, type));\n     }\n     fn_arg_types.push_back(GetMemRefType(p->shape(), ir_type));\n   }\n \n   for (const auto& [index, shape] : ShapeUtil::GetLeafShapes(fusion->shape())) {\n-    TF_ASSIGN_OR_RETURN(Type triton_ty, TritonType(b, shape.element_type()));\n+    TF_ASSIGN_OR_RETURN(Type triton_ty,\n+                        PrimitiveTypeToMlirType(b, shape.element_type()));\n     fn_arg_types.push_back(GetMemRefType(shape, triton_ty));\n   }\n "
        },
        {
            "sha": "523d477e2b018fc65bcdbfa823d21ef7cac787fc",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/stablehlo_lower_to_triton.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 22,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fstablehlo_lower_to_triton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fstablehlo_lower_to_triton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fstablehlo_lower_to_triton.cc?ref=6a5fd8155f88f14cd289f12fafbbae109d20a577",
            "patch": "@@ -400,12 +400,12 @@ struct TritonPrecisionSpec {\n mlir::Type ElementType(mlir::Value v) { return mlir::getElementTypeOrSelf(v); }\n \n using AlgorithmEmitter = absl::StatusOr<Value> (*)(\n-    mlir::ImplicitLocOpBuilder&, const ::xla::gpu::triton::DotOperands&,\n+    mlir::ImplicitLocOpBuilder&, const ::xla::xtile::DotOperands&,\n     const TritonPrecisionSpec&);\n \n absl::StatusOr<Value> EmitDotAlgUnset(\n     mlir::ImplicitLocOpBuilder& b,\n-    const ::xla::gpu::triton::DotOperands& dot_operands,\n+    const ::xla::xtile::DotOperands& dot_operands,\n     const TritonPrecisionSpec& precision_spec) {\n   // Execute matrix multiplication of input tiles and pass the accumulator.\n   // TODO(manany): Should be looked into once we enable Hopper workloads.\n@@ -432,7 +432,7 @@ absl::StatusOr<Value> EmitDotAlgUnset(\n \n absl::StatusOr<Value> EmitRegularDot(\n     mlir::ImplicitLocOpBuilder& b,\n-    const ::xla::gpu::triton::DotOperands& dot_operands,\n+    const ::xla::xtile::DotOperands& dot_operands,\n     const TritonPrecisionSpec& precision_spec) {\n   Value lhs = dot_operands.lhs;\n   Value rhs = dot_operands.rhs;\n@@ -450,11 +450,11 @@ absl::StatusOr<Value> EmitRegularDot(\n   if (precision_spec.algorithm ==\n       ::xla::PrecisionConfig::ALG_DOT_BF16_BF16_F32) {\n     if (ElementType(lhs).isF32()) {\n-      lhs = ::xla::gpu::triton::Cast(b, lhs, b.getBF16Type());\n+      lhs = ::xla::xtile::Cast(b, lhs, b.getBF16Type());\n     }\n \n     if (ElementType(rhs).isF32()) {\n-      rhs = ::xla::gpu::triton::Cast(b, rhs, b.getBF16Type());\n+      rhs = ::xla::xtile::Cast(b, rhs, b.getBF16Type());\n     }\n   }\n \n@@ -472,14 +472,14 @@ absl::StatusOr<Value> EmitRegularDot(\n // must override any accumulated result if the last partial product is\n // non-finite. See b/115844437.\n Value ZeroNaNs(mlir::ImplicitLocOpBuilder& b, Value input) {\n-  Value positive_inf = ::xla::gpu::triton::CreateConst<float>(\n+  Value positive_inf = ::xla::xtile::CreateConst<float>(\n       b, b.getF32Type(), std::numeric_limits<float>::infinity(),\n       mlir::cast<ShapedType>(input.getType()).getShape());\n   Value abs_input = math::AbsFOp::create(b, input);\n   Value is_finite = arith::CmpFOp::create(b, arith::CmpFPredicate::OGT,\n                                           positive_inf, abs_input);\n   return arith::SelectOp::create(b, is_finite, input,\n-                                 ::xla::gpu::triton::ZerosLike(b, input));\n+                                 ::xla::xtile::ZerosLike(b, input));\n }\n \n absl::Status ExpectType(Value v, Type expected_type) {\n@@ -502,10 +502,9 @@ std::vector<Value> SplitF32(mlir::ImplicitLocOpBuilder& b, Value input,\n   std::vector<Value> split_inputs;\n   split_inputs.reserve(split_count);\n   for (int i = 0; i < split_count; ++i) {\n-    Value input_as_bf16 = ::xla::gpu::triton::Cast(b, input, b.getBF16Type());\n+    Value input_as_bf16 = ::xla::xtile::Cast(b, input, b.getBF16Type());\n     if (i != split_count - 1) {\n-      Value input_as_f32 =\n-          ::xla::gpu::triton::Cast(b, input_as_bf16, b.getF32Type());\n+      Value input_as_f32 = ::xla::xtile::Cast(b, input_as_bf16, b.getF32Type());\n       input = arith::SubFOp::create(b, input, input_as_f32);\n     }\n     split_inputs.push_back(input_as_bf16);\n@@ -523,7 +522,7 @@ Value IEEEDot(mlir::ImplicitLocOpBuilder& b, Value lhs, Value rhs, Value acc) {\n // from https://arxiv.org/pdf/1904.06376.pdf.\n absl::StatusOr<Value> EmitBF16x9Matmul(\n     mlir::ImplicitLocOpBuilder& b,\n-    const ::xla::gpu::triton::DotOperands& dot_operands,\n+    const ::xla::xtile::DotOperands& dot_operands,\n     const TritonPrecisionSpec& precision_spec) {\n   constexpr int kNumParts = 3;\n   constexpr int kHigh = 0;\n@@ -538,7 +537,7 @@ absl::StatusOr<Value> EmitBF16x9Matmul(\n   std::vector<Value> lhs_parts = SplitF32(b, dot_operands.lhs, kNumParts);\n   std::vector<Value> rhs_parts = SplitF32(b, dot_operands.rhs, kNumParts);\n \n-  Value result = ::xla::gpu::triton::ZerosLike(b, dot_operands.accumulator);\n+  Value result = ::xla::xtile::ZerosLike(b, dot_operands.accumulator);\n \n   result = IEEEDot(b, lhs_parts[kLow], rhs_parts[kLow], result);\n   result = IEEEDot(b, lhs_parts[kMid], rhs_parts[kLow], result);\n@@ -562,7 +561,7 @@ absl::StatusOr<Value> EmitBF16x9Matmul(\n // from https://arxiv.org/pdf/1904.06376.pdf.\n absl::StatusOr<Value> EmitBF16x6Matmul(\n     mlir::ImplicitLocOpBuilder& b,\n-    const ::xla::gpu::triton::DotOperands& dot_operands,\n+    const ::xla::xtile::DotOperands& dot_operands,\n     const TritonPrecisionSpec& precision_spec) {\n   constexpr int kNumParts = 3;\n   constexpr int kHigh = 0;\n@@ -577,7 +576,7 @@ absl::StatusOr<Value> EmitBF16x6Matmul(\n   std::vector<Value> lhs_parts = SplitF32(b, dot_operands.lhs, kNumParts);\n   std::vector<Value> rhs_parts = SplitF32(b, dot_operands.rhs, kNumParts);\n \n-  Value result = ::xla::gpu::triton::ZerosLike(b, dot_operands.accumulator);\n+  Value result = ::xla::xtile::ZerosLike(b, dot_operands.accumulator);\n \n   result = IEEEDot(b, lhs_parts[kMid], rhs_parts[kMid], result);\n \n@@ -597,7 +596,7 @@ absl::StatusOr<Value> EmitBF16x6Matmul(\n // EmitBF16x6Matmul.\n absl::StatusOr<Value> EmitBF16x3Matmul(\n     mlir::ImplicitLocOpBuilder& b,\n-    const ::xla::gpu::triton::DotOperands& dot_operands,\n+    const ::xla::xtile::DotOperands& dot_operands,\n     const TritonPrecisionSpec& precision_spec) {\n   constexpr int kNumParts = 2;\n   constexpr int kHigh = 0;\n@@ -611,7 +610,7 @@ absl::StatusOr<Value> EmitBF16x3Matmul(\n   std::vector<Value> lhs_bf16 = SplitF32(b, dot_operands.lhs, kNumParts);\n   std::vector<Value> rhs_bf16 = SplitF32(b, dot_operands.rhs, kNumParts);\n \n-  Value result = ::xla::gpu::triton::ZerosLike(b, dot_operands.accumulator);\n+  Value result = ::xla::xtile::ZerosLike(b, dot_operands.accumulator);\n   result = IEEEDot(b, lhs_bf16[kLow], rhs_bf16[kHigh], result);\n   result = IEEEDot(b, lhs_bf16[kHigh], rhs_bf16[kLow], result);\n   result = ZeroNaNs(b, result);\n@@ -658,7 +657,7 @@ absl::StatusOr<AlgorithmEmitter> GetAlgorithmEmitter(\n                    ::xla::PrecisionConfig::Algorithm_Name(algorithm)));\n }\n \n-bool IsTf32Allowed(const ::xla::gpu::triton::PrecisionSpec& precision_spec) {\n+bool IsTf32Allowed(const ::xla::xtile::PrecisionSpec& precision_spec) {\n   if (precision_spec.algorithm == ::xla::PrecisionConfig::ALG_UNSET) {\n     return tsl::tensor_float_32_execution_enabled() &&\n            StableHloPrecisionToXlaPrecision(\n@@ -672,7 +671,7 @@ bool IsTf32Allowed(const ::xla::gpu::triton::PrecisionSpec& precision_spec) {\n }\n \n ttir::InputPrecision InferDotPrecision(\n-    const ::xla::gpu::triton::PrecisionSpec& precision_spec) {\n+    const ::xla::xtile::PrecisionSpec& precision_spec) {\n   if (precision_spec.algorithm ==\n       ::xla::PrecisionConfig::ALG_DOT_TF32_TF32_F32_X3) {\n     return ttir::InputPrecision::TF32x3;\n@@ -714,8 +713,7 @@ LogicalResult RewriteDotGeneralToTritonDot(mlir::PatternRewriter& rewriter,\n \n   mlir::ImplicitLocOpBuilder builder(op->getLoc(), rewriter);\n \n-  ::xla::gpu::triton::DotOperands dot_operands{op.getLhs(), op.getRhs(),\n-                                               accumulator};\n+  ::xla::xtile::DotOperands dot_operands{op.getLhs(), op.getRhs(), accumulator};\n \n   stablehlo::Precision lhs_precision;\n   stablehlo::Precision rhs_precision;\n@@ -725,8 +723,8 @@ LogicalResult RewriteDotGeneralToTritonDot(mlir::PatternRewriter& rewriter,\n     return mlir::failure();\n   }\n \n-  ::xla::gpu::triton::PrecisionSpec precision_spec{hlo_algorithm, lhs_precision,\n-                                                   rhs_precision};\n+  ::xla::xtile::PrecisionSpec precision_spec{hlo_algorithm, lhs_precision,\n+                                             rhs_precision};\n \n   TritonPrecisionSpec triton_precision_spec{hlo_algorithm,\n                                             InferDotPrecision(precision_spec)};"
        },
        {
            "sha": "24a39e70040bb9b1b7a6c7c1548a5ce43ee4e52c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_extract_insert_to_triton_pass.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc?ref=6a5fd8155f88f14cd289f12fafbbae109d20a577",
            "patch": "@@ -68,8 +68,7 @@ namespace mlir::triton::xla {\n #define GEN_PASS_DEF_TRITONXLAEXTRACTINSERTTOTRITONPASS\n #include \"xla/backends/gpu/codegen/triton/transforms/passes.h.inc\"\n \n-namespace xg = ::xla::gpu;\n-namespace xgt = xg::triton;\n+namespace xtile = ::xla::xtile;\n \n namespace {\n \n@@ -86,7 +85,7 @@ bool HasBroadcastConsumer(Operation* op) {\n \n PointerType GetTensorPtrType(Type type) {\n   return PointerType::get(\n-      xgt::StorageType(type),\n+      xtile::StorageType(type),\n       static_cast<unsigned>(mlir::NVVM::NVVMMemorySpace::Global));\n }\n "
        },
        {
            "sha": "be44dfe30f4d82581daec5e4482197d349c48e2f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_math_to_libdevice.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_math_to_libdevice.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_math_to_libdevice.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_math_to_libdevice.cc?ref=6a5fd8155f88f14cd289f12fafbbae109d20a577",
            "patch": "@@ -185,7 +185,7 @@ class ConvertToLibdevice : public mlir::OpRewritePattern<OpTy> {\n     }\n \n     absl::StatusOr<::xla::PrimitiveType> primitive_type_or =\n-        ::xla::gpu::triton::GetPrimitiveType(output_type);\n+        ::xla::xtile::GetPrimitiveType(output_type);\n     if (!primitive_type_or.ok()) {\n       return rewriter.notifyMatchFailure(op, \"could not get primitive type\");\n     }\n@@ -197,7 +197,7 @@ class ConvertToLibdevice : public mlir::OpRewritePattern<OpTy> {\n       // Upcast the inputs to F32.\n       for (auto operand : op->getOperands()) {\n         casted_inputs.push_back(\n-            ::xla::gpu::triton::Cast(builder, operand, rewriter.getF32Type()));\n+            ::xla::xtile::Cast(builder, operand, rewriter.getF32Type()));\n       }\n     } else {\n       casted_inputs = llvm::to_vector(op->getOperands());\n@@ -212,7 +212,7 @@ class ConvertToLibdevice : public mlir::OpRewritePattern<OpTy> {\n \n     if (res.getType() != output_type) {\n       // Downcast back to the original output type.\n-      res = ::xla::gpu::triton::Cast(builder, res, output_type);\n+      res = ::xla::xtile::Cast(builder, res, output_type);\n     }\n \n     rewriter.replaceOp(op, res);"
        },
        {
            "sha": "7d11a484ae82301f46e26185c34f381d95b6acca",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/xtile_lower_to_triton.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fxtile_lower_to_triton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6a5fd8155f88f14cd289f12fafbbae109d20a577/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fxtile_lower_to_triton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fxtile_lower_to_triton.cc?ref=6a5fd8155f88f14cd289f12fafbbae109d20a577",
            "patch": "@@ -74,11 +74,9 @@ class LowerDotScaled\n                                                    : add_op->getOperand(1);\n \n     auto lhs_dot_elem_type_or_status =\n-        ::xla::gpu::triton::internal::GetScaleDotElemType(\n-            op.getLhs().getType());\n+        ::xla::xtile::internal::GetScaleDotElemType(op.getLhs().getType());\n     auto rhs_dot_elem_type_or_status =\n-        ::xla::gpu::triton::internal::GetScaleDotElemType(\n-            op.getRhs().getType());\n+        ::xla::xtile::internal::GetScaleDotElemType(op.getRhs().getType());\n \n     if (!lhs_dot_elem_type_or_status.ok() ||\n         !rhs_dot_elem_type_or_status.ok()) {"
        }
    ],
    "stats": {
        "total": 236,
        "additions": 118,
        "deletions": 118
    }
}