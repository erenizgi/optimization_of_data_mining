{
    "author": "basioli-k",
    "message": "[XLA:CPU][pjrt] Implement PjRt compiler interface for XLA:CPU\n\nPiperOrigin-RevId: 834223272",
    "sha": "ef8be6b18619b396f6ea166f10fb68223164f3a0",
    "files": [
        {
            "sha": "8a6e585da6c592bd8f9a86f5c765ca592e3178e1",
            "filename": "third_party/xla/xla/pjrt/cpu/BUILD",
            "status": "modified",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ef8be6b18619b396f6ea166f10fb68223164f3a0/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ef8be6b18619b396f6ea166f10fb68223164f3a0/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2FBUILD?ref=ef8be6b18619b396f6ea166f10fb68223164f3a0",
            "patch": "@@ -309,3 +309,50 @@ cc_library(\n         \"@com_google_absl//absl/synchronization\",\n     ],\n )\n+\n+cc_library(\n+    name = \"cpu_pjrt_compiler\",\n+    srcs = [\"cpu_pjrt_compiler.cc\"],\n+    hdrs = [\"cpu_pjrt_compiler.h\"],\n+    visibility = internal_visibility([\"//xla/pjrt/cpu:legacy_cpu_client_users\"]),\n+    deps = [\n+        \"//xla:status_macros\",\n+        \"//xla/backends/cpu/collectives:cpu_collectives\",\n+        \"//xla/core/collectives:clique_id\",\n+        \"//xla/core/collectives:clique_key\",\n+        \"//xla/core/collectives:communicator\",\n+        \"//xla/hlo/builder:xla_computation\",\n+        \"//xla/pjrt:pjrt_client\",\n+        \"//xla/pjrt:pjrt_compiler\",\n+        \"//xla/pjrt:pjrt_executable\",\n+        \"//xla/pjrt/plugin/xla_cpu:cpu_client_options\",\n+        \"//xla/pjrt/plugin/xla_cpu:xla_cpu_pjrt_client\",\n+        \"//xla/stream_executor/platform:initialize\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@llvm-project//mlir:IR\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"cpu_pjrt_compiler_test\",\n+    srcs = [\"cpu_pjrt_compiler_test.cc\"],\n+    deps = [\n+        \":cpu_pjrt_compiler\",\n+        \"//xla/hlo/builder:xla_computation\",\n+        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/mlir_hlo\",\n+        \"//xla/pjrt:pjrt_executable\",\n+        \"//xla/pjrt/plugin/xla_cpu:xla_cpu_pjrt_client\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/platform:test_main\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_googletest//:gtest\",\n+        \"@llvm-project//mlir:FuncDialect\",\n+        \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:Parser\",\n+    ],\n+)"
        },
        {
            "sha": "cea9e8f86eba9a51829d52e7b972d627b508ac2e",
            "filename": "third_party/xla/xla/pjrt/cpu/cpu_pjrt_compiler.cc",
            "status": "added",
            "additions": 116,
            "deletions": 0,
            "changes": 116,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ef8be6b18619b396f6ea166f10fb68223164f3a0/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_pjrt_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ef8be6b18619b396f6ea166f10fb68223164f3a0/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_pjrt_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_pjrt_compiler.cc?ref=ef8be6b18619b396f6ea166f10fb68223164f3a0",
            "patch": "@@ -0,0 +1,116 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/pjrt/cpu/cpu_pjrt_compiler.h\"\n+\n+#include <memory>\n+#include <optional>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/log/check.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/types/span.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"xla/backends/cpu/collectives/cpu_collectives.h\"\n+#include \"xla/core/collectives/clique_id.h\"\n+#include \"xla/core/collectives/clique_key.h\"\n+#include \"xla/core/collectives/communicator.h\"\n+#include \"xla/hlo/builder/xla_computation.h\"\n+#include \"xla/pjrt/pjrt_client.h\"\n+#include \"xla/pjrt/pjrt_compiler.h\"\n+#include \"xla/pjrt/pjrt_executable.h\"\n+#include \"xla/pjrt/plugin/xla_cpu/cpu_client_options.h\"\n+#include \"xla/pjrt/plugin/xla_cpu/xla_cpu_pjrt_client.h\"\n+#include \"xla/stream_executor/platform/initialize.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla::cpu {\n+\n+namespace {\n+\n+// A dummy CpuCollectives implementation used for compilation.\n+class DummyCpuCollectives : public xla::cpu::CpuCollectives {\n+ public:\n+  absl::StatusOr<std::vector<std::unique_ptr<xla::Communicator>>>\n+  CreateCommunicators(const xla::CliqueKey& clique_key,\n+                      const std::optional<xla::CliqueIds>& clique_ids,\n+                      absl::Span<const DeviceRank> ranks,\n+                      const Config& config) final {\n+    return absl::UnimplementedError(\n+        \"DummyCpuCollectives::CreateCommunicators is not implemented\");\n+  }\n+};\n+\n+// Creates a PjRt CPU client from the given topology description.\n+//\n+// We only take the number of CPU devices from the topology and assume that the\n+// compilation worker and the worker running the compiled CPU computation are on\n+// the same CPU platform. This may not be true in the future in a highly\n+// disaggregated setup; then, we should extend the PjRt CPU client to\n+// support cross compilation for a non-local platform, and use machine\n+// attributes captured in a CPU topology description.\n+absl::StatusOr<std::unique_ptr<xla::PjRtClient>>\n+CreatePjRtCpuClientFromTopology(\n+    const xla::PjRtTopologyDescription& topology_description) {\n+  // TODO(b/373647598): Use PjRtCompile() without creating a PjRt CPU client\n+  // once it is supported by XLA:CPU.\n+  xla::CpuClientOptions options;\n+  TF_ASSIGN_OR_RETURN(options.cpu_device_count,\n+                      topology_description.CoreCountOfDefaultTypePerProcess());\n+  CHECK_GE(*options.cpu_device_count, 1);\n+  // We need to provide `CpuCollectives` to be able to compile multi-host/-slice\n+  // CPU computations. The details of the collectives is not important because\n+  // the compilation only checks if any `CpuCollectives` exists.\n+  options.collectives = std::make_shared<DummyCpuCollectives>();\n+  return xla::GetXlaPjrtCpuClient(options);\n+}\n+\n+template <typename T>\n+absl::StatusOr<std::unique_ptr<PjRtExecutable>> CompileInternal(\n+    const T& computation, CompileOptions options,\n+    const PjRtTopologyDescription& topology, PjRtClient* client) {\n+  TF_ASSIGN_OR_RETURN(auto cpu_client,\n+                      CreatePjRtCpuClientFromTopology(topology));\n+\n+  TF_ASSIGN_OR_RETURN(auto loaded_executable,\n+                      cpu_client->CompileAndLoad(computation, options));\n+\n+  return std::make_unique<xla::PjRtExecutableForwarder>(\n+      std::move(loaded_executable));\n+}\n+\n+}  // namespace\n+\n+absl::StatusOr<std::unique_ptr<PjRtExecutable>> CpuPjRtCompiler::Compile(\n+    CompileOptions options, const XlaComputation& computation,\n+    const PjRtTopologyDescription& topology, PjRtClient* client) {\n+  return CompileInternal(computation, options, topology, client);\n+}\n+\n+absl::StatusOr<std::unique_ptr<PjRtExecutable>> CpuPjRtCompiler::Compile(\n+    CompileOptions options, mlir::ModuleOp module,\n+    const PjRtTopologyDescription& topology, PjRtClient* client) {\n+  return CompileInternal(module, options, topology, client);\n+}\n+\n+}  // namespace xla::cpu\n+\n+STREAM_EXECUTOR_REGISTER_MODULE_INITIALIZER(pjrt_register_cpu_compiler, {\n+  std::unique_ptr<xla::PjRtCompiler> compiler =\n+      std::make_unique<xla::cpu::CpuPjRtCompiler>();\n+  PjRtRegisterCompiler(xla::CpuName(), std::move(compiler));\n+});"
        },
        {
            "sha": "b92254285524448f9175d3baeda570599ac53bf3",
            "filename": "third_party/xla/xla/pjrt/cpu/cpu_pjrt_compiler.h",
            "status": "added",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ef8be6b18619b396f6ea166f10fb68223164f3a0/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_pjrt_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ef8be6b18619b396f6ea166f10fb68223164f3a0/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_pjrt_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_pjrt_compiler.h?ref=ef8be6b18619b396f6ea166f10fb68223164f3a0",
            "patch": "@@ -0,0 +1,46 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_PJRT_CPU_CPU_PJRT_COMPILER_H_\n+#define XLA_PJRT_CPU_CPU_PJRT_COMPILER_H_\n+\n+#include <memory>\n+#include <string>\n+\n+#include \"absl/status/statusor.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"xla/hlo/builder/xla_computation.h\"\n+#include \"xla/pjrt/pjrt_compiler.h\"\n+#include \"xla/pjrt/pjrt_executable.h\"\n+\n+namespace xla::cpu {\n+\n+class CpuPjRtCompiler : public PjRtCompiler {\n+ public:\n+  // Compiles the 'computation' and returns a 'PjRtExecutable'. The returned\n+  // PjRtExecutable must be loaded by a compatible client before execution.\n+  absl::StatusOr<std::unique_ptr<PjRtExecutable>> Compile(\n+      CompileOptions options, const XlaComputation& computation,\n+      const PjRtTopologyDescription& topology, PjRtClient* client) override;\n+\n+  // Variant of `Compile` that accepts an MLIR module.\n+  absl::StatusOr<std::unique_ptr<PjRtExecutable>> Compile(\n+      CompileOptions options, mlir::ModuleOp module,\n+      const PjRtTopologyDescription& topology, PjRtClient* client) override;\n+};\n+\n+}  // namespace xla::cpu\n+\n+#endif  // XLA_PJRT_CPU_CPU_PJRT_COMPILER_H_"
        },
        {
            "sha": "91b1887f219cd9656192abe23a28a0bf337b2a05",
            "filename": "third_party/xla/xla/pjrt/cpu/cpu_pjrt_compiler_test.cc",
            "status": "added",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ef8be6b18619b396f6ea166f10fb68223164f3a0/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_pjrt_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ef8be6b18619b396f6ea166f10fb68223164f3a0/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_pjrt_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_pjrt_compiler_test.cc?ref=ef8be6b18619b396f6ea166f10fb68223164f3a0",
            "patch": "@@ -0,0 +1,91 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/pjrt/cpu/cpu_pjrt_compiler.h\"\n+\n+#include <gtest/gtest.h>\n+#include \"absl/strings/string_view.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/Parser/Parser.h\"\n+#include \"xla/hlo/builder/xla_computation.h\"\n+#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/mlir_hlo/mhlo/IR/hlo_ops.h\"\n+#include \"xla/pjrt/pjrt_executable.h\"\n+#include \"xla/pjrt/plugin/xla_cpu/xla_cpu_pjrt_client.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla::cpu {\n+namespace {\n+\n+constexpr absl::string_view kProgram = R\"(HloModule Computation\n+\n+ENTRY Computation() -> s32[] {\n+  ROOT result = s32[] constant(2)\n+})\";\n+\n+constexpr absl::string_view kMlirProgram = R\"mlir(\n+  module {\n+    func.func @main() -> tensor<i32> {\n+      %0 = mhlo.constant dense<2> : tensor<i32>\n+      return %0 : tensor<i32>\n+    }\n+  })mlir\";\n+\n+using CpuPjrtCompilerTest = xla::HloHardwareIndependentTestBase;\n+\n+TEST_F(CpuPjrtCompilerTest, CompileXlaComputationSuccess) {\n+  xla::CompileOptions options;\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kProgram));\n+  xla::XlaComputation computation(module->ToProto());\n+\n+  // TODO(basioli): Temporary hack to get the cpu topology easily, will update\n+  // test once cross-compilation is supported.\n+  TF_ASSERT_OK_AND_ASSIGN(auto cpu_client, xla::GetXlaPjrtCpuClient({}));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto topology_description,\n+                          cpu_client->GetTopologyDescription());\n+\n+  xla::cpu::CpuPjRtCompiler compiler;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto executable,\n+      compiler.Compile(options, computation, *topology_description,\n+                       /*client=*/nullptr));\n+}\n+\n+TEST_F(CpuPjrtCompilerTest, CompileMlirOpSuccess) {\n+  xla::CompileOptions options;\n+  mlir::MLIRContext context;\n+  context.loadDialect<mlir::func::FuncDialect, mlir::mhlo::MhloDialect>();\n+  auto mlir_module =\n+      mlir::parseSourceString<mlir::ModuleOp>(kMlirProgram, &context);\n+\n+  // TODO(basioli): Temporary hack to get the cpu topology easily, will update\n+  // test once cross-compilation is supported.\n+  TF_ASSERT_OK_AND_ASSIGN(auto cpu_client, xla::GetXlaPjrtCpuClient({}));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto topology_description,\n+                          cpu_client->GetTopologyDescription());\n+\n+  xla::cpu::CpuPjRtCompiler compiler;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto executable,\n+      compiler.Compile(options, *mlir_module, *topology_description,\n+                       /*client=*/nullptr));\n+}\n+\n+}  // namespace\n+}  // namespace xla::cpu"
        }
    ],
    "stats": {
        "total": 300,
        "additions": 300,
        "deletions": 0
    }
}