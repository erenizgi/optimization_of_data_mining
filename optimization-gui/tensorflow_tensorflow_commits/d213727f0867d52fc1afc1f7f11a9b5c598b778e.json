{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 809521910",
    "sha": "d213727f0867d52fc1afc1f7f11a9b5c598b778e",
    "files": [
        {
            "sha": "a2699c1a572ad7bbbfc954d83006e0748f4d0559",
            "filename": "third_party/xla/xla/python/transfer/event_loop.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fevent_loop.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fevent_loop.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fevent_loop.cc?ref=d213727f0867d52fc1afc1f7f11a9b5c598b778e",
            "patch": "@@ -53,24 +53,24 @@ class PollEventLoopImpl : public PollEventLoop {\n   }\n \n   void RegisterHandler(Handler* handler) override {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     inserts_.push_back(handler);\n     WakeInternal();\n   }\n   void SendWake(Handler* handler) override {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     wakes_.insert(handler);\n     WakeInternal();\n   }\n \n   void Schedule(absl::AnyInvocable<void() &&> cb) override {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     cbs_.push_back(std::move(cb));\n     WakeInternal();\n   }\n \n   void ScheduleAt(absl::Time t, absl::AnyInvocable<void() &&> cb) override {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     bool needs_wake = timeout_cbs_.empty() || timeout_cbs_.top().t > t;\n     timeout_cbs_.push({t, std::move(cb)});\n     if (needs_wake) {\n@@ -111,7 +111,7 @@ class PollEventLoopImpl : public PollEventLoop {\n         eventfd_read(event_fd_, &counter);\n       }\n       {\n-        absl::MutexLock l(&mu_);\n+        absl::MutexLock l(mu_);\n         std::swap(wakes_, wakes);\n         std::swap(inserts, inserts_);\n         std::swap(cbs, cbs_);"
        },
        {
            "sha": "9b35bb9f2646ca9f2d5bacfbbcb0816dce01f1a5",
            "filename": "third_party/xla/xla/python/transfer/pjrt_transfer_server.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fpjrt_transfer_server.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fpjrt_transfer_server.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fpjrt_transfer_server.cc?ref=d213727f0867d52fc1afc1f7f11a9b5c598b778e",
            "patch": "@@ -230,7 +230,7 @@ absl::Status PjRtTransferServer::CrossHostPull(\n   }\n   tsl::RCReference<aux::SocketServer::Connection> connection;\n   {\n-    absl::MutexLock lock(&connections_mu_);\n+    absl::MutexLock lock(connections_mu_);\n     TF_ASSIGN_OR_RETURN(connection, GetConnection(remote_pid));\n   }\n "
        },
        {
            "sha": "529c69922833aa204eb9490723c7b13dab29a471",
            "filename": "third_party/xla/xla/python/transfer/socket-server.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket-server.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket-server.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket-server.cc?ref=d213727f0867d52fc1afc1f7f11a9b5c598b778e",
            "patch": "@@ -112,7 +112,7 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n           recv(fd_, network_buffer_.get() + recv_count_, 4096 - recv_count_, 0);\n       if (recv_size == 0) {\n         {\n-          absl::MutexLock l(&mu_);\n+          absl::MutexLock l(mu_);\n           is_poisoned_ = true;\n           peer_is_closed_ = true;\n           poison_status_ = absl::InternalError(\n@@ -163,7 +163,7 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n     if (events.revents & POLLOUT) {\n       can_send_ = true;\n     }\n-    mu_.Lock();\n+    mu_.lock();\n     while (!frames_.empty() && can_send_) {\n       auto& packet_to_send = frames_.front();\n       if (packet_to_send.empty()) {\n@@ -184,19 +184,19 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n       } else if (send_size < 0 && errno == EAGAIN) {\n         can_send_ = false;\n       } else {\n-        mu_.Unlock();\n+        mu_.unlock();\n         Poison(absl::InternalError(\n             absl::StrFormat(\"%ld = send() failed errno: %d err: %s\", send_size,\n                             errno, strerror(errno))));\n         return true;\n       }\n     }\n     if (peer_is_closed_ && num_refs_ == 0) {\n-      mu_.Unlock();\n+      mu_.unlock();\n       delete this;\n       return false;\n     }\n-    mu_.Unlock();\n+    mu_.unlock();\n     return true;\n   }\n \n@@ -211,7 +211,7 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n         reinterpret_cast<const char*>(&header), sizeof(header)));\n     req.AppendToString(&opacket);\n     {\n-      absl::MutexLock l(&mu_);\n+      absl::MutexLock l(mu_);\n       frames_.push_back(std::move(opacket));\n     }\n     loop()->SendWake(this);\n@@ -221,7 +221,7 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n                                                  size_t size, bool is_largest) {\n     tsl::RCReference<ChunkDestination> dest;\n     {\n-      absl::MutexLock l(&mu_);\n+      absl::MutexLock l(mu_);\n       auto it = dests_.find(req_id);\n       CHECK(it != dests_.end());\n       if (is_largest) {\n@@ -240,37 +240,37 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n   }\n \n   std::optional<size_t> InstallPull(tsl::RCReference<ChunkDestination> dest) {\n-    mu_.Lock();\n+    mu_.lock();\n     if (is_poisoned_) {\n       auto poison_status = poison_status_;\n       dest->Poison(std::move(poison_status));\n-      mu_.Unlock();\n+      mu_.unlock();\n       return std::nullopt;\n     }\n     dests_[next_req_id_].dest = std::move(dest);\n     size_t req_id = next_req_id_;\n     ++next_req_id_;\n-    mu_.Unlock();\n+    mu_.unlock();\n     return req_id;\n   }\n \n   std::optional<size_t> InstallPullList(\n       std::vector<tsl::RCReference<ChunkDestination>> dests) {\n-    mu_.Lock();\n+    mu_.lock();\n     if (is_poisoned_) {\n       auto poison_status = poison_status_;\n       for (auto& dest : dests) {\n         dest->Poison(poison_status);\n       }\n-      mu_.Unlock();\n+      mu_.unlock();\n       return std::nullopt;\n     }\n     size_t req_id = next_req_id_;\n     for (auto& dest : dests) {\n       dests_[next_req_id_].dest = std::move(dest);\n       ++next_req_id_;\n     }\n-    mu_.Unlock();\n+    mu_.unlock();\n     return req_id;\n   }\n \n@@ -335,7 +335,7 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n       SocketNetworkState* state_;\n     };\n     {\n-      absl::MutexLock l(&mu_);\n+      absl::MutexLock l(mu_);\n       ++num_refs_;\n     }\n     table_->Handle(tsl::MakeRef<SocketConnectionState>(this), req,\n@@ -394,7 +394,7 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n \n   void DropRef() {\n     {\n-      absl::MutexLock l(&mu_);\n+      absl::MutexLock l(mu_);\n       CHECK_NE(num_refs_, 0);\n       --num_refs_;\n       ShutdownIfNeeded();\n@@ -409,11 +409,11 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n   }\n \n   void HandlePacket(const SocketTransferHalfClose& half_close) {\n-    mu_.Lock();\n+    mu_.lock();\n     CHECK(!peer_is_closed_);\n     peer_is_closed_ = true;\n     ShutdownIfNeeded();\n-    mu_.Unlock();\n+    mu_.unlock();\n   }\n \n   void ShutdownIfNeeded() {\n@@ -467,7 +467,7 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n         reinterpret_cast<const char*>(&header), sizeof(header)));\n     opacket += \"Injected Failure.\";\n     {\n-      absl::MutexLock l(&mu_);\n+      absl::MutexLock l(mu_);\n       frames_.push_back(std::move(opacket));\n     }\n     loop()->SendWake(this);\n@@ -484,7 +484,7 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n     absl::Status poison_status;\n     absl::flat_hash_map<uint64_t, DestState> dests;\n     {\n-      absl::MutexLock l(&mu_);\n+      absl::MutexLock l(mu_);\n       std::swap(dests, dests_);\n       poison_status = poison_status_;\n     }\n@@ -495,7 +495,7 @@ class SocketServer::SocketNetworkState : public PollEventLoop::Handler {\n \n   void Poison(absl::Status s) {\n     {\n-      absl::MutexLock l(&mu_);\n+      absl::MutexLock l(mu_);\n       is_poisoned_ = true;\n       shutdown(fd_, SHUT_RDWR);\n       poison_status_ = s;"
        },
        {
            "sha": "a450164c48eda73adf74b467e5f846ace80fa1ea",
            "filename": "third_party/xla/xla/python/transfer/socket_bulk_transport.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 26,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket_bulk_transport.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket_bulk_transport.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket_bulk_transport.cc?ref=d213727f0867d52fc1afc1f7f11a9b5c598b778e",
            "patch": "@@ -50,15 +50,15 @@ ZeroCopySendAckTable::ZeroCopySendAckTable() {\n }\n \n void ZeroCopySendAckTable::Send() {\n-  absl::MutexLock l(&mu_);\n+  absl::MutexLock l(mu_);\n   ++n_acks_in_batch_;\n   ack_ids_.push_back(acks_start_ + static_cast<uint32_t>(acks_.size() - 1));\n }\n \n uint32_t ZeroCopySendAckTable::Seal(absl::AnyInvocable<void() &&> on_done) {\n   uint32_t ack_id;\n   {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     ++n_acks_in_batch_;\n     auto& ack = acks_.back();\n     ack_id = static_cast<uint32_t>(ack_ids_.size() - 1) + ack_ids_start_;\n@@ -81,7 +81,7 @@ uint32_t ZeroCopySendAckTable::Seal(absl::AnyInvocable<void() &&> on_done) {\n void ZeroCopySendAckTable::HandleAck(uint32_t v) {\n   absl::AnyInvocable<void() &&> on_done;\n   {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     v -= ack_ids_start_;\n     auto& ack_id = ack_ids_[v];\n     auto& ack = acks_[*ack_id - acks_start_];\n@@ -100,7 +100,7 @@ void ZeroCopySendAckTable::HandleAck(uint32_t v) {\n }\n \n void ZeroCopySendAckTable::PretendCloseToRolloverForTests(uint32_t bump) {\n-  absl::MutexLock l(&mu_);\n+  absl::MutexLock l(mu_);\n   for (auto& ack : ack_ids_) {\n     if (ack.has_value()) {\n       *ack += bump;\n@@ -111,7 +111,7 @@ void ZeroCopySendAckTable::PretendCloseToRolloverForTests(uint32_t bump) {\n }\n \n std::pair<size_t, size_t> ZeroCopySendAckTable::GetTableSizes() {\n-  absl::MutexLock l(&mu_);\n+  absl::MutexLock l(mu_);\n   // Always 1 ahead.\n   return std::make_pair(acks_.size() - 1, ack_ids_.size() - 1);\n }\n@@ -158,7 +158,7 @@ absl::Status ZeroCopySendAckTable::HandleSocketErrors(int fd) {\n }\n \n void ZeroCopySendAckTable::ClearAll() {\n-  absl::MutexLock l(&mu_);\n+  absl::MutexLock l(mu_);\n   for (auto& ack : acks_) {\n     if (ack.on_done) {\n       std::move(ack.on_done)();\n@@ -322,7 +322,7 @@ class SendConnectionHandler : public PollEventLoop::Handler {\n void SharedSendWorkQueue::ScheduleWork(\n     SendConnectionHandler* handler,\n     aux::BulkTransportInterface::SendMessage msg) {\n-  absl::MutexLock l(&mu_);\n+  absl::MutexLock l(mu_);\n   work_items_.push_back({handler, std::move(msg)});\n }\n \n@@ -331,12 +331,12 @@ void SharedSendWorkQueue::Run() {\n     auto cond = [this]() { return !work_items_.empty() || shutdown_; };\n     mu_.LockWhen(absl::Condition(&cond));\n     if (work_items_.empty() && shutdown_) {\n-      mu_.Unlock();\n+      mu_.unlock();\n       break;\n     }\n     auto work = std::move(work_items_.front());\n     work_items_.pop_front();\n-    mu_.Unlock();\n+    mu_.unlock();\n     work.handler->DoSend(std::move(work.msg));\n   }\n   aux::PollEventLoop::GetDefault()->Schedule(\n@@ -347,7 +347,7 @@ void SharedSendWorkQueue::Run() {\n std::shared_ptr<SharedSendWorkQueue> SharedSendWorkQueue::Start() {\n   auto result = std::shared_ptr<SharedSendWorkQueue>(\n       new SharedSendWorkQueue(), [](SharedSendWorkQueue* result) {\n-        absl::MutexLock l(&result->mu_);\n+        absl::MutexLock l(result->mu_);\n         result->shutdown_ = true;\n       });\n   result->thread_ =\n@@ -357,40 +357,40 @@ std::shared_ptr<SharedSendWorkQueue> SharedSendWorkQueue::Start() {\n }\n \n void SharedSendMsgQueue::ReportReadyToSend(SendConnectionHandler* handler) {\n-  mu_.Lock();\n+  mu_.lock();\n   if (!work_items_.empty()) {\n     auto msg = std::move(work_items_.front());\n     work_items_.pop_front();\n-    mu_.Unlock();\n+    mu_.unlock();\n     handler->ScheduleSendWork(std::move(msg));\n   } else if (shutdown_) {\n-    mu_.Unlock();\n+    mu_.unlock();\n     handler->NoMoreMessages();\n   } else {\n     handlers_.push_back(handler);\n-    mu_.Unlock();\n+    mu_.unlock();\n   }\n }\n \n void SharedSendMsgQueue::ScheduleSendWork(\n     aux::BulkTransportInterface::SendMessage msg) {\n-  mu_.Lock();\n+  mu_.lock();\n   DCHECK(!shutdown_);\n   if (work_items_.empty() && !handlers_.empty()) {\n     auto* handler = handlers_.front();\n     handlers_.pop_front();\n-    mu_.Unlock();\n+    mu_.unlock();\n     handler->ScheduleSendWork(std::move(msg));\n   } else {\n     work_items_.push_back(std::move(msg));\n-    mu_.Unlock();\n+    mu_.unlock();\n   }\n }\n \n void SharedSendMsgQueue::NoMoreMessages() {\n   std::deque<SendConnectionHandler*> handlers;\n   {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     shutdown_ = true;\n     if (work_items_.empty()) {\n       std::swap(handlers_, handlers);\n@@ -425,12 +425,12 @@ void RecvThreadState::DoRecvWork() {\n     };\n     recv_mu_.LockWhen(absl::Condition(&cond));\n     if (recv_work_items_.empty() && recv_shutdown_) {\n-      recv_mu_.Unlock();\n+      recv_mu_.unlock();\n       break;\n     }\n     auto work = std::move(recv_work_items_.front());\n     recv_work_items_.pop_front();\n-    recv_mu_.Unlock();\n+    recv_mu_.unlock();\n     auto status = HandleRecvItem(work, zc_send_count, non_zc_send_count);\n     if (!status.ok()) {\n       std::move(work.on_recv)(status);\n@@ -447,7 +447,7 @@ void RecvThreadState::ScheduleRecvWork(\n     absl::AnyInvocable<\n         void(absl::StatusOr<aux::BulkTransportInterface::Message> msg) &&>\n         on_recv) {\n-  absl::MutexLock l(&recv_mu_);\n+  absl::MutexLock l(recv_mu_);\n   recv_work_item work;\n   work.recv_size = recv_size;\n   work.fd = fd;\n@@ -460,7 +460,7 @@ std::shared_ptr<RecvThreadState> RecvThreadState::Create(\n   auto result = std::shared_ptr<RecvThreadState>(\n       new RecvThreadState(allocator, uallocator), [](RecvThreadState* result) {\n         {\n-          absl::MutexLock l(&result->recv_mu_);\n+          absl::MutexLock l(result->recv_mu_);\n           result->recv_shutdown_ = true;\n         }\n       });\n@@ -572,7 +572,7 @@ class SocketBulkTransport : public BulkTransportInterface {\n             absl::AnyInvocable<void(absl::StatusOr<Message> msg) &&> on_recv)\n       override {\n     auto& conn = connections_[bond_id];\n-    absl::MutexLock l(&conn->mu);\n+    absl::MutexLock l(conn->mu);\n     if (conn->fd == -1) {\n       conn->pending_recvs.push_back({size, std::move(on_recv)});\n     } else {\n@@ -597,7 +597,7 @@ class SocketBulkTransport : public BulkTransportInterface {\n       SharedSendMsgQueue::StartSubConnectionSender(\n           accept_fd, connection_id, send_msg_queue, send_work_queue);\n       {\n-        absl::MutexLock l(&mu);\n+        absl::MutexLock l(mu);\n         fd = accept_fd;\n         for (auto& pending_recv : pending_recvs) {\n           thread_state->ScheduleRecvWork(pending_recv.size, accept_fd,\n@@ -725,7 +725,7 @@ class SocketBulkTransportFactory : public BulkTransportFactory {\n     absl::flat_hash_map<uint64_t, std::shared_ptr<SocketBulkTransport::Conn>>\n         waiting_for_connect;\n     void DoAccept(int sockfd, uint64_t uuid) {\n-      absl::MutexLock l(&mu);\n+      absl::MutexLock l(mu);\n       auto it = waiting_for_connect.find(uuid);\n       if (it == waiting_for_connect.end()) {\n         close(sockfd);\n@@ -737,7 +737,7 @@ class SocketBulkTransportFactory : public BulkTransportFactory {\n     }\n     uint64_t AllocateUUIDs(\n         std::vector<std::shared_ptr<SocketBulkTransport::Conn>> connections) {\n-      absl::MutexLock l(&mu);\n+      absl::MutexLock l(mu);\n       uint64_t result = next_id;\n       next_id += connections.size();\n       for (uint64_t i = 0; i < connections.size(); ++i) {"
        },
        {
            "sha": "421df04c319e8ff74eccb80e3622f4ab84e0e61a",
            "filename": "third_party/xla/xla/python/transfer/socket_bulk_transport_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket_bulk_transport_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket_bulk_transport_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fsocket_bulk_transport_test.cc?ref=d213727f0867d52fc1afc1f7f11a9b5c598b778e",
            "patch": "@@ -125,7 +125,7 @@ TEST(SendQueue, SendAndRecvQueuesArtificialLimit) {\n     msg.size = txt_msg.size();\n     msg.on_send = [](int id, size_t size) {};\n     msg.on_done = [&mu, &send_count]() {\n-      absl::MutexLock l(&mu);\n+      absl::MutexLock l(mu);\n       --send_count;\n     };\n     msg_queue->ScheduleSendWork(std::move(msg));\n@@ -149,7 +149,7 @@ TEST(SendQueue, SendAndRecvQueuesArtificialLimit) {\n     std::move(recv_msg->on_done)();\n   }\n   {\n-    absl::MutexLock l(&mu);\n+    absl::MutexLock l(mu);\n     auto cond = [&]() { return send_count == 0; };\n     mu.Await(absl::Condition(&cond));\n   }\n@@ -204,11 +204,11 @@ TEST(SocketBulkTransportFactoryTest, SendAndRecvWithFactory) {\n     msg.data = txt_msgs[i].data();\n     msg.size = txt_msgs[i].size();\n     msg.on_send = [&, i](int id, size_t size) {\n-      absl::MutexLock l(&mu);\n+      absl::MutexLock l(mu);\n       send_queue.push_back({i, id});\n     };\n     msg.on_done = [&mu, &send_count]() {\n-      absl::MutexLock l(&mu);\n+      absl::MutexLock l(mu);\n       --send_count;\n     };\n     bulk_transporta->Send(std::move(msg));\n@@ -220,7 +220,7 @@ TEST(SocketBulkTransportFactoryTest, SendAndRecvWithFactory) {\n     int bond_id = -1;\n     int msg_id = -1;\n     {\n-      absl::MutexLock l(&mu);\n+      absl::MutexLock l(mu);\n       auto cond = [&]() { return !send_queue.empty(); };\n       mu.Await(absl::Condition(&cond));\n       std::tie(msg_id, bond_id) = send_queue.front();\n@@ -241,7 +241,7 @@ TEST(SocketBulkTransportFactoryTest, SendAndRecvWithFactory) {\n     std::move(recv_msg->on_done)();\n   }\n   {\n-    absl::MutexLock l(&mu);\n+    absl::MutexLock l(mu);\n     auto cond = [&]() { return send_count == 0; };\n     mu.Await(absl::Condition(&cond));\n   }"
        },
        {
            "sha": "5a356e6d07fe9bb75c829c4868f1a0aeab236e87",
            "filename": "third_party/xla/xla/python/transfer/streaming.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fstreaming.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fstreaming.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fstreaming.cc?ref=d213727f0867d52fc1afc1f7f11a9b5c598b778e",
            "patch": "@@ -51,7 +51,7 @@ class StringFutureChunkDestination : public aux::ChunkDestination {\n   absl::Status Put(const void* data, int64_t offset, size_t size,\n                    absl::AnyInvocable<void() &&> on_done) override {\n     {\n-      absl::MutexLock l(&mu_);\n+      absl::MutexLock l(mu_);\n       chunks_.emplace_back(\n           offset, std::string(reinterpret_cast<const char*>(data), size));\n     }\n@@ -62,7 +62,7 @@ class StringFutureChunkDestination : public aux::ChunkDestination {\n   void Poison(absl::Status s) override { CHECK_OK(s); }\n \n   absl::StatusOr<std::string> ConsumeFinalResult() {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     std::sort(chunks_.begin(), chunks_.end());\n \n     std::string result;\n@@ -102,11 +102,11 @@ class LocalBulkTransport : public BulkTransportInterface {\n       : send_q_(std::move(send_q)), recv_q_(std::move(recv_q)) {}\n \n   ~LocalBulkTransport() override {\n-    absl::MutexLock l(&send_q_->mu);\n+    absl::MutexLock l(send_q_->mu);\n     send_q_->status = absl::InternalError(\"Connection failure for local bond.\");\n   }\n   void Send(SendMessage msg) override {\n-    absl::MutexLock l(&send_q_->mu);\n+    absl::MutexLock l(send_q_->mu);\n     std::move(msg.on_send)(0, msg.size);\n     send_q_->buffers.push_back(std::move(msg));\n   }\n@@ -115,7 +115,7 @@ class LocalBulkTransport : public BulkTransportInterface {\n       override {\n     absl::StatusOr<Message> result;\n     {\n-      absl::MutexLock l(&recv_q_->mu);\n+      absl::MutexLock l(recv_q_->mu);\n       auto cond = [&]() {\n         return !recv_q_->buffers.empty() || !recv_q_->status.ok();\n       };\n@@ -161,7 +161,7 @@ class LocalBulkTransportFactory : public BulkTransportFactory {\n  public:\n   BulkTransportInitResult InitBulkTransport() override {\n     BulkTransportInitResult out;\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     out.request.set_bulk_transport_impl_kind(\n         SocketTransferEstablishBulkTransport::LOCAL);\n     out.request.add_bulk_transport_uuid(next_bulk_transport_id_);\n@@ -182,7 +182,7 @@ class LocalBulkTransportFactory : public BulkTransportFactory {\n       const SocketTransferEstablishBulkTransport& remote_bulk_transport_info)\n       override {\n     BulkTransportRecvResult out;\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     CHECK_EQ(remote_bulk_transport_info.bulk_transport_impl_kind(),\n              SocketTransferEstablishBulkTransport::LOCAL);\n     CHECK_EQ(remote_bulk_transport_info.bulk_transport_uuid_size(), 1);\n@@ -236,10 +236,10 @@ SlabAllocator::Allocation SlabAllocator::Allocate(size_t size) {\n   state_->slots.pop_back();\n   result.on_done =\n       absl::AnyInvocable<void() &&>([state = state_, data = result.data]() {\n-        absl::MutexLock l(&state->mu);\n+        absl::MutexLock l(state->mu);\n         state->slots.push_back(data);\n       });\n-  state_->mu.Unlock();\n+  state_->mu.unlock();\n   return result;\n }\n \n@@ -287,7 +287,7 @@ absl::StatusOr<std::shared_ptr<absl::Span<uint8_t>>> AllocateAlignedMemory(\n void PullTable::AwaitPull(uint64_t uuid, tsl::RCReference<Entry> entry) {\n   std::vector<PausedFetch> paused_fetches;\n   {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     auto it = paused_fetches_.find(uuid);\n     if (it != paused_fetches_.end()) {\n       paused_fetches = std::move(it->second);\n@@ -305,7 +305,7 @@ void PullTable::Handle(tsl::RCReference<ConnectionState> state,\n                        size_t base_req_id) {\n   tsl::RCReference<Entry> entry;\n   {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     auto it = entries_.find(req.uuid());\n     if (it == entries_.end()) {\n       PausedFetch fetch;\n@@ -318,7 +318,7 @@ void PullTable::Handle(tsl::RCReference<ConnectionState> state,\n     entry = it->second;\n   }\n   if (entry->Handle(std::move(state), req, base_req_id)) {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     auto it = entries_.find(req.uuid());\n     entries_.erase(it);\n   }"
        },
        {
            "sha": "d5c7f8281e4d6080a2d751d7a3c94fb3253cd4b3",
            "filename": "third_party/xla/xla/python/transfer/streaming_ifrt.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fstreaming_ifrt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fstreaming_ifrt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fstreaming_ifrt.cc?ref=d213727f0867d52fc1afc1f7f11a9b5c598b778e",
            "patch": "@@ -131,7 +131,7 @@ void PremappedCopierState::ScheduleCopy(\n                            on_done) {\n   WorkList work_list;\n   {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     work_queue_.push_back(WorkQueueItem{std::move(blob),\n                                         nullptr,\n                                         base_seq_id_ + work_queue_.size(),\n@@ -146,7 +146,7 @@ void PremappedCopierState::ScheduleCopy(\n void PremappedCopierState::ReturnBuffer(void* buffer) {\n   WorkList work_list;\n   {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     available_copy_offsets_.push_back(buffer);\n     work_list = FindWorkLocked();\n   }\n@@ -176,7 +176,7 @@ void PremappedCopierState::StartWorkUnlocked(const WorkList& work_list) {\n                   work_item](absl::Status s) {\n           WorkList work_list2;\n           {\n-            absl::MutexLock l(&mu_);\n+            absl::MutexLock l(mu_);\n             --num_parallel_copies_;\n             work_item->is_ready = true;\n             work_item->result_status = s;\n@@ -278,7 +278,7 @@ class SlicedRawBufferChunkDestination : public ChunkDestination {\n           offset, size, slice_size_));\n     }\n     {\n-      absl::MutexLock l(&mu_);\n+      absl::MutexLock l(mu_);\n       TF_RETURN_IF_ERROR(saved_status_);\n       sent_bytes_ += size;\n     }\n@@ -287,7 +287,7 @@ class SlicedRawBufferChunkDestination : public ChunkDestination {\n     future.OnReady([state = tsl::FormRef(this), on_done = std::move(on_done),\n                     size](absl::Status s) mutable {\n       {\n-        absl::MutexLock l(&state->mu_);\n+        absl::MutexLock l(state->mu_);\n         state->copied_bytes_ += size;\n         state->SendResultsIfDone(std::move(s));\n       }\n@@ -308,7 +308,7 @@ class SlicedRawBufferChunkDestination : public ChunkDestination {\n   }\n \n   void Poison(absl::Status s) override {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     if (slice_size_ == sent_bytes_) {\n       return;\n     }"
        },
        {
            "sha": "5a82527a8176f77b13688bbc5f6e5f94fd15b2d8",
            "filename": "third_party/xla/xla/python/transfer/streaming_ifrt.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fstreaming_ifrt.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fstreaming_ifrt.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fstreaming_ifrt.h?ref=d213727f0867d52fc1afc1f7f11a9b5c598b778e",
            "patch": "@@ -198,7 +198,7 @@ class IsLastSemaphore {\n   auto DoWork(size_t value, T&& cb) -> absl::Status {\n     bool is_last;\n     {\n-      absl::MutexLock l(&mu_);\n+      absl::MutexLock l(mu_);\n       if (is_done_) {\n         return absl::OkStatus();\n       }\n@@ -213,15 +213,15 @@ class IsLastSemaphore {\n       }\n     }\n     auto cleanup = absl::MakeCleanup([&]() {\n-      absl::MutexLock l(&mu_);\n+      absl::MutexLock l(mu_);\n       counter_ -= value;\n     });\n     return cb(is_last);\n   }\n \n   // Return true if this is the first call to poison.\n   bool Poison() {\n-    absl::MutexLock l(&mu_);\n+    absl::MutexLock l(mu_);\n     if (is_done_) {\n       return false;\n     }"
        },
        {
            "sha": "d49cd6c7da2390668b1b118b94ff745fe5ed8c49",
            "filename": "third_party/xla/xla/python/transfer/streaming_ifrt_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fstreaming_ifrt_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fstreaming_ifrt_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fstreaming_ifrt_test.cc?ref=d213727f0867d52fc1afc1f7f11a9b5c598b778e",
            "patch": "@@ -125,7 +125,7 @@ void CopyIntoDest(tsl::RCReference<ChunkDestination> dest,\n                             absl::StatusOr<void*> buf,\n                             const DmaCopyChunk& chunk) {\n           CHECK_OK(buf.status());\n-          absl::MutexLock l(&mu);\n+          absl::MutexLock l(mu);\n           local_queue.push_back(LocalQueueInfo{*buf, chunk.offset, chunk.size});\n         });\n   }\n@@ -134,7 +134,7 @@ void CopyIntoDest(tsl::RCReference<ChunkDestination> dest,\n     mu.LockWhen(absl::Condition(&cond));\n     auto state = local_queue.front();\n     local_queue.pop_front();\n-    mu.Unlock();\n+    mu.unlock();\n     TF_ASSERT_OK(\n         dest->Put(state.buff, state.offset, state.size,\n                   [cstate, buf = state.buff]() { cstate->ReturnBuffer(buf); }));\n@@ -246,10 +246,10 @@ TEST(Semaphore, Async) {\n   auto thread_wait_flip = [&thread_id, &mu](size_t my_thread_id) {\n     auto cond = [&] { return thread_id == my_thread_id; };\n     mu.LockWhen(absl::Condition(&cond));\n-    mu.Unlock();\n+    mu.unlock();\n   };\n   auto thread_flip = [&thread_id, &mu](size_t my_thread_id) {\n-    absl::MutexLock l(&mu);\n+    absl::MutexLock l(mu);\n     thread_id = 1 - thread_id;\n   };\n "
        },
        {
            "sha": "443a63c05552ff1b014104d1609404c65903b160",
            "filename": "third_party/xla/xla/python/transfer/streaming_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fstreaming_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d213727f0867d52fc1afc1f7f11a9b5c598b778e/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fstreaming_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fstreaming_test.cc?ref=d213727f0867d52fc1afc1f7f11a9b5c598b778e",
            "patch": "@@ -49,7 +49,7 @@ class LocalConnectionState : public ConnectionState {\n             bool is_largest, absl::AnyInvocable<void() &&> on_done) override {\n     tsl::RCReference<ChunkDestination> dest;\n     {\n-      absl::MutexLock l(&mu_);\n+      absl::MutexLock l(mu_);\n       auto it = dests_.find(req_id);\n       CHECK(it != dests_.end());\n       if (is_largest) {\n@@ -71,7 +71,7 @@ class LocalConnectionState : public ConnectionState {\n             tsl::RCReference<ChunkDestination> dest) {\n     size_t req_id;\n     {\n-      absl::MutexLock l(&mu_);\n+      absl::MutexLock l(mu_);\n       dests_[next_req_id_].dest = std::move(dest);\n       req_id = next_req_id_;\n       ++next_req_id_;\n@@ -213,7 +213,7 @@ TEST(SlabAllocator, BasicSubAllocations) {\n     auto thread = std::unique_ptr<tsl::Thread>(\n         tsl::Env::Default()->StartThread({}, \"test-thread\", [&] {\n           for (size_t i = 0; i < 200; ++i) {\n-            absl::MutexLock l(&mu);\n+            absl::MutexLock l(mu);\n             auto cond = [&]() {\n               return allocs.size() >= std::min(static_cast<size_t>(200 - i),\n                                                static_cast<size_t>(4));\n@@ -226,7 +226,7 @@ TEST(SlabAllocator, BasicSubAllocations) {\n     SlabAllocator allocator(alloc, 4096);\n     for (size_t i = 0; i < 200; ++i) {\n       auto alloc = allocator.Allocate(allocator.max_allocation_size());\n-      absl::MutexLock l(&mu);\n+      absl::MutexLock l(mu);\n       allocs.push_back(std::move(alloc));\n     }\n   }"
        }
    ],
    "stats": {
        "total": 174,
        "additions": 87,
        "deletions": 87
    }
}