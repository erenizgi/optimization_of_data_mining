{
    "author": "ermilovmaxim",
    "message": "Add Shape to DynamicSliceThunk buffer_uses\n\nModify Thunk's serialization\n\nPiperOrigin-RevId: 845890906",
    "sha": "a8f27858b8af4ddf767059ee152f28820c30c855",
    "files": [
        {
            "sha": "6bd21df61eb9cd5a69d3b866c401eb8dac6152ba",
            "filename": "third_party/xla/xla/backends/gpu/codegen/custom.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 26,
            "changes": 53,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc?ref=a8f27858b8af4ddf767059ee152f28820c30c855",
            "patch": "@@ -308,7 +308,7 @@ absl::Status CollectSliceInfo(\n     std::vector<std::optional<std::vector<DynamicSliceThunk::Offset>>>& offsets,\n     std::vector<std::optional<Shape>>& orig_shapes,\n     std::vector<std::optional<Shape>>& sliced_shapes,\n-    std::vector<std::optional<uint64_t>>& offset_byte_sizes,\n+    std::vector<std::optional<PrimitiveType>>& offset_primitive_types,\n     std::vector<std::unique_ptr<HloModule>>& extracted_offset_modules,\n     unsigned arg_idx, bool can_compute_indvar_on_host,\n     std::optional<const HloInstruction*> while_op,\n@@ -381,8 +381,8 @@ absl::Status CollectSliceInfo(\n   sliced_shapes[arg_idx] = DynCast<HloDynamicSliceInstruction>(arg_slice_instr)\n                                ? arg_slice_instr->shape()\n                                : arg_slice_instr->operand(1)->shape();\n-  offset_byte_sizes[arg_idx] = ShapeUtil::ByteSizeOfPrimitiveType(\n-      arg_slice_instr->index_operands().front()->shape().element_type());\n+  offset_primitive_types[arg_idx] =\n+      arg_slice_instr->index_operands().front()->shape().element_type();\n \n   return absl::OkStatus();\n }\n@@ -556,7 +556,8 @@ absl::StatusOr<FusionEmissionResult> EmitGemm(\n       offset_buffer_indices(4, std::nullopt);\n   std::vector<std::optional<Shape>> orig_shapes(4, std::nullopt);\n   std::vector<std::optional<Shape>> sliced_shapes(4, std::nullopt);\n-  std::vector<std::optional<uint64_t>> offset_byte_sizes(4, std::nullopt);\n+  std::vector<std::optional<PrimitiveType>> offset_primitive_types(\n+      4, std::nullopt);\n \n   std::vector<HloInstruction*> slice_instrs(4, nullptr);\n \n@@ -594,7 +595,7 @@ absl::StatusOr<FusionEmissionResult> EmitGemm(\n                       /*shape_idx=*/{}, arg_idx));\n   TF_RETURN_IF_ERROR(CollectSliceInfo(\n       buffer_assignment, fusion, absl::Span<HloInstruction*>(slice_instrs),\n-      offset_buffer_indices, orig_shapes, sliced_shapes, offset_byte_sizes,\n+      offset_buffer_indices, orig_shapes, sliced_shapes, offset_primitive_types,\n       extracted_offset_modules, arg_idx++, can_compute_indvar_on_host, while_op,\n       indvar_idx, inlined_module));\n \n@@ -605,7 +606,7 @@ absl::StatusOr<FusionEmissionResult> EmitGemm(\n                       /*shape_idx=*/{}, arg_idx));\n   TF_RETURN_IF_ERROR(CollectSliceInfo(\n       buffer_assignment, fusion, absl::Span<HloInstruction*>(slice_instrs),\n-      offset_buffer_indices, orig_shapes, sliced_shapes, offset_byte_sizes,\n+      offset_buffer_indices, orig_shapes, sliced_shapes, offset_primitive_types,\n       extracted_offset_modules, arg_idx++, can_compute_indvar_on_host, while_op,\n       indvar_idx, inlined_module));\n \n@@ -624,9 +625,9 @@ absl::StatusOr<FusionEmissionResult> EmitGemm(\n                                slice_instrs, /*shape_idx=*/{}, arg_idx));\n     TF_RETURN_IF_ERROR(CollectSliceInfo(\n         buffer_assignment, fusion, absl::Span<HloInstruction*>(slice_instrs),\n-        offset_buffer_indices, orig_shapes, sliced_shapes, offset_byte_sizes,\n-        extracted_offset_modules, arg_idx, can_compute_indvar_on_host, while_op,\n-        indvar_idx, inlined_module));\n+        offset_buffer_indices, orig_shapes, sliced_shapes,\n+        offset_primitive_types, extracted_offset_modules, arg_idx,\n+        can_compute_indvar_on_host, while_op, indvar_idx, inlined_module));\n   } else {\n     TF_ASSIGN_OR_RETURN(\n         output,\n@@ -635,9 +636,9 @@ absl::StatusOr<FusionEmissionResult> EmitGemm(\n                        arg_idx));\n     TF_RETURN_IF_ERROR(CollectSliceInfo(\n         buffer_assignment, fusion, absl::Span<HloInstruction*>(slice_instrs),\n-        offset_buffer_indices, orig_shapes, sliced_shapes, offset_byte_sizes,\n-        extracted_offset_modules, arg_idx++, can_compute_indvar_on_host,\n-        while_op, indvar_idx, inlined_module));\n+        offset_buffer_indices, orig_shapes, sliced_shapes,\n+        offset_primitive_types, extracted_offset_modules, arg_idx++,\n+        can_compute_indvar_on_host, while_op, indvar_idx, inlined_module));\n \n     // TODO(vuson): If we want to support slices of workspace, we'd need to\n     // start `HloFindIf` with `get-tuple-element` with the right index.\n@@ -646,9 +647,9 @@ absl::StatusOr<FusionEmissionResult> EmitGemm(\n                                       /*index=*/{kGEMMWorkspaceBufferIndex}));\n     TF_RETURN_IF_ERROR(CollectSliceInfo(\n         buffer_assignment, fusion, absl::Span<HloInstruction*>(slice_instrs),\n-        offset_buffer_indices, orig_shapes, sliced_shapes, offset_byte_sizes,\n-        extracted_offset_modules, arg_idx, can_compute_indvar_on_host, while_op,\n-        indvar_idx, inlined_module));\n+        offset_buffer_indices, orig_shapes, sliced_shapes,\n+        offset_primitive_types, extracted_offset_modules, arg_idx,\n+        can_compute_indvar_on_host, while_op, indvar_idx, inlined_module));\n     fake_allocations[arg_idx] = BufferAllocation(\n         /*index=*/arg_idx, workspace->size(), /*color=*/0);\n     slice_workspace_fake = BufferAllocation::Slice(&fake_allocations[arg_idx],\n@@ -723,7 +724,7 @@ absl::StatusOr<FusionEmissionResult> EmitGemm(\n         thunk_info, std::make_unique<ThunkSequence>(std::move(seq)),\n         std::move(arguments), std::move(fake_allocations),\n         std::move(offset_buffer_indices), std::move(orig_shapes),\n-        std::move(sliced_shapes), std::move(offset_byte_sizes),\n+        std::move(sliced_shapes), std::move(offset_primitive_types),\n         std::move(offset_modules_metadata));\n   } else {\n     thunk = std::make_unique<GemmThunk>(thunk_info, std::move(config),\n@@ -777,8 +778,8 @@ absl::StatusOr<FusionEmissionResult> EmitCustomCall(\n       num_args, std::nullopt);\n   std::vector<std::optional<Shape>> orig_shapes(num_args, std::nullopt);\n   std::vector<std::optional<Shape>> sliced_shapes(num_args, std::nullopt);\n-  std::vector<std::optional<uint64_t>> offset_byte_sizes(num_args,\n-                                                         std::nullopt);\n+  std::vector<std::optional<PrimitiveType>> offset_primitive_types(\n+      num_args, std::nullopt);\n \n   std::vector<HloInstruction*> slice_instrs(num_args, nullptr);\n   std::vector<std::optional<BufferAllocation::Slice>> arguments;\n@@ -830,7 +831,7 @@ absl::StatusOr<FusionEmissionResult> EmitCustomCall(\n           TF_RETURN_IF_ERROR(CollectSliceInfo(\n               buffer_assignment, fusion,\n               absl::Span<HloInstruction*>(slice_instrs), offsets, orig_shapes,\n-              sliced_shapes, offset_byte_sizes, extracted_offset_modules,\n+              sliced_shapes, offset_primitive_types, extracted_offset_modules,\n               arg_idx++, can_compute_indvar_on_host, while_op, indvar_idx,\n               inlined_module));\n \n@@ -858,7 +859,7 @@ absl::StatusOr<FusionEmissionResult> EmitCustomCall(\n         TF_RETURN_IF_ERROR(CollectSliceInfo(\n             buffer_assignment, fusion,\n             absl::Span<HloInstruction*>(slice_instrs), offsets, orig_shapes,\n-            sliced_shapes, offset_byte_sizes, extracted_offset_modules,\n+            sliced_shapes, offset_primitive_types, extracted_offset_modules,\n             arg_idx++, can_compute_indvar_on_host, while_op, indvar_idx,\n             inlined_module));\n \n@@ -1040,7 +1041,7 @@ absl::StatusOr<FusionEmissionResult> EmitCustomCall(\n         thunk_info, std::make_unique<ThunkSequence>(std::move(seq)),\n         std::move(arguments), std::move(fake_allocations), std::move(offsets),\n         std::move(orig_shapes), std::move(sliced_shapes),\n-        std::move(offset_byte_sizes), std::move(offset_modules_metadata));\n+        std::move(offset_primitive_types), std::move(offset_modules_metadata));\n   } else {\n     TF_ASSIGN_OR_RETURN(\n         thunk, found_ffi_handler\n@@ -1073,7 +1074,7 @@ struct SliceDataForCollectives {\n   std::vector<std::optional<std::vector<DynamicSliceThunk::Offset>>>\n       offset_buffer_indices;\n   std::vector<std::optional<Shape>> orig_shapes, sliced_shapes;\n-  std::vector<std::optional<uint64_t>> offset_byte_sizes;\n+  std::vector<std::optional<PrimitiveType>> offset_primitive_types;\n   std::vector<std::unique_ptr<HloModule>> extracted_offset_modules;\n   std::unique_ptr<HloModule> init_module, update_module;\n   bool isDynamic, can_compute_indvar_on_host;\n@@ -1085,7 +1086,7 @@ struct SliceDataForCollectives {\n         offset_buffer_indices(num_args, std::nullopt),\n         orig_shapes(num_args, std::nullopt),\n         sliced_shapes(num_args, std::nullopt),\n-        offset_byte_sizes(num_args, std::nullopt),\n+        offset_primitive_types(num_args, std::nullopt),\n         init_module(nullptr),\n         update_module(nullptr),\n         isDynamic(false),\n@@ -1138,7 +1139,7 @@ CollectSliceArgumentMetadataForCollectives(\n         buffer_assignment, fusion_instr,\n         /*slice_instrs=*/absl::Span<HloInstruction*>(slice_data.slice_instrs),\n         /*offsets=*/slice_data.offset_buffer_indices, slice_data.orig_shapes,\n-        slice_data.sliced_shapes, slice_data.offset_byte_sizes,\n+        slice_data.sliced_shapes, slice_data.offset_primitive_types,\n         slice_data.extracted_offset_modules, arg_idx,\n         slice_data.can_compute_indvar_on_host, while_op, indvar_idx,\n         inlined_module));\n@@ -1165,7 +1166,7 @@ CollectSliceArgumentMetadataForCollectives(\n         buffer_assignment, fusion_instr,\n         /*slice_instrs=*/absl::Span<HloInstruction*>(slice_data.slice_instrs),\n         /*offsets=*/slice_data.offset_buffer_indices, slice_data.orig_shapes,\n-        slice_data.sliced_shapes, slice_data.offset_byte_sizes,\n+        slice_data.sliced_shapes, slice_data.offset_primitive_types,\n         slice_data.extracted_offset_modules, arg_idx,\n         slice_data.can_compute_indvar_on_host, while_op, indvar_idx,\n         inlined_module));\n@@ -1359,7 +1360,7 @@ absl::StatusOr<FusionEmissionResult> EmitCollective(\n         std::move(slice_data.arguments), std::move(slice_data.fake_allocations),\n         std::move(slice_data.offset_buffer_indices),\n         std::move(slice_data.orig_shapes), std::move(slice_data.sliced_shapes),\n-        std::move(slice_data.offset_byte_sizes),\n+        std::move(slice_data.offset_primitive_types),\n         std::move(offset_modules_metadata));\n     result.thunks.push_back(std::move(thunk));\n   } else {"
        },
        {
            "sha": "f0aad60b990ab020e96452e1423f29ea76463ec2",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=a8f27858b8af4ddf767059ee152f28820c30c855",
            "patch": "@@ -245,6 +245,7 @@ cc_library(\n         \"//xla:literal_util\",\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n+        \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/evaluator:hlo_evaluator\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/runtime:buffer_use\","
        },
        {
            "sha": "0ede07cb6a30bdbbd81fdf74a0ecd323b37ad44a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc?ref=a8f27858b8af4ddf767059ee152f28820c30c855",
            "patch": "@@ -2578,7 +2578,7 @@ DynamicSliceFusionCmd::DynamicSliceFusionCmd(\n     std::vector<std::optional<std::vector<DynamicSliceThunk::Offset>>> offsets,\n     std::vector<std::optional<Shape>> orig_shapes,\n     std::vector<std::optional<Shape>> sliced_shapes,\n-    std::vector<std::optional<uint64_t>> offset_byte_sizes,\n+    std::vector<std::optional<PrimitiveType>> offset_primitive_types,\n     std::optional<\n         const DynamicSliceThunk::OffsetAsFunctionOfIndvarModulesMetadata*>\n         offset_as_function_of_indvar_metadata)\n@@ -2588,15 +2588,15 @@ DynamicSliceFusionCmd::DynamicSliceFusionCmd(\n       offset_as_function_of_indvar_metadata_(\n           std::move(offset_as_function_of_indvar_metadata)) {\n   // Zip all arguments together to create a list of SliceDef.\n-  for (auto [arg, offset, orig_shape, sliced_shape, offset_byte_size] :\n+  for (auto [arg, offset, orig_shape, sliced_shape, offset_primitive_type] :\n        llvm::zip_equal(arguments, offsets, orig_shapes, sliced_shapes,\n-                       offset_byte_sizes)) {\n+                       offset_primitive_types)) {\n     slices_.push_back(DynamicSliceThunk::SliceDef{\n         std::move(arg),\n         std::move(offset),\n         std::move(orig_shape),\n         std::move(sliced_shape),\n-        std::move(offset_byte_size),\n+        std::move(offset_primitive_type),\n     });\n   }\n \n@@ -2657,7 +2657,7 @@ absl::Status DynamicSliceFusionCmd::Prepare(\n       TF_RET_CHECK(slice.embedded_thunk_argument.has_value());\n       TF_RET_CHECK(slice.orig_shape.has_value());\n       TF_RET_CHECK(slice.sliced_shape.has_value());\n-      TF_RET_CHECK(slice.offset_byte_size.has_value());\n+      TF_RET_CHECK(slice.offset_primitive_type.has_value());\n       TF_RET_CHECK(slice.orig_shape->IsArray());\n       TF_RET_CHECK(slice.sliced_shape->IsArray());\n       TF_RET_CHECK(slice.offsets->size() ==\n@@ -2771,8 +2771,9 @@ absl::StatusOr<const se::CommandBuffer::Command*> DynamicSliceFusionCmd::Record(\n \n         // Copy the `offset_idx`-th component of the offset for the\n         // `argument_idx`-th argument from device to host.\n-        TF_RETURN_IF_ERROR(\n-            stream.Memcpy(offset_dst, offset_src, *slice.offset_byte_size));\n+        TF_RETURN_IF_ERROR(stream.Memcpy(\n+            offset_dst, offset_src,\n+            ShapeUtil::ByteSizeOfPrimitiveType(*slice.offset_primitive_type)));\n         ++num_transfers;\n       }\n     }"
        },
        {
            "sha": "0b666803dcf0585dec10132d7798afc177698148",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h?ref=a8f27858b8af4ddf767059ee152f28820c30c855",
            "patch": "@@ -1284,7 +1284,7 @@ class DynamicSliceFusionCmd : public CommandBufferCmd {\n           offsets,\n       std::vector<std::optional<Shape>> orig_shapes,\n       std::vector<std::optional<Shape>> sliced_shapes,\n-      std::vector<std::optional<uint64_t>> offset_byte_sizes,\n+      std::vector<std::optional<PrimitiveType>> offset_primitive_types,\n       std::optional<\n           const DynamicSliceThunk::OffsetAsFunctionOfIndvarModulesMetadata*>\n           offset_as_function_of_indvar_metadata = std::nullopt);"
        },
        {
            "sha": "1f2d499e5d6906cd141278fce5e6bc1c04900c19",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd_emitter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc?ref=a8f27858b8af4ddf767059ee152f28820c30c855",
            "patch": "@@ -213,7 +213,7 @@ static absl::StatusOr<Command> Convert(\n   return std::make_unique<DynamicSliceFusionCmd>(\n       std::move(embedded_cmds), thunk.get_arguments(),\n       std::move(fake_allocations), thunk.get_offsets(), thunk.get_orig_shapes(),\n-      thunk.get_sliced_shapes(), thunk.get_offset_byte_sizes(),\n+      thunk.get_sliced_shapes(), thunk.offset_primitive_types(),\n       thunk.get_offset_function());\n }\n "
        },
        {
            "sha": "9a6928a711ce55a6a524d4412986dbaf115dfa97",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_thunk_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_thunk_test.cc?ref=a8f27858b8af4ddf767059ee152f28820c30c855",
            "patch": "@@ -1013,13 +1013,13 @@ TEST(CommandBufferThunkTest, DISABLED_DynamicSliceFusionCmd) {\n   std::vector<std::optional<Shape>> sliced_shapes = {\n       ShapeUtil::MakeShape(PrimitiveType::F32, {2, 4}), std::nullopt,\n       std::nullopt, std::nullopt};\n-  std::vector<std::optional<uint64_t>> offset_byte_sizes = {\n-      sizeof(int64_t), std::nullopt, std::nullopt, std::nullopt};\n+  std::vector<std::optional<PrimitiveType>> offset_primitive_types = {\n+      S64, std::nullopt, std::nullopt, std::nullopt};\n \n   CommandBufferCmdSequence commands;\n   commands.Emplace<DynamicSliceFusionCmd>(\n       std::move(embed_executor), arguments, std::move(fake_allocations),\n-      offsets, orig_shapes, sliced_shapes, offset_byte_sizes);\n+      offsets, orig_shapes, sliced_shapes, offset_primitive_types);\n   TF_ASSERT_OK_AND_ASSIGN(\n       CommandBufferCmdExecutor executor,\n       CommandBufferCmdExecutor::Create(std::move(commands), serialize));"
        },
        {
            "sha": "1feb1410564764a2be61b24081c10878ff11f8a5",
            "filename": "third_party/xla/xla/backends/gpu/runtime/dynamic_slice_thunk.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 32,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.cc?ref=a8f27858b8af4ddf767059ee152f28820c30c855",
            "patch": "@@ -45,6 +45,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/literal.h\"\n #include \"xla/literal_util.h\"\n+#include \"xla/primitive_util.h\"\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n@@ -56,6 +57,7 @@ limitations under the License.\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/xla_data.pb.h\"\n \n namespace xla {\n namespace gpu {\n@@ -149,25 +151,24 @@ std::string DynamicSliceThunk::SliceDef::ToString() const {\n     absl::StrAppend(&result, \", offsets:null\");\n   }\n \n-  // orig_shape\n   if (orig_shape.has_value()) {\n     absl::StrAppend(&result, \", orig_shape:\", orig_shape->ToString());\n   } else {\n     absl::StrAppend(&result, \", orig_shape:null\");\n   }\n \n-  // sliced_shape\n   if (sliced_shape.has_value()) {\n     absl::StrAppend(&result, \", sliced_shape:\", sliced_shape->ToString());\n   } else {\n     absl::StrAppend(&result, \", sliced_shape:null\");\n   }\n \n-  // offset_byte_size\n-  if (offset_byte_size.has_value()) {\n-    absl::StrAppend(&result, \", offset_byte_size:\", *offset_byte_size);\n+  if (offset_primitive_type.has_value()) {\n+    absl::StrAppend(\n+        &result, \", offset_primitive_type:\",\n+        primitive_util::LowercasePrimitiveTypeName(*offset_primitive_type));\n   } else {\n-    absl::StrAppend(&result, \", offset_byte_size:null\");\n+    absl::StrAppend(&result, \", offset_primitive_type:null\");\n   }\n \n   absl::StrAppend(&result, \"}\");\n@@ -181,7 +182,7 @@ DynamicSliceThunk::DynamicSliceThunk(\n     std::vector<std::optional<std::vector<Offset>>> offsets,\n     std::vector<std::optional<Shape>> orig_shapes,\n     std::vector<std::optional<Shape>> sliced_shapes,\n-    std::vector<std::optional<uint64_t>> offset_byte_sizes,\n+    std::vector<std::optional<PrimitiveType>> offset_primitive_types,\n     std::optional<OffsetAsFunctionOfIndvarModulesMetadata>\n         offset_as_function_of_indvar_metadata)\n     : Thunk(Kind::kDynamicSlice, thunk_info),\n@@ -192,19 +193,19 @@ DynamicSliceThunk::DynamicSliceThunk(\n       offsets_(offsets),\n       orig_shapes_(orig_shapes),\n       sliced_shapes_(sliced_shapes),\n-      offset_byte_sizes_(offset_byte_sizes),\n+      offset_primitive_types_(offset_primitive_types),\n       offset_as_function_of_indvar_metadata_(\n           std::move(offset_as_function_of_indvar_metadata)) {\n   // Zip all arguments together to create a list of SliceDef.\n-  for (auto [arg, offsets, orig_shape, sliced_shape, offset_byte_size] :\n+  for (auto [arg, offsets, orig_shape, sliced_shape, offset_primitive_type] :\n        llvm::zip_equal(arguments, offsets, orig_shapes, sliced_shapes,\n-                       offset_byte_sizes)) {\n+                       offset_primitive_types)) {\n     slices_.push_back(SliceDef{\n         std::move(arg),\n         std::move(offsets),\n         std::move(orig_shape),\n         std::move(sliced_shape),\n-        std::move(offset_byte_size),\n+        std::move(offset_primitive_type),\n     });\n   }\n \n@@ -226,7 +227,7 @@ absl::Status DynamicSliceThunk::Prepare(const PrepareParams& params) {\n       TF_RET_CHECK(slice.embedded_thunk_argument.has_value());\n       TF_RET_CHECK(slice.orig_shape.has_value());\n       TF_RET_CHECK(slice.sliced_shape.has_value());\n-      TF_RET_CHECK(slice.offset_byte_size.has_value());\n+      TF_RET_CHECK(slice.offset_primitive_type.has_value());\n \n       TF_RET_CHECK(slice.orig_shape->IsArray());\n       TF_RET_CHECK(slice.sliced_shape->IsArray());\n@@ -358,8 +359,9 @@ absl::Status DynamicSliceThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n         // Copy the `offset_idx`-th component of the offset for the\n         // `argument_idx`-th argument from device to host.\n-        TF_RETURN_IF_ERROR(\n-            stream.Memcpy(offset_dst, offset_src, *slice.offset_byte_size));\n+        TF_RETURN_IF_ERROR(stream.Memcpy(\n+            offset_dst, offset_src,\n+            ShapeUtil::ByteSizeOfPrimitiveType(*slice.offset_primitive_type)));\n         ++num_transfers;\n       }\n     }\n@@ -473,7 +475,9 @@ Thunk::BufferUses DynamicSliceThunk::buffer_uses() const {\n       if (!alloc_slice) {\n         continue;\n       }\n-      res.push_back(BufferUse::Read(*alloc_slice));\n+      res.push_back(BufferUse::Read(\n+          *alloc_slice,\n+          ShapeUtil::MakeShape(*slice.offset_primitive_type, {})));\n     }\n   }\n   return res;\n@@ -630,10 +634,11 @@ absl::StatusOr<ThunkProto> DynamicSliceThunk::ToProto() const {\n   }\n \n   // offset_byte_sizes\n-  for (const auto& size : offset_byte_sizes_) {\n-    auto& proto_size = *dynamic_slice_proto->add_offset_byte_sizes();\n-    if (size.has_value()) {\n-      proto_size.set_value(size.value());\n+  for (const std::optional<PrimitiveType>& primtive_type :\n+       offset_primitive_types_) {\n+    auto& proto_size = *dynamic_slice_proto->add_offset_primitive_types();\n+    if (primtive_type.has_value()) {\n+      proto_size.set_value(primtive_type.value());\n     }\n   }\n \n@@ -667,7 +672,6 @@ absl::StatusOr<std::unique_ptr<DynamicSliceThunk>> DynamicSliceThunk::FromProto(\n             proto.offset_as_function_of_indvar_modules_metadata()));\n   }\n \n-  // arguments\n   std::vector<std::optional<BufferAllocation::Slice>> arguments;\n   for (auto& arg_proto : proto.arguments()) {\n     arguments.push_back(std::nullopt);\n@@ -678,13 +682,11 @@ absl::StatusOr<std::unique_ptr<DynamicSliceThunk>> DynamicSliceThunk::FromProto(\n     }\n   }\n \n-  // offsets\n   TF_ASSIGN_OR_RETURN(\n       std::vector<std::optional<std::vector<Offset>>> offsets,\n       DeserializeOffsetsFromProto(proto, buffer_allocations,\n                                   offset_as_function_of_indvar_metadata));\n \n-  // orig_shapes\n   std::vector<std::optional<Shape>> orig_shapes;\n   for (auto& shape_proto : proto.orig_shapes()) {\n     orig_shapes.push_back(std::nullopt);\n@@ -694,7 +696,6 @@ absl::StatusOr<std::unique_ptr<DynamicSliceThunk>> DynamicSliceThunk::FromProto(\n     }\n   }\n \n-  // sliced_shapes\n   std::vector<std::optional<Shape>> sliced_shapes;\n   for (auto& shape_proto : proto.sliced_shapes()) {\n     sliced_shapes.push_back(std::nullopt);\n@@ -704,23 +705,22 @@ absl::StatusOr<std::unique_ptr<DynamicSliceThunk>> DynamicSliceThunk::FromProto(\n     }\n   }\n \n-  // offset_byte_sizes\n-  std::vector<std::optional<uint64_t>> offset_byte_sizes;\n-  for (auto& size_proto : proto.offset_byte_sizes()) {\n-    offset_byte_sizes.push_back(std::nullopt);\n-    if (size_proto.has_value()) {\n-      offset_byte_sizes.back() = size_proto.value();\n+  std::vector<std::optional<PrimitiveType>> offset_primtitive_types;\n+  offset_primtitive_types.reserve(proto.offset_primitive_types_size());\n+  for (const OptionalPrimitiveType& type_proto :\n+       proto.offset_primitive_types()) {\n+    offset_primtitive_types.push_back(std::nullopt);\n+    if (type_proto.has_value()) {\n+      offset_primtitive_types.back() = type_proto.value();\n     }\n   }\n \n-  // fake_allocations\n   std::vector<BufferAllocation> fake_allocations;\n   for (const auto& fake_allocation_proto : proto.fake_allocations()) {\n     fake_allocations.push_back(\n         BufferAllocation::FromProto(fake_allocation_proto));\n   }\n \n-  // embedded_thunk\n   std::vector<std::unique_ptr<Thunk>> embedded_thunks;\n   for (const auto& thunk_proto : proto.embedded_thunk().thunks()) {\n     TF_ASSIGN_OR_RETURN(std::unique_ptr<Thunk> embedded_thunk,\n@@ -732,7 +732,7 @@ absl::StatusOr<std::unique_ptr<DynamicSliceThunk>> DynamicSliceThunk::FromProto(\n       thunk_info, std::make_unique<ThunkSequence>(std::move(embedded_thunks)),\n       std::move(arguments), std::move(fake_allocations), std::move(offsets),\n       std::move(orig_shapes), std::move(sliced_shapes),\n-      std::move(offset_byte_sizes),\n+      std::move(offset_primtitive_types),\n       std::move(offset_as_function_of_indvar_metadata));\n }\n "
        },
        {
            "sha": "10a620158e965463e2b66e15504d038e9ad37fa5",
            "filename": "third_party/xla/xla/backends/gpu/runtime/dynamic_slice_thunk.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.h?ref=a8f27858b8af4ddf767059ee152f28820c30c855",
            "patch": "@@ -118,7 +118,7 @@ class DynamicSliceThunk : public Thunk {\n       std::vector<std::optional<std::vector<Offset>>> offsets,\n       std::vector<std::optional<Shape>> orig_shapes,\n       std::vector<std::optional<Shape>> sliced_shapes,\n-      std::vector<std::optional<uint64_t>> offset_byte_sizes,\n+      std::vector<std::optional<PrimitiveType>> offset_primitive_types,\n       std::optional<OffsetAsFunctionOfIndvarModulesMetadata>\n           offset_as_function_of_indvar_metadata = std::nullopt);\n   DynamicSliceThunk(const DynamicSliceThunk&) = delete;\n@@ -137,7 +137,7 @@ class DynamicSliceThunk : public Thunk {\n     std::optional<std::vector<Offset>> offsets;\n     std::optional<Shape> orig_shape;\n     std::optional<Shape> sliced_shape;\n-    std::optional<uint64_t> offset_byte_size;\n+    std::optional<PrimitiveType> offset_primitive_type;\n     std::string ToString() const;\n   };\n \n@@ -165,8 +165,8 @@ class DynamicSliceThunk : public Thunk {\n     return sliced_shapes_;\n   }\n \n-  std::vector<std::optional<uint64_t>> get_offset_byte_sizes() const {\n-    return offset_byte_sizes_;\n+  std::vector<std::optional<PrimitiveType>> offset_primitive_types() const {\n+    return offset_primitive_types_;\n   }\n \n   void ForAllThunks(absl::FunctionRef<void(const Thunk*)> fn) const override;\n@@ -204,7 +204,7 @@ class DynamicSliceThunk : public Thunk {\n   std::vector<std::optional<std::vector<Offset>>> offsets_;\n   std::vector<std::optional<Shape>> orig_shapes_;\n   std::vector<std::optional<Shape>> sliced_shapes_;\n-  std::vector<std::optional<uint64_t>> offset_byte_sizes_;\n+  std::vector<std::optional<PrimitiveType>> offset_primitive_types_;\n \n   std::vector<SliceDef> slices_;\n "
        },
        {
            "sha": "98616dd2ca48ac33f5c7eabc2be71705c2bd6d25",
            "filename": "third_party/xla/xla/backends/gpu/runtime/dynamic_slice_thunk.proto",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk.proto?ref=a8f27858b8af4ddf767059ee152f28820c30c855",
            "patch": "@@ -33,8 +33,8 @@ message OptionalShapeProto {\n   optional xla.ShapeProto shape = 1;\n }\n \n-message OptionalInt64Proto {\n-  optional int64 value = 1;\n+message OptionalPrimitiveType {\n+  optional PrimitiveType value = 1;\n }\n \n // Reflects std::optional<std::vector<Offset>>"
        },
        {
            "sha": "99790239ebee453f80229b0810155747637decc1",
            "filename": "third_party/xla/xla/backends/gpu/runtime/dynamic_slice_thunk_test.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk_test.cc?ref=a8f27858b8af4ddf767059ee152f28820c30c855",
            "patch": "@@ -238,8 +238,8 @@ absl::StatusOr<std::unique_ptr<DynamicSliceThunk>> CreateSlicedGemmThunk(\n       std::vector<std::optional<Shape>>{\n           ShapeUtil::MakeShape(PrimitiveType::F32, {1, 3}), std::nullopt,\n           std::nullopt, std::nullopt},\n-      std::vector<std::optional<uint64_t>>{sizeof(int64_t), std::nullopt,\n-                                           std::nullopt, std::nullopt});\n+      std::vector<std::optional<PrimitiveType>>{S64, std::nullopt, std::nullopt,\n+                                                std::nullopt});\n }\n \n TEST_F(DynamicSliceThunkTest, SlicedGemmProtoRoundTrip) {\n@@ -411,8 +411,8 @@ CreateMultipleSlicedOperandsGemmThunk(\n           ShapeUtil::MakeShape(PrimitiveType::F32, {1, 3}),\n           ShapeUtil::MakeShape(PrimitiveType::F32, {3, 1}), std::nullopt,\n           std::nullopt},\n-      std::vector<std::optional<uint64_t>>{sizeof(int64_t), sizeof(int64_t),\n-                                           std::nullopt, std::nullopt});\n+      std::vector<std::optional<PrimitiveType>>{S64, S64, std::nullopt,\n+                                                std::nullopt});\n }\n \n TEST_F(DynamicSliceThunkTest, MultipleSlicedOperandsGemmProtoRoundTrip) {\n@@ -601,7 +601,7 @@ TEST_F(DynamicSliceThunkTest, SlicedMemcpy) {\n       // Make sure to pass a dst shape with the same rank as src shape (i.e.\n       // original slice result and not bitcasted one)\n       {ShapeUtil::MakeShape(PrimitiveType::S32, {1, 1, 8, 8}), std::nullopt},\n-      {sizeof(int64_t), std::nullopt});\n+      {S64, std::nullopt});\n \n   // Step 2:\n   // Execute dynamic slice thunk.\n@@ -767,7 +767,7 @@ TEST_F(DynamicSliceThunkTest, SlicedOutputMemcpy) {\n       // original slice result and not bitcasted one)\n       {ShapeUtil::MakeShape(PrimitiveType::S32, {1, 1, 2, 2}),\n        ShapeUtil::MakeShape(PrimitiveType::S32, {1, 1, 2, 2})},\n-      {sizeof(int64_t), sizeof(int64_t)});\n+      {S64, S64});\n \n   // Step 2:\n   // Execute dynamic slice thunk.\n@@ -945,8 +945,8 @@ CreateSlicedGemmArbitraryArgumentOrderThunk(\n       std::vector<std::optional<Shape>>{\n           ShapeUtil::MakeShape(PrimitiveType::F32, {1, 3}), std::nullopt,\n           std::nullopt, std::nullopt},\n-      std::vector<std::optional<uint64_t>>{sizeof(int64_t), std::nullopt,\n-                                           std::nullopt, std::nullopt});\n+      std::vector<std::optional<PrimitiveType>>{S64, std::nullopt, std::nullopt,\n+                                                std::nullopt});\n }\n \n TEST_F(DynamicSliceThunkTest, SlicedGemmArbitraryArgumentOrderProtoRoundTrip) {\n@@ -1118,8 +1118,8 @@ CreateSlicedGemmArbitraryNumberOfArgumentsThunk(\n       std::vector<std::optional<Shape>>{\n           ShapeUtil::MakeShape(PrimitiveType::F32, {1, 3}), std::nullopt,\n           std::nullopt, std::nullopt},\n-      std::vector<std::optional<uint64_t>>{sizeof(int64_t), std::nullopt,\n-                                           std::nullopt, std::nullopt});\n+      std::vector<std::optional<PrimitiveType>>{S64, std::nullopt, std::nullopt,\n+                                                std::nullopt});\n }\n \n TEST_F(DynamicSliceThunkTest,\n@@ -1282,8 +1282,8 @@ CreateSlicedTupledOperandGemmThunk(\n       std::vector<std::optional<Shape>>{\n           ShapeUtil::MakeShape(PrimitiveType::F32, {1, 3}), std::nullopt,\n           std::nullopt, std::nullopt},\n-      std::vector<std::optional<uint64_t>>{sizeof(int64_t), std::nullopt,\n-                                           std::nullopt, std::nullopt});\n+      std::vector<std::optional<PrimitiveType>>{S64, std::nullopt, std::nullopt,\n+                                                std::nullopt});\n }\n \n TEST_F(DynamicSliceThunkTest, SlicedTupledOperandGemmProtoRoundTrip) {\n@@ -1475,7 +1475,7 @@ TEST_F(DynamicSliceThunkTest, SlicedMemcpyOOB) {\n       // original slice result and not bitcasted one)\n       {ShapeUtil::MakeShape(PrimitiveType::S32, {1, 1, 2, 2}),\n        ShapeUtil::MakeShape(PrimitiveType::S32, {1, 1, 2, 2})},\n-      {sizeof(int64_t), sizeof(int64_t)});\n+      {S64, S64});\n \n   // Step 2:\n   // Execute dynamic slice thunk.\n@@ -1658,8 +1658,8 @@ CreateSlicedOperandsSameBufferGemmThunk(\n       std::vector<std::optional<Shape>>{\n           ShapeUtil::MakeShape(PrimitiveType::F32, {1, 3}), std::nullopt,\n           std::nullopt, std::nullopt},\n-      std::vector<std::optional<uint64_t>>{sizeof(int64_t), std::nullopt,\n-                                           std::nullopt, std::nullopt});\n+      std::vector<std::optional<PrimitiveType>>{S64, std::nullopt, std::nullopt,\n+                                                std::nullopt});\n }\n \n TEST_F(DynamicSliceThunkTest, SlicedOperandsSameBufferGemmProtoRoundTrip) {\n@@ -1876,8 +1876,8 @@ CreateHostInductionVariableAndOffsetEvaluationThunk(\n           ShapeUtil::MakeShape(PrimitiveType::F32, {1, 4}), std::nullopt,\n           std::nullopt, std::nullopt},\n       /*offset_byte_sizes=*/\n-      std::vector<std::optional<uint64_t>>{sizeof(int64_t), std::nullopt,\n-                                           std::nullopt, std::nullopt},\n+      std::vector<std::optional<PrimitiveType>>{S64, std::nullopt, std::nullopt,\n+                                                std::nullopt},\n       /*offset_as_function_of_indvar_metadata=*/\n       std::move(offset_as_function_of_indvar_modules_metadata));\n }"
        },
        {
            "sha": "c60e3ea5c94e36567c41064472d972d80cc60f83",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.proto",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a8f27858b8af4ddf767059ee152f28820c30c855/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto?ref=a8f27858b8af4ddf767059ee152f28820c30c855",
            "patch": "@@ -261,7 +261,7 @@ message DynamicSliceThunkProto {\n   repeated OptionalDynamicSliceOffsetsProto offsets = 3;\n   repeated OptionalShapeProto orig_shapes = 4;\n   repeated OptionalShapeProto sliced_shapes = 5;\n-  repeated OptionalInt64Proto offset_byte_sizes = 6;\n+  repeated OptionalPrimitiveType offset_primitive_types = 6;\n   optional OffsetAsFunctionOfIndvarModulesMetadataProto\n       offset_as_function_of_indvar_modules_metadata = 7;\n   repeated BufferAllocationProto fake_allocations = 8;"
        }
    ],
    "stats": {
        "total": 193,
        "additions": 98,
        "deletions": 95
    }
}