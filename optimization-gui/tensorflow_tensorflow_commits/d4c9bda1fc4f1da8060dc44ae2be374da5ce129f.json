{
    "author": "loislo",
    "message": "Integrate Triton up to 8d445186\n\nhttps://github.com/openxla/triton/tree/triton_integrate_branch-1.15\n\nPiperOrigin-RevId: 843206657",
    "sha": "d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
    "files": [
        {
            "sha": "656b9c894904d8511d2c54523fd3883f09f6e876",
            "filename": "third_party/xla/third_party/triton/llvm_integration/series.bzl",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fseries.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fseries.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fseries.bzl?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -8,9 +8,5 @@ LLVM nor MLIR integrator, please do not add any patches to this list.\n \"\"\"\n \n llvm_patch_list = [\n-    \"//third_party/triton:llvm_integration/cl831451347.patch\",\n-    \"//third_party/triton:llvm_integration/cl833447018.patch\",\n-    \"//third_party/triton:llvm_integration/cl835942347.patch\",\n-    \"//third_party/triton:llvm_integration/cl838780160.patch\",\n     # Add new patches just above this line\n ]"
        },
        {
            "sha": "9386923e48f63763193ade7d24a8da9f2a24e6e1",
            "filename": "third_party/xla/third_party/triton/temporary/launched_cluster_dim_fix.patch",
            "status": "added",
            "additions": 106,
            "deletions": 0,
            "changes": 106,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Flaunched_cluster_dim_fix.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Flaunched_cluster_dim_fix.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Flaunched_cluster_dim_fix.patch?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -0,0 +1,106 @@\n+https://github.com/triton-lang/triton/pull/8645 removed cluster dimensions from the\n+kernel metadata. We don't need to pass them to the launch API anymore.\n+\n+--- a/third_party/nvidia/backend/cuda_utils.cc\t2025-11-17 02:33:44.000000000 -0800\n++++ b/third_party/nvidia/backend/cuda_utils.cc\t2025-11-18 03:58:02.000000000 -0800\n+@@ -119,8 +119,8 @@\n+     constexpr int size() const { return x * y * z; }\n+   };\n+   Dim grid;                     // Number of clusters per grid\n+-  Dim cluster;                  // Number of blocks per cluster\n+   int num_warps;                // number of warps per block\n++  int num_ctas;                 // number of CTAs per block\n+   int shared_memory;            // Size of shared memory in bytes to allocate\n+   int launch_cooperative_grid;  // Non-zero to launch coop grid\n+   int launch_pdl;               // Non-zero to use programatic-dependent launch\n+@@ -137,16 +137,15 @@\n+   // APIs if needed.\n+   Py_BEGIN_ALLOW_THREADS;\n+   const auto& grid = config.grid;\n+-  const auto& cluster = config.cluster;\n+   if (grid.size() == 0) {\n+     PyEval_RestoreThread(_save);\n+     Py_RETURN_NONE;\n+   }\n+ \n+   CUlaunchConfig cu_config;\n+-  cu_config.gridDimX = grid.x * cluster.x;\n+-  cu_config.gridDimY = grid.y * cluster.y;\n+-  cu_config.gridDimZ = grid.z * cluster.z;\n++  cu_config.gridDimX = grid.x * config.num_ctas;\n++  cu_config.gridDimY = grid.y;\n++  cu_config.gridDimZ = grid.z;\n+   cu_config.blockDimX = 32 * config.num_warps;\n+   cu_config.blockDimY = 1;\n+   cu_config.blockDimZ = 1;\n+@@ -169,12 +168,12 @@\n+       .value = { .cooperative =  1}\n+     };\n+   }\n+-  if (config.cluster.size() > 1) {\n++  if (config.num_ctas != 1) {\n+     auto& clusterDimAttr = launchAttr[cu_config.numAttrs++];\n+     clusterDimAttr.id = CU_LAUNCH_ATTRIBUTE_CLUSTER_DIMENSION;\n+-    clusterDimAttr.value.clusterDim.x = cluster.x;\n+-    clusterDimAttr.value.clusterDim.y = cluster.y;\n+-    clusterDimAttr.value.clusterDim.z = cluster.z;\n++    clusterDimAttr.value.clusterDim.x = config.num_ctas;\n++    clusterDimAttr.value.clusterDim.y = 1;\n++    clusterDimAttr.value.clusterDim.z = 1;\n+     auto& clusterDimSchedulingAttr = launchAttr[cu_config.numAttrs++];\n+     clusterDimSchedulingAttr.id =\n+         CU_LAUNCH_ATTRIBUTE_CLUSTER_SCHEDULING_POLICY_PREFERENCE;\n+@@ -518,9 +517,8 @@\n+ :param launch_pdl: enable programmatic dependent launch\n+ :type launch_pdl: bool\n+ :param packed_metadata: Kernel metadata, including in sequence:\n+-    number of warps, number of CTAs, required bytes of shared memory,\n+-    cluster dimensions x, y, and z\n+-:type packed_metadata: 6-tuple\n++    number of warps, number of CTAs, required bytes of shared memory\n++:type packed_metadata: 3-tuple\n+ :param hook_args: arguments to pass to the enter and exit hooks\n+ :type hook_args: object\n+ :param launch_enter_hook: hook to call just before launching the kernel\n+@@ -542,7 +540,6 @@\n+   ensureCudaContext();\n+   TritonLaunchConfig config{};\n+   auto& grid = config.grid;\n+-  auto& cluster = config.cluster;\n+   // PyObject* kernel_metadata = nullptr;\n+   PyObject* hook_args = nullptr;\n+   PyObject* launch_enter_hook = nullptr;\n+@@ -551,15 +548,13 @@\n+   PyObject* kernel_args = nullptr;\n+   PyObject* global_scratch = nullptr;\n+   PyObject* profile_scratch = nullptr;\n+-  int num_ctas = 0;\n+-  if (!PyArg_ParseTuple(args, \"iiiKKpp(iiiiii)OOOSOOO\", &grid.x, &grid.y,\n+-                        &grid.z, &config.stream, &config.function,\n++  if (!PyArg_ParseTuple(args, \"iiiKKpp(iii)OOOSOOO\", &grid.x, &grid.y, &grid.z,\n++                        &config.stream, &config.function,\n+                         &config.launch_cooperative_grid, &config.launch_pdl,\n+-                        &config.num_warps, &num_ctas, &config.shared_memory,\n+-                        &cluster.x, &cluster.y, &cluster.z, &hook_args,\n+-                        &launch_enter_hook, &launch_exit_hook,\n+-                        &signature_metadata_bytes, &global_scratch,\n+-                        &profile_scratch, &kernel_args)) {\n++                        &config.num_warps, &config.num_ctas,\n++                        &config.shared_memory, &hook_args, &launch_enter_hook,\n++                        &launch_exit_hook, &signature_metadata_bytes,\n++                        &global_scratch, &profile_scratch, &kernel_args)) {\n+     return nullptr;\n+   }\n+   llvm::ArrayRef<char> signature_metadata(\n+\n+--- a/third_party/nvidia/backend/driver.py\t2025-11-13 05:31:00.000000000 -0800\n++++ b/third_party/nvidia/backend/driver.py\t2025-11-18 03:45:28.000000000 -0800\n+@@ -223,7 +223,7 @@\n+     def wrapper(grid_dim_x: int, grid_dim_y: int, grid_dim_z: int,\n+                 stream: int, kernel: int, launch_cooperative_grid: bool,\n+                 launch_pdl: bool, global_scratch: any, profile_scratch: any,\n+-                packed_metadata: tuple[int, int, int, int, int, int],\n++                packed_metadata: tuple[int, int, int],\n+                 hook_args: any,\n+                 launch_enter_hook: Callable[..., None],\n+                 launch_exit_hook: Callable[..., None],"
        },
        {
            "sha": "7aa0f2e9551d9aaa6a2cd3433e191237ae363916",
            "filename": "third_party/xla/third_party/triton/temporary/launcher_addition.patch",
            "status": "added",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Flauncher_addition.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Flauncher_addition.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Flauncher_addition.patch?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -0,0 +1,70 @@\n+\n+--- a/third_party/nvidia/backend/cuda_utils.cc\t2025-11-13 05:31:00.000000000 -0800\n++++ b/third_party/nvidia/backend/cuda_utils.cc\t2025-11-14 02:50:50.000000000 -0800\n+@@ -59,7 +59,7 @@\n+ #define CUDA_CHECK_AND_RETURN_NULL(ans)                                        \\\n+   do {                                                                         \\\n+     if (!gpuAssert((ans), __FILE__, __LINE__))                                 \\\n+-      goto cleanup;                                                            \\\n++      return NULL;                                                            \\\n+   } while (0)\n+ \n+ // To be used inside a Py_{BEGIN,END}_ALLOW_THREADS block.\n+@@ -77,7 +77,7 @@\n+     if ((funcPointer) == NULL) {                                               \\\n+       (funcPointer) = (initializerFunction)();                                 \\\n+       if ((funcPointer) == NULL) {                                             \\\n+-        goto cleanup;                                                          \\\n++        return NULL;                                                          \\\n+       }                                                                        \\\n+     }                                                                          \\\n+   } while (0)\n+@@ -912,16 +912,21 @@\n+ \n+ // clang-format off\n+ static PyTypeObject PyCUtensorMapType = {\n+-    PyVarObject_HEAD_INIT(NULL, 0)\n++    .ob_base = {\n++        .ob_base = {\n++            .ob_type = NULL,\n++        },\n++        .ob_size = 0,\n++    },\n+     .tp_name = \"triton.backends.nvidia.PyCUtensorMap\",\n+     .tp_basicsize = sizeof(PyCUtensorMapObject),\n+     .tp_itemsize = 0,\n++    .tp_dealloc = (destructor)PyCUtensorMap_dealloc,\n+     .tp_flags = Py_TPFLAGS_DEFAULT,\n+     .tp_doc = \"<PyCUtensorMap object>\",\n+     .tp_methods = PyCUtensorMap_methods,\n++    .tp_alloc = PyCUtensorMap_alloc,\n+     .tp_new = PyType_GenericNew,\n+-    .tp_alloc = PyCUtensorMap_alloc,\n+-    .tp_dealloc = (destructor)PyCUtensorMap_dealloc,\n+     .tp_free = PyCUtensorMap_free,\n+ };\n+ // clang-format on\n+@@ -1056,9 +1061,11 @@\n+   INITIALIZE_FUNCTION_POINTER_IF_NULL(cuTensorMapEncodeTiled,\n+                                       getCuTensorMapEncodeTiledHandle);\n+   CUresult res = cuTensorMapEncodeTiled(\n+-      &desc->tensorMap, elemType, rank, (void *)global_address, shapeInt,\n+-      stridesLL, blockSizeInt, elementStrides, CU_TENSOR_MAP_INTERLEAVE_NONE,\n+-      swizzle, CU_TENSOR_MAP_L2_PROMOTION_L2_128B, fill);\n++      &desc->tensorMap, (CUtensorMapDataType)elemType, rank,\n++      (void*)global_address, shapeInt, stridesLL, blockSizeInt, elementStrides,\n++      CU_TENSOR_MAP_INTERLEAVE_NONE, (CUtensorMapSwizzle)swizzle,\n++      CU_TENSOR_MAP_L2_PROMOTION_L2_128B, fill);\n++\n+   if (res != CUDA_SUCCESS) {\n+     const char *str;\n+     cuGetErrorString(res, &str);\n+@@ -1104,8 +1111,6 @@\n+   }\n+ \n+   return (PyObject *)desc;\n+-\n+-  return result;\n+ }\n+ \n+ static PyMethodDef ModuleMethods[] = {"
        },
        {
            "sha": "c4e4872e455f08d54f7123b84fe24803e9d3ee76",
            "filename": "third_party/xla/third_party/triton/temporary/launcher_non_portable_clusters.patch",
            "status": "added",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Flauncher_non_portable_clusters.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Flauncher_non_portable_clusters.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Flauncher_non_portable_clusters.patch?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -0,0 +1,19 @@\n+This should be merged with the launcher patch afterwards.\n+\n+--- a/third_party/nvidia/backend/cuda_utils.cc\t2025-11-18 05:00:39.000000000 -0800\n++++ b/third_party/nvidia/backend/cuda_utils.cc\t2025-12-02 08:10:32.000000000 -0800\n+@@ -180,6 +180,14 @@\n+     clusterDimSchedulingAttr.value.clusterSchedulingPolicyPreference =\n+         CU_CLUSTER_SCHEDULING_POLICY_SPREAD;\n+   }\n++\n++  // As per the comment in third_party/nvidia/backend/driver.py,\n++  // \"num_ctas == 16 is non-portable. Does work for H100 and B200 tho\".\n++  if (config.num_ctas == 16) {\n++    CUDA_CHECK(cuFuncSetAttribute(\n++        config.function, CU_FUNC_ATTRIBUTE_NON_PORTABLE_CLUSTER_SIZE_ALLOWED,\n++        1));\n++  }\n+ \n+   // cuLaunchKernelEx was added in CUDA 12, so load it dynamically to be\n+   // able to link on CUDA 11 and earlier."
        },
        {
            "sha": "57d4c2121e37ea0bd56e368a543c23346b627948",
            "filename": "third_party/xla/third_party/triton/temporary/launcher_tma_desc_fix.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 144,
            "changes": 144,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de08322b72d7c6f07e6ae7812142044b0ce4834f/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Flauncher_tma_desc_fix.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de08322b72d7c6f07e6ae7812142044b0ce4834f/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Flauncher_tma_desc_fix.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Flauncher_tma_desc_fix.patch?ref=de08322b72d7c6f07e6ae7812142044b0ce4834f",
            "patch": "@@ -1,144 +0,0 @@\n-diff --git a/third_party/nvidia/backend/cuda_utils.cc b/third_party/nvidia/backend/cuda_utils.cc\n---- a/third_party/nvidia/backend/cuda_utils.cc\n-+++ b/third_party/nvidia/backend/cuda_utils.cc\n-@@ -270,51 +270,16 @@ bool extractPointer(PyObject* obj, void*\n-   return true;\n- }\n- \n-+CUtensorMap* getTmaDesc(PyObject* obj);\n-+\n- // Extract a CUtensorMap descriptor from a python object, and store it to the\n- // memory location pointed by ptr.\n- bool extractTmaDesc(PyObject* obj, void* ptr) {\n--  if (sizeof(CUtensorMap*) != 8) {\n--    PyErr_SetString(PyExc_SystemError,\n--                \"extractTmaDesc() requires 64-bit compilation\");\n--    return false;\n--  }\n--\n--  UniquePyObjectPtr method_ret(\n--      PyObject_CallMethod(obj, \"tma_desc_cpu_ptr\", nullptr));\n--  // Checking the error retains context if tma_desc_cpu_ptr raises an exception.\n--  if (PyErr_Occurred()) {\n--    return false;\n--  }\n--\n--  if (!method_ret) {\n--    PyErr_SetString(PyExc_SystemError, \"Call to tma_desc_cpu_ptr() failed\");\n-+  CUtensorMap* tensor_map = getTmaDesc(obj);\n-+  if (tensor_map == nullptr) {\n-     return false;\n-   }\n--\n--  if (!PyLong_Check(method_ret.get())) {\n--    PyErr_SetString(PyExc_TypeError,\n--                    \"tma_desc_cpu_ptr() must return 64-bit int\");\n--    return false;\n--  }\n--\n--  uint64_t ptr_as_uint = PyLong_AsUnsignedLongLong(method_ret.get());\n--  if (PyErr_Occurred()) {\n--    return false;\n--  }\n--\n--  if (!ptr_as_uint) {\n--    PyErr_SetString(PyExc_ValueError,\n--                    \"received NULL ptr from tma_desc_cpu_ptr()\");\n--    return false;\n--  }\n--  if (ptr_as_uint % 64 != 0) {\n--    PyErr_SetString(PyExc_ValueError,\n--                    \"tma_desc_cpu_ptr() must be 64-byte aligned\");\n--    return false;\n--  }\n--\n--  *static_cast<CUtensorMap*>(ptr) =\n--      *reinterpret_cast<CUtensorMap*>(ptr_as_uint);\n-+  *static_cast<CUtensorMap*>(ptr) = *tensor_map;\n-   return true;\n- }\n- \n-@@ -392,6 +357,7 @@ struct ExtractionInfo {\n-   // Prefixes of types reprs supported by the extractor.\n-   llvm::SmallVector<llvm::StringRef> supported_type_repr_prefixes;\n-   std::size_t size;         // Size required by the extracted value.\n-+  std::size_t alignment;    // Alignment requirement for the extracted value.\n-   ExtractorType extractor;  // Function to call to extract the value.\n- \n-   // Builds an ExtractionInfo for a given type T and a list of type reprs that\n-@@ -400,7 +366,7 @@ struct ExtractionInfo {\n-   static ExtractionInfo build(\n-       std::initializer_list<llvm::StringRef> supported_type_reprs,\n-       ExtractorType extractor = extractValue<T>) {\n--    return {supported_type_reprs, sizeof(T), extractor};\n-+    return {supported_type_reprs, sizeof(T), alignof(T), extractor};\n-   }\n- \n-   // Checks if the extractor supports extracting a given type repr.\n-@@ -428,7 +394,7 @@ const ExtractionInfo kExtractionInfos[]{\n-     // Note: types are e.g. '*fp32', so no closing quote is intentional.\n-     ExtractionInfo::build<void*>({\"'*\"}, extractPointer),\n-     ExtractionInfo{\n--        {\"None\", \"'none'\"}, 0, nullptr},  // Represent constexprs as None\n-+        {\"None\", \"'none'\"}, 0, 0, nullptr},  // Represent constexprs as None\n-     ExtractionInfo::build<CUtensorMap>({\"'nvTmaDesc'\"}, extractTmaDesc),\n- };\n- \n-@@ -628,7 +594,19 @@ PyObject* launch(PyObject* self, PyObjec\n-     if (extraction_info.size == 0) {\n-       continue;  // skip adding constexpr parameters\n-     }\n--    config.params[params_idx] = alloca(extraction_info.size);\n-+    size_t alignment = std::max(1UL, extraction_info.alignment);\n-+\n-+    // Allocate enough space on the stack to guarantee an aligned block.\n-+    size_t size_with_alignment = extraction_info.size + alignment - 1;\n-+    void *param_storage_ptr = alloca(size_with_alignment);\n-+\n-+    void *aligned_ptr = std::align(alignment, extraction_info.size,\n-+                                   param_storage_ptr, size_with_alignment);\n-+    if (aligned_ptr == nullptr) {\n-+      PyErr_SetString(PyExc_MemoryError, \"Failed to align parameter storage\");\n-+      return nullptr;\n-+    }\n-+    config.params[params_idx] = aligned_ptr;\n-     if (!extraction_info.extractor(arg, config.params[params_idx])) {\n-       return nullptr;\n-     }\n-@@ -940,6 +918,36 @@ static PyTypeObject PyCUtensorMapType = \n- };\n- // clang-format on\n- \n-+namespace {\n-+\n-+// Extracts a pointer to `CUtensorMap` from a `PyCUtensorMapObject`.\n-+CUtensorMap* getTmaDesc(PyObject* obj) {\n-+  if (sizeof(CUtensorMap*) != 8) {\n-+    PyErr_SetString(PyExc_SystemError,\n-+                    \"getTmaDesc() requires 64-bit compilation\");\n-+    return nullptr;\n-+  }\n-+  if (Py_TYPE(obj) != static_cast<PyTypeObject*>(&PyCUtensorMapType)) {\n-+    PyErr_Format(PyExc_TypeError,\n-+                 \"object must be of type PyCUtensorMap, got %s\",\n-+                 Py_TYPE(obj)->tp_name);\n-+    return nullptr;\n-+  }\n-+  CUtensorMap* map = &((PyCUtensorMapObject*)obj)->tensorMap;\n-+  // PyCUtensorMapObject aligns tensorMap to 128.\n-+  uintptr_t align_128 = (uintptr_t)map & (128 - 1);\n-+  if (align_128 != 0) {\n-+    PyErr_Format(\n-+        PyExc_ValueError,\n-+        \"CUtensorMap must be aligned to 128B, but got (&map) mod 128 = %ld\",\n-+        align_128);\n-+    return nullptr;\n-+  }\n-+  return map;\n-+}\n-+\n-+}  // namespace\n-+\n- static PyObject *fillTMADescriptor(PyObject *self, PyObject *args) {\n-   unsigned long long global_address;\n-   int swizzle;"
        },
        {
            "sha": "f8cc5d0821f098213f91b9ee67002910896fbee0",
            "filename": "third_party/xla/third_party/triton/temporary/utility-fix.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de08322b72d7c6f07e6ae7812142044b0ce4834f/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Futility-fix.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de08322b72d7c6f07e6ae7812142044b0ce4834f/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Futility-fix.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Futility-fix.patch?ref=de08322b72d7c6f07e6ae7812142044b0ce4834f",
            "patch": "@@ -1,22 +0,0 @@\n-This patch would probably not be accepted upstream because our infrastructure\n-uses Index type for indexing, while they use Integer type. Triton frontend\n-wouldn't generate a situation that would run into this issue.\n-\n-diff --git a/lib/Dialect/Triton/IR/Utility.cpp b/lib/Dialect/Triton/IR/Utility.cpp\n---- a/lib/Dialect/Triton/IR/Utility.cpp\n-+++ b/lib/Dialect/Triton/IR/Utility.cpp\n-@@ -97,8 +97,12 @@ Value tt::getLastInductionValue(OpBuilde\n-   // (ub - lb -1) // step * step + lb\n-   Value diff =\n-       b.create<arith::SubIOp>(loc, loop.getUpperBound(), loop.getLowerBound());\n--  diff = b.create<arith::SubIOp>(\n--      loc, diff, b.create<arith::ConstantOp>(loc, b.getI32IntegerAttr(1)));\n-+  Value one;\n-+  if (diff.getType().isIndex())\n-+    one = b.create<arith::ConstantIndexOp>(loc, 1);\n-+  else\n-+    one = b.create<arith::ConstantOp>(loc, b.getIntegerAttr(diff.getType(), 1));\n-+  diff = b.create<arith::SubIOp>(loc, diff, one);\n-   Value ceilStep = b.create<arith::MulIOp>(\n-       loc, b.create<arith::DivSIOp>(loc, diff, loop.getStep()), loop.getStep());\n-   return b.create<arith::AddIOp>(loc, ceilStep, loop.getLowerBound());"
        },
        {
            "sha": "fd263961ed17fd3255707a6467b198f38cfdbc3a",
            "filename": "third_party/xla/third_party/triton/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fthird_party%2Ftriton%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fthird_party%2Ftriton%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fworkspace.bzl?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -7,8 +7,8 @@ load(\"//third_party/triton:temporary/series.bzl\", \"temporary_patch_list\")\n def repo():\n     \"\"\"Imports Triton.\"\"\"\n \n-    TRITON_COMMIT = \"triton_integrate_branch-1.14\"\n-    TRITON_SHA256 = \"b684cff8d07e839f8a1ea6cc7d331f370615b4c5530489db76f619aa7aa66608\"\n+    TRITON_COMMIT = \"triton_integrate_branch-1.15\"\n+    TRITON_SHA256 = \"a502364ad54bd822dae5d2fc6215695f7d343617a8c643a39a49f40ef474d013\"\n     tf_http_archive(\n         name = \"triton\",\n         sha256 = TRITON_SHA256,"
        },
        {
            "sha": "3fb0c9c2a1e1ded6478d4218141a40f6e7adc14b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 13,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -23,7 +23,6 @@ limitations under the License.\n #include \"xla/codegen/emitters/transforms/passes.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n \n namespace xla::gpu {\n \n@@ -68,30 +67,23 @@ void CreateTritonXlaPipeline(\n void CreateTritonCudaPipeline(\n     mlir::OpPassManager* pm,\n     const stream_executor::CudaComputeCapability& cuda_cc, int num_warps,\n-    int num_ctas, int num_stages,\n-    mlir::triton::nvidia_gpu::ClusterInfo& out_cluster_info);\n+    int num_ctas, int num_stages);\n \n void CreateTritonRocmPipeline(\n     mlir::OpPassManager* pm,\n     const stream_executor::RocmComputeCapability& rocm_cc, int num_warps,\n     int num_ctas, int num_stages);\n \n-void CreateTritonPipeline(\n-    mlir::OpPassManager* pm,\n-    const stream_executor::GpuComputeCapability& gpu_cc, int num_warps,\n-    int num_ctas, int num_stages,\n-    mlir::triton::nvidia_gpu::ClusterInfo& out_cluster_info) {\n+void CreateTritonPipeline(mlir::OpPassManager* pm,\n+                          const stream_executor::GpuComputeCapability& gpu_cc,\n+                          int num_warps, int num_ctas, int num_stages) {\n   if (auto* cuda_cc = gpu_cc.cuda_compute_capability()) {\n     return CreateTritonCudaPipeline(pm, *cuda_cc, num_warps, num_ctas,\n-                                    num_stages, out_cluster_info);\n+                                    num_stages);\n   }\n \n   CreateTritonRocmPipeline(pm, *gpu_cc.rocm_compute_capability(), num_warps,\n                            num_ctas, num_stages);\n-  // There is no clusters in ROCm for now.\n-  out_cluster_info.clusterDimX = 1;\n-  out_cluster_info.clusterDimY = 1;\n-  out_cluster_info.clusterDimZ = 1;\n }\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "2d4fc0ee7fa5b3fa2bf1d322a5af1330c7669be9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline.h",
            "status": "modified",
            "additions": 3,
            "deletions": 14,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.h?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -18,7 +18,6 @@ limitations under the License.\n \n #include \"mlir/Pass/PassManager.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n \n namespace xla::gpu {\n \n@@ -29,19 +28,9 @@ void CreateTritonXlaPipeline(\n     bool allow_tma, int num_stages);\n \n // Creates a Triton compilation pipeline.\n-//\n-// `out_cluster_info` must be kept alive at least until pm.run() is called.\n-// It should be read after that. We have to pass the cluster dims to\n-// LaunchDimensions. Triton currently uses this as an out-parameter to return\n-// the cluster dims determined based on `config.num_ctas` and a heuristic. There\n-// are some signs that show that this was intended to be used as an in-out\n-// parameter which would give a hint to Triton which cluster dims we prefer to\n-// use, but that's not the case currently.\n-void CreateTritonPipeline(\n-    mlir::OpPassManager* pm,\n-    const stream_executor::GpuComputeCapability& gpu_cc, int num_warps,\n-    int num_ctas, int num_stages,\n-    mlir::triton::nvidia_gpu::ClusterInfo& out_cluster_info);\n+void CreateTritonPipeline(mlir::OpPassManager* pm,\n+                          const stream_executor::GpuComputeCapability& gpu_cc,\n+                          int num_warps, int num_ctas, int num_stages);\n \n }  // namespace xla::gpu\n "
        },
        {
            "sha": "00ba104d3bc82e9f33aa8449d07f0e80c6ef29d9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline_cuda.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -61,17 +61,14 @@ static void MakeTTIR(mlir::OpPassManager* pm,\n // @triton//:third_party/nvidia/backend/compiler.py\n static void MakeTTGIR(mlir::OpPassManager* pm,\n                       const stream_executor::CudaComputeCapability& cuda_cc,\n-                      int num_warps, int num_ctas, int num_stages,\n-                      mlir::triton::nvidia_gpu::ClusterInfo& out_cluster_info) {\n+                      int num_warps, int num_ctas, int num_stages) {\n   const int cuda_cc_as_int = cuda_cc.major * 10 + cuda_cc.minor;\n   pm->addPass(mt::createConvertTritonToTritonGPU(\n       {absl::StrFormat(\"cuda:%u\", cuda_cc_as_int), num_warps,\n        /*threads_per_warp=*/32, num_ctas}));\n   pm->addPass(mt::gpu::createTritonGPUCoalesce());\n-  if (cuda_cc.IsAtLeastAmpere()) {\n-    pm->addPass(mt::gpu::createTritonGPUF32DotTC());\n-  }\n-  pm->addPass(ttng::createTritonNvidiaGPUPlanCTAPass(&out_cluster_info));\n+  pm->addPass(mt::gpu::createTritonGPUF32DotTC({cuda_cc.IsAtLeastAmpere()}));\n+  pm->addPass(ttng::createTritonNvidiaGPUPlanCTAPass());\n   pm->addPass(mt::gpu::createTritonGPURemoveLayoutConversions());\n   pm->addPass(mt::gpu::createTritonGPUOptimizeThreadLocality());\n   pm->addPass(mt::gpu::createTritonGPUAccelerateMatmul());\n@@ -169,10 +166,9 @@ static void MakeLLIR(mlir::OpPassManager* pm,\n void CreateTritonCudaPipeline(\n     mlir::OpPassManager* pm,\n     const stream_executor::CudaComputeCapability& cuda_cc, int num_warps,\n-    int num_ctas, int num_stages,\n-    mlir::triton::nvidia_gpu::ClusterInfo& out_cluster_info) {\n+    int num_ctas, int num_stages) {\n   MakeTTIR(pm, cuda_cc);\n-  MakeTTGIR(pm, cuda_cc, num_warps, num_ctas, num_stages, out_cluster_info);\n+  MakeTTGIR(pm, cuda_cc, num_warps, num_ctas, num_stages);\n   MakeLLIR(pm, cuda_cc);\n }\n "
        },
        {
            "sha": "2786b61fc4fd73252477b4ca03fc669484a099bd",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline_rocm.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_rocm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_rocm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_rocm.cc?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -63,6 +63,7 @@ static void MakeTTGIR(mlir::OpPassManager* pm,\n       {absl::StrCat(\"hip:\", rocm_cc.gfx_version()), num_warps, threadsPerWarp,\n        num_ctas}));\n   pm->addPass(mt::gpu::createTritonGPUCoalesce());\n+  pm->addPass(mt::gpu::createTritonGPUF32DotTC({false}));\n   pm->addPass(mt::gpu::createTritonGPURemoveLayoutConversions());\n   pm->addPass(mt::gpu::createTritonGPUOptimizeThreadLocality());\n   // TODO ROCm Pass rocm_cc.gfx_version() after fixing issue with fmfa\n@@ -123,6 +124,7 @@ static void MakeLLIR(mlir::OpPassManager* pm,\n                      const stream_executor::RocmComputeCapability& rocm_cc,\n                      int num_stages) {\n   const int custom_lds_size = 0;\n+  pm->addPass(mlir::createTritonAMDGPUUpdateAsyncWaitCount());\n   pm->addPass(mlir::triton::AMD::createOptimizeLDSUsagePass(\n       rocm_cc.gfx_version(), custom_lds_size));\n   pm->addPass(mlir::createSCFToControlFlowPass());"
        },
        {
            "sha": "a1be422730922cca0cbbfbd62e0585c783db49c4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_test.cc?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"absl/algorithm/container.h\"\n #include \"absl/strings/str_join.h\"\n #include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/Pass/Pass.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n "
        },
        {
            "sha": "88443f240714771e0d290f8b6139f8038f096fea",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -203,9 +203,8 @@ absl::StatusOr<TritonFusion::EmitResult> TritonFusion::Emit(\n         local_module.get()));\n \n     return {{kernel->getName().str(), launch_dimensions,\n-             triton_wrapper_result.cluster_dim,\n-             triton_wrapper_result.shmem_bytes, /*binary=*/\"\",\n-             triton_wrapper_result.tma_metadata}};\n+             /*cluster_dim=*/std::nullopt, triton_wrapper_result.shmem_bytes,\n+             /*binary=*/\"\", triton_wrapper_result.tma_metadata}};\n   };\n \n   auto [status_or_entry, was_cached] =\n@@ -218,7 +217,8 @@ absl::StatusOr<TritonFusion::EmitResult> TritonFusion::Emit(\n           Thunk::ThunkInfo::WithProfileAnnotation(\n               &fusion, ir_emitter_context.GetNextThunkId()),\n           entry->kernel_name, kernel_arguments, entry->launch_dimensions,\n-          entry->cluster_dim, entry->shmem_bytes, entry->tma_metadata),\n+          /*cluster_dim=*/std::nullopt, entry->shmem_bytes,\n+          entry->tma_metadata),\n       was_cached ? nullptr : std::move(local_module)};\n }\n "
        },
        {
            "sha": "36fadbcef8869630bb962b407be24a5be58d7bb2",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/xtile_compiler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 25,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.cc?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -432,9 +432,7 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n         \"(num_warps, num_ctas, num_stages) must be positive, but got: (\",\n         num_warps, \", \", num_ctas, \", \", num_stages, \")\"));\n   }\n-  mlir::triton::nvidia_gpu::ClusterInfo cluster_info;\n-  CreateTritonPipeline(&pm, gpu_cc, num_warps, num_ctas, num_stages,\n-                       cluster_info);\n+  CreateTritonPipeline(&pm, gpu_cc, num_warps, num_ctas, num_stages);\n \n   // Triton generates pointers to the global address space, while XLA needs a\n   // kernel signature with pointers to the generic address space.\n@@ -496,24 +494,6 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n     }\n   }\n \n-  // `cluster_info` must be read after pm.run().\n-  std::optional<se::ClusterDim> cluster_dim;\n-  if (block_level_parameters.num_ctas > 1) {\n-    VLOG(3) << \"num_ctas: \" << block_level_parameters.num_ctas\n-            << \", cluster_info: \" << cluster_info.clusterDimX << \",\"\n-            << cluster_info.clusterDimY << \",\" << cluster_info.clusterDimZ;\n-    if (cluster_info.clusterDimX > 1 || cluster_info.clusterDimY > 1 ||\n-        cluster_info.clusterDimZ > 1) {\n-      cluster_dim =\n-          se::ClusterDim(cluster_info.clusterDimX, cluster_info.clusterDimY,\n-                         cluster_info.clusterDimZ);\n-    }\n-  } else {\n-    TF_RET_CHECK(cluster_info.clusterDimX == 1 &&\n-                 cluster_info.clusterDimY == 1 &&\n-                 cluster_info.clusterDimZ == 1);\n-  }\n-\n   SmallVector<mlir::LLVM::LLVMFuncOp> func_ops;\n   for (auto func : triton_module.getOps<mlir::LLVM::LLVMFuncOp>()) {\n     // Custom calls will also match to LLVMFuncOp, so we are only interested in\n@@ -535,10 +515,7 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n   // - TMA metadata.\n   // - Total threads per block. Computed from module attributes.\n   // - Captured NVVM annotations.\n-  TritonWrapperResult result = {shared_mem_bytes,\n-                                cluster_dim,\n-                                tma_metadata,\n-                                thread_dims,\n+  TritonWrapperResult result = {shared_mem_bytes, tma_metadata, thread_dims,\n                                 captured_nvvm_annotations,\n                                 std::move(ll_triton_module)};\n   return result;"
        },
        {
            "sha": "a1fe63ad299d0458d56b81dca2d38bb3ca3ff71a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/xtile_compiler.h",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.h?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -48,9 +48,6 @@ limitations under the License.\n \n namespace mlir {\n namespace triton {\n-namespace nvidia_gpu {\n-struct ClusterInfo;\n-}\n }  // namespace triton\n }  // namespace mlir\n \n@@ -59,7 +56,6 @@ namespace gpu {\n \n struct TritonWrapperResult {\n   int64_t shmem_bytes = 0;\n-  std::optional<se::ClusterDim> cluster_dim;\n   se::gpu::TmaMetadata tma_metadata;\n   se::ThreadDim thread_dims;\n "
        },
        {
            "sha": "282e84b109e24eae2746edd4d03f14ee85650004",
            "filename": "third_party/xla/xla/pjrt/c/pjrt_c_api_triton_extension.h",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_triton_extension.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_triton_extension.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_triton_extension.h?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -39,11 +39,8 @@ struct PJRT_Triton_Compile_Args {\n   const char* out_asm;  // owned\n   size_t out_asm_size;\n   int64_t out_smem_bytes;\n-  int out_cluster_dim_x;\n-  int out_cluster_dim_y;\n-  int out_cluster_dim_z;\n };\n-PJRT_DEFINE_STRUCT_TRAITS(PJRT_Triton_Compile_Args, out_cluster_dim_z);\n+PJRT_DEFINE_STRUCT_TRAITS(PJRT_Triton_Compile_Args, out_smem_bytes);\n \n // Compiles a given Triton kernel.\n typedef PJRT_Error* PJRT_Triton_Compile(PJRT_Triton_Compile_Args* args);"
        },
        {
            "sha": "4212c92475d24b01e9e681e7cd603dc620994cca",
            "filename": "third_party/xla/xla/pjrt/c/pjrt_c_api_triton_internal.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_triton_internal.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_triton_internal.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_triton_internal.cc?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -42,9 +42,6 @@ PJRT_Error* PJRT_Triton_Compile(PJRT_Triton_Compile_Args* args) {\n   args->out_asm = asm_copy;\n   args->out_asm_size = result.asm_text.size();\n   args->out_smem_bytes = result.smem_bytes;\n-  args->out_cluster_dim_x = result.cluster_dim_x;\n-  args->out_cluster_dim_y = result.cluster_dim_y;\n-  args->out_cluster_dim_z = result.cluster_dim_z;\n   return nullptr;\n }\n "
        },
        {
            "sha": "528922d38558bcd54e0bb953bda318347857db35",
            "filename": "third_party/xla/xla/pjrt/triton.h",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fpjrt%2Ftriton.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fpjrt%2Ftriton.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Ftriton.h?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -27,9 +27,6 @@ namespace xla::triton {\n struct CompilationResult {\n   std::string asm_text;\n   int64_t smem_bytes;\n-  int cluster_dim_x;\n-  int cluster_dim_y;\n-  int cluster_dim_z;\n };\n \n absl::StatusOr<CompilationResult> Compile(absl::string_view module,"
        },
        {
            "sha": "acd8866aa40bb69d09e1e6de77b5d61267808ed3",
            "filename": "third_party/xla/xla/pjrt/triton_cuda.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fpjrt%2Ftriton_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fpjrt%2Ftriton_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Ftriton_cuda.cc?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -228,13 +228,12 @@ absl::StatusOr<CompilationResult> Compile(absl::string_view module,\n \n   mlir::PassManager pm(&context);\n   pm.enableVerifier();\n-  mlir::triton::nvidia_gpu::ClusterInfo cluster_info;\n   TF_ASSIGN_OR_RETURN(\n       auto cuda_cc,\n       stream_executor::CudaComputeCapability::FromString(arch_name));\n   xla::gpu::CreateTritonPipeline(&pm,\n                                  stream_executor::GpuComputeCapability(cuda_cc),\n-                                 num_warps, num_ctas, num_stages, cluster_info);\n+                                 num_warps, num_ctas, num_stages);\n   if (failed(pm.run(*module_op))) {\n     return absl::InternalError(\"Failed to compile Triton IR to LLVM IR\");\n   }\n@@ -247,9 +246,6 @@ absl::StatusOr<CompilationResult> Compile(absl::string_view module,\n   return CompilationResult{\n       ptx,\n       shared_mem_bytes,\n-      cluster_info.clusterDimX,\n-      cluster_info.clusterDimY,\n-      cluster_info.clusterDimZ,\n   };\n }\n "
        },
        {
            "sha": "83a360383f2d60dceafa0ac89c93316ffca55ca0",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotune_cache_key.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.h?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -32,7 +32,7 @@ class AutotuneCacheKey {\n   // Tie a version to the cache key in order to invalidate the cache when\n   // necessary. This should be incremented on triton upgrades or any other\n   // changes that may affect the autotuning results.\n-  static constexpr int kCurrentVersion = 19;\n+  static constexpr int kCurrentVersion = 20;\n \n   AutotuneCacheKey(const se::DeviceDescription& device_description,\n                    const HloInstruction& instruction,"
        },
        {
            "sha": "2a1cf0e0cec05af70a97d716d0a476605d0f150f",
            "filename": "third_party/xla/xla/service/gpu/tests/xla-opt.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fxla-opt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fxla-opt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fxla-opt.cc?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -43,8 +43,6 @@ limitations under the License.\n \n namespace {\n \n-mlir::triton::nvidia_gpu::ClusterInfo cluster_info;\n-\n struct TritonPipelineOptions\n     : public mlir::PassPipelineOptions<TritonPipelineOptions> {\n   Option<std::string> target{*this, \"target\", llvm::cl::init(\"8.0\")};\n@@ -75,8 +73,7 @@ mlir::PassPipelineRegistration<TritonPipelineOptions>\n                                             options.allow_tma,\n                                             options.num_stages);\n           xla::gpu::CreateTritonPipeline(&pm, gpu_cc, options.num_warps,\n-                                         options.num_ctas, options.num_stages,\n-                                         cluster_info);\n+                                         options.num_ctas, options.num_stages);\n         });\n \n }  // namespace"
        },
        {
            "sha": "3822ebc2438363841900aa446c28b441f0e5882e",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d4c9bda1fc4f1da8060dc44ae2be374da5ce129f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc?ref=d4c9bda1fc4f1da8060dc44ae2be374da5ce129f",
            "patch": "@@ -1340,7 +1340,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTritonCustomCall(\n     }\n \n     kernel_modules_.push_back(std::move(result.llvm_module));\n-    return {{kernel_name, launch_dimensions, result.cluster_dim,\n+    return {{kernel_name, launch_dimensions, /*cluster_dim=*/std::nullopt,\n              result.shmem_bytes}};\n   };\n \n@@ -1358,7 +1358,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTritonCustomCall(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n       entry->kernel_name, kernel_arguments, entry->launch_dimensions,\n-      entry->cluster_dim, entry->shmem_bytes, entry->tma_metadata));\n+      /*cluster_dim=*/std::nullopt, entry->shmem_bytes, entry->tma_metadata));\n }\n \n absl::StatusOr<ThunkSequence> ThunkEmitter::EmitAsyncComputation("
        }
    ],
    "stats": {
        "total": 488,
        "additions": 225,
        "deletions": 263
    }
}