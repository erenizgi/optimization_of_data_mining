{
    "author": "ermilovmaxim",
    "message": "Port to new GpuComputeCapability API. Part 2\n\nPiperOrigin-RevId: 822183464",
    "sha": "f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
    "files": [
        {
            "sha": "dcc6cf8e9f1a9ca2a478d003cc828a3ed804490d",
            "filename": "third_party/xla/xla/service/algorithm_util.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 17,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Falgorithm_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Falgorithm_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Falgorithm_util.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -223,28 +223,20 @@ bool IsSupportedDotAlgorithmOnGpu(\n     PrimitiveType output_storage_type) {\n   // Note: We may want to add some complex types here if people request that.\n   const bool is_cuda_ge_ampere =\n-      std::holds_alternative<se::CudaComputeCapability>(\n-          gpu_compute_capability) &&\n-      std::get<se::CudaComputeCapability>(gpu_compute_capability)\n-          .IsAtLeastAmpere();\n+      gpu_compute_capability.IsCuda() &&\n+      gpu_compute_capability.cuda_compute_capability()->IsAtLeastAmpere();\n \n   const bool is_cuda_ge_ada =\n-      std::holds_alternative<se::CudaComputeCapability>(\n-          gpu_compute_capability) &&\n-      std::get<se::CudaComputeCapability>(gpu_compute_capability)\n-          .IsAtLeast(8, 9);\n+      gpu_compute_capability.IsCuda() &&\n+      gpu_compute_capability.cuda_compute_capability()->IsAtLeast(8, 9);\n \n   const bool is_rocm_mi100_and_above =\n-      std::holds_alternative<se::RocmComputeCapability>(\n-          gpu_compute_capability) &&\n-      std::get<se::RocmComputeCapability>(gpu_compute_capability)\n-          .gfx9_mi100_or_later();\n+      gpu_compute_capability.IsRocm() &&\n+      gpu_compute_capability.rocm_compute_capability()->gfx9_mi100_or_later();\n \n-  const bool is_rocm_bf16 =\n-      std::holds_alternative<se::RocmComputeCapability>(\n-          gpu_compute_capability) &&\n-      std::get<se::RocmComputeCapability>(gpu_compute_capability)\n-          .has_bf16_dtype_support();\n+  const bool is_rocm_bf16 = gpu_compute_capability.IsRocm() &&\n+                            gpu_compute_capability.rocm_compute_capability()\n+                                ->has_bf16_dtype_support();\n \n   switch (algorithm) {\n     case PrecisionConfig::ALG_DOT_ANY_F8_ANY_F8_F32:"
        },
        {
            "sha": "a779a273b5ff624af60bc4bf6d8d19bd262c271d",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -332,6 +332,7 @@ cc_library(\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/stream_executor/rocm:rocm_compute_capability\",\n     ],\n )\n "
        },
        {
            "sha": "48579c8cfd3ebaacb04280c2c0762c97e0031e88",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -110,7 +110,7 @@ class ConvBfloat16Support : public FloatSupport {\n }  // namespace\n \n absl::Status AMDGPUCompiler::OptimizeHloConvolutionCanonicalization(\n-    HloModule* hlo_module, se::GpuComputeCapability gpu_version,\n+    HloModule* hlo_module, const se::GpuComputeCapability& gpu_version,\n     se::dnn::VersionInfo dnn_version,\n     const se::SemanticVersion& toolkit_version) {\n   // Convert convolutions into CustomCalls to MIOpen, then canonicalize them\n@@ -121,16 +121,15 @@ absl::Status AMDGPUCompiler::OptimizeHloConvolutionCanonicalization(\n       /*allow_mixed_precision=*/false);\n \n   // Convert unsupported bf16 convolutions to f32.\n-  ConvBfloat16Support conv_bf16_support(\n-      std::get<se::RocmComputeCapability>(gpu_version));\n+  ConvBfloat16Support conv_bf16_support(*gpu_version.rocm_compute_capability());\n   pipeline.AddPass<FloatNormalization>(&conv_bf16_support);\n \n   pipeline.AddPass<GpusolverRewriter>(\n       stream_executor::RocmSolverContext::Create);\n   pipeline.AddPass<ConvRewriter>(gpu_version);\n   pipeline.AddPass<ConvPaddingLegalization>();\n-  auto rcc = std::get<se::RocmComputeCapability>(gpu_version);\n-  pipeline.AddPass<CudnnFusedConvRewriter>(rcc, dnn_version, toolkit_version);\n+  auto rcc = gpu_version.rocm_compute_capability();\n+  pipeline.AddPass<CudnnFusedConvRewriter>(*rcc, dnn_version, toolkit_version);\n \n   // The conv padding/vectorization passes which we need to get rid of.  They\n   // also leave behind unnecessary tuple/get-tuple-element pairs that\n@@ -189,14 +188,12 @@ absl::Status AMDGPUCompiler::OptimizeHloPostLayoutAssignment(\n     const GpuAliasInfo* alias_info, tsl::thread::ThreadPool* thread_pool) {\n   HloPassPipeline pre_pipeline(\"AMDGPU post-layout_assignment part 1\");\n \n-  auto rocm_compute_capability = std::get<se::RocmComputeCapability>(\n-      gpu_target_config.device_description.gpu_compute_capability());\n-\n   pre_pipeline.AddPass<DotDimensionMerger>();\n \n   for (const auto& req : HipblasPaddingRequirements) {\n-    pre_pipeline.AddPass<CublasPadForGemms>(rocm_compute_capability,\n-                                            req.data_type, req.multiple_of);\n+    pre_pipeline.AddPass<CublasPadForGemms>(\n+        gpu_target_config.device_description.gpu_compute_capability(),\n+        req.data_type, req.multiple_of);\n   }\n   // Padding a gemm operand that's a constant results in pad(constant).  Run\n   // constant-folding to simplify this into a new constant."
        },
        {
            "sha": "4de7401a5f2e4a11a618ca46cf3da1b42cb7d306",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.h?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -47,7 +47,7 @@ class AMDGPUCompiler : public GpuCompiler {\n   AMDGPUCompiler();\n \n   absl::Status OptimizeHloConvolutionCanonicalization(\n-      HloModule* hlo_module, se::GpuComputeCapability gpu_version,\n+      HloModule* hlo_module, const se::GpuComputeCapability& gpu_version,\n       se::dnn::VersionInfo dnn_version,\n       const se::SemanticVersion& toolkit_version) override;\n "
        },
        {
            "sha": "cb63391039d22afdb513cf9255ca8846aa87439d",
            "filename": "third_party/xla/xla/service/gpu/conv_layout_normalization_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fconv_layout_normalization_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fconv_layout_normalization_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fconv_layout_normalization_test.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -34,11 +34,11 @@ class ConvolutionLayoutNormalizationTest : public HloTestBase {\n         .cuda_compute_capability();\n   }\n   bool IsRocm() {\n-    return std::holds_alternative<se::RocmComputeCapability>(\n-        backend()\n-            .default_stream_executor()\n-            ->GetDeviceDescription()\n-            .gpu_compute_capability());\n+    return backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .gpu_compute_capability()\n+        .IsRocm();\n   }\n };\n "
        },
        {
            "sha": "aad712107a7e1fd8dcd8a2b7b79277a955fd8d95",
            "filename": "third_party/xla/xla/service/gpu/determinism_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdeterminism_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdeterminism_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdeterminism_test.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -149,11 +149,11 @@ class DeterminismTest : public GpuCodegenTest {\n   bool IsAmpereOrLater() const { return get_cuda_cc().IsAtLeastAmpere(); }\n \n   bool IsRocm() const {\n-    return std::holds_alternative<stream_executor::RocmComputeCapability>(\n-        backend()\n-            .default_stream_executor()\n-            ->GetDeviceDescription()\n-            .gpu_compute_capability());\n+    return backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .gpu_compute_capability()\n+        .IsRocm();\n   }\n \n   bool HasHipblasLt() const {"
        },
        {
            "sha": "ac7338b5049f03c0d6511ab1784214bca2658f03",
            "filename": "third_party/xla/xla/service/gpu/dot_algorithm_support_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdot_algorithm_support_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdot_algorithm_support_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdot_algorithm_support_test.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -158,11 +158,10 @@ TEST_P(DotAlgorithmSupportTest, AlgorithmIsSupportedFromCudaCapability) {\n   bool is_algorithm_supported = false;\n   auto gpu_cc = GetGpuComputeCapability();\n \n-  if (const auto* ccc = std::get_if<se::CudaComputeCapability>(&gpu_cc)) {\n+  if (const auto* ccc = gpu_cc.cuda_compute_capability()) {\n     is_algorithm_supported =\n         ccc->SupportsAllFeaturesOf(params.min_cuda_capability);\n-  } else if (const auto* rcc =\n-                 std::get_if<se::RocmComputeCapability>(&gpu_cc)) {\n+  } else if (const auto* rcc = gpu_cc.rocm_compute_capability()) {\n     is_algorithm_supported = rcc->gfx9_mi100_or_later();\n     if (GetDeviceDescription().runtime_version() < params.min_rocm_version &&\n         (params.lhs_storage_type == F8E5M2 ||"
        },
        {
            "sha": "c3d30adae6dd5a0c3ff37b92ed2b798efc84edcb",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -765,8 +765,9 @@ absl::Status RunOptimizationPasses(\n   pipeline.AddPass<DotDecomposer>();\n \n   HloPredicate upcaster_filter = [&](const HloInstruction* instr) {\n-    const auto* cuda_cc = std::get_if<se::CudaComputeCapability>(\n-        &gpu_target_config.device_description.gpu_compute_capability());\n+    const auto* cuda_cc =\n+        gpu_target_config.device_description.gpu_compute_capability()\n+            .cuda_compute_capability();\n     if (cuda_cc != nullptr &&\n         !cuda_cc->IsAtLeast(se::CudaComputeCapability::kVolta)) {\n       return true;\n@@ -1768,8 +1769,8 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n     se::GpuComputeCapability gpu_version =\n         gpu_target_config.device_description.gpu_compute_capability();\n     pipeline.AddPass<AlgorithmChecker>(gpu_version);\n-    const auto* cuda_cc = std::get_if<se::CudaComputeCapability>(&gpu_version);\n-    const auto* rocm_cc = std::get_if<se::RocmComputeCapability>(&gpu_version);\n+    const auto* cuda_cc = gpu_version.cuda_compute_capability();\n+    const auto* rocm_cc = gpu_version.rocm_compute_capability();\n \n     // Make sure that dots have at least 1 contracting dimension in the\n     // operands. Needs to happen shortly before the dot rewrite, as otherwise\n@@ -3021,8 +3022,8 @@ absl::Status GpuCompiler::RunPostSchedulingPipelines(\n     pipeline.AddPass<FusionWrapper>(gpu_device_info);\n   }\n \n-  const auto* cuda_cc = std::get_if<se::CudaComputeCapability>(\n-      &gpu_device_info.gpu_compute_capability());\n+  const auto* cuda_cc =\n+      gpu_device_info.gpu_compute_capability().cuda_compute_capability();\n   if (cuda_cc != nullptr && cuda_cc->IsAtLeastAmpere()) {\n     // This needs to run after every pass affecting fusions. The last passes\n     // that create new fusions are FusionWrapper and StreamAttributeAnnotator."
        },
        {
            "sha": "c6c49f99a736d84b0b1b432295320b63e0b947c1",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -252,7 +252,7 @@ class GpuCompiler : public LLVMCompiler {\n                                  const GpuAliasInfo* alias_info);\n \n   virtual absl::Status OptimizeHloConvolutionCanonicalization(\n-      HloModule* hlo_module, se::GpuComputeCapability gpu_version,\n+      HloModule* hlo_module, const se::GpuComputeCapability& gpu_version,\n       se::dnn::VersionInfo dnn_version,\n       const se::SemanticVersion& toolkit_version) = 0;\n "
        },
        {
            "sha": "b2343e54f09483e7049b4858739fc91fb5db5884",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 12,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -826,8 +826,7 @@ ENTRY main {\n                     .default_stream_executor()\n                     ->GetDeviceDescription()\n                     .gpu_compute_capability();\n-  bool is_cuda =\n-      std::holds_alternative<stream_executor::CudaComputeCapability>(gpu_cc);\n+  bool is_cuda = gpu_cc.IsCuda();\n   auto cuda_cc = get_cuda_cc();\n   auto rocm_cc = backend()\n                      .default_stream_executor()\n@@ -1388,11 +1387,11 @@ using GpuCompilerPassTest = GpuCompilerTest;\n \n TEST_F(GpuCompilerPassTest,\n        GpuCompilerRunsTritonGemmRewriterByDefaultFromAmpere) {\n-  bool is_rocm = std::holds_alternative<stream_executor::RocmComputeCapability>(\n-      backend()\n-          .default_stream_executor()\n-          ->GetDeviceDescription()\n-          .gpu_compute_capability());\n+  bool is_rocm = backend()\n+                     .default_stream_executor()\n+                     ->GetDeviceDescription()\n+                     .gpu_compute_capability()\n+                     .IsRocm();\n \n   bool expect_triton_gemm_rewriter_has_run =\n       get_cuda_cc().IsAtLeastAmpere() || is_rocm;\n@@ -2150,11 +2149,11 @@ class GpuCompilerSelectKTest\n TEST_P(GpuCompilerSelectKTest, SelectKOrCustomKernelThunk) {\n   auto [n, k, expected_impl] = GetParam();\n \n-  bool is_rocm = std::holds_alternative<stream_executor::RocmComputeCapability>(\n-      backend()\n-          .default_stream_executor()\n-          ->GetDeviceDescription()\n-          .gpu_compute_capability());\n+  bool is_rocm = backend()\n+                     .default_stream_executor()\n+                     ->GetDeviceDescription()\n+                     .gpu_compute_capability()\n+                     .IsRocm();\n \n   if (is_rocm && expected_impl == TopKImpl::kSelectK) {\n     GTEST_SKIP() << \"raft::select_k is not supported in ROCm.\";"
        },
        {
            "sha": "028bd2898b6c42d4f0d23ada452c49012835226e",
            "filename": "third_party/xla/xla/service/gpu/gpu_device_info_for_tests.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_device_info_for_tests.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_device_info_for_tests.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_device_info_for_tests.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n \n #include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/rocm/rocm_compute_capability.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n \n namespace xla {\n@@ -101,8 +102,8 @@ stream_executor::DeviceDescription TestGpuDeviceInfo::RTXB200SXMDeviceInfo(\n \n stream_executor::DeviceDescription TestGpuDeviceInfo::AMDMI210DeviceInfo() {\n   stream_executor::DeviceDescription b;\n-  b.set_gpu_compute_capability(\n-      stream_executor::RocmComputeCapability(\"gfx90a\"));\n+  b.set_gpu_compute_capability(stream_executor::GpuComputeCapability(\n+      stream_executor::RocmComputeCapability(\"gfx90a\")));\n   b.set_threads_per_block_limit(1024);\n   b.set_threads_per_warp(64);\n   b.set_shared_memory_per_block(64 * 1024);\n@@ -125,8 +126,8 @@ stream_executor::DeviceDescription TestGpuDeviceInfo::AMDMI210DeviceInfo() {\n \n stream_executor::DeviceDescription TestGpuDeviceInfo::AMDRX7900DeviceInfo() {\n   stream_executor::DeviceDescription b;\n-  b.set_gpu_compute_capability(\n-      stream_executor::RocmComputeCapability(\"gfx1100\"));\n+  b.set_gpu_compute_capability(stream_executor::GpuComputeCapability(\n+      stream_executor::RocmComputeCapability(\"gfx1100\")));\n   return b;\n }\n "
        },
        {
            "sha": "9261a21f542b5af00bb6b6ac6fe3b8545d8c7031",
            "filename": "third_party/xla/xla/service/gpu/gpu_device_info_for_tests.h",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_device_info_for_tests.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_device_info_for_tests.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_device_info_for_tests.h?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -26,13 +26,16 @@ class TestGpuDeviceInfo {\n  public:\n   static stream_executor::DeviceDescription RTXA6000DeviceInfo(\n       stream_executor::GpuComputeCapability cc =\n-          stream_executor::CudaComputeCapability(8, 9));\n+          stream_executor::GpuComputeCapability{\n+              stream_executor::CudaComputeCapability(8, 9)});\n   static stream_executor::DeviceDescription RTXH100SXMDeviceInfo(\n       stream_executor::GpuComputeCapability cc =\n-          stream_executor::CudaComputeCapability(9, 0));\n+          stream_executor::GpuComputeCapability{\n+              stream_executor::CudaComputeCapability(9, 0)});\n   static stream_executor::DeviceDescription RTXB200SXMDeviceInfo(\n       stream_executor::GpuComputeCapability cc =\n-          stream_executor::CudaComputeCapability(10, 0));\n+          stream_executor::GpuComputeCapability{\n+              stream_executor::CudaComputeCapability(10, 0)});\n   static stream_executor::DeviceDescription AMDMI210DeviceInfo();\n   static stream_executor::DeviceDescription AMDRX7900DeviceInfo();\n   // Returns default RTXA6000 or AMDMI210 device info"
        },
        {
            "sha": "106f18b1289ac35f9615a46552d1ba2485e51093",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 10,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -249,8 +249,7 @@ GpuExecutable::GpuExecutable(\n       constants_(std::move(params.constants)),\n       output_info_(std::move(params.output_info)),\n       enable_debug_info_manager_(params.enable_debug_info_manager) {\n-  if (std::holds_alternative<stream_executor::RocmComputeCapability>(\n-          gpu_version_)) {\n+  if (gpu_version_.IsRocm()) {\n     // ROCm uses hsaco hashes to distinguish between modules.\n     // Bad things happen if multiple modules with identical code are loaded.\n     binary_.resize(binary_.size() + 16);\n@@ -279,18 +278,15 @@ absl::Status GpuExecutable::CheckCompatibilityWithServiceExecutableRunOptions(\n     auto cc = main_stream->GetRocmComputeCapability();\n     std::string stream_arch = cc.gcn_arch_name();\n     std::string gpu_exec_arch =\n-        std::get<se::RocmComputeCapability>(gpu_version_).gcn_arch_name();\n+        gpu_version_.rocm_compute_capability()->gcn_arch_name();\n     TF_RET_CHECK(stream_arch == gpu_exec_arch)\n         << \"AMDGPU GCN ISA version mismatch; expected {\" << gpu_exec_arch\n         << \", but was \" << stream_arch;\n   } else if (platform_id == stream_executor::cuda::kCudaPlatformId) {\n-    se::GpuComputeCapability cc = main_stream->GetCudaComputeCapability();\n-    TF_RET_CHECK(std::get<se::CudaComputeCapability>(cc) ==\n-                 std::get<se::CudaComputeCapability>(gpu_version_))\n-        << \"Compute capability mismatch; expected {\"\n-        << std::get<se::CudaComputeCapability>(gpu_version_).ToString()\n-        << \"}, but was {\" << std::get<se::CudaComputeCapability>(cc).ToString()\n-        << \"}\";\n+    se::CudaComputeCapability cc = main_stream->GetCudaComputeCapability();\n+    TF_RET_CHECK(cc == *gpu_version_.cuda_compute_capability())\n+        << \"Compute capability mismatch; expected {\" << gpu_version_.ToString()\n+        << \"}, but was {\" << cc.ToString() << \"}\";\n   } else if (platform_id == stream_executor::sycl::kSyclPlatformId) {\n     // TODO: Add check.\n   } else {"
        },
        {
            "sha": "453f81e680f0f796583381802aaabe2739669933",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -147,7 +147,7 @@ TEST(GpuExecutableTest, RunThunkPasses) {\n     params.module_name = absl::StrCat(\"test_module\", execution_count++);\n     se::DeviceDescription device_description;\n     device_description.set_gpu_compute_capability(\n-        se::CudaComputeCapability::Volta());\n+        se::GpuComputeCapability{se::CudaComputeCapability::Volta()});\n     device_description.set_driver_version({12, 3, 0});\n     device_description.set_runtime_version({12, 3, 0});\n     params.device_description = device_description;\n@@ -395,7 +395,7 @@ TEST(GpuExecutableTest, DumpsMetadataListProto) {\n     params.module_name = absl::StrCat(\"test_module\", execution_count++);\n     se::DeviceDescription device_description;\n     device_description.set_gpu_compute_capability(\n-        se::CudaComputeCapability::Volta());\n+        se::GpuComputeCapability{se::CudaComputeCapability::Volta()});\n     device_description.set_driver_version({12, 3, 0});\n     device_description.set_runtime_version({12, 3, 0});\n     params.device_description = device_description;"
        },
        {
            "sha": "01576003232bde4a33de8614f187f87e58a6e086",
            "filename": "third_party/xla/xla/service/gpu/gpu_float_support.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -80,7 +80,7 @@ bool GpuFloatSupport::IsSupported(const HloInstruction& hlo) const {\n             TypeAndCC(F8E5M2, se::CudaComputeCapability::Hopper())}) {\n         if (LowPrecisionType() == type) {\n           auto* cuda_compute_capability =\n-              std::get_if<se::CudaComputeCapability>(&compute_capability_);\n+              compute_capability_.cuda_compute_capability();\n           // Do not normalize supported types inside Triton fused computations.\n           return cuda_compute_capability &&\n                  cuda_compute_capability->SupportsAllFeaturesOf(cc) &&\n@@ -120,16 +120,14 @@ bool GpuFloatSupport::IsSupported(const HloInstruction& hlo) const {\n     case HloOpcode::kExp:\n     case HloOpcode::kLog:\n       if (LowPrecisionType() == BF16) {\n-        auto* cuda_compute_capability =\n-            std::get_if<se::CudaComputeCapability>(&compute_capability_);\n-        return cuda_compute_capability != nullptr;\n+        return compute_capability_.IsCuda();\n       }\n       return false;\n     case HloOpcode::kMaximum:\n     case HloOpcode::kMinimum:\n       if (LowPrecisionType() == BF16) {\n         auto* cuda_compute_capability =\n-            std::get_if<se::CudaComputeCapability>(&compute_capability_);\n+            compute_capability_.cuda_compute_capability();\n         return cuda_compute_capability != nullptr &&\n                cuda_compute_capability->IsAtLeastAmpere();\n       }\n@@ -139,7 +137,7 @@ bool GpuFloatSupport::IsSupported(const HloInstruction& hlo) const {\n     case HloOpcode::kSubtract: {\n       if (LowPrecisionType() == BF16) {\n         auto* cuda_compute_capability =\n-            std::get_if<se::CudaComputeCapability>(&compute_capability_);\n+            compute_capability_.cuda_compute_capability();\n         return cuda_compute_capability != nullptr &&\n                cuda_compute_capability->IsAtLeastHopper();\n       }"
        },
        {
            "sha": "0668802ab226449a8a393ee302eba56e91b02051",
            "filename": "third_party/xla/xla/service/gpu/gpu_float_support.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support.h?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -28,7 +28,7 @@ namespace gpu {\n \n class GpuFloatSupport : public FloatSupport {\n  public:\n-  explicit GpuFloatSupport(se::GpuComputeCapability cc,\n+  explicit GpuFloatSupport(const se::GpuComputeCapability& cc,\n                            PrimitiveType low_precision_type,\n                            PrimitiveType high_precision_type = F32)\n       : FloatSupport(low_precision_type, high_precision_type),"
        },
        {
            "sha": "35d7468a28f605a27f322213d798bda1c5634cb9",
            "filename": "third_party/xla/xla/service/gpu/gpu_float_support_test.cc",
            "status": "modified",
            "additions": 171,
            "deletions": 112,
            "changes": 283,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support_test.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -155,107 +155,143 @@ class FloatSupportTest : public HloHardwareIndependentTestBase {\n };\n \n TEST_F(FloatSupportTest, ShouldAlwaysConvertFp8Dot) {\n-  TestDotConversion(F8E4M3FN, F8E4M3FN, F16,\n-                    se::CudaComputeCapability::Hopper(),\n-                    /*should_convert_lhs=*/true,\n-                    /*should_convert_rhs=*/true, F8E4M3FN);\n-\n-  TestDotConversion(F8E4M3FN, F8E4M3FN, F32,\n-                    se::CudaComputeCapability::Hopper(),\n-                    /*should_convert_lhs=*/true,\n-                    /*should_convert_rhs=*/true, F8E4M3FN);\n-\n-  TestDotConversion(F8E4M3FN, F8E4M3FN, F16,\n-                    se::CudaComputeCapability::Ampere(),\n-                    /*should_convert_lhs=*/true,\n-                    /*should_convert_rhs=*/true, F8E4M3FN);\n-\n-  TestDotConversion(F8E4M3FN, F8E4M3FN, F32,\n-                    se::CudaComputeCapability::Hopper(),\n-                    /*should_convert_lhs=*/true,\n-                    /*should_convert_rhs=*/true, F8E4M3FN);\n-\n-  TestDotConversion(F8E5M2, F8E5M2, F16, se::CudaComputeCapability::Ampere(),\n-                    /*should_convert_lhs=*/true,\n-                    /*should_convert_rhs=*/true, F8E5M2);\n-\n-  TestDotConversion(F8E5M2, F8E5M2, F32, se::CudaComputeCapability::Ampere(),\n-                    /*should_convert_lhs=*/true,\n-                    /*should_convert_rhs=*/true, F8E5M2);\n-\n-  TestDotConversion(F8E5M2, F8E4M3FN, F16, se::CudaComputeCapability::Hopper(),\n-                    /*should_convert_lhs=*/true,\n-                    /*should_convert_rhs=*/false, F8E5M2);\n-\n-  TestDotConversion(F8E5M2, F8E4M3FN, F32, se::CudaComputeCapability::Hopper(),\n-                    /*should_convert_lhs=*/true,\n-                    /*should_convert_rhs=*/false, F8E5M2);\n-\n-  TestDotConversion(F8E5M2, F16, F16, se::CudaComputeCapability::Hopper(),\n-                    /*should_convert_lhs=*/true,\n-                    /*should_convert_rhs=*/false, F8E5M2);\n-\n-  TestDotConversion(F8E5M2, F16, F32, se::CudaComputeCapability::Hopper(),\n-                    /*should_convert_lhs=*/true,\n-                    /*should_convert_rhs=*/false, F8E5M2);\n+  TestDotConversion(\n+      F8E4M3FN, F8E4M3FN, F16,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+      /*should_convert_lhs=*/true,\n+      /*should_convert_rhs=*/true, F8E4M3FN);\n+\n+  TestDotConversion(\n+      F8E4M3FN, F8E4M3FN, F32,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+      /*should_convert_lhs=*/true,\n+      /*should_convert_rhs=*/true, F8E4M3FN);\n+\n+  TestDotConversion(\n+      F8E4M3FN, F8E4M3FN, F16,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Ampere()},\n+      /*should_convert_lhs=*/true,\n+      /*should_convert_rhs=*/true, F8E4M3FN);\n+\n+  TestDotConversion(\n+      F8E4M3FN, F8E4M3FN, F32,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+      /*should_convert_lhs=*/true,\n+      /*should_convert_rhs=*/true, F8E4M3FN);\n+\n+  TestDotConversion(\n+      F8E5M2, F8E5M2, F16,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Ampere()},\n+      /*should_convert_lhs=*/true,\n+      /*should_convert_rhs=*/true, F8E5M2);\n+\n+  TestDotConversion(\n+      F8E5M2, F8E5M2, F32,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Ampere()},\n+      /*should_convert_lhs=*/true,\n+      /*should_convert_rhs=*/true, F8E5M2);\n+\n+  TestDotConversion(\n+      F8E5M2, F8E4M3FN, F16,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+      /*should_convert_lhs=*/true,\n+      /*should_convert_rhs=*/false, F8E5M2);\n+\n+  TestDotConversion(\n+      F8E5M2, F8E4M3FN, F32,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+      /*should_convert_lhs=*/true,\n+      /*should_convert_rhs=*/false, F8E5M2);\n+\n+  TestDotConversion(\n+      F8E5M2, F16, F16,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+      /*should_convert_lhs=*/true,\n+      /*should_convert_rhs=*/false, F8E5M2);\n+\n+  TestDotConversion(\n+      F8E5M2, F16, F32,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+      /*should_convert_lhs=*/true,\n+      /*should_convert_rhs=*/false, F8E5M2);\n }\n \n TEST_F(FloatSupportTest, ShouldConvertTritonUnsupportedFp8Dot) {\n-  TestTritonFusedDot(F8E4M3FN, F8E4M3FN, F16,\n-                     se::CudaComputeCapability::Hopper(),\n-                     /*should_convert_lhs=*/true,\n-                     /*should_convert_rhs=*/true, F8E4M3FN);\n-\n-  TestTritonFusedDot(F8E4M3FN, F8E4M3FN, F32,\n-                     se::CudaComputeCapability::Hopper(),\n-                     /*should_convert_lhs=*/false,\n-                     /*should_convert_rhs=*/false, F8E4M3FN);\n-\n-  TestTritonFusedDot(F8E4M3FN, F8E4M3FN, F16,\n-                     se::CudaComputeCapability::Ampere(),\n-                     /*should_convert_lhs=*/true,\n-                     /*should_convert_rhs=*/true, F8E4M3FN);\n-\n-  TestTritonFusedDot(F8E4M3FN, F8E4M3FN, F32,\n-                     se::CudaComputeCapability::Hopper(),\n-                     /*should_convert_lhs=*/false,\n-                     /*should_convert_rhs=*/false, F8E4M3FN);\n-\n-  TestTritonFusedDot(F8E5M2, F8E5M2, F16, se::CudaComputeCapability::Ampere(),\n-                     /*should_convert_lhs=*/true,\n-                     /*should_convert_rhs=*/true, F8E5M2);\n-\n-  TestTritonFusedDot(F8E5M2, F8E5M2, F32, se::CudaComputeCapability::Ampere(),\n-                     /*should_convert_lhs=*/true,\n-                     /*should_convert_rhs=*/true, F8E5M2);\n-\n-  TestTritonFusedDot(F8E5M2, F8E4M3FN, F16, se::CudaComputeCapability::Hopper(),\n-                     /*should_convert_lhs=*/true,\n-                     /*should_convert_rhs=*/false, F8E5M2);\n-\n-  TestTritonFusedDot(F8E5M2, F8E4M3FN, F32, se::CudaComputeCapability::Hopper(),\n-                     /*should_convert_lhs=*/false,\n-                     /*should_convert_rhs=*/false, F8E5M2);\n-\n-  TestTritonFusedDot(F8E5M2, F16, F16, se::CudaComputeCapability::Hopper(),\n-                     /*should_convert_lhs=*/true,\n-                     /*should_convert_rhs=*/false, F8E5M2);\n-\n-  TestTritonFusedDot(F8E5M2, F16, F32, se::CudaComputeCapability::Hopper(),\n-                     /*should_convert_lhs=*/true,\n-                     /*should_convert_rhs=*/false, F8E5M2);\n+  TestTritonFusedDot(\n+      F8E4M3FN, F8E4M3FN, F16,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+      /*should_convert_lhs=*/true,\n+      /*should_convert_rhs=*/true, F8E4M3FN);\n+\n+  TestTritonFusedDot(\n+      F8E4M3FN, F8E4M3FN, F32,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+      /*should_convert_lhs=*/false,\n+      /*should_convert_rhs=*/false, F8E4M3FN);\n+\n+  TestTritonFusedDot(\n+      F8E4M3FN, F8E4M3FN, F16,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Ampere()},\n+      /*should_convert_lhs=*/true,\n+      /*should_convert_rhs=*/true, F8E4M3FN);\n+\n+  TestTritonFusedDot(\n+      F8E4M3FN, F8E4M3FN, F32,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+      /*should_convert_lhs=*/false,\n+      /*should_convert_rhs=*/false, F8E4M3FN);\n+\n+  TestTritonFusedDot(\n+      F8E5M2, F8E5M2, F16,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Ampere()},\n+      /*should_convert_lhs=*/true,\n+      /*should_convert_rhs=*/true, F8E5M2);\n+\n+  TestTritonFusedDot(\n+      F8E5M2, F8E5M2, F32,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Ampere()},\n+      /*should_convert_lhs=*/true,\n+      /*should_convert_rhs=*/true, F8E5M2);\n+\n+  TestTritonFusedDot(\n+      F8E5M2, F8E4M3FN, F16,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+      /*should_convert_lhs=*/true,\n+      /*should_convert_rhs=*/false, F8E5M2);\n+\n+  TestTritonFusedDot(\n+      F8E5M2, F8E4M3FN, F32,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+      /*should_convert_lhs=*/false,\n+      /*should_convert_rhs=*/false, F8E5M2);\n+\n+  TestTritonFusedDot(\n+      F8E5M2, F16, F16,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+      /*should_convert_lhs=*/true,\n+      /*should_convert_rhs=*/false, F8E5M2);\n+\n+  TestTritonFusedDot(\n+      F8E5M2, F16, F32,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+      /*should_convert_lhs=*/true,\n+      /*should_convert_rhs=*/false, F8E5M2);\n }\n \n TEST_F(FloatSupportTest, ShouldKeepBf16OnAmpere) {\n-  TestDotConversion(BF16, BF16, F32, se::CudaComputeCapability::Ampere(),\n-                    /*should_convert_lhs=*/false,\n-                    /*should_convert_rhs=*/false, BF16);\n+  TestDotConversion(\n+      BF16, BF16, F32,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Ampere()},\n+      /*should_convert_lhs=*/false,\n+      /*should_convert_rhs=*/false, BF16);\n }\n \n TEST_F(FloatSupportTest, ShouldKeepBf16OnHopper) {\n-  TestDotConversion(BF16, BF16, F32, se::CudaComputeCapability::Hopper(),\n-                    /*should_convert_lhs=*/false,\n-                    /*should_convert_rhs=*/false, BF16);\n+  TestDotConversion(\n+      BF16, BF16, F32,\n+      se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+      /*should_convert_lhs=*/false,\n+      /*should_convert_rhs=*/false, BF16);\n }\n \n TEST_F(FloatSupportTest, Bf16ReducePrecisionIsNotNormalized) {\n@@ -270,7 +306,8 @@ ENTRY main {\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(kHloModule));\n-  EXPECT_FALSE(Normalize(module.get(), cc, BF16, F32));\n+  EXPECT_FALSE(\n+      Normalize(module.get(), se::GpuComputeCapability{cc}, BF16, F32));\n }\n \n TEST_F(FloatSupportTest, Bf16TotalOrderSortIsNotNormalized) {\n@@ -300,12 +337,17 @@ ENTRY sort {\n }\n )\";\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(hlo_text));\n+  EXPECT_FALSE(Normalize(\n+      module.get(),\n+      se::GpuComputeCapability{se::CudaComputeCapability::Volta()}, BF16, F32));\n   EXPECT_FALSE(\n-      Normalize(module.get(), se::CudaComputeCapability::Volta(), BF16, F32));\n-  EXPECT_FALSE(\n-      Normalize(module.get(), se::CudaComputeCapability::Ampere(), BF16, F32));\n+      Normalize(module.get(),\n+                se::GpuComputeCapability{se::CudaComputeCapability::Ampere()},\n+                BF16, F32));\n   EXPECT_FALSE(\n-      Normalize(module.get(), se::CudaComputeCapability::Hopper(), BF16, F32));\n+      Normalize(module.get(),\n+                se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+                BF16, F32));\n }\n \n TEST_F(FloatSupportTest, Bf16ExpIsNotNormalized) {\n@@ -320,7 +362,8 @@ ENTRY main {\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(kHloModule));\n-  EXPECT_FALSE(Normalize(module.get(), cc, BF16, F32));\n+  EXPECT_FALSE(\n+      Normalize(module.get(), se::GpuComputeCapability{cc}, BF16, F32));\n }\n \n TEST_F(FloatSupportTest, Bf16LogIsNotNormalized) {\n@@ -335,7 +378,8 @@ ENTRY main {\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(kHloModule));\n-  EXPECT_FALSE(Normalize(module.get(), cc, BF16, F32));\n+  EXPECT_FALSE(\n+      Normalize(module.get(), se::GpuComputeCapability{cc}, BF16, F32));\n }\n \n TEST_F(FloatSupportTest, Bf16MinimumIsOnlyNormalizedPreAmpere) {\n@@ -351,11 +395,16 @@ ENTRY main {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(kHloModule));\n   EXPECT_FALSE(\n-      Normalize(module.get(), se::CudaComputeCapability::Hopper(), BF16, F32));\n+      Normalize(module.get(),\n+                se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+                BF16, F32));\n   EXPECT_FALSE(\n-      Normalize(module.get(), se::CudaComputeCapability::Ampere(), BF16, F32));\n-  EXPECT_TRUE(\n-      Normalize(module.get(), se::CudaComputeCapability::Volta(), BF16, F32));\n+      Normalize(module.get(),\n+                se::GpuComputeCapability{se::CudaComputeCapability::Ampere()},\n+                BF16, F32));\n+  EXPECT_TRUE(Normalize(\n+      module.get(),\n+      se::GpuComputeCapability{se::CudaComputeCapability::Volta()}, BF16, F32));\n }\n \n TEST_F(FloatSupportTest, Bf16MaximumIsOnlyNormalizedPreAmpere) {\n@@ -371,11 +420,16 @@ ENTRY main {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(kHloModule));\n   EXPECT_FALSE(\n-      Normalize(module.get(), se::CudaComputeCapability::Hopper(), BF16, F32));\n+      Normalize(module.get(),\n+                se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+                BF16, F32));\n   EXPECT_FALSE(\n-      Normalize(module.get(), se::CudaComputeCapability::Ampere(), BF16, F32));\n-  EXPECT_TRUE(\n-      Normalize(module.get(), se::CudaComputeCapability::Volta(), BF16, F32));\n+      Normalize(module.get(),\n+                se::GpuComputeCapability{se::CudaComputeCapability::Ampere()},\n+                BF16, F32));\n+  EXPECT_TRUE(Normalize(\n+      module.get(),\n+      se::GpuComputeCapability{se::CudaComputeCapability::Volta()}, BF16, F32));\n }\n \n TEST_F(FloatSupportTest,\n@@ -400,13 +454,15 @@ ENTRY main {\n   TF_ASSERT_OK_AND_ASSIGN(auto module_with_supported_reducer,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(kHloModuleTemplate, \"add\")));\n-  EXPECT_FALSE(Normalize(module_with_supported_reducer.get(), cc, BF16, F32));\n+  EXPECT_FALSE(Normalize(module_with_supported_reducer.get(),\n+                         se::GpuComputeCapability{cc}, BF16, F32));\n \n   // There is no bf16 instruction for divide, however.\n   TF_ASSERT_OK_AND_ASSIGN(auto module_with_unsupported_reducer,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(kHloModuleTemplate, \"divide\")));\n-  EXPECT_TRUE(Normalize(module_with_unsupported_reducer.get(), cc, BF16, F32));\n+  EXPECT_TRUE(Normalize(module_with_unsupported_reducer.get(),\n+                        se::GpuComputeCapability{cc}, BF16, F32));\n }\n \n TEST_F(FloatSupportTest, BF16LogAndExpOnRocmIsNormalized) {\n@@ -422,12 +478,14 @@ ENTRY main {\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto module_log,\n       ParseAndReturnVerifiedModule(absl::Substitute(kHloModule, \"log\")));\n-  EXPECT_TRUE(Normalize(module_log.get(), cc, BF16, F32));\n+  EXPECT_TRUE(\n+      Normalize(module_log.get(), se::GpuComputeCapability{cc}, BF16, F32));\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module_exp,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(kHloModule, \"exponential\")));\n-  EXPECT_TRUE(Normalize(module_exp.get(), cc, BF16, F32));\n+  EXPECT_TRUE(\n+      Normalize(module_exp.get(), se::GpuComputeCapability{cc}, BF16, F32));\n }\n \n TEST_F(FloatSupportTest, ScaledDotIsIgnored) {\n@@ -448,7 +506,8 @@ TEST_F(FloatSupportTest, ScaledDotIsIgnored) {\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(kHloModule));\n-  EXPECT_FALSE(Normalize(module.get(), cc, BF16, F32));\n+  EXPECT_FALSE(\n+      Normalize(module.get(), se::GpuComputeCapability{cc}, BF16, F32));\n }\n \n }  // namespace"
        },
        {
            "sha": "9b767237fb406f2e4744de7a00ff3e1192389fc1",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_context.h",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -101,13 +101,11 @@ class IrEmitterContext {\n     return gpu_device_info_.gpu_compute_capability();\n   }\n   se::CudaComputeCapability cuda_compute_capability() const {\n-    auto* cc =\n-        std::get_if<se::CudaComputeCapability>(&gpu_compute_capability());\n+    auto* cc = gpu_compute_capability().cuda_compute_capability();\n     return cc != nullptr ? *cc : se::CudaComputeCapability();\n   }\n   se::RocmComputeCapability rocm_compute_capability() const {\n-    auto* cc =\n-        std::get_if<se::RocmComputeCapability>(&gpu_compute_capability());\n+    auto* cc = gpu_compute_capability().rocm_compute_capability();\n     return cc != nullptr ? *cc : se::RocmComputeCapability();\n   }\n "
        },
        {
            "sha": "1224476da8dc64ab50cdec0f31203d94d92ea8b3",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -825,8 +825,7 @@ absl::Status IrEmitterUnnested::EmitCublasLtMatmulThunkF8(\n       BufferAllocation::Slice b_scale,\n       GetAllocationSliceForHlo(instr->operand(a_scale_index + 1)));\n \n-  bool is_cuda = std::holds_alternative<stream_executor::CudaComputeCapability>(\n-      ir_emitter_context_->gpu_compute_capability());\n+  bool is_cuda = ir_emitter_context_->gpu_compute_capability().IsCuda();\n   bool is_fp8 = instr->shape().tuple_shapes(0).element_type() == F8E4M3FN ||\n                 instr->shape().tuple_shapes(0).element_type() == F8E5M2;\n   // cublasLT requires c_scale/d_scale to be null when C/D is not\n@@ -1402,8 +1401,7 @@ absl::Status IrEmitterUnnested::EmitTopKCustomCall(\n                           GetDefaultBufferAlignment(), instr));\n \n   auto dtype = data_shape.element_type();\n-  bool is_cuda = std::holds_alternative<stream_executor::CudaComputeCapability>(\n-      ir_emitter_context_->gpu_compute_capability());\n+  bool is_cuda = ir_emitter_context_->gpu_compute_capability().IsCuda();\n   if (is_cuda && instr->GetModule()\n                      ->config()\n                      .debug_options()"
        },
        {
            "sha": "2f8950b839117d5283adf748484635e7ea8aadea",
            "filename": "third_party/xla/xla/service/gpu/kernels/cutlass_gemm_fusion.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fcutlass_gemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fcutlass_gemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fcutlass_gemm_fusion.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -248,8 +248,7 @@ std::optional<CustomKernelFusionPattern::Match>\n CutlassGemmWithDynamicUpdateSlicePattern::TryMatch(\n     const se::DeviceDescription& device, HloInstruction* instr) const {\n   // This pattern is disabled for VOLTA. See b/380087823.\n-  if (std::holds_alternative<se::CudaComputeCapability>(\n-          device.gpu_compute_capability())) {\n+  if (device.gpu_compute_capability().IsCuda()) {\n     if (device.cuda_compute_capability().major ==\n         se::CudaComputeCapability::CudaComputeCapabilities::kVolta) {\n       return std::nullopt;"
        },
        {
            "sha": "f38969b5fb552496539be66cd6c29adef641ced1",
            "filename": "third_party/xla/xla/service/gpu/kernels/cutlass_gemm_fusion_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fcutlass_gemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fcutlass_gemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernels%2Fcutlass_gemm_fusion_test.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -289,8 +289,9 @@ TEST_F(CutlassFusionTest, DoNotRewriteOnV100) {\n   CustomKernelFusionPatternRegistry patterns;\n   patterns.Emplace<CutlassGemmWithDynamicUpdateSlicePattern>();\n \n-  auto device = TestGpuDeviceInfo::RTXA6000DeviceInfo(CudaComputeCapability{\n-      CudaComputeCapability::CudaComputeCapabilities::kVolta, 0});\n+  auto device = TestGpuDeviceInfo::RTXA6000DeviceInfo(\n+      stream_executor::GpuComputeCapability{CudaComputeCapability{\n+          CudaComputeCapability::CudaComputeCapabilities::kVolta, 0}});\n   CustomKernelFusionRewriter pass(&device, /*kernel_index=*/0, &patterns);\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(hlo));\n   TF_ASSERT_OK_AND_ASSIGN(bool changed,"
        },
        {
            "sha": "afcff98c67376172aead1d0ef0c1639a43745320",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/amdgpu_backend.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_backend.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_backend.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Famdgpu_backend.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -340,8 +340,7 @@ absl::Status AMDGPUTargetModuleLinker(\n     const std::string& device_bitcode_dir_path) {\n   // Link the input module with ROCDL.\n \n-  auto compute_capability =\n-      std::get_if<se::RocmComputeCapability>(&gpu_version);\n+  auto compute_capability = gpu_version.rocm_compute_capability();\n   if (!compute_capability) {\n     return xla::Internal(\"Incompatible compute capability was specified.\");\n   }\n@@ -421,8 +420,7 @@ std::pair<std::string, std::string> GetFeatureStrFromGCNArchName(\n std::unique_ptr<llvm::TargetMachine> AMDGPUGetTargetMachine(\n     llvm::Triple target_triple, se::GpuComputeCapability gpu_version,\n     const DebugOptions& debug_options) {\n-  auto compute_capability =\n-      std::get_if<se::RocmComputeCapability>(&gpu_version);\n+  auto compute_capability = gpu_version.rocm_compute_capability();\n \n   std::string gcn_arch_name = compute_capability->gcn_arch_name();\n   auto arch = GetFeatureStrFromGCNArchName(gcn_arch_name);\n@@ -606,8 +604,7 @@ absl::StatusOr<std::vector<uint8_t>> CompileToHsaco(\n         tsl::profiler::TraceMeLevel::kInfo);\n     XLA_SCOPED_LOGGING_TIMER(\"Compile module \" + module->getName().str());\n \n-    auto compute_capability =\n-        std::get_if<se::RocmComputeCapability>(&gpu_version);\n+    auto compute_capability = gpu_version.rocm_compute_capability();\n     if (!compute_capability) {\n       return xla::Internal(\"Incompatible compute capability was specified.\");\n     }"
        },
        {
            "sha": "278519790ab1b72578702ca0a19d6c5152a41d2f",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fgpu_backend_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fgpu_backend_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fgpu_backend_lib.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -264,9 +264,9 @@ absl::Status LinkAndOptimizeModule(\n   llvm::ModuleAnalysisManager mam;\n \n   xla::codegen::intrinsics::DeviceType device_type;\n-  if (std::holds_alternative<se::CudaComputeCapability>(gpu_version)) {\n+  if (gpu_version.IsCuda()) {\n     device_type = xla::codegen::intrinsics::DeviceType::kNvidiaGpu;\n-  } else if (std::holds_alternative<se::RocmComputeCapability>(gpu_version)) {\n+  } else if (gpu_version.IsRocm()) {\n     device_type = xla::codegen::intrinsics::DeviceType::kAmdGpu;\n   } else {\n     LOG(FATAL) << \"Unsupported GPU type\";"
        },
        {
            "sha": "18533563d58e2ab0f3391df4def068459a6e5e7e",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fnvptx_backend.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fnvptx_backend.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fnvptx_backend.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -318,8 +318,7 @@ absl::StatusOr<std::string> CompileToPtx(\n       return std::string();\n     }\n \n-    auto compute_capability =\n-        std::get_if<se::CudaComputeCapability>(&gpu_version);\n+    auto compute_capability = gpu_version.cuda_compute_capability();\n     if (!compute_capability) {\n       return xla::Internal(\"Incompatible compute capability was specified.\");\n     }"
        },
        {
            "sha": "1c571864c8d5362e62ba21eb48be53fab41526c0",
            "filename": "third_party/xla/xla/service/gpu/matmul_utils.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmatmul_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmatmul_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmatmul_utils.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -324,7 +324,7 @@ absl::StatusOr<bool> CanFoldTransposeOperandIntoDot(const HloInstruction& dot,\n   Shape c_matrix_shape = c_shape;\n   // hipBlasLt does not yet support the C matrix to be BF16 for fp8 matmul\n   // with fp8 output. Thus only do this for CUDA side.\n-  if (std::holds_alternative<se::CudaComputeCapability>(gpu_version) &&\n+  if (gpu_version.IsCuda() &&\n       primitive_util::IsF8Type(lhs_shape.element_type()) &&\n       primitive_util::IsF8Type(output_shape.element_type()) && (beta == 0.0)) {\n     // By default, if c is not present (i.e., beta is 0), c_shape will be the"
        },
        {
            "sha": "121bbb0282513b39c5836401883b5f65f4304c9f",
            "filename": "third_party/xla/xla/service/gpu/model/hlo_op_profiles.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fhlo_op_profiles.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fhlo_op_profiles.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fhlo_op_profiles.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -44,8 +44,8 @@ namespace gpu {\n \n /*static*/ std::string HloOpProfiles::GetProfileName(\n     const se::DeviceDescription& device_info) {\n-  if (auto* ptr = std::get_if<stream_executor::CudaComputeCapability>(\n-          &device_info.gpu_compute_capability())) {\n+  if (auto* ptr =\n+          device_info.gpu_compute_capability().cuda_compute_capability()) {\n     return absl::StrCat(\"sm_\", ptr->major, ptr->minor);\n   }\n   return \"<unknown>\";"
        },
        {
            "sha": "ca0395e16aa21a7672e05bee74541e96e043c97e",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 23,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -128,7 +128,7 @@ class ConvBfloat16Support : public FloatSupport {\n  public:\n   explicit ConvBfloat16Support(\n       se::dnn::VersionInfo cudnn_version,\n-      se::CudaComputeCapability cuda_compute_capability)\n+      const se::CudaComputeCapability& cuda_compute_capability)\n       : FloatSupport(BF16),\n         is_conv_bf16_supported_(cuda_compute_capability.IsAtLeast(\n             se::CudaComputeCapability::kAmpere)) {}\n@@ -154,7 +154,7 @@ class ConvBfloat16Support : public FloatSupport {\n class MatmulBfloat16Support : public FloatSupport {\n  public:\n   explicit MatmulBfloat16Support(\n-      se::CudaComputeCapability cuda_compute_capability)\n+      const se::CudaComputeCapability& cuda_compute_capability)\n       : FloatSupport(BF16),\n         is_matmul_bf16_supported_(cuda_compute_capability.IsAtLeast(\n             se::CudaComputeCapability::kAmpere)) {}\n@@ -179,11 +179,10 @@ class MatmulBfloat16Support : public FloatSupport {\n }  // namespace\n \n absl::Status NVPTXCompiler::OptimizeHloConvolutionCanonicalization(\n-    HloModule* hlo_module, se::GpuComputeCapability gpu_version,\n+    HloModule* hlo_module, const se::GpuComputeCapability& gpu_version,\n     se::dnn::VersionInfo dnn_version,\n     const se::SemanticVersion& toolkit_version) {\n-  auto cuda_compute_capability =\n-      std::get<se::CudaComputeCapability>(gpu_version);\n+  auto* cuda_compute_capability = gpu_version.cuda_compute_capability();\n   // Convert convolutions into CustomCalls to cudnn, then canonicalize them\n   // (ConvPaddingLegalization). Also expand cuSolver calls.\n   HloPassPipeline pipeline(\"conv_canonicalization\");\n@@ -192,23 +191,23 @@ absl::Status NVPTXCompiler::OptimizeHloConvolutionCanonicalization(\n       /*allow_mixed_precision=*/false);\n \n   // Convert unsupported bf16 convolutions to f32.\n-  ConvBfloat16Support conv_bf16_support(dnn_version, cuda_compute_capability);\n+  ConvBfloat16Support conv_bf16_support(dnn_version, *cuda_compute_capability);\n   pipeline.AddPass<FloatNormalization>(&conv_bf16_support);\n \n   // Convert unsupported bf16 matmuls to f32.\n-  MatmulBfloat16Support matmul_bf16_support(cuda_compute_capability);\n+  MatmulBfloat16Support matmul_bf16_support(*cuda_compute_capability);\n   pipeline.AddPass<FloatNormalization>(&matmul_bf16_support);\n \n   pipeline.AddPass<GpusolverRewriter>(\n       stream_executor::CudaSolverContext::Create);\n   if (!hlo_module->config()\n            .debug_options()\n            .xla_gpu_experimental_disable_binary_libraries()) {\n-    pipeline.AddPass<ConvRewriter>(cuda_compute_capability, dnn_version);\n-    pipeline.AddPass<CudnnFusedConvRewriter>(cuda_compute_capability,\n+    pipeline.AddPass<ConvRewriter>(gpu_version, dnn_version);\n+    pipeline.AddPass<CudnnFusedConvRewriter>(*cuda_compute_capability,\n                                              dnn_version, toolkit_version);\n     pipeline.AddPass<ConvPaddingLegalization>();\n-    pipeline.AddPass<CudnnPadForConvolutions>(cuda_compute_capability);\n+    pipeline.AddPass<CudnnPadForConvolutions>(*cuda_compute_capability);\n   }\n   // The conv padding/vectorization passes which we need to get rid of.  They\n   // also leave behind unnecessary tuple/get-tuple-element pairs that\n@@ -273,20 +272,21 @@ absl::Status NVPTXCompiler::OptimizeHloPostLayoutAssignment(\n     const GpuAliasInfo* alias_info, tsl::thread::ThreadPool* thread_pool) {\n   // This needs to run before GemmRewriter, which is part of\n   // OptimizeHloPostLayoutAssignment().\n-  auto cuda_compute_capability = std::get<se::CudaComputeCapability>(\n-      gpu_target_config.device_description.gpu_compute_capability());\n+  auto* cuda_compute_capability =\n+      gpu_target_config.device_description.gpu_compute_capability()\n+          .cuda_compute_capability();\n \n   HloPassPipeline pre_pipeline(\"nvptx post-layout_assignment part 1\");\n   if (hlo_module->config().debug_options().xla_gpu_enable_cudnn_layer_norm() &&\n       !hlo_module->config()\n            .debug_options()\n            .xla_gpu_experimental_disable_binary_libraries()) {\n     // Rewrite normalization patterns into cuDNN Custom Calls.\n-    pre_pipeline.AddPass<CudnnNormRewriter>(cuda_compute_capability);\n+    pre_pipeline.AddPass<CudnnNormRewriter>(*cuda_compute_capability);\n   }\n \n   pre_pipeline.AddPass<BlockScalingRewriter>(\n-      cuda_compute_capability.IsAtLeastBlackwell()\n+      cuda_compute_capability->IsAtLeastBlackwell()\n           ? gpu_target_config.dnn_version_info\n           : se::dnn::VersionInfo{});\n   pre_pipeline.AddPass<DotDimensionMerger>();\n@@ -296,11 +296,11 @@ absl::Status NVPTXCompiler::OptimizeHloPostLayoutAssignment(\n            .xla_gpu_experimental_disable_binary_libraries()) {\n     for (const CublasPaddingRequirement& requirement :\n          CublasPaddingRequirements) {\n-      if (cuda_compute_capability.SupportsAllFeaturesOf(\n+      if (cuda_compute_capability->SupportsAllFeaturesOf(\n               requirement.min_compute_capability)) {\n-        pre_pipeline.AddPass<CublasPadForGemms>(cuda_compute_capability,\n-                                                requirement.data_type,\n-                                                requirement.multiple_of);\n+        pre_pipeline.AddPass<CublasPadForGemms>(\n+            gpu_target_config.device_description.gpu_compute_capability(),\n+            requirement.data_type, requirement.multiple_of);\n       }\n     }\n   }\n@@ -691,8 +691,8 @@ NVPTXCompiler::CompileTargetBinary(\n           module_config.debug_options(),\n           /*is_autotuning_compilation=*/options.is_autotuning_compilation);\n \n-  se::CudaComputeCapability cc = std::get<se::CudaComputeCapability>(\n-      device_description.gpu_compute_capability());\n+  se::CudaComputeCapability cc =\n+      *device_description.gpu_compute_capability().cuda_compute_capability();\n \n   // This may print multiple lines per HLO compilation because of the\n   // parallelized compilation of LLVM modules.\n@@ -751,8 +751,8 @@ absl::StatusOr<std::vector<uint8_t>> NVPTXCompiler::LinkModules(\n     return std::vector<uint8_t>{};\n   }\n \n-  auto cc = std::get<stream_executor::CudaComputeCapability>(\n-      device_description.gpu_compute_capability());\n+  auto cc =\n+      device_description.gpu_compute_capability().cuda_compute_capability();\n \n   TF_ASSIGN_OR_RETURN(const se::cuda::CompilationProvider* compilation_provider,\n                       GetCompilationProvider(debug_options));\n@@ -772,7 +772,7 @@ absl::StatusOr<std::vector<uint8_t>> NVPTXCompiler::LinkModules(\n           << compilation_provider->name();\n   TF_ASSIGN_OR_RETURN(\n       se::cuda::Assembly assembly,\n-      compilation_provider->CompileAndLink(cc, inputs, compilation_options));\n+      compilation_provider->CompileAndLink(*cc, inputs, compilation_options));\n \n   return std::move(assembly.cubin);\n }"
        },
        {
            "sha": "3eaf8401202f5014067cdb43439779634ecba8d2",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.h?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -56,7 +56,7 @@ class NVPTXCompiler : public GpuCompiler {\n   explicit NVPTXCompiler();\n \n   absl::Status OptimizeHloConvolutionCanonicalization(\n-      HloModule* hlo_module, se::GpuComputeCapability gpu_version,\n+      HloModule* hlo_module, const se::GpuComputeCapability& gpu_version,\n       se::dnn::VersionInfo dnn_version,\n       const se::SemanticVersion& toolkit_version) override;\n "
        },
        {
            "sha": "d37e0f035d47f03b54135138a66835674999fa8c",
            "filename": "third_party/xla/xla/service/gpu/ptx_compilation_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fptx_compilation_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fptx_compilation_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fptx_compilation_test.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -149,15 +149,13 @@ class NVPTXCompilationTests\n                              PtxCompilationMethod compilation_method,\n                              PtxLinkingMethod linking_method) {\n     using CudaComputeCapability = stream_executor::CudaComputeCapability;\n-    if (!::testing::Value(\n-            backend()\n-                .default_stream_executor()\n-                ->GetDeviceDescription()\n-                .gpu_compute_capability(),\n-            ::testing::VariantWith<CudaComputeCapability>(\n-                CudaComputeCapability{9, 0,\n-                                      CudaComputeCapability::FeatureExtension::\n-                                          kAcceleratedFeatures})) &&\n+    auto cc = backend()\n+                  .default_stream_executor()\n+                  ->GetDeviceDescription()\n+                  .gpu_compute_capability();\n+    if ((cc.cuda_compute_capability()->major < 9 ||\n+         cc.cuda_compute_capability()->feature_extension !=\n+             CudaComputeCapability::FeatureExtension::kAcceleratedFeatures) &&\n         name == \"requires_sm90a\") {\n       GTEST_SKIP() << \"This test requires SM 9.0a\";\n     }"
        },
        {
            "sha": "a40b56b6f84f15766b02621b46cd9dbc38b4658c",
            "filename": "third_party/xla/xla/service/gpu/tests/command_buffer_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -47,8 +47,8 @@ se::StreamExecutor* GpuExecutor() {\n \n bool IsAtLeastCuda12900(const se::StreamExecutor* stream_executor) {\n   const auto& device_description = stream_executor->GetDeviceDescription();\n-  const auto* cuda_cc = std::get_if<se::CudaComputeCapability>(\n-      &device_description.gpu_compute_capability());\n+  const auto* cuda_cc =\n+      device_description.gpu_compute_capability().cuda_compute_capability();\n   if (cuda_cc != nullptr) {\n     if (device_description.driver_version() >=\n             stream_executor::SemanticVersion(12, 9, 0) &&"
        },
        {
            "sha": "38bedc99a77a868fe9503557f4318861444c1693",
            "filename": "third_party/xla/xla/service/gpu/tests/gpu_copy_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_copy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_copy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_copy_test.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -412,8 +412,7 @@ TEST_F(GpuCopyTest, UseDynamicMemcpyIntegrationTest) {\n                                 .default_stream_executor()\n                                 ->GetDeviceDescription()\n                                 .gpu_compute_capability();\n-  if (auto cc = std::get_if<stream_executor::CudaComputeCapability>(\n-          &compute_capability);\n+  if (auto cc = compute_capability.cuda_compute_capability();\n       !cc || !cc->IsAtLeastAmpere()) {\n     GTEST_SKIP() << \"Test requires at least Ampere.\";\n   }"
        },
        {
            "sha": "a8a0be5c78c63335360e5b329e75a3a950c5c23f",
            "filename": "third_party/xla/xla/service/gpu/tests/simplify_fp_conversions_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fsimplify_fp_conversions_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fsimplify_fp_conversions_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fsimplify_fp_conversions_test.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -37,8 +37,7 @@ class SimplifyFPConversionsTest : public HloTestBase {\n     const auto& device_description =\n         backend().default_stream_executor()->GetDeviceDescription();\n     const auto& cc = device_description.gpu_compute_capability();\n-    return std::holds_alternative<se::CudaComputeCapability>(cc) &&\n-           std::get<se::CudaComputeCapability>(cc).IsAtLeastHopper();\n+    return cc.IsCuda() && cc.cuda_compute_capability()->IsAtLeastHopper();\n   }\n \n   void SetEnableSimplifyFpConversions(bool enable_simplify_all_fp_conversions) {"
        },
        {
            "sha": "b338f506ea4d6577068affae6d5e55ec6fb1b295",
            "filename": "third_party/xla/xla/service/gpu/tests/swap_conv_operands_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fswap_conv_operands_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fswap_conv_operands_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fswap_conv_operands_test.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -61,8 +61,7 @@ ENTRY swap_conv {\n   TF_ASSERT_OK_AND_ASSIGN(se::GpuComputeCapability gpu_compute_capability,\n                           GpuComputeCapability());\n \n-  if (std::get_if<se::CudaComputeCapability>(&gpu_compute_capability)\n-          ->IsAtLeastHopper()) {\n+  if (gpu_compute_capability.cuda_compute_capability()->IsAtLeastHopper()) {\n     MatchOptimizedHloWithShapes(hlo_text,\n                                 R\"(\n // CHECK: [[cudnn_conv_1_0:%[^ ]+]] = (f32[1,32,32,128]{3,2,1,0}, u8[{{.*}}]{0}) custom-call(f32[1,30,30,512]{3,2,1,0} {{[^ ]+}}, f32[128,3,3,512]{3,2,1,0} {{[^ ]+}}), window={size=3x3 pad=2_2x2_2 rhs_reversal=1x1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\"\n@@ -93,8 +92,7 @@ ENTRY swap_conv {\n   TF_ASSERT_OK_AND_ASSIGN(se::GpuComputeCapability gpu_compute_capability,\n                           GpuComputeCapability());\n \n-  if (std::get_if<se::CudaComputeCapability>(&gpu_compute_capability)\n-          ->IsAtLeastHopper()) {\n+  if (gpu_compute_capability.cuda_compute_capability()->IsAtLeastHopper()) {\n     MatchOptimizedHloWithShapes(hlo_text,\n                                 R\"(\n // CHECK: [[cudnn_conv_1_0:%[^ ]+]] = (f32[1,32,32,128]{3,2,1,0}, u8[{{[0-9]*}}]{0}) custom-call(f32[1,30,30,512]{3,2,1,0} {{[^ ]+}}, f32[128,3,3,512]{3,2,1,0} {{[^ ]+}}), window={size=3x3 pad=2_2x2_2 rhs_reversal=1x1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\""
        },
        {
            "sha": "309498780a40994becc9ec43f510ed80c7033f02",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -143,7 +143,7 @@ bool IsGPUSyncCollective(const HloInstruction& instr) {\n absl::StatusOr<GPUCommunicationType> CommunicationType(\n     int num_devices_per_host, const HloChannelInstruction& instr,\n     const se::GpuComputeCapability& gpu_version) {\n-  if (!std::holds_alternative<se::CudaComputeCapability>(gpu_version)) {\n+  if (!gpu_version.IsCuda()) {\n     return absl::FailedPreconditionError(\"Only CUDA is supported.\");\n   }\n "
        },
        {
            "sha": "8da9a55bd95676967c7cbe902d7246a9345891dd",
            "filename": "third_party/xla/xla/service/gpu/transforms/command_buffer_scheduling_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -782,7 +782,7 @@ TEST_F(CommandBufferSchedulingTest, WhileNotCommand) {\n \n TEST_F(CommandBufferSchedulingTest, While) {\n   const auto& gpu_desc = GetGpuComputeCapability();\n-  if (std::holds_alternative<se::RocmComputeCapability>(gpu_desc)) {\n+  if (gpu_desc.IsRocm()) {\n     GTEST_SKIP() << \"Not supported for ROCm!\";\n   }\n   const char* hlo = R\"(\n@@ -847,7 +847,7 @@ TEST_F(CommandBufferSchedulingTest, While) {\n \n TEST_F(CommandBufferSchedulingTest, Conditional) {\n   const auto& gpu_desc = GetGpuComputeCapability();\n-  if (std::holds_alternative<se::RocmComputeCapability>(gpu_desc)) {\n+  if (gpu_desc.IsRocm()) {\n     GTEST_SKIP() << \"Not supported for ROCm!\";\n   }\n   const char* hlo = R\"("
        },
        {
            "sha": "b09477f06125b3b503e6ae419ae3ed437017d81a",
            "filename": "third_party/xla/xla/service/gpu/transforms/conv_rewriter.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_rewriter.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -80,28 +80,26 @@ absl::Status CheckTypes(HloInstruction* conv, const se::GpuComputeCapability cc,\n             \"but got convolution with FP8 type %s: %s\",\n             primitive_util::LowercasePrimitiveTypeName(type), conv->ToString());\n       }\n-      if (!std::holds_alternative<se::CudaComputeCapability>(cc)) {\n+      if (!cc.IsCuda()) {\n         return Unimplemented(\n             \"FP8 convolutions are only supported on CUDA GPUs, but got \"\n             \"FP8 convolution on ROCm GPU: %s\",\n             conv->ToString());\n       }\n       if (dnn_version >= se::dnn::VersionInfo{9, 8, 0}) {\n-        if (!std::get<se::CudaComputeCapability>(cc).IsAtLeastAda()) {\n+        if (!cc.cuda_compute_capability()->IsAtLeastAda()) {\n           return Unimplemented(\n               \"FP8 convolutions are only supported on CUDA GPUs with compute \"\n               \"capability at least 8.9, but got \"\n               \"FP8 convolution on GPU with compute capability %s: %s\",\n-              std::get<se::CudaComputeCapability>(cc).ToString(),\n-              conv->ToString());\n+              cc.ToString(), conv->ToString());\n         }\n-      } else if (!std::get<se::CudaComputeCapability>(cc).IsAtLeastHopper()) {\n+      } else if (!cc.cuda_compute_capability()->IsAtLeastHopper()) {\n         return Unimplemented(\n             \"FP8 convolutions are only supported on CUDA GPUs with compute \"\n             \"capability at least 9.0, but got \"\n             \"FP8 convolution on GPU with compute capability %s: %s\",\n-            std::get<se::CudaComputeCapability>(cc).ToString(),\n-            conv->ToString());\n+            cc.ToString(), conv->ToString());\n       }\n     }\n     return absl::OkStatus();"
        },
        {
            "sha": "cde6c0291b6c38b37d9ee9e1d05f505a8e434460",
            "filename": "third_party/xla/xla/service/gpu/transforms/conv_rewriter_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconv_rewriter_test.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -791,7 +791,10 @@ TEST_F(ConvRewriterTest, TestInvalidTypes) {\n                      ::testing::HasSubstr(\n                          \"FP8 convolutions are only supported on CUDA \"\n                          \"GPUs with compute capability at least 9.0\")));\n-  s = ConvRewriter(se::RocmComputeCapability{\"gfx942\"}).Run(m.get()).status();\n+  s = ConvRewriter(\n+          se::GpuComputeCapability{se::RocmComputeCapability{\"gfx942\"}})\n+          .Run(m.get())\n+          .status();\n   EXPECT_THAT(s, absl_testing::StatusIs(\n                      absl::StatusCode::kUnimplemented,\n                      ::testing::HasSubstr("
        },
        {
            "sha": "512f70956a2cb26bdd2147d30bb8b08e7a64caf4",
            "filename": "third_party/xla/xla/service/gpu/transforms/cublas_pad_for_gemms.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcublas_pad_for_gemms.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcublas_pad_for_gemms.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcublas_pad_for_gemms.h?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -39,7 +39,7 @@ namespace gpu {\n // so it should go strictly later.\n class CublasPadForGemms : public HloModulePass {\n  public:\n-  CublasPadForGemms(const se::GpuComputeCapability gpu_compute_capability,\n+  CublasPadForGemms(const se::GpuComputeCapability& gpu_compute_capability,\n                     PrimitiveType datatype, int32_t pad_to_multiple_of)\n       : gpu_compute_capability_(gpu_compute_capability),\n         datatype_(datatype),"
        },
        {
            "sha": "5ff4d17cd24a84b80953b24137b3235e30a7b641",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_fused_conv_rewriter.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fused_conv_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fused_conv_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fused_conv_rewriter.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -97,9 +97,7 @@ bool IsNonDepthwiseConvCustomCall(const HloInstruction* instr) {\n   return IsConvCustomCall(instr) && !IsConvDepthwise(instr);\n }\n \n-bool IsROCm(se::GpuComputeCapability cc) {\n-  return std::holds_alternative<se::RocmComputeCapability>(cc);\n-}\n+bool IsROCm(se::GpuComputeCapability cc) { return cc.IsRocm(); }\n \n // elu, relu6, and leaky-relu activations are supported in cudnn via the\n // \"runtime fusion\" engine, which JIT compiles C++ code.  This can be slow to\n@@ -112,7 +110,7 @@ bool IsROCm(se::GpuComputeCapability cc) {\n // due to apparent bugs in cudnn 8.9.0.  See debug_options_flags.cc for details.\n bool ShouldUseCudnnRuntimeFusion(const DebugOptions& debug_opts,\n                                  se::GpuComputeCapability cc) {\n-  const auto* cuda_cc = std::get_if<se::CudaComputeCapability>(&cc);\n+  const auto* cuda_cc = cc.cuda_compute_capability();\n   if (cuda_cc != nullptr)\n     return debug_opts.xla_gpu_use_runtime_fusion() && cuda_cc->IsAtLeast(7, 5);\n   else\n@@ -1697,9 +1695,9 @@ absl::StatusOr<bool> CudnnFusedConvRewriter::Run(\n     // Rewrite FP8 convolutions and supported adjacent pointwise ops into a\n     // ForwardGraph Custom Call.\n     if (!IsROCm(compute_capability_)) {\n-      auto cc = std::get<se::CudaComputeCapability>(compute_capability_);\n+      auto* cc = compute_capability_.cuda_compute_capability();\n       TF_ASSIGN_OR_RETURN(\n-          changed, F8GraphConv(comp, cc, dnn_version_, toolkit_version_));\n+          changed, F8GraphConv(comp, *cc, dnn_version_, toolkit_version_));\n       if (changed) {\n         return changed;\n       }"
        },
        {
            "sha": "2654ff59f6a3049212169e9c1b692be65efea782",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_fused_conv_rewriter.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fused_conv_rewriter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fused_conv_rewriter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fused_conv_rewriter.h?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -106,13 +106,13 @@ namespace gpu {\n // pass returns an error -- cudnn will not be able to run it.\n class CudnnFusedConvRewriter : public HloModulePass {\n  public:\n-  CudnnFusedConvRewriter(se::CudaComputeCapability cc,\n+  CudnnFusedConvRewriter(const se::CudaComputeCapability& cc,\n                          se::dnn::VersionInfo dnn_version,\n                          se::SemanticVersion toolkit_version)\n       : compute_capability_(cc),\n         dnn_version_(dnn_version),\n         toolkit_version_(toolkit_version) {}\n-  CudnnFusedConvRewriter(se::RocmComputeCapability cc,\n+  CudnnFusedConvRewriter(const se::RocmComputeCapability& cc,\n                          se::dnn::VersionInfo dnn_version,\n                          se::SemanticVersion toolkit_version)\n       : compute_capability_(cc),"
        },
        {
            "sha": "b46cfe0d8e1b4c0ee0c8c9a4c111632532becbc2",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_norm_rewriter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_norm_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_norm_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_norm_rewriter.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -1509,7 +1509,7 @@ absl::StatusOr<bool> RunOnComputation(\n }  // anonymous namespace\n \n CudnnNormRewriter::CudnnNormRewriter(\n-    se::CudaComputeCapability cuda_compute_capability)\n+    const se::CudaComputeCapability& cuda_compute_capability)\n     : cuda_compute_capability_(cuda_compute_capability) {}\n \n absl::StatusOr<bool> CudnnNormRewriter::Run("
        },
        {
            "sha": "0f6433dd5df59a6f353e874b88e6b4aff99438b3",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_norm_rewriter.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_norm_rewriter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_norm_rewriter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_norm_rewriter.h?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -30,7 +30,8 @@ namespace gpu {\n // forward and backward passes of layer norm patterns are implemented.\n class CudnnNormRewriter : public HloModulePass {\n  public:\n-  explicit CudnnNormRewriter(se::CudaComputeCapability cuda_compute_capability);\n+  explicit CudnnNormRewriter(\n+      const se::CudaComputeCapability& cuda_compute_capability);\n   absl::string_view name() const override { return \"norm-rewriter\"; }\n \n   using HloPassInterface::Run;"
        },
        {
            "sha": "491357bebc3a4c17c771c71f7fbc8f6880eaf65f",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_fusion_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -203,8 +203,10 @@ ENTRY e {\n   EXPECT_TRUE(CublasRequiresPadding(\n       *xla::Cast<HloDotInstruction>(\n           module->entry_computation()->root_instruction()),\n-      cc));\n-  EXPECT_TRUE(GemmFusion(cc).Run(module.get()).value());\n+      stream_executor::GpuComputeCapability{cc}));\n+  EXPECT_TRUE(GemmFusion(stream_executor::GpuComputeCapability{cc})\n+                  .Run(module.get())\n+                  .value());\n }\n \n TEST_F(GemmFusionTest, FuseSliceOfParameterWithOtherUsers) {\n@@ -902,7 +904,9 @@ ENTRY e {\n   ROOT dot = f32[2,2] dot(p0e, p1c),\n     lhs_contracting_dims={1}, rhs_contracting_dims={0}\n })\"));\n-  EXPECT_TRUE(GemmFusion(se::RocmComputeCapability{}).Run(module.get()).ok());\n+  EXPECT_TRUE(GemmFusion(se::GpuComputeCapability{se::RocmComputeCapability{}})\n+                  .Run(module.get())\n+                  .ok());\n }\n \n TEST_F(GemmFusionTest, ParameterUsedElementwiseTwiceIsFused) {"
        },
        {
            "sha": "3ae9a8088ce089d281a79bbb987294c6e4bdb9df",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_rewriter.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 12,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -806,8 +806,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n       }\n     }\n \n-    const auto is_rocm =\n-        std::holds_alternative<se::RocmComputeCapability>(gpu_version_);\n+    const auto is_rocm = gpu_version_.IsRocm();\n     if (is_rocm &&\n         toolkit_version_ >= stream_executor::SemanticVersion{7, 0, 0}) {\n       // Attempt to match approximate Swish activation\n@@ -1050,25 +1049,25 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n   }\n \n   static bool IsCuda(const se::GpuComputeCapability& gpu_version) {\n-    return std::holds_alternative<se::CudaComputeCapability>(gpu_version);\n+    return gpu_version.IsCuda();\n   }\n \n   static absl::StatusOr<se::CudaComputeCapability> GetCudaComputeCapability(\n       const se::GpuComputeCapability& gpu_version) {\n-    auto* cuda_cc = std::get_if<se::CudaComputeCapability>(&gpu_version);\n+    auto* cuda_cc = gpu_version.cuda_compute_capability();\n     if (cuda_cc == nullptr) {\n       return absl::InvalidArgumentError(\"Compute Capability is not CUDA.\");\n     }\n     return *cuda_cc;\n   }\n \n   static bool IsRocm(const se::GpuComputeCapability& gpu_version) {\n-    return std::holds_alternative<se::RocmComputeCapability>(gpu_version);\n+    return gpu_version.IsRocm();\n   }\n \n   static absl::StatusOr<se::RocmComputeCapability> GetRocmComputeCapability(\n       const se::GpuComputeCapability& gpu_version) {\n-    auto rocm_cc = std::get_if<se::RocmComputeCapability>(&gpu_version);\n+    auto rocm_cc = gpu_version.rocm_compute_capability();\n     if (rocm_cc == nullptr) {\n       return absl::InvalidArgumentError(\"Compute Capability is not ROCm.\");\n     }\n@@ -2416,8 +2415,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n       return false;\n     }\n \n-    if (auto isrocm = std::get_if<se::RocmComputeCapability>(&gpu_version_);\n-        isrocm) {\n+    if (auto isrocm = gpu_version_.rocm_compute_capability(); isrocm) {\n       if (!isrocm->has_hipblaslt()) {\n         return false;\n       }\n@@ -2431,8 +2429,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n       return true;\n     }\n \n-    if (std::holds_alternative<se::CudaComputeCapability>(gpu_version_)) {\n-      if (std::get<se::CudaComputeCapability>(gpu_version_).IsAtLeastAmpere()) {\n+    if (auto* ptr = gpu_version_.cuda_compute_capability()) {\n+      if (ptr->IsAtLeastAmpere()) {\n         // cuBlasLt has an implementation for complex data with compute type\n         // 32F_FAST_32TF that uses tensor cores and that is free from the\n         // restriction. This implementation only works on Ampere\n@@ -2535,11 +2533,11 @@ class GemmWorkspaceRewriteVisitor : public DfsHloRewriteVisitor {\n     // otherwise cuBLAS will use its own internal pool which will be competing\n     // with XLA allocator for device memory.\n     int64_t workspace = GemmConfig::kDefaultWorkspace;\n-    auto* cuda_cc = std::get_if<se::CudaComputeCapability>(&gpu_version_);\n+    auto* cuda_cc = gpu_version_.cuda_compute_capability();\n     if (cuda_cc != nullptr && cuda_cc->IsAtLeastHopper()) {\n       workspace = GemmConfig::kHopperWorkspace;\n     }\n-    auto* rocm_cc = std::get_if<se::RocmComputeCapability>(&gpu_version_);\n+    auto* rocm_cc = gpu_version_.rocm_compute_capability();\n     if (rocm_cc != nullptr) {\n       if (rocm_cc->gfx_version() == \"gfx942\") {\n         workspace = GemmConfig::kGFX942Workspace;"
        },
        {
            "sha": "c76b340ce86906e36670400922ef6f1246618b1e",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_rewriter_fp8_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_fp8_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_fp8_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_fp8_test.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -64,15 +64,15 @@ class ParameterizedFp8GemmRewriteTest\n       replacements_[kF8E4M3AmaxPlaceholder] = \"448.\";\n       return;\n     }\n-    if (IsRocm() && std::get<se::RocmComputeCapability>(Capability())\n-                        .has_ocp_fp8_support()) {\n+    if (IsRocm() &&\n+        Capability().rocm_compute_capability()->has_ocp_fp8_support()) {\n       replacements_[kF8E4M3DatatypePlaceholder] = \"f8e4m3fn\";\n       replacements_[kF8E5M2DatatypePlaceholder] = \"f8e5m2\";\n       replacements_[kF8E4M3AmaxPlaceholder] = \"448.\";\n       return;\n     }\n-    if (IsRocm() && std::get<se::RocmComputeCapability>(Capability())\n-                        .has_nanoo_fp8_support()) {\n+    if (IsRocm() &&\n+        Capability().rocm_compute_capability()->has_nanoo_fp8_support()) {\n       replacements_[kF8E4M3DatatypePlaceholder] = \"f8e4m3fnuz\";\n       replacements_[kF8E5M2DatatypePlaceholder] = \"f8e5m2fnuz\";\n       replacements_[kF8E4M3AmaxPlaceholder] = \"240.\";\n@@ -91,7 +91,7 @@ class ParameterizedFp8GemmRewriteTest\n     }\n \n     if (IsRocm() &&\n-        !std::get<se::RocmComputeCapability>(Capability()).has_fp8_support()) {\n+        !Capability().rocm_compute_capability()->has_fp8_support()) {\n       GTEST_SKIP()\n           << \"F8 gemm rewrite is only supported on MI300 and newer archs.\";\n     }\n@@ -2963,8 +2963,8 @@ TEST_P(ParameterizedFp8GemmRewriteTest, FnuzTypeF8) {\n       ROOT out = f32[16,16] dot(x_unscaled, y_unscaled), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n           }\n )\";\n-  if (IsRocm() && std::get<se::RocmComputeCapability>(Capability())\n-                      .has_nanoo_fp8_support()) {\n+  if (IsRocm() &&\n+      Capability().rocm_compute_capability()->has_nanoo_fp8_support()) {\n     EXPECT_TRUE(RunAndCompare(hlo_text, ErrorSpec{1e-2, 1e-2}));\n     RunAndFilecheckHloRewrite(\n         hlo_text,"
        },
        {
            "sha": "12ce56e59c1919a90a124ae58841e1246e80160d",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_rewriter_test_lib.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 16,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test_lib.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -50,19 +50,13 @@ stream_executor::SemanticVersion GemmRewriteTestBase::GetToolkitVersion()\n       .runtime_version();\n }\n \n-bool GemmRewriteTestBase::IsCuda() const {\n-  return std::holds_alternative<stream_executor::CudaComputeCapability>(\n-      Capability());\n-}\n+bool GemmRewriteTestBase::IsCuda() const { return Capability().IsCuda(); }\n \n-bool GemmRewriteTestBase::IsRocm() const {\n-  return std::holds_alternative<stream_executor::RocmComputeCapability>(\n-      Capability());\n-}\n+bool GemmRewriteTestBase::IsRocm() const { return Capability().IsRocm(); }\n \n bool GemmRewriteTestBase::IsBlackwell() const {\n   if (IsCuda()) {\n-    return std::get<se::CudaComputeCapability>(Capability()).IsBlackwell();\n+    return Capability().cuda_compute_capability()->IsBlackwell();\n   }\n   return false;\n }\n@@ -72,7 +66,8 @@ GemmRewriteTestBase::CudaHopperOrRocmCapability() {\n   if (IsCuda()) {\n     return se::CudaComputeCapability::Hopper();\n   }\n-  return std::get<se::RocmComputeCapability>(Capability());\n+  return stream_executor::GpuComputeCapability{\n+      *Capability().rocm_compute_capability()};\n }\n \n DebugOptions GemmRewriteTestBase::GetDebugOptionsForTest() const {\n@@ -86,22 +81,21 @@ DebugOptions GemmRewriteTestBase::GetDebugOptionsForTest() const {\n \n bool GemmRewriteTestBase::SkipGpuBlasLtTest() {\n   return !IsCuda() &&\n-         !std::get<stream_executor::RocmComputeCapability>(Capability())\n-              .has_hipblaslt() &&\n+         !Capability().rocm_compute_capability()->has_hipblaslt() &&\n          GetDebugOptionsForTest().xla_gpu_enable_cublaslt();\n }\n \n bool GemmRewriteTestBase::HasFp8Support() const {\n   if (IsCuda()) {\n-    return std::get<se::CudaComputeCapability>(Capability()).IsAtLeast(8, 9);\n+    return Capability().cuda_compute_capability()->IsAtLeast(8, 9);\n   }\n-  return std::get<se::RocmComputeCapability>(Capability()).has_fp8_support();\n+  return Capability().rocm_compute_capability()->has_fp8_support();\n }\n \n bool GemmRewriteTestBase::HasCudaComputeCapability(\n     const stream_executor::CudaComputeCapability& cc) const {\n-  return IsCuda() && std::get<se::CudaComputeCapability>(Capability())\n-                         .SupportsAllFeaturesOf(cc);\n+  return IsCuda() &&\n+         Capability().cuda_compute_capability()->SupportsAllFeaturesOf(cc);\n }\n \n ParameterizedGemmRewriteTestBase::ParameterizedGemmRewriteTestBase() {"
        },
        {
            "sha": "766b1e07a1042cbb2c6af29f099aba3190e6a56b",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -140,7 +140,7 @@ HeuristicLayoutAssignment(const HloInstruction* instr,\n   // Despite the specialized logic below for Volta, we expect GPUs with Tensor\n   // Cores work best using NHWC layouts for cuDNN convolutions---as per\n   // https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html#tensor-layout.\n-  if (auto* cc = std::get_if<se::CudaComputeCapability>(&gpu_version)) {\n+  if (auto* cc = gpu_version.cuda_compute_capability()) {\n     // TODO(b/383560056): investigate chips below Hopper as well.\n     if (cc->IsAtLeast(se::CudaComputeCapability::kHopper)) {\n       // With that said, cuDNN's documentation states that NHWC is not supported\n@@ -163,29 +163,27 @@ HeuristicLayoutAssignment(const HloInstruction* instr,\n   }\n \n   const bool isFloat16 = (input_ty == F16) || (input_ty == BF16);\n-  if (std::holds_alternative<se::CudaComputeCapability>(gpu_version)) {\n+  if (const auto* cuda_compute_capability =\n+          gpu_version.cuda_compute_capability()) {\n     // CUDA:\n     // If we're not Volta or not fp16/bfloat16, or not conv2D, the decision is\n     // easy: Use NCHW.\n-    const auto* cuda_compute_capability =\n-        std::get_if<se::CudaComputeCapability>(&gpu_version);\n     bool is_volta =\n         cuda_compute_capability &&\n         cuda_compute_capability->IsAtLeast(se::CudaComputeCapability::kVolta);\n     if (!isFloat16 || !is_volta ||\n         instr->shape().tuple_shapes(0).dimensions().size() != 4) {\n       return kAllNCHW;\n     }\n-  } else if (std::holds_alternative<se::RocmComputeCapability>(gpu_version)) {\n+  } else if (auto rocm_compute_capability =\n+                 gpu_version.rocm_compute_capability()) {\n     // ROCm:\n     // If we do not have NHWC layout support or not fp16/bfloat16, or not\n     // conv2D, or ROCm NHWC is disabled the decision is to use NCHW.\n     bool is_enabled = false;\n     TF_CHECK_OK(tsl::ReadBoolFromEnvVar(\"TF_USE_ROCM_NHWC\",\n                                         /*default_val=*/false, &is_enabled));\n-    auto rocm_compute_capability =\n-        std::get<se::RocmComputeCapability>(gpu_version);\n-    if (!isFloat16 || (!rocm_compute_capability.has_nhwc_layout_support()) ||\n+    if (!isFloat16 || (!rocm_compute_capability->has_nhwc_layout_support()) ||\n         instr->shape().tuple_shapes(0).dimensions().size() != 4 ||\n         !is_enabled) {\n       return kAllNCHW;\n@@ -447,8 +445,7 @@ absl::Status GpuLayoutAssignment::AddDotBackendConstraints(\n                       (rhs.type == PrimitiveType::F8E4M3FN ||\n                        rhs.type == PrimitiveType::F8E5M2FNUZ);\n \n-  const se::CudaComputeCapability* cc =\n-      std::get_if<se::CudaComputeCapability>(&gpu_version_);\n+  const se::CudaComputeCapability* cc = gpu_version_.cuda_compute_capability();\n   const bool both_operands_require_minor_contraction_dims =\n       is_s8_to_s32 || (is_fp8 && !(cc && cc->IsBlackwell()));\n "
        },
        {
            "sha": "4c0c9a74109c6479a23e4f6bba8fda1c3380ed69",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -75,7 +75,7 @@ class NestGemmFusionTest : public HloHardwareIndependentTestBase {\n  protected:\n   const se::DeviceDescription device_description_{\n       TestGpuDeviceInfo::RTXA6000DeviceInfo(\n-          se::CudaComputeCapability::Ampere())};\n+          se::GpuComputeCapability{se::CudaComputeCapability::Ampere()})};\n   mlir::MLIRContext mlir_context_;\n   SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n "
        },
        {
            "sha": "d6f37317bbd6134df260047b5e2f95bd11285f9d",
            "filename": "third_party/xla/xla/service/gpu/transforms/sort_rewriter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsort_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsort_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsort_rewriter.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -398,8 +398,8 @@ bool ShouldRewriteCompatibleSort(se::DeviceDescription device_description,\n   }\n \n   if (SortRewriter::SortMode() == SortRewriter::Mode::kAuto) {\n-    if (auto cuda_cc = std::get_if<se::CudaComputeCapability>(\n-            &device_description.gpu_compute_capability())) {\n+    if (auto* cuda_cc = device_description.gpu_compute_capability()\n+                            .cuda_compute_capability()) {\n       int bitwidth = primitive_util::BitWidth(operand_shape.element_type());\n       int batch_size = Product(operand_shape.dimensions()) / num_elements;\n "
        },
        {
            "sha": "96edae3141c3fd31ea68f755f61acefad5aaa1f1",
            "filename": "third_party/xla/xla/service/gpu/transforms/topk_specializer.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftopk_specializer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftopk_specializer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftopk_specializer.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -127,9 +127,7 @@ class SpecializeTopkVisitor : public DfsHloRewriteVisitor {\n       return absl::OkStatus();\n     }\n     TF_RET_CHECK(topk->operand_count() == 1);\n-    bool is_cuda =\n-        std::holds_alternative<stream_executor::CudaComputeCapability>(\n-            compute_capability_);\n+    bool is_cuda = compute_capability_.IsCuda();\n \n     if (auto small_topk = SmallBufferOptimization(\n             topk, is_cuda,"
        },
        {
            "sha": "b9279cde95d61750ec21a7d01046cd719bed396a",
            "filename": "third_party/xla/xla/service/gpu/triton_fusion_analysis_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftriton_fusion_analysis_test.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -698,8 +698,8 @@ ENTRY e {\n     lhs_contracting_dims={1}, rhs_contracting_dims={0}\n   ROOT bc = bf16[2,2,100] broadcast(dot), dimensions={0,1}\n })\"));\n-  EXPECT_TRUE(GemmFusion(se::CudaComputeCapability{\n-                             se::CudaComputeCapability::kAmpere, 0})\n+  EXPECT_TRUE(GemmFusion(se::GpuComputeCapability{se::CudaComputeCapability{\n+                             se::CudaComputeCapability::kAmpere, 0}})\n                   .Run(module.get())\n                   .value());\n   EXPECT_EQ(module->entry_computation()->root_instruction()->opcode(),"
        },
        {
            "sha": "f1938b0b94513b1935bcbd772232196ffb078aa0",
            "filename": "third_party/xla/xla/service/hlo_runner.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7bc8a8859986ef8b1393f12461ef29d6577b6e8/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.cc?ref=f7bc8a8859986ef8b1393f12461ef29d6577b6e8",
            "patch": "@@ -816,17 +816,15 @@ bool HloRunner::HasProperty(const HloRunnerPropertyTag::Type tag) const {\n   if (tag == HloRunnerPropertyTag::kUsingGpuRocm) {\n     const stream_executor::DeviceDescription& device_description =\n         backend().default_stream_executor()->GetDeviceDescription();\n-    return std::holds_alternative<stream_executor::RocmComputeCapability>(\n-        device_description.gpu_compute_capability());\n+    return device_description.gpu_compute_capability().IsRocm();\n   }\n   if (tag == HloRunnerPropertyTag::kCpu) {\n     return backend().platform()->Name() == \"Host\";\n   }\n   if (tag == HloRunnerPropertyTag::kUsingGpuCuda) {\n     const stream_executor::DeviceDescription& device_description =\n         backend().default_stream_executor()->GetDeviceDescription();\n-    return std::holds_alternative<stream_executor::CudaComputeCapability>(\n-        device_description.gpu_compute_capability());\n+    return device_description.gpu_compute_capability().IsCuda();\n   }\n   return false;\n }"
        }
    ],
    "stats": {
        "total": 717,
        "additions": 369,
        "deletions": 348
    }
}