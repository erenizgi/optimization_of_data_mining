{
    "author": "amd-songpiao",
    "message": "PR #35148: [ROCm] fixed //xla/tools/hlo_opt:tests/gpu_hlo_llvm.hlo.test\n\nImported from GitHub PR https://github.com/openxla/xla/pull/35148\n\nThe PR fixed the failed //xla/tools/hlo_opt:tests/gpu_hlo_llvm.hlo.test on rocm.\n\nThe @wrapped_b kernel (the transpose) was never actually tested because its name doesn't contain \"fusion\".\n\nThe test passed on NVIDIA \"by accident\" - it was checking the reduce kernel twice, not the transpose kernel. The fix to use CHECK-LABEL: wrapped_b actually makes the test check what it was originally intended to check.\n\nFor the second transpose test, both platforms use the direct element-wise copy approach.\n\nWith this PR,  the test can pass on both MI300x and H100.\n\n**./bazel-7.4.1-linux-x86_64 run --test_sharding_strategy=disabled //xla/tools/hlo_opt:tests/gpu_hlo_llvm.hlo.test**\n\n@xla-rotation could you review my PR, please?\nCopybara import of the project:\n\n--\n8d459548d05b7679fbbe21d4205e405004971dcc by Songlin Piao <Songlin.Piao@amd.com>:\n\ncorrect the kernel name as the original test was checking the wrong thing - the wrapped_b kernel doesn't use barriers on either platform.  For this second transpose test, both platforms use the direct element-wise copy approach.\n\nMerging this change closes #35148\n\nPiperOrigin-RevId: 845103912",
    "sha": "be28108d5d3c00ef1a69680784d2f38aace68ef0",
    "files": [
        {
            "sha": "2dac74fda615165e8fa8e079c1d0f8d1ca5c2c43",
            "filename": "third_party/xla/xla/tools/hlo_opt/tests/gpu_hlo_llvm.hlo",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/be28108d5d3c00ef1a69680784d2f38aace68ef0/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Ftests%2Fgpu_hlo_llvm.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/be28108d5d3c00ef1a69680784d2f38aace68ef0/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Ftests%2Fgpu_hlo_llvm.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Ftests%2Fgpu_hlo_llvm.hlo?ref=be28108d5d3c00ef1a69680784d2f38aace68ef0",
            "patch": "@@ -22,9 +22,9 @@ ENTRY e {\n HloModule Test, is_scheduled=true\n \n \n-// CHECK-LABEL: fusion\n-// CHECK-PTX:     call void @llvm.nvvm.barrier.cta.sync.aligned.all(i32 0)\n-// CHECK-GCN:     call void @llvm.amdgcn.s.barrier\n+// CHECK-LABEL: wrapped_b\n+// CHECK-PTX:     call i32 @llvm.nvvm.read.ptx.sreg.tid.x()\n+// CHECK-GCN:     call i32 @llvm.amdgcn.workitem.id.x()\n fused_computation {\n   param_0 = f32[100,200]{1,0} parameter(0)\n   ROOT b.1 = f32[100,200]{0,1} copy(f32[100,200]{1,0} param_0)"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}