{
    "author": "mkuperst",
    "message": "[XLA] Add a pass to remove dead and pass-through parameters from calls.\n\nThis pass:\na) Removes dead (unused) parameters.\nb) Rewrites calls that pass a parameter through s.t. the users of the pass-through parameter instead directly use the operand to the call. If this transformation would make the parameter dead, it is removed.\n\nPiperOrigin-RevId: 829039229",
    "sha": "eb614e99272eecbed4c5489429babf0834f76c0e",
    "files": [
        {
            "sha": "0ce9271f05d5a590f4f3edcd73c8a322fec0c8f7",
            "filename": "third_party/xla/xla/hlo/ir/hlo_computation.cc",
            "status": "modified",
            "additions": 39,
            "deletions": 7,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eb614e99272eecbed4c5489429babf0834f76c0e/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eb614e99272eecbed4c5489429babf0834f76c0e/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.cc?ref=eb614e99272eecbed4c5489429babf0834f76c0e",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include <stack>\n #include <string>\n #include <utility>\n+#include <variant>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n@@ -34,6 +35,7 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/functional/function_ref.h\"\n+#include \"absl/functional/overload.h\"\n #include \"absl/log/check.h\"\n #include \"absl/memory/memory.h\"\n #include \"absl/status/status.h\"\n@@ -2020,7 +2022,8 @@ std::unique_ptr<HloComputation> HloComputation::CloneWithReplacements(\n                               std::unique_ptr<HloInstruction>>* replacements,\n     absl::Span<const HloInstruction* const> extra_parameters,\n     HloCloneContext* context, const std::string& suffix,\n-    const HloInstruction* new_root) {\n+    std::variant<const HloInstruction*, const absl::Span<HloInstruction* const>>\n+        new_root) {\n   std::unique_ptr<HloCloneContext> context_ptr;\n   if (context == nullptr) {\n     context_ptr = std::make_unique<HloCloneContext>(parent(), suffix);\n@@ -2035,11 +2038,9 @@ std::unique_ptr<HloComputation> HloComputation::CloneInContext(\n     const absl::flat_hash_map<const HloInstruction*,\n                               std::unique_ptr<HloInstruction>>* replacements,\n     absl::Span<const HloInstruction* const> extra_parameters,\n-    const std::string& suffix, const HloInstruction* new_root) const {\n-  if (new_root == nullptr) {\n-    new_root = root_instruction();\n-  }\n-\n+    const std::string& suffix,\n+    std::variant<const HloInstruction*, const absl::Span<HloInstruction* const>>\n+        new_root) const {\n   // Look up instr in the replacements map, and return either the replacement,\n   // or instr, if the replacement isn't present.\n   //\n@@ -2132,8 +2133,39 @@ std::unique_ptr<HloComputation> HloComputation::CloneInContext(\n   for (auto& instr : instructions) {\n     builder.AddInstruction(std::move(instr));\n   }\n+\n+  // Figure out the new root instruction for the clone. There are three cases:\n+  // 1. The new root is just the old root (nullptr `new_root` instruction)\n+  // 2. The new root is a different instruction in the computation (non-null\n+  // `new_root` instruction)\n+  // 3. The new root is a tuple of instructions, where the instructions are part\n+  // of the computation, but the tuple did not previously exist (`new_root`\n+  // span).\n+  HloInstruction* new_root_instruction;\n+  std::visit(absl::Overload{\n+                 [&](const HloInstruction* arg) {\n+                   if (arg == nullptr) {\n+                     new_root_instruction =\n+                         context.GetInstruction(replace(root_instruction()));\n+                   } else {\n+                     new_root_instruction =\n+                         context.GetInstruction(replace(arg));\n+                   }\n+                 },\n+                 [&](const absl::Span<HloInstruction* const> arg) {\n+                   std::vector<HloInstruction*> root_replacements;\n+                   for (HloInstruction* instr : arg) {\n+                     root_replacements.push_back(\n+                         context.GetInstruction(replace(instr)));\n+                   }\n+                   new_root_instruction = builder.AddInstruction(\n+                       HloInstruction::CreateTuple(root_replacements));\n+                 },\n+             },\n+             new_root);\n+\n   auto result = builder.Build(\n-      /*root_instruction=*/context.GetInstruction(replace(new_root)));\n+      /*root_instruction=*/new_root_instruction);\n \n   // Clone control dependencies.\n   for (auto instr : postorder) {"
        },
        {
            "sha": "d0b80f7859a87bf3428863b4d32b1894ece3d996",
            "filename": "third_party/xla/xla/hlo/ir/hlo_computation.h",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eb614e99272eecbed4c5489429babf0834f76c0e/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eb614e99272eecbed4c5489429babf0834f76c0e/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.h?ref=eb614e99272eecbed4c5489429babf0834f76c0e",
            "patch": "@@ -23,6 +23,7 @@ limitations under the License.\n #include <string>\n #include <tuple>\n #include <utility>\n+#include <variant>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n@@ -739,14 +740,21 @@ class HloComputation {\n   // 'extra_parameters' allows to specify additional parameters that should be\n   // added to the computation.\n   //\n+  // 'new_root' allows specifying a new root instruction for the clone. If it's\n+  // a pointer to an instruction in the computation being cloned, the new root\n+  // is that instruction. If it's a span, the new root is a tuple instruction,\n+  // where the instructions in the span are the tuple elements.\n+  //\n   // All relevant instructions are cloned, *including* unique_ptr in the\n   // `replacements` map.\n   std::unique_ptr<HloComputation> CloneWithReplacements(\n       const absl::flat_hash_map<const HloInstruction*,\n                                 std::unique_ptr<HloInstruction>>* replacements,\n       absl::Span<const HloInstruction* const> extra_parameters = {},\n       HloCloneContext* context = nullptr, const std::string& suffix = \"clone\",\n-      const HloInstruction* new_root = nullptr);\n+      std::variant<const HloInstruction*,\n+                   const absl::Span<HloInstruction* const>>\n+          new_root = nullptr);\n \n   // Like CloneWithReplacements(), but this is a const method and `context` must\n   // be specified.\n@@ -757,7 +765,9 @@ class HloComputation {\n           nullptr,\n       absl::Span<const HloInstruction* const> extra_parameters = {},\n       const std::string& suffix = \"clone\",\n-      const HloInstruction* new_root = nullptr) const;\n+      std::variant<const HloInstruction*,\n+                   const absl::Span<HloInstruction* const>>\n+          new_root = nullptr) const;\n \n   // Convenience overloads for CloneWithReplacements.  You want to do\n   //"
        },
        {
            "sha": "10d50dc4c990c8a237cd48fe67011154ce92190a",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/BUILD",
            "status": "modified",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eb614e99272eecbed4c5489429babf0834f76c0e/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eb614e99272eecbed4c5489429babf0834f76c0e/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2FBUILD?ref=eb614e99272eecbed4c5489429babf0834f76c0e",
            "patch": "@@ -1849,3 +1849,47 @@ xla_cc_test(\n         \"@com_google_googletest//:gtest_main\",\n     ],\n )\n+\n+cc_library(\n+    name = \"call_parameter_cleanup\",\n+    srcs = [\"call_parameter_cleanup.cc\"],\n+    hdrs = [\"call_parameter_cleanup.h\"],\n+    deps = [\n+        \"//xla:util\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/pass:hlo_pass\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"call_parameter_cleanup_test\",\n+    srcs = [\"call_parameter_cleanup_test.cc\"],\n+    deps = [\n+        \":call_parameter_cleanup\",\n+        \":hlo_dce\",\n+        \":tuple_simplifier\",\n+        \"//xla:util\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/hlo/testlib:pattern_matcher_gmock\",\n+        \"//xla/hlo/testlib:test\",\n+        \"//xla/service:call_inliner\",\n+        \"//xla/service:pattern_matcher\",\n+        \"//xla/tsl/platform:status\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_googletest//:gtest\",\n+        \"@com_google_googletest//:gtest_main\",  # fixdeps: keep\n+    ],\n+)"
        },
        {
            "sha": "8b6a935799ec609a0b12b2dc5d99a9fd9b4f2553",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/call_parameter_cleanup.cc",
            "status": "added",
            "additions": 261,
            "deletions": 0,
            "changes": 261,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eb614e99272eecbed4c5489429babf0834f76c0e/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fcall_parameter_cleanup.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eb614e99272eecbed4c5489429babf0834f76c0e/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fcall_parameter_cleanup.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fcall_parameter_cleanup.cc?ref=eb614e99272eecbed4c5489429babf0834f76c0e",
            "patch": "@@ -0,0 +1,261 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/hlo/transforms/simplifiers/call_parameter_cleanup.h\"\n+\n+#include <memory>\n+#include <variant>\n+#include <vector>\n+\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/hlo/ir/hlo_clone_context.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n+\n+namespace xla {\n+\n+namespace {\n+\n+// Construct a mapping from parameter numbers in the old computation to\n+// parameter numbers in the new computation. This is basically a compaction of\n+// the parameters after skipping the ones we'll remove.\n+// Also figures out if we need to adjust the parameters (for dead and pure\n+// pass-through parameters) and the root (for any kind of pass-through).\n+absl::flat_hash_map<int, int> BuildParameterMap(HloComputation* computation,\n+                                                bool& adjust_params,\n+                                                bool& adjust_root) {\n+  adjust_params = false;\n+  adjust_root = false;\n+\n+  absl::flat_hash_map<int, int> old_to_new_parameter_number;\n+  int curr_old = 0, curr_new = 0;\n+  for (HloInstruction* parameter : computation->parameter_instructions()) {\n+    bool dead = false;\n+    if (parameter->users().empty()) {\n+      // Case 1: Dead parameter, we want to remove it.\n+      dead = true;\n+    } else {\n+      bool found_root_use = false;\n+      for (HloInstruction* user : parameter->users()) {\n+        if (user == computation->root_instruction()) {\n+          found_root_use = true;\n+          break;\n+        }\n+      }\n+      if (found_root_use) {\n+        // Case 2: Pass-through parameter, we want to remove it from the root\n+        // tuple and forward the users to the call operand.\n+        adjust_root = true;\n+        if (parameter->users().size() == 1) {\n+          // Case 2b: Pure pass-through parameter, we want to remove it from\n+          // both the root tuple *and* the parameter list.\n+          dead = true;\n+        }\n+      }\n+    }\n+\n+    if (dead) {\n+      adjust_params = true;\n+    } else {\n+      old_to_new_parameter_number[curr_old] = curr_new;\n+      ++curr_new;\n+    }\n+    ++curr_old;\n+  }\n+\n+  return old_to_new_parameter_number;\n+}\n+\n+// Similarly, construct a mapping from output numbers (i.e. tuple indices) in\n+// the original computation to output numbers in the new computation, by\n+// skipping the ones we'll remove.\n+// Also collects the outputs we want to keep into `new_outputs`.\n+absl::flat_hash_map<int, int> BuildOutputMap(\n+    HloComputation* computation, std::vector<HloInstruction*>& new_outputs) {\n+  absl::flat_hash_map<int, int> old_to_new_output_number;\n+  int curr_old = 0, curr_new = 0;\n+  for (HloInstruction* output : computation->root_instruction()->operands()) {\n+    if (output->opcode() == HloOpcode::kParameter) {\n+      ++curr_old;\n+      continue;\n+    }\n+    old_to_new_output_number[curr_old] = curr_new;\n+    ++curr_old;\n+    ++curr_new;\n+    new_outputs.push_back(output);\n+  }\n+  return old_to_new_output_number;\n+}\n+\n+absl::Status ReplaceCallSite(\n+    HloInstruction* old_call, HloComputation* new_computation,\n+    const absl::flat_hash_map<int, int>& old_to_new_parameter_number,\n+    const absl::flat_hash_map<int, int>& old_to_new_output_number,\n+    bool adjust_root) {\n+  // Create a new call instruction with the new computation and new parameters.\n+  std::vector<HloInstruction*> new_call_operands;\n+  new_call_operands.reserve(old_call->operands().size());\n+\n+  for (int i = 0; i < old_call->operands().size(); ++i) {\n+    if (old_to_new_parameter_number.find(i) !=\n+        old_to_new_parameter_number.end()) {\n+      new_call_operands.push_back(old_call->mutable_operand(i));\n+    }\n+  }\n+\n+  HloComputation* enclosing_computation = old_call->parent();\n+  HloInstruction* new_call =\n+      enclosing_computation->AddInstruction(old_call->CloneWithNewOperands(\n+          new_computation->root_instruction()->shape(), new_call_operands));\n+  new_call->set_to_apply(new_computation);\n+\n+  // If we didn't remove any pass-through parameters, we're done with this\n+  // callsite. Note that we can't unconditionally replace here, because the\n+  // output will create a mismatch.\n+  if (!adjust_root) {\n+    return old_call->ReplaceAllUsesWith(new_call);\n+  }\n+\n+  // The old call produced a tuple. To ensure the shapes match up, create a new\n+  // tuple instruction with the right shape, and populate it based on the call's\n+  // operands (for pass-through parameters) and the new call's outputs (for\n+  // everything else). This creates some cruft, but the tuple simplifier will\n+  // clean it up later.\n+  HloInstruction* old_root = old_call->to_apply()->root_instruction();\n+  std::vector<HloInstruction*> tuple_inputs;\n+  for (int i = 0; i < old_root->operands().size(); ++i) {\n+    auto iter = old_to_new_output_number.find(i);\n+    if (iter != old_to_new_output_number.end()) {\n+      HloInstruction* gte = enclosing_computation->AddInstruction(\n+          HloInstruction::CreateGetTupleElement(new_call, iter->second));\n+      tuple_inputs.push_back(gte);\n+    } else {\n+      tuple_inputs.push_back(\n+          old_call->mutable_operand(old_root->operand(i)->parameter_number()));\n+    }\n+  }\n+\n+  HloInstruction* new_tuple = enclosing_computation->AddInstruction(\n+      HloInstruction::CreateTuple(tuple_inputs));\n+  return old_call->ReplaceAllUsesWith(new_tuple);\n+}\n+\n+absl::StatusOr<bool> RemoveDeadParameters(HloComputation* computation) {\n+  bool adjust_params, adjust_root;\n+  absl::flat_hash_map<int, int> old_to_new_parameter_number =\n+      BuildParameterMap(computation, adjust_params, adjust_root);\n+\n+  // If we don't need to adjust anything, we're done.\n+  if (!adjust_params && !adjust_root) {\n+    return false;\n+  }\n+\n+  absl::flat_hash_map<const HloInstruction*, std::unique_ptr<HloInstruction>>\n+      replacements;\n+  // If we're removing parameters, we need to (a) replace the ones being removed\n+  // with null, and (b) adjust the parameter numbers on the remaining ones so\n+  // that we don't have \"holes\".\n+  if (adjust_params) {\n+    for (HloInstruction* parameter : computation->parameter_instructions()) {\n+      auto iter =\n+          old_to_new_parameter_number.find(parameter->parameter_number());\n+      if (iter == old_to_new_parameter_number.end()) {\n+        replacements.insert({parameter, nullptr});\n+      } else {\n+        replacements.insert({parameter, HloInstruction::CreateParameter(\n+                                            iter->second, parameter->shape(),\n+                                            parameter->name())});\n+      }\n+    }\n+  }\n+\n+  HloComputation* new_computation;\n+  absl::flat_hash_map<int, int> old_to_new_output_number;\n+  if (adjust_root) {\n+    replacements.insert({computation->root_instruction(), nullptr});\n+    std::vector<HloInstruction*> new_outputs;\n+    old_to_new_output_number = BuildOutputMap(computation, new_outputs);\n+    new_computation = computation->parent()->AddEmbeddedComputation(\n+        computation->CloneWithReplacements(\n+            &replacements, /*extra_parameters=*/{},\n+            /*context=*/nullptr, /*suffix=*/\"undead\",\n+            /*new_root=*/new_outputs));\n+  } else {\n+    // Don't fill old_to_new_output_number here, we won't need it.\n+    new_computation = computation->parent()->AddEmbeddedComputation(\n+        computation->CloneWithReplacements(\n+            &replacements, /*extra_parameters=*/{},\n+            /*context=*/nullptr, /*suffix=*/\"undead\"));\n+  }\n+\n+  // The new call computation is ready, now make all the call sites use it.\n+  for (HloInstruction* old_call : computation->caller_instructions()) {\n+    TF_RETURN_IF_ERROR(ReplaceCallSite(old_call, new_computation,\n+                                       old_to_new_parameter_number,\n+                                       old_to_new_output_number, adjust_root));\n+  }\n+\n+  return true;\n+}\n+\n+bool ShouldProcessComputation(HloComputation* computation) {\n+  // Only process computations with tuple roots. In theory we could also remove\n+  // completely dead parameters from a computation with a non-tuple root, but\n+  // since pass-through is only a thing for tuples, and it complicates the code,\n+  // we don't bother for now.\n+  if (computation->root_instruction()->opcode() != HloOpcode::kTuple) {\n+    return false;\n+  }\n+  for (HloInstruction* instruction : computation->caller_instructions()) {\n+    if (instruction->opcode() != HloOpcode::kCall) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+}  // namespace\n+\n+absl::StatusOr<bool> CallParameterCleanup::RunImpl(\n+    HloModule* module,\n+    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n+  std::vector<HloComputation*> computations_to_process;\n+  for (HloComputation* computation :\n+       module->MakeNonfusionComputations(execution_threads)) {\n+    if (ShouldProcessComputation(computation)) {\n+      computations_to_process.push_back(computation);\n+    }\n+  }\n+\n+  bool changed = false;\n+  for (HloComputation* computation : computations_to_process) {\n+    TF_ASSIGN_OR_RETURN(bool removed, RemoveDeadParameters(computation));\n+    changed |= removed;\n+  }\n+  return changed;\n+}\n+\n+}  // namespace xla"
        },
        {
            "sha": "a40572a01a66b3bbb80ddc3fe68da3791ee90b48",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/call_parameter_cleanup.h",
            "status": "added",
            "additions": 50,
            "deletions": 0,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eb614e99272eecbed4c5489429babf0834f76c0e/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fcall_parameter_cleanup.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eb614e99272eecbed4c5489429babf0834f76c0e/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fcall_parameter_cleanup.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fcall_parameter_cleanup.h?ref=eb614e99272eecbed4c5489429babf0834f76c0e",
            "patch": "@@ -0,0 +1,50 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_HLO_TRANSFORMS_SIMPLIFIERS_CALL_PARAMETER_CLEANUP_H_\n+#define XLA_HLO_TRANSFORMS_SIMPLIFIERS_CALL_PARAMETER_CLEANUP_H_\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/pass/hlo_pass_interface.h\"\n+#include \"xla/util.h\"\n+\n+namespace xla {\n+\n+// This pass:\n+// a) Removes dead (unused) parameters.\n+// b) Rewrites calls that pass a parameter through s.t. the users of the\n+//    pass-through parameter instead directly use the operand to the call. If\n+//    this transformation would make the parameter dead, it is removed.\n+class CallParameterCleanup : public HloModulePass {\n+ public:\n+  CallParameterCleanup() = default;\n+  ~CallParameterCleanup() override = default;\n+\n+  static constexpr absl::string_view kName = \"call-parameter-cleanup\";\n+  absl::string_view name() const override { return kName; }\n+\n+ protected:\n+  // Runs the pass on the given module. Returns whether the module was changed\n+  absl::StatusOr<bool> RunImpl(\n+      HloModule* module,\n+      const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n+};\n+}  // namespace xla\n+\n+#endif  // XLA_HLO_TRANSFORMS_SIMPLIFIERS_CALL_PARAMETER_CLEANUP_H_"
        },
        {
            "sha": "8d70e53db36ce5e78483ff7397e3a0cab5a76f1d",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/call_parameter_cleanup_test.cc",
            "status": "added",
            "additions": 310,
            "deletions": 0,
            "changes": 310,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eb614e99272eecbed4c5489429babf0834f76c0e/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fcall_parameter_cleanup_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eb614e99272eecbed4c5489429babf0834f76c0e/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fcall_parameter_cleanup_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fcall_parameter_cleanup_test.cc?ref=eb614e99272eecbed4c5489429babf0834f76c0e",
            "patch": "@@ -0,0 +1,310 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/hlo/transforms/simplifiers/call_parameter_cleanup.h\"\n+\n+#include <memory>\n+#include <string>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/log/log.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/hlo/testlib/pattern_matcher_gmock.h\"\n+#include \"xla/hlo/testlib/test.h\"\n+#include \"xla/hlo/transforms/simplifiers/hlo_dce.h\"\n+#include \"xla/hlo/transforms/simplifiers/tuple_simplifier.h\"\n+#include \"xla/service/pattern_matcher.h\"\n+#include \"xla/tsl/platform/status.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla {\n+\n+class CallParameterCleanupTest : public HloHardwareIndependentTestBase {\n+ protected:\n+  CallParameterCleanupTest()\n+      : HloHardwareIndependentTestBase(\n+            /*verifier_layout_sensitive=*/false,\n+            /*allow_mixed_precision_in_hlo_verifier=*/true) {}\n+};\n+\n+namespace {\n+\n+namespace m = ::xla::match;\n+\n+TEST_F(CallParameterCleanupTest, DeadParameter) {\n+  const std::string module_str = R\"hlo(\n+HloModule module\n+\n+add {\n+  a = s32[] parameter(0)\n+  b = s32[] parameter(1)\n+  c = s32[] parameter(2)\n+  add = s32[] add(a, c)\n+  ROOT tuple = (s32[]) tuple(add)\n+}\n+\n+ENTRY entry {\n+  p0 = s32[] parameter(0)\n+  p1 = s32[] parameter(1)\n+  p2 = s32[] parameter(2)\n+  ROOT call = (s32[]) call(p0, p1, p2), to_apply=add\n+}\n+\n+)hlo\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(module_str));\n+  CallParameterCleanup cleanup;\n+  EXPECT_TRUE(cleanup.Run(module.get()).value());\n+\n+  HloDCE dce;\n+  TupleSimplifier tuple_simplifier;\n+  TF_CHECK_OK(dce.Run(module.get()).status());\n+  TF_CHECK_OK(tuple_simplifier.Run(module.get()).status());\n+\n+  // We expect the parameter at index 1 to be removed.\n+  HloInstruction* call;\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              GmockMatch(m::Call(&call, m::Parameter(0), m::Parameter(2))));\n+  EXPECT_THAT(call->to_apply()->root_instruction(),\n+              GmockMatch(m::Tuple(m::Add(m::Parameter(0), m::Parameter(1)))));\n+}\n+\n+TEST_F(CallParameterCleanupTest, PassThroughParameter) {\n+  const std::string module_str = R\"hlo(\n+HloModule module\n+\n+add {\n+  a = s32[] parameter(0)\n+  b = s32[] parameter(1)\n+  c = s32[] parameter(2)\n+  add = s32[] add(a, c)\n+  ROOT tuple = (s32[], s32[]) tuple(b, add)\n+}\n+\n+ENTRY entry {\n+  p0 = s32[] parameter(0)\n+  p1 = s32[] parameter(1)\n+  p2 = s32[] parameter(2)\n+  ROOT call = (s32[], s32[]) call(p0, p1, p2), to_apply=add\n+}\n+\n+)hlo\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(module_str));\n+  CallParameterCleanup cleanup;\n+  EXPECT_TRUE(cleanup.Run(module.get()).value());\n+\n+  HloDCE dce;\n+  TupleSimplifier tuple_simplifier;\n+  TF_CHECK_OK(dce.Run(module.get()).status());\n+  TF_CHECK_OK(tuple_simplifier.Run(module.get()).status());\n+\n+  // We expect the parameter at index 1 to be passed through directly from the\n+  // entry computation parameter, and removed from the call parameters.\n+  HloInstruction* call;\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              GmockMatch(m::Tuple(\n+                  m::Parameter(1),\n+                  m::GetTupleElement(\n+                      m::Call(&call, m::Parameter(0), m::Parameter(2)), 0))));\n+  EXPECT_THAT(call->to_apply()->root_instruction(),\n+              GmockMatch(m::Tuple(m::Add(m::Parameter(0), m::Parameter(1)))));\n+}\n+\n+TEST_F(CallParameterCleanupTest, UsedPassThroughParameter) {\n+  const std::string module_str = R\"hlo(\n+HloModule module\n+\n+add {\n+  a = s32[] parameter(0)\n+  b = s32[] parameter(1)\n+  c = s32[] parameter(2)\n+  add = s32[] add(a, c)\n+  ROOT tuple = (s32[], s32[]) tuple(c, add)\n+}\n+\n+ENTRY entry {\n+  p0 = s32[] parameter(0)\n+  p1 = s32[] parameter(1)\n+  p2 = s32[] parameter(2)\n+  ROOT call = (s32[], s32[]) call(p0, p1, p2), to_apply=add\n+}\n+\n+)hlo\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(module_str));\n+  CallParameterCleanup cleanup;\n+  EXPECT_TRUE(cleanup.Run(module.get()).value());\n+\n+  HloDCE dce;\n+  TupleSimplifier tuple_simplifier;\n+  TF_CHECK_OK(dce.Run(module.get()).status());\n+  TF_CHECK_OK(tuple_simplifier.Run(module.get()).status());\n+\n+  // We expect the parameter at index 2 to be passed through directly from the\n+  // entry computation parameter, but not removed from the call parameters.\n+  // Parameter 1 gets removed.\n+  HloInstruction* call;\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              GmockMatch(m::Tuple(\n+                  m::Parameter(2),\n+                  m::GetTupleElement(\n+                      m::Call(&call, m::Parameter(0), m::Parameter(2)), 0))));\n+  EXPECT_THAT(call->to_apply()->root_instruction(),\n+              GmockMatch(m::Tuple(m::Add(m::Parameter(0), m::Parameter(1)))));\n+}\n+\n+TEST_F(CallParameterCleanupTest, DeadPassThroughParameterMultipleUses) {\n+  const std::string module_str = R\"hlo(\n+HloModule module\n+\n+add {\n+  a = s32[] parameter(0)\n+  b = s32[] parameter(1)\n+  c = s32[] parameter(2)\n+  add = s32[] add(a, c)\n+  ROOT tuple = (s32[], s32[], s32[]) tuple(b, add, b)\n+}\n+\n+ENTRY entry {\n+  p0 = s32[] parameter(0)\n+  p1 = s32[] parameter(1)\n+  p2 = s32[] parameter(2)\n+  ROOT call = (s32[], s32[], s32[]) call(p0, p1, p2), to_apply=add\n+}\n+\n+)hlo\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(module_str));\n+  CallParameterCleanup cleanup;\n+  EXPECT_TRUE(cleanup.Run(module.get()).value());\n+\n+  HloDCE dce;\n+  TupleSimplifier tuple_simplifier;\n+  TF_CHECK_OK(dce.Run(module.get()).status());\n+  TF_CHECK_OK(tuple_simplifier.Run(module.get()).status());\n+\n+  HloInstruction* call;\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              GmockMatch(m::Tuple(\n+                  m::Parameter(1),\n+                  m::GetTupleElement(\n+                      m::Call(&call, m::Parameter(0), m::Parameter(2)), 0),\n+                  m::Parameter(1))));\n+  EXPECT_THAT(call->to_apply()->root_instruction(),\n+              GmockMatch(m::Tuple(m::Add(m::Parameter(0), m::Parameter(1)))));\n+}\n+\n+TEST_F(CallParameterCleanupTest, UsedPassThroughParameterNoDeadParams) {\n+  const std::string module_str = R\"hlo(\n+HloModule module\n+\n+add {\n+  a = s32[] parameter(0)\n+  b = s32[] parameter(1)\n+  add = s32[] add(a, b)\n+  ROOT tuple = (s32[], s32[]) tuple(b, add)\n+}\n+\n+ENTRY entry {\n+  p0 = s32[] parameter(0)\n+  p1 = s32[] parameter(1)\n+  ROOT call = (s32[], s32[]) call(p0, p1), to_apply=add\n+}\n+\n+)hlo\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(module_str));\n+  CallParameterCleanup cleanup;\n+  EXPECT_TRUE(cleanup.Run(module.get()).value());\n+\n+  HloDCE dce;\n+  TupleSimplifier tuple_simplifier;\n+  TF_CHECK_OK(dce.Run(module.get()).status());\n+  TF_CHECK_OK(tuple_simplifier.Run(module.get()).status());\n+\n+  // We expect the parameter at index 1 to be passed through directly from the\n+  // entry computation parameter, but not removed from the call parameters.\n+  HloInstruction* call;\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              GmockMatch(m::Tuple(\n+                  m::Parameter(1),\n+                  m::GetTupleElement(\n+                      m::Call(&call, m::Parameter(0), m::Parameter(1)), 0))));\n+  EXPECT_THAT(call->to_apply()->root_instruction(),\n+              GmockMatch(m::Tuple(m::Add(m::Parameter(0), m::Parameter(1)))));\n+}\n+\n+TEST_F(CallParameterCleanupTest, MultipleCallSites) {\n+  const std::string module_str = R\"hlo(\n+HloModule module\n+\n+add {\n+  a = s32[] parameter(0)\n+  b = s32[] parameter(1)\n+  c = s32[] parameter(2)\n+  add = s32[] add(a, c)\n+  ROOT tuple = (s32[], s32[]) tuple(c, add)\n+}\n+\n+wrap {\n+  p0 = s32[] parameter(0)\n+  p1 = s32[] parameter(1)\n+  p2 = s32[] parameter(2)\n+  ROOT call = (s32[], s32[]) call(p1, p2, p0), to_apply=add\n+}\n+\n+ENTRY entry {\n+  p0 = s32[] parameter(0)\n+  p1 = s32[] parameter(1)\n+  p2 = s32[] parameter(2)\n+  call0 = (s32[], s32[]) call(p0, p1, p2), to_apply=add\n+  call1 = (s32[], s32[]) call(p0, p1, p2), to_apply=wrap\n+  gte0 = s32[] get-tuple-element(call0), index=1\n+  gte1 = s32[] get-tuple-element(call1), index=0\n+  ROOT mul = s32[] multiply(gte0, gte1)\n+}\n+\n+)hlo\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(module_str));\n+  CallParameterCleanup cleanup;\n+  EXPECT_TRUE(cleanup.Run(module.get()).value());\n+\n+  HloDCE dce;\n+  TupleSimplifier tuple_simplifier;\n+  TF_CHECK_OK(dce.Run(module.get()).status());\n+  TF_CHECK_OK(tuple_simplifier.Run(module.get()).status());\n+\n+  // We expect both call sites to use the same computation, for 3 computations\n+  // total, rather than 4.\n+  EXPECT_EQ(module->computation_count(), 3);\n+}\n+\n+}  // namespace\n+\n+}  // namespace xla"
        }
    ],
    "stats": {
        "total": 725,
        "additions": 716,
        "deletions": 9
    }
}