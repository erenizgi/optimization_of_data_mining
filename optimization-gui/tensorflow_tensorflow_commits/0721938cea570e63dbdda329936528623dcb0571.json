{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Only set channel id it is present in the original instruction.\n\nPiperOrigin-RevId: 818048132",
    "sha": "0721938cea570e63dbdda329936528623dcb0571",
    "files": [
        {
            "sha": "2695344ef745cf22c865319f1f25467cffc241b8",
            "filename": "third_party/xla/xla/service/gpu/transforms/ragged_all_to_all_multi_host_decomposer.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 3,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0721938cea570e63dbdda329936528623dcb0571/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0721938cea570e63dbdda329936528623dcb0571/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc?ref=0721938cea570e63dbdda329936528623dcb0571",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #include \"xla/service/gpu/transforms/ragged_all_to_all_multi_host_decomposer.h\"\n \n #include <cstdint>\n+#include <optional>\n #include <vector>\n \n #include \"absl/container/flat_hash_set.h\"\n@@ -73,7 +74,9 @@ HloInstruction* GetIntraHostMetadata(\n           /*operands=*/{new_input_offsets},\n           /*device_list=*/CollectiveDeviceList(replica_groups),\n           /*constrain_layout=*/false,\n-          /*channel_id=*/NextChannelId(*computation->parent()),\n+          /*channel_id=*/ragged_all_to_all->channel_id().has_value()\n+              ? std::make_optional(NextChannelId(*computation->parent()))\n+              : std::nullopt,\n           /*split_dimension=*/0));\n \n   if (correct_offsets) {\n@@ -194,15 +197,25 @@ absl::StatusOr<bool> DecomposeRaggedAllToAll(\n   new_input_shape.set_dimensions(\n       0, num_hosts * input_operand->shape().dimensions(0));\n \n+  // The collective can run in two modes: cross-replica and cross-partition. If\n+  // the original `ragged-all-to-all` has a channel id set, then it's a\n+  // cross-partition collective. In that case `all-gather` needs a channel_id\n+  // and `use_global_device_ids=true`.\n+  // Otherwise, when `ragged-all-to-all` has no channel id, it's a cross-replica\n+  // collective. In that case `all-gather` doesn't need a `channel_id` and\n+  // `use_global_device_ids` should be set to false.\n   HloInstruction* all_gather_input =\n       computation->AddInstruction(HloInstruction::CreateAllGather(\n           /*shape=*/new_input_shape,\n           /*operands=*/{ragged_all_to_all->mutable_operand(0)},\n           /*all_gather_dimension=*/0,\n           /*device_list=*/CollectiveDeviceList(inter_host_replica_groups),\n           /*constrain_layout=*/false,\n-          /*channel_id=*/NextChannelId(*computation->parent()),\n-          /*use_global_device_ids=*/true));\n+          /*channel_id=*/ragged_all_to_all->channel_id().has_value()\n+              ? std::make_optional(NextChannelId(*computation->parent()))\n+              : std::nullopt,\n+          /*use_global_device_ids=*/\n+          ragged_all_to_all->channel_id().has_value()));\n \n   for (int i = 2; i < 6; ++i) {\n     intra_host_metadata.push_back(GetIntraHostMetadata("
        }
    ],
    "stats": {
        "total": 19,
        "additions": 16,
        "deletions": 3
    }
}