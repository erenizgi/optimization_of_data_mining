{
    "author": "pifon2a",
    "message": "[XLA:GPU] Remove SymbolicExprContext.\n\nPiperOrigin-RevId: 835561005",
    "sha": "b72af58a6b6492d3e45b8a762188ff067a9a1498",
    "files": [
        {
            "sha": "53f2f0dc055c5f35178fb2920e8a46d60a69b8b0",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/cpu_fusion_emitter.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -135,7 +135,7 @@ IndexingMap GetDefaultIndexingMap(\n     absl::Span<const int64_t> thread_tile_sizes,\n     absl::Span<const int64_t> shape,\n     // TODO: b/451959933 - Use reference or absl_nullable pointer.\n-    SymbolicExprContext* symbolic_expr_context) {\n+    mlir::MLIRContext* mlir_context) {\n   CHECK_EQ(thread_tile_sizes.size(), shape.size())\n       << \"thread_tile_sizes and shape must have the same size\";\n   SmallVector<int64_t> thread_tile_counts;\n@@ -144,14 +144,12 @@ IndexingMap GetDefaultIndexingMap(\n     thread_tile_counts.push_back(CeilDiv(dim_size, tile_size));\n   }\n   // Delinearize thread_expr w.r.t. number of thread tiles per dimension.\n-  auto thread_expr =\n-      mlir::getAffineDimExpr(0, symbolic_expr_context->GetMLIRContext());\n+  auto thread_expr = mlir::getAffineDimExpr(0, mlir_context);\n   SmallVector<AffineExpr, 4> thread_ids =\n       DelinearizeInBoundsIndex(thread_expr, thread_tile_counts);\n   SmallVector<AffineExpr, 4> result;\n   result.reserve(thread_ids.size());\n-  auto linear_index =\n-      mlir::getAffineSymbolExpr(0, symbolic_expr_context->GetMLIRContext());\n+  auto linear_index = mlir::getAffineSymbolExpr(0, mlir_context);\n   SmallVector<AffineExpr, 4> indices_in_tile =\n       DelinearizeInBoundsIndex(linear_index, thread_tile_sizes);\n   SmallVector<std::pair<AffineExpr, Interval>, 4> constraints;\n@@ -164,9 +162,8 @@ IndexingMap GetDefaultIndexingMap(\n   int64_t num_threads = Product(thread_tile_counts);\n   int64_t num_tile_elements = Product(thread_tile_sizes);\n \n-  auto affine_map =\n-      mlir::AffineMap::get(/*num_dims=*/1, /*num_symbols=*/1, result,\n-                           symbolic_expr_context->GetMLIRContext());\n+  auto affine_map = mlir::AffineMap::get(/*num_dims=*/1, /*num_symbols=*/1,\n+                                         result, mlir_context);\n   return IndexingMap(\n       affine_map, {IndexingMap::Variable({0, num_threads - 1, \"thread_id\"})},\n       {IndexingMap::Variable({0, num_tile_elements - 1, \"linear_index\"})}, {},\n@@ -278,7 +275,7 @@ absl::StatusOr<emitters::CallTargetProvider> EmitCallTargets(\n       if (subgraph_to_mlir_fn.contains(&subgraph)) {\n         TF_RETURN_IF_ERROR(emitters::SubgraphToMlirFunction(\n             comp, subgraph, subgraph_to_mlir_fn[&subgraph], call_targets,\n-            computations.symbolic_expr_context()));\n+            computations.mlir_context()));\n       }\n     }\n   }\n@@ -288,7 +285,7 @@ absl::StatusOr<emitters::CallTargetProvider> EmitCallTargets(\n         computations.FindPartitionedComputation(\n             fusion.fused_instructions_computation()),\n         epilogue, subgraph_to_mlir_fn[&epilogue], call_targets,\n-        computations.symbolic_expr_context()));\n+        computations.mlir_context()));\n   }\n \n   return call_targets;"
        },
        {
            "sha": "9dc42fa9c96f15daac8204eed38384fd086dd49b",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/cpu_fusion_emitter.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -42,7 +42,7 @@ namespace cpu {\n \n IndexingMap GetDefaultIndexingMap(absl::Span<const int64_t> thread_tile_sizes,\n                                   absl::Span<const int64_t> shape,\n-                                  SymbolicExprContext* symbolic_expr_context);\n+                                  mlir::MLIRContext* mlir_context);\n \n absl::StatusOr<mlir::func::FuncOp> EmitEntryFunctionApi(\n     mlir::ModuleOp fusion_module, const HloFusionInstruction& fusion,"
        },
        {
            "sha": "53b49f4db086dadf7296058170bf275e6aafbb25",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/cpu_fusion_emitter_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_fusion_emitter_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -116,10 +116,7 @@ TEST_F(CpuFusionEmitterTest, ScatterMlir) {\n   auto fusion = Cast<HloFusionInstruction>(\n       hlo_module->entry_computation()->root_instruction());\n   auto mlir_context = FusionCompiler::CreateContext();\n-  auto symbolic_expr_context =\n-      std::make_unique<SymbolicExprContext>(mlir_context.get());\n-  CpuScatterFusion emitter(*buffer_assignment, fusion,\n-                           symbolic_expr_context.get());\n+  CpuScatterFusion emitter(*buffer_assignment, fusion, mlir_context.get());\n   TF_ASSERT_OK_AND_ASSIGN(KernelDefinition kernel_definition,\n                           emitter.EmitKernelDefinition());\n   const auto& mlir_source = kernel_definition.source();\n@@ -147,10 +144,7 @@ TEST_F(CpuFusionEmitterTest, ScatterLlvm) {\n   auto fusion = Cast<HloFusionInstruction>(\n       hlo_module->entry_computation()->root_instruction());\n   auto mlir_context = FusionCompiler::CreateContext();\n-  auto symbolic_expr_context =\n-      std::make_unique<SymbolicExprContext>(mlir_context.get());\n-  CpuScatterFusion emitter(*buffer_assignment, fusion,\n-                           symbolic_expr_context.get());\n+  CpuScatterFusion emitter(*buffer_assignment, fusion, mlir_context.get());\n   TF_ASSERT_OK_AND_ASSIGN(KernelDefinition kernel_definition,\n                           emitter.EmitKernelDefinition());\n   FusionCompiler compiler(mlir_context.get(),"
        },
        {
            "sha": "537569a53404e5a770254ca75afc18bbca6785fd",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/cpu_scatter_emitter.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 12,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_scatter_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_scatter_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_scatter_emitter.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -84,30 +84,29 @@ namespace cpu {\n \n using llvm::SmallVector;\n using mlir::ImplicitLocOpBuilder;\n+using mlir::MLIRContext;\n using mlir::Value;\n using mlir::ValueRange;\n \n namespace ma = ::mlir::arith;\n namespace scf = ::mlir::scf;\n \n std::vector<emitters::EpilogueSpecification> CpuScatterFusion::GetEpilogues(\n-    const HloFusionInstruction& fusion,\n-    SymbolicExprContext* symbolic_expr_context) const {\n+    const HloFusionInstruction& fusion, MLIRContext* mlir_context) const {\n   const auto* scatter = fusion_->fused_expression_root();\n   // We don't actually support epilogues for scatter, but this is how we tell\n   // the base class that we don't want it to generate code for the scatter.\n   return {emitters::EpilogueSpecification::FromIdentityIndexing(\n-      scatter, scatter, symbolic_expr_context)};\n+      scatter, scatter, mlir_context)};\n }\n \n std::optional<IndexingMap> CpuScatterFusion::ComputeThreadIdToOutputIndexing(\n-    int64_t root_index, SymbolicExprContext* ctx) const {\n+    int64_t root_index, MLIRContext* ctx) const {\n   return std::nullopt;\n }\n \n std::optional<IndexingMap> CpuScatterFusion::ComputeThreadIdToInputIndexing(\n-    int64_t root_index, int64_t hero_operand_index,\n-    SymbolicExprContext* ctx) const {\n+    int64_t root_index, int64_t hero_operand_index, MLIRContext* ctx) const {\n   const auto* scatter =\n       DynCast<HloScatterInstruction>(fusion_->fused_expression_root());\n   CHECK(ScatterSimplifier::IsSimplifiedScatter(scatter))\n@@ -184,10 +183,10 @@ SmallVector<Value> EmitScatterComputation(\n \n CpuScatterFusion::CpuScatterFusion(const BufferAssignment& buffer_assignment,\n                                    const HloFusionInstruction* fusion,\n-                                   SymbolicExprContext* symbolic_expr_context)\n+                                   MLIRContext* mlir_context)\n     : buffer_assignment_(buffer_assignment),\n       fusion_(fusion),\n-      symbolic_expr_context_(symbolic_expr_context) {\n+      mlir_context_(mlir_context) {\n   const auto* scatter = Cast<HloScatterInstruction>(\n       fusion->fused_instructions_computation()->root_instruction());\n   auto update_shape = scatter->scatter_updates().front()->shape();\n@@ -251,7 +250,7 @@ IndexingMap GetScatterIndexingMap(\n \n absl::StatusOr<CpuScatterFusion::KernelDefinition>\n CpuScatterFusion::EmitKernelDefinition() {\n-  mlir::OpBuilder builder(symbolic_expr_context_->GetMLIRContext());\n+  mlir::OpBuilder builder(mlir_context_);\n   TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> mlir_module,\n                       CreateNamedMlirModuleOp(*fusion_, builder));\n \n@@ -275,10 +274,9 @@ CpuScatterFusion::EmitKernelDefinition() {\n                            std::string(module_name), buffer_assignment_));\n \n   std::vector<emitters::EpilogueSpecification> epilogues =\n-      GetEpilogues(*fusion_, symbolic_expr_context_);\n+      GetEpilogues(*fusion_, mlir_context_);\n   emitters::PartitionedComputations computations(\n-      fusion_->fused_instructions_computation(), symbolic_expr_context_,\n-      epilogues);\n+      fusion_->fused_instructions_computation(), mlir_context_, epilogues);\n   TF_ASSIGN_OR_RETURN(\n       emitters::CallTargetProvider call_targets,\n       EmitCallTargets(mlir_module.get(), *fusion_, computations, epilogues));"
        },
        {
            "sha": "20c4a7332b86f0bff0d81f69ee3d62cb1ddc1f0d",
            "filename": "third_party/xla/xla/backends/cpu/codegen/emitters/cpu_scatter_emitter.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_scatter_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_scatter_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Femitters%2Fcpu_scatter_emitter.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -42,7 +42,7 @@ class CpuScatterFusion final : public KernelEmitter<MlirKernelSource> {\n  public:\n   CpuScatterFusion(const BufferAssignment& buffer_assignment,\n                    const HloFusionInstruction* fusion,\n-                   SymbolicExprContext* symbolic_expr_context);\n+                   mlir::MLIRContext* mlir_context);\n \n   absl::string_view name() const final { return \"cpu_scatter_fusion\"; }\n   absl::StatusOr<KernelDefinition> EmitKernelDefinition() final;\n@@ -56,21 +56,21 @@ class CpuScatterFusion final : public KernelEmitter<MlirKernelSource> {\n \n   std::vector<emitters::EpilogueSpecification> GetEpilogues(\n       const HloFusionInstruction& fusion,\n-      SymbolicExprContext* symbolic_expr_context) const;\n+      mlir::MLIRContext* mlir_context) const;\n \n   mlir::Value EmitThreadId(mlir::ImplicitLocOpBuilder& builder, int dim) const;\n \n   // These two methods do not seem to be used @ecg?\n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t root_index, SymbolicExprContext* ctx) const;\n+      int64_t root_index, mlir::MLIRContext* ctx) const;\n \n   std::optional<IndexingMap> ComputeThreadIdToInputIndexing(\n       int64_t root_index, int64_t hero_operand_index,\n-      SymbolicExprContext* ctx) const;\n+      mlir::MLIRContext* ctx) const;\n \n   const BufferAssignment& buffer_assignment_;\n   const HloFusionInstruction* fusion_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n \n   int64_t vector_size_;\n   int64_t num_threads_;"
        },
        {
            "sha": "930911b3d6850487f8eecee3afefaba5d4945cce",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_emitter.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -211,7 +211,7 @@ static HloFusionSpec GetLoopFusionSpec(const HloFusionInstruction& fusion) {\n }\n \n static absl::StatusOr<KernelDefinition<MlirKernelSource>> EmitLoopFusionKernel(\n-    SymbolicExprContext& context, const HloFusionInstruction& fusion,\n+    MLIRContext& context, const HloFusionInstruction& fusion,\n     const BufferAssignment* buffer_assignment, absl::string_view name) {\n   VLOG(2) << \"Emitting loop fusion kernel: \" << name;\n   HloFusionSpec fusion_spec = GetLoopFusionSpec(fusion);\n@@ -223,7 +223,7 @@ static absl::StatusOr<KernelDefinition<MlirKernelSource>> EmitLoopFusionKernel(\n   TF_ASSIGN_OR_RETURN(auto mlir_kernel_definition,\n                       loop_fusion_emitter.EmitKernelDefinition());\n \n-  mlir::OpBuilder builder(context.GetMLIRContext());\n+  mlir::OpBuilder builder(&context);\n   mlir_kernel_definition.source().module().getOperation()->setAttr(\n       xla::CpuMemoryRegionNameAttr::name,\n       builder.getStringAttr(\n@@ -233,7 +233,7 @@ static absl::StatusOr<KernelDefinition<MlirKernelSource>> EmitLoopFusionKernel(\n }\n \n static absl::StatusOr<KernelDefinition<MlirKernelSource>>\n-EmitConcatenateFusionKernel(SymbolicExprContext& context,\n+EmitConcatenateFusionKernel(MLIRContext& context,\n                             const HloFusionInstruction& fusion,\n                             const BufferAssignment* buffer_assignment,\n                             absl::string_view name) {\n@@ -247,7 +247,7 @@ EmitConcatenateFusionKernel(SymbolicExprContext& context,\n   TF_ASSIGN_OR_RETURN(auto mlir_kernel_definition,\n                       concatenate_fusion_emitter.EmitKernelDefinition());\n \n-  mlir::OpBuilder builder(context.GetMLIRContext());\n+  mlir::OpBuilder builder(&context);\n   mlir_kernel_definition.source().module().getOperation()->setAttr(\n       xla::CpuMemoryRegionNameAttr::name,\n       builder.getStringAttr(BuildModuleMemoryRegionName(\n@@ -257,7 +257,7 @@ EmitConcatenateFusionKernel(SymbolicExprContext& context,\n }\n \n static absl::StatusOr<KernelDefinition<MlirKernelSource>>\n-EmitDynamicUpdateSliceFusionKernel(SymbolicExprContext& context,\n+EmitDynamicUpdateSliceFusionKernel(MLIRContext& context,\n                                    const HloFusionInstruction& fusion,\n                                    const BufferAssignment* buffer_assignment,\n                                    absl::string_view name) {\n@@ -272,7 +272,7 @@ EmitDynamicUpdateSliceFusionKernel(SymbolicExprContext& context,\n   TF_ASSIGN_OR_RETURN(auto mlir_kernel_definition,\n                       emitter.EmitKernelDefinition());\n \n-  mlir::OpBuilder builder(context.GetMLIRContext());\n+  mlir::OpBuilder builder(&context);\n   mlir_kernel_definition.source().module().getOperation()->setAttr(\n       xla::CpuMemoryRegionNameAttr::name,\n       builder.getStringAttr(\n@@ -282,15 +282,14 @@ EmitDynamicUpdateSliceFusionKernel(SymbolicExprContext& context,\n }\n \n absl::StatusOr<KernelDefinition<MlirKernelSource>> EmitFusionKernel(\n-    MLIRContext& mlir_context, SymbolicExprContext& expr_context,\n-    const HloFusionInstruction& fusion,\n+    MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n     const BufferAssignment* buffer_assignment, bool use_unique_c_name) {\n   if (fusion.fusion_kind() == HloFusionInstruction::FusionKind::kLoop) {\n     TF_ASSIGN_OR_RETURN(std::string name, GetName(fusion, use_unique_c_name));\n     const HloInstruction& hero =\n         FindNonTrivialHero(*fusion.fused_expression_root());\n     if (hero.opcode() == HloOpcode::kConcatenate) {\n-      return EmitConcatenateFusionKernel(expr_context, fusion,\n+      return EmitConcatenateFusionKernel(mlir_context, fusion,\n                                          buffer_assignment, name);\n     }\n     auto fusion_spec = GetLoopFusionSpec(fusion);\n@@ -300,11 +299,11 @@ absl::StatusOr<KernelDefinition<MlirKernelSource>> EmitFusionKernel(\n           CanEmitFusedDynamicUpdateSliceInPlace(fusion_spec.fusion(),\n                                                 buffer_assignment, &fusion));\n       if (dus_inplace) {\n-        return EmitDynamicUpdateSliceFusionKernel(expr_context, fusion,\n+        return EmitDynamicUpdateSliceFusionKernel(mlir_context, fusion,\n                                                   buffer_assignment, name);\n       }\n     }\n-    return EmitLoopFusionKernel(expr_context, fusion, buffer_assignment, name);\n+    return EmitLoopFusionKernel(mlir_context, fusion, buffer_assignment, name);\n   }\n \n   return absl::UnimplementedError(\"Fusion kind not supported.\");"
        },
        {
            "sha": "bf5b791d8367ae82f91adb4349bb514cc3ea25c1",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_emitter.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -30,8 +30,7 @@ namespace xla::cpu {\n emitters::KernelArguments::BufferAlignment GetDefaultBufferAlignment();\n \n absl::StatusOr<KernelDefinition<MlirKernelSource>> EmitFusionKernel(\n-    mlir::MLIRContext& mlir_context, SymbolicExprContext& expr_context,\n-    const HloFusionInstruction& fusion,\n+    mlir::MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n     const BufferAssignment* buffer_assignment, bool use_unique_c_name);\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "54567125487887193cce5b7eea84b2e95de996b6",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_emitter_test.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter_test.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter_test.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_emitter_test.py?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -53,10 +53,8 @@ def test_basic_add_sub(self):\n     hlo_module, buffer_assignment = utilities.parse_hlo_module(hlo)\n     jit_compiler = testlib_cpu.JitCompiler(hlo_module.get_config())\n     mlir_context = testlib_cpu.MLIRContext()\n-    symbolic_expr_context = testlib_cpu.SymbolicExprContext(mlir_context)\n     kernel_definition = testlib_cpu.emit_fusion_kernel(\n         mlir_context,\n-        symbolic_expr_context,\n         hlo_module.get_root_instruction(),\n         buffer_assignment,\n     )\n@@ -115,10 +113,8 @@ def test_convert_f32_bf16_f32(self):\n     hlo_module, buffer_assignment = utilities.parse_hlo_module(hlo)\n     jit_compiler = testlib_cpu.JitCompiler(hlo_module.get_config())\n     mlir_context = testlib_cpu.MLIRContext()\n-    symbolic_expr_context = testlib_cpu.SymbolicExprContext(mlir_context)\n     kernel_definition = testlib_cpu.emit_fusion_kernel(\n         mlir_context,\n-        symbolic_expr_context,\n         hlo_module.get_root_instruction(),\n         buffer_assignment,\n     )\n@@ -172,10 +168,8 @@ def test_convert_f32_bf16_f32_nan(self):\n     hlo_module, buffer_assignment = utilities.parse_hlo_module(hlo)\n     jit_compiler = testlib_cpu.JitCompiler(hlo_module.get_config())\n     mlir_context = testlib_cpu.MLIRContext()\n-    symbolic_expr_context = testlib_cpu.SymbolicExprContext(mlir_context)\n     kernel_definition = testlib_cpu.emit_fusion_kernel(\n         mlir_context,\n-        symbolic_expr_context,\n         hlo_module.get_root_instruction(),\n         buffer_assignment,\n     )\n@@ -226,10 +220,8 @@ def test_constant_with_layout(self):\n     hlo_module, buffer_assignment = utilities.parse_hlo_module(hlo)\n     jit_compiler = testlib_cpu.JitCompiler(hlo_module.get_config())\n     mlir_context = testlib_cpu.MLIRContext()\n-    symbolic_expr_context = testlib_cpu.SymbolicExprContext(mlir_context)\n     kernel_definition = testlib_cpu.emit_fusion_kernel(\n         mlir_context,\n-        symbolic_expr_context,\n         hlo_module.get_root_instruction(),\n         buffer_assignment,\n     )\n@@ -277,10 +269,8 @@ def test_exp_nan_dce(self):\n     hlo_module, buffer_assignment = utilities.parse_hlo_module(hlo)\n     jit_compiler = testlib_cpu.JitCompiler(hlo_module.get_config())\n     mlir_context = testlib_cpu.MLIRContext()\n-    symbolic_expr_context = testlib_cpu.SymbolicExprContext(mlir_context)\n     kernel_definition = testlib_cpu.emit_fusion_kernel(\n         mlir_context,\n-        symbolic_expr_context,\n         hlo_module.get_root_instruction(),\n         buffer_assignment,\n     )"
        },
        {
            "sha": "d6ccc0535a68a2026e03e1c86b92fb88f9db826c",
            "filename": "third_party/xla/xla/backends/cpu/codegen/scatter_kernel_emitter_test.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fscatter_kernel_emitter_test.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fscatter_kernel_emitter_test.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Fscatter_kernel_emitter_test.py?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -95,12 +95,10 @@ def create_scatter_runner(\n   hlo_module, buffer_assignment = utilities.annotate_hlo_module(hlo_module)\n \n   mlir_context = testlib_cpu.MLIRContext()\n-  symbolic_expr_context = testlib_cpu.SymbolicExprContext(mlir_context)\n-\n   scatter_emitter = testlib_cpu.ScatterKernelEmitter(\n       hlo_module.get_root_instruction(),\n       buffer_assignment,\n-      symbolic_expr_context,\n+      mlir_context,\n   )\n   kernel_definition = scatter_emitter.emit_kernel_definition()\n "
        },
        {
            "sha": "08f99af10781cc5a90409d5795eedbde55b7b5fa",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tools/fusion_to_mlir.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftools%2Ffusion_to_mlir.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftools%2Ffusion_to_mlir.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftools%2Ffusion_to_mlir.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -32,14 +32,12 @@ namespace xla::cpu {\n \n absl::Status Run(const std::string& filename) {\n   auto mlir_context = FusionCompiler::CreateContext();\n-  auto expr_context = std::make_unique<SymbolicExprContext>(mlir_context.get());\n   TF_ASSIGN_OR_RETURN(auto module, LoadTestModule(filename));\n   auto fusion = DynCast<HloFusionInstruction>(\n       module->entry_computation()->root_instruction());\n   fusion->SetAndSanitizeName(\"main\");\n-  TF_ASSIGN_OR_RETURN(\n-      KernelDefinition kernel_definition,\n-      EmitFusionKernel(*mlir_context, *expr_context, *fusion, nullptr, false));\n+  TF_ASSIGN_OR_RETURN(KernelDefinition kernel_definition,\n+                      EmitFusionKernel(*mlir_context, *fusion, nullptr, false));\n   llvm::outs() << kernel_definition.source().ToString();\n   return absl::OkStatus();\n }"
        },
        {
            "sha": "3261fc946d3f0abf7404d9cd1fc1b3c513bb3d90",
            "filename": "third_party/xla/xla/backends/cpu/testlib/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2F__init__.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2F__init__.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2F__init__.py?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -29,7 +29,6 @@\n MLIRContext = _extension.MLIRContext\n MlirTestKernelEmitter = _extension.MlirTestKernelEmitter\n ScatterKernelEmitter = _extension.ScatterKernelEmitter\n-SymbolicExprContext = _extension.SymbolicExprContext\n TargetMachineFeatures = _extension.TargetMachineFeatures\n # go/keep-sorted end\n "
        },
        {
            "sha": "a11fd17a2c73b5f40886a560b3fc299ed66a3e94",
            "filename": "third_party/xla/xla/backends/cpu/testlib/kernel_runner_extension.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 11,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2Fkernel_runner_extension.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2Fkernel_runner_extension.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftestlib%2Fkernel_runner_extension.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -46,7 +46,6 @@ limitations under the License.\n #include \"xla/codegen/llvm_kernel_source.h\"\n #include \"xla/codegen/mlir_kernel_source.h\"\n #include \"xla/codegen/testlib/kernel_runner.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -153,9 +152,6 @@ NB_MODULE(_extension, kernel_runner_module) {\n   nb::class_<mlir::MLIRContext>(kernel_runner_module, \"MLIRContext\")\n       .def(nb::new_([] { return FusionCompiler::CreateContext(); }));\n \n-  nb::class_<SymbolicExprContext>(kernel_runner_module, \"SymbolicExprContext\")\n-      .def(nb::init<mlir::MLIRContext*>(), nb::keep_alive<1, 2>());\n-\n   nb::class_<TargetMachineFeatures>(kernel_runner_module,\n                                     \"TargetMachineFeatures\")\n       .def(\"__str__\", &TargetMachineFeatures::get_target_feature_string);\n@@ -194,20 +190,19 @@ NB_MODULE(_extension, kernel_runner_module) {\n           \"__init__\",\n           [](CpuScatterFusion* self, const HloFusionInstruction* instruction,\n              const BufferAssignment* buffer_assignment,\n-             SymbolicExprContext* symbolic_expr_context) {\n-            new (self) CpuScatterFusion(*buffer_assignment, instruction,\n-                                        symbolic_expr_context);\n+             mlir::MLIRContext* mlir_context) {\n+            new (self)\n+                CpuScatterFusion(*buffer_assignment, instruction, mlir_context);\n           },\n           nb::keep_alive<1, 2>(), nb::keep_alive<1, 3>(),\n           nb::keep_alive<1, 4>());\n \n   kernel_runner_module.def(\n       \"emit_fusion_kernel\",\n-      [](mlir::MLIRContext& mlir_context, SymbolicExprContext& expr_context,\n-         const HloFusionInstruction& fusion,\n+      [](mlir::MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n          const BufferAssignment* buffer_assignment) {\n-        auto kernel_definition = EmitFusionKernel(\n-            mlir_context, expr_context, fusion, buffer_assignment, false);\n+        auto kernel_definition =\n+            EmitFusionKernel(mlir_context, fusion, buffer_assignment, false);\n         if (!kernel_definition.ok()) {\n           throw std::runtime_error(kernel_definition.status().ToString());\n         }"
        },
        {
            "sha": "d0dde767ca8dbeb767eddbb26a95eb1a743492e5",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -427,7 +427,6 @@ cc_library(\n         \"//xla:util\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/backends/autotuner:codegen_backend\",\n-        \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/utils:hlo_query\",\n         \"//xla/service:call_inliner\",\n@@ -453,6 +452,7 @@ cc_library(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -504,7 +504,6 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/backends/autotuner:codegen_backend\",\n-        \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/transforms/simplifiers:float_normalization\",\n         \"//xla/hlo/utils:hlo_query\",\n@@ -534,6 +533,7 @@ cc_library(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -727,9 +727,9 @@ cc_library(\n     hdrs = [\"factory.h\"],\n     deps = [\n         \"//xla/backends/autotuner:codegen_backend\",\n-        \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/service:compiler\",\n         \"//xla/stream_executor:stream_executor_h\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -801,7 +801,6 @@ cc_library(\n     deps = [\n         \":gpu_codegen_backend\",\n         \"//xla/backends/autotuner:codegen_backend\",\n-        \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass_pipeline\",\n         \"//xla/service:compiler\",\n@@ -816,6 +815,7 @@ cc_library(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n "
        },
        {
            "sha": "bb2dadf1f7093dbd82a4c26174ceeae6055104c8",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/autotuner_main.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -85,7 +85,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> GetModule(\n \n absl::Status Autotune(HloModule& module, const std::string& cache_dir,\n                       const std::string& autotune_cache_mode_str,\n-                      SymbolicExprContext* symbolic_expr_context) {\n+                      mlir::MLIRContext* mlir_context) {\n   TF_ASSIGN_OR_RETURN(std::string platform_name,\n                       PlatformUtil::CanonicalPlatformName(\"gpu\"));\n \n@@ -107,7 +107,7 @@ absl::Status Autotune(HloModule& module, const std::string& cache_dir,\n                       registry.FindObject<GetCodegenBackends>(platform->id()));\n   std::vector<std::unique_ptr<CodegenBackend>> backends =\n       get_codegen_backends(stream_executor, &debug_options, compiler.get(),\n-                           &target_config, symbolic_expr_context);\n+                           &target_config, mlir_context);\n \n   std::unique_ptr<se::DeviceMemoryAllocator> allocator =\n       std::make_unique<stream_executor::StreamExecutorMemoryAllocator>(\n@@ -182,9 +182,8 @@ int main(int argc, char* argv[]) {\n   auto module = xla::gpu::GetModule(hlo_file);\n   CHECK_OK(module.status());\n   mlir::MLIRContext mlir_context;\n-  xla::SymbolicExprContext symbolic_expr_context(&mlir_context);\n   CHECK_OK(xla::gpu::Autotune(*module.value(), cache_dir, autotune_cache_mode,\n-                              &symbolic_expr_context));\n+                              &mlir_context));\n   std::cout << module.value()->ToString() << std::endl;\n   return 0;\n }"
        },
        {
            "sha": "94cfd7c9fcb5a7d2d244592b63133fab95bb89c0",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -32,7 +32,6 @@ limitations under the License.\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -305,11 +304,9 @@ BlockLevelEmitterBackend::GetCostModelConfig(\n     const HloInstruction& instr) const {\n   auto device_info = target_config().device_description;\n   HloFusionAnalysisCache fusion_analysis_cache(device_info);\n-  mlir::MLIRContext ctx;\n-  SymbolicExprContext symbolic_expr_context(&ctx);\n+  mlir::MLIRContext mlir_context;\n   GpuPerformanceModelWithIndexingAnalysis indexing_performance_model(\n-      &device_info, &fusion_analysis_cache, shape_size_fn_,\n-      &symbolic_expr_context);\n+      &device_info, &fusion_analysis_cache, shape_size_fn_, &mlir_context);\n \n   auto fusion_adaptor =\n       HloFusionAdaptor::ForInstruction(Cast<HloFusionInstruction>(&instr));"
        },
        {
            "sha": "bf9570538e9ace03f9667ff546b08b43f7e84496",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory.h",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -20,8 +20,8 @@ limitations under the License.\n #include <memory>\n #include <vector>\n \n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n \n@@ -32,15 +32,13 @@ namespace gpu {\n struct GetCodegenBackends {\n   using Type = std::function<std::vector<std::unique_ptr<CodegenBackend>>(\n       stream_executor::StreamExecutor*, const DebugOptions*, Compiler*,\n-      const Compiler::GpuTargetConfig*,\n-      SymbolicExprContext* symbolic_expr_context)>;\n+      const Compiler::GpuTargetConfig*, mlir::MLIRContext* mlir_context)>;\n };\n \n struct GetFissionBackends {\n   using Type = std::function<std::vector<std::unique_ptr<CodegenBackend>>(\n       stream_executor::StreamExecutor*, const DebugOptions*, Compiler*,\n-      const Compiler::GpuTargetConfig*,\n-      SymbolicExprContext* symbolic_expr_context)>;\n+      const Compiler::GpuTargetConfig*, mlir::MLIRContext* mlir_context)>;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "1b4f73b9a7c2ecc0e8df32d8a974bdc017c66559",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory_cuda.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -27,7 +27,6 @@ limitations under the License.\n #include \"xla/backends/gpu/autotuner/factory.h\"\n #include \"xla/backends/gpu/autotuner/fission_backend.h\"\n #include \"xla/backends/gpu/autotuner/triton.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/service/gpu/transforms/dot_algorithm_rewriter.h\"\n@@ -39,9 +38,10 @@ limitations under the License.\n \n namespace xla {\n namespace gpu {\n-\n namespace {\n \n+using ::mlir::MLIRContext;\n+\n std::unique_ptr<HloPassPipeline> GetCublasRewriterPipeline(\n     const se::DeviceDescription& device_description) {\n   auto pipeline = std::make_unique<HloPassPipeline>(\"cublas_rewriter_pipeline\");\n@@ -62,11 +62,10 @@ std::unique_ptr<HloPassPipeline> GetCublasRewriterPipeline(\n std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForCuda(\n     stream_executor::StreamExecutor* stream_executor,\n     const DebugOptions* debug_options, Compiler* compiler,\n-    const Compiler::GpuTargetConfig* target_config,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    const Compiler::GpuTargetConfig* target_config, MLIRContext* mlir_context) {\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<TritonBackend>(\n-      debug_options, compiler, target_config, symbolic_expr_context));\n+      debug_options, compiler, target_config, mlir_context));\n   backends.push_back(std::make_unique<CublasBackend>(\n       stream_executor, debug_options, compiler, target_config));\n   backends.push_back(std::make_unique<CublasLtBackend>(\n@@ -79,15 +78,14 @@ std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForCuda(\n std::vector<std::unique_ptr<CodegenBackend>> GetFissionBackendsForCuda(\n     stream_executor::StreamExecutor* stream_executor,\n     const DebugOptions* debug_options, Compiler* compiler,\n-    const Compiler::GpuTargetConfig* target_config,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    const Compiler::GpuTargetConfig* target_config, MLIRContext* mlir_context) {\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<FissionBackend>(\n       debug_options, compiler, target_config,\n       std::make_unique<CublasBackend>(stream_executor, debug_options, compiler,\n                                       target_config),\n       GetCublasRewriterPipeline(target_config->device_description),\n-      symbolic_expr_context));\n+      mlir_context));\n   return backends;\n }\n "
        },
        {
            "sha": "602aa66217ca2b28c6cc5f04027c834d9444d8fd",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory_rocm.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -32,14 +32,15 @@ limitations under the License.\n namespace xla {\n namespace gpu {\n \n+using ::mlir::MLIRContext;\n+\n std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForROCm(\n     stream_executor::StreamExecutor* stream_executor,\n     const DebugOptions* debug_options, Compiler* compiler,\n-    const Compiler::GpuTargetConfig* target_config,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    const Compiler::GpuTargetConfig* target_config, MLIRContext* mlir_context) {\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   backends.push_back(std::make_unique<TritonBackend>(\n-      debug_options, compiler, target_config, symbolic_expr_context));\n+      debug_options, compiler, target_config, mlir_context));\n   backends.push_back(std::make_unique<CublasBackend>(\n       stream_executor, debug_options, compiler, target_config));\n   return backends;\n@@ -48,8 +49,7 @@ std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForROCm(\n std::vector<std::unique_ptr<CodegenBackend>> GetFissionBackendsForROCm(\n     stream_executor::StreamExecutor* stream_executor,\n     const DebugOptions* debug_options, Compiler* compiler,\n-    const Compiler::GpuTargetConfig* target_config,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    const Compiler::GpuTargetConfig* target_config, MLIRContext* mlir_context) {\n   return {};\n }\n "
        },
        {
            "sha": "8953ad72ec566ee12fcdf719f44ce8e42fa13694",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -31,7 +31,6 @@ limitations under the License.\n #include \"xla/backends/gpu/autotuner/cublas.h\"\n #include \"xla/backends/gpu/autotuner/cublaslt.h\"\n #include \"xla/backends/gpu/autotuner/custom_kernel.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -60,6 +59,9 @@ namespace xla {\n namespace gpu {\n \n namespace se = ::stream_executor;\n+\n+using ::mlir::MLIRContext;\n+\n using CublasOrCublasLtBackendConfig = AutotuneResult::GemmKey;\n using CustomKernelBackendConfig = AutotuneResult::CustomKernelFusionKey;\n \n@@ -80,7 +82,7 @@ HloCostAnalysis::Options PriorityFusionOptions() {\n absl::Status FissionToCublas(HloModule* hlo_module,\n                              const se::DeviceDescription& device_description,\n                              bool rewrite_to_cublaslt,\n-                             SymbolicExprContext* symbolic_expr_context) {\n+                             MLIRContext* mlir_context) {\n   hlo_module->mutable_config()\n       .mutable_debug_options()\n       .set_xla_gpu_enable_cublaslt(rewrite_to_cublaslt);\n@@ -124,7 +126,7 @@ absl::Status FissionToCublas(HloModule* hlo_module,\n \n     PriorityFusion fusion_pass(\n         /*thread_pool=*/nullptr, device_description, PriorityFusionOptions(),\n-        symbolic_expr_context);\n+        mlir_context);\n     TF_RETURN_IF_ERROR(fusion_pass.Run(hlo_module).status());\n   }\n \n@@ -137,11 +139,11 @@ absl::Status FissionToCublas(HloModule* hlo_module,\n \n absl::Status FissionToCustomKernel(\n     HloModule* hlo_module, const se::DeviceDescription& device_description,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   CustomKernelFusionRewriter custom_kernel_fusion_rewriter(&device_description);\n   PriorityFusion fusion_pass(\n       /*thread_pool=*/nullptr, device_description, PriorityFusionOptions(),\n-      symbolic_expr_context);\n+      mlir_context);\n   TF_ASSIGN_OR_RETURN(bool is_rewritten_to_custom_kernel,\n                       custom_kernel_fusion_rewriter.Run(hlo_module));\n   TF_RETURN_IF_ERROR(fusion_pass.Run(hlo_module).status());\n@@ -240,7 +242,7 @@ FissionBackend::GetSupportedConfigs(const HloInstruction& instr) {\n   std::unique_ptr<HloModule> cublas_hlo_module = hlo_module->Clone();\n   if (FissionToCublas(cublas_hlo_module.get(),\n                       target_config().device_description,\n-                      /*rewrite_to_cublaslt=*/false, symbolic_expr_context_)\n+                      /*rewrite_to_cublaslt=*/false, mlir_context_)\n           .ok()) {\n     TF_ASSIGN_OR_RETURN(\n         std::vector<std::unique_ptr<BackendConfig>> cublas_configs,\n@@ -255,7 +257,7 @@ FissionBackend::GetSupportedConfigs(const HloInstruction& instr) {\n   std::unique_ptr<HloModule> cublaslt_hlo_module = hlo_module->Clone();\n   if (FissionToCublas(cublaslt_hlo_module.get(),\n                       target_config().device_description,\n-                      /*rewrite_to_cublaslt=*/true, symbolic_expr_context_)\n+                      /*rewrite_to_cublaslt=*/true, mlir_context_)\n           .ok()) {\n     TF_ASSIGN_OR_RETURN(\n         std::vector<std::unique_ptr<BackendConfig>> cublaslt_configs,\n@@ -269,8 +271,7 @@ FissionBackend::GetSupportedConfigs(const HloInstruction& instr) {\n \n   std::unique_ptr<HloModule> custom_kernel_hlo_module = hlo_module->Clone();\n   if (FissionToCustomKernel(custom_kernel_hlo_module.get(),\n-                            target_config().device_description,\n-                            symbolic_expr_context_)\n+                            target_config().device_description, mlir_context_)\n           .ok()) {\n     TF_ASSIGN_OR_RETURN(\n         std::vector<std::unique_ptr<BackendConfig>> custom_kernel_configs,\n@@ -318,7 +319,7 @@ absl::Status FissionBackend::ApplyConfig(HloInstruction& instr,\n   if (!use_cublaslt && config.Is<CublasOrCublasLtBackendConfig>()) {\n     TF_RETURN_IF_ERROR(\n         FissionToCublas(hlo_module, target_config().device_description,\n-                        /*rewrite_to_cublaslt=*/false, symbolic_expr_context_));\n+                        /*rewrite_to_cublaslt=*/false, mlir_context_));\n     for (HloComputation* computation :\n          hlo_module->MakeNonfusionComputations()) {\n       for (HloInstruction* instruction : computation->instructions()) {\n@@ -334,7 +335,7 @@ absl::Status FissionBackend::ApplyConfig(HloInstruction& instr,\n   if (use_cublaslt && config.Is<CublasOrCublasLtBackendConfig>()) {\n     TF_RETURN_IF_ERROR(\n         FissionToCublas(hlo_module, target_config().device_description,\n-                        /*rewrite_to_cublaslt=*/true, symbolic_expr_context_));\n+                        /*rewrite_to_cublaslt=*/true, mlir_context_));\n     for (HloComputation* computation :\n          hlo_module->MakeNonfusionComputations()) {\n       for (HloInstruction* instruction : computation->instructions()) {\n@@ -350,9 +351,8 @@ absl::Status FissionBackend::ApplyConfig(HloInstruction& instr,\n   }\n \n   if (config.Is<CustomKernelBackendConfig>()) {\n-    TF_RETURN_IF_ERROR(FissionToCustomKernel(hlo_module,\n-                                             target_config().device_description,\n-                                             symbolic_expr_context_));\n+    TF_RETURN_IF_ERROR(FissionToCustomKernel(\n+        hlo_module, target_config().device_description, mlir_context_));\n     for (HloComputation* computation : hlo_module->computations()) {\n       if (IsCustomKernel(computation)) {\n         TF_RETURN_IF_ERROR(custom_kernel_backend_.ApplyConfig("
        },
        {
            "sha": "1bb54ed13ab35054dfc443d65d0e1192c4a88986",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -22,12 +22,12 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/cublas.h\"\n #include \"xla/backends/gpu/autotuner/cublaslt.h\"\n #include \"xla/backends/gpu/autotuner/custom_kernel.h\"\n #include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -46,15 +46,15 @@ class FissionBackend : public GpuCodegenBackend {\n   explicit FissionBackend(stream_executor::StreamExecutor* stream_executor,\n                           const DebugOptions* debug_options, Compiler* compiler,\n                           const Compiler::GpuTargetConfig* target_config,\n-                          SymbolicExprContext* symbolic_expr_context)\n+                          mlir::MLIRContext* mlir_context)\n       : GpuCodegenBackend(\"Fission\", debug_options, compiler, target_config),\n         cublas_backend_(stream_executor, debug_options, compiler,\n                         target_config),\n         cublaslt_backend_(stream_executor, debug_options, compiler,\n                           target_config),\n         custom_kernel_backend_(stream_executor, debug_options, compiler,\n                                target_config),\n-        symbolic_expr_context_(symbolic_expr_context) {}\n+        mlir_context_(mlir_context) {}\n \n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n   GetSupportedConfigs(const HloInstruction& instr) override;\n@@ -73,7 +73,7 @@ class FissionBackend : public GpuCodegenBackend {\n   CublasBackend cublas_backend_;\n   CublasLtBackend cublaslt_backend_;\n   CustomKernelBackend custom_kernel_backend_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "2cddcba025c93271e031c3bb314ba7f823c6c0c0",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_backend.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -123,7 +123,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> FissionBackend::RunHloPasses(\n   // TODO: b/407494653 - Get rid of PriorityFusion.\n   PriorityFusion priority_fusion(\n       /*thread_pool=*/nullptr, target_config().device_description,\n-      priority_fusion_options, symbolic_expr_context_);\n+      priority_fusion_options, mlir_context_);\n   TF_RETURN_IF_ERROR(priority_fusion.Run(module.get()).status());\n   return module;\n }"
        },
        {
            "sha": "01f42446ec07d6faa235c0148a3437ace424349f",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_backend.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -23,9 +23,9 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n #include \"xla/service/compiler.h\"\n@@ -47,14 +47,14 @@ class FissionBackend : public GpuCodegenBackend {\n                  const Compiler::GpuTargetConfig* target_config,\n                  std::unique_ptr<GpuCodegenBackend> backend,\n                  std::unique_ptr<HloPassPipeline> rewriter_pipeline,\n-                 SymbolicExprContext* symbolic_expr_context,\n+                 mlir::MLIRContext* mlir_context,\n                  stream_executor::StreamExecutor* stream_executor = nullptr)\n       : GpuCodegenBackend(absl::StrCat(backend->name(), \"_fission\"),\n                           debug_options, compiler, target_config,\n                           stream_executor),\n         rewriter_pipeline_(std::move(rewriter_pipeline)),\n         codegen_backend_(std::move(backend)),\n-        symbolic_expr_context_(symbolic_expr_context) {}\n+        mlir_context_(mlir_context) {}\n   ~FissionBackend() override = default;\n \n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n@@ -80,7 +80,7 @@ class FissionBackend : public GpuCodegenBackend {\n \n   std::unique_ptr<HloPassPipeline> rewriter_pipeline_;\n   std::unique_ptr<GpuCodegenBackend> codegen_backend_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "64877558f905596a733163faafa30de46ec28062",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_backend_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -30,7 +30,6 @@ limitations under the License.\n #include \"xla/backends/gpu/autotuner/cublas.h\"\n #include \"xla/backends/gpu/autotuner/custom_kernel.h\"\n #include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n@@ -163,7 +162,6 @@ class FissionTest : public HloHardwareIndependentTestBase,\n   std::unique_ptr<GpuCodegenBackend> base_codegen_backend_;\n   std::unique_ptr<FissionBackend> fission_backend_;\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n \n   FissionTest()\n       : stream_executor_(PlatformUtil::GetDefaultPlatform()\n@@ -178,7 +176,7 @@ class FissionTest : public HloHardwareIndependentTestBase,\n         fission_backend_(std::make_unique<FissionBackend>(\n             &debug_options_, &compiler_, &target_config_,\n             std::move(base_codegen_backend_), std::move(rewriter_pipeline_),\n-            &symbolic_expr_context_, stream_executor_)) {}\n+            &mlir_context_, stream_executor_)) {}\n };\n \n TEST_P(FissionTest, CanCreateFissionBackend) {"
        },
        {
            "sha": "56b8907c42c13bfeb46d13a202d09bff31e76fe8",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -77,7 +77,6 @@ class FissionBackendTest : public HloHardwareIndependentTestBase {\n   Compiler::GpuTargetConfig target_config_;\n   FissionBackend backend_;\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n \n   FissionBackendTest()\n       : stream_executor_(PlatformUtil::GetDefaultPlatform()\n@@ -86,7 +85,7 @@ class FissionBackendTest : public HloHardwareIndependentTestBase {\n                              .value()),\n         target_config_(stream_executor_),\n         backend_(stream_executor_, &debug_options_, &compiler_, &target_config_,\n-                 &symbolic_expr_context_) {}\n+                 &mlir_context_) {}\n };\n \n TEST_F(FissionBackendTest, CanCreateCublasBackend) {"
        },
        {
            "sha": "f5830a1b3d3ba3b17a15f6a2033bb030825c5524",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -203,15 +203,15 @@ absl::StatusOr<std::unique_ptr<HloModule>> TritonBackend::RunHloPasses(\n   priority_fusion_options.count_multiple_input_accesses = true;\n   PriorityFusion priority_fusion(\n       /*thread_pool=*/nullptr, gpu_device_info, priority_fusion_options,\n-      symbolic_expr_context_);\n+      mlir_context_);\n   TF_RETURN_IF_ERROR(priority_fusion.Run(hlo_module.get()).status());\n \n   // If the priority fusion pass above skipped some instructions, turn them\n   // into fusions.\n   FusionWrapper fusion_wrapper(gpu_device_info);\n   TF_RETURN_IF_ERROR(fusion_wrapper.Run(hlo_module.get()).status());\n \n-  NestGemmFusion nest_gemm_fusion(gpu_device_info, symbolic_expr_context_);\n+  NestGemmFusion nest_gemm_fusion(gpu_device_info, mlir_context_);\n   TF_RETURN_IF_ERROR(nest_gemm_fusion.Run(hlo_module.get()).status());\n \n   bool is_legacy_gemm_disabled = absl::c_contains("
        },
        {
            "sha": "2c36114d0138b769488f51f207b01c5844181c27",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -21,9 +21,9 @@ limitations under the License.\n \n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/compiler.h\"\n@@ -38,9 +38,9 @@ class TritonBackend : public GpuCodegenBackend {\n  public:\n   explicit TritonBackend(const DebugOptions* debug_options, Compiler* compiler,\n                          const Compiler::GpuTargetConfig* target_config,\n-                         SymbolicExprContext* symbolic_expr_context)\n+                         mlir::MLIRContext* mlir_context)\n       : GpuCodegenBackend(\"Triton\", debug_options, compiler, target_config),\n-        symbolic_expr_context_(symbolic_expr_context) {}\n+        mlir_context_(mlir_context) {}\n \n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n   GetSupportedConfigs(const HloInstruction& instr) override;\n@@ -59,7 +59,7 @@ class TritonBackend : public GpuCodegenBackend {\n       std::unique_ptr<HloModule> hlo_module,\n       const Compiler::CompileOptions& options) override;\n \n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "3fa156ab5a25eca91f31ded32d66fd0183e4c883",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -27,7 +27,6 @@ limitations under the License.\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n@@ -79,8 +78,7 @@ class TritonBackendTest : public HloHardwareIndependentTestBase {\n                              ->ExecutorForDevice(0)\n                              .value()),\n         target_config_(stream_executor_),\n-        backend_(&debug_options_, &compiler_, &target_config_,\n-                 &symbolic_expr_context_) {\n+        backend_(&debug_options_, &compiler_, &target_config_, &mlir_context_) {\n     // TODO(b/315957220): Remove the experimental flags once TMA is enabled by\n     // default.\n     debug_options_.set_xla_gpu_experimental_enable_triton_tma(true);\n@@ -92,7 +90,6 @@ class TritonBackendTest : public HloHardwareIndependentTestBase {\n   Compiler::GpuTargetConfig target_config_;\n   TritonBackend backend_;\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(TritonBackendTest, GetSupportedConfigs) {"
        },
        {
            "sha": "22a2e1d159b29dfb753cd853440f973b77584cbc",
            "filename": "third_party/xla/xla/backends/gpu/codegen/custom.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -935,9 +935,7 @@ absl::StatusOr<FusionEmissionResult> EmitCustomCall(\n             : custom_call.raw_backend_config_string();\n     if (!backend_config_str.empty()) {\n       mlir::Attribute attr = mlir::parseAttribute(\n-          backend_config_str,\n-          // TODO: b/451959933 - Use reference or check pointer.\n-          ir_emitter_context.expr_context()->GetMLIRContext());\n+          backend_config_str, ir_emitter_context.mlir_context());\n       auto dict = mlir::dyn_cast_or_null<mlir::DictionaryAttr>(attr);\n       if (dict == nullptr) {\n         return absl::InternalError("
        },
        {
            "sha": "5490797ffd64c669de821d6c3e313018ca3e73ca",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/concatenate.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -43,6 +43,8 @@ limitations under the License.\n namespace xla {\n namespace gpu {\n \n+using ::mlir::MLIRContext;\n+\n using KernelEmitter = emitters::ConcatenateFusionKernelEmitter;\n \n ConcatenateFusion::ConcatenateFusion(const HloFusionAnalysis& analysis)\n@@ -59,13 +61,13 @@ LaunchDimensions ConcatenateFusion::launch_dimensions() const {\n }\n \n std::optional<IndexingMap> ConcatenateFusion::ComputeThreadIdToOutputIndexing(\n-    int64_t root_index, SymbolicExprContext* ctx) const {\n+    int64_t root_index, MLIRContext* ctx) const {\n   return std::nullopt;\n }\n \n std::optional<std::vector<IndexingMap>>\n-ConcatenateFusion::ComputeThreadIdToInputIndexing(\n-    int64_t root_index, SymbolicExprContext* ctx) const {\n+ConcatenateFusion::ComputeThreadIdToInputIndexing(int64_t root_index,\n+                                                  MLIRContext* ctx) const {\n   IndexingMap map_for_largest_shape =\n       KernelEmitter::ComputeWorkItemIdToOutputIndexing(GetWorkDimensions(),\n                                                        largest_shape_, ctx);\n@@ -81,9 +83,8 @@ ConcatenateFusion::CreateMLIRModule(\n     mlir::MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n     const std::string& entry_function_name,\n     const BufferAssignment* buffer_assignment) const {\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n   emitters::ConcatenateFusionKernelEmitter emitter(\n-      symbolic_expr_context, fusion, analysis_.fusion_spec(), buffer_assignment,\n+      mlir_context, fusion, analysis_.fusion_spec(), buffer_assignment,\n       GetDefaultBufferAlignment(), GetWorkDimensions(), entry_function_name,\n       BackendKind::kGpu);\n "
        },
        {
            "sha": "673dbb8e50a32762c95d5e5b2440bc6e75e7a027",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/concatenate.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -47,10 +47,10 @@ class ConcatenateFusion final : public EmitterBase {\n   LaunchDimensions launch_dimensions() const override;\n \n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t root_index, SymbolicExprContext* ctx) const override;\n+      int64_t root_index, mlir::MLIRContext* ctx) const override;\n \n   std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n-      int64_t root_index, SymbolicExprContext* ctx) const override;\n+      int64_t root_index, mlir::MLIRContext* ctx) const override;\n \n  protected:\n   absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateMLIRModule("
        },
        {
            "sha": "6a4e46f25bfb4fcd70de9ab365210a77a465598d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 15,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -368,17 +368,15 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitterBase::CreateMLIRModule(\n                           GetDefaultBufferAlignment(), entry_function_name));\n   SetBackendKind(&mlir_context, entry_func, BackendKind::kGpu);\n \n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n-  TF_RETURN_IF_ERROR(\n-      EmitMlir(module.get(), entry_func, fusion, symbolic_expr_context));\n+  TF_RETURN_IF_ERROR(EmitMlir(module.get(), entry_func, fusion, mlir_context));\n   return module;\n }\n \n emitters::EpilogueSpecification EmitterBase::GetEpilogueForOutputIndexing(\n     const HloFusionAnalysis& analysis,\n     const std::vector<const HloInstruction*>& heroes,\n     const std::vector<const HloInstruction*>& roots,\n-    SymbolicExprContext* symbolic_expr_context) const {\n+    MLIRContext* mlir_context) const {\n   emitters::EpilogueSpecification result;\n \n   absl::flat_hash_map<const HloInstruction*, const HloInstruction*>\n@@ -394,8 +392,8 @@ emitters::EpilogueSpecification EmitterBase::GetEpilogueForOutputIndexing(\n \n   result.root_indexing.reserve(roots.size());\n   for (auto* root : roots) {\n-    auto indexing = ComputeThreadIdToOutputIndexing(root_to_index[root],\n-                                                    symbolic_expr_context);\n+    auto indexing =\n+        ComputeThreadIdToOutputIndexing(root_to_index[root], mlir_context);\n     if (result.index_ranges.empty()) {\n       result.index_ranges.reserve(indexing->GetDimensionCount() +\n                                   indexing->GetSymbolCount());\n@@ -408,8 +406,7 @@ emitters::EpilogueSpecification EmitterBase::GetEpilogueForOutputIndexing(\n     }\n     auto* hero = root_to_hero[root];\n     auto epilogue_indexing = ComputeEpilogueInputToOutputIndexing(\n-        {*hero, &analysis.fusion()}, {*root, &analysis.fusion()},\n-        symbolic_expr_context);\n+        {*hero, &analysis.fusion()}, {*root, &analysis.fusion()}, mlir_context);\n     result.root_indexing.push_back(\n         ComposeIndexingMaps(*indexing, epilogue_indexing));\n   }\n@@ -436,15 +433,13 @@ mlir::DialectRegistry EmitterBase::GetDialectRegistry() {\n   return registry;\n }\n \n-absl::Status EmitterBase::EmitMlir(\n-    mlir::ModuleOp module, FuncOp entry_function,\n-    const HloFusionInstruction& fusion,\n-    SymbolicExprContext& symbolic_expr_context) const {\n+absl::Status EmitterBase::EmitMlir(mlir::ModuleOp module, FuncOp entry_function,\n+                                   const HloFusionInstruction& fusion,\n+                                   MLIRContext& mlir_context) const {\n   std::vector<emitters::EpilogueSpecification> epilogues =\n-      GetEpilogues(fusion, &symbolic_expr_context);\n+      GetEpilogues(fusion, &mlir_context);\n   emitters::PartitionedComputations computations(\n-      fusion.fused_instructions_computation(), &symbolic_expr_context,\n-      epilogues);\n+      fusion.fused_instructions_computation(), &mlir_context, epilogues);\n \n   TF_ASSIGN_OR_RETURN(auto call_targets, emitters::EmitPartitionedComputations(\n                                              module, computations));"
        },
        {
            "sha": "bb0adddf675d1db4518a3c8aecbb1293c6686723",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -79,7 +79,7 @@ class EmitterBase : public KernelFusionInterface {\n   // functions for these instructions.\n   virtual std::vector<emitters::EpilogueSpecification> GetEpilogues(\n       const HloFusionInstruction& fusion,\n-      SymbolicExprContext* symbolic_expr_context) const {\n+      mlir::MLIRContext* mlir_context) const {\n     return {};\n   }\n \n@@ -89,7 +89,7 @@ class EmitterBase : public KernelFusionInterface {\n       const HloFusionAnalysis& analysis,\n       const std::vector<const HloInstruction*>& heroes,\n       const std::vector<const HloInstruction*>& roots,\n-      SymbolicExprContext* symbolic_expr_context) const;\n+      mlir::MLIRContext* mlir_context) const;\n \n   virtual absl::Status EmitEntryFunction(\n       const emitters::PartitionedComputations& computations,\n@@ -111,7 +111,7 @@ class EmitterBase : public KernelFusionInterface {\n   absl::Status EmitMlir(mlir::ModuleOp module,\n                         mlir::func::FuncOp entry_function,\n                         const HloFusionInstruction& fusion,\n-                        SymbolicExprContext& symbolic_expr_context) const;\n+                        mlir::MLIRContext& mlir_context) const;\n };\n \n // Adds passes that transform XLA_GPU and SCF loops, e.g. peel, pipeline,"
        },
        {
            "sha": "90cf3ae25666207d794d9c0f8c88b1a89e9784cf",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -57,12 +57,12 @@ class DummyCopyEmitter : public EmitterBase {\n   LaunchDimensions launch_dimensions() const final { return {1, 100}; }\n \n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t, SymbolicExprContext*) const final {\n+      int64_t, mlir::MLIRContext*) const final {\n     return std::nullopt;\n   }\n \n   std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n-      int64_t, SymbolicExprContext*) const final {\n+      int64_t, mlir::MLIRContext*) const final {\n     return std::nullopt;\n   }\n "
        },
        {
            "sha": "4eb2b55f210f884667f133e764e596f9b745181a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/in_place_dynamic_update_slice.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -46,6 +46,8 @@ namespace xla {\n namespace gpu {\n namespace {\n \n+using ::mlir::MLIRContext;\n+\n constexpr int kDUSUpdateIndex = 1;\n \n }  // namespace\n@@ -59,7 +61,7 @@ LaunchDimensions InPlaceDynamicUpdateSliceFusion::launch_dimensions() const {\n \n std::optional<std::vector<IndexingMap>>\n InPlaceDynamicUpdateSliceFusion::ComputeThreadIdToInputIndexing(\n-    int64_t root_index, SymbolicExprContext* symbolic_expr_context) const {\n+    int64_t root_index, MLIRContext* mlir_context) const {\n   // TODO(b/331355203): Implement thread ID -> operand indexing.\n   std::vector<IndexingMap> result(\n       analysis_.fusion_hero(root_index).GetOperands().size(),\n@@ -68,22 +70,20 @@ InPlaceDynamicUpdateSliceFusion::ComputeThreadIdToInputIndexing(\n   using KernelEmitter = emitters::DynamicUpdateSliceKernelEmitter;\n   result[kDUSUpdateIndex] = KernelEmitter::ComputeWorkItemIdToOutputIndexing(\n       GetWorkDimensions(),\n-      KernelEmitter::GetIndexingShape(analysis_.fusion_spec()),\n-      symbolic_expr_context);\n+      KernelEmitter::GetIndexingShape(analysis_.fusion_spec()), mlir_context);\n   return result;\n }\n \n std::vector<emitters::EpilogueSpecification>\n InPlaceDynamicUpdateSliceFusion::GetEpilogues(\n-    const HloFusionInstruction& fusion,\n-    SymbolicExprContext* symbolic_expr_context) const {\n+    const HloFusionInstruction& fusion, MLIRContext* mlir_context) const {\n   // We don't actually support epilogues for DUS, but this is how we tell\n   // the base class that we don't want it to generate code for the DUS.\n   std::vector<emitters::EpilogueSpecification> epilogues;\n   for (const auto& [dus_op, root] :\n        llvm::zip(dus_ops_, analysis_.fusion_roots())) {\n     epilogues.push_back(emitters::EpilogueSpecification::FromIdentityIndexing(\n-        &dus_op.instruction(), &root.instruction(), symbolic_expr_context));\n+        &dus_op.instruction(), &root.instruction(), mlir_context));\n   }\n   return epilogues;\n }\n@@ -99,9 +99,8 @@ InPlaceDynamicUpdateSliceFusion::CreateMLIRModule(\n     mlir::MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n     const std::string& entry_function_name,\n     const BufferAssignment* buffer_assignment) const {\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n   emitters::DynamicUpdateSliceKernelEmitter emitter(\n-      symbolic_expr_context, fusion, analysis_.fusion_spec(), buffer_assignment,\n+      mlir_context, fusion, analysis_.fusion_spec(), buffer_assignment,\n       GetDefaultBufferAlignment(), GetWorkDimensions(), entry_function_name,\n       BackendKind::kGpu);\n "
        },
        {
            "sha": "2b96e3f1fe54ab9df66c7899bc15b1ac741a17e7",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/in_place_dynamic_update_slice.h",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -56,16 +56,14 @@ class InPlaceDynamicUpdateSliceFusion : public EmitterBase {\n   LaunchDimensions launch_dimensions() const override;\n \n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t root_index,\n-      SymbolicExprContext* symbolic_expr_context) const override {\n+      int64_t root_index, mlir::MLIRContext* mlir_context) const override {\n     // The mapping cannot be statically computed in general, since the offsets\n     // are unknown.\n     return std::nullopt;\n   }\n \n   std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n-      int64_t root_index,\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      int64_t root_index, mlir::MLIRContext* mlir_context) const override;\n \n  protected:\n   absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateMLIRModule(\n@@ -81,7 +79,7 @@ class InPlaceDynamicUpdateSliceFusion : public EmitterBase {\n \n   std::vector<emitters::EpilogueSpecification> GetEpilogues(\n       const HloFusionInstruction& fusion,\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      mlir::MLIRContext* mlir_context) const override;\n \n   WorkDimensions GetWorkDimensions() const;\n "
        },
        {
            "sha": "8a9a546d705fb7564f9fcbbaeea602f4fe9afb66",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/loop.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -49,32 +49,33 @@ namespace xla {\n namespace gpu {\n namespace {\n \n+using ::mlir::MLIRContext;\n+\n const Shape& GetIndexShape(const Shape& shape) {\n   return shape.IsTuple() ? shape.tuple_shapes(0) : shape;\n }\n \n }  // namespace\n \n std::optional<IndexingMap> LoopFusion::ComputeThreadIdToOutputIndexing(\n-    int64_t root_index, SymbolicExprContext* symbolic_expr_context) const {\n+    int64_t root_index, MLIRContext* mlir_context) const {\n   return emitters::LoopFusionKernelEmitter::ComputeWorkItemIdToOutputIndexing(\n       GetWorkDimensions(),\n-      GetIndexShape(analysis_.fusion_root(root_index).shape()),\n-      symbolic_expr_context);\n+      GetIndexShape(analysis_.fusion_root(root_index).shape()), mlir_context);\n }\n \n std::optional<std::vector<IndexingMap>>\n-LoopFusion::ComputeThreadIdToInputIndexing(\n-    int64_t root_index, SymbolicExprContext* symbolic_expr_context) const {\n+LoopFusion::ComputeThreadIdToInputIndexing(int64_t root_index,\n+                                           MLIRContext* mlir_context) const {\n   std::optional<IndexingMap> thread_id_to_output_indexing =\n-      ComputeThreadIdToOutputIndexing(root_index, symbolic_expr_context);\n+      ComputeThreadIdToOutputIndexing(root_index, mlir_context);\n   if (!thread_id_to_output_indexing.has_value()) {\n     return std::nullopt;\n   }\n   const HloInstruction* fusion_root =\n       &analysis_.fusion_root(root_index).instruction();\n-  auto output_to_input_indexing = ComputeOutputToInputIndexing(\n-      fusion_root, /*output_id=*/0, symbolic_expr_context);\n+  auto output_to_input_indexing =\n+      ComputeOutputToInputIndexing(fusion_root, /*output_id=*/0, mlir_context);\n   std::vector<IndexingMap> result;\n   result.reserve(fusion_root->operand_count());\n   for (int64_t operand_index = 0; operand_index < fusion_root->operand_count();\n@@ -110,9 +111,8 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> LoopFusion::CreateMLIRModule(\n     mlir::MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n     const std::string& entry_function_name,\n     const BufferAssignment* buffer_assignment) const {\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n   emitters::LoopFusionKernelEmitter emitter(\n-      symbolic_expr_context, fusion, analysis_.fusion_spec(), buffer_assignment,\n+      mlir_context, fusion, analysis_.fusion_spec(), buffer_assignment,\n       GetDefaultBufferAlignment(), GetWorkDimensions(), entry_function_name,\n       BackendKind::kGpu);\n "
        },
        {
            "sha": "d71111d9f1329bbc89ab72780eadf39c3171a7e7",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/loop.h",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -37,18 +37,15 @@ namespace gpu {\n // Generic loop fusion. Lowers to LLVM via MLIR.\n class LoopFusion final : public EmitterBase {\n  public:\n-  LoopFusion(const HloFusionAnalysis& analysis,\n-             SymbolicExprContext* symbolic_expr_context)\n+  LoopFusion(const HloFusionAnalysis& analysis, mlir::MLIRContext* mlir_context)\n       : analysis_(analysis), config_(ComputeLoopFusionConfig(analysis)) {}\n   LaunchDimensions launch_dimensions() const override;\n \n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t root_index,\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      int64_t root_index, mlir::MLIRContext* mlir_context) const override;\n \n   std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n-      int64_t root_index,\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      int64_t root_index, mlir::MLIRContext* mlir_context) const override;\n \n  private:\n   absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateMLIRModule("
        },
        {
            "sha": "3e13a91f2ef798d5ec01225a0a2583583f276bad",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/reduction.cc",
            "status": "modified",
            "additions": 60,
            "deletions": 89,
            "changes": 149,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -101,7 +101,7 @@ struct ReductionFusion::EmitterState {\n                const HloFusionInstruction& fusion,\n                const PartitionedComputations& computations,\n                const emitters::CallTargetProvider& call_target,\n-               SymbolicExprContext* symbolic_expr_context)\n+               MLIRContext* mlir_context)\n       : owner(owner),\n         entry_function(entry_function),\n         fusion(fusion),\n@@ -110,7 +110,7 @@ struct ReductionFusion::EmitterState {\n         builder(entry_function.getLoc(), entry_function),\n         computation(computations.FindPartitionedComputation(\n             fusion.fused_instructions_computation())),\n-        symbolic_expr_context(symbolic_expr_context) {\n+        mlir_context(mlir_context) {\n     int output_index = 0;\n     for (const auto& [root_index, root] :\n          llvm::enumerate(owner.analysis_.fusion_roots())) {\n@@ -176,13 +176,12 @@ struct ReductionFusion::EmitterState {\n   absl::flat_hash_map<const HloInstruction*, int> fusion_result_index_starts;\n   absl::flat_hash_map<const HloInstruction*, int> root_indices;\n   SmallVector<Value> thread_and_block_ids;\n-  SymbolicExprContext* symbolic_expr_context;\n+  MLIRContext* mlir_context;\n };\n \n PerThreadOutputs ReductionFusion::EmitterState::EmitPerThreadElements(\n     int group_id, const HloValueMap& inits, const SmallVector<Value>& outputs) {\n-  auto tile_indexing =\n-      owner.ComputeReductionInputIndexing(symbolic_expr_context);\n+  auto tile_indexing = owner.ComputeReductionInputIndexing(mlir_context);\n   tile_indexing\n       .GetMutableDimensionBound(\n           KernelFusionInterface::kIndexingMapBlockIdxDims[1])\n@@ -210,7 +209,7 @@ PerThreadOutputs ReductionFusion::EmitterState::EmitPerThreadElements(\n       SmallVector<Value> reduce_args = iter_args.slice(start, arity);\n       auto indices = emitters::ApplyIndexing(\n           GetBitcastMap(owner.input_shape_, reduction->operand(0)->shape(),\n-                        symbolic_expr_context),\n+                        mlir_context),\n           map_results, {}, nested_b);\n       reduce_args.append(ProvideParameterRange(computation, reduction, 0, arity,\n                                                indices, call_target,\n@@ -234,8 +233,7 @@ PerThreadOutputs ReductionFusion::EmitterState::EmitPerThreadElements(\n     llvm::SmallVector<SideOutput> side_output_values;\n     for (auto* side_output : side_outputs) {\n       auto indices = emitters::ApplyIndexing(\n-          GetBitcastMap(owner.input_shape_, side_output->shape(),\n-                        symbolic_expr_context),\n+          GetBitcastMap(owner.input_shape_, side_output->shape(), mlir_context),\n           map_results, {}, builder);\n       auto* root_tuple = fusion.fused_expression_root();\n       Value value = emitters::ProvideParameter(\n@@ -271,7 +269,7 @@ SmallVector<Value> ReductionFusion::EmitterState::WriteToSharedMemory(\n     absl::Span<const HloInstruction* const> reductions,\n     const HloValueMap& values, std::optional<int> padding) {\n   SmallVector<int64_t> shape;\n-  auto map = owner.GetSharedMemoryWriteMap(symbolic_expr_context);\n+  auto map = owner.GetSharedMemoryWriteMap(mlir_context);\n   for (auto result : map.GetAffineMap().getResults()) {\n     shape.push_back(\n         map.GetRangeEvaluator().ComputeExpressionRange(result).upper + 1);\n@@ -338,8 +336,7 @@ mlir::ValueRange ReductionFusion::EmitterState::ReduceViaSharedMemory(\n     int group_id, const PerThreadOutputs& per_thread, const HloValueMap& inits,\n     std::optional<int> padding, int max_dist) {\n   const auto& reductions = owner.reduction_heroes_[group_id];\n-  auto read_indexing =\n-      owner.GetSharedMemoryReductionReadMap(symbolic_expr_context);\n+  auto read_indexing = owner.GetSharedMemoryReductionReadMap(mlir_context);\n   auto loop_indexing = read_indexing;\n   // All threads must participate in the shuffle, so we clear the constraints\n   // for the iteration. Otherwise, some threads might not be part of the loop,\n@@ -381,8 +378,8 @@ mlir::ValueRange ReductionFusion::EmitterState::ReduceViaSharedMemory(\n }\n \n ReductionFusion::ReductionFusion(const HloFusionAnalysis& analysis,\n-                                 SymbolicExprContext* symbolic_expr_context)\n-    : analysis_(analysis), symbolic_expr_context_(symbolic_expr_context) {\n+                                 MLIRContext* mlir_context)\n+    : analysis_(analysis), mlir_context_(mlir_context) {\n   auto* hero_reduction = analysis.FindHeroReduction();\n   CHECK_NE(hero_reduction, nullptr);\n   reduction_dimensions_ =\n@@ -456,21 +453,20 @@ LaunchDimensions ReductionFusion::launch_dimensions() const {\n }\n \n std::vector<emitters::EpilogueSpecification> ReductionFusion::GetEpilogues(\n-    const HloFusionInstruction& fusion,\n-    SymbolicExprContext* symbolic_expr_context) const {\n+    const HloFusionInstruction& fusion, MLIRContext* mlir_context) const {\n   std::vector<emitters::EpilogueSpecification> epilogues;\n   epilogues.reserve(reduction_heroes_.size());\n   for (const auto& [heroes, roots] :\n        llvm::zip(reduction_heroes_, reduction_roots_)) {\n-    epilogues.push_back(GetEpilogueForOutputIndexing(analysis_, heroes, roots,\n-                                                     symbolic_expr_context));\n+    epilogues.push_back(\n+        GetEpilogueForOutputIndexing(analysis_, heroes, roots, mlir_context));\n   }\n   // Add empty epilogues for the side outputs. This ensures their roots don't\n   // get \"fused\" into the tuple function.\n   for (const auto& roots : side_output_roots_) {\n     for (const auto* root : roots) {\n       epilogues.push_back(emitters::EpilogueSpecification::FromIdentityIndexing(\n-          root, root, symbolic_expr_context));\n+          root, root, mlir_context));\n     }\n   }\n   return epilogues;\n@@ -482,7 +478,7 @@ absl::Status ReductionFusion::EmitEntryFunction(\n     mlir::func::FuncOp entry_function,\n     const HloFusionInstruction& fusion) const {\n   EmitterState state{*this,        entry_function, fusion,\n-                     computations, call_targets,   symbolic_expr_context_};\n+                     computations, call_targets,   mlir_context_};\n   auto& b = state.builder;\n   b.setInsertionPointToStart(entry_function.addEntryBlock());\n   state.thread_and_block_ids = EmitThreadAndBlockIds(b);\n@@ -516,13 +512,13 @@ HloValueMap ReductionFusion::GetInits(int group_id, EmitterState& state) const {\n \n std::optional<std::vector<IndexingMap>>\n ReductionFusion::ComputeThreadIdToInputIndexing(\n-    int64_t root_index, SymbolicExprContext* symbolic_expr_context) const {\n+    int64_t root_index, MLIRContext* mlir_context) const {\n   const auto& hero = analysis_.fusion_hero(root_index).instruction();\n   std::vector<IndexingMap> result(hero.operand_count(),\n                                   IndexingMap::GetUndefined());\n   if (!groups_.is_reduction_root[root_index]) {\n     auto thread_id_to_output_indexing =\n-        ComputeThreadIdToOutputIndexing(root_index, symbolic_expr_context);\n+        ComputeThreadIdToOutputIndexing(root_index, mlir_context);\n     if (!thread_id_to_output_indexing.has_value()) {\n       return std::nullopt;\n     }\n@@ -531,8 +527,7 @@ ReductionFusion::ComputeThreadIdToInputIndexing(\n       result[operand_index] = ComposeIndexingMaps(\n           *thread_id_to_output_indexing,\n           ComputeOutputToInputIndexing(\n-              &analysis_.fusion_root(root_index).instruction(), 0,\n-              symbolic_expr_context)\n+              &analysis_.fusion_root(root_index).instruction(), 0, mlir_context)\n               .indexing_maps[operand_index]\n               .begin()\n               ->map());\n@@ -543,31 +538,30 @@ ReductionFusion::ComputeThreadIdToInputIndexing(\n   // We don't have indexing for the init values.\n   for (int64_t operand_index = 0; operand_index < hero.operand_count() / 2;\n        ++operand_index) {\n-    auto projected_map = ComputeReductionInputIndexing(symbolic_expr_context);\n+    auto projected_map = ComputeReductionInputIndexing(mlir_context);\n     AddGroupIdConstraint(projected_map, root_index, groups_);\n     result[operand_index] =\n         projected_map * GetBitcastMap(input_shape_,\n                                       hero.operand(operand_index)->shape(),\n-                                      symbolic_expr_context);\n+                                      mlir_context);\n     result[operand_index].Simplify();\n   }\n   return result;\n }\n \n std::optional<IndexingMap> ReductionFusion::ComputeThreadIdToOutputIndexing(\n-    int64_t root_index, SymbolicExprContext* symbolic_expr_context) const {\n+    int64_t root_index, MLIRContext* mlir_context) const {\n   if (!groups_.is_reduction_root[root_index]) {\n     auto map = ComposeIndexingMaps(\n-        ComputeReductionInputIndexing(symbolic_expr_context),\n+        ComputeReductionInputIndexing(mlir_context),\n         GetBitcastMap(input_shape_, analysis_.fusion_root(root_index).shape(),\n-                      symbolic_expr_context));\n+                      mlir_context));\n     AddGroupIdConstraint(map, root_index, groups_);\n     map.Simplify();\n     return map;\n   }\n \n-  auto projected_indexing =\n-      ComputeReductionOutputIndexing(symbolic_expr_context);\n+  auto projected_indexing = ComputeReductionOutputIndexing(mlir_context);\n   auto output_shape = reduction_dimensions_.GetOutputShape();\n   CHECK_EQ(output_shape.size(),\n            projected_indexing.GetAffineMap().getNumResults());\n@@ -581,7 +575,7 @@ std::optional<IndexingMap> ReductionFusion::ComputeThreadIdToOutputIndexing(\n   auto physical_shape =\n       ShapeUtil::DeleteDimensions(hero.dimensions(), hero.operand(0)->shape());\n   auto map = projected_indexing *\n-             GetBitcastMap(output_shape, physical_shape, symbolic_expr_context);\n+             GetBitcastMap(output_shape, physical_shape, mlir_context);\n   map.Simplify();\n   return map;\n }\n@@ -599,10 +593,9 @@ SmallVector<Value> ReductionFusion::EvaluateEpilogue(\n   auto values = EmitEpilogue(group_id, state.computations, state.entry_function,\n                              results, epilogue_input_indices, b);\n   int first_root_index = state.root_indices[epilogue.roots.front()];\n-  auto thread_has_output =\n-      emitters::CheckConstraints(*ComputeThreadIdToOutputIndexing(\n-                                     first_root_index, symbolic_expr_context_),\n-                                 state.thread_and_block_ids, symbol_values, b);\n+  auto thread_has_output = emitters::CheckConstraints(\n+      *ComputeThreadIdToOutputIndexing(first_root_index, mlir_context_),\n+      state.thread_and_block_ids, symbol_values, b);\n   for (auto [index, root] : llvm::enumerate(epilogue.roots)) {\n     auto output_indices =\n         emitters::ApplyIndexing(epilogue.root_indexing[index],\n@@ -616,10 +609,9 @@ SmallVector<Value> ReductionFusion::EvaluateEpilogue(\n   return outputs;\n }\n \n-ColumnReductionFusion::ColumnReductionFusion(\n-    const HloFusionAnalysis& analysis,\n-    SymbolicExprContext* symbolic_expr_context)\n-    : ReductionFusion(analysis, symbolic_expr_context) {\n+ColumnReductionFusion::ColumnReductionFusion(const HloFusionAnalysis& analysis,\n+                                             MLIRContext* mlir_context)\n+    : ReductionFusion(analysis, mlir_context) {\n   CHECK(!reduction_dimensions_.is_row_reduction);\n \n   input_shape_ = {reduction_dimensions_.dimensions[0],\n@@ -647,8 +639,7 @@ ColumnReductionFusion::ColumnReductionFusion(\n }\n \n IndexingMap ColumnReductionFusion::ComputeReductionOutputIndexing(\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) const {\n   auto thread_id =\n       DelinearizeInBoundsIndex(getAffineDimExpr(0, mlir_context), num_threads_);\n   auto block_id =\n@@ -664,8 +655,7 @@ IndexingMap ColumnReductionFusion::ComputeReductionOutputIndexing(\n }\n \n IndexingMap ColumnReductionFusion::ComputeReductionInputIndexing(\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) const {\n   auto thread_id =\n       DelinearizeInBoundsIndex(getAffineDimExpr(0, mlir_context), num_threads_);\n   auto block_id =\n@@ -685,8 +675,7 @@ IndexingMap ColumnReductionFusion::ComputeReductionInputIndexing(\n }\n \n IndexingMap ColumnReductionFusion::GetSharedMemoryReductionReadMap(\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) const {\n   auto thread_id =\n       DelinearizeInBoundsIndex(getAffineDimExpr(0, mlir_context), num_threads_);\n   auto vector_index = getAffineSymbolExpr(0, mlir_context);\n@@ -696,8 +685,7 @@ IndexingMap ColumnReductionFusion::GetSharedMemoryReductionReadMap(\n }\n \n IndexingMap ColumnReductionFusion::GetSharedMemoryWriteMap(\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) const {\n   auto thread_id =\n       DelinearizeInBoundsIndex(getAffineDimExpr(0, mlir_context), num_threads_);\n   auto vector_index = getAffineSymbolExpr(0, mlir_context);\n@@ -716,9 +704,8 @@ llvm::SmallVector<mlir::Value> ColumnReductionFusion::EmitReduction(\n }\n \n SmallColumnReductionFusion::SmallColumnReductionFusion(\n-    const HloFusionAnalysis& analysis,\n-    SymbolicExprContext* symbolic_expr_context)\n-    : ReductionFusion(analysis, symbolic_expr_context) {\n+    const HloFusionAnalysis& analysis, MLIRContext* mlir_context)\n+    : ReductionFusion(analysis, mlir_context) {\n   CHECK(!reduction_dimensions_.is_row_reduction);\n \n   input_shape_ = {reduction_dimensions_.dimensions[0],\n@@ -749,8 +736,7 @@ SmallColumnReductionFusion::SmallColumnReductionFusion(\n }\n \n IndexingMap SmallColumnReductionFusion::ComputeReductionOutputIndexing(\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) const {\n   auto thread_id = getAffineDimExpr(0, mlir_context);\n   auto block_id = getAffineDimExpr(3, mlir_context);\n   auto vector_index = getAffineSymbolExpr(0, mlir_context);\n@@ -764,8 +750,7 @@ IndexingMap SmallColumnReductionFusion::ComputeReductionOutputIndexing(\n }\n \n IndexingMap SmallColumnReductionFusion::ComputeReductionInputIndexing(\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) const {\n   auto thread_id = getAffineDimExpr(0, mlir_context);\n   auto block_id = getAffineDimExpr(3, mlir_context);\n   AffineExpr loop_index = getAffineSymbolExpr(0, mlir_context);\n@@ -778,7 +763,7 @@ IndexingMap SmallColumnReductionFusion::ComputeReductionInputIndexing(\n       GetBitcastMap({num_blocks_[0], input_shape_[1] * input_shape_[2]},\n                     ShapeUtil::MakeShapeWithDescendingLayout(PrimitiveType::U8,\n                                                              input_shape_),\n-                    symbolic_expr_context);\n+                    mlir_context);\n \n   for (auto [result, dim_size] :\n        llvm::zip(map.GetAffineMap().getResults(), input_shape_)) {\n@@ -788,8 +773,7 @@ IndexingMap SmallColumnReductionFusion::ComputeReductionInputIndexing(\n }\n \n IndexingMap SmallColumnReductionFusion::GetSharedMemoryReductionReadMap(\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) const {\n   auto indices = DelinearizeInBoundsIndex(\n       getAffineDimExpr(0, mlir_context) +\n           getAffineSymbolExpr(0, mlir_context) * num_threads_[0],\n@@ -799,8 +783,7 @@ IndexingMap SmallColumnReductionFusion::GetSharedMemoryReductionReadMap(\n }\n \n IndexingMap SmallColumnReductionFusion::GetSharedMemoryWriteMap(\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) const {\n   auto indices = DelinearizeInBoundsIndex(\n       getAffineDimExpr(0, mlir_context) * vector_size_ +\n           getAffineSymbolExpr(0, mlir_context),\n@@ -825,10 +808,9 @@ llvm::SmallVector<mlir::Value> SmallColumnReductionFusion::EmitReduction(\n                                      shared_rows_ / 2);\n }\n \n-RowReductionFusion::RowReductionFusion(\n-    const HloFusionAnalysis& analysis,\n-    SymbolicExprContext* symbolic_expr_context)\n-    : ReductionFusion(analysis, symbolic_expr_context) {\n+RowReductionFusion::RowReductionFusion(const HloFusionAnalysis& analysis,\n+                                       MLIRContext* mlir_context)\n+    : ReductionFusion(analysis, mlir_context) {\n   CHECK(reduction_dimensions_.is_row_reduction);\n   Vector3 shape = reduction_dimensions_.dimensions;\n   int64_t kMinorReducedElementsPerThread = 8;\n@@ -902,8 +884,7 @@ RowReductionFusion::RowReductionFusion(\n }\n \n IndexingMap RowReductionFusion::ComputeReductionInputIndexing(\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) const {\n   auto thread_id = DelinearizeInBoundsIndex(\n       mlir::getAffineDimExpr(0, mlir_context), num_threads_);\n   auto block_id = DelinearizeInBoundsIndex(\n@@ -928,8 +909,7 @@ IndexingMap RowReductionFusion::ComputeReductionInputIndexing(\n }\n \n IndexingMap RowReductionFusion::ComputeReductionOutputIndexing(\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) const {\n   auto thread_id = DelinearizeInBoundsIndex(\n       mlir::getAffineDimExpr(0, mlir_context), num_threads_);\n   auto block_id = DelinearizeInBoundsIndex(\n@@ -945,8 +925,7 @@ int RowReductionFusion::GetWarpsPerRow() const {\n }\n \n IndexingMap RowReductionFusion::GetSharedMemoryReductionReadMap(\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) const {\n   auto thread_id =\n       DelinearizeInBoundsIndex(getAffineDimExpr(0, mlir_context), num_threads_);\n   auto lane_id = thread_id[1] % WarpSize();\n@@ -955,8 +934,7 @@ IndexingMap RowReductionFusion::GetSharedMemoryReductionReadMap(\n }\n \n IndexingMap RowReductionFusion::GetSharedMemoryWriteMap(\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) const {\n   auto thread_id =\n       DelinearizeInBoundsIndex(getAffineDimExpr(0, mlir_context), num_threads_);\n   // The reduced dimension is tiled; each warp writes one element to shared\n@@ -989,8 +967,8 @@ llvm::SmallVector<mlir::Value> RowReductionFusion::EmitReduction(\n \n MultiRowReductionFusion::MultiRowReductionFusion(\n     const HloFusionAnalysis& analysis, int vector_size,\n-    SymbolicExprContext* symbolic_expr_context)\n-    : ReductionFusion(analysis, symbolic_expr_context) {\n+    MLIRContext* mlir_context)\n+    : ReductionFusion(analysis, mlir_context) {\n   CHECK(reduction_dimensions_.is_row_reduction);\n   Vector3 shape = reduction_dimensions_.dimensions;\n   input_shape_ = {shape[0], shape[1], shape[2]};\n@@ -1000,8 +978,7 @@ MultiRowReductionFusion::MultiRowReductionFusion(\n }\n \n std::unique_ptr<ReductionFusion> MultiRowReductionFusion::TryCreate(\n-    const HloFusionAnalysis& analysis,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    const HloFusionAnalysis& analysis, MLIRContext* mlir_context) {\n   auto* hero_reduction = analysis.FindHeroReduction();\n   CHECK_NE(hero_reduction, nullptr);\n   auto reduction_dimensions =\n@@ -1083,7 +1060,7 @@ std::unique_ptr<ReductionFusion> MultiRowReductionFusion::TryCreate(\n   VLOG(3) << \"MultiRowReductionFusion::TryCreate selected vector_size = \"\n           << vector_size;\n   return std::make_unique<MultiRowReductionFusion>(analysis, vector_size,\n-                                                   symbolic_expr_context);\n+                                                   mlir_context);\n }\n \n absl::InlinedVector<int64_t, 4> MultiRowReductionFusion::GetNumThreads(\n@@ -1113,8 +1090,7 @@ int64_t MultiRowReductionFusion::GetNumBlocks(\n }\n \n IndexingMap MultiRowReductionFusion::ComputeReductionInputIndexing(\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) const {\n   auto thread_id = DelinearizeInBoundsIndex(\n       mlir::getAffineDimExpr(0, mlir_context), num_threads_);\n   auto block_id = num_blocks_.front() == 1\n@@ -1135,8 +1111,7 @@ IndexingMap MultiRowReductionFusion::ComputeReductionInputIndexing(\n }\n \n IndexingMap MultiRowReductionFusion::ComputeReductionOutputIndexing(\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) const {\n   auto thread_id = DelinearizeInBoundsIndex(\n       mlir::getAffineDimExpr(0, mlir_context), num_threads_);\n   auto block_id = num_blocks_.front() == 1\n@@ -1164,29 +1139,25 @@ llvm::SmallVector<mlir::Value> MultiRowReductionFusion::EmitReduction(\n }\n \n std::unique_ptr<ReductionFusion> CreateReductionFusion(\n-    const HloFusionAnalysis& analysis,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    const HloFusionAnalysis& analysis, MLIRContext* mlir_context) {\n   auto* hero_reduction = analysis.FindHeroReduction();\n   CHECK_NE(hero_reduction, nullptr);\n   ReductionDimensions reduction_dimensions =\n       GetReductionKindAndContiguousComponents(*hero_reduction);\n   if (reduction_dimensions.is_row_reduction) {\n     auto multi_row_emitter =\n-        MultiRowReductionFusion::TryCreate(analysis, symbolic_expr_context);\n+        MultiRowReductionFusion::TryCreate(analysis, mlir_context);\n     if (multi_row_emitter != nullptr) {\n       return multi_row_emitter;\n     }\n-    return std::make_unique<RowReductionFusion>(analysis,\n-                                                symbolic_expr_context);\n+    return std::make_unique<RowReductionFusion>(analysis, mlir_context);\n   }\n \n   const int64_t warp_size = analysis.device_info().threads_per_warp();\n   if (warp_size % reduction_dimensions.dimensions[kColMinorKept] == 0) {\n-    return std::make_unique<SmallColumnReductionFusion>(analysis,\n-                                                        symbolic_expr_context);\n+    return std::make_unique<SmallColumnReductionFusion>(analysis, mlir_context);\n   }\n-  return std::make_unique<ColumnReductionFusion>(analysis,\n-                                                 symbolic_expr_context);\n+  return std::make_unique<ColumnReductionFusion>(analysis, mlir_context);\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "5841543ccf51487bb590559c7b40abdec94cc624",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/reduction.h",
            "status": "modified",
            "additions": 31,
            "deletions": 36,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -48,21 +48,19 @@ namespace gpu {\n using HloValueMap =\n     absl::flat_hash_map<const HloInstruction*, llvm::SmallVector<mlir::Value>>;\n \n-// Reduction fusion. Lowers to LLVM via MLIR. Currently not fully\n+// Reduction fusion. Lowers to LLVM viamlir::MLIR. Currently not fully\n // implemented: only single reduction groups, no side outputs, only row\n // reductions.\n class ReductionFusion : public EmitterBase {\n  public:\n   explicit ReductionFusion(const HloFusionAnalysis& analysis,\n-                           SymbolicExprContext* symbolic_expr_context);\n+                           mlir::MLIRContext* mlir_context);\n \n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t root_index,\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      int64_t root_index, mlir::MLIRContext* mlir_context) const override;\n \n   std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n-      int64_t root_index,\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      int64_t root_index, mlir::MLIRContext* mlir_context) const override;\n \n   LaunchDimensions launch_dimensions() const override;\n \n@@ -83,7 +81,7 @@ class ReductionFusion : public EmitterBase {\n \n   std::vector<emitters::EpilogueSpecification> GetEpilogues(\n       const HloFusionInstruction& fusion,\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      mlir::MLIRContext* mlir_context) const override;\n \n   llvm::SmallVector<mlir::Value> EvaluateEpilogue(\n       const HloValueMap& results, llvm::SmallVector<mlir::Value> outputs,\n@@ -104,23 +102,23 @@ class ReductionFusion : public EmitterBase {\n   // Returns the input indexing. The inputs are given in the projected shape\n   // (i.e., the indexing map has three results).\n   virtual IndexingMap ComputeReductionInputIndexing(\n-      SymbolicExprContext* symbolic_expr_context) const = 0;\n+      mlir::MLIRContext* mlir_context) const = 0;\n   // Returns the output indexing. The outputs are given in the  projected\n   // reduced shape (i.e., one or two results, depending on the reduction type).\n   virtual IndexingMap ComputeReductionOutputIndexing(\n-      SymbolicExprContext* symbolic_expr_context) const = 0;\n+      mlir::MLIRContext* mlir_context) const = 0;\n \n   // Returns the (thread ID, vector index) -> (shared index...) map for the\n   // shared memory reduction.\n   virtual IndexingMap GetSharedMemoryReductionReadMap(\n-      SymbolicExprContext* symbolic_expr_context) const {\n+      mlir::MLIRContext* mlir_context) const {\n     return IndexingMap::GetUndefined();\n   }\n \n   // Returns the (thread ID, vector index) -> (shared index...) map for the\n   // write to shared memory.\n   virtual IndexingMap GetSharedMemoryWriteMap(\n-      SymbolicExprContext* symbolic_expr_context) const {\n+      mlir::MLIRContext* mlir_context) const {\n     return IndexingMap::GetUndefined();\n   }\n \n@@ -135,7 +133,7 @@ class ReductionFusion : public EmitterBase {\n   // The side output roots for each reduction group.\n   std::vector<std::vector<const HloInstruction*>> side_output_roots_;\n   const HloFusionAnalysis& analysis_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n \n   // The number of elements in each dimension.\n   absl::InlinedVector<int64_t, 4> input_shape_;\n@@ -155,35 +153,34 @@ class ReductionFusion : public EmitterBase {\n class RowReductionFusion : public ReductionFusion {\n  public:\n   explicit RowReductionFusion(const HloFusionAnalysis& analysis,\n-                              SymbolicExprContext* symbolic_expr_context);\n+                              mlir::MLIRContext* mlir_context);\n \n  protected:\n   // The number of warps working on one output element.\n   int GetWarpsPerRow() const;\n   llvm::SmallVector<mlir::Value> EmitReduction(\n       int group_id, EmitterState& state) const override;\n   IndexingMap ComputeReductionInputIndexing(\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      mlir::MLIRContext* mlir_context) const override;\n   IndexingMap ComputeReductionOutputIndexing(\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      mlir::MLIRContext* mlir_context) const override;\n   IndexingMap GetSharedMemoryReductionReadMap(\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      mlir::MLIRContext* mlir_context) const override;\n   IndexingMap GetSharedMemoryWriteMap(\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      mlir::MLIRContext* mlir_context) const override;\n \n   absl::InlinedVector<int64_t, 4> tile_sizes_per_block_;\n };\n \n class MultiRowReductionFusion : public ReductionFusion {\n  public:\n   MultiRowReductionFusion(const HloFusionAnalysis& analysis, int vector_size,\n-                          SymbolicExprContext* symbolic_expr_context);\n+                          mlir::MLIRContext* mlir_context);\n \n   // Attempts to create a multi-row reduction emitter for the given analysis.\n   // Returns nullptr if the fusion is not supported.\n   static std::unique_ptr<ReductionFusion> TryCreate(\n-      const HloFusionAnalysis& analysis,\n-      SymbolicExprContext* symbolic_expr_context);\n+      const HloFusionAnalysis& analysis, mlir::MLIRContext* mlir_context);\n \n  protected:\n   // Returns the number of {kept, reduced} threads for the given reduction and\n@@ -197,27 +194,27 @@ class MultiRowReductionFusion : public ReductionFusion {\n   llvm::SmallVector<mlir::Value> EmitReduction(\n       int group_id, EmitterState& state) const override;\n   IndexingMap ComputeReductionInputIndexing(\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      mlir::MLIRContext* mlir_context) const override;\n   IndexingMap ComputeReductionOutputIndexing(\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      mlir::MLIRContext* mlir_context) const override;\n };\n \n class ColumnReductionFusion : public ReductionFusion {\n  public:\n   explicit ColumnReductionFusion(const HloFusionAnalysis& analysis,\n-                                 SymbolicExprContext* symbolic_expr_context);\n+                                 mlir::MLIRContext* mlir_context);\n \n  protected:\n   llvm::SmallVector<mlir::Value> EmitReduction(\n       int group_id, EmitterState& state) const override;\n   IndexingMap ComputeReductionInputIndexing(\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      mlir::MLIRContext* mlir_context) const override;\n   IndexingMap ComputeReductionOutputIndexing(\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      mlir::MLIRContext* mlir_context) const override;\n   IndexingMap GetSharedMemoryReductionReadMap(\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      mlir::MLIRContext* mlir_context) const override;\n   IndexingMap GetSharedMemoryWriteMap(\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      mlir::MLIRContext* mlir_context) const override;\n \n   const int64_t kTileSize = 32;\n };\n@@ -226,21 +223,20 @@ class ColumnReductionFusion : public ReductionFusion {\n // the warp size.\n class SmallColumnReductionFusion : public ReductionFusion {\n  public:\n-  explicit SmallColumnReductionFusion(\n-      const HloFusionAnalysis& analysis,\n-      SymbolicExprContext* symbolic_expr_context);\n+  explicit SmallColumnReductionFusion(const HloFusionAnalysis& analysis,\n+                                      mlir::MLIRContext* mlir_context);\n \n  protected:\n   llvm::SmallVector<mlir::Value> EmitReduction(\n       int group_id, EmitterState& state) const override;\n   IndexingMap ComputeReductionInputIndexing(\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      mlir::MLIRContext* mlir_context) const override;\n   IndexingMap ComputeReductionOutputIndexing(\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      mlir::MLIRContext* mlir_context) const override;\n   IndexingMap GetSharedMemoryReductionReadMap(\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      mlir::MLIRContext* mlir_context) const override;\n   IndexingMap GetSharedMemoryWriteMap(\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      mlir::MLIRContext* mlir_context) const override;\n \n   const int64_t kTileSize = 32;\n \n@@ -249,8 +245,7 @@ class SmallColumnReductionFusion : public ReductionFusion {\n };\n \n std::unique_ptr<ReductionFusion> CreateReductionFusion(\n-    const HloFusionAnalysis& analysis,\n-    SymbolicExprContext* symbolic_expr_context);\n+    const HloFusionAnalysis& analysis, mlir::MLIRContext* mlir_context);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "48630983536e6c3f5468c0b24e2b3b113e564a79",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/scatter.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 27,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -361,17 +361,16 @@ Value EmitterHelper::GetElement(ImplicitLocOpBuilder& b, int operand_index,\n \n ScatterFusion::ScatterFusion(const HloFusionAnalysis& analysis,\n                              const ScatterDescription& description,\n-                             int64_t vector_size,\n-                             SymbolicExprContext* symbolic_expr_context)\n+                             int64_t vector_size, MLIRContext* mlir_context)\n     : analysis_(analysis),\n       description_(description),\n-      symbolic_expr_context_(symbolic_expr_context),\n+      mlir_context_(mlir_context),\n       warp_size_(WarpSize(analysis_.device_info())),\n       vector_size_(vector_size) {}\n \n std::optional<std::vector<IndexingMap>>\n ScatterFusion::ComputeThreadIdToInputIndexing(int64_t root_index,\n-                                              SymbolicExprContext* ctx) const {\n+                                              MLIRContext* ctx) const {\n   CHECK(ScatterSimplifier::IsSimplifiedScatter(description_.scatter))\n       << \"Non-simplified HLO Scatter is not supported.\";\n \n@@ -396,19 +395,18 @@ ScatterFusion::ComputeThreadIdToInputIndexing(int64_t root_index,\n }\n \n std::vector<emitters::EpilogueSpecification> ScatterFusion::GetEpilogues(\n-    const HloFusionInstruction& fusion,\n-    SymbolicExprContext* symbolic_expr_context) const {\n+    const HloFusionInstruction& fusion, MLIRContext* mlir_context) const {\n   // We don't actually support epilogues for scatter, but this is how we tell\n   // the base class that we don't want it to generate code for the scatter.\n   return {emitters::EpilogueSpecification::FromIdentityIndexing(\n       &analysis_.fusion_hero(0).instruction(),\n-      &analysis_.fusion_root(0).instruction(), symbolic_expr_context)};\n+      &analysis_.fusion_root(0).instruction(), mlir_context)};\n }\n \n ScatterWithDistributedUpdates::ScatterWithDistributedUpdates(\n     const HloFusionAnalysis& analysis, const ScatterDescription& description,\n-    int64_t vector_size, SymbolicExprContext* symbolic_expr_context)\n-    : ScatterFusion(analysis, description, vector_size, symbolic_expr_context) {\n+    int64_t vector_size, MLIRContext* mlir_context)\n+    : ScatterFusion(analysis, description, vector_size, mlir_context) {\n   // We have to make sure that there is no thread that processes elements of\n   // two different update slice.\n   auto launch_dimensions = CalculateLaunchDimensions(\n@@ -421,23 +419,22 @@ ScatterWithDistributedUpdates::ScatterWithDistributedUpdates(\n }\n \n void ScatterWithDistributedUpdates::ComputeIndexing(\n-    SymbolicExprContext* symbolic_expr_context, IndexingMap* updates_map,\n+    MLIRContext* mlir_context, IndexingMap* updates_map,\n     IndexingMap* indices_map) const {\n   // Compute thread id mapping based on the first update operand.\n-  IndexingMap scatter_update_map = GetDefaultThreadIdIndexingMap(\n-      launch_dimensions(), vector_size_, description_.update_shape,\n-      symbolic_expr_context);\n+  IndexingMap scatter_update_map =\n+      GetDefaultThreadIdIndexingMap(launch_dimensions(), vector_size_,\n+                                    description_.update_shape, mlir_context);\n \n   // For scatter indices we project indexing for scatter updates and take the\n   // first result of the affine map only, because they coincide.\n   if (indices_map) {\n     // Create a map from scatter update to scatter indices.\n     *indices_map = IndexingMap{\n-        AffineMap::get(\n-            6, 1,\n-            {scatter_update_map.GetAffineMap().getResult(0),\n-             getAffineSymbolExpr(0, symbolic_expr_context->GetMLIRContext())},\n-            symbolic_expr_context->GetMLIRContext()),\n+        AffineMap::get(6, 1,\n+                       {scatter_update_map.GetAffineMap().getResult(0),\n+                        getAffineSymbolExpr(0, mlir_context)},\n+                       mlir_context),\n         DimVarsFromGPUGrid({num_warps_ * warp_size_, 1, 1, num_blocks_, 1, 1}),\n         RangeVarsFromTensorSizes({description_.index_vector_length}),\n         /*rt_vars=*/{}};\n@@ -463,7 +460,7 @@ absl::Status ScatterFusion::EmitEntryFunction(\n \n   IndexingMap updates_map = IndexingMap::GetUndefined();\n   IndexingMap indices_map = IndexingMap::GetUndefined();\n-  ComputeIndexing(symbolic_expr_context_, &updates_map, &indices_map);\n+  ComputeIndexing(mlir_context_, &updates_map, &indices_map);\n   updates_map.Simplify();\n \n   return EmitEntryFunctionImpl(b, helper, updates_map, indices_map,\n@@ -552,8 +549,8 @@ ScatterWithDistributedIndices::ScatterWithDistributedIndices(\n     const HloFusionAnalysis& analysis, const ScatterDescription& description,\n     int64_t vector_size, int64_t num_warps_per_slice,\n     int64_t num_indices_per_warp, int64_t indices_vector_size,\n-    SymbolicExprContext* symbolic_expr_context)\n-    : ScatterFusion(analysis, description, vector_size, symbolic_expr_context),\n+    MLIRContext* mlir_context)\n+    : ScatterFusion(analysis, description, vector_size, mlir_context),\n       num_warps_per_slice_(num_warps_per_slice),\n       num_indices_per_warp_(num_indices_per_warp),\n       indices_vector_size_(indices_vector_size) {\n@@ -563,9 +560,8 @@ ScatterWithDistributedIndices::ScatterWithDistributedIndices(\n }\n \n void ScatterWithDistributedIndices::ComputeIndexing(\n-    SymbolicExprContext* symbolic_expr_context, IndexingMap* updates_map,\n+    MLIRContext* mlir_context, IndexingMap* updates_map,\n     IndexingMap* indices_map) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   // Compute thread id mapping based on the first update operand.\n   auto thread_x = getAffineDimExpr(\n       KernelFusionInterface::kIndexingMapThreadIdxDims[0], mlir_context);\n@@ -887,8 +883,7 @@ int64_t GetNumPossibleValidIndices(absl::Span<const int64_t> slice_shape,\n }\n \n std::unique_ptr<ScatterFusion> CreateScatterFusion(\n-    const HloFusionAnalysis& analysis,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    const HloFusionAnalysis& analysis, MLIRContext* mlir_context) {\n   auto description = GetScatterDescription(analysis);\n   int64_t warp_size = WarpSize(analysis.device_info());\n   int64_t num_elements_per_slice = Product(description.slice_shape);\n@@ -938,15 +933,15 @@ std::unique_ptr<ScatterFusion> CreateScatterFusion(\n     }\n     return std::make_unique<ScatterWithDistributedIndices>(\n         analysis, description, vector_size, num_warps_per_slice,\n-        num_indices_per_warp, indices_vector_size, symbolic_expr_context);\n+        num_indices_per_warp, indices_vector_size, mlir_context);\n   }\n   // Otherwise, we distribute the linearized updates tensor.\n   vector_size =\n       std::gcd(num_elements_per_slice,\n                ComputeLoopFusionConfig(analysis, description.update_shape)\n                    .unroll_factor);\n   return std::make_unique<ScatterWithDistributedUpdates>(\n-      analysis, description, vector_size, symbolic_expr_context);\n+      analysis, description, vector_size, mlir_context);\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "d6261abfaadc560bc0f9ba78096311b8d0c92dc5",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/scatter.h",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fscatter.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -64,8 +64,7 @@ class ScatterFusion : public EmitterBase {\n  public:\n   explicit ScatterFusion(const HloFusionAnalysis& analysis,\n                          const ScatterDescription& description,\n-                         int64_t vector_size,\n-                         SymbolicExprContext* symbolic_expr_context);\n+                         int64_t vector_size, mlir::MLIRContext* mlir_context);\n \n   absl::Status EmitEntryFunction(\n       const emitters::PartitionedComputations& computations,\n@@ -78,14 +77,14 @@ class ScatterFusion : public EmitterBase {\n   }\n \n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t root_index, SymbolicExprContext* ctx) const override {\n+      int64_t root_index, mlir::MLIRContext* ctx) const override {\n     // Since the access pattern to the output is not statically known, we cannot\n     // compute the output->input indexing map.\n     return std::nullopt;\n   }\n \n   std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n-      int64_t root_index, SymbolicExprContext* ctx) const override;\n+      int64_t root_index, mlir::MLIRContext* ctx) const override;\n \n  protected:\n   virtual absl::Status EmitEntryFunctionImpl(\n@@ -94,17 +93,16 @@ class ScatterFusion : public EmitterBase {\n       mlir::ValueRange thread_and_block_ids,\n       mlir::Value output_tensor) const = 0;\n \n-  virtual void ComputeIndexing(SymbolicExprContext* ctx,\n-                               IndexingMap* updates_map,\n+  virtual void ComputeIndexing(mlir::MLIRContext* ctx, IndexingMap* updates_map,\n                                IndexingMap* indices_map) const = 0;\n \n   std::vector<emitters::EpilogueSpecification> GetEpilogues(\n       const HloFusionInstruction& fusion,\n-      SymbolicExprContext* symbolic_expr_context) const final;\n+      mlir::MLIRContext* mlir_context) const final;\n \n   const HloFusionAnalysis& analysis_;\n   ScatterDescription description_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n \n   // The grid is {num_warps_ * WarpSize(), 1, 1, num_blocks_, 1, 1}.\n   int64_t warp_size_;\n@@ -122,9 +120,10 @@ class ScatterFusion : public EmitterBase {\n // index to scatter an element(s) of the update.\n class ScatterWithDistributedUpdates : public ScatterFusion {\n  public:\n-  explicit ScatterWithDistributedUpdates(\n-      const HloFusionAnalysis& analysis, const ScatterDescription& description,\n-      int64_t vector_size, SymbolicExprContext* symbolic_expr_context);\n+  explicit ScatterWithDistributedUpdates(const HloFusionAnalysis& analysis,\n+                                         const ScatterDescription& description,\n+                                         int64_t vector_size,\n+                                         mlir::MLIRContext* mlir_context);\n \n  protected:\n   absl::Status EmitEntryFunctionImpl(mlir::ImplicitLocOpBuilder& b,\n@@ -134,7 +133,7 @@ class ScatterWithDistributedUpdates : public ScatterFusion {\n                                      mlir::ValueRange thread_and_block_ids,\n                                      mlir::Value output_tensor) const override;\n \n-  void ComputeIndexing(SymbolicExprContext* symbolic_expr_context,\n+  void ComputeIndexing(mlir::MLIRContext* mlir_context,\n                        IndexingMap* updates_map,\n                        IndexingMap* indices_map) const override;\n };\n@@ -187,14 +186,16 @@ class ScatterWithDistributedUpdates : public ScatterFusion {\n */\n class ScatterWithDistributedIndices : public ScatterFusion {\n  public:\n-  explicit ScatterWithDistributedIndices(\n-      const HloFusionAnalysis& analysis, const ScatterDescription& description,\n-      int64_t vector_size, int64_t num_warps_per_slice,\n-      int64_t num_indices_per_warp, int64_t indices_vector_size,\n-      SymbolicExprContext* symbolic_expr_context);\n+  explicit ScatterWithDistributedIndices(const HloFusionAnalysis& analysis,\n+                                         const ScatterDescription& description,\n+                                         int64_t vector_size,\n+                                         int64_t num_warps_per_slice,\n+                                         int64_t num_indices_per_warp,\n+                                         int64_t indices_vector_size,\n+                                         mlir::MLIRContext* mlir_context);\n \n  protected:\n-  void ComputeIndexing(SymbolicExprContext* symbolic_expr_context,\n+  void ComputeIndexing(mlir::MLIRContext* mlir_context,\n                        IndexingMap* updates_map,\n                        IndexingMap* indices_map) const override;\n \n@@ -219,8 +220,7 @@ class ScatterWithDistributedIndices : public ScatterFusion {\n };\n \n std::unique_ptr<ScatterFusion> CreateScatterFusion(\n-    const HloFusionAnalysis& analysis,\n-    SymbolicExprContext* symbolic_expr_context);\n+    const HloFusionAnalysis& analysis, mlir::MLIRContext* mlir_context);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "219516220c5e329f4a165d99a35d9aa96bf97bb7",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transpose.cc",
            "status": "modified",
            "additions": 50,
            "deletions": 68,
            "changes": 118,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -164,24 +164,22 @@ absl::Status TransposeFusionBase::EmitEntryFunction(\n }\n \n std::vector<emitters::EpilogueSpecification> TransposeFusionBase::GetEpilogues(\n-    const HloFusionInstruction& fusion,\n-    SymbolicExprContext* symbolic_expr_context) const {\n+    const HloFusionInstruction& fusion, MLIRContext* mlir_context) const {\n   std::vector<emitters::EpilogueSpecification> epilogues{\n       GetEpilogueForOutputIndexing(analysis_, shmem_transposes_,\n-                                   shmem_transpose_roots_,\n-                                   symbolic_expr_context)};\n+                                   shmem_transpose_roots_, mlir_context)};\n   // Add empty epilogues for the side outputs. This ensures their roots don't\n   // get \"fused\" into the tuple function.\n   for (const auto* root : side_output_roots_) {\n     epilogues.push_back(emitters::EpilogueSpecification::FromIdentityIndexing(\n-        root, root, symbolic_expr_context));\n+        root, root, mlir_context));\n   }\n   return epilogues;\n }\n \n TransposeFusion::TransposeFusion(const HloFusionAnalysis& analysis,\n-                                 SymbolicExprContext* symbolic_expr_context)\n-    : TransposeFusionBase(analysis, symbolic_expr_context),\n+                                 MLIRContext* mlir_context)\n+    : TransposeFusionBase(analysis, mlir_context),\n       transpose_(analysis.tiled_transpose()),\n       permutation_(transpose_.permutation),\n       input_shape_(\n@@ -263,30 +261,29 @@ TransposeFusion::TransposeFusion(const HloFusionAnalysis& analysis,\n }\n \n std::optional<IndexingMap> TransposeFusion::ComputeThreadIdToOutputIndexing(\n-    int64_t root_index, SymbolicExprContext* symbolic_expr_context) const {\n+    int64_t root_index, MLIRContext* mlir_context) const {\n   const auto& hero = analysis_.fusion_hero(root_index).instruction();\n   if (!GetDescriptionForTiledTransposeEmitter(hero)) {\n     // The shape of non-transpose roots are bitcast compatible with the input\n     // shape of transpose heroes.\n     return GetIndexing(/*input=*/true,\n-                       analysis_.fusion_root(root_index).shape(),\n-                       symbolic_expr_context);\n+                       analysis_.fusion_root(root_index).shape(), mlir_context);\n   }\n-  return GetIndexing(/*input=*/false, hero.shape(), symbolic_expr_context);\n+  return GetIndexing(/*input=*/false, hero.shape(), mlir_context);\n }\n \n std::optional<std::vector<IndexingMap>>\n TransposeFusion::ComputeThreadIdToInputIndexing(\n-    int64_t root_index, SymbolicExprContext* symbolic_expr_context) const {\n+    int64_t root_index, MLIRContext* mlir_context) const {\n   const auto& hero = analysis_.fusion_hero(root_index).instruction();\n   if (GetDescriptionForTiledTransposeEmitter(hero)) {\n     return std::vector<IndexingMap>{GetIndexing(\n-        /*input=*/true, hero.operand(0)->shape(), symbolic_expr_context)};\n+        /*input=*/true, hero.operand(0)->shape(), mlir_context)};\n   }\n   std::vector<IndexingMap> result;\n   result.reserve(hero.operand_count());\n   auto thread_id_to_output_indexing =\n-      ComputeThreadIdToOutputIndexing(root_index, symbolic_expr_context);\n+      ComputeThreadIdToOutputIndexing(root_index, mlir_context);\n   if (!thread_id_to_output_indexing.has_value()) {\n     return std::nullopt;\n   }\n@@ -295,8 +292,7 @@ TransposeFusion::ComputeThreadIdToInputIndexing(\n     auto map = ComposeIndexingMaps(\n         *thread_id_to_output_indexing,\n         ComputeOutputToInputIndexing(\n-            &analysis_.fusion_root(root_index).instruction(), 0,\n-            symbolic_expr_context)\n+            &analysis_.fusion_root(root_index).instruction(), 0, mlir_context)\n             .indexing_maps[operand_index]\n             .begin()\n             ->map());\n@@ -311,9 +307,8 @@ LaunchDimensions TransposeFusion::launch_dimensions() const {\n }\n \n IndexingMap TransposeFusion::GetSharedMemoryIndexing(\n-    bool read, SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n-  auto thread_offsets = GetThreadOffsets(/*read=*/true, symbolic_expr_context);\n+    bool read, MLIRContext* mlir_context) const {\n+  auto thread_offsets = GetThreadOffsets(/*read=*/true, mlir_context);\n   if (!read) {\n     // Regarding shared memory indexing, the permutation we need to apply is\n     // just a swap of the two dimensions that are tiled.\n@@ -367,13 +362,13 @@ TransposeFusion::WriteResult TransposeFusion::EmitWriteToShMemMlir(\n   }\n \n   IndexingMap write_indexing =\n-      GetSharedMemoryIndexing(/*read=*/false, symbolic_expr_context_);\n+      GetSharedMemoryIndexing(/*read=*/false, mlir_context_);\n   auto body_builder = [&](ImplicitLocOpBuilder& nested_b,\n                           ValueRange symbol_values, ValueRange map_results,\n                           ValueRange output_tensors) -> SmallVector<Value> {\n     auto input_indices = [&](const HloInstruction* instr) {\n       return ApplyIndexing(\n-          GetIndexing(/*input=*/true, instr->shape(), symbolic_expr_context_),\n+          GetIndexing(/*input=*/true, instr->shape(), mlir_context_),\n           thread_and_block_ids, symbol_values, nested_b);\n     };\n \n@@ -416,7 +411,7 @@ TransposeFusion::WriteResult TransposeFusion::EmitWriteToShMemMlir(\n \n   auto indexing = GetIndexing(\n       /*input=*/true, shmem_transposes_.front()->operand(0)->shape(),\n-      symbolic_expr_context_);\n+      mlir_context_);\n   auto written_vector = emitters::EmitXlaLoopOp(builder, thread_and_block_ids,\n                                                 inits, indexing, body_builder);\n   ValueRange written = written_vector;\n@@ -442,9 +437,9 @@ void TransposeFusion::EmitReadFromShMemMlir(\n     const emitters::PartitionedComputations& computations,\n     const WriteResult& written, mlir::ValueRange thread_and_block_ids) const {\n   auto output_indexing = *ComputeThreadIdToOutputIndexing(\n-      shmem_transpose_root_indices_[0], symbolic_expr_context_);\n+      shmem_transpose_root_indices_[0], mlir_context_);\n   auto shmem_read_indexing =\n-      GetSharedMemoryIndexing(/*read=*/true, symbolic_expr_context_);\n+      GetSharedMemoryIndexing(/*read=*/true, mlir_context_);\n   auto result_tensors = emitters::EmitXlaLoopOp(\n       builder, thread_and_block_ids, written.updated_outputs, output_indexing,\n       [&](ImplicitLocOpBuilder& nested_b, ValueRange symbol_values,\n@@ -481,8 +476,7 @@ void TransposeFusion::EmitReadFromShMemMlir(\n }\n \n llvm::SmallVector<mlir::AffineExpr, 4> TransposeFusion::GetThreadOffsets(\n-    bool read, SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    bool read, MLIRContext* mlir_context) const {\n   auto thread = getAffineDimExpr(\n       KernelFusionInterface::kIndexingMapThreadIdxDims[0], mlir_context);\n   auto loop = getAffineSymbolExpr(0, mlir_context);\n@@ -496,17 +490,15 @@ llvm::SmallVector<mlir::AffineExpr, 4> TransposeFusion::GetThreadOffsets(\n                                   read ? block_sizes_ : output_block_sizes_);\n }\n \n-IndexingMap TransposeFusion::GetIndexing(\n-    bool input, const xla::Shape& shape,\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+IndexingMap TransposeFusion::GetIndexing(bool input, const xla::Shape& shape,\n+                                         MLIRContext* mlir_context) const {\n   auto raw_id = getAffineDimExpr(\n       KernelFusionInterface::kIndexingMapBlockIdxDims[0], mlir_context);\n   auto block_ids = DelinearizeInBoundsIndex(raw_id, block_counts_);\n   if (!input) {\n     absl::c_copy(Permute(block_ids, permutation_), block_ids.begin());\n   }\n-  auto thread_offsets = GetThreadOffsets(input, symbolic_expr_context);\n+  auto thread_offsets = GetThreadOffsets(input, mlir_context);\n   const auto& permuted_block_sizes = input ? block_sizes_ : output_block_sizes_;\n   llvm::SmallVector<AffineExpr, 3> offsets;\n   for (auto [block_id, block_size, thread] :\n@@ -531,7 +523,7 @@ IndexingMap TransposeFusion::GetIndexing(\n     result.AddConstraint(dim, {0, size - 1});\n   }\n   result = ComposeIndexingMaps(\n-      result, GetBitcastMap(normalized_shape, shape, symbolic_expr_context));\n+      result, GetBitcastMap(normalized_shape, shape, mlir_context));\n   result.Simplify();\n   return result;\n }\n@@ -552,9 +544,8 @@ std::vector<int64_t> GetBlockCounts(absl::Span<const int64_t> shape,\n PackedTranspose::PackedTranspose(const HloFusionAnalysis& analysis,\n                                  const TransposeSpec& spec,\n                                  absl::Span<const int64_t> output_block_tile,\n-                                 int64_t num_warps,\n-                                 SymbolicExprContext* symbolic_expr_context)\n-    : TransposeFusionBase(analysis, symbolic_expr_context),\n+                                 int64_t num_warps, MLIRContext* mlir_context)\n+    : TransposeFusionBase(analysis, mlir_context),\n       spec_(spec),\n       output_tile_(output_block_tile.begin(), output_block_tile.end()),\n       input_tile_(Permute(output_tile_, spec_.canonical_inv_permutation)),\n@@ -591,21 +582,21 @@ PackedTranspose::PackedTranspose(const HloFusionAnalysis& analysis,\n }\n \n std::optional<IndexingMap> PackedTranspose::ComputeThreadIdToOutputIndexing(\n-    int64_t root_index, SymbolicExprContext* symbolic_expr_context) const {\n-  return GetOutputIndexing(symbolic_expr_context);\n+    int64_t root_index, MLIRContext* mlir_context) const {\n+  return GetOutputIndexing(mlir_context);\n }\n \n std::optional<std::vector<IndexingMap>>\n PackedTranspose::ComputeThreadIdToInputIndexing(\n-    int64_t root_index, SymbolicExprContext* symbolic_expr_context) const {\n+    int64_t root_index, MLIRContext* mlir_context) const {\n   const auto& hero = analysis_.fusion_hero(root_index).instruction();\n   if (GetDescriptionForTiledTransposeEmitter(hero)) {\n-    return std::vector<IndexingMap>{GetInputIndexing(symbolic_expr_context)};\n+    return std::vector<IndexingMap>{GetInputIndexing(mlir_context)};\n   }\n   std::vector<IndexingMap> result;\n   result.reserve(hero.operand_count());\n   auto thread_id_to_output_indexing =\n-      ComputeThreadIdToOutputIndexing(root_index, symbolic_expr_context);\n+      ComputeThreadIdToOutputIndexing(root_index, mlir_context);\n   if (!thread_id_to_output_indexing.has_value()) {\n     return std::nullopt;\n   }\n@@ -614,8 +605,7 @@ PackedTranspose::ComputeThreadIdToInputIndexing(\n     auto map = ComposeIndexingMaps(\n         *thread_id_to_output_indexing,\n         ComputeOutputToInputIndexing(\n-            &analysis_.fusion_root(root_index).instruction(), 0,\n-            symbolic_expr_context)\n+            &analysis_.fusion_root(root_index).instruction(), 0, mlir_context)\n             .indexing_maps[operand_index]\n             .begin()\n             ->map());\n@@ -635,9 +625,8 @@ PackedTranspose::WriteResult PackedTranspose::EmitWriteToShMemMlir(\n     const emitters::PartitionedComputation& root_computation,\n     const emitters::CallTargetProvider& call_target_provider,\n     ValueRange output_args, mlir::ValueRange thread_and_block_ids) const {\n-  IndexingMap input_indexing = GetInputIndexing(symbolic_expr_context_);\n-  IndexingMap shmem_write_indexing =\n-      GetShmemWriteIndexing(symbolic_expr_context_);\n+  IndexingMap input_indexing = GetInputIndexing(mlir_context_);\n+  IndexingMap shmem_write_indexing = GetShmemWriteIndexing(mlir_context_);\n \n   int64_t shmem_dim = kNumShmemBanks * vector_size_;\n   SmallVector<Value> shmem_tensors;\n@@ -685,8 +674,8 @@ PackedTranspose::WriteResult PackedTranspose::EmitWriteToShMemMlir(\n     auto* root_tuple = fusion.fused_expression_root();\n     for (auto root : side_output_roots_) {\n       auto indexing = ComposeIndexingMaps(\n-          input_indexing, GetBitcastMap(spec_.input_shape(), root->shape(),\n-                                        symbolic_expr_context_));\n+          input_indexing,\n+          GetBitcastMap(spec_.input_shape(), root->shape(), mlir_context_));\n       indexing.Simplify();\n       side_output_indices.push_back(ApplyIndexing(\n           indexing, thread_and_block_ids, symbol_values, nested_b));\n@@ -739,10 +728,10 @@ void PackedTranspose::EmitReadFromShMemMlir(\n     const HloFusionInstruction& fusion,\n     const emitters::PartitionedComputations& computations,\n     const WriteResult& written, mlir::ValueRange thread_and_block_ids) const {\n-  auto shmem_read_indexing = GetShmemReadIndexing(symbolic_expr_context_);\n+  auto shmem_read_indexing = GetShmemReadIndexing(mlir_context_);\n   auto outer_loop_indexing = ConvertRangeVariablesToDimensions(\n       shmem_read_indexing, /*range_var_indices=*/{1, 2});\n-  auto output_indexing = GetOutputIndexing(symbolic_expr_context_);\n+  auto output_indexing = GetOutputIndexing(mlir_context_);\n   auto output_indexing_over_vectors = ConvertRangeVariablesToDimensions(\n       output_indexing, /*range_var_indices=*/{0});\n \n@@ -817,9 +806,7 @@ void PackedTranspose::EmitReadFromShMemMlir(\n   builder.create<ReturnOp>(outer_loop_results);\n }\n \n-IndexingMap PackedTranspose::GetInputIndexing(\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+IndexingMap PackedTranspose::GetInputIndexing(MLIRContext* mlir_context) const {\n   // Dimensions variables.\n   auto thread_id = getAffineDimExpr(\n       KernelFusionInterface::kIndexingMapThreadIdxDims[0], mlir_context);\n@@ -872,7 +859,7 @@ IndexingMap PackedTranspose::GetInputIndexing(\n \n   // Actual indexing.\n   auto canonical_input_shape_to_real_shape = GetBitcastMap(\n-      spec_.canonical_input_shape, spec_.input_shape(), symbolic_expr_context);\n+      spec_.canonical_input_shape, spec_.input_shape(), mlir_context);\n   // When we compose, the constraints w.r.t. to the input dimension sizes will\n   // be added.\n   auto input_indexing = ComposeIndexingMaps(\n@@ -882,8 +869,7 @@ IndexingMap PackedTranspose::GetInputIndexing(\n }\n \n IndexingMap PackedTranspose::GetShmemWriteIndexing(\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) const {\n   // Dimensions variables.\n   auto thread_id = getAffineDimExpr(\n       KernelFusionInterface::kIndexingMapThreadIdxDims[0], mlir_context);\n@@ -915,8 +901,7 @@ IndexingMap PackedTranspose::GetShmemWriteIndexing(\n }\n \n IndexingMap PackedTranspose::GetShmemReadIndexing(\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) const {\n   // Dimensions variables.\n   auto thread_id = getAffineDimExpr(\n       KernelFusionInterface::kIndexingMapThreadIdxDims[0], mlir_context);\n@@ -952,8 +937,7 @@ IndexingMap PackedTranspose::GetShmemReadIndexing(\n }\n \n IndexingMap PackedTranspose::GetOutputIndexing(\n-    SymbolicExprContext* symbolic_expr_context) const {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) const {\n   // Dimensions variables.\n   auto thread_id = getAffineDimExpr(\n       KernelFusionInterface::kIndexingMapThreadIdxDims[0], mlir_context);\n@@ -1005,9 +989,8 @@ IndexingMap PackedTranspose::GetOutputIndexing(\n   canonical_output_indexing.Simplify();\n \n   // Actual indexing.\n-  auto canonical_output_shape_to_real_shape =\n-      GetBitcastMap(spec_.canonical_output_shape, spec_.output_shape(),\n-                    symbolic_expr_context);\n+  auto canonical_output_shape_to_real_shape = GetBitcastMap(\n+      spec_.canonical_output_shape, spec_.output_shape(), mlir_context);\n   // When we compose, the constraints w.r.t. to the output dimension sizes will\n   // be added.\n   auto output_indexing = ComposeIndexingMaps(\n@@ -1017,17 +1000,16 @@ IndexingMap PackedTranspose::GetOutputIndexing(\n }\n \n std::unique_ptr<EmitterBase> CreateTransposeFusion(\n-    const HloFusionAnalysis& analysis,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    const HloFusionAnalysis& analysis, MLIRContext* mlir_context) {\n   auto spec = GetTransposeSpec(\n       Cast<HloTransposeInstruction>(analysis.tiled_transpose().instr));\n   auto packed_transpose_tile = GetPackedTransposeTileSizes(spec);\n   if (packed_transpose_tile.ok()) {\n-    return std::make_unique<PackedTranspose>(\n-        analysis, spec, *packed_transpose_tile,\n-        /* num_warps= */ 4, symbolic_expr_context);\n+    return std::make_unique<PackedTranspose>(analysis, spec,\n+                                             *packed_transpose_tile,\n+                                             /* num_warps= */ 4, mlir_context);\n   }\n-  return std::make_unique<TransposeFusion>(analysis, symbolic_expr_context);\n+  return std::make_unique<TransposeFusion>(analysis, mlir_context);\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "64c406d5c0949cca0f6bae99b72074647037c298",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transpose.h",
            "status": "modified",
            "additions": 19,
            "deletions": 25,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -50,8 +50,8 @@ namespace gpu {\n class TransposeFusionBase : public EmitterBase {\n  public:\n   explicit TransposeFusionBase(const HloFusionAnalysis& analysis,\n-                               SymbolicExprContext* symbolic_expr_context)\n-      : analysis_(analysis), symbolic_expr_context_(symbolic_expr_context) {}\n+                               mlir::MLIRContext* mlir_context)\n+      : analysis_(analysis), mlir_context_(mlir_context) {}\n \n  protected:\n   absl::Status EmitEntryFunction(\n@@ -62,7 +62,7 @@ class TransposeFusionBase : public EmitterBase {\n \n   std::vector<emitters::EpilogueSpecification> GetEpilogues(\n       const HloFusionInstruction& fusion,\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      mlir::MLIRContext* mlir_context) const override;\n \n   struct WriteResult {\n     // All output tensors of the fusion, with side outputs written to them.\n@@ -87,7 +87,7 @@ class TransposeFusionBase : public EmitterBase {\n       mlir::ValueRange thread_and_block_ids) const = 0;\n \n   const HloFusionAnalysis& analysis_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n \n   // Transpose instructions that require shared memory. Note that not all\n   // transposes require shared memory, e.g. the ones with a large innermost\n@@ -118,16 +118,14 @@ class TransposeFusionBase : public EmitterBase {\n class TransposeFusion : public TransposeFusionBase {\n  public:\n   explicit TransposeFusion(const HloFusionAnalysis& analysis,\n-                           SymbolicExprContext* symbolic_expr_context);\n+                           mlir::MLIRContext* mlir_context);\n   LaunchDimensions launch_dimensions() const override;\n \n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t root_index,\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      int64_t root_index, mlir::MLIRContext* mlir_context) const override;\n \n   std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n-      int64_t root_index,\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      int64_t root_index, mlir::MLIRContext* mlir_context) const override;\n \n  protected:\n   WriteResult EmitWriteToShMemMlir(\n@@ -146,12 +144,12 @@ class TransposeFusion : public TransposeFusionBase {\n \n  private:\n   IndexingMap GetIndexing(bool input, const xla::Shape& shape,\n-                          SymbolicExprContext* symbolic_expr_context) const;\n-  IndexingMap GetSharedMemoryIndexing(\n-      bool read, SymbolicExprContext* symbolic_expr_context) const;\n+                          mlir::MLIRContext* mlir_context) const;\n+  IndexingMap GetSharedMemoryIndexing(bool read,\n+                                      mlir::MLIRContext* mlir_context) const;\n \n   llvm::SmallVector<mlir::AffineExpr, 4> GetThreadOffsets(\n-      bool read, SymbolicExprContext* symbolic_expr_context) const;\n+      bool read, mlir::MLIRContext* mlir_context) const;\n   bool MostMinorDimensionUnchanged() const;\n \n   TransposeDescription transpose_;\n@@ -236,18 +234,15 @@ class PackedTranspose : public TransposeFusionBase {\n   explicit PackedTranspose(const HloFusionAnalysis& analysis,\n                            const TransposeSpec& spec,\n                            absl::Span<const int64_t> output_block_tile,\n-                           int64_t num_warps,\n-                           SymbolicExprContext* symbolic_expr_context);\n+                           int64_t num_warps, mlir::MLIRContext* mlir_context);\n \n   LaunchDimensions launch_dimensions() const override;\n \n   std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t root_index,\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      int64_t root_index, mlir::MLIRContext* mlir_context) const override;\n \n   std::optional<std::vector<IndexingMap>> ComputeThreadIdToInputIndexing(\n-      int64_t root_index,\n-      SymbolicExprContext* symbolic_expr_context) const override;\n+      int64_t root_index, mlir::MLIRContext* mlir_context) const override;\n \n  protected:\n   WriteResult EmitWriteToShMemMlir(\n@@ -266,11 +261,11 @@ class PackedTranspose : public TransposeFusionBase {\n       mlir::ValueRange thread_and_block_ids) const override;\n \n  private:\n-  IndexingMap GetInputIndexing(SymbolicExprContext* ctx) const;\n-  IndexingMap GetShmemWriteIndexing(SymbolicExprContext* ctx) const;\n+  IndexingMap GetInputIndexing(mlir::MLIRContext* ctx) const;\n+  IndexingMap GetShmemWriteIndexing(mlir::MLIRContext* ctx) const;\n \n-  IndexingMap GetShmemReadIndexing(SymbolicExprContext* ctx) const;\n-  IndexingMap GetOutputIndexing(SymbolicExprContext* ctx) const;\n+  IndexingMap GetShmemReadIndexing(mlir::MLIRContext* ctx) const;\n+  IndexingMap GetOutputIndexing(mlir::MLIRContext* ctx) const;\n \n   TransposeSpec spec_;\n \n@@ -303,8 +298,7 @@ class PackedTranspose : public TransposeFusionBase {\n };\n \n std::unique_ptr<EmitterBase> CreateTransposeFusion(\n-    const HloFusionAnalysis& analysis,\n-    SymbolicExprContext* symbolic_expr_context);\n+    const HloFusionAnalysis& analysis, mlir::MLIRContext* mlir_context);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "b2298366aa117948fe668d8b6496b3e9c68f2c7d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -41,7 +41,6 @@ limitations under the License.\n #include \"xla/codegen/emitters/kernel_api_builder.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/runtime/work_dimensions.h\"\n #include \"xla/service/gpu/ir_emitter_context.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n@@ -101,11 +100,11 @@ absl::Status AnnotateKernelLaunchDimensions(\n \n IndexingMap KernelFusionInterface::GetDefaultThreadIdIndexingMap(\n     const LaunchDimensions& launch_dims, int unroll_factor, const Shape& shape,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    mlir::MLIRContext* mlir_context) {\n   WorkDimensions work_dimensions = launch_dims.AsWorkDimensions();\n   work_dimensions.work_tile_size.dimensions.push_back(unroll_factor);\n   return emitters::GetDefaultWorkItemIndexingMap(work_dimensions, shape,\n-                                                 symbolic_expr_context);\n+                                                 mlir_context);\n }\n \n std::string GetSanitizedUniqueName(IrEmitterContext& ir_emitter_context,"
        },
        {
            "sha": "10d2a900f33fb367593f417c9fecfdefe065d0a9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -77,14 +77,14 @@ class KernelFusionInterface : public FusionInterface {\n   // unsupported (scatter, in-place DUS). Implementations will return nullopt.\n   // Note: Work in progress, not implemented for all emitters.\n   virtual std::optional<IndexingMap> ComputeThreadIdToOutputIndexing(\n-      int64_t root_index, SymbolicExprContext* ctx) const = 0;\n+      int64_t root_index, mlir::MLIRContext* ctx) const = 0;\n \n   // Computes indexing maps from thread id to input elements of the root's\n   // **hero**. Note that in many cases this is not computable from the output\n   // indexing. The indexing may only be known for some operands of the hero.\n   virtual std::optional<std::vector<IndexingMap>>\n   ComputeThreadIdToInputIndexing(int64_t root_index,\n-                                 SymbolicExprContext* ctx) const = 0;\n+                                 mlir::MLIRContext* ctx) const = 0;\n \n   static constexpr std::array<int, 3> kIndexingMapThreadIdxDims = {0, 1, 2};\n   static constexpr std::array<int, 3> kIndexingMapBlockIdxDims = {3, 4, 5};\n@@ -96,7 +96,7 @@ class KernelFusionInterface : public FusionInterface {\n   // block sizes in the given launch dimensions.\n   static IndexingMap GetDefaultThreadIdIndexingMap(\n       const LaunchDimensions& launch_dims, int unroll_factor,\n-      const Shape& shape, SymbolicExprContext* ctx);\n+      const Shape& shape, mlir::MLIRContext* ctx);\n };\n \n absl::StatusOr<llvm::Function*> BuildKernelPrototype("
        },
        {
            "sha": "d8e082f8b0a3a8d43211f64d7bd7ce0ca05a1256",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusions.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -31,7 +31,6 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion.h\"\n #include \"xla/codegen/ir_emission_utils.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n@@ -78,7 +77,7 @@ bool HloFusionInfo::CanEmitDynamicUpdateSliceInPlace() const {\n }\n \n std::unique_ptr<FusionInterface> GetFusionEmitter(\n-    const FusionInfo& fusion_info, SymbolicExprContext* symbolic_expr_context) {\n+    const FusionInfo& fusion_info, mlir::MLIRContext* mlir_context) {\n   const auto& analysis = fusion_info.analysis();\n   const FusionBackendConfig& backend_config = analysis.fusion_backend_config();\n \n@@ -109,16 +108,16 @@ std::unique_ptr<FusionInterface> GetFusionEmitter(\n           fusion_info.CanEmitDynamicUpdateSliceInPlace()) {\n         return std::make_unique<InPlaceDynamicUpdateSliceFusion>(analysis);\n       }\n-      return std::make_unique<LoopFusion>(analysis, symbolic_expr_context);\n+      return std::make_unique<LoopFusion>(analysis, mlir_context);\n     }\n     case HloFusionAnalysis::EmitterFusionKind::kReduction: {\n-      return CreateReductionFusion(analysis, symbolic_expr_context);\n+      return CreateReductionFusion(analysis, mlir_context);\n     }\n     case HloFusionAnalysis::EmitterFusionKind::kScatter: {\n-      return CreateScatterFusion(analysis, symbolic_expr_context);\n+      return CreateScatterFusion(analysis, mlir_context);\n     }\n     case HloFusionAnalysis::EmitterFusionKind::kTranspose: {\n-      return CreateTransposeFusion(analysis, symbolic_expr_context);\n+      return CreateTransposeFusion(analysis, mlir_context);\n     }\n     case HloFusionAnalysis::EmitterFusionKind::kConcatenate: {\n       return std::make_unique<ConcatenateFusion>(analysis);"
        },
        {
            "sha": "c1f1aabfed5e0c735fbd34d98edadf73277266c2",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusions.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -20,7 +20,6 @@ limitations under the License.\n \n #include \"absl/status/statusor.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n@@ -96,7 +95,7 @@ class PreBufferAssignmentFusionInfo : public FusionInfo {\n \n // Returns the emitter for the given fusion.\n std::unique_ptr<FusionInterface> GetFusionEmitter(\n-    const FusionInfo& fusion_info, SymbolicExprContext* symbolic_expr_context);\n+    const FusionInfo& fusion_info, mlir::MLIRContext* mlir_context);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "d5aa2878e1ccb7716d23a168f6be4c27b80eae8e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/tools/fusion_to_mlir.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ffusion_to_mlir.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ffusion_to_mlir.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ffusion_to_mlir.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -19,7 +19,6 @@ limitations under the License.\n #include \"llvm/Support/raw_ostream.h\"\n #include \"xla/backends/gpu/codegen/tools/test_lib.h\"\n #include \"xla/codegen/tools/test_lib.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"tsl/platform/init_main.h\"\n #include \"tsl/platform/statusor.h\"\n \n@@ -29,10 +28,8 @@ namespace gpu {\n absl::Status Run(const std::string& filename) {\n   auto mlir_context = GetMlirContextForTest();\n   mlir_context.loadAllAvailableDialects();\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n   TF_ASSIGN_OR_RETURN(auto module, LoadTestModule(filename));\n-  TF_ASSIGN_OR_RETURN(auto emitter_data,\n-                      GetEmitter(*module, symbolic_expr_context));\n+  TF_ASSIGN_OR_RETURN(auto emitter_data, GetEmitter(*module, mlir_context));\n   TF_ASSIGN_OR_RETURN(auto mlir_module,\n                       emitter_data->emitter->CreateMLIRModule(\n                           mlir_context, *emitter_data->fusion, \"main\","
        },
        {
            "sha": "02ab435155c9213c8b7c68dc5f71154e66b0977c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/tools/gpu_test_correctness.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Fgpu_test_correctness.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Fgpu_test_correctness.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Fgpu_test_correctness.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -108,15 +108,14 @@ std::pair<std::string, std::vector<int64_t>> ParseHeroAndIds(\n \n TEST_F(CorrectnessTest, InputIndexingIsBijection) {\n   auto mlir_context = GetMlirContextForTest();\n-  auto symbolic_expr_context = GetSymbolicExprContextForTest(&mlir_context);\n+  RegisterSymbolicExprStorage(&mlir_context);\n   TF_ASSERT_OK_AND_ASSIGN(auto module, LoadTestModule(flags.input_file));\n-  TF_ASSERT_OK_AND_ASSIGN(auto emitter_data,\n-                          GetEmitter(*module, symbolic_expr_context));\n+  TF_ASSERT_OK_AND_ASSIGN(auto emitter_data, GetEmitter(*module, mlir_context));\n   for (const auto& [hero_name, ids] : flags.bijection_inputs) {\n     TF_ASSERT_OK_AND_ASSIGN(int64_t hero_index,\n                             GetHeroIndex(hero_name, *emitter_data->analysis));\n     auto indexing = emitter_data->emitter->ComputeThreadIdToInputIndexing(\n-        hero_index, &symbolic_expr_context);\n+        hero_index, &mlir_context);\n     ASSERT_TRUE(indexing.has_value());\n     for (int64_t id : ids) {\n       TF_ASSERT_OK(TestBijection(indexing.value()[id],\n@@ -132,15 +131,14 @@ TEST_F(CorrectnessTest, InputIndexingIsBijection) {\n \n TEST_F(CorrectnessTest, OutputIndexingIsBijection) {\n   auto mlir_context = GetMlirContextForTest();\n-  auto symbolic_expr_context = GetSymbolicExprContextForTest(&mlir_context);\n+  RegisterSymbolicExprStorage(&mlir_context);\n   TF_ASSERT_OK_AND_ASSIGN(auto module, LoadTestModule(flags.input_file));\n-  TF_ASSERT_OK_AND_ASSIGN(auto emitter_data,\n-                          GetEmitter(*module, symbolic_expr_context));\n+  TF_ASSERT_OK_AND_ASSIGN(auto emitter_data, GetEmitter(*module, mlir_context));\n   for (const auto& hero_name : flags.bijection_outputs) {\n     TF_ASSERT_OK_AND_ASSIGN(int64_t hero_index,\n                             GetHeroIndex(hero_name, *emitter_data->analysis));\n     auto indexing = emitter_data->emitter->ComputeThreadIdToOutputIndexing(\n-        hero_index, &symbolic_expr_context);\n+        hero_index, &mlir_context);\n     ASSERT_TRUE(indexing.has_value());\n     TF_ASSERT_OK(TestBijection(\n         *indexing, GetFirstArrayShape("
        },
        {
            "sha": "c97bd90ae1550adad6ef0850600301d0fe854bc0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/tools/test_lib.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ftest_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ftest_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ftest_lib.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -20,7 +20,6 @@ limitations under the License.\n #include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n #include \"xla/backends/gpu/codegen/emitters/emitter_base.h\"\n #include \"xla/backends/gpu/codegen/fusions.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -33,7 +32,7 @@ namespace xla {\n namespace gpu {\n \n absl::StatusOr<std::unique_ptr<EmitterData>> GetEmitter(\n-    const HloModule& module, SymbolicExprContext& symbolic_expr_context) {\n+    const HloModule& module, mlir::MLIRContext& mlir_context) {\n   auto data = std::make_unique<EmitterData>();\n   data->fusion = DynCast<HloFusionInstruction>(\n       module.entry_computation()->root_instruction());\n@@ -42,7 +41,7 @@ absl::StatusOr<std::unique_ptr<EmitterData>> GetEmitter(\n   data->analysis.emplace(\n       HloFusionAnalysis::Create(*data->fusion, data->device.value()));\n   PreBufferAssignmentFusionInfo info(data->analysis.value());\n-  auto fusion_emitter = GetFusionEmitter(info, &symbolic_expr_context);\n+  auto fusion_emitter = GetFusionEmitter(info, &mlir_context);\n \n   auto emitter = dynamic_cast<EmitterBase*>(fusion_emitter.get());\n   TF_RET_CHECK(emitter != nullptr) << \"Expected emitter to be an EmitterBase\";\n@@ -56,10 +55,5 @@ mlir::MLIRContext GetMlirContextForTest() {\n   return mlir::MLIRContext(EmitterBase::GetDialectRegistry());\n }\n \n-SymbolicExprContext GetSymbolicExprContextForTest(\n-    mlir::MLIRContext* mlir_context) {\n-  return SymbolicExprContext(mlir_context);\n-}\n-\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "0c50c02762eeab5da5db6500821d0644abd59f3a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/tools/test_lib.h",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ftest_lib.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ftest_lib.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ftest_lib.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -21,7 +21,6 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/emitters/emitter_base.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n@@ -40,16 +39,11 @@ struct EmitterData {\n   std::unique_ptr<EmitterBase> emitter;\n };\n absl::StatusOr<std::unique_ptr<EmitterData>> GetEmitter(\n-    const HloModule& module, SymbolicExprContext& symbolic_expr_context);\n+    const HloModule& module, mlir::MLIRContext& mlir_context);\n \n // Returns an MLIR context with all the dialects needed for testing.\n mlir::MLIRContext GetMlirContextForTest();\n \n-// Returns a symbolic expression context with all the dialects needed for\n-// testing.\n-SymbolicExprContext GetSymbolicExprContextForTest(\n-    mlir::MLIRContext* mlir_context);\n-\n }  // namespace gpu\n }  // namespace xla\n "
        },
        {
            "sha": "49c1c7b79b9e3a2f81594cffa6a7d4c6e099aa01",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -947,7 +947,7 @@ cc_library(\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tests:hlo_test_base\",\n-        \"//xla/tests:hlo_test_base_with_symbolic_expr_context\",\n+        \"//xla/tests:hlo_test_base_with_mlir_context\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -1069,7 +1069,7 @@ xla_cc_test(\n         \":test_utils\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service/gpu/model:block_level_parameters\",\n-        \"//xla/tests:hlo_test_base_with_symbolic_expr_context\",\n+        \"//xla/tests:hlo_test_base_with_mlir_context\",\n         \"//xla/tests:xla_internal_test_main\",  # fixdeps: keep\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\","
        },
        {
            "sha": "4fc42b37d0a06315845c048ce47e0accd6350e1f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -33,7 +33,6 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/fusions.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -71,7 +70,6 @@ struct ModuleWithFusion {\n \n struct ModuleWithEmitter : public ModuleWithFusion {\n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context{&mlir_context};\n   std::optional<HloFusionAnalysis> analysis;\n   std::unique_ptr<TritonFusion> emitter;\n   llvm::LLVMContext llvm_context;\n@@ -144,7 +142,7 @@ class CollectiveEmitterTest : public CollectiveBlockLevelConfigTest {\n         HloFusionAnalysis::Create(*result->FusionInstr(), device_info);\n     std::unique_ptr<FusionInterface> fusion_emitter =\n         GetFusionEmitter(PreBufferAssignmentFusionInfo{*result->analysis},\n-                         &result->symbolic_expr_context);\n+                         &result->mlir_context);\n     TritonFusion* triton_emitter =\n         dynamic_cast<TritonFusion*>(fusion_emitter.get());\n     TF_RET_CHECK(triton_emitter != nullptr);\n@@ -232,7 +230,7 @@ TEST_F(CollectiveEmitterTest, AllReduceWithTritonGenerateTritonKernel) {\n       TritonWrapperResult triton_kernel,\n       triton_fusion->GenerateTritonKernelAndWrapper(\n           *result->FusionInstr(), \"test-all-reduce-start\", device_info_,\n-          &result->llvm_module, &result->symbolic_expr_context));\n+          &result->llvm_module, &result->mlir_context));\n }\n \n }  // namespace"
        },
        {
            "sha": "4fc4960fe9bedf34bac04cc0808b89fd0f3c3f26",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -94,7 +94,7 @@ absl::StatusOr<TritonWrapperResult>\n TritonFusion::GenerateTritonKernelAndWrapper(\n     const HloFusionInstruction& fusion, absl::string_view impl_fn_name,\n     const se::DeviceDescription& device_info, llvm::Module* llvm_module,\n-    SymbolicExprContext* symbolic_expr_context) const {\n+    mlir::MLIRContext* mlir_context) const {\n   const se::GpuComputeCapability& cc = device_info.gpu_compute_capability();\n   auto backend_config =\n       fusion.backend_config<GpuBackendConfig>()->fusion_backend_config();\n@@ -116,7 +116,7 @@ TritonFusion::GenerateTritonKernelAndWrapper(\n             impl_fn_name, &fusion, cc, device_info,\n             BlockLevelParameters::FromBlockLevelFusionConfig(\n                 analysis_.fusion_backend_config().block_level_fusion_config()),\n-            llvm_module, *symbolic_expr_context));\n+            llvm_module, *mlir_context));\n   } else {  // Must be a MatMul\n     CHECK_EQ(fusion_kind, kTritonGemmFusionKind);\n     // TODO(bchetioui): port matmul emitter to fully use the new\n@@ -133,10 +133,10 @@ TritonFusion::GenerateTritonKernelAndWrapper(\n       block_level_parameters.num_warps = triton_config.num_warps();\n     }\n \n-    TF_ASSIGN_OR_RETURN(triton_wrapper_result,\n-                        TritonWrapper(impl_fn_name, &fusion, cc, device_info,\n-                                      block_level_parameters, llvm_module,\n-                                      *symbolic_expr_context));\n+    TF_ASSIGN_OR_RETURN(\n+        triton_wrapper_result,\n+        TritonWrapper(impl_fn_name, &fusion, cc, device_info,\n+                      block_level_parameters, llvm_module, *mlir_context));\n   }\n \n   return triton_wrapper_result;\n@@ -170,7 +170,7 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n         GenerateTritonKernelAndWrapper(fusion, impl_fn_name,\n                                        ir_emitter_context.gpu_device_info(),\n                                        ir_emitter_context.llvm_module(),\n-                                       ir_emitter_context.expr_context()));\n+                                       ir_emitter_context.mlir_context()));\n \n     auto backend_config =\n         fusion.backend_config<GpuBackendConfig>()->fusion_backend_config();"
        },
        {
            "sha": "4115f2875990a556ab28d09b06be84f57221d24e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -66,7 +66,7 @@ class TritonFusion : public FusionInterface {\n   absl::StatusOr<TritonWrapperResult> GenerateTritonKernelAndWrapper(\n       const HloFusionInstruction& fusion, absl::string_view impl_fn_name,\n       const se::DeviceDescription& device_info, llvm::Module* llvm_module,\n-      SymbolicExprContext* symbolic_expr_context) const;\n+      mlir::MLIRContext* mlir_context) const;\n \n  private:\n   const HloFusionAnalysis& analysis_;"
        },
        {
            "sha": "b86574cbc865e0a2d7bc9fada00ef80cab5149c4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 15,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -169,6 +169,7 @@ namespace xgt = ::xla::gpu::triton;\n using ::llvm::SmallVector;\n using ::mlir::AffineMap;\n using ::mlir::ArrayRef;\n+using ::mlir::MLIRContext;\n using ::mlir::Type;\n using ::mlir::Value;\n using ::mlir::ValueRange;\n@@ -1403,16 +1404,15 @@ absl::Status EmitGeneric(\n     EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder,\n     const HloFusionInstruction* fusion, xtile::EntryFuncOp fn,\n     const BlockLevelParameters& block_level_parameters,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   if (VLOG_IS_ON(6)) {\n     VLOG(6) << \"Emitting Triton IR for fusion\\n\"\n             << ExtractInstructionIntoNewModule(*fusion)->ToString();\n   }\n   const HloComputation* computation = fusion->fused_instructions_computation();\n   SymbolicTileAnalysisOrError symbolic_tile_analysis_or =\n       SymbolicTileAnalysis::AnalyzeComputation(\n-          *computation, symbolic_expr_context,\n-          emitter_specific_constraints_builder);\n+          *computation, mlir_context, emitter_specific_constraints_builder);\n \n   if (std::holds_alternative<FusionDecision>(symbolic_tile_analysis_or)) {\n     return Internal(\n@@ -1638,17 +1638,16 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n     absl::string_view fn_name, const HloFusionInstruction* fusion,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    SymbolicExprContext& symbolic_expr_context) {\n+    MLIRContext& mlir_context) {\n   TF_RETURN_IF_ERROR(IsTritonSupportedFusion(*fusion, device_info));\n \n   // TODO: b/451959933 - Use reference or check pointer.\n-  mlir::MLIRContext& mlir_context = *symbolic_expr_context.GetMLIRContext();\n \n   TF_ASSIGN_OR_RETURN(\n       auto triton_module,\n       ir_emitter_triton_internal::EmitXTileModule(\n           fn_name, TritonEmitterConstraints::GetBuilder(device_info), fusion,\n-          block_level_parameters, symbolic_expr_context,\n+          block_level_parameters, mlir_context,\n           ir_emitter_triton_internal::LegacyMatmulEmitter(device_info)));\n \n   const HloComputation* hlo_computation =\n@@ -1709,14 +1708,12 @@ absl::StatusOr<TritonWrapperResult> TritonWrapper(\n     const se::GpuComputeCapability& gpu_cc,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    llvm::Module* llvm_module, SymbolicExprContext& symbolic_expr_context) {\n-  mlir::MLIRContext& mlir_context = *symbolic_expr_context.GetMLIRContext();\n+    llvm::Module* llvm_module, MLIRContext& mlir_context) {\n   TF_RETURN_IF_ERROR(CheckAtLeastAmpere(gpu_cc));\n \n-  TF_ASSIGN_OR_RETURN(\n-      mlir::OwningOpRef<mlir::ModuleOp> triton_module,\n-      CreateTritonModule(fn_name, fusion, device_info, block_level_parameters,\n-                         symbolic_expr_context));\n+  TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> triton_module,\n+                      CreateTritonModule(fn_name, fusion, device_info,\n+                                         block_level_parameters, mlir_context));\n \n   VLOG(3) << fusion->ToString(HloPrintOptions::ShortParsable());\n   VLOG(3) << fusion->fused_instructions_computation()->ToString(\n@@ -1951,9 +1948,8 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n     EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder,\n     const HloFusionInstruction* fusion,\n     const BlockLevelParameters& block_level_parameters,\n-    SymbolicExprContext& symbolic_expr_context,\n+    MLIRContext& mlir_context,\n     std::optional<LegacyMatmulEmitter> legacy_matmul_emitter) {\n-  mlir::MLIRContext& mlir_context = *symbolic_expr_context.GetMLIRContext();\n   LoadMlirDialectsForTriton(mlir_context);\n   const auto debug_options = fusion->GetModule()->config().debug_options();\n \n@@ -2032,7 +2028,7 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n              fusion_kind == kTritonCollectiveFusionKind) {\n     TF_RETURN_IF_ERROR(EmitGeneric(b, emitter_specific_constraints_builder,\n                                    fusion, fn, block_level_parameters,\n-                                   &symbolic_expr_context));\n+                                   &mlir_context));\n   } else {\n     return Internal(\"Unsupported fusion kind: %s\", fusion_kind);\n   }"
        },
        {
            "sha": "cd0c6d05746fcb5df17fef063d0ba60166ff0c70",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -79,15 +79,15 @@ absl::StatusOr<TritonWrapperResult> TritonWrapper(\n     const se::GpuComputeCapability& cc,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    llvm::Module* llvm_module, SymbolicExprContext& symbolic_expr_context);\n+    llvm::Module* llvm_module, mlir::MLIRContext& mlir_context);\n \n // Creates the initial Triton module for the given fusion. Visible for testing,\n // use TritonWrapper instead.\n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n     absl::string_view fn_name, const HloFusionInstruction* fusion,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    SymbolicExprContext& symbolic_expr_context);\n+    mlir::MLIRContext& mlir_context);\n \n // Compiles a given Triton module to LLVM IR.\n // If `emit_kernels` is false, then the function skips emitting\n@@ -171,7 +171,7 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n     EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder,\n     const HloFusionInstruction* fusion,\n     const BlockLevelParameters& block_level_parameters,\n-    SymbolicExprContext& symbolic_expr_context,\n+    mlir::MLIRContext& mlir_context,\n     std::optional<LegacyMatmulEmitter> legacy_matmul_emitter = std::nullopt);\n \n // This function lowers the shared dialect module to Triton. It is exposed for"
        },
        {
            "sha": "496fc46714ded6761c590da15a195fa30085cc51",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_port_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 13,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -34,7 +34,6 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter.h\"\n #include \"xla/backends/gpu/codegen/triton/test_utils.h\"\n #include \"xla/error_spec.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -119,9 +118,9 @@ class TritonTest : public GpuCodegenTest {\n   GetModuleAndNestedFusionMetadata(absl::string_view hlo_text) {\n     TF_ASSIGN_OR_RETURN(std::unique_ptr<VerifiedHloModule> module,\n                         ParseAndReturnVerifiedModule(hlo_text));\n-    TF_ASSIGN_OR_RETURN(bool fusion_was_nested,\n-                        NestGemmFusion(device_desc(), &symbolic_expr_context_)\n-                            .Run(module.get()));\n+    TF_ASSIGN_OR_RETURN(\n+        bool fusion_was_nested,\n+        NestGemmFusion(device_desc(), &mlir_context_).Run(module.get()));\n     if (!fusion_was_nested) {\n       return absl::InternalError(\"Failed to nest the GEMM fusion.\");\n     }\n@@ -144,7 +143,6 @@ class TritonTest : public GpuCodegenTest {\n   }\n \n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n class TritonGemmTest : public TritonTest {\n@@ -477,7 +475,6 @@ TEST_F(TritonGemmTest, FailIfTooMuchShmem) {\n       TestGpuDeviceInfo::RTXA6000DeviceInfo();\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n-  mlir::MLIRContext mlir_context;\n \n   constexpr absl::string_view kHloTextTemplate = R\"(\n triton_gemm_dot {\n@@ -507,7 +504,7 @@ ENTRY entry {\n   EXPECT_THAT(\n       TritonWrapper(\"test_fn\", fusion1, se::GpuComputeCapability{cc},\n                     device_info, module1_and_metadata.block_level_parameters,\n-                    &llvm_module, symbolic_expr_context_),\n+                    &llvm_module, mlir_context_),\n       absl_testing::StatusIs(\n           tsl::error::RESOURCE_EXHAUSTED,\n           ::testing::HasSubstr(\"Shared memory size limit exceeded\")));\n@@ -523,7 +520,7 @@ ENTRY entry {\n       const auto result,\n       TritonWrapper(\"test_fn\", fusion2, se::GpuComputeCapability{cc},\n                     device_info, module2_and_metadata.block_level_parameters,\n-                    &llvm_module, symbolic_expr_context_));\n+                    &llvm_module, mlir_context_));\n   // Use optin shared memory which is > shared_memory_per_block.\n   EXPECT_GT(result.shmem_bytes, device_info.shared_memory_per_block());\n }\n@@ -821,7 +818,6 @@ TEST_F(TritonGemmTest, DISABLED_FailForTooComplexTiling) {\n       TestGpuDeviceInfo::RTXA6000DeviceInfo();\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n-  mlir::MLIRContext mlir_context;\n \n   constexpr absl::string_view kHloTextTemplate = R\"(\n HloModule module\n@@ -854,7 +850,7 @@ ENTRY entry {\n   EXPECT_THAT(\n       TritonWrapper(\"test_fn\", fusion1, se::GpuComputeCapability{cc},\n                     device_info, module1_and_metadata.block_level_parameters,\n-                    &llvm_module, symbolic_expr_context_),\n+                    &llvm_module, mlir_context_),\n       absl_testing::StatusIs(tsl::error::RESOURCE_EXHAUSTED,\n                              \"Tiling complexity heuristic exceeded\"));\n \n@@ -869,7 +865,7 @@ ENTRY entry {\n   TF_EXPECT_OK(TritonWrapper(\"test_fn\", fusion2, se::GpuComputeCapability{cc},\n                              device_info,\n                              module2_and_metadata.block_level_parameters,\n-                             &llvm_module, symbolic_expr_context_)\n+                             &llvm_module, mlir_context_)\n                    .status());\n }\n \n@@ -2008,14 +2004,13 @@ ENTRY e {\n       optin_shmem_module_and_metadata.computation->FusionInstruction());\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n-  mlir::MLIRContext mlir_context;\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       const auto result,\n       TritonWrapper(\"test_fn\", triton_dot_fusion, GpuComputeCapability(),\n                     dev_info,\n                     optin_shmem_module_and_metadata.block_level_parameters,\n-                    &llvm_module, symbolic_expr_context_));\n+                    &llvm_module, mlir_context_));\n   // The config is chosen so that the used memory size is slightly above the\n   // 48 kB boundary of standard / opt-in shared memory so that any GPU that\n   // has the opt-in one should be able to execute the test."
        },
        {
            "sha": "22efd30cebd8b89d344ad51d78cf0ff334fdb7cc",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_test.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 23,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -33,7 +33,6 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter.h\"\n #include \"xla/backends/gpu/codegen/triton/test_utils.h\"\n #include \"xla/error_spec.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -722,7 +721,6 @@ ENTRY entry {\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n \n   auto backend_config_or =\n       triton_dot_fusion->backend_config<GpuBackendConfig>();\n@@ -747,12 +745,12 @@ ENTRY entry {\n   block_level_parameters.num_stages = 4;\n   block_level_parameters.num_warps = 8;\n \n-  EXPECT_THAT(TritonWrapper(\"test_fn\", triton_dot_fusion, CudaAmpereOrRocm(),\n-                            dev_info, block_level_parameters, &llvm_module,\n-                            symbolic_expr_context),\n-              absl_testing::StatusIs(\n-                  tsl::error::RESOURCE_EXHAUSTED,\n-                  ::testing::HasSubstr(\"Shared memory size limit exceeded\")));\n+  EXPECT_THAT(\n+      TritonWrapper(\"test_fn\", triton_dot_fusion, CudaAmpereOrRocm(), dev_info,\n+                    block_level_parameters, &llvm_module, mlir_context),\n+      absl_testing::StatusIs(\n+          tsl::error::RESOURCE_EXHAUSTED,\n+          ::testing::HasSubstr(\"Shared memory size limit exceeded\")));\n \n   config.set_block_m(64);\n   config.set_block_n(128);\n@@ -763,8 +761,7 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(\n       const auto result,\n       TritonWrapper(\"test_fn\", triton_dot_fusion, CudaAmpereOrRocm(), dev_info,\n-                    block_level_parameters, &llvm_module,\n-                    symbolic_expr_context));\n+                    block_level_parameters, &llvm_module, mlir_context));\n   // Use optin shared memory which is > shared_memory_per_block.\n   EXPECT_GT(result.shmem_bytes, dev_info.shared_memory_per_block());\n }\n@@ -1319,7 +1316,6 @@ ENTRY entry {\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n \n   auto backend_config_or =\n       triton_dot_fusion->backend_config<GpuBackendConfig>();\n@@ -1343,12 +1339,12 @@ ENTRY entry {\n   block_level_parameters.num_ctas = 1;\n   block_level_parameters.num_stages = 1;\n   block_level_parameters.num_warps = 2;\n-  EXPECT_THAT(TritonWrapper(\"test_fn\", triton_dot_fusion, CudaAmpereOrRocm(),\n-                            dev_info, block_level_parameters, &llvm_module,\n-                            symbolic_expr_context),\n-              absl_testing::StatusIs(\n-                  tsl::error::RESOURCE_EXHAUSTED,\n-                  \"Tiling complexity heuristic exceeded: 147456 > 9000\"));\n+  EXPECT_THAT(\n+      TritonWrapper(\"test_fn\", triton_dot_fusion, CudaAmpereOrRocm(), dev_info,\n+                    block_level_parameters, &llvm_module, mlir_context),\n+      absl_testing::StatusIs(\n+          tsl::error::RESOURCE_EXHAUSTED,\n+          \"Tiling complexity heuristic exceeded: 147456 > 9000\"));\n \n   // Succeeds if the tiling is not too complex.\n   config.set_block_m(32);\n@@ -1358,7 +1354,7 @@ ENTRY entry {\n \n   TF_ASSERT_OK(TritonWrapper(\"test_fn\", triton_dot_fusion, CudaAmpereOrRocm(),\n                              dev_info, block_level_parameters, &llvm_module,\n-                             symbolic_expr_context)\n+                             mlir_context)\n                    .status());\n }\n \n@@ -1898,7 +1894,6 @@ ENTRY e  {\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto gpu_config, triton_dot_fusion->backend_config<GpuBackendConfig>());\n@@ -1911,7 +1906,7 @@ ENTRY e  {\n \n   TF_ASSERT_OK(TritonWrapper(\"test_fn\", triton_dot_fusion, GpuComputeComp(),\n                              dev_info, block_level_parameters, &llvm_module,\n-                             symbolic_expr_context)\n+                             mlir_context)\n                    .status());\n }\n \n@@ -3092,7 +3087,6 @@ ENTRY e {\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto gpu_config, triton_dot_fusion->backend_config<GpuBackendConfig>());\n@@ -3105,8 +3099,7 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(\n       const auto result,\n       TritonWrapper(\"test_fn\", triton_dot_fusion, GpuComputeComp(), dev_info,\n-                    block_level_parameters, &llvm_module,\n-                    symbolic_expr_context));\n+                    block_level_parameters, &llvm_module, mlir_context));\n   // The config is chosen so that the used memory size is slightly above the\n   // 48 kB boundary of standard / optin shared memory so that any GPU that\n   // has the optin one should be able to execute the test."
        },
        {
            "sha": "9cb0ff17fc99c42e19f9d4da12833d07080a3177",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 9,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -42,7 +42,6 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n #include \"xla/backends/gpu/codegen/triton/test_utils.h\"\n #include \"xla/error_spec.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -1554,14 +1553,13 @@ ENTRY entry {\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n \n   EXPECT_THAT(\n       TritonWrapper(\"test_fn\", triton_fusion,\n                     se::CudaComputeCapability{se::CudaComputeCapability::kVolta,\n                                               /*minor=*/0},\n                     dev_info, BlockLevelParameters(), &llvm_module,\n-                    symbolic_expr_context),\n+                    mlir_context),\n       absl_testing::StatusIs(\n           absl::StatusCode::kFailedPrecondition,\n           ::testing::HasSubstr(\"Triton support is only enabled for Ampere GPUs \"\n@@ -1613,7 +1611,6 @@ ENTRY entry_computation {\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n \n   BlockLevelParameters block_level_parameters;\n   block_level_parameters.output_tile_sizes = {{1024, 1}};\n@@ -1624,8 +1621,7 @@ ENTRY entry_computation {\n   // 1048576.\n   EXPECT_THAT(\n       TritonWrapper(\"test_fn\", triton_fusion, compute_capability, dev_info,\n-                    block_level_parameters, &llvm_module,\n-                    symbolic_expr_context),\n+                    block_level_parameters, &llvm_module, mlir_context),\n       absl_testing::StatusIs(\n           absl::StatusCode::kInvalidArgument,\n           ::testing::HasSubstr(\"Tiling does not satisfy constraints.\")));\n@@ -4451,7 +4447,6 @@ TEST_F(TritonEmitterTest, RocmWarpSizeIsSetCorrectly) {\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n   std::vector<std::string> paths;\n   std::string triton_passes_log;\n \n@@ -4461,7 +4456,7 @@ TEST_F(TritonEmitterTest, RocmWarpSizeIsSetCorrectly) {\n   TF_ASSERT_OK(TritonWrapper(\n       \"test_fn\", triton_fusion,\n       se::GpuComputeCapability{se::RocmComputeCapability(\"gfx942\")}, dev_info,\n-      BlockLevelParameters(), &llvm_module, symbolic_expr_context));\n+      BlockLevelParameters(), &llvm_module, mlir_context));\n   TF_EXPECT_OK(tsl::Env::Default()->GetMatchingPaths(\n       tsl::io::JoinPath(output_directory, \"*.triton-passes.log\"), &paths));\n   EXPECT_EQ(paths.size(), 1);\n@@ -4478,7 +4473,7 @@ TEST_F(TritonEmitterTest, RocmWarpSizeIsSetCorrectly) {\n   TF_ASSERT_OK(TritonWrapper(\n       \"test_fn\", triton_fusion,\n       se::GpuComputeCapability{se::RocmComputeCapability(\"gfx1100\")},\n-      dev_info_n, BlockLevelParameters(), &llvm_module, symbolic_expr_context));\n+      dev_info_n, BlockLevelParameters(), &llvm_module, mlir_context));\n   TF_EXPECT_OK(tsl::Env::Default()->GetMatchingPaths(\n       tsl::io::JoinPath(output_directory, \"*.triton-passes.log\"), &paths));\n   EXPECT_EQ(paths.size(), 1);"
        },
        {
            "sha": "a4691bb08da2075a193aeff9550eb0f353dfe5f8",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_deviceless_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -25,7 +25,6 @@ limitations under the License.\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/testlib/filecheck.h\"\n@@ -115,7 +114,6 @@ ENTRY e {\n       module->entry_computation()->root_instruction());\n \n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto triton_module,\n       CreateTritonModule(\"triton_fn\", fusion,\n@@ -124,7 +122,7 @@ ENTRY e {\n                              fusion->backend_config<GpuBackendConfig>()\n                                  ->fusion_backend_config()\n                                  .block_level_fusion_config()),\n-                         symbolic_expr_context));\n+                         mlir_context));\n \n   std::string annotated_ir = DumpTritonIR(triton_module.get(), true);\n \n@@ -167,7 +165,6 @@ ENTRY entry {\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n \n   BlockLevelParameters block_level_parameters;\n   block_level_parameters.output_tile_sizes = {{1, 1}};\n@@ -176,8 +173,7 @@ ENTRY entry {\n   EXPECT_THAT(TritonWrapper(\n                   \"test_fn\", triton_fusion,\n                   se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n-                  dev_info, block_level_parameters, &llvm_module,\n-                  symbolic_expr_context),\n+                  dev_info, block_level_parameters, &llvm_module, mlir_context),\n               absl_testing::StatusIs(\n                   absl::StatusCode::kFailedPrecondition,\n                   ::testing::HasSubstr(\n@@ -249,15 +245,14 @@ ENTRY entry {\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n \n   EXPECT_OK(\n       CreateTritonModule(\"test_fn\", triton_fusion, dev_info,\n                          BlockLevelParameters::FromBlockLevelFusionConfig(\n                              triton_fusion->backend_config<GpuBackendConfig>()\n                                  ->fusion_backend_config()\n                                  .block_level_fusion_config()),\n-                         symbolic_expr_context));\n+                         mlir_context));\n }\n \n TEST_F(WarpSpecializationTritonEmitterTest,\n@@ -323,7 +318,6 @@ ENTRY entry {\n   llvm::LLVMContext llvm_ctx;\n   llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n   TF_ASSERT_OK_AND_ASSIGN(\n       TritonWrapperResult result,\n       TritonWrapper(\"test_fn\", fusion, se::CudaComputeCapability::Blackwell(),\n@@ -332,7 +326,7 @@ ENTRY entry {\n                         fusion->backend_config<GpuBackendConfig>()\n                             ->fusion_backend_config()\n                             .block_level_fusion_config()),\n-                    &llvm_module, symbolic_expr_context));\n+                    &llvm_module, mlir_context));\n \n   // Warp specialization influences the total number of threads we end up\n   // using. Usually we would expect num_warps * warp_size threads per block, but"
        },
        {
            "sha": "6f8e467b719b8dd16aa0d6c301a0449f50d0de80",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_int4_device_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_int4_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_int4_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_int4_device_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -26,7 +26,6 @@ limitations under the License.\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/error_spec.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/testlib/filecheck.h\"\n@@ -61,7 +60,6 @@ class TritonTest : public GpuCodegenTest {\n     return backend().default_stream_executor()->GetDeviceDescription();\n   }\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n // The following tests are for the channel and subchannel dequantization"
        },
        {
            "sha": "53a969bbb6628196f8aa70b4aab987d0748dd006",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_shared_dialect_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -20,7 +20,7 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/test_utils.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n-#include \"xla/tests/hlo_test_base_with_symbolic_expr_context.h\"\n+#include \"xla/tests/hlo_test_base_with_mlir_context.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n@@ -37,7 +37,7 @@ namespace {\n // emitter becomes a reality.\n // *****************************************************************************\n \n-using XTileDialectTest = HloTestBaseWithSymbolicExprContext;\n+using XTileDialectTest = HloTestBaseWithMLIRContext;\n \n TEST_F(XTileDialectTest, HloTransposeIsLoweredToStableHloTranspose) {\n   constexpr absl::string_view kHloText = R\"("
        },
        {
            "sha": "d5af81b56c90bd55904e7df851fd229c5e332b07",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_stub.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -25,7 +25,6 @@ limitations under the License.\n #include \"mlir/IR/OwningOpRef.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_clone_context.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/hlo_module_config.h\"\n@@ -50,15 +49,15 @@ absl::StatusOr<TritonWrapperResult> TritonWrapper(\n     const se::GpuComputeCapability& cc,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    llvm::Module* llvm_module, SymbolicExprContext& symbolic_expr_context) {\n+    llvm::Module* llvm_module, mlir::MLIRContext& mlir_context) {\n   return absl::UnimplementedError(\"not supported for this build configuration\");\n }\n \n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n     absl::string_view fn_name, const HloFusionInstruction* fusion,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    SymbolicExprContext& symbolic_expr_context) {\n+    mlir::MLIRContext& mlir_context) {\n   return absl::UnimplementedError(\"not supported for this build configuration\");\n }\n "
        },
        {
            "sha": "cda6f8284337c32fe4a1778a13f749924a29c6d1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_stub_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -21,7 +21,6 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter_legacy_matmul.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/codegen/tiling/tiled_hlo_instruction.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n@@ -34,14 +33,11 @@ namespace {\n \n TEST(TritonStub, CallStubApi) {\n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n \n   LoadMlirDialectsForTriton(mlir_context);\n   EXPECT_FALSE(\n-      TritonWrapper({}, nullptr, {}, {}, {}, nullptr, symbolic_expr_context)\n-          .ok());\n-  EXPECT_FALSE(\n-      CreateTritonModule({}, nullptr, {}, {}, symbolic_expr_context).ok());\n+      TritonWrapper({}, nullptr, {}, {}, {}, nullptr, mlir_context).ok());\n+  EXPECT_FALSE(CreateTritonModule({}, nullptr, {}, {}, mlir_context).ok());\n   EXPECT_FALSE(CompileTritonToLLVM(\"\", HloModule(\"test\", HloModuleConfig()), {},\n                                    {}, {}, nullptr, mlir_context,\n                                    /*is_xla_fusion=*/true, {})"
        },
        {
            "sha": "ab44a95362252e1e2f9849f0d0eec5bae7458c78",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -68,9 +68,8 @@ ENTRY entry_computation {\n   HloFusionAnalysis analysis = HloFusionAnalysis::Create(*root, device_info);\n \n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n-  std::unique_ptr<FusionInterface> emitter = GetFusionEmitter(\n-      PreBufferAssignmentFusionInfo{analysis}, &symbolic_expr_context);\n+  std::unique_ptr<FusionInterface> emitter =\n+      GetFusionEmitter(PreBufferAssignmentFusionInfo{analysis}, &mlir_context);\n   auto triton_fusion = dynamic_cast<TritonFusion*>(emitter.get());\n   ASSERT_NE(triton_fusion, nullptr);\n   std::optional<TritonFusion::LaunchConfig> launch_config =\n@@ -107,17 +106,16 @@ ENTRY entry_computation {\n   HloFusionAnalysis analysis = HloFusionAnalysis::Create(*root, device_info);\n \n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n-  std::unique_ptr<FusionInterface> emitter = GetFusionEmitter(\n-      PreBufferAssignmentFusionInfo{analysis}, &symbolic_expr_context);\n+  std::unique_ptr<FusionInterface> emitter =\n+      GetFusionEmitter(PreBufferAssignmentFusionInfo{analysis}, &mlir_context);\n   auto triton_fusion_emitter = dynamic_cast<TritonFusion*>(emitter.get());\n   ASSERT_NE(triton_fusion_emitter, nullptr);\n   EXPECT_EQ(triton_fusion_emitter->GetLaunchConfig(), std::nullopt);\n \n   // Ensure that the emitter fails gracefully when the launch config is not set.\n   EXPECT_THAT(triton_fusion_emitter->GenerateTritonKernelAndWrapper(\n                   *::xla::Cast<HloFusionInstruction>(root), \"random_name\",\n-                  device_info, /*llvm_module=*/nullptr, &symbolic_expr_context),\n+                  device_info, /*llvm_module=*/nullptr, &mlir_context),\n               absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n }\n \n@@ -147,9 +145,8 @@ ENTRY entry_computation {\n   HloFusionAnalysis analysis = HloFusionAnalysis::Create(*root, device_info);\n \n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n-  std::unique_ptr<FusionInterface> emitter = GetFusionEmitter(\n-      PreBufferAssignmentFusionInfo{analysis}, &symbolic_expr_context);\n+  std::unique_ptr<FusionInterface> emitter =\n+      GetFusionEmitter(PreBufferAssignmentFusionInfo{analysis}, &mlir_context);\n   auto triton_fusion = dynamic_cast<TritonFusion*>(emitter.get());\n \n   ASSERT_NE(triton_fusion, nullptr);"
        },
        {
            "sha": "5fe2cc82aff6ee566ba88e9429f66e0b23d43d65",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_legacy_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -142,7 +142,7 @@ ENTRY e {\n       EXPECT_THAT(\n           TritonWrapper(\"test_fn\", &ti.TritonFusion(), GetComputeCapability(),\n                         dev_info, block_level_parameters, &llvm_module_,\n-                        symbolic_expr_context_),\n+                        mlir_context_),\n           absl_testing::StatusIs(\n               absl::StatusCode::kInternal,\n               ::testing::HasSubstr(\"Failed to compile Triton kernel\")));\n@@ -470,7 +470,7 @@ ENTRY e {\n               .block_level_fusion_config());\n   TF_EXPECT_OK(TritonWrapper(\n       \"test_fn\", &ti.TritonFusion(), GetComputeCapability(), dev_info,\n-      block_level_parameters, &llvm_module_, symbolic_expr_context_));\n+      block_level_parameters, &llvm_module_, mlir_context_));\n }\n \n TEST_F(TritonSupportTestBase,"
        },
        {
            "sha": "4e5d2cddb014da33a6a773c007ff2c734affb1b4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -270,7 +270,7 @@ class TritonSupportTest : public TritonSupportTestBase {\n     auto run_triton_codegen = [&]() {\n       return TritonWrapper(\"test_fn\", &ti.TritonFusion(), cc, dev_info,\n                            block_level_parameters, &llvm_module_,\n-                           symbolic_expr_context_);\n+                           mlir_context_);\n     };\n \n     if (IsTritonSupportedInstruction(ti.Instruction(), cc)) {"
        },
        {
            "sha": "e839c3a2e967fbf9660fd4fe414a409bd09dd7e0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/test_utils.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 19,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -58,7 +58,7 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tests/hlo_test_base.h\"\n-#include \"xla/tests/hlo_test_base_with_symbolic_expr_context.h\"\n+#include \"xla/tests/hlo_test_base_with_mlir_context.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla.pb.h\"\n@@ -120,12 +120,11 @@ absl::Status CreateTritonIrAndFileCheck(\n   auto* fusion = Cast<HloFusionInstruction>(computation.FusionInstruction());\n \n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n   TF_ASSIGN_OR_RETURN(\n       mlir::OwningOpRef<mlir::ModuleOp> triton_module,\n       CreateTritonModule(\"triton_fn\", fusion,\n                          TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n-                         block_level_parameters, symbolic_expr_context));\n+                         block_level_parameters, mlir_context));\n \n   std::string out;\n   llvm::raw_string_ostream os(out);\n@@ -139,7 +138,7 @@ absl::Status CreateTritonIrAndFileCheck(\n \n absl::StatusOr<\n     std::pair<mlir::OwningOpRef<mlir::ModuleOp>, std::unique_ptr<HloModule>>>\n-CreateXTileIrAndFileCheck(HloTestBaseWithSymbolicExprContext* test,\n+CreateXTileIrAndFileCheck(HloTestBaseWithMLIRContext* test,\n                           absl::string_view hlo_text,\n                           absl::string_view triton_fusion_name,\n                           absl::string_view filecheck_pattern) {\n@@ -162,20 +161,19 @@ CreateXTileIrAndFileCheck(HloTestBaseWithSymbolicExprContext* test,\n }\n \n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateXTileIrAndFileCheck(\n-    HloTestBaseWithSymbolicExprContext* test, const HloComputation& computation,\n+    HloTestBaseWithMLIRContext* test, const HloComputation& computation,\n     const BlockLevelParameters& block_level_parameters,\n     absl::string_view filecheck_pattern) {\n   auto* fusion = Cast<HloFusionInstruction>(computation.FusionInstruction());\n \n-  TF_ASSIGN_OR_RETURN(\n-      mlir::OwningOpRef<mlir::ModuleOp> xtile_dialect_module,\n-      ir_emitter_triton_internal::EmitXTileModule(\n-          \"xtile_dialect_fn\",\n-          TritonEmitterConstraints::GetBuilder(\n-              TestGpuDeviceInfo::RTXA6000DeviceInfo()),\n-          fusion, block_level_parameters, *test->symbolic_expr_context(),\n-          ir_emitter_triton_internal::LegacyMatmulEmitter(\n-              TestGpuDeviceInfo::RTXA6000DeviceInfo())));\n+  TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> xtile_dialect_module,\n+                      ir_emitter_triton_internal::EmitXTileModule(\n+                          \"xtile_dialect_fn\",\n+                          TritonEmitterConstraints::GetBuilder(\n+                              TestGpuDeviceInfo::RTXA6000DeviceInfo()),\n+                          fusion, block_level_parameters, *test->mlir_context(),\n+                          ir_emitter_triton_internal::LegacyMatmulEmitter(\n+                              TestGpuDeviceInfo::RTXA6000DeviceInfo())));\n \n   std::string out;\n   llvm::raw_string_ostream os(out);\n@@ -188,12 +186,11 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateXTileIrAndFileCheck(\n }\n \n absl::Status LowerXTileIrToTritonAndFileCheck(\n-    HloTestBaseWithSymbolicExprContext* test,\n-    mlir::ModuleOp xtile_dialect_module, absl::string_view filecheck_pattern,\n-    const HloFusionInstruction& fusion) {\n+    HloTestBaseWithMLIRContext* test, mlir::ModuleOp xtile_dialect_module,\n+    absl::string_view filecheck_pattern, const HloFusionInstruction& fusion) {\n   TF_RETURN_IF_ERROR(ir_emitter_triton_internal::LowerXTileToTriton(\n-      xtile_dialect_module, *test->symbolic_expr_context()->GetMLIRContext(),\n-      fusion, TestGpuDeviceInfo::RTXH100SXMDeviceInfo()));\n+      xtile_dialect_module, *test->mlir_context(), fusion,\n+      TestGpuDeviceInfo::RTXH100SXMDeviceInfo()));\n \n   std::string out;\n   llvm::raw_string_ostream os(out);"
        },
        {
            "sha": "7688047809bf435749855803b4822eea818a1b59",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/test_utils.h",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -42,7 +42,7 @@ limitations under the License.\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tests/hlo_test_base.h\"\n-#include \"xla/tests/hlo_test_base_with_symbolic_expr_context.h\"\n+#include \"xla/tests/hlo_test_base_with_mlir_context.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -74,7 +74,7 @@ absl::Status CreateTritonIrAndFileCheck(\n // `filecheck_pattern`.\n absl::StatusOr<\n     std::pair<mlir::OwningOpRef<mlir::ModuleOp>, std::unique_ptr<HloModule>>>\n-CreateXTileIrAndFileCheck(HloTestBaseWithSymbolicExprContext* test,\n+CreateXTileIrAndFileCheck(HloTestBaseWithMLIRContext* test,\n                           absl::string_view hlo_text,\n                           absl::string_view triton_fusion_name,\n                           absl::string_view filecheck_pattern);\n@@ -83,16 +83,15 @@ CreateXTileIrAndFileCheck(HloTestBaseWithSymbolicExprContext* test,\n // This function also checks the generated shared dialect IR against the\n // `filecheck_pattern`.\n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateXTileIrAndFileCheck(\n-    HloTestBaseWithSymbolicExprContext* test, const HloComputation& computation,\n+    HloTestBaseWithMLIRContext* test, const HloComputation& computation,\n     const BlockLevelParameters& block_level_parameters,\n     absl::string_view filecheck_pattern);\n \n // Lowers the given shared dialect IR to Triton IR and checks the result against\n // the `filecheck_pattern`.\n absl::Status LowerXTileIrToTritonAndFileCheck(\n-    HloTestBaseWithSymbolicExprContext* test,\n-    mlir::ModuleOp xtile_dialect_module, absl::string_view filecheck_pattern,\n-    const HloFusionInstruction& fusion);\n+    HloTestBaseWithMLIRContext* test, mlir::ModuleOp xtile_dialect_module,\n+    absl::string_view filecheck_pattern, const HloFusionInstruction& fusion);\n \n absl::Status CreateTritonIrAndFileCheckForDot(\n     HloTestBase* test, absl::string_view hlo_text,\n@@ -168,7 +167,6 @@ class TritonSupportTestBase : public HloTestBase {\n   llvm::LLVMContext llvm_ctx_;\n   llvm::Module llvm_module_{\"module\", llvm_ctx_};\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n   TritonGemmConfig config_{16, 32, 512, 1, 4, 8};\n };\n "
        },
        {
            "sha": "1edb1056e039a6c407938bf225eef7bd875b18d9",
            "filename": "third_party/xla/xla/codegen/emitter_loc_op_builder_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitter_loc_op_builder_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitter_loc_op_builder_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitter_loc_op_builder_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -48,7 +48,6 @@ class EmitterLocOpBuilderTest : public HloHardwareIndependentTestBase {\n   void SetUp() override { gpu::LoadMlirDialectsForTriton(mlir_context_); }\n \n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n NameLoc NameLoc(mlir::MLIRContext& context, absl::string_view name) {"
        },
        {
            "sha": "a610c928102591439a03affb5a0754b1597b981b",
            "filename": "third_party/xla/xla/codegen/emitters/computation_partitioner.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 20,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fcomputation_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fcomputation_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fcomputation_partitioner.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -59,6 +59,8 @@ namespace xla {\n namespace emitters {\n namespace {\n \n+using ::mlir::MLIRContext;\n+\n int Arity(const Shape& shape) {\n   return shape.IsTuple() ? shape.tuple_shapes().size() : 1;\n }\n@@ -68,7 +70,7 @@ const Shape& TupleShape(const Shape& shape, int index) {\n }\n \n std::vector<IndexingMapSet> ComputeOperandIndexingMaps(\n-    const HloInstruction* instr, SymbolicExprContext* symbolic_expr_context) {\n+    const HloInstruction* instr, MLIRContext* mlir_context) {\n   std::vector<IndexingMapSet> indexing_maps_per_operand;\n   // For some ops, there is no indexing map implemented for the operands (e.g.\n   // scatter) or there are multiple results and the common iteration space is\n@@ -78,12 +80,12 @@ std::vector<IndexingMapSet> ComputeOperandIndexingMaps(\n     int64_t num_operands = instr->operand_count();\n     indexing_maps_per_operand.reserve(num_operands);\n     for (int64_t i = 0; i < num_operands; ++i) {\n-      indexing_maps_per_operand.push_back({CreateIdentityMap(\n-          instr->operand(i)->shape(), symbolic_expr_context)});\n+      indexing_maps_per_operand.push_back(\n+          {CreateIdentityMap(instr->operand(i)->shape(), mlir_context)});\n     }\n   } else {\n-    auto operands_indexing = ComputeOutputToInputIndexing(\n-        instr, /*output_id=*/0, symbolic_expr_context);\n+    auto operands_indexing =\n+        ComputeOutputToInputIndexing(instr, /*output_id=*/0, mlir_context);\n     operands_indexing.Simplify();\n     indexing_maps_per_operand.reserve(operands_indexing.indexing_maps.size());\n     for (auto& indexing_maps : operands_indexing.indexing_maps) {\n@@ -105,15 +107,15 @@ bool HasNoCompute(const HloInstruction* instr) {\n \n EpilogueSpecification EpilogueSpecification::FromIdentityIndexing(\n     const HloInstruction* hero, const HloInstruction* root,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   EpilogueSpecification result;\n   if (root->shape().IsArray()) {\n     absl::c_copy(root->shape().dimensions(),\n                  std::back_inserter(result.index_ranges));\n   }\n   result.roots.push_back(root);\n   result.root_indexing.push_back(\n-      CreateIdentityMap(root->shape(), symbolic_expr_context));\n+      CreateIdentityMap(root->shape(), mlir_context));\n   result.heroes.push_back(hero);\n   return result;\n }\n@@ -204,8 +206,7 @@ struct HloSubgraphData {\n };\n \n PartitionedComputation::PartitionedComputation(\n-    const HloComputation* computation,\n-    SymbolicExprContext* symbolic_expr_context,\n+    const HloComputation* computation, MLIRContext* mlir_context,\n     std::function<bool(const HloInstruction*)> is_subgraph_root)\n     : computation_(computation) {\n   CHECK_NE(computation, nullptr);\n@@ -253,8 +254,7 @@ PartitionedComputation::PartitionedComputation(\n       instr_subgraph_data.indexings.clear();\n       num_ops_per_subgraph.push_back(1);\n     }\n-    auto operands_indexing =\n-        ComputeOperandIndexingMaps(instr, symbolic_expr_context);\n+    auto operands_indexing = ComputeOperandIndexingMaps(instr, mlir_context);\n     // Iterate over the operands and add the func_ids of the current instruction\n     // to their HloSubgraphIndexing and compute the indexing maps.\n     for (auto [operand_instr, operand_maps] :\n@@ -309,17 +309,16 @@ PartitionedComputation::PartitionedComputation(\n             root_indexing.push_back(root_indexing.front());\n           } else {\n             // Bitcast from the first root to the target shape.\n-            root_indexing.push_back(GetBitcastMap(*first_root_shape,\n-                                                  instruction->shape(),\n-                                                  symbolic_expr_context));\n+            root_indexing.push_back(GetBitcastMap(\n+                *first_root_shape, instruction->shape(), mlir_context));\n           }\n         } else {\n           first_root_shape = &instruction->shape();\n           while (first_root_shape->IsTuple()) {\n             first_root_shape = &first_root_shape->tuple_shapes()[0];\n           }\n           root_indexing.push_back(\n-              CreateIdentityMap(*first_root_shape, symbolic_expr_context));\n+              CreateIdentityMap(*first_root_shape, mlir_context));\n         }\n       }\n     }\n@@ -392,15 +391,17 @@ PartitionedComputation::Subgraph PartitionedComputation::Subgraph::ForEpilogue(\n }\n \n PartitionedComputations::PartitionedComputations(\n-    const HloComputation* fusion, SymbolicExprContext* symbolic_expr_context,\n+    const HloComputation* fusion, MLIRContext* mlir_context,\n     std::vector<EpilogueSpecification> epilogues)\n-    : fusion_(fusion), symbolic_expr_context_(symbolic_expr_context) {\n+    : fusion_(fusion), mlir_context_(mlir_context) {\n   // Collect all transitively called computations (including the fusion itself).\n   absl::flat_hash_set<const HloComputation*> seen;\n   std::vector<const HloComputation*> computations;\n   std::function<void(const HloComputation*)> visit;\n   visit = [&](const HloComputation* computation) {\n-    if (!seen.insert(computation).second) return;\n+    if (!seen.insert(computation).second) {\n+      return;\n+    }\n     computations.push_back(computation);\n     for (auto* instr : computation->instructions()) {\n       absl::c_for_each(instr->called_computations(), visit);\n@@ -426,8 +427,8 @@ PartitionedComputations::PartitionedComputations(\n   partitioned_computations_.reserve(computations.size());\n   for (auto* computation : computations) {\n     computation_to_partitioning_[computation] =\n-        &partitioned_computations_.emplace_back(PartitionedComputation{\n-            computation, symbolic_expr_context, is_root});\n+        &partitioned_computations_.emplace_back(\n+            PartitionedComputation{computation, mlir_context, is_root});\n   }\n }\n "
        },
        {
            "sha": "8fd113f43602da2dc778d0baaf7c3a5dcf4a0269",
            "filename": "third_party/xla/xla/codegen/emitters/computation_partitioner.h",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fcomputation_partitioner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fcomputation_partitioner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fcomputation_partitioner.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -40,7 +40,7 @@ struct EpilogueSpecification {\n   // Creates an epilogue with output indices matching the given root's shape.\n   static EpilogueSpecification FromIdentityIndexing(\n       const HloInstruction* hero, const HloInstruction* root,\n-      SymbolicExprContext* symbolic_expr_context);\n+      mlir::MLIRContext* mlir_context);\n \n   std::vector<const HloInstruction*> heroes;\n   std::vector<const HloInstruction*> roots;\n@@ -82,7 +82,7 @@ struct EpilogueSpecification {\n class PartitionedComputation {\n  public:\n   explicit PartitionedComputation(const HloComputation* computation,\n-                                  SymbolicExprContext* symbolic_expr_context,\n+                                  mlir::MLIRContext* mlir_context,\n                                   std::function<bool(const HloInstruction*)>\n                                       is_subgraph_root = HloPredicateFalse);\n \n@@ -154,7 +154,7 @@ class PartitionedComputations {\n   // Partition the given fusion computation and optionally generate an epilogue\n   // for the given heroes.\n   explicit PartitionedComputations(\n-      const HloComputation* fusion, SymbolicExprContext* symbolic_expr_context,\n+      const HloComputation* fusion, mlir::MLIRContext* mlir_context,\n       std::vector<EpilogueSpecification> epilogues = {});\n \n   const PartitionedComputation& FindPartitionedComputation(\n@@ -177,9 +177,7 @@ class PartitionedComputations {\n \n   const HloComputation* fusion() const { return fusion_; }\n \n-  SymbolicExprContext* symbolic_expr_context() const {\n-    return symbolic_expr_context_;\n-  }\n+  mlir::MLIRContext* mlir_context() const { return mlir_context_; }\n \n   // Creates a call target lookup function for use with SubgraphToMlir.\n   CallTargetProvider CreateCallTargetProvider(\n@@ -198,7 +196,7 @@ class PartitionedComputations {\n       computation_to_partitioning_;\n   const HloComputation* fusion_;\n   std::vector<PartitionedComputation::Subgraph> epilogues_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n // Returns an MLIR function declaration for the given subgraph. For subgraphs of"
        },
        {
            "sha": "39ef286b2d089c9e5e237378c729eee90a5968c8",
            "filename": "third_party/xla/xla/codegen/emitters/computation_partitioner_test.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 19,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fcomputation_partitioner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fcomputation_partitioner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fcomputation_partitioner_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -40,12 +40,11 @@ using ::testing::UnorderedElementsAre;\n \n class ComputationPartitionerTest : public HloHardwareIndependentTestBase {\n  protected:\n-  ComputationPartitionerTest() : symbolic_expr_context_(&mlir_context_) {\n+  ComputationPartitionerTest() {\n     mlir_context_.loadDialect<mlir::func::FuncDialect>();\n   }\n \n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_;\n };\n \n std::string PrintAndErase(mlir::func::FuncOp func) {\n@@ -79,7 +78,7 @@ TEST_F(ComputationPartitionerTest, PartitionDiamonds) {\n \n   auto* fusion = module->GetComputationWithName(\"fused_computation\");\n   ASSERT_NE(fusion, nullptr);\n-  PartitionedComputation computation(fusion, &symbolic_expr_context_);\n+  PartitionedComputation computation(fusion, &mlir_context_);\n \n   constexpr auto kExpected = R\"(PartitionedComputation fused_computation:\n       SUBGRAPH fused_computation_add3 {\n@@ -122,7 +121,7 @@ TEST_F(ComputationPartitionerTest, SimpleConcatenate) {\n \n   auto* fusion = module->GetComputationWithName(\"fused_computation\");\n   ASSERT_NE(fusion, nullptr);\n-  PartitionedComputation computation(fusion, &symbolic_expr_context_);\n+  PartitionedComputation computation(fusion, &mlir_context_);\n \n   EXPECT_THAT(computation.subgraphs(), SizeIs(1));\n }\n@@ -143,7 +142,7 @@ TEST_F(ComputationPartitionerTest, DiamondConcatenate) {\n \n   auto* fusion = module->GetComputationWithName(\"fused_computation\");\n   ASSERT_NE(fusion, nullptr);\n-  PartitionedComputation computation(fusion, &symbolic_expr_context_);\n+  PartitionedComputation computation(fusion, &mlir_context_);\n \n   constexpr auto kExpected = R\"(PartitionedComputation fused_computation:\n       SUBGRAPH fused_computation_concat {\n@@ -174,7 +173,7 @@ TEST_F(ComputationPartitionerTest, TupleRoot) {\n \n   auto* fusion = module->GetComputationWithName(\"fused_computation\");\n   ASSERT_NE(fusion, nullptr);\n-  PartitionedComputation computation(fusion, &symbolic_expr_context_);\n+  PartitionedComputation computation(fusion, &mlir_context_);\n   constexpr auto kExpected = R\"(PartitionedComputation fused_computation:\n       SUBGRAPH fused_computation_root {\n         %p0 = f32[6]{0} parameter(0)\n@@ -217,9 +216,8 @@ TEST_F(ComputationPartitionerTest, Epilogue) {\n       /*index_ranges=*/{1, 42},\n       {CreateIdentityMap(\n           fused_computation->root_instruction()->shape().tuple_shapes(0),\n-          &symbolic_expr_context_)}};\n-  PartitionedComputations fusion(fused_computation, &symbolic_expr_context_,\n-                                 {epilogue});\n+          &mlir_context_)}};\n+  PartitionedComputations fusion(fused_computation, &mlir_context_, {epilogue});\n \n   mlir::ImplicitLocOpBuilder builder(mlir::UnknownLoc::get(&mlir_context_),\n                                      &mlir_context_);\n@@ -248,7 +246,7 @@ TEST_F(ComputationPartitionerTest, TransposeAsRoot) {\n   auto* fusion = module->GetComputationWithName(\"fused_computation\");\n   ASSERT_NE(fusion, nullptr);\n   PartitionedComputation computation(\n-      fusion, &symbolic_expr_context_, [](const HloInstruction* instr) {\n+      fusion, &mlir_context_, [](const HloInstruction* instr) {\n         return instr->opcode() == HloOpcode::kTranspose;\n       });\n   ASSERT_THAT(computation.subgraphs(), SizeIs(2));\n@@ -270,7 +268,7 @@ TEST_F(ComputationPartitionerTest, TransposeReverse) {\n   auto* fusion = module->GetComputationWithName(\"fused_computation\");\n   ASSERT_NE(fusion, nullptr);\n   PartitionedComputation computation(\n-      fusion, &symbolic_expr_context_, [](const HloInstruction* instr) {\n+      fusion, &mlir_context_, [](const HloInstruction* instr) {\n         return instr->opcode() == HloOpcode::kTranspose;\n       });\n   constexpr auto kExpected = R\"(PartitionedComputation fused_computation:\n@@ -299,7 +297,7 @@ TEST_F(ComputationPartitionerTest, PartiallyMergable) {\n \n   auto* fusion = module->GetComputationWithName(\"fused_computation\");\n   ASSERT_NE(fusion, nullptr);\n-  PartitionedComputation computation(fusion, &symbolic_expr_context_);\n+  PartitionedComputation computation(fusion, &mlir_context_);\n \n   auto transpose = fusion->GetInstructionWithName(\"transpose\");\n   auto sub = fusion->GetInstructionWithName(\"sub\");\n@@ -340,7 +338,7 @@ TEST_F(ComputationPartitionerTest, SubgraphSignatures) {\n   mlir::ImplicitLocOpBuilder builder(mlir::UnknownLoc::get(&context), &context);\n \n   PartitionedComputation fusion(module->GetComputationWithName(\"fusion\"),\n-                                &symbolic_expr_context_);\n+                                &mlir_context_);\n   EXPECT_EQ(\n       PrintAndErase(\n           CreateSubgraphMlirFunction(fusion.GetRootSubgraph(), builder)),\n@@ -349,7 +347,7 @@ TEST_F(ComputationPartitionerTest, SubgraphSignatures) {\n       \"index]}) -> f32\");\n \n   PartitionedComputation add(module->GetComputationWithName(\"add\"),\n-                             &symbolic_expr_context_);\n+                             &mlir_context_);\n   EXPECT_EQ(\n       PrintAndErase(CreateSubgraphMlirFunction(add.GetRootSubgraph(), builder)),\n       \"func.func private @add_add(f32, f32) -> f32\");\n@@ -379,7 +377,7 @@ TEST_F(ComputationPartitionerTest, ConcatWithTuple) {\n   mlir::ImplicitLocOpBuilder builder(mlir::UnknownLoc::get(&context), &context);\n \n   PartitionedComputation fusion(module->GetComputationWithName(\"fusion\"),\n-                                &symbolic_expr_context_);\n+                                &mlir_context_);\n   EXPECT_THAT(fusion.subgraphs(), SizeIs(2));\n   PrintAndErase(CreateSubgraphMlirFunction(fusion.GetRootSubgraph(), builder));\n }\n@@ -402,7 +400,7 @@ TEST_F(ComputationPartitionerTest, DUS) {\n   mlir::ImplicitLocOpBuilder builder(mlir::UnknownLoc::get(&context), &context);\n \n   PartitionedComputation fusion(module->GetComputationWithName(\"fusion\"),\n-                                &symbolic_expr_context_);\n+                                &mlir_context_);\n   EXPECT_THAT(fusion.subgraphs(), SizeIs(1));\n   PrintAndErase(CreateSubgraphMlirFunction(fusion.GetRootSubgraph(), builder));\n }\n@@ -440,7 +438,7 @@ TEST_F(ComputationPartitionerTest, ScatterFusion) {\n   mlir::ImplicitLocOpBuilder builder(mlir::UnknownLoc::get(&context), &context);\n \n   PartitionedComputation fusion(module->GetComputationWithName(\"fusion\"),\n-                                &symbolic_expr_context_);\n+                                &mlir_context_);\n   EXPECT_THAT(fusion.subgraphs(), SizeIs(1));\n   PrintAndErase(CreateSubgraphMlirFunction(fusion.GetRootSubgraph(), builder));\n }\n@@ -483,7 +481,7 @@ TEST_F(ComputationPartitionerTest, PartitioningIsDeterministic) {\n \n   auto* fusion = module->GetComputationWithName(\"fused_computation\");\n   ASSERT_NE(fusion, nullptr);\n-  PartitionedComputation computation(fusion, &symbolic_expr_context_);\n+  PartitionedComputation computation(fusion, &mlir_context_);\n   EXPECT_EQ(computation.subgraphs().size(), 1);\n }\n \n@@ -500,7 +498,7 @@ TEST_F(ComputationPartitionerTest, ScaleAndTranslateSamplerE2ETest) {\n \n   auto* fusion = module->GetComputationWithName(\"fused_computation\");\n   ASSERT_NE(fusion, nullptr);\n-  PartitionedComputation computation(fusion, &symbolic_expr_context_);\n+  PartitionedComputation computation(fusion, &mlir_context_);\n   EXPECT_EQ(computation.subgraphs().size(), 1);\n }\n "
        },
        {
            "sha": "090a1dea4567443bdd5734d688b72f79e6c1c80b",
            "filename": "third_party/xla/xla/codegen/emitters/concatenate_kernel_emitter.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 22,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fconcatenate_kernel_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fconcatenate_kernel_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fconcatenate_kernel_emitter.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -72,14 +72,15 @@ limitations under the License.\n \n namespace xla::emitters {\n \n+using ::mlir::MLIRContext;\n+\n ConcatenateFusionKernelEmitter::ConcatenateFusionKernelEmitter(\n-    SymbolicExprContext& symbolic_expr_context,\n-    const HloFusionInstruction& fusion, const HloFusionSpec& fusion_spec,\n-    const BufferAssignment* buffer_assignment,\n+    MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n+    const HloFusionSpec& fusion_spec, const BufferAssignment* buffer_assignment,\n     KernelArguments::BufferAlignment buffer_alignment,\n     WorkDimensions work_dimensions, absl::string_view entry_function_name,\n     BackendKind backend_kind)\n-    : symbolic_expr_context_(symbolic_expr_context),\n+    : mlir_context_(mlir_context),\n       fusion_(fusion),\n       fusion_spec_(fusion_spec),\n       buffer_assignment_(buffer_assignment),\n@@ -91,7 +92,7 @@ ConcatenateFusionKernelEmitter::ConcatenateFusionKernelEmitter(\n \n absl::StatusOr<ConcatenateFusionKernelEmitter::KernelDefinition>\n ConcatenateFusionKernelEmitter::EmitKernelDefinition() {\n-  mlir::OpBuilder builder(symbolic_expr_context_.GetMLIRContext());\n+  mlir::OpBuilder builder(&mlir_context_);\n   auto loc = mlir::NameLoc::get(builder.getStringAttr(fusion_.name()));\n   mlir::OwningOpRef<mlir::ModuleOp> module = llvm_ir::CreateMlirModuleOp(\n       loc, absl::StrCat(fusion_.name(), \"_kernel_module\"));\n@@ -103,14 +104,12 @@ ConcatenateFusionKernelEmitter::EmitKernelDefinition() {\n       mlir::func::FuncOp entry_func,\n       emitters::EmitKernelApi(*module, fusion_, buffer_assignment_,\n                               buffer_alignment_, entry_function_name_));\n-  SetBackendKind(symbolic_expr_context_.GetMLIRContext(), entry_func,\n-                 backend_kind_);\n+  SetBackendKind(&mlir_context_, entry_func, backend_kind_);\n \n   std::vector<emitters::EpilogueSpecification> epilogues =\n-      GetEpilogues(fusion_, &symbolic_expr_context_);\n+      GetEpilogues(fusion_, &mlir_context_);\n   emitters::PartitionedComputations computations(\n-      fusion_.fused_instructions_computation(), &symbolic_expr_context_,\n-      epilogues);\n+      fusion_.fused_instructions_computation(), &mlir_context_, epilogues);\n   TF_ASSIGN_OR_RETURN(auto call_targets, emitters::EmitPartitionedComputations(\n                                              *module, computations));\n \n@@ -152,12 +151,12 @@ int ConcatenateFusionKernelEmitter::GetValidUnrollFactor(\n \n IndexingMap ConcatenateFusionKernelEmitter::ComputeWorkItemIdToOutputIndexing(\n     const WorkDimensions& work_dimensions, const Shape& largest_shape,\n-    SymbolicExprContext* ctx) {\n+    MLIRContext* ctx) {\n   return GetDefaultWorkItemIndexingMap(work_dimensions, largest_shape, ctx);\n }\n \n IndexingMap ConcatenateFusionKernelEmitter::ComputeWorkItemIdToOutputIndexing(\n-    SymbolicExprContext* ctx) const {\n+    MLIRContext* ctx) const {\n   return ComputeWorkItemIdToOutputIndexing(work_dimensions_, largest_shape_,\n                                            ctx);\n }\n@@ -186,10 +185,9 @@ absl::Status ConcatenateFusionKernelEmitter::EmitEntryFunction(\n                                                 output_tensor_args.end()};\n \n   auto work_item_id_to_input_map =\n-      ComputeWorkItemIdToOutputIndexing(&symbolic_expr_context_);\n+      ComputeWorkItemIdToOutputIndexing(&mlir_context_);\n   auto epilogue_indexing = ComputeEpilogueInputToOutputIndexing(\n-      fusion_spec_.fusion_hero(0), fusion_spec_.fusion_root(0),\n-      &symbolic_expr_context_);\n+      fusion_spec_.fusion_hero(0), fusion_spec_.fusion_root(0), &mlir_context_);\n \n   const auto* concat = &fusion_spec_.fusion_hero(0).instruction();\n \n@@ -210,7 +208,7 @@ absl::Status ConcatenateFusionKernelEmitter::EmitEntryFunction(\n     for (auto [operand_index, operand] : llvm::enumerate(concat->operands())) {\n       IndexingMap input_to_output_map =\n           ComputeInputToOutputIndexing(concat, /*input_id=*/operand_index,\n-                                       &symbolic_expr_context_)\n+                                       &mlir_context_)\n               .indexing_maps.front()\n               .begin()\n               ->map();\n@@ -262,7 +260,7 @@ absl::Status ConcatenateFusionKernelEmitter::EmitEntryFunction(\n       llvm::SmallVector<mlir::OpFoldResult> offsets(output_tensor.getRank(),\n                                                     nested_b.getIndexAttr(0));\n       llvm::SmallVector<mlir::OpFoldResult> sizes =\n-          mlir::getAsIndexOpFoldResult(symbolic_expr_context_.GetMLIRContext(),\n+          mlir::getAsIndexOpFoldResult(&mlir_context_,\n                                        output_tensor.getShape());\n       llvm::SmallVector<mlir::OpFoldResult> strides(output_tensor.getRank(),\n                                                     nested_b.getIndexAttr(1));\n@@ -273,7 +271,7 @@ absl::Status ConcatenateFusionKernelEmitter::EmitEntryFunction(\n \n   const NumWorkItems& num_work_items = work_dimensions_.num_work_items;\n   llvm::SmallVector<mlir::OpFoldResult> upper_bounds =\n-      mlir::getAsIndexOpFoldResult(symbolic_expr_context_.GetMLIRContext(),\n+      mlir::getAsIndexOpFoldResult(&mlir_context_,\n                                    {static_cast<int64_t>(num_work_items.x),\n                                     static_cast<int64_t>(num_work_items.y),\n                                     static_cast<int64_t>(num_work_items.z)});\n@@ -287,12 +285,11 @@ absl::Status ConcatenateFusionKernelEmitter::EmitEntryFunction(\n }\n \n std::vector<emitters::EpilogueSpecification>\n-ConcatenateFusionKernelEmitter::GetEpilogues(\n-    const HloFusionInstruction& fusion,\n-    SymbolicExprContext* symbolic_expr_context) const {\n+ConcatenateFusionKernelEmitter::GetEpilogues(const HloFusionInstruction& fusion,\n+                                             MLIRContext* mlir_context) const {\n   return {emitters::EpilogueSpecification::FromIdentityIndexing(\n       &fusion_spec_.fusion_hero(0).instruction(),\n-      &fusion_spec_.fusion_root(0).instruction(), symbolic_expr_context)};\n+      &fusion_spec_.fusion_root(0).instruction(), mlir_context)};\n }\n \n }  // namespace xla::emitters"
        },
        {
            "sha": "b7d5d7d9b9b970db7882bf6342a5158dbb07a7c2",
            "filename": "third_party/xla/xla/codegen/emitters/concatenate_kernel_emitter.h",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fconcatenate_kernel_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fconcatenate_kernel_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fconcatenate_kernel_emitter.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -42,8 +42,8 @@ class ConcatenateFusionKernelEmitter final\n     : public KernelEmitter<MlirKernelSource> {\n  public:\n   ConcatenateFusionKernelEmitter(\n-      SymbolicExprContext& symbolic_expr_context,\n-      const HloFusionInstruction& fusion, const HloFusionSpec& fusion_spec,\n+      mlir::MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n+      const HloFusionSpec& fusion_spec,\n       const BufferAssignment* buffer_assignment,\n       KernelArguments::BufferAlignment buffer_alignment,\n       WorkDimensions work_dimensions, absl::string_view entry_function_name,\n@@ -57,7 +57,7 @@ class ConcatenateFusionKernelEmitter final\n \n   static IndexingMap ComputeWorkItemIdToOutputIndexing(\n       const WorkDimensions& work_dimensions, const Shape& largest_shape,\n-      SymbolicExprContext* ctx);\n+      mlir::MLIRContext* ctx);\n \n   // Get the shape used for indexing.\n   // For concatenate, this is the largest shape.\n@@ -71,7 +71,7 @@ class ConcatenateFusionKernelEmitter final\n                                   int max_unroll_factor);\n \n  private:\n-  IndexingMap ComputeWorkItemIdToOutputIndexing(SymbolicExprContext* ctx) const;\n+  IndexingMap ComputeWorkItemIdToOutputIndexing(mlir::MLIRContext* ctx) const;\n \n   absl::Status EmitEntryFunction(\n       const emitters::PartitionedComputations& computations,\n@@ -81,10 +81,10 @@ class ConcatenateFusionKernelEmitter final\n \n   std::vector<emitters::EpilogueSpecification> GetEpilogues(\n       const HloFusionInstruction& fusion,\n-      SymbolicExprContext* symbolic_expr_context) const;\n+      mlir::MLIRContext* mlir_context) const;\n \n  private:\n-  SymbolicExprContext& symbolic_expr_context_;\n+  mlir::MLIRContext& mlir_context_;\n   const HloFusionInstruction& fusion_;\n   const HloFusionSpec& fusion_spec_;\n   const BufferAssignment* buffer_assignment_;"
        },
        {
            "sha": "fb8d3e6445e32d1fbc5e5d569ac6f6af4dffae5e",
            "filename": "third_party/xla/xla/codegen/emitters/dynamic_update_slice_kernel_emitter.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 21,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fdynamic_update_slice_kernel_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fdynamic_update_slice_kernel_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fdynamic_update_slice_kernel_emitter.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -72,16 +72,17 @@ limitations under the License.\n \n namespace xla::emitters {\n \n+using ::mlir::MLIRContext;\n+\n constexpr int kDUSUpdateIndex = 1;\n \n DynamicUpdateSliceKernelEmitter::DynamicUpdateSliceKernelEmitter(\n-    SymbolicExprContext& symbolic_expr_context,\n-    const HloFusionInstruction& fusion, const HloFusionSpec& fusion_spec,\n-    const BufferAssignment* buffer_assignment,\n+    MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n+    const HloFusionSpec& fusion_spec, const BufferAssignment* buffer_assignment,\n     KernelArguments::BufferAlignment buffer_alignment,\n     WorkDimensions work_dimensions, absl::string_view entry_function_name,\n     BackendKind backend_kind)\n-    : symbolic_expr_context_(symbolic_expr_context),\n+    : mlir_context_(mlir_context),\n       fusion_(fusion),\n       fusion_spec_(fusion_spec),\n       dus_ops_(\n@@ -94,7 +95,7 @@ DynamicUpdateSliceKernelEmitter::DynamicUpdateSliceKernelEmitter(\n \n absl::StatusOr<DynamicUpdateSliceKernelEmitter::KernelDefinition>\n DynamicUpdateSliceKernelEmitter::EmitKernelDefinition() {\n-  mlir::OpBuilder builder(symbolic_expr_context_.GetMLIRContext());\n+  mlir::OpBuilder builder(&mlir_context_);\n   auto loc = mlir::NameLoc::get(builder.getStringAttr(fusion_.name()));\n   mlir::OwningOpRef<mlir::ModuleOp> module = llvm_ir::CreateMlirModuleOp(\n       loc, absl::StrCat(fusion_.name(), \"_kernel_module\"));\n@@ -106,12 +107,10 @@ DynamicUpdateSliceKernelEmitter::EmitKernelDefinition() {\n       mlir::func::FuncOp entry_func,\n       emitters::EmitKernelApi(*module, fusion_, buffer_assignment_,\n                               buffer_alignment_, entry_function_name_));\n-  SetBackendKind(symbolic_expr_context_.GetMLIRContext(), entry_func,\n-                 backend_kind_);\n+  SetBackendKind(&mlir_context_, entry_func, backend_kind_);\n \n   emitters::PartitionedComputations computations(\n-      fusion_.fused_instructions_computation(), &symbolic_expr_context_,\n-      GetEpilogues());\n+      fusion_.fused_instructions_computation(), &mlir_context_, GetEpilogues());\n   TF_ASSIGN_OR_RETURN(auto call_targets, emitters::EmitPartitionedComputations(\n                                              *module, computations));\n \n@@ -125,12 +124,12 @@ DynamicUpdateSliceKernelEmitter::EmitKernelDefinition() {\n }\n \n IndexingMap DynamicUpdateSliceKernelEmitter::ComputeWorkItemIdToInputIndexing(\n-    SymbolicExprContext* symbolic_expr_context) const {\n+    MLIRContext* mlir_context) const {\n   // It is guaranteed that all DUS ops have the same output shape at this point.\n   const auto& update_shape =\n       dus_ops_.front().GetOperand(kDUSUpdateIndex).shape();\n   return ComputeWorkItemIdToOutputIndexing(work_dimensions_, update_shape,\n-                                           symbolic_expr_context);\n+                                           mlir_context);\n }\n \n Shape DynamicUpdateSliceKernelEmitter::GetIndexingShape(\n@@ -142,9 +141,9 @@ Shape DynamicUpdateSliceKernelEmitter::GetIndexingShape(\n \n IndexingMap DynamicUpdateSliceKernelEmitter::ComputeWorkItemIdToOutputIndexing(\n     const WorkDimensions& work_dimensions, const Shape& update_shape,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   return GetDefaultWorkItemIndexingMap(work_dimensions, update_shape,\n-                                       symbolic_expr_context);\n+                                       mlir_context);\n }\n \n absl::StatusOr<KernelSpec> DynamicUpdateSliceKernelEmitter::GetKernelSpec()\n@@ -199,7 +198,7 @@ absl::Status DynamicUpdateSliceKernelEmitter::EmitEntryFunction(\n   mlir::ImplicitLocOpBuilder builder(entry_function.getLoc(), entry_function);\n   builder.setInsertionPointToStart(entry_function.addEntryBlock());\n \n-  auto indexing = ComputeWorkItemIdToInputIndexing(&symbolic_expr_context_);\n+  auto indexing = ComputeWorkItemIdToInputIndexing(&mlir_context_);\n   indexing.Simplify();\n   indexing.RemoveUnusedSymbols();\n \n@@ -244,10 +243,9 @@ absl::Status DynamicUpdateSliceKernelEmitter::EmitEntryFunction(\n           call_targets, entry_function, nested_b);\n       // Handle bitcasts under the DUS.\n       if (dus_instr->shape() != root.shape()) {\n-        update_indices =\n-            ApplyIndexing(GetBitcastMap(dus_instr->shape(), root.shape(),\n-                                        &symbolic_expr_context_),\n-                          update_indices, {}, nested_b);\n+        update_indices = ApplyIndexing(\n+            GetBitcastMap(dus_instr->shape(), root.shape(), &mlir_context_),\n+            update_indices, {}, nested_b);\n       }\n       results.push_back(nested_b.create<mlir::tensor::InsertOp>(\n           updated_value[0], output, update_indices));\n@@ -279,7 +277,7 @@ absl::Status DynamicUpdateSliceKernelEmitter::EmitEntryFunction(\n       llvm::SmallVector<mlir::OpFoldResult> offsets(output_tensor.getRank(),\n                                                     nested_b.getIndexAttr(0));\n       llvm::SmallVector<mlir::OpFoldResult> sizes =\n-          mlir::getAsIndexOpFoldResult(symbolic_expr_context_.GetMLIRContext(),\n+          mlir::getAsIndexOpFoldResult(&mlir_context_,\n                                        output_tensor.getShape());\n       llvm::SmallVector<mlir::OpFoldResult> strides(output_tensor.getRank(),\n                                                     nested_b.getIndexAttr(1));\n@@ -290,7 +288,7 @@ absl::Status DynamicUpdateSliceKernelEmitter::EmitEntryFunction(\n \n   const NumWorkItems& num_work_items = work_dimensions_.num_work_items;\n   llvm::SmallVector<mlir::OpFoldResult> upper_bounds =\n-      mlir::getAsIndexOpFoldResult(symbolic_expr_context_.GetMLIRContext(),\n+      mlir::getAsIndexOpFoldResult(&mlir_context_,\n                                    {static_cast<int64_t>(num_work_items.x),\n                                     static_cast<int64_t>(num_work_items.y),\n                                     static_cast<int64_t>(num_work_items.z)});\n@@ -308,7 +306,7 @@ DynamicUpdateSliceKernelEmitter::GetEpilogues() const {\n   for (const auto& [dus_op, root] :\n        llvm::zip(dus_ops_, fusion_spec_.fusion_roots())) {\n     epilogues.push_back(emitters::EpilogueSpecification::FromIdentityIndexing(\n-        &dus_op.instruction(), &root.instruction(), &symbolic_expr_context_));\n+        &dus_op.instruction(), &root.instruction(), &mlir_context_));\n   }\n   return epilogues;\n }"
        },
        {
            "sha": "e727d8296176a5ac9b468bafde17847ba6285b33",
            "filename": "third_party/xla/xla/codegen/emitters/dynamic_update_slice_kernel_emitter.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fdynamic_update_slice_kernel_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fdynamic_update_slice_kernel_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fdynamic_update_slice_kernel_emitter.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -50,8 +50,8 @@ class DynamicUpdateSliceKernelEmitter final\n     : public KernelEmitter<MlirKernelSource> {\n  public:\n   DynamicUpdateSliceKernelEmitter(\n-      SymbolicExprContext& symbolic_expr_context,\n-      const HloFusionInstruction& fusion, const HloFusionSpec& fusion_spec,\n+      mlir::MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n+      const HloFusionSpec& fusion_spec,\n       const BufferAssignment* buffer_assignment,\n       KernelArguments::BufferAlignment buffer_alignment,\n       WorkDimensions work_dimensions, absl::string_view entry_function_name,\n@@ -69,11 +69,11 @@ class DynamicUpdateSliceKernelEmitter final\n   // Get the mapping from work item id to output.\n   static IndexingMap ComputeWorkItemIdToOutputIndexing(\n       const WorkDimensions& work_dimensions, const Shape& update_shape,\n-      SymbolicExprContext* ctx);\n+      mlir::MLIRContext* ctx);\n \n  private:\n   IndexingMap ComputeWorkItemIdToInputIndexing(\n-      SymbolicExprContext* symbolic_expr_context) const;\n+      mlir::MLIRContext* mlir_context) const;\n   absl::StatusOr<KernelSpec> GetKernelSpec() const;\n \n   absl::Status EmitEntryFunction(\n@@ -85,7 +85,7 @@ class DynamicUpdateSliceKernelEmitter final\n   std::vector<emitters::EpilogueSpecification> GetEpilogues() const;\n \n  private:\n-  SymbolicExprContext& symbolic_expr_context_;\n+  mlir::MLIRContext& mlir_context_;\n   const HloFusionInstruction& fusion_;\n   const HloFusionSpec& fusion_spec_;\n   std::vector<HloInstructionAdaptor> dus_ops_;"
        },
        {
            "sha": "14d5adaadb48c61041de10a7b0acb0927dd3c74b",
            "filename": "third_party/xla/xla/codegen/emitters/elemental_hlo_to_mlir.cc",
            "status": "modified",
            "additions": 39,
            "deletions": 50,
            "changes": 89,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Felemental_hlo_to_mlir.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Felemental_hlo_to_mlir.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Felemental_hlo_to_mlir.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -179,9 +179,9 @@ absl::StatusOr<SmallVector<Value, 1>> EmitReduce(\n     const HloInstruction* instr, ValueRange indices,\n     const OperandProvider& operand_provider,\n     const CallTargetProvider& call_target_provider, ImplicitLocOpBuilder& b,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   HloInstructionIndexing indexing =\n-      ComputeOutputToInputIndexing(instr, 0, symbolic_expr_context);\n+      ComputeOutputToInputIndexing(instr, 0, mlir_context);\n   const IndexingMap& indexing_map = indexing.indexing_maps[0].begin()->map();\n \n   SmallVector<Value, 1> init_values;\n@@ -214,9 +214,9 @@ absl::StatusOr<SmallVector<Value, 1>> EmitReduceWindow(\n     const HloInstruction* instr, ValueRange indices,\n     const OperandProvider& operand_provider,\n     const CallTargetProvider& call_target_provider, ImplicitLocOpBuilder& b,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   HloInstructionIndexing indexing =\n-      ComputeOutputToInputIndexing(instr, 0, symbolic_expr_context);\n+      ComputeOutputToInputIndexing(instr, 0, mlir_context);\n   IndexingMap indexing_map = indexing.indexing_maps[0].begin()->map();\n   indexing_map.RescaleSymbols();\n \n@@ -456,10 +456,10 @@ SmallVector<SmallVector<Value, 3>, 2> GetInputIndices(\n absl::StatusOr<SmallVector<Value, 1>> EmitPad(\n     const HloInstruction* instr, ValueRange indices,\n     const OperandProvider& operand_provider, ImplicitLocOpBuilder& b,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   auto result_element_type =\n       PrimitiveTypeToMlirType(instr->shape().element_type(), b);\n-  auto indexing = ComputeOutputToInputIndexing(instr, 0, symbolic_expr_context);\n+  auto indexing = ComputeOutputToInputIndexing(instr, 0, mlir_context);\n   const IndexingMap& indexing_map = indexing.indexing_maps[0].begin()->map();\n   Value is_in_bounds = CheckConstraints(indexing_map, indices, {}, b);\n \n@@ -525,11 +525,11 @@ absl::StatusOr<Value> EmitMulAdd(Value lhs, Value rhs, Value accumulator,\n absl::StatusOr<SmallVector<Value, 1>> EmitDotLoop(\n     const HloInstruction* instr, ValueRange indices,\n     const OperandProvider& operand_provider, ImplicitLocOpBuilder& b,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   auto result_element_type =\n       PrimitiveTypeToMlirType(instr->shape().element_type(), b);\n-  HloInstructionIndexing indexing = ComputeOutputToInputIndexing(\n-      instr, /*output_id=*/0, symbolic_expr_context);\n+  HloInstructionIndexing indexing =\n+      ComputeOutputToInputIndexing(instr, /*output_id=*/0, mlir_context);\n   const IndexingMap& lhs_indexing_map =\n       indexing.indexing_maps.at(0).begin()->map();\n   const IndexingMap& rhs_indexing_map =\n@@ -595,7 +595,7 @@ absl::StatusOr<SmallVector<Value, 1>> EmitDotLoop(\n absl::StatusOr<SmallVector<Value, 1>> EmitDot(\n     const HloInstruction* instr, ValueRange indices,\n     const OperandProvider& operand_provider, ImplicitLocOpBuilder& b,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   VLOG(10) << \"EmitDot: \" << instr->ToString();\n \n   if (!algorithm_util::IsSupportedByElementalIrEmitter(\n@@ -608,17 +608,15 @@ absl::StatusOr<SmallVector<Value, 1>> EmitDot(\n   auto* dot = DynCast<HloDotInstruction>(instr);\n   TF_RET_CHECK(dot != nullptr);\n \n-  return EmitDotLoop(instr, indices, operand_provider, b,\n-                     symbolic_expr_context);\n+  return EmitDotLoop(instr, indices, operand_provider, b, mlir_context);\n }\n \n absl::StatusOr<SmallVector<Value, 1>> EmitConvolution(\n     const HloInstruction* instr, ValueRange indices,\n     const OperandProvider& operand_provider, ImplicitLocOpBuilder& b,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   VLOG(10) << \"EmitConvolution: \" << instr->ToString();\n-  return EmitDotLoop(instr, indices, operand_provider, b,\n-                     symbolic_expr_context);\n+  return EmitDotLoop(instr, indices, operand_provider, b, mlir_context);\n }\n \n absl::StatusOr<SmallVector<Value, 1>> EmitParameter(const HloInstruction* instr,\n@@ -721,7 +719,7 @@ namespace {\n absl::StatusOr<SmallVector<Value, 1>> EmitTuple(\n     const HloInstruction* instr, ValueRange indices,\n     const OperandProvider& operand_provider, ImplicitLocOpBuilder& builder,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   const auto* first_shape = &instr->shape().tuple_shapes(0);\n   while (first_shape->IsTuple()) {\n     first_shape = &first_shape->tuple_shapes(0);\n@@ -740,8 +738,8 @@ absl::StatusOr<SmallVector<Value, 1>> EmitTuple(\n     }\n     if (i > 0 && !ShapeUtil::EqualIgnoringElementType(*first_shape,\n                                                       *operand_index_shape)) {\n-      auto operand_map = GetBitcastMap(*first_shape, *operand_index_shape,\n-                                       symbolic_expr_context);\n+      auto operand_map =\n+          GetBitcastMap(*first_shape, *operand_index_shape, mlir_context);\n       operand_indices = ApplyIndexing(operand_map, indices, {}, builder);\n     } else {\n       operand_indices = indices;\n@@ -787,7 +785,7 @@ absl::StatusOr<SmallVector<Value, 1>> EmitConstant(\n absl::StatusOr<SmallVector<Value, 2>> GetOperands(\n     const HloInstruction* instr, ValueRange indices,\n     const OperandProvider& operand_provider, ImplicitLocOpBuilder& builder,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   SmallVector<Value, 2> operands;\n   bool is_elementwise = HloInstruction::IsOpElementwise(instr->opcode()) ||\n                         instr->opcode() == HloOpcode::kMap;\n@@ -811,8 +809,7 @@ absl::StatusOr<SmallVector<Value, 2>> GetOperands(\n     }\n   } else {\n     auto input_indices = GetInputIndices(\n-        ComputeOutputToInputIndexing(instr, 0, symbolic_expr_context), indices,\n-        builder);\n+        ComputeOutputToInputIndexing(instr, 0, mlir_context), indices, builder);\n     for (auto&& [operand_number, operand_indices] :\n          llvm::enumerate(input_indices)) {\n       TF_ASSIGN_OR_RETURN(\n@@ -971,7 +968,7 @@ absl::StatusOr<SmallVector<Value, 1>> HloToMlir(\n     const HloInstruction* instr, mlir::func::FuncOp this_fn, ValueRange indices,\n     const OperandProvider& operand_provider,\n     const CallTargetProvider& call_target_provider,\n-    ImplicitLocOpBuilder& builder, SymbolicExprContext* symbolic_expr_context) {\n+    ImplicitLocOpBuilder& builder, MLIRContext* mlir_context) {\n   CHECK(!kUnsupportedOps.contains(instr->opcode())) << instr->ToShortString();\n \n   auto element_type = instr->shape().element_type();\n@@ -985,7 +982,7 @@ absl::StatusOr<SmallVector<Value, 1>> HloToMlir(\n       return EmitConstant(instr, indices, builder);\n     case HloOpcode::kConvolution:\n       return EmitConvolution(instr, indices, operand_provider, builder,\n-                             symbolic_expr_context);\n+                             mlir_context);\n     case HloOpcode::kDynamicSlice:\n       return EmitDynamicSlice(instr, indices, operand_provider, builder);\n     case HloOpcode::kDynamicUpdateSlice:\n@@ -995,23 +992,19 @@ absl::StatusOr<SmallVector<Value, 1>> HloToMlir(\n     case HloOpcode::kIota:\n       return EmitIota(instr, indices, builder);\n     case HloOpcode::kPad:\n-      return EmitPad(instr, indices, operand_provider, builder,\n-                     symbolic_expr_context);\n+      return EmitPad(instr, indices, operand_provider, builder, mlir_context);\n     case HloOpcode::kDot:\n-      return EmitDot(instr, indices, operand_provider, builder,\n-                     symbolic_expr_context);\n+      return EmitDot(instr, indices, operand_provider, builder, mlir_context);\n     case HloOpcode::kParameter:\n       return EmitParameter(instr, this_fn, indices, builder);\n     case HloOpcode::kReduce:\n       return EmitReduce(instr, indices, operand_provider, call_target_provider,\n-                        builder, symbolic_expr_context);\n+                        builder, mlir_context);\n     case HloOpcode::kReduceWindow:\n       return EmitReduceWindow(instr, indices, operand_provider,\n-                              call_target_provider, builder,\n-                              symbolic_expr_context);\n+                              call_target_provider, builder, mlir_context);\n     case HloOpcode::kTuple:\n-      return EmitTuple(instr, indices, operand_provider, builder,\n-                       symbolic_expr_context);\n+      return EmitTuple(instr, indices, operand_provider, builder, mlir_context);\n     case HloOpcode::kGetTupleElement: {\n       // We have to generate the entire tuple, but since we don't support\n       // internal tuple operations (only root tuples), this will always be\n@@ -1032,9 +1025,9 @@ absl::StatusOr<SmallVector<Value, 1>> HloToMlir(\n     arg_types.push_back(operand_element_type);\n   }\n \n-  TF_ASSIGN_OR_RETURN(auto operands,\n-                      GetOperands(instr, indices, operand_provider, builder,\n-                                  symbolic_expr_context));\n+  TF_ASSIGN_OR_RETURN(\n+      auto operands,\n+      GetOperands(instr, indices, operand_provider, builder, mlir_context));\n \n   llvm::SmallVector<mlir::NamedAttribute> attributes;\n   switch (instr->opcode()) {\n@@ -1300,16 +1293,15 @@ class SubgraphConverter {\n                     mlir::func::FuncOp this_fn,\n                     const CallTargetProvider& call_target_provider,\n                     ValueRange parameters, ValueRange indices,\n-                    ImplicitLocOpBuilder& builder,\n-                    SymbolicExprContext* symbolic_expr_context)\n+                    ImplicitLocOpBuilder& builder, MLIRContext* mlir_context)\n       : computation_(computation),\n         subgraph_(subgraph),\n         this_fn_(this_fn),\n         call_target_provider_(call_target_provider),\n         parameters_(parameters),\n         indices_(indices),\n         builder_(builder),\n-        symbolic_expr_context_(symbolic_expr_context),\n+        mlir_context_(mlir_context),\n         provide_operand_fn_(\n             std::bind(std::mem_fn(&SubgraphConverter::ProvideOperand), this,\n                       std::placeholders::_1, std::placeholders::_2,\n@@ -1345,7 +1337,7 @@ class SubgraphConverter {\n   ValueRange parameters_;\n   ValueRange indices_;\n   ImplicitLocOpBuilder& builder_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  MLIRContext* mlir_context_;\n   absl::node_hash_map<std::pair<const HloInstruction*, std::vector<void*>>,\n                       SmallVector<Value>>\n       cached_instructions_;\n@@ -1401,9 +1393,8 @@ absl::StatusOr<SmallVector<Value>> SubgraphConverter::EmitInstruction(\n   }\n \n   TF_ASSIGN_OR_RETURN(\n-      auto entry,\n-      HloToMlir(instr, this_fn_, indices, provide_operand_fn_,\n-                call_target_provider_, builder_, symbolic_expr_context_));\n+      auto entry, HloToMlir(instr, this_fn_, indices, provide_operand_fn_,\n+                            call_target_provider_, builder_, mlir_context_));\n   CHECK(!absl::c_linear_search(entry, nullptr))\n       << \"Failed to lower \" << instr->name();\n   return CacheInstruction(instr, indices, std::move(entry));\n@@ -1439,9 +1430,8 @@ SubgraphConverter::EmitElementwiseInstruction(const HloInstruction* root,\n \n   for (auto* instr : llvm::reverse(pre_order)) {\n     TF_ASSIGN_OR_RETURN(\n-        auto entry,\n-        HloToMlir(instr, this_fn_, indices, provide_operand_fn_,\n-                  call_target_provider_, builder_, symbolic_expr_context_));\n+        auto entry, HloToMlir(instr, this_fn_, indices, provide_operand_fn_,\n+                              call_target_provider_, builder_, mlir_context_));\n     CacheInstruction(instr, indices, std::move(entry));\n   }\n   return cached_instructions_[{root, IndicesToPtrs(indices)}];\n@@ -1507,9 +1497,9 @@ absl::StatusOr<SmallVector<Value>> SubgraphToMlir(\n     const PartitionedComputation::Subgraph& subgraph,\n     mlir::func::FuncOp this_fn, const CallTargetProvider& call_target_provider,\n     ValueRange parameters, ValueRange indices, ImplicitLocOpBuilder& builder,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   return SubgraphConverter(computation, subgraph, this_fn, call_target_provider,\n-                           parameters, indices, builder, symbolic_expr_context)\n+                           parameters, indices, builder, mlir_context)\n       .Convert();\n }\n \n@@ -1532,8 +1522,7 @@ void GetLoopBoundsFromIndexingMap(ImplicitLocOpBuilder& b,\n absl::Status SubgraphToMlirFunction(\n     const PartitionedComputation& computation,\n     const PartitionedComputation::Subgraph& subgraph, mlir::func::FuncOp& func,\n-    const CallTargetProvider& call_target_provider,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    const CallTargetProvider& call_target_provider, MLIRContext* mlir_context) {\n   TF_RET_CHECK(func != nullptr);\n   ImplicitLocOpBuilder builder(func.getLoc(), func->getContext());\n   builder.setInsertionPointToStart(func.addEntryBlock());\n@@ -1546,7 +1535,7 @@ absl::Status SubgraphToMlirFunction(\n   TF_ASSIGN_OR_RETURN(\n       auto results,\n       SubgraphToMlir(computation, subgraph, func, call_target_provider,\n-                     parameters, indices, builder, symbolic_expr_context));\n+                     parameters, indices, builder, mlir_context));\n   CHECK_EQ(results.size(), func.getResultTypes().size());\n \n   for (auto& result : results) {"
        },
        {
            "sha": "35637dcb7d4e8aee82344bdab319c607d9a3da92",
            "filename": "third_party/xla/xla/codegen/emitters/elemental_hlo_to_mlir.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Felemental_hlo_to_mlir.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Felemental_hlo_to_mlir.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Felemental_hlo_to_mlir.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -71,7 +71,7 @@ absl::Status SubgraphToMlirFunction(\n     const PartitionedComputation& computation,\n     const PartitionedComputation::Subgraph& subgraph, mlir::func::FuncOp& func,\n     const CallTargetProvider& call_target_provider,\n-    SymbolicExprContext* symbolic_expr_context);\n+    mlir::MLIRContext* mlir_context);\n \n // Creates an `apply_indexing` op for the given map.\n llvm::SmallVector<mlir::Value, 3> ApplyIndexing(IndexingMap map,"
        },
        {
            "sha": "fd85579d9ccb3fa5b289d576b83dbe3f1807cb01",
            "filename": "third_party/xla/xla/codegen/emitters/elemental_hlo_to_mlir_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Felemental_hlo_to_mlir_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Felemental_hlo_to_mlir_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Felemental_hlo_to_mlir_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -97,7 +97,7 @@ class ElementalHloToMlirTest : public HloHardwareIndependentTestBase {\n       epilogue_spec.push_back(epilogue_spec_fn(entry_computation));\n     }\n     PartitionedComputations partitioned_computations(\n-        entry_computation, &symbolic_expr_context_, epilogue_spec);\n+        entry_computation, &mlir_context_, epilogue_spec);\n     auto fns = partitioned_computations.DeclareFunctions(module.get());\n     auto entry_func = fns[&partitioned_computations\n                                .FindPartitionedComputation(entry_computation)\n@@ -113,13 +113,12 @@ class ElementalHloToMlirTest : public HloHardwareIndependentTestBase {\n     auto call_targets = partitioned_computations.CreateCallTargetProvider(fns);\n     TF_RETURN_IF_ERROR(\n         SubgraphToMlirFunction(entry_pc, entry_pc.GetRootSubgraph(), entry_func,\n-                               call_targets, &symbolic_expr_context_));\n+                               call_targets, &mlir_context_));\n \n     if (!partitioned_computations.epilogues().empty()) {\n       const auto& epilogue = partitioned_computations.epilogues().front();\n-      TF_RETURN_IF_ERROR(SubgraphToMlirFunction(entry_pc, epilogue,\n-                                                fns[&epilogue], call_targets,\n-                                                &symbolic_expr_context_));\n+      TF_RETURN_IF_ERROR(SubgraphToMlirFunction(\n+          entry_pc, epilogue, fns[&epilogue], call_targets, &mlir_context_));\n     }\n \n     // Canonicalize and CSE for better readability of check tests.\n@@ -139,7 +138,6 @@ class ElementalHloToMlirTest : public HloHardwareIndependentTestBase {\n   }\n \n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(ElementalHloToMlirTest, Reduce) {"
        },
        {
            "sha": "1239f8afe06f5e2f2e2eb09c7e1823c3f495fc2e",
            "filename": "third_party/xla/xla/codegen/emitters/ir/xla_ops.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 12,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2Fxla_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2Fxla_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2Fxla_ops.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -297,7 +297,7 @@ struct IndexingMapWithAdditions {\n absl::StatusOr<IndexingMapWithAdditions> GetNewIndexingMapAfterFoldingSequence(\n     IndexingMap indexing_map,\n     SmallVector<std::pair<int, ApplyIndexingOp>, 2> apply_indexing_ops,\n-    mlir::DenseMap<Value, AffineExpr> operand_exprs, SymbolicExprContext* ctx) {\n+    mlir::DenseMap<Value, AffineExpr> operand_exprs, MLIRContext* ctx) {\n   int num_dims = indexing_map.GetDimensionCount();\n \n   SmallVector<Value> added_dim_args;\n@@ -324,8 +324,8 @@ absl::StatusOr<IndexingMapWithAdditions> GetNewIndexingMapAfterFoldingSequence(\n       auto& replacement_expr = operand_exprs[producer_operand.get()];\n       if (!replacement_expr) {\n         int dim_num = producer_operand_number;\n-        replacement_expr = getAffineDimExpr(num_dims + added_dim_args.size(),\n-                                            ctx->GetMLIRContext());\n+        replacement_expr =\n+            getAffineDimExpr(num_dims + added_dim_args.size(), ctx);\n         added_dim_args.push_back(producer_operand.get());\n         new_dim_vars.push_back(producer_map.GetDimVar(dim_num));\n       }\n@@ -448,11 +448,9 @@ struct FoldApplyIndexingSequence\n               : getAffineSymbolExpr(operand_number - num_dims, ctx);\n     }\n \n-    // TODO(b/446856303): Get SymbolicExprContext from IndexingMap.\n-    SymbolicExprContext symbolic_expr_context(ctx);\n+    // TODO(b/446856303): Get MLIRContext from IndexingMap.\n     auto replacement = GetNewIndexingMapAfterFoldingSequence(\n-        indexing_map, apply_indexing_ops, operand_exprs,\n-        &symbolic_expr_context);\n+        indexing_map, apply_indexing_ops, operand_exprs, ctx);\n \n     if (!replacement.ok()) {\n       return rewriter.notifyMatchFailure(indexing_op,\n@@ -938,7 +936,6 @@ struct SimplifyLoopOfApplyIndexing : public mlir::OpRewritePattern<LoopOp> {\n     auto loop_indexing_map = loop_op.getIndexingMap();\n     MLIRContext* mlir_context = loop_op.getContext();\n     // TODO(b/446856303): Get context from IndexingMap instead.\n-    SymbolicExprContext symbolic_expr_context(mlir_context);\n     int num_dims = loop_indexing_map.GetDimVarsCount();\n \n     SmallVector<std::pair<int, ApplyIndexingOp>, 2> apply_indexing_ops;\n@@ -971,8 +968,7 @@ struct SimplifyLoopOfApplyIndexing : public mlir::OpRewritePattern<LoopOp> {\n     }\n \n     auto replacement = GetNewIndexingMapAfterFoldingSequence(\n-        loop_indexing_map, apply_indexing_ops, operand_exprs,\n-        &symbolic_expr_context);\n+        loop_indexing_map, apply_indexing_ops, operand_exprs, mlir_context);\n \n     if (!replacement.ok()) {\n       return rewriter.notifyMatchFailure(loop_op,\n@@ -1115,8 +1111,7 @@ std::optional<IndexingMap> parseChainOfStringsAsIndexingMap(\n   while (parser.parseOptionalAttribute(indexing_map_attr).has_value()) {\n     indexing_map_str.append(indexing_map_attr.getValue());\n   }\n-  SymbolicExprContext symbolic_expr_context(parser.getContext());\n-  return ParseIndexingMap(indexing_map_str, &symbolic_expr_context);\n+  return ParseIndexingMap(indexing_map_str, parser.getContext());\n }\n \n void LoopOp::getCanonicalizationPatterns(mlir::RewritePatternSet& results,"
        },
        {
            "sha": "cc4002b42c5ea51af0c99c96516dc9db459f8902",
            "filename": "third_party/xla/xla/codegen/emitters/ir/xla_ops_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2Fxla_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2Fxla_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fir%2Fxla_ops_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -71,7 +71,6 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> ParseMlirModuleString(\n class XLAOpsTest : public HloPjRtTestBase {\n  public:\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n std::string VariableConstraintsToString(const IndexingMap& map) {\n@@ -129,7 +128,7 @@ TEST_F(XLAOpsTest, GetConstraintsForVariables) {\n     s0 mod 4 in [0, 1],\n     w mod 4 in [0, 2],\n   )\",\n-                               &symbolic_expr_context_);\n+                               &mlir_context_);\n   EXPECT_EQ(VariableConstraintsToString(map),\n             R\"(x: no constraints\n y: y + w in [0, 4], y mod 32 in [0, 6]\n@@ -146,7 +145,7 @@ TEST_F(XLAOpsTest, GetConstraintsForVariablesEmpty) {\n     s0 in [0, 32],\n     s1 in [0, 1024],\n   )\",\n-                               &symbolic_expr_context_);\n+                               &mlir_context_);\n   EXPECT_EQ(VariableConstraintsToString(map),\n             R\"(d0: no constraints\n d1: no constraints"
        },
        {
            "sha": "887763df095a4f04c7a3fad92d26f03b0148562e",
            "filename": "third_party/xla/xla/codegen/emitters/kernel_api_builder.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 21,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_api_builder.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_api_builder.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_api_builder.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -49,7 +49,6 @@ limitations under the License.\n #include \"xla/codegen/emitters/type_util.h\"\n #include \"xla/codegen/kernel_spec.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/layout_util.h\"\n@@ -219,9 +218,9 @@ void SetIndexDataLayout(mlir::ModuleOp module,\n       mlir::DataLayoutSpecAttr::get(module->getContext(), {index_layout}));\n }\n \n-IndexingMap GetDefaultWorkItemIndexingMap(\n-    const WorkDimensions& work_dimensions, const Shape& shape,\n-    SymbolicExprContext* symbolic_expr_context) {\n+IndexingMap GetDefaultWorkItemIndexingMap(const WorkDimensions& work_dimensions,\n+                                          const Shape& shape,\n+                                          mlir::MLIRContext* mlir_context) {\n   std::vector<mlir::AffineExpr> output_dims(shape.dimensions().size());\n \n   const NumWorkItems& num_work_items = work_dimensions.num_work_items;\n@@ -236,15 +235,13 @@ IndexingMap GetDefaultWorkItemIndexingMap(\n       num_work_items.y * num_work_groups.y,\n       num_work_items.z * num_work_groups.z};\n \n-  mlir::AffineExpr c0 =\n-      mlir::getAffineConstantExpr(0, symbolic_expr_context->GetMLIRContext());\n+  mlir::AffineExpr c0 = mlir::getAffineConstantExpr(0, mlir_context);\n   uint64_t stride = 1;\n   mlir::AffineExpr linear_index = c0;\n   // Reverse to get minor to major order.\n   for (auto [idx, dim] : llvm::enumerate(llvm::reverse(work_tile_dimensions))) {\n     uint64_t symbol_index = work_tile_dimensions.size() - idx;\n-    auto tile_coord = mlir::getAffineSymbolExpr(\n-        symbol_index, symbolic_expr_context->GetMLIRContext());\n+    auto tile_coord = mlir::getAffineSymbolExpr(symbol_index, mlir_context);\n     auto tile_component = tile_coord * stride;\n \n     linear_index = linear_index + tile_component;\n@@ -262,10 +259,8 @@ IndexingMap GetDefaultWorkItemIndexingMap(\n   // if its assumptions are not fulfilled.\n   for (int i = 0; i < 3; ++i) {\n     auto coord =\n-        mlir::getAffineDimExpr(kIndexingMapWorkItemDims[i],\n-                               symbolic_expr_context->GetMLIRContext()) +\n-        mlir::getAffineDimExpr(kIndexingMapWorkGroupDims[i],\n-                               symbolic_expr_context->GetMLIRContext()) *\n+        mlir::getAffineDimExpr(kIndexingMapWorkItemDims[i], mlir_context) +\n+        mlir::getAffineDimExpr(kIndexingMapWorkGroupDims[i], mlir_context) *\n             work_item_array[i];\n     auto linear_component = coord * stride;\n     linear_index = linear_index + linear_component;\n@@ -276,8 +271,7 @@ IndexingMap GetDefaultWorkItemIndexingMap(\n   // chunk.\n   uint64_t items_per_chunk = stride;\n \n-  mlir::AffineExpr chunk_id =\n-      mlir::getAffineSymbolExpr(0, symbolic_expr_context->GetMLIRContext());\n+  mlir::AffineExpr chunk_id = mlir::getAffineSymbolExpr(0, mlir_context);\n   linear_index = chunk_id * items_per_chunk + linear_index;\n \n   // See IndexUtil::LinearIndexToMultidimensionalIndex.\n@@ -301,11 +295,11 @@ IndexingMap GetDefaultWorkItemIndexingMap(\n \n   size_t range_vars_size = range_vars.size();\n \n-  IndexingMap indexing_map(\n-      mlir::AffineMap::get(/*dimCount=*/6,\n-                           /*symbolCount=*/range_vars_size, output_dims,\n-                           symbolic_expr_context->GetMLIRContext()),\n-      std::move(dim_vars), std::move(range_vars), /*rt_vars=*/{});\n+  IndexingMap indexing_map(mlir::AffineMap::get(/*dimCount=*/6,\n+                                                /*symbolCount=*/range_vars_size,\n+                                                output_dims, mlir_context),\n+                           std::move(dim_vars), std::move(range_vars),\n+                           /*rt_vars=*/{});\n   indexing_map.AddConstraint(linear_index, Interval{0, num_elements - 1});\n   indexing_map.Simplify();\n   indexing_map.RemoveUnusedSymbols();\n@@ -372,7 +366,7 @@ absl::StatusOr<CallTargetProvider> EmitPartitionedComputations(\n       if (subgraph_to_mlir_fn.contains(&subgraph)) {\n         TF_RETURN_IF_ERROR(SubgraphToMlirFunction(\n             comp, subgraph, subgraph_to_mlir_fn[&subgraph], call_targets,\n-            computations.symbolic_expr_context()));\n+            computations.mlir_context()));\n       }\n     }\n   }\n@@ -385,7 +379,7 @@ absl::StatusOr<CallTargetProvider> EmitPartitionedComputations(\n     TF_RETURN_IF_ERROR(SubgraphToMlirFunction(\n         computations.FindPartitionedComputation(fused_computation), epilogue,\n         subgraph_to_mlir_fn[&epilogue], call_targets,\n-        computations.symbolic_expr_context()));\n+        computations.mlir_context()));\n   }\n \n   return call_targets;"
        },
        {
            "sha": "01f9bff165a36c0ae758e14b1f40dcbb3ccdfaf2",
            "filename": "third_party/xla/xla/codegen/emitters/kernel_api_builder.h",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_api_builder.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_api_builder.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_api_builder.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -27,7 +27,6 @@ limitations under the License.\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n #include \"xla/codegen/kernel_spec.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/runtime/work_dimensions.h\"\n #include \"xla/runtime/work_group.h\"\n@@ -52,9 +51,9 @@ void SetIndexDataLayout(mlir::ModuleOp module,\n \n // Get the default indexing map for the given work dimensions, unroll factor,\n // and output shape.\n-IndexingMap GetDefaultWorkItemIndexingMap(\n-    const WorkDimensions& work_dimensions, const Shape& shape,\n-    SymbolicExprContext* symbolic_expr_context);\n+IndexingMap GetDefaultWorkItemIndexingMap(const WorkDimensions& work_dimensions,\n+                                          const Shape& shape,\n+                                          mlir::MLIRContext* mlir_context);\n \n // Emits the work group id ops annotated with the range of each dimension.\n llvm::SmallVector<mlir::Value> EmitWorkGroupIds("
        },
        {
            "sha": "00d2d508a989fa1dd5aa61bacb343cfa125dd647",
            "filename": "third_party/xla/xla/codegen/emitters/kernel_api_builder_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_api_builder_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_api_builder_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_api_builder_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -20,7 +20,6 @@ limitations under the License.\n #include \"mlir/IR/AffineExpr.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/layout_util.h\"\n #include \"xla/runtime/work_cluster.h\"\n #include \"xla/runtime/work_dimensions.h\"\n@@ -35,7 +34,6 @@ namespace {\n \n TEST(DefaultWorkItemIndexingMap, MultiDimensionTile) {\n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n   mlir_context.loadDialect<mlir::affine::AffineDialect>();\n \n   WorkDimensions work_dimensions{NumWorkClusters{}, NumWorkGroups{2},\n@@ -44,8 +42,8 @@ TEST(DefaultWorkItemIndexingMap, MultiDimensionTile) {\n   Shape shape(PrimitiveType::F32, {6, 4, 5, 6});\n   *shape.mutable_layout() = LayoutUtil::GetDefaultLayoutForShape(shape);\n \n-  IndexingMap indexing_map = GetDefaultWorkItemIndexingMap(\n-      work_dimensions, shape, &symbolic_expr_context);\n+  IndexingMap indexing_map =\n+      GetDefaultWorkItemIndexingMap(work_dimensions, shape, &mlir_context);\n \n   // The shape is the same as the number of elements work dimensions, so there\n   // are no constraints."
        },
        {
            "sha": "467e80fd26888e424fedadfb6219808c1a1b85f0",
            "filename": "third_party/xla/xla/codegen/emitters/loop_kernel_emitter.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 15,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Floop_kernel_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Floop_kernel_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Floop_kernel_emitter.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -68,14 +68,15 @@ limitations under the License.\n \n namespace xla::emitters {\n \n+using ::mlir::MLIRContext;\n+\n LoopFusionKernelEmitter::LoopFusionKernelEmitter(\n-    SymbolicExprContext& symbolic_expr_context,\n-    const HloFusionInstruction& fusion, const HloFusionSpec& fusion_spec,\n-    const BufferAssignment* buffer_assignment,\n+    MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n+    const HloFusionSpec& fusion_spec, const BufferAssignment* buffer_assignment,\n     KernelArguments::BufferAlignment buffer_alignment,\n     WorkDimensions work_dimensions, absl::string_view entry_function_name,\n     BackendKind backend_kind)\n-    : symbolic_expr_context_(symbolic_expr_context),\n+    : mlir_context_(mlir_context),\n       fusion_(fusion),\n       fusion_spec_(fusion_spec),\n       buffer_assignment_(buffer_assignment),\n@@ -86,7 +87,7 @@ LoopFusionKernelEmitter::LoopFusionKernelEmitter(\n \n absl::StatusOr<LoopFusionKernelEmitter::KernelDefinition>\n LoopFusionKernelEmitter::EmitKernelDefinition() {\n-  mlir::OpBuilder builder(symbolic_expr_context_.GetMLIRContext());\n+  mlir::OpBuilder builder(&mlir_context_);\n   auto loc = mlir::NameLoc::get(builder.getStringAttr(fusion_.name()));\n   mlir::OwningOpRef<mlir::ModuleOp> module = llvm_ir::CreateMlirModuleOp(\n       loc, absl::StrCat(fusion_.name(), \"_kernel_module\"));\n@@ -98,12 +99,11 @@ LoopFusionKernelEmitter::EmitKernelDefinition() {\n       mlir::func::FuncOp entry_func,\n       emitters::EmitKernelApi(*module, fusion_, buffer_assignment_,\n                               buffer_alignment_, entry_function_name_));\n-  SetBackendKind(symbolic_expr_context_.GetMLIRContext(), entry_func,\n-                 backend_kind_);\n+  SetBackendKind(&mlir_context_, entry_func, backend_kind_);\n \n   // Loop emitters don't support epilogues.\n   emitters::PartitionedComputations computations(\n-      fusion_.fused_instructions_computation(), &symbolic_expr_context_);\n+      fusion_.fused_instructions_computation(), &mlir_context_);\n   TF_ASSIGN_OR_RETURN(auto call_targets, emitters::EmitPartitionedComputations(\n                                              *module, computations));\n \n@@ -120,12 +120,12 @@ LoopFusionKernelEmitter::EmitKernelDefinition() {\n \n IndexingMap LoopFusionKernelEmitter::ComputeWorkItemIdToOutputIndexing(\n     const WorkDimensions& work_dimensions, const Shape& root_shape,\n-    SymbolicExprContext* ctx) {\n+    MLIRContext* ctx) {\n   return GetDefaultWorkItemIndexingMap(work_dimensions, root_shape, ctx);\n }\n \n IndexingMap LoopFusionKernelEmitter::ComputeWorkItemIdToOutputIndexing(\n-    SymbolicExprContext* ctx) const {\n+    MLIRContext* ctx) const {\n   return ComputeWorkItemIdToOutputIndexing(work_dimensions_,\n                                            GetIndexingShape(fusion_spec_), ctx);\n }\n@@ -151,7 +151,7 @@ absl::Status LoopFusionKernelEmitter::EmitEntryFunction(\n   auto workgroup_ids =\n       EmitWorkGroupIds(builder, work_dimensions_.num_work_groups);\n \n-  auto indexing = ComputeWorkItemIdToOutputIndexing(&symbolic_expr_context_);\n+  auto indexing = ComputeWorkItemIdToOutputIndexing(&mlir_context_);\n \n   int num_inputs = fusion.fused_instructions_computation()->num_parameters();\n   auto output_tensor_args =\n@@ -186,8 +186,7 @@ absl::Status LoopFusionKernelEmitter::EmitEntryFunction(\n     for (auto [root_shape, tensor, value] :\n          llvm::zip(result_shapes, output_tensors, result_scalars)) {\n       llvm::SmallVector<mlir::Value> output_indices = emitters::ApplyIndexing(\n-          GetBitcastMap(*result_shapes.front(), *root_shape,\n-                        &symbolic_expr_context_),\n+          GetBitcastMap(*result_shapes.front(), *root_shape, &mlir_context_),\n           map_results, {}, nested_b);\n       result_tensors.push_back(nested_b.create<mlir::tensor::InsertOp>(\n           value, tensor, output_indices));\n@@ -216,7 +215,7 @@ absl::Status LoopFusionKernelEmitter::EmitEntryFunction(\n       llvm::SmallVector<mlir::OpFoldResult> offsets(output_tensor.getRank(),\n                                                     nested_b.getIndexAttr(0));\n       llvm::SmallVector<mlir::OpFoldResult> sizes =\n-          mlir::getAsIndexOpFoldResult(symbolic_expr_context_.GetMLIRContext(),\n+          mlir::getAsIndexOpFoldResult(&mlir_context_,\n                                        output_tensor.getShape());\n       llvm::SmallVector<mlir::OpFoldResult> strides(output_tensor.getRank(),\n                                                     nested_b.getIndexAttr(1));\n@@ -227,7 +226,7 @@ absl::Status LoopFusionKernelEmitter::EmitEntryFunction(\n \n   const NumWorkItems& num_work_items = work_dimensions_.num_work_items;\n   llvm::SmallVector<mlir::OpFoldResult> upper_bounds =\n-      mlir::getAsIndexOpFoldResult(symbolic_expr_context_.GetMLIRContext(),\n+      mlir::getAsIndexOpFoldResult(&mlir_context_,\n                                    {static_cast<int64_t>(num_work_items.x),\n                                     static_cast<int64_t>(num_work_items.y),\n                                     static_cast<int64_t>(num_work_items.z)});"
        },
        {
            "sha": "46546815d2f8cba1525204f83444e6ddee8f2f77",
            "filename": "third_party/xla/xla/codegen/emitters/loop_kernel_emitter.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Floop_kernel_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Floop_kernel_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Floop_kernel_emitter.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -40,7 +40,7 @@ namespace xla::emitters {\n // Generic loop fusion.\n class LoopFusionKernelEmitter final : public KernelEmitter<MlirKernelSource> {\n  public:\n-  LoopFusionKernelEmitter(SymbolicExprContext& symbolic_expr_context,\n+  LoopFusionKernelEmitter(mlir::MLIRContext& mlir_context,\n                           const HloFusionInstruction& fusion,\n                           const HloFusionSpec& fusion_spec,\n                           const BufferAssignment* buffer_assignment,\n@@ -54,14 +54,14 @@ class LoopFusionKernelEmitter final : public KernelEmitter<MlirKernelSource> {\n \n   static IndexingMap ComputeWorkItemIdToOutputIndexing(\n       const WorkDimensions& work_dimensions, const Shape& root_shape,\n-      SymbolicExprContext* ctx);\n+      mlir::MLIRContext* ctx);\n \n   // Get the shape that will be used for loop indexing for the given fusion\n   // specification.\n   static Shape GetIndexingShape(const HloFusionSpec& fusion_spec);\n \n  private:\n-  IndexingMap ComputeWorkItemIdToOutputIndexing(SymbolicExprContext* ctx) const;\n+  IndexingMap ComputeWorkItemIdToOutputIndexing(mlir::MLIRContext* ctx) const;\n \n   absl::Status EmitEntryFunction(\n       const emitters::PartitionedComputations& computations,\n@@ -70,7 +70,7 @@ class LoopFusionKernelEmitter final : public KernelEmitter<MlirKernelSource> {\n       const HloFusionInstruction& fusion) const;\n \n  private:\n-  SymbolicExprContext& symbolic_expr_context_;\n+  mlir::MLIRContext& mlir_context_;\n   const HloFusionInstruction& fusion_;\n   const HloFusionSpec& fusion_spec_;\n   const BufferAssignment* buffer_assignment_;"
        },
        {
            "sha": "ce9b03c72ec2297eaa6c4d1acb703bdec599d44d",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/flatten_tensors.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Fflatten_tensors.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Fflatten_tensors.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Fflatten_tensors.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -228,10 +228,9 @@ Value LinearizeIndex(Location loc, ShapedType type, ValueRange indices,\n   }\n   auto linear_shape =\n       ShapeUtil::MakeShape(U8, {ShapeUtil::ElementsIn(byte_shape)});\n-  // TODO(b/446856820): Get SymbolicExprContext from a different source..\n-  SymbolicExprContext symbolic_expr_context(rewriter.getContext());\n+  // TODO(b/446856820): Get MLIRContext from a different source..\n   auto linearized_map =\n-      GetBitcastMap(byte_shape, linear_shape, &symbolic_expr_context);\n+      GetBitcastMap(byte_shape, linear_shape, rewriter.getContext());\n   mlir::SmallVector<Value> result;\n   rewriter.createOrFold<ApplyIndexingOp>(result, loc, indices, ValueRange{},\n                                          linearized_map);"
        },
        {
            "sha": "8e7ea28a9b81989dbc64ed49a415bdb065856bbe",
            "filename": "third_party/xla/xla/codegen/mlir_kernel_source.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Fmlir_kernel_source.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Fmlir_kernel_source.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fmlir_kernel_source.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -37,8 +37,6 @@ namespace xla {\n \n absl::StatusOr<MlirKernelSource> MlirKernelSource::ParseFromString(\n     absl::string_view ir, std::unique_ptr<mlir::MLIRContext> mlir_context) {\n-  auto symbolic_expr_context =\n-      std::make_unique<SymbolicExprContext>(mlir_context.get());\n   llvm::SourceMgr source_mgr;\n \n   std::string error_string;\n@@ -57,7 +55,6 @@ absl::StatusOr<MlirKernelSource> MlirKernelSource::ParseFromString(\n   }\n \n   return MlirKernelSource(std::move(mlir_context),\n-                          std::move(symbolic_expr_context),\n                           std::move(mlir_module));\n }\n "
        },
        {
            "sha": "1beeb9b5a3a7083815175648a8b4406ce99365da",
            "filename": "third_party/xla/xla/codegen/mlir_kernel_source.h",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Fmlir_kernel_source.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Fmlir_kernel_source.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fmlir_kernel_source.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -44,16 +44,14 @@ class MlirKernelSource final : public KernelSource {\n   // Construct a MLIR kernel source from a module and take ownership of its MLIR\n   // context.\n   MlirKernelSource(std::unique_ptr<mlir::MLIRContext> mlir_context,\n-                   std::unique_ptr<SymbolicExprContext> symbolic_expr_context,\n                    mlir::OwningOpRef<mlir::ModuleOp> module)\n       : mlir_context_(std::move(mlir_context)),\n-        symbolic_expr_context_(std::move(symbolic_expr_context)),\n         module_(std::move(module)) {}\n \n   // Construct a MLIR kernel source from a module but don't take any ownership\n   // of the MLIR context.\n   explicit MlirKernelSource(mlir::OwningOpRef<mlir::ModuleOp> module)\n-      : MlirKernelSource(nullptr, nullptr, std::move(module)) {}\n+      : MlirKernelSource(nullptr, std::move(module)) {}\n \n   MlirKernelSource(MlirKernelSource&& other) noexcept = default;\n   MlirKernelSource& operator=(MlirKernelSource&& other) noexcept = default;\n@@ -63,13 +61,11 @@ class MlirKernelSource final : public KernelSource {\n \n   mlir::ModuleOp module() { return *module_; }\n \n-  SymbolicExprContext* symbolic_expr_context() {\n-    return symbolic_expr_context_.get();\n-  }\n+  mlir::MLIRContext* mlir_context() { return mlir_context_.get(); }\n \n   // Moves ownership of the module to the caller.\n   mlir::OwningOpRef<mlir::ModuleOp> TakeModule() && {\n-    DCHECK(mlir_context_ == nullptr && symbolic_expr_context_ == nullptr)\n+    DCHECK(mlir_context_ == nullptr && mlir_context_ == nullptr)\n         << \"Can't move ownership of the module owned by the MlirKernelSource\";\n     return std::move(module_);\n   }\n@@ -78,7 +74,6 @@ class MlirKernelSource final : public KernelSource {\n \n  private:\n   std::unique_ptr<mlir::MLIRContext> mlir_context_;\n-  std::unique_ptr<SymbolicExprContext> symbolic_expr_context_;\n   mlir::OwningOpRef<mlir::ModuleOp> module_;\n };\n "
        },
        {
            "sha": "72152b9cee4444045ab11c6cc59357106f0633da",
            "filename": "third_party/xla/xla/codegen/tiling/constraint_expression_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fconstraint_expression_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fconstraint_expression_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fconstraint_expression_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -42,7 +42,7 @@ class ConstraintExpressionTest : public IndexingTestBase {\n  public:\n   ConstraintExpression::Constraint GetConstraint(const std::string& string_expr,\n                                                  int64_t lower, int64_t upper) {\n-    return {ParseAffineExpr(string_expr, &symbolic_expr_context_),\n+    return {ParseAffineExpr(string_expr, &mlir_context_),\n             Interval{lower, upper}};\n   }\n   ConstraintExpression Simplify(ConstraintExpression constraints) {"
        },
        {
            "sha": "c45b9eb185297ac58636c06dea22d1a595bb306f",
            "filename": "third_party/xla/xla/codegen/tiling/symbolic_tile_analysis.cc",
            "status": "modified",
            "additions": 42,
            "deletions": 53,
            "changes": 95,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -157,11 +157,9 @@ llvm::SmallVector<int64_t> GetNumberOfTilesPerDimension(\n absl::StatusOr<OutputTilingInfo> ComputeOutputTilingInfo(\n     const IndexingMap& root_indexing, absl::Span<const int64_t> tile_sizes,\n     absl::Span<const int64_t> major_to_minor_active_tiling_parameters,\n-    const TiledHloSchedule& schedule,\n-    SymbolicExprContext* symbolic_expr_context,\n+    const TiledHloSchedule& schedule, MLIRContext* mlir_context,\n     const std::optional<absl::Span<const Interval>>&\n         parent_output_tile_dim_bounds = std::nullopt) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n   int64_t num_tiling_parameters = root_indexing.GetDimVarsCount();\n   CHECK_EQ(num_tiling_parameters, tile_sizes.size());  // Crash OK\n   CHECK_EQ(0, root_indexing.GetRangeVarsCount())\n@@ -233,10 +231,9 @@ absl::StatusOr<OutputTilingInfo> ComputeOutputTilingInfo(\n       dim_vars, /*range_vars=*/{}, /*rt_vars=*/root_indexing.GetRTVars(),\n       constraints};\n \n-  TF_ASSIGN_OR_RETURN(\n-      IndexingMap program_id_to_tile_offsets,\n-      schedule.Schedule(output_tile_offset_indexing, iteration_space,\n-                        symbolic_expr_context));\n+  TF_ASSIGN_OR_RETURN(IndexingMap program_id_to_tile_offsets,\n+                      schedule.Schedule(output_tile_offset_indexing,\n+                                        iteration_space, mlir_context));\n   return OutputTilingInfo{outer_loop_bounds,\n                           output_tile_offset_indexing,\n                           {major_to_minor_active_tiling_parameters.begin(),\n@@ -264,9 +261,7 @@ class SymbolicTiledHloFusionInstruction : public SymbolicTiledHloInstruction {\n // output.\n absl::StatusOr<IndexingMap> ComputeTileOffsetIndexing(\n     const SymbolicTiledHloInstruction& tiled_hlo,\n-    const IndexingMap& output_tile_offset_indexing,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    const IndexingMap& output_tile_offset_indexing, MLIRContext* mlir_context) {\n   VLOG(4) << \"ComputeTileOffsetIndexing, combining output \"\n           << ToString(output_tile_offset_indexing) << \" with operation \"\n           << tiled_hlo.ToString();\n@@ -423,11 +418,9 @@ FusionDecision ShouldProceedWithSymbolicTileDerivation(\n   if (hlo->opcode() == HloOpcode::kReshape ||\n       hlo->opcode() == HloOpcode::kBitcast) {\n     mlir::MLIRContext* ctx = indexing_map.GetMLIRContext();\n-    // TODO(b/446856303): Get SymbolicExprContext from indexing_map.\n-    SymbolicExprContext symbolic_expr_context(ctx);\n+    // TODO(b/446856303): Get MLIRContext from indexing_map.\n     IndexingMap reshape_indexing_map =\n-        ComputeOutputToInputIndexing(hlo, /*output_id=*/0,\n-                                     &symbolic_expr_context)\n+        ComputeOutputToInputIndexing(hlo, /*output_id=*/0, ctx)\n             .indexing_maps[0]\n             .begin()\n             ->map();\n@@ -836,7 +829,7 @@ std::vector<int64_t> InputSpaceForParameterMapping(\n absl::StatusOr<IndexingMap> IndexingMapForRootInstruction(\n     const HloInstruction* root,\n     const TilingSpecification::ParameterMapping& parameter_mapping,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   std::vector<int64_t> input_space =\n       InputSpaceForParameterMapping(parameter_mapping);\n   int64_t num_output_parameters = root->shape().dimensions().size();\n@@ -852,14 +845,12 @@ absl::StatusOr<IndexingMap> IndexingMapForRootInstruction(\n       for (int64_t parameter_index = num_hidden_parameters;\n            parameter_index < num_tiling_parameters; ++parameter_index) {\n         result_exprs.push_back(\n-            mlir::getAffineDimExpr(dim_offset + parameter_index,\n-                                   symbolic_expr_context->GetMLIRContext()));\n+            mlir::getAffineDimExpr(dim_offset + parameter_index, mlir_context));\n       }\n       CHECK_EQ(result_exprs.size(), num_output_parameters);\n \n       mlir::AffineMap affine_map = mlir::AffineMap::get(\n-          input_space.size(), /*symbolCount=*/0, result_exprs,\n-          symbolic_expr_context->GetMLIRContext());\n+          input_space.size(), /*symbolCount=*/0, result_exprs, mlir_context);\n \n       return IndexingMap::FromTensorSizes(affine_map, std::move(input_space),\n                                           /*symbol_upper_bounds=*/{});\n@@ -876,7 +867,7 @@ absl::StatusOr<IndexingMap> IndexingMapForRootInstruction(\n /*static*/ absl::StatusOr<RootIndexing> SymbolicTileAnalysis::GetRootIndexing(\n     const HloFusionAdaptor& fusion,\n     const TilingSpecification::ParameterMapping& parameter_mapping,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   auto fusion_adaptor_roots = fusion.GetRoots();\n \n   TF_ASSIGN_OR_RETURN(int64_t real_root_index,\n@@ -892,26 +883,25 @@ absl::StatusOr<IndexingMap> IndexingMapForRootInstruction(\n   TF_ASSIGN_OR_RETURN(\n       IndexingMap indexing_map,\n       IndexingMapForRootInstruction(roots[real_root_index], parameter_mapping,\n-                                    symbolic_expr_context));\n+                                    mlir_context));\n \n   return RootIndexing{real_root_index, std::move(roots),\n                       /*real_root_indexing=*/std::move(indexing_map)};\n }\n \n /*static*/ SymbolicTileAnalysisOrError SymbolicTileAnalysis::AnalyzeComputation(\n-    const HloComputation& computation,\n-    SymbolicExprContext* symbolic_expr_context,\n+    const HloComputation& computation, MLIRContext* mlir_context,\n     EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder) {\n   auto fusion = HloFusionAdaptor::ForComputation(&computation);\n   return SymbolicTileAnalysis::AnalyzeFusion(\n-      *fusion, symbolic_expr_context, emitter_specific_constraints_builder);\n+      *fusion, mlir_context, emitter_specific_constraints_builder);\n }\n \n /*static*/ SymbolicTileAnalysisOrError\n SymbolicTileAnalysis::AnalyzeNestedFusion(\n     const HloFusionAdaptor& fusion_adaptor,\n     const TilingSpecification::ParameterMapping& parameter_mapping,\n-    SymbolicExprContext* symbolic_expr_context, const IndexingMap& indexing_map,\n+    MLIRContext* mlir_context, const IndexingMap& indexing_map,\n     IndexingMap::SimplifyPointDimensions simplification_mode,\n     EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder,\n     std::vector<SymbolicTiledHloInstruction*> root_runtime_variables) {\n@@ -930,9 +920,9 @@ SymbolicTileAnalysis::AnalyzeNestedFusion(\n                                     /*real_root_indexing=*/indexing_map};\n \n   return SymbolicTileAnalysis::AnalyzeFusionImpl(\n-      fusion_adaptor, parameter_mapping, symbolic_expr_context,\n-      nested_root_indexing, simplification_mode,\n-      emitter_specific_constraints_builder, root_runtime_variables);\n+      fusion_adaptor, parameter_mapping, mlir_context, nested_root_indexing,\n+      simplification_mode, emitter_specific_constraints_builder,\n+      root_runtime_variables);\n }\n \n namespace {\n@@ -1154,10 +1144,10 @@ ComposeIndexingResult ComposeInstructionIndexing(\n }\n \n std::vector<OperandIndexingSet> GetOperandIndexingMaps(\n-    const HloInstruction* hlo, SymbolicExprContext* symbolic_expr_context) {\n+    const HloInstruction* hlo, MLIRContext* mlir_context) {\n   std::vector<OperandIndexingSet> indexing_maps;\n   HloInstructionIndexing operands_indexing =\n-      ComputeOutputToInputIndexing(hlo, /*output_id=*/0, symbolic_expr_context);\n+      ComputeOutputToInputIndexing(hlo, /*output_id=*/0, mlir_context);\n   if (hlo->opcode() == HloOpcode::kPad) {\n     OperandIndexing pad_indexing_map =\n         *operands_indexing.indexing_maps[0].begin();\n@@ -1181,8 +1171,7 @@ std::vector<OperandIndexingSet> GetOperandIndexingMaps(\n /*static*/ SymbolicTileAnalysisOrError SymbolicTileAnalysis::AnalyzeFusionImpl(\n     const HloFusionAdaptor& fusion,\n     const TilingSpecification::ParameterMapping& parameter_mapping,\n-    SymbolicExprContext* symbolic_expr_context,\n-    const RootIndexing& root_indexing,\n+    MLIRContext* mlir_context, const RootIndexing& root_indexing,\n     IndexingMap::SimplifyPointDimensions simplification_mode,\n     EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder,\n     std::vector<SymbolicTiledHloInstruction*> root_runtime_variables) {\n@@ -1216,8 +1205,8 @@ std::vector<OperandIndexingSet> GetOperandIndexingMaps(\n       continue;  // Don't analyze parameter operands of nested fusions.\n     }\n \n-    auto operands_indexing = GetOperandIndexingMaps(\n-        tiled_hlo_instruction->hlo(), symbolic_expr_context);\n+    auto operands_indexing =\n+        GetOperandIndexingMaps(tiled_hlo_instruction->hlo(), mlir_context);\n \n     HloInstructionAdaptor instruction_adaptor(*hlo, &fusion);\n     for (auto [operand_pos, operand_and_indexing_map_set] : llvm::enumerate(\n@@ -1250,7 +1239,7 @@ std::vector<OperandIndexingSet> GetOperandIndexingMaps(\n             operand.instruction().fused_instructions_computation());\n \n         auto analysis_or = SymbolicTileAnalysis::AnalyzeNestedFusion(\n-            *nested_fusion_adaptor, parameter_mapping, symbolic_expr_context,\n+            *nested_fusion_adaptor, parameter_mapping, mlir_context,\n             composed_indexing.indexing_map, simplification_mode,\n             emitter_specific_constraints_builder,\n             composed_indexing.rt_operands);\n@@ -1311,11 +1300,11 @@ std::vector<OperandIndexingSet> GetOperandIndexingMaps(\n   return SymbolicTileAnalysis(std::move(tiled_hlo_instructions), root_indexing,\n                               std::move(tiling_specification),\n                               std::move(emitter_specific_constraints),\n-                              symbolic_expr_context);\n+                              mlir_context);\n }\n \n /*static*/ SymbolicTileAnalysisOrError SymbolicTileAnalysis::AnalyzeFusion(\n-    const HloFusionAdaptor& fusion, SymbolicExprContext* symbolic_expr_context,\n+    const HloFusionAdaptor& fusion, MLIRContext* mlir_context,\n     EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder) {\n   auto real_root_index_or = GetRealRootIndex(fusion.GetRoots());\n   if (!real_root_index_or.ok()) {\n@@ -1329,7 +1318,7 @@ std::vector<OperandIndexingSet> GetOperandIndexingMaps(\n   }\n \n   auto root_indexing_or =\n-      GetRootIndexing(fusion, *parameter_mapping_or, symbolic_expr_context);\n+      GetRootIndexing(fusion, *parameter_mapping_or, mlir_context);\n   if (!root_indexing_or.ok()) {\n     return FusionDecision(root_indexing_or.status());\n   }\n@@ -1339,7 +1328,7 @@ std::vector<OperandIndexingSet> GetOperandIndexingMaps(\n           : IndexingMap::SimplifyPointDimensions::kPreserve;\n \n   return AnalyzeFusionImpl(fusion, std::move(*parameter_mapping_or),\n-                           symbolic_expr_context, std::move(*root_indexing_or),\n+                           mlir_context, std::move(*root_indexing_or),\n                            simplification_mode,\n                            emitter_specific_constraints_builder,\n                            /*root_runtime_variables=*/{});\n@@ -1404,10 +1393,10 @@ namespace {\n // the buffer sharing logic is adapted.\n // This method assumes that `output` has tile_offset_indexing computed, and\n // returns a FailedPrecondition error if not.\n-absl::StatusOr<bool> IsSafeForBufferSharing(\n-    const TiledHloInstruction& output, int64_t reference_num_output_tiles,\n-    const TiledHloSchedule& schedule,\n-    SymbolicExprContext* symbolic_expr_context) {\n+absl::StatusOr<bool> IsSafeForBufferSharing(const TiledHloInstruction& output,\n+                                            int64_t reference_num_output_tiles,\n+                                            const TiledHloSchedule& schedule,\n+                                            MLIRContext* mlir_context) {\n   // TODO(b/453611980): this function can not behave well with regards to\n   // schedules other than the default major-to-minor. This is because\n   // non-trivial schedules require understanding the semantics of the iteration\n@@ -1438,15 +1427,15 @@ absl::StatusOr<bool> IsSafeForBufferSharing(\n   // iteration order on output tile sizes and a tile stride of 1, which means\n   // that we can take the identity map.\n   auto identity_indexing_map =\n-      CreateIdentityMap(output.hlo()->shape(), symbolic_expr_context);\n+      CreateIdentityMap(output.hlo()->shape(), mlir_context);\n   auto iota = llvm::seq<int64_t>(0, output.hlo()->shape().dimensions().size());\n   std::vector<int64_t> major_to_minor_active_tiling_parameters(iota.begin(),\n                                                                iota.end());\n   TF_ASSIGN_OR_RETURN(\n       auto tiling_info,\n       ComputeOutputTilingInfo(identity_indexing_map, output.tile_sizes(),\n                               major_to_minor_active_tiling_parameters, schedule,\n-                              symbolic_expr_context));\n+                              mlir_context));\n \n   // Check whether the tile_offsets_indexing expression is the same as one\n   // computed directly for this root.\n@@ -1467,7 +1456,7 @@ absl::StatusOr<std::vector<const TiledHloInstruction*>> InitializeTiledRoots(\n         tiled_hlo_instructions,\n     const TiledHloSchedule& schedule,\n     absl::Span<const int64_t> num_output_tiles_per_dim,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   // TODO(b/390559452): Investigate whether it is faster to use linear lookup.\n   absl::flat_hash_map<const HloInstruction*, int64_t> roots_to_output_index;\n   roots_to_output_index.reserve(roots.size());\n@@ -1498,7 +1487,7 @@ absl::StatusOr<std::vector<const TiledHloInstruction*>> InitializeTiledRoots(\n         bool valid, IsSafeForBufferSharing(*tiled_hlo_instr,\n                                            /*reference_num_output_tiles=*/\n                                            Product(num_output_tiles_per_dim),\n-                                           schedule, symbolic_expr_context));\n+                                           schedule, mlir_context));\n     if (!valid) {\n       continue;\n     }\n@@ -1544,7 +1533,7 @@ absl::StatusOr<TiledHloComputation> ComputeTiledHloInstructionsImpl(\n     bool compute_all_tile_offset_indexing_maps,\n     const std::optional<absl::Span<const Interval>>&\n         parent_output_tile_dim_bounds,\n-    SymbolicExprContext* symbolic_expr_context,\n+    MLIRContext* mlir_context,\n     absl::flat_hash_map<const SymbolicTiledHloInstruction*,\n                         TiledHloInstruction*>\n         symbolic_to_tiled_hlo_map) {\n@@ -1628,7 +1617,7 @@ absl::StatusOr<TiledHloComputation> ComputeTiledHloInstructionsImpl(\n       OutputTilingInfo output_tiling_info,\n       ComputeOutputTilingInfo(real_root_indexing, flat_tiling_parameters,\n                               major_to_minor_active_tiling_parameters,\n-                              tiled_hlo_schedule, symbolic_expr_context,\n+                              tiled_hlo_schedule, mlir_context,\n                               parent_output_tile_dim_bounds));\n \n   VLOG(3) << \"output_tiling_info: \" << output_tiling_info.ToString(\"; \");\n@@ -1670,7 +1659,7 @@ absl::StatusOr<TiledHloComputation> ComputeTiledHloInstructionsImpl(\n           ComputeTileOffsetIndexing(\n               *symbolic_tiled_hlo,\n               output_tiling_info.linear_output_tile_offset_indexing,\n-              symbolic_expr_context));\n+              mlir_context));\n       runtime_variables = MapToTiledInstructions(\n           symbolic_tiled_hlo->runtime_variables(), symbolic_to_tiled_hlo_map);\n       // Symbols here can only be runtime variables.\n@@ -1700,7 +1689,7 @@ absl::StatusOr<TiledHloComputation> ComputeTiledHloInstructionsImpl(\n               symbolic_fusion_tiling->analysis_, flat_tiling_parameters,\n               tiled_hlo_schedule, major_to_minor_active_tiling_parameters,\n               compute_all_tile_offset_indexing_maps, fusion_tile_dim_bounds,\n-              symbolic_expr_context, symbolic_to_tiled_hlo_map));\n+              mlir_context, symbolic_to_tiled_hlo_map));\n \n       TF_ASSIGN_OR_RETURN(\n           tiled_instruction,\n@@ -1729,7 +1718,7 @@ absl::StatusOr<TiledHloComputation> ComputeTiledHloInstructionsImpl(\n       auto tiled_roots,\n       InitializeTiledRoots(\n           analysis.GetRoots(), tiled_hlo_instructions, tiled_hlo_schedule,\n-          output_tiling_info.num_output_tiles_per_dim, symbolic_expr_context));\n+          output_tiling_info.num_output_tiles_per_dim, mlir_context));\n   return TiledHloComputation::FromSortedTiledHloInstructions(\n       std::move(tiled_hlo_instructions), tiled_roots,\n       output_tiling_info.num_output_tiles_per_dim);\n@@ -1764,7 +1753,7 @@ SymbolicTileAnalysis::ComputeTiledHloInstructions(\n       *this, flat_tiling_parameters, *tiled_hlo_schedule,\n       /*major_to_minor_active_tiling_parameters=*/{},\n       compute_all_tile_offset_indexing_maps,\n-      /*parent_output_tile_dim_bounds=*/std::nullopt, symbolic_expr_context_,\n+      /*parent_output_tile_dim_bounds=*/std::nullopt, mlir_context_,\n       /*symbolic_to_tiled_hlo_map=*/{});\n }\n "
        },
        {
            "sha": "0a09103afc2c2ff425e95b74d095977178d9d6c8",
            "filename": "third_party/xla/xla/codegen/tiling/symbolic_tile_analysis.h",
            "status": "modified",
            "additions": 10,
            "deletions": 15,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -130,12 +130,11 @@ class SymbolicTileAnalysis {\n   // tiles of these operands may contain expressions with symbols which would\n   // fail to be tiled.\n   static SymbolicTileAnalysisOrError AnalyzeComputation(\n-      const HloComputation& computation,\n-      SymbolicExprContext* symbolic_expr_context,\n+      const HloComputation& computation, mlir::MLIRContext* mlir_context,\n       EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder =\n           nullptr);\n   static SymbolicTileAnalysisOrError AnalyzeFusion(\n-      const HloFusionAdaptor& fusion, SymbolicExprContext* ctx,\n+      const HloFusionAdaptor& fusion, mlir::MLIRContext* ctx,\n       EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder =\n           nullptr);\n \n@@ -202,10 +201,8 @@ class SymbolicTileAnalysis {\n   // specification.\n   absl::StatusOr<bool> ParametersSatisfyConstraints(const Tiling& tiling) const;\n \n-  // Return the underlying MLIRContext.\n-  mlir::MLIRContext* GetMLIRContext() const {\n-    return symbolic_expr_context_->GetMLIRContext();\n-  };\n+  // Return the underlying mlir::MLIRContext.\n+  mlir::MLIRContext* GetMLIRContext() const { return mlir_context_; };\n \n   // Returns a string representation of the analysis. Used only for error\n   // messages and debugging.\n@@ -222,25 +219,24 @@ class SymbolicTileAnalysis {\n       const RootIndexing& root_indexing,\n       TilingSpecification tiling_specification,\n       std::unique_ptr<EmitterSpecificConstraints> emitter_specific_constraints,\n-      SymbolicExprContext* symbolic_expr_context)\n+      mlir::MLIRContext* mlir_context)\n       : symbolic_tiled_hlo_instructions_(\n             std::move(symbolic_tiled_hlo_instructions)),\n         root_indexing_(std::move(root_indexing)),\n         tiling_specification_(std::move(tiling_specification)),\n         emitter_specific_constraints_(std::move(emitter_specific_constraints)),\n-        symbolic_expr_context_(symbolic_expr_context) {}\n+        mlir_context_(mlir_context) {}\n \n   // Computes indexing information for the roots of the computation.\n   static absl::StatusOr<RootIndexing> GetRootIndexing(\n       const HloFusionAdaptor& fusion,\n       const TilingSpecification::ParameterMapping& parameter_mapping,\n-      SymbolicExprContext* symbolic_expr_context);\n+      mlir::MLIRContext* mlir_context);\n \n   static SymbolicTileAnalysisOrError AnalyzeFusionImpl(\n       const HloFusionAdaptor& fusion,\n       const TilingSpecification::ParameterMapping& parameter_mapping,\n-      SymbolicExprContext* symbolic_expr_context,\n-      const RootIndexing& root_indexing,\n+      mlir::MLIRContext* mlir_context, const RootIndexing& root_indexing,\n       IndexingMap::SimplifyPointDimensions simplification_mode,\n       EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder,\n       std::vector<SymbolicTiledHloInstruction*> root_runtime_variables);\n@@ -249,8 +245,7 @@ class SymbolicTileAnalysis {\n   static SymbolicTileAnalysisOrError AnalyzeNestedFusion(\n       const HloFusionAdaptor& fusion,\n       const TilingSpecification::ParameterMapping& parameter_mapping,\n-      SymbolicExprContext* symbolic_expr_context,\n-      const IndexingMap& indexing_map,\n+      mlir::MLIRContext* mlir_context, const IndexingMap& indexing_map,\n       IndexingMap::SimplifyPointDimensions simplification_mode,\n       EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder,\n       std::vector<SymbolicTiledHloInstruction*> root_runtime_variables);\n@@ -270,7 +265,7 @@ class SymbolicTileAnalysis {\n   // no builder was provided when constructing the analysis.\n   std::unique_ptr<EmitterSpecificConstraints> emitter_specific_constraints_;\n \n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n namespace detail {"
        },
        {
            "sha": "dbd2b76cef218b9d909699f09db45d3c4f4b6d55",
            "filename": "third_party/xla/xla/codegen/tiling/symbolic_tile_analysis_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -152,7 +152,7 @@ class SymbolicTileAnalysisTest : public HloHardwareIndependentTestBase {\n             *module->entry_computation()\n                  ->root_instruction()\n                  ->fused_instructions_computation(),\n-            &symbolic_expr_context_, emitter_specific_constraints_builder);\n+            &mlir_context_, emitter_specific_constraints_builder);\n \n     if (std::holds_alternative<SymbolicTileAnalysis>(analysis_or_error)) {\n       SymbolicTileAnalysis analysis =\n@@ -195,7 +195,6 @@ class SymbolicTileAnalysisTest : public HloHardwareIndependentTestBase {\n   mlir::MLIRContext mlir_context_;\n   TiledHloScheduleBuilder default_schedule_builder_ =\n       CreateMajorToMinorTiledHloSchedule;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(SymbolicTileAnalysisTest, SimpleNormalizationDiamondIsSupported) {\n@@ -368,7 +367,7 @@ ENTRY main {\n   auto fusion = HloFusionAdaptor::ForProducerConsumer(producer, consumer);\n \n   SymbolicTileAnalysisOrError analysis_or_error =\n-      SymbolicTileAnalysis::AnalyzeFusion(*fusion, &symbolic_expr_context_);\n+      SymbolicTileAnalysis::AnalyzeFusion(*fusion, &mlir_context_);\n   ASSERT_TRUE(std::holds_alternative<SymbolicTileAnalysis>(analysis_or_error));\n   SymbolicTileAnalysis analysis =\n       std::get<SymbolicTileAnalysis>(std::move(analysis_or_error));\n@@ -436,7 +435,7 @@ ENTRY entry_computation {\n       producer, consumer, /*with_extra_outputs=*/true);\n \n   SymbolicTileAnalysisOrError analysis_or_error =\n-      SymbolicTileAnalysis::AnalyzeFusion(*fusion, &symbolic_expr_context_);\n+      SymbolicTileAnalysis::AnalyzeFusion(*fusion, &mlir_context_);\n   ASSERT_TRUE(std::holds_alternative<SymbolicTileAnalysis>(analysis_or_error));\n   SymbolicTileAnalysis analysis =\n       std::get<SymbolicTileAnalysis>(std::move(analysis_or_error));\n@@ -1833,7 +1832,7 @@ ENTRY main {\n           *module->entry_computation()\n                ->root_instruction()\n                ->fused_instructions_computation(),\n-          &symbolic_expr_context_,\n+          &mlir_context_,\n           /*emitter_specific_constraints_builder=*/nullptr);\n \n   ASSERT_TRUE(std::holds_alternative<FusionDecision>(analysis_or_error));"
        },
        {
            "sha": "197c08b6f05beeb792ccca5b5ba641037dc6082d",
            "filename": "third_party/xla/xla/codegen/tiling/symbolic_tile_test.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 21,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -593,8 +593,8 @@ TEST_F(SymbolicTileTest, CanPropagateTileThroughSummationOfSymbols) {\n   //   reduce_0 = f32[9] reduce(bitcast), dimensions={1}\n   //   reduce_1 = f32[] reduce(reduce_0), dimensions={0}\n   IndexingMap indexing_map = IndexingMap::FromTensorSizes(\n-      ParseAffineMap(\"()[s0, s1] -> (s1 * 2 + s0)\", &symbolic_expr_context_),\n-      {}, {2, 9});\n+      ParseAffineMap(\"()[s0, s1] -> (s1 * 2 + s0)\", &mlir_context_), {},\n+      {2, 9});\n \n   EXPECT_THAT(SymbolicTile::FromIndexingMap(indexing_map),\n               Optional(MatchSymbolicTileString(R\"(\n@@ -613,7 +613,7 @@ TEST_F(SymbolicTileTest, CanPropagateTileModAndFloorDiv) {\n   IndexingMap indexing_map = IndexingMap::FromTensorSizes(\n       ParseAffineMap(\n           \"(d0) -> (d0 floordiv 35, (d0 floordiv 7) mod 5, d0 mod 7)\",\n-          &symbolic_expr_context_),\n+          &mlir_context_),\n       {105}, {});\n \n   EXPECT_THAT(SymbolicTile::FromIndexingMap(indexing_map),\n@@ -784,13 +784,12 @@ TEST_F(SymbolicTileTest,\n   // https://github.com/google/paxml/blob/91893818862645f5e9f23b84f530e611551745f6/paxml/contrib/gpu/scripts_gpu/configs.py#L107-L120.\n   IndexingMap indexing_map = IndexingMap::FromTensorSizes(\n       ParseAffineMap(\"(d0, d1, d2)[s0] -> (d0 * 2048 + d1, s0)\",\n-                     &symbolic_expr_context_),\n+                     &mlir_context_),\n       {4, 2048, 50304}, {50304});\n   // This constraint is redundant, because it can be derived from the domains of\n   // the dimension variables.\n-  indexing_map.AddConstraint(\n-      ParseAffineExpr(\"d0 * 2048 + d1\", &symbolic_expr_context_),\n-      Interval{0, 8191});\n+  indexing_map.AddConstraint(ParseAffineExpr(\"d0 * 2048 + d1\", &mlir_context_),\n+                             Interval{0, 8191});\n \n   EXPECT_THAT(SymbolicTile::FromIndexingMap(indexing_map),\n               Optional(MatchSymbolicTileString(R\"(\n@@ -806,14 +805,13 @@ TEST_F(SymbolicTileTest,\n        CanDeriveTileWhenPreexistingConstraintsModelRightPadding) {\n   IndexingMap indexing_map = IndexingMap::FromTensorSizes(\n       ParseAffineMap(\"(d0, d1, d2)[s0] -> (d0 * 2048 + d1, s0)\",\n-                     &symbolic_expr_context_),\n+                     &mlir_context_),\n       {4, 2048, 50304}, {50304});\n   // This constraint is not redundant, but it doesn't prevent us from deriving\n   // a valid tile (although that tile will need to be interpreted as containing\n   // high padding).\n-  indexing_map.AddConstraint(\n-      ParseAffineExpr(\"d0 * 2048 + d1\", &symbolic_expr_context_),\n-      Interval{0, 4096});\n+  indexing_map.AddConstraint(ParseAffineExpr(\"d0 * 2048 + d1\", &mlir_context_),\n+                             Interval{0, 4096});\n \n   EXPECT_THAT(SymbolicTile::FromIndexingMap(indexing_map),\n               Optional(MatchSymbolicTileString(R\"(\n@@ -829,12 +827,11 @@ TEST_F(SymbolicTileTest,\n        BailsOutOnDerivingTileWhenPreexistingConstraintsModelLeftPadding) {\n   IndexingMap indexing_map = IndexingMap::FromTensorSizes(\n       ParseAffineMap(\"(d0, d1, d2)[s0] -> (d0 * 2048 + d1, s0)\",\n-                     &symbolic_expr_context_),\n+                     &mlir_context_),\n       {4, 2048, 50304}, {50304});\n   // This constraint models left padding, which we do not handle for now.\n-  indexing_map.AddConstraint(\n-      ParseAffineExpr(\"d0 * 2048 + d1\", &symbolic_expr_context_),\n-      Interval{2, 4096});\n+  indexing_map.AddConstraint(ParseAffineExpr(\"d0 * 2048 + d1\", &mlir_context_),\n+                             Interval{2, 4096});\n \n   EXPECT_FALSE(SymbolicTile::FromIndexingMap(indexing_map).has_value());\n }\n@@ -843,13 +840,13 @@ TEST_F(SymbolicTileTest,\n        BailsOutOnDerivingTileWhenPreexistingConstraintsDoesNotApplyToResult) {\n   IndexingMap indexing_map = IndexingMap::FromTensorSizes(\n       ParseAffineMap(\"(d0, d1, d2)[s0] -> (d0 * 2048 + d1, s0)\",\n-                     &symbolic_expr_context_),\n+                     &mlir_context_),\n       {4, 2048, 50304}, {50304});\n   // This constraint does not apply to a result, and actually models dilation\n   // across `d1`. Figuring out how to handle such cases holistically is\n   // difficult, and we bail out for now.\n-  indexing_map.AddConstraint(\n-      ParseAffineExpr(\"d1 mod 5\", &symbolic_expr_context_), Interval{0, 0});\n+  indexing_map.AddConstraint(ParseAffineExpr(\"d1 mod 5\", &mlir_context_),\n+                             Interval{0, 0});\n \n   EXPECT_FALSE(SymbolicTile::FromIndexingMap(indexing_map).has_value());\n }\n@@ -859,7 +856,7 @@ TEST_F(SymbolicTileTest, CanDeriveTileWhenTheIndexingMapHasSymbolsInASum) {\n   // https://github.com/google/paxml/blob/91893818862645f5e9f23b84f530e611551745f6/paxml/contrib/gpu/scripts_gpu/configs.py#L107-L120.\n   IndexingMap indexing_map = IndexingMap::FromTensorSizes(\n       ParseAffineMap(\"(d0, d1, d2)[s0] -> (d0, d1, d2 * 128 + s0)\",\n-                     &symbolic_expr_context_),\n+                     &mlir_context_),\n       {4, 2048, 393}, {128});\n \n   EXPECT_THAT(SymbolicTile::FromIndexingMap(indexing_map),\n@@ -876,7 +873,7 @@ TEST_F(SymbolicTileTest, ResultingConstraintsAreSimplifiedAway) {\n   // https://github.com/google/paxml/blob/91893818862645f5e9f23b84f530e611551745f6/paxml/contrib/gpu/scripts_gpu/configs.py#L107-L120.\n   IndexingMap indexing_map = IndexingMap::FromTensorSizes(\n       ParseAffineMap(\"(d0, d1, d2)[s0] -> (d0, d1, d2 * 128 + s0)\",\n-                     &symbolic_expr_context_),\n+                     &mlir_context_),\n       {4, 2048, 393}, {128});\n \n   EXPECT_THAT(SymbolicTile::FromIndexingMap(indexing_map),\n@@ -890,7 +887,7 @@ TEST_F(SymbolicTileTest, ResultingConstraintsAreSimplifiedAway) {\n \n TEST_F(SymbolicTileTest, PointDimensionsAreNotSimplified) {\n   IndexingMap indexing_map = IndexingMap::FromTensorSizes(\n-      ParseAffineMap(\"(d0) -> (d0)\", &symbolic_expr_context_),\n+      ParseAffineMap(\"(d0) -> (d0)\", &mlir_context_),\n       /*dim_upper_bounds=*/{1},\n       /*symbol_upper_bounds=*/{});\n "
        },
        {
            "sha": "b480395b10dcc6215dbb18da3a1a49b5eef2f584",
            "filename": "third_party/xla/xla/codegen/tiling/symbolic_tiled_hlo_instruction_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tiled_hlo_instruction_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tiled_hlo_instruction_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tiled_hlo_instruction_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -54,12 +54,11 @@ ENTRY main {\n )\"));\n \n   mlir::MLIRContext mlir_ctx;\n-  SymbolicExprContext symbolic_expr_context(&mlir_ctx);\n   auto fusion = module->entry_computation()->root_instruction();\n   auto fusion_adaptor = HloFusionAdaptor::ForInstruction(fusion);\n \n   auto output_to_input_indexing = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &symbolic_expr_context);\n+      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &mlir_ctx);\n \n   HloInstruction* subtract = fusion->fused_expression_root();\n   HloInstruction* p0 = subtract->mutable_operand(0)->mutable_operand(0);"
        },
        {
            "sha": "c1b3e716e5187351e0812490307065b6eab71cc0",
            "filename": "third_party/xla/xla/codegen/tiling/tiled_hlo_fusion_instruction_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_fusion_instruction_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_fusion_instruction_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_fusion_instruction_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -37,7 +37,6 @@ using ::testing::HasSubstr;\n class TiledHloFusionInstructionTest : public HloHardwareIndependentTestBase {\n  public:\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(TiledHloFusionInstructionTest,\n@@ -48,7 +47,7 @@ TEST_F(TiledHloFusionInstructionTest,\n \n   IndexingMap tile_offsets_indexing = IndexingMap::FromTensorSizes(\n       ParseAffineMap(\"(d0) -> (d0 floordiv 16, (d0 mod 16) * 16)\",\n-                     &symbolic_expr_context_),\n+                     &mlir_context_),\n       /*dim_upper_bounds=*/{8},\n       /*symbol_upper_bounds=*/{});\n "
        },
        {
            "sha": "681e1b31f790d42d4e8d70a9aeef870ab90db7e6",
            "filename": "third_party/xla/xla/codegen/tiling/tiled_hlo_instruction_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_instruction_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_instruction_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_instruction_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -40,7 +40,6 @@ using ::tsl::testing::IsOk;\n class TiledHloInstructionTest : public HloHardwareIndependentTestBase {\n  public:\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(TiledHloInstructionTest, TileSizesAndStridesShouldMatchHloShapeRank) {\n@@ -50,7 +49,7 @@ TEST_F(TiledHloInstructionTest, TileSizesAndStridesShouldMatchHloShapeRank) {\n \n   IndexingMap tile_offsets_indexing = IndexingMap::FromTensorSizes(\n       ParseAffineMap(\"(d0) -> (d0 floordiv 16, (d0 mod 16) * 16)\",\n-                     &symbolic_expr_context_),\n+                     &mlir_context_),\n       /*dim_upper_bounds=*/{8},\n       /*symbol_upper_bounds=*/{});\n \n@@ -78,7 +77,7 @@ TEST_F(TiledHloInstructionTest,\n       ShapeUtil::MakeShape(PrimitiveType::F32, {32, 64}), \"p0\");\n \n   IndexingMap tile_offsets_indexing = IndexingMap::FromTensorSizes(\n-      ParseAffineMap(\"(d0) -> (2 * d0)\", &symbolic_expr_context_),\n+      ParseAffineMap(\"(d0) -> (2 * d0)\", &mlir_context_),\n       /*dim_upper_bounds=*/{2},\n       /*symbol_upper_bounds=*/{});\n \n@@ -93,7 +92,7 @@ TEST_F(TiledHloInstructionTest,\n           \"must have the same number of results as the rank of the hlo shape\"));\n \n   IndexingMap tile_offsets_indexing2 = IndexingMap::FromTensorSizes(\n-      ParseAffineMap(\"(d0, d1) -> (d0, d1)\", &symbolic_expr_context_),\n+      ParseAffineMap(\"(d0, d1) -> (d0, d1)\", &mlir_context_),\n       /*dim_upper_bounds=*/{8, 4},\n       /*symbol_upper_bounds=*/{});\n \n@@ -120,12 +119,12 @@ TEST_F(TiledHloInstructionTest,\n           /*tile_sizes=*/{16},\n           /*tile_strides=*/{1},\n           IndexingMap::FromTensorSizes(\n-              ParseAffineMap(\"(d0) -> (d0)\", &symbolic_expr_context_),\n+              ParseAffineMap(\"(d0) -> (d0)\", &mlir_context_),\n               /*dim_upper_bounds=*/{4},\n               /*symbol_upper_bounds=*/{})));\n \n   IndexingMap indexing_map(\n-      ParseAffineMap(\"(d0)[rt0] -> (d0 + rt0)\", &symbolic_expr_context_),\n+      ParseAffineMap(\"(d0)[rt0] -> (d0 + rt0)\", &mlir_context_),\n       /*dimensions=*/\n       {IndexingMap::Variable{0, 32, \"d0\"}},\n       /*range_vars=*/{},"
        },
        {
            "sha": "aa90ba632428597d79af039bf5a59e16343b80aa",
            "filename": "third_party/xla/xla/codegen/tiling/tiled_hlo_schedule.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_schedule.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_schedule.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_schedule.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -44,9 +44,10 @@ limitations under the License.\n #include \"xla/util.h\"\n \n namespace xla {\n-\n namespace {\n \n+using ::mlir::MLIRContext;\n+\n // Helper to validate that an iteration space is compatible with a tile offsets\n // indexing map.\n absl::Status ValidateIterationSpace(const IterationSpace& iteration_space,\n@@ -81,8 +82,7 @@ absl::Status ValidateIterationSpace(const IterationSpace& iteration_space,\n \n absl::StatusOr<IndexingMap> MajorToMinorScheduleImpl(\n     const IndexingMap& tile_offsets_indexing, IterationSpace iteration_space,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  mlir::MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) {\n   mlir::AffineExpr program_id = mlir::getAffineDimExpr(0, mlir_context);\n \n   std::vector<int64_t> iteration_space_sizes;\n@@ -96,8 +96,8 @@ absl::StatusOr<IndexingMap> MajorToMinorScheduleImpl(\n       mlir::getAffineConstantExpr(0, mlir_context));\n \n   for (auto [dim_info, tile_expr] : llvm::zip(\n-           iteration_space, DelinearizeIndex(iteration_space_sizes, program_id,\n-                                             symbolic_expr_context))) {\n+           iteration_space,\n+           DelinearizeIndex(iteration_space_sizes, program_id, mlir_context))) {\n     tile_exprs[dim_info.dimension_id] = tile_expr;\n   }\n   std::vector<IndexingMap::Variable> dim_vars{\n@@ -126,7 +126,7 @@ CreateMajorToMinorTiledHloSchedule(\n \n absl::StatusOr<IndexingMap> MajorToMinorTiledHloSchedule::Schedule(\n     const IndexingMap& tile_offsets_indexing, IterationSpace iteration_space,\n-    SymbolicExprContext* ctx) const {\n+    MLIRContext* ctx) const {\n   TF_RETURN_IF_ERROR(\n       ValidateIterationSpace(iteration_space, tile_offsets_indexing));\n   return MajorToMinorScheduleImpl(tile_offsets_indexing, iteration_space, ctx);\n@@ -207,7 +207,7 @@ TransposedDotTiledHloSchedule::Create(\n \n absl::StatusOr<IndexingMap> TransposedDotTiledHloSchedule::Schedule(\n     const IndexingMap& tile_offsets_indexing, IterationSpace iteration_space,\n-    SymbolicExprContext* ctx) const {\n+    MLIRContext* ctx) const {\n   TF_RETURN_IF_ERROR(\n       ValidateIterationSpace(iteration_space, tile_offsets_indexing));\n "
        },
        {
            "sha": "c7868092a37278079475085e8778dc1c11992f8a",
            "filename": "third_party/xla/xla/codegen/tiling/tiled_hlo_schedule.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_schedule.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_schedule.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_schedule.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -68,7 +68,7 @@ class TiledHloSchedule {\n   //     results are generated, but may not change the results themselves);\n   virtual absl::StatusOr<IndexingMap> Schedule(\n       const IndexingMap& tile_offsets_indexing, IterationSpace iteration_space,\n-      SymbolicExprContext* ctx) const = 0;\n+      mlir::MLIRContext* ctx) const = 0;\n };\n \n // The indexing map returned by this schedule iterates over the iteration space\n@@ -79,7 +79,7 @@ class MajorToMinorTiledHloSchedule : public TiledHloSchedule {\n  public:\n   absl::StatusOr<IndexingMap> Schedule(const IndexingMap& tile_offsets_indexing,\n                                        IterationSpace iteration_space,\n-                                       SymbolicExprContext* ctx) const override;\n+                                       mlir::MLIRContext* ctx) const override;\n };\n \n // Convenience function to produce a `MajorToMinorTiledHloSchedule` that\n@@ -104,7 +104,7 @@ class TransposedDotTiledHloSchedule : public TiledHloSchedule {\n  public:\n   absl::StatusOr<IndexingMap> Schedule(const IndexingMap& tile_offsets_indexing,\n                                        IterationSpace iteration_space,\n-                                       SymbolicExprContext* ctx) const override;\n+                                       mlir::MLIRContext* ctx) const override;\n \n   static absl::StatusOr<std::unique_ptr<TransposedDotTiledHloSchedule>> Create(\n       const TilingSpecification& tiling_specification);"
        },
        {
            "sha": "77a1c5279051b9d805a94fe5b72234494a9acb7d",
            "filename": "third_party/xla/xla/codegen/tiling/tiled_hlo_schedule_test.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 15,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_schedule_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_schedule_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiled_hlo_schedule_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -53,7 +53,6 @@ using ::testing::HasSubstr;\n class TiledHloScheduleTest : public HloHardwareIndependentTestBase {\n  protected:\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n using MajorToMinorTiledHloScheduleTest = TiledHloScheduleTest;\n@@ -63,7 +62,7 @@ TEST_F(MajorToMinorTiledHloScheduleTest,\n   IndexingMap offsets_indexing = *ParseIndexingMap(R\"(\n       (d0, d1, d2, d3) -> (d2, d3),\n       domain: d0 in [0, 1], d1 in [0, 2], d2 in [0, 4], d3 in [0, 6])\",\n-                                                   &symbolic_expr_context_);\n+                                                   &mlir_context_);\n   auto bound = [&offsets_indexing](int64_t dim) {\n     return offsets_indexing.GetDimensionBound(dim).upper + 1;\n   };\n@@ -74,9 +73,9 @@ TEST_F(MajorToMinorTiledHloScheduleTest,\n   };\n \n   MajorToMinorTiledHloSchedule scheduler;\n-  TF_ASSERT_OK_AND_ASSIGN(IndexingMap scheduled_indexing,\n-                          scheduler.Schedule(offsets_indexing, iteration_space,\n-                                             &symbolic_expr_context_));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      IndexingMap scheduled_indexing,\n+      scheduler.Schedule(offsets_indexing, iteration_space, &mlir_context_));\n \n   // (1) the map must have a single input whose range of values is the size of\n   //     the iteration space (i.e. the product of `iteration_space`'s\n@@ -94,7 +93,7 @@ TEST_F(MajorToMinorTiledHloScheduleTest,\n   EXPECT_EQ(scheduled_indexing, *ParseIndexingMap(R\"(\n     (pid_0) -> (pid_0 floordiv 21, pid_0 mod 7), domain: pid_0 in [0, 104]\n   )\",\n-                                                  &symbolic_expr_context_));\n+                                                  &mlir_context_));\n \n   // `pid_0 floordiv 21` has the same upper bound as `d2`.\n   EXPECT_EQ(iteration_space_size / 21, bound(2));\n@@ -104,9 +103,8 @@ TEST_F(MajorToMinorTiledHloScheduleTest,\n \n TEST_F(MajorToMinorTiledHloScheduleTest,\n        MajorToMinorTiledHloScheduleFailsForInvalidIterationSpace) {\n-  IndexingMap offsets_indexing =\n-      *ParseIndexingMap(\"(d0, d1) -> (d1), domain: d0 in [0, 1], d1 in [0, 2]\",\n-                        &symbolic_expr_context_);\n+  IndexingMap offsets_indexing = *ParseIndexingMap(\n+      \"(d0, d1) -> (d1), domain: d0 in [0, 1], d1 in [0, 2]\", &mlir_context_);\n   MajorToMinorTiledHloSchedule scheduler;\n \n   // The iteration space has too many dimensions.\n@@ -115,7 +113,7 @@ TEST_F(MajorToMinorTiledHloScheduleTest,\n                          {{/*dimension_id=*/0, /*dimension_size=*/1},\n                           {/*dimension_id=*/1, /*dimension_size=*/3},\n                           {/*dimension_id=*/2, /*dimension_size=*/0}},\n-                         &symbolic_expr_context_),\n+                         &mlir_context_),\n       StatusIs(\n           absl::StatusCode::kInvalidArgument,\n           HasSubstr(\n@@ -125,7 +123,7 @@ TEST_F(MajorToMinorTiledHloScheduleTest,\n   EXPECT_THAT(scheduler.Schedule(offsets_indexing, /*iteration_space=*/\n                                  {{/*dimension_id=*/0, /*dimension_size=*/1},\n                                   {/*dimension_id=*/2, /*dimension_size=*/0}},\n-                                 &symbolic_expr_context_),\n+                                 &mlir_context_),\n               StatusIs(absl::StatusCode::kInvalidArgument,\n                        HasSubstr(\"Dimension id 2 is out of bounds\")));\n }\n@@ -139,7 +137,7 @@ class TransposedDotTiledHloScheduleTest : public TiledHloScheduleTest {\n             *module->entry_computation()\n                  ->root_instruction()\n                  ->fused_instructions_computation(),\n-            &symbolic_expr_context_,\n+            &mlir_context_,\n             /*emitter_specific_constraints_builder=*/nullptr);\n \n     if (!std::holds_alternative<SymbolicTileAnalysis>(analysis_or_error)) {\n@@ -347,7 +345,7 @@ ENTRY main {\n       (d0, d1, d2, d3, d4) -> (d1, d2, d3, d4),\n       domain: d0 in [0, 0], d1 in [0, 1], d2 in [0, 1], d3 in [0, 3],\n               d4 in [0, 7])\",\n-                                                   &symbolic_expr_context_);\n+                                                   &mlir_context_);\n \n   std::vector<DimensionInfo> iteration_space;\n   iteration_space.reserve(offsets_indexing.GetDimVarsCount());\n@@ -363,11 +361,11 @@ ENTRY main {\n   TF_ASSERT_OK_AND_ASSIGN(\n       IndexingMap major_to_minor_scheduled_indexing,\n       major_to_minor_scheduler.Schedule(offsets_indexing, iteration_space,\n-                                        &symbolic_expr_context_));\n+                                        &mlir_context_));\n   TF_ASSERT_OK_AND_ASSIGN(\n       IndexingMap transposed_scheduled_indexing,\n       transposed_scheduler->Schedule(offsets_indexing, iteration_space,\n-                                     &symbolic_expr_context_));\n+                                     &mlir_context_));\n \n   int64_t m_bound = iteration_space[3].dimension_size;\n   int64_t n_bound = iteration_space[4].dimension_size;"
        },
        {
            "sha": "b5d4c3cb160641cca46c24602345e2cb72ae1e0b",
            "filename": "third_party/xla/xla/codegen/tiling/tiling_specification_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiling_specification_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiling_specification_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Ftiling_specification_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -54,15 +54,14 @@ class TilingSpecificationTest : public HloHardwareIndependentTestBase {\n             *module->entry_computation()\n                  ->root_instruction()\n                  ->fused_instructions_computation(),\n-            &symbolic_expr_context_,\n+            &mlir_context_,\n             /*emitter_specific_constraints_builder=*/nullptr);\n \n     CHECK(std::holds_alternative<SymbolicTileAnalysis>(analysis_or_error));\n     return std::get<SymbolicTileAnalysis>(std::move(analysis_or_error));\n   }\n \n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(TilingSpecificationTest, TilingSpecificationDerivesOutputParameters) {"
        },
        {
            "sha": "f4fc26d98e0932993f6f3c8e90c31ccac3921818",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_analysis.cc",
            "status": "modified",
            "additions": 147,
            "deletions": 216,
            "changes": 363,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -113,8 +113,8 @@ inline bool operator!=(const HLORTVar& lhs, const HLORTVar& rhs) {\n // Note: we had a more complex logic here that handled more instruction types\n // but was removed due to previous version not updating value ranges\n // (b/419279949).\n-std::optional<AffineExpr> OptimizeRTVar(\n-    HLORTVar rt_var, SymbolicExprContext* symbolic_expr_context) {\n+std::optional<AffineExpr> OptimizeRTVar(HLORTVar rt_var,\n+                                        MLIRContext* mlir_context) {\n   if (auto constant_expr = DynCast<HloConstantInstruction>(rt_var.hlo)) {\n     if (rt_var.map.isConstant()) {\n       const auto idx = rt_var.map.getConstantResults();\n@@ -124,8 +124,7 @@ std::optional<AffineExpr> OptimizeRTVar(\n         // the runtime to handle that.\n         return std::nullopt;\n       }\n-      return getAffineConstantExpr(const_value,\n-                                   symbolic_expr_context->GetMLIRContext());\n+      return getAffineConstantExpr(const_value, mlir_context);\n     }\n   }\n   if (auto iota_expr = DynCast<HloIotaInstruction>(rt_var.hlo)) {\n@@ -149,21 +148,19 @@ std::vector<IndexingMap::Variable> ConvertHLORTVarsToRTVars(\n IndexingMap FoldRTVarsAndConstructIndexingMap(\n     AffineMap affine_map, std::vector<IndexingMap::Variable> dim_vars,\n     std::vector<HLORTVar> hlo_rt_vars) {\n-  auto* ctx = affine_map.getContext();\n+  auto* mlir_context = affine_map.getContext();\n   // TODO (b/446856820): Get context from SymbolicMap after refactoring.\n-  SymbolicExprContext symbolic_expr_context(ctx);\n   // Range and runtime variables share the symbol space in the affine map but\n   // currently we never have range variables here.\n   CHECK_EQ(affine_map.getNumSymbols(), hlo_rt_vars.size());\n   for (auto idx = 0; idx < affine_map.getNumSymbols(); ++idx) {\n     auto& rt_var = hlo_rt_vars[idx];\n-    std::optional<AffineExpr> result =\n-        OptimizeRTVar(rt_var, &symbolic_expr_context);\n+    std::optional<AffineExpr> result = OptimizeRTVar(rt_var, mlir_context);\n     if (!result) {\n       continue;\n     }\n     affine_map =\n-        affine_map.replace({{getAffineSymbolExpr(idx, ctx), *result}},\n+        affine_map.replace({{getAffineSymbolExpr(idx, mlir_context), *result}},\n                            affine_map.getNumDims(), affine_map.getNumSymbols());\n   }\n   return IndexingMap(affine_map, std::move(dim_vars), /*range_vars=*/{},\n@@ -191,10 +188,8 @@ OperandIndexing CreateOperandIndexingWithRTVars(\n }\n \n HloInstructionIndexing ComputeOutputToInputCwiseOpIndexing(\n-    const HloInstruction* instr, SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n-  IndexingMap identity_map =\n-      CreateIdentityMap(instr->shape(), symbolic_expr_context);\n+    const HloInstruction* instr, MLIRContext* mlir_context) {\n+  IndexingMap identity_map = CreateIdentityMap(instr->shape(), mlir_context);\n   IndexingMap unit_map(\n       mlir::AffineMap::get(identity_map.GetAffineMap().getNumDims(),\n                            /*symbolCount=*/0, mlir_context),\n@@ -218,16 +213,13 @@ HloInstructionIndexing ComputeOutputToInputCwiseOpIndexing(\n }\n \n HloInstructionIndexing ComputeInputToOutputCwiseOpIndexing(\n-    const HloInstruction* instr, SymbolicExprContext* symbolic_expr_context) {\n-  IndexingMap identity_map =\n-      CreateIdentityMap(instr->shape(), symbolic_expr_context);\n+    const HloInstruction* instr, MLIRContext* mlir_context) {\n+  IndexingMap identity_map = CreateIdentityMap(instr->shape(), mlir_context);\n   return HloInstructionIndexing::FromIndexingMaps({identity_map});\n }\n \n HloInstructionIndexing ComputeOutputToInputBroadcastOpIndexing(\n-    const HloBroadcastInstruction* bcast,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    const HloBroadcastInstruction* bcast, MLIRContext* mlir_context) {\n   auto output_dims = bcast->shape().dimensions();\n \n   std::vector<AffineExpr> exprs;\n@@ -243,9 +235,7 @@ HloInstructionIndexing ComputeOutputToInputBroadcastOpIndexing(\n }\n \n HloInstructionIndexing ComputeInputToOutputBroadcastOpIndexing(\n-    const HloBroadcastInstruction* bcast,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    const HloBroadcastInstruction* bcast, MLIRContext* mlir_context) {\n   absl::Span<const int64_t> bcast_dims = bcast->dimensions();\n \n   const Shape& input_shape = bcast->operand(0)->shape();\n@@ -276,9 +266,7 @@ HloInstructionIndexing ComputeInputToOutputBroadcastOpIndexing(\n }\n \n HloInstructionIndexing ComputeOutputToInputConcatenateOpIndexing(\n-    const HloConcatenateInstruction* concat,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    const HloConcatenateInstruction* concat, MLIRContext* mlir_context) {\n   const auto& operand_0_dims = concat->operand(0)->shape().dimensions();\n \n   // Initialize affine map and domain. Only concat_dim elements of both have to\n@@ -308,8 +296,7 @@ HloInstructionIndexing ComputeOutputToInputConcatenateOpIndexing(\n \n HloInstructionIndexing ComputeInputToOutputConcatenateOpIndexing(\n     const HloConcatenateInstruction* concat, int input_id,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) {\n   int64_t concat_dim = concat->concatenate_dimension();\n   int64_t offset = 0;\n   for (int64_t operand_id = 0; operand_id < input_id; ++operand_id) {\n@@ -331,11 +318,10 @@ HloInstructionIndexing ComputeInputToOutputConcatenateOpIndexing(\n // until the HloParameterInstruction is found.\n HloInstructionIndexing ComputeOutputToInputFusionOpIndexing(\n     const HloFusionInstruction* fusion, int output_id,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   auto fusion_adaptor = HloFusionAdaptor::ForInstruction(fusion);\n   auto grouped_indexing_maps = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, fusion_adaptor->GetRoots()[output_id],\n-      symbolic_expr_context);\n+      *fusion_adaptor, fusion_adaptor->GetRoots()[output_id], mlir_context);\n \n   // After the traversal, `grouped_indexing_maps` is keyed by\n   // HloParameterInstructions. Convert them back to the operand id and return.\n@@ -349,9 +335,7 @@ HloInstructionIndexing ComputeOutputToInputFusionOpIndexing(\n \n std::pair<IndexingMap, IndexingMap> ComputeDotOperandsIndexingImpl(\n     const Shape& lhs_shape, const Shape& rhs_shape, const Shape& output_shape,\n-    const DotDimensionNumbers& dim_numbers,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    const DotDimensionNumbers& dim_numbers, MLIRContext* mlir_context) {\n   absl::Span<const int64_t> lhs_contracting_dims(\n       dim_numbers.lhs_contracting_dimensions());\n   absl::Span<const int64_t> rhs_contracting_dims =\n@@ -446,27 +430,26 @@ IndexingMap RescaleIndexingMap(const IndexingMap& operand_map,\n }\n \n HloInstructionIndexing ComputeOutputToInputDotOpIndexing(\n-    const HloDotInstruction* dot, SymbolicExprContext* symbolic_expr_context) {\n+    const HloDotInstruction* dot, MLIRContext* mlir_context) {\n   const Shape& lhs_shape = dot->operand(0)->shape();\n   const Shape& rhs_shape = dot->operand(1)->shape();\n \n   auto [lhs_map, rhs_map] = ComputeDotOperandsIndexingImpl(\n       lhs_shape, rhs_shape, dot->shape(), dot->dot_dimension_numbers(),\n-      symbolic_expr_context);\n+      mlir_context);\n   return HloInstructionIndexing::FromIndexingMaps({lhs_map, rhs_map});\n }\n \n HloInstructionIndexing ComputeOutputToInputScaledDotOpIndexing(\n-    const HloScaledDotInstruction* scaled_dot,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    const HloScaledDotInstruction* scaled_dot, MLIRContext* mlir_context) {\n   const Shape& lhs_shape = scaled_dot->operand(0)->shape();\n   const Shape& rhs_shape = scaled_dot->operand(1)->shape();\n   const Shape& lhs_scale_shape = scaled_dot->operand(2)->shape();\n   const Shape& rhs_scale_shape = scaled_dot->operand(3)->shape();\n \n   auto [lhs_map, rhs_map] = ComputeDotOperandsIndexingImpl(\n       lhs_shape, rhs_shape, scaled_dot->shape(),\n-      scaled_dot->dot_dimension_numbers(), symbolic_expr_context);\n+      scaled_dot->dot_dimension_numbers(), mlir_context);\n \n   IndexingMap lhs_scale_map =\n       RescaleIndexingMap(lhs_map, lhs_shape, lhs_scale_shape);\n@@ -479,8 +462,7 @@ HloInstructionIndexing ComputeOutputToInputScaledDotOpIndexing(\n \n HloInstructionIndexing ComputeOutputToInputDynamicSliceOpIndexing(\n     const HloDynamicSliceInstruction* dynamic_slice,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) {\n   const Shape& input_shape = dynamic_slice->operand(0)->shape();\n   const Shape& output_shape = dynamic_slice->shape();\n   int64_t rank = output_shape.dimensions().size();\n@@ -522,9 +504,7 @@ HloInstructionIndexing ComputeOutputToInputDynamicSliceOpIndexing(\n }\n \n HloInstructionIndexing ComputeOutputToInputDynamicUpdateSliceOpIndexing(\n-    const HloDynamicUpdateSliceInstruction* dus,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    const HloDynamicUpdateSliceInstruction* dus, MLIRContext* mlir_context) {\n   const Shape& update_shape = dus->update()->shape();\n   const Shape& output_shape = dus->shape();\n   int64_t rank = output_shape.dimensions().size();\n@@ -571,9 +551,7 @@ HloInstructionIndexing ComputeOutputToInputDynamicUpdateSliceOpIndexing(\n }\n \n HloInstructionIndexing ComputeOutputToInputGatherOpIndexing(\n-    const HloGatherInstruction* gather,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    const HloGatherInstruction* gather, MLIRContext* mlir_context) {\n   CHECK(GatherSimplifier::IsSimplifiedGather(gather))\n       << \"Non-simplified HLO Gather is not supported.\";\n   const Shape& operand_shape = gather->operand(0)->shape();\n@@ -645,9 +623,7 @@ IndexingMap ComputeOutputToInputPadOpIndexingImpl(\n     absl::Span<const int64_t> output_dims,\n     absl::Span<const int64_t> padding_low,\n     absl::Span<const int64_t> padding_high,\n-    absl::Span<const int64_t> padding_interior,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  mlir::MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    absl::Span<const int64_t> padding_interior, MLIRContext* mlir_context) {\n   int64_t output_rank = output_dims.size();\n \n   std::vector<AffineExpr> exprs;\n@@ -679,8 +655,7 @@ IndexingMap ComputeOutputToInputPadOpIndexingImpl(\n }\n \n HloInstructionIndexing ComputeOutputToInputPadOpIndexing(\n-    const HloPadInstruction* pad, SymbolicExprContext* symbolic_expr_context) {\n-  mlir::MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    const HloPadInstruction* pad, MLIRContext* mlir_context) {\n   const Shape& output_shape = pad->shape();\n   int64_t rank = output_shape.dimensions().size();\n   SmallVector<int64_t> padding_low, padding_high, padding_interior;\n@@ -694,7 +669,7 @@ HloInstructionIndexing ComputeOutputToInputPadOpIndexing(\n   }\n   IndexingMap input_indexing_map = ComputeOutputToInputPadOpIndexingImpl(\n       output_shape.dimensions(), padding_low, padding_high, padding_interior,\n-      symbolic_expr_context);\n+      mlir_context);\n   IndexingMap padding_value_indexing_map = IndexingMap::FromTensorSizes(\n       AffineMap::get(output_shape.dimensions().size(), /*symbolCount=*/0, {},\n                      mlir_context),\n@@ -704,9 +679,7 @@ HloInstructionIndexing ComputeOutputToInputPadOpIndexing(\n }\n \n HloInstructionIndexing ComputeOutputToInputReduceOpIndexing(\n-    const HloReduceInstruction* reduce,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  mlir::MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    const HloReduceInstruction* reduce, MLIRContext* mlir_context) {\n   absl::flat_hash_set<int64_t> reduce_dims_ids(reduce->dimensions().begin(),\n                                                reduce->dimensions().end());\n \n@@ -751,8 +724,7 @@ HloInstructionIndexing ComputeOutputToInputReduceOpIndexing(\n \n HloInstructionIndexing ComputeInputToOutputReduceOpIndexing(\n     const HloReduceInstruction* reduce, int input_id,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  mlir::MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) {\n   const Shape& output_shape = GetOutputShape(reduce, 0);\n   int64_t output_rank = output_shape.dimensions().size();\n \n@@ -802,8 +774,7 @@ HloInstructionIndexing ComputeInputToOutputReduceOpIndexing(\n IndexingMap ComposeIndexingMapsForWindow(\n     absl::Span<const int64_t> input_dimensions,\n     absl::Span<const int64_t> output_dimensions, const Window& window,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  mlir::MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) {\n   size_t rank = input_dimensions.size();\n \n   // Compute shape of the padded input and the indexing map of pad op required\n@@ -843,7 +814,7 @@ IndexingMap ComposeIndexingMapsForWindow(\n   // Indexing map for pad op that pads the input.\n   IndexingMap padded_input_indexing = ComputeOutputToInputPadOpIndexingImpl(\n       padded_input_dimensions, padding_low, padding_high, padding_interior,\n-      symbolic_expr_context);\n+      mlir_context);\n   // Indexing map for reduce-window, that does not do any padding.\n   IndexingMap input_indexing_no_padding(\n       AffineMap::get(rank, rank, exprs, mlir_context), dim_vars, range_vars,\n@@ -862,15 +833,14 @@ IndexingMap ComposeIndexingMapsForWindow(\n // of bounds.\n HloInstructionIndexing ComputeOutputToInputReduceWindowOpIndexing(\n     const HloReduceWindowInstruction* reduce_window,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  mlir::MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    MLIRContext* mlir_context) {\n   const Shape& input_shape = reduce_window->operand(0)->shape();\n   const Shape& output_shape = GetOutputShape(reduce_window, 0);\n \n   // Indexing map for the input value.\n   IndexingMap inputs_indexing = ComposeIndexingMapsForWindow(\n       input_shape.dimensions(), output_shape.dimensions(),\n-      reduce_window->window(), symbolic_expr_context);\n+      reduce_window->window(), mlir_context);\n \n   // Indexing map for the init value.\n   IndexingMap inits_indexing_map = IndexingMap::FromTensorSizes(\n@@ -892,9 +862,7 @@ HloInstructionIndexing ComputeOutputToInputReduceWindowOpIndexing(\n }\n \n HloInstructionIndexing ComputeOutputToInputConvolutionOpIndexing(\n-    const HloConvolutionInstruction* convolution,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  mlir::MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    const HloConvolutionInstruction* convolution, MLIRContext* mlir_context) {\n   const Shape& input_shape = convolution->operand(0)->shape();\n   const Shape& kernel_shape = convolution->operand(1)->shape();\n   const Shape& output_shape = convolution->shape();\n@@ -919,9 +887,9 @@ HloInstructionIndexing ComputeOutputToInputConvolutionOpIndexing(\n   // Indexing map for the input value (spatial dimensions only).\n   // The dimension numbers in the resulting affine expressions have to be\n   // remapped to correspond to the correct output dimensions.\n-  IndexingMap input_spatial_indexing = ComposeIndexingMapsForWindow(\n-      input_spatial_sizes, output_spatial_sizes, convolution->window(),\n-      symbolic_expr_context);\n+  IndexingMap input_spatial_indexing =\n+      ComposeIndexingMapsForWindow(input_spatial_sizes, output_spatial_sizes,\n+                                   convolution->window(), mlir_context);\n   std::vector<AffineExpr> replacement_dims(spatial_rank);\n   for (int i = 0; i < spatial_rank; ++i) {\n     replacement_dims[i] =\n@@ -1031,9 +999,8 @@ std::vector<int64_t> ComputeStrides(absl::Span<const int64_t> dims) {\n \n AffineExpr LinearizeShape(absl::Span<const int64_t> dims,\n                           absl::Span<const AffineExpr> dimension_exprs,\n-                          SymbolicExprContext* symbolic_expr_context) {\n-  AffineExpr linear_index =\n-      getAffineConstantExpr(0, symbolic_expr_context->GetMLIRContext());\n+                          MLIRContext* mlir_context) {\n+  AffineExpr linear_index = getAffineConstantExpr(0, mlir_context);\n \n   auto strides = ComputeStrides(dims);\n   for (auto [stride, dimension_expr] : llvm::zip(strides, dimension_exprs)) {\n@@ -1042,9 +1009,9 @@ AffineExpr LinearizeShape(absl::Span<const int64_t> dims,\n   return linear_index;\n }\n \n-std::vector<AffineExpr> DelinearizeIndex(\n-    absl::Span<const int64_t> dims, AffineExpr linear_index,\n-    SymbolicExprContext* symbolic_expr_context) {\n+std::vector<AffineExpr> DelinearizeIndex(absl::Span<const int64_t> dims,\n+                                         AffineExpr linear_index,\n+                                         MLIRContext* mlir_context) {\n   std::vector<AffineExpr> multi_index;\n   multi_index.reserve(dims.size());\n \n@@ -1076,8 +1043,7 @@ namespace {\n void ComputeMinimalReshapeIndexing(\n     absl::Span<const int64_t> input_dims, absl::Span<const int64_t> output_dims,\n     absl::Span<const AffineExpr> output_dims_exprs,\n-    std::vector<AffineExpr>* exprs,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    std::vector<AffineExpr>* exprs, MLIRContext* mlir_context) {\n   // The shape does not change.\n   if (input_dims.size() == 1 && output_dims.size() == 1) {\n     absl::c_copy(output_dims_exprs, std::back_inserter(*exprs));\n@@ -1086,21 +1052,20 @@ void ComputeMinimalReshapeIndexing(\n   // Expand shape.\n   if (input_dims.size() == 1) {\n     exprs->push_back(\n-        LinearizeShape(output_dims, output_dims_exprs, symbolic_expr_context));\n+        LinearizeShape(output_dims, output_dims_exprs, mlir_context));\n     return;\n   }\n   // Collapse shape.\n   if (output_dims.size() == 1) {\n-    auto multi_index = DelinearizeIndex(input_dims, output_dims_exprs.front(),\n-                                        symbolic_expr_context);\n+    auto multi_index =\n+        DelinearizeIndex(input_dims, output_dims_exprs.front(), mlir_context);\n     absl::c_copy(multi_index, std::back_inserter(*exprs));\n     return;\n   }\n   // Generic case.\n   AffineExpr linear_index =\n-      LinearizeShape(output_dims, output_dims_exprs, symbolic_expr_context);\n-  auto multi_index =\n-      DelinearizeIndex(input_dims, linear_index, symbolic_expr_context);\n+      LinearizeShape(output_dims, output_dims_exprs, mlir_context);\n+  auto multi_index = DelinearizeIndex(input_dims, linear_index, mlir_context);\n   absl::c_copy(multi_index, std::back_inserter(*exprs));\n }\n \n@@ -1120,10 +1085,8 @@ void ComputeMinimalReshapeIndexing(\n // This is an optimization that allows us to construct simpler affine maps,\n // otherwise we would need to linearize/delinearize even some of the simpler\n // cases.\n-AffineMap ComputeReshapeIndexingMap(\n-    const Shape& input, const Shape& output,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+AffineMap ComputeReshapeIndexingMap(const Shape& input, const Shape& output,\n+                                    MLIRContext* mlir_context) {\n   absl::Span<const int64_t> input_dims = input.dimensions();\n   absl::Span<const int64_t> output_dims = output.dimensions();\n \n@@ -1167,8 +1130,7 @@ AffineMap ComputeReshapeIndexingMap(\n       continue;\n     }\n     ComputeMinimalReshapeIndexing(input_subshape, output_subshape,\n-                                  output_dims_exprs, &exprs,\n-                                  symbolic_expr_context);\n+                                  output_dims_exprs, &exprs, mlir_context);\n     input_num_elements = 1;\n     output_num_elements = 1;\n     input_subshape.clear();\n@@ -1180,36 +1142,32 @@ AffineMap ComputeReshapeIndexingMap(\n };\n \n HloInstructionIndexing ComputeOutputToInputReshapeOpIndexing(\n-    const HloReshapeInstruction* reshape,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    const HloReshapeInstruction* reshape, MLIRContext* mlir_context) {\n   const auto& input = reshape->operand(0)->shape();\n   const auto& output = reshape->shape();\n \n   IndexingMap reshape_indexing_map = IndexingMap::FromTensorSizes(\n-      ComputeReshapeIndexingMap(input, output, symbolic_expr_context),\n+      ComputeReshapeIndexingMap(input, output, mlir_context),\n       output.dimensions(), {});\n   reshape_indexing_map.Simplify(\n       IndexingMap::SimplifyPointDimensions::kPreserve);\n   return HloInstructionIndexing::FromIndexingMaps({reshape_indexing_map});\n }\n HloInstructionIndexing ComputeInputToOutputReshapeOpIndexing(\n-    const HloReshapeInstruction* reshape,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    const HloReshapeInstruction* reshape, MLIRContext* mlir_context) {\n   const auto& input = reshape->operand(0)->shape();\n   const auto& output = reshape->shape();\n \n   IndexingMap reshape_indexing_map = IndexingMap::FromTensorSizes(\n-      ComputeReshapeIndexingMap(output, input, symbolic_expr_context),\n+      ComputeReshapeIndexingMap(output, input, mlir_context),\n       input.dimensions(), {});\n   reshape_indexing_map.Simplify(\n       IndexingMap::SimplifyPointDimensions::kPreserve);\n   return HloInstructionIndexing::FromIndexingMaps({reshape_indexing_map});\n }\n \n HloInstructionIndexing ComputeReverseOpIndexing(\n-    const HloReverseInstruction* reverse,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    const HloReverseInstruction* reverse, MLIRContext* mlir_context) {\n   absl::flat_hash_set<int64_t> reverse_dims(reverse->dimensions().begin(),\n                                             reverse->dimensions().end());\n   auto output_dims = reverse->shape().dimensions();\n@@ -1234,9 +1192,7 @@ HloInstructionIndexing ComputeReverseOpIndexing(\n }\n \n HloInstructionIndexing ComputeOutputToInputSliceOpIndexing(\n-    const HloSliceInstruction* slice,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    const HloSliceInstruction* slice, MLIRContext* mlir_context) {\n   auto output_rank = slice->shape().dimensions().size();\n \n   std::vector<AffineExpr> exprs;\n@@ -1253,9 +1209,7 @@ HloInstructionIndexing ComputeOutputToInputSliceOpIndexing(\n }\n \n HloInstructionIndexing ComputeInputToOutputSliceOpIndexing(\n-    const HloSliceInstruction* slice,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    const HloSliceInstruction* slice, MLIRContext* mlir_context) {\n   auto output_rank = slice->shape().dimensions().size();\n \n   std::vector<AffineExpr> exprs;\n@@ -1283,29 +1237,25 @@ HloInstructionIndexing ComputeInputToOutputSliceOpIndexing(\n   return HloInstructionIndexing::FromIndexingMaps({std::move(indexing_map)});\n }\n \n-AffineMap ComputeTransposeIndexingMap(\n-    absl::Span<const int64_t> permutation,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+AffineMap ComputeTransposeIndexingMap(absl::Span<const int64_t> permutation,\n+                                      MLIRContext* mlir_context) {\n   return AffineMap::getPermutationMap(\n       std::vector<unsigned>(permutation.begin(), permutation.end()),\n       mlir_context);\n }\n \n HloInstructionIndexing ComputeOutputToInputTransposeOpIndexing(\n-    const HloTransposeInstruction* transpose,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    const HloTransposeInstruction* transpose, MLIRContext* mlir_context) {\n   AffineMap inverse_permutation = ComputeTransposeIndexingMap(\n-      InversePermutation(transpose->dimensions()), symbolic_expr_context);\n+      InversePermutation(transpose->dimensions()), mlir_context);\n   return HloInstructionIndexing::FromIndexingMaps({IndexingMap::FromTensorSizes(\n       inverse_permutation, transpose->shape().dimensions(), {})});\n }\n \n HloInstructionIndexing ComputeInputToOutputTransposeOpIndexing(\n-    const HloTransposeInstruction* transpose,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  AffineMap forward_permutation = ComputeTransposeIndexingMap(\n-      transpose->dimensions(), symbolic_expr_context);\n+    const HloTransposeInstruction* transpose, MLIRContext* mlir_context) {\n+  AffineMap forward_permutation =\n+      ComputeTransposeIndexingMap(transpose->dimensions(), mlir_context);\n   return HloInstructionIndexing::FromIndexingMaps({IndexingMap::FromTensorSizes(\n       forward_permutation, transpose->operand(0)->shape().dimensions(), {})});\n }\n@@ -1314,21 +1264,21 @@ HloInstructionIndexing ComputeInputToOutputTransposeOpIndexing(\n \n IndexingMap GetBitcastMap(absl::Span<const int64_t> input_shape,\n                           const Shape& output_shape,\n-                          SymbolicExprContext* symbolic_expr_context) {\n+                          MLIRContext* mlir_context) {\n   return GetBitcastMap(ShapeUtil::MakeShapeWithDescendingLayout(\n                            output_shape.element_type(), input_shape),\n-                       output_shape, symbolic_expr_context);\n+                       output_shape, mlir_context);\n }\n IndexingMap GetBitcastMap(absl::Span<const int64_t> input_shape,\n                           absl::Span<const int64_t> output_shape,\n-                          SymbolicExprContext* symbolic_expr_context) {\n+                          MLIRContext* mlir_context) {\n   return GetBitcastMap(\n       ShapeUtil::MakeShapeWithDescendingLayout(PrimitiveType::S8, input_shape),\n       ShapeUtil::MakeShapeWithDescendingLayout(PrimitiveType::S8, output_shape),\n-      symbolic_expr_context);\n+      mlir_context);\n }\n IndexingMap GetBitcastMap(const Shape& input_shape, const Shape& output_shape,\n-                          SymbolicExprContext* symbolic_expr_context) {\n+                          MLIRContext* mlir_context) {\n   ShapeUtil::BitcastDecomposition decomposed_bitcast =\n       ShapeUtil::DecomposeBitcast(input_shape, output_shape);\n   if (!decomposed_bitcast.has_value()) {\n@@ -1343,26 +1293,25 @@ IndexingMap GetBitcastMap(const Shape& input_shape, const Shape& output_shape,\n         << \"Failed to deduce permutation for a bitcast.\";\n \n     return IndexingMap::FromTensorSizes(\n-        ComputeTransposeIndexingMap(permutation.value(), symbolic_expr_context),\n+        ComputeTransposeIndexingMap(permutation.value(), mlir_context),\n         input_shape.dimensions(), {});\n   }\n   if (std::holds_alternative<ShapeUtil::BitcastDecompositionReshape>(\n           *decomposed_bitcast)) {\n     // Note: ComputeReshapeIndexingMap assumes it's computing an output->input\n     // indexing, so input and output are reversed.\n     return IndexingMap::FromTensorSizes(\n-        ComputeReshapeIndexingMap(output_shape, input_shape,\n-                                  symbolic_expr_context),\n+        ComputeReshapeIndexingMap(output_shape, input_shape, mlir_context),\n         input_shape.dimensions(), {});\n   }\n   // `trt` stands for transpose-reshape-transpose decomposition of bitcast.\n   auto trt = std::get<ShapeUtil::BitcastDecompositionTrt>(*decomposed_bitcast);\n   auto transpose_map_1 =\n-      ComputeTransposeIndexingMap(trt.transpose1_dims, symbolic_expr_context);\n+      ComputeTransposeIndexingMap(trt.transpose1_dims, mlir_context);\n   auto reshape_map = ComputeReshapeIndexingMap(\n-      trt.reshape_shape, trt.transpose1_shape, symbolic_expr_context);\n+      trt.reshape_shape, trt.transpose1_shape, mlir_context);\n   auto transpose_map_2 =\n-      ComputeTransposeIndexingMap(trt.transpose2_dims, symbolic_expr_context);\n+      ComputeTransposeIndexingMap(trt.transpose2_dims, mlir_context);\n   auto bitcast_map =\n       transpose_map_2.compose(reshape_map).compose(transpose_map_1);\n   return IndexingMap::FromTensorSizes(bitcast_map, input_shape.dimensions(),\n@@ -1372,17 +1321,17 @@ IndexingMap GetBitcastMap(const Shape& input_shape, const Shape& output_shape,\n namespace {\n \n HloInstructionIndexing ComputeOutputToInputBitcastOpIndexing(\n-    const HloInstruction* bitcast, SymbolicExprContext* symbolic_expr_context) {\n-  auto bitcast_map = GetBitcastMap(\n-      bitcast->shape(), bitcast->operand(0)->shape(), symbolic_expr_context);\n+    const HloInstruction* bitcast, MLIRContext* mlir_context) {\n+  auto bitcast_map = GetBitcastMap(bitcast->shape(),\n+                                   bitcast->operand(0)->shape(), mlir_context);\n   bitcast_map.Simplify(IndexingMap::SimplifyPointDimensions::kPreserve);\n   return HloInstructionIndexing::FromIndexingMaps({bitcast_map});\n }\n \n HloInstructionIndexing ComputeInputToOutputBitcastOpIndexing(\n-    const HloInstruction* bitcast, SymbolicExprContext* symbolic_expr_context) {\n+    const HloInstruction* bitcast, MLIRContext* mlir_context) {\n   auto bitcast_map = GetBitcastMap(bitcast->operand(0)->shape(),\n-                                   bitcast->shape(), symbolic_expr_context);\n+                                   bitcast->shape(), mlir_context);\n   bitcast_map.Simplify(IndexingMap::SimplifyPointDimensions::kPreserve);\n   return HloInstructionIndexing::FromIndexingMaps({bitcast_map});\n }\n@@ -1399,21 +1348,19 @@ std::vector<int64_t> ToTransposeDimensions(const Layout& l) {\n }  // namespace\n \n IndexingMap CreateIdentityMap(absl::Span<const int64_t> dimensions,\n-                              SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+                              MLIRContext* mlir_context) {\n   return IndexingMap::FromTensorSizes(\n       AffineMap::getMultiDimIdentityMap(dimensions.size(), mlir_context),\n       /*dim_upper_bounds=*/dimensions, /*symbol_upper_bounds=*/{});\n }\n \n-IndexingMap CreateIdentityMap(const Shape& shape,\n-                              SymbolicExprContext* symbolic_expr_context) {\n+IndexingMap CreateIdentityMap(const Shape& shape, MLIRContext* mlir_context) {\n   if (shape.IsTuple()) {\n     // Should happen only for variadic reduce. In that case all tuple shapes are\n     // equal.\n-    return CreateIdentityMap(shape.tuple_shapes(0), symbolic_expr_context);\n+    return CreateIdentityMap(shape.tuple_shapes(0), mlir_context);\n   }\n-  return CreateIdentityMap(shape.dimensions(), symbolic_expr_context);\n+  return CreateIdentityMap(shape.dimensions(), mlir_context);\n }\n \n llvm::SmallVector<AffineExpr, 4> DelinearizeInBoundsIndex(\n@@ -1444,31 +1391,29 @@ llvm::SmallVector<AffineExpr, 4> DelinearizeInBoundsIndex(\n }\n \n IndexingMap GetIndexingMapFromPhysicalLayoutToLogical(\n-    const Shape& shape, SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    const Shape& shape, MLIRContext* mlir_context) {\n   if (shape.dimensions().size() == 0) {\n     return IndexingMap(AffineMap::get(mlir_context),\n                        /*dimensions=*/{}, /*range vars=*/{}, /*rt_vars=*/{});\n   }\n   return IndexingMap::FromTensorSizes(\n       ComputeTransposeIndexingMap(\n           InversePermutation(ToTransposeDimensions(shape.layout())),\n-          symbolic_expr_context),\n+          mlir_context),\n       ShapeUtil::MakeShapeWithDescendingLayoutAndSamePhysicalLayout(shape)\n           .dimensions(),\n       {});\n }\n \n IndexingMap GetIndexingMapFromLogicalToPhysicalLayout(\n-    const Shape& shape, SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* mlir_context = symbolic_expr_context->GetMLIRContext();\n+    const Shape& shape, MLIRContext* mlir_context) {\n   if (shape.dimensions().size() == 0) {\n     return IndexingMap(AffineMap::get(mlir_context),\n                        /*dimensions=*/{}, /*range vars=*/{}, /*rt_vars=*/{});\n   }\n   return IndexingMap::FromTensorSizes(\n       ComputeTransposeIndexingMap(ToTransposeDimensions(shape.layout()),\n-                                  symbolic_expr_context),\n+                                  mlir_context),\n       shape.dimensions(), {});\n }\n \n@@ -1561,9 +1506,9 @@ GroupedByOpIndexing GroupIndexingMapsByProducers(\n \n GroupedByOpIndexing ComputeGroupedOutputToInputIndexing(\n     const HloFusionAdaptor& fusion_adaptor, HloInstructionAdaptor target_instr,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  OperandIndexing initial_map = OperandIndexing(CreateIdentityMap(\n-      target_instr.instruction().shape(), symbolic_expr_context));\n+    MLIRContext* mlir_context) {\n+  OperandIndexing initial_map = OperandIndexing(\n+      CreateIdentityMap(target_instr.instruction().shape(), mlir_context));\n \n   GroupedByOpIndexing grouped_indexing_maps;\n   // If target_instr is a parameter of a fusion, then we create an identity map\n@@ -1587,7 +1532,7 @@ GroupedByOpIndexing ComputeGroupedOutputToInputIndexing(\n   for (; it != post_order.rend(); ++it) {\n     auto producer_indexing =\n         ComputeOutputToInputIndexing(&it->instruction(),\n-                                     /*output_id=*/0, symbolic_expr_context);\n+                                     /*output_id=*/0, mlir_context);\n     auto consumer_indexing_maps =\n         grouped_indexing_maps.find(&it->instruction());\n     if (consumer_indexing_maps == grouped_indexing_maps.end()) {\n@@ -1633,20 +1578,19 @@ Shape GetLinearizedShape(const Shape& shape) {\n \n llvm::SmallVector<IndexingMap, 4> MapLogicalToLinearizedPhysicalShape(\n     absl::Span<const HloInstruction* const> operands,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   llvm::SmallVector<IndexingMap, 4> indexing_maps;\n   // For every operand compute thread ID -> physical layout of operand\n   // indexing map.\n   for (const HloInstruction* operand : operands) {\n     const Shape& operand_shape = operand->shape();\n \n     IndexingMap operand_logical_to_physical_map =\n-        GetIndexingMapFromLogicalToPhysicalLayout(operand_shape,\n-                                                  symbolic_expr_context);\n+        GetIndexingMapFromLogicalToPhysicalLayout(operand_shape, mlir_context);\n     IndexingMap operand_physical_to_linearized_shape = GetBitcastMap(\n         ShapeUtil::MakeShapeWithDescendingLayoutAndSamePhysicalLayout(\n             operand_shape),\n-        GetLinearizedShape(operand_shape), symbolic_expr_context);\n+        GetLinearizedShape(operand_shape), mlir_context);\n     IndexingMap operand_logical_to_linearized_physical_shape =\n         operand_logical_to_physical_map * operand_physical_to_linearized_shape;\n     operand_logical_to_linearized_physical_shape.Simplify();\n@@ -1662,8 +1606,7 @@ void GetThreadIdToInputMemoryLayoutsMaps(\n     const HloInstructionAdaptor& hero,\n     absl::Span<const HloInstruction* const> operands,\n     absl::Span<const IndexingMap> operand_logical_to_linearized_physical_maps,\n-    SymbolicExprContext* symbolic_expr_context,\n-    GroupedByOpIndexingMap& result) {\n+    MLIRContext* mlir_context, GroupedByOpIndexingMap& result) {\n   for (const auto& [hero_operand_index, hero_operand] :\n        llvm::enumerate(hero.GetOperands())) {\n     if (hero_operand.shape().dimensions().empty()) {\n@@ -1675,7 +1618,7 @@ void GetThreadIdToInputMemoryLayoutsMaps(\n     // Compute indexing from output to inputs for logical layout.\n     GroupedByOpIndexing instr_indexing_keyed_by_operands =\n         ComputeGroupedOutputToInputIndexing(fusion_adaptor, hero_operand,\n-                                            symbolic_expr_context);\n+                                            mlir_context);\n     // For every operand compute thread ID -> physical layout of operand\n     // indexing map.\n     for (auto&& [operand, operand_linearized_physical_map] :\n@@ -1713,17 +1656,17 @@ void AssignValuesToRTVars(IndexingMap* indexing_map) {\n   if (indexing_map->GetRTVarsCount() == 0) {\n     return;\n   }\n-  MLIRContext* mlir_context = indexing_map->GetMLIRContext();\n   llvm::SmallVector<AffineExpr, 2> symbol_replacements;\n   for (int64_t symbol_id = 0; symbol_id < indexing_map->GetRangeVarsCount();\n        ++symbol_id) {\n     symbol_replacements.push_back(\n-        mlir::getAffineSymbolExpr(symbol_id, mlir_context));\n+        mlir::getAffineSymbolExpr(symbol_id, indexing_map->GetMLIRContext()));\n   }\n   for (const IndexingMap::Variable& rt_var : indexing_map->GetRTVars()) {\n     // Take midpoint of the feasible interval for the RT variable.\n-    symbol_replacements.push_back(getAffineConstantExpr(\n-        (rt_var.bounds.lower + rt_var.bounds.upper) / 2, mlir_context));\n+    symbol_replacements.push_back(\n+        getAffineConstantExpr((rt_var.bounds.lower + rt_var.bounds.upper) / 2,\n+                              indexing_map->GetMLIRContext()));\n   }\n   AffineMap thread_x_to_input_no_dim_symbols =\n       indexing_map->GetAffineMap().replaceDimsAndSymbols(\n@@ -1738,9 +1681,7 @@ void AssignValuesToRTVars(IndexingMap* indexing_map) {\n }\n \n HloInstructionIndexing ComputeOutputToInputAllGatherOpIndexing(\n-    const HloAllGatherInstruction* instr,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  MLIRContext* ctx = symbolic_expr_context->GetMLIRContext();\n+    const HloAllGatherInstruction* instr, MLIRContext* mlir_context) {\n   // CHECK_EQ(instr->all_gather_dimension(), 0);\n   // if (instr->all_gather_dimension() != 0) {\n   //   return CreateUnknownIndexing(instr->operand_count());\n@@ -1757,112 +1698,106 @@ HloInstructionIndexing ComputeOutputToInputAllGatherOpIndexing(\n       instr->operand(0)->shape().dimensions()[instr->all_gather_dimension()];\n \n   for (int64_t i = 0; i < output_rank; ++i) {\n-    auto dim = mlir::getAffineDimExpr(i, ctx);\n+    auto dim = mlir::getAffineDimExpr(i, mlir_context);\n     exprs.push_back(i == all_gather_dim ? dim % all_gather_input_dim_size\n                                         : dim);\n   }\n \n   IndexingMap indexing_map = IndexingMap::FromTensorSizes(\n-      AffineMap::get(output_rank, /*symbolCount=*/0, exprs, ctx),\n+      AffineMap::get(output_rank, /*symbolCount=*/0, exprs, mlir_context),\n       instr->shape().dimensions(), {});\n \n-  AffineExpr replica_id_expr = mlir::getAffineDimExpr(all_gather_dim, ctx)\n-                                   .floorDiv(all_gather_input_dim_size);\n+  AffineExpr replica_id_expr =\n+      mlir::getAffineDimExpr(all_gather_dim, mlir_context)\n+          .floorDiv(all_gather_input_dim_size);\n \n   IndexingMap replica_id_map = IndexingMap::FromTensorSizes(\n-      AffineMap::get(output_rank, /*symbolCount=*/0, replica_id_expr, ctx),\n+      AffineMap::get(output_rank, /*symbolCount=*/0, replica_id_expr,\n+                     mlir_context),\n       instr->shape().dimensions(), {});\n \n   OperandIndexing operand_indexing(indexing_map, {}, replica_id_map);\n \n   return HloInstructionIndexing::FromOperandIndexing({operand_indexing});\n }\n \n-HloInstructionIndexing ComputeOutputToInputIndexing(\n-    const HloInstruction* instr, int output_id,\n-    SymbolicExprContext* symbolic_expr_context) {\n+HloInstructionIndexing ComputeOutputToInputIndexing(const HloInstruction* instr,\n+                                                    int output_id,\n+                                                    MLIRContext* mlir_context) {\n   if (HloInstruction::IsOpElementwise(instr->opcode()) ||\n       // Note: map has a `dimensions` attribute, but it does nothing. See\n       // b/65689298.\n       instr->opcode() == HloOpcode::kMap ||\n       // For a single device, all-reduce is an elementwise op.\n       instr->opcode() == HloOpcode::kAllReduceStart) {\n-    return ComputeOutputToInputCwiseOpIndexing(instr, symbolic_expr_context);\n+    return ComputeOutputToInputCwiseOpIndexing(instr, mlir_context);\n   }\n   if (instr->opcode() == HloOpcode::kBitcast) {\n-    return ComputeOutputToInputBitcastOpIndexing(instr, symbolic_expr_context);\n+    return ComputeOutputToInputBitcastOpIndexing(instr, mlir_context);\n   }\n   // go/keep-sorted start\n   if (auto all_gather = DynCast<HloAllGatherInstruction>(instr)) {\n-    return ComputeOutputToInputAllGatherOpIndexing(all_gather,\n-                                                   symbolic_expr_context);\n+    return ComputeOutputToInputAllGatherOpIndexing(all_gather, mlir_context);\n   }\n   if (auto broadcast = DynCast<HloBroadcastInstruction>(instr)) {\n-    return ComputeOutputToInputBroadcastOpIndexing(broadcast,\n-                                                   symbolic_expr_context);\n+    return ComputeOutputToInputBroadcastOpIndexing(broadcast, mlir_context);\n   }\n   if (auto concat = DynCast<HloConcatenateInstruction>(instr)) {\n-    return ComputeOutputToInputConcatenateOpIndexing(concat,\n-                                                     symbolic_expr_context);\n+    return ComputeOutputToInputConcatenateOpIndexing(concat, mlir_context);\n   }\n   if (auto constant = DynCast<HloConstantInstruction>(instr)) {\n     return HloInstructionIndexing{};\n   }\n   if (auto convolution = DynCast<HloConvolutionInstruction>(instr)) {\n-    return ComputeOutputToInputConvolutionOpIndexing(convolution,\n-                                                     symbolic_expr_context);\n+    return ComputeOutputToInputConvolutionOpIndexing(convolution, mlir_context);\n   }\n   if (auto dot = DynCast<HloDotInstruction>(instr)) {\n-    return ComputeOutputToInputDotOpIndexing(dot, symbolic_expr_context);\n+    return ComputeOutputToInputDotOpIndexing(dot, mlir_context);\n   }\n   if (auto dus = DynCast<HloDynamicUpdateSliceInstruction>(instr)) {\n-    return ComputeOutputToInputDynamicUpdateSliceOpIndexing(\n-        dus, symbolic_expr_context);\n+    return ComputeOutputToInputDynamicUpdateSliceOpIndexing(dus, mlir_context);\n   }\n   if (auto dynamic_slice = DynCast<HloDynamicSliceInstruction>(instr)) {\n     return ComputeOutputToInputDynamicSliceOpIndexing(dynamic_slice,\n-                                                      symbolic_expr_context);\n+                                                      mlir_context);\n   }\n   if (auto fusion = DynCast<HloFusionInstruction>(instr)) {\n     return ComputeOutputToInputFusionOpIndexing(fusion, output_id,\n-                                                symbolic_expr_context);\n+                                                mlir_context);\n   }\n   if (auto gather = DynCast<HloGatherInstruction>(instr)) {\n-    return ComputeOutputToInputGatherOpIndexing(gather, symbolic_expr_context);\n+    return ComputeOutputToInputGatherOpIndexing(gather, mlir_context);\n   }\n   if (auto iota = DynCast<HloIotaInstruction>(instr)) {\n     return HloInstructionIndexing{};\n   }\n   if (auto pad = DynCast<HloPadInstruction>(instr)) {\n-    return ComputeOutputToInputPadOpIndexing(pad, symbolic_expr_context);\n+    return ComputeOutputToInputPadOpIndexing(pad, mlir_context);\n   }\n   if (auto parameter = DynCast<HloParameterInstruction>(instr)) {\n     return HloInstructionIndexing{};\n   }\n   if (auto reduce = DynCast<HloReduceInstruction>(instr)) {\n-    return ComputeOutputToInputReduceOpIndexing(reduce, symbolic_expr_context);\n+    return ComputeOutputToInputReduceOpIndexing(reduce, mlir_context);\n   }\n   if (auto reduce_window = DynCast<HloReduceWindowInstruction>(instr)) {\n     return ComputeOutputToInputReduceWindowOpIndexing(reduce_window,\n-                                                      symbolic_expr_context);\n+                                                      mlir_context);\n   }\n   if (auto reshape = DynCast<HloReshapeInstruction>(instr)) {\n-    return ComputeOutputToInputReshapeOpIndexing(reshape,\n-                                                 symbolic_expr_context);\n+    return ComputeOutputToInputReshapeOpIndexing(reshape, mlir_context);\n   }\n   if (auto reverse = DynCast<HloReverseInstruction>(instr)) {\n-    return ComputeReverseOpIndexing(reverse, symbolic_expr_context);\n+    return ComputeReverseOpIndexing(reverse, mlir_context);\n   }\n   if (auto scaled_dot = DynCast<HloScaledDotInstruction>(instr)) {\n-    return ComputeOutputToInputScaledDotOpIndexing(scaled_dot,\n-                                                   symbolic_expr_context);\n+    return ComputeOutputToInputScaledDotOpIndexing(scaled_dot, mlir_context);\n   }\n   if (auto slice = DynCast<HloSliceInstruction>(instr)) {\n-    return ComputeOutputToInputSliceOpIndexing(slice, symbolic_expr_context);\n+    return ComputeOutputToInputSliceOpIndexing(slice, mlir_context);\n   }\n   if (auto transpose = DynCast<HloTransposeInstruction>(instr)) {\n-    return ComputeOutputToInputTransposeOpIndexing(transpose,\n-                                                   symbolic_expr_context);\n+    return ComputeOutputToInputTransposeOpIndexing(transpose, mlir_context);\n   }\n   // go/keep-sorted end\n   LOG(ERROR) << \"ComputeOutputToInputIndexing is not implemented for opcode \"\n@@ -1872,51 +1807,47 @@ HloInstructionIndexing ComputeOutputToInputIndexing(\n   return CreateUnknownIndexing(instr->operand_count());\n }\n \n-HloInstructionIndexing ComputeInputToOutputIndexing(\n-    const HloInstruction* instr, int input_id,\n-    SymbolicExprContext* symbolic_expr_context) {\n+HloInstructionIndexing ComputeInputToOutputIndexing(const HloInstruction* instr,\n+                                                    int input_id,\n+                                                    MLIRContext* mlir_context) {\n   if (HloInstruction::IsOpElementwise(instr->opcode()) ||\n       // Note: map has a `dimensions` attribute, but it does nothing. See\n       // b/65689298.\n       instr->opcode() == HloOpcode::kMap ||\n       // For a single device, all-reduce has 1:1 output to input mapping.\n       instr->opcode() == HloOpcode::kAllReduceStart) {\n-    return ComputeInputToOutputCwiseOpIndexing(instr, symbolic_expr_context);\n+    return ComputeInputToOutputCwiseOpIndexing(instr, mlir_context);\n   }\n   if (instr->opcode() == HloOpcode::kBitcast) {\n-    return ComputeInputToOutputBitcastOpIndexing(instr, symbolic_expr_context);\n+    return ComputeInputToOutputBitcastOpIndexing(instr, mlir_context);\n   }\n   // go/keep-sorted start\n   if (auto broadcast = DynCast<HloBroadcastInstruction>(instr)) {\n-    return ComputeInputToOutputBroadcastOpIndexing(broadcast,\n-                                                   symbolic_expr_context);\n+    return ComputeInputToOutputBroadcastOpIndexing(broadcast, mlir_context);\n   }\n   if (auto concat = DynCast<HloConcatenateInstruction>(instr)) {\n     return ComputeInputToOutputConcatenateOpIndexing(concat, input_id,\n-                                                     symbolic_expr_context);\n+                                                     mlir_context);\n   }\n   if (auto reduce = DynCast<HloReduceInstruction>(instr)) {\n-    return ComputeInputToOutputReduceOpIndexing(reduce, input_id,\n-                                                symbolic_expr_context);\n+    return ComputeInputToOutputReduceOpIndexing(reduce, input_id, mlir_context);\n   }\n   if (auto reshape = DynCast<HloReshapeInstruction>(instr)) {\n-    return ComputeInputToOutputReshapeOpIndexing(reshape,\n-                                                 symbolic_expr_context);\n+    return ComputeInputToOutputReshapeOpIndexing(reshape, mlir_context);\n   }\n   if (auto reverse = DynCast<HloReverseInstruction>(instr)) {\n-    return ComputeReverseOpIndexing(reverse, symbolic_expr_context);\n+    return ComputeReverseOpIndexing(reverse, mlir_context);\n   }\n   if (auto slice = DynCast<HloSliceInstruction>(instr)) {\n-    return ComputeInputToOutputSliceOpIndexing(slice, symbolic_expr_context);\n+    return ComputeInputToOutputSliceOpIndexing(slice, mlir_context);\n   }\n   if (auto transpose = DynCast<HloTransposeInstruction>(instr)) {\n-    return ComputeInputToOutputTransposeOpIndexing(transpose,\n-                                                   symbolic_expr_context);\n+    return ComputeInputToOutputTransposeOpIndexing(transpose, mlir_context);\n   }\n   // go/keep-sorted end\n   if (instr->opcode() == HloOpcode::kTuple) {\n     return HloInstructionIndexing::FromIndexingMaps({CreateIdentityMap(\n-        instr->shape().tuple_shapes(input_id), symbolic_expr_context)});\n+        instr->shape().tuple_shapes(input_id), mlir_context)});\n   }\n   // If we cannot compute input-to-output indexing, we return std::nullopt for\n   // every op result.\n@@ -1927,17 +1858,17 @@ HloInstructionIndexing ComputeInputToOutputIndexing(\n \n IndexingMap ComputeEpilogueInputToOutputIndexing(\n     HloInstructionAdaptor epilogue_parent, HloInstructionAdaptor epilogue_root,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   std::vector<HloInstructionAdaptor> chain =\n       HloFindUseChain(epilogue_parent, epilogue_root);\n   CHECK(!chain.empty()) << \"There is no use chain from parent to root\";\n   OperandIndexing root_indexing(\n-      CreateIdentityMap(epilogue_parent.shape(), symbolic_expr_context));\n+      CreateIdentityMap(epilogue_parent.shape(), mlir_context));\n   for (int i = 1; i < chain.size(); ++i) {\n     const auto& producer = chain[i - 1].instruction();\n     const auto& user = chain[i].instruction();\n     auto user_indexing = ComputeInputToOutputIndexing(\n-        &user, user.operand_index(&producer), symbolic_expr_context);\n+        &user, user.operand_index(&producer), mlir_context);\n     root_indexing = ComposeOperandIndexing(\n         {root_indexing}, *user_indexing.indexing_maps[0].begin());\n     root_indexing.Simplify();"
        },
        {
            "sha": "e0650668397b83cc1c72f2d09e67a5d79cecd1d6",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_analysis.h",
            "status": "modified",
            "additions": 17,
            "deletions": 18,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -73,13 +73,12 @@ std::ostream& operator<<(std::ostream& out,\n // of the `output_id` instruction output.\n HloInstructionIndexing ComputeOutputToInputIndexing(\n     const HloInstruction* instr, int output_id,\n-    SymbolicExprContext* symbolic_expr_context);\n+    mlir::MLIRContext* mlir_context);\n \n // Computes indexing maps for all output operands that the element of the\n // `input_id` instruction input will participate in.\n HloInstructionIndexing ComputeInputToOutputIndexing(\n-    const HloInstruction* instr, int input_id,\n-    SymbolicExprContext* symbolic_expr_context);\n+    const HloInstruction* instr, int input_id, mlir::MLIRContext* mlir_context);\n \n // Computes the indexing for `epilogue_parent`'s epilogue. For example, if\n // `epilogue_parent` is a transpose, computes the input to output indexing for\n@@ -107,7 +106,7 @@ HloInstructionIndexing ComputeInputToOutputIndexing(\n // fusion does not make much sense, but they are created sometimes.\n IndexingMap ComputeEpilogueInputToOutputIndexing(\n     HloInstructionAdaptor epilogue_parent, HloInstructionAdaptor epilogue_root,\n-    SymbolicExprContext* symbolic_expr_context);\n+    mlir::MLIRContext* mlir_context);\n \n // Indexing of the runtime variable of the HLO instruction.\n struct RuntimeVarIndexing {\n@@ -206,13 +205,13 @@ using GroupedByOpIndexing =\n // cluster starting with `target_instr` and going from def to use.\n GroupedByOpIndexing ComputeGroupedOutputToInputIndexing(\n     const HloFusionAdaptor& fusion_adaptor, HloInstructionAdaptor target_instr,\n-    SymbolicExprContext* symbolic_expr_context);\n+    mlir::MLIRContext* mlir_context);\n \n // Returns the indexing map from logical to linearized physical shape for each\n // operand.\n llvm::SmallVector<IndexingMap, 4> MapLogicalToLinearizedPhysicalShape(\n     absl::Span<const HloInstruction* const> operands,\n-    SymbolicExprContext* symbolic_expr_context);\n+    mlir::MLIRContext* mlir_context);\n \n // Computes the indexing map from logical to linearized physical shape for each\n // operand and adds them to `result`. `result` may be non-empty when this\n@@ -224,7 +223,7 @@ void GetThreadIdToInputMemoryLayoutsMaps(\n     const HloInstructionAdaptor& hero,\n     absl::Span<const HloInstruction* const> operands,\n     absl::Span<const IndexingMap> operand_logical_to_linearized_physical_maps,\n-    SymbolicExprContext* symbolic_expr_context, GroupedByOpIndexingMap& result);\n+    mlir::MLIRContext* mlir_context, GroupedByOpIndexingMap& result);\n \n // Replaces RTVars with the midpoints of the feasible intervals.\n void AssignValuesToRTVars(IndexingMap* indexing_map);\n@@ -237,23 +236,23 @@ GroupedByOpIndexing GroupIndexingMapsByProducers(\n // Equivalent to linearizing the input_shape index and then delinearizing it\n // to output_shape.\n IndexingMap GetBitcastMap(const Shape& input_shape, const Shape& output_shape,\n-                          SymbolicExprContext* symbolic_expr_context);\n+                          mlir::MLIRContext* mlir_context);\n IndexingMap GetBitcastMap(absl::Span<const int64_t> input_shape,\n                           const Shape& output_shape,\n-                          SymbolicExprContext* symbolic_expr_context);\n+                          mlir::MLIRContext* mlir_context);\n IndexingMap GetBitcastMap(absl::Span<const int64_t> input_shape,\n                           absl::Span<const int64_t> output_shape,\n-                          SymbolicExprContext* symbolic_expr_context);\n+                          mlir::MLIRContext* mlir_context);\n \n // Creates an indexing map from the physical layout of the tensor to its logical\n // layout.\n IndexingMap GetIndexingMapFromPhysicalLayoutToLogical(\n-    const Shape& shape, SymbolicExprContext* symbolic_expr_context);\n+    const Shape& shape, mlir::MLIRContext* mlir_context);\n \n // Creates an indexing map from the logical layout of the tensor to its physical\n // layout.\n IndexingMap GetIndexingMapFromLogicalToPhysicalLayout(\n-    const Shape& shape, SymbolicExprContext* symbolic_expr_context);\n+    const Shape& shape, mlir::MLIRContext* mlir_context);\n \n // Returns the shape of the output of the instruction.\n const Shape& GetOutputShape(const HloInstruction* instr, int64_t output_id);\n@@ -262,18 +261,18 @@ const Shape& GetOutputShape(const HloInstruction* instr, int64_t output_id);\n mlir::AffineExpr LinearizeShape(\n     absl::Span<const int64_t> dims,\n     absl::Span<const mlir::AffineExpr> dimension_exprs,\n-    SymbolicExprContext* symbolic_expr_context);\n+    mlir::MLIRContext* mlir_context);\n \n // Computes N-d indexing expressions given a linear index and a shape.\n-std::vector<mlir::AffineExpr> DelinearizeIndex(\n-    absl::Span<const int64_t> dims, mlir::AffineExpr linear_index,\n-    SymbolicExprContext* symbolic_expr_context);\n+std::vector<mlir::AffineExpr> DelinearizeIndex(absl::Span<const int64_t> dims,\n+                                               mlir::AffineExpr linear_index,\n+                                               mlir::MLIRContext* mlir_context);\n \n // Creates an identity indexing map corresponding to the parameter shape.\n IndexingMap CreateIdentityMap(const Shape& shape,\n-                              SymbolicExprContext* symbolic_expr_context);\n+                              mlir::MLIRContext* mlir_context);\n IndexingMap CreateIdentityMap(absl::Span<const int64_t> dimensions,\n-                              SymbolicExprContext* symbolic_expr_context);\n+                              mlir::MLIRContext* mlir_context);\n \n llvm::SmallVector<mlir::AffineExpr, 4> DelinearizeInBoundsIndex(\n     mlir::AffineExpr linear, absl::Span<const int64_t> sizes);"
        },
        {
            "sha": "74aaf30a25216fbd15b81a6a1c7f97d24dd4a6b3",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_analysis_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_analysis_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -90,7 +90,7 @@ TEST_F(IndexingAnalysisTest, ComputeGroupedOutputToInputIndexing) {\n   auto fusion_adaptor = HloFusionAdaptor::ForProducerConsumer(transpose, root);\n \n   auto grouped_indexing = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &symbolic_expr_context_);\n+      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &mlir_context_);\n   EXPECT_THAT(grouped_indexing,\n               UnorderedElementsAre(\n                   Pair(root, ElementsAre(MatchOperandIndexing(R\"(\n@@ -147,7 +147,7 @@ TEST_F(IndexingAnalysisTest,\n   auto fusion_adaptor = HloFusionAdaptor::ForInstruction(root);\n \n   auto grouped_indexing = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &symbolic_expr_context_);\n+      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &mlir_context_);\n \n   EXPECT_THAT(grouped_indexing,\n               UnorderedElementsAre(\n@@ -200,7 +200,7 @@ TEST_F(IndexingAnalysisTest, ComputeGroupedOutputToInputIndexing_SingleOp) {\n   HloInstructionAdaptor parameter_adaptor =\n       fusion_adaptor->GetRoots()[0].GetOperand(0);\n   auto grouped_indexing = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, parameter_adaptor, &symbolic_expr_context_);\n+      *fusion_adaptor, parameter_adaptor, &mlir_context_);\n   EXPECT_THAT(\n       grouped_indexing,\n       UnorderedElementsAre(Pair(parameter, ElementsAre(MatchOperandIndexing(R\"(\n@@ -241,7 +241,7 @@ TEST_F(IndexingAnalysisTest,\n   auto parameter_0 = bcast.GetOperand(0);\n \n   auto grouped_indexing = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, bcast, &symbolic_expr_context_);\n+      *fusion_adaptor, bcast, &mlir_context_);\n   EXPECT_THAT(\n       grouped_indexing,\n       UnorderedElementsAre(\n@@ -2693,8 +2693,8 @@ TEST_F(IndexingAnalysisTest, EpilogueIndexing) {\n   HloInstructionAdaptor log(*computation->GetInstructionWithName(\"log\"),\n                             fusion.get());\n \n-  EXPECT_THAT(ToString(ComputeEpilogueInputToOutputIndexing(\n-                  transpose, log, &symbolic_expr_context_)),\n+  EXPECT_THAT(ToString(ComputeEpilogueInputToOutputIndexing(transpose, log,\n+                                                            &mlir_context_)),\n               MatchIndexingString(R\"(\n                   (d0, d1) -> (d1 * 1000 + d0),\n                   domain:\n@@ -2723,7 +2723,7 @@ TEST_F(IndexingAnalysisTest, EpilogueIndexing_NoEpilogue) {\n                                   fusion.get());\n \n   EXPECT_THAT(ToString(ComputeEpilogueInputToOutputIndexing(\n-                  transpose, transpose, &symbolic_expr_context_)),\n+                  transpose, transpose, &mlir_context_)),\n               MatchIndexingString(R\"(\n                   (d0, d1) -> (d0, d1),\n                   domain:\n@@ -3036,7 +3036,7 @@ TEST_F(IndexingAnalysisTest, AllGatherFusionWithReshape) {\n   auto fusion_adaptor = HloFusionAdaptor::ForProducerConsumer(all_gather, root);\n \n   auto grouped_indexing = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &symbolic_expr_context_);\n+      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &mlir_context_);\n \n   EXPECT_THAT(grouped_indexing[root], ElementsAre(MatchOperandIndexing(R\"(\n     (d0) -> (d0),\n@@ -3080,7 +3080,7 @@ TEST_F(IndexingAnalysisTest, ChainedAllGatherFusion) {\n   auto fusion_adaptor = HloFusionAdaptor::ForProducerConsumer(all_gather, root);\n \n   auto grouped_indexing = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &symbolic_expr_context_);\n+      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &mlir_context_);\n \n   EXPECT_THAT(grouped_indexing[parameter],\n               ElementsAre(UndefinedOperandIndexing()));\n@@ -3104,7 +3104,7 @@ TEST_F(IndexingAnalysisTest, AllGatherDotFusion_GatherNonContractingDim) {\n   auto fusion_adaptor = HloFusionAdaptor::ForProducerConsumer(all_gather, root);\n \n   auto grouped_indexing = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &symbolic_expr_context_);\n+      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &mlir_context_);\n \n   EXPECT_THAT(grouped_indexing[parameter], ElementsAre(MatchOperandIndexing(R\"(\n     (d0, d1)[s0] -> (d0 mod 64, s0),\n@@ -3139,7 +3139,7 @@ TEST_F(IndexingAnalysisTest, AllGatherDotFusion_GatherContractingDim) {\n   auto fusion_adaptor = HloFusionAdaptor::ForProducerConsumer(all_gather, root);\n \n   auto grouped_indexing = ComputeGroupedOutputToInputIndexing(\n-      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &symbolic_expr_context_);\n+      *fusion_adaptor, fusion_adaptor->GetRoots()[0], &mlir_context_);\n \n   EXPECT_THAT(grouped_indexing[parameter], ElementsAre(MatchOperandIndexing(R\"(\n     (d0, d1)[s0] -> (d0, s0 mod 128),"
        },
        {
            "sha": "bcb0b595ac114e22f96597d44f9d6845d01a14f5",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_map_serialization.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_serialization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_serialization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_serialization.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -393,16 +393,15 @@ bool ParseAffineMapResults(Parser& parser,\n bool ParseAffineExprsWithMLIR(ArrayRef<std::string> dim_var_names,\n                               ArrayRef<std::string> symbol_var_names,\n                               ArrayRef<std::string> affine_expr_strings,\n-                              SymbolicExprContext* context,\n+                              mlir::MLIRContext* context,\n                               SmallVectorImpl<AffineExpr>& affine_exprs) {\n   std::stringstream ss;\n   ss << \"affine_map<(\" << absl::StrJoin(dim_var_names, \", \") << \") \";\n   if (!symbol_var_names.empty()) {\n     ss << '[' << absl::StrJoin(symbol_var_names, \", \") << \"] \";\n   }\n   ss << \" -> (\" << absl::StrJoin(affine_expr_strings, \", \") << \")>\";\n-  auto affine_map_attr =\n-      mlir::parseAttribute(ss.str(), context->GetMLIRContext());\n+  auto affine_map_attr = mlir::parseAttribute(ss.str(), context);\n   if (!affine_map_attr) {\n     llvm::errs() << \"Failed to parse affine map: \" << ss.str() << \"\\n\";\n     return false;\n@@ -598,8 +597,8 @@ void PrintAffineExprImpl(const AffineExpr affine_expr,\n \n }  // namespace\n \n-std::optional<IndexingMap> ParseIndexingMap(\n-    llvm::StringRef input, SymbolicExprContext* symbolic_expr_context) {\n+std::optional<IndexingMap> ParseIndexingMap(llvm::StringRef input,\n+                                            mlir::MLIRContext* mlir_context) {\n   Parser parser(input);\n \n   // Parse variable names.\n@@ -632,7 +631,7 @@ std::optional<IndexingMap> ParseIndexingMap(\n       llvm::errs() << \"Expected an empty indexing map\\n\";\n       return std::nullopt;\n     }\n-    return IndexingMap{AffineMap::get(symbolic_expr_context->GetMLIRContext()),\n+    return IndexingMap{AffineMap::get(mlir_context),\n                        /*dimensions=*/{}, /*range_vars=*/{}, /*rt_vars=*/{}};\n   }\n \n@@ -734,8 +733,7 @@ std::optional<IndexingMap> ParseIndexingMap(\n   symbol_var_names.append(rt_var_names.begin(), rt_var_names.end());\n   SmallVector<AffineExpr> affine_exprs;\n   if (!ParseAffineExprsWithMLIR(dim_var_names, symbol_var_names,\n-                                affine_expr_strs, symbolic_expr_context,\n-                                affine_exprs)) {\n+                                affine_expr_strs, mlir_context, affine_exprs)) {\n     llvm::errs() << \"Failed to parse affine expressions\\n\";\n     return std::nullopt;\n   }\n@@ -752,8 +750,7 @@ std::optional<IndexingMap> ParseIndexingMap(\n     constraints.push_back(std::make_pair(expr, bounds));\n   }\n   auto map = AffineMap::get(dim_vars.size(), range_vars.size() + rt_vars.size(),\n-                            affine_map_results,\n-                            symbolic_expr_context->GetMLIRContext());\n+                            affine_map_results, mlir_context);\n   return IndexingMap{map, std::move(dim_vars), std::move(range_vars),\n                      std::move(rt_vars), constraints};\n }"
        },
        {
            "sha": "b531cdc2ddb14bf1337a5524a44c918ea630d41f",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_map_serialization.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_serialization.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_serialization.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_serialization.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -31,8 +31,8 @@ limitations under the License.\n namespace xla {\n \n // Parses the given string into an IndexingMap.\n-std::optional<IndexingMap> ParseIndexingMap(\n-    llvm::StringRef input, SymbolicExprContext* symbolic_expr_context);\n+std::optional<IndexingMap> ParseIndexingMap(llvm::StringRef input,\n+                                            mlir::MLIRContext* mlir_context);\n \n // Prints AffineExpr using the default (d0, d1, ..., s0, s1, ...) variable\n // names."
        },
        {
            "sha": "2ae50aae18f57be9f03d345164e968f8e4d2bf86",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_map_serialization_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_serialization_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_serialization_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_serialization_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -35,10 +35,8 @@ using ::testing::HasSubstr;\n class IndexingMapSerializationTest : public HloHardwareIndependentTestBase {\n  public:\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n   void ParseAndCheck(absl::string_view indexing_map_str) {\n-    auto indexing_map =\n-        ParseIndexingMap(indexing_map_str, &symbolic_expr_context_);\n+    auto indexing_map = ParseIndexingMap(indexing_map_str, &mlir_context_);\n     ASSERT_TRUE(indexing_map.has_value());\n     EXPECT_THAT(ToString(*indexing_map), MatchIndexingString(indexing_map_str));\n   }\n@@ -52,8 +50,8 @@ TEST_F(IndexingMapSerializationTest, UndefinedMap) {\n }\n \n TEST_F(IndexingMapSerializationTest, KnownEmptyMap) {\n-  auto map = ParseIndexingMap(\"(d0) -> (), domain: d0 in [1, 0]\",\n-                              &symbolic_expr_context_);\n+  auto map =\n+      ParseIndexingMap(\"(d0) -> (), domain: d0 in [1, 0]\", &mlir_context_);\n   EXPECT_TRUE(map->IsKnownEmpty());\n   EXPECT_THAT(ToString(*map), MatchIndexingString(\"KNOWN EMPTY\"));\n }"
        },
        {
            "sha": "a00ee29785b81b29144c2977895588b68755694b",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_map_test.cc",
            "status": "modified",
            "additions": 31,
            "deletions": 44,
            "changes": 75,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_map_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -48,17 +48,13 @@ using ::testing::ElementsAre;\n \n class IndexingMapTest : public HloHardwareIndependentTestBase {\n  public:\n-  IndexingMapTest() : symbolic_expr_context_(&mlir_context_) {}\n-\n   IndexingMap Parse(absl::string_view indexing_map_str) {\n-    auto indexing_map =\n-        ParseIndexingMap(indexing_map_str, &symbolic_expr_context_);\n+    auto indexing_map = ParseIndexingMap(indexing_map_str, &mlir_context_);\n     EXPECT_TRUE(indexing_map.has_value());\n     return *indexing_map;\n   }\n \n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_;\n };\n \n std::vector<bool> ConvertToSTL(const llvm::SmallBitVector& bit_vector) {\n@@ -94,7 +90,7 @@ TEST_F(IndexingMapTest, VariableKind) {\n \n TEST_F(IndexingMapTest, VerifyDimensions) {\n   auto indexing_map = IndexingMap::FromTensorSizes(\n-      ParseAffineMap(\"(d0) -> (d0)\", &symbolic_expr_context_),\n+      ParseAffineMap(\"(d0) -> (d0)\", &mlir_context_),\n       /*dim_upper_bounds=*/{10, 10}, /*symbol_upper_bounds=*/{});\n \n   std::stringstream ss;\n@@ -106,7 +102,7 @@ TEST_F(IndexingMapTest, VerifyDimensions) {\n \n TEST_F(IndexingMapTest, VerifySymbols) {\n   auto indexing_map = IndexingMap::FromTensorSizes(\n-      ParseAffineMap(\"(d0) -> (d0)\", &symbolic_expr_context_),\n+      ParseAffineMap(\"(d0) -> (d0)\", &mlir_context_),\n       /*dim_upper_bounds=*/{10}, /*symbol_upper_bounds=*/{10});\n \n   std::stringstream ss;\n@@ -119,7 +115,7 @@ TEST_F(IndexingMapTest, VerifySymbols) {\n TEST_F(IndexingMapTest, RTVar) {\n   IndexingMap indexing_map(\n       ParseAffineMap(\"(d0, d1)[range, rt0, rt1] -> (d1, d0, range + rt0, rt1)\",\n-                     &symbolic_expr_context_),\n+                     &mlir_context_),\n       {IndexingMap::Variable{0, 99, \"d0\"}, IndexingMap::Variable{0, 43, \"d1\"}},\n       {IndexingMap::Variable{-99, 99, \"range\"}},\n       {IndexingMap::Variable{Interval{0, 2}},\n@@ -146,10 +142,8 @@ TEST_F(IndexingMapTest, EvaluateIgnoresDomainRanges) {\n   )\");\n \n   auto results = indexing_map.Evaluate(\n-      mlir::getAffineConstantExprs({1, 2},\n-                                   symbolic_expr_context_.GetMLIRContext()),\n-      mlir::getAffineConstantExprs({3, 4},\n-                                   symbolic_expr_context_.GetMLIRContext()));\n+      mlir::getAffineConstantExprs({1, 2}, &mlir_context_),\n+      mlir::getAffineConstantExprs({3, 4}, &mlir_context_));\n \n   EXPECT_THAT(results, ElementsAre(2, 1, 4, 3));\n }\n@@ -165,20 +159,16 @@ TEST_F(IndexingMapTest, ConstraintsSatisfied) {\n   )\");\n \n   auto feasible = indexing_map.ConstraintsSatisfied(\n-      mlir::getAffineConstantExprs({1, 2},\n-                                   symbolic_expr_context_.GetMLIRContext()),\n-      mlir::getAffineConstantExprs({3, 4},\n-                                   symbolic_expr_context_.GetMLIRContext()));\n+      mlir::getAffineConstantExprs({1, 2}, &mlir_context_),\n+      mlir::getAffineConstantExprs({3, 4}, &mlir_context_));\n   EXPECT_TRUE(feasible);\n \n-  indexing_map.AddConstraint(\n-      ParseAffineExpr(\"s0 mod 4\", &symbolic_expr_context_), Interval{0, 0});\n+  indexing_map.AddConstraint(ParseAffineExpr(\"s0 mod 4\", &mlir_context_),\n+                             Interval{0, 0});\n \n   auto infeasible = indexing_map.ConstraintsSatisfied(\n-      mlir::getAffineConstantExprs({1, 2},\n-                                   symbolic_expr_context_.GetMLIRContext()),\n-      mlir::getAffineConstantExprs({5, 4},\n-                                   symbolic_expr_context_.GetMLIRContext()));\n+      mlir::getAffineConstantExprs({1, 2}, &mlir_context_),\n+      mlir::getAffineConstantExprs({5, 4}, &mlir_context_));\n   EXPECT_FALSE(infeasible);\n }\n \n@@ -293,13 +283,13 @@ TEST_F(IndexingMapTest, Composition_RTVar) {\n   IndexingMap producer(\n       ParseAffineMap(\n           \"(d0, d1, d2)[rt0, rt1, rt2] -> (d0 + rt0, d1 + rt1, d2 + rt2)\",\n-          &symbolic_expr_context_),\n+          &mlir_context_),\n       {IndexingMap::Variable{{0, 0}}, IndexingMap::Variable{{0, 1}},\n        IndexingMap::Variable{{0, 226}}},\n       {}, std::move(rt_vars));\n \n   IndexingMap consumer(\n-      ParseAffineMap(\"(d0, d1)[s] -> (0, d1, s)\", &symbolic_expr_context_),\n+      ParseAffineMap(\"(d0, d1)[s] -> (0, d1, s)\", &mlir_context_),\n       {IndexingMap::Variable{0, 0}, IndexingMap::Variable{0, 1}},\n       {IndexingMap::Variable{0, 31, \"s\"}}, {});\n \n@@ -319,15 +309,15 @@ TEST_F(IndexingMapTest, Composition_RTVar) {\n TEST_F(IndexingMapTest, Composition_OnlyRTVars) {\n   IndexingMap producer(\n       ParseAffineMap(\"(d0, d1)[s0, s1] -> (d0 + s0, d1 + 4 * s1)\",\n-                     &symbolic_expr_context_),\n+                     &mlir_context_),\n       {IndexingMap::Variable{0, 24}, IndexingMap::Variable{0, 15}}, {},\n       {IndexingMap::Variable{Interval{0, 2}, \"ps_0\"},\n        IndexingMap::Variable{Interval{0, 1}, \"ps_1\"}});\n \n   std::vector<IndexingMap::Variable> consumer_rt_vars;\n   IndexingMap consumer(\n       ParseAffineMap(\"(d0, d1)[s0, s1] -> (d0 + 2 * s0, d1 + 3 * s1)\",\n-                     &symbolic_expr_context_),\n+                     &mlir_context_),\n       {IndexingMap::Variable{0, 24}, IndexingMap::Variable{0, 15}}, {},\n       {IndexingMap::Variable{Interval{0, 25}, \"cs_0\"},\n        IndexingMap::Variable{Interval{0, 16}, \"cs_1\"}});\n@@ -505,15 +495,14 @@ TEST_F(IndexingMapTest, RemoveUnusedSymbols_ConstraintsWithManySymbols) {\n TEST_F(IndexingMapTest, RemoveUnusedSymbols_ConstraintsWithRTVars) {\n   IndexingMap indexing_map(\n       ParseAffineMap(\"(d0)[s0, s1, s2, s3, s4] -> (d0 * 4 + s1 + s3 - 42)\",\n-                     &symbolic_expr_context_),\n+                     &mlir_context_),\n       {IndexingMap::Variable{{0, 31}}},\n       {IndexingMap::Variable{{0, 0}}, IndexingMap::Variable{{0, 1}},\n        IndexingMap::Variable{{0, 2}}},\n       {IndexingMap::Variable{Interval{0, 3}},\n        IndexingMap::Variable{Interval{0, 4}}});\n   indexing_map.AddConstraint(\n-      ParseAffineExpr(\"d0 * 4 + s1 + s3\", &symbolic_expr_context_),\n-      Interval{24, 459});\n+      ParseAffineExpr(\"d0 * 4 + s1 + s3\", &mlir_context_), Interval{24, 459});\n   indexing_map.RemoveUnusedSymbols();\n   // Symbols s0, s2, s4 will be removed and s1 and s3 will become s0 and s1.\n   EXPECT_THAT(indexing_map, MatchIndexingMap(R\"(\n@@ -601,13 +590,13 @@ TEST_F(IndexingMapTest, ConvertSymbolsToDimensions) {\n   IndexingMap indexing_map(\n       ParseAffineMap(\n           \"(d0)[s0, s1, s2, s3] -> (d0 * 4 + s0 + s1 + 2 * s2 + 3 * s3 - 42)\",\n-          &symbolic_expr_context_),\n+          &mlir_context_),\n       {IndexingMap::Variable{{0, 31}}},\n       {IndexingMap::Variable{{0, 0}}, IndexingMap::Variable{{0, 1}}},\n       {IndexingMap::Variable{Interval{0, 3}},\n        IndexingMap::Variable{Interval{0, 4}}});\n   indexing_map.AddConstraint(\n-      ParseAffineExpr(\"d0 * 4 + s0 + 2 * s2\", &symbolic_expr_context_),\n+      ParseAffineExpr(\"d0 * 4 + s0 + 2 * s2\", &mlir_context_),\n       Interval{24, 459});\n   EXPECT_THAT(indexing_map.ConvertSymbolsToDimensions(), MatchIndexingMap(R\"(\n       (d0, d1, d2, d3, d4) -> (d0 * 4 + d1 + d2 + d3 * 2 + d4 * 3 - 42),\n@@ -1353,14 +1342,13 @@ TEST_F(IndexingMapTest,\n   // important for now.\n   EXPECT_THAT(\n       std::make_tuple(result3, constraint_expr, constraint_interval),\n-      AnyOf(std::make_tuple(\n-                ParseAffineExpr(\"s0 * 6 + 3\", &symbolic_expr_context_),\n-                ParseAffineExpr(\"(s0 * 6 + 3) mod 7\", &symbolic_expr_context_),\n-                Interval{5, 5}),\n-            std::make_tuple(\n-                ParseAffineExpr(\"s0 * 7 + 5\", &symbolic_expr_context_),\n-                ParseAffineExpr(\"(s0 * 7 + 5) mod 6\", &symbolic_expr_context_),\n-                Interval{3, 3})));\n+      AnyOf(\n+          std::make_tuple(ParseAffineExpr(\"s0 * 6 + 3\", &mlir_context_),\n+                          ParseAffineExpr(\"(s0 * 6 + 3) mod 7\", &mlir_context_),\n+                          Interval{5, 5}),\n+          std::make_tuple(ParseAffineExpr(\"s0 * 7 + 5\", &mlir_context_),\n+                          ParseAffineExpr(\"(s0 * 7 + 5) mod 6\", &mlir_context_),\n+                          Interval{3, 3})));\n }\n \n TEST_F(IndexingMapTest, RescaleSymbolsKeepsHashmapConsistent) {\n@@ -1391,10 +1379,9 @@ TEST_F(IndexingMapTest, RangeEvaluatorTest) {\n     d2 in [-1, 2],\n     d3 in [0, 0]\n   )\");\n-  RangeEvaluator range_evaluator(indexing_map,\n-                                 symbolic_expr_context_.GetMLIRContext());\n+  RangeEvaluator range_evaluator(indexing_map, &mlir_context_);\n   mlir::AffineExpr d0, d1, d2, d3;\n-  bindDims(symbolic_expr_context_.GetMLIRContext(), d0, d1, d2, d3);\n+  bindDims(&mlir_context_, d0, d1, d2, d3);\n \n   // d0 is always positive.\n   EXPECT_TRUE(range_evaluator.IsAlwaysPositiveOrZero(d0));\n@@ -1542,15 +1529,15 @@ TEST_F(IndexingMapTest, IndexingMapSupportsAbslHashAndEqAndNe) {\n       )\"),\n        IndexingMap(\n            ParseAffineMap(\"(d0)[s0, s1, s2, s3, s4] -> (d0 * 4 + s1 + s3 - 42)\",\n-                          &symbolic_expr_context_),\n+                          &mlir_context_),\n            {IndexingMap::Variable{{0, 31}}},\n            {IndexingMap::Variable{{0, 0}}, IndexingMap::Variable{{0, 1}},\n             IndexingMap::Variable{{0, 2}}},\n            {IndexingMap::Variable{Interval{0, 3}},\n             IndexingMap::Variable{Interval{0, 4}}}),\n        IndexingMap(\n            ParseAffineMap(\"(d0)[s0, s1, s2, s3, s4] -> (d0 * 4 + s1 + s3 - 42)\",\n-                          &symbolic_expr_context_),\n+                          &mlir_context_),\n            {IndexingMap::Variable{{0, 31}}},\n            {IndexingMap::Variable{{0, 0}}, IndexingMap::Variable{{0, 1}},\n             IndexingMap::Variable{{0, 2}}},"
        },
        {
            "sha": "f7db83e6f76331ec9a97d9fec8da3ff35cd1aa14",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_test_utils.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 12,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_test_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_test_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_test_utils.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -72,19 +72,19 @@ HloInstruction* IndexingTestBase::ParseAndGetRoot(\n HloInstructionIndexing IndexingTestBase::GetOutputToInputIndexing(\n     const HloInstruction* instr, int output_id, bool use_physical_layout) {\n   HloInstructionIndexing indexing =\n-      ComputeOutputToInputIndexing(instr, output_id, &symbolic_expr_context_);\n+      ComputeOutputToInputIndexing(instr, output_id, &mlir_context_);\n \n   if (!use_physical_layout) {\n     return indexing;\n   }\n \n   IndexingMap output_permutation = GetIndexingMapFromPhysicalLayoutToLogical(\n-      GetOutputShape(instr, output_id), &symbolic_expr_context_);\n+      GetOutputShape(instr, output_id), &mlir_context_);\n \n   for (const auto& [operand_id, indexing_maps] :\n        llvm::enumerate(indexing.indexing_maps)) {\n     IndexingMap operand_permutation = GetIndexingMapFromLogicalToPhysicalLayout(\n-        instr->operand(operand_id)->shape(), &symbolic_expr_context_);\n+        instr->operand(operand_id)->shape(), &mlir_context_);\n \n     OperandIndexingSet operand_indexing_maps;\n     for (const OperandIndexing& indexing_map : indexing_maps) {\n@@ -107,21 +107,21 @@ HloInstructionIndexing IndexingTestBase::GetOutputToInputIndexing(\n HloInstructionIndexing IndexingTestBase::GetInputToOutputIndexing(\n     const HloInstruction* instr, int input_id, bool use_physical_layout) {\n   HloInstructionIndexing indexing =\n-      ComputeInputToOutputIndexing(instr, input_id, &symbolic_expr_context_);\n+      ComputeInputToOutputIndexing(instr, input_id, &mlir_context_);\n \n   if (!use_physical_layout) {\n     return indexing;\n   }\n \n   OperandIndexing input_permutation =\n       OperandIndexing(GetIndexingMapFromPhysicalLayoutToLogical(\n-          instr->operand(input_id)->shape(), &symbolic_expr_context_));\n+          instr->operand(input_id)->shape(), &mlir_context_));\n \n   for (const auto& [output_id, indexing_maps] :\n        llvm::enumerate(indexing.indexing_maps)) {\n     OperandIndexing operand_permutation =\n         OperandIndexing(GetIndexingMapFromLogicalToPhysicalLayout(\n-            GetOutputShape(instr, output_id), &symbolic_expr_context_));\n+            GetOutputShape(instr, output_id), &mlir_context_));\n \n     OperandIndexingSet operand_indexing_maps;\n     for (const OperandIndexing& indexing_map : indexing_maps) {\n@@ -142,26 +142,24 @@ HloInstructionIndexing IndexingTestBase::GetInputToOutputIndexing(\n }\n \n AffineMap ParseAffineMap(absl::string_view serialized_affine_map,\n-                         SymbolicExprContext* symbolic_expr_context) {\n+                         mlir::MLIRContext* mlir_context) {\n   std::string full_affine_map_string =\n       absl::StrCat(\"affine_map<\", serialized_affine_map, \">\");\n   return mlir::cast<mlir::AffineMapAttr>(\n-             mlir::parseAttribute(full_affine_map_string,\n-                                  symbolic_expr_context->GetMLIRContext()))\n+             mlir::parseAttribute(full_affine_map_string, mlir_context))\n       .getValue();\n }\n \n // Since MLIR does not have AffineExprAttr, we construct an AffineMap and then\n // retrieve its first result.\n AffineExpr ParseAffineExpr(absl::string_view serialized_affine_expr,\n-                           SymbolicExprContext* symbolic_expr_context) {\n+                           mlir::MLIRContext* mlir_context) {\n   std::string full_affine_map_string = absl::StrCat(\n       \"affine_map<(d0, d1, d2, d3, d4, d5, d6, d7, d8, d9)\"\n       \"[s0, s1, s2, s3, s4, s5, s6, s7, s8, s9] -> (\",\n       serialized_affine_expr, \")>\");\n   return mlir::cast<mlir::AffineMapAttr>(\n-             mlir::parseAttribute(full_affine_map_string,\n-                                  symbolic_expr_context->GetMLIRContext()))\n+             mlir::parseAttribute(full_affine_map_string, mlir_context))\n       .getValue()\n       .getResult(0);\n }"
        },
        {
            "sha": "2c8a67f3ae65cc9fa65ae8c8b2bba6205572a7b1",
            "filename": "third_party/xla/xla/hlo/analysis/indexing_test_utils.h",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_test_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_test_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Findexing_test_utils.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -68,8 +68,6 @@ MATCHER_P(MatchIndexingString, indexing_string, \"\") {\n \n class IndexingTestBase : public HloHardwareIndependentTestBase {\n  public:\n-  IndexingTestBase() : symbolic_expr_context_(&mlir_context_) {}\n-\n   HloInstruction* ParseAndGetRoot(absl::string_view hlo_string);\n \n   HloInstructionIndexing GetOutputToInputIndexing(\n@@ -81,15 +79,14 @@ class IndexingTestBase : public HloHardwareIndependentTestBase {\n       bool use_physical_layout = false);\n \n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_;\n   std::unique_ptr<VerifiedHloModule> module_;\n };\n \n mlir::AffineMap ParseAffineMap(absl::string_view serialized_affine_map,\n-                               SymbolicExprContext* symbolic_expr_context);\n+                               mlir::MLIRContext* mlir_context);\n \n mlir::AffineExpr ParseAffineExpr(absl::string_view serialized_affine_expr,\n-                                 SymbolicExprContext* symbolic_expr_context);\n+                                 mlir::MLIRContext* mlir_context);\n \n // Safely evaluates the given expression, returning nullopt if the result is\n // undefined (due to undefined behavior, e.g. division by zero or overflow)."
        },
        {
            "sha": "bf69914223b1a132943c55299a8f17e416e4410e",
            "filename": "third_party/xla/xla/hlo/analysis/symbolic_expr.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fsymbolic_expr.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fsymbolic_expr.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fsymbolic_expr.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -1030,37 +1030,6 @@ SymbolicExpr ParseSymbolicExpr(absl::string_view expr_str,\n   return Parser(expr_str, mlir_context).Parse();\n }\n \n-SymbolicExprContext::SymbolicExprContext(mlir::MLIRContext* mlir_context)\n-    : mlir_context_(mlir_context) {}\n-\n-SymbolicExpr SymbolicExprContext::GetOrCreate(SymbolicExprType type,\n-                                              int64_t value, SymbolicExpr lhs,\n-                                              SymbolicExpr rhs) {\n-  return GetOrCreateSymbolicExpr(type, value, lhs, rhs, mlir_context_);\n-}\n-\n-SymbolicExpr SymbolicExprContext::CreateConstant(int64_t value) {\n-  return CreateSymbolicConstant(value, mlir_context_);\n-}\n-\n-SymbolicExpr SymbolicExprContext::CreateVariable(int64_t var_id) {\n-  return CreateSymbolicVariable(var_id, mlir_context_);\n-}\n-\n-SymbolicExpr SymbolicExprContext::CreateBinaryOp(SymbolicExprType type,\n-                                                 SymbolicExpr lhs,\n-                                                 SymbolicExpr rhs) {\n-  return CreateSymbolicBinaryOp(type, lhs, rhs, mlir_context_);\n-}\n-\n-SymbolicExpr SymbolicExprContext::Parse(absl::string_view expr_str) {\n-  return ParseSymbolicExpr(expr_str, mlir_context_);\n-}\n-\n-bool SymbolicExprContext::operator==(const SymbolicExprContext& other) const {\n-  return mlir_context_ == other.mlir_context_;\n-}\n-\n void SymbolicExpr::Walk(\n     const std::function<void(SymbolicExpr)>& callback) const {\n   if (!*this) {"
        },
        {
            "sha": "677ed8b6136c7e464df7246358e96ae3221279bf",
            "filename": "third_party/xla/xla/hlo/analysis/symbolic_expr.h",
            "status": "modified",
            "additions": 2,
            "deletions": 29,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fsymbolic_expr.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fsymbolic_expr.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fsymbolic_expr.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -33,7 +33,6 @@ limitations under the License.\n \n namespace xla {\n \n-class SymbolicExprContext;\n class SymbolicExprStorage;\n \n typedef int64_t VariableID;\n@@ -153,8 +152,8 @@ H AbslHashValue(H h, const SymbolicExpr& expr) {\n   return H::combine(std::move(h), hash_value(expr));\n }\n \n-// This method should be called once per MLIRContext to register the\n-// SymbolicExprStorage type with the MLIRContext's uniquifier. It should be\n+// This method should be called once permlir::MLIRContext to register the\n+// SymbolicExprStorage type with themlir::MLIRContext's uniquifier. It should be\n // called before any SymbolicExprs are created.\n void RegisterSymbolicExprStorage(mlir::MLIRContext* mlir_context);\n \n@@ -171,32 +170,6 @@ SymbolicExpr CreateSymbolicBinaryOp(SymbolicExprType type, SymbolicExpr lhs,\n llvm::SmallVector<SymbolicExpr> CreateSymbolicConstantExprs(\n     llvm::ArrayRef<int64_t> constants, mlir::MLIRContext* mlir_context);\n \n-// Deprecated. Use free functions taking mlir::MLIRContext* instead.\n-class SymbolicExprContext {\n- public:\n-  explicit SymbolicExprContext(mlir::MLIRContext* mlir_context);\n-  SymbolicExpr Parse(absl::string_view expr_str);\n-  SymbolicExpr CreateConstant(int64_t value);\n-  SymbolicExpr CreateVariable(int64_t var_id);\n-  SymbolicExpr CreateBinaryOp(SymbolicExprType type, SymbolicExpr lhs,\n-                              SymbolicExpr rhs);\n-\n-  bool operator==(const SymbolicExprContext& other) const;\n-  bool operator!=(const SymbolicExprContext& other) const {\n-    return !(*this == other);\n-  }\n-\n-  mlir::MLIRContext* GetMLIRContext() const { return mlir_context_; }\n-\n- private:\n-  SymbolicExpr GetOrCreate(SymbolicExprType type, int64_t value,\n-                           SymbolicExpr lhs, SymbolicExpr rhs);\n-  // TODO(b/446856305): MLIRContext is only used here temporarily while we have\n-  // AffineMap <-> SymbolicMap convertors. In the future, we only will need a\n-  // StorageUniquer pointer.\n-  mlir::MLIRContext* mlir_context_;\n-};\n-\n }  // namespace xla\n \n namespace llvm {"
        },
        {
            "sha": "0c857ea6c08a5d91dceea0b5f919c36481eb5c4d",
            "filename": "third_party/xla/xla/service/cpu/parallel_fusion_emitter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fparallel_fusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fparallel_fusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fparallel_fusion_emitter.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -35,7 +35,6 @@ limitations under the License.\n #include \"xla/codegen/kernel_spec.h\"\n #include \"xla/codegen/llvm_kernel_source.h\"\n #include \"xla/codegen/mlir_kernel_source.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -45,7 +44,6 @@ namespace xla::cpu {\n \n struct ParallelFusionEmitter::CompilerInstance {\n   std::unique_ptr<mlir::MLIRContext> mlir_context;\n-  std::unique_ptr<SymbolicExprContext> symbolic_expr_context;\n   std::unique_ptr<FusionCompiler> compiler;\n };\n \n@@ -101,14 +99,10 @@ auto ParallelFusionEmitter::FusionCompilerPool::GetInstance()\n   std::unique_ptr<mlir::MLIRContext> mlir_context =\n       FusionCompiler::CreateContext();\n \n-  auto symbolic_expr_context =\n-      std::make_unique<SymbolicExprContext>(mlir_context.get());\n-\n   auto compiler = std::make_unique<FusionCompiler>(mlir_context.get(), options_,\n                                                    GetNestedHooks());\n \n   return CreateSharedInstance({std::move(mlir_context),\n-                               std::move(symbolic_expr_context),\n                                std::move(compiler)});\n }\n \n@@ -172,8 +166,7 @@ absl::StatusOr<KernelSpec> ParallelFusionEmitter::AddFusion(\n   auto compiler_instance = fusion_compiler_pool_->GetInstance();\n   TF_ASSIGN_OR_RETURN(\n       KernelDefinition mlir_kernel_definition,\n-      EmitFusionKernel(*compiler_instance->mlir_context,\n-                       *compiler_instance->symbolic_expr_context, *fusion,\n+      EmitFusionKernel(*compiler_instance->mlir_context, *fusion,\n                        buffer_assignment_, use_unique_c_name_));\n \n   {"
        },
        {
            "sha": "9662c8ccd52e955650c7e114e95e21a178e9a67a",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -847,7 +847,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitFusionKernelThunk(\n   if (ir_emitter_.IsSupportedByFusionEmitter(fusion) &&\n       fusion->fused_expression_root()->opcode() == HloOpcode::kScatter) {\n     auto kernel_emitter = std::make_unique<CpuScatterFusion>(\n-        buffer_assignment_, fusion, &symbolic_expr_context_);\n+        buffer_assignment_, fusion, mlir_context_.get());\n \n     TF_ASSIGN_OR_RETURN(KernelDefinition kernel_definition,\n                         kernel_emitter->EmitKernelDefinition());"
        },
        {
            "sha": "6d034ddc6234aa470bd3e01c2c8ed466ceb283c3",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.h",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -274,7 +274,6 @@ class ThunkEmitter {\n   std::vector<EmittedKernel> kernels_;\n \n   std::unique_ptr<mlir::MLIRContext> mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{mlir_context_.get()};\n   FusionCompiler fusion_compiler_;\n \n   absl::flat_hash_map<std::string, KernelSpec> kernel_spec_cache_;"
        },
        {
            "sha": "e3bd2f349c9aa3ac5ae74d95fc8b616202ef2405",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -318,7 +318,7 @@ absl::Status AMDGPUCompiler::AddGemmFusionAutotuningPasses(\n     se::StreamExecutor* stream_executor) {\n   pipeline->AddPass<GemmFusionAutotuner>(autotune_config, toolkit_version,\n                                          thread_pool, key_value_store,\n-                                         symbolic_expr_context());\n+                                         mlir_context());\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "cec6e49159723a4d20795e0a064c83620a78eb3a",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 21,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -135,6 +135,8 @@ limitations under the License.\n namespace xla {\n namespace gpu {\n \n+using ::mlir::MLIRContext;\n+\n using BackendConfig = GemmFusionAutotunerImpl::BackendConfig;\n using BackendConfigs = GemmFusionAutotunerImpl::BackendConfigs;\n using ProfilingOutput = AutotunerCompileUtil::ProfilingOutput;\n@@ -299,7 +301,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> TritonGemmAutotuneExtractor(\n     const TritonGemmConfig& config,\n     const se::DeviceDescription& gpu_device_info,\n     const HloFusionInstruction* fusion, DebugOptions debug_opts,\n-    SymbolicExprContext* symbolic_expr_context,\n+    MLIRContext* mlir_context,\n     bool allow_filtering_kernels_spilling_registers) {\n   tsl::profiler::TraceMe traceme(\"TritonGemmAutotuneExtractor\");\n   std::unique_ptr<HloModule> new_module =\n@@ -333,7 +335,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> TritonGemmAutotuneExtractor(\n \n     PriorityFusion priority_fusion(\n         /*thread_pool=*/nullptr, gpu_device_info, PriorityFusionOptions(),\n-        symbolic_expr_context);\n+        mlir_context);\n     TF_RETURN_IF_ERROR(priority_fusion.Run(new_module.get()).status());\n \n     // If the priority fusion pass above skipped some instructions, turn them\n@@ -342,7 +344,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> TritonGemmAutotuneExtractor(\n     TF_RETURN_IF_ERROR(fusion_wrapper.Run(new_module.get()).status());\n   }\n \n-  NestGemmFusion nest_gemm_fusion(gpu_device_info, symbolic_expr_context);\n+  NestGemmFusion nest_gemm_fusion(gpu_device_info, mlir_context);\n   TF_RETURN_IF_ERROR(nest_gemm_fusion.Run(new_module.get()).status());\n   bool is_legacy_gemm_disabled = absl::c_contains(\n       debug_opts.xla_gpu_unsupported_generic_triton_emitter_features(),\n@@ -362,7 +364,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> CublasGemmAutotuneExtractor(\n     const AutotuneConfig& config, const se::DeviceDescription& gpu_device_info,\n     const se::SemanticVersion& toolkit_version,\n     const HloFusionInstruction* fusion, const DebugOptions& debug_opts,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   tsl::profiler::TraceMe traceme(\"CublasGemmAutotuneExtractor\");\n   const HloComputation* fusion_computation = fusion->called_computation();\n   std::unique_ptr<HloModule> new_module =\n@@ -393,7 +395,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> CublasGemmAutotuneExtractor(\n     DotAlgorithmRewriter dot_algorithm_rewriter;\n     PriorityFusion fusion_pass(\n         /*thread_pool=*/nullptr, gpu_device_info, PriorityFusionOptions(),\n-        symbolic_expr_context);\n+        mlir_context);\n     TF_RETURN_IF_ERROR(scaled_dot_rewriter.Run(new_module.get()).status());\n     TF_RETURN_IF_ERROR(dot_algorithm_rewriter.Run(new_module.get()).status());\n     TF_RETURN_IF_ERROR(gemm_rewriter.Run(new_module.get()).status());\n@@ -418,7 +420,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> CustomFusionKernelAutotuneExtractor(\n     const GemmFusionAutotunerImpl::CustomKernelFusionConfig& cutlass_config,\n     const AutotuneConfig& config, const se::SemanticVersion& toolkit_version,\n     const HloFusionInstruction* fusion, const DebugOptions& debug_opts,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   tsl::profiler::TraceMe traceme(\"CustomFusionKernelAutotuneExtractor\");\n   const HloComputation* fusion_computation = fusion->called_computation();\n   std::unique_ptr<HloModule> new_module =\n@@ -428,7 +430,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> CustomFusionKernelAutotuneExtractor(\n   CustomKernelFusionRewriter rewriter(&config.GetDeviceDescription());\n   PriorityFusion fusion_pass(\n       /*thread_pool=*/nullptr, config.GetDeviceDescription(),\n-      PriorityFusionOptions(), symbolic_expr_context);\n+      PriorityFusionOptions(), mlir_context);\n   TF_RETURN_IF_ERROR(rewriter.Run(new_module.get()).status());\n   TF_RETURN_IF_ERROR(fusion_pass.Run(new_module.get()).status());\n \n@@ -536,7 +538,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> GetAutotunedModule(\n     const AutotuneConfig& autotune_config,\n     const se::SemanticVersion& toolkit_version, AutotunerCompileUtil& util,\n     const AutotuneResult result, const HloFusionInstruction* fusion,\n-    int fusion_id, SymbolicExprContext* symbolic_expr_context) {\n+    int fusion_id, MLIRContext* mlir_context) {\n   TritonGemmConfig triton_gemm_config;\n   if (result.has_triton()) {\n     TF_ASSIGN_OR_RETURN(triton_gemm_config,\n@@ -557,14 +559,13 @@ absl::StatusOr<std::unique_ptr<HloModule>> GetAutotunedModule(\n         }\n         if (result.has_triton()) {\n           return TritonGemmAutotuneExtractor(\n-              triton_gemm_config, device_desc, fusion, debug_opts,\n-              symbolic_expr_context,\n+              triton_gemm_config, device_desc, fusion, debug_opts, mlir_context,\n               /*allow_filtering_kernels_spilling_registers=*/true);\n         }\n         if (result.has_gemm()) {\n           return CublasGemmAutotuneExtractor(autotune_config, device_desc,\n                                              toolkit_version, fusion,\n-                                             debug_opts, symbolic_expr_context);\n+                                             debug_opts, mlir_context);\n         }\n         LOG(FATAL) << \"Unknown result type: \" << result.DebugString();\n       }));\n@@ -1088,7 +1089,7 @@ GemmFusionAutotunerImpl::CompileAll(AutotunerCompileUtil& compile_util,\n       auto executable_or = compile_util.Compile([&](const DebugOptions& opts) {\n         return TritonGemmAutotuneExtractor(\n             std::get<TritonGemmConfig>(config), config_.GetDeviceDescription(),\n-            fusion, opts, symbolic_expr_context_,\n+            fusion, opts, mlir_context_,\n             allow_filtering_kernels_spilling_registers);\n       });\n       if (absl::c_contains(\n@@ -1127,15 +1128,15 @@ GemmFusionAutotunerImpl::CompileAll(AutotunerCompileUtil& compile_util,\n       return compile_util.Compile([&](const DebugOptions& opts) {\n         return CublasGemmAutotuneExtractor(\n             config_, config_.GetDeviceDescription(), toolkit_version_, fusion,\n-            opts, symbolic_expr_context_);\n+            opts, mlir_context_);\n       });\n     }\n \n     if (std::holds_alternative<CustomKernelFusionConfig>(config)) {\n       return compile_util.Compile([&](const DebugOptions& opts) {\n         return CustomFusionKernelAutotuneExtractor(\n             std::get<CustomKernelFusionConfig>(config), config_,\n-            toolkit_version_, fusion, opts, symbolic_expr_context_);\n+            toolkit_version_, fusion, opts, mlir_context_);\n       });\n     }\n \n@@ -1446,9 +1447,8 @@ absl::Status GemmFusionAutotunerImpl::Autotune(\n     if (debug_options_.xla_gpu_dump_autotuned_gemm_fusions() ||\n         !debug_options_.xla_gpu_dump_autotune_logs_to().empty()) {\n       TF_ASSIGN_OR_RETURN(\n-          module,\n-          GetAutotunedModule(config_, toolkit_version_, compile_util, best,\n-                             fusion, fusion_id, symbolic_expr_context_));\n+          module, GetAutotunedModule(config_, toolkit_version_, compile_util,\n+                                     best, fusion, fusion_id, mlir_context_));\n     }\n \n     if (debug_options_.xla_gpu_dump_autotuned_gemm_fusions()) {\n@@ -1573,7 +1573,7 @@ absl::StatusOr<bool> GemmFusionAutotuner::RunImpl(\n   }\n \n   GemmFusionAutotunerImpl autotuner(config_, toolkit_version_, debug_options,\n-                                    thread_pool_, symbolic_expr_context_);\n+                                    thread_pool_, mlir_context_);\n   GemmFusionCollector fusion_collector(&autotuner);\n   TF_ASSIGN_OR_RETURN(\n       GemmFusionCollectorResult fusions,\n@@ -1705,14 +1705,13 @@ absl::StatusOr<bool> GemmFusionAutotuner::RunViaNewInfra(\n   std::unique_ptr<Compiler::GpuTargetConfig> target_config;\n   target_config = std::make_unique<Compiler::GpuTargetConfig>(stream_exec);\n   backends.push_back(std::make_unique<TritonBackend>(\n-      &debug_options, compiler.get(), target_config.get(),\n-      symbolic_expr_context_));\n+      &debug_options, compiler.get(), target_config.get(), mlir_context_));\n   backends.push_back(std::make_unique<FissionBackend>(\n       &debug_options, compiler.get(), target_config.get(),\n       std::make_unique<CublasBackend>(stream_exec, &debug_options,\n                                       compiler.get(), target_config.get()),\n       GetCublasRewriterPipeline(target_config->device_description),\n-      symbolic_expr_context_));\n+      mlir_context_));\n   auto should_autotune = [](const HloInstruction& instruction) -> bool {\n     if (instruction.opcode() != HloOpcode::kFusion) {\n       return false;"
        },
        {
            "sha": "5d9f32948a6ca28ca9b56c79c779016368737974",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.h",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -79,12 +79,12 @@ class GemmFusionAutotuner : public HloModulePass {\n                                const se::SemanticVersion& toolkit_version,\n                                tsl::thread::ThreadPool* thread_pool,\n                                const MultiProcessKeyValueStore& key_value_store,\n-                               SymbolicExprContext* symbolic_expr_context)\n+                               mlir::MLIRContext* mlir_context)\n       : config_(config),\n         toolkit_version_(toolkit_version),\n         thread_pool_(thread_pool),\n         key_value_store_(key_value_store),\n-        symbolic_expr_context_(symbolic_expr_context) {}\n+        mlir_context_(mlir_context) {}\n \n   absl::string_view name() const override { return \"gemm-fusion-autotuner\"; }\n \n@@ -102,7 +102,7 @@ class GemmFusionAutotuner : public HloModulePass {\n   se::SemanticVersion toolkit_version_;\n   tsl::thread::ThreadPool* thread_pool_;\n   MultiProcessKeyValueStore key_value_store_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n class GemmFusionAutotunerImpl {\n@@ -111,12 +111,12 @@ class GemmFusionAutotunerImpl {\n       AutotuneConfig& config,\n       const stream_executor::SemanticVersion& toolkit_version,\n       DebugOptions debug_options, tsl::thread::ThreadPool* thread_pool,\n-      SymbolicExprContext* symbolic_expr_context)\n+      mlir::MLIRContext* mlir_context)\n       : config_(std::move(config)),\n         toolkit_version_(toolkit_version),\n         debug_options_(std::move(debug_options)),\n         thread_pool_(thread_pool),\n-        symbolic_expr_context_(symbolic_expr_context) {}\n+        mlir_context_(mlir_context) {}\n \n   struct CuBlasConfig {\n     bool operator<(const CuBlasConfig& other) const;\n@@ -217,7 +217,7 @@ class GemmFusionAutotunerImpl {\n   DebugOptions debug_options_;\n   tsl::thread::ThreadPool* thread_pool_;\n   std::vector<TritonGemmConfig> triton_configs_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "11bf4964b587411afb3512e29cd848a417ad6c60",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 42,
            "changes": 69,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -86,6 +86,8 @@ namespace {\n \n namespace m = ::xla::match;\n \n+using ::mlir::MLIRContext;\n+\n using HloExtractionTest = HloTestBase;\n \n TEST_F(HloExtractionTest, InstructionExtractionIsCorrect) {\n@@ -193,8 +195,7 @@ class StatelessAutotunerTest : public HloTestBase {\n       const HloModule& module,\n       const se::GpuComputeCapability& compute_capability,\n       const se::SemanticVersion& toolkit_version,\n-      const DebugOptions& debug_options,\n-      SymbolicExprContext* symbolic_expr_context) {\n+      const DebugOptions& debug_options, MLIRContext* mlir_context) {\n     const HloFusionInstruction& fusion = *Cast<HloFusionInstruction>(\n         module.entry_computation()->root_instruction());\n     if (GpuComputeComp().IsCuda()) {\n@@ -211,8 +212,7 @@ class StatelessAutotunerTest : public HloTestBase {\n     AutotuneConfig autotune_config = AutotuneConfig::FromDebugOptions(\n         DeviceOrDevicelessConfig{test_config}, debug_options);\n     GemmFusionAutotunerImpl autotuner(autotune_config, toolkit_version,\n-                                      debug_options, nullptr,\n-                                      symbolic_expr_context);\n+                                      debug_options, nullptr, mlir_context);\n     return autotuner.GenerateConfigs(fusion);\n   }\n \n@@ -246,7 +246,7 @@ class StatelessAutotunerTest : public HloTestBase {\n         DeviceOrDevicelessConfig{device_config}, GetDebugOptionsForTest());\n     GemmFusionAutotunerImpl autotuner(autotune_config, GetToolkitVersion(),\n                                       GetDebugOptionsForTest(), nullptr,\n-                                      &symbolic_expr_context_);\n+                                      &mlir_context_);\n     const HloFusionInstruction& fusion = *Cast<HloFusionInstruction>(\n         module.entry_computation()->root_instruction());\n     return autotuner.GenerateConfigs(fusion);\n@@ -264,7 +264,6 @@ class StatelessAutotunerTest : public HloTestBase {\n \n  protected:\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n constexpr absl::string_view kHloDotFusionWithAlgorithm = R\"(\n@@ -384,8 +383,7 @@ class GemmFusionAutotunerTest : public StatelessAutotunerTest {\n                 DeviceConfig{backend().default_stream_executor(),\n                              backend().memory_allocator()}},\n             opts),\n-        GetToolkitVersion(), &thread_pool, key_value_store,\n-        &symbolic_expr_context_);\n+        GetToolkitVersion(), &thread_pool, key_value_store, &mlir_context_);\n \n     RunAndFilecheckHloRewrite(\n         hlo, std::move(pipeline), expected, [](const HloModule* m) {\n@@ -419,8 +417,7 @@ absl::StatusOr<std::vector<TritonGemmConfig>>\n GetPossibleMatmulAutotuneTritonConfigs(\n     const D& dot, const se::CudaComputeCapability& compute_capability,\n     const se::SemanticVersion& toolkit_version,\n-    const DebugOptions& debug_options,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    const DebugOptions& debug_options, MLIRContext* mlir_context) {\n   TF_ASSIGN_OR_RETURN(se::DeviceDescription device_description,\n                       se::DeviceDescription::FromProto(\n                           se::GpuDeviceInfoProto::default_instance()));\n@@ -438,8 +435,7 @@ GetPossibleMatmulAutotuneTritonConfigs(\n   AutotuneConfig autotune_config = AutotuneConfig::FromDebugOptions(\n       DeviceOrDevicelessConfig{test_config}, debug_options);\n   GemmFusionAutotunerImpl autotuner(autotune_config, toolkit_version,\n-                                    debug_options, nullptr,\n-                                    symbolic_expr_context);\n+                                    debug_options, nullptr, mlir_context);\n   return autotuner.GenerateTritonConfigs(dot);\n }\n \n@@ -463,7 +459,7 @@ ENTRY e {\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           compute_capability, GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &symbolic_expr_context_));\n+          &mlir_context_));\n   EXPECT_TRUE(std::any_of(\n       configs.begin(), configs.end(),\n       [](const TritonGemmConfig& config) { return config.num_stages > 2; }));\n@@ -486,7 +482,7 @@ ENTRY e {\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           compute_capability, GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &symbolic_expr_context_));\n+          &mlir_context_));\n   EXPECT_TRUE(std::any_of(\n       configs.begin(), configs.end(),\n       [](const TritonGemmConfig& config) { return config.split_k >= 4; }));\n@@ -509,7 +505,7 @@ ENTRY e {\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           compute_capability, GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &symbolic_expr_context_));\n+          &mlir_context_));\n   EXPECT_FALSE(std::any_of(\n       configs.begin(), configs.end(),\n       [](const TritonGemmConfig& config) { return config.split_k > 1; }));\n@@ -830,7 +826,7 @@ ENTRY main {\n   MultiProcessKeyValueStore key_value_store;\n   pipeline.AddPass<GemmFusionAutotuner>(autotune_config, GetToolkitVersion(),\n                                         &thread_pool, key_value_store,\n-                                        &symbolic_expr_context_);\n+                                        &mlir_context_);\n   pipeline.AddPass<CallInliner>();\n   for (GemmRewriterOptions::DType dtype :\n        {GemmRewriterOptions::DType::kFp8Only,\n@@ -1051,8 +1047,7 @@ ENTRY e {\n           DeviceOrDevicelessConfig{DevicelessConfig{\n               backend().default_stream_executor()->GetDeviceDescription()}},\n           opts),\n-      GetToolkitVersion(), &thread_pool, key_value_store,\n-      &symbolic_expr_context_);\n+      GetToolkitVersion(), &thread_pool, key_value_store, &mlir_context_);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(hlo));\n@@ -1101,7 +1096,7 @@ ENTRY e {\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           compute_capability, GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &symbolic_expr_context_));\n+          &mlir_context_));\n \n   if (GetDebugOptionsForTest().xla_gpu_autotune_level() == 0) {\n     EXPECT_EQ(configs.size(), 1);\n@@ -1168,7 +1163,7 @@ ENTRY e {\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           compute_capability, GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &symbolic_expr_context_));\n+          &mlir_context_));\n   EXPECT_TRUE(std::all_of(\n       configs.begin(), configs.end(),\n       [](const TritonGemmConfig& config) { return config.split_k == 1; }));\n@@ -1190,7 +1185,7 @@ TEST_F(GemmFusionAutotunerTest, SplitKFLoatNormalization) {\n       DeviceOrDevicelessConfig{test_config}, GetDebugOptionsForTest());\n   GemmFusionAutotunerImpl autotuner(autotune_config, GetToolkitVersion(),\n                                     GetDebugOptionsForTest(), nullptr,\n-                                    &symbolic_expr_context_);\n+                                    &mlir_context_);\n   TF_ASSERT_OK_AND_ASSIGN(\n       AutotunerCompileUtil compile_util,\n       AutotunerCompileUtil::Create(autotune_config.DeviceConfig(),\n@@ -1257,7 +1252,7 @@ TEST_F(GemmFusionAutotunerTest, CreatesCustomKernelFusionConfigs) {\n       const std::vector<GemmFusionAutotunerImpl::BackendConfig> configs,\n       GetPossibleMatmulAutotuneConfigs(\n           *module, compute_capability, GetToolkitVersion(),\n-          GetDebugOptionsForTest(), &symbolic_expr_context_));\n+          GetDebugOptionsForTest(), &mlir_context_));\n   EXPECT_TRUE(std::any_of(\n       configs.begin(), configs.end(),\n       [](const GemmFusionAutotunerImpl::BackendConfig& config) {\n@@ -1300,8 +1295,7 @@ TEST_F(GemmFusionAutotunerTest, GeneratesTwoConfigsForUpcastGemmWithPrologue) {\n       const std::vector<GemmFusionAutotunerImpl::BackendConfig> configs,\n       GetPossibleMatmulAutotuneConfigs(\n           *module, se::GpuComputeCapability{compute_capability},\n-          GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &symbolic_expr_context_));\n+          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n   EXPECT_EQ(\n       2, std::count_if(\n              configs.begin(), configs.end(),\n@@ -1347,8 +1341,7 @@ TEST_F(GemmFusionAutotunerTest, GeneratesOneConfigForUpcastGemmWithPrologue) {\n       const std::vector<GemmFusionAutotunerImpl::BackendConfig> configs,\n       GetPossibleMatmulAutotuneConfigs(\n           *module, se::GpuComputeCapability{compute_capability},\n-          GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &symbolic_expr_context_));\n+          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n   EXPECT_EQ(\n       1, std::count_if(\n              configs.begin(), configs.end(),\n@@ -1397,8 +1390,7 @@ TEST_F(GemmFusionAutotunerTest,\n       const std::vector<GemmFusionAutotunerImpl::BackendConfig> configs,\n       GetPossibleMatmulAutotuneConfigs(\n           *module, se::GpuComputeCapability{compute_capability},\n-          GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &symbolic_expr_context_));\n+          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n   EXPECT_EQ(\n       2, std::count_if(\n              configs.begin(), configs.end(),\n@@ -1507,8 +1499,7 @@ class GemmFusionShardedAutotunerTest : public GemmFusionAutotunerTest {\n       MultiProcessKeyValueStore& multi_process_key_value_store) {\n     return GemmFusionAutotuner(GetAutotuneConfigForTest(), GetToolkitVersion(),\n                                /*thread_pool=*/{},\n-                               multi_process_key_value_store,\n-                               &symbolic_expr_context_);\n+                               multi_process_key_value_store, &mlir_context_);\n   }\n };\n \n@@ -1759,16 +1750,14 @@ TEST_F(GemmFusionAutotunerTest, VerifyHopperConfigsAreDifferentFromBlackwell) {\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           se::CudaComputeCapability(se::CudaComputeCapability::kBlackwell, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &symbolic_expr_context_));\n+          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n   TF_ASSERT_OK_AND_ASSIGN(\n       const std::vector<TritonGemmConfig> hopper_configs,\n       GetPossibleMatmulAutotuneTritonConfigs(\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           se::CudaComputeCapability(se::CudaComputeCapability::kHopper, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &symbolic_expr_context_));\n+          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n \n   std::set<TritonGemmConfig> blackwell_configs_set(blackwell_configs.begin(),\n                                                    blackwell_configs.end());\n@@ -1802,8 +1791,7 @@ TEST_F(GemmFusionAutotunerTest, ScaledDotConfigsAreGenerated) {\n           *Cast<HloScaledDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           se::CudaComputeCapability(se::CudaComputeCapability::kBlackwell, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &symbolic_expr_context_));\n+          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n   std::set<TritonGemmConfig> blackwell_configs_set(blackwell_configs.begin(),\n                                                    blackwell_configs.end());\n   EXPECT_GT(blackwell_configs_set.size(), 0);\n@@ -1877,17 +1865,15 @@ TEST_F(GemmFusionAutotunerEnableTma,\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           se::CudaComputeCapability(se::CudaComputeCapability::kAmpere, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &symbolic_expr_context_));\n+          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       const std::vector<TritonGemmConfig> hopper_configs,\n       GetPossibleMatmulAutotuneTritonConfigs(\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           se::CudaComputeCapability(se::CudaComputeCapability::kHopper, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &symbolic_expr_context_));\n+          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n \n   std::set<TritonGemmConfig> ampere_configs_set(ampere_configs.begin(),\n                                                 ampere_configs.end());\n@@ -1933,8 +1919,7 @@ TEST_F(GemmFusionAutotunerEnableTma, TmaRunCorrectlyForDotsOfBroadcasts) {\n           *Cast<HloDotInstruction>(\n               module->entry_computation()->root_instruction()),\n           se::CudaComputeCapability(se::CudaComputeCapability::kHopper, 0),\n-          GetToolkitVersion(), GetDebugOptionsForTest(),\n-          &symbolic_expr_context_));\n+          GetToolkitVersion(), GetDebugOptionsForTest(), &mlir_context_));\n \n   EXPECT_TRUE(RunAndCompare(std::move(module),\n                             ErrorSpec{/*aabs=*/5e-3, /*arel=*/5e-3}));"
        },
        {
            "sha": "fccf842e2974b3c288810deb5fff9f650d1770bb",
            "filename": "third_party/xla/xla/service/gpu/compile_module_to_llvm_ir.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -48,7 +48,6 @@ limitations under the License.\n #include \"mlir/Support/LogicalResult.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n #include \"xla/hlo/analysis/hlo_ordering.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/buffer_assignment.h\"\n@@ -80,9 +79,10 @@ limitations under the License.\n #include \"tsl/profiler/lib/traceme.h\"\n \n namespace xla::gpu {\n-\n namespace {\n \n+using ::mlir::MLIRContext;\n+\n using tsl::profiler::ScopedAnnotation;\n \n // Prints mlir diagnostic messages to VLOG level 2.\n@@ -308,8 +308,6 @@ absl::StatusOr<CompileModuleResults> CompileModuleToLlvmIr(\n           << \": \" << hlo_module->GetFingerprint128();\n \n   std::unique_ptr<mlir::MLIRContext> mlir_context = CreateMlirContext();\n-  auto symbolic_expr_context =\n-      std::make_unique<SymbolicExprContext>(mlir_context.get());\n   IrEmitterContext ir_emitter_context(\n       hlo_module, results.buffer_assignment.get(),\n       results.execution_stream_assignment.get(), platform->Name(), device_desc,"
        },
        {
            "sha": "7c12784e2bfc9b09eb7935d80be72720c857451f",
            "filename": "third_party/xla/xla/service/gpu/custom_kernel_emitter_cuda.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_kernel_emitter_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_kernel_emitter_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_kernel_emitter_cuda.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -43,7 +43,7 @@ absl::StatusOr<std::unique_ptr<Thunk>> EmitPtxCustomKernelThunk(\n \n   TF_ASSIGN_OR_RETURN(\n       KernelCall call,\n-      KernelCall::Parse(backend_config_str, context->expr_context()));\n+      KernelCall::Parse(backend_config_str, context->mlir_context()));\n   if (call.kernel_type != KernelCall::KernelType::kPtxSource) {\n     return absl::InvalidArgumentError(\n         \"PTX custom call backend config is not a PTX source\");"
        },
        {
            "sha": "992bef7b2ae8e2c40ff55a398e0e85afcdbf9dee",
            "filename": "third_party/xla/xla/service/gpu/fusion_dispatch_pipeline.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_dispatch_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_dispatch_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_dispatch_pipeline.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -30,11 +30,11 @@ namespace gpu {\n HloPassPipeline FusionDispatchPipeline(\n     const se::DeviceDescription& device_description,\n     HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    mlir::MLIRContext* mlir_context) {\n   HloPassPipeline pipeline(\"fusion-dispatch-pipeline\");\n   pipeline.AddPass<HloDCE>();\n   pipeline.AddPass<FusionBlockLevelRewriter>(device_description, shape_size_fn,\n-                                             symbolic_expr_context);\n+                                             mlir_context);\n   pipeline.AddPass<FusionDynamicMemcpyRewriter>();\n   return pipeline;\n }"
        },
        {
            "sha": "8d8b26a2ef9387131e31df008fea052d4afa3a30",
            "filename": "third_party/xla/xla/service/gpu/fusion_dispatch_pipeline.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_dispatch_pipeline.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_dispatch_pipeline.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_dispatch_pipeline.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -30,7 +30,7 @@ namespace gpu {\n HloPassPipeline FusionDispatchPipeline(\n     const se::DeviceDescription& device_description,\n     HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n-    SymbolicExprContext* symbolic_expr_context);\n+    mlir::MLIRContext* mlir_context);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "2922bb466c6d52574d01cdbe5c330c997e758232",
            "filename": "third_party/xla/xla/service/gpu/fusion_pipeline.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -45,7 +45,7 @@ HloPassPipeline FusionPipeline(\n     HloCostAnalysis::ShapeSizeFunction shape_size_bytes_function,\n     tsl::thread::ThreadPool* thread_pool,\n     const se::DeviceDescription& gpu_device_info,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    mlir::MLIRContext* mlir_context) {\n   HloPassFix<HloPassPipeline> fusion(\"fusion\");\n   // We try to split variadic ops with many parameters into several such ops\n   // to avoid exceeding the parameter space.\n@@ -64,7 +64,7 @@ HloPassPipeline FusionPipeline(\n       /*count_multiple_input_accesses=*/true};\n   fusion.AddPass<PriorityFusion>(thread_pool, gpu_device_info,\n                                  std::move(cost_analysis_options),\n-                                 symbolic_expr_context);\n+                                 mlir_context);\n \n   // Running CSE affects how many users an op has. This plays a role in what\n   // we detect as a tiled transpose fusion.\n@@ -73,7 +73,7 @@ HloPassPipeline FusionPipeline(\n       /*should_eliminate_computation=*/&HloComputation::IsFusionComputation);\n   fusion.AddPass<HloDCE>();\n   fusion.AddPass<MultiOutputFusion>(gpu_device_info, shape_size_bytes_function,\n-                                    symbolic_expr_context);\n+                                    mlir_context);\n   fusion.AddPass<HloCSE>(\n       /*is_layout_sensitive=*/true, /*ignore_control_dependencies=*/false,\n       /*should_eliminate_computation=*/&HloComputation::IsFusionComputation);"
        },
        {
            "sha": "e1f25ec6eb3141e43be295377e2ea57779fa90b1",
            "filename": "third_party/xla/xla/service/gpu/fusion_pipeline.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ffusion_pipeline.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -33,7 +33,7 @@ HloPassPipeline FusionPipeline(\n     HloCostAnalysis::ShapeSizeFunction shape_size_bytes_function,\n     tsl::thread::ThreadPool* thread_pool,\n     const se::DeviceDescription& gpu_device_info,\n-    SymbolicExprContext* symbolic_expr_context);\n+    mlir::MLIRContext* mlir_context);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "c7adc7cc50c59ba657aee0e3c0f6ad7d22ccb794",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 25,
            "changes": 47,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -1006,7 +1006,7 @@ absl::Status RunFusionPasses(HloModule* hlo_module,\n                              const Compiler::GpuTargetConfig& gpu_target_config,\n                              tsl::thread::ThreadPool* thread_pool,\n                              HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n-                             SymbolicExprContext* symbolic_expr_context) {\n+                             mlir::MLIRContext* mlir_context) {\n   const se::DeviceDescription& gpu_device_info =\n       gpu_target_config.device_description;\n \n@@ -1016,7 +1016,7 @@ absl::Status RunFusionPasses(HloModule* hlo_module,\n \n   TF_RETURN_IF_ERROR(\n       FusionPipeline(hlo_module->config().debug_options(), shape_size_fn,\n-                     thread_pool, gpu_device_info, symbolic_expr_context)\n+                     thread_pool, gpu_device_info, mlir_context)\n           .Run(hlo_module, {HloInstruction::kMainExecutionThread})\n           .status());\n \n@@ -1075,13 +1075,13 @@ void AddCollectiveCombinerPasses(\n     const se::DeviceDescription& device_description,\n     const GpuAliasInfo* alias_info, int pointer_size,\n     const GpuCompiler::CompileOptions& options,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    mlir::MLIRContext* mlir_context) {\n   const DebugOptions& opts = module.config().debug_options();\n \n   if (EnableHeuristicCollectiveCombining(module.config(), device_description,\n                                          options.slice_size)) {\n     pipeline.AddPass<CollectiveCombinerAnnotator>(\n-        device_description, alias_info, pointer_size, symbolic_expr_context);\n+        device_description, alias_info, pointer_size, mlir_context);\n   }\n \n   pipeline.AddPass<GpuAllGatherCombiner>(\n@@ -1107,12 +1107,11 @@ absl::Status RunPostFusionPasses(\n     HloModule* hlo_module, const se::DeviceDescription& device_description,\n     const GpuAliasInfo* alias_info, int pointer_size,\n     const GpuCompiler::CompileOptions& options,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    mlir::MLIRContext* mlir_context) {\n   HloPassPipeline pipeline(\"post-fusion optimization\");\n   pipeline.AddPass<RenameFusions>();\n   AddCollectiveCombinerPasses(pipeline, *hlo_module, device_description,\n-                              alias_info, pointer_size, options,\n-                              symbolic_expr_context);\n+                              alias_info, pointer_size, options, mlir_context);\n \n   pipeline.AddPass<AllReduceContiguous>();\n \n@@ -1164,7 +1163,7 @@ absl::Status RunPostFusionVerificationPasses(\n     HloModule* hlo_module, se::StreamExecutor* stream_exec,\n     const GpuCompiler::CompileOptions& options,\n     const Compiler::GpuTargetConfig& gpu_target_config,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    mlir::MLIRContext* mlir_context) {\n   HloPassPipeline pipeline(\"post-fusion-verification-pipeline optimization\");\n \n   if (hlo_module->config()\n@@ -1174,7 +1173,7 @@ absl::Status RunPostFusionVerificationPasses(\n         GetDeviceConfig(stream_exec, options, gpu_target_config);\n     if (!device_config.IsDeviceless()) {\n       pipeline.AddPass<TritonFusionNumericsVerifier>(device_config,\n-                                                     symbolic_expr_context);\n+                                                     mlir_context);\n     }\n   }\n \n@@ -1430,12 +1429,12 @@ absl::Status GpuCompiler::OptimizeHloModule(\n   TF_RETURN_IF_ERROR(\n       RunDynamicSliceFusionPasses(hlo_module, /*platform_id=*/PlatformId()));\n \n-  TF_RETURN_IF_ERROR(\n-      RunFusionPasses(hlo_module, gpu_target_config, thread_pool.get_mutable(),\n-                      ShapeSizeBytesFunction(), &symbolic_expr_context_));\n+  TF_RETURN_IF_ERROR(RunFusionPasses(hlo_module, gpu_target_config,\n+                                     thread_pool.get_mutable(),\n+                                     ShapeSizeBytesFunction(), &mlir_context_));\n   TF_RETURN_IF_ERROR(RunPostFusionPasses(hlo_module, device_description,\n                                          alias_info, pointer_size_, options,\n-                                         &symbolic_expr_context_));\n+                                         &mlir_context_));\n   TF_RETURN_IF_ERROR(RunAsyncCollectivesConversionPasses(hlo_module));\n   TF_RETURN_IF_ERROR(RunPostFusionSimplificationPasses(\n       hlo_module,\n@@ -1445,9 +1444,8 @@ absl::Status GpuCompiler::OptimizeHloModule(\n           gpu_target_config.platform_name == \"ROCM\"),\n       gpu_version, gpu_target_config));\n \n-  TF_RETURN_IF_ERROR(RunPostFusionVerificationPasses(hlo_module, stream_exec,\n-                                                     options, gpu_target_config,\n-                                                     &symbolic_expr_context_));\n+  TF_RETURN_IF_ERROR(RunPostFusionVerificationPasses(\n+      hlo_module, stream_exec, options, gpu_target_config, &mlir_context_));\n \n   TF_RETURN_IF_ERROR(\n       RunCollectiveScheduleLinearizerPasses(hlo_module, stream_exec));\n@@ -1658,7 +1656,7 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n       pipeline.AddPass<HloDCE>();\n       pipeline.AddPass<SoftmaxRewriterTriton>(\n           gpu_target_config.device_description, ShapeSizeBytesFunction(),\n-          &symbolic_expr_context_,\n+          &mlir_context_,\n           /*only_fuse_if_profitable=*/true);\n     }\n \n@@ -1737,7 +1735,7 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n   // Match the location of this pass in `gemm_fusion_autotuner.cc` to make sure\n   // that there is no discrepancy.\n   pipeline.AddPass<NestGemmFusion>(gpu_target_config.device_description,\n-                                   &symbolic_expr_context_);\n+                                   &mlir_context_);\n \n   // Clean up new_tuple described above.\n   pipeline.AddPass<TupleSimplifier>();\n@@ -2370,10 +2368,9 @@ GpuCompiler::CompileToBackendResult(\n   std::unique_ptr<GpuAliasInfo> alias_info = GetAliasInfo(gpu_device_info);\n   TF_RETURN_IF_ERROR(\n       RunPreSchedulingPasses(module, gpu_device_info, alias_info.get()));\n-  TF_ASSIGN_OR_RETURN(\n-      ScheduleMetadata schedule_metadata,\n-      ScheduleGpuModule(module, pointer_size_, gpu_device_info,\n-                        &symbolic_expr_context_, alias_info.get()));\n+  TF_ASSIGN_OR_RETURN(ScheduleMetadata schedule_metadata,\n+                      ScheduleGpuModule(module, pointer_size_, gpu_device_info,\n+                                        &mlir_context_, alias_info.get()));\n   HloPassPipeline pipeline(\"scheduled-gpu-module\");\n   AddHloVerifier(&pipeline);\n   TF_RETURN_IF_ERROR(pipeline.Run(module).status());\n@@ -2744,14 +2741,14 @@ absl::Status GpuCompiler::RunPreSchedulingPasses(\n         /*count_multiple_input_accesses=*/true};\n     // Cost model analysis for compute.\n     pipeline.AddPass<GpuCostModelStatsCollection>(\n-        gpu_device_info, cost_analysis_options, &symbolic_expr_context_);\n+        gpu_device_info, cost_analysis_options, &mlir_context_);\n     // S-curve model analysis for collectives.\n     if (module->config()\n             .debug_options()\n             .xla_gpu_enable_analytical_sol_latency_estimator()) {\n       pipeline.AddPass<SolGpuCostModelStatsCollection>(\n           gpu_device_info, ShapeSizeBytesFunction(), pointer_size_,\n-          &symbolic_expr_context_);\n+          &mlir_context_);\n     }\n \n     // Perf tables model analysis for collectives.\n@@ -2898,7 +2895,7 @@ absl::Status GpuCompiler::RunPostSchedulingPipelines(\n     // This needs to run after every pass affecting fusions. The last passes\n     // that create new fusions are FusionWrapper and StreamAttributeAnnotator.\n     main_pipeline.AddPass<HloPassPipeline>(FusionDispatchPipeline(\n-        gpu_device_info, ShapeSizeBytesFunction(), &symbolic_expr_context_));\n+        gpu_device_info, ShapeSizeBytesFunction(), &mlir_context_));\n   }\n \n   // Pipeline with passes which wrap a scheduled module into command buffers."
        },
        {
            "sha": "17249beeb6c4012dce07f118a64ed61cb0237aad",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.h",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -110,9 +110,6 @@ class GpuCompiler : public LLVMCompiler {\n       se::StreamExecutor* executor);\n \n   mlir::MLIRContext* mlir_context() { return &mlir_context_; }\n-  SymbolicExprContext* symbolic_expr_context() {\n-    return &symbolic_expr_context_;\n-  }\n \n   virtual std::unique_ptr<GpuAliasInfo> GetAliasInfo(\n       const se::DeviceDescription& device_description) const {\n@@ -298,9 +295,6 @@ class GpuCompiler : public LLVMCompiler {\n   // A MLIR context that can be used by pre-codegen passes. For codegen, we will\n   // need to have a context with more dialects registered.\n   mlir::MLIRContext mlir_context_;\n-  // A symbolic expression context that can be used by pre-codegen passes to\n-  // create symbolic expressions.\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "33312833e0e57f7e8101379fd908a17713d65613",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -123,7 +123,7 @@ class GpuCompilerTest : public HloTestBase {\n     std::unique_ptr<GpuAliasInfo> alias_info =\n         gpu_compiler->GetAliasInfo(gpu_device_info);\n     TF_RETURN_IF_ERROR(ScheduleGpuModule(module, 4, gpu_device_info,\n-                                         gpu_compiler->symbolic_expr_context(),\n+                                         gpu_compiler->mlir_context(),\n                                          alias_info.get())\n                            .status());\n     return gpu_compiler->RunPostSchedulingPipelines("
        },
        {
            "sha": "2afc4787298d11c9ad7bc4e1c336b93994bde726",
            "filename": "third_party/xla/xla/service/gpu/gpu_hlo_schedule.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -463,7 +463,7 @@ std::string TagWithFingerprint(HloModule* module) {\n std::unique_ptr<LatencyEstimator> GetLatencyEstimator(\n     const HloModule& module, int pointer_size,\n     const se::DeviceDescription& gpu_device_info, absl::string_view fingerprint,\n-    const SchedulerConfig& config, SymbolicExprContext* symbolic_expr_context) {\n+    const SchedulerConfig& config, mlir::MLIRContext* mlir_context) {\n   const DebugOptions& options = module.config().debug_options();\n \n   auto gpu_latency_estimator =\n@@ -488,7 +488,7 @@ std::unique_ptr<LatencyEstimator> GetLatencyEstimator(\n     return std::make_unique<AnalyticalLatencyEstimator>(\n         config, std::move(gpu_latency_estimator), gpu_device_info,\n         ShapeSizeBytesFunction(pointer_size), module.entry_computation(),\n-        symbolic_expr_context);\n+        mlir_context);\n   }\n \n   if (SolLatencyEstimator::IsSupportedForModule(module, gpu_device_info)) {\n@@ -511,7 +511,7 @@ std::unique_ptr<LatencyEstimator> GetLatencyEstimator(\n     auto sol_latency_estimator = SolLatencyEstimator::Create(\n         config, std::move(gpu_latency_estimator), gpu_device_info,\n         ShapeSizeBytesFunction(pointer_size), module.entry_computation(),\n-        symbolic_expr_context, std::move(cost_analysis));\n+        mlir_context, std::move(cost_analysis));\n     if (sol_latency_estimator.ok()) {\n       return std::move(*sol_latency_estimator);\n     }\n@@ -561,8 +561,7 @@ LegalizeSchedulingAnnotations::Config SchedulingAnnotationsConfig() {\n absl::Status RunLatencyHidingSchedulerPasses(\n     HloModule* module, int pointer_size, absl::string_view fingerprint,\n     uint64_t memory_limit, const se::DeviceDescription& gpu_device_info,\n-    SymbolicExprContext* symbolic_expr_context,\n-    const GpuAliasInfo* alias_info) {\n+    mlir::MLIRContext* mlir_context, const GpuAliasInfo* alias_info) {\n   tsl::profiler::TraceMe traceme(\"RunLatencyHidingSchedulerPasses\");\n   HloPassPipeline pipeline(\"latency-hiding-scheduler\");\n   const DebugOptions& options = module->config().debug_options();\n@@ -577,7 +576,7 @@ absl::Status RunLatencyHidingSchedulerPasses(\n \n   std::unique_ptr<LatencyEstimator> estimator =\n       GetLatencyEstimator(*module, pointer_size, gpu_device_info, fingerprint,\n-                          config, symbolic_expr_context);\n+                          config, mlir_context);\n \n   if (NeedAccuracyChecker(options, *estimator)) {\n     pipeline.AddPass<PGLEAccuracyChecker>(\n@@ -720,8 +719,7 @@ absl::Status RunAsyncCollectivesConversionPasses(HloModule* module) {\n absl::StatusOr<ScheduleMetadata> ScheduleGpuModule(\n     HloModule* module, int64_t pointer_size,\n     const se::DeviceDescription& gpu_device_info,\n-    SymbolicExprContext* symbolic_expr_context,\n-    const GpuAliasInfo* alias_info) {\n+    mlir::MLIRContext* mlir_context, const GpuAliasInfo* alias_info) {\n   tsl::profiler::TraceMe traceme(\"ScheduleGpuModule\");\n \n   // Tag the module with its 128 bit fingerprint. The fingerprint should include\n@@ -755,7 +753,7 @@ absl::StatusOr<ScheduleMetadata> ScheduleGpuModule(\n   if (enable_latency_hiding_scheduler) {\n     TF_RETURN_IF_ERROR(RunLatencyHidingSchedulerPasses(\n         module, pointer_size, fingerprint, memory_limit, gpu_device_info,\n-        symbolic_expr_context, alias_info));\n+        mlir_context, alias_info));\n   }\n \n   return ScheduleMetadata{memory_limit, peak_memory_bytes};"
        },
        {
            "sha": "ef5c5dacda34b6be5e91d1d125ec34ff6daf61ce",
            "filename": "third_party/xla/xla/service/gpu/gpu_hlo_schedule.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -57,7 +57,7 @@ uint64_t GetSchedulerMemoryLimit(const HloModule& module,\n absl::StatusOr<ScheduleMetadata> ScheduleGpuModule(\n     HloModule* module, int64_t pointer_size,\n     const se::DeviceDescription& gpu_device_info,\n-    SymbolicExprContext* symbolic_expr_context, const GpuAliasInfo* alias_info);\n+    mlir::MLIRContext* mlir_context, const GpuAliasInfo* alias_info);\n \n HloInstructionSequence PostProcessSchedule(const HloInstructionSequence& input);\n "
        },
        {
            "sha": "e0774c0378e738ee83d49f2291d185161c020a96",
            "filename": "third_party/xla/xla/service/gpu/gpu_hlo_schedule_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -82,7 +82,7 @@ class GpuHloScheduleTest : public HloTestBase {\n         gpu_compiler->GetAliasInfo(gpu_device_info);\n     int64_t pointer_size = gpu_compiler->GetPointerSize();\n     return xla::gpu::ScheduleGpuModule(module, pointer_size, gpu_device_info,\n-                                       gpu_compiler->symbolic_expr_context(),\n+                                       gpu_compiler->mlir_context(),\n                                        alias_info.get());\n   }\n "
        },
        {
            "sha": "28eabcb1cd768080fc86ddd3e3de4e0664939d72",
            "filename": "third_party/xla/xla/service/gpu/gpu_latency_hiding_scheduler_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -78,8 +78,8 @@ class GpuLatencyHidingSchedulerBaseTest\n     options.set_xla_gpu_pgle_accuracy_checker(strictness);\n \n     TF_RETURN_IF_ERROR(ScheduleGpuModule(module, /*pointer_size=*/8,\n-                                         gpu_device_info,\n-                                         &symbolic_expr_context_, &alias_info)\n+                                         gpu_device_info, &mlir_context_,\n+                                         &alias_info)\n                            .status());\n     return module;\n   }\n@@ -99,7 +99,6 @@ class GpuLatencyHidingSchedulerBaseTest\n   }\n \n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(GpuLatencyHidingSchedulerBaseTest,"
        },
        {
            "sha": "3eacbbc339ae8b3694b69a173724fb029d8d144d",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_context.h",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -78,7 +78,6 @@ class IrEmitterContext {\n         platform_name_(std::move(platform_name)),\n         gpu_device_info_(gpu_device_info),\n         mlir_context_(mlir_context),\n-        expr_context_(mlir_context_),\n         llvm_module_(llvm_module),\n         llvm_module_constants_(llvm_module_constants),\n         emit_kernels_(emit_kernels) {}\n@@ -103,13 +102,8 @@ class IrEmitterContext {\n   }\n \n   mlir::MLIRContext* mlir_context() { return mlir_context_; }\n-\n-  // TODO: b/451959933 - Add nullability annotation to be explicit about this\n-  // pointer: go/totw/230. Alternatively, return by reference instead of pointer\n-  // (and require reference in ctor) to signal that it is always present.\n-  SymbolicExprContext* expr_context() { return &expr_context_; }\n-\n   llvm::Module* llvm_module() { return llvm_module_; }\n+\n   // A separate module can optionally be used to emit constants.\n   llvm::Module* llvm_module_constants() {\n     return (llvm_module_constants_ == nullptr) ? llvm_module_\n@@ -160,7 +154,7 @@ class IrEmitterContext {\n   std::string platform_name_;\n   const se::DeviceDescription& gpu_device_info_;\n   mlir::MLIRContext* mlir_context_;\n-  SymbolicExprContext expr_context_;\n+  mlir::MLIRContext expr_context_;\n   llvm::Module* llvm_module_;\n   llvm::Module* llvm_module_constants_;\n   NameUniquer name_uniquer_;"
        },
        {
            "sha": "0957cf861e392227e1154a4591b90b51c813a100",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -1186,8 +1186,7 @@ absl::Status IrEmitterUnnested::EmitCustomCallThunk(\n             : instr->raw_backend_config_string();\n     if (!backend_config_str.empty()) {\n       mlir::Attribute attr = mlir::parseAttribute(\n-          backend_config_str,\n-          ir_emitter_context_->expr_context()->GetMLIRContext());\n+          backend_config_str, ir_emitter_context_->mlir_context());\n       auto dict = mlir::dyn_cast_or_null<mlir::DictionaryAttr>(attr);\n       if (dict == nullptr) {\n         return absl::InternalError(\n@@ -1420,8 +1419,7 @@ absl::Status IrEmitterUnnested::EmitTopKCustomCall(\n absl::Status IrEmitterUnnested::EmitTritonCustomCall(\n     const HloCustomCallInstruction* instr) {\n   auto generate = [this, &instr]() -> absl::StatusOr<KernelReuseCache::Entry> {\n-    mlir::MLIRContext& mlir_context =\n-        *ir_emitter_context_->expr_context()->GetMLIRContext();\n+    mlir::MLIRContext& mlir_context = *ir_emitter_context_->mlir_context();\n     LoadMlirDialectsForTriton(mlir_context);\n     auto call =\n         TritonCall::Parse(instr->raw_backend_config_string(), &mlir_context);\n@@ -1598,7 +1596,7 @@ absl::Status IrEmitterUnnested::EmitFusion(const HloFusionInstruction* instr) {\n           /*buffer_assignment=*/\n           &ir_emitter_context_->buffer_assignment(),\n           /*call_graph=*/*call_graph_),\n-      ir_emitter_context_->expr_context());\n+      ir_emitter_context_->mlir_context());\n   TF_ASSIGN_OR_RETURN(auto result, emitter->Emit(*ir_emitter_context_, *instr));\n \n   const ExecutionStreamAssignment& stream_assignment ="
        },
        {
            "sha": "c5c876e02837bfddd3b11956553345e2725e7318",
            "filename": "third_party/xla/xla/service/gpu/kernel_call.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_call.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_call.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_call.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -52,11 +52,10 @@ absl::StatusOr<KernelCall::KernelType> ParseKernelType(\n   }\n }\n \n-absl::StatusOr<KernelCall> KernelCall::Parse(\n-    absl::string_view backend_config,\n-    SymbolicExprContext* symbolic_expr_context) {\n-  auto attrs = mlir::cast<mlir::DictionaryAttr>(mlir::parseAttribute(\n-      backend_config, symbolic_expr_context->GetMLIRContext()));\n+absl::StatusOr<KernelCall> KernelCall::Parse(absl::string_view backend_config,\n+                                             mlir::MLIRContext* mlir_context) {\n+  auto attrs = mlir::cast<mlir::DictionaryAttr>(\n+      mlir::parseAttribute(backend_config, mlir_context));\n \n   // Check for required \"name\" field\n   auto name_attr = attrs.getAs<mlir::StringAttr>(\"name\");"
        },
        {
            "sha": "104ef7fcbdf548abf8fddc407debfd8b9138c091",
            "filename": "third_party/xla/xla/service/gpu/kernel_call.h",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_call.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_call.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_call.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -41,9 +41,8 @@ struct KernelCall {\n   std::vector<int32_t> output_indices;\n \n   // Parse the metadata of a __gpu$xla.gpu.ptx call.\n-  static absl::StatusOr<KernelCall> Parse(\n-      absl::string_view backend_config,\n-      SymbolicExprContext* symbolic_expr_context);\n+  static absl::StatusOr<KernelCall> Parse(absl::string_view backend_config,\n+                                          mlir::MLIRContext* mlir_context);\n };\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "030324a94548e41fd32b2e870c59c8fe046d50bc",
            "filename": "third_party/xla/xla/service/gpu/kernel_call_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_call_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_call_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_call_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -34,12 +34,9 @@ class KernelCallTest : public ::testing::Test {\n  protected:\n   void SetUp() override {\n     mlir_context_ = std::make_unique<mlir::MLIRContext>();\n-    symbolic_expr_context_ =\n-        std::make_unique<SymbolicExprContext>(mlir_context_.get());\n   }\n \n   std::unique_ptr<mlir::MLIRContext> mlir_context_;\n-  std::unique_ptr<SymbolicExprContext> symbolic_expr_context_;\n };\n \n TEST_F(KernelCallTest, ParseBasicConfiguration) {\n@@ -58,7 +55,7 @@ TEST_F(KernelCallTest, ParseBasicConfiguration) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       KernelCall kernel_call,\n-      KernelCall::Parse(backend_config, symbolic_expr_context_.get()));\n+      KernelCall::Parse(backend_config, mlir_context_.get()));\n \n   EXPECT_EQ(kernel_call.name, \"test_kernel\");\n   EXPECT_EQ(kernel_call.kernel_data,\n@@ -90,7 +87,7 @@ TEST_F(KernelCallTest, ParseWithOutputIndices) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       KernelCall kernel_call,\n-      KernelCall::Parse(backend_config, symbolic_expr_context_.get()));\n+      KernelCall::Parse(backend_config, mlir_context_.get()));\n \n   EXPECT_EQ(kernel_call.name, \"kernel_with_outputs\");\n   EXPECT_EQ(kernel_call.block_dim.x, 10);\n@@ -123,7 +120,7 @@ TEST_F(KernelCallTest, ParseMinimalConfiguration) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       KernelCall kernel_call,\n-      KernelCall::Parse(backend_config, symbolic_expr_context_.get()));\n+      KernelCall::Parse(backend_config, mlir_context_.get()));\n \n   EXPECT_EQ(kernel_call.name, \"minimal_kernel\");\n   EXPECT_EQ(kernel_call.kernel_data, \".entry minimal_kernel() { ret; }\");\n@@ -153,7 +150,7 @@ TEST_F(KernelCallTest, ParseLargeDimensions) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       KernelCall kernel_call,\n-      KernelCall::Parse(backend_config, symbolic_expr_context_.get()));\n+      KernelCall::Parse(backend_config, mlir_context_.get()));\n \n   EXPECT_EQ(kernel_call.name, \"large_kernel\");\n   EXPECT_EQ(kernel_call.block_dim.x, 65535);\n@@ -182,7 +179,7 @@ TEST_F(KernelCallTest, ParseEmptyOutputIndices) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       KernelCall kernel_call,\n-      KernelCall::Parse(backend_config, symbolic_expr_context_.get()));\n+      KernelCall::Parse(backend_config, mlir_context_.get()));\n \n   EXPECT_EQ(kernel_call.name, \"no_outputs\");\n   EXPECT_EQ(kernel_call.shared_mem, 512);\n@@ -206,7 +203,7 @@ TEST_F(KernelCallTest, ParseSingleOutputIndex) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       KernelCall kernel_call,\n-      KernelCall::Parse(backend_config, symbolic_expr_context_.get()));\n+      KernelCall::Parse(backend_config, mlir_context_.get()));\n \n   EXPECT_EQ(kernel_call.name, \"single_output\");\n   EXPECT_EQ(kernel_call.shared_mem, 256);\n@@ -231,7 +228,7 @@ TEST_F(KernelCallTest, ParseComplexkernel_data) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       KernelCall kernel_call,\n-      KernelCall::Parse(backend_config, symbolic_expr_context_.get()));\n+      KernelCall::Parse(backend_config, mlir_context_.get()));\n \n   EXPECT_EQ(kernel_call.name, \"complex_kernel\");\n   EXPECT_THAT(kernel_call.kernel_data, HasSubstr(\".version 7.5\"));"
        },
        {
            "sha": "d40e923cece3abf8c420e282f17d1af284cc2a0a",
            "filename": "third_party/xla/xla/service/gpu/model/analytical_latency_estimator.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -77,10 +77,10 @@ AnalyticalLatencyEstimator::AnalyticalLatencyEstimator(\n     std::unique_ptr<LatencyEstimator> latency_estimator,\n     const se::DeviceDescription& gpu_info,\n     HloCostAnalysis::ShapeSizeFunction shape_size_function,\n-    HloComputation* computation, SymbolicExprContext* symbolic_expr_context)\n+    HloComputation* computation, mlir::MLIRContext* mlir_context)\n     : config_(config),\n       gpu_info_(gpu_info),\n-      gpu_performance_model_(gpu_info, symbolic_expr_context),\n+      gpu_performance_model_(gpu_info, mlir_context),\n       latency_estimator_(std::move(latency_estimator)),\n       shape_size_function_(shape_size_function) {\n   cost_analysis_.emplace("
        },
        {
            "sha": "c23b78bbaefedab25476c2087e0a090293a5bf5d",
            "filename": "third_party/xla/xla/service/gpu/model/analytical_latency_estimator.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -41,7 +41,7 @@ class AnalyticalLatencyEstimator : public LatencyEstimator {\n       std::unique_ptr<LatencyEstimator> latency_estimator,\n       const se::DeviceDescription& gpu_info,\n       HloCostAnalysis::ShapeSizeFunction shape_size_function,\n-      HloComputation* computation, SymbolicExprContext* symbolic_expr_context);\n+      HloComputation* computation, mlir::MLIRContext* mlir_context);\n \n   TimeCost GetLatencyBetween(const HloGraphNode& from,\n                              const HloGraphNode& target) const override;"
        },
        {
            "sha": "9e9e60691146ac57d43ffecf7b1af361bed32665",
            "filename": "third_party/xla/xla/service/gpu/model/analytical_latency_estimator_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fanalytical_latency_estimator_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -166,13 +166,11 @@ ENTRY entry {\n   EXPECT_TRUE(hlo_module->has_entry_computation());\n \n   auto mlir_context = std::make_unique<mlir::MLIRContext>();\n-  auto symbolic_expr_context =\n-      std::make_unique<SymbolicExprContext>(mlir_context.get());\n   auto scheduler_config = GetDefaultSchedulerConfig();\n   auto latency_estimator = std::make_unique<AnalyticalLatencyEstimator>(\n       scheduler_config, std::make_unique<ApproximateLatencyEstimator>(),\n       dev_info, HloCostAnalysis::DefaultShapeSize,\n-      hlo_module->entry_computation(), symbolic_expr_context.get());\n+      hlo_module->entry_computation(), mlir_context.get());\n   auto alias_info = GetAliasInfo();\n   EXPECT_TRUE(RunScheduler(hlo_module.get(), scheduler_config, alias_info.get(),\n                            std::move(latency_estimator))"
        },
        {
            "sha": "5346566cb7c2814873396d2aaef1045a186d9bf2",
            "filename": "third_party/xla/xla/service/gpu/model/coalescing_analysis.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -505,9 +505,9 @@ bool IsIndexingCoalesced(IndexingMap& thread_x_to_linearized_input,\n std::optional<CoalescingMap> ComputeCoalescingForAllOperands(\n     const HloFusionAnalysis& fusion_analysis,\n     absl::Span<const HloInstruction* const> operands,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   auto emitter = GetFusionEmitter(\n-      PreBufferAssignmentFusionInfo{fusion_analysis}, symbolic_expr_context);\n+      PreBufferAssignmentFusionInfo{fusion_analysis}, mlir_context);\n   const auto* fusion_interface =\n       dynamic_cast<const KernelFusionInterface*>(emitter.get());\n \n@@ -516,21 +516,21 @@ std::optional<CoalescingMap> ComputeCoalescingForAllOperands(\n   }\n   llvm::SmallVector<IndexingMap, 4>\n       operand_logical_to_linearized_physical_maps =\n-          MapLogicalToLinearizedPhysicalShape(operands, symbolic_expr_context);\n+          MapLogicalToLinearizedPhysicalShape(operands, mlir_context);\n   GroupedByOpIndexingMap thread_id_to_input_memory_layouts;\n   for (const auto& [root_index, hero] :\n        llvm::enumerate(fusion_analysis.fusion_heroes())) {\n     // Compute thread ID -> hero operand indexing maps.\n     std::optional<std::vector<IndexingMap>> hero_indexing_maps =\n         fusion_interface->ComputeThreadIdToInputIndexing(root_index,\n-                                                         symbolic_expr_context);\n+                                                         mlir_context);\n     if (!hero_indexing_maps.has_value()) {\n       return std::nullopt;\n     }\n     GetThreadIdToInputMemoryLayoutsMaps(\n         fusion_analysis.fusion(), *hero_indexing_maps,\n         fusion_analysis.fusion_hero(root_index), operands,\n-        operand_logical_to_linearized_physical_maps, symbolic_expr_context,\n+        operand_logical_to_linearized_physical_maps, mlir_context,\n         thread_id_to_input_memory_layouts);\n   }\n \n@@ -570,23 +570,23 @@ std::optional<CoalescingMap> ComputeCoalescingForAllOperands(\n CoalescingAnalysis CoalescingAnalysis::Create(\n     const HloInstruction* instr,\n     absl::Span<const HloInstruction* const> operands,\n-    const HloFusionAnalysis& fusion_analysis,\n-    SymbolicExprContext* symbolic_expr_context, bool use_heuristic) {\n+    const HloFusionAnalysis& fusion_analysis, MLIRContext* mlir_context,\n+    bool use_heuristic) {\n   return Create(/*producer=*/instr, /*consumer=*/nullptr, operands,\n-                fusion_analysis, symbolic_expr_context, use_heuristic);\n+                fusion_analysis, mlir_context, use_heuristic);\n }\n \n /*static*/\n CoalescingAnalysis CoalescingAnalysis::Create(\n     const HloInstruction* producer, const HloInstruction* consumer,\n     absl::Span<const HloInstruction* const> operands,\n-    const HloFusionAnalysis& fusion_analysis,\n-    SymbolicExprContext* symbolic_expr_context, bool use_heuristic) {\n+    const HloFusionAnalysis& fusion_analysis, MLIRContext* mlir_context,\n+    bool use_heuristic) {\n   std::optional<CoalescingMap> coalescing_per_operand;\n \n   if (!use_heuristic) {\n     coalescing_per_operand = ComputeCoalescingForAllOperands(\n-        fusion_analysis, operands, symbolic_expr_context);\n+        fusion_analysis, operands, mlir_context);\n   }\n \n   if (coalescing_per_operand.has_value()) {"
        },
        {
            "sha": "c3ac7c3a9a9054ba987404fbd946f16c18fdb761",
            "filename": "third_party/xla/xla/service/gpu/model/coalescing_analysis.h",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -43,16 +43,14 @@ class CoalescingAnalysis {\n       const HloInstruction* instr,\n       absl::Span<const HloInstruction* const> operands,\n       const HloFusionAnalysis& fusion_analysis,\n-      SymbolicExprContext* symbolic_expr_context = nullptr,\n-      bool use_heuristic = true);\n+      mlir::MLIRContext* mlir_context = nullptr, bool use_heuristic = true);\n \n   // Computes read coalescing for operands of fused `producer` and `consumer`.\n   static CoalescingAnalysis Create(\n       const HloInstruction* producer, const HloInstruction* consumer,\n       absl::Span<const HloInstruction* const> operands,\n       const HloFusionAnalysis& fusion_analysis,\n-      SymbolicExprContext* symbolic_expr_context = nullptr,\n-      bool use_heuristic = true);\n+      mlir::MLIRContext* mlir_context = nullptr, bool use_heuristic = true);\n \n   // Returns true if the operand is read coalesced.\n   bool IsReadCoalesced(const HloInstruction* operand) const;"
        },
        {
            "sha": "d682e41e49b3650a81393c54afd5acff7e54b28a",
            "filename": "third_party/xla/xla/service/gpu/model/coalescing_analysis_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -64,12 +64,12 @@ class CoalescingTest : public HloHardwareIndependentTestBase {\n     auto fusion_adaptor = HloFusionAdaptor::ForInstruction(root);\n     auto analysis = HloFusionAnalysis::Create(*root, device_info_);\n     auto emitter = GetFusionEmitter(PreBufferAssignmentFusionInfo{analysis},\n-                                    &symbolic_expr_context_);\n+                                    &mlir_context_);\n     auto fusion = dynamic_cast<KernelFusionInterface*>(emitter.get());\n     EXPECT_NE(fusion, nullptr);\n \n     CoalescingAnalysis coalescing_analysis = CoalescingAnalysis::Create(\n-        root, root->operands(), analysis, &symbolic_expr_context_,\n+        root, root->operands(), analysis, &mlir_context_,\n         /*use_heuristic=*/false);\n \n     std::vector<bool> results;\n@@ -91,7 +91,6 @@ class CoalescingTest : public HloHardwareIndependentTestBase {\n   stream_executor::DeviceDescription device_info_ =\n       TestGpuDeviceInfo::RTXA6000DeviceInfo();\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(CoalescingTest, IdentityLayout) {\n@@ -593,7 +592,7 @@ class CoalescingForTiledHloTest : public CoalescingTest {\n \n     SymbolicTileAnalysis symbolic_tile_analysis =\n         std::get<SymbolicTileAnalysis>(SymbolicTileAnalysis::AnalyzeFusion(\n-            *fusion_adaptor, &symbolic_expr_context_));\n+            *fusion_adaptor, &mlir_context_));\n \n     TiledHloComputation tiled_hlo_computation =\n         *symbolic_tile_analysis.ComputeTiledHloInstructions(\n@@ -617,7 +616,7 @@ class CoalescingForTiledHloTest : public CoalescingTest {\n \n     SymbolicTileAnalysis symbolic_tile_analysis =\n         std::get<SymbolicTileAnalysis>(SymbolicTileAnalysis::AnalyzeFusion(\n-            *fusion_adaptor, &symbolic_expr_context_));\n+            *fusion_adaptor, &mlir_context_));\n \n     TiledHloComputation tiled_hlo_computation =\n         *symbolic_tile_analysis.ComputeTiledHloInstructions("
        },
        {
            "sha": "888d0c0799f768390132fa6250dfe14214d7f281",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_cost_model_stats_collection.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -125,8 +125,7 @@ absl::StatusOr<bool> GpuCostModelStatsCollection::RunImpl(\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n   // Scan all computations for fusion instructions.\n \n-  GpuPerformanceModelOwning gpu_performance_model{device_info_,\n-                                                  symbolic_expr_context_};\n+  GpuPerformanceModelOwning gpu_performance_model{device_info_, mlir_context_};\n   for (auto* computation : module->MakeComputationPostOrder()) {\n     TF_CHECK_OK(computation->Accept(&cost_analysis_));\n "
        },
        {
            "sha": "8c0fdd437b86146f8b3770fefbdd1da950161ddb",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_cost_model_stats_collection.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -37,10 +37,10 @@ class GpuCostModelStatsCollection : public HloModulePass {\n   explicit GpuCostModelStatsCollection(\n       const se::DeviceDescription& d,\n       const GpuHloCostAnalysis::Options& cost_analysis_options,\n-      SymbolicExprContext* symbolic_expr_context)\n+      mlir::MLIRContext* mlir_context)\n       : device_info_(d),\n         cost_analysis_(cost_analysis_options, device_info_),\n-        symbolic_expr_context_(symbolic_expr_context) {}\n+        mlir_context_(mlir_context) {}\n \n   absl::string_view name() const override {\n     return \"gpu_cost_model_stats_collection\";\n@@ -54,7 +54,7 @@ class GpuCostModelStatsCollection : public HloModulePass {\n  private:\n   se::DeviceDescription device_info_;\n   GpuHloCostAnalysis cost_analysis_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "2e39608c5780ca169c3d250a7351cdabbb35d246",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_cost_model_stats_collection_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_cost_model_stats_collection_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -37,6 +37,7 @@ namespace gpu {\n namespace {\n \n using ::absl_testing::IsOkAndHolds;\n+using ::mlir::MLIRContext;\n using ::testing::Contains;\n using ::testing::Truly;\n \n@@ -45,11 +46,10 @@ class GpuCostModelStatsCollectionTest : public HloHardwareIndependentTestBase {\n   GpuCostModelStatsCollection cost_model_stats_{\n       TestGpuDeviceInfo::RTXH100SXMDeviceInfo(),\n       GpuHloCostAnalysis::Options{.count_multiple_input_accesses = true},\n-      &symbolic_expr_context_};\n+      &mlir_context_};\n \n  protected:\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(GpuCostModelStatsCollectionTest, FusionInEntryComputation) {"
        },
        {
            "sha": "958d898ff6cb26f92c1fefba026e6cfe6a449533",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_indexing_performance_model.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -310,15 +310,15 @@ GpuPerformanceModelWithIndexingAnalysis::EstimateRunTimeForFusion(\n   auto root_shape = roots.front().shape();\n \n   LaunchDimensions launch_dimensions =\n-      EstimateFusionLaunchDimensions(fusion_analysis, symbolic_expr_context_);\n+      EstimateFusionLaunchDimensions(fusion_analysis, mlir_context_);\n \n   int64_t num_blocks = launch_dimensions.num_blocks();\n \n   // Compute indexing from root to each instruction in the fusion and fusion\n   // operands. For each instruction, tells which elements of the instructions\n   // result will be used to compute one result element of the fusion.\n   auto grouped_fusion_indexing = ComputeGroupedOutputToInputIndexing(\n-      fusion_adaptor, roots[0], symbolic_expr_context_);\n+      fusion_adaptor, roots[0], mlir_context_);\n \n   int64_t flops = 0;\n   int64_t bytes_read = 0;\n@@ -571,7 +571,7 @@ GpuPerformanceModelWithIndexingAnalysis::EstimateRunTimeForTiledFusion(\n   // TODO(b/332714755): Add caching for SymbolicTileAnalysis.\n   SymbolicTileAnalysisOrError analysis_or_error =\n       SymbolicTileAnalysis::AnalyzeFusion(\n-          fusion_adaptor, symbolic_expr_context_,\n+          fusion_adaptor, mlir_context_,\n           /*emitter_specific_constraints_builder=*/nullptr);\n   if (const auto* fusion_decision =\n           std::get_if<FusionDecision>(&analysis_or_error)) {\n@@ -638,7 +638,7 @@ GpuPerformanceModelWithIndexingAnalysis::TryFindBestTilingForFusion(\n     const HloFusionAdaptor& fusion_adaptor) {\n   SymbolicTileAnalysisOrError analysis_or_error =\n       SymbolicTileAnalysis::AnalyzeFusion(\n-          fusion_adaptor, symbolic_expr_context_,\n+          fusion_adaptor, mlir_context_,\n           TritonEmitterConstraints::GetBuilder(*device_info_));\n \n   if (const auto* fusion_decision ="
        },
        {
            "sha": "88c15cd960926ac2da6904b5f1c17f7862967414",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_indexing_performance_model.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -58,7 +58,7 @@ class GpuPerformanceModelWithIndexingAnalysis : public GpuPerformanceModelBase {\n       const se::DeviceDescription* device_info,\n       HloFusionAnalysisCache* fusion_analysis_cache,\n       HloCostAnalysis::ShapeSizeFunction shape_size,\n-      SymbolicExprContext* symbolic_expr_context)\n+      mlir::MLIRContext* mlir_context)\n       : hlo_op_profile_(&HloOpProfiles::Singleton().GetProfile(*device_info)),\n         device_info_(device_info),\n         fusion_analysis_cache_(fusion_analysis_cache),\n@@ -69,7 +69,7 @@ class GpuPerformanceModelWithIndexingAnalysis : public GpuPerformanceModelBase {\n                                         /*min_latencies_seconds=*/{},\n                                         /*count_multiple_input_accesses=*/true},\n             *device_info_),\n-        symbolic_expr_context_(symbolic_expr_context) {}\n+        mlir_context_(mlir_context) {}\n \n   // Returns the launch dimensions for the given tiled HLO computation.\n   static LaunchDimensions GetLaunchDimensionsForTiledFusion(\n@@ -134,7 +134,7 @@ class GpuPerformanceModelWithIndexingAnalysis : public GpuPerformanceModelBase {\n   HloFusionAnalysisCache* fusion_analysis_cache_;\n   HloCostAnalysis::ShapeSizeFunction shape_size_;\n   GpuHloCostAnalysis cost_analysis_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "4e54b43717475005dde5b7edd1cc0b93b157ea78",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_indexing_performance_model_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -60,14 +60,13 @@ using ::testing::HasSubstr;\n class GpuIndexingPerformanceModelTest : public HloHardwareIndependentTestBase {\n  public:\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n   // The reference times in the test cases below are measured\n   // on A6000 by profiling the execution of the HLOs.\n   se::DeviceDescription device_info_{TestGpuDeviceInfo::RTXA6000DeviceInfo()};\n   HloFusionAnalysisCache fusion_analysis_cache_{device_info_};\n   GpuPerformanceModelWithIndexingAnalysis indexing_cost_model_{\n       &device_info_, &fusion_analysis_cache_, HloCostAnalysis::DefaultShapeSize,\n-      &symbolic_expr_context_};\n+      &mlir_context_};\n \n   size_t WarpSize() const { return ::xla::gpu::WarpSize(device_info_); }\n };\n@@ -850,7 +849,7 @@ ENTRY main {\n \n   SymbolicTileAnalysisOrError analysis_or_error =\n       SymbolicTileAnalysis::AnalyzeFusion(\n-          *fusion_adaptor, &symbolic_expr_context_,\n+          *fusion_adaptor, &mlir_context_,\n           /*emitter_specific_constraints_builder=*/nullptr);\n   ASSERT_TRUE(std::holds_alternative<SymbolicTileAnalysis>(analysis_or_error));\n \n@@ -900,7 +899,7 @@ ENTRY main {\n \n   SymbolicTileAnalysisOrError analysis_or_error =\n       SymbolicTileAnalysis::AnalyzeFusion(\n-          *fusion_adaptor, &symbolic_expr_context_,\n+          *fusion_adaptor, &mlir_context_,\n           /*emitter_specific_constraints_builder=*/nullptr);\n   ASSERT_TRUE(std::holds_alternative<SymbolicTileAnalysis>(analysis_or_error));\n "
        },
        {
            "sha": "15874d34ffb2ad70289887d89f60caa982f5512b",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -48,11 +48,11 @@ GpuPerformanceModel::GpuPerformanceModel(\n     const se::DeviceDescription& device_info,\n     HloFusionAnalysisCache& fusion_analysis_cache,\n     GpuPerformanceModelCache& gpu_performance_model_cache,\n-    SymbolicExprContext* symbolic_expr_context)\n+    mlir::MLIRContext* mlir_context)\n     : device_info_(device_info),\n       fusion_analysis_cache_(fusion_analysis_cache),\n       gpu_performance_model_cache_(gpu_performance_model_cache),\n-      symbolic_expr_context_(symbolic_expr_context) {};\n+      mlir_context_(mlir_context) {};\n \n EstimateRunTimeData GpuPerformanceModel::EstimateRunTimeForInstructionImpl(\n     const HloInstruction* instr, const GpuHloCostAnalysis* cost_analysis) {\n@@ -63,7 +63,7 @@ EstimateRunTimeData GpuPerformanceModel::EstimateRunTimeForInstructionImpl(\n \n   const auto& fusion_analysis = fusion_analysis_cache_.Get(*instr);\n   LaunchDimensions launch_dimensions =\n-      EstimateFusionLaunchDimensions(fusion_analysis, symbolic_expr_context_);\n+      EstimateFusionLaunchDimensions(fusion_analysis, mlir_context_);\n   int64_t num_blocks = launch_dimensions.num_blocks();\n \n   absl::Duration compute_time =\n@@ -145,7 +145,7 @@ absl::Duration GpuPerformanceModel::EstimateRunTimeForFusionImpl(\n       fusion_analysis_cache_.Get(*producer, *consumer);\n \n   LaunchDimensions launch_dimensions =\n-      EstimateFusionLaunchDimensions(fusion_analysis, symbolic_expr_context_);\n+      EstimateFusionLaunchDimensions(fusion_analysis, mlir_context_);\n \n   int64_t flops = producer_runtime.flops * utilization_by_this_consumer +\n                   consumer_runtime.flops;"
        },
        {
            "sha": "9882cec1c2824ffce514ca613ffcc2a4242d7b52",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -36,7 +36,7 @@ class GpuPerformanceModel : public GpuPerformanceModelBase {\n   GpuPerformanceModel(const se::DeviceDescription& device_info,\n                       HloFusionAnalysisCache& fusion_analysis_cache,\n                       GpuPerformanceModelCache& gpu_performance_model_cache,\n-                      SymbolicExprContext* symbolic_expr_context);\n+                      mlir::MLIRContext* mlir_context);\n \n   EstimateRunTimeData EstimateRunTimeForInstruction(\n       const HloInstruction* instr, const GpuHloCostAnalysis* cost_analysis);\n@@ -78,7 +78,7 @@ class GpuPerformanceModel : public GpuPerformanceModelBase {\n   // this is not possible because the cache is used directly by\n   // xla::gpu::PriorityFusionQueue\n   GpuPerformanceModelCache& gpu_performance_model_cache_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n // An owning wrapper around GpuPerformanceModel that also owns the caches.\n@@ -89,11 +89,11 @@ class GpuPerformanceModel : public GpuPerformanceModelBase {\n class GpuPerformanceModelOwning {\n  public:\n   GpuPerformanceModelOwning(const se::DeviceDescription& device_info,\n-                            SymbolicExprContext* symbolic_expr_context)\n+                            mlir::MLIRContext* mlir_context)\n       : fusion_analysis_cache_(device_info),\n         gpu_performance_model_(std::make_unique<GpuPerformanceModel>(\n             device_info, fusion_analysis_cache_, gpu_performance_model_cache_,\n-            symbolic_expr_context)) {};\n+            mlir_context)) {};\n \n   GpuPerformanceModel& Get() const { return *gpu_performance_model_; }\n "
        },
        {
            "sha": "3975bbfafe32a0ba41eefb5e6f658771da7dfccd",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model_base.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -143,10 +143,9 @@ void GpuPerformanceModelCache::Invalidate(const HloInstruction& instruction) {\n \n /*static*/\n LaunchDimensions GpuPerformanceModelBase::EstimateFusionLaunchDimensions(\n-    const HloFusionAnalysis& fusion_analysis,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    const HloFusionAnalysis& fusion_analysis, mlir::MLIRContext* mlir_context) {\n   auto emitter = GetFusionEmitter(\n-      PreBufferAssignmentFusionInfo{fusion_analysis}, symbolic_expr_context);\n+      PreBufferAssignmentFusionInfo{fusion_analysis}, mlir_context);\n   if (const auto* kernel_emitter =\n           dynamic_cast<const KernelFusionInterface*>(emitter.get())) {\n     return kernel_emitter->launch_dimensions();"
        },
        {
            "sha": "166fc8e6842e9c5527afda75c985f8515ce7c346",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model_base.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -156,7 +156,7 @@ class GpuPerformanceModelBase {\n   // blocks that the IR emitter will use.\n   static LaunchDimensions EstimateFusionLaunchDimensions(\n       const HloFusionAnalysis& fusion_analysis,\n-      SymbolicExprContext* symbolic_expr_context);\n+      mlir::MLIRContext* mlir_context);\n \n   // Returns bytes accessed of operand output by instruction. Returns 0, if the\n   // operand is not used by the instruction."
        },
        {
            "sha": "cbdf16d34ffcda64743f61f50a312ba33cf291b3",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model_base_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -38,15 +38,16 @@ namespace xla {\n namespace gpu {\n namespace {\n \n+using ::mlir::MLIRContext;\n+\n class GpuPerformanceModelBaseTest : public HloHardwareIndependentTestBase {\n  public:\n   GpuHloCostAnalysis::Options options_;\n   // The reference times in the test cases below are measured\n   // on A6000 by profiling the execution of the HLOs.\n   se::DeviceDescription device_info_{TestGpuDeviceInfo::RTXA6000DeviceInfo()};\n   std::unique_ptr<GpuHloCostAnalysis> analysis_;\n-  mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n+  MLIRContext mlir_context_;\n \n   GpuPerformanceModelBaseTest() {\n     options_.count_multiple_input_accesses = true;\n@@ -243,8 +244,8 @@ ENTRY entry_computation {\n   auto fusion_analysis = HloFusionAnalysis::Create(\n       *module->entry_computation()->root_instruction(), device_info_);\n   auto launch_dimensions =\n-      GpuPerformanceModelBase::EstimateFusionLaunchDimensions(\n-          fusion_analysis, &symbolic_expr_context_);\n+      GpuPerformanceModelBase::EstimateFusionLaunchDimensions(fusion_analysis,\n+                                                              &mlir_context_);\n \n   EXPECT_EQ(launch_dimensions.num_blocks(), 128);\n   EXPECT_EQ(launch_dimensions.num_threads_per_block(), 128);\n@@ -280,8 +281,8 @@ ENTRY e {\n   auto fusion_analysis = HloFusionAnalysis::Create(\n       *module->entry_computation()->root_instruction(), device_info_);\n   auto launch_dimensions =\n-      GpuPerformanceModelBase::EstimateFusionLaunchDimensions(\n-          fusion_analysis, &symbolic_expr_context_);\n+      GpuPerformanceModelBase::EstimateFusionLaunchDimensions(fusion_analysis,\n+                                                              &mlir_context_);\n \n   EXPECT_EQ(launch_dimensions.num_blocks(), 16);\n   EXPECT_EQ(launch_dimensions.num_threads_per_block(), 64);\n@@ -310,8 +311,8 @@ ENTRY e {\n   auto fusion_analysis = HloFusionAnalysis::Create(\n       *module->entry_computation()->root_instruction(), device_info_);\n   auto launch_dimensions =\n-      GpuPerformanceModelBase::EstimateFusionLaunchDimensions(\n-          fusion_analysis, &symbolic_expr_context_);\n+      GpuPerformanceModelBase::EstimateFusionLaunchDimensions(fusion_analysis,\n+                                                              &mlir_context_);\n \n   // CuNnnFusion doesn't implement KernelLaunchInsterface, so\n   // EstimateFusionLaunchDimensions returns a default estimate."
        },
        {
            "sha": "e14d7dfc1ed7a7af248a90268738fbf224c87f85",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -49,6 +49,8 @@ namespace xla {\n namespace gpu {\n namespace {\n \n+using ::mlir::MLIRContext;\n+\n class GpuPerformanceModelTest : public HloHardwareIndependentTestBase {\n  public:\n   GpuPerformanceModel::RunTimes EstimateRunTimes(\n@@ -63,8 +65,7 @@ class GpuPerformanceModelTest : public HloHardwareIndependentTestBase {\n                                                    fused_consumers);\n   }\n \n-  mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n+  MLIRContext mlir_context_;\n   GpuHloCostAnalysis::Options options_{.count_multiple_input_accesses = true};\n   // The reference times in the test cases below are measured\n   // on A6000 by profiling the execution of the HLOs.\n@@ -74,11 +75,11 @@ class GpuPerformanceModelTest : public HloHardwareIndependentTestBase {\n   GpuPerformanceModelCache gpu_performance_model_cache_;\n   GpuPerformanceModel gpu_performance_model_{\n       device_info_, fusion_analysis_cache_, gpu_performance_model_cache_,\n-      &symbolic_expr_context_};\n+      &mlir_context_};\n \n   GpuPerformanceModelWithIndexingAnalysis indexing_cost_model_{\n       &device_info_, &fusion_analysis_cache_, HloCostAnalysis::DefaultShapeSize,\n-      &symbolic_expr_context_};\n+      &mlir_context_};\n };\n \n TEST_F(GpuPerformanceModelTest, LargeWrite) {"
        },
        {
            "sha": "c684a2ff97773eb9e9617e89da3a0598e7cd293c",
            "filename": "third_party/xla/xla/service/gpu/model/sol_gpu_cost_model_stats_collection.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -118,8 +118,8 @@ absl::StatusOr<bool> SolGpuCostModelStatsCollection::RunImpl(\n       SolLatencyEstimator::Create(\n           scheduler_config,\n           std::make_unique<GpuLatencyEstimator>(pointer_size_), device_info_,\n-          shape_size_in_bytes_fn_, module->entry_computation(),\n-          symbolic_expr_context_, std::move(cost_analysis)));\n+          shape_size_in_bytes_fn_, module->entry_computation(), mlir_context_,\n+          std::move(cost_analysis)));\n \n   for (HloComputation* comp : module->MakeComputationPostOrder()) {\n     for (HloInstruction* instr : comp->MakeInstructionPostOrder()) {"
        },
        {
            "sha": "6d7ccde25851f5fc9182c318ad3bd71bf2ee5dfa",
            "filename": "third_party/xla/xla/service/gpu/model/sol_gpu_cost_model_stats_collection.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -32,11 +32,11 @@ class SolGpuCostModelStatsCollection : public HloModulePass {\n   explicit SolGpuCostModelStatsCollection(\n       const se::DeviceDescription& device_description,\n       ShapeSizeFn shape_size_in_bytes_fn, int pointer_size,\n-      SymbolicExprContext* symbolic_expr_context)\n+      mlir::MLIRContext* mlir_context)\n       : device_info_(device_description),\n         shape_size_in_bytes_fn_(shape_size_in_bytes_fn),\n         pointer_size_(pointer_size),\n-        symbolic_expr_context_(symbolic_expr_context) {}\n+        mlir_context_(mlir_context) {}\n \n   absl::string_view name() const override {\n     return \"sol-gpu-cost-model-stats-collection\";\n@@ -51,7 +51,7 @@ class SolGpuCostModelStatsCollection : public HloModulePass {\n   se::DeviceDescription device_info_;\n   ShapeSizeFn shape_size_in_bytes_fn_;\n   int pointer_size_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "ea03349b3ab584a51cff30081aa57e2a6fec95a4",
            "filename": "third_party/xla/xla/service/gpu/model/sol_gpu_cost_model_stats_collection_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -38,6 +38,7 @@ limitations under the License.\n namespace xla::gpu {\n namespace {\n \n+using ::mlir::MLIRContext;\n using ::testing::Gt;\n using ::testing::Property;\n \n@@ -49,8 +50,7 @@ class SolGpuCostModelStatsCollectionTest\n   se::DeviceDescription device_info_ =\n       TestGpuDeviceInfo::RTXA6000DeviceInfo(se::CudaComputeCapability(9, 0));\n   int pointer_size_ = 8;\n-  mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n+  MLIRContext mlir_context_;\n };\n \n TEST_F(SolGpuCostModelStatsCollectionTest,\n@@ -77,7 +77,7 @@ TEST_F(SolGpuCostModelStatsCollectionTest,\n   TF_ASSERT_OK_AND_ASSIGN(bool changed,\n                           SolGpuCostModelStatsCollection(\n                               device_info_, HloCostAnalysis::DefaultShapeSize,\n-                              pointer_size_, &symbolic_expr_context_)\n+                              pointer_size_, &mlir_context_)\n                               .Run(module.get()));\n \n   VLOG(1) << module->ToString();\n@@ -114,7 +114,7 @@ TEST_F(SolGpuCostModelStatsCollectionTest,\n   TF_ASSERT_OK_AND_ASSIGN(bool changed,\n                           SolGpuCostModelStatsCollection(\n                               device_info_, HloCostAnalysis::DefaultShapeSize,\n-                              pointer_size_, &symbolic_expr_context_)\n+                              pointer_size_, &mlir_context_)\n                               .Run(module.get()));\n   VLOG(1) << module->ToString();\n   EXPECT_FALSE(changed);"
        },
        {
            "sha": "7c47c4cea5976e3556d7d2e651bff5fc87b6cbbf",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 22,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -27,7 +27,6 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/time/time.h\"\n #include \"xla/hlo/analysis/hlo_dataflow_analysis.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -55,9 +54,10 @@ limitations under the License.\n \n namespace xla {\n namespace gpu {\n-\n namespace {\n \n+using ::mlir::MLIRContext;\n+\n bool IsSupportedCollectiveOp(const HloInstruction& instr) {\n   return HloPredicateIsOp<HloOpcode::kAllReduceStart, HloOpcode::kAllReduce,\n                           HloOpcode::kReduceScatter, HloOpcode::kAllGatherStart,\n@@ -106,8 +106,7 @@ absl::StatusOr<absl::Duration> DCNCollectiveDuration(\n     int num_participating_hosts, int num_communicators,\n     const HloInstruction& instr, const se::DeviceDescription& gpu_device_info,\n     const SolGPUCostModel::Config& sol_flags,\n-    const GpuHloCostAnalysis& analysis,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    const GpuHloCostAnalysis& analysis, MLIRContext* mlir_context) {\n   SolGPUCostModel sol_model(sol_flags);\n   const int64_t msg_size = analysis.BytesTransferred(instr);\n \n@@ -116,7 +115,7 @@ absl::StatusOr<absl::Duration> DCNCollectiveDuration(\n   absl::Duration result = absl::Seconds(1.0f * analysis.bytes_accessed(instr) /\n                                         gpu_device_info.memory_bandwidth());\n   GpuPerformanceModelOwning gpu_performance_model{gpu_device_info,\n-                                                  symbolic_expr_context};\n+                                                  mlir_context};\n   switch (instr.opcode()) {\n     case HloOpcode::kAllGather:\n     case HloOpcode::kAllGatherStart: {\n@@ -207,7 +206,7 @@ absl::StatusOr<absl::Duration> DispatchEstimation(\n     const SolGPUCostModel::Config& sol_flags,\n     const GpuHloCostAnalysis& analysis,\n     const CollectiveInterpolator* collective_interpolator,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   TF_RETURN_IF_ERROR(communication_type.status());\n \n   GPUCommunicationType comm = *communication_type;\n@@ -220,13 +219,13 @@ absl::StatusOr<absl::Duration> DispatchEstimation(\n       return DCNCollectiveDuration(\n           num_groups_and_devices->second / partition_size,\n           /*num_communicators=*/num_groups_and_devices->first, instr,\n-          gpu_device_info, sol_flags, analysis, symbolic_expr_context);\n+          gpu_device_info, sol_flags, analysis, mlir_context);\n     }\n     case GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL: {\n       return DCNCollectiveDuration(\n           num_groups_and_devices->second,\n           /*num_communicators=*/num_groups_and_devices->first, instr,\n-          gpu_device_info, sol_flags, analysis, symbolic_expr_context);\n+          gpu_device_info, sol_flags, analysis, mlir_context);\n     }\n     case GPUCommunicationType::SINGLE_PARTITION: {\n       if (collective_interpolator == nullptr) {\n@@ -279,8 +278,7 @@ absl::StatusOr<std::unique_ptr<MatmulInterpolator>> CreateMatmulInterpolator(\n SolLatencyEstimator::ComputeCollectiveTime(\n     const HloInstruction& instr, const se::DeviceDescription& gpu_device_info,\n     HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n-    const SolGPUCostModel::Config& sol_flags,\n-    SymbolicExprContext* symbolic_expr_context,\n+    const SolGPUCostModel::Config& sol_flags, MLIRContext* mlir_context,\n     const CollectiveInterpolator* collective_interpolator) {\n   GpuHloCostAnalysis analysis(\n       GpuHloCostAnalysis::Options{shape_size_fn,\n@@ -296,17 +294,16 @@ SolLatencyEstimator::ComputeCollectiveTime(\n   }\n \n   return SolLatencyEstimator::ComputeCollectiveTime(\n-      instr, gpu_device_info, shape_size_fn, sol_flags, analysis,\n-      symbolic_expr_context, collective_interpolator);\n+      instr, gpu_device_info, shape_size_fn, sol_flags, analysis, mlir_context,\n+      collective_interpolator);\n }\n \n /*static*/ absl::StatusOr<absl::Duration>\n SolLatencyEstimator::ComputeCollectiveTime(\n     const HloInstruction& instr, const se::DeviceDescription& gpu_device_info,\n     HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n     const SolGPUCostModel::Config& sol_flags,\n-    const GpuHloCostAnalysis& analysis,\n-    SymbolicExprContext* symbolic_expr_context,\n+    const GpuHloCostAnalysis& analysis, MLIRContext* mlir_context,\n     const CollectiveInterpolator* collective_interpolator) {\n   if (HloDataflowAnalysis::IsAsynchronousOperationDone(instr.opcode())) {\n     VLOG(8) << \"Returning 0 cost for async done op \" << instr.name();\n@@ -331,7 +328,7 @@ SolLatencyEstimator::ComputeCollectiveTime(\n       absl::Duration result,\n       DispatchEstimation(communication_type, *collective_instr, gpu_device_info,\n                          sol_flags, analysis, collective_interpolator,\n-                         symbolic_expr_context));\n+                         mlir_context));\n   return result;\n }\n \n@@ -341,8 +338,7 @@ SolLatencyEstimator::Create(\n     std::unique_ptr<LatencyEstimator> latency_estimator,\n     const se::DeviceDescription& gpu_info,\n     HloCostAnalysis::ShapeSizeFunction shape_size_function,\n-    const HloComputation* computation,\n-    SymbolicExprContext* symbolic_expr_context,\n+    const HloComputation* computation, MLIRContext* mlir_context,\n     std::unique_ptr<GpuHloCostAnalysis> cost_analysis) {\n   if (cost_analysis == nullptr) {\n     cost_analysis =\n@@ -367,7 +363,7 @@ SolLatencyEstimator::Create(\n   return std::unique_ptr<SolLatencyEstimator>(new SolLatencyEstimator(\n       config, std::move(latency_estimator), gpu_info, std::move(cost_analysis),\n       shape_size_function, sol_config, std::move(collective_interpolator),\n-      std::move(matmul_interpolator), symbolic_expr_context));\n+      std::move(matmul_interpolator), mlir_context));\n }\n \n /*static*/ bool SolLatencyEstimator::IsSupportedForModule(\n@@ -413,7 +409,7 @@ LatencyEstimator::TimeCost SolLatencyEstimator::GetLatencyBetween(\n \n   absl::StatusOr<absl::Duration> coll_time = ComputeCollectiveTime(\n       from.GetInstr(), gpu_info_, shape_size_function_, sol_flags_,\n-      *cost_analysis_, symbolic_expr_context_, collective_interpolator_.get());\n+      *cost_analysis_, mlir_context_, collective_interpolator_.get());\n   if (!coll_time.ok()) {\n     VLOG(1) << \"Failed to compute collective time: \" << coll_time.status()\n             << \" for \" << from.GetInstr().name();\n@@ -478,17 +474,17 @@ SolLatencyEstimator::SolLatencyEstimator(\n     const SolGPUCostModel::Config sol_flags,\n     std::unique_ptr<CollectiveInterpolator> collective_interpolator,\n     std::unique_ptr<MatmulInterpolator> matmul_interpolator,\n-    SymbolicExprContext* symbolic_expr_context)\n+    MLIRContext* mlir_context)\n     : config_(config),\n       gpu_info_(gpu_info),\n-      gpu_performance_model_(gpu_info, symbolic_expr_context),\n+      gpu_performance_model_(gpu_info, mlir_context),\n       cost_analysis_(std::move(cost_analysis)),\n       latency_estimator_(std::move(latency_estimator)),\n       shape_size_function_(shape_size_function),\n       sol_flags_(sol_flags),\n       collective_interpolator_(std::move(collective_interpolator)),\n       matmul_interpolator_(std::move(matmul_interpolator)),\n-      symbolic_expr_context_(symbolic_expr_context) {}\n+      mlir_context_(mlir_context) {}\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "92a284613114b4b1169e9dae60313064e5238103",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator.h",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -21,7 +21,6 @@ limitations under the License.\n \n #include \"absl/status/statusor.h\"\n #include \"absl/time/time.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/gpu/model/collective_interpolator.h\"\n@@ -66,8 +65,7 @@ class SolLatencyEstimator : public LatencyEstimator {\n   static absl::StatusOr<absl::Duration> ComputeCollectiveTime(\n       const HloInstruction& instr, const se::DeviceDescription& gpu_device_info,\n       HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n-      const SolGPUCostModel::Config& sol_flags,\n-      SymbolicExprContext* symbolic_expr_context,\n+      const SolGPUCostModel::Config& sol_flags, mlir::MLIRContext* mlir_context,\n       const CollectiveInterpolator* collective_interpolator = nullptr);\n \n   // Computes the time it takes to execute the given collective instruction.\n@@ -79,8 +77,7 @@ class SolLatencyEstimator : public LatencyEstimator {\n       const HloInstruction& instr, const se::DeviceDescription& gpu_device_info,\n       HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n       const SolGPUCostModel::Config& sol_flags,\n-      const GpuHloCostAnalysis& cost_analysis,\n-      SymbolicExprContext* symbolic_expr_context,\n+      const GpuHloCostAnalysis& cost_analysis, mlir::MLIRContext* mlir_context,\n       const CollectiveInterpolator* collective_interpolator = nullptr);\n \n   // Factory method to create a `SolLatencyEstimator`.\n@@ -89,8 +86,7 @@ class SolLatencyEstimator : public LatencyEstimator {\n       std::unique_ptr<LatencyEstimator> latency_estimator,\n       const se::DeviceDescription& gpu_info,\n       HloCostAnalysis::ShapeSizeFunction shape_size_function,\n-      const HloComputation* computation,\n-      SymbolicExprContext* symbolic_expr_context,\n+      const HloComputation* computation, mlir::MLIRContext* mlir_context,\n       std::unique_ptr<GpuHloCostAnalysis> cost_analysis = nullptr);\n \n   // Returns true if the module is supported by the SoL latency estimator.\n@@ -112,7 +108,7 @@ class SolLatencyEstimator : public LatencyEstimator {\n       SolGPUCostModel::Config sol_flags,\n       std::unique_ptr<CollectiveInterpolator> collective_interpolator,\n       std::unique_ptr<MatmulInterpolator> matmul_interpolator,\n-      SymbolicExprContext* symbolic_expr_context);\n+      mlir::MLIRContext* mlir_context);\n \n   const SchedulerConfig config_;\n   const se::DeviceDescription& gpu_info_;\n@@ -123,7 +119,7 @@ class SolLatencyEstimator : public LatencyEstimator {\n   const SolGPUCostModel::Config sol_flags_;\n   const std::unique_ptr<const CollectiveInterpolator> collective_interpolator_;\n   const std::unique_ptr<const MatmulInterpolator> matmul_interpolator_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "1914cf8819d19c0fc9565763c316c53122bec132",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -26,7 +26,6 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/time/time.h\"\n #include \"mlir/IR/MLIRContext.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n@@ -97,17 +96,16 @@ class SolLatencyEstimatorTest : public HloHardwareIndependentTestBase,\n   absl::StatusOr<absl::Duration> ComputeCollectiveTime(\n       const HloInstruction& instr) {\n     return SolLatencyEstimator::ComputeCollectiveTime(\n-        instr, gpu_device_info_, shape_size_fn_, sol_flags_,\n-        &symbolic_expr_context_, collective_interpolator_.get());\n+        instr, gpu_device_info_, shape_size_fn_, sol_flags_, &mlir_context_,\n+        collective_interpolator_.get());\n   }\n \n   absl::Duration ComputeNodeCost(const HloInstruction& instr,\n                                  const HloComputation* computation) {\n     std::unique_ptr<SolLatencyEstimator> estimator =\n-        *SolLatencyEstimator::Create(scheduler_config_,\n-                                     std::make_unique<DummyLatencyEstimator>(),\n-                                     gpu_device_info_, shape_size_fn_,\n-                                     computation, &symbolic_expr_context_);\n+        *SolLatencyEstimator::Create(\n+            scheduler_config_, std::make_unique<DummyLatencyEstimator>(),\n+            gpu_device_info_, shape_size_fn_, computation, &mlir_context_);\n     LatencyEstimator::TimeCost cost_val = estimator->NodeCost(&instr);\n     return absl::Microseconds(static_cast<int64_t>(cost_val));\n   }\n@@ -116,10 +114,9 @@ class SolLatencyEstimatorTest : public HloHardwareIndependentTestBase,\n                                    const HloGraphNode& target,\n                                    const HloComputation* computation) {\n     std::unique_ptr<SolLatencyEstimator> estimator =\n-        *SolLatencyEstimator::Create(scheduler_config_,\n-                                     std::make_unique<DummyLatencyEstimator>(),\n-                                     gpu_device_info_, shape_size_fn_,\n-                                     computation, &symbolic_expr_context_);\n+        *SolLatencyEstimator::Create(\n+            scheduler_config_, std::make_unique<DummyLatencyEstimator>(),\n+            gpu_device_info_, shape_size_fn_, computation, &mlir_context_);\n     LatencyEstimator::TimeCost cost_val =\n         estimator->GetLatencyBetween(from, target);\n     return absl::Microseconds(static_cast<int64_t>(cost_val));\n@@ -131,7 +128,6 @@ class SolLatencyEstimatorTest : public HloHardwareIndependentTestBase,\n   SchedulerConfig scheduler_config_;\n   std::unique_ptr<CollectiveInterpolator> collective_interpolator_;\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_P(SolLatencyEstimatorTest, TestLatencyEstimation) {"
        },
        {
            "sha": "3cde9c3465eff3b64128cad3505c3eec72e35354",
            "filename": "third_party/xla/xla/service/gpu/model/triton_emitter_constraints.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ftriton_emitter_constraints.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ftriton_emitter_constraints.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ftriton_emitter_constraints.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -121,10 +121,8 @@ TritonEmitterConstraints::DeriveCustomConstraints(\n \n       // TODO(b/446856820): Remove this once we use SymbolicMap here and\n       // therefore we can get the context directly.\n-      SymbolicExprContext symbolic_expr_context{ctx};\n       IndexingMap reshape_indexing_map =\n-          ComputeOutputToInputIndexing(hlo, /*output_id=*/0,\n-                                       &symbolic_expr_context)\n+          ComputeOutputToInputIndexing(hlo, /*output_id=*/0, ctx)\n               .indexing_maps[0]\n               .begin()\n               ->map();"
        },
        {
            "sha": "06875cea1f74edc4141f7c7599823295688bc607",
            "filename": "third_party/xla/xla/service/gpu/model/triton_emitter_constraints_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ftriton_emitter_constraints_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ftriton_emitter_constraints_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Ftriton_emitter_constraints_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -59,7 +59,7 @@ class TritonEmitterConstraintsTest : public HloHardwareIndependentTestBase {\n             *module->entry_computation()\n                  ->root_instruction()\n                  ->fused_instructions_computation(),\n-            &symbolic_expr_context_, constraints_builder);\n+            &mlir_context_, constraints_builder);\n \n     if (std::holds_alternative<SymbolicTileAnalysis>(analysis_or_error)) {\n       return std::get<SymbolicTileAnalysis>(std::move(analysis_or_error));\n@@ -70,7 +70,6 @@ class TritonEmitterConstraintsTest : public HloHardwareIndependentTestBase {\n   }\n \n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n   se::DeviceDescription device_description_ =\n       TestGpuDeviceInfo::RTXA6000DeviceInfo();\n };"
        },
        {
            "sha": "c3d154630d33ec40b0e1893179afdaf2be9faabb",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -386,7 +386,7 @@ absl::Status NVPTXCompiler::AddGemmFusionAutotuningPasses(\n     se::StreamExecutor* stream_executor) {\n   pipeline->AddPass<GemmFusionAutotuner>(autotune_config, toolkit_version,\n                                          thread_pool, key_value_store,\n-                                         symbolic_expr_context());\n+                                         mlir_context());\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "f003564f9b2bfede00b01089d325f0a40dd7661f",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -76,8 +76,7 @@ class NVPTXCompilerTest : public HloTestBase {\n     std::unique_ptr<GpuAliasInfo> alias_info =\n         compiler.GetAliasInfo(gpu_device_info);\n     TF_RETURN_IF_ERROR(ScheduleGpuModule(module, pointer_size, gpu_device_info,\n-                                         &symbolic_expr_context_,\n-                                         alias_info.get())\n+                                         &mlir_context_, alias_info.get())\n                            .status());\n \n     auto buffer_size_bytes_function =\n@@ -94,7 +93,6 @@ class NVPTXCompilerTest : public HloTestBase {\n \n  protected:\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n class NVPTXCompilerTestTriton : public NVPTXCompilerTest {"
        },
        {
            "sha": "395cb785cded84a85727c0a88690b4ac4d4975f1",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -196,7 +196,6 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/backends/gpu/codegen/triton:support\",\n-        \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/hlo/utils:hlo_traversal\",\n@@ -217,6 +216,7 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -1869,7 +1869,6 @@ cc_library(\n         \"//xla:debug_options_flags\",\n         \"//xla:shape_util\",\n         \"//xla/hlo/analysis:hlo_dfs_reachability\",\n-        \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/service:hlo_cost_analysis\",\n@@ -1890,6 +1889,7 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -2043,6 +2043,7 @@ cc_library(\n         \"@com_google_absl//absl/time\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n "
        },
        {
            "sha": "d9bbc6808310dbe494303c6bfa922af6da90eff7",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_combiner_annotator.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -97,16 +97,14 @@ struct Metadata {\n //   synchronous post scheduling.\n absl::StatusOr<Metadata> GetSchedulingMetadata(\n     const HloModule& module, int64_t pointer_size,\n-    const se::DeviceDescription& device_info,\n-    SymbolicExprContext* symbolic_expr_context,\n+    const se::DeviceDescription& device_info, mlir::MLIRContext* mlir_context,\n     const GpuAliasInfo* alias_info) {\n   std::unique_ptr<HloModule> cloned_module = module.Clone();\n   AnnotateCollectives(cloned_module.get());\n   TF_RETURN_IF_ERROR(RunAsyncCollectivesConversionPasses(cloned_module.get()));\n-  TF_ASSIGN_OR_RETURN(\n-      ScheduleMetadata schedule_metadata,\n-      ScheduleGpuModule(cloned_module.get(), pointer_size, device_info,\n-                        symbolic_expr_context, alias_info));\n+  TF_ASSIGN_OR_RETURN(ScheduleMetadata schedule_metadata,\n+                      ScheduleGpuModule(cloned_module.get(), pointer_size,\n+                                        device_info, mlir_context, alias_info));\n   TF_RETURN_IF_ERROR(AnnotateSyncCollectives(cloned_module.get()));\n   return Metadata{schedule_metadata.peak_memory_usage,\n                   SyncCollectiveIds(*cloned_module)};\n@@ -129,8 +127,8 @@ absl::StatusOr<bool> CollectiveCombinerAnnotator::RunImpl(\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n   TF_ASSIGN_OR_RETURN(\n       Metadata metadata,\n-      GetSchedulingMetadata(*module, pointer_size_, device_info_,\n-                            symbolic_expr_context_, alias_info_));\n+      GetSchedulingMetadata(*module, pointer_size_, device_info_, mlir_context_,\n+                            alias_info_));\n   int64_t combiner_threshold =\n       MaxAvailableMemory(*module, device_info_) - metadata.peak_memory_bytes;\n   if (combiner_threshold <= 0) {"
        },
        {
            "sha": "c8854d79b7861aded6b41161f82fa4b44eb1f686",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_combiner_annotator.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -39,11 +39,11 @@ class CollectiveCombinerAnnotator : public HloModulePass {\n   CollectiveCombinerAnnotator(se::DeviceDescription device_info,\n                               const GpuAliasInfo* alias_info,\n                               int64_t pointer_size,\n-                              SymbolicExprContext* symbolic_expr_context)\n+                              mlir::MLIRContext* mlir_context)\n       : device_info_(std::move(device_info)),\n         alias_info_(alias_info),\n         pointer_size_(pointer_size),\n-        symbolic_expr_context_(symbolic_expr_context) {}\n+        mlir_context_(mlir_context) {}\n \n   absl::string_view name() const override {\n     return \"collective-combiner-annotator\";\n@@ -58,7 +58,7 @@ class CollectiveCombinerAnnotator : public HloModulePass {\n   const se::DeviceDescription device_info_;\n   const GpuAliasInfo* alias_info_;\n   const int64_t pointer_size_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n // Returns true if `instr` is a combinable sync collective. False otherwise."
        },
        {
            "sha": "522094a560e74917ee6d48233a60da6eea2a272e",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_combiner_annotator_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_combiner_annotator_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -47,11 +47,10 @@ class CollectiveCombinerAnnotatorTest : public HloHardwareIndependentTestBase {\n     GpuAliasInfo alias_info(device_info);\n     return RunHloPass(\n         CollectiveCombinerAnnotator(std::move(device_info), &alias_info,\n-                                    pointer_size, &symbolic_expr_context_),\n+                                    pointer_size, &mlir_context_),\n         module);\n   }\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(CollectiveCombinerAnnotatorTest, SynchronousCollectivesNoOverlap) {"
        },
        {
            "sha": "3f510f056458146db296b8adcd1d92f125ddc93f",
            "filename": "third_party/xla/xla/service/gpu/transforms/fusion_block_level_rewriter.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -29,7 +29,6 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"llvm/Support/MathExtras.h\"\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -164,7 +163,7 @@ absl::StatusOr<bool> ProcessFusionInstruction(\n     HloFusionInstruction* fusion_instruction,\n     const se::DeviceDescription& device_info,\n     HloCostAnalysis::ShapeSizeFunction shape_size,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    mlir::MLIRContext* mlir_context) {\n   TF_ASSIGN_OR_RETURN(bool should_try_rewrite,\n                       ShouldTryRewriteFusion(fusion_instruction, device_info));\n   if (!should_try_rewrite) {\n@@ -195,7 +194,7 @@ absl::StatusOr<bool> ProcessFusionInstruction(\n \n   HloFusionAnalysisCache fusion_analysis_cache(device_info);\n   GpuPerformanceModelWithIndexingAnalysis indexing_performance_model(\n-      &device_info, &fusion_analysis_cache, shape_size, symbolic_expr_context);\n+      &device_info, &fusion_analysis_cache, shape_size, mlir_context);\n \n   auto fusion_adaptor = HloFusionAdaptor::ForInstruction(\n       Cast<HloFusionInstruction>(fusion_instruction));\n@@ -252,9 +251,9 @@ absl::StatusOr<bool> FusionBlockLevelRewriter::RunImpl(\n     }\n     HloFusionInstruction* fusion_instruction =\n         ::xla::Cast<HloFusionInstruction>(computation->FusionInstruction());\n-    TF_ASSIGN_OR_RETURN(bool changed, ProcessFusionInstruction(\n-                                          fusion_instruction, device_info_,\n-                                          shape_size_, symbolic_expr_context_));\n+    TF_ASSIGN_OR_RETURN(\n+        bool changed, ProcessFusionInstruction(fusion_instruction, device_info_,\n+                                               shape_size_, mlir_context_));\n \n     has_changed |= changed;\n   }"
        },
        {
            "sha": "81e076423f2830d3338e1603abdad74d0b80da82",
            "filename": "third_party/xla/xla/service/gpu/transforms/fusion_block_level_rewriter.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -19,7 +19,7 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n@@ -33,10 +33,10 @@ class FusionBlockLevelRewriter : public HloModulePass {\n   explicit FusionBlockLevelRewriter(\n       const se::DeviceDescription& device_info,\n       HloCostAnalysis::ShapeSizeFunction shape_size,\n-      SymbolicExprContext* symbolic_expr_context)\n+      mlir::MLIRContext* mlir_context)\n       : device_info_(device_info),\n         shape_size_(shape_size),\n-        symbolic_expr_context_(symbolic_expr_context) {}\n+        mlir_context_(mlir_context) {}\n \n   absl::string_view name() const override {\n     return \"fusion-block-level-rewriter\";\n@@ -50,7 +50,7 @@ class FusionBlockLevelRewriter : public HloModulePass {\n  private:\n   const se::DeviceDescription& device_info_;\n   HloCostAnalysis::ShapeSizeFunction shape_size_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "dfe161f86da219f155161f9f9d4a3e251d93c5af",
            "filename": "third_party/xla/xla/service/gpu/transforms/fusion_block_level_rewriter_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -27,7 +27,6 @@ License.\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n #include \"xla/codegen/tiling/symbolic_tile_analysis.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n@@ -70,7 +69,6 @@ class FusionBlockLevelRewriterTest : public HloHardwareIndependentTestBase {\n     return debug_options;\n   }\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n TEST_F(FusionBlockLevelRewriterTest,\n@@ -91,7 +89,7 @@ ENTRY entry {\n                           ParseAndReturnVerifiedModule(hlo_text));\n   EXPECT_THAT(\n       FusionBlockLevelRewriter(device_info_, HloCostAnalysis::DefaultShapeSize,\n-                               &symbolic_expr_context_)\n+                               &mlir_context_)\n           .Run(module.get()),\n       absl_testing::IsOkAndHolds(false));\n }\n@@ -113,7 +111,7 @@ ENTRY entry {\n \n   EXPECT_THAT(\n       FusionBlockLevelRewriter(device_info_, HloCostAnalysis::DefaultShapeSize,\n-                               &symbolic_expr_context_)\n+                               &mlir_context_)\n           .Run(module.get()),\n       absl_testing::IsOkAndHolds(true));\n   const HloInstruction* root = module->entry_computation()->root_instruction();\n@@ -141,10 +139,10 @@ ENTRY entry {\n   ASSERT_FALSE(std::holds_alternative<SymbolicTileAnalysis>(\n       SymbolicTileAnalysis::AnalyzeComputation(\n           *module->GetComputationWithName(\"fusion_computation\"),\n-          &symbolic_expr_context_)));\n+          &mlir_context_)));\n   EXPECT_THAT(\n       FusionBlockLevelRewriter(device_info_, HloCostAnalysis::DefaultShapeSize,\n-                               &symbolic_expr_context_)\n+                               &mlir_context_)\n           .Run(module.get()),\n       absl_testing::IsOkAndHolds(false));\n }\n@@ -169,7 +167,7 @@ ENTRY entry {\n       device_info_.gpu_compute_capability()));\n   EXPECT_THAT(\n       FusionBlockLevelRewriter(device_info_, HloCostAnalysis::DefaultShapeSize,\n-                               &symbolic_expr_context_)\n+                               &mlir_context_)\n           .Run(module.get()),\n       absl_testing::IsOkAndHolds(false));\n }\n@@ -205,7 +203,7 @@ ENTRY entry  {\n   se::DeviceDescription device_info{TestGpuDeviceInfo::RTXA6000DeviceInfo(\n       se::CudaComputeCapability::Ampere())};\n   FusionBlockLevelRewriter rewriter(\n-      device_info, HloCostAnalysis::DefaultShapeSize, &symbolic_expr_context_);\n+      device_info, HloCostAnalysis::DefaultShapeSize, &mlir_context_);\n   EXPECT_THAT(rewriter.Run(module.get()), absl_testing::IsOkAndHolds(true));\n   const HloInstruction* root = module->entry_computation()->root_instruction();\n   EXPECT_EQ(root->opcode(), HloOpcode::kFusion);"
        },
        {
            "sha": "777edf78812081ca7f03e7a00e4231b47720e6b6",
            "filename": "third_party/xla/xla/service/gpu/transforms/multi_output_fusion.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -431,8 +431,7 @@ absl::StatusOr<bool> MultiOutputFusion::DoMultiOutputFusion() {\n       computation_->MakeInstructionPostOrder();\n \n   FusionInfoCache fusion_info_cache(device_info_);\n-  GpuPerformanceModelOwning gpu_performance_model(device_info_,\n-                                                  symbolic_expr_context_);\n+  GpuPerformanceModelOwning gpu_performance_model(device_info_, mlir_context_);\n   // Traverse the HLO in uses-before-defs order.\n   for (auto it = defs_before_uses.rbegin(); it != defs_before_uses.rend();\n        ++it) {"
        },
        {
            "sha": "bfcf181e889b0a682df0fb7f7112c0a5d4781301",
            "filename": "third_party/xla/xla/service/gpu/transforms/multi_output_fusion.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -21,8 +21,8 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/analysis/hlo_dfs_reachability.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -98,10 +98,10 @@ class MultiOutputFusion : public HloModulePass {\n   explicit MultiOutputFusion(\n       const se::DeviceDescription& device_info,\n       HloCostAnalysis::ShapeSizeFunction shape_size_function,\n-      SymbolicExprContext* symbolic_expr_context)\n+      mlir::MLIRContext* mlir_context)\n       : device_info_(device_info),\n         shape_size_function_(shape_size_function),\n-        symbolic_expr_context_(symbolic_expr_context) {}\n+        mlir_context_(mlir_context) {}\n \n   absl::string_view name() const override { return \"multi_output_fusion\"; }\n \n@@ -130,7 +130,7 @@ class MultiOutputFusion : public HloModulePass {\n \n   se::DeviceDescription device_info_;\n   HloCostAnalysis::ShapeSizeFunction shape_size_function_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "a8847e3910be735d55ced6b5773a31b927e11744",
            "filename": "third_party/xla/xla/service/gpu/transforms/multi_output_fusion_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fmulti_output_fusion_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -24,7 +24,6 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n #include \"mlir/IR/MLIRContext.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -48,22 +47,19 @@ namespace m = ::xla::match;\n class MultiOutputFusionTest : public HloHardwareIndependentTestBase {\n  public:\n   MultiOutputFusion mof_{TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n-                         HloCostAnalysis::DefaultShapeSize,\n-                         &symbolic_expr_context_};\n+                         HloCostAnalysis::DefaultShapeSize, &mlir_context_};\n \n   void CheckMultiOutputFusion(absl::string_view hlo,\n                               std::optional<absl::string_view> expected) {\n     RunAndFilecheckHloRewrite(\n         hlo,\n         MultiOutputFusion{TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n-                          HloCostAnalysis::DefaultShapeSize,\n-                          &symbolic_expr_context_},\n+                          HloCostAnalysis::DefaultShapeSize, &mlir_context_},\n         expected);\n   }\n \n  protected:\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n const char kModulePrefix[] = R\"("
        },
        {
            "sha": "c42679d070238542af0d1f76808cfb8d1ff47670",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -74,9 +74,10 @@ limitations under the License.\n #include \"xla/xla_data.pb.h\"\n \n namespace xla::gpu {\n-\n namespace {\n \n+using ::mlir::MLIRContext;\n+\n // Creates a fusion for instructions starting from 'root' and returns it.\n absl::StatusOr<HloInstruction*> FuseInstructionsFromRoot(HloInstruction& root) {\n   std::vector<HloInstruction*> instructions =\n@@ -268,7 +269,7 @@ absl::Status FuseAndAnnotateConcatOperands(HloComputation* computation) {\n // Transforms a fusion into an equivalent nested fusion if it has a single dot.\n // Returns ok if the transformation was successful.\n absl::Status MakeNestedFusionFromGemmFusion(\n-    HloFusionInstruction* fusion, HloInstruction* dot, SymbolicExprContext* ctx,\n+    HloFusionInstruction* fusion, HloInstruction* dot, MLIRContext* ctx,\n     const se::DeviceDescription& device_description) {\n   TF_RETURN_IF_ERROR(IsDot(*dot));\n   const bool is_scaled_dot = dot->opcode() == HloOpcode::kScaledDot;\n@@ -1110,9 +1111,9 @@ bool IsFeatureEnabled(const HloModule* module,\n class NestGemmFusionVisitor : public DfsHloRewriteVisitor {\n  public:\n   explicit NestGemmFusionVisitor(\n-      SymbolicExprContext* symbolic_expr_context, CallGraph* call_graph,\n+      MLIRContext* mlir_context, CallGraph* call_graph,\n       const se::DeviceDescription& device_description)\n-      : symbolic_expr_context_(symbolic_expr_context),\n+      : mlir_context_(mlir_context),\n         call_graph_(call_graph),\n         device_description_(device_description) {}\n \n@@ -1220,7 +1221,7 @@ class NestGemmFusionVisitor : public DfsHloRewriteVisitor {\n     TF_RETURN_IF_ERROR(\n         TryHoistBitcastsInComputationToCallers(instr, call_graph));\n     TF_RETURN_IF_ERROR(MakeNestedFusionFromGemmFusion(\n-        fusion, instr, symbolic_expr_context_, device_description_));\n+        fusion, instr, mlir_context_, device_description_));\n \n     MarkAsChanged();\n     bool scaled_dot_enabled =\n@@ -1299,7 +1300,7 @@ class NestGemmFusionVisitor : public DfsHloRewriteVisitor {\n   }\n \n  private:\n-  SymbolicExprContext* symbolic_expr_context_;\n+  MLIRContext* mlir_context_;\n   CallGraph* call_graph_;\n   const se::DeviceDescription& device_description_;\n };\n@@ -1313,7 +1314,7 @@ absl::StatusOr<bool> NestGemmFusion::RunOnModule(\n   auto call_graph = CallGraph::Build(module, execution_threads);\n   for (HloComputation* computation :\n        module->MakeNonfusionComputations(execution_threads)) {\n-    NestGemmFusionVisitor visitor(symbolic_expr_context_, call_graph.get(),\n+    NestGemmFusionVisitor visitor(mlir_context_, call_graph.get(),\n                                   device_description_);\n     TF_RETURN_IF_ERROR(computation->Accept(&visitor));\n     changed |= visitor.changed();\n@@ -1341,8 +1342,8 @@ absl::StatusOr<bool> NestGemmFusion::RunImpl(\n namespace detail {\n \n absl::StatusOr<BlockLevelParameters> FindBlockLevelParameters(\n-    HloInstruction* dot, const TritonGemmConfig& config,\n-    SymbolicExprContext* ctx, const se::DeviceDescription& device_description) {\n+    HloInstruction* dot, const TritonGemmConfig& config, MLIRContext* ctx,\n+    const se::DeviceDescription& device_description) {\n   TF_RETURN_IF_ERROR(IsDot(*dot));\n   HloComputation* computation = dot->parent();\n   VLOG(3) << \"FindOutputTileSizesForEpilogue of computation: \""
        },
        {
            "sha": "2d94ad1c4417f6dd0ce5a8cffaf2229d3edd9d68",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion.h",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -47,9 +47,8 @@ namespace xla::gpu {\n class NestGemmFusion : public HloModulePass {\n  public:\n   explicit NestGemmFusion(const se::DeviceDescription& device_description,\n-                          SymbolicExprContext* symbolic_expr_context)\n-      : device_description_(device_description),\n-        symbolic_expr_context_(symbolic_expr_context) {}\n+                          mlir::MLIRContext* mlir_context)\n+      : device_description_(device_description), mlir_context_(mlir_context) {}\n \n   absl::string_view name() const override { return \"nest_gemm_fusion\"; }\n \n@@ -60,7 +59,7 @@ class NestGemmFusion : public HloModulePass {\n \n  private:\n   const se::DeviceDescription device_description_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n   absl::StatusOr<bool> RunOnModule(\n       HloModule* module,\n       const absl::flat_hash_set<absl::string_view>& execution_threads);\n@@ -79,7 +78,7 @@ namespace detail {\n // is implemented.\n absl::StatusOr<BlockLevelParameters> FindBlockLevelParameters(\n     HloInstruction* dot, const TritonGemmConfig& config,\n-    SymbolicExprContext* symbolic_expr_context,\n+    mlir::MLIRContext* mlir_context,\n     const se::DeviceDescription& device_description);\n \n }  // namespace detail"
        },
        {
            "sha": "5a748621c56de7eaa2a24f3cc053ec4b84a47ced",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion_test.cc",
            "status": "modified",
            "additions": 94,
            "deletions": 95,
            "changes": 189,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -77,7 +77,6 @@ class NestGemmFusionTest : public HloHardwareIndependentTestBase {\n       TestGpuDeviceInfo::RTXA6000DeviceInfo(\n           se::GpuComputeCapability{se::CudaComputeCapability::Ampere()})};\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n \n   DebugOptions GetDebugOptionsForTest() const override {\n     DebugOptions options =\n@@ -117,9 +116,9 @@ ENTRY entry {\n })\";\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(hlo));\n-  ASSERT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  ASSERT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n \n   const HloInstruction* fusion = nullptr;\n@@ -176,9 +175,9 @@ ENTRY e {\n                          \"num_ctas\":1}}}\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(hlo));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   HloComputation* fusion_computation = module->entry_computation()\n                                            ->root_instruction()\n@@ -241,8 +240,8 @@ ENTRY e {\n )\";\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(hlo));\n   TF_ASSERT_OK_AND_ASSIGN(\n-      bool updated, NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                        .Run(module.get()));\n+      bool updated,\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()));\n   EXPECT_TRUE(updated);\n   HloInstruction* root = module->entry_computation()->root_instruction();\n   EXPECT_EQ(root->opcode(), HloOpcode::kTuple);\n@@ -295,9 +294,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  ASSERT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  ASSERT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n \n   const HloInstruction* fusion = nullptr;\n@@ -342,9 +341,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  ASSERT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  ASSERT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n \n   const HloInstruction* fusion = nullptr;\n@@ -389,9 +388,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -424,9 +423,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -458,9 +457,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n CHECK: f16[3,11]{1,0} convert(\n@@ -504,9 +503,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -540,9 +539,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n   CHECK: ENTRY\n@@ -584,9 +583,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -623,9 +622,9 @@ ENTRY entry_computation {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -659,9 +658,9 @@ ENTRY entry_computation {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -694,9 +693,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n }\n \n@@ -726,9 +725,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -772,7 +771,7 @@ ENTRY e {\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n   // We can nest the fusion including the broadcast.\n-  EXPECT_TRUE(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+  EXPECT_TRUE(NestGemmFusion(device_description_, &mlir_context_)\n                   .Run(module.get())\n                   .ok());\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n@@ -814,7 +813,7 @@ ENTRY e {\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n   // We can nest the fusion including the broadcast.\n-  EXPECT_TRUE(NestGemmFusion(device_description_, &symbolic_expr_context_)\n+  EXPECT_TRUE(NestGemmFusion(device_description_, &mlir_context_)\n                   .Run(module.get())\n                   .ok());\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n@@ -857,9 +856,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -897,9 +896,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()),\n                            R\"(\n@@ -948,9 +947,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()),\n                            absl::Substitute(R\"(\n@@ -999,9 +998,9 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1035,9 +1034,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1072,9 +1071,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1109,9 +1108,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()),\n                            absl::Substitute(R\"(\n@@ -1148,9 +1147,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   // Checks that transpose is on rank 3 tensor from hoisting bitcast1, not rank\n   // 4 tensor from hoisting bitcast0 first and then failing to hoist bitcast1.\n@@ -1187,9 +1186,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1223,9 +1222,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1260,9 +1259,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()),\n                            absl::Substitute(R\"(\n@@ -1295,9 +1294,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1332,9 +1331,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   TF_ASSERT_OK(verifier().Run(module.get()).status());\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n@@ -1373,9 +1372,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n CHECK-NOT: bitcast\n@@ -1424,9 +1423,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n CHECK-NOT: bitcast\n@@ -1472,9 +1471,9 @@ ENTRY e {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(\n                               absl::Substitute(hlo, HloOpcodeString(opcode))));\n-  EXPECT_THAT(NestGemmFusion(device_description_, &symbolic_expr_context_)\n-                  .Run(module.get()),\n-              absl_testing::IsOkAndHolds(true));\n+  EXPECT_THAT(\n+      NestGemmFusion(device_description_, &mlir_context_).Run(module.get()),\n+      absl_testing::IsOkAndHolds(true));\n   EXPECT_THAT(\n       RunFileCheck(module->ToString(HloPrintOptions::ShortParsable()), R\"(\n CHECK-NOT: bitcast"
        },
        {
            "sha": "05dfb5414d4fe7a8e7ca4c5cb5e4e32ce1797cfe",
            "filename": "third_party/xla/xla/service/gpu/transforms/priority_fusion.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -159,7 +159,7 @@ class PriorityFusionQueue {\n                       const se::DeviceDescription* device_info,\n                       FusionProcessDumpProto* fusion_process_dump,\n                       tsl::thread::ThreadPool* thread_pool,\n-                      SymbolicExprContext* symbolic_expr_context,\n+                      mlir::MLIRContext* mlir_context,\n                       HloFusionAnalysisCache& fusion_analysis_cache,\n                       FusionDeduplicationCache& fusion_deduplication_cache,\n                       bool triton_heroless_fusion_enabled)\n@@ -168,13 +168,12 @@ class PriorityFusionQueue {\n         cost_analysis_(cost_analysis_options, *device_info),\n         gpu_indexing_performance_model_(device_info, &fusion_analysis_cache,\n                                         cost_analysis_options.shape_size,\n-                                        symbolic_expr_context),\n+                                        mlir_context),\n         fusion_process_dump_(fusion_process_dump),\n         thread_pool_(thread_pool),\n         fusion_analysis_cache_(fusion_analysis_cache),\n         gpu_performance_model_(*device_info, fusion_analysis_cache,\n-                               gpu_performance_model_cache_,\n-                               symbolic_expr_context),\n+                               gpu_performance_model_cache_, mlir_context),\n         fusion_deduplication_cache_(fusion_deduplication_cache),\n         fusion_info_cache_(*device_info_),\n         reachability_(HloDfsReachability::Build(computation)),\n@@ -1170,7 +1169,7 @@ absl::StatusOr<bool> PriorityFusion::RunImpl(\n \n     auto fusion_queue = std::make_unique<PriorityFusionQueue>(\n         computation, cost_analysis_options_, &device_info_,\n-        fusion_process_dump_.get(), thread_pool_, symbolic_expr_context_,\n+        fusion_process_dump_.get(), thread_pool_, mlir_context_,\n         fusion_analysis_cache_, fusion_deduplication_cache,\n         triton_heroless_fusion_enabled);\n "
        },
        {
            "sha": "df6d50234c84bb8bb98f5f2df93892df292aa45c",
            "filename": "third_party/xla/xla/service/gpu/transforms/priority_fusion.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -22,7 +22,7 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -44,12 +44,12 @@ class PriorityFusion : public HloModulePass {\n   PriorityFusion(tsl::thread::ThreadPool* thread_pool,\n                  const se::DeviceDescription& device,\n                  GpuHloCostAnalysis::Options cost_analysis_options,\n-                 SymbolicExprContext* symbolic_expr_context)\n+                 mlir::MLIRContext* mlir_context)\n       : thread_pool_(thread_pool),\n         device_info_(device),\n         cost_analysis_options_(std::move(cost_analysis_options)),\n         fusion_analysis_cache_(device_info_),\n-        symbolic_expr_context_(symbolic_expr_context) {}\n+        mlir_context_(mlir_context) {}\n \n   absl::string_view name() const override { return \"priority-fusion\"; }\n \n@@ -85,7 +85,7 @@ class PriorityFusion : public HloModulePass {\n \n   HloFusionAnalysisCache fusion_analysis_cache_;\n \n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "fd559471f03ed472f662463fd0a2f41d8eca90d3",
            "filename": "third_party/xla/xla/service/gpu/transforms/priority_fusion_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fpriority_fusion_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -76,11 +76,10 @@ class PriorityFusionTest : public HloHardwareIndependentTestBase {\n \n   se::DeviceDescription device_info_ = TestGpuDeviceInfo::RTXA6000DeviceInfo();\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n   PriorityFusion priority_fusion_{\n       /*thread_pool=*/nullptr, device_info_,\n       GpuHloCostAnalysis::Options{.count_multiple_input_accesses = true},\n-      &symbolic_expr_context_};\n+      &mlir_context_};\n };\n \n TEST_F(PriorityFusionTest, FuseWithSharedArgument) {\n@@ -1379,7 +1378,7 @@ TEST_F(PriorityFusionWithTritonEnabledTest,\n   GpuHloCostAnalysis::Options options;\n   options.count_multiple_input_accesses = true;\n   PriorityFusion priority_fusion_with_thread_pool{\n-      /*thread_pool=*/&pool, device_info_, options, &symbolic_expr_context_};\n+      /*thread_pool=*/&pool, device_info_, options, &mlir_context_};\n   EXPECT_THAT(priority_fusion_with_thread_pool.Run(module.get()),\n               absl_testing::IsOkAndHolds(true));\n   HloInstruction* root = module->entry_computation()->root_instruction();"
        },
        {
            "sha": "517fe9179c617cab168ad5221ad5747f350eba33",
            "filename": "third_party/xla/xla/service/gpu/transforms/softmax_rewriter_triton.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 26,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -70,6 +70,8 @@ namespace xla {\n namespace gpu {\n namespace {\n \n+using ::mlir::MLIRContext;\n+\n using hlo_query::IsBroadcastOfParameter;\n using hlo_query::IsBroadcastOfScalarConstant;\n \n@@ -267,7 +269,7 @@ absl::StatusOr<HloFusionInstruction*> MakeFusionForDiamond(\n absl::Status RunFusionPipeline(\n     HloModule* module, const se::DeviceDescription& device_info,\n     const HloCostAnalysis::ShapeSizeFunction& shape_size,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   HloPassPipeline reduction_pipeline(\"reduction_pipeline\");\n   // Passes that run after SoftmaxRewriterTriton and before PriorityFusion and\n   // transform reductions.\n@@ -280,8 +282,7 @@ absl::Status RunFusionPipeline(\n   TF_RETURN_IF_ERROR(reduction_pipeline.Run(module).status());\n \n   return FusionPipeline(module->config().debug_options(), shape_size,\n-                        /*thread_pool=*/nullptr, device_info,\n-                        symbolic_expr_context)\n+                        /*thread_pool=*/nullptr, device_info, mlir_context)\n       .Run(module)\n       .status();\n }\n@@ -300,14 +301,14 @@ EstimateOptimizedHloRunTimeWithoutSoftMaxRewriterTriton(\n     const HloFusionInstruction* fusion,\n     const se::DeviceDescription& device_info,\n     const HloCostAnalysis::ShapeSizeFunction& shape_size,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    MLIRContext* mlir_context) {\n   auto new_module = ExtractComputationIntoNewModule(\n       *fusion->fused_instructions_computation());\n \n   // After this call, the `new_module` will have instruction fused without\n   // SoftmaxRewriterTriton.\n   TF_RETURN_IF_ERROR(RunFusionPipeline(new_module.get(), device_info,\n-                                       shape_size, symbolic_expr_context));\n+                                       shape_size, mlir_context));\n \n   VLOG(3) << \"priority fusion module: \" << new_module->ToString();\n \n@@ -322,8 +323,7 @@ EstimateOptimizedHloRunTimeWithoutSoftMaxRewriterTriton(\n \n   absl::Duration total_run_time = absl::ZeroDuration();\n \n-  GpuPerformanceModelOwning gpu_performance_model(device_info,\n-                                                  symbolic_expr_context);\n+  GpuPerformanceModelOwning gpu_performance_model(device_info, mlir_context);\n   for (const HloInstruction* instr : entry_computation->instructions()) {\n     total_run_time += gpu_performance_model.Get()\n                           .EstimateRunTimeForInstruction(instr, &cost_analysis)\n@@ -348,8 +348,7 @@ DecideIfShouldFuseAndMaybeSetBlockLevelParameters(\n     GpuPerformanceModelWithIndexingAnalysis& indexing_performance_model,\n     const se::DeviceDescription& device_info,\n     const HloCostAnalysis::ShapeSizeFunction& shape_size,\n-    SymbolicExprContext* symbolic_expr_context,\n-    bool use_cost_model_to_evaluate_fusions) {\n+    MLIRContext* mlir_context, bool use_cost_model_to_evaluate_fusions) {\n   auto fusion_adaptor = HloFusionAdaptor::ForInstruction(normalization_fusion);\n \n   TF_ASSIGN_OR_RETURN(\n@@ -366,10 +365,10 @@ DecideIfShouldFuseAndMaybeSetBlockLevelParameters(\n       std::get<TiledRunTimeData>(std::move(tiled_runtime_data_or));\n \n   if (use_cost_model_to_evaluate_fusions) {\n-    TF_ASSIGN_OR_RETURN(absl::Duration run_time_without_softmax_rewriter,\n-                        EstimateOptimizedHloRunTimeWithoutSoftMaxRewriterTriton(\n-                            normalization_fusion, device_info, shape_size,\n-                            symbolic_expr_context));\n+    TF_ASSIGN_OR_RETURN(\n+        absl::Duration run_time_without_softmax_rewriter,\n+        EstimateOptimizedHloRunTimeWithoutSoftMaxRewriterTriton(\n+            normalization_fusion, device_info, shape_size, mlir_context));\n \n     VLOG(2) << \"run time estimate if normalization diamond fused together: \"\n             << tiled_runtime_data.runtime_data.exec_time;\n@@ -401,19 +400,18 @@ absl::StatusOr<bool> MaybeFuseDiamondImpl(\n     GpuPerformanceModelWithIndexingAnalysis& indexing_performance_model,\n     const se::DeviceDescription& device_info,\n     const HloCostAnalysis::ShapeSizeFunction& shape_size,\n-    SymbolicExprContext* symbolic_expr_context,\n-    bool use_cost_model_to_evaluate_fusions) {\n+    MLIRContext* mlir_context, bool use_cost_model_to_evaluate_fusions) {\n   TF_ASSIGN_OR_RETURN(HloFusionInstruction * normalization_fusion,\n                       MakeFusionForDiamond(diamond));\n   HloInstruction* root = diamond.root;\n \n   VLOG(2) << \"MaybeFuseDiamondImpl: \" << normalization_fusion->ToString();\n \n-  TF_ASSIGN_OR_RETURN(FusionDecision fusion_decision,\n-                      DecideIfShouldFuseAndMaybeSetBlockLevelParameters(\n-                          normalization_fusion, indexing_performance_model,\n-                          device_info, shape_size, symbolic_expr_context,\n-                          use_cost_model_to_evaluate_fusions));\n+  TF_ASSIGN_OR_RETURN(\n+      FusionDecision fusion_decision,\n+      DecideIfShouldFuseAndMaybeSetBlockLevelParameters(\n+          normalization_fusion, indexing_performance_model, device_info,\n+          shape_size, mlir_context, use_cost_model_to_evaluate_fusions));\n \n   if (!fusion_decision.CanFuse()) {\n     VLOG(2) << \"Not fusing: \" << fusion_decision.Explain();\n@@ -441,11 +439,10 @@ absl::StatusOr<bool> CanSymbolicTileAnalysisTileDiamond(\n     const se::DeviceDescription& device_info) {\n   TF_ASSIGN_OR_RETURN(HloFusionInstruction * normalization_fusion,\n                       MakeFusionForDiamond(diamond));\n-  mlir::MLIRContext context;\n-  SymbolicExprContext symbolic_expr_context(&context);\n+  mlir::MLIRContext mlir_context;\n   SymbolicTileAnalysisOrError symbolic_tile_analysis_or_error =\n       SymbolicTileAnalysis::AnalyzeComputation(\n-          *normalization_fusion->called_computation(), &symbolic_expr_context,\n+          *normalization_fusion->called_computation(), &mlir_context,\n           TritonEmitterConstraints::GetBuilder(device_info));\n \n   bool can_tile = std::holds_alternative<SymbolicTileAnalysis>(\n@@ -639,11 +636,10 @@ absl::StatusOr<bool> SoftmaxRewriterTriton::MaybeFuseNormalizationDiamond(\n     const DiamondDescriptor& diamond) {\n   HloFusionAnalysisCache fusion_analysis_cache(device_info_);\n   GpuPerformanceModelWithIndexingAnalysis indexing_performance_model(\n-      &device_info_, &fusion_analysis_cache, shape_size_,\n-      symbolic_expr_context_);\n+      &device_info_, &fusion_analysis_cache, shape_size_, mlir_context_);\n \n   return MaybeFuseDiamondImpl(diamond, indexing_performance_model, device_info_,\n-                              shape_size_, symbolic_expr_context_,\n+                              shape_size_, mlir_context_,\n                               use_cost_model_to_evaluate_fusions_);\n }\n "
        },
        {
            "sha": "efead2c4059611a3ff69417586b8c84cbd374d7d",
            "filename": "third_party/xla/xla/service/gpu/transforms/softmax_rewriter_triton.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -51,12 +51,12 @@ class SoftmaxRewriterTriton : public HloModulePass {\n  public:\n   explicit SoftmaxRewriterTriton(const se::DeviceDescription& device_info,\n                                  HloCostAnalysis::ShapeSizeFunction shape_size,\n-                                 SymbolicExprContext* symbolic_expr_context,\n+                                 mlir::MLIRContext* mlir_context,\n                                  bool only_fuse_if_profitable = false)\n       : device_info_(device_info),\n         shape_size_(shape_size),\n         use_cost_model_to_evaluate_fusions_(only_fuse_if_profitable),\n-        symbolic_expr_context_(symbolic_expr_context) {}\n+        mlir_context_(mlir_context) {}\n \n   absl::string_view name() const override { return \"triton-softmax-rewriter\"; }\n \n@@ -106,7 +106,7 @@ class SoftmaxRewriterTriton : public HloModulePass {\n   const se::DeviceDescription& device_info_;\n   const HloCostAnalysis::ShapeSizeFunction shape_size_;\n   bool use_cost_model_to_evaluate_fusions_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "02928b64289165724342725c357d646f2e96ddd0",
            "filename": "third_party/xla/xla/service/gpu/transforms/softmax_rewriter_triton_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsoftmax_rewriter_triton_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -64,9 +64,8 @@ class SoftmaxRewriterTritonTest\n  protected:\n   se::DeviceDescription device_info_{TestGpuDeviceInfo::RTXA6000DeviceInfo()};\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n   SoftmaxRewriterTriton fusion_rewriter_{\n-      device_info_, HloCostAnalysis::DefaultShapeSize, &symbolic_expr_context_};\n+      device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_};\n };\n \n TEST_F(SoftmaxRewriterTritonTest, CanFuseSingleNormalizationF32) {\n@@ -566,7 +565,7 @@ ENTRY main {\n       SoftmaxRewriterTriton(\n           TestGpuDeviceInfo::RTXA6000DeviceInfo(\n               se::CudaComputeCapability{se::CudaComputeCapability::kVolta, 0}),\n-          HloCostAnalysis::DefaultShapeSize, &symbolic_expr_context_)\n+          HloCostAnalysis::DefaultShapeSize, &mlir_context_)\n           .Run(module.get()),\n       absl_testing::StatusIs(\n           tsl::error::FAILED_PRECONDITION,\n@@ -595,7 +594,7 @@ ENTRY main {\n \n   EXPECT_TRUE(SoftmaxRewriterTriton(TestGpuDeviceInfo::AMDMI210DeviceInfo(),\n                                     HloCostAnalysis::DefaultShapeSize,\n-                                    &symbolic_expr_context_)\n+                                    &mlir_context_)\n                   .Run(module.get())\n                   .ok());\n }\n@@ -681,7 +680,7 @@ ENTRY main {\n )\";\n   auto module = ParseAndReturnVerifiedModule(hlo_string).value();\n   SoftmaxRewriterTriton fusion_rewriter(\n-      device_info_, HloCostAnalysis::DefaultShapeSize, &symbolic_expr_context_);\n+      device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_);\n   EXPECT_FALSE(fusion_rewriter_.Run(module.get()).value());\n }\n \n@@ -828,7 +827,7 @@ ENTRY main {\n \n   auto module = ParseAndReturnVerifiedModule(hlo_string).value();\n   SoftmaxRewriterTriton softmax_rewriter_triton(\n-      device_info_, HloCostAnalysis::DefaultShapeSize, &symbolic_expr_context_);\n+      device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_);\n   int unmatched = 0, matched = 0;\n   for (HloInstruction* instruction :\n        module->entry_computation()->MakeInstructionPostOrder()) {\n@@ -1083,8 +1082,7 @@ ENTRY main {\n     // Verify that SoftmaxRewriterTriton without Cost Model will fuse the\n     // normalization diamond.\n     SoftmaxRewriterTriton fusion_rewriter_without_cost_model{\n-        device_info_, HloCostAnalysis::DefaultShapeSize,\n-        &symbolic_expr_context_,\n+        device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_,\n         /*only_fuse_if_profitable=*/false};\n \n     auto module = ParseAndReturnVerifiedModule(hlo_string).value();\n@@ -1099,8 +1097,7 @@ ENTRY main {\n     // SoftmaxRewriterTriton with Cost Model will discard the normalization\n     // diamond, because row size is too large.\n     SoftmaxRewriterTriton fusion_rewriter_with_cost_model{\n-        device_info_, HloCostAnalysis::DefaultShapeSize,\n-        &symbolic_expr_context_,\n+        device_info_, HloCostAnalysis::DefaultShapeSize, &mlir_context_,\n         /*only_fuse_if_profitable=*/true};\n \n     auto module = ParseAndReturnVerifiedModule(hlo_string).value();"
        },
        {
            "sha": "774bd1185fd1842d52a101dff11ac8d1695e8371",
            "filename": "third_party/xla/xla/service/gpu/transforms/triton_fusion_numerics_verifier.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -64,9 +64,10 @@ limitations under the License.\n #include \"xla/xla.pb.h\"\n \n namespace xla::gpu {\n-\n namespace {\n \n+using ::mlir::MLIRContext;\n+\n using ProfilingOutput = AutotunerCompileUtil::ProfilingOutput;\n \n // Returns the input instruction as a fusion instruction, if it represents a\n@@ -125,8 +126,7 @@ absl::Status InlineModuleFusions(HloModule* hlo_module) {\n // days instead of milliseconds).\n absl::StatusOr<std::unique_ptr<HloModule>> NewHloModuleFromFusionComputation(\n     const HloFusionInstruction& fusion, const DebugOptions& debug_opts,\n-    const se::DeviceDescription& gpu_device_info,\n-    SymbolicExprContext* symbolic_expr_context) {\n+    const se::DeviceDescription& gpu_device_info, MLIRContext* mlir_context) {\n   std::unique_ptr<HloModule> new_module =\n       ExtractComputationIntoNewModule(*fusion.fused_instructions_computation());\n   new_module->mutable_config().set_debug_options(debug_opts);\n@@ -150,7 +150,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> NewHloModuleFromFusionComputation(\n           .status());\n   PriorityFusion fusion_pass(\n       /*thread_pool=*/nullptr, gpu_device_info, HloCostAnalysis::Options{},\n-      symbolic_expr_context);\n+      mlir_context);\n   TF_RETURN_IF_ERROR(fusion_pass.Run(new_module.get()).status());\n \n   // If the priority fusion pass above skipped some instructions, turn them\n@@ -176,13 +176,13 @@ namespace triton_fusion_numerics_pass_internal {\n absl::StatusOr<ScopedShapedBuffer> CompileAndRunFusion(\n     AutotunerCompileUtil& util, const HloFusionInstruction& fusion,\n     const DeviceOrDevicelessConfig& config, const DebugOptions& debug_opts,\n-    bool disable_triton, SymbolicExprContext* symbolic_expr_context) {\n+    bool disable_triton, MLIRContext* mlir_context) {\n   TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<Executable> executable,\n       util.Compile([&](const DebugOptions& opts) {\n         return disable_triton ? NewHloModuleFromFusionComputation(\n                                     fusion, opts, config.GetDeviceDescription(),\n-                                    symbolic_expr_context)\n+                                    mlir_context)\n                               : NewHloModuleWithTritonFromFusion(fusion, opts);\n       }));\n   if (executable == nullptr) {\n@@ -254,15 +254,15 @@ absl::Status VerifyTritonFusion(AutotunerCompileUtil& util,\n                                 const HloFusionInstruction& fusion,\n                                 const DeviceOrDevicelessConfig& config,\n                                 const DebugOptions& debug_opts,\n-                                SymbolicExprContext* symbolic_expr_context) {\n+                                MLIRContext* mlir_context) {\n   TF_ASSIGN_OR_RETURN(auto triton_result,\n                       triton_fusion_numerics_pass_internal::CompileAndRunFusion(\n                           util, fusion, config, debug_opts,\n-                          /*disable_triton=*/false, symbolic_expr_context));\n+                          /*disable_triton=*/false, mlir_context));\n   TF_ASSIGN_OR_RETURN(auto emitters_result,\n                       triton_fusion_numerics_pass_internal::CompileAndRunFusion(\n                           util, fusion, config, debug_opts,\n-                          /*disable_triton=*/true, symbolic_expr_context));\n+                          /*disable_triton=*/true, mlir_context));\n \n   TF_ASSIGN_OR_RETURN(auto stream, config.GetStream());\n   auto status = triton_fusion_numerics_pass_internal::CompareBuffers(\n@@ -325,7 +325,7 @@ absl::StatusOr<bool> TritonFusionNumericsVerifier::RunImpl(\n           return it->second;\n         }\n         auto result = VerifyTritonFusion(compile_util, fusion, config_,\n-                                         debug_options, symbolic_expr_context_);\n+                                         debug_options, mlir_context_);\n         fusion_result_cache_[key] = result;\n         return result;\n       }));"
        },
        {
            "sha": "5f7101cb05ae9eab95582ae57b4aea7fb3a38695",
            "filename": "third_party/xla/xla/service/gpu/transforms/triton_fusion_numerics_verifier.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -43,8 +43,8 @@ namespace xla::gpu {\n class TritonFusionNumericsVerifier : public HloModulePass {\n  public:\n   TritonFusionNumericsVerifier(const DeviceOrDevicelessConfig& config,\n-                               SymbolicExprContext* symbolic_expr_context)\n-      : config_(config), symbolic_expr_context_(symbolic_expr_context) {}\n+                               mlir::MLIRContext* mlir_context)\n+      : config_(config), mlir_context_(mlir_context) {}\n \n   static absl::string_view Name() { return \"triton-numerics-verifier\"; }\n   absl::string_view name() const override { return Name(); }\n@@ -60,7 +60,7 @@ class TritonFusionNumericsVerifier : public HloModulePass {\n \n  private:\n   DeviceOrDevicelessConfig config_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n \n   // In some models there are many identical fusions. These are cached to avoid\n   // expensive recomputations.\n@@ -73,7 +73,7 @@ namespace triton_fusion_numerics_pass_internal {\n absl::StatusOr<ScopedShapedBuffer> CompileAndRunFusion(\n     AutotunerCompileUtil& util, const HloFusionInstruction& fusion,\n     const DeviceOrDevicelessConfig& config, const DebugOptions& debug_opts,\n-    bool disable_triton, SymbolicExprContext* symbolic_expr_context);\n+    bool disable_triton, mlir::MLIRContext* mlir_context);\n absl::Status CompareBuffers(const ScopedShapedBuffer& current,\n                             const ScopedShapedBuffer& expected,\n                             const Shape& shape, const DebugOptions& debug_opts,"
        },
        {
            "sha": "51a96ed64cb9e19a6609c072a0de1eea57cb7479",
            "filename": "third_party/xla/xla/service/gpu/transforms/triton_fusion_numerics_verifier_test.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ftriton_fusion_numerics_verifier_test.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -41,6 +41,8 @@ limitations under the License.\n namespace xla::gpu {\n namespace {\n \n+using ::mlir::MLIRContext;\n+\n class TritonFusionNumericsVerifierTest\n     : public HloPjRtTestBase,\n       public ::testing::WithParamInterface<PrimitiveType> {\n@@ -89,8 +91,7 @@ class TritonFusionNumericsVerifierTest\n     return std::move(compile_util_or).value();\n   }\n \n-  mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n+  MLIRContext mlir_context_;\n };\n \n constexpr absl::string_view kSoftmaxHlo = R\"(\n@@ -136,7 +137,7 @@ TEST_P(TritonFusionNumericsVerifierTest, VerifyExactSoftmaxFusionNumerics) {\n \n   EXPECT_NE(TritonFusion(*module), nullptr);\n   auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n-                                               &symbolic_expr_context_);\n+                                               &mlir_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n }\n \n@@ -191,7 +192,7 @@ ENTRY entry {\n \n   EXPECT_NE(TritonFusion(*module), nullptr);\n   auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n-                                               &symbolic_expr_context_);\n+                                               &mlir_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n }\n \n@@ -222,7 +223,7 @@ ENTRY main{\n \n   EXPECT_NE(TritonFusion(*module), nullptr);\n   auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n-                                               &symbolic_expr_context_);\n+                                               &mlir_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n }\n \n@@ -312,7 +313,7 @@ ENTRY main (p0: bf16[128,512], p1: bf16[256,512], p2: bf16[512,512]) -> bf16[384\n \n   EXPECT_NE(TritonFusion(*module), nullptr);\n   auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n-                                               &symbolic_expr_context_);\n+                                               &mlir_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n }\n \n@@ -344,12 +345,12 @@ TEST_F(TritonFusionNumericsVerifierTest, CheckMismatch) {\n \n   auto f64_result = triton_fusion_numerics_pass_internal::CompileAndRunFusion(\n       compile_util, *fusion_f64, autotune_config, debug_options,\n-      /*disable_triton=*/false, &symbolic_expr_context_);\n+      /*disable_triton=*/false, &mlir_context_);\n   TF_EXPECT_OK(f64_result);\n \n   auto f32_result = triton_fusion_numerics_pass_internal::CompileAndRunFusion(\n       compile_util, *fusion_f32, autotune_config, debug_options,\n-      /*disable_triton=*/false, &symbolic_expr_context_);\n+      /*disable_triton=*/false, &mlir_context_);\n   TF_EXPECT_OK(f32_result);\n \n   auto stream = autotune_config.GetStream();\n@@ -399,7 +400,7 @@ ENTRY main {\n                        \"\");\n \n   auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n-                                               &symbolic_expr_context_);\n+                                               &mlir_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n   auto fusion = TritonFusion(*module);\n   EXPECT_NE(fusion, nullptr);\n@@ -410,7 +411,7 @@ ENTRY main {\n   auto compilation_result =\n       triton_fusion_numerics_pass_internal::CompileAndRunFusion(\n           compile_util, *fusion, autotune_config, GetDebugOptionsForTest(),\n-          /*disable_triton=*/false, &symbolic_expr_context_);\n+          /*disable_triton=*/false, &mlir_context_);\n \n   // Verify that the compilation with default flags fails. The compilation\n   // fails, because the kernel will spill registers, but the error is\n@@ -469,7 +470,7 @@ ENTRY main {\n \n   std::unique_ptr<HloModule> module = Module(hlo_text, \"\");\n   auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n-                                               &symbolic_expr_context_);\n+                                               &mlir_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n   EXPECT_EQ(verifier.CacheHitsForTestingOnly(), 1);\n }\n@@ -524,7 +525,7 @@ ENTRY main {\n   auto module = Module(hlo_text, \"\");\n   EXPECT_NE(TritonFusion(*module), nullptr);\n   auto verifier = TritonFusionNumericsVerifier(CreateDeviceOrDevicelessConfig(),\n-                                               &symbolic_expr_context_);\n+                                               &mlir_context_);\n   TF_EXPECT_OK(verifier.Run(module.get(), /*execution_threads=*/{}));\n }\n "
        },
        {
            "sha": "fdeff8f5dbdaaa91f08e4f520f8f43d8c919ce8b",
            "filename": "third_party/xla/xla/tests/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2FBUILD?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -417,7 +417,7 @@ cc_library(\n     srcs = [\"codegen_test_base.cc\"],\n     hdrs = [\"codegen_test_base.h\"],\n     deps = [\n-        \":hlo_test_base_with_symbolic_expr_context\",\n+        \":hlo_test_base_with_mlir_context\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:compiler\",\n         \"//xla/service:executable\",\n@@ -3891,9 +3891,9 @@ cc_library(\n )\n \n cc_library(\n-    name = \"hlo_test_base_with_symbolic_expr_context\",\n+    name = \"hlo_test_base_with_mlir_context\",\n     testonly = True,\n-    hdrs = [\"hlo_test_base_with_symbolic_expr_context.h\"],\n+    hdrs = [\"hlo_test_base_with_mlir_context.h\"],\n     deps = [\n         \":hlo_test_base\",\n         \"//xla/hlo/analysis:symbolic_expr\","
        },
        {
            "sha": "d4f4ff7d5ad441616388cfc0c4fb63a313e1d21b",
            "filename": "third_party/xla/xla/tests/codegen_test_base.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Ftests%2Fcodegen_test_base.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Ftests%2Fcodegen_test_base.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcodegen_test_base.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -21,12 +21,12 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/service/executable.h\"\n-#include \"xla/tests/hlo_test_base_with_symbolic_expr_context.h\"\n+#include \"xla/tests/hlo_test_base_with_mlir_context.h\"\n \n namespace xla {\n \n // Provides access to both the JIT and the AOT compiler for testing.\n-class CodegenTestBase : public HloTestBaseWithSymbolicExprContext {\n+class CodegenTestBase : public HloTestBaseWithMLIRContext {\n  protected:\n   // Compiles hlo_module with the JIT compiler.\n   absl::StatusOr<std::unique_ptr<Executable>> CompileToExecutable("
        },
        {
            "sha": "516aec73ef38e404b92344ea06eacefe2e70b3d4",
            "filename": "third_party/xla/xla/tests/hlo_test_base_with_mlir_context.h",
            "status": "renamed",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Ftests%2Fhlo_test_base_with_mlir_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Ftests%2Fhlo_test_base_with_mlir_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fhlo_test_base_with_mlir_context.h?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -13,26 +13,23 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#ifndef XLA_TESTS_HLO_TEST_BASE_WITH_SYMBOLIC_EXPR_CONTEXT_H_\n-#define XLA_TESTS_HLO_TEST_BASE_WITH_SYMBOLIC_EXPR_CONTEXT_H_\n+#ifndef XLA_TESTS_HLO_TEST_BASE_WITH_MLIR_CONTEXT_H_\n+#define XLA_TESTS_HLO_TEST_BASE_WITH_MLIR_CONTEXT_H_\n \n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/tests/hlo_test_base.h\"\n \n namespace xla {\n \n-class HloTestBaseWithSymbolicExprContext : public HloTestBase {\n+class HloTestBaseWithMLIRContext : public HloTestBase {\n  public:\n-  SymbolicExprContext* symbolic_expr_context() {\n-    return &symbolic_expr_context_;\n-  }\n+  mlir::MLIRContext* mlir_context() { return &mlir_context_; }\n \n  private:\n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n };\n \n }  // namespace xla\n \n-#endif  // XLA_TESTS_HLO_TEST_BASE_WITH_SYMBOLIC_EXPR_CONTEXT_H_\n+#endif  // XLA_TESTS_HLO_TEST_BASE_WITH_MLIR_CONTEXT_H_",
            "previous_filename": "third_party/xla/xla/tests/hlo_test_base_with_symbolic_expr_context.h"
        },
        {
            "sha": "1daea09f0824f043ae85f59a41275d06fd17fd5b",
            "filename": "third_party/xla/xla/tools/hlo_opt/gpu_opt.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_opt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_opt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_opt.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -211,12 +211,11 @@ class GpuOptProvider : public CompiledOptProvider {\n     std::unique_ptr<gpu::GpuAliasInfo> alias_info =\n         gpu_compiler->GetAliasInfo(device_description);\n     if (!optimized_module->has_schedule()) {\n-      TF_ASSIGN_OR_RETURN(\n-          gpu::ScheduleMetadata schedule_metadata,\n-          gpu::ScheduleGpuModule(\n-              optimized_module, gpu_compiler->GetPointerSize(),\n-              device_description, gpu_compiler->symbolic_expr_context(),\n-              alias_info.get()));\n+      TF_ASSIGN_OR_RETURN(gpu::ScheduleMetadata schedule_metadata,\n+                          gpu::ScheduleGpuModule(\n+                              optimized_module, gpu_compiler->GetPointerSize(),\n+                              device_description, gpu_compiler->mlir_context(),\n+                              alias_info.get()));\n       TF_RETURN_IF_ERROR(gpu_compiler->RunPostSchedulingPipelines(\n           optimized_module, schedule_metadata.scheduler_mem_limit,\n           device_description, alias_info.get()));"
        },
        {
            "sha": "ae0a38153582aeab39872d85360a5cc0ae267348",
            "filename": "third_party/xla/xla/tools/print_indexing.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Ftools%2Fprint_indexing.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b72af58a6b6492d3e45b8a762188ff067a9a1498/third_party%2Fxla%2Fxla%2Ftools%2Fprint_indexing.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fprint_indexing.cc?ref=b72af58a6b6492d3e45b8a762188ff067a9a1498",
            "patch": "@@ -28,7 +28,6 @@ limitations under the License.\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/hlo/analysis/indexing_analysis.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/tools/hlo_module_loader.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -52,7 +51,6 @@ absl::Status Run(const std::string& filename, int operand_id, int output_id) {\n     get_operand_id = 0;\n   }\n   mlir::MLIRContext mlir_context;\n-  SymbolicExprContext symbolic_expr_context(&mlir_context);\n   VLOG(1) << \"module:\\n\" << module->ToString() << std::endl;\n   LOG(INFO) << \"root instruction is: \" << root->ToString() << std::endl;\n   VLOG(1) << \"root is tuple: \" << root->shape().IsTuple();\n@@ -76,7 +74,7 @@ absl::Status Run(const std::string& filename, int operand_id, int output_id) {\n \n   for (int out_id : output_ids) {\n     HloInstructionIndexing indexing =\n-        ComputeOutputToInputIndexing(root, out_id, &symbolic_expr_context);\n+        ComputeOutputToInputIndexing(root, out_id, &mlir_context);\n     LOG(INFO) << absl::StrFormat(\"output id %d has %d indexing maps\", out_id,\n                                  indexing.indexing_maps.size());\n     if (indexing.indexing_maps.empty()) {"
        }
    ],
    "stats": {
        "total": 3395,
        "additions": 1443,
        "deletions": 1952
    }
}