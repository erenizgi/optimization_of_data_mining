{
    "author": "unknown",
    "message": "[XLA:GPU] Fix BuffersChecksumThunk breaking on multi-GPU setups\n\nBug discovered when investigating NaN checker slowdown. Same issue\napplies to BuffersChecksumThunk and BuffersFloatCheckThunk.\n\nPiperOrigin-RevId: 830456321",
    "sha": "f4382ba1f8bc9f268230ff85022ca4986a4e42cc",
    "files": [
        {
            "sha": "cbc176031ec6e6cd6edd2aa9840a7fd17188bc01",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f4382ba1f8bc9f268230ff85022ca4986a4e42cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f4382ba1f8bc9f268230ff85022ca4986a4e42cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=f4382ba1f8bc9f268230ff85022ca4986a4e42cc",
            "patch": "@@ -3139,16 +3139,23 @@ cc_library(\n         \"//xla/stream_executor/gpu:gpu_kernel_registry\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/synchronization\",\n     ],\n )\n \n xla_test(\n     name = \"buffers_checksum_thunk_test\",\n     srcs = [\"buffers_checksum_thunk_test.cc\"],\n+    backend_tags = {\n+        \"gpu\": [\n+            \"multi_gpu\",\n+        ],\n+    },\n     backends = [\"gpu\"],\n     tags = [\n         \"cuda-only\",\n@@ -3164,7 +3171,10 @@ xla_test(\n         \"//xla/service:executable\",\n         \"//xla/service/gpu:buffer_allocations\",\n         \"//xla/service/gpu:resource_requests\",\n+        \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:mock_stream\",\n+        \"//xla/stream_executor:mock_stream_executor\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream\",\n@@ -3173,6 +3183,7 @@ xla_test(\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n )"
        },
        {
            "sha": "e10f21dfb9007fdaffca1d88dc3c85a49902be70",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_checksum_thunk.cc",
            "status": "modified",
            "additions": 38,
            "deletions": 14,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f4382ba1f8bc9f268230ff85022ca4986a4e42cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f4382ba1f8bc9f268230ff85022ca4986a4e42cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk.cc?ref=f4382ba1f8bc9f268230ff85022ca4986a4e42cc",
            "patch": "@@ -16,11 +16,14 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/buffers_checksum_thunk.h\"\n \n #include <cstdint>\n+#include <memory>\n #include <string>\n+#include <utility>\n \n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n+#include \"absl/synchronization/mutex.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_entry_metadata_store.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n@@ -55,27 +58,48 @@ absl::Status BuffersDebugChecksumThunk::Initialize(\n     return absl::OkStatus();\n   }\n \n-  se::gpu::GpuKernelRegistry registry =\n-      se::gpu::GpuKernelRegistry::GetGlobalRegistry();\n-  TF_ASSIGN_OR_RETURN(\n-      kernel_, registry.LoadKernel<se::gpu::BufferDebugXorChecksumKernel>(\n-                   params.executor));\n-\n-  VLOG(1) << \"Checksum kernel loaded\";\n+  {\n+    absl::MutexLock lock(kernels_mutex_);\n+    if (!kernels_.contains(params.executor)) {\n+      se::gpu::GpuKernelRegistry registry =\n+          se::gpu::GpuKernelRegistry::GetGlobalRegistry();\n+      TF_ASSIGN_OR_RETURN(\n+          auto kernel,\n+          registry.LoadKernel<se::gpu::BufferDebugXorChecksumKernel>(\n+              params.executor));\n+      kernels_[params.executor] =\n+          std::make_unique<se::gpu::BufferDebugXorChecksumKernel::KernelType>(\n+              std::move(kernel));\n+      VLOG(1) << \"Checksum kernel loaded on device \"\n+              << params.executor->device_ordinal()\n+              << \" (stream_executor: \" << params.executor\n+              << \"), kernel: \" << kernels_[params.executor].get();\n+    }\n+  }\n   return absl::OkStatus();\n }\n \n absl::Status BuffersDebugChecksumThunk::ExecuteOnStream(\n     const ExecuteParams& params) {\n   se::StreamExecutor* executor = params.stream->parent();\n-  if (!kernel_.has_value()) {\n-    // Initialize didn't load the kernel. This can happen when we're running on\n-    // an unsupported platform.\n-    VLOG(1) << \"Checksum kernel not loaded, skipping\";\n-    return absl::OkStatus();\n+\n+  se::gpu::BufferDebugXorChecksumKernel::KernelType* kernel = nullptr;\n+  {\n+    absl::MutexLock lock(kernels_mutex_);\n+    auto kernel_it = kernels_.find(executor);\n+    if (kernel_it == kernels_.end()) {\n+      // Initialize didn't load the kernel. This can happen when we're running\n+      // on an unsupported platform.\n+      VLOG(1) << \"Checksum kernel not loaded on device \"\n+              << executor->device_ordinal() << \", skipping\";\n+      return absl::OkStatus();\n+    }\n+    kernel = kernel_it->second.get();\n   }\n \n-  VLOG(1) << \"BuffersDebugChecksumThunk::ExecuteOnStream\";\n+  VLOG(1) << \"BuffersDebugChecksumThunk::ExecuteOnStream, device \"\n+          << executor->device_ordinal() << \" (stream_executor: \" << executor\n+          << \"), kernel: \" << kernel;\n   const uint32_t execution_id = execution_count_.fetch_add(1);\n \n   const se::ThreadDim thread_dim(\n@@ -100,7 +124,7 @@ absl::Status BuffersDebugChecksumThunk::ExecuteOnStream(\n     se::DeviceMemory<uint8_t> device_buffer(\n         params.buffer_allocations->GetDeviceAddress(buffer));\n \n-    TF_RETURN_IF_ERROR(kernel_->Launch(\n+    TF_RETURN_IF_ERROR(kernel->Launch(\n         thread_dim, se::BlockDim(1, 1, 1), params.stream, log_entry_id,\n         device_buffer, device_buffer.size(), buffer_debug_log.GetDeviceHeader(),\n         buffer_debug_log.GetDeviceEntries<BufferDebugLogEntry>()));"
        },
        {
            "sha": "4baf4fb10d7efa73e8d6f591f88878212fd96676",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_checksum_thunk.h",
            "status": "modified",
            "additions": 19,
            "deletions": 6,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f4382ba1f8bc9f268230ff85022ca4986a4e42cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f4382ba1f8bc9f268230ff85022ca4986a4e42cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk.h?ref=f4382ba1f8bc9f268230ff85022ca4986a4e42cc",
            "patch": "@@ -19,17 +19,19 @@ limitations under the License.\n #include <atomic>\n #include <cstddef>\n #include <memory>\n-#include <optional>\n #include <string>\n #include <utility>\n \n+#include \"absl/base/thread_annotations.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/synchronization/mutex.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_entry_metadata_store.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk_id.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_xor_checksum_kernel.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n \n namespace xla::gpu {\n \n@@ -50,8 +52,10 @@ class BuffersDebugChecksumThunk : public Thunk {\n         checked_thunk_buffers_(std::move(checked_thunk_buffers)),\n         runs_before_checked_thunk_(runs_before_checked_thunk) {}\n \n-  absl::Status Initialize(const InitializeParams& params) override;\n-  absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n+  absl::Status Initialize(const InitializeParams& params) override\n+      ABSL_LOCKS_EXCLUDED(kernels_mutex_);\n+  absl::Status ExecuteOnStream(const ExecuteParams& params) override\n+      ABSL_LOCKS_EXCLUDED(kernels_mutex_);\n \n   std::string ToString(int indent) const override;\n \n@@ -78,9 +82,18 @@ class BuffersDebugChecksumThunk : public Thunk {\n   }\n \n  private:\n-  // Loaded in Initialize.\n-  std::optional<stream_executor::gpu::BufferDebugXorChecksumKernel::KernelType>\n-      kernel_;\n+  absl::Mutex kernels_mutex_;\n+  // Each loaded kernel is associated with a specific device (represented by its\n+  // StreamExecutor).\n+  //\n+  // ExecuteOnStream implementation requires pointer stability of values, hence\n+  // unique_ptr.\n+  absl::flat_hash_map<\n+      stream_executor::StreamExecutor*,\n+      std::unique_ptr<\n+          stream_executor::gpu::BufferDebugXorChecksumKernel::KernelType>>\n+      kernels_ ABSL_GUARDED_BY(kernels_mutex_);\n+\n   BufferAllocation::Slice log_slice_;\n   std::shared_ptr<BufferDebugLogEntryMetadataStore> metadata_store_;\n   ThunkId checked_thunk_id_;"
        },
        {
            "sha": "c313001124cca4c8e2f143b07e6a26a07881f250",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffers_checksum_thunk_test.cc",
            "status": "modified",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f4382ba1f8bc9f268230ff85022ca4986a4e42cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f4382ba1f8bc9f268230ff85022ca4986a4e42cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffers_checksum_thunk_test.cc?ref=f4382ba1f8bc9f268230ff85022ca4986a4e42cc",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_entry_metadata_store.h\"\n #include \"xla/backends/gpu/runtime/buffer_debug_log_structs.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n@@ -33,6 +34,7 @@ limitations under the License.\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/service/gpu/resource_requests.h\"\n #include \"xla/service/service_executable_run_options.h\"\n+#include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/buffer_debug_log.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -195,5 +197,73 @@ TEST_F(BuffersDebugChecksumThunkTest, CalculatesChecksums) {\n                         Field(&BufferDebugLogEntry::value, 56785678))));\n }\n \n+TEST_F(BuffersDebugChecksumThunkTest,\n+       ExecutesCorrectKernelsForDifferentDevices) {\n+  // Loaded kernels are associated with a specific device represented by its\n+  // StreamExecutor. The same Thunk will be Initialized once for each device,\n+  // which will load the kernel onto that device. During ExecuteOnStream, the\n+  // correct kernel needs to be launched.\n+  if (platform_->VisibleDeviceCount() < 2) {\n+    GTEST_SKIP() << \"need at least 2 devices for this test\";\n+  }\n+\n+  static constexpr size_t kLogSizeBytes = 1024;\n+  static constexpr size_t kInputSizeBytes = 1024;\n+\n+  struct TestDevice {\n+    se::StreamExecutor* executor;\n+    std::unique_ptr<se::Stream> stream;\n+    std::unique_ptr<se::StreamExecutorMemoryAllocator> allocator;\n+    BufferAllocations allocations;\n+  };\n+  auto setup_device = [this](int device_ordinal) -> absl::StatusOr<TestDevice> {\n+    TF_ASSIGN_OR_RETURN(se::StreamExecutor * executor,\n+                        platform_->ExecutorForDevice(device_ordinal));\n+    TF_ASSIGN_OR_RETURN(std::unique_ptr<se::Stream> stream,\n+                        executor->CreateStream());\n+    auto allocator =\n+        std::make_unique<se::StreamExecutorMemoryAllocator>(executor);\n+    BufferAllocations allocations(\n+        {executor->AllocateArray<uint8_t>(kLogSizeBytes + kInputSizeBytes)},\n+        executor->device_ordinal(), allocator.get());\n+\n+    return TestDevice{std::move(executor), std::move(stream),\n+                      std::move(allocator), std::move(allocations)};\n+  };\n+  TF_ASSERT_OK_AND_ASSIGN(TestDevice device0, setup_device(0));\n+  TF_ASSERT_OK_AND_ASSIGN(TestDevice device1, setup_device(1));\n+  BufferAllocation allocation(0, kLogSizeBytes + kInputSizeBytes, 0);\n+  BufferAllocation::Slice log_slice(&allocation, 0, kLogSizeBytes);\n+  BufferAllocation::Slice input_slice(&allocation, kLogSizeBytes,\n+                                      kInputSizeBytes);\n+  BuffersDebugChecksumThunk thunk(\n+      Thunk::ThunkInfo(), log_slice,\n+      /*checked_thunk_id=*/ThunkId(123), {{/*buffer_idx=*/0, input_slice}},\n+      /*runs_before_checked_thunk=*/true,\n+      std::make_shared<BufferDebugLogEntryMetadataStore>());\n+\n+  // Initialize the Thunk on both devices and run the kernel. An attempt to run\n+  // a kernel on the wrong device will fail with CUDA_ERROR_INVALID_HANDLE. The\n+  // error may be reported from the next operation on the stream, so assert on\n+  // BlockHostUntilDone as well.\n+  TF_ASSERT_OK(\n+      thunk.Initialize(Thunk::InitializeParams{/*executor=*/device0.executor}));\n+  TF_ASSERT_OK(thunk.ExecuteOnStream(Thunk::ExecuteParams::Create(\n+      ServiceExecutableRunOptions(), device0.allocations, device0.stream.get(),\n+      /*command_buffer_trace_stream=*/device0.stream.get(),\n+      /*collective_params=*/nullptr,\n+      /*collective_cliques=*/nullptr)));\n+  TF_ASSERT_OK(device0.stream->BlockHostUntilDone());\n+\n+  TF_ASSERT_OK(\n+      thunk.Initialize(Thunk::InitializeParams{/*executor=*/device1.executor}));\n+  TF_ASSERT_OK(thunk.ExecuteOnStream(Thunk::ExecuteParams::Create(\n+      ServiceExecutableRunOptions(), device1.allocations, device1.stream.get(),\n+      /*command_buffer_trace_stream=*/device1.stream.get(),\n+      /*collective_params=*/nullptr,\n+      /*collective_cliques=*/nullptr)));\n+  TF_ASSERT_OK(device1.stream->BlockHostUntilDone());\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 158,
        "additions": 138,
        "deletions": 20
    }
}