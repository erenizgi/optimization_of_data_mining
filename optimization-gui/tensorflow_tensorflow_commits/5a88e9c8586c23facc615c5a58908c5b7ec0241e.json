{
    "author": "loislo",
    "message": "[XLA:GPU] Fix the test and guard cuBLAS config in autotuner.\n\nIn `fusion_emitter_device_legacy_port_test.cc`, replace `MatchOptimizedHlo` with `RunFileCheck`. Otherwise we optimize the hlo twice. In `gemm_fusion_autotuner.cc`, add a check to ensure the cuBLAS reference config is only added when autotuning is enabled. Otherwise the default config for autotuning level 0 will be cublas even if the cublas fallback is disabled.\n\nPiperOrigin-RevId: 811297565",
    "sha": "5a88e9c8586c23facc615c5a58908c5b7ec0241e",
    "files": [
        {
            "sha": "f5e3bdebed5e91c5a985a3688fa91da742c85fc0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_port_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a88e9c8586c23facc615c5a58908c5b7ec0241e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a88e9c8586c23facc615c5a58908c5b7ec0241e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc?ref=5a88e9c8586c23facc615c5a58908c5b7ec0241e",
            "patch": "@@ -3448,12 +3448,12 @@ ENTRY e {\n \n   TF_ASSERT_OK_AND_ASSIGN(auto optimized_module,\n                           GetOptimizedModule(std::move(module)));\n-  MatchOptimizedHlo(optimized_module->ToString(), R\"(\n+  EXPECT_TRUE(*RunFileCheck(optimized_module->ToString(), R\"(\n     CHECK: fusion\n     CHECK: ROOT {{.*}} scaled-dot\n     CHECK: ENTRY\n     CHECK: __triton_nested_gemm_fusion\n-  )\");\n+  )\"));\n   EXPECT_TRUE(RunAndCompareNoHloPasses(\n       std::move(optimized_module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n }"
        },
        {
            "sha": "ff681d5980726c4ff231ba1fed160b9c7e67a71c",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a88e9c8586c23facc615c5a58908c5b7ec0241e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a88e9c8586c23facc615c5a58908c5b7ec0241e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=5a88e9c8586c23facc615c5a58908c5b7ec0241e",
            "patch": "@@ -933,7 +933,8 @@ GemmFusionAutotunerImpl::GenerateScaledDotConfigs(\n     const HloFusionInstruction& fusion, const HloScaledDotInstruction* dot) {\n   std::vector<BackendConfig> configs;\n \n-  if (!debug_options_.xla_gpu_experimental_disable_binary_libraries()) {\n+  if (!debug_options_.xla_gpu_experimental_disable_binary_libraries() &&\n+      IsAutotuningEnabled() && !config_.IsDeviceless()) {\n     // Add cuBLAS reference config, if available.\n     configs.push_back(CuBlasConfig{});\n   }"
        }
    ],
    "stats": {
        "total": 7,
        "additions": 4,
        "deletions": 3
    }
}