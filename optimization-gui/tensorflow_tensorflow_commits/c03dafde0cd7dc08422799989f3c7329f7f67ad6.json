{
    "author": "pizzud",
    "message": "all_reduce_thunk: Make GetAllReduceConfigInst not a template.\n\nC++ requires that template functions either be implemented in the header or\nthat all specializations are explicitly declared in the header. Fortunately\nwe don't need this function to be a template, so we can simply provide a\nconcrete type instead and avoid the whole issue.\n\nPiperOrigin-RevId: 834345599",
    "sha": "c03dafde0cd7dc08422799989f3c7329f7f67ad6",
    "files": [
        {
            "sha": "22cfba07d0de9affb6049723840b7ae1ecc7494a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c03dafde0cd7dc08422799989f3c7329f7f67ad6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c03dafde0cd7dc08422799989f3c7329f7f67ad6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc?ref=c03dafde0cd7dc08422799989f3c7329f7f67ad6",
            "patch": "@@ -65,14 +65,14 @@ absl::Status CheckImplementableInst(const HloInstruction* inst,\n }\n \n template <typename HloInstType>\n-CollectiveOpGroupMode GetGroupModeInst(HloInstType* inst) {\n+CollectiveOpGroupMode GetGroupModeInst(const HloInstType* inst) {\n   return GetAllReduceConfigInst(inst).config.group_mode;\n }\n \n }  // namespace\n \n-template <typename HloInstType>\n-AllReduceConfig GetAllReduceConfigInst(HloInstType* inst) {\n+AllReduceConfig GetAllReduceConfigInst(\n+    const HloAllReduceInstructionBase* inst) {\n   std::optional<ReductionKind> reduction_kind =\n       MatchReductionComputation(inst->called_computations().front());\n   CHECK(reduction_kind.has_value());"
        },
        {
            "sha": "46bf4af79ff7592b59b9c6d578bc0e45a322d7e3",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c03dafde0cd7dc08422799989f3c7329f7f67ad6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c03dafde0cd7dc08422799989f3c7329f7f67ad6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.h?ref=c03dafde0cd7dc08422799989f3c7329f7f67ad6",
            "patch": "@@ -38,8 +38,7 @@ struct AllReduceConfig {\n   ReductionKind reduction_kind;\n };\n \n-template <typename HloInstType>\n-AllReduceConfig GetAllReduceConfigInst(HloInstType* inst);\n+AllReduceConfig GetAllReduceConfigInst(const HloAllReduceInstructionBase* inst);\n \n // Thunk that performs a NCCL-based All-Reduce or Reduce-Scatter among CUDA\n // GPU-based replicas."
        }
    ],
    "stats": {
        "total": 9,
        "additions": 4,
        "deletions": 5
    }
}