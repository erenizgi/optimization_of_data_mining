{
    "author": "penpornk",
    "message": "[xla:cpu:onednn] Change the remaining `INTEL_MKL` ifdef guards in XLA to `XLA_ONEDNN`.\n\nPiperOrigin-RevId: 814158994",
    "sha": "8394453d57e76e2f15433a12bf1f6be0be12f321",
    "files": [
        {
            "sha": "1d7b6bfd6f9a08cc31ea565aa81d8299936a230d",
            "filename": "third_party/xla/xla/service/change_op_data_type.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8394453d57e76e2f15433a12bf1f6be0be12f321/third_party%2Fxla%2Fxla%2Fservice%2Fchange_op_data_type.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8394453d57e76e2f15433a12bf1f6be0be12f321/third_party%2Fxla%2Fxla%2Fservice%2Fchange_op_data_type.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fchange_op_data_type.cc?ref=8394453d57e76e2f15433a12bf1f6be0be12f321",
            "patch": "@@ -18,9 +18,9 @@ limitations under the License.\n #include <optional>\n \n #include \"xla/service/hlo_creation_utils.h\"\n-#if defined(INTEL_MKL)\n+#ifdef XLA_ONEDNN\n #include \"xla/service/cpu/onednn_contraction_rewriter.h\"\n-#endif  // INTEL_MKL\n+#endif  // XLA_ONEDNN\n \n namespace xla {\n namespace {\n@@ -62,11 +62,11 @@ absl::StatusOr<bool> ChangeOpDataType::Run(\n       if (it == to_type_map_.end()) {\n         continue;\n       }\n-#if defined(INTEL_MKL)\n+#ifdef XLA_ONEDNN\n       if (cpu::OneDnnContractionRewriter::ShouldRewriteInstr(instr, true)) {\n         continue;\n       }\n-#endif  // INTEL_MKL\n+#endif  // XLA_ONEDNN\n       const PrimitiveType to_type = it->second;\n       absl::InlinedVector<HloInstruction*, 8> new_operands;\n       for (HloInstruction* operand : instr->mutable_operands()) {"
        },
        {
            "sha": "2683541ac9229842eb7671101912bb972f01d646",
            "filename": "third_party/xla/xla/service/cpu/cpu_compiler.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8394453d57e76e2f15433a12bf1f6be0be12f321/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8394453d57e76e2f15433a12bf1f6be0be12f321/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc?ref=8394453d57e76e2f15433a12bf1f6be0be12f321",
            "patch": "@@ -253,12 +253,12 @@ limitations under the License.\n #include \"llvm/TargetParser/X86TargetParser.h\"\n #endif\n \n-#if defined(INTEL_MKL)\n+#ifdef XLA_ONEDNN\n #include \"xla/hlo/transforms/simplifiers/simplify_fp_conversions.h\"\n #include \"xla/service/cpu/onednn_contraction_rewriter.h\"\n #include \"xla/service/cpu/onednn_float_support.h\"\n #include \"xla/service/cpu/onednn_ops_rewriter.h\"\n-#endif\n+#endif  // XLA_ONEDNN\n \n namespace xla {\n namespace {\n@@ -679,7 +679,7 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n   pipeline.AddPass<DotDecomposer>();\n \n   // Rewrite to custom calls with target as oneDNN library calls.\n-#if defined(INTEL_MKL)\n+#ifdef XLA_ONEDNN\n   // This pass is not supported in the thunk runtime yet.\n   bool is_thunk_runtime = true;\n   bool use_onednn_custom_call =\n@@ -695,7 +695,7 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n     // but in future plan to rename oneDNNrewriter to specific to onednn matmul\n     pipeline.AddPass<OneDnnOpsRewriter>();\n   }\n-#endif  // INTEL_MKL\n+#endif  // XLA_ONEDNN\n \n   // Promote BF16 all-reduce to F32.\n   const std::pair<PrimitiveType, PrimitiveType> ar_promoted_types[] = {\n@@ -706,7 +706,7 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n   // BF16/F8 lowering for most ops.\n   CpuFloatSupport bf16_support(BF16, call_library_for_dot,\n                                target_machine_features);\n-#if defined(INTEL_MKL)\n+#ifdef XLA_ONEDNN\n   OneDnnFloatSupport onednn_bf16_support(BF16);\n   if (is_onednn_compatible) {\n     pipeline.AddPass<FloatNormalization>(&onednn_bf16_support);\n@@ -715,7 +715,7 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n   }\n #else\n   pipeline.AddPass<FloatNormalization>(&bf16_support);\n-#endif\n+#endif  // XLA_ONEDNN\n   FloatSupport f8e5m2_support(F8E5M2, F16);\n   pipeline.AddPass<FloatNormalization>(&f8e5m2_support);\n   FloatSupport f8e4m3_support(F8E4M3, F16);\n@@ -903,7 +903,7 @@ absl::Status CpuCompiler::RunHloPassesAfterLayoutAssn(\n           ? module->config().intra_op_parallelism_threads()\n           : tsl::port::NumSchedulableCPUs();\n \n-#if defined(INTEL_MKL)\n+#ifdef XLA_ONEDNN\n   // AOT compiled code runs in single thread.\n   bool use_onednn_custom_call =\n       debug_options.xla_cpu_experimental_onednn_custom_call() &&\n@@ -923,7 +923,7 @@ absl::Status CpuCompiler::RunHloPassesAfterLayoutAssn(\n       pipeline.AddPass<SimplifyFPConversions>();\n     }\n   }\n-#endif  // INTEL_MKL\n+#endif  // XLA_ONEDNN\n \n   // Guard this experimental pipeline with flags until we make sure that\n   // calling `DotDecomposer` early is okay."
        },
        {
            "sha": "f1bec59da533d0e5e904f96265327fdf294fd395",
            "filename": "third_party/xla/xla/service/cpu/ir_emitter.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8394453d57e76e2f15433a12bf1f6be0be12f321/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8394453d57e76e2f15433a12bf1f6be0be12f321/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.cc?ref=8394453d57e76e2f15433a12bf1f6be0be12f321",
            "patch": "@@ -111,9 +111,9 @@ limitations under the License.\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n \n-#if defined(INTEL_MKL)\n+#ifdef XLA_ONEDNN\n #include \"xla/service/cpu/onednn_memory_util.h\"\n-#endif\n+#endif  // XLA_ONEDNN\n \n namespace xla {\n \n@@ -2422,7 +2422,7 @@ absl::Status IrEmitter::HandleTopK(HloInstruction* hlo) {\n   return absl::OkStatus();\n }\n \n-#if defined(INTEL_MKL)\n+#ifdef XLA_ONEDNN\n \n // Emits operands alloca vector for oneDNN custom calls.\n std::vector<StackAlloca> IrEmitter::EmitOneDnnOperandsAlloca(\n@@ -2762,7 +2762,7 @@ absl::Status IrEmitter::HandleOneDnnSoftmax(HloInstruction* custom_call) {\n \n   return absl::OkStatus();\n }\n-#endif  // INTEL_MKL\n+#endif  // XLA_ONEDNN\n \n absl::Status IrEmitter::HandleCustomCall(HloInstruction* custom_call) {\n   if (custom_call->custom_call_target() == \"PadToStatic\") {\n@@ -2774,7 +2774,7 @@ absl::Status IrEmitter::HandleCustomCall(HloInstruction* custom_call) {\n   if (custom_call->custom_call_target() == \"TopK\") {\n     return HandleTopK(custom_call);\n   }\n-#if defined(INTEL_MKL)\n+#ifdef XLA_ONEDNN\n   if (custom_call->custom_call_target() == \"__onednn$matmul\") {\n     return HandleOneDnnMatMulCalls(custom_call,\n                                    runtime::kOneDnnMatMulSymbolName);\n@@ -2792,7 +2792,7 @@ absl::Status IrEmitter::HandleCustomCall(HloInstruction* custom_call) {\n     return HandleOneDnnMatMulCalls(custom_call,\n                                    runtime::kOneDnnMatMulReorderSymbolName);\n   }\n-#endif  // INTEL_MKL\n+#endif  // XLA_ONEDNN\n   absl::Span<HloInstruction* const> operands(custom_call->operands());\n   auto typed_custom_call = Cast<HloCustomCallInstruction>(custom_call);\n   auto is_typed_ffi = typed_custom_call->api_version() =="
        },
        {
            "sha": "2e3922ba4782e896179a865a968063a0c5e8d51f",
            "filename": "third_party/xla/xla/service/cpu/ir_emitter.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8394453d57e76e2f15433a12bf1f6be0be12f321/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8394453d57e76e2f15433a12bf1f6be0be12f321/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.h?ref=8394453d57e76e2f15433a12bf1f6be0be12f321",
            "patch": "@@ -60,9 +60,9 @@ limitations under the License.\n #include \"xla/service/name_uniquer.h\"\n #include \"xla/xla_data.pb.h\"\n \n-#if defined(INTEL_MKL)\n+#ifdef XLA_ONEDNN\n #include \"xla/service/cpu/onednn_memory_util.h\"\n-#endif\n+#endif  // XLA_ONEDNN\n \n namespace xla {\n namespace cpu {\n@@ -336,7 +336,7 @@ class IrEmitter : public DfsHloVisitorWithDefault,\n   absl::Status HandleTopK(HloInstruction* hlo) override;\n   absl::Status HandleAllReduceSingleReplica(HloInstruction* crs);\n   absl::Status HandleAllReduceMultipleReplica(HloInstruction* crs);\n-#if defined(INTEL_MKL)\n+#ifdef XLA_ONEDNN\n   std::vector<StackAlloca> EmitOneDnnOperandsAlloca(HloInstruction* custom_call,\n                                                     llvm::Value*& args_val,\n                                                     int& arg_indx);\n@@ -347,7 +347,7 @@ class IrEmitter : public DfsHloVisitorWithDefault,\n   absl::Status HandleOneDnnSoftmax(HloInstruction* hlo);\n   absl::Status HandleOneDnnLayerNorm(HloInstruction* hlo);\n   absl::Status HandleOneDnnConvolution(HloInstruction* hlo);\n-#endif  // INTEL_MKL\n+#endif  // XLA_ONEDNN\n   // Private helper to initialize an IR function for the computation.\n   void InitializeIrFunction(const std::string& function_name);\n "
        },
        {
            "sha": "2edf64d495651137b7b12b0efb96568f82b0b4c2",
            "filename": "third_party/xla/xla/service/cpu/runtime_symbol_generator.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8394453d57e76e2f15433a12bf1f6be0be12f321/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_symbol_generator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8394453d57e76e2f15433a12bf1f6be0be12f321/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_symbol_generator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_symbol_generator.cc?ref=8394453d57e76e2f15433a12bf1f6be0be12f321",
            "patch": "@@ -54,12 +54,12 @@ limitations under the License.\n #include \"xla/service/custom_call_target_registry.h\"\n #include \"tsl/platform/logging.h\"\n \n-#if defined(INTEL_MKL)\n+#ifdef XLA_ONEDNN\n #include \"xla/service/cpu/onednn_convolution.h\"\n #include \"xla/service/cpu/onednn_layer_norm.h\"\n #include \"xla/service/cpu/onednn_matmul.h\"\n #include \"xla/service/cpu/onednn_softmax.h\"\n-#endif\n+#endif  // XLA_ONEDNN\n \n namespace xla::cpu {\n \n@@ -187,13 +187,13 @@ static bool RegisterKnownJITSymbols() {\n   REGISTER_CPU_RUNTIME_SYMBOL(StatusIsSuccess);\n   REGISTER_CPU_RUNTIME_SYMBOL(KeyValueSort);\n   REGISTER_CPU_RUNTIME_SYMBOL(TopKF32);\n-#if defined(INTEL_MKL)\n+#ifdef XLA_ONEDNN\n   REGISTER_CPU_RUNTIME_SYMBOL(OneDnnMatMul);\n   REGISTER_CPU_RUNTIME_SYMBOL(OneDnnSoftmax);\n   REGISTER_CPU_RUNTIME_SYMBOL(OneDnnLayerNorm);\n   REGISTER_CPU_RUNTIME_SYMBOL(OneDnnConvolution);\n   REGISTER_CPU_RUNTIME_SYMBOL(OneDnnMatMulReorder);\n-#endif  // INTEL_MKL\n+#endif  // XLA_ONEDNN\n \n   registry->Register(\"__gnu_f2h_ieee\", reinterpret_cast<void*>(__gnu_f2h_ieee),\n                      \"Host\");"
        },
        {
            "sha": "72da6e383ce23e53ad4bcb5bdf572aa0dd98436f",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8394453d57e76e2f15433a12bf1f6be0be12f321/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8394453d57e76e2f15433a12bf1f6be0be12f321/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc?ref=8394453d57e76e2f15433a12bf1f6be0be12f321",
            "patch": "@@ -116,9 +116,9 @@ limitations under the License.\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/profiler/lib/traceme.h\"\n \n-#ifdef INTEL_MKL\n+#ifdef XLA_ONEDNN\n #include \"xla/backends/cpu/runtime/onednn/onednn_op_thunk.h\"\n-#endif  // INTEL_MKL\n+#endif  // XLA_ONEDNN\n \n #if XLA_ONEDNN_USE_GRAPH_API\n #include \"xla/backends/cpu/onednn_emitter.h\"\n@@ -1205,7 +1205,7 @@ static absl::StatusOr<OpBuffers> GetOpBuffers(\n   };\n }\n \n-#ifdef INTEL_MKL\n+#ifdef XLA_ONEDNN\n absl::StatusOr<ThunkSequence> ThunkEmitter::EmitOneDnnOpThunk(\n     const HloInstruction* instruction) {\n   auto custom_call = Cast<HloCustomCallInstruction>(instruction);\n@@ -1226,7 +1226,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitOneDnnOpThunk(\n   return ThunkSequence::Of<OneDnnOpThunk>(\n       custom_call_target, ThunkInfo(custom_call), op_buffers, config);\n }\n-#endif  // INTEL_MKL\n+#endif  // XLA_ONEDNN\n \n static bool IsValidCustomCallApiVersion(CustomCallApiVersion api_version) {\n   switch (api_version) {\n@@ -1258,11 +1258,11 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCustomCallThunk(\n   } else if (custom_call_target == \"SliceToDynamic\") {\n     return EmitSliceToDynamicThunk(instruction);\n   } else if (absl::StartsWith(custom_call->custom_call_target(), \"__onednn$\")) {\n-#ifdef INTEL_MKL\n+#ifdef XLA_ONEDNN\n     return EmitOneDnnOpThunk(instruction);\n #else\n     return Unimplemented(\"XLA is not built with oneDNN.\");\n-#endif  // INTEL_MKL\n+#endif  // XLA_ONEDNN\n   }\n \n   // Check the API version."
        },
        {
            "sha": "bee4987149a4bf488510d01599c6eba0188b67b0",
            "filename": "third_party/xla/xla/tsl/tsl.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8394453d57e76e2f15433a12bf1f6be0be12f321/third_party%2Fxla%2Fxla%2Ftsl%2Ftsl.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8394453d57e76e2f15433a12bf1f6be0be12f321/third_party%2Fxla%2Fxla%2Ftsl%2Ftsl.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Ftsl.bzl?ref=8394453d57e76e2f15433a12bf1f6be0be12f321",
            "patch": "@@ -338,7 +338,7 @@ def tsl_copts(\n         if_tensorrt([\"-DGOOGLE_TENSORRT=1\"]) +\n         if_rocm([\"-DTENSORFLOW_USE_ROCM=1\"]) +\n         # Compile in oneDNN based ops when building for x86 platforms\n-        if_mkl([\"-DINTEL_MKL\"]) +\n+        if_mkl([\"-DXLA_ONEDNN\"]) +\n         # Enable additional ops (e.g., ops with non-NHWC data layout) and\n         # optimizations for Intel builds using oneDNN if configured\n         if_enable_mkl([\"-DENABLE_MKL\"]) +\n@@ -411,7 +411,7 @@ def tsl_gpu_library(deps = None, cuda_deps = None, copts = tsl_copts(), **kwargs\n             \"@local_config_rocm//rocm:hip\",\n             \"@local_config_rocm//rocm:rocm_headers\",\n         ]),\n-        copts = (copts + if_cuda([\"-DGOOGLE_CUDA=1\", \"-DNV_CUDNN_DISABLE_EXCEPTION\"]) + if_rocm([\"-DTENSORFLOW_USE_ROCM=1\"]) + if_xla_available([\"-DTENSORFLOW_USE_XLA=1\"]) + if_mkl([\"-DINTEL_MKL=1\"]) + if_enable_mkl([\"-DENABLE_MKL\"]) + if_tensorrt([\"-DGOOGLE_TENSORRT=1\"])),\n+        copts = (copts + if_cuda([\"-DGOOGLE_CUDA=1\", \"-DNV_CUDNN_DISABLE_EXCEPTION\"]) + if_rocm([\"-DTENSORFLOW_USE_ROCM=1\"]) + if_xla_available([\"-DTENSORFLOW_USE_XLA=1\"]) + if_mkl([\"-DXLA_ONEDNN\"]) + if_enable_mkl([\"-DENABLE_MKL\"]) + if_tensorrt([\"-DGOOGLE_TENSORRT=1\"])),\n         **kwargs\n     )\n "
        }
    ],
    "stats": {
        "total": 68,
        "additions": 34,
        "deletions": 34
    }
}