{
    "author": "ermilovmaxim",
    "message": "Include shape info to (Custom)KernelThunk::buffer_uses\n\nPiperOrigin-RevId: 833562810",
    "sha": "c51d830d034e458dff1b1613774904308957b630",
    "files": [
        {
            "sha": "c5671027d4abe8fa52eaccecf10c567894b73ff9",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_kernel_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c51d830d034e458dff1b1613774904308957b630/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c51d830d034e458dff1b1613774904308957b630/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.cc?ref=c51d830d034e458dff1b1613774904308957b630",
            "patch": "@@ -47,6 +47,7 @@ CustomKernelThunk::CustomKernelThunk(\n     const emitters::KernelArguments& kernel_arguments)\n     : Thunk(Kind::kCustomKernel, std::move(thunk_info)),\n       args_(kernel_arguments.GetArgumentBufferSlices()),\n+      args_shape_(kernel_arguments.GetArgumentBufferShapes()),\n       written_(kernel_arguments.GetArgumentOutputFlags()),\n       custom_kernel_(std::move(custom_kernel)) {}\n \n@@ -112,9 +113,9 @@ Thunk::BufferUses CustomKernelThunk::buffer_uses() const {\n     // We assume that any buffer is either an input or an output of the\n     // kernel, and inout buffers are represented as 2 separate arguments.\n     if (written_[i]) {\n-      buffers.push_back(BufferUse::Write(args_[i]));\n+      buffers.push_back(BufferUse::Write(args_[i], args_shape_[i]));\n     } else {\n-      buffers.push_back(BufferUse::Read(args_[i]));\n+      buffers.push_back(BufferUse::Read(args_[i], args_shape_[i]));\n     }\n   }\n   return buffers;"
        },
        {
            "sha": "dca9143fb559004882d5c20d24b8e4165ebcd1d7",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_kernel_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c51d830d034e458dff1b1613774904308957b630/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c51d830d034e458dff1b1613774904308957b630/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk.h?ref=c51d830d034e458dff1b1613774904308957b630",
            "patch": "@@ -73,6 +73,7 @@ class CustomKernelThunk : public Thunk {\n  private:\n   // Buffer slices passed to the kernel as arguments.\n   std::vector<BufferAllocation::Slice> args_;\n+  std::vector<Shape> args_shape_;\n \n   // args_[i] is written iff (written_[i] == true).\n   std::vector<bool> written_;"
        },
        {
            "sha": "2e3e6cbda224c141dd68dc3fc8a6ae499728af28",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_kernel_thunk_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c51d830d034e458dff1b1613774904308957b630/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c51d830d034e458dff1b1613774904308957b630/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_kernel_thunk_test.cc?ref=c51d830d034e458dff1b1613774904308957b630",
            "patch": "@@ -30,6 +30,7 @@ namespace xla::gpu {\n namespace {\n \n TEST(CustomKernelThunkTest, BufferUsesReturnsCorrectBuffers) {\n+  Shape arg_shape = ShapeUtil::MakeShape(F32, {512});\n   CustomKernel kernel(\n       /*name=*/\"\",\n       se::KernelLoaderSpec::CreateCudaPtxInMemorySpec(\n@@ -38,17 +39,18 @@ TEST(CustomKernelThunkTest, BufferUsesReturnsCorrectBuffers) {\n   BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n   BufferAllocation::Slice slice0(&alloc, /*offset=*/0, /*size=*/512);\n   BufferAllocation::Slice slice1(&alloc, /*offset=*/512, /*size=*/512);\n-  emitters::KernelArgument arg0(ShapeUtil::MakeShape(F32, {512}), slice0);\n-  emitters::KernelArgument arg1(ShapeUtil::MakeShape(F32, {512}), slice1);\n+  emitters::KernelArgument arg0(arg_shape, slice0);\n+  emitters::KernelArgument arg1(arg_shape, slice1);\n   arg0.set_written(false);\n   arg1.set_written(true);\n   emitters::KernelArguments kernel_arguments({arg0, arg1});\n   CustomKernelThunk thunk(Thunk::ThunkInfo{}, kernel, kernel_arguments);\n \n   Thunk::BufferUses buffers = thunk.buffer_uses();\n \n-  ASSERT_THAT(buffers, testing::UnorderedElementsAre(BufferUse::Read(slice0),\n-                                                     BufferUse::Write(slice1)));\n+  ASSERT_THAT(buffers, testing::UnorderedElementsAre(\n+                           BufferUse::Read(slice0, arg_shape),\n+                           BufferUse::Write(slice1, arg_shape)));\n }\n \n TEST(CustomKernelThunkTest, BufferUsesReturnsBuffersInConsistentOrder) {"
        },
        {
            "sha": "7af58fec0956fa14ebc019107466c802d8982719",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 10,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c51d830d034e458dff1b1613774904308957b630/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c51d830d034e458dff1b1613774904308957b630/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc?ref=c51d830d034e458dff1b1613774904308957b630",
            "patch": "@@ -65,6 +65,7 @@ KernelThunk::KernelThunk(Thunk::ThunkInfo thunk_info, std::string kernel_name,\n                          stream_executor::gpu::TmaMetadata tma_metadata)\n     : Thunk(Kind::kKernel, std::move(thunk_info)),\n       args_(kernel_arguments.GetArgumentBufferSlices()),\n+      args_shape_(kernel_arguments.GetArgumentBufferShapes()),\n       written_(kernel_arguments.GetArgumentOutputFlags()),\n       kernel_name_(std::move(kernel_name)),\n       launch_dimensions_(std::move(launch_dimensions)),\n@@ -84,11 +85,10 @@ absl::StatusOr<ThunkProto> KernelThunk::ToProto() const {\n   *proto.mutable_thunk_info() = thunk_info().ToProto();\n \n   auto* kernel_proto = proto.mutable_kernel_thunk();\n-  for (const auto& arg : args_) {\n-    TF_ASSIGN_OR_RETURN(*kernel_proto->add_args(), arg.ToProto());\n-  }\n-  for (bool written : written_) {\n-    kernel_proto->add_written(written);\n+  for (int i = 0; i < args_.size(); i++) {\n+    TF_ASSIGN_OR_RETURN(*kernel_proto->add_args(), args_[i].ToProto());\n+    *kernel_proto->add_args_shape() = args_shape_[i].ToProto();\n+    kernel_proto->add_written(written_[i]);\n   }\n   kernel_proto->set_kernel_name(kernel_name_);\n   *kernel_proto->mutable_launch_dimensions() = launch_dimensions_.ToProto();\n@@ -112,9 +112,11 @@ absl::StatusOr<std::unique_ptr<KernelThunk>> KernelThunk::FromProto(\n         stream_executor::ClusterDim::FromProto(proto.cluster_dim()));\n   }\n \n-  if (proto.written().size() != proto.args().size()) {\n+  if (proto.written().size() != proto.args().size() ||\n+      proto.args().size() != proto.args_shape().size()) {\n     return absl::InvalidArgumentError(\n-        \"Proto fields `written` and `args` need to have the same cardinality.\");\n+        \"Proto fields `written`, `args` and `args_shape` need to have the same \"\n+        \"cardinality.\");\n   }\n \n   std::vector<emitters::KernelArgument> arguments;\n@@ -123,7 +125,9 @@ absl::StatusOr<std::unique_ptr<KernelThunk>> KernelThunk::FromProto(\n     TF_ASSIGN_OR_RETURN(BufferAllocation::Slice slice,\n                         BufferAllocation::Slice::FromProto(proto.args().at(i),\n                                                            buffer_allocations));\n-    emitters::KernelArgument argument{Shape{}, slice};\n+    TF_ASSIGN_OR_RETURN(Shape shape,\n+                        Shape::FromProto(proto.args_shape().at(i)));\n+    emitters::KernelArgument argument{shape, slice};\n     argument.set_written(proto.written().at(i));\n     arguments.push_back(std::move(argument));\n   }\n@@ -251,9 +255,9 @@ Thunk::BufferUses KernelThunk::buffer_uses() const {\n     // We assume that any buffer is either an input or an output of the\n     // kernel, and inout buffers are represented as 2 separate arguments.\n     if (written_[i]) {\n-      buffers.push_back(BufferUse::Write(args_[i]));\n+      buffers.push_back(BufferUse::Write(args_[i], args_shape_[i]));\n     } else {\n-      buffers.push_back(BufferUse::Read(args_[i]));\n+      buffers.push_back(BufferUse::Read(args_[i], args_shape_[i]));\n     }\n   }\n   return buffers;"
        },
        {
            "sha": "9d0f87152ded45d324731700017db78ee0cca926",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c51d830d034e458dff1b1613774904308957b630/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c51d830d034e458dff1b1613774904308957b630/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.h?ref=c51d830d034e458dff1b1613774904308957b630",
            "patch": "@@ -33,6 +33,7 @@ limitations under the License.\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/shape.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -106,7 +107,7 @@ class KernelThunk : public Thunk {\n  private:\n   // Buffer slices passed to the kernel as arguments.\n   std::vector<BufferAllocation::Slice> args_;\n-\n+  std::vector<Shape> args_shape_;\n   // args_[i] is written iff (written_[i] == true).\n   std::vector<bool> written_;\n "
        },
        {
            "sha": "49145f8316f26c91928439b12e2426df5130a425",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk_test.cc",
            "status": "modified",
            "additions": 39,
            "deletions": 7,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c51d830d034e458dff1b1613774904308957b630/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c51d830d034e458dff1b1613774904308957b630/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk_test.cc?ref=c51d830d034e458dff1b1613774904308957b630",
            "patch": "@@ -178,6 +178,18 @@ TEST(KernelThunkTest, ToProto) {\n         kernel_thunk {\n           args { size: 1024 }\n           args { size: 256 }\n+          args_shape {\n+            element_type: F32\n+            dimensions: 1024\n+            layout { minor_to_major: 0 tail_padding_alignment_in_elements: 1 }\n+            is_dynamic_dimension: false\n+          }\n+          args_shape {\n+            element_type: F32\n+            dimensions: 256\n+            layout { minor_to_major: 0 tail_padding_alignment_in_elements: 1 }\n+            is_dynamic_dimension: false\n+          }\n           written: false\n           written: true\n           kernel_name: \"kernel123\"\n@@ -271,11 +283,12 @@ TEST(KernelThunkTest, ToAndFromProto) {\n }\n \n TEST(KernelThunkTest, BufferUsesReturnsCorrectBuffers) {\n+  Shape arg_shape = ShapeUtil::MakeShape(F32, {512});\n   BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n   BufferAllocation::Slice slice0(&alloc, /*offset=*/0, /*size=*/512);\n   BufferAllocation::Slice slice1(&alloc, /*offset=*/512, /*size=*/512);\n-  emitters::KernelArgument arg0(ShapeUtil::MakeShape(F32, {512}), slice0);\n-  emitters::KernelArgument arg1(ShapeUtil::MakeShape(F32, {512}), slice1);\n+  emitters::KernelArgument arg0(arg_shape, slice0);\n+  emitters::KernelArgument arg1(arg_shape, slice1);\n   arg0.set_written(false);\n   arg1.set_written(true);\n   emitters::KernelArguments kernel_arguments({arg0, arg1});\n@@ -285,16 +298,18 @@ TEST(KernelThunkTest, BufferUsesReturnsCorrectBuffers) {\n \n   Thunk::BufferUses buffers = thunk.buffer_uses();\n \n-  ASSERT_THAT(buffers, testing::UnorderedElementsAre(BufferUse::Read(slice0),\n-                                                     BufferUse::Write(slice1)));\n+  ASSERT_THAT(buffers, testing::UnorderedElementsAre(\n+                           BufferUse::Read(slice0, arg_shape),\n+                           BufferUse::Write(slice1, arg_shape)));\n }\n \n TEST(KernelThunkTest, BufferUsesReturnsBuffersInConsistentOrder) {\n+  Shape arg_shape = ShapeUtil::MakeShape(F32, {512});\n   BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n   BufferAllocation::Slice slice0(&alloc, /*offset=*/0, /*size=*/512);\n   BufferAllocation::Slice slice1(&alloc, /*offset=*/512, /*size=*/512);\n-  emitters::KernelArgument arg0(ShapeUtil::MakeShape(F32, {512}), slice0);\n-  emitters::KernelArgument arg1(ShapeUtil::MakeShape(F32, {512}), slice1);\n+  emitters::KernelArgument arg0(arg_shape, slice0);\n+  emitters::KernelArgument arg1(arg_shape, slice1);\n   arg0.set_written(false);\n   arg1.set_written(true);\n   emitters::KernelArguments kernel_arguments({arg0, arg1});\n@@ -308,7 +323,6 @@ TEST(KernelThunkTest, BufferUsesReturnsBuffersInConsistentOrder) {\n   ASSERT_THAT(buffers1, testing::ContainerEq(buffers2));\n }\n \n-\n class KernelThunkTmaPTXTest : public ::testing::TestWithParam<bool> {\n  public:\n   absl::StatusOr<std::unique_ptr<KernelThunk>> GetTmaKernelThunk() {\n@@ -318,6 +332,24 @@ class KernelThunkTmaPTXTest : public ::testing::TestWithParam<bool> {\n         args { size: 1048576 buffer_allocation_index: 0 }\n         args { size: 1048576 offset: 1048576 }\n         args { size: 4194304 offset: 2097152 }\n+        args_shape {\n+          element_type: F32\n+          dimensions: 262144\n+          layout { minor_to_major: 0 tail_padding_alignment_in_elements: 1 }\n+          is_dynamic_dimension: false\n+        }\n+        args_shape {\n+          element_type: F32\n+          dimensions: 262144\n+          layout { minor_to_major: 0 tail_padding_alignment_in_elements: 1 }\n+          is_dynamic_dimension: false\n+        }\n+        args_shape {\n+          element_type: F32\n+          dimensions: 1048576\n+          layout { minor_to_major: 0 tail_padding_alignment_in_elements: 1 }\n+          is_dynamic_dimension: false\n+        }\n         written: false\n         written: false\n         written: true"
        },
        {
            "sha": "6442e3c19fc54da9ad6de1727278a36c2ad6a729",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.proto",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c51d830d034e458dff1b1613774904308957b630/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c51d830d034e458dff1b1613774904308957b630/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto?ref=c51d830d034e458dff1b1613774904308957b630",
            "patch": "@@ -85,6 +85,7 @@ message WhileThunkProto {\n \n message KernelThunkProto {\n   repeated xla.buffer_assignment.BufferAllocationSliceProto args = 1;\n+  repeated xla.ShapeProto args_shape = 8;\n   repeated bool written = 2;\n   string kernel_name = 3;\n   LaunchDimensionsProto launch_dimensions = 4;"
        },
        {
            "sha": "c0e807183dd58bb7aa77ff92980972539781c89a",
            "filename": "third_party/xla/xla/codegen/emitters/kernel_arguments.h",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c51d830d034e458dff1b1613774904308957b630/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_arguments.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c51d830d034e458dff1b1613774904308957b630/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_arguments.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_arguments.h?ref=c51d830d034e458dff1b1613774904308957b630",
            "patch": "@@ -112,6 +112,15 @@ class KernelArguments {\n     return arg_slices;\n   }\n \n+  std::vector<Shape> GetArgumentBufferShapes() const {\n+    std::vector<Shape> arg_shapes;\n+    arg_shapes.reserve(args_.size());\n+    for (const KernelArgument& arg : args_) {\n+      arg_shapes.push_back(arg.shape());\n+    }\n+    return arg_shapes;\n+  }\n+\n   std::vector<bool> GetArgumentOutputFlags() const {\n     std::vector<bool> output_flags;\n     output_flags.reserve(args_.size());"
        }
    ],
    "stats": {
        "total": 99,
        "additions": 75,
        "deletions": 24
    }
}