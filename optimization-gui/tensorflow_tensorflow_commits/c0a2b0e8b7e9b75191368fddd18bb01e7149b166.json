{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 847189104",
    "sha": "c0a2b0e8b7e9b75191368fddd18bb01e7149b166",
    "files": [
        {
            "sha": "b011a24cb1ed1e8de1ccd1fe9776824b43c3b7ad",
            "filename": "tensorflow/core/kernels/in_topk_op_gpu.cu.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c0a2b0e8b7e9b75191368fddd18bb01e7149b166/tensorflow%2Fcore%2Fkernels%2Fin_topk_op_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c0a2b0e8b7e9b75191368fddd18bb01e7149b166/tensorflow%2Fcore%2Fkernels%2Fin_topk_op_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fin_topk_op_gpu.cu.cc?ref=c0a2b0e8b7e9b75191368fddd18bb01e7149b166",
            "patch": "@@ -39,7 +39,7 @@ template <typename T, typename TargetT>\n __global__ void ComputePredictionMaskKernel(\n     const T* __restrict__ predictions,    // dims: [ num_targets x num_classes ]\n     const TargetT* __restrict__ targets,  // dims: [ num_targets ]\n-    int64* __restrict__ mask,             // dims: [ num_targets x num_classes ]\n+    int64_t* __restrict__ mask,           // dims: [ num_targets x num_classes ]\n     int num_targets, int num_classes) {\n   GPU_1D_KERNEL_LOOP(i, num_targets * num_classes) {\n     const int batch_index = i / num_classes;\n@@ -67,7 +67,8 @@ __global__ void ComputePredictionMaskKernel(\n // larger than the target, or to '-1' if target class in invalid of predictions\n // in a batch have non-finite values.\n struct MaskSum {\n-  __host__ __device__ int64 operator()(const int64& a, const int64& b) const {\n+  __host__ __device__ int64_t operator()(const int64_t& a,\n+                                         const int64_t& b) const {\n     if (a < 0 || b < 0)\n       return -1;\n     else\n@@ -77,8 +78,8 @@ struct MaskSum {\n \n namespace reduction_op_helper {\n template <>\n-struct IdentityValue<int64, MaskSum> {\n-  int64 operator()() { return 0; }\n+struct IdentityValue<int64_t, MaskSum> {\n+  int64_t operator()() { return 0; }\n };\n \n }  // namespace reduction_op_helper\n@@ -138,8 +139,8 @@ struct InTopKFunctor<GPUDevice, T, TargetT> {\n       auto in = predictions_mask.matrix<int64_t>();\n       auto out = num_larger_prediction.flat<int64_t>();\n \n-      ReduceImpl<int64, MaskSum, int64*, int64*, Dims<1>>(\n-          context, (int64*)out.data(), (int64*)in.data(), in.rank(),\n+      ReduceImpl<int64_t, MaskSum, int64_t*, int64_t*, Dims<1>>(\n+          context, (int64_t*)out.data(), (int64_t*)in.data(), in.rank(),\n           in.dimension(0), in.rank() >= 2 ? in.dimension(1) : 1,\n           in.rank() >= 3 ? in.dimension(2) : 1, out.rank(), Dims<1>(1),\n           MaskSum());\n@@ -152,8 +153,9 @@ struct InTopKFunctor<GPUDevice, T, TargetT> {\n       if (k.k_tensor->dtype() == DT_INT32) {\n         output.device(d) =\n             (cnt >= cnt.constant(0)) &&\n-            (cnt < k.k_tensor->flat<int32>().template cast<int64_t>().broadcast(\n-                       Dims<1>(num_targets)));\n+            (cnt <\n+             k.k_tensor->flat<int32_t>().template cast<int64_t>().broadcast(\n+                 Dims<1>(num_targets)));\n       } else {\n         output.device(d) =\n             (cnt >= cnt.constant(0)) &&"
        },
        {
            "sha": "6ba369ebdb43461aeeea68d09b767ac65021c3b7",
            "filename": "tensorflow/core/kernels/inplace_ops_functor_gpu.cu.cc",
            "status": "modified",
            "additions": 33,
            "deletions": 33,
            "changes": 66,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c0a2b0e8b7e9b75191368fddd18bb01e7149b166/tensorflow%2Fcore%2Fkernels%2Finplace_ops_functor_gpu.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c0a2b0e8b7e9b75191368fddd18bb01e7149b166/tensorflow%2Fcore%2Fkernels%2Finplace_ops_functor_gpu.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Finplace_ops_functor_gpu.cu.cc?ref=c0a2b0e8b7e9b75191368fddd18bb01e7149b166",
            "patch": "@@ -27,38 +27,38 @@ namespace functor {\n typedef Eigen::GpuDevice Device;\n \n template <typename T>\n-__global__ void DoParallelConcatOpKernel(int nthreads, const int64 rows,\n-                                         const int64 cols, int32 loc,\n+__global__ void DoParallelConcatOpKernel(int nthreads, const int64_t rows,\n+                                         const int64_t cols, int32_t loc,\n                                          const T* __restrict__ src,\n                                          T* __restrict__ dst) {\n   GPU_1D_KERNEL_LOOP(idx, nthreads) {\n-    int64 c = idx % cols;\n-    int64 r = (loc % rows + rows) % rows;  // Guard index range.\n+    int64_t c = idx % cols;\n+    int64_t r = (loc % rows + rows) % rows;  // Guard index range.\n     T* p = dst + r * cols + c;\n     const T* q = src + idx;\n     *p = ldg(q);\n   }\n }\n \n template <typename T>\n-Status DoParallelConcatUpdate(const Device& d, const Tensor& value, int32 loc,\n-                              Tensor* output) {\n-  const int64 nelem = value.NumElements();\n+absl::Status DoParallelConcatUpdate(const Device& d, const Tensor& value,\n+                                    int32_t loc, Tensor* output) {\n+  const int64_t nelem = value.NumElements();\n   GpuLaunchConfig cfg = GetGpuLaunchConfig(nelem, d);\n   auto Toutput = output->flat_outer_dims<T>();\n-  const int64 nrows = Toutput.dimension(0);\n-  const int64 ncols = Toutput.dimension(1);\n+  const int64_t nrows = Toutput.dimension(0);\n+  const int64_t ncols = Toutput.dimension(1);\n   const T* src = value.flat<T>().data();\n   T* dst = output->flat<T>().data();\n   TF_CHECK_OK(GpuLaunchKernel(\n       DoParallelConcatOpKernel<T>, cfg.block_count, cfg.thread_per_block, 0,\n       d.stream(), cfg.virtual_thread_count, nrows, ncols, loc, src, dst));\n-  return OkStatus();\n+  return absl::OkStatus();\n }\n \n template <>\n-Status DoParallelConcat(const Device& d, const Tensor& value, int32 loc,\n-                        Tensor* output) {\n+absl::Status DoParallelConcat(const Device& d, const Tensor& value, int32_t loc,\n+                              Tensor* output) {\n   CHECK_EQ(value.dtype(), output->dtype());\n   switch (value.dtype()) {\n #define CASE(type)                                              \\\n@@ -77,18 +77,18 @@ Status DoParallelConcat(const Device& d, const Tensor& value, int32 loc,\n       return errors::InvalidArgument(\"Unsupported data type: \",\n                                      DataTypeString(value.dtype()));\n   }\n-  return OkStatus();\n+  return absl::OkStatus();\n }\n \n template <typename T, InplaceOpType op>\n-__global__ void DoInplaceOpKernel(int nthreads, const int64 rows,\n-                                  const int64 cols, const int64 n,\n+__global__ void DoInplaceOpKernel(int nthreads, const int64_t rows,\n+                                  const int64_t cols, const int64_t n,\n                                   const T* __restrict__ src,\n-                                  const int32* __restrict__ rowids,\n+                                  const int32_t* __restrict__ rowids,\n                                   T* __restrict__ dst) {\n   GPU_1D_KERNEL_LOOP(idx, nthreads) {\n-    int64 r = idx / cols;\n-    int64 c = idx % cols;\n+    int64_t r = idx / cols;\n+    int64_t c = idx % cols;\n     r = (rowids[r] % rows + rows) % rows;  // Guard index range.\n     T* p = dst + r * cols + c;\n     const T* q = src + idx;\n@@ -109,15 +109,15 @@ __global__ void DoInplaceOpKernel(int nthreads, const int64 rows,\n template <typename T>\n void DoInplaceOp(const Device& d, InplaceOpType op, const Tensor& i,\n                  const Tensor& v, Tensor* y) {\n-  const int64 nelem = v.NumElements();\n+  const int64_t nelem = v.NumElements();\n   GpuLaunchConfig cfg = GetGpuLaunchConfig(nelem, d);\n   auto Ty = y->flat_outer_dims<T>();\n-  const int64 nrows = Ty.dimension(0);\n-  const int64 ncols = Ty.dimension(1);\n-  const int64 n = i.NumElements();\n+  const int64_t nrows = Ty.dimension(0);\n+  const int64_t ncols = Ty.dimension(1);\n+  const int64_t n = i.NumElements();\n   const T* src = v.flat<T>().data();\n   // TODO(sjhwang): Check that first dimension fits in int32 range.\n-  const int32* rowids = i.flat<int32>().data();\n+  const int32_t* rowids = i.flat<int32_t>().data();\n   T* dst = y->flat<T>().data();\n   switch (op) {\n     case I_UPDATE:\n@@ -144,15 +144,15 @@ void DoInplaceOp(const Device& d, InplaceOpType op, const Tensor& i,\n template <bool>\n void DoInplaceOp(const Device& d, InplaceOpType op, const Tensor& i,\n                  const Tensor& v, Tensor* y) {\n-  const int64 nelem = v.NumElements();\n+  const int64_t nelem = v.NumElements();\n   GpuLaunchConfig cfg = GetGpuLaunchConfig(nelem, d);\n   auto Ty = y->flat_outer_dims<bool>();\n-  const int64 nrows = Ty.dimension(0);\n-  const int64 ncols = Ty.dimension(1);\n-  const int64 n = i.NumElements();\n+  const int64_t nrows = Ty.dimension(0);\n+  const int64_t ncols = Ty.dimension(1);\n+  const int64_t n = i.NumElements();\n   const bool* src = v.flat<bool>().data();\n   // TODO(sjhwang): Check that first dimension fits in int32 range.\n-  const int32* rowids = i.flat<int32>().data();\n+  const int32_t* rowids = i.flat<int32_t>().data();\n   bool* dst = y->flat<bool>().data();\n   if (op == I_UPDATE) {\n     TF_CHECK_OK(GpuLaunchKernel(DoInplaceOpKernel<bool, I_UPDATE>,\n@@ -163,8 +163,8 @@ void DoInplaceOp(const Device& d, InplaceOpType op, const Tensor& i,\n }\n \n template <>\n-Status DoInplace(const Device& d, InplaceOpType op, const Tensor& i,\n-                 const Tensor& v, Tensor* y) {\n+absl::Status DoInplace(const Device& d, InplaceOpType op, const Tensor& i,\n+                       const Tensor& v, Tensor* y) {\n   CHECK_EQ(v.dtype(), y->dtype());\n   switch (v.dtype()) {\n #define CASE(type)                     \\\n@@ -186,11 +186,11 @@ Status DoInplace(const Device& d, InplaceOpType op, const Tensor& i,\n       return errors::InvalidArgument(\"Unsupported data type from DoInplace: \",\n                                      DataTypeString(v.dtype()));\n   }\n-  return OkStatus();\n+  return absl::OkStatus();\n }\n \n template <>\n-Status DoCopy(const Device& d, const Tensor& x, Tensor* y) {\n+absl::Status DoCopy(const Device& d, const Tensor& x, Tensor* y) {\n   CHECK_EQ(x.dtype(), y->dtype());\n   switch (x.dtype()) {\n #define CASE(type)                              \\\n@@ -214,7 +214,7 @@ Status DoCopy(const Device& d, const Tensor& x, Tensor* y) {\n       return errors::InvalidArgument(\"Unsupported dtype from DoCopy: \",\n                                      DataTypeString(x.dtype()));\n   }\n-  return OkStatus();\n+  return absl::OkStatus();\n }\n \n }  // end namespace functor"
        }
    ],
    "stats": {
        "total": 84,
        "additions": 43,
        "deletions": 41
    }
}