{
    "author": "pschuh",
    "message": "Update users not to set untuple_result now that it is true by default.\n\nPiperOrigin-RevId: 825230517",
    "sha": "d7b371034b32d89ab9ad28d8e3968afcf1a2110c",
    "files": [
        {
            "sha": "0d707e1e1f72577d2ec5fc34f71468ee9f5bf1ea",
            "filename": "tensorflow/compiler/jit/xla_launch_util.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/tensorflow%2Fcompiler%2Fjit%2Fxla_launch_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/tensorflow%2Fcompiler%2Fjit%2Fxla_launch_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fxla_launch_util.cc?ref=d7b371034b32d89ab9ad28d8e3968afcf1a2110c",
            "patch": "@@ -810,7 +810,6 @@ xla::ExecuteOptions GetPjRtExecuteOptions(\n     absl::flat_hash_set<int> non_donatable_input_indices) {\n   xla::ExecuteOptions options;\n   options.arguments_are_tupled = false;\n-  options.untuple_result = true;\n   // Hardcode run id to always be one: TF distributed strategy\n   // differentiates between subsequent runs using dependency edges. This\n   // is safe, as only TF dist-strat can produce distributed ops, and we"
        },
        {
            "sha": "6df663037f9567ce0c0d9729b30114d3c009a3da",
            "filename": "tensorflow/compiler/jit/xla_launch_util_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/tensorflow%2Fcompiler%2Fjit%2Fxla_launch_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/tensorflow%2Fcompiler%2Fjit%2Fxla_launch_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fjit%2Fxla_launch_util_test.cc?ref=d7b371034b32d89ab9ad28d8e3968afcf1a2110c",
            "patch": "@@ -208,7 +208,6 @@ class PjRtExecutionUtilTest : public OpsTestBase {\n \n     xla::ExecuteOptions exe_options;\n     exe_options.arguments_are_tupled = false;\n-    exe_options.untuple_result = true;\n \n     // TODO(b/257548614): currently PJRT is compiled as portable (num_replica =\n     // 1 and num_partition = 1). Support multiple partitions case.\n@@ -521,7 +520,6 @@ TEST(XlaLaunchUtilTest, GetPjRtExecuteOptions) {\n   xla::ExecuteOptions options =\n       GetPjRtExecuteOptions(DeviceType(DEVICE_GPU), {});\n   EXPECT_FALSE(options.arguments_are_tupled);\n-  EXPECT_TRUE(options.untuple_result);\n   EXPECT_FALSE(options.strict_shape_checking);\n   EXPECT_TRUE(options.use_major_to_minor_data_layout_for_callbacks);\n }"
        },
        {
            "sha": "0aeeb6d69a12bc1c5d49afb29c74c27bfeddacf4",
            "filename": "third_party/xla/xla/backends/cpu/benchmarks/hlo_benchmark_runner.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Fhlo_benchmark_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Fhlo_benchmark_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2Fhlo_benchmark_runner.cc?ref=d7b371034b32d89ab9ad28d8e3968afcf1a2110c",
            "patch": "@@ -228,7 +228,6 @@ absl::Status RunHloBenchmark(benchmark::State& state,\n   // thread pool if we need to run multiple executions in parallel.\n   ExecuteOptions execute_options;\n   execute_options.execution_mode = ExecuteOptions::ExecutionMode::kSynchronous;\n-  execute_options.untuple_result = true;\n \n   std::vector<std::vector<PjRtBuffer*>> execution_args_ptrs(\n       benchmark_options.num_executions);"
        },
        {
            "sha": "158378bdc4b6b29536374fe1e75ba508c427fa4c",
            "filename": "third_party/xla/xla/core/host_offloading/host_offloading_pjrt_executable.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2Fhost_offloading_pjrt_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2Fhost_offloading_pjrt_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2Fhost_offloading_pjrt_executable.cc?ref=d7b371034b32d89ab9ad28d8e3968afcf1a2110c",
            "patch": "@@ -258,8 +258,6 @@ HostOffloadingPjRtExecutable::Execute(\n \n   // TODO(b/340666998) Add additional context needed to support megascale ops\n   ::xla::ExecuteOptions pjrt_execute_options{\n-      // By default untuple results.\n-      .untuple_result = true,\n       // Forward launch id to the host offloading executable because logically\n       // it executes as a part of parent device execution.\n       .launch_id = execute_options.launch_id,"
        },
        {
            "sha": "0723dbfa94663de350d7a0fbcfb162c7c9e46527",
            "filename": "third_party/xla/xla/pjrt/c/pjrt_c_api_wrapper_impl.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_wrapper_impl.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_wrapper_impl.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_wrapper_impl.cc?ref=d7b371034b32d89ab9ad28d8e3968afcf1a2110c",
            "patch": "@@ -1818,7 +1818,6 @@ PJRT_Error* PJRT_LoadedExecutable_Execute(\n   }\n   options.strict_shape_checking = true;\n   options.arguments_are_tupled = false;\n-  options.untuple_result = true;\n   options.context = args->options->context\n                         ? args->options->context->execute_context.get()\n                         : nullptr;"
        },
        {
            "sha": "18c5931d84b8ca4d7d1b508083348a7f8ebc5931",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc?ref=d7b371034b32d89ab9ad28d8e3968afcf1a2110c",
            "patch": "@@ -1814,7 +1814,6 @@ TEST(StreamExecutorGpuClientTest, ExecutePinnedHostOutputTupleTest) {\n   // Untuple the result so that we get separate buffers.\n   // This is how JAX invokes XLA.\n   ExecuteOptions execute_options;\n-  execute_options.untuple_result = true;\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto result, executable->Execute({{input.get()}}, execute_options));\n "
        },
        {
            "sha": "1b906549df2b183a6bb7f9a4956afcfb74306af4",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_client_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc?ref=d7b371034b32d89ab9ad28d8e3968afcf1a2110c",
            "patch": "@@ -239,7 +239,6 @@ ENTRY %Add.6 (a.1: f32[], b.2: f32[]) -> (f32[], f32[]) {\n                           CompileExecutable(kAddProgram, *client));\n \n   ExecuteOptions options;\n-  options.untuple_result = true;\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto result,\n       executable->Execute({{buffer.get(), buffer.get()}}, /*options=*/options));\n@@ -1440,7 +1439,6 @@ TEST(TfrtGpuClientTest, ExecutePinnedHostOutputTupleTest) {\n   // Untuple the result so that we get separate buffers.\n   // This is how JAX invokes XLA.\n   ExecuteOptions execute_options;\n-  execute_options.untuple_result = true;\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto result, executable->Execute({{input.get()}}, execute_options));\n "
        },
        {
            "sha": "8118aae6a4d5777febfa444ff6fb2bae884c64aa",
            "filename": "third_party/xla/xla/pjrt/pjrt_client_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_client_test.cc?ref=d7b371034b32d89ab9ad28d8e3968afcf1a2110c",
            "patch": "@@ -571,7 +571,6 @@ ENTRY DuplicateDonationError() -> (f32[2, 2], f32[2, 2]) {\n                           MakeFloatBuffer(client.get(), data, {2, 2}));\n \n   xla::ExecuteOptions options;\n-  options.untuple_result = true;\n   {\n     auto result = pjrt_executable->Execute(/*argument_handles=*/{{\n                                                buffer0.get(),"
        },
        {
            "sha": "a0a4e0f3d30c0fbc275b053ffe776e8da56a2a20",
            "filename": "third_party/xla/xla/pjrt/pjrt_executable_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable_test.cc?ref=d7b371034b32d89ab9ad28d8e3968afcf1a2110c",
            "patch": "@@ -76,7 +76,6 @@ TEST(CompileOptionsTest, Defaults) {\n TEST(ExecuteOptionsTest, Serialization) {\n   ExecuteOptions src;\n   src.arguments_are_tupled = true;\n-  src.untuple_result = false;\n   src.launch_id = 1234;\n   src.strict_shape_checking = true;\n   src.execution_mode = ExecuteOptions::ExecutionMode::kAsynchronous;"
        },
        {
            "sha": "3b62e1aefa0b31f4f31a9198eb98f8b4c5e27f36",
            "filename": "third_party/xla/xla/pjrt/pjrt_stream_executor_client_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client_test.cc?ref=d7b371034b32d89ab9ad28d8e3968afcf1a2110c",
            "patch": "@@ -113,7 +113,6 @@ absl::Status ExecuteWithSameInputBuffer(\n   TF_ASSIGN_OR_RETURN(auto executable,\n                       ToyExecutable(*client, shape, std::move(set_up_aliases)));\n   xla::ExecuteOptions options;\n-  options.untuple_result = true;\n   return executable->Execute({{buffer.get(), buffer.get()}}, options).status();\n }\n "
        },
        {
            "sha": "1ce48e498067cc0b776fd273a386f86533619ffd",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_executable.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc?ref=d7b371034b32d89ab9ad28d8e3968afcf1a2110c",
            "patch": "@@ -714,7 +714,6 @@ PjRtLoadedExecutable::Execute(absl::Span<ArrayRef> args,\n   }\n \n   xla::ExecuteOptions opts;\n-  opts.untuple_result = true;\n   opts.launch_id = options.launch_id;\n   opts.use_major_to_minor_data_layout_for_callbacks = true;\n   opts.non_donatable_input_indices = options.non_donatable_input_indices;"
        },
        {
            "sha": "448ed711457e9ca717ffba4faa884d4166b6c824",
            "filename": "third_party/xla/xla/service/hlo_runner_pjrt.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner_pjrt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner_pjrt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner_pjrt.cc?ref=d7b371034b32d89ab9ad28d8e3968afcf1a2110c",
            "patch": "@@ -137,11 +137,6 @@ absl::StatusOr<std::vector<Layout>> FlattenedParameterLayouts(\n \n absl::StatusOr<ExecuteOptions> GenerateExecuteOptions(const HloModule& module) {\n   ExecuteOptions execute_options;\n-\n-  // PjRt requires untuple_result if the output is a tuple.\n-  if (module.result_shape().IsTuple()) {\n-    execute_options.untuple_result = true;\n-  }\n   return execute_options;\n }\n \n@@ -567,7 +562,6 @@ absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicated(\n                       HloRunnerPjRtExecutable::TryUnwrap(*this, executable));\n \n   xla::ExecuteOptions execute_options;\n-  execute_options.untuple_result = true;\n   return ExecuteReplicatedImpl(\n       [&](absl::Span<const std::vector<PjRtBuffer*>> argument_buffer_slices)\n           -> absl::StatusOr<\n@@ -631,7 +625,6 @@ absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicated(\n                            args = argument_buffer_slices[i], device_ptr]() {\n               std::optional<Future<>> returned_future = {};\n               xla::ExecuteOptions options;\n-              options.untuple_result = true;\n               per_replica_results[i] = pjrt_executable->ExecuteSharded(\n                   args, device_ptr, options,\n                   /*returned_future=*/returned_future,"
        },
        {
            "sha": "dc66ebbf3add1363a7a6857b1f21de628f74e37c",
            "filename": "third_party/xla/xla/tools/multihost_hlo_runner/functional_hlo_runner.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Ffunctional_hlo_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d7b371034b32d89ab9ad28d8e3968afcf1a2110c/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Ffunctional_hlo_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fmultihost_hlo_runner%2Ffunctional_hlo_runner.cc?ref=d7b371034b32d89ab9ad28d8e3968afcf1a2110c",
            "patch": "@@ -115,31 +115,17 @@ absl::Span<PjRtDevice* const> GetLocalDevices(const PjRtClient& client) {\n // ExecuteOptions::arguments_are_tupled = false.\n // This enables PjRtClient::Execute to assemble the tupled arguments from\n // a flat list of buffers.\n-// Additionally, we set ExecuteOptions::untuple_result = true if the module's\n-// output is a tuple. Thus we can use the aliased output buffer as input\n-// arguments and reuse the non-aliased argument buffers. In this mode, users may\n-// provide the argument literals as a list of tuples (for the convenience of\n-// future use cases) or a tuple literal (to support existing use cases).\n //\n // Case 2: the HLO module is compiled with\n // CompileOptions::parameter_is_tupled_arguments = false\n // and the HLO module is executed with\n // ExecuteOptions::arguments_are_tupled = false.\n-// Same as above, we set ExecuteOptions::untuple_result = true if the module's\n-// output is a tuple. This allows us to reuse on-device buffers in the same way\n-// as case 1.\n //\n // Case 3: the HLO module is compiled with\n // CompileOptions::parameter_is_tupled_arguments = false\n // and the HLO module is executed with\n // ExecuteOptions::arguments_are_tupled = false.\n // We will create new on-device buffers for each repeated execution.\n-//\n-// Irrespective of the above, if the output is a tuple with leaves mixing host\n-// and device memory spaces, we set ExecuteOptions::untuple_result = true.\n-// Otherwise PJRT cannot correctly represent these tuples, because a PjRtBuffer\n-// can only belong to one memory space. By \"untupling\", PJRT assigns a separate\n-// PjRtBuffer to each leaf.\n \n enum class ParameterType {\n   kOneTupleOfArrays = 0,\n@@ -612,7 +598,6 @@ absl::StatusOr<PerDeviceLiteralVecType> RunInternal(\n                                                 flatten_arguments));\n         argument_ptrs = CreateArgumentPointersFromDeviceBuffers(device_buffers);\n       }\n-      execute_options.untuple_result = true;\n       execute_options.launch_id = repeat + 1 + running_options.base_run_id;\n       if (running_options.execution_profiles != nullptr) {\n         execute_options.execution_profile ="
        }
    ],
    "stats": {
        "total": 36,
        "additions": 0,
        "deletions": 36
    }
}