{
    "author": "derdrdirk",
    "message": "[Autotuner] Add allocator parameter to GpuProfiler. When initializing XLA we preallocate device memory using a \"device allocator\" we should reuse this allocated memory in the new autotuner to avoid OOM.\n\nPiperOrigin-RevId: 797684492",
    "sha": "5818c1fc8192dae9f9674027704121d926f59560",
    "files": [
        {
            "sha": "1dbeadaf47f5f9276f186c47683dc001aa4ba007",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=5818c1fc8192dae9f9674027704121d926f59560",
            "patch": "@@ -547,6 +547,8 @@ cc_library(\n     srcs = [\"gpu_profiler.cc\"],\n     hdrs = [\"gpu_profiler.h\"],\n     deps = [\n+        \"//xla:executable_run_options\",\n+        \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/autotuner:profiler\",\n         \"//xla/backends/gpu/runtime:buffer_comparator\",\n@@ -556,9 +558,9 @@ cc_library(\n         \"//xla/service:shaped_buffer\",\n         \"//xla/service/gpu:gpu_executable_run_options\",\n         \"//xla/service/gpu/autotuning:redzone_buffers\",\n+        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:stream_executor_h\",\n-        \"//xla/stream_executor:stream_executor_memory_allocator\",\n         \"//xla/stream_executor/gpu:redzone_allocator\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -684,8 +686,10 @@ xla_cc_binary(\n         \"//xla/service:compiler\",\n         \"//xla/service:gpu_plugin\",\n         \"//xla/service:platform_util\",\n+        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n+        \"//xla/stream_executor:stream_executor_memory_allocator\",\n         \"//xla/stream_executor/cuda:cuda_platform_id\",\n         \"//xla/stream_executor/platform:platform_object_registry\",\n         \"//xla/tsl/platform:env\","
        },
        {
            "sha": "067d40a7e127f0f887fbc4db320df97810789cf0",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/autotuner_main.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc?ref=5818c1fc8192dae9f9674027704121d926f59560",
            "patch": "@@ -40,9 +40,11 @@ limitations under the License.\n #include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/service/platform_util.h\"\n+#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform/platform_object_registry.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n+#include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -101,7 +103,11 @@ absl::Status Autotune(HloModule& module, const std::string& autotune_cache_dir,\n   std::vector<std::unique_ptr<CodegenBackend>> backends =\n       get_codegen_backends(stream_executor, &debug_options, compiler.get());\n \n-  auto profiler = GpuProfiler::Create(stream_executor, ProfileOptions());\n+  std::unique_ptr<se::DeviceMemoryAllocator> allocator =\n+      std::make_unique<stream_executor::StreamExecutorMemoryAllocator>(\n+          stream_executor);\n+  auto profiler =\n+      GpuProfiler::Create(stream_executor, allocator.get(), ProfileOptions());\n   if (profiler == nullptr) {\n     return absl::InternalError(\"Failed to create profiler\");\n   }"
        },
        {
            "sha": "bb24067345e09492f7d5347a6af9ebc9339ecfa3",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_profiler.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 13,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.cc?ref=5818c1fc8192dae9f9674027704121d926f59560",
            "patch": "@@ -28,16 +28,19 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/backends/autotuner/profiler.h\"\n #include \"xla/backends/gpu/runtime/buffer_comparator.h\"\n+#include \"xla/executable_run_options.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/executable.h\"\n #include \"xla/service/gpu/autotuning/redzone_buffers.h\"\n #include \"xla/service/gpu/gpu_executable_run_options.h\"\n #include \"xla/service/maybe_owning_device_memory.h\"\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/service/shaped_buffer.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/gpu/redzone_allocator.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n-#include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"tsl/platform/casts.h\"\n@@ -85,20 +88,15 @@ int GetScratchBytes(const Executable* executable) {\n }  // namespace\n \n std::unique_ptr<GpuProfiler> GpuProfiler::Create(\n-    se::StreamExecutor* stream_executor, ProfileOptions options) {\n+    se::StreamExecutor* stream_executor, se::DeviceMemoryAllocator* allocator,\n+    ProfileOptions options) {\n   auto stream = stream_executor->CreateStream();\n-  auto allocator =\n-      std::make_unique<stream_executor::StreamExecutorMemoryAllocator>(\n-          stream_executor);\n   if (!stream.ok()) {\n     LOG(ERROR) << \"Failed to create stream: \" << stream.status();\n     return nullptr;\n   }\n-  return absl::WrapUnique(new GpuProfiler(\n-      stream_executor,\n-      std::make_unique<stream_executor::StreamExecutorMemoryAllocator>(\n-          stream_executor),\n-      std::move(stream.value()), options));\n+  return absl::WrapUnique(new GpuProfiler(stream_executor, allocator,\n+                                          std::move(stream.value()), options));\n }\n \n absl::StatusOr<std::unique_ptr<InputBuffers>> GpuProfiler::CreateInputBuffers(\n@@ -111,8 +109,8 @@ absl::StatusOr<std::unique_ptr<InputBuffers>> GpuProfiler::CreateInputBuffers(\n   TF_ASSIGN_OR_RETURN(\n       RedzoneBuffers buffers,\n       RedzoneBuffers::FromComputation(\n-          *executable->module().entry_computation(), allocator_.get(),\n-          stream_.get(), RedzoneBuffers::BuffersToCreate::kAllInputs,\n+          *executable->module().entry_computation(), allocator_, stream_.get(),\n+          RedzoneBuffers::BuffersToCreate::kAllInputs,\n           options_.should_init_buffers,\n           /*should_check_correctness=*/true, options_.redzone_padding_bytes));\n   auto gpu_buffers = std::make_unique<GpuInputBuffers>();\n@@ -166,7 +164,7 @@ absl::StatusOr<ExecutionOutput> GpuProfiler::Execute(\n   ExecutableRunOptions run_options;\n   run_options.set_device_ordinal(stream_executor_->device_ordinal());\n   run_options.set_stream(stream_.get());\n-  run_options.set_allocator(allocator_.get());\n+  run_options.set_allocator(allocator_);\n   run_options.set_gpu_executable_run_options(&gpu_opts);\n   run_options.set_execution_profile(profile);\n   ServiceExecutableRunOptions service_run_options(run_options);"
        },
        {
            "sha": "b757813484c129626a826b7830352e8a1ae3881c",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_profiler.h",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.h?ref=5818c1fc8192dae9f9674027704121d926f59560",
            "patch": "@@ -41,7 +41,8 @@ struct GpuInputBuffers : public InputBuffers {\n class GpuProfiler : public Profiler {\n  public:\n   static std::unique_ptr<GpuProfiler> Create(\n-      stream_executor::StreamExecutor* stream_executor, ProfileOptions options);\n+      stream_executor::StreamExecutor* stream_executor,\n+      se::DeviceMemoryAllocator* allocator, ProfileOptions options);\n \n   // The input buffers shapes are taken from the attatched HloModule to the\n   // executable.\n@@ -59,22 +60,22 @@ class GpuProfiler : public Profiler {\n                                  float rtol) override;\n \n  private:\n-  explicit GpuProfiler(\n-      stream_executor::StreamExecutor* stream_executor,\n-      std::unique_ptr<stream_executor::DeviceMemoryAllocator> allocator,\n-      std::unique_ptr<stream_executor::Stream> stream, ProfileOptions options)\n+  explicit GpuProfiler(se::StreamExecutor* stream_executor,\n+                       se::DeviceMemoryAllocator* allocator,\n+                       std::unique_ptr<se::Stream> stream,\n+                       ProfileOptions options)\n       : stream_executor_(stream_executor),\n-        allocator_(std::move(allocator)),\n+        allocator_(allocator),\n         stream_(std::move(stream)),\n         options_(options) {}\n \n   absl::StatusOr<ExecutionOutput> Execute(Executable* executable,\n                                           std::vector<ExecutionInput> inputs,\n                                           ExecutionProfile* profile);\n \n-  stream_executor::StreamExecutor* stream_executor_;\n-  std::unique_ptr<stream_executor::DeviceMemoryAllocator> allocator_;\n-  std::unique_ptr<stream_executor::Stream> stream_;\n+  se::StreamExecutor* stream_executor_;\n+  se::DeviceMemoryAllocator* allocator_;\n+  std::unique_ptr<se::Stream> stream_;\n   ProfileOptions options_;\n };\n "
        },
        {
            "sha": "bfe0da9bd697e93079b7a1ae296257e561767260",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_profiler_test.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 8,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler_test.cc?ref=5818c1fc8192dae9f9674027704121d926f59560",
            "patch": "@@ -115,8 +115,11 @@ class GpuProfilerTest : public HloHardwareIndependentTestBase {\n     std::vector<se::StreamExecutor*> executors =\n         PlatformUtil::GetStreamExecutors(platform).value();\n     stream_exec_ = executors[0];\n+    allocator_ =\n+        std::make_unique<se::StreamExecutorMemoryAllocator>(stream_exec_);\n   }\n   se::StreamExecutor* stream_exec_;\n+  std::unique_ptr<se::DeviceMemoryAllocator> allocator_;\n };\n \n TEST_F(GpuProfilerTest, ProfileWithSharedBuffersWithoutOutputBuffer) {\n@@ -134,7 +137,7 @@ TEST_F(GpuProfilerTest, ProfileWithSharedBuffersWithoutOutputBuffer) {\n \n   ProfileOptions options;\n   options.should_populate_output_buffer = false;\n-  auto profiler = GpuProfiler::Create(stream_exec_, options);\n+  auto profiler = GpuProfiler::Create(stream_exec_, allocator_.get(), options);\n   TF_ASSERT_OK_AND_ASSIGN(auto profiles, profiler->ProfileWithSharedBuffers(\n                                              std::move(executables)));\n   EXPECT_EQ(profiles.size(), 2);\n@@ -164,7 +167,8 @@ TEST_F(GpuProfilerTest, ProfileWithSharedBuffers) {\n   std::vector<std::unique_ptr<Executable>> executables;\n   executables.push_back(std::make_unique<MockExecutable>(module, 1));\n \n-  auto profiler = GpuProfiler::Create(stream_exec_, ProfileOptions());\n+  auto profiler =\n+      GpuProfiler::Create(stream_exec_, allocator_.get(), ProfileOptions());\n   TF_ASSERT_OK_AND_ASSIGN(auto profiles, profiler->ProfileWithSharedBuffers(\n                                              std::move(executables)));\n   EXPECT_THAT(profiles, ElementsAre(IsOkAndHolds(Field(\n@@ -186,7 +190,8 @@ TEST_F(GpuProfilerTest, FailingExecutablesReturnStatus) {\n       std::make_unique<MockExecutable>(module, 2000, /*should_fail=*/true));\n   executables.push_back(std::make_unique<MockExecutable>(module, 3000));\n \n-  auto profiler = GpuProfiler::Create(stream_exec_, ProfileOptions());\n+  auto profiler =\n+      GpuProfiler::Create(stream_exec_, allocator_.get(), ProfileOptions());\n   TF_ASSERT_OK_AND_ASSIGN(auto profiles, profiler->ProfileWithSharedBuffers(\n                                              std::move(executables)));\n   EXPECT_EQ(profiles.size(), 3);\n@@ -210,7 +215,8 @@ TEST_F(GpuProfilerTest, CreateInputBuffersAndProfile) {\n                           ParseAndReturnVerifiedModule(kHloModule));\n   MockExecutable mock_executable(module, 1000);\n \n-  auto profiler = GpuProfiler::Create(stream_exec_, ProfileOptions());\n+  auto profiler =\n+      GpuProfiler::Create(stream_exec_, allocator_.get(), ProfileOptions());\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<InputBuffers> buffers,\n                           profiler->CreateInputBuffers(&mock_executable));\n   TF_ASSERT_OK_AND_ASSIGN(ProfileResult profile,\n@@ -234,7 +240,7 @@ TEST_P(GpuProfilerTestWithRedzonePadding, CheckInputBuffers) {\n   MockExecutable mock_executable(module, 1000);\n   ProfileOptions options;\n   options.redzone_padding_bytes = GetParam();\n-  auto profiler = GpuProfiler::Create(stream_exec_, options);\n+  auto profiler = GpuProfiler::Create(stream_exec_, allocator_.get(), options);\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<InputBuffers> buffers,\n                           profiler->CreateInputBuffers(&mock_executable));\n   TF_EXPECT_OK(profiler->CheckInputBuffers(*buffers));\n@@ -246,7 +252,7 @@ INSTANTIATE_TEST_SUITE_P(GpuProfilerTestWithRedzonePadding,\n \n TEST_F(GpuProfilerTest, CheckOutputBufferWhenBuffersAreSame) {\n   ProfileOptions options;\n-  auto profiler = GpuProfiler::Create(stream_exec_, options);\n+  auto profiler = GpuProfiler::Create(stream_exec_, allocator_.get(), options);\n \n   TF_ASSERT_OK_AND_ASSIGN(auto stream, stream_exec_->CreateStream());\n   auto allocator =\n@@ -264,7 +270,7 @@ TEST_F(GpuProfilerTest, CheckOutputBufferWhenBuffersAreSame) {\n \n TEST_F(GpuProfilerTest, CheckOutputBufferWhenBuffersAreDifferent) {\n   ProfileOptions options;\n-  auto profiler = GpuProfiler::Create(stream_exec_, options);\n+  auto profiler = GpuProfiler::Create(stream_exec_, allocator_.get(), options);\n   TF_ASSERT_OK_AND_ASSIGN(auto stream, stream_exec_->CreateStream());\n   auto allocator =\n       std::make_unique<stream_executor::StreamExecutorMemoryAllocator>(\n@@ -304,7 +310,8 @@ ENTRY %entry_computation (transpose.562: bf16[32,120,6,512], Arg_1.2: f32[3072,5\n   TF_ASSERT_OK_AND_ASSIGN(auto gpu_executable,\n                           compiler.RunBackend(std::move(module), stream_exec_,\n                                               GpuCompiler::CompileOptions()));\n-  auto profiler = GpuProfiler::Create(stream_exec_, ProfileOptions());\n+  auto profiler =\n+      GpuProfiler::Create(stream_exec_, allocator_.get(), ProfileOptions());\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<InputBuffers> buffers,\n                           profiler->CreateInputBuffers(gpu_executable.get()));\n   TF_ASSERT_OK_AND_ASSIGN(ProfileResult profile,"
        },
        {
            "sha": "63b344a0da9ae03bc8ad0ed87768682656afef7e",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc?ref=5818c1fc8192dae9f9674027704121d926f59560",
            "patch": "@@ -256,7 +256,8 @@ absl::Status AMDGPUCompiler::AddConvAndGemmAutotuningPasses(\n         std::make_unique<CublasBackend>(stream_exec, &debug_options, this));\n     TF_ASSIGN_OR_RETURN(\n         std::unique_ptr<AutotunerPass> autotuner_pass,\n-        AutotunerPass::Create(std::move(backends), debug_options, stream_exec,\n+        AutotunerPass::Create(std::move(backends), debug_options,\n+                              options.device_allocator, stream_exec,\n                               thread_pool));\n     pipeline->AddPass(std::move(autotuner_pass));\n   } else {"
        },
        {
            "sha": "e3c58181e948d675698f6085dc31fc83d04332db",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=5818c1fc8192dae9f9674027704121d926f59560",
            "patch": "@@ -325,8 +325,10 @@ xla_test(\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n+        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor:stream_executor_memory_allocator\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:status_matchers\",\n@@ -789,6 +791,7 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/service/gpu:cublas_cudnn\",\n+        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\","
        },
        {
            "sha": "7cb21f3f666847d843fbac3ab66a78b2b10359f3",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc?ref=5818c1fc8192dae9f9674027704121d926f59560",
            "patch": "@@ -36,6 +36,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/service/gpu/cublas_cudnn.h\"\n+#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -46,11 +47,11 @@ namespace gpu {\n \n absl::StatusOr<std::unique_ptr<AutotunerPass>> AutotunerPass::Create(\n     std::vector<std::unique_ptr<CodegenBackend>> backends,\n-    const DebugOptions& debug_options,\n+    const DebugOptions& debug_options, se::DeviceMemoryAllocator* allocator,\n     stream_executor::StreamExecutor* stream_executor,\n     tsl::thread::ThreadPool* thread_pool) {\n   std::unique_ptr<GpuProfiler> profiler =\n-      GpuProfiler::Create(stream_executor, ProfileOptions());\n+      GpuProfiler::Create(stream_executor, allocator, ProfileOptions());\n \n   std::unique_ptr<AutotunerCacheInterface> cache = nullptr;\n   const std::string& cache_dir ="
        },
        {
            "sha": "2b0faa37d7dfa7fa5bdc90366c4e840f5f289bf3",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.h",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h?ref=5818c1fc8192dae9f9674027704121d926f59560",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n+#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n #include \"xla/xla.pb.h\"\n@@ -38,8 +39,8 @@ class AutotunerPass : public HloModulePass {\n  public:\n   static absl::StatusOr<std::unique_ptr<AutotunerPass>> Create(\n       std::vector<std::unique_ptr<CodegenBackend>> backends,\n-      const DebugOptions& debug_options,\n-      stream_executor::StreamExecutor* stream_executor,\n+      const DebugOptions& debug_options, se::DeviceMemoryAllocator* allocator,\n+      se::StreamExecutor* stream_executor,\n       tsl::thread::ThreadPool* thread_pool);\n \n   absl::string_view name() const override { return \"autotuner\"; }"
        },
        {
            "sha": "79eb83c001fdd7b98b8163eb1d03a4368defd8a7",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass_test.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 11,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc?ref=5818c1fc8192dae9f9674027704121d926f59560",
            "patch": "@@ -35,8 +35,10 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n #include \"xla/service/platform_util.h\"\n+#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/status_matchers.h\"\n@@ -48,7 +50,6 @@ namespace xla {\n namespace gpu {\n namespace {\n \n-using ::tsl::testing::IsOkAndHolds;\n namespace se = stream_executor;\n \n se::StreamExecutor* GpuExecutor() {\n@@ -60,9 +61,13 @@ se::StreamExecutor* GpuExecutor() {\n \n class AutotunerPassTest : public HloHardwareIndependentTestBase {\n  protected:\n-  AutotunerPassTest() : stream_executor_(GpuExecutor()) {}\n+  AutotunerPassTest()\n+      : stream_executor_(GpuExecutor()),\n+        allocator_(std::make_unique<se::StreamExecutorMemoryAllocator>(\n+            stream_executor_)) {}\n \n-  stream_executor::StreamExecutor* stream_executor_;\n+  se::StreamExecutor* stream_executor_;\n+  std::unique_ptr<se::DeviceMemoryAllocator> allocator_;\n   NVPTXCompiler compiler_;\n };\n \n@@ -100,8 +105,8 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotuned) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<AutotunerPass> pass,\n       AutotunerPass::Create(std::move(backends),\n-                            module->config().debug_options(), stream_executor_,\n-                            &thread_pool));\n+                            module->config().debug_options(), allocator_.get(),\n+                            stream_executor_, &thread_pool));\n   EXPECT_THAT(pass->Run(module.get(), /*execution_threads=*/{}),\n               tsl::testing::IsOkAndHolds(true));\n }\n@@ -145,11 +150,12 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotunedAndCached) {\n     std::vector<std::unique_ptr<CodegenBackend>> backends;\n     backends.push_back(std::make_unique<CublasBackend>(\n         stream_executor_, &module->config().debug_options(), &compiler_));\n+\n     TF_ASSERT_OK_AND_ASSIGN(\n         std::unique_ptr<AutotunerPass> pass,\n-        AutotunerPass::Create(std::move(backends),\n-                              module->config().debug_options(),\n-                              stream_executor_, &thread_pool));\n+        AutotunerPass::Create(\n+            std::move(backends), module->config().debug_options(),\n+            allocator_.get(), stream_executor_, &thread_pool));\n     EXPECT_THAT(pass->Run(module.get(), /*execution_threads=*/{}),\n                 tsl::testing::IsOkAndHolds(true));\n   }\n@@ -202,9 +208,9 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotunedAndCached) {\n         stream_executor_, &module->config().debug_options(), &compiler_));\n     TF_ASSERT_OK_AND_ASSIGN(\n         std::unique_ptr<AutotunerPass> pass2,\n-        AutotunerPass::Create(std::move(backends2),\n-                              module->config().debug_options(),\n-                              stream_executor_, &thread_pool));\n+        AutotunerPass::Create(\n+            std::move(backends2), module->config().debug_options(),\n+            allocator_.get(), stream_executor_, &thread_pool));\n     EXPECT_THAT(pass2->Run(module.get(), /*execution_threads=*/{}),\n                 tsl::testing::IsOkAndHolds(true));\n   }"
        },
        {
            "sha": "ef9c0340ae6394f37b5350fff55b5f4e87ff05d2",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5818c1fc8192dae9f9674027704121d926f59560/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=5818c1fc8192dae9f9674027704121d926f59560",
            "patch": "@@ -366,7 +366,8 @@ absl::Status NVPTXCompiler::AddConvAndGemmAutotuningPasses(\n         std::make_unique<CublasLtBackend>(stream_exec, &debug_options, this));\n     TF_ASSIGN_OR_RETURN(\n         std::unique_ptr<AutotunerPass> autotuner_pass,\n-        AutotunerPass::Create(std::move(backends), debug_options, stream_exec,\n+        AutotunerPass::Create(std::move(backends), debug_options,\n+                              options.device_allocator, stream_exec,\n                               thread_pool));\n     pipeline->AddPass(std::move(autotuner_pass));\n   } else {"
        }
    ],
    "stats": {
        "total": 127,
        "additions": 78,
        "deletions": 49
    }
}