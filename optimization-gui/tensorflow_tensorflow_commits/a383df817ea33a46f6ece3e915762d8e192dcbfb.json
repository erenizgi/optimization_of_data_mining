{
    "author": "dsharletg",
    "message": "Remove XNNPACK support\n\nThis is now mostly unused, because YNNPACK is now enabled by default in all cases. XNNPACK was only ever enabled by default for F32 dots, and only when a cost model allowed it to be used, for a subset of shapes, on AMD CPUs only.\n\nPiperOrigin-RevId: 843537066",
    "sha": "a383df817ea33a46f6ece3e915762d8e192dcbfb",
    "files": [
        {
            "sha": "8a6efe40f3a71ffc896f261033bd7864399f3115",
            "filename": "third_party/xla/build_tools/ci/build.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fbuild_tools%2Fci%2Fbuild.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fbuild_tools%2Fci%2Fbuild.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fci%2Fbuild.py?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -672,7 +672,6 @@ def nvidia_gpu_build_with_compute_capability(\n         **_DEFAULT_BAZEL_OPTIONS,\n         \"macos_minimum_os\": \"10.15\",\n         \"test_tmpdir\": \"/Volumes/BuildData/bazel_output\",\n-        \"define\": \"xnn_enable_avxvnniint8=false\",\n         \"//xla/tsl:ci_build\": True,\n     },\n     build_tag_filters=macos_tag_filter,\n@@ -708,7 +707,6 @@ def nvidia_gpu_build_with_compute_capability(\n         \"macos_minimum_os\": \"10.15\",\n         \"test_tmpdir\": \"/tmpfs/bazel_output\",\n         \"test_size_filters\": \"small,medium\",\n-        \"define\": \"xnn_enable_avxvnniint8=false\",\n         \"//xla/tsl:ci_build\": True,\n     },\n     build_tag_filters=macos_tag_filter,"
        },
        {
            "sha": "feccb0fc7cdd78570fb6edd2e58050ffe6b532b5",
            "filename": "third_party/xla/build_tools/ci/golden_commands.txt",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fbuild_tools%2Fci%2Fgolden_commands.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fbuild_tools%2Fci%2Fgolden_commands.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fci%2Fgolden_commands.txt?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -110,15 +110,15 @@ bazel analyze-profile profile.json.gz\n df -h\n bazel --version\n mkdir -p /tmpfs/bazel_output\n-bazel test --build_tag_filters=-no_oss,-gpu,-no_mac,-mac_excluded,-requires-gpu-nvidia,-requires-gpu-amd,-requires-gpu-intel --test_tag_filters=-no_oss,-gpu,-no_mac,-mac_excluded,-requires-gpu-nvidia,-requires-gpu-amd,-requires-gpu-intel --config=nonccl --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --macos_minimum_os=10.15 --test_tmpdir=/tmpfs/bazel_output --test_size_filters=small,medium --define=xnn_enable_avxvnniint8=false --//xla/tsl:ci_build -- //xla/... -//xla/hlo/experimental/... -//xla/python_api/... -//xla/python/... -//xla/service/gpu/...\n+bazel test --build_tag_filters=-no_oss,-gpu,-no_mac,-mac_excluded,-requires-gpu-nvidia,-requires-gpu-amd,-requires-gpu-intel --test_tag_filters=-no_oss,-gpu,-no_mac,-mac_excluded,-requires-gpu-nvidia,-requires-gpu-amd,-requires-gpu-intel --config=nonccl --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --macos_minimum_os=10.15 --test_tmpdir=/tmpfs/bazel_output --test_size_filters=small,medium --//xla/tsl:ci_build -- //xla/... -//xla/hlo/experimental/... -//xla/python_api/... -//xla/python/... -//xla/service/gpu/...\n bazel analyze-profile profile.json.gz\n # END BuildType.XLA_MACOS_ARM64_CPU_KOKORO\n # BEGIN BuildType.XLA_MACOS_X86_CPU_KOKORO\n sudo wget --no-verbose -O /usr/local/bin/bazel https://github.com/bazelbuild/bazelisk/releases/download/v1.11.0/bazelisk-darwin-amd64\n chmod +x /usr/local/bin/bazel\n bazel --version\n mkdir -p /Volumes/BuildData/bazel_output\n-bazel test --build_tag_filters=-no_oss,-gpu,-no_mac,-mac_excluded,-requires-gpu-nvidia,-requires-gpu-amd,-requires-gpu-intel --test_tag_filters=-no_oss,-gpu,-no_mac,-mac_excluded,-requires-gpu-nvidia,-requires-gpu-amd,-requires-gpu-intel --config=nonccl --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --macos_minimum_os=10.15 --test_tmpdir=/Volumes/BuildData/bazel_output --define=xnn_enable_avxvnniint8=false --//xla/tsl:ci_build -- //xla/... -//xla/hlo/experimental/... -//xla/python_api/... -//xla/python/... -//xla/service/gpu/...\n+bazel test --build_tag_filters=-no_oss,-gpu,-no_mac,-mac_excluded,-requires-gpu-nvidia,-requires-gpu-amd,-requires-gpu-intel --test_tag_filters=-no_oss,-gpu,-no_mac,-mac_excluded,-requires-gpu-nvidia,-requires-gpu-amd,-requires-gpu-intel --config=nonccl --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --macos_minimum_os=10.15 --test_tmpdir=/Volumes/BuildData/bazel_output --//xla/tsl:ci_build -- //xla/... -//xla/hlo/experimental/... -//xla/python_api/... -//xla/python/... -//xla/service/gpu/...\n bazel analyze-profile profile.json.gz\n # END BuildType.XLA_MACOS_X86_CPU_KOKORO\n # BEGIN BuildType.XLA_WINDOWS_X86_CPU_GITHUB_ACTIONS"
        },
        {
            "sha": "a54852db554b356f29f9fa810d5e8d4062961bca",
            "filename": "third_party/xla/build_tools/configure/configure.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fbuild_tools%2Fconfigure%2Fconfigure.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fbuild_tools%2Fconfigure%2Fconfigure.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fconfigure%2Fconfigure.py?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -452,18 +452,6 @@ def to_bazelrc_lines(\n     if dpav.clang_major_version and dpav.clang_major_version >= 19:\n       self.compiler_options.append(\"-Wno-c23-extensions\")\n \n-    # Avoid XNNPACK using `-mavxvnniint8` (needs clang-16+/gcc-13+)\n-    if (\n-        dpav.clang_major_version is not None and dpav.clang_major_version < 16\n-    ) or (dpav.gcc_major_version is not None and dpav.gcc_major_version < 13):\n-      rc.append(\"build --define=xnn_enable_avxvnniint8=false\")\n-\n-    # Avoid XNNPACK using `-mavx512fp16` (needs clang-14+/gcc-12+).\n-    if (\n-        dpav.clang_major_version is not None and dpav.clang_major_version < 14\n-    ) or (dpav.gcc_major_version is not None and dpav.gcc_major_version < 12):\n-      rc.append(\"build --define=xnn_enable_avx512fp16=false\")\n-\n     rc.append(f\"build --action_env PYTHON_BIN_PATH={self.python_bin_path}\")\n     rc.append(f\"build --python_path {self.python_bin_path}\")\n     rc.append(\"test --test_env LD_LIBRARY_PATH\")"
        },
        {
            "sha": "8eefec15ee8efbcdefeb34193ebc90007b4cf520",
            "filename": "third_party/xla/build_tools/configure/testdata/gcc.bazelrc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fbuild_tools%2Fconfigure%2Ftestdata%2Fgcc.bazelrc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fbuild_tools%2Fconfigure%2Ftestdata%2Fgcc.bazelrc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fconfigure%2Ftestdata%2Fgcc.bazelrc?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -1,6 +1,4 @@\n build --action_env GCC_HOST_COMPILER_PATH=/usr/bin/gcc\n-build --define=xnn_enable_avxvnniint8=false\n-build --define=xnn_enable_avx512fp16=false\n build --action_env PYTHON_BIN_PATH=/usr/bin/python3\n build --python_path /usr/bin/python3\n test --test_env LD_LIBRARY_PATH"
        },
        {
            "sha": "373613415c1f7c3a6fd170de58de2d221388ad76",
            "filename": "third_party/xla/build_tools/configure/testdata/nvcc_gcc.bazelrc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fbuild_tools%2Fconfigure%2Ftestdata%2Fnvcc_gcc.bazelrc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fbuild_tools%2Fconfigure%2Ftestdata%2Fnvcc_gcc.bazelrc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fconfigure%2Ftestdata%2Fnvcc_gcc.bazelrc?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -5,8 +5,6 @@ build:cuda --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=7.5\n build:cuda --repo_env HERMETIC_CUDNN_VERSION=\"9.8.0\"\n build --config nonccl\n build --action_env LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n-build --define=xnn_enable_avxvnniint8=false\n-build --define=xnn_enable_avx512fp16=false\n build --action_env PYTHON_BIN_PATH=/usr/bin/python3\n build --python_path /usr/bin/python3\n test --test_env LD_LIBRARY_PATH"
        },
        {
            "sha": "a08ecafdedae0a60b85ae94fd9b03d5b8fa5c1ad",
            "filename": "third_party/xla/tensorflow.bazelrc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Ftensorflow.bazelrc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Ftensorflow.bazelrc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Ftensorflow.bazelrc?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -274,8 +274,6 @@ common:rocm_base --copt=-Wno-gnu-offsetof-extensions\n common:rocm_base --crosstool_top=@local_config_rocm//crosstool:toolchain\n common:rocm_base --define=using_rocm_hipcc=true\n common:rocm_base --define=tensorflow_mkldnn_contraction_kernel=0\n-common:rocm_base --define=xnn_enable_avxvnniint8=false\n-common:rocm_base --define=xnn_enable_avx512fp16=false\n common:rocm_base --repo_env TF_NEED_ROCM=1\n \n common:rocm_clang_official --config=rocm_base"
        },
        {
            "sha": "690cbbcb3a3b139cb2741032d853f051b62ea8da",
            "filename": "third_party/xla/xla/backends/cpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 76,
            "changes": 77,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -120,34 +120,12 @@ onednn_graph_cc_library(\n     ],\n )\n \n+# TODO: b/467367981, this is deprecated and should be removed.\n tf_proto_library(\n     name = \"xnn_fusion_options_proto\",\n     srcs = [\"xnn_fusion_options.proto\"],\n )\n \n-cc_library(\n-    name = \"xnn_emitter\",\n-    srcs = [\"xnn_emitter.cc\"],\n-    hdrs = [\"xnn_emitter.h\"],\n-    deps = [\n-        \":xnn_support\",\n-        \"//xla:literal\",\n-        \"//xla:shape_util\",\n-        \"//xla:util\",\n-        \"//xla:xla_data_proto_cc\",\n-        \"//xla/backends/cpu/runtime/xnnpack:xnn_interop\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/tsl/platform:logging\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@XNNPACK\",\n-        \"@com_google_absl//absl/container:flat_hash_map\",\n-        \"@com_google_absl//absl/functional:any_invocable\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings:str_format\",\n-        \"@com_google_absl//absl/types:span\",\n-    ],\n-)\n-\n cc_library(\n     name = \"ynn_emitter\",\n     srcs = [\"ynn_emitter.cc\"],\n@@ -175,59 +153,6 @@ cc_library(\n     ],\n )\n \n-cc_library(\n-    name = \"xnn_gemm_config\",\n-    srcs = [\"xnn_gemm_config.cc\"],\n-    hdrs = [\"xnn_gemm_config.h\"],\n-    deps = [\n-        \"//xla:xla_data_proto_cc\",\n-        \"//xla/backends/cpu/codegen:target_machine_features\",\n-        \"//xla/backends/cpu/runtime:dot_dims\",\n-        \"@com_google_absl//absl/base:no_destructor\",\n-        \"@com_google_absl//absl/log:check\",\n-        \"@llvm-project//llvm:Target\",\n-    ],\n-)\n-\n-cc_library(\n-    name = \"xnn_support\",\n-    srcs = [\"xnn_support.cc\"],\n-    hdrs = [\"xnn_support.h\"],\n-    deps = [\n-        \":xnn_gemm_config\",\n-        \"//xla:shape_util\",\n-        \"//xla:util\",\n-        \"//xla:xla_data_proto_cc\",\n-        \"//xla/backends/cpu/codegen:target_machine_features\",\n-        \"//xla/backends/cpu/runtime:dot_dims\",\n-        \"//xla/backends/cpu/runtime/xnnpack:xnn_interop\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/service:pattern_matcher\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@XNNPACK\",\n-        \"@com_google_absl//absl/base:no_destructor\",\n-        \"@com_google_absl//absl/container:flat_hash_map\",\n-        \"@com_google_absl//absl/container:flat_hash_set\",\n-        \"@com_google_absl//absl/log\",\n-        \"@com_google_absl//absl/log:check\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings:string_view\",\n-        \"@com_google_absl//absl/types:span\",\n-    ],\n-)\n-\n-xla_cc_test(\n-    name = \"xnn_support_test\",\n-    srcs = [\"xnn_support_test.cc\"],\n-    deps = [\n-        \":xnn_support\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"@XNNPACK\",\n-        \"@com_google_absl//absl/container:flat_hash_map\",\n-        \"@com_google_googletest//:gtest_main\",\n-    ],\n-)\n-\n tf_proto_library(\n     name = \"ynn_fusion_options_proto\",\n     srcs = [\"ynn_fusion_options.proto\"],"
        },
        {
            "sha": "34e11696f7d15fa8bdbc904f6f66ae5fe00d0b94",
            "filename": "third_party/xla/xla/backends/cpu/autotuner/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2FBUILD?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -79,48 +79,6 @@ xla_cc_test(\n     ],\n )\n \n-cc_library(\n-    name = \"xnnpack_backend\",\n-    srcs = [\"xnnpack_backend.cc\"],\n-    hdrs = [\"xnnpack_backend.h\"],\n-    deps = [\n-        \":cpu_codegen_backend\",\n-        \"//xla:util\",\n-        \"//xla/backends/autotuner:codegen_backend\",\n-        \"//xla/backends/cpu:xnn_fusion_options_proto_cc\",\n-        \"//xla/backends/cpu:xnn_support\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/service:compiler\",\n-        \"//xla/service/cpu:backend_config_proto_cc\",\n-        \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/memory\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings:string_view\",\n-    ],\n-)\n-\n-xla_cc_test(\n-    name = \"xnnpack_backend_test\",\n-    srcs = [\"xnnpack_backend_test.cc\"],\n-    deps = [\n-        \":cpu_codegen_backend\",\n-        \":xnnpack_backend\",\n-        \"//xla/backends/autotuner:codegen_backend\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n-        \"//xla/service:compiler\",\n-        \"//xla/service/cpu:backend_config_proto_cc\",\n-        \"//xla/service/cpu:cpu_compiler\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:status_matchers\",\n-        \"@com_google_absl//absl/strings:string_view\",\n-        \"@com_google_googletest//:gtest_main\",\n-    ],\n-)\n-\n cc_library(\n     name = \"llvm_kernel_backend\",\n     srcs = [\"llvm_kernel_backend.cc\"],"
        },
        {
            "sha": "765a50a887cd54129e01a3ac9f4ede3cfd66aeed",
            "filename": "third_party/xla/xla/backends/cpu/autotuner/xnnpack_backend.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 124,
            "changes": 124,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fxnnpack_backend.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fxnnpack_backend.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fxnnpack_backend.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,124 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/cpu/autotuner/xnnpack_backend.h\"\n-\n-#include <memory>\n-#include <utility>\n-#include <vector>\n-\n-#include \"absl/memory/memory.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"xla/backends/autotuner/codegen_backend.h\"\n-#include \"xla/backends/cpu/xnn_fusion_options.pb.h\"\n-#include \"xla/backends/cpu/xnn_support.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/service/compiler.h\"\n-#include \"xla/service/cpu/backend_config.pb.h\"\n-#include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/util.h\"\n-\n-namespace xla::cpu {\n-\n-absl::Status CheckIfXnnFusion(const HloInstruction& instr) {\n-  if (instr.opcode() != HloOpcode::kFusion) {\n-    return xla::InvalidArgument(\n-        \"XnnpackBackend only supports fusion instructions. Received %s.\",\n-        HloOpcodeString(instr.opcode()));\n-  }\n-  if (!instr.has_backend_config()) {\n-    return xla::InvalidArgument(\"Instruction %s does not have backend config.\",\n-                                instr.ToString());\n-  }\n-\n-  TF_ASSIGN_OR_RETURN(auto backend_config,\n-                      instr.backend_config<BackendConfig>());\n-\n-  if (!backend_config.has_fusion_config()) {\n-    return xla::InvalidArgument(\n-        \"Backend config %s does not have an fusion config.\",\n-        backend_config.DebugString());\n-  }\n-\n-  if (backend_config.fusion_config().kind() != kXnnFusionKind) {\n-    return xla::InvalidArgument(\n-        \"Backend kind %s doesn't match expected kind %s.\",\n-        backend_config.fusion_config().kind(), kXnnFusionKind);\n-  }\n-\n-  return absl::OkStatus();\n-}\n-\n-absl::StatusOr<std::unique_ptr<CodegenBackend>> XnnpackBackend::Create(\n-    Compiler* compiler) {\n-  return absl::WrapUnique(new XnnpackBackend(compiler));\n-}\n-\n-bool XnnpackBackend::IsSupported(const HloInstruction& instr) {\n-  return CheckIfXnnFusion(instr).ok();\n-}\n-\n-absl::StatusOr<std::vector<std::unique_ptr<xla::BackendConfig>>>\n-XnnpackBackend::GetSupportedConfigs(const HloInstruction& instr) {\n-  TF_RETURN_IF_ERROR(CheckIfXnnFusion(instr));\n-  std::vector<std::unique_ptr<xla::BackendConfig>> configs;\n-  {\n-    XnnFusionOptions options;\n-    options.set_use_threadpool(true);\n-    auto any = std::make_unique<xla::BackendConfig>();\n-    any->PackFrom(options);\n-    configs.push_back(std::move(any));\n-  }\n-\n-  {\n-    XnnFusionOptions options;\n-    options.set_use_threadpool(false);\n-    auto any = std::make_unique<xla::BackendConfig>();\n-    any->PackFrom(options);\n-    configs.push_back(std::move(any));\n-  }\n-  return configs;\n-}\n-absl::StatusOr<std::unique_ptr<xla::BackendConfig>>\n-XnnpackBackend::GetDefaultConfig(const HloInstruction& instr) {\n-  TF_RETURN_IF_ERROR(CheckIfXnnFusion(instr));\n-  auto config = std::make_unique<XnnFusionOptions>();\n-  config->set_use_threadpool(true);\n-  auto any = std::make_unique<xla::BackendConfig>();\n-  any->PackFrom(*config);\n-  return any;\n-}\n-\n-absl::Status XnnpackBackend::ApplyConfig(HloInstruction& instr,\n-                                         const xla::BackendConfig& config) {\n-  TF_RETURN_IF_ERROR(CheckIfXnnFusion(instr));\n-  TF_ASSIGN_OR_RETURN(auto backend_config,\n-                      instr.backend_config<xla::cpu::BackendConfig>());\n-\n-  XnnFusionOptions options;\n-  config.UnpackTo(&options);\n-\n-  *backend_config.mutable_fusion_config()->mutable_xnn_fusion_options() =\n-      options;\n-\n-  TF_RETURN_IF_ERROR(instr.set_backend_config(backend_config));\n-\n-  return absl::OkStatus();\n-}\n-\n-}  // namespace xla::cpu"
        },
        {
            "sha": "71b8b8c86d8011a53c12efd1d777cb3f897881b8",
            "filename": "third_party/xla/xla/backends/cpu/autotuner/xnnpack_backend.h",
            "status": "removed",
            "additions": 0,
            "deletions": 58,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fxnnpack_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fxnnpack_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fxnnpack_backend.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,58 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_BACKENDS_CPU_AUTOTUNER_XNNPACK_BACKEND_H_\n-#define XLA_BACKENDS_CPU_AUTOTUNER_XNNPACK_BACKEND_H_\n-\n-#include <memory>\n-#include <vector>\n-\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"xla/backends/autotuner/codegen_backend.h\"\n-#include \"xla/backends/cpu/autotuner/cpu_codegen_backend.h\"\n-#include \"xla/backends/cpu/xnn_fusion_options.pb.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/service/compiler.h\"\n-\n-namespace xla::cpu {\n-\n-inline constexpr absl::string_view kXnnpackBackendName = \"xnnpack\";\n-\n-class XnnpackBackend : public CpuCodegenBackend {\n- public:\n-  static absl::StatusOr<std::unique_ptr<CodegenBackend>> Create(\n-      Compiler* compiler);\n-\n-  bool IsSupported(const HloInstruction& instr);\n-\n-  absl::StatusOr<std::vector<std::unique_ptr<xla::BackendConfig>>>\n-  GetSupportedConfigs(const HloInstruction& instr) final;\n-\n-  absl::StatusOr<std::unique_ptr<xla::BackendConfig>> GetDefaultConfig(\n-      const HloInstruction& instr) final;\n-\n-  absl::Status ApplyConfig(HloInstruction& instr,\n-                           const xla::BackendConfig& config) final;\n-\n- protected:\n-  explicit XnnpackBackend(Compiler* compiler)\n-      : CpuCodegenBackend(compiler, kXnnpackBackendName) {}\n-};\n-\n-}  // namespace xla::cpu\n-\n-#endif  // XLA_BACKENDS_CPU_AUTOTUNER_XNNPACK_BACKEND_H_"
        },
        {
            "sha": "9dea563a7b6d0a4575a2e0aa5e3c837cfc0259ca",
            "filename": "third_party/xla/xla/backends/cpu/autotuner/xnnpack_backend_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 170,
            "changes": 170,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fxnnpack_backend_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fxnnpack_backend_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fautotuner%2Fxnnpack_backend_test.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,170 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/cpu/autotuner/xnnpack_backend.h\"\n-\n-#include <memory>\n-\n-#include <gmock/gmock.h>\n-#include <gtest/gtest.h>\n-#include \"absl/status/status.h\"\n-#include \"absl/status/status_matchers.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"xla/backends/autotuner/codegen_backend.h\"\n-#include \"xla/backends/cpu/autotuner/cpu_codegen_backend.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n-#include \"xla/service/compiler.h\"\n-#include \"xla/service/cpu/backend_config.pb.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-\n-namespace xla::cpu {\n-namespace {\n-\n-constexpr absl::string_view kXnnpackFusionHlo = R\"(\n-    HloModule eltwise_f32_0\n-\n-    xnn_fusion {\n-      p0 = f32[1024,1024] parameter(0)\n-      p1 = f32[1024,1024] parameter(1)\n-      add0 = f32[1024,1024] add(p0, p1)\n-      mul0 = f32[1024,1024] multiply(add0, add0)\n-      ROOT sub = f32[1024,1024] subtract(mul0, p0)\n-    }\n-\n-    ENTRY e {\n-      p0 = f32[1024,1024] parameter(0)\n-      p1 = f32[1024,1024] parameter(1)\n-      ROOT %result = f32[1024,1024] fusion(%p0, %p1), kind=kCustom,\n-        calls=xnn_fusion,\n-        backend_config={\"fusion_config\": {\"kind\": \"__xnn_fusion\"}}\n-    }\n-  )\";\n-\n-class XnnpackBackendTest : public HloHardwareIndependentTestBase {\n- protected:\n-  void SetUp() override {\n-    TF_ASSERT_OK_AND_ASSIGN(compiler_,\n-                            CpuCodegenBackend::CreateBackendCompiler());\n-    TF_ASSERT_OK_AND_ASSIGN(backend_, XnnpackBackend::Create(compiler_.get()));\n-  }\n-  std::unique_ptr<CodegenBackend> backend_;\n-  std::unique_ptr<Compiler> compiler_;\n-};\n-\n-TEST_F(XnnpackBackendTest, NameTest) {\n-  EXPECT_THAT(backend_->name(), \"xnnpack\");\n-}\n-\n-TEST_F(XnnpackBackendTest, GetDefaultConfigTest) {\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(kXnnpackFusionHlo));\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto config, backend_->GetDefaultConfig(\n-                       *module->entry_computation()->root_instruction()));\n-  XnnFusionOptions xnn_fusion_options;\n-  config->UnpackTo(&xnn_fusion_options);\n-\n-  EXPECT_TRUE(xnn_fusion_options.use_threadpool());\n-}\n-\n-TEST_F(XnnpackBackendTest, InvalidFusionKind) {\n-  constexpr absl::string_view bad_fusion_kind_hlo = R\"(\n-    HloModule eltwise_f32_0\n-\n-    not_xnn_fusion {\n-      p0 = f32[1024,1024] parameter(0)\n-      p1 = f32[1024,1024] parameter(1)\n-      add0 = f32[1024,1024] add(p0, p1)\n-      mul0 = f32[1024,1024] multiply(add0, add0)\n-      ROOT sub = f32[1024,1024] subtract(mul0, p0)\n-    }\n-\n-    ENTRY e {\n-      p0 = f32[1024,1024] parameter(0)\n-      p1 = f32[1024,1024] parameter(1)\n-      ROOT %result = f32[1024,1024] fusion(%p0, %p1), kind=kCustom,\n-        calls=not_xnn_fusion,\n-        backend_config={fusion_config: {kind: \"__not_xnn_fusion\"}}\n-    }\n-  )\";\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(bad_fusion_kind_hlo));\n-  auto config = backend_->GetDefaultConfig(\n-      *module->entry_computation()->root_instruction());\n-\n-  EXPECT_THAT(config,\n-              absl_testing::StatusIs(\n-                  absl::StatusCode::kInvalidArgument,\n-                  testing::HasSubstr(\"Backend kind __not_xnn_fusion doesn't \"\n-                                     \"match expected kind __xnn_fusion.\")));\n-}\n-\n-TEST_F(XnnpackBackendTest, GetSupportedConfigsTest) {\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(kXnnpackFusionHlo));\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto configs, backend_->GetSupportedConfigs(\n-                        *module->entry_computation()->root_instruction()));\n-\n-  EXPECT_EQ(configs.size(), 2);\n-  XnnFusionOptions xnn_fusion_options0;\n-  configs[0]->UnpackTo(&xnn_fusion_options0);\n-  EXPECT_TRUE(xnn_fusion_options0.use_threadpool());\n-  XnnFusionOptions xnn_fusion_options1;\n-  configs[1]->UnpackTo(&xnn_fusion_options1);\n-  EXPECT_FALSE(xnn_fusion_options1.use_threadpool());\n-}\n-\n-TEST_F(XnnpackBackendTest, CompileSupportedBackends) {\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(kXnnpackFusionHlo));\n-  HloInstruction* fusion_instruction =\n-      module->entry_computation()->root_instruction();\n-  TF_ASSERT_OK_AND_ASSIGN(auto configs,\n-                          backend_->GetSupportedConfigs(*fusion_instruction));\n-  for (auto& config : configs) {\n-    TF_ASSERT_OK_AND_ASSIGN(auto executable,\n-                            backend_->Compile(*fusion_instruction, *config));\n-  }\n-}\n-\n-TEST_F(XnnpackBackendTest, EnsureConfigIsApplied) {\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(kXnnpackFusionHlo));\n-  HloInstruction* fusion_instruction =\n-      module->entry_computation()->root_instruction();\n-  TF_ASSERT_OK_AND_ASSIGN(auto configs,\n-                          backend_->GetSupportedConfigs(*fusion_instruction));\n-\n-  for (const auto& config : configs) {\n-    XnnFusionOptions xnn_fusion_options;\n-    config->UnpackTo(&xnn_fusion_options);\n-    EXPECT_TRUE(backend_->ApplyConfig(*fusion_instruction, *config).ok());\n-\n-    TF_ASSERT_OK_AND_ASSIGN(\n-        auto instruction_backend_config,\n-        fusion_instruction->backend_config<BackendConfig>());\n-\n-    EXPECT_EQ(instruction_backend_config.fusion_config()\n-                  .xnn_fusion_options()\n-                  .use_threadpool(),\n-              xnn_fusion_options.use_threadpool());\n-  }\n-}\n-\n-}  // namespace\n-}  // namespace xla::cpu"
        },
        {
            "sha": "b90dd573c2d6b7d069ec4ebccb78ae376e56703d",
            "filename": "third_party/xla/xla/backends/cpu/benchmarks/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fbenchmarks%2FBUILD?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -586,28 +586,6 @@ xla_cc_test(\n     ],\n )\n \n-xla_cc_test(\n-    name = \"xnn_fusion_benchmark_test\",\n-    srcs = [\"xnn_fusion_benchmark_test.cc\"],\n-    fail_if_no_test_linked = False,  # NOLINT=This contains benchmarks only, no tests.\n-    fail_if_no_test_selected = False,  # NOLINT=This contains benchmarks only, no tests.\n-    linkstatic = 1,  # required to override pthreadpool symbols\n-    deps = [\n-        \":hlo_benchmark_runner\",\n-        \":multi_benchmark_config\",\n-        \"//xla:literal\",\n-        \"//xla:literal_util\",\n-        \"//xla:shape_util\",\n-        \"//xla:xla_data_proto_cc\",\n-        \"//xla/tsl/platform:logging\",\n-        \"//xla/tsl/platform:test_benchmark\",\n-        \"//xla/tsl/platform:test_main\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/strings:string_view\",\n-        \"@com_google_absl//absl/types:span\",\n-    ],\n-)\n-\n xla_cc_test(\n     name = \"snapshot_loading_test\",\n     srcs = [\"snapshot_loading_test.cc\"],"
        },
        {
            "sha": "b567d7f7aefe9ee4935dd6beb41c24d24032de65",
            "filename": "third_party/xla/xla/backends/cpu/runtime/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -177,8 +177,6 @@ cc_library(\n         \"//xla:executable_run_options\",\n         \"//xla/backends/cpu/collectives:cpu_collectives\",\n         \"//xla/backends/cpu/collectives:in_process_collectives\",\n-        \"//xla/backends/cpu/runtime/xnnpack:xnn_interop\",\n-        \"//xla/backends/cpu/runtime/xnnpack:xnn_threadpool\",\n         \"//xla/ffi:execution_context\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/runtime:device_id\",\n@@ -1252,11 +1250,7 @@ cc_library(\n         \":while_thunk\",\n         \"//xla:shape_util\",\n         \"//xla:util\",\n-        \"//xla/backends/cpu:xnn_fusion_options_proto_cc\",\n         \"//xla/backends/cpu:ynn_fusion_options_proto_cc\",\n-        \"//xla/backends/cpu/runtime/xnnpack:xnn_convolution_thunk\",\n-        \"//xla/backends/cpu/runtime/xnnpack:xnn_dot_thunk\",\n-        \"//xla/backends/cpu/runtime/xnnpack:xnn_fusion_thunk\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/runtime:resource_use\",\n         \"//xla/runtime:work_group\",\n@@ -1351,9 +1345,6 @@ xla_cc_test(\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/backends/cpu/runtime/xnnpack:xnn_convolution_thunk\",\n-        \"//xla/backends/cpu/runtime/xnnpack:xnn_dot_thunk\",\n-        \"//xla/backends/cpu/runtime/xnnpack:xnn_fusion_thunk\",\n         \"//xla/ffi\",\n         \"//xla/ffi:ffi_api\",\n         \"//xla/runtime:resource_use\","
        },
        {
            "sha": "d675604b4ae202ae7129038639b4ea83019465ae",
            "filename": "third_party/xla/xla/backends/cpu/runtime/thunk.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.cc?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -29,8 +29,6 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"xla/backends/cpu/collectives/cpu_collectives.h\"\n #include \"xla/backends/cpu/collectives/in_process_collectives.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_interop.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_threadpool.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/runtime/device_id.h\"\n #include \"xla/service/cpu/cpu_executable_run_options.h\"\n@@ -91,8 +89,6 @@ absl::string_view Thunk::KindToString(Kind kind) {\n       return \"topk\";\n     case Kind::kWhile:\n       return \"while\";\n-    case Kind::kXnnFusion:\n-      return \"xnn-fusion\";\n     case Kind::kYnnFusion:\n       return \"ynn-fusion\";\n     case Kind::kOneDnnFusion:\n@@ -165,16 +161,6 @@ Thunk::CustomCallExecuteParams::CustomCallExecuteParams(\n       intra_op_thread_pool(intra_op_thread_pool),\n       ffi_execution_context(ffi_execution_context) {}\n \n-absl::StatusOr<Thunk::XnnParams> Thunk::XnnParams::Create(\n-    const ExecutableRunOptions* run_options) {\n-  TF_ASSIGN_OR_RETURN(XnnThreadpool threadpool,\n-                      CreateXnnThreadpool(run_options->intra_op_thread_pool()));\n-  return XnnParams(std::move(threadpool));\n-}\n-\n-Thunk::XnnParams::XnnParams(XnnThreadpool threadpool)\n-    : threadpool(std::move(threadpool)) {}\n-\n #ifdef XLA_YNNPACK\n absl::StatusOr<Thunk::YnnParams> Thunk::YnnParams::Create(\n     const ExecutableRunOptions* run_options) {"
        },
        {
            "sha": "0c48855c06622b7f189013bd8de3b57c4c380a08",
            "filename": "third_party/xla/xla/backends/cpu/runtime/thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 19,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.h?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -35,8 +35,6 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/buffer_allocations.h\"\n #include \"xla/backends/cpu/runtime/function_library.h\"\n #include \"xla/backends/cpu/runtime/xfeed_manager.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_interop.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_threadpool.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/ffi/execution_context.h\"\n #include \"xla/runtime/buffer_use.h\"\n@@ -91,7 +89,6 @@ class Thunk {\n     kSort,\n     kTopK,\n     kWhile,\n-    kXnnFusion,\n     kYnnFusion,\n     kOneDnnFusion,\n   };\n@@ -254,20 +251,6 @@ class Thunk {\n                             const ffi::ExecutionContext* ffi_execution_context);\n   };\n \n-  //===--------------------------------------------------------------------===//\n-  // XnnParams\n-  //===--------------------------------------------------------------------===//\n-\n-  // Parameters capturing all the details required for running XNNPACK fusions.\n-  struct XnnParams {\n-    static absl::StatusOr<XnnParams> Create(\n-        const ExecutableRunOptions* run_options);\n-\n-    XnnThreadpool threadpool = nullptr;\n-\n-    explicit XnnParams(XnnThreadpool threadpool);\n-  };\n-\n   //===--------------------------------------------------------------------===//\n   // YnnParams\n   //===--------------------------------------------------------------------===//\n@@ -284,7 +267,7 @@ class Thunk {\n   };\n #else\n   // Use XnnParams for placeholder. The parameter won't be used anyway.\n-  using YnnParams = XnnParams;\n+  struct YnnParams {};\n #endif  // XLA_YNNPACK\n \n   //===--------------------------------------------------------------------===//\n@@ -301,7 +284,6 @@ class Thunk {\n     TaskRunner* task_runner = nullptr;\n     CollectiveExecuteParams* collective_params = nullptr;\n     CustomCallExecuteParams* custom_call_params = nullptr;\n-    XnnParams* xnn_params = nullptr;\n     YnnParams* ynn_params = nullptr;\n     int64_t run_id = -1;          // -1 means no run id is set.\n     int64_t device_ordinal = -1;  // -1 means no device ordinal is set."
        },
        {
            "sha": "f4501ddce0a0c8399b8c91b1b98cfc3c9efe58b2",
            "filename": "third_party/xla/xla/backends/cpu/runtime/thunk.proto",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk.proto?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -145,6 +145,7 @@ message SortThunkProto {\n   repeated ShapeBufferAllocationSliceProto inputs_shapes = 5;\n }\n \n+// TODO: b/467367981, this is deprecated and should be removed.\n message XnnDotThunkProto {\n   DotDimensionNumbers dot_dimensions = 1;\n   ShapeBufferAllocationSliceProto lhs_buffer_shape = 2;\n@@ -295,6 +296,7 @@ message ThunkProto {\n     CallThunkProto call_thunk = 3;\n     ConditionalThunkProto conditional_thunk = 4;\n     SortThunkProto sort_thunk = 5;\n+    // TODO: b/467367981, this is deprecated and should be removed.\n     XnnFusionThunkProto xnn_fusion_thunk = 6;\n     DotThunkProto dot_thunk = 7;\n     RngGetAndUpdateStateThunkProto rng_get_and_update_state_thunk = 8;"
        },
        {
            "sha": "4f5255b2681b2ab7018d22dbaea31d4919cdcade",
            "filename": "third_party/xla/xla/backends/cpu/runtime/thunk_proto_serdes.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 198,
            "changes": 199,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_proto_serdes.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_proto_serdes.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_proto_serdes.cc?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -61,10 +61,6 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/thunk.pb.h\"\n #include \"xla/backends/cpu/runtime/topk_thunk.h\"\n #include \"xla/backends/cpu/runtime/while_thunk.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_convolution_thunk.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_dot_thunk.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk.h\"\n-#include \"xla/backends/cpu/xnn_fusion_options.pb.h\"\n #include \"xla/backends/cpu/ynn_fusion_options.pb.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n@@ -125,20 +121,6 @@ ProtoCollectiveThunkToCollectiveThunkKind(const CollectiveThunkProto& proto) {\n   }\n }\n \n-static absl::StatusOr<XnnFusionThunk::XnnFusionKind>\n-ProtoXnnFusionThunkToXnnFusionThunkKind(const XnnFusionThunkProto& proto) {\n-  switch (proto.impl_case()) {\n-    case XnnFusionThunkProto::ImplCase::kXnnFusionThunk:\n-      return XnnFusionThunk::XnnFusionKind::kFusion;\n-    case XnnFusionThunkProto::ImplCase::kXnnDotThunk:\n-      return XnnFusionThunk::XnnFusionKind::kDot;\n-    case XnnFusionThunkProto::ImplCase::kXnnConvolutionThunk:\n-      return XnnFusionThunk::XnnFusionKind::kConvolution;\n-    case XnnFusionThunkProto::ImplCase::IMPL_NOT_SET:\n-      return Internal(\"XNN fusion thunk kind not set.\");\n-  }\n-}\n-\n static absl::StatusOr<Thunk::Kind> ProtoThunkToThunkKind(\n     const ThunkProto& proto) {\n   switch (proto.impl_case()) {\n@@ -173,7 +155,7 @@ static absl::StatusOr<Thunk::Kind> ProtoThunkToThunkKind(\n     case ThunkProto::ImplCase::kWhileThunk:\n       return Thunk::Kind::kWhile;\n     case ThunkProto::ImplCase::kXnnFusionThunk:\n-      return Thunk::Kind::kXnnFusion;\n+      return Internal(\"Thunk kind kXnnFusionThunk is deprecated.\");\n     case ThunkProto::ImplCase::kPartitionIdThunk:\n       return Thunk::Kind::kPartitionId;\n     case ThunkProto::ImplCase::kReplicaIdThunk:\n@@ -756,66 +738,6 @@ static absl::Status ToProto(const YnnFusionThunk& thunk, ThunkProto& proto) {\n }\n #endif  // XLA_YNNPACK\n \n-static absl::Status ToProto(const XnnFusionThunk& thunk, ThunkProto& proto) {\n-  // TODO(basioli) XnnFusionThunk is not serializable because it contains\n-  // a builder function that is not serializable.\n-  // This would require a serialization of the XNNPACK subgraph.\n-  return absl::UnimplementedError(\"XnnFusionThunk is not serializable.\");\n-}\n-\n-static absl::Status ToProto(const XnnDotThunk& thunk, ThunkProto& proto) {\n-  XnnDotThunkProto* xnn_dot_thunk_proto =\n-      proto.mutable_xnn_fusion_thunk()->mutable_xnn_dot_thunk();\n-  *xnn_dot_thunk_proto->mutable_dot_dimensions() = thunk.dot_dimensions();\n-  TF_RETURN_IF_ERROR(SerializeSliceShapeIntoProto(\n-      thunk.dot_slices().lhs_buffer, thunk.dot_slices().lhs_shape,\n-      xnn_dot_thunk_proto->mutable_lhs_buffer_shape()));\n-  TF_RETURN_IF_ERROR(SerializeSliceShapeIntoProto(\n-      thunk.dot_slices().rhs_buffer, thunk.dot_slices().rhs_shape,\n-      xnn_dot_thunk_proto->mutable_rhs_buffer_shape()));\n-  TF_RETURN_IF_ERROR(SerializeSliceShapeIntoProto(\n-      thunk.dot_slices().out_buffer, thunk.dot_slices().out_shape,\n-      xnn_dot_thunk_proto->mutable_out_buffer_shape()));\n-  proto.mutable_xnn_fusion_thunk()->mutable_options()->set_use_threadpool(\n-      thunk.options().use_threadpool);\n-  xnn_dot_thunk_proto->set_capture_rhs(thunk.capture_rhs());\n-  return absl::OkStatus();\n-}\n-\n-static absl::Status ToProto(const XnnConvolutionThunk& thunk,\n-                            ThunkProto& proto) {\n-  XnnConvolutionThunkProto* convolution_thunk_proto =\n-      proto.mutable_xnn_fusion_thunk()->mutable_xnn_convolution_thunk();\n-\n-  const std::string dnums_as_str = thunk.dnums().SerializeAsString();\n-  convolution_thunk_proto->mutable_dimension_numbers()->ParseFromString(\n-      dnums_as_str);\n-\n-  const std::string window_as_str = thunk.window().SerializeAsString();\n-  convolution_thunk_proto->mutable_window()->ParseFromString(window_as_str);\n-\n-  convolution_thunk_proto->set_feature_group_count(thunk.feature_group_count());\n-\n-  const ConvolutionSlices& convolution_slices = thunk.convolution_slices();\n-\n-  TF_RETURN_IF_ERROR(SerializeSliceShapeIntoProto(\n-      convolution_slices.input_buffer, convolution_slices.input_shape,\n-      convolution_thunk_proto->mutable_input_buffer_shape()));\n-\n-  TF_RETURN_IF_ERROR(SerializeSliceShapeIntoProto(\n-      convolution_slices.output_buffer, convolution_slices.output_shape,\n-      convolution_thunk_proto->mutable_output_buffer_shape()));\n-\n-  TF_RETURN_IF_ERROR(SerializeSliceShapeIntoProto(\n-      convolution_slices.kernel_buffer, convolution_slices.kernel_shape,\n-      convolution_thunk_proto->mutable_kernel_buffer_shape()));\n-\n-  proto.mutable_xnn_fusion_thunk()->mutable_options()->set_use_threadpool(\n-      thunk.options().use_threadpool);\n-\n-  return absl::OkStatus();\n-}\n-\n static absl::Status ToProto(const FftThunk& thunk, ThunkProto& proto) {\n   FftThunkProto* fft_thunk_proto = proto.mutable_fft_thunk();\n \n@@ -983,25 +905,6 @@ absl::StatusOr<ThunkProto> ThunkSerDesProtobuf::ToProto(\n       TF_RETURN_IF_ERROR(\n           ::xla::cpu::ToProto(tsl::down_cast<const WhileThunk&>(thunk), proto));\n       break;\n-    case Thunk::Kind::kXnnFusion: {\n-      const XnnFusionThunk& xnn_fusion_thunk =\n-          tsl::down_cast<const XnnFusionThunk&>(thunk);\n-      switch (xnn_fusion_thunk.xnn_fusion_kind()) {\n-        case XnnFusionThunk::XnnFusionKind::kFusion:\n-          TF_RETURN_IF_ERROR(::xla::cpu::ToProto(\n-              tsl::down_cast<const XnnFusionThunk&>(thunk), proto));\n-          break;\n-        case XnnFusionThunk::XnnFusionKind::kDot:\n-          TF_RETURN_IF_ERROR(::xla::cpu::ToProto(\n-              tsl::down_cast<const XnnDotThunk&>(thunk), proto));\n-          break;\n-        case XnnFusionThunk::XnnFusionKind::kConvolution:\n-          TF_RETURN_IF_ERROR(::xla::cpu::ToProto(\n-              tsl::down_cast<const XnnConvolutionThunk&>(thunk), proto));\n-          break;\n-      }\n-      break;\n-    }\n     case Thunk::Kind::kPartitionId:\n       TF_RETURN_IF_ERROR(::xla::cpu::ToProto(\n           static_cast<const PartitionIdThunk&>(\n@@ -1632,93 +1535,6 @@ static absl::StatusOr<std::unique_ptr<YnnFusionThunk>> YnnFusionThunkFromProto(\n }\n #endif  // XLA_YNNPACK\n \n-static absl::StatusOr<std::unique_ptr<XnnFusionThunk>> XnnFusionThunkFromProto(\n-    const ThunkProto& proto,\n-    const std::vector<BufferAllocation>& buffer_allocations) {\n-  return absl::UnimplementedError(\"XnnFusionThunkFromProto is not implemented\");\n-}\n-\n-static absl::StatusOr<std::unique_ptr<XnnDotThunk>> XnnDotThunkFromProto(\n-    const ThunkProto& proto,\n-    const std::vector<BufferAllocation>& buffer_allocations) {\n-  TF_ASSIGN_OR_RETURN(Thunk::Info info, ThunkInfoFromProto(proto.info()));\n-\n-  XnnDotThunk::Options options = {\n-      proto.xnn_fusion_thunk().options().use_threadpool(),\n-  };\n-\n-  TF_ASSIGN_OR_RETURN(\n-      auto lhs_slice_shape,\n-      DeserializeSliceShapeFromProto(\n-          proto.xnn_fusion_thunk().xnn_dot_thunk().lhs_buffer_shape(),\n-          buffer_allocations));\n-\n-  TF_ASSIGN_OR_RETURN(\n-      auto rhs_slice_shape,\n-      DeserializeSliceShapeFromProto(\n-          proto.xnn_fusion_thunk().xnn_dot_thunk().rhs_buffer_shape(),\n-          buffer_allocations));\n-  TF_ASSIGN_OR_RETURN(\n-      auto out_slice_shape,\n-      DeserializeSliceShapeFromProto(\n-          proto.xnn_fusion_thunk().xnn_dot_thunk().out_buffer_shape(),\n-          buffer_allocations));\n-\n-  const auto& [lhs_buffer, lhs_shape] = lhs_slice_shape;\n-  const auto& [rhs_buffer, rhs_shape] = rhs_slice_shape;\n-  const auto& [out_buffer, out_shape] = out_slice_shape;\n-\n-  bool capture_rhs = proto.xnn_fusion_thunk().xnn_dot_thunk().capture_rhs();\n-\n-  return XnnDotThunk::Create(\n-      std::move(options), std::move(info),\n-      proto.xnn_fusion_thunk().xnn_dot_thunk().dot_dimensions(), lhs_buffer,\n-      lhs_shape, rhs_buffer, rhs_shape, out_buffer, out_shape, capture_rhs);\n-}\n-\n-static absl::StatusOr<std::unique_ptr<XnnConvolutionThunk>>\n-XnnConvolutionThunkFromProto(\n-    const ThunkProto& proto,\n-    const std::vector<BufferAllocation>& buffer_allocations) {\n-  TF_ASSIGN_OR_RETURN(Thunk::Info info, ThunkInfoFromProto(proto.info()));\n-\n-  XnnConvolutionThunk::Options options = {\n-      proto.xnn_fusion_thunk().options().use_threadpool(),\n-  };\n-\n-  const auto& conv_proto = proto.xnn_fusion_thunk().xnn_convolution_thunk();\n-\n-  // Dimension numbers.\n-  ConvolutionDimensionNumbers dnums = conv_proto.dimension_numbers();\n-\n-  // Window.\n-  Window window = conv_proto.window();\n-\n-  // Feature group count.\n-  int64_t feature_group_count = conv_proto.feature_group_count();\n-\n-  TF_ASSIGN_OR_RETURN(auto input_slice_shape,\n-                      DeserializeSliceShapeFromProto(\n-                          conv_proto.input_buffer_shape(), buffer_allocations));\n-  TF_ASSIGN_OR_RETURN(\n-      auto kernel_slice_shape,\n-      DeserializeSliceShapeFromProto(conv_proto.kernel_buffer_shape(),\n-                                     buffer_allocations));\n-  TF_ASSIGN_OR_RETURN(\n-      auto output_slice_shape,\n-      DeserializeSliceShapeFromProto(conv_proto.output_buffer_shape(),\n-                                     buffer_allocations));\n-\n-  const auto& [input_buffer, input_shape] = input_slice_shape;\n-  const auto& [kernel_buffer, kernel_shape] = kernel_slice_shape;\n-  const auto& [output_buffer, output_shape] = output_slice_shape;\n-\n-  return XnnConvolutionThunk::Create(\n-      std::move(options), std::move(info), std::move(input_buffer), input_shape,\n-      std::move(kernel_buffer), kernel_shape, std::move(output_buffer),\n-      output_shape, dnums, window, feature_group_count);\n-}\n-\n static absl::StatusOr<std::unique_ptr<Thunk>> PartitionIdThunkFromProto(\n     const ThunkProto& proto,\n     const std::vector<BufferAllocation>& buffer_allocations) {\n@@ -1813,19 +1629,6 @@ absl::StatusOr<std::unique_ptr<Thunk>> ThunkSerDesProtobuf::FromProto(\n       return TopKThunkFromProto(proto, *buffer_allocations_);\n     case Thunk::Kind::kWhile:\n       return WhileThunkFromProto(proto, hlo_module_, buffer_allocations_);\n-    case Thunk::Kind::kXnnFusion: {\n-      TF_ASSIGN_OR_RETURN(\n-          auto xnn_fusion_kind,\n-          ProtoXnnFusionThunkToXnnFusionThunkKind(proto.xnn_fusion_thunk()));\n-      switch (xnn_fusion_kind) {\n-        case XnnFusionThunk::XnnFusionKind::kFusion:\n-          return XnnFusionThunkFromProto(proto, *buffer_allocations_);\n-        case XnnFusionThunk::XnnFusionKind::kDot:\n-          return XnnDotThunkFromProto(proto, *buffer_allocations_);\n-        case XnnFusionThunk::XnnFusionKind::kConvolution:\n-          return XnnConvolutionThunkFromProto(proto, *buffer_allocations_);\n-      }\n-    }\n     case Thunk::Kind::kPartitionId:\n       return PartitionIdThunkFromProto(proto, *buffer_allocations_);\n     case Thunk::Kind::kReplicaId:"
        },
        {
            "sha": "9fd66395fa1a286c3340e2d219378006c8f23f55",
            "filename": "third_party/xla/xla/backends/cpu/runtime/thunk_sequence_serdes_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 202,
            "changes": 202,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_sequence_serdes_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_sequence_serdes_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_sequence_serdes_test.cc?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -59,9 +59,6 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/thunk_testlib.h\"\n #include \"xla/backends/cpu/runtime/topk_thunk.h\"\n #include \"xla/backends/cpu/runtime/while_thunk.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_convolution_thunk.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_dot_thunk.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk.h\"\n #include \"xla/ffi/ffi.h\"\n #include \"xla/ffi/ffi_api.h\"\n #include \"xla/literal.h\"\n@@ -185,9 +182,6 @@ class ThunkSequenceSerdesTest : public ::testing::Test {\n     TF_ASSIGN_OR_RETURN(thunk_sequence.emplace_back(), CreateTopKThunk());\n     TF_ASSIGN_OR_RETURN(thunk_sequence.emplace_back(), CreateWhileThunk());\n     TF_ASSIGN_OR_RETURN(thunk_sequence.emplace_back(), CreateWhileThunk(1));\n-    TF_ASSIGN_OR_RETURN(thunk_sequence.emplace_back(), CreateXnnDotThunk());\n-    TF_ASSIGN_OR_RETURN(thunk_sequence.emplace_back(),\n-                        CreateXnnConvolutionThunk());\n     TF_ASSIGN_OR_RETURN(thunk_sequence.emplace_back(), CreateKernelThunk());\n     TF_ASSIGN_OR_RETURN(thunk_sequence.emplace_back(),\n                         CreateConvolutionThunk());\n@@ -606,79 +600,6 @@ class ThunkSequenceSerdesTest : public ::testing::Test {\n         /*trip_count=*/trip_count);\n   }\n \n-  absl::StatusOr<std::unique_ptr<Thunk>> CreateXnnDotThunk() {\n-    TF_RETURN_IF_ERROR(AddBufferAllocations(3));\n-    DotDimensionNumbers dot_dimensions;\n-    dot_dimensions.add_lhs_contracting_dimensions(1);\n-    dot_dimensions.add_rhs_contracting_dimensions(0);\n-    return XnnDotThunk::Create(\n-        XnnFusionThunk::Options(), Thunk::Info(),\n-        /*dot_dimensions=*/dot_dimensions,\n-        /*lhs_buffer=*/\n-        CreateBufferAllocationSlice(\n-            buffer_allocations_[buffer_allocations_.size() - 3]),\n-        /*lhs_shape=*/literals_[buffer_allocations_.size() - 3].shape(),\n-        /*rhs_buffer=*/\n-        CreateBufferAllocationSlice(\n-            buffer_allocations_[buffer_allocations_.size() - 2]),\n-        /*rhs_shape=*/literals_[buffer_allocations_.size() - 2].shape(),\n-        /*out_buffer=*/\n-        CreateBufferAllocationSlice(\n-            buffer_allocations_[buffer_allocations_.size() - 1]),\n-        /*out_shape=*/literals_[buffer_allocations_.size() - 1].shape(), true);\n-  }\n-\n-  absl::StatusOr<std::unique_ptr<Thunk>> CreateXnnConvolutionThunk() {\n-    std::vector<int64_t> input_dims = {1, 8, 8, 16};\n-    std::vector<int64_t> kernel_dims = {32, 1, 1, 16};\n-    std::vector<int64_t> output_dims = {1, 8, 8, 32};\n-\n-    // Convolution rank inferred from the input dimensions.\n-    int convolution_rank = input_dims.size() - 2;\n-\n-    // Convolution parameters.\n-    ConvolutionDimensionNumbers conv_dims =\n-        MakeConvolutionDimensionNumbers(convolution_rank);\n-    Window window = MakeWindow(convolution_rank);\n-\n-    // Adjust kernel dimensions for XNNPACK.\n-    conv_dims.set_kernel_input_feature_dimension(3);\n-    conv_dims.set_kernel_output_feature_dimension(0);\n-    conv_dims.set_kernel_spatial_dimensions(0, 1);\n-    conv_dims.set_kernel_spatial_dimensions(1, 2);\n-\n-    // Actual data.\n-    literals_.push_back(\n-        LiteralUtil::CreateFull<float>(input_dims, 0.0));  // input\n-    literals_.push_back(\n-        LiteralUtil::CreateFull<float>(kernel_dims, 0.0));  // kernel\n-    literals_.push_back(\n-        LiteralUtil::CreateFull<float>(output_dims, 0.0));  // output\n-\n-    TF_RETURN_IF_ERROR(buffer_allocations_.push_back(CreateBufferAllocation(\n-        buffer_allocations_.size(), literals_[literals_.size() - 3])));\n-    TF_RETURN_IF_ERROR(buffer_allocations_.push_back(CreateBufferAllocation(\n-        buffer_allocations_.size(), literals_[literals_.size() - 2])));\n-    TF_RETURN_IF_ERROR(buffer_allocations_.push_back(CreateBufferAllocation(\n-        buffer_allocations_.size(), literals_[literals_.size() - 1])));\n-\n-    return XnnConvolutionThunk::Create(\n-        XnnFusionThunk::Options(), Thunk::Info(),\n-        /*input_buffer=*/\n-        CreateBufferAllocationSlice(\n-            buffer_allocations_[buffer_allocations_.size() - 3]),\n-        /*input_shape=*/literals_[buffer_allocations_.size() - 3].shape(),\n-        /*kernel_buffer=*/\n-        CreateBufferAllocationSlice(\n-            buffer_allocations_[buffer_allocations_.size() - 2]),\n-        /*kernel_shape=*/literals_[buffer_allocations_.size() - 2].shape(),\n-        /*output_buffer=*/\n-        CreateBufferAllocationSlice(\n-            buffer_allocations_[buffer_allocations_.size() - 1]),\n-        /*output_shape=*/literals_[buffer_allocations_.size() - 1].shape(),\n-        conv_dims, window, /*feature_group_count=*/1);\n-  }\n-\n   absl::StatusOr<std::unique_ptr<Thunk>> CreateKernelThunk() {\n     TF_RETURN_IF_ERROR(AddBufferAllocations(2));\n     return KernelThunk::Create(\n@@ -1107,13 +1028,6 @@ class ThunkSequenceSerdesTest : public ::testing::Test {\n            thunk_1.trip_count() == thunk_2.trip_count();\n   }\n \n-  bool VerifyXnnFusionThunkEquality(const XnnFusionThunk& thunk_1,\n-                                    const XnnFusionThunk& thunk_2) {\n-    // TODO(basioli) assume this is always false until we implement\n-    // serialization of XnnFusionThunk.\n-    return false;\n-  }\n-\n #ifdef XLA_YNNPACK\n   bool VerifyYnnFusionThunkEquality(const YnnFusionThunk& thunk_1,\n                                     const YnnFusionThunk& thunk_2) {\n@@ -1123,98 +1037,6 @@ class ThunkSequenceSerdesTest : public ::testing::Test {\n   }\n #endif  // XLA_YNNPACK\n \n-  bool VerifyXnnDotThunkEquality(const XnnDotThunk& thunk_1,\n-                                 const XnnDotThunk& thunk_2) {\n-    const bool are_dot_dimensions_equal =\n-        absl::c_equal(thunk_1.dot_dimensions().lhs_batch_dimensions(),\n-                      thunk_2.dot_dimensions().lhs_batch_dimensions()) &&\n-        absl::c_equal(thunk_1.dot_dimensions().rhs_batch_dimensions(),\n-                      thunk_2.dot_dimensions().rhs_batch_dimensions()) &&\n-        absl::c_equal(thunk_1.dot_dimensions().lhs_contracting_dimensions(),\n-                      thunk_2.dot_dimensions().lhs_contracting_dimensions()) &&\n-        absl::c_equal(thunk_1.dot_dimensions().rhs_contracting_dimensions(),\n-                      thunk_2.dot_dimensions().rhs_contracting_dimensions());\n-\n-    const bool are_options_equal =\n-        thunk_1.options().use_threadpool == thunk_2.options().use_threadpool;\n-\n-    const bool is_capturing_rhs_equal =\n-        thunk_1.capture_rhs() == thunk_2.capture_rhs();\n-\n-    return are_options_equal && are_dot_dimensions_equal &&\n-           is_capturing_rhs_equal &&\n-           VerifySliceShapeEquality(thunk_1.dot_slices().lhs_buffer,\n-                                    thunk_1.dot_slices().lhs_shape,\n-                                    thunk_2.dot_slices().lhs_buffer,\n-                                    thunk_2.dot_slices().lhs_shape) &&\n-           VerifySliceShapeEquality(thunk_1.dot_slices().rhs_buffer,\n-                                    thunk_1.dot_slices().rhs_shape,\n-                                    thunk_2.dot_slices().rhs_buffer,\n-                                    thunk_2.dot_slices().rhs_shape) &&\n-           VerifySliceShapeEquality(\n-               thunk_1.dot_slices().out_buffer, thunk_1.dot_slices().out_shape,\n-               thunk_2.dot_slices().out_buffer, thunk_2.dot_slices().out_shape);\n-  }\n-\n-  bool VerifyXnnConvolutionThunkEquality(const XnnConvolutionThunk& thunk_1,\n-                                         const XnnConvolutionThunk& thunk_2) {\n-    const bool are_dnums_equal =\n-        absl::c_equal(thunk_1.dnums().input_spatial_dimensions(),\n-                      thunk_2.dnums().input_spatial_dimensions()) &&\n-        absl::c_equal(thunk_1.dnums().kernel_spatial_dimensions(),\n-                      thunk_2.dnums().kernel_spatial_dimensions()) &&\n-        absl::c_equal(thunk_1.dnums().output_spatial_dimensions(),\n-                      thunk_2.dnums().output_spatial_dimensions()) &&\n-        thunk_1.dnums().input_batch_dimension() ==\n-            thunk_2.dnums().input_batch_dimension() &&\n-        thunk_1.dnums().input_feature_dimension() ==\n-            thunk_2.dnums().input_feature_dimension() &&\n-        thunk_1.dnums().kernel_input_feature_dimension() ==\n-            thunk_2.dnums().kernel_input_feature_dimension() &&\n-        thunk_1.dnums().kernel_output_feature_dimension() ==\n-            thunk_2.dnums().kernel_output_feature_dimension() &&\n-        thunk_1.dnums().output_batch_dimension() ==\n-            thunk_2.dnums().output_batch_dimension() &&\n-        thunk_1.dnums().output_feature_dimension() ==\n-            thunk_2.dnums().output_feature_dimension();\n-\n-    const bool are_options_equal =\n-        thunk_1.options().use_threadpool == thunk_2.options().use_threadpool;\n-\n-    const bool are_windows_equal = absl::c_equal(\n-        thunk_1.window().dimensions(), thunk_2.window().dimensions(),\n-        [](const WindowDimension& window_dimension_1,\n-           const WindowDimension& window_dimension_2) {\n-          return window_dimension_1.size() == window_dimension_2.size() &&\n-                 window_dimension_1.stride() == window_dimension_2.stride() &&\n-                 window_dimension_1.padding_low() ==\n-                     window_dimension_2.padding_low() &&\n-                 window_dimension_1.padding_high() ==\n-                     window_dimension_2.padding_high() &&\n-                 window_dimension_1.window_dilation() ==\n-                     window_dimension_2.window_dilation() &&\n-                 window_dimension_1.base_dilation() ==\n-                     window_dimension_2.base_dilation() &&\n-                 window_dimension_1.window_reversal() ==\n-                     window_dimension_2.window_reversal();\n-        });\n-\n-    return are_dnums_equal && are_windows_equal && are_options_equal &&\n-           thunk_1.feature_group_count() == thunk_2.feature_group_count() &&\n-           VerifySliceShapeEquality(thunk_1.convolution_slices().input_buffer,\n-                                    thunk_1.convolution_slices().input_shape,\n-                                    thunk_2.convolution_slices().input_buffer,\n-                                    thunk_2.convolution_slices().input_shape);\n-    VerifySliceShapeEquality(thunk_1.convolution_slices().kernel_buffer,\n-                             thunk_1.convolution_slices().kernel_shape,\n-                             thunk_2.convolution_slices().kernel_buffer,\n-                             thunk_2.convolution_slices().kernel_shape);\n-    VerifySliceShapeEquality(thunk_1.convolution_slices().output_buffer,\n-                             thunk_1.convolution_slices().output_shape,\n-                             thunk_2.convolution_slices().output_buffer,\n-                             thunk_2.convolution_slices().output_shape);\n-  }\n-\n   bool VerifyKernelThunkEquality(const KernelThunkBase& thunk_1,\n                                  const KernelThunkBase& thunk_2) {\n     return thunk_1.kernel_name() == thunk_2.kernel_name() &&\n@@ -1408,30 +1230,6 @@ class ThunkSequenceSerdesTest : public ::testing::Test {\n         return VerifyWhileThunkEquality(\n             tsl::down_cast<const WhileThunk&>(thunk_1),\n             tsl::down_cast<const WhileThunk&>(thunk_2));\n-      case Thunk::Kind::kXnnFusion: {\n-        const XnnFusionThunk& xnn_fusion_thunk_1 =\n-            tsl::down_cast<const XnnFusionThunk&>(thunk_1);\n-        const XnnFusionThunk& xnn_fusion_thunk_2 =\n-            tsl::down_cast<const XnnFusionThunk&>(thunk_2);\n-        if (xnn_fusion_thunk_1.xnn_fusion_kind() !=\n-            xnn_fusion_thunk_2.xnn_fusion_kind()) {\n-          return false;\n-        }\n-        switch (xnn_fusion_thunk_1.xnn_fusion_kind()) {\n-          case XnnFusionThunk::XnnFusionKind::kFusion:\n-            return VerifyXnnFusionThunkEquality(\n-                tsl::down_cast<const XnnFusionThunk&>(thunk_1),\n-                tsl::down_cast<const XnnFusionThunk&>(thunk_2));\n-          case XnnFusionThunk::XnnFusionKind::kDot:\n-            return VerifyXnnDotThunkEquality(\n-                tsl::down_cast<const XnnDotThunk&>(thunk_1),\n-                tsl::down_cast<const XnnDotThunk&>(thunk_2));\n-          case XnnFusionThunk::XnnFusionKind::kConvolution:\n-            return VerifyXnnConvolutionThunkEquality(\n-                tsl::down_cast<const XnnConvolutionThunk&>(thunk_1),\n-                tsl::down_cast<const XnnConvolutionThunk&>(thunk_2));\n-        }\n-      }\n       case Thunk::Kind::kYnnFusion: {\n #ifdef XLA_YNNPACK\n         const YnnFusionThunk& ynn_fusion_thunk_1 ="
        },
        {
            "sha": "dc32dac687585b26ee3d585f47e46ed2ff6bf850",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/BUILD",
            "status": "removed",
            "additions": 0,
            "deletions": 213,
            "changes": 213,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2FBUILD?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,213 +0,0 @@\n-load(\"//xla:xla.default.bzl\", \"xla_cc_test\")\n-load(\"//xla/tsl/platform:rules_cc.bzl\", \"cc_library\")\n-\n-package(\n-    # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n-    default_visibility = [\":friends\"],\n-    licenses = [\"notice\"],\n-)\n-\n-package_group(\n-    name = \"friends\",\n-    includes = [\n-        \"//xla:friends\",\n-    ],\n-)\n-\n-cc_library(\n-    name = \"xnn_interop\",\n-    srcs = [\"xnn_interop.cc\"],\n-    hdrs = [\"xnn_interop.h\"],\n-    deps = [\n-        \"//xla:shape_util\",\n-        \"//xla:util\",\n-        \"//xla/tsl/platform:logging\",\n-        \"@XNNPACK\",\n-        \"@com_google_absl//absl/base:core_headers\",\n-        \"@com_google_absl//absl/functional:function_ref\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n-    ],\n-)\n-\n-cc_library(\n-    name = \"xnn_threadpool\",\n-    srcs = [\"xnn_threadpool.cc\"],\n-    hdrs = [\"xnn_threadpool.h\"],\n-    deps = [\n-        \":xnn_interop\",\n-        \"@XNNPACK\",\n-        \"@com_google_absl//absl/base:core_headers\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@eigen_archive//:eigen3\",\n-    ],\n-)\n-\n-cc_library(\n-    name = \"xnn_convolution_thunk\",\n-    srcs = [\"xnn_convolution_thunk.cc\"],\n-    hdrs = [\"xnn_convolution_thunk.h\"],\n-    deps = [\n-        \":xnn_fusion_thunk\",\n-        \":xnn_interop\",\n-        \"//xla:shape_util\",\n-        \"//xla:util\",\n-        \"//xla:xla_data_proto_cc\",\n-        \"//xla/backends/cpu/runtime:convolution_dims\",\n-        \"//xla/backends/cpu/runtime:thunk\",\n-        \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_address\",\n-        \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:logging\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@XNNPACK\",\n-        \"@com_google_absl//absl/memory\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/strings:str_format\",\n-        \"@com_google_absl//absl/types:span\",\n-    ],\n-)\n-\n-xla_cc_test(\n-    name = \"xnn_convolution_thunk_test\",\n-    srcs = [\"xnn_convolution_thunk_test.cc\"],\n-    deps = [\n-        \":xnn_convolution_thunk\",\n-        \":xnn_interop\",\n-        \":xnn_threadpool\",\n-        \"//xla:error_spec\",\n-        \"//xla:literal\",\n-        \"//xla:literal_util\",\n-        \"//xla:shape_util\",\n-        \"//xla:xla_data_proto_cc\",\n-        \"//xla/backends/cpu/runtime:buffer_allocations\",\n-        \"//xla/backends/cpu/runtime:thunk\",\n-        \"//xla/backends/cpu/runtime:thunk_testlib\",\n-        \"//xla/hlo/evaluator:hlo_evaluator\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/parser:hlo_parser\",\n-        \"//xla/hlo/utils:hlo_query\",\n-        \"//xla/service:hlo_module_config\",\n-        \"//xla/tests:literal_test_util\",\n-        \"//xla/tsl/concurrency:async_value\",\n-        \"//xla/tsl/platform:env\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"//xla/tsl/platform:test\",\n-        \"@com_google_absl//absl/log:check\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/types:span\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@eigen_archive//:eigen3\",\n-    ],\n-)\n-\n-cc_library(\n-    name = \"xnn_dot_thunk\",\n-    srcs = [\"xnn_dot_thunk.cc\"],\n-    hdrs = [\"xnn_dot_thunk.h\"],\n-    deps = [\n-        \":xnn_fusion_thunk\",\n-        \":xnn_interop\",\n-        \"//xla:shape_util\",\n-        \"//xla:util\",\n-        \"//xla:xla_data_proto_cc\",\n-        \"//xla/backends/cpu/runtime:dot_dims\",\n-        \"//xla/backends/cpu/runtime:thunk\",\n-        \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_address\",\n-        \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:logging\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@XNNPACK\",\n-        \"@com_google_absl//absl/functional:bind_front\",\n-        \"@com_google_absl//absl/memory\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/strings:str_format\",\n-        \"@com_google_absl//absl/types:span\",\n-    ],\n-)\n-\n-xla_cc_test(\n-    name = \"xnn_dot_thunk_test\",\n-    srcs = [\"xnn_dot_thunk_test.cc\"],\n-    deps = [\n-        \":xnn_dot_thunk\",\n-        \":xnn_interop\",\n-        \":xnn_threadpool\",\n-        \"//xla:literal_util\",\n-        \"//xla:shape_util\",\n-        \"//xla:xla_data_proto_cc\",\n-        \"//xla/backends/cpu/runtime:buffer_allocations\",\n-        \"//xla/backends/cpu/runtime:thunk\",\n-        \"//xla/backends/cpu/runtime:thunk_testlib\",\n-        \"//xla/tsl/concurrency:async_value\",\n-        \"//xla/tsl/platform:env\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"//xla/tsl/platform:test\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@eigen_archive//:eigen3\",\n-        \"@local_tsl//tsl/platform:platform_port\",\n-    ],\n-)\n-\n-cc_library(\n-    name = \"xnn_fusion_thunk\",\n-    srcs = [\"xnn_fusion_thunk.cc\"],\n-    hdrs = [\"xnn_fusion_thunk.h\"],\n-    deps = [\n-        \":xnn_interop\",\n-        \"//xla:shape_util\",\n-        \"//xla/backends/cpu/runtime:thunk\",\n-        \"//xla/runtime:buffer_use\",\n-        \"//xla/runtime:object_pool\",\n-        \"//xla/service:buffer_assignment\",\n-        \"//xla/stream_executor:device_address\",\n-        \"//xla/tsl/concurrency:async_value\",\n-        \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:logging\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@XNNPACK\",\n-        \"@com_google_absl//absl/algorithm:container\",\n-        \"@com_google_absl//absl/base:no_destructor\",\n-        \"@com_google_absl//absl/container:inlined_vector\",\n-        \"@com_google_absl//absl/functional:any_invocable\",\n-        \"@com_google_absl//absl/functional:bind_front\",\n-        \"@com_google_absl//absl/functional:function_ref\",\n-        \"@com_google_absl//absl/log:check\",\n-        \"@com_google_absl//absl/memory\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/strings:str_format\",\n-        \"@com_google_absl//absl/types:span\",\n-    ],\n-)\n-\n-xla_cc_test(\n-    name = \"xnn_fusion_thunk_test\",\n-    srcs = [\"xnn_fusion_thunk_test.cc\"],\n-    deps = [\n-        \":xnn_fusion_thunk\",\n-        \":xnn_interop\",\n-        \":xnn_threadpool\",\n-        \"//xla:literal_util\",\n-        \"//xla:shape_util\",\n-        \"//xla:xla_data_proto_cc\",\n-        \"//xla/backends/cpu/runtime:buffer_allocations\",\n-        \"//xla/backends/cpu/runtime:thunk\",\n-        \"//xla/backends/cpu/runtime:thunk_testlib\",\n-        \"//xla/tsl/concurrency:async_value\",\n-        \"//xla/tsl/platform:env\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"//xla/tsl/platform:test\",\n-        \"@XNNPACK\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/types:span\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@eigen_archive//:eigen3\",\n-    ],\n-)"
        },
        {
            "sha": "0d83fbec77698d76d562ce46783164a81995c428",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_convolution_thunk.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 192,
            "changes": 192,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_convolution_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_convolution_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_convolution_thunk.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,192 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_convolution_thunk.h\"\n-\n-#include <cstddef>\n-#include <cstdint>\n-#include <functional>\n-#include <limits>\n-#include <memory>\n-#include <string>\n-#include <utility>\n-#include <vector>\n-\n-#include \"xnnpack.h\"\n-#include \"absl/memory/memory.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/str_cat.h\"\n-#include \"absl/strings/str_format.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/backends/cpu/runtime/convolution_dims.h\"\n-#include \"xla/backends/cpu/runtime/thunk.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_interop.h\"\n-#include \"xla/service/buffer_assignment.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_address.h\"\n-#include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/logging.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/util.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-namespace xla::cpu {\n-\n-absl::StatusOr<XnnSubgraph> XnnConvolutionThunk::BuildConvolutionSubgraph(\n-    absl::Span<const Argument> arguments, absl::Span<const Result> results,\n-    absl::Span<const se::DeviceAddressBase> arguments_buffers) {\n-  TF_ASSIGN_OR_RETURN(XnnSubgraph subgraph,\n-                      CreateXnnSubgraph([&](xnn_subgraph_t* subgraph) {\n-                        return xnn_create_subgraph(\n-                            /*external_value_ids=*/3,\n-                            /*flags=*/0, subgraph);\n-                      }));\n-\n-  uint32_t input_id = XNN_INVALID_VALUE_ID;\n-  uint32_t kernel_id = XNN_INVALID_VALUE_ID;\n-  uint32_t out_id = XNN_INVALID_VALUE_ID;\n-\n-  auto dims = [](absl::Span<const int64_t> dims) -> std::vector<size_t> {\n-    return {dims.begin(), dims.end()};\n-  };\n-\n-  VLOG(3) << absl::StreamFormat(\n-      \"Create XNNPACK convolution: input_shape=%s kernel_shape=%s out_shape=%s\",\n-      convolution_slices_.input_shape.ToString(true),\n-      convolution_slices_.kernel_shape.ToString(true),\n-      convolution_slices_.output_shape.ToString(true));\n-\n-  std::vector<size_t> input_dims =\n-      dims(convolution_slices_.input_shape.dimensions());\n-  std::vector<size_t> kernel_dims =\n-      dims(convolution_slices_.kernel_shape.dimensions());\n-  std::vector<size_t> out_dims =\n-      dims(convolution_slices_.output_shape.dimensions());\n-\n-  XNN_RETURN_IF_ERROR(xnn_define_tensor_value(\n-      subgraph.get(), xnn_datatype_fp32, input_dims.size(), input_dims.data(),\n-      nullptr,\n-      /*external_id=*/0, XNN_VALUE_FLAG_EXTERNAL_INPUT, &input_id));\n-\n-  XNN_RETURN_IF_ERROR(xnn_define_tensor_value(\n-      subgraph.get(), xnn_datatype_fp32, kernel_dims.size(), kernel_dims.data(),\n-      /*data=*/arguments_buffers[1].opaque(),\n-      /*external_id=*/1, /*flags=*/0, &kernel_id));\n-\n-  XNN_RETURN_IF_ERROR(xnn_define_tensor_value(\n-      subgraph.get(), xnn_datatype_fp32, out_dims.size(), out_dims.data(),\n-      nullptr,\n-      /*external_id=*/2, XNN_VALUE_FLAG_EXTERNAL_OUTPUT, &out_id));\n-\n-  auto& ds = convolution_canonical_dims_;\n-  XNN_RETURN_IF_ERROR(xnn_define_convolution_2d(\n-      subgraph.get(),  //\n-      /*input_padding_top=*/ds.padding_before.x,\n-      /*input_padding_right=*/ds.padding_before.y,\n-      /*input_padding_bottom=*/ds.padding_after.x,\n-      /*input_padding_left=*/ds.padding_after.y,\n-      /*kernel_height=*/ds.kernel_dims.x,\n-      /*kernel_width=*/ds.kernel_dims.y,\n-      /*subsampling_height=*/ds.strides.x,\n-      /*subsampling_width=*/ds.strides.y,\n-      /*dilation_height=*/ds.base_dilation.x,\n-      /*dilation_width=*/ds.base_dilation.y,\n-      /*groups=*/ds.feature_group_count,\n-      /*group_input_channels=*/ds.input_channels,\n-      /*group_output_channels=*/ds.kernel_filters,\n-      /*output_min=*/std::numeric_limits<float>::lowest(),\n-      /*output_max=*/std::numeric_limits<float>::max(), input_id, kernel_id,\n-      /*bias_id=*/XNN_INVALID_VALUE_ID, out_id,\n-      /*flags=*/XNN_FLAG_TENSORFLOW_SAME_PADDING));\n-\n-  return subgraph;\n-}\n-\n-absl::StatusOr<std::unique_ptr<XnnConvolutionThunk>>\n-XnnConvolutionThunk::Create(\n-    Options options, Info info, BufferAllocation::Slice input_buffer,\n-    const Shape& input_shape, BufferAllocation::Slice kernel_buffer,\n-    const Shape& kernel_shape, BufferAllocation::Slice output_buffer,\n-    const Shape& output_shape, const ConvolutionDimensionNumbers& dnums,\n-    const Window& window, int64_t feature_group_count) {\n-  TF_RETURN_IF_ERROR(InitializeXnnPack());\n-\n-  if (dnums.kernel_input_feature_dimension() != 3 ||\n-      dnums.kernel_output_feature_dimension() != 0) {\n-    return InvalidArgument(\n-        \"XNNPACK convolution expects kernel (filter) in OHWI format\");\n-  }\n-\n-  ConvolutionSlices slices = {input_buffer, input_shape,   kernel_buffer,\n-                              kernel_shape, output_buffer, output_shape};\n-\n-  TF_ASSIGN_OR_RETURN(\n-      ConvolutionCanonicalDims canonical_dims,\n-      GetConvolutionCanonicalDims(slices, dnums, window, feature_group_count));\n-\n-  return absl::WrapUnique(new XnnConvolutionThunk(\n-      std::move(options), std::move(info), std::move(slices),\n-      std::move(canonical_dims), dnums, window));\n-}\n-\n-static std::vector<XnnFusionThunk::Argument> ConvolutionArguments(\n-    const ConvolutionSlices& slices) {\n-  return {XnnFusionThunk::Argument{slices.input_buffer, slices.input_shape},\n-          XnnFusionThunk::Argument{slices.kernel_buffer, slices.kernel_shape}};\n-}\n-\n-static std::vector<XnnFusionThunk::Result> ConvolutionResults(\n-    const ConvolutionSlices& slices) {\n-  return {XnnFusionThunk::Result{slices.output_buffer, slices.output_shape}};\n-}\n-\n-XnnConvolutionThunk::XnnConvolutionThunk(\n-    Options options, Info info, ConvolutionSlices convolution_slices,\n-    ConvolutionCanonicalDims convolution_canonical_dims,\n-    ConvolutionDimensionNumbers dnums, Window window)\n-    : XnnFusionThunk(XnnFusionKind::kConvolution, std::move(options),\n-                     std::move(info), ConvolutionArguments(convolution_slices),\n-                     ConvolutionResults(convolution_slices),\n-                     CapturingBuilder(std::bind(\n-                         &XnnConvolutionThunk::BuildConvolutionSubgraph, this,\n-                         std::placeholders::_1, std::placeholders::_2,\n-                         std::placeholders::_3)),\n-                     /*captured_arguments_ids=*/{1}),\n-      convolution_slices_(std::move(convolution_slices)),\n-      convolution_canonical_dims_(std::move(convolution_canonical_dims)),\n-      dnums_(std::move(dnums)),\n-      window_(std::move(window)) {}\n-\n-std::string XnnConvolutionThunk::fusion_kind() const { return \"convolution\"; }\n-\n-std::string XnnConvolutionThunk::fusion_description() const {\n-  return absl::StrFormat(\"convolution_rank=%d\",\n-                         convolution_canonical_dims_.convolution_rank());\n-}\n-\n-std::vector<std::string> XnnConvolutionThunk::fusion_details() const {\n-  return {absl::StrCat(convolution_canonical_dims_)};\n-}\n-\n-std::string XnnConvolutionThunk::argument_name(size_t index) const {\n-  return index == 0 ? \"input\" : \"kernel\";\n-}\n-\n-std::string XnnConvolutionThunk::result_name(size_t index) const {\n-  return \"out\";\n-}\n-\n-}  // namespace xla::cpu"
        },
        {
            "sha": "7269ddff7f20d9145a905ffd3e8d67c491d890d8",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_convolution_thunk.h",
            "status": "removed",
            "additions": 0,
            "deletions": 89,
            "changes": 89,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_convolution_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_convolution_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_convolution_thunk.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,89 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_BACKENDS_CPU_RUNTIME_XNNPACK_XNN_CONVOLUTION_THUNK_H_\n-#define XLA_BACKENDS_CPU_RUNTIME_XNNPACK_XNN_CONVOLUTION_THUNK_H_\n-\n-#include <cstddef>\n-#include <cstdint>\n-#include <memory>\n-#include <string>\n-#include <vector>\n-\n-#include \"absl/status/statusor.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/backends/cpu/runtime/convolution_dims.h\"\n-#include \"xla/backends/cpu/runtime/thunk.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk.h\"\n-#include \"xla/service/buffer_assignment.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_address.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-namespace xla::cpu {\n-\n-// Convolution operation implemented on top of XNNPACK.\n-class XnnConvolutionThunk final : public XnnFusionThunk {\n- public:\n-  static absl::StatusOr<std::unique_ptr<XnnConvolutionThunk>> Create(\n-      Options options, Info info, BufferAllocation::Slice input_buffer,\n-      const Shape& input_shape, BufferAllocation::Slice kernel_buffer,\n-      const Shape& kernel_shape, BufferAllocation::Slice output_buffer,\n-      const Shape& output_shape, const ConvolutionDimensionNumbers& dnums,\n-      const Window& window, int64_t feature_group_count);\n-\n-  ConvolutionDimensionNumbers dnums() const { return dnums_; }\n-  Window window() const { return window_; }\n-\n-  int64_t feature_group_count() const {\n-    return convolution_canonical_dims_.feature_group_count;\n-  }\n-\n-  const ConvolutionSlices& convolution_slices() const {\n-    return convolution_slices_;\n-  }\n-\n- protected:\n-  std::string fusion_kind() const final;\n-  std::string fusion_description() const final;\n-\n-  bool has_fusion_details() const final { return true; }\n-  std::vector<std::string> fusion_details() const final;\n-\n-  std::string argument_name(size_t index) const final;\n-  std::string result_name(size_t index) const final;\n-\n- private:\n-  XnnConvolutionThunk(Options options, Info info,\n-                      ConvolutionSlices convolution_slices,\n-                      ConvolutionCanonicalDims convolution_canonical_dims,\n-                      ConvolutionDimensionNumbers dnums, Window window);\n-\n-  absl::StatusOr<XnnSubgraph> BuildConvolutionSubgraph(\n-      absl::Span<const Argument> arguments, absl::Span<const Result> results,\n-      absl::Span<const se::DeviceAddressBase> arguments_buffers);\n-\n-  ConvolutionSlices convolution_slices_;\n-  ConvolutionCanonicalDims convolution_canonical_dims_;\n-\n-  // Convolution operation parameters that were used to construct this thunk. We\n-  // only keep them around to be able to serialize/deserialize thunk.\n-  ConvolutionDimensionNumbers dnums_;\n-  Window window_;\n-};\n-\n-}  // namespace xla::cpu\n-\n-#endif  // XLA_BACKENDS_CPU_RUNTIME_XNNPACK_XNN_CONVOLUTION_THUNK_H_"
        },
        {
            "sha": "32eb78cf23f09c54f8af73cec60fb61433b80515",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_convolution_thunk_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 202,
            "changes": 202,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_convolution_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_convolution_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_convolution_thunk_test.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,202 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_convolution_thunk.h\"\n-\n-#include <cstdint>\n-#include <memory>\n-#include <random>\n-#include <string>\n-#include <tuple>\n-#include <utility>\n-#include <vector>\n-\n-#include \"absl/log/check.h\"\n-#include \"absl/strings/str_cat.h\"\n-#include \"absl/strings/substitute.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/backends/cpu/runtime/buffer_allocations.h\"\n-#include \"xla/backends/cpu/runtime/thunk.h\"\n-#include \"xla/backends/cpu/runtime/thunk_testlib.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_interop.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_threadpool.h\"\n-#include \"xla/error_spec.h\"\n-#include \"xla/hlo/evaluator/hlo_evaluator.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/parser/hlo_parser.h\"\n-#include \"xla/hlo/utils/hlo_query.h\"\n-#include \"xla/layout.h\"\n-#include \"xla/layout_util.h\"\n-#include \"xla/literal.h\"\n-#include \"xla/literal_util.h\"\n-#include \"xla/service/hlo_module_config.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/shape_util.h\"\n-#include \"xla/tests/literal_test_util.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n-#include \"xla/tsl/platform/env.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/tsl/platform/test.h\"\n-#include \"xla/tsl/platform/threadpool.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-#define EIGEN_USE_THREADS\n-#include \"unsupported/Eigen/CXX11/Tensor\"\n-\n-namespace xla::cpu {\n-namespace {\n-\n-class XnnConvolutionThunkTest\n-    : public ::testing::TestWithParam<std::tuple<bool, std::vector<int32_t>>> {\n- protected:\n-  bool use_threadpool() const { return std::get<0>(GetParam()); }\n-\n-  int32_t dimension(int32_t index) const {\n-    return std::get<1>(GetParam())[index];\n-  }\n-\n-  bool IsOdd(int n) { return n % 2 == 1; }\n-};\n-\n-TEST_P(XnnConvolutionThunkTest, SimpleConvolution) {\n-  int32_t batch = dimension(0);\n-  int32_t height = dimension(1);\n-  int32_t width = dimension(2);\n-  int32_t input_channels = dimension(3);\n-  int32_t kernel_h = dimension(4);\n-  int32_t kernel_w = dimension(5);\n-  int32_t output_channels = dimension(6);\n-\n-  // Padding values for 'SAME' padding. Only odd kernel sizes are supported.\n-  CHECK(IsOdd(kernel_h) && IsOdd(kernel_w));\n-  int padding_h = (kernel_h - 1) / 2;\n-  int padding_w = (kernel_w - 1) / 2;\n-\n-  std::minstd_rand0 engine;\n-\n-  // Input format is NHWC.\n-  auto input_shape =\n-      ShapeUtil::MakeShape(F32, {batch, height, width, input_channels});\n-\n-  // Kernel format is HWIO.\n-  auto kernel_shape = ShapeUtil::MakeShape(\n-      F32, {kernel_h, kernel_w, input_channels, output_channels});\n-\n-  auto input =\n-      *LiteralUtil::CreateRandomLiteral<F32>(input_shape, &engine, 1.0f, 0.1f);\n-  auto kernel =\n-      *LiteralUtil::CreateRandomLiteral<F32>(kernel_shape, &engine, 1.0f, 0.1f);\n-\n-  // Create a reference HLO module that we can use to compare the results.\n-  std::string hlo_module_template = R\"(\n-    HloModule convolution\n-\n-    ENTRY TestComputation {\n-      %p0 = $0 parameter(0)\n-      %p1 = $1 parameter(1)\n-      ROOT conv = convolution(p0, p1), window={size=$2 pad=$3},\n-        dim_labels=b01f_01io->b01f\n-    }\n-  )\";\n-\n-  std::string hlo_module = absl::Substitute(\n-      hlo_module_template, input_shape.ToString(), kernel_shape.ToString(),\n-      absl::StrCat(kernel_h, \"x\", kernel_w),\n-      absl::StrCat(padding_h, \"_\", padding_h, \"x\", padding_w, \"_\", padding_w));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::unique_ptr<HloModule> module,\n-      ParseAndReturnUnverifiedModule(hlo_module, HloModuleConfig()));\n-\n-  HloEvaluator evaluator;\n-  TF_ASSERT_OK_AND_ASSIGN(Literal expected_result,\n-                          evaluator.Evaluate(*module, {&input, &kernel}));\n-\n-  HloInstruction* conv =\n-      hlo_query::FindInstruction(module->entry_computation(), \"conv\");\n-  ASSERT_NE(conv, nullptr);\n-\n-  tsl::thread::ThreadPool threads(tsl::Env::Default(), \"test\", 8);\n-  Eigen::ThreadPoolDevice device(threads.AsEigenThreadPool(),\n-                                 threads.NumThreads());\n-\n-  // XNNPACK expects OHWI format for the kernel.\n-  Literal kernel_transposed =\n-      kernel.Transpose({3, 0, 1, 2})\n-          .Relayout(LayoutUtil::MakeLayout({3, 2, 1, 0}));\n-\n-  // Create a Literal with the expected shape.\n-  const Shape& out_shape = expected_result.shape();\n-  auto out = LiteralUtil::CreateFull(out_shape.dimensions(), 0.f);\n-\n-  BufferAllocations allocations =\n-      CreateBufferAllocations(input, kernel_transposed, out);\n-\n-  auto [input_alloc, kernel_transposed_alloc, out_alloc] =\n-      CreateBufferAllocation(input, kernel_transposed, out);\n-  auto [input_slice, kernel_transposed_slice, out_slice] =\n-      CreateBufferAllocationSlice(input_alloc, kernel_transposed_alloc,\n-                                  out_alloc);\n-\n-  // Adjust kernel dimensions for XNNPACK.\n-  ConvolutionDimensionNumbers dnums = conv->convolution_dimension_numbers();\n-  dnums.set_kernel_input_feature_dimension(3);\n-  dnums.set_kernel_output_feature_dimension(0);\n-  dnums.set_kernel_spatial_dimensions(0, 1);\n-  dnums.set_output_spatial_dimensions(1, 2);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto thunk,\n-      XnnConvolutionThunk::Create(\n-          XnnConvolutionThunk::Options{use_threadpool()}, {\"convolution\"},\n-          input_slice, input_shape, kernel_transposed_slice,\n-          kernel_transposed.shape(), out_slice, out_shape, dnums,\n-          conv->window(), conv->feature_group_count()));\n-\n-  XnnThreadpool threadpool;\n-  if (use_threadpool()) {\n-    TF_ASSERT_OK_AND_ASSIGN(threadpool, CreateXnnThreadpool(&device));\n-  }\n-  Thunk::XnnParams xnn_params(std::move(threadpool));\n-\n-  Thunk::ExecuteParams params;\n-  params.buffer_allocations = &allocations;\n-  params.intra_op_threadpool = use_threadpool() ? &device : nullptr;\n-  params.xnn_params = &xnn_params;\n-\n-  auto execute_event = thunk->Execute(params);\n-  tsl::BlockUntilReady(execute_event);\n-  ASSERT_FALSE(execute_event.IsError()) << execute_event.GetError();\n-\n-  ErrorSpec error_spec{1e-5};\n-  EXPECT_TRUE(LiteralTestUtil::Near(expected_result, out, error_spec));\n-\n-  // Execute thunk one more time to test that we reuse XNN runtime.\n-  execute_event = thunk->Execute(params);\n-  tsl::BlockUntilReady(execute_event);\n-  ASSERT_FALSE(execute_event.IsError()) << execute_event.GetError();\n-\n-  EXPECT_TRUE(LiteralTestUtil::Near(expected_result, out, error_spec));\n-}\n-\n-INSTANTIATE_TEST_SUITE_P(\n-    XnnConvolution, XnnConvolutionThunkTest,\n-    ::testing::Combine(::testing::Values(true, false),\n-                       ::testing::Values(std::vector<int32_t>{1, 8, 8, 16, 1, 1,\n-                                                              32})));\n-\n-}  // namespace\n-}  // namespace xla::cpu"
        },
        {
            "sha": "44ec1b8139bfc5dd761724b60e766578709e3e3f",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_dot_thunk.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 191,
            "changes": 191,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_dot_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_dot_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_dot_thunk.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,191 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_dot_thunk.h\"\n-\n-#include <cstddef>\n-#include <cstdint>\n-#include <memory>\n-#include <string>\n-#include <utility>\n-#include <vector>\n-\n-#include \"xnnpack.h\"\n-#include \"absl/functional/bind_front.h\"\n-#include \"absl/memory/memory.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/str_format.h\"\n-#include \"absl/strings/str_join.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/backends/cpu/runtime/dot_dims.h\"\n-#include \"xla/backends/cpu/runtime/thunk.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_interop.h\"\n-#include \"xla/primitive_util.h\"\n-#include \"xla/service/buffer_assignment.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_address.h\"\n-#include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/logging.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/util.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-namespace xla::cpu {\n-\n-absl::StatusOr<XnnSubgraph> XnnDotThunk::BuildDotSubgraph(\n-    absl::Span<const Argument> arguments, absl::Span<const Result> results,\n-    absl::Span<const se::DeviceAddressBase> arguments_buffers) {\n-  TF_ASSIGN_OR_RETURN(XnnSubgraph subgraph,\n-                      CreateXnnSubgraph([](xnn_subgraph_t* subgraph) {\n-                        return xnn_create_subgraph(\n-                            /*external_value_ids=*/3,\n-                            /*flags=*/0, subgraph);\n-                      }));\n-\n-  uint32_t lhs_id = XNN_INVALID_VALUE_ID;\n-  uint32_t rhs_id = XNN_INVALID_VALUE_ID;\n-  uint32_t out_id = XNN_INVALID_VALUE_ID;\n-\n-  auto dims = [](absl::Span<const int64_t> dims) -> std::vector<size_t> {\n-    return {dims.begin(), dims.end()};\n-  };\n-\n-  std::vector<size_t> lhs_dims = dims(dot_slices_.lhs_shape.dimensions());\n-  std::vector<size_t> rhs_dims = dims(dot_slices_.rhs_shape.dimensions());\n-  std::vector<size_t> out_dims = dims(dot_slices_.out_shape.dimensions());\n-\n-  PrimitiveType dtype = dot_slices_.lhs_shape.element_type();\n-  if (dtype != F32 && dtype != BF16) {\n-    return InvalidArgument(\"Unsupported input data type for XnnDotThunk: %s\",\n-                           primitive_util::LowercasePrimitiveTypeName(dtype));\n-  }\n-  xnn_datatype input_dtype =\n-      (dtype == F32) ? xnn_datatype_fp32 : xnn_datatype_bf16;\n-  xnn_datatype output_dtype = xnn_datatype_fp32;\n-\n-  XNN_RETURN_IF_ERROR(xnn_define_tensor_value(\n-      subgraph.get(), input_dtype, lhs_dims.size(), lhs_dims.data(), nullptr,\n-      /*external_id=*/0, XNN_VALUE_FLAG_EXTERNAL_INPUT, &lhs_id));\n-\n-  XNN_RETURN_IF_ERROR(xnn_define_tensor_value(\n-      subgraph.get(), input_dtype, rhs_dims.size(), rhs_dims.data(),\n-      capture_rhs_ ? arguments_buffers[1].opaque() : nullptr,\n-      /*external_id=*/1, XNN_VALUE_FLAG_EXTERNAL_INPUT, &rhs_id));\n-\n-  XNN_RETURN_IF_ERROR(xnn_define_tensor_value(\n-      subgraph.get(), output_dtype, out_dims.size(), out_dims.data(), nullptr,\n-      /*external_id=*/2, XNN_VALUE_FLAG_EXTERNAL_OUTPUT, &out_id));\n-\n-  XNN_RETURN_IF_ERROR(xnn_define_batch_matrix_multiply(\n-      subgraph.get(), lhs_id, rhs_id, out_id,\n-      (/*flags=*/dot_canonical_dims_.rhs_canonical ? 0 : XNN_FLAG_TRANSPOSE_B) |\n-          XNN_FLAG_NO_BROADCAST));\n-\n-  return subgraph;\n-}\n-\n-absl::StatusOr<std::unique_ptr<XnnDotThunk>> XnnDotThunk::Create(\n-    Options options, Info info, DotDimensionNumbers dot_dimensions,\n-    BufferAllocation::Slice lhs_buffer, Shape lhs_shape,\n-    BufferAllocation::Slice rhs_buffer, Shape rhs_shape,\n-    BufferAllocation::Slice out_buffer, Shape out_shape, bool capture_rhs) {\n-  TF_RETURN_IF_ERROR(InitializeXnnPack());\n-\n-  TF_ASSIGN_OR_RETURN(DotShape dot_shape, GetDotShape(dot_dimensions, lhs_shape,\n-                                                      rhs_shape, out_shape));\n-\n-  TF_ASSIGN_OR_RETURN(DotCanonicalDims dot_canonical_dims,\n-                      GetDotCanonicalDims(dot_dimensions, dot_shape));\n-\n-  DotSlices dot_slices{lhs_buffer, std::move(lhs_shape),\n-                       rhs_buffer, std::move(rhs_shape),\n-                       out_buffer, std::move(out_shape)};\n-\n-  return absl::WrapUnique(new XnnDotThunk(\n-      std::move(options), std::move(info), std::move(dot_dimensions),\n-      std::move(dot_slices), std::move(dot_shape),\n-      std::move(dot_canonical_dims), capture_rhs));\n-}\n-\n-static std::vector<XnnFusionThunk::Argument> DotArguments(\n-    const DotSlices& slices) {\n-  return {XnnFusionThunk::Argument{slices.lhs_buffer, slices.lhs_shape},\n-          XnnFusionThunk::Argument{slices.rhs_buffer, slices.rhs_shape}};\n-}\n-\n-static std::vector<XnnFusionThunk::Result> DotResults(const DotSlices& slices) {\n-  return {XnnFusionThunk::Result{slices.out_buffer, slices.out_shape}};\n-}\n-\n-static absl::Span<const int64_t> DotCapturedArgumentIds(bool capture_rhs) {\n-  static constexpr int64_t kRhsIndex = 1;\n-  return capture_rhs ? absl::Span<const int64_t>(&kRhsIndex, 1)\n-                     : absl::Span<const int64_t>();\n-}\n-\n-XnnDotThunk::XnnDotThunk(Options options, Info info,\n-                         DotDimensionNumbers dot_dimensions,\n-                         DotSlices dot_slices, DotShape dot_shape,\n-                         DotCanonicalDims dot_canonical_dims, bool capture_rhs)\n-    : XnnFusionThunk(XnnFusionKind::kDot, std::move(options), std::move(info),\n-                     DotArguments(dot_slices), DotResults(dot_slices),\n-                     CapturingBuilder(absl::bind_front(\n-                         &XnnDotThunk::BuildDotSubgraph, this)),\n-                     DotCapturedArgumentIds(capture_rhs)),\n-      dot_dimensions_(std::move(dot_dimensions)),\n-      dot_slices_(std::move(dot_slices)),\n-      dot_shape_(std::move(dot_shape)),\n-      dot_canonical_dims_(std::move(dot_canonical_dims)),\n-      capture_rhs_(capture_rhs) {}\n-\n-std::string XnnDotThunk::fusion_kind() const { return \"dot\"; }\n-\n-std::string XnnDotThunk::fusion_description() const {\n-  return absl::StrFormat(\n-      \"lhs_batch_dims=[%s], rhs_batch_dims=[%s], \"\n-      \"lhs_contract_dims=[%s], rhs_contract_dims=[%s], capture_rhs=%v\",\n-      absl::StrJoin(dot_dimensions_.lhs_batch_dimensions(), \",\"),\n-      absl::StrJoin(dot_dimensions_.rhs_batch_dimensions(), \",\"),\n-      absl::StrJoin(dot_dimensions_.lhs_contracting_dimensions(), \",\"),\n-      absl::StrJoin(dot_dimensions_.rhs_contracting_dimensions(), \",\"),\n-      capture_rhs_);\n-}\n-\n-std::vector<std::string> XnnDotThunk::fusion_details() const {\n-  return {\n-      absl::StrFormat(\"  matmul shape: batch_size=%d, lhs=%s, rhs=%s, out=%s\",\n-                      dot_shape_.batch_size,\n-                      dot_shape_.lhs_matmul_shape.ToString(true),\n-                      dot_shape_.rhs_matmul_shape.ToString(true),\n-                      dot_shape_.out_matmul_shape.ToString(true)),\n-      absl::StrFormat(\"  matmul dims: m=%d, k=%d, n=%d, lhs_column_major=%v, \"\n-                      \"lhs_canonical=%v rhs_column_major=%v, rhs_canonical=%v\",\n-                      dot_canonical_dims_.m, dot_canonical_dims_.k,\n-                      dot_canonical_dims_.n,\n-                      dot_canonical_dims_.lhs_column_major,\n-                      dot_canonical_dims_.lhs_canonical,\n-                      dot_canonical_dims_.rhs_column_major,\n-                      dot_canonical_dims_.rhs_canonical),\n-  };\n-}\n-\n-std::string XnnDotThunk::argument_name(size_t index) const {\n-  return index == 0 ? \"lhs\" : \"rhs\";\n-}\n-\n-std::string XnnDotThunk::result_name(size_t index) const { return \"out\"; }\n-\n-}  // namespace xla::cpu"
        },
        {
            "sha": "448897ad0eb662052ebb9f6758be6e018ec2065f",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_dot_thunk.h",
            "status": "removed",
            "additions": 0,
            "deletions": 81,
            "changes": 81,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_dot_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_dot_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_dot_thunk.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,81 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_BACKENDS_CPU_RUNTIME_XNNPACK_XNN_DOT_THUNK_H_\n-#define XLA_BACKENDS_CPU_RUNTIME_XNNPACK_XNN_DOT_THUNK_H_\n-\n-#include <cstddef>\n-#include <cstdint>\n-#include <memory>\n-#include <string>\n-#include <vector>\n-\n-#include \"absl/status/statusor.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/backends/cpu/runtime/dot_dims.h\"\n-#include \"xla/backends/cpu/runtime/thunk.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk.h\"\n-#include \"xla/service/buffer_assignment.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_address.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-namespace xla::cpu {\n-\n-// Dot operation implemented on top of XNNPACK.\n-class XnnDotThunk final : public XnnFusionThunk {\n- public:\n-  static absl::StatusOr<std::unique_ptr<XnnDotThunk>> Create(\n-      Options options, Info info, DotDimensionNumbers dot_dimensions,\n-      BufferAllocation::Slice lhs_buffer, Shape lhs_shape,\n-      BufferAllocation::Slice rhs_buffer, Shape rhs_shape,\n-      BufferAllocation::Slice out_buffer, Shape out_shape, bool capture_rhs);\n-\n-  DotDimensionNumbers dot_dimensions() const { return dot_dimensions_; }\n-  DotSlices dot_slices() const { return dot_slices_; }\n-  bool capture_rhs() const { return capture_rhs_; }\n-\n- protected:\n-  std::string fusion_kind() const final;\n-  std::string fusion_description() const final;\n-\n-  bool has_fusion_details() const final { return true; }\n-  std::vector<std::string> fusion_details() const final;\n-\n-  std::string argument_name(size_t index) const final;\n-  std::string result_name(size_t index) const final;\n-\n- private:\n-  XnnDotThunk(Options options, Info info, DotDimensionNumbers dot_dimensions,\n-              DotSlices dot_slices, DotShape dot_shape,\n-              DotCanonicalDims dot_canonical_dims, bool capture_rhs);\n-\n-  absl::StatusOr<XnnSubgraph> BuildDotSubgraph(\n-      absl::Span<const Argument> arguments, absl::Span<const Result> results,\n-      absl::Span<const se::DeviceAddressBase> arguments_buffers);\n-\n-  DotDimensionNumbers dot_dimensions_;\n-  DotSlices dot_slices_;\n-  DotShape dot_shape_;\n-  DotCanonicalDims dot_canonical_dims_;\n-\n-  // If true, the RHS buffer might be captured by XNNPACK graph by value. This\n-  // allows XNNPACK to do packing at graph compile time.\n-  bool capture_rhs_;\n-};\n-\n-}  // namespace xla::cpu\n-\n-#endif  // XLA_BACKENDS_CPU_RUNTIME_XNNPACK_XNN_DOT_THUNK_H_"
        },
        {
            "sha": "16f0efb0910c92b3bce404ad6d4e1c13ccdc6740",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_dot_thunk_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 124,
            "changes": 124,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_dot_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_dot_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_dot_thunk_test.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,124 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_dot_thunk.h\"\n-\n-#include <string>\n-#include <tuple>\n-#include <utility>\n-\n-#include \"absl/strings/str_cat.h\"\n-#include \"xla/backends/cpu/runtime/buffer_allocations.h\"\n-#include \"xla/backends/cpu/runtime/thunk.h\"\n-#include \"xla/backends/cpu/runtime/thunk_testlib.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_interop.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_threadpool.h\"\n-#include \"xla/literal_util.h\"\n-#include \"xla/primitive_util.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/shape_util.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n-#include \"xla/tsl/platform/env.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/tsl/platform/test.h\"\n-#include \"xla/tsl/platform/threadpool.h\"\n-#include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/cpu_info.h\"\n-\n-#define EIGEN_USE_THREADS\n-#include \"unsupported/Eigen/CXX11/Tensor\"\n-\n-namespace xla::cpu {\n-namespace {\n-\n-using XnnDotThunkTestSpec = std::tuple<PrimitiveType, bool, bool>;\n-\n-class XnnDotThunkTest : public testing::TestWithParam<XnnDotThunkTestSpec> {\n- public:\n-  static std::string Name(\n-      const ::testing::TestParamInfo<XnnDotThunkTestSpec>& info) {\n-    return absl::StrCat(\n-        primitive_util::LowercasePrimitiveTypeName(std::get<0>(info.param)),\n-        \"_\", std::get<1>(info.param) ? \"threadpool\" : \"single_threaded\", \"_\",\n-        std::get<2>(info.param) ? \"capture_rhs\" : \"no_capture_rhs\");\n-  }\n-};\n-\n-TEST_P(XnnDotThunkTest, SimpleDot) {\n-  auto [input_type, use_threadpool, capture_rhs] = GetParam();\n-\n-  if (input_type == BF16 &&\n-      !tsl::port::TestCPUFeature(tsl::port::AVX512_BF16)) {\n-    GTEST_SKIP() << \"CPU needs AVX512_BF16 for this test.\";\n-  }\n-\n-  tsl::thread::ThreadPool threads(tsl::Env::Default(), \"test\", 8);\n-  Eigen::ThreadPoolDevice device(threads.AsEigenThreadPool(),\n-                                 threads.NumThreads());\n-\n-  auto lhs = LiteralUtil::CreateR2<float>({{1.0, 2.0}, {3.0, 4.0}});\n-  auto rhs = LiteralUtil::CreateR2<float>({{4.0, 3.0}, {2.0, 1.0}});\n-  auto out = LiteralUtil::CreateR2<float>({{0.0, 0.0}, {0.0, 0.0}});\n-  if (input_type == BF16) {\n-    lhs = LiteralUtil::ConvertF32ToBF16(lhs);\n-    rhs = LiteralUtil::ConvertF32ToBF16(rhs);\n-  }\n-\n-  BufferAllocations allocations = CreateBufferAllocations(lhs, rhs, out);\n-\n-  auto [lhs_alloc, rhs_alloc, out_alloc] =\n-      CreateBufferAllocation(lhs, rhs, out);\n-  auto [lhs_slice, rhs_slice, out_slice] =\n-      CreateBufferAllocationSlice(lhs_alloc, rhs_alloc, out_alloc);\n-\n-  Shape input_shape = ShapeUtil::MakeShape(input_type, {2, 2});\n-  Shape output_shape = ShapeUtil::MakeShape(F32, {2, 2});\n-\n-  DotDimensionNumbers dot_dimensions;\n-  dot_dimensions.add_lhs_contracting_dimensions(1);\n-  dot_dimensions.add_rhs_contracting_dimensions(0);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto thunk,\n-      XnnDotThunk::Create(XnnDotThunk::Options{use_threadpool}, {\"dot\"},\n-                          dot_dimensions, lhs_slice, input_shape, rhs_slice,\n-                          input_shape, out_slice, output_shape, capture_rhs));\n-\n-  XnnThreadpool threadpool;\n-  if (use_threadpool) {\n-    TF_ASSERT_OK_AND_ASSIGN(threadpool, CreateXnnThreadpool(&device));\n-  }\n-  Thunk::XnnParams xnn_params(std::move(threadpool));\n-\n-  Thunk::ExecuteParams params;\n-  params.buffer_allocations = &allocations;\n-  params.intra_op_threadpool = use_threadpool ? &device : nullptr;\n-  params.xnn_params = &xnn_params;\n-\n-  auto execute_event = thunk->Execute(params);\n-  tsl::BlockUntilReady(execute_event);\n-  ASSERT_FALSE(execute_event.IsError()) << execute_event.GetError();\n-\n-  EXPECT_EQ(out, LiteralUtil::CreateR2<float>({{8.0, 5.0}, {20.0, 13.0}}));\n-}\n-\n-INSTANTIATE_TEST_SUITE_P(XnnDot, XnnDotThunkTest,\n-                         ::testing::Combine(::testing::ValuesIn({F32, BF16}),\n-                                            ::testing::Bool(),\n-                                            ::testing::Bool()),\n-                         XnnDotThunkTest::Name);\n-\n-}  // namespace\n-}  // namespace xla::cpu"
        },
        {
            "sha": "6ab367af62fce711288c20eea5d74cff4d3c1cd1",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 364,
            "changes": 364,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,364 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk.h\"\n-\n-#include <cstddef>\n-#include <cstdint>\n-#include <memory>\n-#include <ostream>\n-#include <utility>\n-#include <vector>\n-\n-#include \"experimental.h\"  // xnnpack\n-#include \"xnnpack.h\"\n-#include \"absl/algorithm/container.h\"\n-#include \"absl/base/no_destructor.h\"\n-#include \"absl/container/inlined_vector.h\"\n-#include \"absl/functional/bind_front.h\"\n-#include \"absl/functional/function_ref.h\"\n-#include \"absl/log/check.h\"\n-#include \"absl/memory/memory.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/str_format.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/backends/cpu/runtime/thunk.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_interop.h\"\n-#include \"xla/runtime/buffer_use.h\"\n-#include \"xla/stream_executor/device_address.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n-#include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/logging.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-\n-namespace xla::cpu {\n-\n-absl::string_view XnnFusionThunk::XnnFusionKindToString(XnnFusionKind kind) {\n-  switch (kind) {\n-    case XnnFusionKind::kFusion:\n-      return \"xnn-fusion\";\n-    case XnnFusionKind::kDot:\n-      return \"xnn-dot\";\n-    case XnnFusionKind::kConvolution:\n-      return \"xnn-convolution\";\n-  }\n-}\n-\n-std::ostream& operator<<(std::ostream& os, XnnFusionThunk::XnnFusionKind kind) {\n-  return os << XnnFusionThunk::XnnFusionKindToString(kind);\n-}\n-\n-// XNNPACK executable instantiated for the fusion operation.\n-struct XnnFusionThunk::XnnExecutable {\n-  tsl::AsyncValueRef<XnnFusionThunk::ExecuteEvent> Invoke(\n-      const XnnThreadpool& threadpool,\n-      absl::Span<se::DeviceAddressBase> arguments,\n-      absl::Span<se::DeviceAddressBase> results,\n-      absl::FunctionRef<bool(size_t)> is_captured_argument);\n-\n-  // Resets XNNPACK runtime and subgraph.\n-  absl::Status Reset();\n-\n-  XnnSubgraph subgraph = nullptr;\n-  XnnRuntime runtime = nullptr;\n-\n-  // TODO(ezhulenev): Today we rely on device memory as an identity of the\n-  // captured argument, and this is not correct as we can have multiple\n-  // arguments allocated to the heap address. This is work in progress, and will\n-  // be migrated to a buffer identity passed to XLA by the client (PjRt).\n-  std::vector<se::DeviceAddressBase> captured_arguments;\n-};\n-\n-tsl::AsyncValueRef<XnnFusionThunk::ExecuteEvent>\n-XnnFusionThunk::XnnExecutable::Invoke(\n-    const XnnThreadpool& threadpool,\n-    absl::Span<se::DeviceAddressBase> arguments,\n-    absl::Span<se::DeviceAddressBase> results,\n-    absl::FunctionRef<bool(size_t)> is_captured_argument) {\n-  // Create external values for all arguments and results.\n-  absl::InlinedVector<xnn_external_value, 8> external_values;\n-  external_values.reserve(arguments.size() + results.size());\n-\n-  // External tensor id for arguments and results.\n-  uint32_t id = 0;\n-\n-  for (const se::DeviceAddressBase& argument : arguments) {\n-    xnn_external_value value{id++, argument.opaque()};\n-    if (!is_captured_argument(value.id)) {\n-      external_values.push_back(value);\n-    }\n-  }\n-\n-  for (const se::DeviceAddressBase& result : results) {\n-    xnn_external_value value{id++, result.opaque()};\n-    external_values.push_back(value);\n-  }\n-\n-  DCHECK_NE(runtime.get(), nullptr) << \"XNNPACK runtime is not initialized\";\n-  XNN_RETURN_IF_ERROR(xnn_setup_runtime_v2(\n-      runtime.get(), external_values.size(), external_values.data()));\n-\n-  // Update threadpool used by the XNNPACK runtime.\n-  xnn_update_runtime_with_threadpool(runtime.get(), threadpool.get());\n-\n-  // Execute XNNPACK runtime in the caller thread.\n-  XNN_RETURN_IF_ERROR(xnn_invoke_runtime(runtime.get()));\n-  return OkExecuteEvent();\n-}\n-\n-absl::Status XnnFusionThunk::XnnExecutable::Reset() {\n-  runtime.reset();\n-  subgraph.reset();\n-  return absl::OkStatus();\n-}\n-\n-absl::StatusOr<XnnFusionThunk::XnnExecutable>\n-XnnFusionThunk::CreateXnnExecutable(\n-    const XnnThreadpool& threadpool,\n-    absl::Span<const se::DeviceAddressBase> arguments_buffers) {\n-  bool capturing = !captured_arguments_ids_.empty();\n-  VLOG(3) << absl::StreamFormat(\n-      \"Create %s XNN executable for `%s` operation: num_created=%d\",\n-      capturing ? \"capturing\" : \"pooled\", info().op_name,\n-      capturing ? num_capturing_created_.fetch_add(1)\n-                : xnn_executable_pool_.num_created());\n-\n-  XnnExecutable executable;\n-\n-  // Keep track of the arguments captured by value.\n-  executable.captured_arguments = CaptureArguments(arguments_buffers);\n-\n-  if (builder_) {\n-    TF_ASSIGN_OR_RETURN(executable.subgraph, builder_(arguments_, results_));\n-  } else {\n-    TF_ASSIGN_OR_RETURN(\n-        executable.subgraph,\n-        capturing_builder_(arguments_, results_, arguments_buffers));\n-  }\n-\n-  uint32_t flags = XNN_FLAG_SLINKY_ENABLED | XNN_FLAG_SLINKY_STATIC_BOUNDS |\n-                   XNN_FLAG_DONT_SPIN_WORKERS;\n-\n-  TF_ASSIGN_OR_RETURN(\n-      executable.runtime, CreateXnnRuntime([&](xnn_runtime_t* runtime) {\n-        return xnn_create_runtime_with_threadpool(\n-            executable.subgraph.get(), /*weights_cache=*/nullptr,\n-            threadpool.get(), flags, runtime);\n-      }));\n-  XNN_RETURN_IF_ERROR(xnn_reshape_runtime(executable.runtime.get()));\n-\n-  return {std::move(executable)};\n-}\n-\n-absl::Status XnnFusionThunk::UpdateXnnExecutable(\n-    const XnnThreadpool& threadpool, XnnExecutable& executable,\n-    absl::Span<const se::DeviceAddressBase> arguments_buffers) {\n-  DCHECK(capturing_builder_) << \"XNN executable is not capturing arguments\";\n-  DCHECK_EQ(executable.captured_arguments.size(),\n-            captured_arguments_ids_.size())\n-      << \"Unexpected number of captured arguments\";\n-\n-  // If all arguments captured by value are the same as the last execution,\n-  // we can reuse the XNN executable.\n-  auto capture_arguments = CaptureArguments(arguments_buffers);\n-  if (executable.captured_arguments == capture_arguments) {\n-    VLOG(3) << absl::StreamFormat(\"Reuse XNN executable for `%s` operation\",\n-                                  info().op_name);\n-    return absl::OkStatus();\n-  }\n-\n-  VLOG(3) << absl::StreamFormat(\"Update XNN executable for `%s` operation\",\n-                                info().op_name);\n-\n-  TF_RETURN_IF_ERROR(executable.Reset());\n-\n-  // Keep track of the updated arguments captured by value.\n-  executable.captured_arguments = std::move(capture_arguments);\n-\n-  TF_ASSIGN_OR_RETURN(\n-      executable.subgraph,\n-      capturing_builder_(arguments_, results_, arguments_buffers));\n-\n-  uint32_t flags = XNN_FLAG_SLINKY_ENABLED | XNN_FLAG_SLINKY_STATIC_BOUNDS |\n-                   XNN_FLAG_DONT_SPIN_WORKERS;\n-\n-  TF_ASSIGN_OR_RETURN(\n-      executable.runtime, CreateXnnRuntime([&](xnn_runtime_t* runtime) {\n-        return xnn_create_runtime_with_threadpool(\n-            executable.subgraph.get(), /*weights_cache=*/nullptr,\n-            threadpool.get(), flags, runtime);\n-      }));\n-  XNN_RETURN_IF_ERROR(xnn_reshape_runtime(executable.runtime.get()));\n-\n-  return absl::OkStatus();\n-}\n-\n-std::vector<se::DeviceAddressBase> XnnFusionThunk::CaptureArguments(\n-    absl::Span<const se::DeviceAddressBase> arguments_buffers) {\n-  std::vector<se::DeviceAddressBase> captured_arguments_ids;\n-  captured_arguments_ids.reserve(captured_arguments_ids_.size());\n-  for (int64_t i = 0; i < captured_arguments_ids_.size(); ++i) {\n-    int32_t arg_index = captured_arguments_ids_[i];\n-    captured_arguments_ids.push_back(arguments_buffers[arg_index]);\n-  }\n-  return captured_arguments_ids;\n-}\n-\n-absl::StatusOr<std::unique_ptr<XnnFusionThunk>> XnnFusionThunk::Create(\n-    Options options, Info info, std::vector<Argument> arguments,\n-    std::vector<Result> results, Builder builder) {\n-  TF_RETURN_IF_ERROR(InitializeXnnPack());\n-\n-  return absl::WrapUnique(new XnnFusionThunk(\n-      XnnFusionKind::kFusion, std::move(options), std::move(info),\n-      std::move(arguments), std::move(results), std::move(builder)));\n-}\n-\n-absl::StatusOr<std::unique_ptr<XnnFusionThunk>> XnnFusionThunk::Create(\n-    Options options, Info info, std::vector<Argument> arguments,\n-    std::vector<Result> results, CapturingBuilder capturing_builder,\n-    absl::Span<const int64_t> captured_arguments_ids) {\n-  TF_RETURN_IF_ERROR(InitializeXnnPack());\n-\n-  return absl::WrapUnique(new XnnFusionThunk(\n-      XnnFusionKind::kFusion, std::move(options), std::move(info),\n-      std::move(arguments), std::move(results), std::move(capturing_builder),\n-      captured_arguments_ids));\n-}\n-\n-XnnFusionThunk::XnnFusionThunk(XnnFusionKind kind, Options options, Info info,\n-                               std::vector<Argument> arguments,\n-                               std::vector<Result> results, Builder builder)\n-    : Thunk(Kind::kXnnFusion, std::move(info)),\n-      xnn_fusion_kind_(kind),\n-      options_(std::move(options)),\n-      arguments_(std::move(arguments)),\n-      results_(std::move(results)),\n-      builder_(std::move(builder)),\n-      xnn_executable_pool_(\n-          absl::bind_front(&XnnFusionThunk::CreateXnnExecutable, this)) {}\n-\n-XnnFusionThunk::XnnFusionThunk(XnnFusionKind kind, Options options, Info info,\n-                               std::vector<Argument> arguments,\n-                               std::vector<Result> results,\n-                               CapturingBuilder capturing_builder,\n-                               absl::Span<const int64_t> captured_arguments_ids)\n-    : Thunk(Kind::kXnnFusion, std::move(info)),\n-      xnn_fusion_kind_(kind),\n-      options_(std::move(options)),\n-      arguments_(std::move(arguments)),\n-      results_(std::move(results)),\n-      capturing_builder_(std::move(capturing_builder)),\n-      captured_arguments_ids_(captured_arguments_ids.begin(),\n-                              captured_arguments_ids.end()),\n-      xnn_executable_pool_(\n-          absl::bind_front(&XnnFusionThunk::CreateXnnExecutable, this)) {}\n-\n-XnnFusionThunk::~XnnFusionThunk() = default;\n-\n-XnnFusionThunk::BufferUses XnnFusionThunk::buffer_uses() const {\n-  BufferUses buffer_uses;\n-  for (const Argument& argument : arguments_) {\n-    buffer_uses.push_back(BufferUse::Read(argument.slice, argument.shape));\n-  }\n-  for (const Result& result : results_) {\n-    buffer_uses.push_back(BufferUse::Write(result.slice, result.shape));\n-  }\n-\n-  return buffer_uses;\n-}\n-\n-const XnnThreadpool& GetXnnThreadpool(const Thunk::ExecuteParams& params) {\n-  static absl::NoDestructor<XnnThreadpool> no_threadpool(nullptr);\n-  return params.xnn_params ? params.xnn_params->threadpool : *no_threadpool;\n-}\n-\n-tsl::AsyncValueRef<XnnFusionThunk::ExecuteEvent> XnnFusionThunk::Execute(\n-    const ExecuteParams& params) {\n-  VLOG(3) << absl::StreamFormat(\"XNN %s `%s`: %s\", fusion_kind(),\n-                                info().op_name, fusion_description());\n-\n-  if (VLOG_IS_ON(3) && has_fusion_details()) {\n-    for (auto& detail : fusion_details()) {\n-      VLOG(3) << detail;\n-    }\n-  }\n-\n-  // Resolve device memory for arguments.\n-  absl::InlinedVector<se::DeviceAddressBase, 8> arguments_buffers;\n-  arguments_buffers.resize(arguments_.size());\n-  for (size_t i = 0; i < arguments_.size(); ++i) {\n-    Argument& argument = arguments_[i];\n-\n-    TF_ASSIGN_OR_RETURN(\n-        arguments_buffers[i],\n-        params.buffer_allocations->GetDeviceAddress(argument.slice));\n-\n-    VLOG(3) << absl::StreamFormat(\"  %s: %s in slice %s (%p)\", argument_name(i),\n-                                  argument.shape.ToString(true),\n-                                  argument.slice.ToString(),\n-                                  arguments_buffers[i].opaque());\n-  }\n-\n-  // Resolve device memory for results.\n-  absl::InlinedVector<se::DeviceAddressBase, 4> results_buffers;\n-  results_buffers.resize(results_.size());\n-  for (size_t i = 0; i < results_.size(); ++i) {\n-    Result& result = results_[i];\n-\n-    TF_ASSIGN_OR_RETURN(\n-        results_buffers[i],\n-        params.buffer_allocations->GetDeviceAddress(results_[i].slice));\n-\n-    VLOG(3) << absl::StreamFormat(\"  %s: %s in slice %s (%p)\", result_name(i),\n-                                  result.shape.ToString(true),\n-                                  result.slice.ToString(),\n-                                  results_buffers[i].opaque());\n-  }\n-\n-  DCHECK(builder_ || capturing_builder_) << \"One of the builders must be set.\";\n-\n-  auto invoke = [&](typename XnnExecutablePool::BorrowedObject executable) {\n-    auto executed = executable->Invoke(\n-        GetXnnThreadpool(params), absl::MakeSpan(arguments_buffers),\n-        absl::MakeSpan(results_buffers), [&](size_t id) {\n-          return absl::c_linear_search(captured_arguments_ids_, id);\n-        });\n-\n-    // Do not return executable to the pool until the execution is done.\n-    executed.AndThen([executable = std::move(executable)] {});\n-    return executed;\n-  };\n-\n-  // Borrow XnnExecutable from the pool.\n-  TF_ASSIGN_OR_RETURN(auto executable,\n-                      xnn_executable_pool_.GetOrCreate(GetXnnThreadpool(params),\n-                                                       arguments_buffers));\n-\n-  // If XNN graph doesn't capture any of the arguments by value, we can execute\n-  // XnnExecutable immediately.\n-  if (captured_arguments_ids_.empty()) {\n-    return invoke(std::move(executable));\n-  }\n-\n-  // Otherwise reset XnnExecutable to capture new arguments buffers.\n-  TF_RETURN_IF_ERROR(UpdateXnnExecutable(GetXnnThreadpool(params), *executable,\n-                                         arguments_buffers));\n-  return invoke(std::move(executable));\n-}\n-\n-}  // namespace xla::cpu"
        },
        {
            "sha": "21deb08bfd6fd6cb895b4e5436caec5bcd0e2c5c",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk.h",
            "status": "removed",
            "additions": 0,
            "deletions": 184,
            "changes": 184,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,184 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_BACKENDS_CPU_RUNTIME_XNNPACK_XNN_FUSION_THUNK_H_\n-#define XLA_BACKENDS_CPU_RUNTIME_XNNPACK_XNN_FUSION_THUNK_H_\n-\n-#include <stdbool.h>\n-\n-#include <atomic>\n-#include <cstddef>\n-#include <cstdint>\n-#include <memory>\n-#include <ostream>\n-#include <string>\n-#include <vector>\n-\n-#include \"absl/functional/any_invocable.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/str_cat.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/backends/cpu/runtime/thunk.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_interop.h\"\n-#include \"xla/runtime/object_pool.h\"\n-#include \"xla/service/buffer_assignment.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/stream_executor/device_address.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n-\n-namespace xla::cpu {\n-\n-// XNN fusion thunk encapsulates XNNPACK subgraph contructed from an XLA fusion\n-// operation, where each HLO op has a corresponding XNNPACK operator.\n-class XnnFusionThunk : public Thunk {\n- public:\n-  enum class XnnFusionKind {\n-    kFusion,\n-    kDot,\n-    kConvolution,\n-  };\n-\n-  static absl::string_view XnnFusionKindToString(XnnFusionKind kind);\n-\n-  ~XnnFusionThunk() override;\n-\n-  struct Options {\n-    // Pass XnnThreadpool constructed from the intra-op threadpool to the\n-    // XNNPACK runtime to allow XNNPACK to parallelize the execution.\n-    bool use_threadpool = true;\n-  };\n-\n-  struct Argument {\n-    BufferAllocation::Slice slice;\n-    Shape shape;\n-  };\n-\n-  struct Result {\n-    BufferAllocation::Slice slice;\n-    Shape shape;\n-  };\n-\n-  // Builder function constructs XNNPACK subgraph for the fusion operation.\n-  using Builder = absl::AnyInvocable<absl::StatusOr<XnnSubgraph>(\n-      absl::Span<const Argument> arguments, absl::Span<const Result> results)>;\n-\n-  // Builder function that constructs XNNPACK subgraph for the fusion operation\n-  // and captures some of the arguments buffers by value. Such XNNPACK subgraphs\n-  // can't be reused if captured arguments are not the same, and can lead to\n-  // crashes and undefined behavior if captured arguments are destroyed.\n-  // Capturing arguments by value allows XNNPACK to do packing at graph compile\n-  // time, and avoid re-packing costs at run time (at inference weights stay\n-  // constant, i.e. convolution filters and one of the dot arguments).\n-  using CapturingBuilder = absl::AnyInvocable<absl::StatusOr<XnnSubgraph>(\n-      absl::Span<const Argument> arguments, absl::Span<const Result> results,\n-      absl::Span<const se::DeviceAddressBase> arguments_buffers)>;\n-\n-  static absl::StatusOr<std::unique_ptr<XnnFusionThunk>> Create(\n-      Options options, Info info, std::vector<Argument> arguments,\n-      std::vector<Result> results, Builder builder);\n-\n-  static absl::StatusOr<std::unique_ptr<XnnFusionThunk>> Create(\n-      Options options, Info info, std::vector<Argument> arguments,\n-      std::vector<Result> results, CapturingBuilder capturing_builder,\n-      absl::Span<const int64_t> captured_arguments_ids);\n-\n-  tsl::AsyncValueRef<ExecuteEvent> Execute(const ExecuteParams& params) final;\n-\n-  bool ExecuteMayBlock() const final { return true; }\n-\n-  BufferUses buffer_uses() const final;\n-\n-  Options options() const { return options_; }\n-\n-  XnnFusionKind xnn_fusion_kind() const { return xnn_fusion_kind_; }\n-\n- protected:\n-  XnnFusionThunk(XnnFusionKind kind, Options options, Info info,\n-                 std::vector<Argument> arguments, std::vector<Result> results,\n-                 Builder builder);\n-\n-  XnnFusionThunk(XnnFusionKind kind, Options options, Info info,\n-                 std::vector<Argument> arguments, std::vector<Result> results,\n-                 CapturingBuilder capturing_builder,\n-                 absl::Span<const int64_t> captured_arguments_ids);\n-\n-  // Extension points for subclasses to customize the logging behavior.\n-  virtual std::string fusion_kind() const { return \"fusion\"; }\n-  virtual std::string fusion_description() const { return \"\"; }\n-\n-  virtual bool has_fusion_details() const { return false; }\n-  virtual std::vector<std::string> fusion_details() const { return {}; }\n-\n-  virtual std::string argument_name(size_t index) const {\n-    return absl::StrCat(\"arg #\", index);\n-  }\n-\n-  virtual std::string result_name(size_t index) const {\n-    return absl::StrCat(\"res #\", index);\n-  }\n-\n- private:\n-  // XNNPACK subgraph + runtime instantiated and ready for execution.\n-  struct XnnExecutable;\n-\n-  // Creates XnnExecutable for the fusion operation using one of the builders.\n-  absl::StatusOr<XnnExecutable> CreateXnnExecutable(\n-      const XnnThreadpool& threadpool,\n-      absl::Span<const se::DeviceAddressBase> arguments_buffers);\n-\n-  // Updates XnnExecutable to the XNN subgraph constructed with the given\n-  // arguments buffers.\n-  absl::Status UpdateXnnExecutable(\n-      const XnnThreadpool& threadpool, XnnExecutable& executable,\n-      absl::Span<const se::DeviceAddressBase> arguments_buffers);\n-\n-  // Returns the list of captured arguments buffers.\n-  std::vector<se::DeviceAddressBase> CaptureArguments(\n-      absl::Span<const se::DeviceAddressBase> arguments_buffers);\n-\n-  XnnFusionKind xnn_fusion_kind_;\n-  Options options_;\n-\n-  std::vector<Argument> arguments_;\n-  std::vector<Result> results_;\n-\n-  // Builder that constructs XNNPACK subgraph for the fusion operation.\n-  Builder builder_;\n-\n-  // Builder that constructs XNNPACK subgraph for the fusion operation and\n-  // captures some of the arguments buffers by value. Such subgraphs can't be\n-  // reused if captured arguments changed since the last execution.\n-  CapturingBuilder capturing_builder_;\n-\n-  // Indices of arguments that are captured by XNNPACK subgraph by value.\n-  std::vector<int64_t> captured_arguments_ids_;\n-\n-  // XLA:CPU executable can be called concurrently from multiple threads,\n-  // and we need to keep a pool of XNNPACK executables to avoid data races.\n-  using XnnExecutablePool = ObjectPool<XnnExecutable, const XnnThreadpool&,\n-                                       absl::Span<const se::DeviceAddressBase>>;\n-  XnnExecutablePool xnn_executable_pool_;\n-\n-  // The number of XNNPACK executables created for capturing graphs.\n-  std::atomic<int64_t> num_capturing_created_{0};\n-};\n-\n-std::ostream& operator<<(std::ostream& os, XnnFusionThunk::XnnFusionKind kind);\n-\n-}  // namespace xla::cpu\n-\n-#endif  // XLA_BACKENDS_CPU_RUNTIME_XNNPACK_XNN_FUSION_THUNK_H_"
        },
        {
            "sha": "4f802f04bc6530a8138736ac117ac5306eb5a031",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 155,
            "changes": 155,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_fusion_thunk_test.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,155 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk.h\"\n-\n-#include <cstddef>\n-#include <cstdint>\n-#include <limits>\n-#include <string>\n-#include <utility>\n-#include <vector>\n-\n-#include \"xnnpack.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/str_cat.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/backends/cpu/runtime/buffer_allocations.h\"\n-#include \"xla/backends/cpu/runtime/thunk.h\"\n-#include \"xla/backends/cpu/runtime/thunk_testlib.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_interop.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_threadpool.h\"\n-#include \"xla/literal_util.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/shape_util.h\"\n-#include \"xla/tsl/concurrency/async_value_ref.h\"\n-#include \"xla/tsl/platform/env.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/tsl/platform/test.h\"\n-#include \"xla/tsl/platform/threadpool.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-#define EIGEN_USE_THREADS\n-#include \"unsupported/Eigen/CXX11/Tensor\"\n-\n-namespace xla::cpu {\n-namespace {\n-\n-static absl::StatusOr<XnnSubgraph> BuildBinaryAddSubgraph(\n-    absl::Span<const XnnFusionThunk::Argument> arguments,\n-    absl::Span<const XnnFusionThunk::Result> results) {\n-  TF_ASSIGN_OR_RETURN(XnnSubgraph subgraph,\n-                      CreateXnnSubgraph([&](xnn_subgraph_t* subgraph) {\n-                        return xnn_create_subgraph(\n-                            /*external_value_ids=*/3,\n-                            /*flags=*/0, subgraph);\n-                      }));\n-\n-  auto dims = [](absl::Span<const int64_t> dims) -> std::vector<size_t> {\n-    return {dims.begin(), dims.end()};\n-  };\n-\n-  uint32_t lhs_id = XNN_INVALID_VALUE_ID;\n-  uint32_t rhs_id = XNN_INVALID_VALUE_ID;\n-  uint32_t out_id = XNN_INVALID_VALUE_ID;\n-\n-  std::vector<size_t> lhs_dims = dims(arguments[0].shape.dimensions());\n-  std::vector<size_t> rhs_dims = dims(arguments[1].shape.dimensions());\n-  std::vector<size_t> out_dims = dims(results[0].shape.dimensions());\n-\n-  XNN_RETURN_IF_ERROR(xnn_define_tensor_value(\n-      subgraph.get(), xnn_datatype_fp32, lhs_dims.size(), lhs_dims.data(),\n-      nullptr,\n-      /*external_id=*/0, XNN_VALUE_FLAG_EXTERNAL_INPUT, &lhs_id));\n-\n-  XNN_RETURN_IF_ERROR(xnn_define_tensor_value(\n-      subgraph.get(), xnn_datatype_fp32, rhs_dims.size(), rhs_dims.data(),\n-      nullptr,\n-      /*external_id=*/1, XNN_VALUE_FLAG_EXTERNAL_INPUT, &rhs_id));\n-\n-  XNN_RETURN_IF_ERROR(xnn_define_tensor_value(\n-      subgraph.get(), xnn_datatype_fp32, out_dims.size(), out_dims.data(),\n-      nullptr,\n-      /*external_id=*/2, XNN_VALUE_FLAG_EXTERNAL_OUTPUT, &out_id));\n-\n-  xnn_binary_params params = {-std::numeric_limits<float>::infinity(),\n-                              std::numeric_limits<float>::infinity()};\n-\n-  XNN_RETURN_IF_ERROR(xnn_define_binary(subgraph.get(), xnn_binary_add, &params,\n-                                        lhs_id, rhs_id, out_id, /*flags=*/0));\n-\n-  return subgraph;\n-}\n-\n-class XnnFusionThunkTest : public testing::TestWithParam<bool> {\n- public:\n-  static std::string Name(const ::testing::TestParamInfo<bool>& info) {\n-    return absl::StrCat(info.param ? \"threadpool\" : \"single_threaded\");\n-  }\n-\n- protected:\n-  bool use_threadpool() const { return GetParam(); }\n-};\n-\n-TEST_P(XnnFusionThunkTest, ElementwiseAdd) {\n-  tsl::thread::ThreadPool threads(tsl::Env::Default(), \"test\", 8);\n-  Eigen::ThreadPoolDevice device(threads.AsEigenThreadPool(),\n-                                 threads.NumThreads());\n-\n-  auto lhs = LiteralUtil::CreateR1<float>({1.0, 2.0, 3.0, 4.0});\n-  auto rhs = LiteralUtil::CreateR1<float>({4.0, 3.0, 2.0, 1.0});\n-  auto out = LiteralUtil::CreateR1<float>({0.0, 0.0, 0.0, 0.0});\n-\n-  BufferAllocations allocations = CreateBufferAllocations(lhs, rhs, out);\n-\n-  auto [lhs_alloc, rhs_alloc, out_alloc] =\n-      CreateBufferAllocation(lhs, rhs, out);\n-  auto [lhs_slice, rhs_slice, out_slice] =\n-      CreateBufferAllocationSlice(lhs_alloc, rhs_alloc, out_alloc);\n-\n-  Shape shape = ShapeUtil::MakeShape(F32, {2, 2});\n-\n-  XnnFusionThunk::Argument lhs_arg = {lhs_slice, shape};\n-  XnnFusionThunk::Argument rhs_arg = {rhs_slice, shape};\n-  XnnFusionThunk::Result out_res = {out_slice, shape};\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto thunk, XnnFusionThunk::Create(\n-                      XnnFusionThunk::Options{use_threadpool()}, {\"fusion\"},\n-                      {lhs_arg, rhs_arg}, {out_res}, &BuildBinaryAddSubgraph));\n-\n-  XnnThreadpool threadpool;\n-  if (use_threadpool()) {\n-    TF_ASSERT_OK_AND_ASSIGN(threadpool, CreateXnnThreadpool(&device));\n-  }\n-  Thunk::XnnParams xnn_params(std::move(threadpool));\n-\n-  Thunk::ExecuteParams params;\n-  params.buffer_allocations = &allocations;\n-  params.intra_op_threadpool = use_threadpool() ? &device : nullptr;\n-  params.xnn_params = &xnn_params;\n-\n-  auto execute_event = thunk->Execute(params);\n-  tsl::BlockUntilReady(execute_event);\n-  ASSERT_FALSE(execute_event.IsError()) << execute_event.GetError();\n-\n-  EXPECT_EQ(out, LiteralUtil::CreateR1<float>({5.0, 5.0, 5.0, 5.0}));\n-}\n-\n-INSTANTIATE_TEST_SUITE_P(XnnFusion, XnnFusionThunkTest, ::testing::Bool(),\n-                         XnnFusionThunkTest::Name);\n-\n-}  // namespace\n-}  // namespace xla::cpu"
        },
        {
            "sha": "3d219ee8b267f5fc408d8b6ad396f0f02e065c46",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_interop.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 71,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_interop.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_interop.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_interop.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,71 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_interop.h\"\n-\n-#include \"experimental.h\"  // xnnpack\n-#include \"xnnpack.h\"\n-#include \"absl/functional/function_ref.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"xla/primitive_util.h\"\n-#include \"xla/util.h\"\n-\n-namespace xla::cpu {\n-\n-absl::Status InitializeXnnPack() {\n-  static xnn_status status = xnn_initialize(/*allocator=*/nullptr);\n-  if (status != xnn_status_success) {\n-    return Internal(\"XNNPACK initialization failed\");\n-  }\n-  return absl::OkStatus();\n-}\n-\n-absl::StatusOr<XnnSubgraph> CreateXnnSubgraph(\n-    absl::FunctionRef<xnn_status(xnn_subgraph_t*)> builder) {\n-  xnn_subgraph_t subgraph = nullptr;\n-  XNN_RETURN_IF_ERROR(builder(&subgraph));\n-  return XnnSubgraph(subgraph);\n-}\n-\n-absl::StatusOr<XnnRuntime> CreateXnnRuntime(\n-    absl::FunctionRef<xnn_status(xnn_runtime_t*)> builder) {\n-  xnn_runtime_t runtime = nullptr;\n-  XNN_RETURN_IF_ERROR(builder(&runtime));\n-  return XnnRuntime(runtime);\n-}\n-\n-absl::StatusOr<XnnThreadpool> CreateXnnThreadpool(\n-    absl::FunctionRef<xnn_status(xnn_threadpool_t*)> builder) {\n-  xnn_threadpool_t threadpool = nullptr;\n-  XNN_RETURN_IF_ERROR(builder(&threadpool));\n-  return XnnThreadpool(threadpool);\n-}\n-\n-absl::StatusOr<xnn_datatype> XnnDatatype(const PrimitiveType& type) {\n-  switch (type) {\n-    case BF16:\n-      return xnn_datatype_bf16;\n-    case F16:\n-      return xnn_datatype_fp16;\n-    case F32:\n-      return xnn_datatype_fp32;\n-    default:\n-      return InvalidArgument(\"Unsupported XNNPACK data type: %s\",\n-                             primitive_util::LowercasePrimitiveTypeName(type));\n-  }\n-}\n-\n-}  // namespace xla::cpu"
        },
        {
            "sha": "e591665c0f38f3df7f07b4819d212d8ad0e3e325",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_interop.h",
            "status": "removed",
            "additions": 0,
            "deletions": 125,
            "changes": 125,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_interop.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_interop.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_interop.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,125 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_BACKENDS_CPU_RUNTIME_XNNPACK_XNN_INTEROP_H_\n-#define XLA_BACKENDS_CPU_RUNTIME_XNNPACK_XNN_INTEROP_H_\n-\n-#include <memory>\n-\n-#include \"experimental.h\"  // xnnpack\n-#include \"xnnpack.h\"\n-#include \"absl/base/optimization.h\"\n-#include \"absl/functional/function_ref.h\"\n-#include \"absl/status/status.h\"\n-#include \"xla/tsl/platform/logging.h\"\n-#include \"xla/util.h\"\n-\n-namespace xla::cpu {\n-\n-//===----------------------------------------------------------------------===//\n-// XNNPACK status to ABSL status conversion macros.\n-//===----------------------------------------------------------------------===//\n-\n-#define XNN_RETURN_IF_ERROR(expr)             \\\n-  do {                                        \\\n-    absl::Status s = XnnStatusToStatus(expr); \\\n-    if (!s.ok()) {                            \\\n-      return s;                               \\\n-    }                                         \\\n-  } while (0)\n-\n-#define XNN_LOG_IF_ERROR(expr)                         \\\n-  do {                                                 \\\n-    absl::Status s = XnnStatusToStatus(expr);          \\\n-    if (!s.ok()) {                                     \\\n-      LOG(ERROR) << \"XNNPACK operation failed: \" << s; \\\n-    }                                                  \\\n-  } while (0)\n-\n-// Statically initializes XNNPACK for the current process.\n-absl::Status InitializeXnnPack();\n-\n-// Converts XNNPACK status to absl::Status.\n-inline absl::Status XnnStatusToStatus(xnn_status status) {\n-  if (ABSL_PREDICT_TRUE(status == xnn_status_success)) {\n-    return absl::OkStatus();\n-  }\n-\n-  auto error_message = [](xnn_status status) {\n-    switch (status) {\n-      case xnn_status_success:\n-        return \"\";\n-      case xnn_status_uninitialized:\n-        return \"uninitialized\";\n-      case xnn_status_invalid_parameter:\n-        return \"invalid parameter\";\n-      case xnn_status_invalid_state:\n-        return \"invalid state\";\n-      case xnn_status_unsupported_parameter:\n-        return \"unsupported parameter\";\n-      case xnn_status_unsupported_hardware:\n-        return \"unsupported hardware\";\n-      case xnn_status_out_of_memory:\n-        return \"out of memory\";\n-      case xnn_status_reallocation_required:\n-        return \"reallocation required\";\n-      case xnn_status_deprecated:\n-        return \"deprecated\";\n-    }\n-  };\n-\n-  return Internal(\"XNNPACK operation failed: %s\", error_message(status));\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// XLA to XNNPACK type conversions.\n-//===----------------------------------------------------------------------===//\n-\n-absl::StatusOr<xnn_datatype> XnnDatatype(const PrimitiveType& type);\n-\n-//===----------------------------------------------------------------------===//\n-// RAII wrappers for XNNPACK types.\n-//===----------------------------------------------------------------------===//\n-\n-namespace internal {\n-struct XnnDeleter {\n-  void operator()(xnn_subgraph* subgraph) {\n-    XNN_LOG_IF_ERROR(xnn_delete_subgraph(subgraph));\n-  }\n-  void operator()(xnn_runtime* runtime) {\n-    XNN_LOG_IF_ERROR(xnn_delete_runtime(runtime));\n-  }\n-  void operator()(xnn_threadpool* threadpool) {\n-    XNN_LOG_IF_ERROR(xnn_delete_threadpool(threadpool));\n-  }\n-};\n-}  // namespace internal\n-\n-using XnnSubgraph = std::unique_ptr<xnn_subgraph, internal::XnnDeleter>;\n-using XnnRuntime = std::unique_ptr<xnn_runtime, internal::XnnDeleter>;\n-using XnnThreadpool = std::unique_ptr<xnn_threadpool, internal::XnnDeleter>;\n-\n-absl::StatusOr<XnnSubgraph> CreateXnnSubgraph(\n-    absl::FunctionRef<xnn_status(xnn_subgraph_t*)> builder);\n-\n-absl::StatusOr<XnnRuntime> CreateXnnRuntime(\n-    absl::FunctionRef<xnn_status(xnn_runtime_t*)> builder);\n-\n-absl::StatusOr<XnnThreadpool> CreateXnnThreadpool(\n-    absl::FunctionRef<xnn_status(xnn_threadpool_t*)> builder);\n-\n-}  // namespace xla::cpu\n-\n-#endif  // XLA_BACKENDS_CPU_RUNTIME_XNNPACK_XNN_INTEROP_H_"
        },
        {
            "sha": "8ca982278e689e6b3e42945300288927fd1fde7c",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_threadpool.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 62,
            "changes": 62,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_threadpool.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_threadpool.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_threadpool.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,62 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_threadpool.h\"\n-\n-#include <cstdint>\n-\n-#include \"experimental.h\"  // xnnpack\n-#include \"absl/base/optimization.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_interop.h\"\n-\n-#define EIGEN_USE_THREADS\n-#include \"Eigen/ThreadPool\"\n-#include \"unsupported/Eigen/CXX11/Tensor\"\n-\n-namespace xla::cpu {\n-\n-static int32_t NumThreads(void* pool) {\n-  if (ABSL_PREDICT_FALSE(pool == nullptr)) {\n-    return 0;\n-  }\n-  return reinterpret_cast<Eigen::ThreadPoolInterface*>(pool)->NumThreads();\n-}\n-\n-static void Schedule(void* pool, void* context, void (*task)(void* context)) {\n-  if (ABSL_PREDICT_FALSE(pool == nullptr)) {\n-    (*task)(context);\n-  }\n-  reinterpret_cast<Eigen::ThreadPoolInterface*>(pool)->Schedule(\n-      [task, context]() { (*task)(context); });\n-}\n-\n-// And adaptor from Eigen::ThreadPoolInterface to xnn_threadpool_t.\n-static constexpr xnn_scheduler_v2 kXnnScheduler = {&NumThreads, &Schedule};\n-\n-absl::StatusOr<XnnThreadpool> CreateXnnThreadpool(\n-    Eigen::ThreadPoolInterface* threadpool) {\n-  return CreateXnnThreadpool([&](xnn_threadpool_t* xnn_threadpool) {\n-    return xnn_create_threadpool_v2(kXnnScheduler, threadpool, /*flags=*/1,\n-                                    xnn_threadpool);\n-  });\n-}\n-\n-absl::StatusOr<XnnThreadpool> CreateXnnThreadpool(\n-    const Eigen::ThreadPoolDevice* device) {\n-  return CreateXnnThreadpool(device->getPool());\n-}\n-\n-}  // namespace xla::cpu"
        },
        {
            "sha": "d154af861814ff55d5cad86b6926f18a7d0f2f3b",
            "filename": "third_party/xla/xla/backends/cpu/runtime/xnnpack/xnn_threadpool.h",
            "status": "removed",
            "additions": 0,
            "deletions": 39,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_threadpool.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_threadpool.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fxnnpack%2Fxnn_threadpool.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,39 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_BACKENDS_CPU_RUNTIME_XNNPACK_XNN_THREADPOOL_H_\n-#define XLA_BACKENDS_CPU_RUNTIME_XNNPACK_XNN_THREADPOOL_H_\n-\n-#include \"absl/status/statusor.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_interop.h\"\n-\n-namespace Eigen {\n-struct ThreadPoolDevice;\n-class ThreadPoolInterface;\n-}  // namespace Eigen\n-\n-namespace xla::cpu {\n-\n-// Creates an XNNPACK threadpool from an Eigen threadpool.\n-absl::StatusOr<XnnThreadpool> CreateXnnThreadpool(\n-    Eigen::ThreadPoolInterface* threadpool);\n-\n-// Creates an XNNPACK threadpool from an Eigen ThreadPoolDevice.\n-absl::StatusOr<XnnThreadpool> CreateXnnThreadpool(\n-    const Eigen::ThreadPoolDevice* device);\n-\n-}  // namespace xla::cpu\n-\n-#endif  // XLA_BACKENDS_CPU_RUNTIME_XNNPACK_XNN_THREADPOOL_H_"
        },
        {
            "sha": "fde7907429e1eb7dd6b2cefca3d96a24f861e801",
            "filename": "third_party/xla/xla/backends/cpu/transforms/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 55,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2FBUILD?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -26,7 +26,6 @@ cc_library(\n     deps = [\n         \":library_matcher\",\n         \":onednn_matcher\",\n-        \":xnn_matcher\",\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n@@ -54,7 +53,6 @@ xla_cc_test(\n         \":library_rewriter\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n-        \"//xla/backends/cpu:xnn_gemm_config\",\n         \"//xla/backends/cpu/codegen:target_machine_features\",\n         \"//xla/backends/cpu/codegen:target_machine_test_base\",\n         \"//xla/hlo/ir:hlo\",\n@@ -100,22 +98,6 @@ onednn_graph_cc_library(\n     ],\n )\n \n-cc_library(\n-    name = \"xnn_matcher\",\n-    hdrs = [\"xnn_matcher.h\"],\n-    deps = [\n-        \":library_matcher\",\n-        \"//xla/backends/cpu:xnn_support\",\n-        \"//xla/backends/cpu/codegen:target_machine_features\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"@com_google_absl//absl/base:no_destructor\",\n-        \"@com_google_absl//absl/container:flat_hash_set\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings:string_view\",\n-        \"@local_tsl//tsl/platform:protobuf\",\n-    ],\n-)\n-\n cc_library(\n     name = \"ynn_matcher\",\n     hdrs = [\"ynn_matcher.h\"],\n@@ -130,40 +112,3 @@ cc_library(\n         \"@local_tsl//tsl/platform:protobuf\",\n     ] + if_ynnpack([\"//xla/backends/cpu:ynn_support\"]),\n )\n-\n-cc_library(\n-    name = \"xnn_graph_fusion\",\n-    srcs = [\"xnn_graph_fusion.cc\"],\n-    hdrs = [\"xnn_graph_fusion.h\"],\n-    deps = [\n-        \"//xla:shape_util\",\n-        \"//xla:xla_proto_cc\",\n-        \"//xla/backends/cpu:xnn_support\",\n-        \"//xla/backends/cpu/runtime/xnnpack:xnn_interop\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/service:call_graph\",\n-        \"//xla/service:instruction_fusion\",\n-        \"//xla/service/cpu:backend_config_proto_cc\",\n-        \"@com_google_absl//absl/container:flat_hash_set\",\n-        \"@com_google_absl//absl/log:check\",\n-        \"@com_google_absl//absl/strings:string_view\",\n-    ],\n-)\n-\n-xla_cc_test(\n-    name = \"xnn_graph_fusion_test\",\n-    srcs = [\"xnn_graph_fusion_test.cc\"],\n-    deps = [\n-        \":xnn_graph_fusion\",\n-        \"//xla:xla_data_proto_cc\",\n-        \"//xla:xla_proto_cc\",\n-        \"//xla/backends/cpu:xnn_support\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n-        \"//xla/hlo/utils:hlo_matchers\",\n-        \"//xla/service/cpu:backend_config_proto_cc\",\n-        \"//xla/tests:xla_internal_test_main\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@com_google_googletest//:gtest\",\n-    ],\n-)"
        },
        {
            "sha": "38dba96cac1c676327ff291bf25ef0d2a079be8d",
            "filename": "third_party/xla/xla/backends/cpu/transforms/library_rewriter.h",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Flibrary_rewriter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Flibrary_rewriter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Flibrary_rewriter.h?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -28,7 +28,6 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"xla/backends/cpu/codegen/target_machine_features.h\"\n #include \"xla/backends/cpu/transforms/library_matcher.h\"\n-#include \"xla/backends/cpu/transforms/xnn_matcher.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n@@ -53,10 +52,8 @@ enum class FusionDirection {\n \n struct LibraryRewriterOptions {\n   bool use_onednn = false;\n-  bool use_xnnpack = false;\n   bool use_ynnpack = false;\n   const tsl::protobuf::RepeatedField<int>* onednn_fusion_types = nullptr;\n-  const tsl::protobuf::RepeatedField<int>* xnn_fusion_types = nullptr;\n   const tsl::protobuf::RepeatedField<int>* ynn_fusion_types = nullptr;\n };\n \n@@ -75,11 +72,6 @@ class LibraryRewriter : public HloModulePass {\n           target_machine_features_, options_.onednn_fusion_types));\n     }\n #endif  // XLA_ONEDNN_USE_GRAPH_API\n-    if (options_.use_xnnpack && options_.xnn_fusion_types != nullptr &&\n-        !options_.xnn_fusion_types->empty()) {\n-      libs_.push_back(std::make_unique<XnnMatcher>(target_machine_features_,\n-                                                   options_.xnn_fusion_types));\n-    }\n #ifdef XLA_YNNPACK\n     if (options_.use_ynnpack && options_.ynn_fusion_types != nullptr &&\n         !options_.ynn_fusion_types->empty()) {"
        },
        {
            "sha": "430986457d215f538d818557c4eb346f25af2a69",
            "filename": "third_party/xla/xla/backends/cpu/transforms/library_rewriter_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Flibrary_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Flibrary_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Flibrary_rewriter_test.cc?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -30,7 +30,6 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"xla/backends/cpu/codegen/target_machine_features.h\"\n #include \"xla/backends/cpu/codegen/target_machine_test_base.h\"\n-#include \"xla/backends/cpu/xnn_gemm_config.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -79,9 +78,6 @@ class CpuLibraryTest : public TargetMachineTestBase {\n             /*triple_string=*/\"x86_64-unknown-linux-gnu\", spec.cpu_name,\n             spec.features);\n \n-    // Override XnnGemmConfig.\n-    GetXnnGemmConfig().SetTestFilter([](const XnnGemm&) { return true; });\n-\n     // Create an HLO module with the specified input and output data types.\n     std::string hlo_text = absl::StrReplaceAll(\n         hlo_template,\n@@ -100,15 +96,12 @@ class CpuLibraryTest : public TargetMachineTestBase {\n     }\n     tsl::protobuf::RepeatedField<int> empty_fusion_types;\n     bool use_onednn = spec.lib == \"onednn\";\n-    bool use_xnnpack = spec.lib == \"xnn\";\n     bool use_ynnpack = spec.lib == \"ynn\";\n     LibraryRewriterOptions options = {\n         use_onednn,\n-        use_xnnpack,\n         use_ynnpack,\n         /*onednn_fusion_types=*/\n         use_onednn ? &fusion_types : &empty_fusion_types,\n-        /*xnn_fusion_types=*/use_xnnpack ? &fusion_types : &empty_fusion_types,\n         /*ynn_fusion_types=*/use_ynnpack ? &fusion_types : &empty_fusion_types,\n     };\n     LibraryRewriter rewriter(features.get(), options);"
        },
        {
            "sha": "b360691f66f6d6a59aed116759cc2b9de140bbac",
            "filename": "third_party/xla/xla/backends/cpu/transforms/xnn_graph_fusion.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 156,
            "changes": 156,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fxnn_graph_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fxnn_graph_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fxnn_graph_fusion.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,156 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/cpu/transforms/xnn_graph_fusion.h\"\n-\n-#include <algorithm>\n-#include <cstdint>\n-#include <memory>\n-#include <vector>\n-\n-#include \"absl/container/flat_hash_set.h\"\n-#include \"absl/log/check.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_interop.h\"\n-#include \"xla/backends/cpu/xnn_support.h\"\n-#include \"xla/hlo/ir/hlo_casting_utils.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/primitive_util.h\"\n-#include \"xla/service/call_graph.h\"\n-#include \"xla/service/cpu/backend_config.pb.h\"\n-#include \"xla/service/instruction_fusion.h\"\n-#include \"xla/xla.pb.h\"\n-\n-namespace xla::cpu {\n-\n-namespace {\n-\n-bool IsWideningConvert(const HloInstruction* instr) {\n-  return instr->opcode() == HloOpcode::kConvert &&\n-         primitive_util::BitWidth(instr->operand(0)->shape().element_type()) <\n-             primitive_util::BitWidth(instr->shape().element_type());\n-}\n-\n-}  // namespace\n-\n-FusionDecision XnnGraphFusion::ShouldFuse(HloInstruction* consumer,\n-                                          int64_t operand_index) {\n-  if (!IsXnnGraphFusion(consumer) && !IsOpSupported(consumer)) {\n-    return FusionDecision::Forbid(\"Unsupported consumer\");\n-  }\n-\n-  if (consumer->opcode() == HloOpcode::kBroadcast) {\n-    return FusionDecision::Forbid(\n-        \"Do not start growing fusions from broadcasts\");\n-  }\n-\n-  if (IsWideningConvert(consumer)) {\n-    // We don't want to start a fusion with a widening convert, because that\n-    // makes the buffer the fusion writes to bigger, and it would be better to\n-    // fuse the convert into the consumer of the convert.\n-    return FusionDecision::Forbid(\n-        \"Do not start growing fusions from widening converts\");\n-  }\n-\n-  HloInstruction* producer = consumer->mutable_operand(operand_index);\n-  if (!(producer->opcode() == HloOpcode::kParameter ||\n-        IsOpSupported(producer))) {\n-    return FusionDecision::Forbid(\"Unsupported producer\");\n-  }\n-  return FusionDecision::Allow();\n-}\n-\n-HloInstruction::FusionKind XnnGraphFusion::ChooseKind(\n-    const HloInstruction* producer, const HloInstruction* consumer) {\n-  return HloInstruction::FusionKind::kCustom;\n-}\n-\n-HloInstruction* XnnGraphFusion::Fuse(HloInstruction* producer,\n-                                     HloInstruction* consumer,\n-                                     HloComputation* computation) {\n-  HloInstruction* fusion =\n-      InstructionFusion::Fuse(producer, consumer, computation);\n-\n-  BackendConfig backend_config;\n-  FusionBackendConfig* fusion_config = backend_config.mutable_fusion_config();\n-  fusion_config->set_kind(kXnnFusionKind);\n-  CHECK(backend_config.has_fusion_config());\n-  CHECK_OK(fusion->set_backend_config(backend_config));\n-  return fusion;\n-}\n-\n-std::vector<HloComputation*> XnnGraphFusion::GetNonFusionComputations(\n-    HloModule* module,\n-    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n-  std::vector<HloComputation*> non_fusion_computations =\n-      InstructionFusion::GetNonFusionComputations(module, execution_threads);\n-  std::unique_ptr<CallGraph> call_graph =\n-      CallGraph::Build(module, execution_threads);\n-  auto SkipComputation = [&](HloComputation* c) {\n-    auto callers = call_graph->GetComputationCallers(c);\n-    return std::any_of(\n-        callers.begin(), callers.end(),\n-        [&](HloInstruction* caller) { return caller->has_to_apply(); });\n-  };\n-  auto it = std::remove_if(non_fusion_computations.begin(),\n-                           non_fusion_computations.end(), SkipComputation);\n-  non_fusion_computations.erase(it, non_fusion_computations.end());\n-  return non_fusion_computations;\n-}\n-\n-bool XnnGraphFusion::IsOpSupported(const HloInstruction* instr) {\n-  if (!IsLayoutSupportedByXnn(instr->shape())) {\n-    return false;\n-  }\n-  if (!XnnDatatype(instr->shape().element_type()).ok()) {\n-    return false;\n-  }\n-  if (instr->IsConstant()) {\n-    return IsConstantSupportedByXnn(instr);\n-  }\n-  if (instr->IsElementwise()) {\n-    return IsElementwiseOpSupportedByXnn(instr);\n-  }\n-\n-  switch (instr->opcode()) {\n-    case HloOpcode::kBitcast:\n-      return IsBitcastOpSupportedByXnn(instr);\n-    case HloOpcode::kBroadcast:\n-      return IsBroadcastOpSupportedByXnn(instr);\n-    case HloOpcode::kReduce:\n-      return IsReduceOpSupportedByXnn(instr);\n-    default:\n-      return false;\n-  }\n-}\n-\n-bool XnnGraphFusion::IsXnnGraphFusion(const HloInstruction* instr) {\n-  if (instr->opcode() != HloOpcode::kFusion) {\n-    return false;\n-  }\n-  const HloFusionInstruction* fusion = Cast<HloFusionInstruction>(instr);\n-  if (fusion->fusion_kind() != HloInstruction::FusionKind::kCustom) {\n-    return false;\n-  }\n-  auto backend_config = fusion->backend_config<BackendConfig>();\n-  if (!backend_config.ok() || !backend_config->has_fusion_config()) {\n-    return false;\n-  }\n-  return backend_config->fusion_config().kind() == kXnnFusionKind;\n-}\n-\n-}  // namespace xla::cpu"
        },
        {
            "sha": "ca596ad7a94ac01c8bdab9bb9a737114e13bc5e7",
            "filename": "third_party/xla/xla/backends/cpu/transforms/xnn_graph_fusion.h",
            "status": "removed",
            "additions": 0,
            "deletions": 55,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fxnn_graph_fusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fxnn_graph_fusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fxnn_graph_fusion.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,55 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_BACKENDS_CPU_TRANSFORMS_XNN_GRAPH_FUSION_H_\n-#define XLA_BACKENDS_CPU_TRANSFORMS_XNN_GRAPH_FUSION_H_\n-\n-#include <cstdint>\n-\n-#include \"absl/container/flat_hash_set.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/service/instruction_fusion.h\"\n-\n-namespace xla {\n-namespace cpu {\n-\n-class XnnGraphFusion : public InstructionFusion {\n- public:\n-  XnnGraphFusion() : InstructionFusion(XnnGraphFusion::IsExpensive) {}\n-  ~XnnGraphFusion() override = default;\n-\n- private:\n-  FusionDecision ShouldFuse(HloInstruction* consumer,\n-                            int64_t operand_index) override;\n-  HloInstruction::FusionKind ChooseKind(\n-      const HloInstruction* producer, const HloInstruction* consumer) override;\n-\n-  HloInstruction* Fuse(HloInstruction* producer, HloInstruction* consumer,\n-                       HloComputation* computation) override;\n-\n-  std::vector<HloComputation*> GetNonFusionComputations(\n-      HloModule* module,\n-      const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n-\n-  static bool IsOpSupported(const HloInstruction* instr);\n-\n-  static bool IsXnnGraphFusion(const HloInstruction* instr);\n-};\n-\n-}  // namespace cpu\n-}  // namespace xla\n-\n-#endif  // XLA_BACKENDS_CPU_TRANSFORMS_XNN_GRAPH_FUSION_H_"
        },
        {
            "sha": "b992f7aa6b74d68ed721d39e087d317f297bad9a",
            "filename": "third_party/xla/xla/backends/cpu/transforms/xnn_graph_fusion_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 333,
            "changes": 333,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fxnn_graph_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fxnn_graph_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fxnn_graph_fusion_test.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,333 +0,0 @@\n-/* Copyright 2017 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/cpu/transforms/xnn_graph_fusion.h\"\n-\n-#include <memory>\n-#include <string>\n-\n-#include <gmock/gmock.h>\n-#include <gtest/gtest.h>\n-#include \"xla/backends/cpu/xnn_support.h\"\n-#include \"xla/hlo/ir/hlo_casting_utils.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n-#include \"xla/hlo/utils/hlo_matchers.h\"\n-#include \"xla/service/cpu/backend_config.pb.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/xla.pb.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-namespace op = xla::testing::opcode_matchers;\n-\n-namespace xla::cpu {\n-namespace {\n-\n-using XnnGraphFusionTest = HloHardwareIndependentTestBase;\n-\n-TEST_F(XnnGraphFusionTest, BasicFusion) {\n-  std::string hlo_string = R\"(\n-HloModule FusionDemonstration\n-\n-ENTRY entry {\n-   %param.0 = f32[2,2] parameter(0)\n-   %constant.0 = f32[2,2] constant({ { 1, 2 }, { 3, 4 } })\n-   %add.0 = f32[2,2] add(f32[2,2] %param.0, f32[2,2]{1,0} %constant.0)\n-   %sub.0 = f32[2,2] subtract(f32[2,2] %param.0, f32[2,2] %constant.0)\n-   ROOT %result = f32[2,2] multiply(f32[2,2] %add.0, f32[2,2] %sub.0)\n-}\n-)\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(hlo_string));\n-  TF_ASSERT_OK_AND_ASSIGN(bool changed, XnnGraphFusion().Run(module.get()));\n-  ASSERT_TRUE(changed);\n-  EXPECT_THAT(module.get()->entry_computation()->root_instruction(),\n-              op::Fusion());\n-  HloInstruction* root = module->entry_computation()->root_instruction();\n-  ASSERT_EQ(root->opcode(), HloOpcode::kFusion);\n-  HloFusionInstruction* fusion = Cast<HloFusionInstruction>(root);\n-  TF_ASSERT_OK_AND_ASSIGN(auto backend_config,\n-                          fusion->backend_config<BackendConfig>());\n-  ASSERT_TRUE(backend_config.has_fusion_config());\n-  EXPECT_EQ(backend_config.fusion_config().kind(), kXnnFusionKind);\n-}\n-\n-TEST_F(XnnGraphFusionTest, BasicFusionUnsupportedType) {\n-  std::string hlo_string = R\"(\n-HloModule FusionDemonstration\n-\n-ENTRY entry {\n-   %param.0 = s2[2,2] parameter(0)\n-   %constant.0 = s2[2,2] constant({ { 0, 1 }, { 1, 0 } })\n-   %add.0 = s2[2,2] add(s2[2,2] %param.0, s2[2,2] %constant.0)\n-   %sub.0 = s2[2,2] subtract(s2[2,2] %param.0, s2[2,2] %constant.0)\n-   ROOT %result = s2[2,2] multiply(s2[2,2] %add.0, s2[2,2] %sub.0)\n-}\n-)\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(hlo_string));\n-  TF_ASSERT_OK_AND_ASSIGN(bool changed, XnnGraphFusion().Run(module.get()));\n-  ASSERT_FALSE(changed);\n-}\n-\n-TEST_F(XnnGraphFusionTest, BasicFusionUnsupportedLayout) {\n-  std::string hlo_string = R\"(\n-HloModule FusionDemonstration\n-\n-ENTRY entry {\n-   %param.0 = f32[2,2]{0,1} parameter(0)\n-   %constant.0 = f32[2,2]{0,1} constant({ { 0, 1 }, { 1, 0 } })\n-   %add.0 = f32[2,2]{0,1} add(f32[2,2]{0,1} %param.0, f32[2,2]{0,1} %constant.0)\n-   %sub.0 = f32[2,2]{0,1} subtract(f32[2,2]{0,1} %param.0, f32[2,2]{0,1} %constant.0)\n-   ROOT %result = f32[2,2]{0,1} multiply(f32[2,2]{0,1} %add.0, f32[2,2]{0,1} %sub.0)\n-}\n-)\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(hlo_string));\n-  TF_ASSERT_OK_AND_ASSIGN(bool changed, XnnGraphFusion().Run(module.get()));\n-  ASSERT_FALSE(changed);\n-}\n-\n-static void SetFusionMode(HloModule* module,\n-                          DebugOptions::XnnGraphFusionMode mode) {\n-  module->mutable_config()\n-      .mutable_debug_options()\n-      .set_xla_cpu_experimental_xnn_graph_fusion_mode(mode);\n-}\n-\n-TEST_F(XnnGraphFusionTest, BasicBroadcast) {\n-  std::string hlo_string = R\"(\n-HloModule BroadcastFusion\n-\n-ENTRY entry {\n-  %param.0 = f32[] parameter(0)\n-  %broadcast.0 = f32[2,2] broadcast(f32[] %param.0), dimensions={}\n-  ROOT result = f32[2,2] add(f32[2,2] %broadcast.0, f32[2,2] %broadcast.0)\n-}\n-\n-)\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(hlo_string));\n-  SetFusionMode(module.get(),\n-                DebugOptions::XNN_GRAPH_FUSION_MODE_GREEDY_SLINKY);\n-  TF_ASSERT_OK_AND_ASSIGN(bool changed, XnnGraphFusion().Run(module.get()));\n-  ASSERT_TRUE(changed);\n-  EXPECT_THAT(module.get()->entry_computation()->root_instruction(),\n-              op::Fusion());\n-\n-  HloInstruction* root = module->entry_computation()->root_instruction();\n-  ASSERT_EQ(root->opcode(), HloOpcode::kFusion);\n-  HloFusionInstruction* fusion = Cast<HloFusionInstruction>(root);\n-  TF_ASSERT_OK_AND_ASSIGN(auto backend_config,\n-                          fusion->backend_config<BackendConfig>());\n-  ASSERT_TRUE(backend_config.has_fusion_config());\n-  EXPECT_EQ(backend_config.fusion_config().kind(), kXnnFusionKind);\n-}\n-\n-TEST_F(XnnGraphFusionTest, SkipRootBroadcast) {\n-  std::string hlo_string = R\"(\n-HloModule SkipRootBroadcast\n-\n-ENTRY entry {\n-  %param.0 = f32[] parameter(0)\n-  %add.0 = f32[] add(f32[] %param.0, f32[] %param.0)\n-  ROOT result = f32[2,2] broadcast(f32[] %param.0), dimensions={}\n-}\n-\n-)\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(hlo_string));\n-  SetFusionMode(module.get(),\n-                DebugOptions::XNN_GRAPH_FUSION_MODE_GREEDY_SLINKY);\n-  TF_ASSERT_OK_AND_ASSIGN(bool changed, XnnGraphFusion().Run(module.get()));\n-  ASSERT_FALSE(changed);\n-}\n-\n-TEST_F(XnnGraphFusionTest, SkipUnsupportedBroadcast) {\n-  // Broadcast changes the relative order of dimensions.\n-  std::string hlo_string = R\"(\n-HloModule SkipUnsupportedBroadcast\n-\n-ENTRY entry {\n-  %param.0 = f32[2,3] parameter(0)\n-  %broadcast.0 = f32[4,3,2] broadcast(f32[2,3] %param.0), dimensions={2,1}\n-  ROOT result = f32[4,3,2] add(f32[4,3,2] %broadcast.0, f32[4,3,2] %broadcast.0)\n-}\n-\n-)\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(hlo_string));\n-  SetFusionMode(module.get(),\n-                DebugOptions::XNN_GRAPH_FUSION_MODE_GREEDY_SLINKY);\n-  TF_ASSERT_OK_AND_ASSIGN(bool changed, XnnGraphFusion().Run(module.get()));\n-  ASSERT_FALSE(changed);\n-}\n-\n-TEST_F(XnnGraphFusionTest, SkipRootWideningConvert) {\n-  std::string hlo_string = R\"(\n-HloModule SkipRootWideningConvert\n-\n-ENTRY entry {\n-  %param.0 = f32[4] parameter(0)\n-  %to_bf16.0 = bf16[4] convert(f32[4] %param.0)\n-  ROOT result = f32[4] convert(bf16[4] %to_bf16.0)\n-}\n-\n-)\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(hlo_string));\n-  SetFusionMode(module.get(),\n-                DebugOptions::XNN_GRAPH_FUSION_MODE_GREEDY_SLINKY);\n-  TF_ASSERT_OK_AND_ASSIGN(bool changed, XnnGraphFusion().Run(module.get()));\n-  ASSERT_FALSE(changed);\n-}\n-\n-TEST_F(XnnGraphFusionTest, BasicFusionUnsupportedOperandType) {\n-  std::string hlo_string = R\"(\n-HloModule BasicFusionUnsupportedOperandType\n-\n-ENTRY entry {\n-   %param.0 = s1[2,2] parameter(0)\n-   ROOT %converted_param.0 = f32[2,2] convert(s1[2,2] %param.0)\n-}\n-)\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(hlo_string));\n-  TF_ASSERT_OK_AND_ASSIGN(bool changed, XnnGraphFusion().Run(module.get()));\n-  ASSERT_FALSE(changed);\n-}\n-\n-TEST_F(XnnGraphFusionTest, BasicReduce) {\n-  std::string hlo_string = R\"(\n-HloModule BasicReduce\n-\n-reducer {\n-  arg_0 = f32[] parameter(0)\n-  arg_1 = f32[] parameter(1)\n-  ROOT maximum = f32[] maximum(arg_0, arg_1)\n-}\n-\n-ENTRY main {\n-  arg_0 = f32[3,2] parameter(0)\n-  init = f32[] constant(-inf)\n-  ROOT result = f32[] reduce(arg_0, init), dimensions={0,1}, to_apply=reducer\n-}\n-)\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(hlo_string));\n-  SetFusionMode(module.get(),\n-                DebugOptions::XNN_GRAPH_FUSION_MODE_GREEDY_SLINKY);\n-  TF_ASSERT_OK_AND_ASSIGN(bool changed, XnnGraphFusion().Run(module.get()));\n-  ASSERT_TRUE(changed);\n-  EXPECT_THAT(module.get()->entry_computation()->root_instruction(),\n-              op::Fusion());\n-\n-  HloInstruction* root = module->entry_computation()->root_instruction();\n-  HloFusionInstruction* fusion = Cast<HloFusionInstruction>(root);\n-  TF_ASSERT_OK_AND_ASSIGN(auto backend_config,\n-                          fusion->backend_config<BackendConfig>());\n-  ASSERT_TRUE(backend_config.has_fusion_config());\n-  EXPECT_EQ(backend_config.fusion_config().kind(), kXnnFusionKind);\n-}\n-\n-TEST_F(XnnGraphFusionTest, SkipReduceWithUnsupportedInit) {\n-  std::string hlo_string = R\"(\n-HloModule SkipReduceWithUnsupportedInit\n-\n-reducer {\n-  arg_0 = f32[] parameter(0)\n-  arg_1 = f32[] parameter(1)\n-  ROOT maximum = f32[] maximum(arg_0, arg_1)\n-}\n-\n-ENTRY main {\n-  arg_0 = f32[3,2] parameter(0)\n-  init = f32[] constant(1.33)\n-  ROOT result = f32[] reduce(arg_0, init), dimensions={0,1}, to_apply=reducer\n-}\n-)\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(hlo_string));\n-  SetFusionMode(module.get(),\n-                DebugOptions::XNN_GRAPH_FUSION_MODE_GREEDY_SLINKY);\n-  TF_ASSERT_OK_AND_ASSIGN(bool changed, XnnGraphFusion().Run(module.get()));\n-  ASSERT_FALSE(changed);\n-}\n-\n-TEST_F(XnnGraphFusionTest, SkipReduceWithUnsupportedReducer) {\n-  std::string hlo_string = R\"(\n-HloModule SkipReduceWithUnsupportedReducer\n-\n-reducer {\n-  arg_0 = f32[] parameter(0)\n-  arg_1 = f32[] parameter(1)\n-  ROOT sub = f32[] subtract(arg_0, arg_1)\n-}\n-\n-ENTRY main {\n-  arg_0 = f32[3,2] parameter(0)\n-  init = f32[] constant(1.33)\n-  ROOT result = f32[] reduce(arg_0, init), dimensions={0,1}, to_apply=reducer\n-}\n-)\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(hlo_string));\n-  SetFusionMode(module.get(),\n-                DebugOptions::XNN_GRAPH_FUSION_MODE_GREEDY_SLINKY);\n-  TF_ASSERT_OK_AND_ASSIGN(bool changed, XnnGraphFusion().Run(module.get()));\n-  ASSERT_FALSE(changed);\n-}\n-\n-TEST_F(XnnGraphFusionTest, NoFusionInsideReducer) {\n-  std::string hlo_string = R\"(\n-HloModule NoFusionInsideReducer\n-\n-reducer {\n-  arg_0 = f32[] parameter(0)\n-  arg_1 = f32[] parameter(1)\n-  mul = f32[] multiply(arg_0, arg_1)\n-  ROOT result = f32[] add(arg_0, mul)\n-}\n-\n-ENTRY main {\n-  arg_0 = f32[3,2] parameter(0)\n-  init = f32[] constant(1.33)\n-  ROOT result = f32[] reduce(arg_0, init), dimensions={0,1}, to_apply=reducer\n-}\n-)\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(hlo_string));\n-  SetFusionMode(module.get(),\n-                DebugOptions::XNN_GRAPH_FUSION_MODE_GREEDY_SLINKY);\n-  TF_ASSERT_OK_AND_ASSIGN(bool changed, XnnGraphFusion().Run(module.get()));\n-  ASSERT_FALSE(changed);\n-}\n-\n-}  // namespace\n-}  // namespace xla::cpu"
        },
        {
            "sha": "faa943fa4ce929d1253e2a9ab2c6a06640779aeb",
            "filename": "third_party/xla/xla/backends/cpu/transforms/xnn_matcher.h",
            "status": "removed",
            "additions": 0,
            "deletions": 118,
            "changes": 118,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fxnn_matcher.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fxnn_matcher.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Ftransforms%2Fxnn_matcher.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,118 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_BACKENDS_CPU_TRANSFORMS_XNN_MATCHER_H_\n-#define XLA_BACKENDS_CPU_TRANSFORMS_XNN_MATCHER_H_\n-\n-#include <string>\n-\n-#include \"absl/base/no_destructor.h\"\n-#include \"absl/container/flat_hash_set.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"xla/backends/cpu/codegen/target_machine_features.h\"\n-#include \"xla/backends/cpu/transforms/library_matcher.h\"\n-#include \"xla/backends/cpu/xnn_support.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"tsl/platform/protobuf.h\"\n-\n-namespace xla::cpu {\n-\n-class XnnMatcher : public LibraryMatcher {\n- public:\n-  explicit XnnMatcher(const TargetMachineFeatures* target_machine_features,\n-                      const tsl::protobuf::RepeatedField<int>* fusion_types)\n-      : LibraryMatcher(target_machine_features, fusion_types) {}\n-  ~XnnMatcher() override = default;\n-\n-  // Returns the set of supported HLO instructions.\n-  absl::flat_hash_set<HloOpcode> SupportedOps() const override {\n-    static const absl::NoDestructor<absl::flat_hash_set<HloOpcode>>\n-        kSupportedOps{[]() {\n-          absl::flat_hash_set<HloOpcode> supported_ops{\n-              HloOpcode::kDot, HloOpcode::kReduce, HloOpcode::kConstant};\n-          for (const auto& [op, _] : GetXnnUnaryOpMap()) {\n-            supported_ops.insert(op);\n-          }\n-          for (const auto& [op, _] : GetXnnBinaryOpMap()) {\n-            supported_ops.insert(op);\n-          }\n-          return supported_ops;\n-        }()};\n-    return *kSupportedOps;\n-  }\n-\n-  // Returns true if the HLO instruction is supported by the library.\n-  absl::StatusOr<bool> IsOpSupported(const HloInstruction* instr) override {\n-    if (instr->opcode() == HloOpcode::kDot) {\n-      return IsDotSupportedByXnn(\n-          instr->dot_dimension_numbers(), instr->operand(0)->shape(),\n-          instr->operand(1)->shape(), instr->shape(), target_machine_features_);\n-    }\n-    if (instr->opcode() == HloOpcode::kReduce) {\n-      return IsReduceOpSupportedByXnn(instr);\n-    }\n-    if (instr->IsConstant()) {\n-      return IsConstantSupportedByXnn(instr);\n-    }\n-    // TODO(b/441837668): Need to get the reduction performance/cost model\n-    // right before enabling fusions. Fusions make performance analysis quite\n-    // challenging.\n-    if (fuse_reduce_) {\n-      return false;\n-    }\n-    if (instr->IsElementwise()) {\n-      return IsElementwiseOpSupportedByXnn(instr);\n-    }\n-    return false;\n-  }\n-\n-  // Returns true if we should start a new fusion containing just the given HLO\n-  // instruction. We control the instructions that can start a fusion with the\n-  // `--xla_cpu_experimental_xnn_fusion_type` flag.\n-  bool ShouldCreateFusion(const HloInstruction* instr) override {\n-    if (fuse_dot_ && instr->opcode() == HloOpcode::kDot) {\n-      return true;\n-    }\n-    if (fuse_reduce_ && instr->opcode() == HloOpcode::kReduce) {\n-      return true;\n-    }\n-    return fuse_eltwise_ && instr->IsElementwise();\n-  }\n-\n-  // Returns the output type of the XNN op, so we can insert a convert node if\n-  // the op does not support the original HLO output type.\n-  PrimitiveType LibraryOpOutputType(const HloInstruction* instr) override {\n-    auto out_type = instr->shape().element_type();\n-    if (instr->opcode() != HloOpcode::kDot) {\n-      return out_type;\n-    }\n-    return out_type == BF16 ? F32 : out_type;\n-  }\n-\n-  // Returns a prefix string for the fusion op's name.\n-  std::string fusion_prefix() const override { return \"xnn_\"; }\n-\n-  // Returns a string for FusionBackendConfig's fusion kind.\n-  absl::string_view fusion_kind() const override { return kXnnFusionKind; }\n-\n- private:\n-  absl::flat_hash_set<DebugOptions::LibraryFusionType> fusion_types_;\n-};\n-\n-}  // namespace xla::cpu\n-\n-#endif  // XLA_BACKENDS_CPU_TRANSFORMS_XNN_MATCHER_H_"
        },
        {
            "sha": "30473a0bc2ed9a0616ff3b00044aaaa7e6d21d0f",
            "filename": "third_party/xla/xla/backends/cpu/xnn_emitter.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 507,
            "changes": 507,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_emitter.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,507 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/cpu/xnn_emitter.h\"\n-\n-#include <algorithm>\n-#include <cstddef>\n-#include <cstdint>\n-#include <limits>\n-#include <memory>\n-#include <vector>\n-\n-#include \"xnnpack.h\"\n-#include \"absl/container/flat_hash_map.h\"\n-#include \"absl/functional/any_invocable.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/str_format.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_interop.h\"\n-#include \"xla/backends/cpu/xnn_support.h\"\n-#include \"xla/hlo/ir/hlo_casting_utils.h\"\n-#include \"xla/hlo/ir/hlo_computation.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/literal.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/tsl/platform/logging.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/util.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-namespace xla::cpu {\n-\n-// A mapping from HloInstruction to XNNPACK subgraph tensor id.\n-using TensorIdMap = absl::flat_hash_map<const HloInstruction*, uint32_t>;\n-\n-//===----------------------------------------------------------------------===//\n-// XLA <-> XNNPACK type conversion library.\n-//===----------------------------------------------------------------------===//\n-\n-static std::vector<size_t> XnnDimensions(const Shape& shape) {\n-  std::vector<size_t> dims;\n-  for (auto& dim : shape.dimensions()) {\n-    dims.push_back(dim);\n-  }\n-  return dims;\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// XLA <-> XNNPACK emitters.\n-//===----------------------------------------------------------------------===//\n-\n-static absl::StatusOr<uint32_t> FindTensorValue(const TensorIdMap& tensor_ids,\n-                                                const HloInstruction* instr) {\n-  if (auto it = tensor_ids.find(instr); it != tensor_ids.end()) {\n-    return it->second;\n-  }\n-  return Internal(\"Can't fine XNNPACK tensor value for instruction %s\",\n-                  instr->ToString());\n-}\n-\n-static absl::StatusOr<uint32_t> DefineTensorValue(\n-    xnn_subgraph_t subgraph, xnn_datatype type, absl::Span<const size_t> dims) {\n-  uint32_t tensor_id = XNN_INVALID_VALUE_ID;\n-  uint32_t tensor_flags = 0;\n-\n-  XNN_RETURN_IF_ERROR(xnn_define_tensor_value(\n-      subgraph, type, dims.size(), dims.data(), nullptr,\n-      /*external_id=*/tensor_id, tensor_flags, &tensor_id));\n-\n-  return tensor_id;\n-}\n-\n-static absl::StatusOr<uint32_t> DefineTensorValue(xnn_subgraph_t subgraph,\n-                                                  const HloInstruction* instr) {\n-  // We do not support instructions with multiple results (tuples).\n-  if (!instr->shape().IsArray()) {\n-    return Internal(\"Unsupported XNNPACK instruction shape: %s\",\n-                    instr->ToString());\n-  }\n-\n-  auto dims = XnnDimensions(instr->shape());\n-  TF_ASSIGN_OR_RETURN(auto type, XnnDatatype(instr->shape().element_type()));\n-\n-  uint32_t tensor_id = XNN_INVALID_VALUE_ID;\n-  uint32_t tensor_flags = 0;\n-\n-  // If instruction is a root instruction of the parent computation we assign it\n-  // an external tensor id corresponding to the result index.\n-  const HloComputation* computation = instr->parent();\n-  if (computation->root_instruction() == instr) {\n-    tensor_id = computation->num_parameters();\n-    tensor_flags = XNN_VALUE_FLAG_EXTERNAL_OUTPUT;\n-  }\n-\n-  XNN_RETURN_IF_ERROR(xnn_define_tensor_value(\n-      subgraph, type, dims.size(), dims.data(), nullptr,\n-      /*external_id=*/tensor_id, tensor_flags, &tensor_id));\n-\n-  return tensor_id;\n-}\n-\n-static absl::StatusOr<uint32_t> DefineConstant(\n-    xnn_subgraph_t subgraph, std::vector<std::unique_ptr<Literal>>& literals,\n-    const HloInstruction* instr) {\n-  // We do not support instructions with multiple results (tuples).\n-  if (!instr->shape().IsArray()) {\n-    return Internal(\"Unsupported XNNPACK instruction shape: %s\",\n-                    instr->ToString());\n-  }\n-\n-  auto dims = XnnDimensions(instr->shape());\n-  TF_ASSIGN_OR_RETURN(auto type, XnnDatatype(instr->shape().element_type()));\n-\n-  uint32_t tensor_id = XNN_INVALID_VALUE_ID;\n-  uint32_t tensor_flags = 0;\n-\n-  literals.push_back(instr->literal().CloneToUnique());\n-  const void* value = literals.back()->untyped_data();\n-\n-  XNN_RETURN_IF_ERROR(xnn_define_tensor_value(\n-      subgraph, type, dims.size(), dims.data(), value,\n-      /*external_id=*/tensor_id, tensor_flags, &tensor_id));\n-\n-  return tensor_id;\n-}\n-\n-static absl::StatusOr<uint32_t> DefineParameter(xnn_subgraph_t subgraph,\n-                                                const HloInstruction* param) {\n-  VLOG(3) << absl::StreamFormat(\"Define tensor value for parameter: %s\",\n-                                param->ToString());\n-\n-  auto dims = XnnDimensions(param->shape());\n-  TF_ASSIGN_OR_RETURN(auto type, XnnDatatype(param->shape().element_type()));\n-\n-  uint32_t tensor_id = param->parameter_number();\n-  XNN_RETURN_IF_ERROR(xnn_define_tensor_value(\n-      subgraph, type, dims.size(), dims.data(), nullptr,\n-      /*external_id=*/tensor_id, XNN_VALUE_FLAG_EXTERNAL_INPUT, &tensor_id));\n-\n-  return tensor_id;\n-}\n-\n-static absl::StatusOr<uint32_t> DefineBitcastOp(xnn_subgraph_t subgraph,\n-                                                TensorIdMap& tensor_ids,\n-                                                const HloInstruction* instr) {\n-  VLOG(3) << absl::StreamFormat(\"Define tensor value for bitcast op: %s\",\n-                                instr->ToString());\n-  CHECK_EQ(instr->opcode(), HloOpcode::kBitcast);\n-  const HloInstruction* input = instr->operand(0);\n-  CHECK_EQ(input->shape().element_type(), instr->shape().element_type());\n-  TF_ASSIGN_OR_RETURN(auto in, FindTensorValue(tensor_ids, input));\n-  TF_ASSIGN_OR_RETURN(auto out, DefineTensorValue(subgraph, instr));\n-\n-  auto dims = XnnDimensions(instr->shape());\n-  XNN_RETURN_IF_ERROR(xnn_define_static_reshape(subgraph, dims.size(),\n-                                                dims.data(), in, out,\n-                                                /*flags=*/0));\n-  return out;\n-}\n-\n-static absl::StatusOr<uint32_t> DefineBroadcastOp(xnn_subgraph_t subgraph,\n-                                                  TensorIdMap& tensor_ids,\n-                                                  const HloInstruction* instr) {\n-  VLOG(3) << absl::StreamFormat(\"Define tensor value for broadcast op: %s\",\n-                                instr->ToString());\n-  CHECK_EQ(instr->opcode(), HloOpcode::kBroadcast);\n-  const HloBroadcastInstruction* broadcast_instr =\n-      Cast<HloBroadcastInstruction>(instr);\n-  const HloInstruction* input = broadcast_instr->operand(0);\n-  CHECK_EQ(input->shape().element_type(), instr->shape().element_type());\n-\n-  const absl::Span<const int64_t> input_dims = input->shape().dimensions();\n-  const absl::Span<const int64_t> output_dims = instr->shape().dimensions();\n-  const absl::Span<const int64_t> dims = broadcast_instr->dimensions();\n-  CHECK(std::is_sorted(dims.begin(), dims.end()));\n-  CHECK_LE(input_dims.size(), output_dims.size());\n-\n-  const size_t num_new_axes = output_dims.size() - input_dims.size();\n-  // New axis positions used by XNNPACK expand_dims.\n-  std::vector<size_t> xnn_expand_dims_new_axes;\n-  xnn_expand_dims_new_axes.reserve(num_new_axes);\n-  std::vector<size_t> xnn_expand_dims_dimensions;\n-  xnn_expand_dims_dimensions.reserve(output_dims.size());\n-\n-  // Mask used by XNNPACK broadcast.\n-  std::vector<size_t> xnn_new_shape;\n-  xnn_new_shape.reserve(output_dims.size());\n-\n-  for (size_t dim_idx = 0; dim_idx < output_dims.size(); ++dim_idx) {\n-    const auto it = std::find(dims.begin(), dims.end(), dim_idx);\n-    if (it == dims.end()) {\n-      // New dimension case.\n-      xnn_expand_dims_new_axes.push_back(dim_idx);\n-      xnn_expand_dims_dimensions.push_back(1u);\n-      // Broadcasted dimension.\n-      xnn_new_shape.push_back(output_dims[dim_idx]);\n-    } else {\n-      // Pass through the input dimension.\n-      const size_t input_dim_idx = it - dims.begin();\n-      CHECK_EQ(*it, dim_idx);\n-      const size_t input_dim = input_dims[input_dim_idx];\n-      CHECK_EQ(input_dim, output_dims[dim_idx]);\n-      xnn_expand_dims_dimensions.push_back(input_dim);\n-      // 0 means keeping the dimension of the input.\n-      // See the description of xnn_define_static_broadcast in xnnpack.h\n-      xnn_new_shape.push_back(0u);\n-    }\n-  }\n-\n-  CHECK_EQ(xnn_expand_dims_dimensions.size(), output_dims.size());\n-  CHECK_EQ(xnn_expand_dims_new_axes.size(), num_new_axes);\n-  CHECK_EQ(xnn_new_shape.size(), output_dims.size());\n-\n-  TF_ASSIGN_OR_RETURN(auto type, XnnDatatype(input->shape().element_type()));\n-  TF_ASSIGN_OR_RETURN(auto in, FindTensorValue(tensor_ids, input));\n-  TF_ASSIGN_OR_RETURN(\n-      auto xnn_dims_expanded,\n-      DefineTensorValue(subgraph, type, xnn_expand_dims_dimensions));\n-  TF_ASSIGN_OR_RETURN(auto xnn_broadcast, DefineTensorValue(subgraph, instr));\n-\n-  XNN_RETURN_IF_ERROR(xnn_define_static_expand_dims(\n-      subgraph, num_new_axes, xnn_expand_dims_new_axes.data(), in,\n-      xnn_dims_expanded, /*flags=*/0));\n-\n-  XNN_RETURN_IF_ERROR(xnn_define_static_broadcast(\n-      subgraph, xnn_new_shape.size(), xnn_new_shape.data(), xnn_dims_expanded,\n-      xnn_broadcast, /*flags=*/0));\n-\n-  return xnn_broadcast;\n-}\n-\n-static absl::StatusOr<uint32_t> DefineReduceOp(xnn_subgraph_t subgraph,\n-                                               TensorIdMap& tensor_ids,\n-                                               const HloInstruction* instr) {\n-  VLOG(3) << absl::StreamFormat(\"Define tensor value for reduce op: %s\",\n-                                instr->ToString());\n-  CHECK_EQ(instr->opcode(), HloOpcode::kReduce);\n-  const HloReduceInstruction* reduce_instr = Cast<HloReduceInstruction>(instr);\n-  const HloInstruction* input = instr->operand(0);\n-  CHECK_EQ(input->shape().element_type(), instr->shape().element_type());\n-\n-  xnn_reduce_operator xnn_reduce_op = xnn_reduce_invalid;\n-  CHECK_EQ(reduce_instr->to_apply()->num_parameters(), 2);\n-  CHECK_EQ(reduce_instr->to_apply()->instruction_count(), 3);\n-\n-  switch (reduce_instr->to_apply()->root_instruction()->opcode()) {\n-    case HloOpcode::kAdd:\n-      xnn_reduce_op = xnn_reduce_sum;\n-      break;\n-    case HloOpcode::kMaximum:\n-      xnn_reduce_op = xnn_reduce_max;\n-      break;\n-    case HloOpcode::kMinimum:\n-      xnn_reduce_op = xnn_reduce_min;\n-      break;\n-    default:\n-      LOG(FATAL) << \"Unsupported reduction: \" << instr->to_apply()->ToString();\n-  }\n-\n-  const absl::Span<const int64_t> dims = reduce_instr->dimensions();\n-  TF_ASSIGN_OR_RETURN(auto in, FindTensorValue(tensor_ids, input));\n-  TF_ASSIGN_OR_RETURN(auto out, DefineTensorValue(subgraph, instr));\n-  XNN_RETURN_IF_ERROR(xnn_define_static_reduce(\n-      subgraph, xnn_reduce_op, dims.size(),\n-      reinterpret_cast<const size_t*>(dims.data()), in, out,\n-      /*flags=*/0));\n-  return out;\n-}\n-\n-static absl::StatusOr<uint32_t> DefineUnaryOp(xnn_subgraph_t subgraph,\n-                                              TensorIdMap& tensor_ids,\n-                                              const HloInstruction* instr) {\n-  VLOG(3) << absl::StreamFormat(\"Define tensor value for unary op: %s\",\n-                                instr->ToString());\n-  TF_ASSIGN_OR_RETURN(auto unary_op, XnnUnaryOperator(instr->opcode()));\n-\n-  TF_ASSIGN_OR_RETURN(auto in, FindTensorValue(tensor_ids, instr->operand(0)));\n-  TF_ASSIGN_OR_RETURN(auto out, DefineTensorValue(subgraph, instr));\n-\n-  VLOG(3) << absl::StreamFormat(\"  tensors: in=%d, out=%d\", in, out);\n-\n-  xnn_unary_params params;\n-  XNN_RETURN_IF_ERROR(\n-      xnn_define_unary(subgraph, unary_op, &params, in, out, /*flags=*/0));\n-\n-  return out;\n-}\n-\n-static absl::StatusOr<uint32_t> DefineBinaryOp(xnn_subgraph_t subgraph,\n-                                               TensorIdMap& tensor_ids,\n-                                               const HloInstruction* instr) {\n-  VLOG(3) << absl::StreamFormat(\"Define tensor value for binary op: %s\",\n-                                instr->ToString());\n-\n-  TF_ASSIGN_OR_RETURN(auto binary_op, XnnBinaryOperator(instr->opcode()));\n-\n-  TF_ASSIGN_OR_RETURN(auto lhs, FindTensorValue(tensor_ids, instr->operand(0)));\n-  TF_ASSIGN_OR_RETURN(auto rhs, FindTensorValue(tensor_ids, instr->operand(1)));\n-  TF_ASSIGN_OR_RETURN(auto out, DefineTensorValue(subgraph, instr));\n-\n-  VLOG(3) << absl::StreamFormat(\"  tensors: lhs=%d, rhs=%d, out=%d\", lhs, rhs,\n-                                out);\n-\n-  xnn_binary_params params = {-std::numeric_limits<float>::infinity(),\n-                              std::numeric_limits<float>::infinity()};\n-\n-  // In XLA, broadcasts are explicit ops, allowing XNNPACK to assume there is no\n-  // broadcasting in the elementwise operation itself, which simplifies data\n-  // dependencies.\n-  const uint32_t flags = XNN_FLAG_NO_BROADCAST;\n-  XNN_RETURN_IF_ERROR(xnn_define_binary(subgraph, binary_op, &params, lhs, rhs,\n-                                        out, /*flags=*/flags));\n-\n-  return out;\n-}\n-\n-static absl::StatusOr<uint32_t> DefineBatchMatMul(xnn_subgraph_t subgraph,\n-                                                  TensorIdMap& tensor_ids,\n-                                                  const HloInstruction* instr) {\n-  // Verify that this Dot is supported by XNNPACK.\n-  const DotDimensionNumbers& dnums = instr->dot_dimension_numbers();\n-  const Shape& lhs_shape = instr->operand(0)->shape();\n-  const Shape& rhs_shape = instr->operand(1)->shape();\n-  TF_ASSIGN_OR_RETURN(\n-      bool is_supported,\n-      IsDotSupportedByXnn(dnums, lhs_shape, rhs_shape, instr->shape(),\n-                          /*cpu_features=*/nullptr, /*use_cost_model=*/false));\n-\n-  if (!is_supported) {\n-    return InvalidArgument(\"Unsupported XNNPACK Dot op variation: %s\",\n-                           instr->ToString());\n-  }\n-\n-  VLOG(3) << \"Define tensor values for batch_matrix_multiply op\";\n-\n-  TF_ASSIGN_OR_RETURN(uint32_t lhs,\n-                      FindTensorValue(tensor_ids, instr->operand(0)));\n-  TF_ASSIGN_OR_RETURN(uint32_t rhs,\n-                      FindTensorValue(tensor_ids, instr->operand(1)));\n-  TF_ASSIGN_OR_RETURN(uint32_t out, DefineTensorValue(subgraph, instr));\n-\n-  VLOG(3) << absl::StreamFormat(\"  tensors: lhs=%d, rhs=%d, out=%d\", lhs, rhs,\n-                                out);\n-\n-  // In XLA, broadcasts are explicit ops, allowing XNNPACK to assume there is no\n-  // broadcasting in the elementwise operation itself, which simplifies data\n-  // dependencies.\n-  uint32_t flags = XNN_FLAG_NO_BROADCAST;\n-  // IsXnnDotSupported has verified that rhs_contracting_dimensions has size 1.\n-  if (dnums.rhs_contracting_dimensions(0) !=\n-      dnums.rhs_batch_dimensions_size()) {\n-    flags |= XNN_FLAG_TRANSPOSE_B;\n-  }\n-  XNN_RETURN_IF_ERROR(xnn_define_batch_matrix_multiply(subgraph, lhs, rhs, out,\n-                                                       /*flags=*/flags));\n-\n-  return out;\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// Emit XNNPACK subgraph for the given HLO computation.\n-//===----------------------------------------------------------------------===//\n-\n-static absl::StatusOr<XnnSubgraph> EmitXnnSubgraph(\n-    const HloComputation* computation,\n-    std::vector<std::unique_ptr<Literal>>& literals) {\n-  VLOG(3) << \"Emit XNNPACK subgraph for computation: \" << computation->name();\n-\n-  TF_ASSIGN_OR_RETURN(\n-      XnnSubgraph subgraph, CreateXnnSubgraph([&](xnn_subgraph_t* subgraph) {\n-        return xnn_create_subgraph(\n-            /*external_value_ids=*/computation->num_parameters() + 1,\n-            /*flags=*/0, subgraph);\n-      }));\n-\n-  // Traverse fused computation in post-order and define XNNPACK operations\n-  // corresponding to each HLO instruction.\n-  TensorIdMap tensor_ids;\n-  auto instructions = computation->MakeInstructionPostOrder();\n-\n-  for (const HloInstruction* instr : instructions) {\n-    if (!IsLayoutSupportedByXnn(instr->shape())) {\n-      return InvalidArgument(\n-          \"Instruction with unsupported layout in XNN fusion: %s\",\n-          instr->ToString());\n-    }\n-\n-    if (instr->IsConstant()) {\n-      if (!IsConstantSupportedByXnn(instr)) {\n-        return InvalidArgument(\n-            \"Unsupported constant instruction in XNN fusion: %s\",\n-            instr->ToString());\n-      }\n-      TF_ASSIGN_OR_RETURN(tensor_ids[instr],\n-                          DefineConstant(subgraph.get(), literals, instr));\n-      continue;\n-    }\n-\n-    if (instr->IsElementwise()) {\n-      if (!IsElementwiseOpSupportedByXnn(instr)) {\n-        return InvalidArgument(\n-            \"Unsupported elementwise instruction in XNN fusion: %s\",\n-            instr->ToString());\n-      }\n-      if (instr->operand_count() == 1) {\n-        TF_ASSIGN_OR_RETURN(tensor_ids[instr],\n-                            DefineUnaryOp(subgraph.get(), tensor_ids, instr));\n-      } else if (instr->operand_count() == 2) {\n-        TF_ASSIGN_OR_RETURN(tensor_ids[instr],\n-                            DefineBinaryOp(subgraph.get(), tensor_ids, instr));\n-      } else {\n-        LOG(FATAL) << \"Unexpected operand count \" << instr->operand_count();\n-      }\n-      continue;\n-    }\n-\n-    switch (instr->opcode()) {\n-      case HloOpcode::kParameter: {\n-        TF_ASSIGN_OR_RETURN(tensor_ids[instr],\n-                            DefineParameter(subgraph.get(), instr));\n-      } break;\n-\n-      case HloOpcode::kBitcast: {\n-        if (!IsBitcastOpSupportedByXnn(instr)) {\n-          return InvalidArgument(\n-              \"Unsupported bitcast instruction in XNN fusion: %s\",\n-              instr->ToString());\n-        }\n-        TF_ASSIGN_OR_RETURN(tensor_ids[instr],\n-                            DefineBitcastOp(subgraph.get(), tensor_ids, instr));\n-      } break;\n-\n-      case HloOpcode::kBroadcast: {\n-        if (!IsBroadcastOpSupportedByXnn(instr)) {\n-          return InvalidArgument(\n-              \"Unsupported broadcast instruction in XNN fusion: %s\",\n-              instr->ToString());\n-        }\n-        TF_ASSIGN_OR_RETURN(\n-            tensor_ids[instr],\n-            DefineBroadcastOp(subgraph.get(), tensor_ids, instr));\n-      } break;\n-\n-      case HloOpcode::kReduce: {\n-        // FIXME: Validate the reduce instruction.\n-        // One cannot directly use IsReduceOpSupportedByXnn since the invariant\n-        // value is not necessarily included into the same fusion. This might\n-        // happen if the original instruction has multiple users or was rejected\n-        // by the fusion compiler pass.\n-        TF_ASSIGN_OR_RETURN(tensor_ids[instr],\n-                            DefineReduceOp(subgraph.get(), tensor_ids, instr));\n-      } break;\n-\n-      case HloOpcode::kDot: {\n-        TF_ASSIGN_OR_RETURN(\n-            tensor_ids[instr],\n-            DefineBatchMatMul(subgraph.get(), tensor_ids, instr));\n-      } break;\n-\n-      default: {\n-        return InvalidArgument(\"Unsupported XNNPACK fusion instruction: %s\",\n-                               instr->ToString());\n-      }\n-    }\n-  }\n-\n-  return subgraph;\n-}\n-\n-absl::StatusOr<absl::AnyInvocable<absl::StatusOr<XnnSubgraph>()>>\n-EmitXnnFusionBuilder(const HloComputation* computation) {\n-  // We do not support non-array parameters for XNNPACK operations.\n-  for (auto& param : computation->parameter_instructions()) {\n-    if (!param->shape().IsArray()) {\n-      return InvalidArgument(\n-          \"XNNPACK fusion parameters must have array shapes, got %s\",\n-          param->shape().ToString());\n-    }\n-  }\n-\n-  // Result also must be a single array.\n-  if (!computation->root_instruction()->shape().IsArray()) {\n-    return InvalidArgument(\"XNNPACK fusion result must be an array, got %s\",\n-                           computation->root_instruction()->shape().ToString());\n-  }\n-\n-  return [computation,\n-          literals = std::vector<std::unique_ptr<Literal>>()]() mutable {\n-    return EmitXnnSubgraph(computation, literals);\n-  };\n-}\n-\n-}  // namespace xla::cpu"
        },
        {
            "sha": "439e7f25d84e0c7140b64ec997ad7b7a9d3645d3",
            "filename": "third_party/xla/xla/backends/cpu/xnn_emitter.h",
            "status": "removed",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_emitter.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,31 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_BACKENDS_CPU_XNN_EMITTER_H_\n-#define XLA_BACKENDS_CPU_XNN_EMITTER_H_\n-\n-#include \"absl/functional/any_invocable.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_interop.h\"\n-#include \"xla/hlo/ir/hlo_computation.h\"\n-\n-namespace xla::cpu {\n-\n-absl::StatusOr<absl::AnyInvocable<absl::StatusOr<XnnSubgraph>()>>\n-EmitXnnFusionBuilder(const HloComputation* computation);\n-\n-}  // namespace xla::cpu\n-\n-#endif  // XLA_BACKENDS_CPU_XNN_EMITTER_H_"
        },
        {
            "sha": "6750a8495247413f368b9fa63d859085cadcdfb7",
            "filename": "third_party/xla/xla/backends/cpu/xnn_gemm_config.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 323,
            "changes": 323,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_gemm_config.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_gemm_config.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_gemm_config.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,323 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/cpu/xnn_gemm_config.h\"\n-\n-#include <algorithm>\n-#include <array>\n-#include <cmath>\n-#include <cstddef>\n-#include <limits>\n-#include <numeric>\n-\n-#include \"absl/base/no_destructor.h\"\n-#include \"absl/log/check.h\"\n-#include \"llvm/Target/TargetMachine.h\"\n-#include \"xla/backends/cpu/codegen/target_machine_features.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-namespace xla::cpu {\n-\n-namespace {\n-\n-double Relu(double x) { return std::max(0.0, x); }\n-\n-template <size_t Size>\n-std::array<double, Size> Relu(const std::array<double, Size>& input) {\n-  std::array<double, Size> output{};\n-  for (size_t i = 0; i < Size; ++i) {\n-    output[i] = Relu(input[i]);\n-  }\n-  return output;\n-}\n-\n-double Sigmoid(double x) { return 1.0 / (1.0 + std::exp(-x)); }\n-\n-double Sigmoid(std::array<double, 1> input) { return Sigmoid(input[0]); }\n-\n-template <size_t InSize, size_t OutSize>\n-struct Layer {\n-  std::array<std::array<double, InSize>, OutSize> weights;\n-  std::array<double, OutSize> biases;\n-\n-  std::array<double, OutSize> operator()(\n-      const std::array<double, InSize>& input) const {\n-    std::array<double, OutSize> output{};\n-    for (size_t i = 0; i < OutSize; ++i) {\n-      output[i] = std::inner_product(input.begin(), input.end(),\n-                                     weights[i].begin(), 0.0);\n-      output[i] += biases[i];\n-    }\n-    return output;\n-  }\n-};\n-\n-template <size_t InSize>\n-struct Scaler {\n-  std::array<double, InSize> mean;\n-  std::array<double, InSize> scale;\n-\n-  std::array<double, InSize> operator()(\n-      const std::array<double, InSize>& features) const {\n-    std::array<double, InSize> out;\n-    for (size_t i = 0; i < features.size(); ++i) {\n-      out[i] = (features[i] - mean[i]) / scale[i];\n-    }\n-    return out;\n-  }\n-};\n-\n-std::array<double, 6> ExtractFeatures(int m, int k, int n) {\n-  std::array<double, 6> features = {static_cast<double>(m),\n-                                    static_cast<double>(k),\n-                                    static_cast<double>(n),\n-                                    std::log(m),\n-                                    std::log(k),\n-                                    std::log(n)};\n-  return features;\n-}\n-\n-struct Net {\n-  static constexpr size_t kNumFeatures = 6;\n-  static constexpr size_t kHiddenLayer1Size = 8;\n-  static constexpr size_t kHiddenLayer2Size = 8;\n-\n-  Scaler<kNumFeatures> scaler;\n-  Layer<kNumFeatures, kHiddenLayer1Size> hidden_layer_1;\n-  Layer<kHiddenLayer1Size, kHiddenLayer2Size> hidden_layer_2;\n-  Layer<kHiddenLayer2Size, 1> output_layer;\n-  double threshold;\n-\n-  int operator()(double m, double k, double n) const {\n-    std::array<double, kNumFeatures> features = ExtractFeatures(m, k, n);\n-    double probability = Sigmoid(output_layer(\n-        Relu(hidden_layer_2(Relu(hidden_layer_1(scaler(features)))))));\n-    return probability < threshold ? 1 : 0;\n-  }\n-};\n-\n-struct Range {\n-  int min;\n-  int max;\n-\n-  template <class... Args>\n-  bool Contains(Args... args) const {\n-    auto check = [this](int x) -> bool { return min <= x && x <= max; };\n-    return (check(args) && ...);\n-  }\n-};\n-\n-struct GemmFilter {\n-  Range input_range;\n-  PrimitiveType lhs_dtype;\n-  PrimitiveType rhs_dtype;\n-  PrimitiveType out_dtype;\n-\n-  bool operator()(const XnnGemm& gemm) const {\n-    return input_range.Contains(gemm.dot_canonical_dims.m,\n-                                gemm.dot_canonical_dims.k,\n-                                gemm.dot_canonical_dims.n) &&\n-           gemm.lhs_dtype == lhs_dtype && gemm.rhs_dtype == rhs_dtype &&\n-           gemm.out_dtype == out_dtype &&\n-           gemm.dot_canonical_dims.lhs_canonical &&\n-           !gemm.dot_canonical_dims.rhs_column_major &&\n-           gemm.dot_canonical_dims.rhs_canonical &&\n-           !gemm.dot_canonical_dims.output_column_major;\n-  }\n-};\n-\n-// NOLINTBEGIN\n-// clang-format off\n-\n-static constexpr GemmFilter BF16BF16F32GemmFilter{\n-  /*input_range=*/{0, std::numeric_limits<int>::max()},\n-  /*lhs_dtype=*/PrimitiveType::BF16,\n-  /*rhs_dtype=*/PrimitiveType::BF16,\n-  /*out_dtype=*/PrimitiveType::F32,\n-};\n-\n-static constexpr GemmFilter AMDRomeGemmFilter{\n-  /*input_range=*/{16, 4096},\n-  /*lhs_dtype=*/PrimitiveType::F32,\n-  /*rhs_dtype=*/PrimitiveType::F32,\n-  /*out_dtype=*/PrimitiveType::F32,\n-};\n-\n-static constexpr Net AMDRomeNet{\n-  /*scaler=*/{\n-    /*mean=*/\n-    {{ 2031.4479060265578, 2036.3171603677222, 2062.2170582226763, 7.29227087924762, 7.308301476602625, 7.331674465299577 }},\n-    /*scale=*/\n-    {{ 1188.2177375470617, 1178.7350461452038, 1179.7790996965598, 1.0416890873676914, 1.0053399234375506, 0.9757991392501179 }},\n-  },\n-  /*hidden_layer_1=*/{\n-    /*weights=*/{{\n-      {{ 0.5255922128957278, -0.8065013670906714, -0.5264014380189966, -1.2772498330118651, 1.3840216299823802, 0.7322759674330881 }},\n-      {{ -0.7597171548555842, -1.2571169773685882, -0.32518437620636936, 1.0212806356673838, 0.9165371224616725, -0.19250317971610814 }},\n-      {{ -2.3497882574965994, 0.23878289300722322, -2.5867259166595944, 0.8432052252434499, -0.7374592701571068, 0.6061228206232958 }},\n-      {{ 0.3412638507438349, 0.009127030753615727, -0.43271581733053577, 0.3058216852138156, 0.4132978840654225, 0.08892908864656021 }},\n-      {{ -0.3843556431761765, -0.5398088470059381, -2.0478454682095735, -1.9041927205327738, -1.0368295384919808, -0.1653666006655781 }},\n-      {{ 0.9415170642828504, -0.4671602009419241, -2.594401365132767, 0.5011818371933664, 2.6743454901058725, 1.090931094328555 }},\n-      {{ -2.030867525769208, 0.9360281369657524, -2.179490537456837, 0.6315631977398317, -0.2797813498393135, 1.1780045163240112 }},\n-      {{ 2.026780502536945, 1.1382782700184098, 0.7076892737809293, -0.5003242829913847, 1.7337823655903326, 0.676979521067241 }},\n-    }},\n-    /*biases=*/{\n-      { 2.827760670625431, -0.9347274494671962, 1.7748650815163647, -0.5102747570142624, 1.1443725632238269, 2.0573020231014616, 0.33721201132380757, 2.7437956980307643 },\n-    }\n-  },\n-  /*hidden_layer_2=*/{\n-    /*weights=*/{{\n-      {{ 2.571821311709108, 0.16869445337763503, 0.3541411973512104, 0.31040383433531593, -1.9138308971941267, 1.577267326066108, 1.0358680188904088, -0.48597239908310547 }},\n-      {{ -0.3168524372865204, -0.8109707535168992, -0.6883758912881943, 0.20041683878416458, 0.29562419861502953, 2.9699371941875183, -0.06378706528945598, -1.2627270412739198 }},\n-      {{ 1.2121865841893051, 0.4324679330555888, 0.5756742637802713, -0.3965637421226802, -0.8316876650525071, 1.4267737797853521, 0.6590628275882154, 1.0969896994507335 }},\n-      {{ 0.08152092107879703, 0.987281670566132, 2.711801967605775, 0.03262333498333622, -0.24851434369301018, 0.5857580261361529, -0.14172228489696118, 1.0096244465236095 }},\n-      {{ -1.099617291565094, -0.96182176932886, 1.1198642662894356, 0.09569259551658717, 0.9865508260397995, -1.7073686127591108, 0.8545686868857858, 1.276785903326864 }},\n-      {{ 0.6284115174399925, -0.5692706408214737, -0.3776497427936689, 0.2850473804130665, 0.5611912673866001, 0.7074167980672433, 1.3602397130866593, -2.4641849404042104 }},\n-      {{ -0.2235255127724266, -0.6066818030776572, 2.098453748102861, -0.551860833640914, -0.6607678541967575, -1.0968858307838945, -3.097129404864497, 1.22936241411423 }},\n-      {{ -0.35359032516179434, 0.16659401401800453, 0.7409562527506246, 0.12880569714035928, 1.6235584538175323, 0.35055754805485, -0.5085408039033421, 0.03832167245213557 }},\n-    }},\n-    /*biases=*/{\n-      { -0.9650088973529635, 0.18404512445819377, -1.1301082618712814, -0.4114680200097482, -2.16829227705252, -0.792693003568079, 2.0186809343196432, 0.6651750830570318 },\n-    }\n-  },\n-  /*output_layer=*/{\n-    /*weights=*/{{\n-      {{ -3.4950798141841886, 3.052869401349734, -1.9332425183341917, -2.4468455334890375, 3.1182134156177734, 2.662143418701658, 3.609609051057281, -1.6114776062537006 }},\n-    }},\n-    /*biases=*/{\n-      { -0.8627209596023582 },\n-    }\n-  },\n-  /*threshold=*/0.03,\n-};\n-\n-static constexpr GemmFilter AMDGenoaGemmFilter{\n-  /*input_range=*/{16, 4096},\n-  /*lhs_dtype=*/PrimitiveType::F32,\n-  /*rhs_dtype=*/PrimitiveType::F32,\n-  /*out_dtype=*/PrimitiveType::F32,\n-};\n-\n-static constexpr Net AMDGenoaNet {\n-  /*scaler=*/{\n-    /*mean=*/\n-    {{ 2048.487742594484, 2032.4805924412667, 2042.0275791624106, 7.311636506981553, 7.331182177414692, 7.324348610024091 }},\n-    /*scale=*/\n-    {{ 1191.317145630777, 1166.4230415375375, 1162.7572402044934, 1.0130577584567735, 0.9372130582909888, 0.9819331632142719 }},\n-  },\n-  /*hidden_layer_1=*/{\n-    /*weights=*/{{\n-      {{ -0.3975566315544443, 0.5914998393825349, 0.6099048505253704, -2.2657754130482575, 0.36614796953745665, -0.9019941522654611 }},\n-      {{ -1.634528631004246, -1.0247790097319367, 0.7441596497436759, 1.1627072134985457, 0.05409335988074912, -0.12091065051829138 }},\n-      {{ 0.38395072299848293, 0.6541884828037803, 0.417837898603066, -0.9405446354332785, 2.184810649384631, -0.36876630139170674 }},\n-      {{ 1.4311717327837925, 0.9019482519954495, 0.010222966815173684, 0.3734603575926762, -0.48722286699557477, 0.6097423536728197 }},\n-      {{ -0.7136793187709407, -1.9428210404652928, 0.4274609198312262, 0.7241649472475438, 0.7127139917668667, -0.17169269406677637 }},\n-      {{ 0.7274093691413374, 1.5619764328746881, 0.3132760663502329, 0.1150444561729908, 0.2015964262316955, -1.6488397218364703 }},\n-      {{ -0.2753144111803734, 0.851664634951511, -0.7668837132534746, 0.8536953128922471, 0.5346385907475031, -0.3903852123459044 }},\n-      {{ -0.33049518181245935, -0.1445885038395346, 0.33671360297244707, 0.19923558301288513, 0.47714692266995923, 2.673625950077934 }},\n-    }},\n-    /*biases=*/{\n-      { 1.8781920773242509, 0.6510580145727756, 1.3641835181490685, -1.237083419397511, 0.09563962519162661, 1.0633713668067988, -0.2750294272946441, 0.4082406241441991 },\n-    }\n-  },\n-  /*hidden_layer_2=*/{\n-    /*weights=*/{{\n-      {{ 1.482788775138106, -0.5911919348052194, -0.35265948412831416, 0.5693173975201452, 0.08299331485534553, -1.0926309595949408, 0.334160671733911, -0.8259113265483281 }},\n-      {{ -0.7244072332431708, 1.7167578358580047, -0.4425799291591407, 0.38193961610444616, -0.3131049026459214, 0.7057668457879581, -0.8977670579096759, -1.1564071580034785 }},\n-      {{ 0.2358887563481682, 0.845047198622242, 0.3965633248481624, -0.9292260319808021, 0.38780851270938177, 0.9073719197977955, 0.8942857890487362, 2.2078844573893486 }},\n-      {{ 0.7588397006376895, 0.39649528525833017, 1.1922103753418032, -0.2623025347145879, -1.8688404509544276, 0.23950836230216038, 0.15018196046213705, 1.1091046070474726 }},\n-      {{ -0.06639877236719088, 0.09408482409872725, 0.08853697547037886, -0.027191640785169502, -0.025050403848262424, -0.14821218627938373, -0.05119778874800481, -0.003846457076482196 }},\n-      {{ -1.3626737341753659, -0.509211567650967, -1.3709529389911908, 0.8181695565961004, -0.9154056938786789, 1.6786394527771, -0.38910973671573107, 0.6109302318778375 }},\n-      {{ -0.9490250745418807, -0.22890259271729135, -0.7669763564967859, -1.2378100390537607, 0.9325554827865082, -0.7707072257516585, -0.6101643395959798, 0.6438447441624673 }},\n-      {{ 1.1581876959277013, 1.4439015663052703, -1.4659507082977212, 1.0425420146162472, -0.20891484120663645, 0.3292514803046433, 0.38947771607697135, 0.06588859566944062 }},\n-    }},\n-    /*biases=*/{\n-      { 2.0991435035679293, 0.9220598032166089, 0.001237522670163396, -0.2035381110666839, -0.7214610628375114, -2.275782698263265, 3.2572710355363337, -1.309956720253099 },\n-    }\n-  },\n-  /*output_layer=*/{\n-    /*weights=*/{{\n-      {{ -2.214950317234679, 2.3173207097966624, -2.4148863077632057, 2.440952250974181, 0.016504153668811035, 3.00219780922754, 2.454200734592688, 2.444832006369846 }},\n-    }},\n-    /*biases=*/{\n-      { -0.2538826384470055 },\n-    }\n-  },\n-  /*threshold=*/0.05,\n-};\n-\n-// clang-format on\n-// NOLINTEND\n-\n-bool IsAMDRome(const llvm::TargetMachine* target_machine) {\n-  CHECK(target_machine);\n-  return target_machine->getTargetCPU() == \"znver2\";\n-}\n-\n-bool IsAMDMilan(const llvm::TargetMachine* target_machine) {\n-  CHECK(target_machine);\n-  return target_machine->getTargetCPU() == \"znver3\";\n-}\n-\n-bool IsAMDGenoa(const llvm::TargetMachine* target_machine) {\n-  CHECK(target_machine);\n-  return target_machine->getTargetCPU() == \"znver4\";\n-}\n-\n-}  // namespace\n-\n-XnnGemmConfig::Opinion XnnGemmConfig::Evaluate(\n-    const XnnGemm& gemm, const TargetMachineFeatures* cpu_features) const {\n-  if (test_filter_) {\n-    return test_filter_(gemm) ? XnnGemmConfig::Opinion::kAccept\n-                              : XnnGemmConfig::Opinion::kReject;\n-  }\n-\n-  if (!cpu_features || !cpu_features->target_machine()) {\n-    return XnnGemmConfig::Opinion::kNoIdea;\n-  }\n-\n-  CHECK(cpu_features);\n-  CHECK(cpu_features->target_machine());\n-\n-  if (BF16BF16F32GemmFilter(gemm)) {\n-    return XnnGemmConfig::Opinion::kAccept;\n-  }\n-\n-  if ((IsAMDRome(cpu_features->target_machine()) ||\n-       IsAMDMilan(cpu_features->target_machine())) &&\n-      AMDRomeGemmFilter(gemm)) {\n-    int out = AMDRomeNet(gemm.dot_canonical_dims.m, gemm.dot_canonical_dims.k,\n-                         gemm.dot_canonical_dims.n);\n-    return out == 1 ? XnnGemmConfig::Opinion::kAccept\n-                    : XnnGemmConfig::Opinion::kReject;\n-  }\n-\n-  if (IsAMDGenoa(cpu_features->target_machine()) && AMDGenoaGemmFilter(gemm)) {\n-    int out = AMDGenoaNet(gemm.dot_canonical_dims.m, gemm.dot_canonical_dims.k,\n-                          gemm.dot_canonical_dims.n);\n-    return out == 1 ? XnnGemmConfig::Opinion::kAccept\n-                    : XnnGemmConfig::Opinion::kReject;\n-  }\n-\n-  return XnnGemmConfig::Opinion::kNoIdea;\n-}\n-\n-const XnnGemmConfig& GetXnnGemmConfig() {\n-  static const absl::NoDestructor<XnnGemmConfig> gemm_config;\n-  return *gemm_config;\n-}\n-\n-}  // namespace xla::cpu"
        },
        {
            "sha": "83bac68b0c2ce895838775e561b0d7151ddabb79",
            "filename": "third_party/xla/xla/backends/cpu/xnn_gemm_config.h",
            "status": "removed",
            "additions": 0,
            "deletions": 62,
            "changes": 62,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_gemm_config.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_gemm_config.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_gemm_config.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,62 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_BACKENDS_CPU_XNN_GEMM_CONFIG_H_\n-#define XLA_BACKENDS_CPU_XNN_GEMM_CONFIG_H_\n-\n-#include <functional>\n-\n-#include \"xla/backends/cpu/codegen/target_machine_features.h\"\n-#include \"xla/backends/cpu/runtime/dot_dims.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-namespace xla::cpu {\n-\n-struct XnnGemm {\n-  DotCanonicalDims dot_canonical_dims;\n-  PrimitiveType lhs_dtype;\n-  PrimitiveType rhs_dtype;\n-  PrimitiveType out_dtype;\n-};\n-\n-// XnnGemmConfig is a static lightweight  mechanism for determining if a given\n-// gemm should be offloaded to XNNPACK vs handled by OneDNN/Eigen.\n-// Currently it uses a classifier - neural network with: 6 input features\n-// m, k, n, log(m), log(k), log(n), two hidden layers of size 8 and a cut-off\n-// threshold for the predicted probability tuned to keep the false positive rate\n-// below 1%. The classifier was trained on synthetic data (20K random gemms).\n-// TODO(ashaposhnikov): add a reference to documentation / collab.\n-class XnnGemmConfig {\n-  mutable std::function<bool(const XnnGemm&)> test_filter_ = nullptr;\n-\n- public:\n-  XnnGemmConfig() = default;\n-\n-  enum class Opinion { kAccept, kReject, kNoIdea };\n-\n-  Opinion Evaluate(const XnnGemm& xnn_gemm,\n-                   const TargetMachineFeatures* cpu_features) const;\n-\n-  template <typename Filter>\n-  void SetTestFilter(Filter&& test_filter) const {\n-    test_filter_ = std::forward<Filter>(test_filter);\n-  }\n-};\n-\n-const XnnGemmConfig& GetXnnGemmConfig();\n-\n-}  // namespace xla::cpu\n-\n-#endif  // XLA_BACKENDS_CPU_XNN_GEMM_CONFIG_H_"
        },
        {
            "sha": "307d7adb85947241955cb7acd59bb03c44e8d14b",
            "filename": "third_party/xla/xla/backends/cpu/xnn_support.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 315,
            "changes": 315,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_support.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_support.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_support.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,315 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/cpu/xnn_support.h\"\n-\n-#include <algorithm>\n-#include <cstdint>\n-#include <limits>\n-#include <utility>\n-\n-#include \"xnnpack.h\"\n-#include \"absl/base/no_destructor.h\"\n-#include \"absl/container/flat_hash_map.h\"\n-#include \"absl/container/flat_hash_set.h\"\n-#include \"absl/log/check.h\"\n-#include \"absl/log/log.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/backends/cpu/codegen/target_machine_features.h\"\n-#include \"xla/backends/cpu/runtime/dot_dims.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_interop.h\"\n-#include \"xla/backends/cpu/xnn_gemm_config.h\"\n-#include \"xla/hlo/ir/hlo_casting_utils.h\"\n-#include \"xla/hlo/ir/hlo_computation.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/layout_util.h\"\n-#include \"xla/primitive_util.h\"\n-#include \"xla/service/pattern_matcher.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/shape_util.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/util.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-namespace xla::cpu {\n-\n-bool AreDtypesSupported(const Shape& lhs_shape, const Shape& rhs_shape,\n-                        const Shape& out_shape,\n-                        const TargetMachineFeatures* cpu_features) {\n-  // Stores tuple of allowed (input, output) dtypes.\n-  static const auto* kAllowedTypes =\n-      new absl::flat_hash_set<std::pair<PrimitiveType, PrimitiveType>>(\n-          {{F32, F32}, {BF16, F32}, {BF16, BF16}});\n-\n-  // Types must be in the allowed set.\n-  PrimitiveType lhs_dtype = lhs_shape.element_type();\n-  PrimitiveType rhs_dtype = rhs_shape.element_type();\n-  PrimitiveType out_dtype = out_shape.element_type();\n-  if (lhs_dtype != rhs_dtype ||\n-      !kAllowedTypes->contains({lhs_dtype, out_dtype})) {\n-    return false;\n-  }\n-\n-  // BF16 matmuls can only run when CPU has AVX512_BF16.\n-  if (lhs_dtype == BF16) {\n-    return cpu_features == nullptr || cpu_features->has_avx512bf16();\n-  }\n-  return true;\n-}\n-\n-absl::StatusOr<bool> IsDotSupportedByXnn(\n-    const DotDimensionNumbers& dot_dimensions, const Shape& lhs_shape,\n-    const Shape& rhs_shape, const Shape& out_shape,\n-    const TargetMachineFeatures* cpu_features, bool use_cost_model) {\n-  // Check data types.\n-  if (!AreDtypesSupported(lhs_shape, rhs_shape, out_shape, cpu_features)) {\n-    return false;\n-  }\n-  if (!IsLayoutSupportedByXnn(lhs_shape) ||\n-      !IsLayoutSupportedByXnn(rhs_shape) ||\n-      !IsLayoutSupportedByXnn(out_shape)) {\n-    return false;\n-  }\n-\n-  // Check shapes.\n-  TF_ASSIGN_OR_RETURN(DotShape dot_shape, GetDotShape(dot_dimensions, lhs_shape,\n-                                                      rhs_shape, out_shape));\n-\n-  TF_ASSIGN_OR_RETURN(DotCanonicalDims dot_canonical_dims,\n-                      GetDotCanonicalDims(dot_dimensions, dot_shape));\n-\n-  if (dot_canonical_dims.m == 1 && dot_canonical_dims.n == 1 &&\n-      dot_shape.batch_size > 1) {\n-    // TODO(b/430079105): XNNPACK does not handle batch dimensions that are not\n-    // matrix dimensions. We could handle this case by fully implementing dot\n-    // (b/430079105), but we also could just insert dummy dimensions of size 1\n-    // for the matrix dimensions, so the batch dimensions get handled correctly.\n-    return false;\n-  }\n-\n-  // XNNPACK does not support transposing LHS or col-major layouts.\n-  if (!dot_canonical_dims.lhs_canonical ||\n-      dot_canonical_dims.lhs_column_major ||\n-      dot_canonical_dims.rhs_column_major) {\n-    return false;\n-  }\n-\n-  if (!use_cost_model) {\n-    return true;\n-  }\n-\n-  const XnnGemm gemm{/*dot_canonical_dims=*/dot_canonical_dims,\n-                     /*lhs_dtype=*/lhs_shape.element_type(),\n-                     /*rhs_dtype=*/rhs_shape.element_type(),\n-                     /*out_dtype=*/out_shape.element_type()};\n-  switch (GetXnnGemmConfig().Evaluate(gemm, cpu_features)) {\n-    case XnnGemmConfig::Opinion::kAccept:\n-      return true;\n-    default:\n-      return false;\n-  }\n-}\n-\n-const absl::flat_hash_map<HloOpcode, xnn_unary_operator>& GetXnnUnaryOpMap() {\n-  // TODO(ashaposhnikov): Investigate adding support for kErf, kExpm1, kLog1p,\n-  // kNot, kRoundNearestAfz, kTan.\n-  static absl::NoDestructor<absl::flat_hash_map<HloOpcode, xnn_unary_operator>>\n-      unary_op_map({\n-          {HloOpcode::kAbs, xnn_unary_abs},\n-          {HloOpcode::kCeil, xnn_unary_ceiling},\n-          {HloOpcode::kClz, xnn_unary_count_leading_zeros},\n-          {HloOpcode::kConvert, xnn_unary_convert},\n-          {HloOpcode::kCos, xnn_unary_cosine},\n-          {HloOpcode::kExp, xnn_unary_exp},\n-          {HloOpcode::kCbrt, xnn_unary_cube_root},\n-          {HloOpcode::kFloor, xnn_unary_floor},\n-          {HloOpcode::kLog, xnn_unary_log},\n-          {HloOpcode::kLogistic, xnn_unary_sigmoid},\n-          {HloOpcode::kNegate, xnn_unary_negate},\n-          {HloOpcode::kRoundNearestEven, xnn_unary_bankers_rounding},\n-          {HloOpcode::kRsqrt, xnn_unary_reciprocal_square_root},\n-          {HloOpcode::kSign, xnn_unary_sign},\n-          {HloOpcode::kSin, xnn_unary_sine},\n-          {HloOpcode::kSqrt, xnn_unary_square_root},\n-          {HloOpcode::kTanh, xnn_unary_tanh},\n-      });\n-  return *unary_op_map;\n-}\n-\n-absl::StatusOr<xnn_unary_operator> XnnUnaryOperator(const HloOpcode& opcode) {\n-  const auto& unary_op_map = GetXnnUnaryOpMap();\n-  auto result = unary_op_map.find(opcode);\n-  if (result == unary_op_map.end()) {\n-    return InvalidArgument(\"Unsupported XNNPACK unary operator: %s\",\n-                           HloOpcodeString(opcode));\n-  }\n-  return result->second;\n-}\n-\n-const absl::flat_hash_map<HloOpcode, xnn_binary_operator>& GetXnnBinaryOpMap() {\n-  static absl::NoDestructor<absl::flat_hash_map<HloOpcode, xnn_binary_operator>>\n-      binary_op_map({\n-          {HloOpcode::kAdd, xnn_binary_add},\n-          {HloOpcode::kAnd, xnn_binary_bitwise_and},\n-          {HloOpcode::kDivide, xnn_binary_divide},\n-          {HloOpcode::kMaximum, xnn_binary_maximum},\n-          {HloOpcode::kMinimum, xnn_binary_minimum},\n-          {HloOpcode::kMultiply, xnn_binary_multiply},\n-          {HloOpcode::kOr, xnn_binary_bitwise_or},\n-          {HloOpcode::kPower, xnn_binary_pow},\n-          {HloOpcode::kRemainder, xnn_binary_modulus},\n-          {HloOpcode::kShiftLeft, xnn_binary_shift_left},\n-          {HloOpcode::kShiftRightArithmetic, xnn_binary_shift_right_arithmetic},\n-          {HloOpcode::kShiftRightLogical, xnn_binary_shift_right_logical},\n-          {HloOpcode::kSubtract, xnn_binary_subtract},\n-          {HloOpcode::kXor, xnn_binary_bitwise_xor},\n-      });\n-  return *binary_op_map;\n-}\n-\n-absl::StatusOr<xnn_binary_operator> XnnBinaryOperator(const HloOpcode& opcode) {\n-  const auto& binary_op_map = GetXnnBinaryOpMap();\n-  auto result = binary_op_map.find(opcode);\n-  if (result == binary_op_map.end()) {\n-    return InvalidArgument(\"Unsupported XNNPACK binary operator: %s\",\n-                           HloOpcodeString(opcode));\n-  }\n-  return result->second;\n-}\n-\n-bool IsLayoutSupportedByXnn(const Shape& shape) {\n-  return !shape.has_layout() || LayoutUtil::HasDescendingLayout(shape.layout());\n-}\n-\n-bool IsConstantSupportedByXnn(const HloInstruction* hlo) {\n-  CHECK(hlo->IsConstant());\n-\n-  if (!XnnDatatype(hlo->shape().element_type()).ok()) {\n-    return false;\n-  }\n-\n-  return hlo->shape().IsArray();\n-}\n-\n-bool IsElementwiseOpSupportedByXnn(const HloInstruction* hlo) {\n-  CHECK(hlo->IsElementwise());\n-  // In XLA IsElementwise is true for constants.\n-  CHECK(!hlo->IsConstant());\n-\n-  if (!XnnDatatype(hlo->shape().element_type()).ok()) {\n-    return false;\n-  }\n-\n-  if (!std::all_of(hlo->operands().begin(), hlo->operands().end(),\n-                   [](const HloInstruction* op) {\n-                     return XnnDatatype(op->shape().element_type()).ok();\n-                   })) {\n-    return false;\n-  }\n-\n-  switch (hlo->operand_count()) {\n-    case 1:\n-      return XnnUnaryOperator(hlo->opcode()).ok();\n-    case 2:\n-      return XnnBinaryOperator(hlo->opcode()).ok();\n-    default:\n-      return false;\n-  }\n-}\n-\n-bool IsBitcastOpSupportedByXnn(const HloInstruction* hlo) {\n-  CHECK_EQ(hlo->opcode(), HloOpcode::kBitcast);\n-  if (!XnnDatatype(hlo->shape().element_type()).ok()) {\n-    return false;\n-  }\n-  const HloInstruction* input = hlo->operand(0);\n-  return hlo->shape().element_type() == input->shape().element_type();\n-}\n-\n-bool IsBroadcastOpSupportedByXnn(const HloInstruction* hlo) {\n-  CHECK_EQ(hlo->opcode(), HloOpcode::kBroadcast);\n-  if (!XnnDatatype(hlo->shape().element_type()).ok()) {\n-    return false;\n-  }\n-  const absl::Span<const int64_t> dims =\n-      Cast<HloBroadcastInstruction>(hlo)->dimensions();\n-  if (dims.empty()) {\n-    return true;\n-  }\n-  if (!std::is_sorted(dims.begin(), dims.end())) {\n-    return false;\n-  }\n-  // TODO(ashaposhnikov): this case works well, but we should investigate the\n-  // performance regressions that occur if this condition is removed.\n-  return dims.back() + 1 == dims.size();\n-}\n-\n-template <class T>\n-static T InvariantValueFor(HloOpcode opcode) {\n-  switch (opcode) {\n-    case HloOpcode::kAdd:\n-      return T{0};\n-    case HloOpcode::kMinimum:\n-      return std::numeric_limits<T>::infinity();\n-    case HloOpcode::kMaximum:\n-      return -std::numeric_limits<T>::infinity();\n-    default:\n-      LOG(FATAL) << \"Unexpected opcode \" << opcode;\n-  }\n-}\n-\n-bool IsReduceOpSupportedByXnn(const HloInstruction* hlo) {\n-  CHECK_EQ(hlo->opcode(), HloOpcode::kReduce);\n-  if (!XnnDatatype(hlo->shape().element_type()).ok()) {\n-    return false;\n-  }\n-  const HloReduceInstruction* reduce = Cast<HloReduceInstruction>(hlo);\n-  CHECK_NE(reduce, nullptr);\n-  // TODO(ashaposhnikov): we can support this edge case,\n-  // planning to come back to this later.\n-  if (reduce->dimensions().empty()) {\n-    return false;\n-  }\n-  const HloComputation* to_apply = reduce->to_apply();\n-  CHECK_NE(to_apply, nullptr);\n-  if (!Match(to_apply->root_instruction(),\n-             match::AnyOf<HloInstruction>(match::Add(), match::Maximum(),\n-                                          match::Minimum())\n-                 .WithBinaryOperandsAnyOrder(match::Parameter(0),\n-                                             match::Parameter(1)))) {\n-    return false;\n-  }\n-  if (reduce->init_values().size() != 1) {\n-    return false;\n-  }\n-  HloInstruction* init = reduce->init_values().front();\n-  CHECK_EQ(init->shape().element_type(), hlo->shape().element_type());\n-  const HloOpcode opcode = to_apply->root_instruction()->opcode();\n-  const PrimitiveType ty = init->shape().element_type();\n-  return primitive_util::FloatingPointTypeSwitch(\n-      [&](auto primitive_type) {\n-        return Match(\n-            init,\n-            match::ConstantScalar(\n-                InvariantValueFor<primitive_util::NativeTypeOf<primitive_type>>(\n-                    opcode)));\n-      },\n-      ty);\n-}\n-\n-}  // namespace xla::cpu"
        },
        {
            "sha": "2e39e9430e3eb0e28f4723a2f183d92fc33012bb",
            "filename": "third_party/xla/xla/backends/cpu/xnn_support.h",
            "status": "removed",
            "additions": 0,
            "deletions": 77,
            "changes": 77,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_support.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_support.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_support.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,77 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_BACKENDS_CPU_XNN_SUPPORT_H_\n-#define XLA_BACKENDS_CPU_XNN_SUPPORT_H_\n-\n-#include \"xnnpack.h\"\n-#include \"absl/container/flat_hash_map.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"xla/backends/cpu/codegen/target_machine_features.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/xla_data.pb.h\"\n-\n-namespace xla::cpu {\n-\n-inline constexpr absl::string_view kXnnFusionKind = \"__xnn_fusion\";\n-\n-// Returns true if the dot operation is supported by XNNPACK. Returns an error\n-// if the dot operation shape is invalid.\n-absl::StatusOr<bool> IsDotSupportedByXnn(\n-    const DotDimensionNumbers& dot_dimensions, const Shape& lhs_shape,\n-    const Shape& rhs_shape, const Shape& out_shape,\n-    const TargetMachineFeatures* cpu_features = nullptr,\n-    bool use_cost_model = true);\n-\n-// Returns the mappings from HLO opcodes to XNNPACK unary operators.\n-const absl::flat_hash_map<HloOpcode, xnn_unary_operator>& GetXnnUnaryOpMap();\n-\n-// Returns the XNNPACK unary operator corresponding to the given HLO opcode.\n-// Returns `InvalidArgument` if the opcode is not supported.\n-absl::StatusOr<xnn_unary_operator> XnnUnaryOperator(const HloOpcode& opcode);\n-\n-// Returns the mappings from HLO opcodes to XNNPACK binary operators.\n-const absl::flat_hash_map<HloOpcode, xnn_binary_operator>& GetXnnBinaryOpMap();\n-\n-// Returns the XNNPACK binary operator corresponding to the given HLO opcode.\n-// Returns `InvalidArgument` if the opcode is not supported.\n-absl::StatusOr<xnn_binary_operator> XnnBinaryOperator(const HloOpcode& opcode);\n-\n-// Returns true if the shape either doesn't have a layout or the layout is\n-// descending. Shapes without layout are accepted to make HLO tests less\n-// verbose.\n-bool IsLayoutSupportedByXnn(const Shape& shape);\n-\n-// Returns true if the constant is supported by XNNPACK.\n-bool IsConstantSupportedByXnn(const HloInstruction* hlo);\n-\n-// Returns true if the nonconstant elementwise op is supported by XNNPACK.\n-bool IsElementwiseOpSupportedByXnn(const HloInstruction* hlo);\n-\n-// Returns true if the bitcast op is supported by XNNPACK.\n-bool IsBitcastOpSupportedByXnn(const HloInstruction* hlo);\n-\n-// Returns true if the broadcast op is supported by XNNPACK.\n-bool IsBroadcastOpSupportedByXnn(const HloInstruction* hlo);\n-\n-// Returns true if the reduce op is supported by XNNPACK.\n-bool IsReduceOpSupportedByXnn(const HloInstruction* hlo);\n-\n-}  // namespace xla::cpu\n-\n-#endif  // XLA_BACKENDS_CPU_XNN_SUPPORT_H_"
        },
        {
            "sha": "a69b3287f54d8ab9dc6316e7768262b83fdf663c",
            "filename": "third_party/xla/xla/backends/cpu/xnn_support_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 69,
            "changes": 69,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_support_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_support_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fxnn_support_test.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,69 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/backends/cpu/xnn_support.h\"\n-\n-#include <gtest/gtest.h>\n-#include \"xnnpack.h\"\n-#include \"absl/container/flat_hash_map.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-\n-namespace xla::cpu {\n-namespace {\n-\n-class XnnSupportTest : public ::testing::Test {};\n-\n-TEST_F(XnnSupportTest, UnaryEltwiseOpMap) {\n-  const auto& unary_map = GetXnnUnaryOpMap();\n-\n-  auto check = [&](const HloOpcode opcode, const xnn_unary_operator expected) {\n-    auto result = unary_map.find(opcode);\n-    EXPECT_NE(result, unary_map.end());\n-    EXPECT_EQ(result->second, expected);\n-  };\n-\n-  // Supported unary ops.\n-  check(HloOpcode::kAbs, xnn_unary_abs);\n-  check(HloOpcode::kExp, xnn_unary_exp);\n-  check(HloOpcode::kFloor, xnn_unary_floor);\n-  check(HloOpcode::kSqrt, xnn_unary_square_root);\n-\n-  // Unsupported unary ops.\n-  EXPECT_EQ(unary_map.find(HloOpcode::kErf), unary_map.end());\n-  EXPECT_EQ(unary_map.find(HloOpcode::kSort), unary_map.end());\n-}\n-\n-TEST_F(XnnSupportTest, BinaryEltwiseOpMap) {\n-  const auto& binary_map = GetXnnBinaryOpMap();\n-\n-  auto check = [&](const HloOpcode opcode, const xnn_binary_operator expected) {\n-    auto result = binary_map.find(opcode);\n-    EXPECT_NE(result, binary_map.end());\n-    EXPECT_EQ(result->second, expected);\n-  };\n-\n-  // Supported unary ops.\n-  check(HloOpcode::kAdd, xnn_binary_add);\n-  check(HloOpcode::kMultiply, xnn_binary_multiply);\n-  check(HloOpcode::kSubtract, xnn_binary_subtract);\n-  check(HloOpcode::kDivide, xnn_binary_divide);\n-\n-  // Unsupported unary ops.\n-  EXPECT_EQ(binary_map.find(HloOpcode::kAtan2), binary_map.end());\n-  EXPECT_EQ(binary_map.find(HloOpcode::kComplex), binary_map.end());\n-}\n-\n-}  // namespace\n-}  // namespace xla::cpu"
        },
        {
            "sha": "06f551239b61633cc93d54a228d8663567f377f5",
            "filename": "third_party/xla/xla/pjrt/cpu/cpu_client.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.cc?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -1647,17 +1647,13 @@ absl::StatusOr<PjRtLoadedExecutable::Result> PjRtCpuExecutable::ExecuteHelper(\n           cpu::Thunk::CustomCallExecuteParams custom_call_execute_params,\n           cpu::Thunk::CustomCallExecuteParams::Create(&run_options));\n \n-      std::optional<cpu::Thunk::XnnParams> xnn_params;\n-      if (cpu_executable->has_xnn_fusions()) {\n-        TF_ASSIGN_OR_RETURN(xnn_params,\n-                            cpu::Thunk::XnnParams::Create(&run_options));\n-      }\n-\n       std::optional<cpu::Thunk::YnnParams> ynn_params;\n+#ifdef XLA_YNNPACK\n       if (cpu_executable->has_ynn_fusions()) {\n         TF_ASSIGN_OR_RETURN(ynn_params,\n                             cpu::Thunk::YnnParams::Create(&run_options));\n       }\n+#endif  // XLA_YNNPACK\n \n       cpu::ThreadPoolTaskRunner task_runner(\n           run_options.intra_op_thread_pool()->getPool());\n@@ -1670,7 +1666,6 @@ absl::StatusOr<PjRtLoadedExecutable::Result> PjRtCpuExecutable::ExecuteHelper(\n           &task_runner,\n           &collective_params,\n           &custom_call_execute_params,\n-          xnn_params ? &*xnn_params : nullptr,\n           ynn_params ? &*ynn_params : nullptr,\n           run_options.run_id().ToInt(),\n           run_options.device_ordinal(),\n@@ -1796,17 +1791,13 @@ absl::StatusOr<PjRtLoadedExecutable::Result> PjRtCpuExecutable::ExecuteHelper(\n                 custom_call_params =\n                     cpu::Thunk::CustomCallExecuteParams::Create(&run_options);\n \n-            absl::StatusOr<std::optional<cpu::Thunk::XnnParams>> xnn_params(\n-                std::nullopt);\n-            if (cpu_executable->has_xnn_fusions()) {\n-              xnn_params = cpu::Thunk::XnnParams::Create(&run_options);\n-            }\n-\n             absl::StatusOr<std::optional<cpu::Thunk::YnnParams>> ynn_params(\n                 std::nullopt);\n+#ifdef XLA_YNNPACK\n             if (cpu_executable->has_ynn_fusions()) {\n               ynn_params = cpu::Thunk::YnnParams::Create(&run_options);\n             }\n+#endif  // XLA_YNNPACK\n \n             cpu::ThreadPoolTaskRunner task_runner(\n                 run_options.intra_op_thread_pool()->getPool());\n@@ -1820,7 +1811,6 @@ absl::StatusOr<PjRtLoadedExecutable::Result> PjRtCpuExecutable::ExecuteHelper(\n                   &task_runner,\n                   &*collective_params,\n                   &*custom_call_params,\n-                  *xnn_params ? &**xnn_params : nullptr,\n                   *ynn_params ? &**ynn_params : nullptr,\n                   run_options.run_id().ToInt(),\n                   run_options.device_ordinal(),"
        },
        {
            "sha": "3b5fcce8539a677f8fcc16b7eadc7ad47f2e337d",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -158,7 +158,6 @@ cc_library(\n         \"//xla/backends/cpu:alignment\",\n         \"//xla/backends/cpu:constant_allocation\",\n         \"//xla/backends/cpu:target_machine_options\",\n-        \"//xla/backends/cpu:xnn_support\",\n         \"//xla/backends/cpu/codegen:builtin_definition_generator\",\n         \"//xla/backends/cpu/codegen:compiled_function_library\",\n         \"//xla/backends/cpu/codegen:cpu_features\",\n@@ -174,7 +173,6 @@ cc_library(\n         \"//xla/backends/cpu/runtime:thunk_proto_cc_impl\",\n         \"//xla/backends/cpu/runtime:thunk_proto_serdes\",\n         \"//xla/backends/cpu/transforms:library_rewriter\",\n-        \"//xla/backends/cpu/transforms:xnn_graph_fusion\",\n         \"//xla/backends/cpu/transforms/collectives:all_reduce_combiner\",\n         \"//xla/hlo/analysis:alias_info\",\n         \"//xla/hlo/analysis:hlo_ordering\",\n@@ -821,8 +819,6 @@ cc_library(\n         \"//xla/backends/cpu:alignment\",\n         \"//xla/backends/cpu:onednn_emitter\",\n         \"//xla/backends/cpu:onednn_support\",\n-        \"//xla/backends/cpu:xnn_emitter\",\n-        \"//xla/backends/cpu:xnn_support\",\n         \"//xla/backends/cpu/codegen:computation_kernel_emitter\",\n         \"//xla/backends/cpu/codegen:fusion_compiler\",\n         \"//xla/backends/cpu/codegen:fusion_emitter\",\n@@ -855,8 +851,6 @@ cc_library(\n         \"//xla/backends/cpu/runtime:topk_thunk\",\n         \"//xla/backends/cpu/runtime:while_thunk\",\n         \"//xla/backends/cpu/runtime/onednn:onednn_fusion_thunk\",\n-        \"//xla/backends/cpu/runtime/xnnpack:xnn_dot_thunk\",\n-        \"//xla/backends/cpu/runtime/xnnpack:xnn_fusion_thunk\",\n         \"//xla/codegen:kernel_definition\",\n         \"//xla/codegen:kernel_spec\",\n         \"//xla/codegen:llvm_kernel_source\","
        },
        {
            "sha": "1071345acd574a74968990d702fce8b6d5f0deeb",
            "filename": "third_party/xla/xla/service/cpu/backend_config.proto",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fbackend_config.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fbackend_config.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fbackend_config.proto?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -20,6 +20,7 @@ message CustomCallBackendConfig {\n message FusionBackendConfig {\n   string kind = 1;\n   oneof custom_fusion_config_oneof {\n+    // TODO: b/467367981, this is deprecated and should be removed.\n     XnnFusionOptions xnn_fusion_options = 2;\n     YnnFusionOptions ynn_fusion_options = 3;\n   }"
        },
        {
            "sha": "2edab7cd05064fc5861d5dbafc9640b3acc0aa2d",
            "filename": "third_party/xla/xla/service/cpu/cpu_compiler.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 45,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -98,8 +98,6 @@ limitations under the License.\n #include \"xla/backends/cpu/target_machine_options.h\"\n #include \"xla/backends/cpu/transforms/collectives/all_reduce_combiner.h\"\n #include \"xla/backends/cpu/transforms/library_rewriter.h\"\n-#include \"xla/backends/cpu/transforms/xnn_graph_fusion.h\"\n-#include \"xla/backends/cpu/xnn_support.h\"\n #include \"xla/hlo/analysis/alias_info.h\"\n #include \"xla/hlo/analysis/hlo_ordering.h\"\n #include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n@@ -492,15 +490,7 @@ std::unique_ptr<HloPassFix<HloPassPipeline>> CreateSimplificationPipeline(\n     pipeline->AddPass<GatherSimplifier>();\n   }\n \n-  if (module->config()\n-              .debug_options()\n-              .xla_cpu_experimental_xnn_graph_fusion_mode() ==\n-          DebugOptions::XNN_GRAPH_FUSION_MODE_DISABLED &&\n-      !absl::c_contains(module->config()\n-                            .debug_options()\n-                            .xla_cpu_experimental_xnn_fusion_type(),\n-                        DebugOptions::LIBRARY_FUSION_TYPE_REDUCE) &&\n-      !absl::c_contains(module->config()\n+  if (!absl::c_contains(module->config()\n                             .debug_options()\n                             .xla_cpu_experimental_ynn_fusion_type(),\n                         DebugOptions::LIBRARY_FUSION_TYPE_REDUCE)) {\n@@ -544,58 +534,41 @@ std::unique_ptr<HloPassFix<HloPassPipeline>> CreateSimplificationPipeline(\n \n auto LibrarySupportsConvolution(\n     HloModule* module, TargetMachineFeatures* target_machine_features) {\n+#ifdef XLA_YNNPACK\n   const bool ynnpack_convolution_enabled = absl::c_linear_search(\n       module->config().debug_options().xla_cpu_experimental_ynn_fusion_type(),\n       DebugOptions::LIBRARY_FUSION_TYPE_INDIVIDUAL_CONVOLUTION);\n   return [=](const HloInstruction& instr) {\n-#ifdef XLA_YNNPACK\n     return ynnpack_convolution_enabled && IsConvolutionOpSupportedByYnn(&instr);\n-#endif  // XLA_YNNPACK\n-    return false;\n   };\n+#else\n+  return [](const HloInstruction&) { return false; };\n+#endif  // XLA_YNNPACK\n }\n \n auto LibrarySupportsDot(HloModule* module,\n                         TargetMachineFeatures* target_machine_features) {\n   // TODO(b/406806134): Stop calling XNNPACK from regular Dot thunks. All XNN\n   // Dots should be wrapped in an `__xnn_fusion` fusion region and processed in\n   // `XnnFusionThunk`.\n-  const bool xnnpack_enabled =\n-      module->config().debug_options().xla_cpu_use_xnnpack();\n-  const auto xnn_graph_fusion_mode =\n-      module->config()\n-          .debug_options()\n-          .xla_cpu_experimental_xnn_graph_fusion_mode();\n-  const bool xnnpack_use_cost_model =\n-      xnn_graph_fusion_mode !=\n-      DebugOptions::XNN_GRAPH_FUSION_MODE_BYPASS_COST_MODEL;\n-  const bool xnnpack_dot_enabled =\n-      xnnpack_enabled &&\n-      xnn_graph_fusion_mode != DebugOptions::XNN_GRAPH_FUSION_MODE_DISABLED;\n+#ifdef XLA_YNNPACK\n   const bool ynnpack_dot_enabled = absl::c_linear_search(\n       module->config().debug_options().xla_cpu_experimental_ynn_fusion_type(),\n       DebugOptions::LIBRARY_FUSION_TYPE_INDIVIDUAL_DOT);\n   return [=](const HloInstruction& instr) {\n-#ifdef XLA_YNNPACK\n     if (ynnpack_dot_enabled &&\n         IsDotSupportedByYnn(instr.dot_dimension_numbers(),\n                             instr.operand(0)->shape(),\n                             instr.operand(1)->shape(), instr.shape())\n             .value_or(false)) {\n       return true;\n     }\n-#endif  // XLA_YNNPACK\n \n-    if (xnnpack_dot_enabled &&\n-        IsDotSupportedByXnn(instr.dot_dimension_numbers(),\n-                            instr.operand(0)->shape(),\n-                            instr.operand(1)->shape(), instr.shape(),\n-                            target_machine_features, xnnpack_use_cost_model)\n-            .value_or(false)) {\n-      return true;\n-    }\n     return false;\n   };\n+#else\n+  return [](const HloInstruction&) { return false; };\n+#endif  // XLA_YNNPACK\n }\n \n }  // namespace\n@@ -1011,26 +984,18 @@ absl::Status CpuCompiler::RunHloPassesAfterLayoutAssn(\n       !debug_options.xla_cpu_experimental_ynn_fusion_type().empty();\n   LibraryRewriterOptions options = {\n       /*use_onednn=*/debug_options.xla_cpu_use_onednn(),\n-      /*use_xnnpack=*/debug_options.xla_cpu_use_xnnpack(),\n       /*use_ynnpack=*/use_ynnpack,\n       /*onednn_fusion_types=*/\n       &debug_options.xla_cpu_experimental_onednn_fusion_type(),\n-      /*xnn_fusion_types=*/\n-      &debug_options.xla_cpu_experimental_xnn_fusion_type(),\n       /*ynn_fusion_types=*/\n       &debug_options.xla_cpu_experimental_ynn_fusion_type()};\n-  if (options.use_onednn || options.use_xnnpack || options.use_ynnpack) {\n+  if (options.use_onednn || options.use_ynnpack) {\n     HloPassPipeline lib_pipeline(\"dot-library-passes\");\n     lib_pipeline.AddPass<DotDecomposer>();\n     lib_pipeline.AddPass<LibraryRewriter>(target_machine_features, options);\n     TF_RETURN_IF_ERROR(lib_pipeline.Run(module).status());\n   }\n \n-  if (debug_options.xla_cpu_experimental_xnn_graph_fusion_mode() !=\n-      DebugOptions::XNN_GRAPH_FUSION_MODE_DISABLED) {\n-    pipeline.AddPass<XnnGraphFusion>();\n-  }\n-\n   bool use_multi_output_fusion =\n       options::UseMultiOutputFusion(module->config());\n   pipeline.AddPass<CpuInstructionFusion>("
        },
        {
            "sha": "97bcff96d927ecb1657785c7ff516e862c8ee26e",
            "filename": "third_party/xla/xla/service/cpu/cpu_executable.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.cc?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -103,12 +103,6 @@ absl::StatusOr<std::unique_ptr<CpuExecutable>> CpuExecutable::Create(\n       executable->thunks_,\n       ThunkExecutor::Create(std::move(thunks), thunk_executor_options));\n \n-  // Find if the thunk sequence contains any XNN fusion thunks. If we do have\n-  // any, we will prepare the XNNPACK thread pool for them at run time.\n-  executable->thunks_->thunk_sequence().ForEach([&](const Thunk& thunk) {\n-    executable->has_xnn_fusions_ |= thunk.kind() == Thunk::Kind::kXnnFusion;\n-  });\n-\n   // Find if the thunk sequence contains any YNN fusion thunks. If we do have\n   // any, we will prepare the YNNPACK thread pool for them at run time.\n   executable->thunks_->thunk_sequence().ForEach([&](const Thunk& thunk) {\n@@ -262,17 +256,13 @@ absl::Status CpuExecutable::ExecuteThunks(\n   TF_ASSIGN_OR_RETURN(Thunk::CustomCallExecuteParams custom_call_execute_params,\n                       Thunk::CustomCallExecuteParams::Create(run_options));\n \n-  // Prepare for executing XNNPACK fusions.\n-  std::optional<Thunk::XnnParams> xnn_params;\n-  if (has_xnn_fusions()) {\n-    TF_ASSIGN_OR_RETURN(xnn_params, Thunk::XnnParams::Create(run_options));\n-  }\n-\n   // Prepare for executing YNNPACK fusions.\n   std::optional<Thunk::YnnParams> ynn_params;\n+#ifdef XLA_YNNPACK\n   if (has_ynn_fusions()) {\n     TF_ASSIGN_OR_RETURN(ynn_params, Thunk::YnnParams::Create(run_options));\n   }\n+#endif  // XLA_YNNPACK\n \n   // Use the intra-op thread pool to offload thunk executor tasks.\n   auto* intra_op_thread_pool = run_options->intra_op_thread_pool();\n@@ -287,7 +277,6 @@ absl::Status CpuExecutable::ExecuteThunks(\n       &task_runner,\n       &collective_execute_params,\n       &custom_call_execute_params,\n-      xnn_params ? &*xnn_params : nullptr,\n       ynn_params ? &*ynn_params : nullptr};\n \n   auto executed_event = thunks_->Execute(execute_params);"
        },
        {
            "sha": "7c8f397f542d905259da699d3e5d27664387a55c",
            "filename": "third_party/xla/xla/service/cpu/tests/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2FBUILD?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -426,23 +426,6 @@ xla_cc_test(\n     ],\n )\n \n-xla_cc_test(\n-    name = \"xnn_fusion_test\",\n-    srcs = [\"xnn_fusion_test.cc\"],\n-    deps = [\n-        \"//xla:error_spec\",\n-        \"//xla/backends/cpu:xnn_gemm_config\",\n-        \"//xla/service:cpu_plugin\",\n-        \"//xla/tests:hlo_test_base\",\n-        \"//xla/tsl/platform:test\",\n-        \"@com_google_absl//absl/container:flat_hash_map\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/strings:string_view\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@local_tsl//tsl/platform:platform_port\",\n-    ],\n-)\n-\n xla_cc_test(\n     name = \"cpu_copy_test\",\n     srcs = [\"cpu_copy_test.cc\"],"
        },
        {
            "sha": "095805fdfffe2f4d447a578d3cbebb6efe1d2685",
            "filename": "third_party/xla/xla/service/cpu/tests/xnn_fusion_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 388,
            "changes": 388,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fxnn_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fxnn_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fxnn_fusion_test.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1,388 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include <string>\n-#include <vector>\n-\n-#include <gtest/gtest.h>\n-#include \"absl/container/flat_hash_map.h\"\n-#include \"absl/strings/str_cat.h\"\n-#include \"absl/strings/str_replace.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"xla/backends/cpu/xnn_gemm_config.h\"\n-#include \"xla/error_spec.h\"\n-#include \"xla/tests/hlo_test_base.h\"\n-#include \"xla/tsl/platform/test.h\"\n-#include \"tsl/platform/cpu_info.h\"\n-\n-namespace xla::cpu {\n-namespace {\n-\n-using ::testing::HasSubstr;\n-\n-struct XnnFusionTestParams {\n-  std::string in_dtype;\n-  std::string out_dtype;  // Only used for mixed input/output types.\n-};\n-\n-class XnnFusionTest\n-    : public HloTestBase,\n-      public ::testing::WithParamInterface<XnnFusionTestParams> {\n- public:\n-  static std::string Name(\n-      const ::testing::TestParamInfo<XnnFusionTestParams>& info) {\n-    return absl::StrCat(info.param.in_dtype, \"_\", info.param.out_dtype);\n-  }\n-\n- protected:\n-  XnnFusionTest() {\n-    // Override XnnGemmConfig.\n-    GetXnnGemmConfig().SetTestFilter([](const XnnGemm&) { return true; });\n-  }\n-\n-  ~XnnFusionTest() override { GetXnnGemmConfig().SetTestFilter(nullptr); }\n-\n-  void RunTest(absl::string_view hlo_template, absl::string_view check_str) {\n-    XnnFusionTestParams params = GetParam();\n-    std::string hlo_text =\n-        absl::StrReplaceAll(hlo_template, {{\"$dtype\", params.in_dtype},\n-                                           {\"$in_dtype\", params.in_dtype},\n-                                           {\"$out_dtype\", params.out_dtype}});\n-    bool bf16_compute = params.in_dtype == \"bf16\" || params.out_dtype == \"bf16\";\n-    double tolerance = bf16_compute ? 1e-2 : 1e-7;\n-    EXPECT_TRUE(RunAndCompare(\n-        hlo_text, ErrorSpec{/*aabs=*/tolerance, /*arel=*/tolerance}));\n-\n-    if (bf16_compute && !check_str.empty()) {\n-      std::string check_text =\n-          absl::StrReplaceAll(check_str, {{\"$dtype\", params.in_dtype},\n-                                          {\"$in_dtype\", params.in_dtype},\n-                                          {\"$out_dtype\", params.out_dtype}});\n-      MatchOptimizedHlo(hlo_text, check_text);\n-    }\n-  }\n-};\n-\n-bool ShouldSkipDotBf16Test(absl::string_view in_dtype) {\n-  return in_dtype == \"bf16\" &&\n-         !tsl::port::TestCPUFeature(tsl::port::AVX512_BF16);\n-}\n-\n-absl::string_view GetOutputTypeSupportedByXnnBatchMatMul(\n-    absl::string_view in_dtype) {\n-  static const auto* kSupportedOutputTypes =\n-      new absl::flat_hash_map<absl::string_view, absl::string_view>(\n-          {{\"f32\", \"f32\"}, {\"bf16\", \"f32\"}});\n-\n-  return kSupportedOutputTypes->at(in_dtype);\n-}\n-\n-std::string InsertConvertIfNecessary(absl::string_view hlo_text,\n-                                     absl::string_view in_dtype,\n-                                     absl::string_view out_dtype,\n-                                     absl::string_view convert_text) {\n-  absl::string_view supported_dtype =\n-      GetOutputTypeSupportedByXnnBatchMatMul(in_dtype);\n-  bool need_convert = out_dtype != supported_dtype;\n-  return absl::StrReplaceAll(\n-      hlo_text, {{\"$root \", need_convert ? \"\" : \"ROOT \"},\n-                 {\"$dot_dtype\", need_convert ? supported_dtype : \"$out_dtype\"},\n-                 {\"$convert_if_necessary\", need_convert ? convert_text : \"\"},\n-                 {\"$dot_or_convert\", need_convert ? \"%convert\" : \"%dot\"}});\n-}\n-\n-// For tests that always have same input/output types.\n-using SameTypeTest = XnnFusionTest;\n-\n-TEST_P(SameTypeTest, AddAndMultiply) {\n-  constexpr absl::string_view kModuleStr = R\"(\n-    HloModule add_and_multiply\n-\n-    xnn_fusion {\n-      %lhs = $dtype[4] parameter(0)\n-      %rhs = $dtype[4] parameter(1)\n-      %add = $dtype[4] add(%lhs, %rhs)\n-      ROOT %mul = $in_dtype[4] multiply(%add, %add)\n-    }\n-\n-    ENTRY entry {\n-      %p0 = $dtype[4] parameter(0)\n-      %p1 = $dtype[4] parameter(1)\n-      ROOT %fusion = $dtype[4] fusion(%p0, %p1), kind=kCustom, calls=xnn_fusion,\n-        backend_config={\"fusion_config\": {kind: \"__xnn_fusion\"}}\n-    })\";\n-\n-  // Optimized HLO shouldn't have any convert.\n-  constexpr absl::string_view kCheckStr = R\"(\n-    CHECK:      %xnn_fusion\n-    CHECK-NOT:  convert\n-    CHECK:      multiply\n-  )\";\n-\n-  RunTest(kModuleStr, kCheckStr);\n-}\n-\n-TEST_P(SameTypeTest, DotAddMultiply) {\n-  XnnFusionTestParams params = GetParam();\n-  if (ShouldSkipDotBf16Test(params.in_dtype)) {\n-    GTEST_SKIP() << \"XNNPACK bf16 matmul requires AVX512_BF16 which this CPU \"\n-                    \"doesn't have.\";\n-  }\n-\n-  constexpr absl::string_view kModuleStr = R\"(\n-    HloModule dot_add_multiply\n-\n-    xnn_fusion {\n-      %lhs = $dtype[4,5] parameter(0)\n-      %rhs = $dtype[5,6] parameter(1)\n-      %addend = $dtype[4,6] parameter(2)\n-      %multiplier = $dtype[4,6] parameter(3)\n-      %dot = $dot_dtype[4,6] dot(%lhs, %rhs),\n-        lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-      $convert_if_necessary\n-      %add = $dtype[4,6] add($dot_or_convert, %addend)\n-      ROOT %mul = $dtype[4,6] multiply(%add, %multiplier)\n-    }\n-\n-    ENTRY entry {\n-      %lhs = $dtype[4,5] parameter(0)\n-      %rhs = $dtype[5,6] parameter(1)\n-      %addend = $dtype[4, 6] parameter(2)\n-      %multiplier = $dtype[4, 6] parameter(3)\n-      ROOT %fusion = $dtype[4,6] fusion(%lhs, %rhs, %addend, %multiplier),\n-        kind=kCustom, calls=xnn_fusion,\n-        backend_config={\"fusion_config\": {kind: \"__xnn_fusion\"}}\n-    })\";\n-\n-  constexpr absl::string_view kConvertStr =\n-      \"%convert = $dtype[4,6] convert(%dot)\";\n-\n-  // Optimized HLO shouldn't have any convert before the dot.\n-  constexpr absl::string_view kCheckStr = R\"(\n-    CHECK:      %xnn_fusion\n-    CHECK-NOT:  convert\n-    CHECK:      dot\n-  )\";\n-\n-  RunTest(InsertConvertIfNecessary(kModuleStr, params.in_dtype,\n-                                   params.out_dtype, kConvertStr),\n-          kCheckStr);\n-}\n-\n-TEST_P(SameTypeTest, DotRhsTransposedAndMultiply) {\n-  XnnFusionTestParams params = GetParam();\n-  if (ShouldSkipDotBf16Test(params.in_dtype)) {\n-    GTEST_SKIP() << \"XNNPACK bf16 matmul requires AVX512_BF16 which this CPU \"\n-                    \"doesn't have.\";\n-  }\n-\n-  constexpr absl::string_view kModuleStr = R\"(\n-    HloModule dot_rhs_transposed_and_multiply\n-\n-    xnn_fusion {\n-      %lhs = $dtype[4,5] parameter(0)\n-      %rhs = $dtype[6,5] parameter(1)\n-      %multiplier = $dtype[4,6] parameter(2)\n-      %dot = $dot_dtype[4,6] dot(%lhs, %rhs),\n-        lhs_contracting_dims={1}, rhs_contracting_dims={1}\n-      $convert_if_necessary\n-      ROOT %mul = $dtype[4,6] multiply($dot_or_convert, %multiplier)\n-    }\n-\n-    ENTRY entry {\n-      %lhs = $dtype[4,5] parameter(0)\n-      %rhs = $dtype[6,5] parameter(1)\n-      %multiplier = $dtype[4, 6] parameter(2)\n-      ROOT %fusion = $dtype[4,6] fusion(%lhs, %rhs, %multiplier),\n-        kind=kCustom, calls=xnn_fusion,\n-        backend_config={\"fusion_config\": {kind: \"__xnn_fusion\"}}\n-    })\";\n-\n-  constexpr absl::string_view kConvertStr =\n-      \"%convert = $dtype[4,6] convert(%dot)\";\n-\n-  // Optimized HLO shouldn't have any convert before the dot.\n-  constexpr absl::string_view kCheckStr = R\"(\n-    CHECK:      %xnn_fusion\n-    CHECK-NOT:  convert\n-    CHECK:      dot\n-  )\";\n-\n-  RunTest(InsertConvertIfNecessary(kModuleStr, params.in_dtype,\n-                                   params.out_dtype, kConvertStr),\n-          kCheckStr);\n-}\n-\n-std::vector<XnnFusionTestParams> GetSameTypeTestCases() {\n-  return std::vector<XnnFusionTestParams>({\n-      XnnFusionTestParams{\"f32\", \"f32\" /*unused*/},\n-      XnnFusionTestParams{\"bf16\", \"bf16\" /*unused*/},\n-  });\n-}\n-\n-INSTANTIATE_TEST_SUITE_P(SameTypeTestInstantiation, SameTypeTest,\n-                         ::testing::ValuesIn(GetSameTypeTestCases()),\n-                         XnnFusionTest::Name);\n-\n-// For tests that we might want to use different input/output types.\n-using MixedTypesTest = XnnFusionTest;\n-\n-TEST_P(MixedTypesTest, BatchedDot) {\n-  XnnFusionTestParams params = GetParam();\n-  if (ShouldSkipDotBf16Test(params.in_dtype)) {\n-    GTEST_SKIP() << \"XNNPACK bf16 matmul requires AVX512_BF16 which this CPU\"\n-                    \"doesn't have.\";\n-  }\n-\n-  constexpr absl::string_view kModuleStr = R\"(\n-    HloModule dot_add_multiply\n-\n-    xnn_fusion {\n-      %lhs = $in_dtype[2,3,4,5] parameter(0)\n-      %rhs = $in_dtype[2,3,5,6] parameter(1)\n-      $root %dot = $dot_dtype[2,3,4,6] dot(%lhs, %rhs),\n-        lhs_batch_dims={0,1}, rhs_batch_dims={0,1},\n-        lhs_contracting_dims={3}, rhs_contracting_dims={2}\n-      $convert_if_necessary\n-    }\n-\n-    ENTRY entry {\n-      %lhs = $in_dtype[2,3,4,5] parameter(0)\n-      %rhs = $in_dtype[2,3,5,6] parameter(1)\n-      ROOT %fusion = $out_dtype[2,3,4,6] fusion(%lhs, %rhs),\n-        kind=kCustom, calls=xnn_fusion,\n-        backend_config={\"fusion_config\": {kind: \"__xnn_fusion\"}}\n-    })\";\n-\n-  constexpr absl::string_view kConvertStr =\n-      \"ROOT %convert = $out_dtype[2,3,4,6] convert(%dot)\";\n-\n-  // Optimized HLO shouldn't have any convert before the dot.\n-  constexpr absl::string_view kCheckStr = R\"(\n-    CHECK:      %xnn_fusion\n-    CHECK-NOT:  convert\n-    CHECK:      dot\n-  )\";\n-\n-  RunTest(InsertConvertIfNecessary(kModuleStr, params.in_dtype,\n-                                   params.out_dtype, kConvertStr),\n-          kCheckStr);\n-}\n-\n-std::vector<XnnFusionTestParams> GetMixedTypesTestCases() {\n-  return std::vector<XnnFusionTestParams>({\n-      XnnFusionTestParams{\"f32\", \"f32\"},\n-      XnnFusionTestParams{\"bf16\", \"f32\"},\n-      XnnFusionTestParams{\"bf16\", \"bf16\"},\n-  });\n-}\n-\n-INSTANTIATE_TEST_SUITE_P(MixedTypesTestInstantiation, MixedTypesTest,\n-                         ::testing::ValuesIn(GetMixedTypesTestCases()),\n-                         XnnFusionTest::Name);\n-\n-TEST_F(XnnFusionTest, ConvertF32ToBF16) {\n-  constexpr absl::string_view kModuleStr = R\"(\n-    HloModule convert\n-\n-    xnn_fusion {\n-      %input = f32[2,3,4,5] parameter(0)\n-      ROOT %dot = bf16[2,3,4,5] convert(%input)\n-    }\n-\n-    ENTRY entry {\n-      %input = f32[2,3,4,5] parameter(0)\n-      ROOT %fusion = bf16[2,3,4,5] fusion(%input),\n-        kind=kCustom, calls=xnn_fusion,\n-        backend_config={\"fusion_config\": {kind: \"__xnn_fusion\"}}\n-    })\";\n-\n-  EXPECT_TRUE(RunAndCompare(kModuleStr, ErrorSpec{1e-2}));\n-}\n-\n-// The following tests don't need to be run with different data types.\n-TEST_F(XnnFusionTest, UnsupportedDot) {\n-  constexpr absl::string_view kModuleStr = R\"(\n-    HloModule unsupported_dot\n-\n-    xnn_fusion {\n-      %lhs = f32[5,4] parameter(0)\n-      %rhs = f32[5,6] parameter(1)\n-      ROOT %dot = f32[4,6] dot(%lhs, %rhs),\n-        lhs_contracting_dims={0}, rhs_contracting_dims={0}\n-    }\n-\n-    ENTRY entry {\n-      %lhs = f32[5,4] parameter(0)\n-      %rhs = f32[5,6] parameter(1)\n-      ROOT %fusion = f32[4,6] fusion(%lhs, %rhs),\n-        kind=kCustom, calls=xnn_fusion,\n-        backend_config={\"fusion_config\": {kind: \"__xnn_fusion\"}}\n-    })\";\n-\n-  auto status = RunAndCompare(kModuleStr, ErrorSpec{0.0});\n-  EXPECT_FALSE(status);\n-  EXPECT_THAT(status.message(),\n-              HasSubstr(\"Unsupported XNNPACK Dot op variation\"));\n-}\n-\n-TEST_F(XnnFusionTest, UnsupportedBatchDot) {\n-  constexpr absl::string_view kModuleStr = R\"(\n-    HloModule unsupported_dot\n-\n-    xnn_fusion {\n-      %lhs = f32[64,64] parameter(0)\n-      %rhs = f32[64,64] parameter(1)\n-      ROOT %dot = f32[64]{0} dot(%lhs, %rhs),\n-        lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n-    }\n-\n-    ENTRY entry {\n-      %lhs = f32[64,64] parameter(0)\n-      %rhs = f32[64,64] parameter(1)\n-      ROOT %fusion = f32[64] fusion(%lhs, %rhs),\n-        kind=kCustom, calls=xnn_fusion,\n-        backend_config={\"fusion_config\": {kind: \"__xnn_fusion\"}}\n-    })\";\n-\n-  auto status = RunAndCompare(kModuleStr, ErrorSpec{0.0});\n-  EXPECT_FALSE(status);\n-  EXPECT_THAT(status.message(),\n-              HasSubstr(\"Unsupported XNNPACK Dot op variation\"));\n-}\n-\n-TEST_F(XnnFusionTest, UnsupportedOp) {\n-  constexpr absl::string_view kModuleStr = R\"(\n-    HloModule unsupported_sqrt\n-\n-    xnn_fusion {\n-      %x = f32[10] parameter(0)\n-      ROOT %e = f32[10] erf(%x)\n-    }\n-\n-    ENTRY entry {\n-      %x = f32[10] parameter(0)\n-      ROOT %e = f32[10] fusion(%x), kind=kCustom, calls=xnn_fusion,\n-        backend_config={\"fusion_config\": {kind: \"__xnn_fusion\"}}\n-    })\";\n-\n-  auto status = RunAndCompare(kModuleStr, ErrorSpec{0.0});\n-  EXPECT_FALSE(status);\n-  EXPECT_THAT(status.message(),\n-              HasSubstr(\"Unsupported elementwise instruction in XNN fusion\"));\n-}\n-\n-}  // namespace\n-}  // namespace xla::cpu"
        },
        {
            "sha": "80a9fe9245982f63a98d2d24d17fc62dfc67a190",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 68,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a383df817ea33a46f6ece3e915762d8e192dcbfb/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc?ref=a383df817ea33a46f6ece3e915762d8e192dcbfb",
            "patch": "@@ -69,10 +69,6 @@ limitations under the License.\n #include \"xla/backends/cpu/runtime/thunk.h\"\n #include \"xla/backends/cpu/runtime/topk_thunk.h\"\n #include \"xla/backends/cpu/runtime/while_thunk.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_dot_thunk.h\"\n-#include \"xla/backends/cpu/runtime/xnnpack/xnn_fusion_thunk.h\"\n-#include \"xla/backends/cpu/xnn_emitter.h\"\n-#include \"xla/backends/cpu/xnn_support.h\"\n #include \"xla/codegen/emitters/computation_fingerprint.h\"\n #include \"xla/codegen/emitters/kernel_api_builder.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n@@ -445,10 +441,6 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitHloInstruction(\n         }\n #endif  // XLA_ONEDNN_USE_GRAPH_API\n \n-        if (backend_config.fusion_config().kind() == kXnnFusionKind) {\n-          return EmitXnnFusionThunk(instruction);\n-        }\n-\n #ifdef XLA_YNNPACK\n         if (backend_config.fusion_config().kind() == kYnnFusionKind) {\n           return EmitYnnFusionThunk(instruction);\n@@ -1114,31 +1106,9 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitDotThunk(\n       }\n #endif  // XLA_YNNPACK\n \n-      // Decide whether to use XNNPACK or Eigen.\n-      bool use_xnn = hlo_module_config_.debug_options().xla_cpu_use_xnnpack();\n-      if (use_xnn) {\n-        const bool use_cost_model =\n-            hlo_module_config_.debug_options()\n-                .xla_cpu_experimental_xnn_graph_fusion_mode() !=\n-            DebugOptions::XNN_GRAPH_FUSION_MODE_BYPASS_COST_MODEL;\n-        TF_ASSIGN_OR_RETURN(\n-            use_xnn,\n-            IsDotSupportedByXnn(dnums, lhs->shape(), rhs->shape(),\n-                                instruction->shape(), &target_machine_features_,\n-                                use_cost_model));\n-      }\n-\n-      if (use_xnn) {\n-        bool capture_rhs = HloPredicateIsOp<HloOpcode::kParameter>(rhs);\n-        return ThunkSequence::Of<XnnDotThunk>(\n-            XnnDotThunk::Options{}, ThunkInfo(instruction), dnums, lhs_slice,\n-            lhs->shape(), rhs_slice, rhs->shape(), out_slice,\n-            instruction->shape(), capture_rhs);\n-      } else {\n-        return ThunkSequence::Of<DotThunk>(\n-            ThunkInfo(instruction), dnums, lhs_slice, lhs->shape(), rhs_slice,\n-            rhs->shape(), out_slice, instruction->shape());\n-      }\n+      return ThunkSequence::Of<DotThunk>(\n+          ThunkInfo(instruction), dnums, lhs_slice, lhs->shape(), rhs_slice,\n+          rhs->shape(), out_slice, instruction->shape());\n     }\n   }\n }\n@@ -1502,41 +1472,6 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitOneDnnFusionThunk(\n #endif  // XLA_ONEDNN_USE_GRAPH_API\n }\n \n-absl::StatusOr<ThunkSequence> ThunkEmitter::EmitXnnFusionThunk(\n-    const HloInstruction* instruction) {\n-  auto* fusion = Cast<HloFusionInstruction>(instruction);\n-\n-  // Collect XNNPACK fusion arguments.\n-  std::vector<XnnFusionThunk::Argument> arguments;\n-  for (HloInstruction* operand : instruction->operands()) {\n-    for (auto& indexed : ShapeUtil::GetLeafShapes(operand->shape())) {\n-      TF_ASSIGN_OR_RETURN(\n-          BufferAllocation::Slice slice,\n-          buffer_assignment_.GetUniqueSlice(operand, indexed.index));\n-      arguments.push_back(XnnFusionThunk::Argument{slice, indexed.shape});\n-    }\n-  }\n-\n-  // Collect XNNPACK fusion results.\n-  std::vector<XnnFusionThunk::Result> results;\n-  for (auto& indexed : ShapeUtil::GetLeafShapes(instruction->shape())) {\n-    TF_ASSIGN_OR_RETURN(\n-        BufferAllocation::Slice slice,\n-        buffer_assignment_.GetUniqueSlice(instruction, indexed.index));\n-    results.push_back(XnnFusionThunk::Result{slice, indexed.shape});\n-  }\n-\n-  const HloComputation* computation = fusion->fused_instructions_computation();\n-\n-  // Construct XNNPACK subgraph builder from the fusion computation.\n-  TF_ASSIGN_OR_RETURN(auto builder, EmitXnnFusionBuilder(computation));\n-\n-  return ThunkSequence::Of<XnnFusionThunk>(\n-      XnnFusionThunk::Options{}, ThunkInfo(instruction), std::move(arguments),\n-      std::move(results),\n-      [b = std::move(builder)](auto, auto) mutable { return b(); });\n-}\n-\n absl::StatusOr<ThunkSequence> ThunkEmitter::EmitYnnFusionThunk(\n     const HloInstruction* instruction) {\n #ifdef XLA_YNNPACK"
        }
    ],
    "stats": {
        "total": 5742,
        "additions": 27,
        "deletions": 5715
    }
}