{
    "author": "ezhulenev",
    "message": "[xla:gpu] Extract collective_execution library from collective_thunk for reuse in GPU FFI\n\nWe want to support collective operations via GPU FFI calls, and don't want to depend on irrelevant parts of thunk runtime from the FFI library.\n\nPiperOrigin-RevId: 838808711",
    "sha": "82bf6501fd3c6e49be45b0066492835c56aefbd1",
    "files": [
        {
            "sha": "7cac56aef12cdf2659dc989ba8fbc82c9eaed56b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/82bf6501fd3c6e49be45b0066492835c56aefbd1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/82bf6501fd3c6e49be45b0066492835c56aefbd1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=82bf6501fd3c6e49be45b0066492835c56aefbd1",
            "patch": "@@ -67,6 +67,7 @@ cc_library(\n         \":all_to_all_thunk\",\n         \":annotation\",\n         \":collective_broadcast_thunk\",\n+        \":collective_execution\",\n         \":collective_permute_thunk\",\n         \":collective_thunk\",\n         \":copy_thunk\",\n@@ -1657,12 +1658,36 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"collective_execution\",\n+    srcs = [\"collective_execution.cc\"],\n+    hdrs = [\"collective_execution.h\"],\n+    deps = [\n+        \":collective_cliques\",\n+        \":collective_params\",\n+        \"//xla:debug_options_flags\",\n+        \"//xla:status_macros\",\n+        \"//xla:util\",\n+        \"//xla/backends/gpu/collectives:gpu_clique_key\",\n+        \"//xla/core/collectives:communicator\",\n+        \"//xla/core/collectives:rank_id\",\n+        \"//xla/runtime:device_id\",\n+        \"//xla/service:collective_ops_utils\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n cc_library(\n     name = \"collective_thunk\",\n     srcs = [\"collective_thunk.cc\"],\n     hdrs = [\"collective_thunk.h\"],\n     deps = [\n         \":collective_cliques\",\n+        \":collective_execution\",\n         \":collective_params\",\n         \":thunk\",\n         \":thunk_proto_cc\","
        },
        {
            "sha": "65da7c2a7761d002797285b34ee519fedf4bb9f2",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_execution.cc",
            "status": "added",
            "additions": 150,
            "deletions": 0,
            "changes": 150,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/82bf6501fd3c6e49be45b0066492835c56aefbd1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_execution.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/82bf6501fd3c6e49be45b0066492835c56aefbd1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_execution.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_execution.cc?ref=82bf6501fd3c6e49be45b0066492835c56aefbd1",
            "patch": "@@ -0,0 +1,150 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/collective_execution.h\"\n+\n+#include <cstdint>\n+#include <optional>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/backends/gpu/runtime/collective_cliques.h\"\n+#include \"xla/backends/gpu/runtime/collective_params.h\"\n+#include \"xla/core/collectives/communicator.h\"\n+#include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/debug_options_flags.h\"\n+#include \"xla/runtime/device_id.h\"\n+#include \"xla/service/collective_ops_utils.h\"\n+#include \"xla/status_macros.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n+\n+namespace xla::gpu {\n+\n+static int64_t GetNumLocalParticipants(\n+    const CollectiveParams& params,\n+    absl::Span<const GlobalDeviceId> participants) {\n+  if (!params.global_device_id_map) {\n+    return participants.size();\n+  }\n+\n+  std::vector<GlobalDeviceId> local_devices;\n+  local_devices.reserve(params.global_device_id_map->size());\n+  for (const auto& entry : *params.global_device_id_map) {\n+    local_devices.push_back(entry.second);\n+  }\n+\n+  return absl::c_count_if(participants, [&](const GlobalDeviceId& device_id) {\n+    return absl::c_linear_search(local_devices, device_id);\n+  });\n+}\n+\n+absl::StatusOr<GpuCliqueKey> GetGpuCliqueKey(\n+    const CollectiveParams& params,\n+    absl::Span<const ReplicaGroup> replica_groups,\n+    CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind,\n+    bool include_participant_groups) {\n+  TF_RET_CHECK(params.collectives) << \"Collectives API is not provided\";\n+\n+  GlobalDeviceId global_device_id = params.global_device_id;\n+\n+  if (params.device_assn == nullptr) {\n+    return InvalidArgument(\n+        \"Device assignment is null, but must be specified when running a \"\n+        \"collective thunk. If running multi-device HLO , make sure you're not \"\n+        \"using a tool designed for only one device like run_hlo_module.\");\n+  }\n+\n+  // Get the list of all devices that are participating in the collective\n+  // operation.\n+  TF_ASSIGN_OR_RETURN(\n+      std::vector<GlobalDeviceId> participants,\n+      GetParticipatingDevices(global_device_id, *params.device_assn,\n+                              replica_groups, group_mode));\n+\n+  // Get grouping of participating devices.\n+  std::vector<std::vector<GlobalDeviceId>> participant_groups;\n+  if (include_participant_groups) {\n+    // If splitting is enabled, participating groups must match in order for a\n+    // clique to be reused from the cache. We can ignore the participating\n+    // groups otherwise.\n+    static const bool enable_nccl_comm_splitting =\n+        xla::GetDebugOptionsFromFlags().xla_gpu_enable_nccl_comm_splitting();\n+    if (enable_nccl_comm_splitting) {\n+      TF_ASSIGN_OR_RETURN(participant_groups,\n+                          GetParticipatingDevicesGroups(\n+                              *params.device_assn, replica_groups, group_mode));\n+    }\n+\n+    if (params.collectives->IsGlobalConfig() &&\n+        (participants.size() != params.device_assn->replica_count())) {\n+      return InvalidArgument(\n+          \"Partial replica groups are not allowed when using NCCL_COMM_ID \"\n+          \"environment configuration.\");\n+    }\n+  }\n+\n+  // Remove trivial group that contains all participants, as we do not want to\n+  // create two sets of communicator handles for these cases.\n+  if (participant_groups.size() == 1 && participant_groups[0] == participants) {\n+    participant_groups.clear();\n+  }\n+\n+  int64_t num_local_participants =\n+      GetNumLocalParticipants(params, participants);\n+\n+  GlobalDeviceId root_device = GlobalDeviceId(-1);\n+\n+  absl::flat_hash_set<IncarnationId> unique_incarnations;\n+  if (params.incarnations) {\n+    for (GlobalDeviceId id : participants) {\n+      auto it = params.incarnations->find(id);\n+      if (it == params.incarnations->end()) {\n+        return FailedPrecondition(\"Incarnation for device %d not found\",\n+                                  id.value());\n+      }\n+      unique_incarnations.insert(it->second);\n+    }\n+  }\n+  std::vector<IncarnationId> incarnations(unique_incarnations.begin(),\n+                                          unique_incarnations.end());\n+  absl::c_sort(incarnations);\n+\n+  return GpuCliqueKey(std::move(participants), num_local_participants,\n+                      xla::gpu::IsP2PStreamKind(stream_kind),\n+                      std::move(participant_groups), root_device, incarnations);\n+}\n+\n+absl::StatusOr<CommunicatorHandle> GetComm(\n+    const CollectiveParams& params, const CollectiveCliques& collective_cliques,\n+    absl::Span<const ReplicaGroup> replica_groups,\n+    CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind) {\n+  TF_ASSIGN_OR_RETURN(\n+      GpuCliqueKey clique_key,\n+      GetGpuCliqueKey(params, replica_groups, group_mode, stream_kind));\n+\n+  std::optional<RankId> rank = clique_key.rank(params.global_device_id);\n+  TF_ASSIGN_OR_RETURN(Communicator * comm,\n+                      collective_cliques.GetComm(clique_key, *rank));\n+\n+  return CommunicatorHandle(comm, std::move(clique_key));\n+}\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "9d3eaf71a791b1dc398d5ed121bcad8ae766b24c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_execution.h",
            "status": "added",
            "additions": 62,
            "deletions": 0,
            "changes": 62,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/82bf6501fd3c6e49be45b0066492835c56aefbd1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_execution.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/82bf6501fd3c6e49be45b0066492835c56aefbd1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_execution.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_execution.h?ref=82bf6501fd3c6e49be45b0066492835c56aefbd1",
            "patch": "@@ -0,0 +1,62 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_EXECUTION_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_EXECUTION_H_\n+\n+#include <utility>\n+\n+#include \"absl/status/statusor.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/backends/gpu/runtime/collective_cliques.h\"\n+#include \"xla/backends/gpu/runtime/collective_params.h\"\n+#include \"xla/core/collectives/communicator.h\"\n+\n+namespace xla::gpu {\n+\n+// Handle to a communicator object with corresponding clique key.\n+struct CommunicatorHandle {\n+  CommunicatorHandle(Communicator* comm, GpuCliqueKey clique_key)\n+      : comm(comm), clique_key(std::move(clique_key)) {}\n+\n+  Communicator* comm;       // communicator object\n+  GpuCliqueKey clique_key;  // clique key\n+};\n+\n+// Returns a clique key for a collective operation executed for a given set of\n+// replica groups, group mode and stream kind, based on the `params` argument\n+// that identifies device that participates in the collective operation.\n+//\n+// The `include_participant_groups` argument controls whether the participant\n+// groups are included in the clique key. Including participant groups is needed\n+// for safe communicator splitting, as it defines a total order between all\n+// cliques in the XLA program.\n+absl::StatusOr<GpuCliqueKey> GetGpuCliqueKey(\n+    const CollectiveParams& params,\n+    absl::Span<const ReplicaGroup> replica_groups,\n+    CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind,\n+    bool include_participant_groups = true);\n+\n+// Returns a communicator handle from the set of acquired cliques acquired\n+// before the XLA:GPU execution.\n+absl::StatusOr<CommunicatorHandle> GetComm(\n+    const CollectiveParams& params, const CollectiveCliques& collective_cliques,\n+    absl::Span<const ReplicaGroup> replica_groups,\n+    CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind);\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_COLLECTIVE_EXECUTION_H_"
        },
        {
            "sha": "00ae441343504c82a166c54762cc14c05a819b6e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 108,
            "changes": 109,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/82bf6501fd3c6e49be45b0066492835c56aefbd1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/82bf6501fd3c6e49be45b0066492835c56aefbd1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc?ref=82bf6501fd3c6e49be45b0066492835c56aefbd1",
            "patch": "@@ -38,6 +38,7 @@ limitations under the License.\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n #include \"xla/backends/gpu/runtime/collective_cliques.h\"\n+#include \"xla/backends/gpu/runtime/collective_execution.h\"\n #include \"xla/backends/gpu/runtime/collective_params.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n@@ -100,24 +101,6 @@ bool IsTypeSupportedBy(PrimitiveType element_type, Thunk::Kind reduction_op) {\n   }\n }\n \n-int64_t GetNumLocalParticipants(\n-    const CollectiveParams& params,\n-    const std::vector<GlobalDeviceId>& participants) {\n-  if (!params.global_device_id_map) {\n-    return participants.size();\n-  }\n-\n-  std::vector<GlobalDeviceId> local_devices;\n-  local_devices.reserve(params.global_device_id_map->size());\n-  for (const auto& entry : *params.global_device_id_map) {\n-    local_devices.push_back(entry.second);\n-  }\n-\n-  return absl::c_count_if(participants, [&](const GlobalDeviceId& device_id) {\n-    return absl::c_linear_search(local_devices, device_id);\n-  });\n-}\n-\n }  // namespace\n \n // Returns if the collective communication operation is degenerate because all\n@@ -197,81 +180,6 @@ CollectiveThunk::CollectiveThunk(Kind kind, ThunkInfo thunk_info, bool is_sync,\n       stream_kind_(stream_kind),\n       async_events_(is_sync ? nullptr : std::make_shared<AsyncEvents>()) {}\n \n-absl::StatusOr<GpuCliqueKey> GetGpuCliqueKey(\n-    const CollectiveParams& params,\n-    absl::Span<const ReplicaGroup> replica_groups,\n-    CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind,\n-    bool include_participant_groups) {\n-  TF_RET_CHECK(params.collectives) << \"Collectives API is not provided\";\n-\n-  GlobalDeviceId global_device_id = params.global_device_id;\n-\n-  if (params.device_assn == nullptr) {\n-    return InvalidArgument(\n-        \"Device assignment is null, but must be specified when running a \"\n-        \"collective thunk. If running multi-device HLO , make sure you're not \"\n-        \"using a tool designed for only one device like run_hlo_module.\");\n-  }\n-\n-  // Get the list of all devices that are participating in the collective\n-  // operation.\n-  TF_ASSIGN_OR_RETURN(\n-      std::vector<GlobalDeviceId> participants,\n-      GetParticipatingDevices(global_device_id, *params.device_assn,\n-                              replica_groups, group_mode));\n-\n-  // Get grouping of participating devices.\n-  std::vector<std::vector<GlobalDeviceId>> participant_groups;\n-  if (include_participant_groups) {\n-    // If splitting is enabled, participating groups must match in order for a\n-    // clique to be reused from the cache. We can ignore the participating\n-    // groups otherwise.\n-    static const bool enable_nccl_comm_splitting =\n-        xla::GetDebugOptionsFromFlags().xla_gpu_enable_nccl_comm_splitting();\n-    if (enable_nccl_comm_splitting) {\n-      TF_ASSIGN_OR_RETURN(participant_groups,\n-                          GetParticipatingDevicesGroups(\n-                              *params.device_assn, replica_groups, group_mode));\n-    }\n-\n-    if (params.collectives->IsGlobalConfig() &&\n-        (participants.size() != params.device_assn->replica_count())) {\n-      return InvalidArgument(\n-          \"Partial replica groups are not allowed when using NCCL_COMM_ID \"\n-          \"environment configuration.\");\n-    }\n-  }\n-\n-  // Remove trivial group that contains all participants, as we do not want to\n-  // create two sets of communicator handles for these cases.\n-  if (participant_groups.size() == 1 && participant_groups[0] == participants) {\n-    participant_groups.clear();\n-  }\n-\n-  int64_t num_local_participants =\n-      GetNumLocalParticipants(params, participants);\n-\n-  absl::flat_hash_set<IncarnationId> unique_incarnations;\n-  if (params.incarnations) {\n-    for (GlobalDeviceId id : participants) {\n-      auto it = params.incarnations->find(id);\n-      if (it == params.incarnations->end()) {\n-        return FailedPrecondition(\"Incarnation for device %d not found\",\n-                                  id.value());\n-      }\n-      unique_incarnations.insert(it->second);\n-    }\n-  }\n-  std::vector<IncarnationId> incarnations(unique_incarnations.begin(),\n-                                          unique_incarnations.end());\n-  absl::c_sort(incarnations);\n-\n-  return GpuCliqueKey(std::move(participants), num_local_participants,\n-                      xla::gpu::IsP2PStreamKind(stream_kind),\n-                      std::move(participant_groups), GlobalDeviceId(-1),\n-                      incarnations);\n-}\n-\n absl::StatusOr<GpuCliqueKey> GetCollectiveGpuCliqueKey(\n     const CollectiveParams& params, const CollectiveConfig& collective_config,\n     bool include_participant_groups) {\n@@ -281,21 +189,6 @@ absl::StatusOr<GpuCliqueKey> GetCollectiveGpuCliqueKey(\n                          include_participant_groups);\n }\n \n-absl::StatusOr<CommunicatorHandle> GetComm(\n-    const CollectiveParams& params, const CollectiveCliques& collective_cliques,\n-    absl::Span<const ReplicaGroup> replica_groups,\n-    CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind) {\n-  TF_ASSIGN_OR_RETURN(\n-      GpuCliqueKey clique_key,\n-      GetGpuCliqueKey(params, replica_groups, group_mode, stream_kind));\n-\n-  std::optional<RankId> rank = clique_key.rank(params.global_device_id);\n-  TF_ASSIGN_OR_RETURN(Communicator * comm,\n-                      collective_cliques.GetComm(clique_key, *rank));\n-\n-  return CommunicatorHandle(comm, std::move(clique_key));\n-}\n-\n absl::StatusOr<std::vector<DeviceBufferPair>> ConvertToDeviceBuffers(\n     const Thunk::ExecuteParams& params,\n     const std::vector<CollectiveThunk::Buffer>& buffers,"
        },
        {
            "sha": "27cf48fd0a4900d4efb5c44a5fad8c9c30cb62f9",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.h",
            "status": "modified",
            "additions": 2,
            "deletions": 27,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/82bf6501fd3c6e49be45b0066492835c56aefbd1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/82bf6501fd3c6e49be45b0066492835c56aefbd1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h?ref=82bf6501fd3c6e49be45b0066492835c56aefbd1",
            "patch": "@@ -1,8 +1,3 @@\n-#include <cstddef>\n-\n-#include \"absl/types/span.h\"\n-#include \"xla/backends/gpu/runtime/collective_cliques.h\"\n-#include \"xla/backends/gpu/runtime/thunk.pb.h\"\n /* Copyright 2019 The OpenXLA Authors.\n \n Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -35,9 +30,10 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n-#include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n+#include \"xla/backends/gpu/runtime/collective_execution.h\"\n #include \"xla/backends/gpu/runtime/collective_params.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/hlo/ir/collective_op_group_mode.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -67,15 +63,6 @@ struct CollectiveConfig {\n CollectiveConfig GetCollectiveConfig(const HloInstruction* hlo,\n                                      std::optional<bool> use_global_device_ids);\n \n-// Handle to a communicator object with corresponding clique key.\n-struct CommunicatorHandle {\n-  CommunicatorHandle(Communicator* comm, GpuCliqueKey clique_key)\n-      : comm(comm), clique_key(std::move(clique_key)) {}\n-\n-  Communicator* comm;       // communicator object\n-  GpuCliqueKey clique_key;  // clique key\n-};\n-\n // Wrap GpuCliqueKey into a unique struct to guarantee we do not accidentally\n // try to run multiple unrelated rendezvous for a same key.\n struct FirstCallRendezvousKey {\n@@ -273,23 +260,11 @@ absl::Status AddOpDescription(absl::Status status, OpT op,\n \n //===----------------------------------------------------------------------===//\n \n-absl::StatusOr<GpuCliqueKey> GetGpuCliqueKey(\n-    const CollectiveParams& params,\n-    absl::Span<const ReplicaGroup> replica_groups,\n-    CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind,\n-    bool include_participant_groups = true);\n-\n // Helper over GetGpuCliqueKey that builds key for AsyncStreamKind::kCollective.\n absl::StatusOr<GpuCliqueKey> GetCollectiveGpuCliqueKey(\n     const CollectiveParams& params, const CollectiveConfig& collective_config,\n     bool include_participant_groups = true);\n \n-// Returns a communicator and additional information about the clique.\n-absl::StatusOr<CommunicatorHandle> GetComm(\n-    const CollectiveParams& params, const CollectiveCliques& collective_cliques,\n-    absl::Span<const ReplicaGroup> replica_groups,\n-    CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind);\n-\n struct DeviceBufferPair {\n   PrimitiveType element_type;\n   int64_t element_count;"
        },
        {
            "sha": "8f59db81eb132ff1a91cfc78cb58ccf02961d37f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/82bf6501fd3c6e49be45b0066492835c56aefbd1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/82bf6501fd3c6e49be45b0066492835c56aefbd1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc?ref=82bf6501fd3c6e49be45b0066492835c56aefbd1",
            "patch": "@@ -52,6 +52,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/all_to_all_thunk.h\"\n #include \"xla/backends/gpu/runtime/annotation.h\"\n #include \"xla/backends/gpu/runtime/collective_broadcast_thunk.h\"\n+#include \"xla/backends/gpu/runtime/collective_execution.h\"\n #include \"xla/backends/gpu/runtime/collective_permute_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\""
        }
    ],
    "stats": {
        "total": 376,
        "additions": 241,
        "deletions": 135
    }
}