{
    "author": "allanrenucci",
    "message": "[NFC] Move `EnableHeuristicCollectiveCombining` to `gpu_collective_combiner_utils`.\n\nPiperOrigin-RevId: 814852559",
    "sha": "7edebdba8b670c6689cd4a2a21ef8db9f26a0927",
    "files": [
        {
            "sha": "0e75419b2ddfef82614a47ad84b17d87b9ab082b",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/BUILD",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7edebdba8b670c6689cd4a2a21ef8db9f26a0927/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7edebdba8b670c6689cd4a2a21ef8db9f26a0927/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2FBUILD?ref=7edebdba8b670c6689cd4a2a21ef8db9f26a0927",
            "patch": "@@ -119,11 +119,9 @@ xla_cc_test(\n         \":collective_ops_utils\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/parser:hlo_parser\",\n-        \"//xla/service:hlo_module_config\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/strings:string_view\",\n@@ -139,7 +137,11 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:collective_ops_utils\",\n+        \"//xla/service:hlo_module_config\",\n         \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\",\n     ],\n@@ -159,7 +161,11 @@ xla_cc_test(\n         \"//xla/hlo/utils:hlo_query\",\n         \"//xla/service:collective_pipeliner\",\n         \"//xla/service:collective_pipeliner_utils\",\n+        \"//xla/service:hlo_module_config\",\n         \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\","
        },
        {
            "sha": "94c312ea15b1910ed7603cfebf906399788732c9",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7edebdba8b670c6689cd4a2a21ef8db9f26a0927/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7edebdba8b670c6689cd4a2a21ef8db9f26a0927/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc?ref=7edebdba8b670c6689cd4a2a21ef8db9f26a0927",
            "patch": "@@ -18,7 +18,6 @@ limitations under the License.\n #include <algorithm>\n #include <cstddef>\n #include <cstdint>\n-#include <optional>\n #include <variant>\n #include <vector>\n \n@@ -163,32 +162,5 @@ absl::StatusOr<GPUCommunicationType> CommunicationType(\n   return GPUCommunicationType::UNDEFINED;\n }\n \n-bool EnableHeuristicCollectiveCombining(\n-    const HloModuleConfig& config,\n-    const se::DeviceDescription& device_description,\n-    int64_t nvlink_slice_size) {\n-  if (!config.debug_options()\n-           .xla_gpu_experimental_enable_heuristic_collective_combining()) {\n-    return false;\n-  }\n-  se::CudaComputeCapability cc = device_description.cuda_compute_capability();\n-  // Heuristic collective combining is not turned on before Ampere GPUs.\n-  if (!cc.IsAtLeastAmpere()) {\n-    return false;\n-  }\n-  int hlo_device_count = config.num_partitions() * config.replica_count();\n-  if (hlo_device_count <= nvlink_slice_size) {\n-    VLOG(1) << \"Disabled heuristic collective combining for intra-NVLink \"\n-               \"domain communication: HLO device count \"\n-            << hlo_device_count << \" <= NVLink slice size \"\n-            << nvlink_slice_size;\n-    return false;\n-  }\n-  VLOG(1) << \"Enabled heuristic collective combining for inter-NVLink domain \"\n-             \"communication: HLO device count \"\n-          << hlo_device_count << \" > NVLink slice size \" << nvlink_slice_size;\n-  return true;\n-}\n-\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "65dd348b2461c8c8407a28cc01cf4798fea9b90a",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils.h",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7edebdba8b670c6689cd4a2a21ef8db9f26a0927/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7edebdba8b670c6689cd4a2a21ef8db9f26a0927/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h?ref=7edebdba8b670c6689cd4a2a21ef8db9f26a0927",
            "patch": "@@ -16,12 +16,9 @@ limitations under the License.\n #ifndef XLA_SERVICE_GPU_TRANSFORMS_COLLECTIVES_COLLECTIVE_OPS_UTILS_H_\n #define XLA_SERVICE_GPU_TRANSFORMS_COLLECTIVES_COLLECTIVE_OPS_UTILS_H_\n \n-#include <cstdint>\n-\n #include \"absl/status/statusor.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/service/hlo_module_config.h\"\n #include \"xla/stream_executor/device_description.h\"\n \n namespace xla {\n@@ -42,19 +39,6 @@ absl::StatusOr<GPUCommunicationType> CommunicationType(\n // Returns true if instruction is a synchronous collective op.\n bool IsGPUSyncCollective(const HloInstruction& instr);\n \n-enum class GPUTopologyType {\n-  UNKNOWN = 0,\n-  SINGLE_HOST = 1,\n-  MULTI_HOST = 2,\n-};\n-\n-// Returns true if heuristic collective combining is enabled.\n-// Heuristic collective combining enables more aggressive optimizations based\n-// on the platform and HLO's topology.\n-bool EnableHeuristicCollectiveCombining(\n-    const HloModuleConfig& config,\n-    const se::DeviceDescription& device_description, int64_t nvlink_slice_size);\n-\n }  // namespace gpu\n }  // namespace xla\n "
        },
        {
            "sha": "7ec6d51709da03de3d3fa1ec340a07055fb6c8f9",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 132,
            "changes": 132,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7edebdba8b670c6689cd4a2a21ef8db9f26a0927/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7edebdba8b670c6689cd4a2a21ef8db9f26a0927/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc?ref=7edebdba8b670c6689cd4a2a21ef8db9f26a0927",
            "patch": "@@ -15,8 +15,6 @@ limitations under the License.\n \n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n \n-#include <cstdint>\n-\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"absl/status/status_matchers.h\"\n@@ -25,10 +23,8 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n-#include \"xla/service/hlo_module_config.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla::gpu {\n@@ -37,134 +33,6 @@ namespace {\n using ::absl_testing::IsOkAndHolds;\n using ::testing::Test;\n \n-bool EnableHeuristicCollectiveCombining(\n-    se::CudaComputeCapability compute_capability, int num_partitions,\n-    int replica_count, int64_t nvlink_slice_size) {\n-  HloModuleConfig config;\n-  config.mutable_debug_options()\n-      .set_xla_gpu_experimental_enable_heuristic_collective_combining(true);\n-  config.set_num_partitions(num_partitions);\n-  config.set_replica_count(replica_count);\n-  se::DeviceDescription device_description =\n-      TestGpuDeviceInfo::RTXA6000DeviceInfo(compute_capability);\n-  return xla::gpu::EnableHeuristicCollectiveCombining(\n-      config, device_description, nvlink_slice_size);\n-}\n-\n-TEST(EnableHeuristicCollectiveCombiningTest, SingleHostSingleDevice) {\n-  // B200\n-  EXPECT_FALSE(\n-      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Blackwell(),\n-                                         /*num_partitions=*/1,\n-                                         /*replica_count=*/1,\n-                                         /*nvlink_slice_size=*/8));\n-  // H100\n-  EXPECT_FALSE(\n-      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Hopper(),\n-                                         /*num_partitions=*/1,\n-                                         /*replica_count=*/1,\n-                                         /*nvlink_slice_size=*/8));\n-  // A100\n-  EXPECT_FALSE(\n-      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Ampere(),\n-                                         /*num_partitions=*/1,\n-                                         /*replica_count=*/1,\n-                                         /*nvlink_slice_size=*/16));\n-}\n-\n-TEST(EnableHeuristicCollectiveCombiningTest, SingleHostMultiDevices) {\n-  // B200\n-  EXPECT_FALSE(\n-      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Blackwell(),\n-                                         /*num_partitions=*/8,\n-                                         /*replica_count=*/1,\n-                                         /*nvlink_slice_size=*/8));\n-  EXPECT_FALSE(\n-      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Blackwell(),\n-                                         /*num_partitions=*/1,\n-                                         /*replica_count=*/8,\n-                                         /*nvlink_slice_size=*/8));\n-  // H100\n-  EXPECT_FALSE(\n-      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Hopper(),\n-                                         /*num_partitions=*/8,\n-                                         /*replica_count=*/1,\n-                                         /*nvlink_slice_size=*/8));\n-  EXPECT_FALSE(\n-      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Hopper(),\n-                                         /*num_partitions=*/1,\n-                                         /*replica_count=*/8,\n-                                         /*nvlink_slice_size=*/8));\n-  // A100\n-  EXPECT_FALSE(\n-      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Ampere(),\n-                                         /*num_partitions=*/1,\n-                                         /*replica_count=*/16,\n-                                         /*nvlink_slice_size=*/16));\n-  EXPECT_FALSE(\n-      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Ampere(),\n-                                         /*num_partitions=*/16,\n-                                         /*replica_count=*/1,\n-                                         /*nvlink_slice_size=*/16));\n-}\n-\n-TEST(EnableHeuristicCollectiveCombiningTest, MultiHosts) {\n-  // B200\n-  EXPECT_TRUE(\n-      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Blackwell(),\n-                                         /*num_partitions=*/16,\n-                                         /*replica_count=*/1,\n-                                         /*nvlink_slice_size=*/8));\n-  EXPECT_TRUE(\n-      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Blackwell(),\n-                                         /*num_partitions=*/1,\n-                                         /*replica_count=*/16,\n-                                         /*nvlink_slice_size=*/8));\n-  // H100\n-  EXPECT_TRUE(\n-      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Hopper(),\n-                                         /*num_partitions=*/16,\n-                                         /*replica_count=*/1,\n-                                         /*nvlink_slice_size=*/8));\n-  EXPECT_TRUE(\n-      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Hopper(),\n-                                         /*num_partitions=*/1,\n-                                         /*replica_count=*/16,\n-                                         /*nvlink_slice_size=*/8));\n-  // A100\n-  EXPECT_TRUE(\n-      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Ampere(),\n-                                         /*num_partitions=*/1,\n-                                         /*replica_count=*/32,\n-                                         /*nvlink_slice_size=*/16));\n-  EXPECT_TRUE(\n-      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Ampere(),\n-                                         /*num_partitions=*/32,\n-                                         /*replica_count=*/1,\n-                                         /*nvlink_slice_size=*/16));\n-}\n-\n-TEST(EnableHeuristicCollectiveCombiningTest, UnsupportedGPU) {\n-  EXPECT_FALSE(\n-      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Volta(),\n-                                         /*num_partitions=*/1,\n-                                         /*replica_count=*/1,\n-                                         /*nvlink_slice_size=*/8));\n-}\n-\n-TEST(EnableHeuristicCollectiveCombiningTest, DisabledByFlag) {\n-  HloModuleConfig config;\n-  config.mutable_debug_options()\n-      .set_xla_gpu_experimental_enable_heuristic_collective_combining(false);\n-  config.set_num_partitions(16);\n-  config.set_replica_count(1);\n-  se::DeviceDescription device_description =\n-      TestGpuDeviceInfo::RTXA6000DeviceInfo(\n-          se::CudaComputeCapability::Blackwell());\n-  EXPECT_FALSE(xla::gpu::EnableHeuristicCollectiveCombining(\n-      config, device_description, /*nvlink_slice_size=*/8));\n-}\n-\n class CommunicationTypeTest : public Test {\n  protected:\n   se::DeviceDescription& device_info() { return device_info_; }"
        },
        {
            "sha": "1c99bd414fb5b5044340972c5b8f56114885f5fc",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/gpu_collective_combiner_utils.cc",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7edebdba8b670c6689cd4a2a21ef8db9f26a0927/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fgpu_collective_combiner_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7edebdba8b670c6689cd4a2a21ef8db9f26a0927/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fgpu_collective_combiner_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fgpu_collective_combiner_utils.cc?ref=7edebdba8b670c6689cd4a2a21ef8db9f26a0927",
            "patch": "@@ -15,13 +15,19 @@ limitations under the License.\n \n #include \"xla/service/gpu/transforms/collectives/gpu_collective_combiner_utils.h\"\n \n+#include <cstdint>\n+\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_schedule.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/hlo_module_config.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n \n namespace xla::gpu {\n@@ -57,4 +63,31 @@ bool ContainsPipelinedInstruction(const HloModule& module) {\n   return false;\n }\n \n+bool EnableHeuristicCollectiveCombining(\n+    const HloModuleConfig& config,\n+    const se::DeviceDescription& device_description,\n+    int64_t nvlink_slice_size) {\n+  if (!config.debug_options()\n+           .xla_gpu_experimental_enable_heuristic_collective_combining()) {\n+    return false;\n+  }\n+  se::CudaComputeCapability cc = device_description.cuda_compute_capability();\n+  // Heuristic collective combining is not turned on before Ampere GPUs.\n+  if (!cc.IsAtLeastAmpere()) {\n+    return false;\n+  }\n+  int hlo_device_count = config.num_partitions() * config.replica_count();\n+  if (hlo_device_count <= nvlink_slice_size) {\n+    VLOG(1) << \"Disabled heuristic collective combining for intra-NVLink \"\n+               \"domain communication: HLO device count \"\n+            << hlo_device_count << \" <= NVLink slice size \"\n+            << nvlink_slice_size;\n+    return false;\n+  }\n+  VLOG(1) << \"Enabled heuristic collective combining for inter-NVLink domain \"\n+             \"communication: HLO device count \"\n+          << hlo_device_count << \" > NVLink slice size \" << nvlink_slice_size;\n+  return true;\n+}\n+\n }  // namespace xla::gpu"
        },
        {
            "sha": "f8fa9b39d654eb9b81f0c3a2c4ba3b51d5ea0a07",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/gpu_collective_combiner_utils.h",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7edebdba8b670c6689cd4a2a21ef8db9f26a0927/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fgpu_collective_combiner_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7edebdba8b670c6689cd4a2a21ef8db9f26a0927/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fgpu_collective_combiner_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fgpu_collective_combiner_utils.h?ref=7edebdba8b670c6689cd4a2a21ef8db9f26a0927",
            "patch": "@@ -16,10 +16,14 @@ limitations under the License.\n #ifndef XLA_SERVICE_GPU_TRANSFORMS_COLLECTIVES_GPU_COLLECTIVE_COMBINER_UTILS_H_\n #define XLA_SERVICE_GPU_TRANSFORMS_COLLECTIVES_GPU_COLLECTIVE_COMBINER_UTILS_H_\n \n+#include <cstdint>\n+\n #include \"absl/status/status.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_schedule.h\"\n+#include \"xla/service/hlo_module_config.h\"\n+#include \"xla/stream_executor/device_description.h\"\n \n namespace xla::gpu {\n \n@@ -35,6 +39,13 @@ bool IsPipelinedCollective(const HloInstruction& instr);\n // Returns true if module contains any pipelined instruction. False otherwise.\n bool ContainsPipelinedInstruction(const HloModule& module);\n \n+// Returns true if heuristic collective combining is enabled.\n+// Heuristic collective combining enables more aggressive optimizations based\n+// on the platform and HLO's topology.\n+bool EnableHeuristicCollectiveCombining(\n+    const HloModuleConfig& config,\n+    const se::DeviceDescription& device_description, int64_t nvlink_slice_size);\n+\n }  // namespace xla::gpu\n \n #endif  // XLA_SERVICE_GPU_TRANSFORMS_COLLECTIVES_GPU_COLLECTIVE_COMBINER_UTILS_H_"
        },
        {
            "sha": "6478bac78a9f6d831a8301c67f0aa51fd58ab678",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/gpu_collective_combiner_utils_test.cc",
            "status": "modified",
            "additions": 132,
            "deletions": 0,
            "changes": 132,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7edebdba8b670c6689cd4a2a21ef8db9f26a0927/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fgpu_collective_combiner_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7edebdba8b670c6689cd4a2a21ef8db9f26a0927/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fgpu_collective_combiner_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fgpu_collective_combiner_utils_test.cc?ref=7edebdba8b670c6689cd4a2a21ef8db9f26a0927",
            "patch": "@@ -29,6 +29,10 @@ limitations under the License.\n #include \"xla/service/collective_pipeliner.h\"\n #include \"xla/service/collective_pipeliner_utils.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/hlo_module_config.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n \n@@ -452,5 +456,133 @@ TEST_F(CollectiveCombinerUtilsTest,\n   EXPECT_FALSE(ContainsPipelinedInstruction(*module));\n }\n \n+bool EnableHeuristicCollectiveCombining(\n+    se::CudaComputeCapability compute_capability, int num_partitions,\n+    int replica_count, int64_t nvlink_slice_size) {\n+  HloModuleConfig config;\n+  config.mutable_debug_options()\n+      .set_xla_gpu_experimental_enable_heuristic_collective_combining(true);\n+  config.set_num_partitions(num_partitions);\n+  config.set_replica_count(replica_count);\n+  se::DeviceDescription device_description =\n+      TestGpuDeviceInfo::RTXA6000DeviceInfo(compute_capability);\n+  return xla::gpu::EnableHeuristicCollectiveCombining(\n+      config, device_description, nvlink_slice_size);\n+}\n+\n+TEST(EnableHeuristicCollectiveCombiningTest, SingleHostSingleDevice) {\n+  // B200\n+  EXPECT_FALSE(\n+      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Blackwell(),\n+                                         /*num_partitions=*/1,\n+                                         /*replica_count=*/1,\n+                                         /*nvlink_slice_size=*/8));\n+  // H100\n+  EXPECT_FALSE(\n+      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Hopper(),\n+                                         /*num_partitions=*/1,\n+                                         /*replica_count=*/1,\n+                                         /*nvlink_slice_size=*/8));\n+  // A100\n+  EXPECT_FALSE(\n+      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Ampere(),\n+                                         /*num_partitions=*/1,\n+                                         /*replica_count=*/1,\n+                                         /*nvlink_slice_size=*/16));\n+}\n+\n+TEST(EnableHeuristicCollectiveCombiningTest, SingleHostMultiDevices) {\n+  // B200\n+  EXPECT_FALSE(\n+      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Blackwell(),\n+                                         /*num_partitions=*/8,\n+                                         /*replica_count=*/1,\n+                                         /*nvlink_slice_size=*/8));\n+  EXPECT_FALSE(\n+      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Blackwell(),\n+                                         /*num_partitions=*/1,\n+                                         /*replica_count=*/8,\n+                                         /*nvlink_slice_size=*/8));\n+  // H100\n+  EXPECT_FALSE(\n+      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Hopper(),\n+                                         /*num_partitions=*/8,\n+                                         /*replica_count=*/1,\n+                                         /*nvlink_slice_size=*/8));\n+  EXPECT_FALSE(\n+      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Hopper(),\n+                                         /*num_partitions=*/1,\n+                                         /*replica_count=*/8,\n+                                         /*nvlink_slice_size=*/8));\n+  // A100\n+  EXPECT_FALSE(\n+      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Ampere(),\n+                                         /*num_partitions=*/1,\n+                                         /*replica_count=*/16,\n+                                         /*nvlink_slice_size=*/16));\n+  EXPECT_FALSE(\n+      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Ampere(),\n+                                         /*num_partitions=*/16,\n+                                         /*replica_count=*/1,\n+                                         /*nvlink_slice_size=*/16));\n+}\n+\n+TEST(EnableHeuristicCollectiveCombiningTest, MultiHosts) {\n+  // B200\n+  EXPECT_TRUE(\n+      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Blackwell(),\n+                                         /*num_partitions=*/16,\n+                                         /*replica_count=*/1,\n+                                         /*nvlink_slice_size=*/8));\n+  EXPECT_TRUE(\n+      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Blackwell(),\n+                                         /*num_partitions=*/1,\n+                                         /*replica_count=*/16,\n+                                         /*nvlink_slice_size=*/8));\n+  // H100\n+  EXPECT_TRUE(\n+      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Hopper(),\n+                                         /*num_partitions=*/16,\n+                                         /*replica_count=*/1,\n+                                         /*nvlink_slice_size=*/8));\n+  EXPECT_TRUE(\n+      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Hopper(),\n+                                         /*num_partitions=*/1,\n+                                         /*replica_count=*/16,\n+                                         /*nvlink_slice_size=*/8));\n+  // A100\n+  EXPECT_TRUE(\n+      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Ampere(),\n+                                         /*num_partitions=*/1,\n+                                         /*replica_count=*/32,\n+                                         /*nvlink_slice_size=*/16));\n+  EXPECT_TRUE(\n+      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Ampere(),\n+                                         /*num_partitions=*/32,\n+                                         /*replica_count=*/1,\n+                                         /*nvlink_slice_size=*/16));\n+}\n+\n+TEST(EnableHeuristicCollectiveCombiningTest, UnsupportedGPU) {\n+  EXPECT_FALSE(\n+      EnableHeuristicCollectiveCombining(se::CudaComputeCapability::Volta(),\n+                                         /*num_partitions=*/1,\n+                                         /*replica_count=*/1,\n+                                         /*nvlink_slice_size=*/8));\n+}\n+\n+TEST(EnableHeuristicCollectiveCombiningTest, DisabledByFlag) {\n+  HloModuleConfig config;\n+  config.mutable_debug_options()\n+      .set_xla_gpu_experimental_enable_heuristic_collective_combining(false);\n+  config.set_num_partitions(16);\n+  config.set_replica_count(1);\n+  se::DeviceDescription device_description =\n+      TestGpuDeviceInfo::RTXA6000DeviceInfo(\n+          se::CudaComputeCapability::Blackwell());\n+  EXPECT_FALSE(xla::gpu::EnableHeuristicCollectiveCombining(\n+      config, device_description, /*nvlink_slice_size=*/8));\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 362,
        "additions": 184,
        "deletions": 178
    }
}