{
    "author": "felixwqp",
    "message": "Add support for custom call latency annotations in GPU schedulers.\n\nTo debug the cost model, we want to allows custom call hlo instruction to specify their execution latency in nanoseconds using a `latency_metadata` frontend attribute.\n\nChanges are added to two latency estimators used by GPUs `GpuLatencyEstimator` and `sol_latency_estimator`\n\nA new test is added to verify that different latency annotations result in different scheduling decisions by the latency hiding scheduler.\n\nPiperOrigin-RevId: 840572100",
    "sha": "86ad67459072ebe72c6d43974f9aef90822024c8",
    "files": [
        {
            "sha": "45ee3a096c9708249c5610dd021c06f90b689a7b",
            "filename": "third_party/xla/xla/service/collective_ops_utils.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/86ad67459072ebe72c6d43974f9aef90822024c8/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/86ad67459072ebe72c6d43974f9aef90822024c8/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.cc?ref=86ad67459072ebe72c6d43974f9aef90822024c8",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/numbers.h\"\n #include \"absl/strings/str_join.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n@@ -906,6 +907,22 @@ bool HasDuplicateSourcesOrTargets(const SourceTargetPairs& pairs) {\n   return false;\n }\n \n+std::optional<double> GetCustomCallLatencyMetadata(\n+    const HloInstruction* instr) {\n+  if (instr->opcode() == HloOpcode::kCustomCall &&\n+      instr->has_frontend_attributes()) {\n+    auto it = instr->frontend_attributes().map().find(\"latency_metadata\");\n+    if (it != instr->frontend_attributes().map().end()) {\n+      int64_t latency_metadata_ns = 0;\n+      CHECK(absl::SimpleAtoi(it->second, &latency_metadata_ns))\n+          << \"Failed to parse latency from custom call for \" << instr->name()\n+          << \" from latency_metadata:\" << it->second;\n+      return static_cast<double>(latency_metadata_ns) / 1000.0;\n+    }\n+  }\n+  return std::nullopt;\n+}\n+\n int64_t GetSubgroupSize(const HloCollectiveInstruction* hlo,\n                         CollectiveOpGroupMode group_mode) {\n   const HloModuleConfig& config = hlo->GetModule()->config();"
        },
        {
            "sha": "4c86d1fab8b06c288e5b8c3491d25784d778c037",
            "filename": "third_party/xla/xla/service/collective_ops_utils.h",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/86ad67459072ebe72c6d43974f9aef90822024c8/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/86ad67459072ebe72c6d43974f9aef90822024c8/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.h?ref=86ad67459072ebe72c6d43974f9aef90822024c8",
            "patch": "@@ -324,6 +324,11 @@ inline constexpr absl::string_view kCollectiveStreamAttrName =\n     \"_xla_gpu_collective_stream\";\n inline constexpr absl::string_view kCollectiveStreamP2P = \"p2p\";\n \n+// Returns latency metadata in microseconds(us) if the instruction is a custom\n+// call with latency metadata. Returns `std::nullopt` if the instruction is not\n+// a custom call with latency metadata or invalid latency metadata is provided.\n+std::optional<double> GetCustomCallLatencyMetadata(const HloInstruction* instr);\n+\n int64_t GetSubgroupSize(const HloCollectiveInstruction* hlo,\n                         CollectiveOpGroupMode group_mode);\n "
        },
        {
            "sha": "d7e9c91ee9c438e6a44dc351e6e158389b305572",
            "filename": "third_party/xla/xla/service/collective_ops_utils_test.cc",
            "status": "modified",
            "additions": 34,
            "deletions": 0,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/86ad67459072ebe72c6d43974f9aef90822024c8/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/86ad67459072ebe72c6d43974f9aef90822024c8/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils_test.cc?ref=86ad67459072ebe72c6d43974f9aef90822024c8",
            "patch": "@@ -533,6 +533,40 @@ TEST(HasDuplicateSourcesOrTargetsTest, DuplicateTargets) {\n   EXPECT_TRUE(HasDuplicateSourcesOrTargets(pairs));\n }\n \n+TEST(CollectiveOpsUtilsTest, GetCustomCallLatencyMetadata) {\n+  HloComputation::Builder builder(\"GetCustomCallLatencyMetadata\");\n+  HloInstruction* param =\n+      builder.AddInstruction(HloInstruction::CreateParameter(\n+          0, ShapeUtil::MakeShape(F32, {}), \"param\"));\n+  HloInstruction* custom_call =\n+      builder.AddInstruction(HloInstruction::CreateCustomCall(\n+          ShapeUtil::MakeShape(F32, {}), {param}, \"SomeCustomCall\"));\n+  EXPECT_FALSE(GetCustomCallLatencyMetadata(custom_call).has_value());\n+\n+  FrontendAttributes attributes;\n+  (*attributes.mutable_map())[\"latency_metadata\"] = \"12345\";\n+  custom_call->set_frontend_attributes(attributes);\n+  std::optional<double> latency = GetCustomCallLatencyMetadata(custom_call);\n+  ASSERT_TRUE(latency.has_value());\n+  EXPECT_EQ(*latency, 12.345);\n+}\n+\n+TEST(CollectiveOpsUtilsDeathTest, GetCustomCallLatencyMetadataInvalid) {\n+  HloComputation::Builder builder(\"GetCustomCallLatencyMetadataInvalid\");\n+  HloInstruction* param =\n+      builder.AddInstruction(HloInstruction::CreateParameter(\n+          0, ShapeUtil::MakeShape(F32, {}), \"param\"));\n+  HloInstruction* custom_call =\n+      builder.AddInstruction(HloInstruction::CreateCustomCall(\n+          ShapeUtil::MakeShape(F32, {}), {param}, \"SomeCustomCall\"));\n+  FrontendAttributes attributes;\n+  (*attributes.mutable_map())[\"latency_metadata\"] = \"invalid\";\n+  custom_call->set_frontend_attributes(attributes);\n+  EXPECT_DEATH(\n+      { GetCustomCallLatencyMetadata(custom_call); },\n+      \"Failed to parse latency from custom call\");\n+}\n+\n }  // namespace\n \n // Tests for GetCollectOpGroupMode"
        },
        {
            "sha": "92ad8a523c6e900e4b38c6802f4017582e79aa13",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/86ad67459072ebe72c6d43974f9aef90822024c8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/86ad67459072ebe72c6d43974f9aef90822024c8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=86ad67459072ebe72c6d43974f9aef90822024c8",
            "patch": "@@ -2569,6 +2569,7 @@ xla_test(\n         \"@com_google_absl//absl/log:scoped_mock_log\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest\",\n         \"@com_google_protobuf//:protobuf\",\n@@ -3278,6 +3279,7 @@ cc_library(\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n     ],\n )"
        },
        {
            "sha": "c4669b077b1f42cf36b051ecccc45df04a86bd6c",
            "filename": "third_party/xla/xla/service/gpu/gpu_hlo_schedule_test.cc",
            "status": "modified",
            "additions": 89,
            "deletions": 0,
            "changes": 89,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/86ad67459072ebe72c6d43974f9aef90822024c8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/86ad67459072ebe72c6d43974f9aef90822024c8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule_test.cc?ref=86ad67459072ebe72c6d43974f9aef90822024c8",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include <optional>\n #include <string>\n #include <tuple>\n+#include <utility>\n #include <vector>\n \n #include <gmock/gmock.h>\n@@ -33,6 +34,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/status_matchers.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/strings/substitute.h\"\n #include \"google/protobuf/text_format.h\"\n #include \"xla/hlo/analysis/hlo_ordering.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n@@ -380,9 +382,11 @@ TEST_P(GpuHloScheduleParameterizedTest, LHSCostModel) {\n   for (const HloInstruction* inst :\n        order.SequentialOrder(*entry)->instructions()) {\n     if (inst->opcode() == HloOpcode::kAllReduceStart) {\n+      ASSERT_FALSE(in_between);\n       in_between = true;\n       count_between_pairs.push_back(0);\n     } else if (inst->opcode() == HloOpcode::kAllReduceDone) {\n+      ASSERT_TRUE(in_between);\n       in_between = false;\n     } else if (in_between && inst->opcode() == HloOpcode::kCustomCall) {\n       count_between_pairs.back()++;\n@@ -394,6 +398,89 @@ TEST_P(GpuHloScheduleParameterizedTest, LHSCostModel) {\n   EXPECT_TRUE(HasValidFingerprint(module.get()));\n }\n \n+TEST_P(GpuHloScheduleParameterizedTest,\n+       LHSCostModelWithCustomCallLatencyAnnotationDiffScheduling) {\n+  const char* hlo_text = R\"(\n+  HloModule AsyncAR\n+  apply_op {\n+    x = f32[] parameter(0)\n+    y = f32[] parameter(1)\n+    ROOT apply_op = f32[] add(x, y)\n+  }\n+\n+  ENTRY ar {\n+    p0 = f32[16] parameter(0)\n+    p1 = f32[16, 16] parameter(1)\n+    p2 = f32[16, 16] parameter(2)\n+    p3 = f32[16] parameter(3)\n+\n+    dot0 = f32[16,16]{1,0} custom-call(p1, p2), custom_call_target=\"__cublas$$gemm\", frontend_attributes={latency_metadata=\"$0\"}\n+    dot1 = f32[16,16]{1,0} custom-call(dot0, p2), custom_call_target=\"__cublas$$gemm\", frontend_attributes={latency_metadata=\"$0\"}\n+    dot2 = f32[16,16]{1,0} custom-call(dot1, p2), custom_call_target=\"__cublas$$gemm\", frontend_attributes={latency_metadata=\"$0\"}\n+    dot3 = f32[16,16]{1,0} custom-call(dot2, p2), custom_call_target=\"__cublas$$gemm\", frontend_attributes={latency_metadata=\"$0\"}\n+    dot4 = f32[16,16]{1,0} custom-call(dot3, p2), custom_call_target=\"__cublas$$gemm\", frontend_attributes={latency_metadata=\"$0\"}\n+    dot5 = f32[16,16]{1,0} custom-call(dot4, p2), custom_call_target=\"__cublas$$gemm\", frontend_attributes={latency_metadata=\"$0\"}\n+    dot6 = f32[16,16]{1,0} custom-call(dot5, p2), custom_call_target=\"__cublas$$gemm\", frontend_attributes={latency_metadata=\"$0\"}\n+\n+    ar-start = f32[16] all-reduce-start(p0), to_apply=apply_op\n+    ar-done = f32[16] all-reduce-done(ar-start)\n+\n+    ar-start1 = f32[16] all-reduce-start(p3), to_apply=apply_op\n+    ar-done1 = f32[16] all-reduce-done(ar-start1)\n+\n+    add0 = f32[16,16] add(dot0, dot1)\n+    add1 = f32[16,16] add(add0, dot2)\n+    add2 = f32[16,16] add(add1, dot3)\n+    add3 = f32[16,16] add(add2, dot4)\n+    add4 = f32[16,16] add(add3, dot5)\n+    add5 = f32[16,16] add(add4, dot6)\n+\n+    ROOT t = (f32[16], f32[16], f32[16,16]) tuple(ar-done, ar-done1, add5)\n+  })\";\n+\n+  auto get_count_between_pairs =\n+      [&](const std::string& hlo_latency) -> std::vector<int64_t> {\n+    TestConfig test_config;\n+    test_config.enable_latency_hiding_scheduler = true;\n+    test_config.enable_sol_latency_estimator = std::get<1>(GetParam());\n+    auto module_or = ParseAndReturnVerifiedModule(\n+        absl::Substitute(hlo_text, hlo_latency), GetModuleConfig(test_config));\n+    CHECK_OK(module_or.status());\n+    auto module = std::move(module_or.value());\n+    SequentialHloOrdering order = BuildHloOrdering(module.get());\n+\n+    HloComputation* entry = module->entry_computation();\n+    std::vector<int64_t> count_between_pairs;\n+    bool in_between = false;\n+    for (const HloInstruction* inst :\n+         order.SequentialOrder(*entry)->instructions()) {\n+      if (inst->opcode() == HloOpcode::kAllReduceStart) {\n+        CHECK(!in_between);\n+        in_between = true;\n+        count_between_pairs.push_back(0);\n+      } else if (inst->opcode() == HloOpcode::kAllReduceDone) {\n+        CHECK(in_between);\n+        in_between = false;\n+      } else if (in_between && inst->opcode() == HloOpcode::kCustomCall) {\n+        count_between_pairs.back()++;\n+      }\n+    }\n+    return count_between_pairs;\n+  };\n+\n+  std::vector<int64_t> count_between_pairs_high_latency =\n+      get_count_between_pairs(\"100000000000\");\n+  std::vector<int64_t> count_between_pairs_low_latency =\n+      get_count_between_pairs(\"1\");\n+\n+  EXPECT_EQ(count_between_pairs_high_latency.size(), 2);\n+  EXPECT_EQ(count_between_pairs_low_latency.size(), 2);\n+  EXPECT_NE(count_between_pairs_high_latency[0],\n+            count_between_pairs_low_latency[0]);\n+  EXPECT_NE(count_between_pairs_high_latency[1],\n+            count_between_pairs_low_latency[1]);\n+}\n+\n TEST_P(GpuHloScheduleParameterizedTest,\n        ScheduleGpuModuleReturnsPeakMemoryBytes) {\n   absl::string_view kHloText = R\"(\n@@ -459,9 +546,11 @@ TEST_P(GpuHloScheduleParameterizedTest, LHSCostModelCostlyAR) {\n   for (const HloInstruction* inst :\n        order.SequentialOrder(*entry)->instructions()) {\n     if (inst->opcode() == HloOpcode::kAllReduceStart) {\n+      ASSERT_FALSE(in_between);\n       in_between = true;\n       count_between_pairs.push_back(0);\n     } else if (inst->opcode() == HloOpcode::kAllReduceDone) {\n+      ASSERT_TRUE(in_between);\n       in_between = false;\n     } else if (in_between && inst->opcode() == HloOpcode::kCustomCall) {\n       count_between_pairs.back()++;"
        },
        {
            "sha": "c41dd1de0a6b2114af4131e654b914b6a1a6042d",
            "filename": "third_party/xla/xla/service/gpu/gpu_latency_hiding_scheduler.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 1,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/86ad67459072ebe72c6d43974f9aef90822024c8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/86ad67459072ebe72c6d43974f9aef90822024c8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_latency_hiding_scheduler.cc?ref=86ad67459072ebe72c6d43974f9aef90822024c8",
            "patch": "@@ -18,19 +18,20 @@ limitations under the License.\n #include <algorithm>\n #include <cstddef>\n #include <cstdint>\n+#include <optional>\n #include <tuple>\n #include <utility>\n #include <vector>\n \n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n+#include \"absl/strings/numbers.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/utils/hlo_query.h\"\n #include \"xla/service/collective_ops_utils.h\"\n-#include \"xla/service/collective_permute_decomposer.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n@@ -604,6 +605,20 @@ ApproximateLatencyEstimator::TimeCost GpuLatencyEstimator::NodeCost(\n   // custom call is 1000, the LHS will try to schedule approximately 5 of\n   // these in between each start/end pair.\n   if (instr->opcode() == HloOpcode::kCustomCall) {\n+    if (instr->has_frontend_attributes() &&\n+        instr->frontend_attributes().map().contains(\"latency_metadata\")) {\n+      int64_t latency_metadata_ns = 0;\n+      CHECK(absl::SimpleAtoi(\n+          instr->frontend_attributes().map().at(\"latency_metadata\"),\n+          &latency_metadata_ns))\n+          << \"Failed to parse latency from custom call for \" << instr->name()\n+          << \" from latency_metadata:\"\n+          << instr->frontend_attributes().map().at(\"latency_metadata\");\n+      VLOG(10) << \"NodeCost: Returning latency from custom call for \"\n+               << instr->name() << \": \" << latency_metadata_ns / 1000.0\n+               << \" ns\";\n+      return (LatencyEstimator::TimeCost)latency_metadata_ns / 1000.0;\n+    }\n     if (IsCublasGemm(*instr) || IsCustomCallToDnnConvolution(*instr)) {\n       return ApproximateLatencyEstimator::kMediumCost;\n     }"
        },
        {
            "sha": "72bb3f5033896e8ac60450122ff3b7d3bf88853f",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/86ad67459072ebe72c6d43974f9aef90822024c8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/86ad67459072ebe72c6d43974f9aef90822024c8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc?ref=86ad67459072ebe72c6d43974f9aef90822024c8",
            "patch": "@@ -427,6 +427,11 @@ LatencyEstimator::TimeCost SolLatencyEstimator::GetLatencyBetween(\n \n LatencyEstimator::TimeCost SolLatencyEstimator::NodeCost(\n     const HloInstruction* instr) const {\n+  if (std::optional<double> latency = GetCustomCallLatencyMetadata(instr)) {\n+    VLOG(10) << \"NodeCost: Returning latency from custom call for \"\n+             << instr->name() << \": \" << *latency << \" us\";\n+    return *latency;\n+  }\n   if (hlo_query::IsAsyncCollectiveStartOp(instr, /*include_send_recv=*/true) ||\n       hlo_query::IsAsyncCollectiveDoneOp(instr, /*include_send_recv=*/true)) {\n     VLOG(10) << \"NodeCost: Returning kLowCost for async start/done op \""
        },
        {
            "sha": "4ecd0253fee7ab22df271c662498b55fccadcc4a",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator_test.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/86ad67459072ebe72c6d43974f9aef90822024c8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/86ad67459072ebe72c6d43974f9aef90822024c8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc?ref=86ad67459072ebe72c6d43974f9aef90822024c8",
            "patch": "@@ -504,6 +504,22 @@ ENTRY e {\n       /*expected_latency=*/absl::Microseconds(8),\n   };\n \n+  EstimatorTestCase pallas_custom_call = {\n+      /*test_name=*/\"pallas_custom_call\",\n+      /*module_string=*/R\"(\n+HloModule m\n+\n+ENTRY e {\n+  p0 = bf16[128,128] parameter(0)\n+  ROOT _ =  bf16[128,128] custom-call(p0),\n+    custom_call_target=\"mosaic_gpu_v2\",\n+    frontend_attributes={latency_metadata=\"30000\"}\n+})\",\n+      /*opcode_to_find=*/HloOpcode::kCustomCall,\n+      /*cost_type=*/CostType::kNodeCost,\n+      /*expected_latency=*/absl::Microseconds(30),\n+  };\n+\n   EstimatorTestCase noop = {\n       /*test_name=*/\"noop\",\n       /*module_string=*/R\"(\n@@ -527,6 +543,7 @@ ENTRY e {\n           triton_matmul_bf16_batch1_1024_1024_1024,\n           cublas_matmul_bf16_batch1_1024_1024_1024,\n           simple_fusion_elementwise,\n+          pallas_custom_call,\n           noop};\n }\n "
        }
    ],
    "stats": {
        "total": 186,
        "additions": 185,
        "deletions": 1
    }
}