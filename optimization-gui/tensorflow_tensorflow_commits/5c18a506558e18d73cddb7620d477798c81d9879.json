{
    "author": "ermilovmaxim",
    "message": "Use nvml impl lib based wrapper\n\nPiperOrigin-RevId: 820376041",
    "sha": "5c18a506558e18d73cddb7620d477798c81d9879",
    "files": [
        {
            "sha": "065ea208b924582b4819096ad12c912a24abfd42",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 10,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=5c18a506558e18d73cddb7620d477798c81d9879",
            "patch": "@@ -133,6 +133,7 @@ limitations under the License.\n #if GOOGLE_CUDA\n #include \"third_party/gpus/cuda/include/cuda.h\"\n #include \"third_party/gpus/cuda/include/cuda_runtime_api.h\"\n+#include \"third_party/gpus/cuda/nvml/include/nvml.h\"\n #include \"xla/service/gpu/model/gpu_collective_performance_model.h\"\n #include \"xla/stream_executor/gpu/gpu_cudamallocasync_allocator.h\"\n #elif TENSORFLOW_USE_ROCM\n@@ -1705,26 +1706,29 @@ absl::StatusOr<std::string> GetDeviceFabricInfo(const int device_ordinal) {\n     return absl::InternalError(\"Failed to initialize NVML library.\");\n   }\n \n+  char pciBusId[] = \"00000000:00:00.0\";\n+  cudaDeviceGetPCIBusId(pciBusId, sizeof(pciBusId), device_ordinal);\n+  nvmlDevice_t device;\n+\n+  nvmlReturn_t get_bus_id_status =\n+      nvmlDeviceGetHandleByPciBusId_v2(pciBusId, &device);\n   // NVML library is not a part of the CUDA toolkit, so there might be a\n   // situation when user is using CUDA 12.4 an higher, but the host NVML\n   // version doen't have the required functions.\n-  if (xla_nvmlDeviceGetHandleByPciBusId_v2 == nullptr ||\n-      xla_nvmlDeviceGetGpuFabricInfoV == nullptr) {\n+  if (get_bus_id_status == NVML_ERROR_FUNCTION_NOT_FOUND) {\n     return absl::InternalError(\"NVML library doesn't have required functions.\");\n   }\n-\n-  char pciBusId[] = \"00000000:00:00.0\";\n-  cudaDeviceGetPCIBusId(pciBusId, sizeof(pciBusId), device_ordinal);\n-  nvmlDevice_t device;\n-  auto get_bus_id_status =\n-      xla_nvmlDeviceGetHandleByPciBusId_v2(pciBusId, &device);\n   CHECK_EQ(get_bus_id_status, NVML_SUCCESS);\n \n   nvmlGpuFabricInfoV_t fabricInfo = {\n       .version = nvmlGpuFabricInfo_v2,\n       .state = NVML_GPU_FABRIC_STATE_NOT_SUPPORTED};\n-  auto get_fabric_info_status =\n-      xla_nvmlDeviceGetGpuFabricInfoV(device, &fabricInfo);\n+\n+  nvmlReturn_t get_fabric_info_status =\n+      nvmlDeviceGetGpuFabricInfoV(device, &fabricInfo);\n+  if (get_fabric_info_status == NVML_ERROR_FUNCTION_NOT_FOUND) {\n+    return absl::InternalError(\"NVML library doesn't have required functions.\");\n+  }\n   CHECK_EQ(get_fabric_info_status, NVML_SUCCESS);\n \n   if (fabricInfo.state == NVML_GPU_FABRIC_STATE_NOT_SUPPORTED) {"
        },
        {
            "sha": "aafce806e66866318b85dc335af8f9cb7601803f",
            "filename": "third_party/xla/xla/service/gpu/model/BUILD",
            "status": "modified",
            "additions": 15,
            "deletions": 9,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD?ref=5c18a506558e18d73cddb7620d477798c81d9879",
            "patch": "@@ -5,7 +5,7 @@ load(\"//xla:xla.default.bzl\", \"xla_cc_test\")\n \n # Libraries for performance modeling of HLO.\n load(\"//xla/tests:build_defs.bzl\", \"xla_test\")\n-load(\"//xla/tsl:tsl.bzl\", \"if_google\", \"if_oss\", \"internal_visibility\")\n+load(\"//xla/tsl:tsl.bzl\", \"if_google\", \"internal_visibility\")\n load(\"//xla/tsl:tsl.default.bzl\", \"get_compatible_with_portable\")\n load(\"//xla/tsl/platform:build_config.bzl\", \"tf_proto_library\")\n load(\"//xla/tsl/platform/default:cuda_build_defs.bzl\", \"if_cuda_is_configured\")\n@@ -158,11 +158,6 @@ xla_test(\n         \"b200\",\n         \"amdgpu_any\",\n     ],\n-    tags =\n-        if_oss(\n-            # TODO(b/435404154): Reenable once this is fixed.\n-            [\"no_oss\"],\n-        ),\n     deps = [\n         \":analytical_latency_estimator\",\n         \"//xla:shape_util\",\n@@ -499,18 +494,29 @@ cc_library(\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:IR\",\n         \"@local_tsl//tsl/platform:status\",\n-    ] + if_cuda_is_configured([\"@local_config_cuda//cuda:cuda_headers\"]),\n+    ] + if_cuda_is_configured([\n+        \"//xla/tsl/cuda:nvml\",\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+    ]),\n )\n \n-xla_cc_test(\n+xla_test(\n     name = \"gpu_collective_performance_model_test\",\n     srcs = [\"gpu_collective_performance_model_test.cc\"],\n+    backends = [\"nvgpu_any\"],\n+    tags = [\n+        \"cuda-only\",\n+        \"gpu\",\n+    ],\n     deps = [\n+        \":gpu_collective_performance_model\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"@com_google_googletest//:gtest\",\n-    ],\n+    ] + if_cuda_is_configured([\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+    ]),\n )\n \n cc_library("
        },
        {
            "sha": "aa43729fe200958ce9538adf306229ac7708eb60",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_collective_performance_model.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 39,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.cc?ref=5c18a506558e18d73cddb7620d477798c81d9879",
            "patch": "@@ -382,42 +382,10 @@ RocmBandwidthSettings CreateSettings(\n \n /*static*/ bool GpuPerformanceWithCollectiveModel::InitNvml() {\n #if GOOGLE_CUDA && (defined(PLATFORM_POSIX) || defined(PLATFORM_GOOGLE))\n-  void* libhandle = dlopen(\"libnvidia-ml.so.1\", RTLD_NOW);\n-  CHECK(libhandle != nullptr) << \"Failed to open libnvidia-ml.so.1\";\n-\n-  struct SymbolEntry {\n-    void** functor;\n-    char const* name;\n-  };\n-\n-  std::vector<SymbolEntry> symbols = {\n-      {(void**)&xla_nvmlInit, \"nvmlInit_v2\"},\n-      {(void**)&xla_nvmlShutdown, \"nvmlShutdown\"},\n-      {(void**)&xla_nvmlDeviceGetHandleByIndex, \"nvmlDeviceGetHandleByIndex\"},\n-      {(void**)&xla_nvmlDeviceGetNvLinkCapability,\n-       \"nvmlDeviceGetNvLinkCapability\"},\n-      {(void**)&xla_nvmlSystemGetNVMLVersion, \"nvmlSystemGetNVMLVersion\"},\n-  };\n-\n-#if GOOGLE_CUDA && CUDA_VERSION >= 12040 && !defined(PLATFORM_GOOGLE)\n-  // Some hosts might still have older driver version(b/414617899).\n-  symbols.push_back({(void**)&xla_nvmlDeviceGetHandleByPciBusId_v2,\n-                     \"nvmlDeviceGetHandleByPciBusId_v2\"});\n-  symbols.push_back({(void**)&xla_nvmlDeviceGetGpuFabricInfoV,\n-                     \"nvmlDeviceGetGpuFabricInfoV\"});\n-#endif  // CUDA_VERSION >= 12040\n-  for (SymbolEntry se : symbols) {\n-    *se.functor = dlsym(libhandle, se.name);\n-    if (*se.functor == nullptr) {\n-      const char* dlsym_error = dlerror();\n-      if (dlsym_error) {\n-        VLOG(0) << \"Failed to load symbol \" << se.name << \": \" << dlsym_error;\n-        VLOG(0) << \"This is likely caused by insufficient CUDA driver version. \"\n-                   \"Please upgrade CUDA driver to 550 or higher.\";\n-      }\n-    }\n+  nvmlReturn_t init_result = nvmlInit();\n+  if (init_result != NVML_SUCCESS) {\n+    LOG(ERROR) << \"NVML init failed with \" << init_result;\n   }\n-  nvmlReturn_t init_result = xla_nvmlInit();\n   return init_result == NVML_SUCCESS;\n #elif TENSORFLOW_USE_ROCM\n   return true;\n@@ -428,7 +396,7 @@ RocmBandwidthSettings CreateSettings(\n \n /*static*/ bool GpuPerformanceWithCollectiveModel::ShutdownNvml() {\n #if GOOGLE_CUDA\n-  nvmlReturn_t shutdown_result = xla_nvmlShutdown();\n+  nvmlReturn_t shutdown_result = nvmlShutdown();\n   return shutdown_result == NVML_SUCCESS;\n #elif TENSORFLOW_USE_ROCM\n   return true;\n@@ -449,13 +417,12 @@ GpuPerformanceWithCollectiveModel::CheckIfNvlinkSupportsP2P() {\n   // to have the same capability.\n   CHECK(InitNvml()) << \"NVML init failed.\";\n   nvmlDevice_t nvml_device;\n-  nvmlReturn_t get_device_result =\n-      xla_nvmlDeviceGetHandleByIndex(0, &nvml_device);\n+  nvmlReturn_t get_device_result = nvmlDeviceGetHandleByIndex(0, &nvml_device);\n   CHECK(get_device_result == NVML_SUCCESS);\n \n   uint32_t supported_p2p = 0;\n \n-  nvmlReturn_t nvlink_cap_result = xla_nvmlDeviceGetNvLinkCapability(\n+  nvmlReturn_t nvlink_cap_result = nvmlDeviceGetNvLinkCapability(\n       nvml_device, /*nvlink link number*/ 0, NVML_NVLINK_CAP_P2P_SUPPORTED,\n       &supported_p2p);\n   CHECK(nvlink_cap_result == NVML_SUCCESS ||"
        },
        {
            "sha": "be93dfadd21eadc9fc52008f3327e515e57b5e93",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_collective_performance_model.h",
            "status": "modified",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.h?ref=5c18a506558e18d73cddb7620d477798c81d9879",
            "patch": "@@ -26,37 +26,6 @@ limitations under the License.\n #include \"xla/service/gpu/model/gpu_performance_model_base.h\"\n #include \"xla/stream_executor/device_description.h\"\n \n-#if GOOGLE_CUDA\n-#include \"third_party/gpus/cuda/include/cuda.h\"\n-#if defined(PLATFORM_POSIX) || defined(PLATFORM_GOOGLE)\n-#include <dlfcn.h>\n-#endif\n-\n-#include \"third_party/gpus/cuda/nvml/include/nvml.h\"\n-// Below is a list of function pointers to be used\n-// for querying device properties through nvml library.\n-#define NVML_FUNCTOR(name, rettype, args) \\\n-  inline rettype(*xla_##name) args = nullptr;\n-\n-NVML_FUNCTOR(nvmlInit, nvmlReturn_t, ())\n-NVML_FUNCTOR(nvmlShutdown, nvmlReturn_t, ())\n-NVML_FUNCTOR(nvmlDeviceGetHandleByIndex, nvmlReturn_t,\n-             (unsigned int index, nvmlDevice_t* device))\n-NVML_FUNCTOR(nvmlDeviceGetNvLinkCapability, nvmlReturn_t,\n-             (nvmlDevice_t device, unsigned int link,\n-              nvmlNvLinkCapability_t capability, unsigned int* capResult))\n-NVML_FUNCTOR(nvmlSystemGetNVMLVersion, nvmlReturn_t,\n-             (char* version, size_t versionSize))\n-\n-#if CUDA_VERSION >= 12040\n-NVML_FUNCTOR(nvmlDeviceGetHandleByPciBusId_v2, nvmlReturn_t,\n-             (const char* pciBusId, nvmlDevice_t* device))\n-NVML_FUNCTOR(nvmlDeviceGetGpuFabricInfoV, nvmlReturn_t,\n-             (nvmlDevice_t device, nvmlGpuFabricInfoV_t* gpuFabricInfo))\n-#endif  // CUDA_VERSION >= 12040\n-\n-#endif\n-\n namespace xla {\n namespace gpu {\n "
        },
        {
            "sha": "a394d7b602df5c89510bd3a0f9199ed25710bc31",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_collective_performance_model_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model_test.cc?ref=5c18a506558e18d73cddb7620d477798c81d9879",
            "patch": "@@ -13,7 +13,10 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include \"xla/service/gpu/model/gpu_collective_performance_model.h\"\n+\n #include <gtest/gtest.h>\n+#include \"third_party/gpus/cuda/nvml/include/nvml.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n \n@@ -24,18 +27,14 @@ namespace {\n using GpuPerformanceWithCollectiveModelTest = HloHardwareIndependentTestBase;\n \n TEST_F(GpuPerformanceWithCollectiveModelTest, TestNvmlLibraryLoading) {\n-#if GOOGLE_CUDA\n   EXPECT_TRUE(GpuPerformanceWithCollectiveModel::InitNvml());\n   // After successful init, we try to use one of the\n   // nvml functions to see if the result is good.\n   nvmlDevice_t nvml_device;\n-  nvmlReturn_t get_device_result =\n-      xla_nvmlDeviceGetHandleByIndex(0, &nvml_device);\n+  nvmlReturn_t get_device_result = nvmlDeviceGetHandleByIndex(0, &nvml_device);\n   EXPECT_TRUE(get_device_result == NVML_SUCCESS);\n \n   EXPECT_TRUE(GpuPerformanceWithCollectiveModel::InitNvml());\n-\n-#endif  // GOOGLE_CUDA\n }\n \n }  // namespace"
        },
        {
            "sha": "bb23b6e4b0f85852b822e296e2e2682df3714066",
            "filename": "third_party/xla/xla/tsl/cuda/BUILD.bazel",
            "status": "modified",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Ftsl%2Fcuda%2FBUILD.bazel",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Ftsl%2Fcuda%2FBUILD.bazel",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fcuda%2FBUILD.bazel?ref=5c18a506558e18d73cddb7620d477798c81d9879",
            "patch": "@@ -411,3 +411,32 @@ alias(\n     actual = if_cuda_libs(\"@nvidia_nvshmem//:nvshmem\", \":nvshmem\"),\n     visibility = [\"//visibility:public\"],\n )\n+\n+cuda_stub(\n+    name = \"nvml\",\n+    srcs = [\"nvml.symbols\"],\n+)\n+\n+cc_library(\n+    name = \"nvml_stub\",  # buildifier: disable=duplicated-name\n+    srcs = if_cuda_is_configured([\n+        \"nvml_stub.cc\",\n+        \"nvml.tramp.S\",\n+    ]),\n+    local_defines = [\n+        \"IMPLIB_EXPORT_SHIMS=1\",\n+    ],\n+    textual_hdrs = [\"nvml.inc\"],\n+    visibility = [\"//visibility:public\"],\n+    deps = if_cuda_is_configured([\n+        \"@local_tsl//tsl/platform:dso_loader\",\n+        \"@local_tsl//tsl/platform:logging\",\n+        \"@local_tsl//tsl/platform:load_library\",\n+    ]),\n+)\n+\n+alias(\n+    name = \"nvml\",  # buildifier: disable=duplicated-name\n+    actual = \":nvml_stub\",\n+    visibility = [\"//visibility:public\"],\n+)"
        },
        {
            "sha": "0e1282f059dd1f2454d7a37fe516d7152a9650c9",
            "filename": "third_party/xla/xla/tsl/cuda/nvml.symbols",
            "status": "added",
            "additions": 376,
            "deletions": 0,
            "changes": 376,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Ftsl%2Fcuda%2Fnvml.symbols",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Ftsl%2Fcuda%2Fnvml.symbols",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fcuda%2Fnvml.symbols?ref=5c18a506558e18d73cddb7620d477798c81d9879",
            "patch": "@@ -0,0 +1,376 @@\n+nvmlComputeInstanceDestroy\n+nvmlComputeInstanceGetInfo\n+nvmlComputeInstanceGetInfo_v2\n+nvmlDeviceClearAccountingPids\n+nvmlDeviceClearCpuAffinity\n+nvmlDeviceClearEccErrorCounts\n+nvmlDeviceClearFieldValues\n+nvmlDeviceCreateGpuInstance\n+nvmlDeviceCreateGpuInstanceWithPlacement\n+nvmlDeviceDiscoverGpus\n+nvmlDeviceFreezeNvLinkUtilizationCounter\n+nvmlDeviceGetAPIRestriction\n+nvmlDeviceGetAccountingBufferSize\n+nvmlDeviceGetAccountingMode\n+nvmlDeviceGetAccountingPids\n+nvmlDeviceGetAccountingStats\n+nvmlDeviceGetActiveVgpus\n+nvmlDeviceGetAdaptiveClockInfoStatus\n+nvmlDeviceGetApplicationsClock\n+nvmlDeviceGetArchitecture\n+nvmlDeviceGetAttributes\n+nvmlDeviceGetAttributes_v2\n+nvmlDeviceGetAutoBoostedClocksEnabled\n+nvmlDeviceGetBAR1MemoryInfo\n+nvmlDeviceGetBoardId\n+nvmlDeviceGetBoardPartNumber\n+nvmlDeviceGetBrand\n+nvmlDeviceGetBridgeChipInfo\n+nvmlDeviceGetBusType\n+nvmlDeviceGetC2cModeInfoV\n+nvmlDeviceGetCapabilities\n+nvmlDeviceGetClkMonStatus\n+nvmlDeviceGetClock\n+nvmlDeviceGetClockInfo\n+nvmlDeviceGetClockOffsets\n+nvmlDeviceGetComputeInstanceId\n+nvmlDeviceGetComputeMode\n+nvmlDeviceGetComputeRunningProcesses\n+nvmlDeviceGetComputeRunningProcesses_v2\n+nvmlDeviceGetComputeRunningProcesses_v3\n+nvmlDeviceGetConfComputeGpuAttestationReport\n+nvmlDeviceGetConfComputeGpuCertificate\n+nvmlDeviceGetConfComputeMemSizeInfo\n+nvmlDeviceGetConfComputeProtectedMemoryUsage\n+nvmlDeviceGetCoolerInfo\n+nvmlDeviceGetCount\n+nvmlDeviceGetCount_v2\n+nvmlDeviceGetCpuAffinity\n+nvmlDeviceGetCpuAffinityWithinScope\n+nvmlDeviceGetCreatableVgpus\n+nvmlDeviceGetCudaComputeCapability\n+nvmlDeviceGetCurrPcieLinkGeneration\n+nvmlDeviceGetCurrPcieLinkWidth\n+nvmlDeviceGetCurrentClockFreqs\n+nvmlDeviceGetCurrentClocksEventReasons\n+nvmlDeviceGetCurrentClocksThrottleReasons\n+nvmlDeviceGetDecoderUtilization\n+nvmlDeviceGetDefaultApplicationsClock\n+nvmlDeviceGetDefaultEccMode\n+nvmlDeviceGetDetailedEccErrors\n+nvmlDeviceGetDeviceHandleFromMigDeviceHandle\n+nvmlDeviceGetDisplayActive\n+nvmlDeviceGetDisplayMode\n+nvmlDeviceGetDramEncryptionMode\n+nvmlDeviceGetDriverModel\n+nvmlDeviceGetDriverModel_v2\n+nvmlDeviceGetDynamicPstatesInfo\n+nvmlDeviceGetEccMode\n+nvmlDeviceGetEncoderCapacity\n+nvmlDeviceGetEncoderSessions\n+nvmlDeviceGetEncoderStats\n+nvmlDeviceGetEncoderUtilization\n+nvmlDeviceGetEnforcedPowerLimit\n+nvmlDeviceGetFBCSessions\n+nvmlDeviceGetFBCStats\n+nvmlDeviceGetFanControlPolicy_v2\n+nvmlDeviceGetFanSpeed\n+nvmlDeviceGetFanSpeedRPM\n+nvmlDeviceGetFanSpeed_v2\n+nvmlDeviceGetFieldValues\n+nvmlDeviceGetGpcClkMinMaxVfOffset\n+nvmlDeviceGetGpcClkVfOffset\n+nvmlDeviceGetGpuFabricInfo\n+nvmlDeviceGetGpuFabricInfoV\n+nvmlDeviceGetGpuInstanceById\n+nvmlDeviceGetGpuInstanceId\n+nvmlDeviceGetGpuInstancePossiblePlacements\n+nvmlDeviceGetGpuInstancePossiblePlacements_v2\n+nvmlDeviceGetGpuInstanceProfileInfo\n+nvmlDeviceGetGpuInstanceProfileInfoV\n+nvmlDeviceGetGpuInstanceRemainingCapacity\n+nvmlDeviceGetGpuInstances\n+nvmlDeviceGetGpuMaxPcieLinkGeneration\n+nvmlDeviceGetGpuOperationMode\n+nvmlDeviceGetGraphicsRunningProcesses\n+nvmlDeviceGetGraphicsRunningProcesses_v2\n+nvmlDeviceGetGraphicsRunningProcesses_v3\n+nvmlDeviceGetGridLicensableFeatures\n+nvmlDeviceGetGridLicensableFeatures_v2\n+nvmlDeviceGetGridLicensableFeatures_v3\n+nvmlDeviceGetGridLicensableFeatures_v4\n+nvmlDeviceGetGspFirmwareMode\n+nvmlDeviceGetGspFirmwareVersion\n+nvmlDeviceGetHandleByIndex\n+nvmlDeviceGetHandleByIndex_v2\n+nvmlDeviceGetHandleByPciBusId\n+nvmlDeviceGetHandleByPciBusId_v2\n+nvmlDeviceGetHandleBySerial\n+nvmlDeviceGetHandleByUUID\n+nvmlDeviceGetHostVgpuMode\n+nvmlDeviceGetIndex\n+nvmlDeviceGetInforomConfigurationChecksum\n+nvmlDeviceGetInforomImageVersion\n+nvmlDeviceGetInforomVersion\n+nvmlDeviceGetIrqNum\n+nvmlDeviceGetJpgUtilization\n+nvmlDeviceGetLastBBXFlushTime\n+nvmlDeviceGetMPSComputeRunningProcesses\n+nvmlDeviceGetMPSComputeRunningProcesses_v2\n+nvmlDeviceGetMPSComputeRunningProcesses_v3\n+nvmlDeviceGetMarginTemperature\n+nvmlDeviceGetMaxClockInfo\n+nvmlDeviceGetMaxCustomerBoostClock\n+nvmlDeviceGetMaxMigDeviceCount\n+nvmlDeviceGetMaxPcieLinkGeneration\n+nvmlDeviceGetMaxPcieLinkWidth\n+nvmlDeviceGetMemClkMinMaxVfOffset\n+nvmlDeviceGetMemClkVfOffset\n+nvmlDeviceGetMemoryAffinity\n+nvmlDeviceGetMemoryBusWidth\n+nvmlDeviceGetMemoryErrorCounter\n+nvmlDeviceGetMemoryInfo\n+nvmlDeviceGetMemoryInfo_v2\n+nvmlDeviceGetMigDeviceHandleByIndex\n+nvmlDeviceGetMigMode\n+nvmlDeviceGetMinMaxClockOfPState\n+nvmlDeviceGetMinMaxFanSpeed\n+nvmlDeviceGetMinorNumber\n+nvmlDeviceGetModuleId\n+nvmlDeviceGetMultiGpuBoard\n+nvmlDeviceGetName\n+nvmlDeviceGetNumFans\n+nvmlDeviceGetNumGpuCores\n+nvmlDeviceGetNumaNodeId\n+nvmlDeviceGetNvLinkCapability\n+nvmlDeviceGetNvLinkErrorCounter\n+nvmlDeviceGetNvLinkRemoteDeviceType\n+nvmlDeviceGetNvLinkRemotePciInfo\n+nvmlDeviceGetNvLinkRemotePciInfo_v2\n+nvmlDeviceGetNvLinkState\n+nvmlDeviceGetNvLinkUtilizationControl\n+nvmlDeviceGetNvLinkUtilizationCounter\n+nvmlDeviceGetNvLinkVersion\n+nvmlDeviceGetNvlinkBwMode\n+nvmlDeviceGetNvlinkSupportedBwModes\n+nvmlDeviceGetOfaUtilization\n+nvmlDeviceGetP2PStatus\n+nvmlDeviceGetPciInfo\n+nvmlDeviceGetPciInfoExt\n+nvmlDeviceGetPciInfo_v2\n+nvmlDeviceGetPciInfo_v3\n+nvmlDeviceGetPcieLinkMaxSpeed\n+nvmlDeviceGetPcieReplayCounter\n+nvmlDeviceGetPcieSpeed\n+nvmlDeviceGetPcieThroughput\n+nvmlDeviceGetPerformanceModes\n+nvmlDeviceGetPerformanceState\n+nvmlDeviceGetPersistenceMode\n+nvmlDeviceGetPgpuMetadataString\n+nvmlDeviceGetPlatformInfo\n+nvmlDeviceGetPowerManagementDefaultLimit\n+nvmlDeviceGetPowerManagementLimit\n+nvmlDeviceGetPowerManagementLimitConstraints\n+nvmlDeviceGetPowerManagementMode\n+nvmlDeviceGetPowerSource\n+nvmlDeviceGetPowerState\n+nvmlDeviceGetPowerUsage\n+nvmlDeviceGetProcessUtilization\n+nvmlDeviceGetProcessesUtilizationInfo\n+nvmlDeviceGetRemappedRows\n+nvmlDeviceGetRetiredPages\n+nvmlDeviceGetRetiredPagesPendingStatus\n+nvmlDeviceGetRetiredPages_v2\n+nvmlDeviceGetRowRemapperHistogram\n+nvmlDeviceGetRunningProcessDetailList\n+nvmlDeviceGetSamples\n+nvmlDeviceGetSerial\n+nvmlDeviceGetSramEccErrorStatus\n+nvmlDeviceGetSupportedClocksEventReasons\n+nvmlDeviceGetSupportedClocksThrottleReasons\n+nvmlDeviceGetSupportedEventTypes\n+nvmlDeviceGetSupportedGraphicsClocks\n+nvmlDeviceGetSupportedMemoryClocks\n+nvmlDeviceGetSupportedPerformanceStates\n+nvmlDeviceGetSupportedVgpus\n+nvmlDeviceGetTargetFanSpeed\n+nvmlDeviceGetTemperature\n+nvmlDeviceGetTemperatureThreshold\n+nvmlDeviceGetTemperatureV\n+nvmlDeviceGetThermalSettings\n+nvmlDeviceGetTopologyCommonAncestor\n+nvmlDeviceGetTopologyNearestGpus\n+nvmlDeviceGetTotalEccErrors\n+nvmlDeviceGetTotalEnergyConsumption\n+nvmlDeviceGetUUID\n+nvmlDeviceGetUtilizationRates\n+nvmlDeviceGetVbiosVersion\n+nvmlDeviceGetVgpuCapabilities\n+nvmlDeviceGetVgpuHeterogeneousMode\n+nvmlDeviceGetVgpuInstancesUtilizationInfo\n+nvmlDeviceGetVgpuMetadata\n+nvmlDeviceGetVgpuProcessUtilization\n+nvmlDeviceGetVgpuProcessesUtilizationInfo\n+nvmlDeviceGetVgpuSchedulerCapabilities\n+nvmlDeviceGetVgpuSchedulerLog\n+nvmlDeviceGetVgpuSchedulerState\n+nvmlDeviceGetVgpuTypeCreatablePlacements\n+nvmlDeviceGetVgpuTypeSupportedPlacements\n+nvmlDeviceGetVgpuUtilization\n+nvmlDeviceGetViolationStatus\n+nvmlDeviceGetVirtualizationMode\n+nvmlDeviceIsMigDeviceHandle\n+nvmlDeviceModifyDrainState\n+nvmlDeviceOnSameBoard\n+nvmlDevicePowerSmoothingActivatePresetProfile\n+nvmlDevicePowerSmoothingSetState\n+nvmlDevicePowerSmoothingUpdatePresetProfileParam\n+nvmlDeviceQueryDrainState\n+nvmlDeviceRegisterEvents\n+nvmlDeviceRemoveGpu\n+nvmlDeviceRemoveGpu_v2\n+nvmlDeviceResetApplicationsClocks\n+nvmlDeviceResetGpuLockedClocks\n+nvmlDeviceResetMemoryLockedClocks\n+nvmlDeviceResetNvLinkErrorCounters\n+nvmlDeviceResetNvLinkUtilizationCounter\n+nvmlDeviceSetAPIRestriction\n+nvmlDeviceSetAccountingMode\n+nvmlDeviceSetApplicationsClocks\n+nvmlDeviceSetAutoBoostedClocksEnabled\n+nvmlDeviceSetClockOffsets\n+nvmlDeviceSetComputeMode\n+nvmlDeviceSetConfComputeUnprotectedMemSize\n+nvmlDeviceSetCpuAffinity\n+nvmlDeviceSetDefaultAutoBoostedClocksEnabled\n+nvmlDeviceSetDefaultFanSpeed_v2\n+nvmlDeviceSetDramEncryptionMode\n+nvmlDeviceSetDriverModel\n+nvmlDeviceSetEccMode\n+nvmlDeviceSetFanControlPolicy\n+nvmlDeviceSetFanSpeed_v2\n+nvmlDeviceSetGpcClkVfOffset\n+nvmlDeviceSetGpuLockedClocks\n+nvmlDeviceSetGpuOperationMode\n+nvmlDeviceSetMemClkVfOffset\n+nvmlDeviceSetMemoryLockedClocks\n+nvmlDeviceSetMigMode\n+nvmlDeviceSetNvLinkDeviceLowPowerThreshold\n+nvmlDeviceSetNvLinkUtilizationControl\n+nvmlDeviceSetNvlinkBwMode\n+nvmlDeviceSetPersistenceMode\n+nvmlDeviceSetPowerManagementLimit\n+nvmlDeviceSetPowerManagementLimit_v2\n+nvmlDeviceSetTemperatureThreshold\n+nvmlDeviceSetVgpuCapabilities\n+nvmlDeviceSetVgpuHeterogeneousMode\n+nvmlDeviceSetVgpuSchedulerState\n+nvmlDeviceSetVirtualizationMode\n+nvmlDeviceValidateInforom\n+nvmlDeviceWorkloadPowerProfileClearRequestedProfiles\n+nvmlDeviceWorkloadPowerProfileGetCurrentProfiles\n+nvmlDeviceWorkloadPowerProfileGetProfilesInfo\n+nvmlDeviceWorkloadPowerProfileSetRequestedProfiles\n+nvmlErrorString\n+nvmlEventSetCreate\n+nvmlEventSetFree\n+nvmlEventSetWait\n+nvmlEventSetWait_v2\n+nvmlGetBlacklistDeviceCount\n+nvmlGetBlacklistDeviceInfoByIndex\n+nvmlGetExcludedDeviceCount\n+nvmlGetExcludedDeviceInfoByIndex\n+nvmlGetVgpuCompatibility\n+nvmlGetVgpuDriverCapabilities\n+nvmlGetVgpuVersion\n+nvmlGpmMetricsGet\n+nvmlGpmMigSampleGet\n+nvmlGpmQueryDeviceSupport\n+nvmlGpmQueryIfStreamingEnabled\n+nvmlGpmSampleAlloc\n+nvmlGpmSampleFree\n+nvmlGpmSampleGet\n+nvmlGpmSetStreamingEnabled\n+nvmlGpuInstanceCreateComputeInstance\n+nvmlGpuInstanceCreateComputeInstanceWithPlacement\n+nvmlGpuInstanceDestroy\n+nvmlGpuInstanceGetComputeInstanceById\n+nvmlGpuInstanceGetComputeInstancePossiblePlacements\n+nvmlGpuInstanceGetComputeInstanceProfileInfo\n+nvmlGpuInstanceGetComputeInstanceProfileInfoV\n+nvmlGpuInstanceGetComputeInstanceRemainingCapacity\n+nvmlGpuInstanceGetComputeInstances\n+nvmlGpuInstanceGetInfo\n+nvmlInit\n+nvmlInitWithFlags\n+nvmlInit_v2\n+nvmlInternalGetExportTable\n+nvmlSetVgpuVersion\n+nvmlShutdown\n+nvmlSystemGetConfComputeCapabilities\n+nvmlSystemGetConfComputeGpusReadyState\n+nvmlSystemGetConfComputeKeyRotationThresholdInfo\n+nvmlSystemGetConfComputeSettings\n+nvmlSystemGetConfComputeState\n+nvmlSystemGetCudaDriverVersion\n+nvmlSystemGetCudaDriverVersion_v2\n+nvmlSystemGetDriverBranch\n+nvmlSystemGetDriverVersion\n+nvmlSystemGetHicVersion\n+nvmlSystemGetNVMLVersion\n+nvmlSystemGetNvlinkBwMode\n+nvmlSystemGetProcessName\n+nvmlSystemGetTopologyGpuSet\n+nvmlSystemSetConfComputeGpusReadyState\n+nvmlSystemSetConfComputeKeyRotationThresholdInfo\n+nvmlSystemSetNvlinkBwMode\n+nvmlUnitGetCount\n+nvmlUnitGetDevices\n+nvmlUnitGetFanSpeedInfo\n+nvmlUnitGetHandleByIndex\n+nvmlUnitGetLedState\n+nvmlUnitGetPsuInfo\n+nvmlUnitGetTemperature\n+nvmlUnitGetUnitInfo\n+nvmlUnitSetLedState\n+nvmlVgpuInstanceClearAccountingPids\n+nvmlVgpuInstanceGetAccountingMode\n+nvmlVgpuInstanceGetAccountingPids\n+nvmlVgpuInstanceGetAccountingStats\n+nvmlVgpuInstanceGetEccMode\n+nvmlVgpuInstanceGetEncoderCapacity\n+nvmlVgpuInstanceGetEncoderSessions\n+nvmlVgpuInstanceGetEncoderStats\n+nvmlVgpuInstanceGetFBCSessions\n+nvmlVgpuInstanceGetFBCStats\n+nvmlVgpuInstanceGetFbUsage\n+nvmlVgpuInstanceGetFrameRateLimit\n+nvmlVgpuInstanceGetGpuInstanceId\n+nvmlVgpuInstanceGetGpuPciId\n+nvmlVgpuInstanceGetLicenseInfo\n+nvmlVgpuInstanceGetLicenseInfo_v2\n+nvmlVgpuInstanceGetLicenseStatus\n+nvmlVgpuInstanceGetMdevUUID\n+nvmlVgpuInstanceGetMetadata\n+nvmlVgpuInstanceGetPlacementId\n+nvmlVgpuInstanceGetRuntimeStateSize\n+nvmlVgpuInstanceGetType\n+nvmlVgpuInstanceGetUUID\n+nvmlVgpuInstanceGetVmDriverVersion\n+nvmlVgpuInstanceGetVmID\n+nvmlVgpuInstanceSetEncoderCapacity\n+nvmlVgpuTypeGetBAR1Info\n+nvmlVgpuTypeGetCapabilities\n+nvmlVgpuTypeGetClass\n+nvmlVgpuTypeGetDeviceID\n+nvmlVgpuTypeGetFbReservation\n+nvmlVgpuTypeGetFrameRateLimit\n+nvmlVgpuTypeGetFramebufferSize\n+nvmlVgpuTypeGetGpuInstanceProfileId\n+nvmlVgpuTypeGetGspHeapSize\n+nvmlVgpuTypeGetLicense\n+nvmlVgpuTypeGetMaxInstances\n+nvmlVgpuTypeGetMaxInstancesPerVm\n+nvmlVgpuTypeGetName\n+nvmlVgpuTypeGetNumDisplayHeads\n+nvmlVgpuTypeGetResolution"
        },
        {
            "sha": "ac6e82986e5764ae5809a27149a95f3a0562de38",
            "filename": "third_party/xla/xla/tsl/cuda/nvml_stub.cc",
            "status": "added",
            "additions": 72,
            "deletions": 0,
            "changes": 72,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Ftsl%2Fcuda%2Fnvml_stub.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Ftsl%2Fcuda%2Fnvml_stub.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fcuda%2Fnvml_stub.cc?ref=5c18a506558e18d73cddb7620d477798c81d9879",
            "patch": "@@ -0,0 +1,72 @@\n+/* Copyright 2025 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include \"third_party/gpus/cuda/nvml/include/nvml.h\"\n+#include \"xla/tsl/platform/logging.h\"\n+#include \"tsl/platform/dso_loader.h\"\n+#include \"tsl/platform/load_library.h\"\n+\n+// Implements the NVML API by forwarding to NVML loaded from the DSO.\n+\n+namespace {\n+// Returns DSO handle or null if loading the DSO fails.\n+void* GetDsoHandle() {\n+#ifdef PLATFORM_GOOGLE\n+  return nullptr;\n+#else\n+  static auto handle = []() -> void* {\n+    auto handle_or = tsl::internal::DsoLoader::GetNvmlDsoHandle();\n+    if (!handle_or.ok()) return nullptr;\n+    return handle_or.value();\n+  }();\n+  return handle;\n+#endif\n+}\n+\n+void* LoadSymbol(const char* symbol_name) {\n+  void* symbol = nullptr;\n+  if (auto handle = GetDsoHandle()) {\n+    tsl::internal::GetSymbolFromLibrary(handle, symbol_name, &symbol)\n+        .IgnoreError();\n+  }\n+  return symbol;\n+}\n+\n+const char* kSymbols[] = {\n+#include \"xla/tsl/cuda/nvml.inc\"\n+};\n+\n+constexpr size_t kNumSymbols = sizeof(kSymbols) / sizeof(const char*);\n+\n+}  // namespace\n+\n+extern \"C\" {\n+\n+static nvmlReturn_t GetSymbolNotFoundError() {\n+  return NVML_ERROR_FUNCTION_NOT_FOUND;\n+}\n+\n+extern void* _nvml_tramp_table[];\n+\n+void _nvml_tramp_resolve(int i) {\n+  CHECK_LE(0, i);\n+  CHECK_LT(i, kNumSymbols);\n+  void* p = LoadSymbol(kSymbols[i]);\n+  if (!p) {\n+    p = reinterpret_cast<void*>(&GetSymbolNotFoundError);\n+  }\n+  _nvml_tramp_table[i] = p;\n+}\n+\n+}  // extern \"C\""
        },
        {
            "sha": "ae6f6076ce5bcc7e0a566c35ee4c39ca82411301",
            "filename": "third_party/xla/xla/tsl/platform/default/dso_loader.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fdefault%2Fdso_loader.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fdefault%2Fdso_loader.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fdefault%2Fdso_loader.cc?ref=5c18a506558e18d73cddb7620d477798c81d9879",
            "patch": "@@ -161,6 +161,10 @@ absl::StatusOr<void*> GetCudaDriverDsoHandle() {\n   return GetDsoHandle(\"cuda\", \"1\");\n }\n \n+absl::StatusOr<void*> GetNvmlDsoHandle() {\n+  return GetDsoHandle(\"nvidia-ml\", \"1\");\n+}\n+\n absl::StatusOr<void*> GetCudaRuntimeDsoHandle() {\n   return GetDsoHandle(\"cudart\", GetCudaRtVersion());\n }"
        },
        {
            "sha": "2e544da0a439262cb693c8c1fcb6b48cd0c8ace7",
            "filename": "third_party/xla/xla/tsl/platform/default/dso_loader.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fdefault%2Fdso_loader.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5c18a506558e18d73cddb7620d477798c81d9879/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fdefault%2Fdso_loader.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fplatform%2Fdefault%2Fdso_loader.h?ref=5c18a506558e18d73cddb7620d477798c81d9879",
            "patch": "@@ -41,6 +41,7 @@ absl::StatusOr<void*> GetNcclDsoHandle();\n absl::StatusOr<void*> GetNvshmemDsoHandle();\n absl::StatusOr<void*> GetNvInferDsoHandle();\n absl::StatusOr<void*> GetNvInferPluginDsoHandle();\n+absl::StatusOr<void*> GetNvmlDsoHandle();\n \n absl::StatusOr<void*> GetRocblasDsoHandle();\n absl::StatusOr<void*> GetMiopenDsoHandle();"
        }
    ],
    "stats": {
        "total": 615,
        "additions": 521,
        "deletions": 94
    }
}