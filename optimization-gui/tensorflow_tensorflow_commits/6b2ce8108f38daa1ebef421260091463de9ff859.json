{
    "author": "sergachev",
    "message": "PR #33794: [GPU] Support int4 in cuDNN GEMM fusions.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/33794\n\nðŸ“ Summary of Changes\nSupport int4 in cuDNN GEMM fusions.\n\nðŸŽ¯ Justification\nAccelerates some int4 GEMM fusions (under the flag xla_gpu_cudnn_gemm_fusion_level).\n\nðŸš€ Kind of Contribution\nâš¡ï¸ Performance Improvement\n\nðŸ“Š Benchmark (for Performance Improvements)\n> Please measure and include speedups for one of the public HLOs in\n`compiler/xla/tools/benchmarks/hlo/`.\n\nThese do not use int4.\n\nðŸ§ª Unit Tests:\nyes\n\nðŸ§ª Execution Tests:\nyes\nCopybara import of the project:\n\n--\ne1b8dc7daff4963b93152d2a5c81c4d91a9f14d8 by Ilia Sergachev <isergachev@nvidia.com>:\n\n[GPU] Support int4 in cuDNN GEMM fusions.\n\nMerging this change closes #33794\n\nPiperOrigin-RevId: 831264661",
    "sha": "6b2ce8108f38daa1ebef421260091463de9ff859",
    "files": [
        {
            "sha": "dd519c2580f627524e4f2750e65bd61f074ff368",
            "filename": "third_party/xla/xla/backends/gpu/codegen/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b2ce8108f38daa1ebef421260091463de9ff859/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b2ce8108f38daa1ebef421260091463de9ff859/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD?ref=6b2ce8108f38daa1ebef421260091463de9ff859",
            "patch": "@@ -104,7 +104,6 @@ xla_test(\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n-        \"//xla/tsl/lib/core:status_test_util\",\n         \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\","
        },
        {
            "sha": "32841d971beaa602f7e4002be3c00a32ee225fe7",
            "filename": "third_party/xla/xla/backends/gpu/codegen/cudnn_test.cc",
            "status": "modified",
            "additions": 28,
            "deletions": 6,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b2ce8108f38daa1ebef421260091463de9ff859/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b2ce8108f38daa1ebef421260091463de9ff859/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcudnn_test.cc?ref=6b2ce8108f38daa1ebef421260091463de9ff859",
            "patch": "@@ -46,7 +46,6 @@ limitations under the License.\n #include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n-#include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/env.h\"\n@@ -59,8 +58,6 @@ namespace xla {\n namespace gpu {\n namespace {\n \n-using ::tsl::testing::IsOkAndHolds;\n-\n class CuDnnFusionTest : public GpuCodegenTest {\n  public:\n   DebugOptions GetDebugOptionsForTest() const override {\n@@ -80,12 +77,14 @@ class CuDnnFusionTest : public GpuCodegenTest {\n     return get_cuda_cc().IsAtLeastAmpere() &&\n            GetDnnVersionInfoOrDefault(executor).major_version() >= 9;\n   }\n-  bool IsAtLeastCuDnn91() {\n+  bool IsAtLeastCuDnnVersion(int major, int minor) {\n     se::StreamExecutor* executor = backend().default_stream_executor();\n     const se::dnn::VersionInfo version = GetDnnVersionInfoOrDefault(executor);\n-    return (version.major_version() == 9 && version.minor_version() >= 1) ||\n-           version.major_version() > 9;\n+    return (version.major_version() == major &&\n+            version.minor_version() >= minor) ||\n+           version.major_version() > major;\n   }\n+  bool IsAtLeastCuDnn91() { return IsAtLeastCuDnnVersion(9, 1); }\n \n  protected:\n   void SetUp() override {\n@@ -457,6 +456,29 @@ ENTRY e {\n                             ErrorSpec{/*aabs=*/1e-6, /*arel=*/1e-6}));\n }\n \n+TEST_F(CuDnnFusionExecutionTest, DotS4BF16ExecutesCorrectly) {\n+  if (!IsAtLeastCuDnnVersion(9, 12)) {\n+    GTEST_SKIP() << \"This test case requires cuDNN 9.12+.\";\n+  }\n+  EXPECT_TRUE(RunAndCompare(R\"(\n+f {\n+  a = s4[3,128,128] parameter(0)\n+  c = bf16[3,128,128] convert(a)\n+  b = bf16[3,128,128] parameter(1)\n+  d = bf16[3,128,128] dot(c, b),\n+    lhs_batch_dims={0}, rhs_batch_dims={0},\n+    lhs_contracting_dims={2}, rhs_contracting_dims={1}\n+}\n+\n+e {\n+  a = s4[3,128,128] parameter(0)\n+  b = bf16[3,128,128] parameter(1)\n+  f = bf16[3,128,128] fusion(a, b), kind=kCustom, calls=f,\n+    backend_config={\"fusion_backend_config\": {kind: \"__cudnn$fusion\"}}\n+})\",\n+                            ErrorSpec{/*aabs=*/1e-6, /*arel=*/1e-6}));\n+}\n+\n TEST_F(CuDnnFusionExecutionTest, DotF32WithOutputSubtractionExecutesCorrectly) {\n   EXPECT_TRUE(RunAndCompare(R\"(\n fusion1 {"
        },
        {
            "sha": "ac0c2e5daf0d98cb829fc80566a40598b240ff30",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_fusion_compiler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b2ce8108f38daa1ebef421260091463de9ff859/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b2ce8108f38daa1ebef421260091463de9ff859/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fusion_compiler.cc?ref=6b2ce8108f38daa1ebef421260091463de9ff859",
            "patch": "@@ -149,6 +149,8 @@ inline std::optional<fe::DataType_t> ToCudnnDataType(const PrimitiveType type) {\n       return t::BFLOAT16;\n     case PrimitiveType::S32:\n       return t::INT32;\n+    case PrimitiveType::S4:\n+      return t::INT4;\n     case PrimitiveType::S8:\n       return t::INT8;\n     case PrimitiveType::PRED:"
        }
    ],
    "stats": {
        "total": 37,
        "additions": 30,
        "deletions": 7
    }
}