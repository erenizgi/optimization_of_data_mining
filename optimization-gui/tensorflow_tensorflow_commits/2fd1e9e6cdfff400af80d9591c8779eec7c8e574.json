{
    "author": "Cjkkkk",
    "message": "PR #30996: [XLA:GPU] remove cudnn sdpa dbias constraint\n\nImported from GitHub PR https://github.com/openxla/xla/pull/30996\n\nüìù Summary of Changes\nAllow cudnn dbias sdpa with bias shape [1, 1, s, s], [b,1, s, s], [b, h, s, s] in addition to [1, h, s, s].\n\nüéØ Justification\nSupport dbias computation with more types of bias shape broadcasting.\n\nüöÄ Kind of Contribution\nPlease remove what does not apply:\n‚ú® New Feature\n\nüìä Benchmark (for Performance Improvements)\nNo public hlo uses new shapes of bias.\n\nüß™ Unit Tests:\nNo Hlo changes, mostly rely on cuDNN to do the job\n\nüß™ Execution Tests:\nXla had 2 dbias tests **Flash_Attention_BMM1_Bias_Softmax_BMM2_BF16_Dbias** and **Flash_Attention_Training_BMM1_Bias_Softmax_BMM2_BF16** for same [1, h, s, s] shape bias, modify **Flash_Attention_BMM1_Bias_Softmax_BMM2_BF16_Dbias** test to use [1,1, s, s] shape bias.\n\nCopybara import of the project:\n\n--\n63513e17936089a919ffe0901034f0fc6ed79452 by Cjkkkk <ske@nvidia.com>:\n\nremove dbias constraint\n\n--\n2778c7091defeff961a6d9b14e99d1afd85aa4fe by Cjkkkk <ske@nvidia.com>:\n\nadd one unit test\n\n--\nd83841496d4f0c16c9f170d4407186819f4e03ef by Cjkkkk <ske@nvidia.com>:\n\nremove unused variable\n\n--\n748e83f12cea6f0a79adfb3bfce9abb412d2ada2 by Cjkkkk <ske@nvidia.com>:\n\nfix typo\n\nMerging this change closes #30996\n\nPiperOrigin-RevId: 808470976",
    "sha": "2fd1e9e6cdfff400af80d9591c8779eec7c8e574",
    "files": [
        {
            "sha": "53003473933e9d4a496521b7e2d40bd0b7c2d82f",
            "filename": "third_party/xla/xla/service/gpu/tests/gpu_fused_mha_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2fd1e9e6cdfff400af80d9591c8779eec7c8e574/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_fused_mha_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2fd1e9e6cdfff400af80d9591c8779eec7c8e574/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_fused_mha_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_fused_mha_test.cc?ref=2fd1e9e6cdfff400af80d9591c8779eec7c8e574",
            "patch": "@@ -612,7 +612,7 @@ class FlashAttentionBMMScaleBiasSoftmaxBMM : public MultiHeadedAttentionTest {\n   const std::string  // NOLINT\n   GetModuleFlash_Attention_BMM1_Bias_Softmax_BMM2_Dbias_HloString_BF16() {  // NOLINT\n     const std::string hlo_text = R\"(\n-      HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[4,1024,1024]{2,1,0})->(bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[4,1024,1024]{2,1,0})}, allow_spmd_sharding_propagation_to_parameters={true,true,true,true,true}, allow_spmd_sharding_propagation_to_output={true,true,true,true,true}\n+      HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[1024,1024]{1,0})->(bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[4,1024,1024]{2,1,0})}, allow_spmd_sharding_propagation_to_parameters={true,true,true,true,true}, allow_spmd_sharding_propagation_to_output={true,true,true,true,true}\n \n       region_0.14 {\n         Arg_0.15 = bf16[] parameter(0)\n@@ -640,8 +640,8 @@ class FlashAttentionBMMScaleBiasSoftmaxBMM : public MultiHeadedAttentionTest {\n         Arg_1.2 = bf16[2,1024,4,64]{3,2,1,0} parameter(1)\n         transpose.15 = bf16[2,4,64,1024]{3,2,1,0} transpose(Arg_1.2), dimensions={0,2,3,1}\n         dot = bf16[2,4,1024,1024]{3,2,1,0} dot(transpose.13, transpose.15), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n-        Arg_4.5 = bf16[4,1024,1024]{2,1,0} parameter(4)\n-        broadcast.9 = bf16[2,4,1024,1024]{3,2,1,0} broadcast(Arg_4.5), dimensions={1,2,3}\n+        Arg_4.5 = bf16[1024,1024]{1,0} parameter(4)\n+        broadcast.9 = bf16[2,4,1024,1024]{3,2,1,0} broadcast(Arg_4.5), dimensions={2,3}\n         add.2 = bf16[2,4,1024,1024]{3,2,1,0} add(dot, broadcast.9)\n         constant.10 = bf16[] constant(-inf)\n         reduce.18 = bf16[2,4,1024]{2,1,0} reduce(add.2, constant.10), dimensions={3}, to_apply=region_0.14\n@@ -691,7 +691,7 @@ class FlashAttentionBMMScaleBiasSoftmaxBMM : public MultiHeadedAttentionTest {\n   const std::string  // NOLINT\n   GetModuleFlash_Attention_CuDNN_BMM1_Bias_Softmax_BMM2_Dbias_HloString_BF16() {  // NOLINT\n     const std::string hlo_text = R\"(\n-    HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[4,1024,1024]{2,1,0})->(bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[4,1024,1024]{2,1,0})}, allow_spmd_sharding_propagation_to_parameters={true,true,true,true,true}, allow_spmd_sharding_propagation_to_output={true,true,true,true,true}\n+    HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[1024,1024]{1,0})->(bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[2,1024,4,64]{3,2,1,0}, bf16[4,1024,1024]{2,1,0})}, allow_spmd_sharding_propagation_to_parameters={true,true,true,true,true}, allow_spmd_sharding_propagation_to_output={true,true,true,true,true}\n \n     ENTRY main.87 {\n       Arg_0.1 = bf16[2,1024,4,64]{3,2,1,0} parameter(0)\n@@ -702,9 +702,9 @@ class FlashAttentionBMMScaleBiasSoftmaxBMM : public MultiHeadedAttentionTest {\n       Arg_2.3 = bf16[2,1024,4,64]{3,2,1,0} parameter(2)\n       transpose.1 = bf16[2,4,64,1024]{3,2,1,0} transpose(Arg_2.3), dimensions={0,2,3,1}\n       transpose.27 = bf16[2,4,1024,64]{3,2,1,0} transpose(transpose.1), dimensions={0,1,3,2}\n-      Arg_4.5 = bf16[4,1024,1024]{2,1,0} parameter(4)\n-      reshape.131 = bf16[1,4,1024,1024]{3,2,1,0} reshape(Arg_4.5)\n-      fmha-bmm-scale-bias-softmax-bmm = (bf16[2,4,1024,64]{3,2,1,0}, f32[2,4,1024]{2,1,0}, u8[0]{0}) custom-call(transpose.2, transpose.26, transpose.27, reshape.131), custom_call_target=\"__cudnn$fmhaScaleBiasSoftmax\", operand_layout_constraints={bf16[2,4,1024,64]{3,2,1,0}, bf16[2,4,1024,64]{3,2,1,0}, bf16[2,4,1024,64]{3,2,1,0}, bf16[1,4,1024,1024]{3,2,1,0}}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_fmha_backend_config\":{\"algorithm\":{\"algo_id\":\"0\",\"math_type\":\"TENSOR_OP_MATH\",\"tuning_knobs\":{\"17\":\"1\",\"24\":\"0\"},\"workspace_size\":\"0\"},\"fmha_scale\":1,\"dropout_rate\":0,\"bmm1_dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"3\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"bmm2_dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"intermediate_tensor_shape\":{\"element_type\":\"BF16\",\"dimensions\":[\"2\",\"4\",\"1024\",\"1024\"],\"tuple_shapes\":[],\"layout\":{\"dim_level_types\":[],\"dim_unique\":[],\"dim_ordered\":[],\"minor_to_major\":[\"3\",\"2\",\"1\",\"0\"],\"tiles\":[],\"tail_padding_alignment_in_elements\":\"1\",\"element_size_in_bits\":\"0\",\"memory_space\":\"0\",\"index_primitive_type\":\"PRIMITIVE_TYPE_INVALID\",\"pointer_primitive_type\":\"PRIMITIVE_TYPE_INVALID\",\"dynamic_shape_metadata_prefix_bytes\":\"0\",\"split_configs\":[]},\"is_dynamic_dimension\":[false,false,false,false]},\"seed\":\"42\",\"is_flash_attention\":false,\"is_causal_mask\":false,\"mask_type\":\"NO_MASK\",\"force_deterministic\":false,\"sliding_window_length\":0},\"force_earliest_schedule\":false}\n+      Arg_4.5 = bf16[1024,1024]{1,0} parameter(4)\n+      reshape.131 = bf16[1,1,1024,1024]{3,2,1,0} reshape(Arg_4.5)\n+      fmha-bmm-scale-bias-softmax-bmm = (bf16[2,4,1024,64]{3,2,1,0}, f32[2,4,1024]{2,1,0}, u8[0]{0}) custom-call(transpose.2, transpose.26, transpose.27, reshape.131), custom_call_target=\"__cudnn$fmhaScaleBiasSoftmax\", operand_layout_constraints={bf16[2,4,1024,64]{3,2,1,0}, bf16[2,4,1024,64]{3,2,1,0}, bf16[2,4,1024,64]{3,2,1,0}, bf16[1,1,1024,1024]{3,2,1,0}}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_fmha_backend_config\":{\"algorithm\":{\"algo_id\":\"0\",\"math_type\":\"TENSOR_OP_MATH\",\"tuning_knobs\":{\"17\":\"1\",\"24\":\"0\"},\"workspace_size\":\"0\"},\"fmha_scale\":1,\"dropout_rate\":0,\"bmm1_dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"3\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"bmm2_dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"intermediate_tensor_shape\":{\"element_type\":\"BF16\",\"dimensions\":[\"2\",\"4\",\"1024\",\"1024\"],\"tuple_shapes\":[],\"layout\":{\"dim_level_types\":[],\"dim_unique\":[],\"dim_ordered\":[],\"minor_to_major\":[\"3\",\"2\",\"1\",\"0\"],\"tiles\":[],\"tail_padding_alignment_in_elements\":\"1\",\"element_size_in_bits\":\"0\",\"memory_space\":\"0\",\"index_primitive_type\":\"PRIMITIVE_TYPE_INVALID\",\"pointer_primitive_type\":\"PRIMITIVE_TYPE_INVALID\",\"dynamic_shape_metadata_prefix_bytes\":\"0\",\"split_configs\":[]},\"is_dynamic_dimension\":[false,false,false,false]},\"seed\":\"42\",\"is_flash_attention\":false,\"is_causal_mask\":false,\"mask_type\":\"NO_MASK\",\"force_deterministic\":false,\"sliding_window_length\":0},\"force_earliest_schedule\":false}\n       get-tuple-element.5 = bf16[2,4,1024,64]{3,2,1,0} get-tuple-element(fmha-bmm-scale-bias-softmax-bmm), index=0\n       transpose.21 = bf16[2,4,64,1024]{3,2,1,0} transpose(get-tuple-element.5), dimensions={0,1,3,2}\n       transpose.11 = bf16[2,1024,4,64]{3,2,1,0} transpose(transpose.21), dimensions={0,3,1,2}\n@@ -713,7 +713,7 @@ class FlashAttentionBMMScaleBiasSoftmaxBMM : public MultiHeadedAttentionTest {\n       get-tuple-element.7 = f32[2,4,1024]{2,1,0} get-tuple-element(fmha-bmm-scale-bias-softmax-bmm), index=1\n       Arg_3.4 = bf16[2,1024,4,64]{3,2,1,0} parameter(3)\n       transpose.6 = bf16[2,4,1024,64]{3,2,1,0} transpose(Arg_3.4), dimensions={0,2,1,3}\n-      fmha-bmm-scale-bias-softmax-bmm-backward = (bf16[2,4,1024,64]{3,2,1,0}, bf16[2,4,1024,64]{3,2,1,0}, bf16[2,4,1024,64]{3,2,1,0}, bf16[1,4,1024,1024]{3,2,1,0}, u8[0]{0}) custom-call(transpose.2, transpose.7, transpose.29, get-tuple-element.7, transpose.6, /*index=5*/reshape.131, get-tuple-element.5), custom_call_target=\"__cudnn$fmhaScaleBiasSoftmaxBackward\", operand_layout_constraints={bf16[2,4,1024,64]{3,2,1,0}, bf16[2,4,1024,64]{3,2,1,0}, bf16[2,4,1024,64]{3,2,1,0}, f32[2,4,1024]{2,1,0}, bf16[2,4,1024,64]{3,2,1,0}, bf16[1,4,1024,1024]{3,2,1,0}, bf16[2,4,1024,64]{3,2,1,0}}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_fmha_backend_config\":{\"algorithm\":{\"algo_id\":\"0\",\"math_type\":\"TENSOR_OP_MATH\",\"tuning_knobs\":{\"24\":\"0\",\"17\":\"1\"},\"workspace_size\":\"0\"},\"fmha_scale\":1,\"dropout_rate\":0,\"intermediate_tensor_shape\":{\"element_type\":\"BF16\",\"dimensions\":[\"2\",\"4\",\"1024\",\"1024\"],\"tuple_shapes\":[],\"layout\":{\"dim_level_types\":[],\"dim_unique\":[],\"dim_ordered\":[],\"minor_to_major\":[\"3\",\"2\",\"1\",\"0\"],\"tiles\":[],\"tail_padding_alignment_in_elements\":\"1\",\"element_size_in_bits\":\"0\",\"memory_space\":\"0\",\"index_primitive_type\":\"PRIMITIVE_TYPE_INVALID\",\"pointer_primitive_type\":\"PRIMITIVE_TYPE_INVALID\",\"dynamic_shape_metadata_prefix_bytes\":\"0\",\"split_configs\":[]},\"is_dynamic_dimension\":[false,false,false,false]},\"bmm1_grad_gemm1_dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"2\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"bmm1_grad_gemm2_dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"bmm2_grad_gemm1_dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"2\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"bmm2_grad_gemm2_dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"3\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"seed\":\"42\",\"is_flash_attention\":false,\"is_causal_mask\":false,\"mask_type\":\"NO_MASK\",\"force_deterministic\":false,\"sliding_window_length\":0},\"force_earliest_schedule\":false}\n+      fmha-bmm-scale-bias-softmax-bmm-backward = (bf16[2,4,1024,64]{3,2,1,0}, bf16[2,4,1024,64]{3,2,1,0}, bf16[2,4,1024,64]{3,2,1,0}, bf16[1,4,1024,1024]{3,2,1,0}, u8[0]{0}) custom-call(transpose.2, transpose.7, transpose.29, get-tuple-element.7, transpose.6, /*index=5*/reshape.131, get-tuple-element.5), custom_call_target=\"__cudnn$fmhaScaleBiasSoftmaxBackward\", operand_layout_constraints={bf16[2,4,1024,64]{3,2,1,0}, bf16[2,4,1024,64]{3,2,1,0}, bf16[2,4,1024,64]{3,2,1,0}, f32[2,4,1024]{2,1,0}, bf16[2,4,1024,64]{3,2,1,0}, bf16[1,1,1024,1024]{3,2,1,0}, bf16[2,4,1024,64]{3,2,1,0}}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_fmha_backend_config\":{\"algorithm\":{\"algo_id\":\"0\",\"math_type\":\"TENSOR_OP_MATH\",\"tuning_knobs\":{\"24\":\"0\",\"17\":\"1\"},\"workspace_size\":\"0\"},\"fmha_scale\":1,\"dropout_rate\":0,\"intermediate_tensor_shape\":{\"element_type\":\"BF16\",\"dimensions\":[\"2\",\"4\",\"1024\",\"1024\"],\"tuple_shapes\":[],\"layout\":{\"dim_level_types\":[],\"dim_unique\":[],\"dim_ordered\":[],\"minor_to_major\":[\"3\",\"2\",\"1\",\"0\"],\"tiles\":[],\"tail_padding_alignment_in_elements\":\"1\",\"element_size_in_bits\":\"0\",\"memory_space\":\"0\",\"index_primitive_type\":\"PRIMITIVE_TYPE_INVALID\",\"pointer_primitive_type\":\"PRIMITIVE_TYPE_INVALID\",\"dynamic_shape_metadata_prefix_bytes\":\"0\",\"split_configs\":[]},\"is_dynamic_dimension\":[false,false,false,false]},\"bmm1_grad_gemm1_dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"2\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"bmm1_grad_gemm2_dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"bmm2_grad_gemm1_dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"2\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"bmm2_grad_gemm2_dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"3\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"seed\":\"42\",\"is_flash_attention\":false,\"is_causal_mask\":false,\"mask_type\":\"NO_MASK\",\"force_deterministic\":false,\"sliding_window_length\":0},\"force_earliest_schedule\":false}\n       get-tuple-element.8 = bf16[2,4,1024,64]{3,2,1,0} get-tuple-element(fmha-bmm-scale-bias-softmax-bmm-backward), index=0\n       transpose.14 = bf16[2,1024,4,64]{3,2,1,0} transpose(get-tuple-element.8), dimensions={0,2,1,3}\n       get-tuple-element.9 = bf16[2,4,1024,64]{3,2,1,0} get-tuple-element(fmha-bmm-scale-bias-softmax-bmm-backward), index=1"
        },
        {
            "sha": "3ae9947dc56312bcc61a56cb29accb1c2f9951fd",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_dnn.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2fd1e9e6cdfff400af80d9591c8779eec7c8e574/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2fd1e9e6cdfff400af80d9591c8779eec7c8e574/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc?ref=2fd1e9e6cdfff400af80d9591c8779eec7c8e574",
            "patch": "@@ -4795,21 +4795,17 @@ absl::StatusOr<CudnnGraph> GetCudnnFlashAttentionBackwardOperationGraph(\n     DCHECK(bias_descriptor != std::nullopt);\n     auto bias_dims = bias_descriptor->dimensions();\n     auto bias_strides = bias_descriptor->GetLogicalStrides();\n-    auto b = bias_dims[0];\n-    auto n = bias_dims[1];\n-    auto q_n = q_dims[1];\n     auto bias_tensor = graph.tensor(Tensor_attributes()\n                                         .set_name(\"bias\")\n                                         .set_dim(bias_dims)\n                                         .set_stride(bias_strides)\n                                         .set_uid(next_uid()));\n     sdpa_backward_options.set_bias(bias_tensor);\n \n-    // shapes [1, 1, s, s], [b, 1, s, s], [b, h, s, s] are not supported for\n-    // dbias calculation but they are supported for forward bias calculation\n+    // shapes [1, 1, s, s], [1, h, s, s], [b, 1, s, s], [b, h, s, s] are\n+    // supported for dbias calculation.\n     // Set UID later: this is the last output tuple element.\n     if (dbias_descriptor != std::nullopt) {\n-      DCHECK(b == 1 && n == q_n);\n       d_bias_tensor =\n           graph.tensor(Tensor_attributes()\n                            .set_name(\"dBias\")"
        }
    ],
    "stats": {
        "total": 24,
        "additions": 10,
        "deletions": 14
    }
}