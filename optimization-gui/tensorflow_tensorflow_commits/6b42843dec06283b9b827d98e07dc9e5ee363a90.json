{
    "author": "derdrdirk",
    "message": "[Autotuner] Get supported configs from Cublas/Lt without initializing allocating buffers on the device.\n\nPiperOrigin-RevId: 798225392",
    "sha": "6b42843dec06283b9b827d98e07dc9e5ee363a90",
    "files": [
        {
            "sha": "8cc5c9065accd03137b29894388b567becca22bb",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b42843dec06283b9b827d98e07dc9e5ee363a90/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b42843dec06283b9b827d98e07dc9e5ee363a90/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=6b42843dec06283b9b827d98e07dc9e5ee363a90",
            "patch": "@@ -127,6 +127,7 @@ cc_library(\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/stream_executor/gpu:gpu_blas_lt\",\n         \"//xla/stream_executor/gpu:redzone_allocator\",\n         \"//xla/tools:hlo_decomposer_lib\",\n         \"//xla/tsl/lib/gtl:iterator_range\",\n@@ -187,6 +188,7 @@ cc_library(\n     deps = [\n         \":gpu_codegen_backend\",\n         \"//xla:autotuning_proto_cc\",\n+        \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/backends/autotuner:codegen_backend\",\n@@ -195,11 +197,9 @@ cc_library(\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:cublas_cudnn\",\n         \"//xla/service/gpu:matmul_utils\",\n-        \"//xla/service/gpu/autotuning:redzone_buffers\",\n         \"//xla/stream_executor:blas\",\n         \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n         \"//xla/stream_executor/gpu:gpu_blas_lt\",\n@@ -451,17 +451,14 @@ xla_test(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:filecheck\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n-        \"//xla/service:compiler\",\n-        \"//xla/service:hlo_module_util\",\n         \"//xla/service:platform_util\",\n-        \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n         \"//xla/stream_executor:device_description_proto_cc\",\n-        \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\","
        },
        {
            "sha": "c51ceb43bfff4fbf9b28fcd29736a0d88fc94a3b",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublas.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 17,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b42843dec06283b9b827d98e07dc9e5ee363a90/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b42843dec06283b9b827d98e07dc9e5ee363a90/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.cc?ref=6b42843dec06283b9b827d98e07dc9e5ee363a90",
            "patch": "@@ -27,8 +27,6 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/utils/hlo_query.h\"\n-#include \"xla/service/compiler.h\"\n-#include \"xla/service/gpu/autotuning/redzone_buffers.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n@@ -40,7 +38,7 @@ limitations under the License.\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n-#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/stream_executor/gpu/gpu_blas_lt.h\"\n #include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -74,27 +72,44 @@ CublasBackend::GetSupportedConfigs(const HloInstruction& instr) {\n           &instr, backend_config,\n           target_config().device_description.gpu_compute_capability()));\n \n-  TF_ASSIGN_OR_RETURN(RedzoneBuffers rz_buffers,\n-                      RedzoneBuffers::FromInstruction(\n-                          instr, allocator.get(), stream,\n-                          RedzoneBuffers::kAllInputsAllOutputs, true, true,\n-                          instr.GetModule()\n-                              ->config()\n-                              .debug_options()\n-                              .xla_gpu_redzone_padding_bytes()));\n-\n+  auto create_matrix_desc = [](const se::gpu::MatrixLayout& layout)\n+      -> absl::StatusOr<se::gpu::MatrixDescriptor> {\n+    TF_ASSIGN_OR_RETURN(se::blas::DataType type,\n+                        se::gpu::AsBlasDataType(layout.dtype));\n+    return se::gpu::MatrixDescriptor{\n+        /*data=*/se::DeviceMemoryBase(), layout.leading_dim_stride,\n+        layout.batch_stride, type,\n+        // BLAS is column-major by default.\n+        (layout.order == se::gpu::MatrixLayout::Order::kColumnMajor\n+             ? se::blas::Transpose::kNoTranspose\n+             : se::blas::Transpose::kTranspose)};\n+  };\n+\n+  TF_ASSIGN_OR_RETURN(se::gpu::MatrixDescriptor lhs_desc,\n+                      create_matrix_desc(gemm_config.lhs_layout));\n+  TF_ASSIGN_OR_RETURN(se::gpu::MatrixDescriptor rhs_desc,\n+                      create_matrix_desc(gemm_config.rhs_layout));\n+  TF_ASSIGN_OR_RETURN(se::gpu::MatrixDescriptor output_desc_base,\n+                      create_matrix_desc(gemm_config.output_layout));\n+\n+  se::gpu::OutputMatrixDescriptor out_desc(std::move(output_desc_base));\n+  out_desc.batch_size = gemm_config.output_layout.batch_size;\n+  out_desc.m = gemm_config.output_layout.num_rows;\n+  out_desc.n = gemm_config.output_layout.num_cols;\n+  out_desc.k = gemm_config.lhs_layout.num_cols;\n   TF_ASSIGN_OR_RETURN(\n-      GemmConfig::DescriptorsTuple desc,\n-      gemm_config.GetMatrixDescriptors(rz_buffers.input_buffers().at(0),\n-                                       rz_buffers.input_buffers().at(1),\n-                                       rz_buffers.output_buffers().at(0)));\n+      out_desc.compute_type,\n+      se::gpu::GetBlasComputationType(\n+          gemm_config.precision_algorithm, gemm_config.lhs_layout.dtype,\n+          gemm_config.output_layout.dtype, gemm_config.compute_precision));\n \n   se::blas::BlasSupport* blas = stream_executor()->AsBlas();\n   if (blas == nullptr) {\n     return absl::InternalError(\"Failed to getBlas support.\");\n   }\n   std::vector<se::blas::AlgorithmType> algorithms;\n-  blas->GetBlasGemmAlgorithms(stream, desc.lhs, desc.rhs, &desc.output,\n+\n+  blas->GetBlasGemmAlgorithms(stream, lhs_desc, rhs_desc, &out_desc,\n                               &gemm_config.alpha, &gemm_config.beta,\n                               &algorithms);\n "
        },
        {
            "sha": "ef03149115ccbfa838c8c766435c4cd710f15242",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublaslt.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 26,
            "changes": 47,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b42843dec06283b9b827d98e07dc9e5ee363a90/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b42843dec06283b9b827d98e07dc9e5ee363a90/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.cc?ref=6b42843dec06283b9b827d98e07dc9e5ee363a90",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/backends/gpu/autotuner/cublaslt.h\"\n \n+#include <cstdint>\n #include <memory>\n #include <utility>\n #include <vector>\n@@ -26,17 +27,15 @@ limitations under the License.\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/compiler.h\"\n-#include \"xla/service/gpu/autotuning/redzone_buffers.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n #include \"xla/stream_executor/blas.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/gpu/gpu_blas_lt.h\"\n-#include \"xla/stream_executor/stream_executor.h\"\n-#include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n+#include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -99,30 +98,26 @@ CublasLtBackend::GetSupportedConfigs(const HloInstruction& instr) {\n   TF_ASSIGN_OR_RETURN(BlasLt::Epilogue epilogue,\n                       AsBlasLtEpilogue(backend_config.epilogue()));\n \n-  auto allocator =\n-      std::make_unique<se::StreamExecutorMemoryAllocator>(stream_executor());\n-  TF_ASSIGN_OR_RETURN(\n-      se::Stream * stream,\n-      allocator->GetStream(stream_executor()->device_ordinal()));\n+  TF_ASSIGN_OR_RETURN(std::unique_ptr<se::Stream> stream,\n+                      stream_executor()->CreateStream());\n \n   TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<BlasLt::MatmulPlan> plan,\n-      se::gpu::BlasLt::GetMatmulPlan(stream, gemm_config, epilogue));\n-\n-  TF_ASSIGN_OR_RETURN(RedzoneBuffers rz_buffers,\n-                      RedzoneBuffers::FromInstruction(\n-                          instr, allocator.get(), stream,\n-                          RedzoneBuffers::kAllInputsAllOutputs, true, true,\n-                          instr.GetModule()\n-                              ->config()\n-                              .debug_options()\n-                              .xla_gpu_redzone_padding_bytes()));\n-  se::DeviceMemoryBase workspace_buffer =\n-      rz_buffers.output_buffers().at(instr.shape().tuple_shapes().size() - 1);\n-\n-  TF_ASSIGN_OR_RETURN(std::vector<BlasLt::MatmulAlgorithm> algorithms,\n-                      plan->GetAlgorithms(stream, GemmConfig::kNumAlgorithms,\n-                                          workspace_buffer.size()));\n+      se::gpu::BlasLt::GetMatmulPlan(stream.get(), gemm_config, epilogue));\n+\n+  const Shape& output_shape = instr.shape();\n+  if (!output_shape.IsTuple() || output_shape.tuple_shapes().empty()) {\n+    return Internal(\n+        \"Invalid shape for CublasLt matmul: output is not a non-empty tuple.\");\n+  }\n+  // The last element of the output tuple is the workspace.\n+  const int64_t workspace_size =\n+      ShapeUtil::ByteSizeOf(output_shape.tuple_shapes().back());\n+\n+  TF_ASSIGN_OR_RETURN(\n+      std::vector<BlasLt::MatmulAlgorithm> algorithms,\n+      plan->GetAlgorithms(stream.get(), GemmConfig::kNumAlgorithms,\n+                          workspace_size));\n   int num_algorithms = algorithms.size();\n   std::vector<std::unique_ptr<BackendConfig>> configs;\n   configs.reserve(num_algorithms);"
        },
        {
            "sha": "a0e4e1d54ba83861d21fc9d6faa8cbd0113ca3ee",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b42843dec06283b9b827d98e07dc9e5ee363a90/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b42843dec06283b9b827d98e07dc9e5ee363a90/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc?ref=6b42843dec06283b9b827d98e07dc9e5ee363a90",
            "patch": "@@ -241,6 +241,7 @@ FissionBackend::GetSupportedConfigs(const HloInstruction& instr) {\n         std::vector<std::unique_ptr<BackendConfig>> cublas_configs,\n         GetCublasConfigs(cublas_backend_, std::move(cublas_hlo_module),\n                          stream_executor()));\n+    VLOG(2) << \"Found \" << cublas_configs.size() << \" cublas configs.\";\n     configs.insert(configs.end(),\n                    std::make_move_iterator(cublas_configs.begin()),\n                    std::make_move_iterator(cublas_configs.end()));\n@@ -255,6 +256,7 @@ FissionBackend::GetSupportedConfigs(const HloInstruction& instr) {\n         std::vector<std::unique_ptr<BackendConfig>> cublaslt_configs,\n         GetCublasLtConfigs(cublaslt_backend_, std::move(cublaslt_hlo_module),\n                            stream_executor()));\n+    VLOG(2) << \"Found \" << cublaslt_configs.size() << \" cublasLt configs.\";\n     configs.insert(configs.end(),\n                    std::make_move_iterator(cublaslt_configs.begin()),\n                    std::make_move_iterator(cublaslt_configs.end()));\n@@ -269,6 +271,8 @@ FissionBackend::GetSupportedConfigs(const HloInstruction& instr) {\n         GetCustomKernelConfigs(custom_kernel_backend_,\n                                std::move(custom_kernel_hlo_module),\n                                stream_executor()));\n+    VLOG(2) << \"Found \" << custom_kernel_configs.size()\n+            << \" custom kernel configs. \";\n     configs.insert(configs.end(),\n                    std::make_move_iterator(custom_kernel_configs.begin()),\n                    std::make_move_iterator(custom_kernel_configs.end()));"
        },
        {
            "sha": "6590facbd624dd465d5d7d2c35405cd0991075d6",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6b42843dec06283b9b827d98e07dc9e5ee363a90/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6b42843dec06283b9b827d98e07dc9e5ee363a90/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc?ref=6b42843dec06283b9b827d98e07dc9e5ee363a90",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/autotuning.pb.h\"\n@@ -32,7 +33,6 @@ limitations under the License.\n #include \"xla/service/gpu/nvptx_compiler.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n-#include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -116,8 +116,7 @@ TEST_F(FissionBackendTest, GetSupportedConfigsForUnsupportedInstructionFails) {\n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> configs =\n       backend_.GetSupportedConfigs(\n           (*module->entry_computation()->root_instruction()));\n-  EXPECT_THAT(configs.status(),\n-              absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n+  EXPECT_THAT(configs.status(), StatusIs(absl::StatusCode::kInvalidArgument));\n }\n \n TEST_F(FissionBackendTest, GetDefaultConfigFails) {\n@@ -127,8 +126,7 @@ TEST_F(FissionBackendTest, GetDefaultConfigFails) {\n   absl::StatusOr<std::unique_ptr<BackendConfig>> config =\n       backend_.GetDefaultConfig(\n           (*module->entry_computation()->root_instruction()));\n-  EXPECT_THAT(config.status(),\n-              absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n+  EXPECT_THAT(config.status(), StatusIs(absl::StatusCode::kInvalidArgument));\n }\n \n TEST_F(FissionBackendTest, ApplyCublasConfigToFusionInstruction) {\n@@ -142,7 +140,7 @@ TEST_F(FissionBackendTest, ApplyCublasConfigToFusionInstruction) {\n       *hlo_module->entry_computation()->root_instruction(), any));\n   EXPECT_THAT(RunFileCheck(hlo_module->ToString(),\n                            \"CHECK: \\\"selected_algorithm\\\":\\\"3\\\"\"),\n-              absl_testing::IsOkAndHolds(true));\n+              IsOkAndHolds(true));\n }\n \n TEST_F(FissionBackendTest, ApplyCustomKernelConfigToFusionInstruction) {\n@@ -155,7 +153,7 @@ TEST_F(FissionBackendTest, ApplyCustomKernelConfigToFusionInstruction) {\n   TF_EXPECT_OK(backend_.ApplyConfig(\n       *hlo_module->entry_computation()->root_instruction(), any));\n   EXPECT_THAT(RunFileCheck(hlo_module->ToString(), \"CHECK: \\\"kernel_index\\\":3\"),\n-              absl_testing::IsOkAndHolds(true));\n+              IsOkAndHolds(true));\n }\n \n }  // namespace"
        }
    ],
    "stats": {
        "total": 123,
        "additions": 66,
        "deletions": 57
    }
}