{
    "author": "mtsokol",
    "message": "PR #33199: Docs: Error 0100\n\nImported from GitHub PR https://github.com/openxla/xla/pull/33199\n\nAfter #32628\n\nðŸš€ Kind of Contribution\n\nðŸ“š Documentation\n\nCopybara import of the project:\n\n--\n87aa8d2e818a7633eac51a79f8bcfe9c913f81eb by Mateusz SokÃ³Å‚ <mat646@gmail.com>:\n\nDoc page for Error 0100\n\nMerging this change closes #33199\n\nPiperOrigin-RevId: 834399621",
    "sha": "6c463c65897790db312e2318a0402454c707dd80",
    "files": [
        {
            "sha": "af77e3682d147a6f4b3eb677ac1432376d6b8e2f",
            "filename": "third_party/xla/docs/error_codes.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c463c65897790db312e2318a0402454c707dd80/third_party%2Fxla%2Fdocs%2Ferror_codes.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c463c65897790db312e2318a0402454c707dd80/third_party%2Fxla%2Fdocs%2Ferror_codes.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Ferror_codes.md?ref=6c463c65897790db312e2318a0402454c707dd80",
            "patch": "@@ -2,4 +2,5 @@\n \n This page is a list of all error codes emitted by the XLA compiler.\n \n+-   [E0100](./errors/error_0100.md)\n -   [E0102](./errors/error_0102.md)"
        },
        {
            "sha": "36a7d62aae95ba90aa8fb98769bdb9314ad8f946",
            "filename": "third_party/xla/docs/errors/error_0100.md",
            "status": "added",
            "additions": 68,
            "deletions": 0,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c463c65897790db312e2318a0402454c707dd80/third_party%2Fxla%2Fdocs%2Ferrors%2Ferror_0100.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c463c65897790db312e2318a0402454c707dd80/third_party%2Fxla%2Fdocs%2Ferrors%2Ferror_0100.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Ferrors%2Ferror_0100.md?ref=6c463c65897790db312e2318a0402454c707dd80",
            "patch": "@@ -0,0 +1,68 @@\n+# Error code: 0100\n+\n+**Category:** Buffer allocation failure - TPU\n+\n+**Type:** Runtime\n+\n+## Error log example\n+\n+```\n+ValueError: RESOURCE_EXHAUSTED: Error allocating device buffer: Attempting to allocate 8.00M. That was not possible. There are 6.43M free.; (0x0x1_HBM0)\n+```\n+\n+## Why do these happen?\n+\n+XLA:TPU runtimeâ€™s memory allocator failed to find a suitable block of memory on\n+the acceleratorâ€™s HBM for the requested operation. These operations are\n+typically user initiated buffer allocations via\n+[`jax.device_put`](https://docs.jax.dev/en/latest/_autosummary/jax.device_put.html)\n+or allocations for program outputs. These failures stem from a couple of\n+reasons: - Out of Memory (OOM) - The user is trying to allocate a chunk of\n+memory that is larger than the total amount of free memory available on the\n+TPUâ€™s HBM. - Memory Fragmentation - The allocation fails because **no single\n+contiguous free block** in the memory space is large enough to satisfy the\n+requested size. The total amount of free memory is sufficient for the\n+allocation, but it is scattered across the memory space in small, non-contiguous\n+blocks.\n+\n+The TPU runtime has a number of mechanisms in-place to retry allocation failures\n+including: - If there are queued deallocations, the runtime retries failed\n+allocations, - On OOMs caused by a fragmentation the runtime can automatically\n+trigger a defragmentation and a retry. - The TPU runtime prioritizes buffer\n+allocations over keeping programs loaded. If a buffer allocation fails due to\n+insufficient HBM, the system will evict loaded TPU programs until enough memory\n+is available for the buffer.\n+\n+So an error encountered after the above mitigations typically require user\n+action.\n+\n+## How can a user fix their program when they do happen?\n+\n+-   Reduce your model's memory footprint:\n+    -   Decrease Batch Size: Reducing the batch size directly lowers memory\n+        usage.\n+    -   Parameter Sharding: For very large models, use techniques like model\n+        parallelism or sharding to distribute parameters across the HBM of\n+        multiple TPU cores or hosts.\n+    -   Shorten Sequence/Context Length: For models that operate on sequences\n+        (like language models), reducing the input sequence length can\n+        significantly decrease the memory footprint.\n+    -   Buffer Donation: Utilize framework features (such as: `jax.jit(...,\n+        donate_argnums=...)`) to signal to XLA that certain input buffers can be\n+        overwritten and reused for outputs.\n+    -   Optimize Checkpoint Strategy: Instead of saving the entire model state\n+        at once, consider saving only the model weights or using a sharded\n+        checkpointing strategy.\n+-   Address Memory Layout and Padding:\n+    -   TPU memory is allocated in chunks, and padding can increase the actual\n+        size of tensors.\n+-   Ensure no memory leaks:\n+    -   Ensure references to `jax.Array` objects are not being held longer than\n+        intended. Holding on to `jax.Array` objects might prevent automatic\n+        de-allocation even after program compilation is completed.\n+\n+## How can a user debug these failures?\n+\n+Enable the `tpu_log_allocations_on_oom` flag for which the allocator will dump a\n+detailed report of all current allocations when an OOM occurs, which can be\n+invaluable for debugging."
        }
    ],
    "stats": {
        "total": 69,
        "additions": 69,
        "deletions": 0
    }
}