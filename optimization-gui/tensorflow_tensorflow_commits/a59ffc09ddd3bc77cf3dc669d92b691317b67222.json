{
    "author": "tensorflower-gardener",
    "message": "Reverts 408bf09796590bc66233afff288bf926e2736a9d\n\nPiperOrigin-RevId: 846257722",
    "sha": "a59ffc09ddd3bc77cf3dc669d92b691317b67222",
    "files": [
        {
            "sha": "52e8470478101026bc360cf92541fbc1db096f6a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/triton_gemm_fusion_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 16,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftriton_gemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftriton_gemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftriton_gemm_fusion_test.cc?ref=a59ffc09ddd3bc77cf3dc669d92b691317b67222",
            "patch": "@@ -562,10 +562,7 @@ ENTRY e {\n                                ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n }\n \n-// TODO: b/422676780 - Enable the tests once the indexing maps-based tiling is\n-// deprecated. The test is disabled after we remove TransposeDimensionGrouper\n-// pass, because the infra currently requires grouping of adjacent dimensions.\n-TEST_F(TritonGemmTest, DISABLED_SplitLhsNoncontractingTransposeRhs) {\n+TEST_F(TritonGemmTest, SplitLhsNoncontractingTransposeRhs) {\n   constexpr absl::string_view kHloText = R\"(\n HloModule t\n \n@@ -590,10 +587,7 @@ ENTRY e {\n   EXPECT_TRUE(RunAndCompare(kHloText, ErrorSpec{/*aabs=*/0, /*arel=*/0}));\n }\n \n-// TODO: b/422676780 - Enable the tests once the indexing maps-based tiling is\n-// deprecated. The test is disabled after we remove TransposeDimensionGrouper\n-// pass, because the infra currently requires grouping of adjacent dimensions.\n-TEST_F(TritonGemmTest, DISABLED_SplitLhsNoncontracting) {\n+TEST_F(TritonGemmTest, SplitLhsNoncontracting) {\n   constexpr absl::string_view kHloText = R\"(\n ENTRY e {\n   p0 = f32[72,72] parameter(0)\n@@ -1782,17 +1776,12 @@ ENTRY e {\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           GetOptimizedModule(kHloText));\n-  const HloInstruction* root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(\n-      root,\n-      GmockMatch(\n+      module->entry_computation()->root_instruction(),\n+      GmockMatch(m::Bitcast(\n           m::Fusion(m::Fusion(m::Parameter(), m::Parameter())\n                         .WithFusionKind(HloInstruction::FusionKind::kCustom))\n-              .WithFusionKind(HloInstruction::FusionKind::kInput)));\n-\n-  const HloFusionInstruction* root_fusion = Cast<HloFusionInstruction>(root);\n-  EXPECT_EQ(root_fusion->fused_expression_root()->opcode(),\n-            HloOpcode::kTranspose);\n+              .WithFusionKind(HloInstruction::FusionKind::kInput))));\n \n   EXPECT_TRUE(RunAndCompare(kHloText, ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n }"
        },
        {
            "sha": "36471b34d7a2ac14e27ff12b668c5e1b3aef5ac5",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=a59ffc09ddd3bc77cf3dc669d92b691317b67222",
            "patch": "@@ -1751,6 +1751,7 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n       // introduced the nested fusions. We also want to keep it close to the\n       // gemm rewriter to avoid the possibility of new passes to rewrite the\n       // transpose.\n+      pipeline.AddPass<TransposeDimensionGrouper>();\n       pipeline.AddPass<GemmFusion>(gpu_version);\n       pipeline.AddPass<GemmFusionSwapOperands>();\n     } else if (cuda_cc != nullptr &&\n@@ -1778,6 +1779,8 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n     // also have unsorted update_window_dims.\n     pipeline.AddPass<ScatterSimplifier>();\n     pipeline.AddPass<BroadcastCanonicalizer>();\n+    // BroadcastCanonicalizer can create transposes.\n+    pipeline.AddPass<TransposeDimensionGrouper>();\n     pipeline.AddPass<ReductionDegenerateDimRemover>();\n     pipeline.AddPass<ReductionLayoutNormalizer>();\n     // Run Softmax fusion after layout normalization. We expect a default layout"
        },
        {
            "sha": "855267bd792c9a37aace5c27fb109360f44730c3",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=a59ffc09ddd3bc77cf3dc669d92b691317b67222",
            "patch": "@@ -1855,6 +1855,34 @@ TEST_F(PassOrderTest, NestGemmFusionRunsAfterHoistFusedBitcasts) {\n   VerifyPassOrder(\"hoist-fused-bitcasts\", \"nest_gemm_fusion\");\n }\n \n+TEST_F(PassOrderTest, TransposeDimensionGrouperRunsBeforeGemmRewriter) {\n+  if (!get_cuda_cc().IsAtLeastAmpere()) {\n+    GTEST_SKIP() << \"triton-gemm-rewriter requires at least Ampere to run.\";\n+  }\n+  if (!optimized_module_) {\n+    CompileModule(GetModuleConfigForTest());\n+  }\n+  // DebugOptions options = GetDebugOptionsForTest();\n+  // options.set_xla_gpu_enable_triton_gemm(true);\n+  // SetDebugOptions(options);\n+  // Verify that transpose-dimension-grouper runs immediately before\n+  // triton-gemm-rewriter. We want to keep them close together to avoid the\n+  // possibility of new passes to rewrite the transpose and make it\n+  // not compatible with the generic triton emitter.\n+  // Simple VerifyPassOrder does not work here as we want to check that passes\n+  // are run next to each other, also transpose-dimension-grouper runs one more\n+  // time after the gemm rewriter.\n+  CHECK(optimized_module_);\n+  std::string previous_pass_name;\n+  for (const HloPassMetadata& pass_metadata :\n+       optimized_module_->metadata().proto().pass_metadata()) {\n+    if (pass_metadata.pass_name() == \"triton-gemm-rewriter\") {\n+      EXPECT_EQ(previous_pass_name, \"transpose-dimension-grouper\");\n+    }\n+    previous_pass_name = pass_metadata.pass_name();\n+  }\n+}\n+\n TEST_F(PassOrderTest,\n        ReducePrecisionIsRemovedAfterAllCallsToSimplifyFPConversions) {\n   // Because of an issue with JAX remat and `SimplifyFPConversions` (see PR:"
        },
        {
            "sha": "67d67b9594af5fe673ce08c76875026783c4c1d3",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test_autotune_db.textproto",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto?ref=a59ffc09ddd3bc77cf3dc669d92b691317b67222",
            "patch": "@@ -63,7 +63,7 @@ results {\n }\n results {\n   device: \"CUDA: 9.0, Cores: 132, GPU clock: 1.98 GHz, Memory bandwidth: 3352 GB/s, L2 cache: 50 MB, DNN version: 1.2.3\"\n-  hlo: \"{\\n  tmp_0 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(0)\\n  tmp_1 = bf16[] constant({...})\\n  tmp_2 = bf16[1,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[] tmp_1), dimensions={}\\n  tmp_3 = bf16[1,4,32,1024,1024]{4,3,2,1,0} multiply(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_0, bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_2)\\n  tmp_4 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_3)\\n  tmp_5 = bf16[4,32,1024,1024]{3,2,1,0} transpose(bf16[4,32,1024,1024]{3,2,1,0} tmp_4), dimensions={0,1,3,2}\\n  tmp_6 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_5)\\n  tmp_7 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(1)\\n  tmp_8 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_7)\\n  tmp_9 = bf16[128,1024,1024]{2,1,0} dot(bf16[128,1024,1024]{2,1,0} tmp_6, bf16[128,1024,1024]{2,1,0} tmp_8), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}\\n  ROOT tmp_10 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_9)\\n}\"\n+  hlo: \"{\\n  tmp_0 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(0)\\n  tmp_1 = bf16[] constant({...})\\n  tmp_2 = bf16[1,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[] tmp_1), dimensions={}\\n  tmp_3 = bf16[1,4,32,1024,1024]{4,3,2,1,0} multiply(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_0, bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_2)\\n  tmp_4 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_3)\\n  tmp_5 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_4)\\n  tmp_6 = bf16[128,1024,1024]{2,1,0} transpose(bf16[128,1024,1024]{2,1,0} tmp_5), dimensions={0,2,1}\\n  tmp_7 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_6)\\n  tmp_8 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_7)\\n  tmp_9 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(1)\\n  tmp_10 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_9)\\n  tmp_11 = bf16[128,1024,1024]{2,1,0} dot(bf16[128,1024,1024]{2,1,0} tmp_8, bf16[128,1024,1024]{2,1,0} tmp_10), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}\\n  ROOT tmp_12 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_11)\\n}\"\n   result {\n     gemm {\n       algorithm: -1\n@@ -183,7 +183,7 @@ results {\n }\n results {\n   device: \"CUDA: 9.0, Cores: 132, GPU clock: 1.98 GHz, Memory bandwidth: 3352 GB/s, L2 cache: 50 MB, DNN version: 1.2.3\"\n-  hlo: \"{\\n  tmp_0 = bf16[3,32,1024,4,1024]{4,3,2,1,0} parameter(0)\\n  tmp_1 = bf16[3,4,32,1024,1024]{4,3,2,1,0} transpose(bf16[3,32,1024,4,1024]{4,3,2,1,0} tmp_0), dimensions={0,3,1,2,4}\\n  tmp_2 = bf16[1,3,32,1024]{3,2,1,0} parameter(1)\\n  tmp_3 = bf16[3,32,1024]{2,1,0} bitcast(bf16[1,3,32,1024]{3,2,1,0} tmp_2)\\n  tmp_4 = bf16[3,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[3,32,1024]{2,1,0} tmp_3), dimensions={0,2,3}\\n  tmp_5 = bf16[3,4,32,1024,1024]{4,3,2,1,0} add(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_1, bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_4)\\n  tmp_6 = bf16[1,4,32,1024,1024]{4,3,2,1,0} slice(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_5), slice={[1:2], [0:4], [0:32], [0:1024], [0:1024]}\\n  tmp_7 = bf16[1,4,32,1024,1024]{4,3,2,1,0} slice(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_5), slice={[0:1], [0:4], [0:32], [0:1024], [0:1024]}\\n  tmp_8 = bf16[] constant({...})\\n  tmp_9 = bf16[1,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[] tmp_8), dimensions={}\\n  tmp_10 = bf16[1,4,32,1024,1024]{4,3,2,1,0} multiply(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_7, bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_9)\\n  tmp_11 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_10)\\n  tmp_12 = bf16[4,32,1024,1024]{3,2,1,0} transpose(bf16[4,32,1024,1024]{3,2,1,0} tmp_11), dimensions={0,1,3,2}\\n  ROOT tmp_13 = (bf16[1,4,32,1024,1024]{4,3,2,1,0}, bf16[4,32,1024,1024]{3,2,1,0}) tuple(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_6, bf16[4,32,1024,1024]{3,2,1,0} tmp_12)\\n}\"\n+  hlo: \"{\\n  tmp_0 = bf16[3,32,1024,4,1024]{4,3,2,1,0} parameter(0)\\n  tmp_1 = bf16[3,32768,4,1024]{3,2,1,0} bitcast(bf16[3,32,1024,4,1024]{4,3,2,1,0} tmp_0)\\n  tmp_2 = bf16[3,4,32768,1024]{3,2,1,0} transpose(bf16[3,32768,4,1024]{3,2,1,0} tmp_1), dimensions={0,2,1,3}\\n  tmp_3 = bf16[3,4,32,1024,1024]{4,3,2,1,0} bitcast(bf16[3,4,32768,1024]{3,2,1,0} tmp_2)\\n  tmp_4 = bf16[1,3,32,1024]{3,2,1,0} parameter(1)\\n  tmp_5 = bf16[3,32,1024]{2,1,0} bitcast(bf16[1,3,32,1024]{3,2,1,0} tmp_4)\\n  tmp_6 = bf16[3,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[3,32,1024]{2,1,0} tmp_5), dimensions={0,2,3}\\n  tmp_7 = bf16[3,4,32,1024,1024]{4,3,2,1,0} add(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_3, bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_6)\\n  tmp_8 = bf16[1,4,32,1024,1024]{4,3,2,1,0} slice(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_7), slice={[1:2], [0:4], [0:32], [0:1024], [0:1024]}\\n  tmp_9 = bf16[1,4,32,1024,1024]{4,3,2,1,0} slice(bf16[3,4,32,1024,1024]{4,3,2,1,0} tmp_7), slice={[0:1], [0:4], [0:32], [0:1024], [0:1024]}\\n  tmp_10 = bf16[] constant({...})\\n  tmp_11 = bf16[1,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[] tmp_10), dimensions={}\\n  tmp_12 = bf16[1,4,32,1024,1024]{4,3,2,1,0} multiply(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_9, bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_11)\\n  tmp_13 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_12)\\n  tmp_14 = bf16[128,1024,1024]{2,1,0} transpose(bf16[128,1024,1024]{2,1,0} tmp_13), dimensions={0,2,1}\\n  ROOT tmp_15 = (bf16[1,4,32,1024,1024]{4,3,2,1,0}, bf16[128,1024,1024]{2,1,0}) tuple(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_8, bf16[128,1024,1024]{2,1,0} tmp_14)\\n}\"\n   result {\n     other {\n       name: \"NativeEmitter\""
        },
        {
            "sha": "5f665644c5719e142316faf44746187d96fe14d6",
            "filename": "third_party/xla/xla/service/gpu/gpu_fusible.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 16,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc?ref=a59ffc09ddd3bc77cf3dc669d92b691317b67222",
            "patch": "@@ -64,22 +64,9 @@ bool ContainsTransposeWithSmallMostMinorDim(const HloFusionAdaptor& fusion,\n       return false;\n     }\n     const HloInstruction& transpose = instr.instruction();\n-    // The kLoop emitter operates on the original transpose, but it handles the\n-    // index calculation. The critical factor for performance (coalescing) is\n-    // the size of the contiguous memory block being accessed in the minor\n-    // dimension. Normalization reveals this true physical dimension size by\n-    // merging adjacent logical dimensions. If this normalized dimension is\n-    // large enough, the unrolled accesses will be coalesced, justifying the\n-    // unroll factor.\n-    absl::InlinedVector<int64_t, 3> permutation;\n-    auto normalized_dims_or = ShapeUtil::GetNormalizedLogicalTransposeShape(\n-        transpose.operand(0)->shape(), transpose.shape(),\n-        transpose.dimensions(), permutation);\n-    if (normalized_dims_or.ok()) {\n-      return normalized_dims_or.value().back() < unroll_factor;\n-    } else {\n-      return transpose.shape().dimensions().back() < unroll_factor;\n-    }\n+    // We can assume that TransposeDimensionGrouper pass has run, so no need\n+    // to try to combine adjacent dimensions.\n+    return transpose.shape().dimensions().back() < unroll_factor;\n   });\n }\n "
        },
        {
            "sha": "d1ad49adfd127805fa770b09314eae97e8ab9cb9",
            "filename": "third_party/xla/xla/service/gpu/gpu_fusible_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 32,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible_test.cc?ref=a59ffc09ddd3bc77cf3dc669d92b691317b67222",
            "patch": "@@ -603,7 +603,9 @@ TEST_F(GpuFusibleTest, FusionHeroesAreCompatible_TransposeFusionNotCompatible) {\n     fused_computation_1 {\n       p0.1 = f32[64,32]{1,0} parameter(0)\n       neg = f32[64,32]{1,0} negate(p0.1)\n-      ROOT transpose = f32[32,64]{1,0} transpose(neg), dimensions={1,0}\n+      bc = f32[1,64,32]{2,1,0} bitcast(neg)\n+      transpose = f32[1,32,64]{2,1,0} transpose(bc), dimensions={0,2,1}\n+      ROOT bc2 = f32[32,64]{1,0} bitcast(transpose)\n     }\n \n     fused_computation_2 {\n@@ -621,10 +623,12 @@ TEST_F(GpuFusibleTest, FusionHeroesAreCompatible_TransposeFusionNotCompatible) {\n   const HloInstruction* fusion_1 =\n       module->entry_computation()->root_instruction();\n   const HloInstruction* fusion_2 = fusion_1->operand(0);\n-  EXPECT_FALSE(FusionHeroesAreCompatible(fusion_1->fused_expression_root(),\n-                                         fusion_2->fused_expression_root()));\n-  EXPECT_FALSE(FusionHeroesAreCompatible(fusion_2->fused_expression_root(),\n-                                         fusion_1->fused_expression_root()));\n+  EXPECT_FALSE(\n+      FusionHeroesAreCompatible(fusion_1->fused_expression_root(),\n+                                fusion_2->fused_expression_root()->operand(0)));\n+  EXPECT_FALSE(\n+      FusionHeroesAreCompatible(fusion_2->fused_expression_root()->operand(0),\n+                                fusion_1->fused_expression_root()));\n }\n \n TEST_F(GpuFusibleTest, ShapesCompatibleForMultiOutputFusion_LoopFusions) {\n@@ -1306,9 +1310,9 @@ TEST_F(GpuFusibleTest, ChooseFusionKind) {\n HloModule module\n \n ENTRY computation {\n-    p = f32[5000,6000]{1,0} parameter(0)\n-    c = f32[6000,5000] transpose(p), dimensions={1,0}\n-    ROOT r = f32[300,20,5000] reshape(c)\n+    p = f32[1,5000,6000]{2,1,0} parameter(0)\n+    c = f32[1,6000,5000]{2,1,0} transpose(p), dimensions={0,2,1}\n+    ROOT r = f32[300,20,5000]{2,1,0} reshape(c)\n }\n )\")\n                     .value();\n@@ -1798,30 +1802,6 @@ ENTRY main {\n   EXPECT_EQ(config.unroll_factor, 8);\n }\n \n-TEST_F(GpuFusibleTest,\n-       ComputeLoopFusionConfigForLoopTransposeEffectiveLargerMinorDim) {\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n-HloModule m\n-\n-ENTRY main {\n-  p0 = f16[256,2048,4,2]{3,2,1,0} parameter(0)\n-  ROOT res = f16[2048,256,4,2]{3,2,1,0} transpose(p0), dimensions={1,0,2,3}\n-}\n-)\"));\n-  const HloInstruction* root = module->entry_computation()->root_instruction();\n-  se::DeviceDescription device_info_h100{\n-      TestGpuDeviceInfo::RTXH100SXMDeviceInfo()};\n-  auto analysis = HloFusionAnalysis::Create(*root, device_info_h100);\n-  auto config = ComputeLoopFusionConfig(analysis, root->shape());\n-  EXPECT_EQ(config.unroll_factor, 4);\n-\n-  se::DeviceDescription device_info_b200{\n-      TestGpuDeviceInfo::RTXB200SXMDeviceInfo()};\n-  analysis = HloFusionAnalysis::Create(*root, device_info_b200);\n-  config = ComputeLoopFusionConfig(analysis, root->shape());\n-  EXPECT_EQ(config.unroll_factor, 8);\n-}\n-\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "72c74c7b8ea8d2441a031fc014dd5e52fd01fc66",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 23,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc?ref=a59ffc09ddd3bc77cf3dc669d92b691317b67222",
            "patch": "@@ -245,23 +245,20 @@ std::optional<TransposeDescription> GetDescriptionForTiledTransposeEmitter(\n     return std::nullopt;\n   }\n \n-  absl::InlinedVector<int64_t, 3> permutation;\n-  auto normalized_dims_or = ShapeUtil::GetNormalizedLogicalTransposeShape(\n-      hero.operand(0)->shape(), hero.shape(), hero.dimensions(), permutation);\n-  if (!normalized_dims_or.ok()) {\n-    return std::nullopt;\n-  }\n-  auto normalized_dims = normalized_dims_or.value();\n-  auto normalized_operand_dims =\n-      Permute(normalized_dims, InversePermutation(permutation));\n+  // We can assume that TransposeDimensionGrouper pass has run, so no need to\n+  // call GetNormalizedLogicalTransposeShape here.\n+  absl::InlinedVector<int64_t, 3> permutation(hero.dimensions().begin(),\n+                                              hero.dimensions().end());\n   // A real transpose needs at least 2 transpose dimensions.\n   if (permutation.size() < 2) {\n     return std::nullopt;\n   }\n   auto bit_width = GetBitwidth(hero.shape().element_type());\n-  int64_t operand_most_minor_dim = normalized_operand_dims.back();\n+  absl::InlinedVector<int64_t, 3> dimensions(hero.shape().dimensions().begin(),\n+                                             hero.shape().dimensions().end());\n+  int64_t operand_most_minor_dim = hero.operand(0)->shape().dimensions().back();\n \n-  TransposeDescription desc{&hero, normalized_dims, permutation,\n+  TransposeDescription desc{&hero, dimensions, permutation,\n                             /*shmem_usage=*/0};\n   if (CanEmitPackedTranspose(desc)) {\n     int64_t vector_size =\n@@ -270,28 +267,27 @@ std::optional<TransposeDescription> GetDescriptionForTiledTransposeEmitter(\n         kNumShmemBanks * (kBankBitwidth / 8) * kNumShmemBanks * vector_size;\n     return desc;\n   }\n-  // Minor dimension is preserved.\n-  if (permutation.back() == normalized_dims.size() - 1) {\n+  if (permutation.back() == dimensions.size() - 1) {\n     operand_most_minor_dim =\n-        normalized_operand_dims[normalized_dims.size() - 2];\n-    if (bit_width * normalized_dims.back() <= kMaxBitsInMostMinorDimension &&\n-        bit_width * normalized_dims.back() *\n+        hero.operand(0)->shape().dimensions(dimensions.size() - 2);\n+    if (bit_width * dimensions.back() <= kMaxBitsInMostMinorDimension &&\n+        bit_width * dimensions.back() *\n                 std::min(operand_most_minor_dim,\n-                         normalized_dims[normalized_dims.size() - 2]) >=\n+                         dimensions[dimensions.size() - 2]) >=\n             8 * kMinDimensionToTransposeTiled) {\n       // Tile size for transposition.\n       int64_t shmem_usage_bytes =\n           CeilOfRatio(kNumShmemBanks * (kNumShmemBanks + 1LL) * bit_width *\n-                          normalized_dims.back(),\n+                          dimensions.back(),\n                       8LL);\n-      return TransposeDescription{&hero, normalized_dims, permutation,\n+      return TransposeDescription{&hero, dimensions, permutation,\n                                   shmem_usage_bytes};\n     }\n   } else if ((operand_most_minor_dim >= kMinDimensionToTransposeTiled &&\n-              normalized_dims.back() >= kMinDimensionToTransposeTiled) ||\n+              dimensions.back() >= kMinDimensionToTransposeTiled) ||\n              (operand_most_minor_dim >= kMinDimensionToTransposeTiled2 &&\n-              normalized_dims.back() >= kMinDimensionToTransposeTiled2 &&\n-              operand_most_minor_dim * normalized_dims.back() >=\n+              dimensions.back() >= kMinDimensionToTransposeTiled2 &&\n+              operand_most_minor_dim * dimensions.back() >=\n                   kMinTotalDimensionsToTransposeTiled)) {\n     // TODO(b/415741994): TransposeEmitter is regressing for S4 when the last\n     // dimension is being transposed. The issue seems to be related to bank\n@@ -301,7 +297,7 @@ std::optional<TransposeDescription> GetDescriptionForTiledTransposeEmitter(\n     }\n     int64_t shmem_usage_bytes =\n         CeilOfRatio(kNumShmemBanks * (kNumShmemBanks + 1LL) * bit_width, 8LL);\n-    return TransposeDescription{&hero, normalized_dims, permutation,\n+    return TransposeDescription{&hero, dimensions, permutation,\n                                 shmem_usage_bytes};\n   }\n   return std::nullopt;"
        },
        {
            "sha": "f14ff91e330107d4e51da6c2199992b3cbe24a81",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils_test.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 61,
            "changes": 81,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils_test.cc?ref=a59ffc09ddd3bc77cf3dc669d92b691317b67222",
            "patch": "@@ -82,13 +82,12 @@ TEST_F(IrEmissionUtilsTest, FindTiledLogicalTranspose) {\n HloModule module\n \n ENTRY entry {\n-  p = f32[32,48,64]{2,1,0} parameter(0)\n-  ROOT t = f32[64,32,48]{2,1,0} transpose(p), dimensions={2,0,1}\n+  p = f32[1536,64]{1,0} parameter(0)\n+  ROOT t = f32[64,1536]{1,0} transpose(p), dimensions={1,0}\n }\n )\";\n-  ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                       ParseAndReturnVerifiedModule(hlo));\n-\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo));\n   HloInstruction* tr = module->entry_computation()->root_instruction();\n \n   auto result = GetDescriptionForTiledTransposeEmitter(*tr);\n@@ -103,12 +102,12 @@ TEST_F(IrEmissionUtilsTest, FindTiledLogical102Transpose) {\n HloModule module\n \n ENTRY entry {\n-  p = f32[32,48,1,2]{3,2,1,0} parameter(0)\n-  ROOT t = f32[48,32,1,2]{3,2,1,0} transpose(p), dimensions={1,0,2,3}\n+  p = f32[32,48,2]{2,1,0} parameter(0)\n+  ROOT t = f32[48,32,2]{2,1,0} transpose(p), dimensions={1,0,2}\n }\n )\";\n-  ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                       ParseAndReturnVerifiedModule(hlo));\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo));\n   HloInstruction* tr = module->entry_computation()->root_instruction();\n \n   auto result = GetDescriptionForTiledTransposeEmitter(*tr);\n@@ -413,8 +412,10 @@ fusion {\n   p = f32[32,48,64]{2,1,0} parameter(0)\n   p2 = f32[48,32,64]{2,1,0} parameter(1)\n   t = f32[64,48,32]{2,1,0} transpose(p), dimensions={2,1,0}\n-  t2 = f32[64,48,32]{2,1,0} transpose(p2), dimensions={2,0,1}\n-  ROOT add = f32[64,48,32]{2,1,0} add(t, t2)\n+  bc = f32[1,1536,64]{2,1,0} bitcast(p2)\n+  t2 = f32[1,64,1536]{2,1,0} transpose(bc), dimensions={0,2,1}\n+  bc2 = f32[64,48,32]{2,1,0} bitcast(t2)\n+  ROOT add = f32[64,48,32]{2,1,0} add(t, bc2)\n }\n \n ENTRY main {\n@@ -433,26 +434,6 @@ ENTRY main {\n   EXPECT_EQ(&FindNonTrivialHero(*r), r);\n }\n \n-TEST_F(IrEmissionUtilsTest, FindTiledLogicalTransposeWithGrouping) {\n-  const char* hlo = R\"(\n-HloModule module\n-\n-ENTRY entry {\n-  p = f32[32,32,64]{2,1,0} parameter(0)\n-  ROOT t = f32[64,32,32]{2,1,0} transpose(p), dimensions={2,0,1}\n-}\n-)\";\n-  ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                       ParseAndReturnVerifiedModule(hlo));\n-  HloInstruction* tr = module->entry_computation()->root_instruction();\n-\n-  auto result = GetDescriptionForTiledTransposeEmitter(*tr);\n-  EXPECT_TRUE(result.has_value());\n-  EXPECT_EQ(result->instr, tr);\n-  EXPECT_EQ(result->dimensions, InlinedVector({64, 1024}));\n-  EXPECT_EQ(result->permutation, InlinedVector({1, 0}));\n-}\n-\n TEST_F(IrEmissionUtilsTest, FindNonTrivialHeroOutsideFusion) {\n   const char* hlo = R\"(\n HloModule module\n@@ -552,13 +533,13 @@ TEST_F(IrEmissionUtilsTest, FindTiledLogicalTransposeOneSwapDimIsSmall) {\n HloModule module\n \n fusion {\n-  p = f32[100,11,12,8]{3,2,1,0} parameter(0)\n-  ROOT t = f32[8,12,100,11]{3,2,1,0} transpose(p), dimensions={3,2,0,1}\n+  p = f32[1100,12,8]{2,1,0} parameter(0)\n+  ROOT t = f32[8,12,1100]{2,1,0} transpose(p), dimensions={2,1,0}\n }\n \n ENTRY main {\n-  param = f32[100,11,12,8]{3,2,1,0} parameter(0)\n-  ROOT fusion = f32[8,12,100,11]{3,2,1,0} fusion(param), kind=kInput, calls=fusion\n+  param = f32[1100,12,8]{2,1,0} parameter(0)\n+  ROOT fusion = f32[8,12,1100]{2,1,0} fusion(param), kind=kInput, calls=fusion\n }\n )\";\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n@@ -578,13 +559,13 @@ TEST_F(IrEmissionUtilsTest, FindTiledLogicalTransposeOtherSwapDimIsSmall) {\n HloModule module\n \n fusion {\n-  p = f32[8,12,100,11]{3,2,1,0} parameter(0)\n-  ROOT t = f32[100,11,12,8]{3,2,1,0} transpose(p), dimensions={2,3,1,0}\n+  p = f32[8,12,1100]{2,1,0} parameter(0)\n+  ROOT t = f32[1100,12,8]{2,1,0} transpose(p), dimensions={2,1,0}\n }\n \n ENTRY main {\n-  param = f32[8,12,100,11]{3,2,1,0} parameter(0)\n-  ROOT fusion = f32[100,11,12,8]{3,2,1,0} fusion(param), kind=kInput, calls=fusion\n+  param = f32[8,12,1100]{2,1,0} parameter(0)\n+  ROOT fusion = f32[1100,12,8]{2,1,0} fusion(param), kind=kInput, calls=fusion\n }\n )\";\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n@@ -599,28 +580,6 @@ ENTRY main {\n   EXPECT_EQ(result->permutation, InlinedVector({2, 1, 0}));\n }\n \n-TEST_F(IrEmissionUtilsTest,\n-       FindTiledLogicalTransposeWithSize1DimensionInRawShape) {\n-  const char* hlo = R\"(\n-HloModule module\n-\n-ENTRY entry {\n-  p = f32[32,1,16,2]{3,2,1,0} parameter(0)\n-  ROOT t = f32[16,1,32,2]{3,2,1,0} transpose(p), dimensions={2,1,0,3}\n-}\n-)\";\n-  ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                       ParseAndReturnVerifiedModule(hlo));\n-\n-  HloInstruction* tr = module->entry_computation()->root_instruction();\n-\n-  auto result = GetDescriptionForTiledTransposeEmitter(*tr);\n-  EXPECT_TRUE(result.has_value());\n-  EXPECT_EQ(result->instr, tr);\n-  EXPECT_EQ(result->dimensions, InlinedVector({16, 32, 2}));\n-  EXPECT_EQ(result->permutation, InlinedVector({1, 0, 2}));\n-}\n-\n TEST_F(IrEmissionUtilsTest, IsContiguousSlice) {\n   const char* hlo = R\"(\n HloModule module"
        },
        {
            "sha": "6d81b8c935ec175642dc32ac5b8a9ee52facba94",
            "filename": "third_party/xla/xla/service/gpu/model/coalescing_analysis_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis_test.cc?ref=a59ffc09ddd3bc77cf3dc669d92b691317b67222",
            "patch": "@@ -177,13 +177,13 @@ TEST_F(CoalescingTest, Transpose) {\n     HloModule module\n \n     fusion {\n-      %input = f32[100, 64, 32] parameter(0)\n-      ROOT transpose = f32[32, 100, 64] transpose(%input), dimensions={2, 0, 1}\n+      %input = f32[1, 6400, 32] parameter(0)\n+      ROOT transpose = f32[1, 32, 6400] transpose(%input), dimensions={0, 2, 1}\n     }\n \n     ENTRY entry {\n-      %input = f32[100, 64, 32] parameter(0)\n-      ROOT %fusion = f32[32, 100, 64] fusion(%input), kind=kLoop, calls=fusion\n+      %input = f32[1, 6400, 32] parameter(0)\n+      ROOT %fusion = f32[1, 32, 6400] fusion(%input), kind=kLoop, calls=fusion\n   })\";\n   // thread_x to linearized input mapping for thread_x in [0, 31]:\n   // Operand 1:  (thread_x)[s0] -> (thread_x + s0 * 128) for s0 in [0, 7]\n@@ -258,15 +258,15 @@ TEST_F(CoalescingTest, TransposeOfBroadcastHeuristic) {\n     HloModule module\n \n     fusion {\n-      input = f32[32, 100, 64] parameter(0)\n-      ROOT slice = f32[32, 100, 1] slice(input), slice={[0:32:1], [0:100:1], [0:1:1]}\n+      input = f32[1, 32, 6400] parameter(0)\n+      ROOT slice = f32[1, 32, 100] slice(input), slice={[0:1:1], [0:32:1], [0:6400:64]}\n     }\n \n     ENTRY entry {\n       p0 = f32[32] parameter(0)\n-      broadcast = f32[100, 64, 32] broadcast(p0), dimensions={2}\n-      transpose = f32[32, 100, 64] transpose(broadcast), dimensions={2, 0, 1}\n-      ROOT %fusion = f32[32, 100, 1] fusion(transpose), kind=kLoop, calls=fusion\n+      broadcast = f32[1, 6400, 32] broadcast(p0), dimensions={2}\n+      transpose = f32[1, 32, 6400] transpose(broadcast), dimensions={0, 2, 1}\n+      ROOT %fusion = f32[1, 32, 100] fusion(transpose), kind=kLoop, calls=fusion\n   })\";\n   EXPECT_TRUE(IsReadCoalescedHeuristic(ir));\n }"
        },
        {
            "sha": "0b702ad9af2acb8dcd142f5d635aff78591af3a8",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=a59ffc09ddd3bc77cf3dc669d92b691317b67222",
            "patch": "@@ -210,11 +210,11 @@ cc_library(\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n-        \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:IR\",\n     ],"
        },
        {
            "sha": "24a469ff6c3d8fc85f5ba1a764fd676f8f079b33",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_norm_rewriter_test.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 13,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_norm_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_norm_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_norm_rewriter_test.cc?ref=a59ffc09ddd3bc77cf3dc669d92b691317b67222",
            "patch": "@@ -287,7 +287,7 @@ TEST_F(CudnnNormRewriterTest, LayerNorm4D2) {\n \n ; CHECK-LABEL: ENTRY %test ({{.*}}: f32[2,4,6,8], {{.*}}: f32[6], {{.*}}: f32[6]) -> f32[2,4,6,8] {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[2,4,6,8]{3,2,1,0} parameter(0)\n-; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[2,4,8,6]{3,2,1,0} fusion([[P0]])\n+; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[8,8,6]{2,1,0} fusion([[P0]])\n ; CHECK-NEXT:    [[P0_BITCAST:%[^ ]+]] = f32[64,6,1,1]{3,2,1,0} bitcast([[TRANSPOSE]])\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[6]{0} parameter(1)\n ; CHECK-NEXT:    [[P1_BITCAST:%[^ ]+]] = f32[1,6,1,1]{3,2,1,0} bitcast([[P1]])\n@@ -299,7 +299,8 @@ TEST_F(CudnnNormRewriterTest, LayerNorm4D2) {\n ; CHECK-DAG:         \"epsilon\":0.001\n ; CHECK:           }\n ; CHECK-NEXT:    [[GTE:%[^ ]+]] = f32[64,6,1,1]{3,2,1,0} get-tuple-element([[CC]]), index=0\n-; CHECK-NEXT:  ROOT {{.*}} = f32[2,4,6,8]{3,2,1,0} fusion([[GTE]]), kind={{.*}}, calls=[[FUSED_COMPUTATION:%[^ ]+]]\n+; CHECK-NEXT:    [[FUSION:%[^ ]+]] = f32[8,6,8]{2,1,0} fusion([[GTE]]), kind={{.*}}, calls=[[FUSED_COMPUTATION:%[^ ]+]]\n+; CHECK-NEXT:  ROOT {{.*}} = f32[2,4,6,8]{3,2,1,0} bitcast([[FUSION]])\n   )\";\n \n   TestNorm(hlo_text, optimized_hlo);\n@@ -347,7 +348,7 @@ TEST_F(CudnnNormRewriterTest, LayerNorm4D2Degenerate1) {\n \n ; CHECK-LABEL: ENTRY %test ({{.*}}: f32[2,1,6,8], {{.*}}: f32[6], {{.*}}: f32[6]) -> f32[2,1,6,8] {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[2,1,6,8]{3,2,1,0} parameter(0)\n-; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[1,2,8,6]{3,2,1,0} fusion([[P0]])\n+; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[2,8,6]{2,1,0} fusion([[P0]])\n ; CHECK-NEXT:    [[P0_BITCAST:%[^ ]+]] = f32[16,6,1,1]{3,2,1,0} bitcast([[TRANSPOSE]])\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[6]{0} parameter(1)\n ; CHECK-NEXT:    [[P1_BITCAST:%[^ ]+]] = f32[1,6,1,1]{3,2,1,0} bitcast([[P1]])\n@@ -359,7 +360,8 @@ TEST_F(CudnnNormRewriterTest, LayerNorm4D2Degenerate1) {\n ; CHECK-DAG:         \"epsilon\":0.001\n ; CHECK:           }\n ; CHECK-NEXT:    [[GTE:%[^ ]+]] = f32[16,6,1,1]{3,2,1,0} get-tuple-element([[CC]]), index=0\n-; CHECK-NEXT:  ROOT {{.*}} = f32[2,1,6,8]{3,2,1,0} fusion([[GTE]]), kind={{.*}}, calls=[[FUSED_COMPUTATION:%[^ ]+]]\n+; CHECK-NEXT:    [[FUSION:%[^ ]+]] = f32[2,6,8]{2,1,0} fusion([[GTE]]), kind={{.*}}, calls=[[FUSED_COMPUTATION:%[^ ]+]]\n+; CHECK-NEXT:  ROOT {{.*}} = f32[2,1,6,8]{3,2,1,0} bitcast([[FUSION]])\n   )\";\n \n   TestNorm(hlo_text, optimized_hlo);\n@@ -407,7 +409,7 @@ TEST_F(CudnnNormRewriterTest, LayerNorm4D12) {\n \n ; CHECK-LABEL: ENTRY %test ({{.*}}: f32[2,4,6,8], {{.*}}: f32[4,6], {{.*}}: f32[4,6]) -> f32[2,4,6,8] {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[2,4,6,8]{3,2,1,0} parameter(0)\n-; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[2,8,4,6]{3,2,1,0} fusion([[P0]])\n+; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[2,8,24]{2,1,0} fusion([[P0]])\n ; CHECK-NEXT:    [[P0_BITCAST:%[^ ]+]] = f32[16,4,6,1]{3,2,1,0} bitcast([[TRANSPOSE]])\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[4,6]{1,0} parameter(1)\n ; CHECK-NEXT:    [[P1_BITCAST:%[^ ]+]] = f32[1,4,6,1]{3,2,1,0} bitcast([[P1]])\n@@ -419,7 +421,8 @@ TEST_F(CudnnNormRewriterTest, LayerNorm4D12) {\n ; CHECK-DAG:         \"epsilon\":0.001\n ; CHECK:           }\n ; CHECK-NEXT:    [[GTE:%[^ ]+]] = f32[16,4,6,1]{3,2,1,0} get-tuple-element([[CC]]), index=0\n-; CHECK-NEXT:  ROOT {{.*}} = f32[2,4,6,8]{3,2,1,0} fusion([[GTE]]), kind={{.*}}, calls=[[FUSED_COMPUTATION:%[^ ]+]]\n+; CHECK-NEXT:    [[FUSION:%[^ ]+]] = f32[2,24,8]{2,1,0} fusion([[GTE]]), kind={{.*}}, calls=[[FUSED_COMPUTATION:%[^ ]+]]\n+; CHECK-NEXT:  ROOT {{.*}} = f32[2,4,6,8]{3,2,1,0} bitcast([[FUSION]])\n   )\";\n \n   TestNorm(hlo_text, optimized_hlo);\n@@ -467,7 +470,7 @@ TEST_F(CudnnNormRewriterTest, LayerNorm4D12Degenerate2) {\n \n ; CHECK-LABEL: ENTRY %test ({{.*}}: f32[2,4,1,8], {{.*}}: f32[4,1], {{.*}}: f32[4,1]) -> f32[2,4,1,8] {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[2,4,1,8]{3,2,1,0} parameter(0)\n-; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[1,2,8,4]{3,2,1,0} fusion([[P0]])\n+; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[2,8,4]{2,1,0} fusion([[P0]])\n ; CHECK-NEXT:    [[P0_BITCAST:%[^ ]+]] = f32[16,4,1,1]{3,2,1,0} bitcast([[TRANSPOSE]])\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[4,1]{1,0} parameter(1)\n ; CHECK-NEXT:    [[P1_BITCAST:%[^ ]+]] = f32[1,4,1,1]{3,2,1,0} bitcast([[P1]])\n@@ -479,7 +482,8 @@ TEST_F(CudnnNormRewriterTest, LayerNorm4D12Degenerate2) {\n ; CHECK-DAG:         \"epsilon\":0.001\n ; CHECK:           }\n ; CHECK-NEXT:    [[GTE:%[^ ]+]] = f32[16,4,1,1]{3,2,1,0} get-tuple-element([[CC]]), index=0\n-; CHECK-NEXT:  ROOT {{.*}} = f32[2,4,1,8]{3,2,1,0} fusion([[GTE]]), kind={{.*}}, calls=[[FUSED_COMPUTATION:%[^ ]+]]\n+; CHECK-NEXT:    [[FUSION:%[^ ]+]] = f32[2,4,8]{2,1,0} fusion([[GTE]]), kind={{.*}}, calls=[[FUSED_COMPUTATION:%[^ ]+]]\n+; CHECK-NEXT:  ROOT {{.*}} = f32[2,4,1,8]{3,2,1,0} bitcast([[FUSION]])\n   )\";\n \n   TestNorm(hlo_text, optimized_hlo);\n@@ -821,7 +825,7 @@ TEST_F(CudnnNormRewriterTest, LayerNormTrain4D12) {\n \n ; CHECK-LABEL: ENTRY %test ({{.*}}: f32[2,4,6,8], {{.*}}: f32[4,6], {{.*}}: f32[4,6]) -> (f32[2,4,6,8], f32[2,8], f32[2,8], f32[2,8]) {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[2,4,6,8]{3,2,1,0} parameter(0)\n-; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[2,8,4,6]{3,2,1,0} fusion([[P0]])\n+; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[2,8,24]{2,1,0} fusion([[P0]])\n ; CHECK-NEXT:    [[P0_BITCAST:%[^ ]+]] = f32[16,4,6,1]{3,2,1,0} bitcast([[TRANSPOSE]])\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[4,6]{1,0} parameter(1)\n ; CHECK-NEXT:    [[P1_BITCAST:%[^ ]+]] = f32[1,4,6,1]{3,2,1,0} bitcast([[P1]])\n@@ -881,7 +885,7 @@ TEST_F(CudnnNormRewriterTest, LayerNormTrain4D12Degenerate2) {\n \n ; CHECK-LABEL: ENTRY %test ({{.*}}: f32[2,4,1,8], {{.*}}: f32[4,1], {{.*}}: f32[4,1]) -> (f32[2,4,1,8], f32[2,8], f32[2,8], f32[2,8]) {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[2,4,1,8]{3,2,1,0} parameter(0)\n-; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[1,2,8,4]{3,2,1,0} fusion([[P0]])\n+; CHECK-NEXT:    [[TRANSPOSE:%[^ ]+]] = f32[2,8,4]{2,1,0} fusion([[P0]])\n ; CHECK-NEXT:    [[P0_BITCAST:%[^ ]+]] = f32[16,4,1,1]{3,2,1,0} bitcast([[TRANSPOSE]])\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[4,1]{1,0} parameter(1)\n ; CHECK-NEXT:    [[P1_BITCAST:%[^ ]+]] = f32[1,4,1,1]{3,2,1,0} bitcast([[P1]])\n@@ -1177,7 +1181,7 @@ TEST_F(CudnnNormRewriterTest, LayerNormTrainBackward4D2) {\n \n ; CHECK-LABEL: ENTRY %test ({{.*}}: f32[2,4,6,8], {{.*}}: f32[6], {{.*}}: f32[6], {{.*}}: f32[2,4,6,8]) -> (f32[2,4,6,8], f32[2,4,6,8], f32[6], f32[6]) {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[2,4,6,8]{3,2,1,0} parameter(0)\n-; CHECK-NEXT:    [[TRANSPOSE0:%[^ ]+]] = f32[2,4,8,6]{3,2,1,0} fusion([[P0]])\n+; CHECK-NEXT:    [[TRANSPOSE0:%[^ ]+]] = f32[8,8,6]{2,1,0} fusion([[P0]])\n ; CHECK-NEXT:    [[P0_BITCAST:%[^ ]+]] = f32[64,6,1,1]{3,2,1,0} bitcast([[TRANSPOSE0]])\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[6]{0} parameter(1)\n ; CHECK-NEXT:    [[P1_BITCAST:%[^ ]+]] = f32[1,6,1,1]{3,2,1,0} bitcast([[P1]])\n@@ -1270,7 +1274,7 @@ TEST_F(CudnnNormRewriterTest, LayerNormTrainBackward4D12) {\n \n ; CHECK-LABEL: ENTRY %test ({{.*}}: f32[2,4,6,8], {{.*}}: f32[4,6], {{.*}}: f32[4,6], {{.*}}: f32[2,4,6,8]) -> (f32[2,4,6,8], f32[2,4,6,8], f32[4,6], f32[4,6]) {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[2,4,6,8]{3,2,1,0} parameter(0)\n-; CHECK-NEXT:    [[TRANSPOSE0:%[^ ]+]] = f32[2,8,4,6]{3,2,1,0} fusion([[P0]])\n+; CHECK-NEXT:    [[TRANSPOSE0:%[^ ]+]] = f32[2,8,24]{2,1,0} fusion([[P0]])\n ; CHECK-NEXT:    [[P0_BITCAST:%[^ ]+]] = f32[16,4,6,1]{3,2,1,0} bitcast([[TRANSPOSE0]])\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[4,6]{1,0} parameter(1)\n ; CHECK-NEXT:    [[P1_BITCAST:%[^ ]+]] = f32[1,4,6,1]{3,2,1,0} bitcast([[P1]])\n@@ -1363,7 +1367,7 @@ TEST_F(CudnnNormRewriterTest, LayerNormTrainBackward4D12Degenerate2) {\n \n ; CHECK-LABEL: ENTRY %test ({{.*}}: f32[2,4,1,8], {{.*}}: f32[4,1], {{.*}}: f32[4,1], {{.*}}: f32[2,4,1,8]) -> (f32[2,4,1,8], f32[2,4,1,8], f32[4,1], f32[4,1]) {\n ; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[2,4,1,8]{3,2,1,0} parameter(0)\n-; CHECK-NEXT:    [[TRANSPOSE0:%[^ ]+]] = f32[1,2,8,4]{3,2,1,0} fusion([[P0]])\n+; CHECK-NEXT:    [[TRANSPOSE0:%[^ ]+]] = f32[2,8,4]{2,1,0} fusion([[P0]])\n ; CHECK-NEXT:    [[P0_BITCAST:%[^ ]+]] = f32[16,4,1,1]{3,2,1,0} bitcast([[TRANSPOSE0]])\n ; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[4,1]{1,0} parameter(1)\n ; CHECK-NEXT:    [[P1_BITCAST:%[^ ]+]] = f32[1,4,1,1]{3,2,1,0} bitcast([[P1]])"
        },
        {
            "sha": "3f510f056458146db296b8adcd1d92f125ddc93f",
            "filename": "third_party/xla/xla/service/gpu/transforms/fusion_block_level_rewriter.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 18,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter.cc?ref=a59ffc09ddd3bc77cf3dc669d92b691317b67222",
            "patch": "@@ -20,13 +20,13 @@ limitations under the License.\n #include <variant>\n \n #include \"absl/container/flat_hash_set.h\"\n-#include \"absl/container/inlined_vector.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_join.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n #include \"llvm/Support/MathExtras.h\"\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n@@ -44,7 +44,6 @@ limitations under the License.\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/instruction_fusion.h\"\n #include \"xla/service/pattern_matcher.h\"\n-#include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -79,28 +78,20 @@ bool ShouldRewriteLoopTransposeFusion(\n   // is neither the minormost nor the second minormost dimension in the output,\n   // and the output minormost dimension is swapped with the new minormost\n   // dimension.\n+  int64_t rank = root->shape().dimensions().size();\n \n-  // We use the normalized logical transpose shape so it should be enough to\n-  // check that the minormost dimension's index within the result is smaller\n-  // than rank - 2, and that the new minormost dimension is swapped with it.\n-  absl::InlinedVector<int64_t, 3> permutation;\n-  auto normalized_dims_or = ShapeUtil::GetNormalizedLogicalTransposeShape(\n-      root->operand(0)->shape(), root->shape(), root->dimensions(),\n-      permutation);\n-  if (!normalized_dims_or.ok()) {\n-    return false;\n-  }\n-  auto normalized_dims = normalized_dims_or.value();\n-  int64_t rank = normalized_dims.size();\n-\n+  // The transpose dimension grouper has run, so it should be enough to check\n+  // that the minormost dimension's index within the result is smaller than\n+  // rank - 2, and that the new minormost dimension is swapped with it.\n   // This only triggers for transposes with major-to-minor layout.\n   bool has_major_to_minor_layout =\n       LayoutUtil::IsMonotonicWithDim0Major(root->shape().layout());\n-  int64_t result_minormost_dim_in_operand = permutation.back();\n+  absl::Span<int64_t const> transpose_dimensions = root->dimensions();\n+  int64_t result_minormost_dim_in_operand = transpose_dimensions.back();\n \n   if (!(has_major_to_minor_layout &&\n-        permutation[result_minormost_dim_in_operand] == rank - 1 &&\n-        permutation[rank - 1] < rank - 2)) {\n+        transpose_dimensions[result_minormost_dim_in_operand] == rank - 1 &&\n+        transpose_dimensions[rank - 1] < rank - 2)) {\n     return false;\n   }\n "
        },
        {
            "sha": "dfe161f86da219f155161f9f9d4a3e251d93c5af",
            "filename": "third_party/xla/xla/service/gpu/transforms/fusion_block_level_rewriter_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Ffusion_block_level_rewriter_test.cc?ref=a59ffc09ddd3bc77cf3dc669d92b691317b67222",
            "patch": "@@ -211,38 +211,6 @@ ENTRY entry  {\n   EXPECT_TRUE(HasTritonBlockLevelFusionConfig(root));\n }\n \n-TEST_F(FusionBlockLevelRewriterTest,\n-       RewritesLoopTransposeFusionWithSplitDimensions) {\n-  // This test checks if the rewriter can handle a transpose where dimensions\n-  // are split in the HLO but logically contiguous.\n-  // Logical shape: [100, 200, 300] -> [300, 200, 100] (Swap dim 0 and 2).\n-  // Physical shape: [100, 200, 10, 30] -> [10, 30, 200, 100].\n-  // The normalized logical transpose shape should recover the logical swap.\n-  const absl::string_view hlo_text = R\"(\n-fusion_computation {\n-  p0 = f32[100,200,10,30] parameter(0)\n-  ROOT transpose = f32[10,30,200,100] transpose(p0), dimensions={2,3,1,0}\n-}\n-\n-ENTRY entry {\n-  p0 = f32[100,200,10,30] parameter(0)\n-  ROOT fusion = f32[10,30,200,100] fusion(p0), kind=kLoop,\n-    calls=fusion_computation\n-})\";\n-  ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                       ParseAndReturnVerifiedModule(hlo_text));\n-\n-  EXPECT_THAT(\n-      FusionBlockLevelRewriter(device_info_, HloCostAnalysis::DefaultShapeSize,\n-                               &mlir_context_)\n-          .Run(module.get()),\n-      absl_testing::IsOkAndHolds(true));\n-  const HloInstruction* root = module->entry_computation()->root_instruction();\n-  EXPECT_EQ(root->opcode(), HloOpcode::kFusion);\n-  EXPECT_EQ(root->fusion_kind(), HloInstruction::FusionKind::kCustom);\n-  EXPECT_TRUE(HasTritonBlockLevelFusionConfig(root));\n-}\n-\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "0281e68b03e4ba619ebf8dd9bf6e645ed421f7f5",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment_a100.hlo",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_a100.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_a100.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_a100.hlo?ref=a59ffc09ddd3bc77cf3dc669d92b691317b67222",
            "patch": "@@ -1,9 +1,10 @@\n // RUN: hlo-opt %s --platform=gpu --stage=hlo --xla_gpu_target_config_filename=%S/../../../backends/gpu/target_config/specs/a100_pcie_80.txtpb --split-input-file | FileCheck %s\n \n-// CHECK: %wrapped_transpose_computation\n+// CHECK: fused_transpose\n // CHECK-NEXT: bf16[3,3,16,32]{3,2,1,0} parameter(0)\n-// CHECK-NEXT: bf16[32,3,3,16]{3,2,1,0} transpose\n-// CHECK-SAME: dimensions={3,0,1,2}\n+// CHECK-NEXT: bf16[144,32]{1,0} bitcast\n+// CHECK-NEXT: bf16[32,144]{1,0} transpose\n+// CHECK-SAME: dimensions={1,0}\n // CHECK: (bf16[1,64,64,32]{3,2,1,0}, u8[0]{0}) custom-call\n // CHECK-SAME: window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\n "
        },
        {
            "sha": "10cc948cf6a288ac298da8c773a5aa4c2420c2fd",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment_h100.hlo",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_h100.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_h100.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_h100.hlo?ref=a59ffc09ddd3bc77cf3dc669d92b691317b67222",
            "patch": "@@ -1,9 +1,10 @@\n // RUN: hlo-opt %s --platform=gpu --stage=hlo --xla_gpu_target_config_filename=%S/../../../backends/gpu/target_config/specs/h100_sxm.txtpb --split-input-file | FileCheck %s\n \n-// CHECK: %wrapped_transpose_computation\n+// CHECK: fused_transpose\n // CHECK-NEXT: f8e4m3fn[3,3,16,32]{3,2,1,0} parameter(0)\n-// CHECK-NEXT: f8e4m3fn[32,3,3,16]{3,2,1,0} transpose\n-// CHECK-SAME: dimensions={3,0,1,2}\n+// CHECK-NEXT: f8e4m3fn[144,32]{1,0} bitcast\n+// CHECK-NEXT: f8e4m3fn[32,144]{1,0} transpose\n+// CHECK-SAME: dimensions={1,0}\n // CHECK: (f8e4m3fn[1,64,64,32]{3,2,1,0}, u8[0]{0}) custom-call\n // CHECK-SAME: window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\n "
        },
        {
            "sha": "5ae06c318a1cf99159460e97fb6e5f7cb6a78c36",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment_v100.hlo",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_v100.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_v100.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_v100.hlo?ref=a59ffc09ddd3bc77cf3dc669d92b691317b67222",
            "patch": "@@ -1,9 +1,10 @@\n // RUN: hlo-opt %s --platform=gpu --stage=hlo --xla_gpu_target_config_filename=%S/../../../backends/gpu/target_config/specs/v100.txtpb --split-input-file | FileCheck %s\n \n-// CHECK: %wrapped_transpose_computation\n+// CHECK: fused_transpose\n // CHECK-NEXT: f16[3,3,16,32]{3,2,1,0} parameter(0)\n-// CHECK-NEXT: f16[32,3,3,16]{3,2,1,0} transpose\n-// CHECK-SAME: dimensions={3,0,1,2}\n+// CHECK-NEXT: f16[144,32]{1,0} bitcast\n+// CHECK-NEXT: f16[32,144]{1,0} transpose\n+// CHECK-SAME: dimensions={1,0}\n // CHECK: (f16[1,64,64,32]{3,2,1,0}, u8[0]{0}) custom-call\n // CHECK-SAME: window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\n "
        },
        {
            "sha": "a9e0f6dee030c24c8f65ae53f6cba92284ffd663",
            "filename": "third_party/xla/xla/shape_util.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fshape_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fshape_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fshape_util.cc?ref=a59ffc09ddd3bc77cf3dc669d92b691317b67222",
            "patch": "@@ -2379,7 +2379,6 @@ absl::InlinedVector<int64_t, 3> GetNormalizedTransposeShapeHelper(\n       normalized_shape.dimensions().begin(),\n       normalized_shape.dimensions().end());\n   if (segments.size() == 1) {\n-    permutation.push_back(0);\n     return normalized_dims;\n   }\n   // Derive the permutation from the segments."
        },
        {
            "sha": "8d3cedb6d1d21a16dc68fcc73d1981c30cab79e2",
            "filename": "third_party/xla/xla/shape_util_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fshape_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a59ffc09ddd3bc77cf3dc669d92b691317b67222/third_party%2Fxla%2Fxla%2Fshape_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fshape_util_test.cc?ref=a59ffc09ddd3bc77cf3dc669d92b691317b67222",
            "patch": "@@ -1815,20 +1815,7 @@ TEST(ShapeUtilTest, GetNormalizedLogicalTransposeShape_NoTranspose) {\n                            input_shape, output_shape, dimensions, permutation));\n \n   EXPECT_THAT(normalized_shape, ElementsAre(8192));\n-  EXPECT_THAT(permutation, ElementsAre(0));\n-}\n-\n-TEST(ShapeUtilTest, GetNormalizedLogicalTransposeShape_IdentityWithMerges) {\n-  Shape output_shape = ShapeUtil::MakeShape(F32, {10, 20});\n-  Shape input_shape = ShapeUtil::MakeShape(F32, {20, 10});\n-  // Identity transpose that allows merging dimensions.\n-  absl::InlinedVector<int64_t, 3> dimensions = {0, 1};\n-  absl::InlinedVector<int64_t, 3> permutation;\n-  ASSERT_OK_AND_ASSIGN(auto normalized_shape,\n-                       ShapeUtil::GetNormalizedLogicalTransposeShape(\n-                           input_shape, output_shape, dimensions, permutation));\n-  EXPECT_THAT(normalized_shape, ElementsAre(200));\n-  EXPECT_THAT(permutation, ElementsAre(0));\n+  EXPECT_THAT(permutation, IsEmpty());\n }\n \n TEST(ShapeUtilTest, GetNormalizedLogicalTransposeShape_Simple2D) {"
        }
    ],
    "stats": {
        "total": 388,
        "additions": 141,
        "deletions": 247
    }
}