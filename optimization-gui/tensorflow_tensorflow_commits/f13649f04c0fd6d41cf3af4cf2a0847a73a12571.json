{
    "author": "dimitar-asenov",
    "message": "Reverts 4bf20e19f440a47a276cf1988b48683778f673a6\n\nPiperOrigin-RevId: 836359130",
    "sha": "f13649f04c0fd6d41cf3af4cf2a0847a73a12571",
    "files": [
        {
            "sha": "00dfc9ebb31e9e7761daf560f3aeeb31e7510028",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=f13649f04c0fd6d41cf3af4cf2a0847a73a12571",
            "patch": "@@ -69,7 +69,6 @@ cc_library(\n         \":collective_broadcast_thunk\",\n         \":collective_permute_thunk\",\n         \":collective_thunk\",\n-        \":convolution_thunk\",\n         \":copy_thunk\",\n         \":custom_call_thunk\",\n         \":dynamic_slice_thunk\",\n@@ -3002,7 +3001,6 @@ xla_test(\n         \":command_buffer_conversion_pass\",\n         \":command_buffer_thunk\",\n         \":conditional_thunk\",\n-        \":convolution_thunk\",\n         \":copy_thunk\",\n         \":cudnn_thunk\",\n         \":custom_call_thunk\","
        },
        {
            "sha": "e4e003aee2e047d2e61bd78a25b1495bd6e67104",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 47,
            "changes": 47,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc?ref=f13649f04c0fd6d41cf3af4cf2a0847a73a12571",
            "patch": "@@ -1804,53 +1804,6 @@ CommandBufferCmd::BufferUseVector CublasLtCmd::buffers() const {\n   return buffer_usage;\n }\n \n-//===----------------------------------------------------------------------===//\n-// ConvolutionCmd\n-//===----------------------------------------------------------------------===//\n-\n-ConvolutionCmd::ConvolutionCmd(const ConvolutionThunk& thunk)\n-    : TracedCommandBufferCmd(CommandBufferCmdType::kConvolutionCmd),\n-      operand_buffers_(thunk.operand_buffers_),\n-      result_buffers_(thunk.result_buffers_),\n-      scratch_buffer_(thunk.scratch_buffer_),\n-      config_(thunk.config_) {}\n-\n-absl::Status ConvolutionCmd::Initialize(const Thunk::InitializeParams& params,\n-                                        StateManager& state) {\n-  // populate cache of ConvRunner\n-  cache_.GetOrCreate(config_, params.stream);\n-  return absl::OkStatus();\n-}\n-\n-absl::StatusOr<const se::CommandBuffer::Command*> ConvolutionCmd::Record(\n-    const Thunk::ExecuteParams& execute_params,\n-    const RecordParams& record_params, RecordAction record_action,\n-    se::CommandBuffer* command_buffer) {\n-  VLOG(5) << \"ConvolutionCmd\";\n-\n-  return RecordTracedCommand(\n-      execute_params, record_params, std::move(record_action), command_buffer,\n-      [&](se::Stream* stream) {\n-        return RunConvolutionOnStream(execute_params, operand_buffers_,\n-                                      result_buffers_, scratch_buffer_, config_,\n-                                      cache_, stream);\n-      });\n-}\n-\n-CommandBufferCmd::BufferUseVector ConvolutionCmd::buffers() const {\n-  BufferUseVector buffer_usage;\n-  buffer_usage.reserve(operand_buffers_.size() + result_buffers_.size() + 1);\n-\n-  for (BufferAllocation::Slice buffer : operand_buffers_) {\n-    buffer_usage.push_back({buffer, MemoryAccess::kRead});\n-  }\n-  for (BufferAllocation::Slice buffer : result_buffers_) {\n-    buffer_usage.push_back({buffer, MemoryAccess::kWrite});\n-  }\n-  buffer_usage.push_back({scratch_buffer_, MemoryAccess::kWrite});\n-  return buffer_usage;\n-}\n-\n //===----------------------------------------------------------------------===//\n // CuDnnCmd\n //===----------------------------------------------------------------------===//"
        },
        {
            "sha": "6bd03ebda93ccd076e686515e9095c84321a1f48",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.h",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h?ref=f13649f04c0fd6d41cf3af4cf2a0847a73a12571",
            "patch": "@@ -39,7 +39,6 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/runtime/collective_permute_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n-#include \"xla/backends/gpu/runtime/convolution_thunk.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n #include \"xla/backends/gpu/runtime/dynamic_slice_thunk.h\"\n@@ -84,7 +83,6 @@ namespace xla::gpu {\n   V(kLaunchCmd, \"LaunchCmd\")                                     \\\n   V(kCustomKernelLaunchCmd, \"CustomKernelLaunchCmd\")             \\\n   V(kCublasLtCmd, \"CublasLtCmd\")                                 \\\n-  V(kConvolutionCmd, \"ConvolutionCmd\")                           \\\n   V(kCuDnnCmd, \"CuDnnCmd\")                                       \\\n   V(kGemmCmd, \"GemmCmd\")                                         \\\n   V(kMemcpyDeviceToDeviceCmd, \"MemcpyDeviceToDeviceCmd\")         \\\n@@ -963,34 +961,6 @@ class CublasLtCmd : public TracedCommandBufferCmd, public CublasLtMatmulThunk {\n   bool IsNestedCommandBuffer() const final { return true; }\n };\n \n-//===----------------------------------------------------------------------===//\n-// ConvolutionCmd\n-//===----------------------------------------------------------------------===//\n-\n-class ConvolutionCmd : public TracedCommandBufferCmd {\n- public:\n-  ConvolutionCmd(const ConvolutionThunk& conv_thunk);\n-\n-  absl::Status Initialize(const Thunk::InitializeParams& params,\n-                          StateManager& state) override;\n-\n-  absl::StatusOr<const se::CommandBuffer::Command*> Record(\n-      const Thunk::ExecuteParams& execute_params,\n-      const RecordParams& record_params, RecordAction record_action,\n-      se::CommandBuffer* command_buffer) override;\n-\n-  BufferUseVector buffers() const override;\n-\n-  bool IsNestedCommandBuffer() const final { return true; }\n-\n- private:\n-  std::vector<BufferAllocation::Slice> operand_buffers_;\n-  std::vector<BufferAllocation::Slice> result_buffers_;\n-  BufferAllocation::Slice scratch_buffer_;\n-  GpuConvConfig config_;\n-  ConvRunnerCache cache_;\n-};\n-\n //===----------------------------------------------------------------------===//\n // CuDnnCmd\n //===----------------------------------------------------------------------===//"
        },
        {
            "sha": "a4532883b8215dde8aa36b0d6f5eac15acfe4229",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd_emitter.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc?ref=f13649f04c0fd6d41cf3af4cf2a0847a73a12571",
            "patch": "@@ -244,10 +244,6 @@ static absl::StatusOr<Command> Convert(const CuDnnThunk& thunk) {\n   return std::make_unique<CuDnnCmd>(thunk.arguments(), thunk.graph());\n }\n \n-static absl::StatusOr<Command> Convert(const ConvolutionThunk& thunk) {\n-  return std::make_unique<ConvolutionCmd>(thunk);\n-}\n-\n //===----------------------------------------------------------------------===//\n static absl::StatusOr<Command> CopyMetadata(absl::StatusOr<Command> cmd,\n                                             const Thunk& thunk) {\n@@ -319,8 +315,6 @@ static absl::Status AppendCommands(CommandBufferCmdSequence& cmd_sequence,\n       return append(Convert<WhileThunk>(thunk, options));\n     case Thunk::Kind::kCuDnn:\n       return append(Convert<CuDnnThunk>(thunk));\n-    case Thunk::Kind::kConvolution:\n-      return append(Convert<ConvolutionThunk>(thunk));\n     case Thunk::Kind::kDynamicSlice:\n       return append(Convert<DynamicSliceThunk>(thunk, options));\n "
        },
        {
            "sha": "f6ef881b0966e3b2f4a50d31b58590403c4fd40d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_conversion_pass.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc?ref=f13649f04c0fd6d41cf3af4cf2a0847a73a12571",
            "patch": "@@ -149,7 +149,6 @@ std::optional<DebugOptions::CommandBufferCmdType> GetCommandBufferCmdType(\n     case Thunk::kSend:\n       return DebugOptions::COLLECTIVES;\n     case Thunk::kCuDnn:\n-    case Thunk::kConvolution:\n       return DebugOptions::CUDNN;\n     case Thunk::kCustomCall:\n       return DebugOptions::CUSTOM_CALL;"
        },
        {
            "sha": "772404cef107d27e07722951e0a3694572374e5f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_conversion_pass_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 95,
            "changes": 95,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass_test.cc?ref=f13649f04c0fd6d41cf3af4cf2a0847a73a12571",
            "patch": "@@ -32,7 +32,6 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/command_buffer_thunk.h\"\n #include \"xla/backends/gpu/runtime/conditional_thunk.h\"\n-#include \"xla/backends/gpu/runtime/convolution_thunk.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/cudnn_thunk.h\"\n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n@@ -153,58 +152,6 @@ std::unique_ptr<GemmThunk> CreateGemmThunk(const BufferAllocation& alloc1) {\n                                      slice1, slice1, slice1, true);\n }\n \n-std::unique_ptr<ConvolutionThunk> CreateConvolutionThunk(\n-    const BufferAllocation& alloc) {\n-  std::vector<BufferAllocation::Slice> operand_slices, result_slices;\n-  for (int i = 0, num = 3; i < num; i++) {\n-    operand_slices.emplace_back(&alloc, i * 16, 16);\n-    result_slices.emplace_back(&alloc, (i + num) * 16, 16);\n-  }\n-\n-  ConvolutionDimensionNumbers dnums;\n-  dnums.set_input_batch_dimension(0);\n-  dnums.set_input_feature_dimension(1);\n-  dnums.add_input_spatial_dimensions(2);\n-  dnums.add_input_spatial_dimensions(3);\n-  dnums.set_kernel_input_feature_dimension(0);\n-  dnums.set_kernel_output_feature_dimension(1);\n-  dnums.add_kernel_spatial_dimensions(2);\n-  dnums.add_kernel_spatial_dimensions(3);\n-  dnums.set_output_batch_dimension(0);\n-  dnums.set_output_feature_dimension(1);\n-  dnums.add_output_spatial_dimensions(2);\n-  dnums.add_output_spatial_dimensions(3);\n-\n-  Window window;\n-  const auto dim0 = window.add_dimensions();\n-  const auto dim1 = window.add_dimensions();\n-  dim0->set_size(4);\n-  dim1->set_size(4);\n-  dim0->set_base_dilation(1);\n-  dim1->set_base_dilation(1);\n-  dim0->set_stride(1);\n-  dim1->set_stride(1);\n-  dim0->set_window_dilation(3);\n-  dim1->set_window_dilation(2);\n-\n-  GpuConvDescriptor desc{\n-      .kind = CudnnConvKind::kForward,\n-      .backend_config = CudnnConvBackendConfig{},\n-      .operand0_shape = ShapeUtil::MakeShape(F32, {60, 38, 17, 13}),\n-      .operand1_shape = ShapeUtil::MakeShapeWithDenseLayout(F32, {38, 10, 4, 4},\n-                                                            {3, 2, 0, 1}),\n-      .result_shape = ShapeUtil::MakeShapeWithType<float>({64, 64, 64, 13}),\n-      .scratch_size = 128 * 1024,\n-      .window = window,\n-      .dnums = dnums,\n-      .feature_group_count = 1};\n-  auto thunk =\n-      ConvolutionThunk::Create(Thunk::ThunkInfo(), desc, operand_slices,\n-                               result_slices, result_slices.back());\n-  TF_CHECK_OK(thunk.status());\n-  return std::move(thunk).value();\n-}\n-\n std::unique_ptr<CollectiveDoneThunk> CreateAllGatherDoneThunk(\n     Thunk* start_thunk) {\n   auto async_events =\n@@ -368,48 +315,6 @@ TEST(CommandBufferConversionPassTest, PartiallyConvertsToCommandBufferThunk) {\n   EXPECT_THAT(thunks_in_command_buffer1, ThunkKindsAre(Thunk::kCopy));\n }\n \n-TEST(CommandBufferConversionPassTest, ConvertConvolutionAndGemmThunks) {\n-  CommandBufferConversionPass pass{\"test\"};\n-\n-  std::vector<std::unique_ptr<Thunk>> thunks;\n-\n-  // Create a {CopyThunk, GemmThunk, ConvolutionThunk}\n-  BufferAllocation alloc0(0, 1024, 0);\n-  BufferAllocation alloc1(1, 2048, 0);\n-  BufferAllocation alloc2(2, 2048, 0);\n-  thunks.push_back(CreateCopyThunk(alloc0));\n-  thunks.push_back(CreateGemmThunk(alloc1));\n-  thunks.push_back(CreateConvolutionThunk(alloc0));\n-\n-  auto root_thunk =\n-      std::make_unique<SequentialThunk>(Thunk::ThunkInfo(), std::move(thunks));\n-  DebugOptions debug_options;\n-\n-  // Enable only FUSION, which means GemmThunk should not be converted.\n-  debug_options.clear_xla_gpu_enable_command_buffer();\n-  debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n-  debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::CUDNN);\n-  debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::CUBLAS);\n-\n-  se::DeviceDescription device_info = TestGpuDeviceInfo::CudaOrRocmDeviceInfo();\n-  FakeErrorAllocator allocator;\n-\n-  ASSERT_EQ(root_thunk->thunks().size(), 3);\n-\n-  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, /*hlo_module=*/nullptr,\n-                       device_info, allocator),\n-              IsOkAndHolds(true));\n-\n-  ASSERT_EQ(root_thunk->thunks().size(), 1);\n-\n-  const auto* command_buffer_thunk =\n-      static_cast<const CommandBufferThunk*>(root_thunk->thunks()[0].get());\n-  const auto& thunks_in_command_buffer =\n-      command_buffer_thunk->thunks()->thunks();\n-  EXPECT_THAT(thunks_in_command_buffer,\n-              ThunkKindsAre(Thunk::kCopy, Thunk::kGemm, Thunk::kConvolution));\n-}\n-\n TEST(CommandBufferConversionPassTest, ConvertsAsyncPairToCommandBuffer) {\n   std::vector<std::unique_ptr<Thunk>> thunks;\n   // Create a start thunk"
        },
        {
            "sha": "004a003aa07172a30d8dac177cdf93084aa1bb86",
            "filename": "third_party/xla/xla/backends/gpu/runtime/convolution_thunk.cc",
            "status": "modified",
            "additions": 33,
            "deletions": 38,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc?ref=f13649f04c0fd6d41cf3af4cf2a0847a73a12571",
            "patch": "@@ -69,81 +69,76 @@ ConvolutionThunk::ConvolutionThunk(\n       descriptor_(std::move(descriptor)),\n       config_(std::move(config)) {}\n \n-std::pair<RunConvOptions, bool> ConvRunnerCache::GetOrCreate(\n-    const GpuConvConfig& config, const se::Stream* stream) {\n+GenericConvRunner& ConvolutionThunk::GetOrCreateRunner(\n+    const stream_executor::Stream* stream, bool* runner_created) {\n   absl::MutexLock lock(mu_);\n-  auto [it, inserted] =\n-      cache_.emplace(stream->parent(), std::unique_ptr<GenericConvRunner>{});\n-  if (inserted) {\n-    it->second = std::make_unique<GenericConvRunner>(config);\n+  auto it = runner_cache_.find(stream);\n+  *runner_created = (it == runner_cache_.end());\n+  if (*runner_created) {\n+    it = runner_cache_\n+             .insert({stream, std::make_unique<GenericConvRunner>(config_)})\n+             .first;\n   }\n-  return std::pair{RunConvOptions{nullptr, it->second.get()}, inserted};\n+  return *it->second;\n }\n \n-absl::Status RunConvolutionOnStream(\n-    const Thunk::ExecuteParams& params,\n-    const std::vector<BufferAllocation::Slice>& operand_buffers,\n-    const std::vector<BufferAllocation::Slice>& result_buffers,\n-    const BufferAllocation::Slice& scratch_buffer, const GpuConvConfig& config,\n-    ConvRunnerCache& cache, se::Stream* stream) {\n+absl::Status ConvolutionThunk::ExecuteOnStream(const ExecuteParams& params) {\n   const auto& buffer_allocations = *params.buffer_allocations;\n \n   std::vector<se::DeviceMemoryBase> operand_se_buffers, result_se_buffers;\n-  operand_se_buffers.reserve(operand_buffers.size());\n-\n-  for (BufferAllocation::Slice buffer : operand_buffers) {\n+  operand_se_buffers.reserve(operand_buffers_.size());\n+  for (BufferAllocation::Slice buffer : operand_buffers_) {\n     operand_se_buffers.push_back(buffer_allocations.GetDeviceAddress(buffer));\n-    VLOG(5) << \"operand buffer: \" << buffer.ToString()\n-            << \" addr: \" << operand_se_buffers.back().opaque();\n   }\n \n-  result_se_buffers.reserve(result_buffers.size());\n-  for (BufferAllocation::Slice buffer : result_buffers) {\n+  result_se_buffers.reserve(result_buffers_.size());\n+  for (BufferAllocation::Slice buffer : result_buffers_) {\n     result_se_buffers.push_back(buffer_allocations.GetDeviceAddress(buffer));\n-    VLOG(5) << \"result buffer: \" << buffer.ToString()\n-            << \" addr: \" << result_se_buffers.back().opaque();\n   }\n \n   se::DeviceMemoryBase scratch =\n-      buffer_allocations.GetDeviceAddress(scratch_buffer);\n-  VLOG(5) << \"scratch buffer: \" << scratch_buffer\n-          << \" addr: \" << scratch.opaque();\n+      buffer_allocations.GetDeviceAddress(scratch_buffer_);\n+\n+  bool runner_created = false;\n+  RunConvOptions opts;\n+  opts.runner_cache = &GetOrCreateRunner(params.stream, &runner_created);\n \n-  auto [opts, runner_created] = cache.GetOrCreate(config, stream);\n-  if (runner_created && stream->parent()\n+  if (runner_created && params.stream->parent()\n                             ->GetDeviceDescription()\n                             .gpu_compute_capability()\n                             .IsRocm()) {\n     TF_ASSIGN_OR_RETURN(\n         GpuConvParams conv_params,\n-        GetGpuConvParams(config, operand_se_buffers, result_se_buffers));\n+        GetGpuConvParams(config_, operand_se_buffers, result_se_buffers));\n \n     TF_ASSIGN_OR_RETURN(se::dnn::DataType input_type,\n-                        GetDNNDataTypeFromPrimitiveType(config.input_type));\n+                        GetDNNDataTypeFromPrimitiveType(config_.input_type));\n \n     TF_ASSIGN_OR_RETURN(se::dnn::DataType output_type,\n-                        GetDNNDataTypeFromPrimitiveType(config.output_type));\n+                        GetDNNDataTypeFromPrimitiveType(config_.output_type));\n \n-    TF_ASSIGN_OR_RETURN(auto dnn, se::dnn::internal::GetDnnFromStream(stream));\n+    TF_ASSIGN_OR_RETURN(auto dnn,\n+                        se::dnn::internal::GetDnnFromStream(params.stream));\n     se::OwningScratchAllocator<> scratch_allocator(\n         buffer_allocations.device_ordinal(),\n         buffer_allocations.memory_allocator());\n \n     std::vector<se::dnn::ProfileResult> profile_results;\n     dnn->GetMIOpenConvolveAlgorithms(\n-        CudnnConvKindToProto(config.kind), input_type, output_type, stream,\n-        config.input_descriptor, conv_params.input_buf,\n-        config.filter_descriptor, conv_params.filter_buf,\n-        config.output_descriptor, conv_params.output_buf, config.conv_desc,\n+        CudnnConvKindToProto(config_.kind), input_type, output_type,\n+        params.stream, config_.input_descriptor, conv_params.input_buf,\n+        config_.filter_descriptor, conv_params.filter_buf,\n+        config_.output_descriptor, conv_params.output_buf, config_.conv_desc,\n         &scratch_allocator, &profile_results);\n   }\n-  TF_RETURN_IF_ERROR(RunGpuConv(config, absl::MakeSpan(operand_se_buffers),\n+\n+  TF_RETURN_IF_ERROR(RunGpuConv(config_, absl::MakeSpan(operand_se_buffers),\n                                 absl::MakeSpan(result_se_buffers), scratch,\n-                                stream, opts));\n+                                params.stream, opts));\n \n   // Note: Convolution has a tuple buffer as an output, but we don't need to\n   // populate it as no one should be reading from the tuple directly.\n-  if (!stream->ok()) {\n+  if (!params.stream->ok()) {\n     return Internal(\"ConvolutionThunk::ExecuteOnStream failed.\");\n   }\n   return absl::OkStatus();"
        },
        {
            "sha": "e456e23b816d8692ba0c7d920817bf830ba8d4db",
            "filename": "third_party/xla/xla/backends/gpu/runtime/convolution_thunk.h",
            "status": "modified",
            "additions": 8,
            "deletions": 36,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.h?ref=f13649f04c0fd6d41cf3af4cf2a0847a73a12571",
            "patch": "@@ -33,40 +33,12 @@ limitations under the License.\n namespace xla {\n namespace gpu {\n \n-struct ConvRunnerCache {\n-  ConvRunnerCache() = default;\n-  ConvRunnerCache(const ConvRunnerCache&) = delete;\n-  ConvRunnerCache& operator=(const ConvRunnerCache&) = delete;\n-\n-  std::pair<RunConvOptions, bool> GetOrCreate(const GpuConvConfig& config,\n-                                              const se::Stream* stream);\n-\n- private:\n-  absl::Mutex mu_;\n-  absl::flat_hash_map<const se::StreamExecutor*,\n-                      std::unique_ptr<GenericConvRunner>>\n-      cache_ ABSL_GUARDED_BY(mu_);\n-};\n-\n-absl::Status RunConvolutionOnStream(\n-    const Thunk::ExecuteParams& params,\n-    const std::vector<BufferAllocation::Slice>& operand_buffers,\n-    const std::vector<BufferAllocation::Slice>& result_buffers,\n-    const BufferAllocation::Slice& scratch_buffer, const GpuConvConfig& config,\n-    ConvRunnerCache& cache, se::Stream* stream);\n-\n-// Forward declaration needed to initialize ConvolutionCmd with ConvolutionThunk\n-// members.\n-class ConvolutionCmd;\n-\n // This class stores everything that StreamExecutor needs to launch a DNN\n // convolution. It is generated by IrEmitter.\n //\n // This is thread-compatible.\n class ConvolutionThunk : public Thunk {\n  public:\n-  friend class ConvolutionCmd;\n-\n   // Constructs a thunk for launching a DNN convolution.\n   //\n   // operand_slices should be in the same order as cudnn_call->operands().\n@@ -79,12 +51,7 @@ class ConvolutionThunk : public Thunk {\n   ConvolutionThunk(const ConvolutionThunk&) = delete;\n   ConvolutionThunk& operator=(const ConvolutionThunk&) = delete;\n \n-  absl::Status ExecuteOnStream(const ExecuteParams& params) override {\n-    VLOG(5) << \"ConvolutionThunk\";\n-    return RunConvolutionOnStream(params, operand_buffers_, result_buffers_,\n-                                  scratch_buffer_, config_, cache_,\n-                                  params.stream);\n-  }\n+  absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n \n   static absl::StatusOr<std::unique_ptr<ConvolutionThunk>> FromProto(\n       ThunkInfo thunk_info, const ConvolutionThunkProto& proto,\n@@ -99,17 +66,22 @@ class ConvolutionThunk : public Thunk {\n                    std::vector<BufferAllocation::Slice> result_slices,\n                    BufferAllocation::Slice scratch_slice);\n \n- protected:\n   std::vector<BufferAllocation::Slice> operand_buffers_;\n   std::vector<BufferAllocation::Slice> result_buffers_;\n   BufferAllocation::Slice scratch_buffer_;\n+  GenericConvRunner& GetOrCreateRunner(const stream_executor::Stream* stream,\n+                                       bool* runner_created);\n+\n   // Technically this is only needed during initialization to create the\n   // GpuConvConfig, but the actual GpuConvConfig is hard to serialize. So we\n   // keep the descriptor around for serialization purposes.\n   const GpuConvDescriptor descriptor_;\n   // Convolution config\n   const GpuConvConfig config_;\n-  ConvRunnerCache cache_;\n+  absl::Mutex mu_;\n+  absl::flat_hash_map<const stream_executor::Stream*,\n+                      std::unique_ptr<GenericConvRunner>>\n+      runner_cache_ ABSL_GUARDED_BY(mu_);\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "236ee1dd1a5d17eceee1f163cecbcbcc9063f687",
            "filename": "third_party/xla/xla/service/gpu/transforms/command_buffer_scheduling.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 21,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc?ref=f13649f04c0fd6d41cf3af4cf2a0847a73a12571",
            "patch": "@@ -234,22 +234,18 @@ static bool IsCommand(const HloCustomCallInstruction* hlo,\n     return true;\n   }\n \n-  if (config.enabled_commands.contains(DebugOptions::CUDNN)) {\n-    if (IsCustomCallToBlockScaledDot(*hlo)) {\n-      VLOG(3) << \"Recording BlockScaledDot, target \"\n-              << hlo->custom_call_target() << \" into command buffer.\";\n-      return true;\n-    }\n-    if (IsCustomCallTofMHA(*hlo)) {\n-      VLOG(3) << \"Recording FusedMHA, target \" << hlo->custom_call_target()\n-              << \" into command buffer.\";\n-      return true;\n-    }\n-    if (IsCustomCallToDnnConvolution(*hlo)) {\n-      VLOG(3) << \"Recording convolution, target \" << hlo->custom_call_target()\n-              << \" into command buffer.\";\n-      return true;\n-    }\n+  if (config.enabled_commands.contains(DebugOptions::CUDNN) &&\n+      IsCustomCallToBlockScaledDot(*hlo)) {\n+    VLOG(3) << \"Recording BlockScaledDot, target \" << hlo->custom_call_target()\n+            << \" into command buffer.\";\n+    return true;\n+  }\n+\n+  if (config.enabled_commands.contains(DebugOptions::CUDNN) &&\n+      IsCustomCallTofMHA(*hlo)) {\n+    VLOG(3) << \"Recording FusedMHA, target \" << hlo->custom_call_target()\n+            << \" into command buffer.\";\n+    return true;\n   }\n \n   if (!config.enabled_commands.contains(DebugOptions::CUSTOM_CALL)) {\n@@ -394,9 +390,7 @@ CommandBufferScheduling::CollectCommandBufferSequences(\n   int64_t num_commands_in_current_seq = 0;\n \n   // Adds `current_seq` to `sequences` if it has enough commands in it.\n-  auto collect_current_seq = [&](HloInstruction* instr) {\n-    VLOG(1) << \"Stopped at: \" << (instr ? instr->ToString() : \"<end>\")\n-            << \" #commands: \" << num_commands_in_current_seq;\n+  auto collect_current_seq = [&]() {\n     if (num_commands_in_current_seq >= std::max(1, min_num_commands)) {\n       RemoveTrailingNoOps(current_seq);\n       sequences.push_back(std::move(current_seq));\n@@ -547,11 +541,11 @@ CommandBufferScheduling::CollectCommandBufferSequences(\n \n     // If we didn't find the next command, collect the current sequence and\n     // start a new one.\n-    collect_current_seq(inst);\n+    collect_current_seq();\n   }\n \n   // Don't forget to collect the final command sequence.\n-  collect_current_seq(nullptr);\n+  collect_current_seq();\n   return sequences;\n }\n "
        },
        {
            "sha": "8da9a55bd95676967c7cbe902d7246a9345891dd",
            "filename": "third_party/xla/xla/service/gpu/transforms/command_buffer_scheduling_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 44,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f13649f04c0fd6d41cf3af4cf2a0847a73a12571/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc?ref=f13649f04c0fd6d41cf3af4cf2a0847a73a12571",
            "patch": "@@ -461,50 +461,6 @@ TEST_F(CommandBufferSchedulingTest, DoNotCaptureUnmatchedAsyncDone) {\n                             });\n }\n \n-TEST_F(CommandBufferSchedulingTest, ConvolutionCustomCallAndAllGather) {\n-  const char* hlo = R\"(\n-    HloModule TestModule, is_scheduled=true\n-\n-    ENTRY main {\n-      a = bf16[4,16,3,68,120]{4,3,2,1,0} parameter(0)\n-      b = bf16[768,16,1,2,2]{4,3,2,1,0} parameter(1)\n-      c = bf16[96]{0} parameter(2)\n-\n-      start = (bf16[96]{0}, bf16[768]{0}) all-gather-start(c),\n-        channel_id=555, replica_groups={{0,1,2,3,4,5,6,7}}, dimensions={0}\n-\n-      done = bf16[768]{0} all-gather-done(start)\n-      ROOT %cudnn-conv-bias-activation = (bf16[4,768,3,34,60]{4,3,2,1,0}, u8[783360]{0}) \n-            custom-call(a, b, done), window={size=1x2x2 stride=1x2x2}, \n-            dim_labels=bf012_oi012->bf012, \n-            custom_call_target=\"__cudnn$convBiasActivationForward\"\n-    })\";\n-\n-  const char* expected = R\"(\n-    CHECK: %command_buffer ([[P0:.+]]: bf16[96], [[P1:.+]]: bf16[4,16,3,68,120], [[P2:.+]]: bf16[768,16,1,2,2]) -> {{.*}} {\n-    CHECK:   %[[P0]] = bf16[96]{0} parameter(0)\n-    CHECK:   %[[P1]] = bf16[4,16,3,68,120]{4,3,2,1,0} parameter(1)\n-    CHECK:   %[[P2]] = bf16[768,16,1,2,2]{4,3,2,1,0} parameter(2)\n-    CHECK:   %[[START:.+]] = {{.*}} all-gather-start(%[[P0]])\n-    CHECK:   %[[DONE:.+]] = bf16[768]{0} all-gather-done(%[[START]])\n-    CHECK:   ROOT %[[CUDNN:.+]] =  {{.*}} custom-call(%[[P1]], %[[P2]], %[[DONE]])\n-    CHECK: }\n-\n-    CHECK: ENTRY %{{.*}} {\n-    CHECK:   %[[A:.+]] = bf16[4,16,3,68,120]{4,3,2,1,0} parameter(0)\n-    CHECK:   %[[B:.+]] = bf16[768,16,1,2,2]{4,3,2,1,0} parameter(1)\n-    CHECK:   %[[C:.+]] = bf16[96]{0} parameter(2)\n-    CHECK:   ROOT %[[CALL:.+]] =  {{.*}} call(%[[C]], %[[A]], %[[B]]),\n-    CHECK:     to_apply=%command_buffer\n-    CHECK: })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            expected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              TF_CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n TEST_F(CommandBufferSchedulingTest, CollectCommandBufferSequence) {\n   const char* hlo = R\"(\n       HloModule TestModule, is_scheduled=true"
        }
    ],
    "stats": {
        "total": 376,
        "additions": 56,
        "deletions": 320
    }
}