{
    "author": "Moerafaat",
    "message": "[XLA:GPU/TMA] Restrict TMA configurations for cases that are triggering CUDA_ERROR_MISALIGNED_ADDRESS errors. The observation is that these occur for dots with a broadcast ancestor on configurations that pipeline with more than 2 stages.\n\nPiperOrigin-RevId: 798143974",
    "sha": "ebc7b1b14a8096bfec775bad30c2a7b0bd240eff",
    "files": [
        {
            "sha": "d1f8a4a8d9fd44b05d8581d1a75f0a1f2e36f7f4",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ebc7b1b14a8096bfec775bad30c2a7b0bd240eff/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ebc7b1b14a8096bfec775bad30c2a7b0bd240eff/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=ebc7b1b14a8096bfec775bad30c2a7b0bd240eff",
            "patch": "@@ -253,6 +253,7 @@ xla_test(\n         \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\","
        },
        {
            "sha": "ae59872da4d9cd58e45224733f0ef1e2b8f7ef61",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 30,
            "deletions": 2,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ebc7b1b14a8096bfec775bad30c2a7b0bd240eff/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ebc7b1b14a8096bfec775bad30c2a7b0bd240eff/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=ebc7b1b14a8096bfec775bad30c2a7b0bd240eff",
            "patch": "@@ -621,6 +621,26 @@ std::string GetSelectedGemmBackendAsString(const HloModule* module) {\n   return \"\";\n }\n \n+bool HasBroadcastProducer(const HloInstruction& instr) {\n+  return HloBfsFindIf({&instr},\n+                      [](const HloInstruction* node) {\n+                        return node->opcode() == HloOpcode::kBroadcast;\n+                      })\n+      .has_value();\n+}\n+\n+// CUDA_ERROR_MISALIGNED_ADDRESS errors are happening for some cases when\n+// pipelining stages are > 2. The pattern observed is that these happen in the\n+// presence of a broadcast.\n+void RestrictTmaConfigs(std::vector<TritonGemmConfig>& configs) {\n+  configs.erase(std::remove_if(configs.begin(), configs.end(),\n+                               [&](const TritonGemmConfig& config) {\n+                                 return config.is_tma_allowed &&\n+                                        config.num_stages > 2;\n+                               }),\n+                configs.end());\n+}\n+\n }  // anonymous namespace\n \n absl::Status GemmFusionAutotunerRewriterVisitor::HandleFusion(\n@@ -879,10 +899,18 @@ GemmFusionAutotunerImpl::GenerateTritonConfigs(const HloDotInstruction& dot) {\n           : std::make_optional(1),\n       /*autotune_tma=*/autotune_tma);\n \n+  // TODO(b/421858850): Restricting configs for dots from broadcasts is a\n+  // temporary solution. We should remove this once we have a fix for the error.\n+  auto default_configs = GetDefaultTritonConfigs();\n+  if (HasBroadcastProducer(dot)) {\n+    RestrictTmaConfigs(configs);\n+    RestrictTmaConfigs(default_configs);\n+  }\n+\n   if (!debug_options_.xla_gpu_exhaustive_tiling_search()) {\n     VLOG(1) << \"Restricting configs to the default set.\";\n-    configs = search_space.OptimizeConfigSet(\n-        configs, /*hints=*/GetDefaultTritonConfigs());\n+    configs =\n+        search_space.OptimizeConfigSet(configs, /*hints=*/default_configs);\n   }\n   if (!IsAutotuningEnabled()) {\n     // Keep the first config, which likely does not spill registers."
        },
        {
            "sha": "a1a5d566207e676c8ff0b3e4f6a75396ccc7182b",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ebc7b1b14a8096bfec775bad30c2a7b0bd240eff/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ebc7b1b14a8096bfec775bad30c2a7b0bd240eff/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=ebc7b1b14a8096bfec775bad30c2a7b0bd240eff",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n@@ -1772,6 +1773,43 @@ TEST_F(GemmFusionAutotunerEnableTma,\n   EXPECT_TRUE(RunAndCompare(std::move(module),\n                             ErrorSpec{/*aabs=*/5e-3, /*arel=*/5e-3}));\n }\n+\n+TEST_F(GemmFusionAutotunerEnableTma,\n+       TmaConfigsGeneratedAndRunCorrectlyForDotsOfBroadcasts) {\n+  if (isRocm()) {\n+    GTEST_SKIP() << \"Not supported on ROCm.\";\n+  }\n+\n+  std::unique_ptr<VerifiedHloModule> module = ParseAndReturnVerifiedModule(R\"(\n+    ENTRY e {\n+      p0 = f32[64] parameter(0)\n+      p0b = f32[64,64] broadcast(p0), dimensions={0}\n+      p1 = f32[64,64] parameter(1)\n+      ROOT r = f32[64,64] dot(p0b, p1),\n+        lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+    })\")\n+                                                  .value();\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      const std::vector<TritonGemmConfig> hopper_configs,\n+      GetPossibleMatmulAutotuneTritonConfigs(\n+          *Cast<HloDotInstruction>(\n+              module->entry_computation()->root_instruction()),\n+          se::CudaComputeCapability(se::CudaComputeCapability::kHopper, 0),\n+          GetToolkitVersion(), GetDebugOptionsForTest()));\n+\n+  auto is_disallowed_tma_config = [](const TritonGemmConfig& c) {\n+    return c.num_stages > 2 && c.is_tma_allowed;\n+  };\n+  auto is_allowed_tma_config = [](const TritonGemmConfig& c) {\n+    return c.num_stages <= 2 && c.is_tma_allowed;\n+  };\n+  EXPECT_FALSE(absl::c_any_of(hopper_configs, is_disallowed_tma_config));\n+  EXPECT_TRUE(absl::c_any_of(hopper_configs, is_allowed_tma_config));\n+\n+  EXPECT_TRUE(RunAndCompare(std::move(module),\n+                            ErrorSpec{/*aabs=*/5e-3, /*arel=*/5e-3}));\n+}\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 71,
        "additions": 69,
        "deletions": 2
    }
}