{
    "author": "sohaibiftikhar",
    "message": "[XLA:GPU]: Instantiate CollectiveKernelThunk in IREmitter.\n\nMoves instantiation out of all-reduce so that when the collective code is\ncodegened it can be directly plumbed into the kernel instead of passing it from\noutside.\n\nPiperOrigin-RevId: 833388652",
    "sha": "0c8f9d1ec66de3ff005812f0cc24b57b70ffd6b9",
    "files": [
        {
            "sha": "9973b226ebbf5f66a8656a958ba3e1a3cf4bd9e6",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_thunk.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 22,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0c8f9d1ec66de3ff005812f0cc24b57b70ffd6b9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0c8f9d1ec66de3ff005812f0cc24b57b70ffd6b9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc?ref=0c8f9d1ec66de3ff005812f0cc24b57b70ffd6b9",
            "patch": "@@ -16,16 +16,17 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/all_reduce_thunk.h\"\n \n #include <cstdint>\n+#include <memory>\n #include <optional>\n #include <utility>\n #include <vector>\n \n #include \"absl/status/status.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n #include \"xla/backends/gpu/collectives/gpu_communicator.h\"\n+#include \"xla/backends/gpu/runtime/collective_kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n@@ -118,25 +119,15 @@ AllReduceReduceScatterThunkBase::AllReduceReduceScatterThunkBase(\n   CHECK_EQ(config_.config.operand_count, buffers_.size());\n }\n \n-AllReduceStartThunk::AllReduceStartThunk(ThunkInfo thunk_info,\n-                                         const HloAllReduceInstruction* inst,\n-                                         std::vector<Buffer> buffers,\n-                                         bool p2p_memcpy_enabled)\n+AllReduceStartThunk::AllReduceStartThunk(\n+    ThunkInfo thunk_info, const HloAllReduceInstruction* inst,\n+    std::vector<Buffer> buffers,\n+    std::unique_ptr<CollectiveKernelThunk> collective_kernel_thunk,\n+    bool p2p_memcpy_enabled)\n     : AllReduceReduceScatterThunkBase(\n           Thunk::kAllReduceStart, thunk_info, GetAllReduceConfigInst(inst),\n           std::move(buffers), IsGPUSyncCollective(*inst)),\n-      collective_kernel_thunk_{\n-          thunk_info,\n-          config_.config,\n-          config_.reduction_kind,\n-          IsAsync(),\n-          buffers_,\n-          /*is_collective_kernel_enabled=*/\n-          inst->GetModule()\n-              ->config()\n-              .debug_options()\n-              .xla_gpu_unsupported_use_all_reduce_one_shot_kernel(),\n-      } {}\n+      collective_kernel_thunk_(std::move(collective_kernel_thunk)) {}\n \n absl::Status AllReduceStartThunk::CheckImplementable(\n     const HloAllReduceInstruction* inst, int64_t replica_count,\n@@ -154,7 +145,7 @@ CollectiveOpGroupMode AllReduceStartThunk::GetGroupMode(\n absl::Status AllReduceStartThunk::Prepare(\n     const PrepareParams& params, ResourceRequestsInterface& resource_requests) {\n   TF_RETURN_IF_ERROR(CollectiveThunk::Prepare(params, resource_requests));\n-  return collective_kernel_thunk_.Prepare(params, resource_requests);\n+  return collective_kernel_thunk_->Prepare(params, resource_requests);\n }\n \n absl::Status AllReduceStartThunk::Initialize(const InitializeParams& params) {\n@@ -163,10 +154,10 @@ absl::Status AllReduceStartThunk::Initialize(const InitializeParams& params) {\n       GpuCliqueKey clique_key,\n       GetCollectiveGpuCliqueKey(*params.collective_params, config()));\n   TF_ASSIGN_OR_RETURN(bool use_collective_kernel,\n-                      collective_kernel_thunk_.IsSupported(\n+                      collective_kernel_thunk_->IsSupported(\n                           clique_key, params.collective_cliques));\n   if (use_collective_kernel) {\n-    TF_RETURN_IF_ERROR(collective_kernel_thunk_.Initialize(params));\n+    TF_RETURN_IF_ERROR(collective_kernel_thunk_->Initialize(params));\n   }\n   return absl::OkStatus();\n }\n@@ -180,11 +171,11 @@ absl::StatusOr<bool> AllReduceStartThunk::RunCollective(\n                              config_.config.operand_element_type));\n \n   TF_ASSIGN_OR_RETURN(bool use_collective_kernel,\n-                      collective_kernel_thunk_.IsSupported(\n+                      collective_kernel_thunk_->IsSupported(\n                           comm_handle.clique_key, params.collective_cliques));\n \n   if (use_collective_kernel) {\n-    TF_RETURN_IF_ERROR(collective_kernel_thunk_.ExecuteOnStream(params));\n+    TF_RETURN_IF_ERROR(collective_kernel_thunk_->ExecuteOnStream(params));\n     return false;  // No need for \"first\" invocation to rendezvous when not\n                    // using nccl.\n   }"
        },
        {
            "sha": "14793f44d4e0b703376b02600099c19996a3e8eb",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_thunk.h",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0c8f9d1ec66de3ff005812f0cc24b57b70ffd6b9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0c8f9d1ec66de3ff005812f0cc24b57b70ffd6b9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.h?ref=0c8f9d1ec66de3ff005812f0cc24b57b70ffd6b9",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n #define XLA_BACKENDS_GPU_RUNTIME_ALL_REDUCE_THUNK_H_\n \n #include <cstdint>\n+#include <memory>\n #include <vector>\n \n #include \"absl/status/status.h\"\n@@ -64,9 +65,11 @@ class AllReduceReduceScatterThunkBase : public CollectiveThunk {\n \n class AllReduceStartThunk : public AllReduceReduceScatterThunkBase {\n  public:\n-  AllReduceStartThunk(ThunkInfo thunk_info, const HloAllReduceInstruction* inst,\n-                      std::vector<Buffer> buffers,\n-                      bool p2p_memcpy_enabled = false);\n+  AllReduceStartThunk(\n+      ThunkInfo thunk_info, const HloAllReduceInstruction* inst,\n+      std::vector<Buffer> buffers,\n+      std::unique_ptr<CollectiveKernelThunk> collective_kernel_thunk,\n+      bool p2p_memcpy_enabled = false);\n \n   static const char* GetHloOpName() { return \"all-reduce-start\"; }\n \n@@ -87,7 +90,7 @@ class AllReduceStartThunk : public AllReduceReduceScatterThunkBase {\n                                      CommunicatorHandle comm) override;\n \n  private:\n-  CollectiveKernelThunk collective_kernel_thunk_;\n+  std::unique_ptr<CollectiveKernelThunk> collective_kernel_thunk_;\n };\n \n // -----------------------------------------------------------------------------"
        },
        {
            "sha": "4ef4d1354141059c11c2182ecb453404bfa45b8b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.h",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0c8f9d1ec66de3ff005812f0cc24b57b70ffd6b9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0c8f9d1ec66de3ff005812f0cc24b57b70ffd6b9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.h?ref=0c8f9d1ec66de3ff005812f0cc24b57b70ffd6b9",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.*/\n #include <memory>\n #include <string>\n #include <utility>\n+#include <vector>\n \n #include \"absl/base/thread_annotations.h\"\n #include \"absl/container/flat_hash_map.h\"\n@@ -59,7 +60,7 @@ class CollectiveKernelThunk : public Thunk {\n \n   CollectiveKernelThunk(ThunkInfo info, CollectiveConfig collective_config,\n                         ReductionKind reduction_kind, bool is_async,\n-                        absl::Span<const CollectiveThunk::Buffer> buffers,\n+                        std::vector<CollectiveThunk::Buffer> buffers,\n                         bool is_collective_kernel_enabled,\n                         absl::string_view kernel_name = \"\",\n                         bool is_multimem_enabled = false)\n@@ -69,7 +70,7 @@ class CollectiveKernelThunk : public Thunk {\n         collective_config_(std::move(collective_config)),\n         reduction_kind_(reduction_kind),\n         kernel_name_(kernel_name),\n-        buffers_(buffers),\n+        buffers_(std::move(buffers)),\n         is_multimem_enabled_(is_multimem_enabled) {\n     per_stream_state_.reserve(kMaxNumExecutors);\n   }\n@@ -168,7 +169,7 @@ class CollectiveKernelThunk : public Thunk {\n   // Must match the kernel name in the generated PTX kernel.\n   const std::string kernel_name_;\n   // Reference to the buffer related information required for the collective.\n-  absl::Span<const CollectiveThunk::Buffer> buffers_;\n+  std::vector<CollectiveThunk::Buffer> buffers_;\n \n   std::unique_ptr<stream_executor::gpu::GpuExecutor::MulticastMemory>\n       multicast_memory_;"
        },
        {
            "sha": "af8bea920e676af6fa1c6f8f223b54ffe86ceb44",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0c8f9d1ec66de3ff005812f0cc24b57b70ffd6b9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0c8f9d1ec66de3ff005812f0cc24b57b70ffd6b9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=0c8f9d1ec66de3ff005812f0cc24b57b70ffd6b9",
            "patch": "@@ -485,6 +485,7 @@ cc_library(\n         \"//xla/backends/gpu/runtime:all_to_all_thunk\",\n         \"//xla/backends/gpu/runtime:collective_broadcast_thunk\",\n         \"//xla/backends/gpu/runtime:collective_group_thunk\",\n+        \"//xla/backends/gpu/runtime:collective_kernel_thunk\",\n         \"//xla/backends/gpu/runtime:collective_permute_thunk\",\n         \"//xla/backends/gpu/runtime:collective_thunk\",\n         \"//xla/backends/gpu/runtime:command_buffer_cmd\",\n@@ -540,10 +541,12 @@ cc_library(\n         \"//xla/service:custom_call_status\",\n         \"//xla/service:custom_call_target_registry\",\n         \"//xla/service:global_device_id\",\n+        \"//xla/service:hlo_creation_utils\",\n         \"//xla/service:name_uniquer\",\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu/kernels:custom_kernel\",\n         \"//xla/service/gpu/model:block_level_parameters\",\n+        \"//xla/service/gpu/transforms/collectives:collective_ops_utils\",\n         \"//xla/service/llvm_ir:buffer_assignment_util\",\n         \"//xla/service/llvm_ir:ir_array\",\n         \"//xla/service/llvm_ir:kernel_support_library\","
        },
        {
            "sha": "4feaf9eb19b0faebd319141b33332e81b381141c",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 55,
            "deletions": 3,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0c8f9d1ec66de3ff005812f0cc24b57b70ffd6b9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0c8f9d1ec66de3ff005812f0cc24b57b70ffd6b9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=0c8f9d1ec66de3ff005812f0cc24b57b70ffd6b9",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include <optional>\n #include <string>\n #include <tuple>\n+#include <type_traits>\n #include <utility>\n #include <vector>\n \n@@ -78,6 +79,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/all_to_all_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_broadcast_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_group_thunk.h\"\n+#include \"xla/backends/gpu/runtime/collective_kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_permute_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/command_buffer_cmd.h\"\n@@ -157,6 +159,7 @@ limitations under the License.\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/service/gpu/parallel_loop_emitter.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n+#include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n #include \"xla/service/gpu/triton_call.h\"\n #include \"xla/service/llvm_ir/buffer_assignment_util.h\"\n #include \"xla/service/llvm_ir/ir_array.h\"\n@@ -195,6 +198,39 @@ bool IsHostExecuteCustomCall(const HloInstruction& hlo) {\n                              // the TPU one\n }\n \n+template <typename ThunkType>\n+static constexpr bool kRequiresCollectiveKernelThunk =\n+    std::is_constructible_v<ThunkType, Thunk::ThunkInfo,\n+                            const HloAllReduceInstruction*,\n+                            std::vector<CollectiveThunk::Buffer>,\n+                            std::unique_ptr<CollectiveKernelThunk>,\n+                            /*p2p_memcpy_enabled=*/bool>;\n+\n+// The signature of this function would change to absl::Status once we lift the\n+// CollectiveKernelThunk out as a top level thunk. It would then become a member\n+// function of IrEmitterUnnested.\n+// As it stands now the collective kernel thunk is wrapped inside other\n+// collective thunks such as AllReduceStart. So this function is only\n+// responsible for emitting the collective kernel thunk and its dependencies.\n+// If nullptr is returned it means that the collective kernel thunk could not be\n+// emitted. This is not an error.\n+absl::StatusOr<std::unique_ptr<CollectiveKernelThunk>>\n+EmitCollectiveKernelThunk(IrEmitterContext* ir_emitter_context,\n+                          const CallGraph* call_graph,\n+                          Thunk::ThunkInfo thunk_info,\n+                          std::vector<CollectiveThunk::Buffer> buffers,\n+                          const HloAllReduceInstruction* instr,\n+                          const AllReduceConfig& config) {\n+  return std::make_unique<CollectiveKernelThunk>(\n+      thunk_info, config.config, config.reduction_kind,\n+      /*is_async=*/!IsGPUSyncCollective(*instr), std::move(buffers),\n+      /*is_collective_kernel_enabled=*/\n+      instr->GetModule()\n+          ->config()\n+          .debug_options()\n+          .xla_gpu_unsupported_use_all_reduce_one_shot_kernel());\n+}\n+\n }  // namespace\n \n IrEmitterUnnested::IrEmitterUnnested(IrEmitterContext* ir_emitter_context)\n@@ -2098,9 +2134,25 @@ absl::Status IrEmitterUnnested::EmitCollectiveThunk(\n     if (ir_emitter_context_->debug_options().xla_syntax_sugar_async_ops()) {\n       thunk_info.profile_annotation = async_start->name();\n     }\n-    auto thunk = std::make_unique<CollectiveThunkType>(\n-        thunk_info, inst, /*buffers=*/std::move(buffers),\n-        ir_emitter_context_->debug_options().xla_gpu_use_memcpy_local_p2p());\n+    std::unique_ptr<CollectiveThunkType> thunk;\n+    // TODO(b/828435206) Remove this constexpr once collective kernel thunk is\n+    // lifted out of the all reduce thunk.\n+    if constexpr (kRequiresCollectiveKernelThunk<CollectiveThunkType>) {\n+      TF_ASSIGN_OR_RETURN(\n+          auto collective_kernel_thunk,\n+          EmitCollectiveKernelThunk(ir_emitter_context_, call_graph_.get(),\n+                                    thunk_info, buffers,\n+                                    Cast<HloAllReduceInstruction>(inst),\n+                                    GetAllReduceConfigInst(inst)));\n+      thunk = std::make_unique<CollectiveThunkType>(\n+          thunk_info, inst, /*buffers=*/std::move(buffers),\n+          std::move(collective_kernel_thunk),\n+          ir_emitter_context_->debug_options().xla_gpu_use_memcpy_local_p2p());\n+    } else {\n+      thunk = std::make_unique<CollectiveThunkType>(\n+          thunk_info, inst, /*buffers=*/std::move(buffers),\n+          ir_emitter_context_->debug_options().xla_gpu_use_memcpy_local_p2p());\n+    }\n     GetCollectivesAsyncEvents().insert({async_start, thunk->async_events()});\n     AddThunkToThunkSequence(std::move(thunk));\n     return absl::OkStatus();"
        }
    ],
    "stats": {
        "total": 114,
        "additions": 82,
        "deletions": 32
    }
}