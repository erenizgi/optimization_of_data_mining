{
    "author": "loislo",
    "message": "[XLA:GPU] Pass `EmitterLocOpBuilder` by value for tracing the emitting points of the triton instructions.\n\nThe builder captures the location of its constructor call. That happens when we pass it by value. As a result we could see the call stack for every triton instruction we emitted.\n\nThe location will be added to the triton instructions if xla_gpu_unsupported_annotate_with_emitter_loc is set.\n\nPiperOrigin-RevId: 800456717",
    "sha": "1eb4347c9dc450f18b3495dfdb4b74a22b8e4abb",
    "files": [
        {
            "sha": "aab9cbd608da63c436f3993e32762da889658df5",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1eb4347c9dc450f18b3495dfdb4b74a22b8e4abb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1eb4347c9dc450f18b3495dfdb4b74a22b8e4abb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc?ref=1eb4347c9dc450f18b3495dfdb4b74a22b8e4abb",
            "patch": "@@ -75,7 +75,7 @@ struct PrecisionSpec {\n   ttir::InputPrecision ttir_input_precision;\n };\n \n-using AlgorithmEmitter = absl::StatusOr<Value> (*)(EmitterLocOpBuilder&,\n+using AlgorithmEmitter = absl::StatusOr<Value> (*)(EmitterLocOpBuilder,\n                                                    const DotOperands&,\n                                                    const PrecisionSpec&);\n \n@@ -86,7 +86,7 @@ using AlgorithmEmitter = absl::StatusOr<Value> (*)(EmitterLocOpBuilder&,\n // We would get the wrong result if we sum these partial products. Instead, we\n // must override any accumulated result if the last partial product is\n // non-finite. See b/115844437.\n-Value ZeroNaNs(EmitterLocOpBuilder& b, Value input) {\n+Value ZeroNaNs(EmitterLocOpBuilder b, Value input) {\n   Value positive_inf =\n       CreateConst<float>(b, b.getF32Type(),\n                          std::numeric_limits<float>::infinity(),\n@@ -136,7 +136,7 @@ Value IEEEDot(EmitterLocOpBuilder b, Value lhs, Value rhs, Value acc) {\n \n // Leverages BF16 datatype for F32 matmul computation. It follows the guidance\n // from https://arxiv.org/pdf/1904.06376.pdf.\n-absl::StatusOr<Value> EmitBF16x9Matmul(EmitterLocOpBuilder& b,\n+absl::StatusOr<Value> EmitBF16x9Matmul(EmitterLocOpBuilder b,\n                                        const DotOperands& dot_operands,\n                                        const PrecisionSpec& precision_spec) {\n   constexpr int kNumParts = 3;\n@@ -174,7 +174,7 @@ absl::StatusOr<Value> EmitBF16x9Matmul(EmitterLocOpBuilder& b,\n \n // Leverages BF16 datatype for F32 matmul computation. It follows the guidance\n // from https://arxiv.org/pdf/1904.06376.pdf.\n-absl::StatusOr<Value> EmitBF16x6Matmul(EmitterLocOpBuilder& b,\n+absl::StatusOr<Value> EmitBF16x6Matmul(EmitterLocOpBuilder b,\n                                        const DotOperands& dot_operands,\n                                        const PrecisionSpec& precision_spec) {\n   constexpr int kNumParts = 3;\n@@ -208,7 +208,7 @@ absl::StatusOr<Value> EmitBF16x6Matmul(EmitterLocOpBuilder& b,\n \n // Compute F32 matmul with 3 BF16 dots. It is less accurate than\n // EmitBF16x6Matmul.\n-absl::StatusOr<Value> EmitBF16x3Matmul(EmitterLocOpBuilder& b,\n+absl::StatusOr<Value> EmitBF16x3Matmul(EmitterLocOpBuilder b,\n                                        const DotOperands& dot_operands,\n                                        const PrecisionSpec& precision_spec) {\n   constexpr int kNumParts = 2;\n@@ -252,7 +252,7 @@ ttir::InputPrecision InferDotPrecision(const HloDotInstruction& dot) {\n                             : ttir::InputPrecision::IEEE;\n }\n \n-absl::StatusOr<Type> GetAlgUnsetAccumulatorType(EmitterLocOpBuilder& b,\n+absl::StatusOr<Type> GetAlgUnsetAccumulatorType(EmitterLocOpBuilder b,\n                                                 const HloDotInstruction& dot) {\n   TF_ASSIGN_OR_RETURN(Type lhs_type,\n                       TritonType(b, dot.operand(0)->shape().element_type()));\n@@ -279,7 +279,7 @@ absl::StatusOr<Type> GetAlgUnsetAccumulatorType(EmitterLocOpBuilder& b,\n                                                         : b.getF32Type();\n }\n \n-absl::StatusOr<Value> EmitDotAlgUnset(EmitterLocOpBuilder& b,\n+absl::StatusOr<Value> EmitDotAlgUnset(EmitterLocOpBuilder b,\n                                       const DotOperands& dot_operands,\n                                       const PrecisionSpec& precision_spec) {\n   // Execute matrix multiplication of input tiles and pass the accumulator.\n@@ -305,7 +305,7 @@ absl::StatusOr<Value> EmitDotAlgUnset(EmitterLocOpBuilder& b,\n       /*maxNumImpreciseAcc=*/max_num_imprecise_acc);\n }\n \n-absl::StatusOr<Value> EmitRegularDot(EmitterLocOpBuilder& b,\n+absl::StatusOr<Value> EmitRegularDot(EmitterLocOpBuilder b,\n                                      const DotOperands& dot_operands,\n                                      const PrecisionSpec& precision_spec) {\n   Value lhs = dot_operands.lhs;\n@@ -380,7 +380,7 @@ absl::StatusOr<AlgorithmEmitter> GetAlgorithmEmitter(\n // the operands do not already conform to any of them. Returns `std::nullopt` if\n // no casting is a priori needed.\n absl::StatusOr<std::optional<Type>> GetForceOperandsType(\n-    EmitterLocOpBuilder& b, const HloDotInstruction& dot,\n+    EmitterLocOpBuilder b, const HloDotInstruction& dot,\n     const DotOperands& dot_operands) {\n   PrecisionConfig::Algorithm algorithm = dot.precision_config().algorithm();\n   if (algorithm == PrecisionConfig::ALG_UNSET) {\n@@ -429,7 +429,7 @@ absl::StatusOr<std::optional<Type>> GetForceOperandsType(\n \n }  // namespace\n \n-absl::StatusOr<Type> GetDotAccumulatorType(EmitterLocOpBuilder& b,\n+absl::StatusOr<Type> GetDotAccumulatorType(EmitterLocOpBuilder b,\n                                            const HloDotInstruction& dot) {\n   const PrecisionConfig::Algorithm algorithm =\n       dot.precision_config().algorithm();\n@@ -443,7 +443,7 @@ absl::StatusOr<Type> GetDotAccumulatorType(EmitterLocOpBuilder& b,\n   return TritonType(b, accumulator_type);\n }\n \n-absl::StatusOr<Value> EmitSingleTileDot(EmitterLocOpBuilder& b,\n+absl::StatusOr<Value> EmitSingleTileDot(EmitterLocOpBuilder b,\n                                         const HloDotInstruction& dot,\n                                         DotOperands dot_operands) {\n   PrecisionConfig::Algorithm algorithm = dot.precision_config().algorithm();"
        },
        {
            "sha": "46738e015314285ae03eb00371b87aa9811261fc",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1eb4347c9dc450f18b3495dfdb4b74a22b8e4abb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1eb4347c9dc450f18b3495dfdb4b74a22b8e4abb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.h?ref=1eb4347c9dc450f18b3495dfdb4b74a22b8e4abb",
            "patch": "@@ -37,12 +37,12 @@ struct DotOperands {\n // Returns the type to use for accumulation for the given `dot` instruction.\n // This also handles the case where the algorithm is `ALG_UNSET`.\n absl::StatusOr<::mlir::Type> GetDotAccumulatorType(\n-    EmitterLocOpBuilder& b, const HloDotInstruction& dot);\n+    EmitterLocOpBuilder b, const HloDotInstruction& dot);\n \n // Emits a single-tile dot, considering the given `dot` instruction's algorithm\n // and operand precisions. Raises an `UnimplementedError` if the algorithm is\n // not supported.\n-absl::StatusOr<::mlir::Value> EmitSingleTileDot(EmitterLocOpBuilder& b,\n+absl::StatusOr<::mlir::Value> EmitSingleTileDot(EmitterLocOpBuilder b,\n                                                 const HloDotInstruction& dot,\n                                                 DotOperands dot_operands);\n "
        },
        {
            "sha": "d4acc5c5e1473bc7b8adf419c76d696277c6da08",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 29,
            "deletions": 29,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1eb4347c9dc450f18b3495dfdb4b74a22b8e4abb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1eb4347c9dc450f18b3495dfdb4b74a22b8e4abb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=1eb4347c9dc450f18b3495dfdb4b74a22b8e4abb",
            "patch": "@@ -173,7 +173,7 @@ using ::xla::gpu::triton::TritonType;\n namespace {\n \n // Emit a value as Index clamped to [lower, upper].\n-Value EmitClampedIndex(EmitterLocOpBuilder& b, Value value, int64_t lower,\n+Value EmitClampedIndex(EmitterLocOpBuilder b, Value value, int64_t lower,\n                        int64_t upper) {\n   Value clamped_index = b.create<arith::MaxSIOp>(\n       value, CreateConst(b, value.getType(), lower).UnwrapUnsafe());\n@@ -183,7 +183,7 @@ Value EmitClampedIndex(EmitterLocOpBuilder& b, Value value, int64_t lower,\n }\n \n absl::StatusOr<SmallVector<Value>> ComputeOffsetsForTile(\n-    EmitterLocOpBuilder& b, Value pid, ValueRange runtime_values,\n+    EmitterLocOpBuilder b, Value pid, ValueRange runtime_values,\n     const TiledHloInstruction& tiled_hlo) {\n   TF_ASSIGN_OR_RETURN(IndexingMap tile_offsets_indexing,\n                       tiled_hlo.tile_offsets_indexing());\n@@ -212,7 +212,7 @@ absl::StatusOr<SmallVector<Value>> ComputeOffsetsForTile(\n                                  /*symbols=*/{}, b);\n }\n \n-SmallVector<Value> CreateIndexValues(EmitterLocOpBuilder& builder,\n+SmallVector<Value> CreateIndexValues(EmitterLocOpBuilder builder,\n                                      const ArrayRef<int64_t>& values) {\n   SmallVector<Value> result;\n   result.reserve(values.size());\n@@ -229,7 +229,7 @@ SmallVector<Value> CreateIndexValues(EmitterLocOpBuilder& builder,\n class TileInfo {\n  public:\n   static absl::StatusOr<TileInfo> Construct(\n-      EmitterLocOpBuilder& b, Value pid, ValueRange runtime_values,\n+      EmitterLocOpBuilder b, Value pid, ValueRange runtime_values,\n       const TiledHloInstruction& tiled_hlo);\n \n   // Tile offsets. Its size is equal to the rank of the output shape.\n@@ -275,7 +275,7 @@ class TileInfo {\n };\n \n absl::StatusOr<TileInfo> TileInfo::Construct(\n-    EmitterLocOpBuilder& b, Value pid, ValueRange runtime_values,\n+    EmitterLocOpBuilder b, Value pid, ValueRange runtime_values,\n     const TiledHloInstruction& tiled_hlo) {\n   TF_ASSIGN_OR_RETURN(SmallVector<Value> offsets,\n                       ComputeOffsetsForTile(b, pid, runtime_values, tiled_hlo));\n@@ -300,15 +300,15 @@ absl::StatusOr<TileInfo> TileInfo::Construct(\n \n using TensorValue = mlir::TypedValue<mlir::RankedTensorType>;\n \n-ScalarOrTensor Broadcast(EmitterLocOpBuilder& b, TensorValue value,\n+ScalarOrTensor Broadcast(EmitterLocOpBuilder b, TensorValue value,\n                          ArrayRef<int64_t> shape) {\n   return ScalarOrTensor(\n       b.create<ttir::BroadcastOp>(value.getType().clone(shape), value));\n }\n \n // Same as HLO BroadcastInDims. The sorted indices in `dims` specify the mapping\n // of the input dimensions to the output dimensions.\n-ScalarOrTensor BroadcastInDims(EmitterLocOpBuilder& b, ScalarOrTensor value,\n+ScalarOrTensor BroadcastInDims(EmitterLocOpBuilder b, ScalarOrTensor value,\n                                ArrayRef<int64_t> output_shape,\n                                ArrayRef<int64_t> dims) {\n   CHECK(llvm::is_sorted(dims)) << \"broadcast dims must be sorted\";\n@@ -334,12 +334,12 @@ ScalarOrTensor BroadcastInDims(EmitterLocOpBuilder& b, ScalarOrTensor value,\n   return Broadcast(b, input_tensor, output_shape);\n }\n \n-ScalarOrTensor Range(EmitterLocOpBuilder& b, int32_t limit) {\n+ScalarOrTensor Range(EmitterLocOpBuilder b, int32_t limit) {\n   auto type = mlir::RankedTensorType::get(limit, b.getI32Type());\n   return ScalarOrTensor(b.create<ttir::MakeRangeOp>(type, 0, limit));\n }\n \n-ScalarOrTensor EmitParameterExtract(EmitterLocOpBuilder& b,\n+ScalarOrTensor EmitParameterExtract(EmitterLocOpBuilder b,\n                                     const TileInfo& tile_info,\n                                     Value parent_base_ptr) {\n   // For a pointer to a scalar or a zero-dimensional tensor, load the base\n@@ -360,14 +360,14 @@ ScalarOrTensor EmitParameterExtract(EmitterLocOpBuilder& b,\n }\n \n absl::StatusOr<ScalarOrTensor> EmitScope(\n-    EmitterLocOpBuilder& b, absl::string_view libdevice_path,\n+    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info,\n     const TritonFusionAnalysis* analysis,\n     absl::Span<const HloInstruction* const> instructions,\n     absl::flat_hash_map<const HloInstruction*, ScalarOrTensor>& values);\n \n absl::StatusOr<ScalarOrTensor> EmitReduce(\n-    EmitterLocOpBuilder& b, const TiledHloInstruction& tiled_hlo_reduce,\n+    EmitterLocOpBuilder b, const TiledHloInstruction& tiled_hlo_reduce,\n     absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values,\n     absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info) {\n@@ -459,7 +459,7 @@ absl::StatusOr<ScalarOrTensor> EmitReduce(\n //\n // TODO(b/331413981): get rid of this special handling once this is solved.\n absl::StatusOr<ScalarOrTensor> EmitNestedFusion(\n-    EmitterLocOpBuilder& b, absl::string_view libdevice_path,\n+    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info,\n     const HloFusionInstruction& fusion_instruction,\n     absl::flat_hash_map<const HloInstruction*, ScalarOrTensor>& values) {\n@@ -495,7 +495,7 @@ ArrayRef<T> MakeArrayRef(const absl::Span<const T> span) {\n }\n \n ScalarOrTensor EmitTiledBroadcast(\n-    EmitterLocOpBuilder& b, const TiledHloInstruction& tiled_broadcast,\n+    EmitterLocOpBuilder b, const TiledHloInstruction& tiled_broadcast,\n     absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values) {\n   const SmallVector<int64_t>& input_tile_shape =\n       tiled_broadcast.operand(0)->tile_sizes();\n@@ -515,7 +515,7 @@ ScalarOrTensor EmitTiledBroadcast(\n }\n \n absl::StatusOr<ScalarOrTensor> EmitTiledIota(\n-    EmitterLocOpBuilder& b, Value pid, const TiledHloInstruction& tiled_iota) {\n+    EmitterLocOpBuilder b, Value pid, const TiledHloInstruction& tiled_iota) {\n   const HloIotaInstruction* hlo_iota =\n       ::xla::Cast<HloIotaInstruction>(tiled_iota.hlo());\n   int64_t iota_dim = hlo_iota->iota_dimension();\n@@ -576,7 +576,7 @@ SmallVector<Value> GetRuntimeValues(\n }\n \n // Reshapes a non-0D tensor of shape [1, 1, 1, ...] to a scalar.\n-ScalarOrTensor ReshapeTensorToScalar(EmitterLocOpBuilder& b, Value input) {\n+ScalarOrTensor ReshapeTensorToScalar(EmitterLocOpBuilder b, Value input) {\n   auto element_type = mlir::cast<ShapedType>(input.getType()).getElementType();\n \n   // First, reshape to a 1D tensor if not already the case. This is needed\n@@ -611,7 +611,7 @@ ScalarOrTensor ReshapeTensorToScalar(EmitterLocOpBuilder& b, Value input) {\n   return ScalarOrTensor(reduction.getResult().front());\n }\n \n-absl::StatusOr<ScalarOrTensor> EmitTiledReshape(EmitterLocOpBuilder& b,\n+absl::StatusOr<ScalarOrTensor> EmitTiledReshape(EmitterLocOpBuilder b,\n                                                 ArrayRef<int64_t> tile_sizes,\n                                                 ScalarOrTensor input) {\n   SmallVector<int64_t> padded_tile_sizes = GetPaddedTileSizes(tile_sizes);\n@@ -645,7 +645,7 @@ absl::StatusOr<ScalarOrTensor> EmitTiledReshape(EmitterLocOpBuilder& b,\n   return ScalarOrTensor(reshape.getResult());\n }\n \n-Value EmitTiledTranspose(EmitterLocOpBuilder& b, ArrayRef<int64_t> tile_sizes,\n+Value EmitTiledTranspose(EmitterLocOpBuilder b, ArrayRef<int64_t> tile_sizes,\n                          SmallVector<int64_t> dimensions, Value input) {\n   SmallVector<int64_t> padded_tile_sizes = GetPaddedTileSizes(tile_sizes);\n \n@@ -660,7 +660,7 @@ Value EmitTiledTranspose(EmitterLocOpBuilder& b, ArrayRef<int64_t> tile_sizes,\n }\n \n absl::StatusOr<ScalarOrTensor> EmitTiledBitcast(\n-    EmitterLocOpBuilder& b, const TiledHloInstruction& tiled_bitcast,\n+    EmitterLocOpBuilder b, const TiledHloInstruction& tiled_bitcast,\n     Value input) {\n   Shape input_shape = tiled_bitcast.hlo()->operand(0)->shape();\n   const Shape& output_shape = tiled_bitcast.hlo()->shape();\n@@ -737,7 +737,7 @@ absl::StatusOr<ScalarOrTensor> EmitTiledBitcast(\n }\n \n absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n-    EmitterLocOpBuilder& b, absl::string_view libdevice_path,\n+    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n     const TiledHloComputation& tiled_computation, mlir::FunctionOpInterface fn,\n@@ -779,7 +779,7 @@ absl::StatusOr<int64_t> GetDotLoopIterationCount(\n //\n // Note: we currently assume that contracting_dimension_tile_index is an i32\n // scalar.\n-absl::StatusOr<Value> MaskDotOperand(EmitterLocOpBuilder& b,\n+absl::StatusOr<Value> MaskDotOperand(EmitterLocOpBuilder b,\n                                      const TiledHloInstruction& dot_operand,\n                                      Value dot_operand_value,\n                                      Value contracting_dimension_tile_index,\n@@ -882,7 +882,7 @@ enum class DotOperandSide { kLhs, kRhs };\n // RHS).\n //\n // Returns an error if canonicalization is not possible.\n-absl::StatusOr<Value> CanonicalizeDotOperand(EmitterLocOpBuilder& b,\n+absl::StatusOr<Value> CanonicalizeDotOperand(EmitterLocOpBuilder b,\n                                              Value operand,\n                                              int64_t contracting_dim_idx,\n                                              DotOperandSide side) {\n@@ -919,7 +919,7 @@ absl::StatusOr<Value> CanonicalizeDotOperand(EmitterLocOpBuilder& b,\n }\n \n absl::StatusOr<ScalarOrTensor> EmitDot(\n-    EmitterLocOpBuilder& b, absl::string_view libdevice_path,\n+    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_hlo_dot, mlir::FunctionOpInterface fn,\n@@ -1089,7 +1089,7 @@ absl::StatusOr<ScalarOrTensor> EmitDot(\n }\n \n absl::StatusOr<ScalarOrTensor> EmitConcatenate(\n-    EmitterLocOpBuilder& b, absl::string_view libdevice_path,\n+    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_concatenate, mlir::FunctionOpInterface fn,\n@@ -1197,7 +1197,7 @@ absl::StatusOr<ScalarOrTensor> EmitConcatenate(\n }\n \n absl::StatusOr<ScalarOrTensor> EmitPad(\n-    EmitterLocOpBuilder& b, const se::DeviceDescription& device_info,\n+    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const TiledHloInstruction& tiled_pad,\n     absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values,\n     Value pid) {\n@@ -1263,7 +1263,7 @@ absl::StatusOr<ScalarOrTensor> EmitPad(\n }\n \n absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n-    EmitterLocOpBuilder& b, absl::string_view libdevice_path,\n+    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion, const TiledHloInstruction& tiled_hlo,\n     mlir::FunctionOpInterface fn, Value pid,\n@@ -1398,7 +1398,7 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n // ordered before consumers in `tiled_computation`. Returns the results for the\n // roots of `tiled_computation`.\n absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n-    EmitterLocOpBuilder& b, absl::string_view libdevice_path,\n+    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n     const TiledHloComputation& tiled_computation, mlir::FunctionOpInterface fn,\n@@ -1439,7 +1439,7 @@ absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n // Emit sequence of instructions using compatible tiling ordered producers\n // before consumers.\n absl::StatusOr<ScalarOrTensor> EmitScope(\n-    EmitterLocOpBuilder& b, absl::string_view libdevice_path,\n+    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info,\n     const TritonFusionAnalysis* analysis,\n     absl::Span<const HloInstruction* const> instructions,\n@@ -1733,7 +1733,7 @@ void AppendFuncArgType(absl::Span<const int64_t> dims, Type ir_type,\n \n // Legacy emitter works with tt.func. New emitter works with func.func.\n // TODO(b/393299275): Remove legacy optionality once migration is complete.\n-mlir::FunctionOpInterface CreateFuncOp(EmitterLocOpBuilder& b,\n+mlir::FunctionOpInterface CreateFuncOp(EmitterLocOpBuilder b,\n                                        absl::string_view fn_name,\n                                        absl::string_view fusion_kind,\n                                        SmallVector<Type>& fn_arg_types) {\n@@ -1752,7 +1752,7 @@ mlir::FunctionOpInterface CreateFuncOp(EmitterLocOpBuilder& b,\n \n // Legacy emitter works with tt.return. New emitter works with func.return.\n // TODO(b/393299275): Remove legacy optionality once migration is complete.\n-void EmitReturnOp(EmitterLocOpBuilder& b, absl::string_view fusion_kind) {\n+void EmitReturnOp(EmitterLocOpBuilder b, absl::string_view fusion_kind) {\n   if (fusion_kind == kTritonGemmFusionKind) {\n     b.create<ttir::ReturnOp>();\n   } else {"
        },
        {
            "sha": "2db04e7a592cb276f0542a6732ec938c82ed332e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1eb4347c9dc450f18b3495dfdb4b74a22b8e4abb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1eb4347c9dc450f18b3495dfdb4b74a22b8e4abb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h?ref=1eb4347c9dc450f18b3495dfdb4b74a22b8e4abb",
            "patch": "@@ -109,7 +109,7 @@ namespace ir_emitter_triton_internal {\n \n // Computes the transformation from a 1-d program_id to a tile multi-index.\n llvm::SmallVector<mlir::Value, 3> ComputeDelinearizedTileIndex(\n-    EmitterLocOpBuilder& b, absl::Span<const int64_t> num_output_tiles_per_dim);\n+    EmitterLocOpBuilder b, absl::Span<const int64_t> num_output_tiles_per_dim);\n \n // Dumps the Triton IR to a string.\n //"
        }
    ],
    "stats": {
        "total": 86,
        "additions": 43,
        "deletions": 43
    }
}