{
    "author": "hyeontaek",
    "message": "[PjRt-IFRT] `ifrt::PjRtArray::pjrt_layout()` uses `nullptr` to indicate a default layout\n\nPjRt-IFRT now returns a `nullptr` if it knows that the Array layout represents a default layout. The user code previously has been migrated to handle this new behavior gracefully, obtaining a concrete default layout as before.\n\n`ifrt::PjRtArray` creation now request extra information on whether the underlying `PjRtBuffer` is using a custom layout as IFRT tracks the defaultness of array layouts. This information cannot be inferred correctly from `PjRtBuffer` alone because `PjRtBuffer::layout()` only returns a concrete layout. PjRt would mostly work fine today if a default layout is said to be a custom layout, but some strict layout equality check can fail and require more precise information to be supplied.\n\nA few test cases in IFRT ArrayImplTest against PjRt CPU and GPU clients\nhave been disabled because the output array does not track the\nnon-default-ness of the layout correctly when\n`MakeArraysFromHostBufferShards()` is implemented using\n`ClientMakeArraysFromHostBufferShards()`.\n\nPiperOrigin-RevId: 819995407",
    "sha": "55371dfcb4717b4425ece2d10cf707fd7bca67b9",
    "files": [
        {
            "sha": "7c7ea2646d9cc31e4ba76572508ce084b2e86978",
            "filename": "third_party/xla/xla/python/ifrt/array_impl_test_lib.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 18,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Farray_impl_test_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Farray_impl_test_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Farray_impl_test_lib.cc?ref=55371dfcb4717b4425ece2d10cf707fd7bca67b9",
            "patch": "@@ -302,10 +302,6 @@ TEST(ArrayImplTest, MakeArrayFromHostBufferDefaultLayout) {\n   for (Memory* const memory : device->Memories()) {\n     SCOPED_TRACE(absl::StrCat(memory->Kind()));\n \n-    TF_ASSERT_OK_AND_ASSIGN(auto default_layout,\n-                            client->GetDefaultPjRtLayout(\n-                                dtype, shape.dims(), device, memory->Kind()));\n-\n     TF_ASSERT_OK_AND_ASSIGN(\n         auto array,\n         client->MakeArrayFromHostBuffer(\n@@ -316,8 +312,14 @@ TEST(ArrayImplTest, MakeArrayFromHostBufferDefaultLayout) {\n     TF_ASSERT_OK(array->GetReadyFuture().Await());\n \n     TF_ASSERT_OK_AND_ASSIGN(auto layout, array->pjrt_layout());\n-    ASSERT_NE(layout, nullptr);\n-    EXPECT_EQ(*layout, *default_layout);\n+    // `layout` should be either nullptr or a concrete default layout.\n+    if (layout != nullptr) {\n+      TF_ASSERT_OK_AND_ASSIGN(auto default_layout,\n+                              client->GetDefaultPjRtLayout(\n+                                  dtype, shape.dims(), device, memory->Kind()));\n+\n+      EXPECT_EQ(*layout, *default_layout);\n+    }\n   }\n }\n \n@@ -1451,25 +1453,29 @@ TEST(ArrayImplTest, CopyPreservesDefaultLayouts) {\n       TF_ASSERT_OK(array->GetReadyFuture().Await());\n \n       TF_ASSERT_OK_AND_ASSIGN(auto src_layout, array->pjrt_layout());\n-      ASSERT_NE(src_layout, nullptr);\n-      TF_ASSERT_OK_AND_ASSIGN(\n-          auto src_default_layout,\n-          client->GetDefaultPjRtLayout(dtype, shape.dims(), device,\n-                                       src_memory->Kind()));\n-      EXPECT_EQ(*src_layout, *src_default_layout);\n+      // `layout` should be either nullptr or a concrete default layout.\n+      if (src_layout != nullptr) {\n+        TF_ASSERT_OK_AND_ASSIGN(\n+            auto src_default_layout,\n+            client->GetDefaultPjRtLayout(dtype, shape.dims(), device,\n+                                         src_memory->Kind()));\n+        EXPECT_EQ(*src_layout, *src_default_layout);\n+      }\n \n       TF_ASSERT_OK_AND_ASSIGN(\n           auto new_arrays, client->CopyArrays(absl::MakeSpan(&array, 1),\n                                               std::nullopt, dst_memory->Kind(),\n                                               ArrayCopySemantics::kAlwaysCopy));\n       ASSERT_THAT(new_arrays, SizeIs(1));\n       TF_ASSERT_OK_AND_ASSIGN(auto dst_layout, new_arrays[0]->pjrt_layout());\n-      ASSERT_NE(dst_layout, nullptr);\n-      TF_ASSERT_OK_AND_ASSIGN(\n-          auto dst_default_layout,\n-          client->GetDefaultPjRtLayout(dtype, shape.dims(), device,\n-                                       dst_memory->Kind()));\n-      EXPECT_EQ(*dst_layout, *dst_default_layout);\n+      // `layout` should be either nullptr or a concrete default layout.\n+      if (dst_layout != nullptr) {\n+        TF_ASSERT_OK_AND_ASSIGN(\n+            auto dst_default_layout,\n+            client->GetDefaultPjRtLayout(dtype, shape.dims(), device,\n+                                         dst_memory->Kind()));\n+        EXPECT_EQ(*dst_layout, *dst_default_layout);\n+      }\n     }\n   }\n }"
        },
        {
            "sha": "bd2736b76c41ffc58e7a243ffc4e8ea21e9e0d44",
            "filename": "third_party/xla/xla/python/ifrt_proxy/integration_tests/array_impl_test_tfrt_cpu.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fintegration_tests%2Farray_impl_test_tfrt_cpu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fintegration_tests%2Farray_impl_test_tfrt_cpu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fintegration_tests%2Farray_impl_test_tfrt_cpu.cc?ref=55371dfcb4717b4425ece2d10cf707fd7bca67b9",
            "patch": "@@ -24,6 +24,14 @@ int main(int argc, char** argv) {\n       // destination literal.\n       \"ArrayImplTest.MakeArrayFromHostBufferAndCopyToHostBufferWithByteStrides\",\n \n+      // Arrays created using `MakeArraysFromHostBufferShards()` do not indicate\n+      // correct custom layouts even if the given layout is a concrete default\n+      // layout. PjRt-IFRT uses `ClientMakeArraysFromHostBufferShards()`\n+      // internally, which lowers `MakeArraysFromHostBufferShards()` call into\n+      // legacy API calls that do not yet support custom layouts, and thus the\n+      // output arrays only can have default layouts.\n+      \"ArrayImplTest.MakeArraysFromHostBufferShardsWithLayout\",\n+\n       // `ShardingParamSharding` does not support serialization yet.\n       // TODO(b/282757875): Enable the test once IFRT implements\n       // `ShardingParamShardingSerDes`."
        },
        {
            "sha": "91b5ddb2dc855190b7bde0996d1448b37641cd16",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2FBUILD?ref=55371dfcb4717b4425ece2d10cf707fd7bca67b9",
            "patch": "@@ -329,6 +329,7 @@ cc_library(\n         \"//xla/hlo/translate/mhlo_to_hlo:type_to_shape\",\n         \"//xla/pjrt:host_callback\",\n         \"//xla/pjrt:host_memory_spaces\",\n+        \"//xla/pjrt:layout_mode\",\n         \"//xla/pjrt:pjrt_client\",\n         \"//xla/pjrt:pjrt_common\",\n         \"//xla/pjrt:pjrt_compiler\","
        },
        {
            "sha": "05be2489eaafeecf6d2b1107c4ae8306b11d21ec",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_array.cc",
            "status": "modified",
            "additions": 41,
            "deletions": 16,
            "changes": 57,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array.cc?ref=55371dfcb4717b4425ece2d10cf707fd7bca67b9",
            "patch": "@@ -174,16 +174,20 @@ absl::StatusOr<tsl::RCReference<PjRtArray>> PjRtArray::Create(\n }\n \n absl::StatusOr<tsl::RCReference<PjRtArray>> PjRtArray::Create(\n-    PjRtCompatibleClient* client, std::shared_ptr<PjRtBuffer> pjrt_buffer) {\n+    PjRtCompatibleClient* client, std::shared_ptr<PjRtBuffer> pjrt_buffer,\n+    bool has_custom_layout) {\n   TF_ASSIGN_OR_RETURN(auto dtype, ToDType(pjrt_buffer->element_type()));\n   Shape shape(pjrt_buffer->dimensions());\n   TF_ASSIGN_OR_RETURN(auto device,\n                       client->LookupPjRtDevice(pjrt_buffer->device()));\n   auto sharding = SingleDeviceSharding::Create(\n       device, MakeMemoryKindFromPjRtBuffer(pjrt_buffer.get()));\n-  auto layout = (dtype.kind() == DType::kToken)\n-                    ? std::make_shared<xla::PjRtLayout>(xla::Layout())\n-                    : pjrt_buffer->layout();\n+  std::shared_ptr<const xla::PjRtLayout> layout;\n+  if (has_custom_layout) {\n+    layout = (dtype.kind() == DType::kToken)\n+                 ? std::make_shared<xla::PjRtLayout>(xla::Layout())\n+                 : pjrt_buffer->layout();\n+  }\n   return tsl::MakeRef<PjRtArray>(\n       client, dtype, std::move(shape), std::move(sharding),\n       PjRtBuffers({std::move(pjrt_buffer)}), std::move(layout));\n@@ -195,7 +199,8 @@ absl::StatusOr<ArrayRef> PjRtArray::FullyReplicatedShard(\n     return FailedPrecondition(\n         \"FullyReplicatedShard: Array has no addressable shards.\");\n   }\n-  return PjRtArray::Create(client(), GetPjRtBuffer(semantics, 0));\n+  return PjRtArray::Create(client(), GetPjRtBuffer(semantics, 0),\n+                           /*has_custom_layout=*/(layout_ != nullptr));\n }\n \n std::shared_ptr<PjRtBuffer> PjRtArray::GetPjRtBuffer(\n@@ -215,7 +220,8 @@ std::shared_ptr<PjRtBuffer> PjRtArray::GetPjRtBuffer(\n }\n \n absl::StatusOr<tsl::RCReference<PjRtArray>> PjRtArray::Create(\n-    PjRtCompatibleClient* client, Shape shape, PjRtBuffers pjrt_buffers) {\n+    PjRtCompatibleClient* client, Shape shape, PjRtBuffers pjrt_buffers,\n+    bool has_custom_layout) {\n   if (pjrt_buffers.empty()) {\n     return InvalidArgument(\"PjRtBuffers must be non-empty.\");\n   }\n@@ -239,14 +245,19 @@ absl::StatusOr<tsl::RCReference<PjRtArray>> PjRtArray::Create(\n       BasicDeviceList::Create(std::move(devices)), memory_kind,\n       /*shape=*/shape,\n       /*shard_shapes=*/shapes);\n-  auto layout = pjrt_buffers.front()->layout();\n+  std::shared_ptr<const xla::PjRtLayout> layout;\n+  if (has_custom_layout) {\n+    layout = (dtype.kind() == DType::kToken)\n+                 ? std::make_shared<xla::PjRtLayout>(xla::Layout())\n+                 : pjrt_buffers.front()->layout();\n+  }\n   return PjRtArray::Create(client, dtype, std::move(shape), std::move(sharding),\n                            std::move(pjrt_buffers), std::move(layout));\n }\n \n absl::StatusOr<tsl::RCReference<PjRtArray>> PjRtArray::Create(\n     PjRtCompatibleClient* client, DynamicShape dynamic_shape,\n-    PjRtBuffers pjrt_buffers) {\n+    PjRtBuffers pjrt_buffers, bool has_custom_layout) {\n   if (pjrt_buffers.empty()) {\n     return InvalidArgument(\"PjRtBuffers must be non-empty.\");\n   }\n@@ -276,7 +287,12 @@ absl::StatusOr<tsl::RCReference<PjRtArray>> PjRtArray::Create(\n       BasicDeviceList::Create(std::move(devices)), memory_kind,\n       /*dynamic_shape=*/dynamic_shape,\n       /*shard_dynamic_shapes=*/dynamic_shapes);\n-  auto layout = pjrt_buffers.front()->layout();\n+  std::shared_ptr<const xla::PjRtLayout> layout;\n+  if (has_custom_layout) {\n+    layout = (dtype.kind() == DType::kToken)\n+                 ? std::make_shared<xla::PjRtLayout>(xla::Layout())\n+                 : pjrt_buffers.front()->layout();\n+  }\n   return PjRtArray::Create(client, dtype, std::move(dynamic_shape),\n                            std::move(sharding), std::move(pjrt_buffers),\n                            std::move(layout));\n@@ -534,13 +550,22 @@ absl::StatusOr<ArrayRef> PjRtArray::Copy(\n   if (new_client == nullptr) {\n     new_client = client_;\n   }\n+  std::shared_ptr<const xla::PjRtLayout> layout;\n+  static MemoryKind kUnpinnedHostMemoryKind(UnpinnedHostMemorySpace::kKind);\n+  // Unpinned host supports default layouts only; a custom layout would be\n+  // ignored.\n+  // TODO(hyeontaek): This behavior should be informed by the underlying PjRt\n+  // client instead of following a convention.\n+  if (layout_ != nullptr &&\n+      canonicalized_sharding_memory_kind != kUnpinnedHostMemoryKind) {\n+    layout = layout_;\n+  }\n   return std::visit(\n-      [this, new_client, &new_sharding, &buffers](const auto& shape) {\n-        std::shared_ptr<const xla::PjRtLayout> buffer_layout =\n-            buffers[0]->layout();\n+      [this, new_client, &new_sharding, &buffers,\n+       layout = std::move(layout)](const auto& shape) {\n         return PjRtArray::Create(new_client, dtype_, shape,\n                                  std::move(new_sharding), std::move(buffers),\n-                                 std::move(buffer_layout));\n+                                 layout);\n       },\n       shape_);\n }\n@@ -609,10 +634,10 @@ absl::StatusOr<std::shared_ptr<const xla::PjRtLayout>> PjRtArray::pjrt_layout()\n   for (int i = 1; i < pjrt_buffers_.size(); ++i) {\n     std::shared_ptr<const xla::PjRtLayout> layout_i =\n         pjrt_buffers_[i]->layout();\n-    DCHECK(*layout_ == *layout_i)\n+    DCHECK(*pjrt_buffers_[0]->layout() == *layout_i)\n         << \"PjRtArray has mismatched layouts across shards! \"\n-        << \"shard 0: \" << layout_->ToString() << \", shard \" << i << \": \"\n-        << layout_i->ToString();\n+        << \"shard 0: \" << pjrt_buffers_[0]->layout()->ToString() << \", shard \"\n+        << i << \": \" << layout_i->ToString();\n   }\n #endif\n   return layout_;"
        },
        {
            "sha": "6bb7c5cdf5f7db5be5d10655219939ef0ec57d46",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_array.h",
            "status": "modified",
            "additions": 17,
            "deletions": 7,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array.h?ref=55371dfcb4717b4425ece2d10cf707fd7bca67b9",
            "patch": "@@ -68,34 +68,44 @@ class PjRtArray final\n   using PjRtBuffers =\n       absl::InlinedVector<std::shared_ptr<PjRtBuffer>, kPjRtBufferInlineSize>;\n \n-  // General array construction (with static shape). pjrt_buffers may be empty.\n+  // General array construction (with static shape). `pjrt_buffers` may be\n+  // empty. `layout == nullptr` indicates a default layout.\n   static absl::StatusOr<tsl::RCReference<PjRtArray>> Create(\n       PjRtCompatibleClient* client, DType dtype, Shape shape,\n       ShardingRef sharding, PjRtBuffers pjrt_buffers,\n       std::shared_ptr<const xla::PjRtLayout> layout);\n \n-  // General array construction (with dynamic shape). pjrt_buffers may be empty.\n+  // General array construction (with dynamic shape). `pjrt_buffers` may be\n+  // empty. `layout == nullptr` indicates a default layout.\n   static absl::StatusOr<tsl::RCReference<PjRtArray>> Create(\n       PjRtCompatibleClient* client, DType dtype, DynamicShape dynamic_shape,\n       ShardingRef sharding, PjRtBuffers pjrt_buffers,\n       std::shared_ptr<const xla::PjRtLayout> layout);\n \n   // Shorthand for a single-shard array construction.\n+  // See `PjRtCompatibleClient::CreatePjRtArray()` for the meaning of\n+  // `has_custom_layout`.\n   static absl::StatusOr<tsl::RCReference<PjRtArray>> Create(\n-      PjRtCompatibleClient* client, std::shared_ptr<PjRtBuffer> pjrt_buffer);\n+      PjRtCompatibleClient* client, std::shared_ptr<PjRtBuffer> pjrt_buffer,\n+      bool has_custom_layout);\n \n   // Shorthand for a multi-shard array construction using ConcreteSharding.\n-  // pjrt_buffers must be non-empty.\n+  // `pjrt_buffers` must be non-empty.\n+  // See `PjRtCompatibleClient::CreatePjRtArray()` for the meaning of\n+  // `has_custom_layout`.\n   // TODO(hyeontaek): Remove this once IFRT Sharding and JAX Sharding is unified\n   // so that ConcreteSharding can be replaced with a real Sharding.\n   static absl::StatusOr<tsl::RCReference<PjRtArray>> Create(\n-      PjRtCompatibleClient* client, Shape shape, PjRtBuffers pjrt_buffers);\n+      PjRtCompatibleClient* client, Shape shape, PjRtBuffers pjrt_buffers,\n+      bool has_custom_layout);\n \n   // Shorthand for a multi-shard array construction using ConcreteSharding with\n-  // DynamicShape. pjrt_buffers must be non-empty.\n+  // DynamicShape. `pjrt_buffers` must be non-empty.\n+  // See `PjRtCompatibleClient::CreatePjRtArray()` for the meaning of\n+  // `has_custom_layout`.\n   static absl::StatusOr<tsl::RCReference<PjRtArray>> Create(\n       PjRtCompatibleClient* client, DynamicShape dynamic_shape,\n-      PjRtBuffers pjrt_buffers);\n+      PjRtBuffers pjrt_buffers, bool has_custom_layout);\n \n   // PjRtCompatibleArray implementation.\n "
        },
        {
            "sha": "7a5eb7e7651c1e0e9e6db5c20f51585dde9676ba",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_array_impl_test_cpu.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array_impl_test_cpu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array_impl_test_cpu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array_impl_test_cpu.cc?ref=55371dfcb4717b4425ece2d10cf707fd7bca67b9",
            "patch": "@@ -18,11 +18,18 @@ limitations under the License.\n #include \"xla/python/ifrt/test_util.h\"\n \n int main(int argc, char** argv) {\n-  // CpuBuffer::ToLiteral() currently does not respect the layout of the\n-  // destination literal.\n   static constexpr absl::string_view kFilter =\n+      // CpuBuffer::ToLiteral() currently does not respect the layout of the\n+      // destination literal.\n       \"-ArrayImplTest.\"\n-      \"MakeArrayFromHostBufferAndCopyToHostBufferWithByteStrides\";\n+      \"MakeArrayFromHostBufferAndCopyToHostBufferWithByteStrides:\"\n+      // Arrays created using `MakeArraysFromHostBufferShards()` do not indicate\n+      // correct custom layouts even if the given layout is a concrete default\n+      // layout. PjRt-IFRT uses `ClientMakeArraysFromHostBufferShards()`\n+      // internally, which lowers `MakeArraysFromHostBufferShards()` call into\n+      // legacy API calls that do not yet support custom layouts, and thus the\n+      // output arrays only can have default layouts.\n+      \"ArrayImplTest.MakeArraysFromHostBufferShardsWithLayout\";\n   xla::ifrt::test_util::SetTestFilterIfNotUserSpecified(kFilter);\n \n   testing::InitGoogleTest(&argc, argv);"
        },
        {
            "sha": "ac0f28a8a2146dc4196552b8e77df58e6936a019",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_client.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 28,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_client.cc?ref=55371dfcb4717b4425ece2d10cf707fd7bca67b9",
            "patch": "@@ -725,6 +725,17 @@ const char kKeyPrefix[] = \"ifrt_cross_host_transfer_\";\n char PjRtCompatibleClient::ID = 0;\n char PjRtClient::ID = 0;\n \n+absl::StatusOr<tsl::RCReference<PjRtCompatibleArray>>\n+PjRtCompatibleClient::CreatePjRtArray(std::shared_ptr<PjRtBuffer> pjrt_buffer) {\n+  return CreatePjRtArray(std::move(pjrt_buffer), /*has_custom_layout=*/true);\n+}\n+\n+absl::StatusOr<tsl::RCReference<PjRtCompatibleArray>>\n+PjRtCompatibleClient::CreatePjRtArray(Shape shape, PjRtBuffers pjrt_buffers) {\n+  return CreatePjRtArray(std::move(shape), std::move(pjrt_buffers),\n+                         /*has_custom_layout=*/true);\n+}\n+\n absl::StatusOr<std::unique_ptr<PjRtClient>> PjRtClient::Create(\n     PjRtClient::CreateOptions options) {\n   auto client =\n@@ -955,16 +966,21 @@ absl::StatusOr<DeviceListRef> PjRtClient::MakeDeviceList(\n const AttributeMap& PjRtClient::Attributes() const { return attributes_; }\n \n absl::StatusOr<tsl::RCReference<PjRtCompatibleArray>>\n-PjRtClient::CreatePjRtArray(std::shared_ptr<PjRtBuffer> pjrt_buffer) {\n-  TF_ASSIGN_OR_RETURN(auto array,\n-                      PjRtArray::Create(this, std::move(pjrt_buffer)));\n+PjRtClient::CreatePjRtArray(std::shared_ptr<PjRtBuffer> pjrt_buffer,\n+                            bool has_custom_layout) {\n+  TF_ASSIGN_OR_RETURN(\n+      auto array,\n+      PjRtArray::Create(this, std::move(pjrt_buffer), has_custom_layout));\n   return tsl::RCReference<PjRtCompatibleArray>(std::move(array));\n }\n \n absl::StatusOr<tsl::RCReference<PjRtCompatibleArray>>\n-PjRtClient::CreatePjRtArray(Shape shape, PjRtBuffers pjrt_buffers) {\n+PjRtClient::CreatePjRtArray(Shape shape, PjRtBuffers pjrt_buffers,\n+                            bool has_custom_layout) {\n+  std::shared_ptr<const xla::PjRtLayout> layout;\n   TF_ASSIGN_OR_RETURN(auto array, PjRtArray::Create(this, std::move(shape),\n-                                                    std::move(pjrt_buffers)));\n+                                                    std::move(pjrt_buffers),\n+                                                    has_custom_layout));\n   return tsl::RCReference<PjRtCompatibleArray>(std::move(array));\n }\n \n@@ -1055,7 +1071,8 @@ absl::StatusOr<ArrayRef> PjRtClient::MakeArrayFromHostBuffer(\n     }\n     buffers.push_back(std::move(buffer));\n   }\n-  auto layout = buffers.front()->layout();\n+  // `MakeArrayFromHostBuffer` only creates buffers with a default layout.\n+  std::shared_ptr<const xla::PjRtLayout> layout = nullptr;\n   return PjRtArray::Create(this, dtype, std::move(shape), std::move(sharding),\n                            std::move(buffers), std::move(layout));\n }\n@@ -1121,12 +1138,11 @@ absl::StatusOr<std::vector<ArrayRef>> PjRtClient::MakeErrorArrays(\n               error, xla_shape,\n               tensorflow::down_cast<PjRtMemory*>(memory)->pjrt_memory()));\n     }\n-    auto layout = buffers.front()->layout();\n     TF_ASSIGN_OR_RETURN(\n         arrays.emplace_back(),\n         PjRtArray::Create(this, array_spec.dtype, std::move(shard_shape),\n                           array_spec.sharding, std::move(buffers),\n-                          std::move(layout)));\n+                          array_spec.layout));\n   }\n   return arrays;\n }\n@@ -1211,16 +1227,8 @@ absl::StatusOr<ArrayRef> PjRtClient::AssembleArrayFromSingleDeviceArrays(\n   }\n   // TODO(emilyaf): Remove the following logic once layout is plumbed through.\n   std::shared_ptr<const xla::PjRtLayout> layout;\n-  if (dtype.kind() == DType::kToken) {\n-    layout = std::make_shared<xla::PjRtLayout>(xla::Layout());\n-  } else if (buffers.empty()) {\n-    TF_ASSIGN_OR_RETURN(auto shard_shape, sharding->GetShardShape(shape));\n-    TF_ASSIGN_OR_RETURN(\n-        layout, GetDefaultPjRtLayout(dtype, shard_shape.dims(),\n-                                     sharding->devices()->devices().front(),\n-                                     sharding->memory_kind()));\n-  } else {\n-    layout = buffers.front()->layout();\n+  if (!arrays.empty()) {\n+    TF_ASSIGN_OR_RETURN(layout, arrays.front()->pjrt_layout());\n   }\n   return PjRtArray::Create(this, dtype, std::move(shape), std::move(sharding),\n                            std::move(buffers), std::move(layout));\n@@ -1387,16 +1395,6 @@ PjRtClient::CopyArraysForCrossHost(absl::Span<ArrayRef> arrays,\n                         arrays[i]->shared_ptr_sharding()->WithDeviceAssignment(\n                             dst_devices, memory_kind));\n     TF_ASSIGN_OR_RETURN(auto new_layout, arrays[i]->pjrt_layout());\n-    if (new_layout == nullptr) {\n-      TF_ASSIGN_OR_RETURN(\n-          xla::ifrt::Shape shard_shape,\n-          arrays[i]->sharding().GetShardShape(arrays[i]->shape()));\n-      TF_ASSIGN_OR_RETURN(\n-          new_layout, GetDefaultPjRtLayout(\n-                          arrays[i]->dtype(), shard_shape.dims(),\n-                          arrays[i]->sharding().devices()->devices().front(),\n-                          arrays[i]->sharding().memory_kind()));\n-    }\n     TF_ASSIGN_OR_RETURN(\n         new_arrays.emplace_back(),\n         PjRtArray::Create(this, arrays[i]->dtype(), arrays[i]->shape(),"
        },
        {
            "sha": "d4f559b1416d23876eac52f6b1e4793a629bd9d8",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_client.h",
            "status": "modified",
            "additions": 27,
            "deletions": 4,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_client.h?ref=55371dfcb4717b4425ece2d10cf707fd7bca67b9",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include <memory>\n #include <optional>\n #include <tuple>\n+#include <utility>\n #include <vector>\n \n #include \"absl/base/thread_annotations.h\"\n@@ -91,10 +92,32 @@ class PjRtCompatibleClient\n   // operations.\n   virtual xla::PjRtClient* pjrt_client() = 0;\n   virtual std::shared_ptr<xla::PjRtClient> shared_ptr_pjrt_client() = 0;\n+\n+  // Creates an IFRT `PjRtCompatibleArray` from `PjRtBuffer`(s).\n+  //\n+  // Most array properties will be inferred from the input `PjRtBuffer`(s),\n+  // except for the layout's defaultness that is absent information at the PjRt\n+  // level.\n+  //\n+  // `has_custom_layout` indicates that the layout of the input `PjRtBuffer`(s)\n+  // is intended to be a user-chosen custom layout, and\n+  // `PjRtCompatibleArray::pjrt_layout()` should return a non-null value.\n+  // Treating a default layout as a custom layout is typically allowed in PjRt\n+  // if their concrete layouts match, but it may not pass a strict check that\n+  // unconditionally says a default layout != any non-default layout designed\n+  // for portability. Thus, it is useful for the caller to provide as accurate\n+  // information as possible.\n   virtual absl::StatusOr<tsl::RCReference<PjRtCompatibleArray>> CreatePjRtArray(\n-      std::shared_ptr<PjRtBuffer> pjrt_buffer) = 0;\n+      std::shared_ptr<PjRtBuffer> pjrt_buffer, bool has_custom_layout) = 0;\n   virtual absl::StatusOr<tsl::RCReference<PjRtCompatibleArray>> CreatePjRtArray(\n-      Shape shape, PjRtBuffers pjrt_buffers) = 0;\n+      Shape shape, PjRtBuffers pjrt_buffers, bool has_custom_layout) = 0;\n+\n+  // Temporary overloads for API transition.\n+  absl::StatusOr<tsl::RCReference<PjRtCompatibleArray>> CreatePjRtArray(\n+      std::shared_ptr<PjRtBuffer> pjrt_buffer);\n+  absl::StatusOr<tsl::RCReference<PjRtCompatibleArray>> CreatePjRtArray(\n+      Shape shape, PjRtBuffers pjrt_buffers);\n+\n   virtual absl::StatusOr<PjRtCompatibleDevice*> LookupPjRtDevice(\n       xla::PjRtDevice* pjrt_device) const = 0;\n   virtual absl::StatusOr<PjRtCompatibleMemory*> LookupPjRtMemory(\n@@ -178,9 +201,9 @@ class PjRtClient final\n     return pjrt_client_;\n   }\n   absl::StatusOr<tsl::RCReference<PjRtCompatibleArray>> CreatePjRtArray(\n-      std::shared_ptr<PjRtBuffer> pjrt_buffer) override;\n+      std::shared_ptr<PjRtBuffer> pjrt_buffer, bool has_custom_layout) override;\n   absl::StatusOr<tsl::RCReference<PjRtCompatibleArray>> CreatePjRtArray(\n-      Shape shape, PjRtBuffers pjrt_buffers) override;\n+      Shape shape, PjRtBuffers pjrt_buffers, bool has_custom_layout) override;\n \n   // Client implementation.\n "
        },
        {
            "sha": "2ae4b341e58c0c14086933e6df6272a50c3a7f9e",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_executable.cc",
            "status": "modified",
            "additions": 105,
            "deletions": 45,
            "changes": 150,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc?ref=55371dfcb4717b4425ece2d10cf707fd7bca67b9",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/python/pjrt_ifrt/pjrt_executable.h\"\n \n+#include <cstddef>\n #include <memory>\n #include <optional>\n #include <string>\n@@ -27,6 +28,7 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_split.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/Support/Casting.h\"\n@@ -38,11 +40,13 @@ limitations under the License.\n #include \"xla/hlo/translate/mhlo_to_hlo/type_to_shape.h\"\n #include \"xla/layout.h\"\n #include \"xla/pjrt/host_callback.h\"\n+#include \"xla/pjrt/layout_mode.h\"\n #include \"xla/pjrt/pjrt_client.h\"\n #include \"xla/pjrt/pjrt_compiler.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n #include \"xla/pjrt/pjrt_future.h\"\n #include \"xla/pjrt/pjrt_layout.h\"\n+#include \"xla/pjrt/utils.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/python/ifrt/array.h\"\n #include \"xla/python/ifrt/attribute_map.h\"\n@@ -147,6 +151,7 @@ absl::StatusOr<std::optional<xla::HloSharding>> GetFirstModuleOutputSharding(\n }\n \n // Returns the flattened output memory_kinds of the first module in a\n+// `PjRtLoadedExecutable`.\n // `UnimplementedError` will be converted into `std::nullopt`.\n absl::StatusOr<std::optional<std::vector<absl::string_view>>>\n GetFirstModuleOutputMemoryKinds(\n@@ -165,6 +170,39 @@ GetFirstModuleOutputMemoryKinds(\n   return std::move(output_memory_kinds)->front();\n }\n \n+// Returns the flattened output layouts of the first module in a\n+// `PjRtLoadedExecutable`.\n+// `UnimplementedError` will be converted into a vector of `nullptr`.\n+absl::StatusOr<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n+GetFirstModuleOutputLayouts(\n+    xla::PjRtLoadedExecutable* pjrt_loaded_executable,\n+    absl::Span<const xla::LayoutMode> output_layout_modes) {\n+  absl::StatusOr<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n+      executable_output_layouts = pjrt_loaded_executable->GetOutputLayouts();\n+  // An unimplemented error is converted into all-default layouts.\n+  if (absl::IsUnimplemented(executable_output_layouts.status())) {\n+    return std::vector<std::shared_ptr<const xla::PjRtLayout>>(\n+        /*size=*/output_layout_modes.size(), /*value=*/nullptr);\n+  }\n+  TF_RETURN_IF_ERROR(executable_output_layouts.status());\n+  std::vector<std::shared_ptr<const xla::PjRtLayout>> output_layouts;\n+  if (executable_output_layouts->size() != output_layout_modes.size()) {\n+    return FailedPrecondition(\n+        \"Output memory kinds and output layout modes have different sizes: %d \"\n+        \"vs. %d\",\n+        executable_output_layouts->size(), output_layout_modes.size());\n+  }\n+  output_layouts.reserve(executable_output_layouts->size());\n+  for (int i = 0; i < executable_output_layouts->size(); ++i) {\n+    if (output_layout_modes[i].mode == xla::LayoutMode::Mode::kDefault) {\n+      output_layouts.push_back(nullptr);\n+    } else {\n+      output_layouts.push_back(std::move((*executable_output_layouts)[i]));\n+    }\n+  }\n+  return output_layouts;\n+}\n+\n struct ShapePartialInfo {\n   std::vector<xla::PrimitiveType> element_types;\n   std::vector<xla::DimensionVector> dimensions;\n@@ -188,6 +226,36 @@ absl::StatusOr<ShapePartialInfo> CreateShapePartialInfo(\n   return partial_info;\n }\n \n+// Special `xla::GetLayoutModes()` implementation for obtaining layout modes\n+// from `hlo_module` without serializing it into proto.\n+\n+static const char* kDelimiter = \";\";\n+\n+static absl::StatusOr<std::vector<LayoutMode>> GetLayoutModesFromFrontendAttr(\n+    absl::string_view attr) {\n+  // SkipEmpty() needed to avoid returning the empty string when attr is empty.\n+  std::vector<std::string> str_modes =\n+      absl::StrSplit(attr, kDelimiter, absl::SkipEmpty());\n+  std::vector<LayoutMode> result;\n+  for (const std::string& str_mode : str_modes) {\n+    TF_ASSIGN_OR_RETURN(LayoutMode mode, LayoutMode::FromString(str_mode));\n+    result.emplace_back(std::move(mode));\n+  }\n+  return result;\n+}\n+\n+static absl::StatusOr<std::vector<LayoutMode>> GetLayoutModes(\n+    const HloModule& hlo_module, absl::string_view frontend_attr_name,\n+    size_t num_values) {\n+  const auto& frontend_attrs = hlo_module.frontend_attributes().map();\n+  auto iter = frontend_attrs.find(frontend_attr_name);\n+  if (iter == frontend_attrs.end()) {\n+    // Return all default layouts if frontend attr isn't present.\n+    return std::vector<LayoutMode>(num_values);\n+  }\n+  return GetLayoutModesFromFrontendAttr(iter->second);\n+}\n+\n }  // namespace\n \n char PjRtCompatibleExecutable::ID = 0;\n@@ -228,11 +296,22 @@ absl::StatusOr<LoadedExecutableRef> PjRtLoadedExecutable::Create(\n   TF_ASSIGN_OR_RETURN(\n       auto result_memory_kinds,\n       GetFirstModuleOutputMemoryKinds(pjrt_loaded_executable.get()));\n+  TF_ASSIGN_OR_RETURN(auto hlo_modules,\n+                      pjrt_loaded_executable->GetHloModules());\n+  if (hlo_modules.empty()) {\n+    return FailedPrecondition(\"Requires at least one HloModule.\");\n+  }\n+  TF_ASSIGN_OR_RETURN(std::vector<xla::LayoutMode> output_layout_modes,\n+                      GetLayoutModes(*hlo_modules.front(), \"out_layout_modes\",\n+                                     result_element_types.size()));\n+  TF_ASSIGN_OR_RETURN(auto output_layouts,\n+                      GetFirstModuleOutputLayouts(pjrt_loaded_executable.get(),\n+                                                  output_layout_modes));\n   return CreateInternal(client, std::move(pjrt_loaded_executable),\n                         result_element_types, result_dimensions,\n                         /*result_hlo_sharding=*/std::nullopt,\n-                        result_memory_kinds, loaded_host_callbacks,\n-                        std::move(executable_devices));\n+                        result_memory_kinds, output_layouts,\n+                        loaded_host_callbacks, std::move(executable_devices));\n }\n \n static absl::StatusOr<std::vector<xla::Shape>> ResultShapesOfModule(\n@@ -271,7 +350,10 @@ absl::StatusOr<LoadedExecutableRef> PjRtLoadedExecutable::Create(\n \n   // We have to do process the MLIR before the compile call, since the latter\n   // will use the MLIR as scratch space, or possibly even deallocate it.\n-  TF_ASSIGN_OR_RETURN(auto result_shapes, ResultShapesOfModule(module));\n+  TF_ASSIGN_OR_RETURN(const std::vector<xla::Shape> result_shapes,\n+                      ResultShapesOfModule(module));\n+  TF_ASSIGN_OR_RETURN(const std::vector<xla::LayoutMode> output_layout_modes,\n+                      GetOutputLayoutModes(module));\n \n   TF_ASSIGN_OR_RETURN(auto pjrt_loaded_executable,\n                       client->pjrt_client()->CompileAndLoad(\n@@ -290,10 +372,14 @@ absl::StatusOr<LoadedExecutableRef> PjRtLoadedExecutable::Create(\n     TF_ASSIGN_OR_RETURN(\n         auto result_memory_kinds,\n         GetFirstModuleOutputMemoryKinds(pjrt_loaded_executable.get()));\n+    TF_ASSIGN_OR_RETURN(auto output_layouts,\n+                        GetFirstModuleOutputLayouts(\n+                            pjrt_loaded_executable.get(), output_layout_modes));\n     return CreateInternal(client, std::move(pjrt_loaded_executable),\n                           result_element_types, result_dimensions,\n                           /*result_hlo_sharding=*/std::nullopt,\n-                          result_memory_kinds, std::move(loaded_host_callbacks),\n+                          result_memory_kinds, output_layouts,\n+                          std::move(loaded_host_callbacks),\n                           std::move(executable_devices));\n   } else {\n     VLOG(3) << \"Using full shape\";\n@@ -319,11 +405,14 @@ absl::StatusOr<LoadedExecutableRef> PjRtLoadedExecutable::Create(\n     TF_ASSIGN_OR_RETURN(\n         auto result_memory_kinds,\n         GetFirstModuleOutputMemoryKinds(pjrt_loaded_executable.get()));\n-    return CreateInternal(client, std::move(pjrt_loaded_executable),\n-                          shape_partial_info.element_types,\n-                          shape_partial_info.dimensions, result_hlo_sharding,\n-                          result_memory_kinds, std::move(loaded_host_callbacks),\n-                          std::move(executable_devices));\n+    TF_ASSIGN_OR_RETURN(auto output_layouts,\n+                        GetFirstModuleOutputLayouts(\n+                            pjrt_loaded_executable.get(), output_layout_modes));\n+    return CreateInternal(\n+        client, std::move(pjrt_loaded_executable),\n+        shape_partial_info.element_types, shape_partial_info.dimensions,\n+        result_hlo_sharding, result_memory_kinds, output_layouts,\n+        std::move(loaded_host_callbacks), std::move(executable_devices));\n   }\n }\n \n@@ -334,6 +423,7 @@ absl::StatusOr<LoadedExecutableRef> PjRtLoadedExecutable::CreateInternal(\n     absl::Span<const xla::DimensionVector> result_dimensions,\n     const std::optional<xla::HloSharding>& result_hlo_sharding,\n     const std::optional<std::vector<absl::string_view>>& result_memory_kinds,\n+    const std::vector<std::shared_ptr<const xla::PjRtLayout>>& output_layouts,\n     std::vector<tsl::RCReference<LoadedHostCallback>> loaded_host_callbacks,\n     DeviceListRef executable_devices) {\n   // For jit(pmap(...)), the device assignment (passed as `executable_devices`)\n@@ -493,7 +583,8 @@ absl::StatusOr<LoadedExecutableRef> PjRtLoadedExecutable::CreateInternal(\n       client, std::move(pjrt_loaded_executable), std::move(executable_devices),\n       std::move(addressable_devices), std::move(loaded_host_callbacks),\n       std::move(host_send_and_recv_callbacks), std::move(output_dtypes),\n-      std::move(output_shapes), std::move(output_shardings)));\n+      std::move(output_shapes), std::move(output_shardings),\n+      std::move(output_layouts)));\n }\n \n PjRtLoadedExecutable::PjRtLoadedExecutable(\n@@ -504,7 +595,8 @@ PjRtLoadedExecutable::PjRtLoadedExecutable(\n     std::vector<PjRtHostSendAndRecvLoadedHostCallback*>\n         host_send_recv_callbacks,\n     std::vector<DType> output_dtypes, std::vector<Shape> output_shapes,\n-    std::vector<ShardingRef> output_shardings)\n+    std::vector<ShardingRef> output_shardings,\n+    std::vector<std::shared_ptr<const xla::PjRtLayout>> output_layouts)\n     : client_(client),\n       pjrt_loaded_executable_(std::move(pjrt_loaded_executable)),\n       devices_(std::move(devices)),\n@@ -516,6 +608,7 @@ PjRtLoadedExecutable::PjRtLoadedExecutable(\n       output_dtypes_(std::move(output_dtypes)),\n       output_shapes_(std::move(output_shapes)),\n       output_shardings_(std::move(output_shardings)),\n+      output_layouts_(std::move(output_layouts)),\n       user_context_(UserContextScope::current()) {}\n \n PjRtLoadedExecutable::~PjRtLoadedExecutable() = default;\n@@ -719,39 +812,6 @@ PjRtLoadedExecutable::Execute(absl::Span<ArrayRef> args,\n   // memory_kind shares the same Sharding object.\n   absl::flat_hash_map<MemoryKind, ShardingRef> single_device_shardings;\n \n-  // TODO(emilyaf): Simplify the handling of layouts here when they're plumbed\n-  // through from JAX.\n-  std::vector<std::shared_ptr<const xla::PjRtLayout>> layouts;\n-  layouts.reserve(num_outputs);\n-  if (!pjrt_outputs.empty()) {\n-    for (int i = 0; i < num_outputs; ++i) {\n-      auto layout = output_dtypes_[i].kind() == xla::ifrt::DType::kToken\n-                        ? std::make_shared<xla::PjRtLayout>(xla::Layout())\n-                        : pjrt_outputs.front()[i]->layout();\n-      layouts.push_back(std::move(layout));\n-    }\n-  } else {\n-    auto maybe_layouts = GetOutputLayouts();\n-    if (absl::IsUnimplemented(maybe_layouts.status())) {\n-      for (int i = 0; i < num_outputs; ++i) {\n-        std::shared_ptr<const xla::PjRtLayout> layout;\n-        if (output_dtypes_[i].kind() == xla::ifrt::DType::kToken) {\n-          layout = std::make_shared<xla::PjRtLayout>(xla::Layout());\n-        } else {\n-          TF_ASSIGN_OR_RETURN(layout,\n-                              client_->GetDefaultPjRtLayout(\n-                                  output_dtypes_[i], output_shapes_[i].dims(),\n-                                  devices_->devices().front(),\n-                                  output_shardings_[i]->memory_kind()));\n-        }\n-        layouts.push_back(std::move(layout));\n-      }\n-    } else {\n-      TF_RETURN_IF_ERROR(maybe_layouts.status());\n-      layouts = *std::move(maybe_layouts);\n-    }\n-  }\n-\n   for (int i = 0; i < num_outputs; ++i) {\n     PjRtArray::PjRtBuffers buffers;\n     buffers.reserve(num_computations);\n@@ -792,7 +852,7 @@ PjRtLoadedExecutable::Execute(absl::Span<ArrayRef> args,\n     }\n     outputs.push_back(*PjRtArray::Create(\n         client_, output_dtypes_[i], output_shapes_[i], *std::move(sharding),\n-        std::move(buffers), std::move(layouts[i])));\n+        std::move(buffers), output_layouts_[i]));\n   }\n \n   ExecuteResult result;"
        },
        {
            "sha": "c9c25de1bcf5b6df7561934554d5e878f637253d",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_executable.h",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.h?ref=55371dfcb4717b4425ece2d10cf707fd7bca67b9",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_sharding.h\"\n #include \"xla/pjrt/pjrt_client.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n+#include \"xla/pjrt/pjrt_layout.h\"\n #include \"xla/python/ifrt/array.h\"\n #include \"xla/python/ifrt/attribute_map.h\"\n #include \"xla/python/ifrt/device.h\"\n@@ -127,6 +128,9 @@ class PjRtExecutable final\n \n   absl::StatusOr<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n   GetOutputLayouts() const override {\n+    // TODO(hyeontaek): Return `output_layouts_` instead, which can distinguish\n+    // between default and custom layouts, once the users of\n+    // `GetOutputLayouts()` understand `nullptr` elements.\n     DCHECK(this);\n     return pjrt_executable_->GetOutputLayouts();\n   }\n@@ -335,6 +339,7 @@ class PjRtLoadedExecutable final\n       absl::Span<const xla::DimensionVector> result_dimensions,\n       const std::optional<xla::HloSharding>& result_hlo_sharding,\n       const std::optional<std::vector<absl::string_view>>& result_memory_kinds,\n+      const std::vector<std::shared_ptr<const xla::PjRtLayout>>& output_layouts,\n       std::vector<tsl::RCReference<LoadedHostCallback>> loaded_host_callbacks,\n       DeviceListRef executable_devices);\n \n@@ -347,7 +352,8 @@ class PjRtLoadedExecutable final\n       std::vector<PjRtHostSendAndRecvLoadedHostCallback*>\n           host_send_recv_callbacks,\n       std::vector<DType> output_dtypes, std::vector<Shape> output_shapes,\n-      std::vector<ShardingRef> output_shardings);\n+      std::vector<ShardingRef> output_shardings,\n+      std::vector<std::shared_ptr<const xla::PjRtLayout>> output_layouts);\n \n   PjRtClient* client_;\n   std::shared_ptr<xla::PjRtLoadedExecutable> pjrt_loaded_executable_;\n@@ -366,6 +372,7 @@ class PjRtLoadedExecutable final\n   std::vector<DType> output_dtypes_;\n   std::vector<Shape> output_shapes_;\n   std::vector<ShardingRef> output_shardings_;\n+  std::vector<std::shared_ptr<const xla::PjRtLayout>> output_layouts_;\n   const xla::ifrt::UserContextRef user_context_;\n };\n "
        },
        {
            "sha": "f4a83f08f315512fc6e5c3ab2e6ed6229e0e53b5",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/reshard_impl_test_lib.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Freshard_impl_test_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Freshard_impl_test_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Freshard_impl_test_lib.cc?ref=55371dfcb4717b4425ece2d10cf707fd7bca67b9",
            "patch": "@@ -401,8 +401,18 @@ TEST_F(ReshardTest, DifferentDestinationLayout) {\n \n   // Make sure that the destination layout is actually different from the source\n   // layout in order to ensure the test coverage.\n-  TF_ASSERT_OK_AND_ASSIGN(const auto src_layout, src_array->pjrt_layout());\n-  ASSERT_NE(src_layout, nullptr);\n+  TF_ASSERT_OK_AND_ASSIGN(std::shared_ptr<const xla::PjRtLayout> src_layout,\n+                          src_array->pjrt_layout());\n+  if (src_layout == nullptr) {\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        Shape shard_shape,\n+        src_array->sharding().GetShardShape(src_array->shape()));\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        src_layout, client_->GetDefaultPjRtLayout(\n+                        src_array->dtype(), shard_shape.dims(),\n+                        src_array->sharding().devices()->devices().front(),\n+                        src_array->sharding().memory_kind()));\n+  }\n   ASSERT_NE(src_layout->xla_layout(), dst_array_spec.layout->xla_layout());\n \n   TF_ASSERT_OK_AND_ASSIGN("
        },
        {
            "sha": "27b57346270bcb078f9918023803cc056e67e6e9",
            "filename": "third_party/xla/xla/python/transfer/pjrt_transfer_server.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fpjrt_transfer_server.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fpjrt_transfer_server.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fpjrt_transfer_server.cc?ref=55371dfcb4717b4425ece2d10cf707fd7bca67b9",
            "patch": "@@ -249,18 +249,18 @@ absl::Status PjRtTransferServer::CrossHostPull(\n         xla::DimensionVector(shape.dims().begin(), shape.dims().end())};\n     shape_specs.push_back(shape_spec);\n \n-    auto pjrt_layout = arrays[i]->pjrt_layout();\n+    absl::StatusOr<std::shared_ptr<const xla::PjRtLayout>> pjrt_layout =\n+        arrays[i]->pjrt_layout();\n     std::optional<xla::Layout> layout;\n     if (pjrt_layout.ok() && *pjrt_layout == nullptr) {\n       TF_ASSIGN_OR_RETURN(\n           xla::ifrt::Shape shard_shape,\n           arrays[i]->sharding().GetShardShape(arrays[i]->shape()));\n       TF_ASSIGN_OR_RETURN(\n-          std::shared_ptr<const xla::PjRtLayout> layout,\n-          arrays[i]->client()->GetDefaultPjRtLayout(\n-              arrays[i]->dtype(), shard_shape.dims(),\n-              arrays[i]->sharding().devices()->devices().front(),\n-              arrays[i]->sharding().memory_kind()));\n+          pjrt_layout, arrays[i]->client()->GetDefaultPjRtLayout(\n+                           arrays[i]->dtype(), shard_shape.dims(),\n+                           arrays[i]->sharding().devices()->devices().front(),\n+                           arrays[i]->sharding().memory_kind()));\n     }\n     if (pjrt_layout.ok()) {\n       layout = (*pjrt_layout)->xla_layout();"
        },
        {
            "sha": "fadeda5b8b9387e6d10094a6328222f2d1034b99",
            "filename": "third_party/xla/xla/python/transfer/streaming_ifrt_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fstreaming_ifrt_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fstreaming_ifrt_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Ftransfer%2Fstreaming_ifrt_test.cc?ref=55371dfcb4717b4425ece2d10cf707fd7bca67b9",
            "patch": "@@ -89,8 +89,10 @@ absl::StatusOr<SingleBufferCopyPlan> SetupTransferDestList(\n   size_t copy_size = xla::ShapeUtil::ByteSizeOf(shape);\n \n   results.dests.push_back(MakeDmaDestination(atm, 0, copy_size));\n-  TF_ASSIGN_OR_RETURN(auto arr,\n-                      ifrt_client->CreatePjRtArray(atm->RetrieveBuffer(0)));\n+  // `CreateBuffersForAsyncHostToDevice` uses a default layout.\n+  TF_ASSIGN_OR_RETURN(\n+      auto arr, ifrt_client->CreatePjRtArray(atm->RetrieveBuffer(0),\n+                                             /*has_custom_layout=*/false));\n   results.arrays.push_back(std::move(arr));\n   return results;\n }"
        },
        {
            "sha": "0cb49f9027b50656bfc84ac4dbd8ea2d009c253c",
            "filename": "third_party/xla/xla/python/version.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fversion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55371dfcb4717b4425ece2d10cf707fd7bca67b9/third_party%2Fxla%2Fxla%2Fpython%2Fversion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fversion.h?ref=55371dfcb4717b4425ece2d10cf707fd7bca67b9",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n \n // An increasing version number to protect jax code against breaking changes.\n // In JAX, reference this via jax._src.lib.ifrt_version.\n-#define JAX_IFRT_VERSION_NUMBER 33\n+#define JAX_IFRT_VERSION_NUMBER \\\n+  34  // Explicit `has_custom_layout` argument in PjRt-IFRT Array creation.\n \n #endif  // XLA_PYTHON_VERSION_H_"
        }
    ],
    "stats": {
        "total": 424,
        "additions": 291,
        "deletions": 133
    }
}