{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 832193113",
    "sha": "3a02032e1660aa8d1901f70912c3af6e2de4213f",
    "files": [
        {
            "sha": "d8d38eb58e9ae2fd12d46438af567b6591853d46",
            "filename": "tensorflow/core/framework/attr_value_util.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 23,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fattr_value_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fattr_value_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fattr_value_util.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -96,7 +96,7 @@ constexpr int kMaxTensorNestDepth = 100;\n // to serialize, compute hash based on TensorProto string representation.\n // This approach may result different hash codes with identical Tensors if they\n // are defined with different TensorProto representations.\n-uint64 TensorProtoHash(const TensorProto& tp) {\n+uint64_t TensorProtoHash(const TensorProto& tp) {\n   Tensor tensor(tp.dtype());\n   bool success = tensor.FromProto(tp);\n   if (success) {\n@@ -112,7 +112,7 @@ uint64 TensorProtoHash(const TensorProto& tp) {\n // string representation. Tensors with identical content potentially can have a\n // different hash code if they are defined with different TensorProto\n // representations.\n-uint64 FastTensorProtoHash(const TensorProto& tp) {\n+uint64_t FastTensorProtoHash(const TensorProto& tp) {\n   if (attr_value_util_internal::TensorByteSize(tp) >\n       kMaxAttrValueTensorByteSize) {\n     return DeterministicProtoHash64(tp);\n@@ -180,15 +180,17 @@ bool AreTensorProtosEqual(const TensorProto& lhs, const TensorProto& rhs,\n   return AreSerializedProtosEqual(lhs_tp, rhs_tp);\n }\n \n-using TensorProtoHasher = std::function<uint64(const TensorProto&)>;\n+using TensorProtoHasher = std::function<uint64_t(const TensorProto&)>;\n \n-uint64 AttrValueHash(const AttrValue& a, const TensorProtoHasher& tensor_hash) {\n+uint64_t AttrValueHash(const AttrValue& a,\n+                       const TensorProtoHasher& tensor_hash) {\n   if (a.has_tensor()) return tensor_hash(a.tensor());\n \n   if (a.has_func()) {\n     const NameAttrList& func = a.func();\n-    uint64 h = Hash64(func.name());\n-    std::map<string, AttrValue> map(func.attr().begin(), func.attr().end());\n+    uint64_t h = Hash64(func.name());\n+    std::map<std::string, AttrValue> map(func.attr().begin(),\n+                                         func.attr().end());\n     for (const auto& pair : map) {\n       h = Hash64(pair.first.data(), pair.first.size(), h);\n       h = Hash64Combine(AttrValueHash(pair.second, tensor_hash), h);\n@@ -200,8 +202,8 @@ uint64 AttrValueHash(const AttrValue& a, const TensorProtoHasher& tensor_hash) {\n   return DeterministicProtoHash64(a);\n }\n \n-string SummarizeString(const string& str) {\n-  string escaped = absl::CEscape(str);\n+std::string SummarizeString(const std::string& str) {\n+  std::string escaped = absl::CEscape(str);\n \n   // If the string is long, replace the middle with ellipses.\n   constexpr int kMaxStringSummarySize = 80;\n@@ -216,7 +218,7 @@ string SummarizeString(const string& str) {\n   }\n }\n \n-string SummarizeTensor(const TensorProto& tensor_proto) {\n+std::string SummarizeTensor(const TensorProto& tensor_proto) {\n   Tensor t;\n   int64_t tensor_byte_size =\n       attr_value_util_internal::TensorByteSize(tensor_proto);\n@@ -233,16 +235,17 @@ string SummarizeTensor(const TensorProto& tensor_proto) {\n   return t.DebugString();\n }\n \n-string SummarizeFunc(const NameAttrList& func) {\n-  std::vector<string> entries;\n+std::string SummarizeFunc(const NameAttrList& func) {\n+  std::vector<std::string> entries;\n   for (const auto& p : func.attr()) {\n     entries.push_back(absl::StrCat(p.first, \"=\", SummarizeAttrValue(p.second)));\n   }\n   std::sort(entries.begin(), entries.end());\n   return absl::StrCat(func.name(), \"[\", absl::StrJoin(entries, \", \"), \"]\");\n }\n \n-bool ParseAttrValueHelper_TensorNestsUnderLimit(int limit, string to_parse) {\n+bool ParseAttrValueHelper_TensorNestsUnderLimit(int limit,\n+                                                std::string to_parse) {\n   int nests = 0;\n   int maxed_out = to_parse.length();\n   int open_curly = to_parse.find('{');\n@@ -292,7 +295,7 @@ bool ParseAttrValueHelper_TensorNestsUnderLimit(int limit, string to_parse) {\n \n }  // namespace\n \n-string SummarizeAttrValue(const AttrValue& attr_value) {\n+std::string SummarizeAttrValue(const AttrValue& attr_value) {\n   switch (attr_value.value_case()) {\n     case AttrValue::kS:\n       return SummarizeString(attr_value.s());\n@@ -309,7 +312,7 @@ string SummarizeAttrValue(const AttrValue& attr_value) {\n     case AttrValue::kTensor:\n       return SummarizeTensor(attr_value.tensor());\n     case AttrValue::kList: {\n-      std::vector<string> pieces;\n+      std::vector<std::string> pieces;\n       if (attr_value.list().s_size() > 0) {\n         for (int i = 0; i < attr_value.list().s_size(); ++i) {\n           pieces.push_back(SummarizeString(attr_value.list().s(i)));\n@@ -472,7 +475,7 @@ absl::Status AttrValueHasType(const AttrValue& attr_value,\n bool ParseAttrValue(absl::string_view type, absl::string_view text,\n                     AttrValue* out) {\n   // Parse type.\n-  string field_name;\n+  std::string field_name;\n   bool is_list = absl::ConsumePrefix(&type, \"list(\");\n   if (absl::ConsumePrefix(&type, \"string\")) {\n     field_name = \"s\";\n@@ -500,7 +503,7 @@ bool ParseAttrValue(absl::string_view type, absl::string_view text,\n   }\n \n   // Construct a valid text proto message to parse.\n-  string to_parse;\n+  std::string to_parse;\n   if (is_list) {\n     // TextFormat parser considers \"i: 7\" to be the same as \"i: [7]\",\n     // but we only want to allow list values with [].\n@@ -550,8 +553,8 @@ void SetAttrValue(const AttrValue& value, AttrValue* out) { *out = value; }\n   DEFINE_SET_ATTR_VALUE_ONE(ARG_TYPE, FIELD)        \\\n   DEFINE_SET_ATTR_VALUE_LIST(gtl::ArraySlice<ARG_TYPE>, FIELD)\n \n-DEFINE_SET_ATTR_VALUE_ONE(const string&, s)\n-DEFINE_SET_ATTR_VALUE_LIST(absl::Span<const string>, s)\n+DEFINE_SET_ATTR_VALUE_ONE(const std::string&, s)\n+DEFINE_SET_ATTR_VALUE_LIST(absl::Span<const std::string>, s)\n DEFINE_SET_ATTR_VALUE_BOTH(const char*, s)\n DEFINE_SET_ATTR_VALUE_BOTH(int64_t, i)\n DEFINE_SET_ATTR_VALUE_BOTH(int32_t, i)\n@@ -585,7 +588,7 @@ void SetAttrValue(const absl::Span<const absl::string_view> value,\n   }\n }\n \n-void MoveAttrValue(std::vector<string>&& value, AttrValue* out) {\n+void MoveAttrValue(std::vector<std::string>&& value, AttrValue* out) {\n   out->mutable_list()->Clear();  // Create list() even if value empty.\n   for (auto& v : value) {\n     out->mutable_list()->add_s(std::move(v));\n@@ -689,8 +692,8 @@ bool AreAttrValuesEqual(const AttrValue& a, const AttrValue& b,\n     const NameAttrList& af = a.func();\n     const NameAttrList& bf = b.func();\n     if (af.name() != bf.name()) return false;\n-    std::unordered_map<string, AttrValue> am(af.attr().begin(),\n-                                             af.attr().end());\n+    std::unordered_map<std::string, AttrValue> am(af.attr().begin(),\n+                                                  af.attr().end());\n     for (const auto& bm_pair : bf.attr()) {\n       const auto& iter = am.find(bm_pair.first);\n       if (iter == am.end()) return false;\n@@ -708,11 +711,11 @@ bool AreAttrValuesEqual(const AttrValue& a, const AttrValue& b,\n   return AreSerializedProtosEqual(a, b);\n }\n \n-uint64 AttrValueHash(const AttrValue& a) {\n+uint64_t AttrValueHash(const AttrValue& a) {\n   return AttrValueHash(a, TensorProtoHash);\n }\n \n-uint64 FastAttrValueHash(const AttrValue& a) {\n+uint64_t FastAttrValueHash(const AttrValue& a) {\n   return AttrValueHash(a, FastTensorProtoHash);\n }\n "
        },
        {
            "sha": "135bfe67231f37bdf1509df2ac66e91703fa0333",
            "filename": "tensorflow/core/framework/attr_value_util.h",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fattr_value_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fattr_value_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fattr_value_util.h?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -76,12 +76,12 @@ void SetAttrValue(const Tensor& value, AttrValue* out);\n void SetAttrValue(const TensorProto& value, AttrValue* out);\n void SetAttrValue(const NameAttrList& value, AttrValue* out);\n \n-void SetAttrValue(absl::Span<const string> value, AttrValue* out);\n+void SetAttrValue(absl::Span<const std::string> value, AttrValue* out);\n void SetAttrValue(absl::Span<const tstring> value, AttrValue* out);\n void SetAttrValue(absl::Span<const char* const> value, AttrValue* out);\n void SetAttrValue(absl::Span<const absl::string_view> value, AttrValue* out);\n void SetAttrValue(absl::Span<const int64_t> value, AttrValue* out);\n-void SetAttrValue(absl::Span<const int32> value, AttrValue* out);\n+void SetAttrValue(absl::Span<const int32_t> value, AttrValue* out);\n void SetAttrValue(absl::Span<const float> value, AttrValue* out);\n void SetAttrValue(absl::Span<const double> value, AttrValue* out);\n void SetAttrValue(absl::Span<const bool> value, AttrValue* out);\n@@ -97,15 +97,15 @@ void SetAttrValue(absl::Span<const NameAttrList> value, AttrValue* out);\n \n void SetAttrValue(const AttrValue& value, AttrValue* out);\n \n-void MoveAttrValue(std::vector<string>&& value, AttrValue* out);\n+void MoveAttrValue(std::vector<std::string>&& value, AttrValue* out);\n \n // Returns a hash of `a` that is consistent with AreAttrValuesEqual. In other\n // words, if two AttrValues compare equal according to AreAttrValuesEqual,\n // they will have the same hash value.\n // Similarly to protobuf deterministic serialization, hash value is\n // guaranteed to be stable only for a given binary. In particular, one should\n // probably not persist the returned value.\n-uint64 AttrValueHash(const AttrValue& a);\n+uint64_t AttrValueHash(const AttrValue& a);\n \n // WARNING: Equality check might return false-negative for large (> 32mb)\n // tensors defined with different TensorProto representations.\n@@ -117,7 +117,7 @@ uint64 AttrValueHash(const AttrValue& a);\n // bool_val), they will have different hash code and equals will return false.\n // Small (less than 32mb) tensors with different TensorProto representations\n // hashed/compared by their tensor content.\n-uint64 FastAttrValueHash(const AttrValue& a);\n+uint64_t FastAttrValueHash(const AttrValue& a);\n // Returns true if a and b have the same value. If false negatives are allowed,\n // then compares proto representation to avoid construction of large (> 32mb)\n // tensors.\n@@ -134,7 +134,7 @@ bool HasPlaceHolder(const AttrValue& val);\n // SubstituteFunc is given a placeholder string. If the placeholder is\n // unknown, SubstituteFunc returns false. Otherwise, overwrites the\n // attr value and returns true.\n-using SubstituteFunc = std::function<bool(const string&, AttrValue*)>;\n+using SubstituteFunc = std::function<bool(const std::string&, AttrValue*)>;\n bool SubstitutePlaceholders(const SubstituteFunc& substitute, AttrValue* value);\n \n }  // namespace tensorflow"
        },
        {
            "sha": "d6d685ef4c49f0f0417484463d2c255cbf0ca846",
            "filename": "tensorflow/core/framework/attr_value_util_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fattr_value_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fattr_value_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fattr_value_util_test.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -36,22 +36,23 @@ AttrValue V(T value) {\n   return ret;\n }\n \n-AttrValue P(const string& p) {\n+AttrValue P(const std::string& p) {\n   AttrValue ret;\n   ret.set_placeholder(p);\n   return ret;\n }\n \n-AttrValue F(const string& name,\n-            std::vector<std::pair<string, AttrValue>> pairs) {\n+AttrValue F(const std::string& name,\n+            std::vector<std::pair<std::string, AttrValue>> pairs) {\n   AttrValue ret;\n   ret.mutable_func()->set_name(name);\n   ret.mutable_func()->mutable_attr()->insert(pairs.begin(), pairs.end());\n   return ret;\n }\n \n AttrValue Fs(\n-    std::vector<std::pair<string, std::vector<std::pair<string, AttrValue>>>>\n+    std::vector<\n+        std::pair<std::string, std::vector<std::pair<std::string, AttrValue>>>>\n         funcs) {\n   AttrValue ret;\n   for (const auto& func : funcs) {\n@@ -82,7 +83,7 @@ TEST(AttrValueUtil, HasType) {\n }\n \n SubstituteFunc ReplaceTWith(const AttrValue& val) {\n-  return [val](const string& placeholder, AttrValue* target) {\n+  return [val](const std::string& placeholder, AttrValue* target) {\n     if (placeholder == \"T\") {\n       *target = val;\n       return true;\n@@ -142,14 +143,14 @@ TEST(AttrValueUtil, DeepAttr) {\n \n TEST(AttrValueUtil, SummarizeAttrValueDoesNotElideShortStrings) {\n   AttrValue attr_value;\n-  SetAttrValue(string(40, '-'), &attr_value);\n-  EXPECT_EQ(absl::StrCat(\"\\\"\", string(40, '-'), \"\\\"\"),\n+  SetAttrValue(std::string(40, '-'), &attr_value);\n+  EXPECT_EQ(absl::StrCat(\"\\\"\", std::string(40, '-'), \"\\\"\"),\n             SummarizeAttrValue(attr_value));\n }\n \n TEST(AttrValueUtil, SummarizeAttrValueElidesLongStrings) {\n   AttrValue attr_value;\n-  SetAttrValue(string(80, '-'), &attr_value);\n+  SetAttrValue(std::string(80, '-'), &attr_value);\n   EXPECT_EQ(\"\\\"----------...----------\\\"\", SummarizeAttrValue(attr_value));\n }\n \n@@ -197,7 +198,7 @@ TEST(AttrValueUtil, TensorByteSizeShouldNotOverflow) {\n   }\n }\n \n-AttrValue FromText(const string& text) {\n+AttrValue FromText(const std::string& text) {\n   AttrValue attr;\n   EXPECT_TRUE(protobuf::TextFormat::MergeFromString(text, &attr));\n   return attr;"
        },
        {
            "sha": "e4456d888df736cd4a041bc4042072b45df70260",
            "filename": "tensorflow/core/framework/collective.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 14,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fcollective.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fcollective.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fcollective.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -32,11 +32,11 @@ struct RegistrationInfo {\n   // what is effectively a static instance of the collective implementation.\n   // During param resolution of collective ops we return this static instance.\n   // The actual op execution gets a fresh instance using `factory`.\n-  RegistrationInfo(const string& n, CollectiveRegistry::Factory f)\n+  RegistrationInfo(const std::string& n, CollectiveRegistry::Factory f)\n       : name(n),\n         factory(std::move(f)),\n         param_resolver_instance(this->factory()) {}\n-  string name;\n+  std::string name;\n   CollectiveRegistry::Factory factory;\n   CollectiveImplementationInterface* param_resolver_instance;\n };\n@@ -48,13 +48,13 @@ std::vector<RegistrationInfo>* MutableCollectiveRegistry() {\n }\n }  // namespace\n \n-string CollGroupRuntimeDetails::ToString() const {\n+std::string CollGroupRuntimeDetails::ToString() const {\n   return absl::StrCat(\"CollGroupRuntimeDetails {communicator_key=\",\n                       absl::CEscape(communicator_key), \"}\");\n }\n \n-string CollGroupParams::ToString() const {\n-  string v = strings::StrCat(\n+std::string CollGroupParams::ToString() const {\n+  std::string v = strings::StrCat(\n       \"CollGroupParams {group_key=\", group_key, \" group_size=\", group_size,\n       \" device_type=\", device_type.type_string(), \" num_tasks=\", num_tasks,\n       \" runtime_details=\", runtime_details.ToString(), \" devices {\");\n@@ -94,8 +94,8 @@ CollInstanceParams& CollInstanceParams::operator=(\n   return *this;\n }\n \n-string CollInstanceParams::ToString() const {\n-  string v =\n+std::string CollInstanceParams::ToString() const {\n+  std::string v =\n       strings::StrCat(\"CollInstanceParams { instance_key=\", instance_key,\n                       \" type=\", type, \" data_type=\", DataTypeString(data_type),\n                       \" shape=\", shape.DebugString(), \" devices {\");\n@@ -134,8 +134,9 @@ string CollInstanceParams::ToString() const {\n   return v;\n }\n \n-string CollectiveParams::ToString() const {\n-  string v = absl::StrCat(\"CollectiveParams \", name, \" {\", group.ToString());\n+std::string CollectiveParams::ToString() const {\n+  std::string v =\n+      absl::StrCat(\"CollectiveParams \", name, \" {\", group.ToString());\n   absl::StrAppend(&v, \" \", instance.ToString());\n   strings::StrAppend(&v, \" default_rank=\", default_rank,\n                      \" is_source=\", is_source, \" source_rank=\", source_rank,\n@@ -156,7 +157,7 @@ CollectiveContext::CollectiveContext(\n     CollectiveExecutor* col_exec, NcclCommunicatorInterface* nccl_communicator,\n     const DeviceMgr* dev_mgr, OpKernelContext* ctx,\n     OpKernelContext::Params* op_params, const CollectiveParams* col_params,\n-    const string& exec_key, int64_t step_id, const Tensor* input,\n+    const std::string& exec_key, int64_t step_id, const Tensor* input,\n     Tensor* output)\n     : col_exec(col_exec),\n       nccl_communicator(nccl_communicator),\n@@ -177,14 +178,14 @@ int64_t CollectiveExecutor::kInvalidId = -1;\n \n /*static*/\n absl::Status CollectiveRegistry::Lookup(\n-    const string& collective_name,\n+    const std::string& collective_name,\n     CollectiveImplementationInterface** implementation) {\n   return LookupHelper(collective_name, implementation, false);\n }\n \n /*static*/\n absl::Status CollectiveRegistry::LookupParamResolverInstance(\n-    const string& collective_name,\n+    const std::string& collective_name,\n     CollectiveImplementationInterface** implementation) {\n   return LookupHelper(collective_name, implementation, true);\n }\n@@ -198,7 +199,7 @@ void CollectiveRegistry::GetAll(\n }\n \n /*static*/\n-absl::Status CollectiveRegistry::Register(const string& collective_name,\n+absl::Status CollectiveRegistry::Register(const std::string& collective_name,\n                                           Factory factory) {\n   std::vector<RegistrationInfo>* registry = MutableCollectiveRegistry();\n   for (const RegistrationInfo& reg_info : *registry) {\n@@ -212,7 +213,7 @@ absl::Status CollectiveRegistry::Register(const string& collective_name,\n \n /*static*/\n absl::Status CollectiveRegistry::LookupHelper(\n-    const string& collective_name,\n+    const std::string& collective_name,\n     CollectiveImplementationInterface** implementation, bool param_resolver) {\n   std::vector<RegistrationInfo>* registry = MutableCollectiveRegistry();\n   for (const RegistrationInfo& reg_info : *registry) {"
        },
        {
            "sha": "cdb22129e813d4c0e838addc51986d0669143197",
            "filename": "tensorflow/core/framework/collective.h",
            "status": "modified",
            "additions": 47,
            "deletions": 46,
            "changes": 93,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fcollective.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fcollective.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fcollective.h?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -56,25 +56,25 @@ enum CollectiveType {\n // the OpKernel.  Currently, this struct is used to set communicator key for\n // NCCL-based collective implementation.\n struct CollGroupRuntimeDetails {\n-  string communicator_key;  // for communicator-based techniques e.g. NCCL\n-  string ToString() const;\n+  std::string communicator_key;  // for communicator-based techniques e.g. NCCL\n+  std::string ToString() const;\n };\n \n struct CollGroupMember {\n   DeviceAttributes device;\n-  string task;\n+  std::string task;\n   bool is_local;\n   // User provided rank\n-  int32 rank = -1;\n+  int32_t rank = -1;\n };\n \n // Data common to all members of a device group.\n // All members share the same device set but its order is\n // particular to an instance so it is stored there.\n struct CollGroupParams {\n   // Inputs from Collective ops:\n-  int32 group_key;\n-  int32 group_size;\n+  int32_t group_key;\n+  int32_t group_size;\n   DeviceType device_type;\n   int user_specified_rank = -1;  // rank provided by the user.\n   // Generated from Collective Group Resolver:\n@@ -83,10 +83,10 @@ struct CollGroupParams {\n   // True if every task has the same number of devices.\n   bool same_num_devices_per_task = false;\n   // Task -> number of devices on that task.\n-  std::unordered_map<string, int32> num_devices_per_task;\n-  int32 num_tasks;  // number of distinct tasks in group\n+  std::unordered_map<std::string, int32_t> num_devices_per_task;\n+  int32_t num_tasks;  // number of distinct tasks in group\n   CollGroupRuntimeDetails runtime_details;\n-  string ToString() const;\n+  std::string ToString() const;\n   CollGroupParams()\n       : group_key(0), group_size(0), device_type(DEVICE_CPU), num_tasks(0) {}\n };\n@@ -99,7 +99,7 @@ struct CollGroupParams {\n // interpretation.  On first execution the runtime will update this\n // structure with decisions that will guide all subsequent executions.\n struct CollImplDetails {\n-  string collective_name;\n+  std::string collective_name;\n   std::vector<std::vector<int>> subdiv_permutations;\n   // subdiv_offsets and max_subdivs_per_device are used together as follows:\n   // When subdiv_offsets is provided (non-empty) it is used as is. When\n@@ -110,10 +110,10 @@ struct CollImplDetails {\n   int max_subdivs_per_device = -1;  // Upper bound on subdivisions per device.\n   std::vector<int> subdiv_offsets;\n   std::vector<int> subdiv_source_rank;  // rank of source in each subdiv\n-  std::vector<int32>\n-      dependencies;           // collective instances on which this node depends\n-  string communication_hint;  // user-supplied hint for implementation choice,\n-                              // e.g. ring or nccl\n+  std::vector<int32_t>\n+      dependencies;  // collective instances on which this node depends\n+  std::string communication_hint;  // user-supplied hint for implementation\n+                                   // choice, e.g. ring or nccl\n   float timeout_seconds;      // If non zero, set a completion timeout for the\n                               // collective op to detect staleness.\n };\n@@ -122,16 +122,16 @@ struct CollImplDetails {\n // TODO(b/163171014) Refactor this struct to not be a union of all fields.\n struct CollInstanceParams {\n   // Identifies all participating graph nodes.\n-  int32 instance_key = -1;\n+  int32_t instance_key = -1;\n   // The full identifier includes both instance_key and step_id.\n   int64_t step_id = 0;\n   CollectiveType type = UNDEFINED_COLLECTIVE;\n   DataType data_type = DT_FLOAT;\n   TensorShape shape = {0};\n   CollImplDetails impl_details;\n-  string ToString() const;\n+  std::string ToString() const;\n   CollInstanceParams& operator=(const struct CollInstanceParams& other);\n-  std::vector<string> devices;  // permuter only\n+  std::vector<std::string> devices;  // permuter only\n \n   // For permuter only\n   // Each rank in the permutation is a receiver.\n@@ -148,15 +148,15 @@ struct CollectiveParams : public core::RefCounted {\n   CollGroupParams group;\n   CollInstanceParams instance;\n \n-  string name = \"\";        // node name used only for log or error messages\n+  std::string name = \"\";   // node name used only for log or error messages\n   int default_rank = -1;   // index of this op within device_names\n   bool is_source = false;  // broadcast only\n   int source_rank = -1;    // broadcast only\n   // Rank of this device in each subdivision permutation.\n   std::vector<int> subdiv_rank;\n   OpKernel* merge_op = nullptr;  // reduction only\n   OpKernel* final_op = nullptr;  // reduction only\n-  string ToString() const;\n+  std::string ToString() const;\n   bool run_group_initialization = true;\n   bool is_stateless = false;\n };\n@@ -169,12 +169,12 @@ class DeviceResolverInterface {\n   virtual ~DeviceResolverInterface() {}\n \n   // Populates *attributes with the DeviceAttributes of the specified device.\n-  virtual absl::Status GetDeviceAttributes(const string& device,\n+  virtual absl::Status GetDeviceAttributes(const std::string& device,\n                                            DeviceAttributes* attributes) = 0;\n \n   // Returns all device attributes of a task.\n   virtual absl::Status GetAllDeviceAttributes(\n-      const string& task, std::vector<DeviceAttributes>* attributes) = 0;\n+      const std::string& task, std::vector<DeviceAttributes>* attributes) = 0;\n \n   // Updates device attributes. It returns error if any device already\n   // exists in the DeviceResolver and has a different incarnation.\n@@ -284,19 +284,17 @@ class CollectiveRemoteAccess {\n  public:\n   virtual ~CollectiveRemoteAccess() {}\n \n-  virtual void RecvFromPeer(const string& peer_device, const string& peer_task,\n-                            bool peer_is_local, const string& key,\n-                            Device* to_device, DeviceContext* to_device_ctx,\n-                            const AllocatorAttributes& to_alloc_attr,\n-                            Tensor* to_tensor,\n-                            const DeviceLocality& client_locality,\n-                            int dev_to_dev_stream_index,\n-                            CancellationManager* cancellation_manager,\n-                            const StatusCallback& done) = 0;\n-\n-  virtual void PostToPeer(const string& peer_device, const string& peer_task,\n-                          const string& key, Device* from_device,\n-                          DeviceContext* from_device_ctx,\n+  virtual void RecvFromPeer(\n+      const std::string& peer_device, const std::string& peer_task,\n+      bool peer_is_local, const std::string& key, Device* to_device,\n+      DeviceContext* to_device_ctx, const AllocatorAttributes& to_alloc_attr,\n+      Tensor* to_tensor, const DeviceLocality& client_locality,\n+      int dev_to_dev_stream_index, CancellationManager* cancellation_manager,\n+      const StatusCallback& done) = 0;\n+\n+  virtual void PostToPeer(const std::string& peer_device,\n+                          const std::string& peer_task, const std::string& key,\n+                          Device* from_device, DeviceContext* from_device_ctx,\n                           const AllocatorAttributes& from_alloc_attr,\n                           const Tensor* from_tensor,\n                           const DeviceLocality& client_locality,\n@@ -306,7 +304,8 @@ class CollectiveRemoteAccess {\n   // Checks the health of a collective peer. It probes the peer to see if it is\n   // alive. Note that if a peer has restarted, it's considered a different one,\n   // so CheckPeerHealth fails.\n-  virtual void CheckPeerHealth(const string& peer_task, int64_t timeout_in_ms,\n+  virtual void CheckPeerHealth(const std::string& peer_task,\n+                               int64_t timeout_in_ms,\n                                const StatusCallback& done) = 0;\n \n   virtual BufRendezvous* buf_rendezvous() = 0;\n@@ -322,7 +321,7 @@ class CollectiveExecutor : public core::RefCounted {\n \n   virtual void ExecuteAsync(OpKernelContext* ctx,\n                             const CollectiveParams* col_params,\n-                            const string& exec_key, StatusCallback done) {\n+                            const std::string& exec_key, StatusCallback done) {\n     done(errors::Internal(\n         \"A collective Op has been called in a context in which \"\n         \"a CollectiveExecutor has not been provided.\"));\n@@ -404,27 +403,28 @@ struct CollectiveContext {\n   OpKernelContext* op_ctx;                       // Not owned\n   OpKernelContext::Params* op_params;            // Not owned\n   core::IntrusivePtr<const CollectiveParams> col_params;\n-  const string exec_key;\n+  const std::string exec_key;\n   const int64_t step_id;\n   const Tensor* input;  // Not owned\n   Tensor* output;       // Not owned\n   Device* device;       // The device for which this instance labors\n-  const string device_name;\n+  const std::string device_name;\n   DeviceLocality device_locality;\n \n   CollectiveContext(CollectiveExecutor* col_exec,\n                     NcclCommunicatorInterface* nccl_communicator,\n                     const DeviceMgr* dev_mgr, OpKernelContext* ctx,\n                     OpKernelContext::Params* op_params,\n-                    const CollectiveParams* col_params, const string& exec_key,\n-                    int64_t step_id, const Tensor* input, Tensor* output);\n+                    const CollectiveParams* col_params,\n+                    const std::string& exec_key, int64_t step_id,\n+                    const Tensor* input, Tensor* output);\n };\n \n class NcclCommunicatorInterface {\n  public:\n   virtual ~NcclCommunicatorInterface() = default;\n \n-  virtual string GenerateCommunicatorKey() = 0;\n+  virtual std::string GenerateCommunicatorKey() = 0;\n \n   virtual void Enqueue(std::shared_ptr<CollectiveContext> col_ctx,\n                        StatusCallback done) = 0;\n@@ -474,15 +474,15 @@ class CollectiveRegistry {\n   // `collective_name`.  If found, creates an instance of the implementation and\n   // assign to `implementation`.\n   static absl::Status Lookup(\n-      const string& collective_name,\n+      const std::string& collective_name,\n       CollectiveImplementationInterface** implementation);\n \n   // Looks up a previously registered CollectiveImplementation under\n   // `collective_name`.  If found, returns the static instance of this\n   // implementation via `implementation`.  This instance should only be used to\n   // call InitializateCollectiveParams.\n   static absl::Status LookupParamResolverInstance(\n-      const string& collective_name,\n+      const std::string& collective_name,\n       CollectiveImplementationInterface** implementation);\n \n   // Returns all registered collective implementations.\n@@ -496,18 +496,19 @@ class CollectiveRegistry {\n   // the CollectiveImplementation.  Also creates a static instance of the\n   // implementation - this instance is used during param resolution and should\n   // only be used to call InitializeCollectiveParams.\n-  static absl::Status Register(const string& collective_name, Factory factory);\n+  static absl::Status Register(const std::string& collective_name,\n+                               Factory factory);\n \n   static absl::Status LookupHelper(\n-      const string& collective_name,\n+      const std::string& collective_name,\n       CollectiveImplementationInterface** implementation, bool param_resolver);\n };\n \n // Class used to call CollectiveRegistry::Register.  This should only be used to\n // create a global static object.\n class CollectiveRegistration {\n  public:\n-  CollectiveRegistration(const string& collective_name,\n+  CollectiveRegistration(const std::string& collective_name,\n                          CollectiveRegistry::Factory factory) {\n     TF_CHECK_OK(CollectiveRegistry::Register(collective_name, factory));\n   }"
        },
        {
            "sha": "bcfd94424e59c7e77791b3de747d15a29b1cdd1b",
            "filename": "tensorflow/core/framework/common_shape_fns.cc",
            "status": "modified",
            "additions": 51,
            "deletions": 50,
            "changes": 101,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fcommon_shape_fns.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fcommon_shape_fns.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fcommon_shape_fns.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -164,10 +164,10 @@ absl::Status EinsumShape(shape_inference::InferenceContext* c) {\n   // We assume that the equation has a valid format. Either (x),(y)->(z)\n   // or (x)->(z), where each of (x), (y) and (z) are concatenation of zero or\n   // more latin alphabets and contains at most one ellipsis ('...').\n-  string equation;\n+  std::string equation;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"equation\", &equation));\n-  absl::InlinedVector<string, 2> input_labels;\n-  string output_labels;\n+  absl::InlinedVector<std::string, 2> input_labels;\n+  std::string output_labels;\n   TF_RETURN_IF_ERROR(\n       ValidateEinsumEquation(equation, &input_labels, &output_labels));\n \n@@ -391,7 +391,7 @@ absl::Status BiasAddShape(shape_inference::InferenceContext* c) {\n   ShapeHandle input_shape;\n \n   // Fetch the data_format attribute, which may not exist.\n-  string data_format;\n+  std::string data_format;\n   absl::Status s = c->GetAttr(\"data_format\", &data_format);\n \n   if (s.ok() && data_format == \"NCHW\") {\n@@ -449,7 +449,7 @@ absl::Status BiasAddShape(shape_inference::InferenceContext* c) {\n absl::Status BiasAddGradShape(shape_inference::InferenceContext* c) {\n   ShapeHandle input_shape;\n   // Fetch the data_format attribute, which may not exist.\n-  string data_format;\n+  std::string data_format;\n   absl::Status s = c->GetAttr(\"data_format\", &data_format);\n \n   if (s.ok() && data_format == \"NCHW\") {\n@@ -465,7 +465,7 @@ absl::Status BiasAddGradShape(shape_inference::InferenceContext* c) {\n \n absl::Status CheckFormatConstraintsOnShape(\n     const TensorFormat tensor_format, const ShapeHandle shape_handle,\n-    const string& tensor_name, shape_inference::InferenceContext* c) {\n+    const std::string& tensor_name, shape_inference::InferenceContext* c) {\n   if (tensor_format == FORMAT_NCHW_VECT_C) {\n     // Check that the vect dim has size 4 or 32.\n     const int num_dims = c->Rank(shape_handle);\n@@ -593,7 +593,7 @@ namespace {\n \n absl::Status Conv2DShapeImpl(shape_inference::InferenceContext* c,\n                              bool supports_explicit_padding) {\n-  string data_format_str, filter_format_str;\n+  std::string data_format_str, filter_format_str;\n   if (!c->GetAttr(\"data_format\", &data_format_str).ok()) {\n     data_format_str = \"NHWC\";\n   }\n@@ -626,7 +626,7 @@ absl::Status Conv2DShapeImpl(shape_inference::InferenceContext* c,\n   TF_RETURN_IF_ERROR(\n       CheckFormatConstraintsOnShape(data_format, filter_shape, \"filter\", c));\n \n-  std::vector<int32> dilations;\n+  std::vector<int32_t> dilations;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"dilations\", &dilations));\n \n   if (dilations.size() != 4) {\n@@ -635,7 +635,7 @@ absl::Status Conv2DShapeImpl(shape_inference::InferenceContext* c,\n         dilations.size());\n   }\n \n-  std::vector<int32> strides;\n+  std::vector<int32_t> strides;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n \n   // strides.size() should be 4 (NCHW) even if the input is 5 (NCHW_VECT_C).\n@@ -808,7 +808,7 @@ absl::Status ConvShape(shape_inference::InferenceContext* c) {\n   }\n \n   // Default format is NHWC for 2D and NDHWC for 3D.\n-  string data_format_str;\n+  std::string data_format_str;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"data_format\", &data_format_str));\n   bool channels_last_format;\n   if (data_format_str == \"CHANNELS_LAST\") {\n@@ -827,7 +827,7 @@ absl::Status ConvShape(shape_inference::InferenceContext* c) {\n   // Determine number of spatial dims.\n   int spatial_dims = standard_input_rank - 2;\n \n-  std::vector<int32> dilations;\n+  std::vector<int32_t> dilations;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"dilations\", &dilations));\n   // Default case.\n   if (dilations.empty()) {\n@@ -840,7 +840,7 @@ absl::Status ConvShape(shape_inference::InferenceContext* c) {\n         \" values, but got: \", dilations.size()));\n   }\n \n-  std::vector<int32> strides;\n+  std::vector<int32_t> strides;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n   if (strides.size() != standard_input_rank) {\n     return absl::InvalidArgumentError(\n@@ -1004,10 +1004,10 @@ absl::Status Conv3DShape(shape_inference::InferenceContext* c) {\n   ShapeHandle filter_shape;\n   TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 5, &filter_shape));\n \n-  string data_format;\n+  std::string data_format;\n   absl::Status s = c->GetAttr(\"data_format\", &data_format);\n \n-  std::vector<int32> dilations;\n+  std::vector<int32_t> dilations;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"dilations\", &dilations));\n \n   if (dilations.size() != 5) {\n@@ -1016,7 +1016,7 @@ absl::Status Conv3DShape(shape_inference::InferenceContext* c) {\n         dilations.size());\n   }\n \n-  std::vector<int32> strides;\n+  std::vector<int32_t> strides;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n   if (strides.size() != 5) {\n     return errors::InvalidArgument(\n@@ -1113,7 +1113,7 @@ absl::Status Conv3DShape(shape_inference::InferenceContext* c) {\n }\n \n absl::Status Conv2DBackpropInputShape(shape_inference::InferenceContext* c) {\n-  string data_format_str;\n+  std::string data_format_str;\n   if (!c->GetAttr(\"data_format\", &data_format_str).ok()) {\n     data_format_str = \"NHWC\";\n   }\n@@ -1188,7 +1188,7 @@ absl::Status Conv2DBackpropFilterWithBiasShape(\n     shape_inference::InferenceContext* c) {\n   ShapeHandle input_shape;\n   // Fetch the data_format attribute, which may not exist.\n-  string data_format;\n+  std::string data_format;\n   absl::Status s = c->GetAttr(\"data_format\", &data_format);\n \n   TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n@@ -1213,7 +1213,7 @@ absl::Status DepthwiseConv2DNativeShapeImpl(\n   ShapeHandle filter_shape;\n   TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 4, &filter_shape));\n \n-  std::vector<int32> strides;\n+  std::vector<int32_t> strides;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n \n   if (strides.size() != 4) {\n@@ -1223,7 +1223,7 @@ absl::Status DepthwiseConv2DNativeShapeImpl(\n         strides.size());\n   }\n \n-  std::vector<int32> dilations;\n+  std::vector<int32_t> dilations;\n   if (!c->GetAttr(\"dilations\", &dilations).ok()) {\n     dilations.resize(4, 1);\n   }\n@@ -1235,7 +1235,7 @@ absl::Status DepthwiseConv2DNativeShapeImpl(\n         dilations.size());\n   }\n \n-  string data_format_str;\n+  std::string data_format_str;\n   absl::Status s = c->GetAttr(\"data_format\", &data_format_str);\n   TensorFormat data_format;\n   if (!s.ok() || !FormatFromString(data_format_str, &data_format)) {\n@@ -1338,7 +1338,7 @@ absl::Status DepthwiseConv2DNativeShapeWithExplicitPadding(\n }\n \n absl::Status AvgPoolShape(shape_inference::InferenceContext* c) {\n-  string data_format_str;\n+  std::string data_format_str;\n   TensorFormat data_format;\n   absl::Status s = c->GetAttr(\"data_format\", &data_format_str);\n   if (s.ok()) {\n@@ -1354,15 +1354,15 @@ absl::Status AvgPoolShape(shape_inference::InferenceContext* c) {\n   TF_RETURN_IF_ERROR(\n       CheckFormatConstraintsOnShape(data_format, input_shape, \"input\", c));\n \n-  std::vector<int32> strides;\n+  std::vector<int32_t> strides;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n   if (strides.size() != 4) {\n     return errors::InvalidArgument(\n         \"AvgPool requires the stride attribute to contain 4 values, but got: \",\n         strides.size());\n   }\n \n-  std::vector<int32> kernel_sizes;\n+  std::vector<int32_t> kernel_sizes;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"ksize\", &kernel_sizes));\n   if (kernel_sizes.size() != 4) {\n     return errors::InvalidArgument(\n@@ -1415,7 +1415,7 @@ absl::Status AvgPoolGradShape(shape_inference::InferenceContext* c) {\n }\n \n absl::Status FusedBatchNormShape(shape_inference::InferenceContext* c) {\n-  string data_format_str;\n+  std::string data_format_str;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"data_format\", &data_format_str));\n   TensorFormat data_format;\n   if (!FormatFromString(data_format_str, &data_format)) {\n@@ -1465,7 +1465,7 @@ absl::Status FusedBatchNormV3Shape(shape_inference::InferenceContext* c) {\n absl::Status FusedBatchNormExShape(shape_inference::InferenceContext* c) {\n   TF_RETURN_IF_ERROR(FusedBatchNormV3Shape(c));\n \n-  string data_format_str;\n+  std::string data_format_str;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"data_format\", &data_format_str));\n   TensorFormat data_format;\n   if (!FormatFromString(data_format_str, &data_format)) {\n@@ -1488,7 +1488,7 @@ absl::Status FusedBatchNormExShape(shape_inference::InferenceContext* c) {\n }\n \n absl::Status FusedBatchNormGradShape(shape_inference::InferenceContext* c) {\n-  string data_format_str;\n+  std::string data_format_str;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"data_format\", &data_format_str));\n   TensorFormat data_format;\n   if (!FormatFromString(data_format_str, &data_format)) {\n@@ -1537,7 +1537,7 @@ absl::Status FusedBatchNormGradExShape(shape_inference::InferenceContext* c) {\n     return absl::OkStatus();\n   }\n \n-  string data_format_str;\n+  std::string data_format_str;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"data_format\", &data_format_str));\n   TensorFormat data_format;\n   if (!FormatFromString(data_format_str, &data_format)) {\n@@ -1565,19 +1565,20 @@ absl::Status FusedBatchNormGradExShape(shape_inference::InferenceContext* c) {\n }\n \n absl::Status ReadDiagIndex(InferenceContext* c, const Tensor* diag_index_tensor,\n-                           int32* lower_diag_index, int32* upper_diag_index) {\n+                           int32_t* lower_diag_index,\n+                           int32_t* upper_diag_index) {\n   // This function assumes that the shape of diag_index_tensor is fully defined.\n   if (diag_index_tensor->dims() == 0) {\n-    *lower_diag_index = diag_index_tensor->scalar<int32>()();\n+    *lower_diag_index = diag_index_tensor->scalar<int32_t>()();\n     *upper_diag_index = *lower_diag_index;\n   } else {\n     int32_t num_elements = diag_index_tensor->dim_size(0);\n     if (num_elements == 1) {\n-      *lower_diag_index = diag_index_tensor->vec<int32>()(0);\n+      *lower_diag_index = diag_index_tensor->vec<int32_t>()(0);\n       *upper_diag_index = *lower_diag_index;\n     } else if (num_elements == 2) {\n-      *lower_diag_index = diag_index_tensor->vec<int32>()(0);\n-      *upper_diag_index = diag_index_tensor->vec<int32>()(1);\n+      *lower_diag_index = diag_index_tensor->vec<int32_t>()(0);\n+      *upper_diag_index = diag_index_tensor->vec<int32_t>()(1);\n     } else {\n       return errors::InvalidArgument(\n           \"diag_index must be a vector with one or two elements. It has \",\n@@ -1815,7 +1816,7 @@ absl::Status MatrixSetDiagV2Shape(shape_inference::InferenceContext* c) {\n \n absl::Status MaxPoolShapeImpl(shape_inference::InferenceContext* c,\n                               bool supports_explicit_padding) {\n-  string data_format_str;\n+  std::string data_format_str;\n   TensorFormat data_format;\n   absl::Status s = c->GetAttr(\"data_format\", &data_format_str);\n   if (s.ok()) {\n@@ -1831,15 +1832,15 @@ absl::Status MaxPoolShapeImpl(shape_inference::InferenceContext* c,\n   TF_RETURN_IF_ERROR(\n       CheckFormatConstraintsOnShape(data_format, input_shape, \"input\", c));\n \n-  std::vector<int32> strides;\n+  std::vector<int32_t> strides;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n   if (strides.size() != 4) {\n     return errors::InvalidArgument(\n         \"MaxPool requires the stride attribute to contain 4 values, but got: \",\n         strides.size());\n   }\n \n-  std::vector<int32> kernel_sizes;\n+  std::vector<int32_t> kernel_sizes;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"ksize\", &kernel_sizes));\n   if (kernel_sizes.size() != 4) {\n     return errors::InvalidArgument(\n@@ -1924,7 +1925,7 @@ absl::Status MaxPoolShapeWithExplicitPadding(\n \n absl::Status MaxPoolV2Shape(shape_inference::InferenceContext* c,\n                             int num_inputs) {\n-  string data_format_str;\n+  std::string data_format_str;\n   TensorFormat data_format;\n   absl::Status s = c->GetAttr(\"data_format\", &data_format_str);\n   if (s.ok()) {\n@@ -1940,8 +1941,8 @@ absl::Status MaxPoolV2Shape(shape_inference::InferenceContext* c,\n   TF_RETURN_IF_ERROR(\n       CheckFormatConstraintsOnShape(data_format, input_shape, \"input\", c));\n \n-  std::vector<int32> kernel_sizes;\n-  std::vector<int32> strides;\n+  std::vector<int32_t> kernel_sizes;\n+  std::vector<int32_t> strides;\n \n   if (c->num_inputs() + 2 == num_inputs) {\n     TF_RETURN_IF_ERROR(c->GetAttr(\"ksize\", &kernel_sizes));\n@@ -1962,7 +1963,7 @@ absl::Status MaxPoolV2Shape(shape_inference::InferenceContext* c,\n       return absl::OkStatus();\n     }\n     kernel_sizes.resize(kernel_sizes_tensor->shape().num_elements());\n-    auto kernel_sizes_vec = kernel_sizes_tensor->flat<int32>();\n+    auto kernel_sizes_vec = kernel_sizes_tensor->flat<int32_t>();\n     std::copy_n(&kernel_sizes_vec(0), kernel_sizes.size(),\n                 kernel_sizes.begin());\n \n@@ -1972,7 +1973,7 @@ absl::Status MaxPoolV2Shape(shape_inference::InferenceContext* c,\n       return absl::OkStatus();\n     }\n     strides.resize(strides_tensor->shape().num_elements());\n-    auto strides_vec = strides_tensor->flat<int32>();\n+    auto strides_vec = strides_tensor->flat<int32_t>();\n     std::copy_n(&strides_vec(0), strides.size(), strides.begin());\n   }\n \n@@ -2029,10 +2030,10 @@ absl::Status Pool3DShape(shape_inference::InferenceContext* c) {\n   ShapeHandle input_shape;\n   TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 5, &input_shape));\n \n-  string data_format;\n+  std::string data_format;\n   absl::Status s = c->GetAttr(\"data_format\", &data_format);\n \n-  std::vector<int32> strides;\n+  std::vector<int32_t> strides;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n   if (strides.size() != 5) {\n     return errors::InvalidArgument(\n@@ -2041,7 +2042,7 @@ absl::Status Pool3DShape(shape_inference::InferenceContext* c) {\n         strides.size());\n   }\n \n-  std::vector<int32> kernel_sizes;\n+  std::vector<int32_t> kernel_sizes;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"ksize\", &kernel_sizes));\n   if (kernel_sizes.size() != 5) {\n     return errors::InvalidArgument(\n@@ -2181,8 +2182,8 @@ absl::Status ReductionShape(InferenceContext* c) {\n   const int32_t input_rank = c->Rank(input);\n   std::set<int64_t> true_indices;\n   if (reduction_indices_t->dtype() == DataType::DT_INT32) {\n-    TF_RETURN_IF_ERROR(ReductionShapeHelper<int32>(reduction_indices_t,\n-                                                   input_rank, &true_indices));\n+    TF_RETURN_IF_ERROR(ReductionShapeHelper<int32_t>(\n+        reduction_indices_t, input_rank, &true_indices));\n   } else if (reduction_indices_t->dtype() == DataType::DT_INT64) {\n     TF_RETURN_IF_ERROR(ReductionShapeHelper<int64_t>(\n         reduction_indices_t, input_rank, &true_indices));\n@@ -2247,13 +2248,13 @@ absl::Status ConcatShapeHelper(InferenceContext* c, int start_value_index,\n   // shape.\n   int64_t concat_dim;\n   if (concat_dim_t->dtype() == DT_INT32) {\n-    concat_dim = static_cast<int64_t>(concat_dim_t->flat<int32>()(0));\n+    concat_dim = static_cast<int64_t>(concat_dim_t->flat<int32_t>()(0));\n   } else {\n     concat_dim = concat_dim_t->flat<int64_t>()(0);\n   }\n \n   // Minimum required number of dimensions.\n-  const int64 min_rank = concat_dim < 0 ? -concat_dim : concat_dim + 1;\n+  const int64_t min_rank = concat_dim < 0 ? -concat_dim : concat_dim + 1;\n \n   ShapeHandle output_before;\n   ShapeHandle output_after;\n@@ -2510,7 +2511,7 @@ absl::Status SliceShape(InferenceContext* c) {\n           SliceHelper<int64_t>(c, begin_value, sizes_value, &dims));\n     } else {\n       TF_RETURN_IF_ERROR(\n-          SliceHelper<int32>(c, begin_value, sizes_value, &dims));\n+          SliceHelper<int32_t>(c, begin_value, sizes_value, &dims));\n     }\n     c->set_output(0, c->MakeShape(dims));\n     return absl::OkStatus();\n@@ -2749,7 +2750,7 @@ absl::Status SparseReduceShapeFn(InferenceContext* c) {\n   const Tensor* axes_tensor = c->input_tensor(3);\n   if (shape_tensor != nullptr && axes_tensor != nullptr) {\n     auto shape_vec = shape_tensor->flat<int64_t>();\n-    auto axes_vec = axes_tensor->flat<int32>();\n+    auto axes_vec = axes_tensor->flat<int32_t>();\n \n     int64_t ndims = shape_vec.size();\n     absl::flat_hash_set<int64_t> axes;\n@@ -2797,7 +2798,7 @@ absl::Status QuantizedConv2DShape(InferenceContext* c) {\n }\n \n absl::Status FusedQuantizedConvShape(InferenceContext* c, int num_dims) {\n-  std::vector<string> fused_ops;\n+  std::vector<std::string> fused_ops;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"fused_ops\", &fused_ops));\n   ShapeHandle unused, channel;\n   bool fused_sum, fused_bias, fused_requantize;"
        },
        {
            "sha": "a97915901cc027a3c84fa2993aaca35ca705e651",
            "filename": "tensorflow/core/framework/common_shape_fns_test.cc",
            "status": "modified",
            "additions": 60,
            "deletions": 50,
            "changes": 110,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fcommon_shape_fns_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fcommon_shape_fns_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fcommon_shape_fns_test.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -220,7 +220,7 @@ TEST(CommonShapeFnsTest, MatMulShapeTest) {\n \n TEST(CommonShapeFnsTest, Einsum_ShapeFn) {\n   ShapeInferenceTestOp op(\"Einsum\");\n-  auto set_equation = [&op](int n, string equation) {\n+  auto set_equation = [&op](int n, std::string equation) {\n     std::vector<NodeDefBuilder::NodeOut> input_list;\n     input_list.reserve(n);\n     for (int i = 0; i < n; ++i) {\n@@ -629,8 +629,9 @@ TEST(CommonShapeFnsTest, BiasAddGradShapeTest) {\n \n TEST(CommonShapeFnsTest, ConvTest) {\n   ShapeInferenceTestOp op(\"Conv\");\n-  auto set_op = [&op](const std::vector<int32>& strides, const string& padding,\n-                      string data_format, int batch_dims, int groups) {\n+  auto set_op = [&op](const std::vector<int32_t>& strides,\n+                      const std::string& padding, std::string data_format,\n+                      int batch_dims, int groups) {\n     TF_CHECK_OK(NodeDefBuilder(\"test\", op.name)\n                     .Input(\"input\", 0, DT_FLOAT)\n                     .Input(\"filter\", 0, DT_FLOAT)\n@@ -715,9 +716,11 @@ TEST(CommonShapeFnsTest, ConvTest) {\n \n TEST(CommonShapeFnsTest, Conv2DFormatsTest) {\n   ShapeInferenceTestOp op(\"Conv2D\");\n-  auto set_op = [&op](const std::vector<int32>& strides, const string& padding,\n-                      const string& data_format, const string& filter_format,\n-                      const std::vector<int32>& explicit_paddings = {}) {\n+  auto set_op = [&op](const std::vector<int32_t>& strides,\n+                      const std::string& padding,\n+                      const std::string& data_format,\n+                      const std::string& filter_format,\n+                      const std::vector<int32_t>& explicit_paddings = {}) {\n     TF_CHECK_OK(NodeDefBuilder(\"test\", op.name)\n                     .Input(\"input\", 0, DT_FLOAT)\n                     .Input(\"filter\", 0, DT_FLOAT)\n@@ -761,15 +764,17 @@ TEST(CommonShapeFnsTest, Conv2DFormatsTest) {\n   INFER_OK(op, \"[1,1,4,4,32];[32,1,2,1,32]\", \"[d0_0,1,3,2,d0_4]\");\n }\n \n-class Conv2DShapeTest : public ::testing::TestWithParam<string> {};\n+class Conv2DShapeTest : public ::testing::TestWithParam<std::string> {};\n \n TEST_P(Conv2DShapeTest, Conv2DShapeTest) {\n-  const string op_name = GetParam();\n+  const std::string op_name = GetParam();\n   ShapeInferenceTestOp op(op_name);\n-  auto set_op = [&op](const std::vector<int32>& strides, const string& padding,\n-                      const string& data_format, const string& filter_format,\n-                      const std::vector<int32>& explicit_paddings = {}) {\n-    string format;\n+  auto set_op = [&op](const std::vector<int32_t>& strides,\n+                      const std::string& padding,\n+                      const std::string& data_format,\n+                      const std::string& filter_format,\n+                      const std::vector<int32_t>& explicit_paddings = {}) {\n+    std::string format;\n     if (op.name == \"Conv\")\n       format = (data_format == \"NHWC\") ? \"CHANNELS_LAST\" : \"CHANNELS_FIRST\";\n     else\n@@ -974,13 +979,14 @@ TEST_P(Conv2DShapeTest, Conv2DShapeTest) {\n }\n \n TEST_P(Conv2DShapeTest, Conv2DDilatedShapeTest) {\n-  const string op_name = GetParam();\n+  const std::string op_name = GetParam();\n   ShapeInferenceTestOp op(op_name);\n-  auto set_op = [&op](const std::vector<int32>& dilations,\n-                      const std::vector<int32>& strides, const string& padding,\n-                      const string& data_format,\n-                      const std::vector<int32>& explicit_paddings = {}) {\n-    string format;\n+  auto set_op = [&op](const std::vector<int32_t>& dilations,\n+                      const std::vector<int32_t>& strides,\n+                      const std::string& padding,\n+                      const std::string& data_format,\n+                      const std::vector<int32_t>& explicit_paddings = {}) {\n+    std::string format;\n     if (op.name == \"Conv\")\n       format = (data_format == \"NHWC\") ? \"CHANNELS_LAST\" : \"CHANNELS_FIRST\";\n     else\n@@ -1129,8 +1135,8 @@ TEST(CommonShapeFnsTest, Conv3DShapeRankTest) {\n \n TEST(CommonShapeFnsTest, Conv3DGroupsTest) {\n   ShapeInferenceTestOp op(\"Conv3D\");\n-  auto set_op = [&op](const std::vector<int32>& strides,\n-                      const string& padding) {\n+  auto set_op = [&op](const std::vector<int32_t>& strides,\n+                      const std::string& padding) {\n     TF_CHECK_OK(NodeDefBuilder(\"test\", \"Conv3D\")\n                     .Input(\"input\", 0, DT_FLOAT)\n                     .Input(\"filter\", 0, DT_FLOAT)\n@@ -1166,13 +1172,13 @@ TEST(CommonShapeFnsTest, Conv3DGroupsTest) {\n INSTANTIATE_TEST_SUITE_P(CommonShapeFnsTest, Conv2DShapeTest,\n                          ::testing::Values(\"Conv2D\", \"Conv\"));\n \n-class Conv3DShapeTest : public ::testing::TestWithParam<string> {};\n+class Conv3DShapeTest : public ::testing::TestWithParam<std::string> {};\n \n TEST_P(Conv3DShapeTest, Conv3DShapeTest) {\n-  const string op_name = GetParam();\n+  const std::string op_name = GetParam();\n   ShapeInferenceTestOp op(op_name);\n-  auto set_op = [&op](const std::vector<int32>& strides,\n-                      const string& padding) {\n+  auto set_op = [&op](const std::vector<int32_t>& strides,\n+                      const std::string& padding) {\n     TF_CHECK_OK(NodeDefBuilder(\"test\", op.name)\n                     .Input(\"input\", 0, DT_FLOAT)\n                     .Input(\"filter\", 0, DT_FLOAT)\n@@ -1245,11 +1251,11 @@ TEST_P(Conv3DShapeTest, Conv3DShapeTest) {\n }\n \n TEST_P(Conv3DShapeTest, Conv3DDilatedShapeTest) {\n-  const string op_name = GetParam();\n+  const std::string op_name = GetParam();\n   ShapeInferenceTestOp op(op_name);\n-  auto set_op = [&op](const std::vector<int32>& dilations,\n-                      const std::vector<int32>& strides,\n-                      const string& padding) {\n+  auto set_op = [&op](const std::vector<int32_t>& dilations,\n+                      const std::vector<int32_t>& strides,\n+                      const std::string& padding) {\n     TF_CHECK_OK(NodeDefBuilder(\"test\", op.name)\n                     .Input(\"input\", 0, DT_FLOAT)\n                     .Input(\"filter\", 0, DT_FLOAT)\n@@ -1300,7 +1306,7 @@ INSTANTIATE_TEST_SUITE_P(CommonShapeFnsTest, Conv3DShapeTest,\n \n TEST(CommonShapeFnsTest, DepthwiseConv2DShapeTest) {\n   ShapeInferenceTestOp op(\"DepthwiseConv2dNative\");\n-  std::vector<int32> strides = {{1, 1, 1, 1}};\n+  std::vector<int32_t> strides = {{1, 1, 1, 1}};\n   TF_CHECK_OK(NodeDefBuilder(\"test\", \"DepthwiseConv2dNative\")\n                   .Input(\"input\", 0, DT_FLOAT)\n                   .Input(\"filter\", 0, DT_FLOAT)\n@@ -1344,9 +1350,10 @@ TEST(CommonShapeFnsTest, DepthwiseConv2DShapeTest) {\n \n TEST(CommonShapeFnsTest, AvgPool2DShapeTest) {\n   ShapeInferenceTestOp op(\"AvgPool\");\n-  auto set_op = [&op](const std::vector<int32>& strides,\n-                      const std::vector<int32>& ksizes, const string& padding,\n-                      const string& data_format) {\n+  auto set_op = [&op](const std::vector<int32_t>& strides,\n+                      const std::vector<int32_t>& ksizes,\n+                      const std::string& padding,\n+                      const std::string& data_format) {\n     TF_CHECK_OK(NodeDefBuilder(\"test\", \"AvgPool\")\n                     .Input(\"input\", 0, DT_FLOAT)\n                     .Attr(\"strides\", strides)\n@@ -1390,9 +1397,10 @@ TEST(CommonShapeFnsTest, AvgPool2DShapeTest) {\n \n TEST(CommonShapeFnsTest, MaxPool2DShapeTest) {\n   ShapeInferenceTestOp op(\"MaxPool\");\n-  auto set_op = [&op](const std::vector<int32>& strides,\n-                      const std::vector<int32>& ksizes, const string& padding,\n-                      const string& data_format) {\n+  auto set_op = [&op](const std::vector<int32_t>& strides,\n+                      const std::vector<int32_t>& ksizes,\n+                      const std::string& padding,\n+                      const std::string& data_format) {\n     TF_CHECK_OK(NodeDefBuilder(\"test\", \"MaxPool\")\n                     .Input(\"input\", 0, DT_FLOAT)\n                     .Attr(\"strides\", strides)\n@@ -1426,21 +1434,22 @@ TEST(CommonShapeFnsTest, MaxPoolV22DShapeTest) {\n   ShapeInferenceTestOp op(\"MaxPoolV2\");\n   Tensor ksizes_tensor, strides_tensor;\n   auto set_op = [&op, &ksizes_tensor, &strides_tensor](\n-                    const std::vector<int32>& strides,\n-                    const std::vector<int32>& ksizes, const string& padding,\n-                    const string& data_format) {\n+                    const std::vector<int32_t>& strides,\n+                    const std::vector<int32_t>& ksizes,\n+                    const std::string& padding,\n+                    const std::string& data_format) {\n     TF_CHECK_OK(NodeDefBuilder(\"test\", \"MaxPoolV2\")\n                     .Input(\"input\", 0, DT_FLOAT)\n                     .Input(\"ksize\", 1, DT_INT32)\n                     .Input(\"strides\", 2, DT_INT32)\n                     .Attr(\"padding\", padding)\n                     .Attr(\"data_format\", data_format)\n                     .Finalize(&op.node_def));\n-    ksizes_tensor = test::AsTensor<int32>(ksizes);\n+    ksizes_tensor = test::AsTensor<int32_t>(ksizes);\n     op.input_tensors.resize(3);\n     op.input_tensors[0] = nullptr;\n     op.input_tensors[1] = &ksizes_tensor;\n-    strides_tensor = test::AsTensor<int32>(strides);\n+    strides_tensor = test::AsTensor<int32_t>(strides);\n     op.input_tensors[2] = &strides_tensor;\n   };\n \n@@ -1466,8 +1475,9 @@ TEST(CommonShapeFnsTest, MaxPoolV22DShapeTest) {\n \n TEST(CommonShapeFnsTest, Pool3DShapeTest) {\n   ShapeInferenceTestOp op(\"MaxPool3D\");\n-  auto set_op = [&op](const std::vector<int32>& strides,\n-                      const std::vector<int32>& ksizes, const string& padding) {\n+  auto set_op = [&op](const std::vector<int32_t>& strides,\n+                      const std::vector<int32_t>& ksizes,\n+                      const std::string& padding) {\n     TF_CHECK_OK(NodeDefBuilder(\"test\", \"MaxPool3D\")\n                     .Input(\"input\", 0, DT_FLOAT)\n                     .Attr(\"strides\", strides)\n@@ -1524,28 +1534,28 @@ TEST(CommonShapeFnsTest, Reduce_ShapeFn) {\n   INFER_OK(op, \"[2,4,5];[2]\", \"?\");\n   INFER_OK(op, \"?;[2]\", \"?\");\n \n-  Tensor indices = test::AsTensor<int32>({1, 2});\n+  Tensor indices = test::AsTensor<int32_t>({1, 2});\n   op.input_tensors[1] = &indices;\n \n   // Reduction indices available\n   INFER_OK(op, \"[2,4,5];[2]\", \"[d0_0]\");\n \n   // Wrapped indices\n-  indices = test::AsTensor<int32>({-1, -2});\n+  indices = test::AsTensor<int32_t>({-1, -2});\n   op.input_tensors[1] = &indices;\n   INFER_OK(op, \"[2,4,5];[2]\", \"[d0_0]\");\n \n   // Scalar\n-  indices = test::AsScalar<int32>(0);\n+  indices = test::AsScalar<int32_t>(0);\n   op.input_tensors[1] = &indices;\n   INFER_OK(op, \"[2,4,5];[]\", \"[d0_1,d0_2]\");\n \n-  indices = test::AsScalar<int32>(-4);\n+  indices = test::AsScalar<int32_t>(-4);\n   op.input_tensors[1] = &indices;\n   INFER_ERROR(\"Invalid reduction dimension\", op, \"[2,4,5];[]\");\n \n   // Empty reduction indices\n-  indices = test::AsTensor<int32>({});\n+  indices = test::AsTensor<int32_t>({});\n   op.input_tensors[1] = &indices;\n   INFER_OK(op, \"[2,4,5];[0]\", \"[d0_0,d0_1,d0_2]\");\n \n@@ -1555,7 +1565,7 @@ TEST(CommonShapeFnsTest, Reduce_ShapeFn) {\n                    .Input(\"reduction_indices\", 1, DT_INT32)\n                    .Attr(\"keep_dims\", true)\n                    .Finalize(&op.node_def));\n-  indices = test::AsTensor<int32>({-1, -2});\n+  indices = test::AsTensor<int32_t>({-1, -2});\n   op.input_tensors[1] = &indices;\n   INFER_OK(op, \"[2,4,5];[2]\", \"[d0_0, 1, 1]\");\n \n@@ -1572,9 +1582,9 @@ TEST(CommonShapeFnsTest, Reduce_ShapeFn) {\n   INFER_OK(op, \"[?,?,?];[?,?]\", \"[?,?,?]\");\n   // And when the tensor is specified, it's still allowed.\n   op.input_tensors[1] = &indices;\n-  indices = test::AsTensor<int32>({-1, -2}, TensorShape({2, 1}));\n+  indices = test::AsTensor<int32_t>({-1, -2}, TensorShape({2, 1}));\n   INFER_OK(op, \"[2,4,5];[2,1]\", \"[d0_0, 1, 1]\");\n-  indices = test::AsTensor<int32>({-1, -2}, TensorShape({1, 2}));\n+  indices = test::AsTensor<int32_t>({-1, -2}, TensorShape({1, 2}));\n   INFER_OK(op, \"[2,4,5];[1,2]\", \"[d0_0, 1, 1]\");\n }\n "
        },
        {
            "sha": "a70ecb85214e31b82bfddc85374c10a2a7a1971c",
            "filename": "tensorflow/core/framework/control_flow.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fcontrol_flow.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fcontrol_flow.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fcontrol_flow.h?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -22,20 +22,20 @@ limitations under the License.\n \n namespace tensorflow {\n \n-const uint64 kIllegalFrameId = ~0uLL;\n+const uint64_t kIllegalFrameId = ~0uLL;\n const int64_t kIllegalIterId = -1;\n \n // For the purpose of control flow, every tensor produced by TensorFlow is\n // conceptually tagged by a 'FrameAndIter'. FrameAndIter consists of a\n // 'frame_id' and an 'iter_id'. The tensor value it represents is produced\n // in the frame with frame_id at the iteration of iter_id.\n struct FrameAndIter {\n-  uint64 frame_id = kIllegalFrameId;\n+  uint64_t frame_id = kIllegalFrameId;\n   int64_t iter_id = kIllegalIterId;\n \n   FrameAndIter() {}\n \n-  FrameAndIter(uint64 frame, int64_t iter) {\n+  FrameAndIter(uint64_t frame, int64_t iter) {\n     frame_id = frame;\n     iter_id = iter;\n   }\n@@ -48,7 +48,7 @@ struct FrameAndIter {\n struct FrameAndIterHash {\n   size_t operator()(const FrameAndIter& key) const {\n     // Make sure there are no padding bytes that we don't want\n-    CHECK_EQ(sizeof(uint64) + sizeof(int64_t), sizeof(FrameAndIter));\n+    CHECK_EQ(sizeof(uint64_t) + sizeof(int64_t), sizeof(FrameAndIter));\n     return Hash64(reinterpret_cast<const char*>(&key), sizeof(FrameAndIter));\n   }\n };"
        },
        {
            "sha": "69593a67d903525cc991b92b00feed5dc6ad0f67",
            "filename": "tensorflow/core/framework/dataset.cc",
            "status": "modified",
            "additions": 29,
            "deletions": 25,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fdataset.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fdataset.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fdataset.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -52,8 +52,9 @@ static mutex* get_dataset_op_registry_lock() {\n   return &dataset_op_registry_lock;\n }\n \n-static std::unordered_set<string>* get_dataset_op_registry() {\n-  static std::unordered_set<string>* names = new std::unordered_set<string>;\n+static std::unordered_set<std::string>* get_dataset_op_registry() {\n+  static std::unordered_set<std::string>* names =\n+      new std::unordered_set<std::string>;\n   return names;\n }\n \n@@ -97,8 +98,8 @@ class DatasetVariantWrapper {\n \n   DatasetBase* get() const { return dataset_; }\n \n-  string TypeName() const { return \"tensorflow::DatasetVariantWrapper\"; }\n-  string DebugString() const {\n+  std::string TypeName() const { return \"tensorflow::DatasetVariantWrapper\"; }\n+  std::string DebugString() const {\n     if (dataset_) {\n       return dataset_->DebugString();\n     } else {\n@@ -131,9 +132,11 @@ class WrappedDatasetVariantWrapper {\n \n   Tensor get() const { return ds_tensor_; }\n \n-  string TypeName() const { return \"tensorflow::WrappedDatasetVariantWrapper\"; }\n+  std::string TypeName() const {\n+    return \"tensorflow::WrappedDatasetVariantWrapper\";\n+  }\n \n-  string DebugString() const {\n+  std::string DebugString() const {\n     return \"tensorflow::WrappedDatasetVariantWrapper::DebugString\";\n   }\n \n@@ -324,7 +327,7 @@ absl::Status GraphDefBuilderWrapper::AddDataset(\n }\n \n absl::Status GraphDefBuilderWrapper::AddFunction(\n-    SerializationContext* ctx, const string& function_name,\n+    SerializationContext* ctx, const std::string& function_name,\n     const FunctionLibraryDefinition& lib_def) {\n   if (b_->HasFunction(function_name)) {\n     VLOG(1) << \"Function with name \" << function_name << \"already exists in\"\n@@ -338,7 +341,7 @@ absl::Status GraphDefBuilderWrapper::AddFunction(\n   }\n   FunctionDefLibrary def;\n   *def.add_function() = *f_def;\n-  const string gradient_func = lib_def.FindGradient(function_name);\n+  const std::string gradient_func = lib_def.FindGradient(function_name);\n   if (!gradient_func.empty()) {\n     GradientDef* g_def = def.add_gradient();\n     g_def->set_function_name(function_name);\n@@ -380,8 +383,8 @@ void GraphDefBuilderWrapper::AddTensorInternal(const Tensor& val,\n       b_->opts().WithAttr(\"dtype\", val.dtype()).WithAttr(\"value\", val));\n }\n \n-bool GraphDefBuilderWrapper::HasAttr(const string& name,\n-                                     const string& attr_name) const {\n+bool GraphDefBuilderWrapper::HasAttr(const std::string& name,\n+                                     const std::string& attr_name) const {\n   const OpDef* op_def = nullptr;\n   absl::Status s = b_->opts().op_registry()->LookUpOpDef(name, &op_def);\n   if (!s.ok() || op_def == nullptr) {\n@@ -535,11 +538,11 @@ absl::Status MemoryCheckpoint::Save(IteratorStateWriter* writer) const {\n absl::Status IteratorBase::InitializeBase(IteratorContext* ctx,\n                                           const IteratorBase* parent) {\n   parent_ = parent;\n-  id_ =\n-      Hash64CombineUnordered(Hash64(prefix()), reinterpret_cast<uint64>(this));\n+  id_ = Hash64CombineUnordered(Hash64(prefix()),\n+                               reinterpret_cast<uint64_t>(this));\n   if (parent_) {\n     parent_id_ = Hash64CombineUnordered(Hash64(parent_->prefix()),\n-                                        reinterpret_cast<uint64>(parent_));\n+                                        reinterpret_cast<uint64_t>(parent_));\n     // This block of code is executed only when `parent_` is not a `nullptr`\n     // because we do not create a `Node` in the `Model` for `RootDataset`.\n     if (const auto& model = ctx->model()) {\n@@ -626,17 +629,17 @@ std::string FullName(const std::string& prefix, const std::string& name) {\n   return strings::StrCat(kFullNameRandomHex, kPipe, prefix, kColon, name);\n }\n \n-absl::Status ExtractIteratorPrefix(absl::string_view key, string* prefix) {\n+absl::Status ExtractIteratorPrefix(absl::string_view key, std::string* prefix) {\n   if (!absl::StartsWith(key, data::kFullNameRandomHex)) {\n     return errors::InvalidArgument(\"Key: \", key,\n                                    \" was not generated using full_name.\");\n   }\n-  std::vector<string> split_keys = str_util::Split(key, data::kPipe);\n+  std::vector<std::string> split_keys = str_util::Split(key, data::kPipe);\n   if (split_keys.size() != 2) {\n     return errors::InvalidArgument(\"Key: \", key,\n                                    \" was not generated using full_name.\");\n   }\n-  string real_key = split_keys[1];\n+  std::string real_key = split_keys[1];\n   const int pos = real_key.rfind(kColon);\n   *prefix = real_key.substr(0, pos);\n   return absl::OkStatus();\n@@ -811,10 +814,11 @@ absl::Status DatasetBase::ComputeNumSources() {\n   return absl::OkStatus();\n }\n \n-absl::Status DatasetBase::CheckRandomAccessCompatible(const int64 index) const {\n+absl::Status DatasetBase::CheckRandomAccessCompatible(\n+    const int64_t index) const {\n   CardinalityOptions options;\n   options.set_compute_level(CardinalityOptions::CARDINALITY_COMPUTE_MODERATE);\n-  int64 cardinality = Cardinality(options);\n+  int64_t cardinality = Cardinality(options);\n   if (cardinality == kInfiniteCardinality ||\n       cardinality == kUnknownCardinality) {\n     return tensorflow::errors::FailedPrecondition(\n@@ -829,13 +833,13 @@ absl::Status DatasetBase::CheckRandomAccessCompatible(const int64 index) const {\n   return absl::OkStatus();\n }\n \n-absl::Status DatasetBase::Get(OpKernelContext* ctx, int64 index,\n+absl::Status DatasetBase::Get(OpKernelContext* ctx, int64_t index,\n                               std::vector<Tensor>* out_tensors) const {\n   return errors::Unimplemented(\"Random access is not implemented for dataset \",\n                                DebugString());\n }\n \n-absl::Status DatasetBase::Get(AnyContext ctx, int64 index,\n+absl::Status DatasetBase::Get(AnyContext ctx, int64_t index,\n                               std::vector<Tensor>* out_tensors) const {\n   return errors::Unimplemented(\"Random access is not implemented for dataset \",\n                                DebugString());\n@@ -876,7 +880,7 @@ absl::Status DatasetBase::MergeOptionsFromInputs() {\n \n absl::Status DatasetBase::MakeIterator(\n     IteratorContext* ctx, const IteratorBase* parent,\n-    const string& output_prefix,\n+    const std::string& output_prefix,\n     std::unique_ptr<IteratorBase>* iterator) const {\n   if (type_string() == \"OptionsDataset\" || type_string() == \"FinalizeDataset\") {\n     std::vector<const DatasetBase*> inputs;\n@@ -1103,8 +1107,8 @@ DatasetBaseIterator::~DatasetBaseIterator() {\n   params_.dataset->Unref();\n }\n \n-string DatasetBaseIterator::BuildTraceMeName() {\n-  string result =\n+std::string DatasetBaseIterator::BuildTraceMeName() {\n+  std::string result =\n       strings::StrCat(params_.prefix, \"#\", traceme_metadata_, \",id=\", id_);\n   if (parent_) {\n     absl::StrAppend(&result, \",parent_id=\", parent_id_);\n@@ -1274,8 +1278,8 @@ void DatasetOpKernel::Compute(OpKernelContext* ctx) {\n   }\n }\n \n-string DatasetOpKernel::TraceString(const OpKernelContext& ctx,\n-                                    bool verbose) const {\n+std::string DatasetOpKernel::TraceString(const OpKernelContext& ctx,\n+                                         bool verbose) const {\n   return tsl::profiler::TraceMeOp(name_view(), type_string_view());\n }\n "
        },
        {
            "sha": "2471c3dc08cc0a60626bf0233103287ca358d0ff",
            "filename": "tensorflow/core/framework/dataset.h",
            "status": "modified",
            "additions": 49,
            "deletions": 46,
            "changes": 95,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fdataset.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fdataset.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fdataset.h?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -87,7 +87,7 @@ void MergeOptions(const protobuf::MessageLite& source,\n                   protobuf::MessageLite* destination);\n }  // namespace internal\n \n-using TraceMeMetadata = std::vector<std::pair<absl::string_view, string>>;\n+using TraceMeMetadata = std::vector<std::pair<absl::string_view, std::string>>;\n \n // Maps the index of dataset elements to a globally shuffled index. See the\n // comment for IteratorContext::Params::index_mapper for more details.\n@@ -211,7 +211,7 @@ class IteratorStateWriter {\n std::string FullName(const std::string& prefix, const std::string& name);\n \n // Extracts iterator prefix from key generated by `FullName`.\n-absl::Status ExtractIteratorPrefix(absl::string_view key, string* prefix);\n+absl::Status ExtractIteratorPrefix(absl::string_view key, std::string* prefix);\n \n // Interface for objects that can be checkpointed.\n class Checkpointable {\n@@ -264,7 +264,7 @@ class GraphDefBuilderWrapper {\n     return absl::OkStatus();\n   }\n \n-  absl::Status AddVector(const std::vector<string>& val, Node** output) {\n+  absl::Status AddVector(const std::vector<std::string>& val, Node** output) {\n     Tensor val_t = Tensor(DataTypeToEnum<tstring>::v(),\n                           TensorShape({static_cast<int64_t>(val.size())}));\n     for (size_t i = 0; i < val.size(); i++) {\n@@ -350,7 +350,7 @@ class GraphDefBuilderWrapper {\n   // or any of its dependent functions are stateful, and the context does not\n   // explicitly permit stateful functions, returns an InvalidArgument error.\n   absl::Status AddFunction(SerializationContext* ctx,\n-                           const string& function_name,\n+                           const std::string& function_name,\n                            const FunctionLibraryDefinition& lib_def);\n \n   template <typename T>\n@@ -371,9 +371,10 @@ class GraphDefBuilderWrapper {\n  private:\n   void AddPlaceholderInternal(const Tensor& val, Node** output);\n   void AddTensorInternal(const Tensor& val, Node** output);\n-  bool HasAttr(const string& op_type_name, const string& attr_name) const;\n+  bool HasAttr(const std::string& op_type_name,\n+               const std::string& attr_name) const;\n \n-  bool HasAttr(const OpDef* op_def, const string& attr_name) const {\n+  bool HasAttr(const OpDef* op_def, const std::string& attr_name) const {\n     for (const auto& attr : op_def->attr()) {\n       if (attr.name() == attr_name) {\n         return true;\n@@ -509,35 +510,35 @@ class MemoryCheckpoint final : public IteratorStateWriter {\n \n   // BEGIN implementation of `IteratorStateWriter` interface\n   absl::Status WriteScalar(absl::string_view key, int64_t val) override {\n-    string prefix;\n+    std::string prefix;\n     TF_RETURN_IF_ERROR(ExtractIteratorPrefix(key, &prefix));\n     return WriteScalar(prefix, key, val);\n   }\n   absl::Status WriteScalar(absl::string_view name, absl::string_view key,\n                            int64_t val) override {\n-    auto id = id_registry_->Add(string(name), string(key));\n+    auto id = id_registry_->Add(std::string(name), std::string(key));\n     int_values_[id] = val;\n     return absl::OkStatus();\n   }\n   absl::Status WriteScalar(absl::string_view key, const tstring& val) override {\n-    string prefix;\n+    std::string prefix;\n     TF_RETURN_IF_ERROR(ExtractIteratorPrefix(key, &prefix));\n     return WriteScalar(prefix, key, val);\n   }\n   absl::Status WriteScalar(absl::string_view name, absl::string_view key,\n                            const tstring& val) override {\n-    auto id = id_registry_->Add(string(name), string(key));\n+    auto id = id_registry_->Add(std::string(name), std::string(key));\n     str_values_[id] = val;\n     return absl::OkStatus();\n   }\n   absl::Status WriteTensor(absl::string_view key, const Tensor& val) override {\n-    string prefix;\n+    std::string prefix;\n     TF_RETURN_IF_ERROR(ExtractIteratorPrefix(key, &prefix));\n     return WriteTensor(prefix, key, val);\n   }\n   absl::Status WriteTensor(absl::string_view name, absl::string_view key,\n                            const Tensor& val) override {\n-    auto id = id_registry_->Add(string(name), string(key));\n+    auto id = id_registry_->Add(std::string(name), std::string(key));\n     tensor_values_[id] = val;\n     return absl::OkStatus();\n   }\n@@ -614,7 +615,8 @@ class SerializationContext {\n         : resource_mgr(ctx->resource_manager()),\n           device_name(ctx->device()->attributes().name()) {}\n \n-    std::vector<std::pair<string, Tensor>>* input_list = nullptr;  // Not owned.\n+    std::vector<std::pair<std::string, Tensor>>* input_list =\n+        nullptr;  // Not owned.\n \n     // Indicates what to do if the dataset depends on external state.\n     ExternalStatePolicy external_state_policy =\n@@ -653,7 +655,7 @@ class SerializationContext {\n \n   explicit SerializationContext(Params params) : params_(params) {}\n \n-  std::vector<std::pair<string, Tensor>>* input_list() {\n+  std::vector<std::pair<std::string, Tensor>>* input_list() {\n     return params_.input_list;\n   }\n \n@@ -773,7 +775,7 @@ class IteratorContext {\n     // Records the number of ParallelInterleave operations in the path from the\n     // root node to this node (not including this node) in the input pipeline\n     // tree.\n-    int64 interleave_depth = 0;\n+    int64_t interleave_depth = 0;\n \n     // Marks whether the iterator is restored from a checkpoint.\n     bool is_restoring = false;\n@@ -795,7 +797,7 @@ class IteratorContext {\n     std::function<void(std::function<void()>)> runner = nullptr;\n \n     // Number of threads used for executing user-defined functions.\n-    int32 runner_threadpool_size = 0;\n+    int32_t runner_threadpool_size = 0;\n \n     // Split providers indicating which splits to process. May be empty,\n     // indicating that the iterator should process all splits.\n@@ -891,7 +893,7 @@ class IteratorContext {\n \n   MemoryCheckpoint* checkpoint() { return &checkpoint_; }\n \n-  int64 interleave_depth() { return params_.interleave_depth; }\n+  int64_t interleave_depth() { return params_.interleave_depth; }\n \n   bool is_restoring() { return params_.is_restoring; }\n \n@@ -909,7 +911,7 @@ class IteratorContext {\n     return &params_.runner;\n   }\n \n-  int32 runner_threadpool_size() { return params_.runner_threadpool_size; }\n+  int32_t runner_threadpool_size() { return params_.runner_threadpool_size; }\n \n   std::vector<std::shared_ptr<SplitProvider>> split_providers() const {\n     return params_.split_providers;\n@@ -949,7 +951,7 @@ class IteratorContext {\n     params_.index_mapper = index_mapper;\n   };\n \n-  std::unique_ptr<thread::ThreadPool> CreateThreadPool(const string& name,\n+  std::unique_ptr<thread::ThreadPool> CreateThreadPool(const std::string& name,\n                                                        int num_threads) {\n     if (params_.thread_pool) {\n       // Create a `ThreadPool` instance by wrapping `params_.thread_pool` (which\n@@ -1010,7 +1012,7 @@ class IteratorContext {\n     }\n   }\n \n-  std::unique_ptr<Thread> StartThread(const string& name,\n+  std::unique_ptr<Thread> StartThread(const std::string& name,\n                                       std::function<void()> fn) {\n     if (params_.thread_factory) {\n       return params_.thread_factory->StartThread(name, std::move(fn));\n@@ -1133,7 +1135,7 @@ class IteratorBase : public Checkpointable {\n \n   // Returns a string that identifies the sequence of iterators leading up to\n   // this iterator.\n-  virtual const string& prefix() const = 0;\n+  virtual const std::string& prefix() const = 0;\n \n   // Indicates whether the iterator is compatible with symbolic checkpointing.\n   virtual bool SymbolicCheckpointCompatible() const { return false; }\n@@ -1248,9 +1250,9 @@ class IteratorBase : public Checkpointable {\n class DatasetContext {\n  public:\n   struct Params {\n-    string type_string;  // op type name of this dataset.\n-    string node_name;    // graph node name of this dataset op, uniquely\n-                         // identifying the dataset in the graph.\n+    std::string type_string;  // op type name of this dataset.\n+    std::string node_name;    // graph node name of this dataset op, uniquely\n+                              // identifying the dataset in the graph.\n   };\n \n   explicit DatasetContext(Params params) : params_(std::move(params)) {}\n@@ -1260,8 +1262,8 @@ class DatasetContext {\n     params_.node_name = ctx->op_kernel().name();\n   }\n \n-  const string& type_string() const { return params_.type_string; }\n-  const string& node_name() const { return params_.node_name; }\n+  const std::string& type_string() const { return params_.type_string; }\n+  const std::string& node_name() const { return params_.node_name; }\n \n  private:\n   Params params_;\n@@ -1304,11 +1306,11 @@ class DatasetBase : public core::RefCounted {\n       : type_string_(ctx.type_string()), node_name_(ctx.node_name()) {}\n \n   // Op type name of this dataset.\n-  const string& type_string() const { return type_string_; }\n+  const std::string& type_string() const { return type_string_; }\n \n   // Graph node name of this dataset op, uniquely identifying the dataset in\n   // the graph.\n-  const string& node_name() const { return node_name_; }\n+  const std::string& node_name() const { return node_name_; }\n \n   const Metadata& metadata() const { return metadata_; }\n \n@@ -1330,18 +1332,18 @@ class DatasetBase : public core::RefCounted {\n   // The prefix identifies the sequence of iterators leading up to the newly\n   // created iterator.\n   absl::Status MakeIterator(IteratorContext* ctx, const IteratorBase* parent,\n-                            const string& output_prefix,\n+                            const std::string& output_prefix,\n                             std::unique_ptr<IteratorBase>* iterator) const;\n \n   absl::Status MakeIterator(IteratorContext&& ctx, const IteratorBase* parent,\n-                            const string& output_prefix,\n+                            const std::string& output_prefix,\n                             std::unique_ptr<IteratorBase>* iterator) const {\n     return MakeIterator(&ctx, parent, output_prefix, iterator);\n   }\n \n   // Returns a new iterator restored from the checkpoint data in `reader`.\n   absl::Status MakeIteratorFromCheckpoint(\n-      IteratorContext* ctx, const string& output_prefix,\n+      IteratorContext* ctx, const std::string& output_prefix,\n       IteratorStateReader* reader,\n       std::unique_ptr<IteratorBase>* iterator) const {\n     std::unique_ptr<IteratorBase> it;\n@@ -1357,7 +1359,7 @@ class DatasetBase : public core::RefCounted {\n   }\n \n   absl::Status MakeIteratorFromCheckpoint(\n-      IteratorContext&& ctx, const string& output_prefix,\n+      IteratorContext&& ctx, const std::string& output_prefix,\n       IteratorStateReader* reader,\n       std::unique_ptr<IteratorBase>* iterator) const {\n     return MakeIteratorFromCheckpoint(&ctx, output_prefix, reader, iterator);\n@@ -1405,7 +1407,7 @@ class DatasetBase : public core::RefCounted {\n   }\n \n   // A human-readable debug string for this dataset.\n-  virtual string DebugString() const = 0;\n+  virtual std::string DebugString() const = 0;\n \n   // Stores the dataset's input datasets in `*inputs`. The pointers stored in\n   // `*inputs` are borrowed. The only valid non-ok return status is\n@@ -1423,16 +1425,16 @@ class DatasetBase : public core::RefCounted {\n   virtual absl::Status CheckExternalState() const = 0;\n \n   // Indicates whether the dataset is compatible with random access.\n-  absl::Status CheckRandomAccessCompatible(const int64 index) const;\n+  absl::Status CheckRandomAccessCompatible(const int64_t index) const;\n \n   // Return the element at a particular index for a randomly accessible dataset.\n-  virtual absl::Status Get(OpKernelContext* ctx, int64 index,\n+  virtual absl::Status Get(OpKernelContext* ctx, int64_t index,\n                            std::vector<Tensor>* out_tensors) const;\n \n   // Same as above, but with an `AnyContext`, which can be constructed from\n   // either an `OpKernelContext` or `IteratorContext`. Used to support datasets\n   // that provide random access through both the dataset and iterator APIs.\n-  virtual absl::Status Get(AnyContext ctx, int64 index,\n+  virtual absl::Status Get(AnyContext ctx, int64_t index,\n                            std::vector<Tensor>* out_tensors) const;\n \n   // Returns true if the dataset and its inputs support random access.\n@@ -1487,7 +1489,7 @@ class DatasetBase : public core::RefCounted {\n                                           Node** node) const = 0;\n \n   virtual std::unique_ptr<IteratorBase> MakeIteratorInternal(\n-      const string& prefix) const = 0;\n+      const std::string& prefix) const = 0;\n \n   void set_options(const Options& options) { options_ = options; }\n \n@@ -1505,8 +1507,8 @@ class DatasetBase : public core::RefCounted {\n   // how they appear for this dataset.\n   absl::Status MergeOptionsFromInputs();\n \n-  const string type_string_;\n-  const string node_name_;\n+  const std::string type_string_;\n+  const std::string node_name_;\n   Metadata metadata_;\n   Options options_;\n   mutable mutex mu_;\n@@ -1527,7 +1529,7 @@ class DatasetBaseIterator : public IteratorBase {\n     const DatasetBase* dataset;\n \n     // Identifies the sequence of iterators leading up to this iterator.\n-    const string prefix;\n+    const std::string prefix;\n   };\n \n   explicit DatasetBaseIterator(const BaseParams& params);\n@@ -1544,13 +1546,13 @@ class DatasetBaseIterator : public IteratorBase {\n     return params_.dataset->output_shapes();\n   }\n \n-  const string& prefix() const override { return params_.prefix; }\n+  const std::string& prefix() const override { return params_.prefix; }\n \n   // Returns a name to be used for the TraceMe event.\n   //\n   // NOTE: TraceMe supports passing key-value pairs of \"arguments\" using the\n   // following format \"name#arg_1=value_,...,arg_n=value_n\".\n-  string BuildTraceMeName();\n+  std::string BuildTraceMeName();\n \n   absl::Status GetNext(IteratorContext* ctx, std::vector<Tensor>* out_tensors,\n                        bool* end_of_sequence) final;\n@@ -1602,7 +1604,7 @@ class DatasetBaseIterator : public IteratorBase {\n   virtual absl::Status SkipInternal(IteratorContext* ctx, int num_to_skip,\n                                     bool* end_of_sequence, int* num_skipped);\n \n-  string full_name(const string& name) const {\n+  std::string full_name(const std::string& name) const {\n     return FullName(params_.prefix, name);\n   }\n \n@@ -1693,7 +1695,7 @@ class DatasetBaseIterator : public IteratorBase {\n     return ctx->model() && node_;\n   }\n \n-  string traceme_metadata_;\n+  std::string traceme_metadata_;\n   BaseParams params_;\n };\n \n@@ -1707,7 +1709,7 @@ class DatasetIterator : public DatasetBaseIterator {\n     const DatasetType* dataset;\n \n     // Identifies the sequence of iterators leading up to this iterator.\n-    const string prefix;\n+    const std::string prefix;\n   };\n \n   explicit DatasetIterator(const Params& params)\n@@ -1774,7 +1776,8 @@ class DatasetOpKernel : public OpKernel {\n   // names that end with \"Dataset\" or \"DatasetV[0-9]+\".\n   static bool IsDatasetOp(const OpDef& op_def);\n \n-  string TraceString(const OpKernelContext& ctx, bool verbose) const override;\n+  std::string TraceString(const OpKernelContext& ctx,\n+                          bool verbose) const override;\n \n  protected:\n   // Subclasses should implement this method. It will be called during Compute"
        },
        {
            "sha": "14b16b309ea5c11cf52d80f3ab52aa9243038168",
            "filename": "tensorflow/core/framework/dataset_stateful_op_allowlist.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fdataset_stateful_op_allowlist.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fdataset_stateful_op_allowlist.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fdataset_stateful_op_allowlist.h?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -25,17 +25,17 @@ namespace data {\n // See below macro for usage details.\n class AllowlistedStatefulOpRegistry {\n  public:\n-  absl::Status Add(string op_name) {\n+  absl::Status Add(std::string op_name) {\n     op_names_.insert(std::move(op_name));\n     return absl::OkStatus();\n   }\n \n-  absl::Status Remove(string op_name) {\n+  absl::Status Remove(std::string op_name) {\n     op_names_.erase(op_name);\n     return absl::OkStatus();\n   }\n \n-  bool Contains(const string& op_name) { return op_names_.count(op_name); }\n+  bool Contains(const std::string& op_name) { return op_names_.count(op_name); }\n \n   static AllowlistedStatefulOpRegistry* Global() {\n     static auto* reg = new AllowlistedStatefulOpRegistry;\n@@ -49,7 +49,7 @@ class AllowlistedStatefulOpRegistry {\n   AllowlistedStatefulOpRegistry operator=(\n       AllowlistedStatefulOpRegistry const& copy) = delete;\n \n-  std::unordered_set<string> op_names_;\n+  std::unordered_set<std::string> op_names_;\n };\n \n }  // namespace data"
        },
        {
            "sha": "b572de72e54113b1f396bf136ab82c900a56cdf7",
            "filename": "tensorflow/core/framework/dataset_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fdataset_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fdataset_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fdataset_test.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -68,8 +68,8 @@ TEST_P(DatasetTestTotalBytes, TestTotalBytes) {\n }\n \n std::vector<Tensor> tensor_tf_int_32s() {\n-  return {test::AsTensor<int32>({1, 2, 3, 4, 5}),\n-          test::AsTensor<int32>({1, 2, 3, 4})};\n+  return {test::AsTensor<int32_t>({1, 2, 3, 4, 5}),\n+          test::AsTensor<int32_t>({1, 2, 3, 4})};\n }\n \n std::vector<Tensor> tensor_tf_int_64s() {"
        },
        {
            "sha": "1adb6e7eaf164169df59d89cc558ac8bc6ac1c7f",
            "filename": "tensorflow/core/framework/device.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fdevice.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fdevice.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fdevice.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -41,8 +41,8 @@ void Device::Sync(const DoneCallback& done) { done(Sync()); }\n \n // static\n DeviceAttributes Device::BuildDeviceAttributes(\n-    const string& name, DeviceType device, Bytes memory_limit,\n-    const DeviceLocality& locality, const string& physical_device_desc) {\n+    const std::string& name, DeviceType device, Bytes memory_limit,\n+    const DeviceLocality& locality, const std::string& physical_device_desc) {\n   DeviceAttributes da;\n   da.set_name(name);\n   do {"
        },
        {
            "sha": "891d45f237e61e768bbfe61829c544f62fb1aa4a",
            "filename": "tensorflow/core/framework/device_base.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fdevice_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fdevice_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fdevice_base.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -66,7 +66,7 @@ const DeviceAttributes& DeviceBase::attributes() const {\n   std::abort();\n }\n \n-const string& DeviceBase::name() const {\n+const std::string& DeviceBase::name() const {\n   LOG(FATAL) << \"DeviceBase does not implement name()\";  // Crash OK\n   std::abort();\n }"
        },
        {
            "sha": "15c4e6bba6ae9edc87a153b33625faf5f502664c",
            "filename": "tensorflow/core/framework/device_base.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fdevice_base.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fdevice_base.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fdevice_base.h?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -269,7 +269,7 @@ class DeviceBase {\n   // device memory tagged with an earlier freed-at count is really unencumbered\n   // by pending uses.  For this to be useful the device memory allocator must\n   // be tagging deallocated memory chunks using the same counter.\n-  virtual uint64 SafeAllocFrontier(uint64 old_value) { return 0; }\n+  virtual uint64_t SafeAllocFrontier(uint64_t old_value) { return 0; }\n \n   // Copies `input_tensor` to `output_tensor`, where both tensors are on this\n   // device. This function assumes that `output_tensor` has already been"
        },
        {
            "sha": "d6374d41a93bb77263f8718d71e0b0e3f80103da",
            "filename": "tensorflow/core/framework/device_factory.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fdevice_factory.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fdevice_factory.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fdevice_factory.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -47,14 +47,14 @@ struct FactoryItem {\n   bool is_pluggable_device;\n };\n \n-std::unordered_map<string, FactoryItem>& device_factories() {\n-  static std::unordered_map<string, FactoryItem>* factories =\n-      new std::unordered_map<string, FactoryItem>;\n+std::unordered_map<std::string, FactoryItem>& device_factories() {\n+  static std::unordered_map<std::string, FactoryItem>* factories =\n+      new std::unordered_map<std::string, FactoryItem>;\n   return *factories;\n }\n \n-bool IsDeviceFactoryEnabled(const string& device_type) {\n-  std::vector<string> enabled_devices;\n+bool IsDeviceFactoryEnabled(const std::string& device_type) {\n+  std::vector<std::string> enabled_devices;\n   TF_CHECK_OK(tensorflow::ReadStringsFromEnvVar(\n       /*env_var_name=*/\"TF_ENABLED_DEVICE_TYPES\", /*default_val=*/\"\",\n       &enabled_devices));\n@@ -67,9 +67,9 @@ bool IsDeviceFactoryEnabled(const string& device_type) {\n }  // namespace\n \n // static\n-int32 DeviceFactory::DevicePriority(const string& device_type) {\n+int32_t DeviceFactory::DevicePriority(const std::string& device_type) {\n   tf_shared_lock l(*get_device_factory_lock());\n-  std::unordered_map<string, FactoryItem>& factories = device_factories();\n+  std::unordered_map<std::string, FactoryItem>& factories = device_factories();\n   auto iter = factories.find(device_type);\n   if (iter != factories.end()) {\n     return iter->second.priority;\n@@ -78,9 +78,9 @@ int32 DeviceFactory::DevicePriority(const string& device_type) {\n   return -1;\n }\n \n-bool DeviceFactory::IsPluggableDevice(const string& device_type) {\n+bool DeviceFactory::IsPluggableDevice(const std::string& device_type) {\n   tf_shared_lock l(*get_device_factory_lock());\n-  std::unordered_map<string, FactoryItem>& factories = device_factories();\n+  std::unordered_map<std::string, FactoryItem>& factories = device_factories();\n   auto iter = factories.find(device_type);\n   if (iter != factories.end()) {\n     return iter->second.is_pluggable_device;\n@@ -89,7 +89,7 @@ bool DeviceFactory::IsPluggableDevice(const string& device_type) {\n }\n \n // static\n-void DeviceFactory::Register(const string& device_type,\n+void DeviceFactory::Register(const std::string& device_type,\n                              std::unique_ptr<DeviceFactory> factory,\n                              int priority, bool is_pluggable_device) {\n   if (!IsDeviceFactoryEnabled(device_type)) {\n@@ -98,7 +98,7 @@ void DeviceFactory::Register(const string& device_type,\n     return;\n   }\n   mutex_lock l(*get_device_factory_lock());\n-  std::unordered_map<string, FactoryItem>& factories = device_factories();\n+  std::unordered_map<std::string, FactoryItem>& factories = device_factories();\n   auto iter = factories.find(device_type);\n   if (iter == factories.end()) {\n     factories[device_type] = {std::move(factory), priority,\n@@ -113,7 +113,7 @@ void DeviceFactory::Register(const string& device_type,\n   }\n }\n \n-DeviceFactory* DeviceFactory::GetFactory(const string& device_type) {\n+DeviceFactory* DeviceFactory::GetFactory(const std::string& device_type) {\n   tf_shared_lock l(*get_device_factory_lock());\n   auto it = device_factories().find(device_type);\n   if (it == device_factories().end()) {\n@@ -128,7 +128,7 @@ DeviceFactory* DeviceFactory::GetFactory(const string& device_type) {\n }\n \n absl::Status DeviceFactory::ListAllPhysicalDevices(\n-    std::vector<string>* devices) {\n+    std::vector<std::string>* devices) {\n   // CPU first. A CPU device is required.\n   // TODO(b/183974121): Consider merge the logic into the loop below.\n   auto cpu_factory = GetFactory(\"CPU\");\n@@ -156,7 +156,7 @@ absl::Status DeviceFactory::ListAllPhysicalDevices(\n }\n \n absl::Status DeviceFactory::ListPluggablePhysicalDevices(\n-    std::vector<string>* devices) {\n+    std::vector<std::string>* devices) {\n   tf_shared_lock l(*get_device_factory_lock());\n   for (auto& p : device_factories()) {\n     if (p.second.is_pluggable_device) {\n@@ -168,7 +168,7 @@ absl::Status DeviceFactory::ListPluggablePhysicalDevices(\n }\n \n absl::Status DeviceFactory::GetAnyDeviceDetails(\n-    int device_index, std::unordered_map<string, string>* details) {\n+    int device_index, std::unordered_map<std::string, std::string>* details) {\n   if (device_index < 0) {\n     return errors::InvalidArgument(\"Device index out of bounds: \",\n                                    device_index);\n@@ -183,7 +183,7 @@ absl::Status DeviceFactory::GetAnyDeviceDetails(\n   }\n \n   // TODO(b/183974121): Consider merge the logic into the loop below.\n-  std::vector<string> devices;\n+  std::vector<std::string> devices;\n   TF_RETURN_IF_ERROR(cpu_factory->ListPhysicalDevices(&devices));\n   if (device_index < devices.size()) {\n     return cpu_factory->GetDeviceDetails(device_index, details);\n@@ -211,7 +211,7 @@ absl::Status DeviceFactory::GetAnyDeviceDetails(\n }\n \n absl::Status DeviceFactory::AddCpuDevices(\n-    const SessionOptions& options, const string& name_prefix,\n+    const SessionOptions& options, const std::string& name_prefix,\n     std::vector<std::unique_ptr<Device>>* devices) {\n   auto cpu_factory = GetFactory(\"CPU\");\n   if (!cpu_factory) {\n@@ -228,7 +228,7 @@ absl::Status DeviceFactory::AddCpuDevices(\n }\n \n absl::Status DeviceFactory::AddDevices(\n-    const SessionOptions& options, const string& name_prefix,\n+    const SessionOptions& options, const std::string& name_prefix,\n     std::vector<std::unique_ptr<Device>>* devices) {\n   // CPU first. A CPU device is required.\n   // TODO(b/183974121): Consider merge the logic into the loop below.\n@@ -263,9 +263,9 @@ absl::Status DeviceFactory::AddDevices(\n   return absl::OkStatus();\n }\n \n-std::unique_ptr<Device> DeviceFactory::NewDevice(const string& type,\n-                                                 const SessionOptions& options,\n-                                                 const string& name_prefix) {\n+std::unique_ptr<Device> DeviceFactory::NewDevice(\n+    const std::string& type, const SessionOptions& options,\n+    const std::string& name_prefix) {\n   auto device_factory = GetFactory(type);\n   if (!device_factory) {\n     return nullptr;"
        },
        {
            "sha": "e30a4538fa939a7fc743918b80e4f58998936740",
            "filename": "tensorflow/core/framework/device_factory.h",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fdevice_factory.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fdevice_factory.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fdevice_factory.h?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -58,34 +58,35 @@ class DeviceFactory {\n   // Helper for tests.  Create a single device of type \"type\".  The\n   // returned device is always numbered zero, so if creating multiple\n   // devices of the same type, supply distinct name_prefix arguments.\n-  static std::unique_ptr<Device> NewDevice(const string& type,\n+  static std::unique_ptr<Device> NewDevice(const std::string& type,\n                                            const SessionOptions& options,\n-                                           const string& name_prefix);\n+                                           const std::string& name_prefix);\n \n   // Iterate through all device factories and build a list of all of the\n   // possible physical devices.\n   //\n   // CPU is are added first.\n-  static absl::Status ListAllPhysicalDevices(std::vector<string>* devices);\n+  static absl::Status ListAllPhysicalDevices(std::vector<std::string>* devices);\n \n   // Iterate through all device factories and build a list of all of the\n   // possible pluggable physical devices.\n   static absl::Status ListPluggablePhysicalDevices(\n-      std::vector<string>* devices);\n+      std::vector<std::string>* devices);\n \n   // Get details for a specific device among all device factories.\n   // 'device_index' indexes into devices from ListAllPhysicalDevices.\n   static absl::Status GetAnyDeviceDetails(\n-      int device_index, std::unordered_map<string, string>* details);\n+      int device_index, std::unordered_map<std::string, std::string>* details);\n \n   // For a specific device factory list all possible physical devices.\n-  virtual absl::Status ListPhysicalDevices(std::vector<string>* devices) = 0;\n+  virtual absl::Status ListPhysicalDevices(\n+      std::vector<std::string>* devices) = 0;\n \n   // Get details for a specific device for a specific factory. Subclasses\n   // can store arbitrary device information in the map. 'device_index' indexes\n   // into devices from ListPhysicalDevices.\n   virtual absl::Status GetDeviceDetails(\n-      int device_index, std::unordered_map<string, string>* details) {\n+      int device_index, std::unordered_map<std::string, std::string>* details) {\n     return absl::OkStatus();\n   }\n \n@@ -106,7 +107,7 @@ class DeviceFactory {\n   // higher than the packaged devices.  See calls to\n   // REGISTER_LOCAL_DEVICE_FACTORY to see the existing priorities used\n   // for built-in devices.\n-  static int32 DevicePriority(const std::string& device_type);\n+  static int32_t DevicePriority(const std::string& device_type);\n \n   // Returns true if 'device_type' is registered from plugin. Returns false if\n   // 'device_type' is a first-party device."
        },
        {
            "sha": "c295e18bca197b2a5dc0c1d123e5347fa325acea",
            "filename": "tensorflow/core/framework/fake_input.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffake_input.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffake_input.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ffake_input.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -36,15 +36,15 @@ class FakeInputImpl {\n   absl::Status AddInputToBuilder();\n \n  private:\n-  static string FakeNodeName(int in_index);\n+  static std::string FakeNodeName(int in_index);\n   absl::Status GetN(int* n) const;\n   absl::Status GetDataType(DataType* dt) const;\n   void NSources(int n, DataType dt) const;\n   void SourceList(DataTypeSlice dts) const;\n \n   const OpDef* const op_def_;\n   const OpDef::ArgDef* const arg_;\n-  const string in_node_;\n+  const std::string in_node_;\n   const NodeDef* const node_def_;\n   NodeDefBuilder* const builder_;\n \n@@ -120,9 +120,9 @@ absl::Status FakeInputImpl::AddInputToBuilder() {\n }\n \n // static\n-string FakeInputImpl::FakeNodeName(int in_index) {\n+std::string FakeInputImpl::FakeNodeName(int in_index) {\n   char c = 'a' + (in_index % 26);\n-  return string(&c, 1);\n+  return std::string(&c, 1);\n }\n \n absl::Status FakeInputImpl::GetN(int* n) const {"
        },
        {
            "sha": "2fc478466337e7439fd280672c5e7bc085df16e2",
            "filename": "tensorflow/core/framework/full_type_inference_util.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffull_type_inference_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffull_type_inference_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ffull_type_inference_util.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -342,7 +342,7 @@ TypeInferenceFn MapCovariant(FullTypeId t, FullTypeId u, int input_idx) {\n       };\n }\n \n-TypeInferenceFn FunctionCall(const string& func_attr_name) {\n+TypeInferenceFn FunctionCall(const std::string& func_attr_name) {\n   return [func_attr_name](const TypeRefVector& input_types,\n                           const FunctionTypeInferrer& infer_function_rets)\n              -> absl::StatusOr<FullTypeDef> {"
        },
        {
            "sha": "211768f4a0083b1c041f344589c7680e47c7e161",
            "filename": "tensorflow/core/framework/full_type_inference_util.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffull_type_inference_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffull_type_inference_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ffull_type_inference_util.h?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -122,7 +122,7 @@ TypeInferenceFn MapCovariant(FullTypeId t, FullTypeId u, int input_idx);\n // Helper for ops with semantics of calling a function. The function is\n // specified indirectly, as the name of an attribute that holds the actual\n // function name.\n-TypeInferenceFn FunctionCall(const string& func_attr_name);\n+TypeInferenceFn FunctionCall(const std::string& func_attr_name);\n \n // Compose the type of a function by concatenating the outputs of multiple\n // type inference functions. If func_list is {type inference function 1, type"
        },
        {
            "sha": "ea5fad4f704ff3c9fa91be98ba5a54f7e973233a",
            "filename": "tensorflow/core/framework/full_type_util.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffull_type_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffull_type_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ffull_type_util.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -51,7 +51,7 @@ OpTypeConstructor Nullary(FullTypeId t) {\n   };\n }\n \n-OpTypeConstructor Unary(FullTypeId t, const string& var_name) {\n+OpTypeConstructor Unary(FullTypeId t, const std::string& var_name) {\n   return [t, var_name](OpDef* op_def) {\n     FullTypeDef* tdef =\n         op_def->mutable_output_arg(0)->mutable_experimental_full_type();\n@@ -93,7 +93,8 @@ OpTypeConstructor UnaryTensorContainer(FullTypeId t, FullTypeId dtype) {\n   };\n }\n \n-OpTypeConstructor UnaryTensorContainer(FullTypeId t, const string& var_name) {\n+OpTypeConstructor UnaryTensorContainer(FullTypeId t,\n+                                       const std::string& var_name) {\n   return [t, var_name](OpDef* op_def) {\n     FullTypeDef* tdef =\n         op_def->mutable_output_arg(0)->mutable_experimental_full_type();\n@@ -110,7 +111,7 @@ OpTypeConstructor UnaryTensorContainer(FullTypeId t, const string& var_name) {\n }\n \n OpTypeConstructor VariadicTensorContainer(FullTypeId t,\n-                                          const string& var_name) {\n+                                          const std::string& var_name) {\n   return [t, var_name](OpDef* op_def) {\n     FullTypeDef* tdef =\n         op_def->mutable_output_arg(0)->mutable_experimental_full_type();"
        },
        {
            "sha": "392871a189651ecf36ec7e952fefd61b5ef4a778",
            "filename": "tensorflow/core/framework/full_type_util.h",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffull_type_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffull_type_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ffull_type_util.h?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -52,7 +52,7 @@ OpTypeConstructor NoOutputs();\n OpTypeConstructor Nullary(FullTypeId t);\n \n // Helper for a type constructor of <t>[FT_VAR[<var_name>]].\n-OpTypeConstructor Unary(FullTypeId t, const string& var_name);\n+OpTypeConstructor Unary(FullTypeId t, const std::string& var_name);\n \n // Helper for a type constructor of <t>[FT_ANY].\n OpTypeConstructor UnaryGeneric(FullTypeId t);\n@@ -61,15 +61,17 @@ OpTypeConstructor UnaryGeneric(FullTypeId t);\n OpTypeConstructor UnaryTensorContainer(FullTypeId t, FullTypeId dtype);\n \n // Helper for a type constructor of <t>[FT_VAR[<var_name>]].\n-OpTypeConstructor UnaryTensorContainer(FullTypeId t, const string& var_name);\n+OpTypeConstructor UnaryTensorContainer(FullTypeId t,\n+                                       const std::string& var_name);\n \n // Helper for a type constructor of\n // <t>[FT_FOR_EACH[\n //     FT_PRODUCT,\n //     FT_TENSOR[FT_VAR[<var_name>]],\n //     FT_VAR[<var_name>]].\n // Multi-valued type variables will expand the template (see full_type.proto).\n-OpTypeConstructor VariadicTensorContainer(FullTypeId t, const string& var_name);\n+OpTypeConstructor VariadicTensorContainer(FullTypeId t,\n+                                          const std::string& var_name);\n \n // Type specialization and inference logic. This function narrows the type\n // specified in an op definition. Such types are usually generic and dependent"
        },
        {
            "sha": "91653d2cb0936f41983f2abd8514df50140bfc61",
            "filename": "tensorflow/core/framework/function.cc",
            "status": "modified",
            "additions": 164,
            "deletions": 154,
            "changes": 318,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffunction.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffunction.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ffunction.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -122,7 +122,7 @@ absl::Status ArgNumType(AttrSlice attrs, const OpDef::ArgDef& arg_def,\n namespace {\n \n template <typename T>\n-void AddAttr(const string& name, const T& val, NodeDef* ndef) {\n+void AddAttr(const std::string& name, const T& val, NodeDef* ndef) {\n   SetAttrValue(val, &((*ndef->mutable_attr())[name]));\n }\n \n@@ -207,7 +207,7 @@ class FunctionInstantiationHelper {\n             \"Expected arg_index to be equal to the number of nodes in result.\",\n             \" Got \", arg_index, \" and \", result_.nodes.size());\n       }\n-      string name = arg_def.name();\n+      std::string name = arg_def.name();\n       if (dtypes.size() > 1) {\n         absl::StrAppend(&name, \"_\", i);\n       }\n@@ -259,7 +259,7 @@ class FunctionInstantiationHelper {\n           ArgNumType(attrs, node_sig->output_arg(i), &is_type_list, &dtypes));\n       // Note that we rely on the backwards-compatibility test enforcing\n       // that output_arg(*).name() doesn't change here.\n-      const string base_name =\n+      const std::string base_name =\n           absl::StrCat(node.name(), \":\", node_sig->output_arg(i).name());\n       TF_RETURN_IF_ERROR(\n           AddItem(base_name, {false, arg_index, start, is_type_list, dtypes}));\n@@ -299,7 +299,7 @@ class FunctionInstantiationHelper {\n               \" >= \", fnode.input_size());\n         }\n         // Look up the next input.\n-        const string& input_name = fnode.input(fnode_arg_index);\n+        const std::string& input_name = fnode.input(fnode_arg_index);\n         const auto* item = GetItemOrNull(input_name);\n         if (item == nullptr) {\n           return errors::InvalidArgument(\n@@ -331,15 +331,15 @@ class FunctionInstantiationHelper {\n \n     // Control deps.\n     for (int i = fnode_arg_index; i < fnode.input_size(); ++i) {\n-      const string& input = fnode.input(i);\n+      const std::string& input = fnode.input(i);\n       if (input.empty() || input[0] != '^') {\n         return errors::InvalidArgument(\"Expected input[\", i, \"] == '\", input,\n                                        \"' to be a control input.\");\n       }\n       int nid = -1;\n-      const string node_name = input.substr(1);\n-      const string node_colon = node_name + \":\";\n-      const string node_colon_bound = node_name + \";\";\n+      const std::string node_name = input.substr(1);\n+      const std::string node_colon = node_name + \":\";\n+      const std::string node_colon_bound = node_name + \";\";\n       // index_ is a map sorted lexicographically, so the key we are looking for\n       // must lie in the range [node_name, node_colon_bound).\n       auto it = index_.lower_bound(node_name);\n@@ -379,7 +379,7 @@ class FunctionInstantiationHelper {\n \n   absl::Status AddReturnNode(\n       const OpDef::ArgDef& ret_def, AttrSlice attrs,\n-      const ::tensorflow::protobuf::Map<string, string>& ret_map,\n+      const ::tensorflow::protobuf::Map<std::string, std::string>& ret_map,\n       bool ints_on_device, int* ret_index) {\n     auto ret_iter = ret_map.find(ret_def.name());\n     if (ret_iter == ret_map.end()) {\n@@ -401,7 +401,7 @@ class FunctionInstantiationHelper {\n                                      DataTypeVectorString(item->dtypes));\n     }\n     for (size_t i = 0; i < dtypes.size(); ++i) {\n-      string name = absl::StrCat(ret_def.name(), \"_RetVal\");\n+      std::string name = absl::StrCat(ret_def.name(), \"_RetVal\");\n       if (dtypes.size() > 1) {\n         absl::StrAppend(&name, \"_\", i);\n       }\n@@ -456,7 +456,7 @@ class FunctionInstantiationHelper {\n   };\n \n   // Adds an item into the input name index.\n-  absl::Status AddItem(const string& name, const NameInfoItem& item) {\n+  absl::Status AddItem(const std::string& name, const NameInfoItem& item) {\n     if (!index_.insert({name, item}).second) {\n       return errors::InvalidArgument(\n           absl::StrCat(\"Duplicated \", item.is_func_arg ? \"arg\" : \"ret\",\n@@ -466,28 +466,28 @@ class FunctionInstantiationHelper {\n     return absl::OkStatus();\n   }\n \n-  const NameInfoItem* GetItemOrNull(const string& name) const {\n+  const NameInfoItem* GetItemOrNull(const std::string& name) const {\n     return gtl::FindOrNull(index_, name);\n   }\n \n-  string Dep(int node_index) const {\n+  std::string Dep(int node_index) const {\n     return absl::StrCat(\"^\", Name(node_index));\n   }\n \n-  string Name(int node_index) const {\n+  std::string Name(int node_index) const {\n     CHECK_LT(node_index, nodes_.size());\n     return nodes_[node_index].name;\n   }\n \n-  string Name(int node_index, int output_index) const {\n+  std::string Name(int node_index, int output_index) const {\n     if (output_index == 0) {\n       return Name(node_index);\n     } else {\n       return absl::StrCat(Name(node_index), \":\", output_index);\n     }\n   }\n \n-  NodeDef* AddNode(const string& name) {\n+  NodeDef* AddNode(const std::string& name) {\n     result_.nodes.emplace_back();\n     NodeDef* gnode = &result_.nodes.back();\n     gnode->set_name(name);\n@@ -510,11 +510,11 @@ class FunctionInstantiationHelper {\n   GetFunctionSignature get_function_;\n   InstantiationResult& result_;\n   // A small index for all names that can be used as a node's input arguments.\n-  std::map<string, NameInfoItem> index_;\n+  std::map<std::string, NameInfoItem> index_;\n   // This contains information about a node in the new graph including the node\n   // names and input nodes' indexes.\n   struct NodeInfo {\n-    string name;\n+    std::string name;\n     // Data inputs where <n, k> means arg k of node n.\n     std::vector<std::pair<int, int>> data_inputs;\n     // Control inputs (dependencies).\n@@ -525,8 +525,8 @@ class FunctionInstantiationHelper {\n };\n \n // Various helpers Print(proto) to print relevant protos to ascii.\n-string Print(const OpDef::ArgDef& arg) {\n-  string out;\n+std::string Print(const OpDef::ArgDef& arg) {\n+  std::string out;\n   absl::StrAppend(&out, arg.name(), \":\");\n   if (arg.is_ref()) absl::StrAppend(&out, \"Ref(\");\n   if (!arg.number_attr().empty()) {\n@@ -545,13 +545,13 @@ string Print(const OpDef::ArgDef& arg) {\n // When hash_string_attrs = true, string attributes are hashed instead of being\n // truncated with ellipses. This is done to reduce the chance of collisions when\n // looking up functions using the canonical representation.\n-string Print(const AttrValue& attr_value,\n-             const bool hash_string_attrs = false) {\n+std::string Print(const AttrValue& attr_value,\n+                  const bool hash_string_attrs = false) {\n   if (attr_value.value_case() == AttrValue::kType) {\n     return DataTypeString(attr_value.type());\n   } else if ((attr_value.value_case() == AttrValue::kList) &&\n              (attr_value.list().type_size() > 0)) {\n-    string ret = \"{\";\n+    std::string ret = \"{\";\n     for (int i = 0; i < attr_value.list().type_size(); ++i) {\n       if (i > 0) absl::StrAppend(&ret, \", \");\n       absl::StrAppend(&ret, DataTypeString(attr_value.list().type(i)));\n@@ -562,7 +562,7 @@ string Print(const AttrValue& attr_value,\n     if (attr_value.func().attr_size() == 0) {\n       return attr_value.func().name();\n     }\n-    std::vector<string> entries;\n+    std::vector<std::string> entries;\n     for (const auto& p : attr_value.func().attr()) {\n       entries.push_back(absl::StrCat(p.first, \"=\", Print(p.second)));\n     }\n@@ -576,11 +576,11 @@ string Print(const AttrValue& attr_value,\n }\n \n // TODO(josh11b): Merge this with SummarizeNodeDef().\n-string Print(const NodeDef& n) {\n-  string out;\n+std::string Print(const NodeDef& n) {\n+  std::string out;\n   absl::StrAppend(&out, n.name(), \" = \", n.op());\n   if (n.attr_size() > 0) {\n-    std::vector<string> entries;\n+    std::vector<std::string> entries;\n     for (auto& a : n.attr()) {\n       entries.push_back(absl::StrCat(a.first, \"=\", Print(a.second)));\n     }\n@@ -598,7 +598,7 @@ string Print(const NodeDef& n) {\n   }\n   absl::StrAppend(&out, \"(\");\n   std::vector<absl::string_view> dat;\n-  std::vector<string> dep;\n+  std::vector<std::string> dep;\n   for (absl::string_view s : n.input()) {\n     if (absl::ConsumePrefix(&s, \"^\")) {\n       dep.emplace_back(s);\n@@ -613,8 +613,8 @@ string Print(const NodeDef& n) {\n   return out;\n }\n \n-string Print(const FunctionDef& fdef) {\n-  string out;\n+std::string Print(const FunctionDef& fdef) {\n+  std::string out;\n   const OpDef& sig = fdef.signature();\n   absl::StrAppend(&out, \"\\n\", sig.name());\n   if (sig.attr_size() > 0) {\n@@ -654,7 +654,7 @@ string Print(const FunctionDef& fdef) {\n   return out;\n }\n \n-string Print(absl::Span<const NodeDef* const> nodes) {\n+std::string Print(absl::Span<const NodeDef* const> nodes) {\n   std::vector<const NodeDef*> arg;\n   std::vector<const NodeDef*> ret;\n   std::vector<const NodeDef*> body;\n@@ -678,7 +678,7 @@ string Print(absl::Span<const NodeDef* const> nodes) {\n   };\n   std::sort(arg.begin(), arg.end(), comp);\n   std::sort(ret.begin(), ret.end(), comp);\n-  string out;\n+  std::string out;\n   absl::StrAppend(&out, \"\\n(\");\n   auto get_type_and_device = [](const NodeDef& n) {\n     DataType dt;\n@@ -714,7 +714,7 @@ string Print(absl::Span<const NodeDef* const> nodes) {\n     // The _RetVal op should have a unique non-control input. We assert that\n     // here and add it to the output.\n     bool found_non_control_input = false;\n-    for (const string& input : n->input()) {\n+    for (const std::string& input : n->input()) {\n       if (!input.empty() && input[0] != '^') {\n         DCHECK_EQ(found_non_control_input, false)\n             << \"RetVal node has more than one non-control input: \"\n@@ -735,7 +735,7 @@ string Print(absl::Span<const NodeDef* const> nodes) {\n   return out;\n }\n \n-absl::Status AddDefaultAttrs(const string& op,\n+absl::Status AddDefaultAttrs(const std::string& op,\n                              const GetFunctionSignature& get_function,\n                              AttrValueMap* attrs) {\n   const OpDef* op_def = nullptr;\n@@ -799,7 +799,8 @@ absl::Status InstantiateFunction(const FunctionDef& fdef, AttrSlice attr_values,\n     }\n   }\n \n-  auto substitute = [attr_values, &sig](const string& name, AttrValue* val) {\n+  auto substitute = [attr_values, &sig](const std::string& name,\n+                                        AttrValue* val) {\n     // Look for a specified value...\n     if (const AttrValue* v = attr_values.FindByString(name)) {\n       *val = *v;\n@@ -870,26 +871,26 @@ absl::Status InstantiateFunction(const FunctionDef& fdef, AttrSlice attr_values,\n   return absl::OkStatus();\n }\n \n-string DebugString(const FunctionDef& func_def) { return Print(func_def); }\n+std::string DebugString(const FunctionDef& func_def) { return Print(func_def); }\n \n-string DebugString(const GraphDef& instantiated_func_def) {\n+std::string DebugString(const GraphDef& instantiated_func_def) {\n   std::vector<const NodeDef*> ptrs;\n   for (const NodeDef& n : instantiated_func_def.node()) {\n     ptrs.push_back(&n);\n   }\n   return Print(ptrs);\n }\n \n-string DebugString(absl::Span<const NodeDef> instantiated_func_nodes) {\n+std::string DebugString(absl::Span<const NodeDef> instantiated_func_nodes) {\n   std::vector<const NodeDef*> ptrs;\n   for (const NodeDef& n : instantiated_func_nodes) {\n     ptrs.push_back(&n);\n   }\n   return Print(ptrs);\n }\n \n-string DebugStringWhole(const GraphDef& gdef) {\n-  string ret;\n+std::string DebugStringWhole(const GraphDef& gdef) {\n+  std::string ret;\n   for (const auto& fdef : gdef.library().function()) {\n     absl::StrAppend(&ret, Print(fdef));\n   }\n@@ -905,8 +906,8 @@ namespace {\n // Returns the name -> attr mapping of fdef's attrs that have a value set. In\n // Python, it's possible to access unset attrs, which returns a default value\n // and adds an unset attr to the map.\n-std::map<string, AttrValue> GetSetAttrs(const FunctionDef& fdef) {\n-  std::map<string, AttrValue> set_attrs;\n+std::map<std::string, AttrValue> GetSetAttrs(const FunctionDef& fdef) {\n+  std::map<std::string, AttrValue> set_attrs;\n   for (const auto& pair : fdef.attr()) {\n     if (pair.second.value_case() != AttrValue::VALUE_NOT_SET) {\n       set_attrs[pair.first] = pair.second;\n@@ -920,8 +921,8 @@ std::map<string, AttrValue> GetSetAttrs(const FunctionDef& fdef) {\n bool FunctionDefsEqual(const FunctionDef& f1, const FunctionDef& f2) {\n   if (!OpDefEqual(f1.signature(), f2.signature())) return false;\n \n-  std::map<string, AttrValue> f1_attrs = GetSetAttrs(f1);\n-  std::map<string, AttrValue> f2_attrs = GetSetAttrs(f2);\n+  std::map<std::string, AttrValue> f1_attrs = GetSetAttrs(f1);\n+  std::map<std::string, AttrValue> f2_attrs = GetSetAttrs(f2);\n   if (f1_attrs.size() != f2_attrs.size()) return false;\n   for (const auto& iter1 : f1_attrs) {\n     auto iter2 = f2_attrs.find(iter1.first);\n@@ -933,25 +934,25 @@ bool FunctionDefsEqual(const FunctionDef& f1, const FunctionDef& f2) {\n     return false;\n   }\n \n-  std::map<string, string> ret1(f1.ret().begin(), f1.ret().end());\n-  std::map<string, string> ret2(f2.ret().begin(), f2.ret().end());\n+  std::map<std::string, std::string> ret1(f1.ret().begin(), f1.ret().end());\n+  std::map<std::string, std::string> ret2(f2.ret().begin(), f2.ret().end());\n   if (ret1 != ret2) return false;\n \n-  std::map<string, string> control_ret1(f1.control_ret().begin(),\n-                                        f1.control_ret().end());\n-  std::map<string, string> control_ret2(f2.control_ret().begin(),\n-                                        f2.control_ret().end());\n+  std::map<std::string, std::string> control_ret1(f1.control_ret().begin(),\n+                                                  f1.control_ret().end());\n+  std::map<std::string, std::string> control_ret2(f2.control_ret().begin(),\n+                                                  f2.control_ret().end());\n   if (control_ret1 != control_ret2) return false;\n \n   return true;\n }\n \n-uint64 FunctionDefHash(const FunctionDef& fdef) {\n+uint64_t FunctionDefHash(const FunctionDef& fdef) {\n   // signature\n-  uint64 h = OpDefHash(fdef.signature());\n+  uint64_t h = OpDefHash(fdef.signature());\n \n   // attrs\n-  std::map<string, AttrValue> attrs = GetSetAttrs(fdef);\n+  std::map<std::string, AttrValue> attrs = GetSetAttrs(fdef);\n   for (const auto& p : attrs) {\n     h = Hash64(p.first.data(), p.first.size(), h);\n     h = Hash64Combine(AttrValueHash(p.second), h);\n@@ -961,15 +962,15 @@ uint64 FunctionDefHash(const FunctionDef& fdef) {\n   h = Hash64Combine(RepeatedNodeDefHash(fdef.node_def()), h);\n \n   // output names\n-  std::map<string, string> ret(fdef.ret().begin(), fdef.ret().end());\n+  std::map<std::string, std::string> ret(fdef.ret().begin(), fdef.ret().end());\n   for (const auto& p : ret) {\n     h = Hash64(p.first.data(), p.first.size(), h);\n     h = Hash64(p.second.data(), p.second.size(), h);\n   }\n \n   // control output names\n-  std::map<string, string> control_ret(fdef.control_ret().begin(),\n-                                       fdef.control_ret().end());\n+  std::map<std::string, std::string> control_ret(fdef.control_ret().begin(),\n+                                                 fdef.control_ret().end());\n   for (const auto& p : control_ret) {\n     h = Hash64(p.first.data(), p.first.size(), h);\n     h = Hash64(p.second.data(), p.second.size(), h);\n@@ -981,14 +982,14 @@ uint64 FunctionDefHash(const FunctionDef& fdef) {\n static constexpr const char* const kExecutorAttr = \"_executor\";\n \n /* static */\n-string FunctionLibraryRuntime::ExecutorType(const InstantiateOptions& options,\n-                                            AttrSlice attrs) {\n+std::string FunctionLibraryRuntime::ExecutorType(\n+    const InstantiateOptions& options, AttrSlice attrs) {\n   if (!options.executor_type.empty()) {\n     return options.executor_type;\n   } else if (const AttrValue* executor_attr = attrs.Find(kExecutorAttr)) {\n     return executor_attr->s();\n   } else {\n-    return string();\n+    return std::string();\n   }\n }\n \n@@ -999,7 +1000,7 @@ class AttrKeyAndValue {\n     kRaw,\n     kCEscape,\n   };\n-  AttrKeyAndValue(absl::string_view key_name, int key_suffix, string value,\n+  AttrKeyAndValue(absl::string_view key_name, int key_suffix, std::string value,\n                   ValueRepresentationOp value_op = kRaw)\n       : key_name_(key_name),\n         key_suffix_(key_suffix),\n@@ -1016,7 +1017,7 @@ class AttrKeyAndValue {\n     }\n   }\n \n-  void AppendTo(bool first, string* s) const {\n+  void AppendTo(bool first, std::string* s) const {\n     absl::string_view v;\n     bool add_escaped = false;\n     if ((value_op_ == kCEscape) && NeedsEscaping(value_)) {\n@@ -1037,7 +1038,7 @@ class AttrKeyAndValue {\n   }\n \n  private:\n-  static bool NeedsEscaping(const string& s) {\n+  static bool NeedsEscaping(const std::string& s) {\n     for (auto c : s) {\n       if (!absl::ascii_isalnum(c) && (c != ' ')) {\n         return true;\n@@ -1049,16 +1050,17 @@ class AttrKeyAndValue {\n   absl::string_view key_name_;\n   int key_suffix_;  // -1 if missing\n   ValueRepresentationOp value_op_;\n-  string value_;\n+  std::string value_;\n };\n }  // namespace\n \n-string GetFunctionResourceInputDevice(\n+std::string GetFunctionResourceInputDevice(\n     const Tensor& input, const int arg_index, const FunctionDef& function_def,\n-    absl::flat_hash_map<string, std::vector<string>>* composite_devices) {\n+    absl::flat_hash_map<std::string, std::vector<std::string>>*\n+        composite_devices) {\n   const auto& handles = input.flat<ResourceHandle>();\n   const ResourceHandle& handle0 = handles(0);\n-  string composite_device;\n+  std::string composite_device;\n   auto iter = function_def.arg_attr().find(arg_index);\n   if (iter != function_def.arg_attr().end()) {\n     auto arg_attr = iter->second.attr().find(\"_composite_device\");\n@@ -1078,8 +1080,9 @@ string GetFunctionResourceInputDevice(\n   }\n }\n \n-string Canonicalize(const string& funcname, AttrSlice attrs,\n-                    const FunctionLibraryRuntime::InstantiateOptions& options) {\n+std::string Canonicalize(\n+    const std::string& funcname, AttrSlice attrs,\n+    const FunctionLibraryRuntime::InstantiateOptions& options) {\n   absl::InlinedVector<AttrKeyAndValue, 8> entries;\n   entries.reserve(attrs.size() + static_cast<int>(!options.target.empty()) +\n                   options.input_devices.size());\n@@ -1118,20 +1121,21 @@ string Canonicalize(const string& funcname, AttrSlice attrs,\n     entries.push_back(\n         AttrKeyAndValue(\"_state_handle\", -1, options.state_handle));\n   }\n-  string executor_type = FunctionLibraryRuntime::ExecutorType(options, attrs);\n+  std::string executor_type =\n+      FunctionLibraryRuntime::ExecutorType(options, attrs);\n   if (!executor_type.empty()) {\n     entries.push_back(AttrKeyAndValue(kExecutorAttr, -1, executor_type));\n   }\n   if (options.config_proto.ByteSize() > 0) {\n-    string config_proto_serialized;\n+    std::string config_proto_serialized;\n     SerializeToStringDeterministic(options.config_proto,\n                                    &config_proto_serialized);\n     entries.push_back(AttrKeyAndValue(\"_config_proto\", -1,\n                                       config_proto_serialized,\n                                       AttrKeyAndValue::kCEscape));\n   }\n   std::sort(entries.begin(), entries.end());\n-  string result = absl::StrCat(funcname, \"[\");\n+  std::string result = absl::StrCat(funcname, \"[\");\n   bool first = true;\n   for (const auto& entry : entries) {\n     entry.AppendTo(first, &result);\n@@ -1141,7 +1145,7 @@ string Canonicalize(const string& funcname, AttrSlice attrs,\n   return result;\n }\n \n-string Canonicalize(const string& funcname, AttrSlice attrs) {\n+std::string Canonicalize(const std::string& funcname, AttrSlice attrs) {\n   static const FunctionLibraryRuntime::InstantiateOptions* kEmptyOptions =\n       new FunctionLibraryRuntime::InstantiateOptions;\n   return Canonicalize(funcname, attrs, *kEmptyOptions);\n@@ -1373,12 +1377,13 @@ void FunctionLibraryDefinition::Initialize(\n   }\n }\n \n-bool FunctionLibraryDefinition::Contains(const string& func) const {\n+bool FunctionLibraryDefinition::Contains(const std::string& func) const {\n   tf_shared_lock l(mu_);\n   return records_.find(func) != records_.end();\n }\n \n-const FunctionDef* FunctionLibraryDefinition::Find(const string& func) const {\n+const FunctionDef* FunctionLibraryDefinition::Find(\n+    const std::string& func) const {\n   tf_shared_lock l(mu_);\n   auto result = FindHelper(func);\n   if (result) {\n@@ -1389,13 +1394,13 @@ const FunctionDef* FunctionLibraryDefinition::Find(const string& func) const {\n }\n \n core::RefCountPtr<FunctionRecord> FunctionLibraryDefinition::FindRecord(\n-    const string& func) const {\n+    const std::string& func) const {\n   tf_shared_lock l(mu_);\n   return FindHelper(func);\n }\n \n core::RefCountPtr<FunctionRecord> FunctionLibraryDefinition::FindHelper(\n-    const string& func) const {\n+    const std::string& func) const {\n   auto iter = records_.find(func);\n   if (iter == records_.end()) {\n     return nullptr;\n@@ -1474,7 +1479,7 @@ absl::Status FunctionLibraryDefinition::AddHelper(FunctionRecord* registration,\n }\n \n absl::Status FunctionLibraryDefinition::CopyFunctionDefFrom(\n-    const string& name, const FunctionLibraryDefinition& other) {\n+    const std::string& name, const FunctionLibraryDefinition& other) {\n   if (default_registry() != other.default_registry()) {\n     return errors::InvalidArgument(\n         \"Cannot copy function '\", name,\n@@ -1516,7 +1521,7 @@ absl::Status FunctionLibraryDefinition::AddGradientDef(\n absl::Status FunctionLibraryDefinition::AddGradientDefHelper(\n     const GradientDef& grad, bool* added) {\n   *added = false;\n-  string* entry = &func_grad_[grad.function_name()];\n+  std::string* entry = &func_grad_[grad.function_name()];\n   if (!entry->empty()) {\n     if (*entry != grad.gradient_func()) {\n       return errors::InvalidArgument(\n@@ -1545,8 +1550,8 @@ absl::Status FunctionLibraryDefinition::AddLibrary(\n   mutex_lock l2(other.mu_);\n   // Remember the funcs and grads that we added successfully so that\n   // we can roll them back on error.\n-  std::vector<string> funcs;\n-  std::vector<string> funcs_with_grads;\n+  std::vector<std::string> funcs;\n+  std::vector<std::string> funcs_with_grads;\n   absl::Status s;\n   bool added;\n   for (const auto& [name, record] : other.records_) {\n@@ -1603,8 +1608,8 @@ absl::Status FunctionLibraryDefinition::AddLibrary(\n   // Remember the funcs and grads that we added successfully so that\n   // we can roll them back on error.\n   mutex_lock l(mu_);\n-  std::vector<string> funcs;\n-  std::vector<string> funcs_with_grads;\n+  std::vector<std::string> funcs;\n+  std::vector<std::string> funcs_with_grads;\n   absl::Status s;\n   bool added;\n   for (FunctionDef& fdef : *lib_def.mutable_function()) {\n@@ -1641,7 +1646,7 @@ absl::Status FunctionLibraryDefinition::AddLibrary(\n }\n \n absl::Status FunctionLibraryDefinition::ReplaceFunction(\n-    const string& func, const FunctionDef& fdef,\n+    const std::string& func, const FunctionDef& fdef,\n     const StackTracesMap& stack_traces) {\n   mutex_lock l(mu_);\n   bool added;\n@@ -1660,14 +1665,15 @@ absl::Status FunctionLibraryDefinition::ReplaceGradient(\n   return absl::OkStatus();\n }\n \n-absl::Status FunctionLibraryDefinition::RemoveFunction(const string& func) {\n+absl::Status FunctionLibraryDefinition::RemoveFunction(\n+    const std::string& func) {\n   mutex_lock l(mu_);\n   TF_RETURN_IF_ERROR(RemoveFunctionHelper(func));\n   return absl::OkStatus();\n }\n \n absl::Status FunctionLibraryDefinition::RemoveFunctionHelper(\n-    const string& func) {\n+    const std::string& func) {\n   auto iter = records_.find(func);\n   if (iter == records_.end()) {\n     return errors::InvalidArgument(\"Tried to remove non-existent function '\",\n@@ -1688,7 +1694,8 @@ void FunctionLibraryDefinition::Clear() {\n   func_grad_.clear();\n }\n \n-absl::Status FunctionLibraryDefinition::RemoveGradient(const string& func) {\n+absl::Status FunctionLibraryDefinition::RemoveGradient(\n+    const std::string& func) {\n   const auto& i = func_grad_.find(func);\n   if (i == func_grad_.end()) {\n     return errors::InvalidArgument(\"Tried to remove non-existent gradient '\",\n@@ -1699,16 +1706,16 @@ absl::Status FunctionLibraryDefinition::RemoveGradient(const string& func) {\n }\n \n absl::Status FunctionLibraryDefinition::Remove(\n-    const std::vector<string>& funcs,\n-    const std::vector<string>& funcs_with_grads) {\n+    const std::vector<std::string>& funcs,\n+    const std::vector<std::string>& funcs_with_grads) {\n   absl::Status s;\n-  for (const string& f : funcs) {\n+  for (const std::string& f : funcs) {\n     s = RemoveFunctionHelper(f);\n     if (!s.ok()) {\n       return s;\n     }\n   }\n-  for (const string& f : funcs_with_grads) {\n+  for (const std::string& f : funcs_with_grads) {\n     s = RemoveGradient(f);\n     if (!s.ok()) {\n       return s;\n@@ -1717,17 +1724,19 @@ absl::Status FunctionLibraryDefinition::Remove(\n   return absl::OkStatus();\n }\n \n-string FunctionLibraryDefinition::FindGradient(const string& func) const {\n+std::string FunctionLibraryDefinition::FindGradient(\n+    const std::string& func) const {\n   tf_shared_lock l(mu_);\n   return gtl::FindWithDefault(func_grad_, func, \"\");\n }\n \n-string FunctionLibraryDefinition::FindGradientHelper(const string& func) const {\n+std::string FunctionLibraryDefinition::FindGradientHelper(\n+    const std::string& func) const {\n   return gtl::FindWithDefault(func_grad_, func, \"\");\n }\n \n absl::Status FunctionLibraryDefinition::LookUp(\n-    const string& op, const OpRegistrationData** op_reg_data) const {\n+    const std::string& op, const OpRegistrationData** op_reg_data) const {\n   tf_shared_lock l(mu_);\n   auto iter = records_.find(op);\n   if (iter != records_.end()) {\n@@ -1737,11 +1746,11 @@ absl::Status FunctionLibraryDefinition::LookUp(\n   return default_registry_->LookUp(op, op_reg_data);\n }\n \n-string FunctionLibraryDefinition::UniqueFunctionName(\n+std::string FunctionLibraryDefinition::UniqueFunctionName(\n     absl::string_view prefix) const {\n   tf_shared_lock l(mu_);\n   int index = 0;\n-  string name = absl::StrCat(prefix, index);\n+  std::string name = absl::StrCat(prefix, index);\n   while (records_.find(name) != records_.end()) {\n     ++index;\n     name = absl::StrCat(prefix, index);\n@@ -1763,8 +1772,8 @@ const FunctionDef* FunctionLibraryDefinition::GetAttrImpl(\n   if (!TryGetNodeAttr(ndef, kFuncAttr, &forward_func_attrs)) {\n     return nullptr;\n   }\n-  const string& func_name = forward_func_attrs->name();\n-  const string& grad_name = FindGradient(func_name);\n+  const std::string& func_name = forward_func_attrs->name();\n+  const std::string& grad_name = FindGradient(func_name);\n   // If 'func' has a user-defined gradient function, uses the grad\n   // function's attrs to see if noinline is specified. Otherwise,\n   // uses func's attrs.\n@@ -1782,8 +1791,8 @@ const FunctionDef* FunctionLibraryDefinition::GetAttrImpl(\n   }\n }\n \n-std::vector<string> FunctionLibraryDefinition::ListFunctionNames() const {\n-  std::vector<string> function_names;\n+std::vector<std::string> FunctionLibraryDefinition::ListFunctionNames() const {\n+  std::vector<std::string> function_names;\n   tf_shared_lock l(mu_);\n   function_names.reserve(records_.size());\n   for (const auto& it : records_) {\n@@ -1808,7 +1817,7 @@ FunctionDefLibrary FunctionLibraryDefinition::ToProto() const {\n \n template <typename T>\n absl::Status FunctionLibraryDefinition::GetAttr(const NodeDef& ndef,\n-                                                const string& attr,\n+                                                const std::string& attr,\n                                                 T* value) const {\n   const FunctionDef* fdef = GetAttrImpl(ndef);\n   if (fdef && TryGetNodeAttr(AttrSlice(&fdef->attr()), attr, value)) {\n@@ -1819,7 +1828,7 @@ absl::Status FunctionLibraryDefinition::GetAttr(const NodeDef& ndef,\n \n template <typename T>\n absl::Status FunctionLibraryDefinition::GetAttr(const Node& node,\n-                                                const string& attr,\n+                                                const std::string& attr,\n                                                 T* value) const {\n   return GetAttr(node.def(), attr, value);\n }\n@@ -1839,25 +1848,25 @@ constexpr char kApiImplements[] = \"api_implements\";\n \n template <typename NodeType, typename NodeIter, typename OpTypeGetter,\n           typename AttrGetter>\n-std::set<string> ReachableFunctions(const FunctionLibraryDefinition& flib,\n-                                    NodeIter begin, NodeIter end,\n-                                    OpTypeGetter op_type_getter,\n-                                    AttrGetter attr_getter) {\n+std::set<std::string> ReachableFunctions(const FunctionLibraryDefinition& flib,\n+                                         NodeIter begin, NodeIter end,\n+                                         OpTypeGetter op_type_getter,\n+                                         AttrGetter attr_getter) {\n   // Functions that are reachable from the graph.\n-  std::set<string> reachable_funcs;\n+  std::set<std::string> reachable_funcs;\n \n   // For any functions, if it has attribute \"api_implements\" =\n   // \"some_interface\" and it is reachable, then it means any other\n   // function with same attribute name and value could also be potentially\n   // reachable, eg via implementation_selector swapping the nodedef.\n-  absl::flat_hash_set<string> reachable_api_interface;\n+  absl::flat_hash_set<std::string> reachable_api_interface;\n \n   // Functions might be reachable from the nested function calls, so we keep a\n   // queue of functions that we have to check.\n   absl::InlinedVector<core::RefCountPtr<FunctionRecord>, 4> func_queue;\n \n   // Add reachable and not already processed functions to the functions queue.\n-  const auto add_to_func_queue = [&](const string& func_name) {\n+  const auto add_to_func_queue = [&](const std::string& func_name) {\n     auto record = flib.FindRecord(func_name);\n     if (record && reachable_funcs.find(func_name) == reachable_funcs.end()) {\n       func_queue.push_back(std::move(record));\n@@ -1866,19 +1875,20 @@ std::set<string> ReachableFunctions(const FunctionLibraryDefinition& flib,\n \n   // If any function with certain API name is reachable, all the other functions\n   // with same API name should also be checked.\n-  const auto add_function_with_api_interface = [&](const string& api_name) {\n-    if (!reachable_api_interface.contains(api_name)) {\n-      reachable_api_interface.insert(api_name);\n-      for (const auto& func_name : flib.ListFunctionNames()) {\n-        const auto record = flib.FindRecord(func_name);\n-        const auto attr_it = record->fdef().attr().find(kApiImplements);\n-        if (attr_it != record->fdef().attr().end() &&\n-            attr_it->second.s() == api_name) {\n-          add_to_func_queue(func_name);\n+  const auto add_function_with_api_interface =\n+      [&](const std::string& api_name) {\n+        if (!reachable_api_interface.contains(api_name)) {\n+          reachable_api_interface.insert(api_name);\n+          for (const auto& func_name : flib.ListFunctionNames()) {\n+            const auto record = flib.FindRecord(func_name);\n+            const auto attr_it = record->fdef().attr().find(kApiImplements);\n+            if (attr_it != record->fdef().attr().end() &&\n+                attr_it->second.s() == api_name) {\n+              add_to_func_queue(func_name);\n+            }\n+          }\n         }\n-      }\n-    }\n-  };\n+      };\n \n   const auto process_attr_value = [&](const AttrValue& attr_value) {\n     // 1. AttrValue.func\n@@ -1913,7 +1923,7 @@ std::set<string> ReachableFunctions(const FunctionLibraryDefinition& flib,\n     auto func = std::move(func_queue.back());\n     func_queue.pop_back();\n \n-    const string& func_name = func->fdef().signature().name();\n+    const std::string& func_name = func->fdef().signature().name();\n     reachable_funcs.insert(func_name);\n \n     const auto attr_it = func->fdef().attr().find(kApiImplements);\n@@ -1937,7 +1947,7 @@ std::set<string> ReachableFunctions(const FunctionLibraryDefinition& flib,\n     std::for_each(func_body.begin(), func_body.end(), process_node_def);\n \n     // Check if the function has a registered gradient.\n-    const string grad_func_name = flib.FindGradient(func_name);\n+    const std::string grad_func_name = flib.FindGradient(func_name);\n     if (!grad_func_name.empty()) add_to_func_queue(grad_func_name);\n   }\n \n@@ -1949,19 +1959,19 @@ template <typename NodeType, typename NodeIter, typename OpTypeGetter,\n FunctionLibraryDefinition ReachableFunctionLibraryDefinition(\n     const FunctionLibraryDefinition& flib, NodeIter begin, NodeIter end,\n     OpTypeGetter op_type_getter, AttrGetter attr_getter) {\n-  std::set<string> reachable_funcs = ReachableFunctions<NodeType>(\n+  std::set<std::string> reachable_funcs = ReachableFunctions<NodeType>(\n       flib, begin, end, op_type_getter, attr_getter);\n \n   FunctionLibraryDefinition reachable_flib(flib.default_registry(),\n                                            FunctionDefLibrary());\n \n-  for (const string& func_name : reachable_funcs) {\n+  for (const std::string& func_name : reachable_funcs) {\n     // This should never fail, because we copy functions from a valid flib and\n     // use the same default registry.\n     absl::Status added = reachable_flib.CopyFunctionDefFrom(func_name, flib);\n     TF_DCHECK_OK(added);\n \n-    const string grad_func_name = flib.FindGradient(func_name);\n+    const std::string grad_func_name = flib.FindGradient(func_name);\n     if (!grad_func_name.empty()) {\n       GradientDef grad;\n       grad.set_function_name(func_name);\n@@ -1975,9 +1985,9 @@ FunctionLibraryDefinition ReachableFunctionLibraryDefinition(\n   return reachable_flib;\n }\n \n-string AllocatorAttributesToString(\n+std::string AllocatorAttributesToString(\n     const std::vector<AllocatorAttributes>& attrs) {\n-  string result(\"[\");\n+  std::string result(\"[\");\n   // AllocatorAttribute::DebugString produces around 85 bytes now.\n   result.reserve(100 * attrs.size());\n   for (const AllocatorAttributes& attr : attrs) {\n@@ -2036,7 +2046,7 @@ FunctionLibraryDefinition::ReachableDefinitions(\n   }\n }\n \n-string FunctionLibraryRuntime::Options::DebugString() const {\n+std::string FunctionLibraryRuntime::Options::DebugString() const {\n   return absl::StrCat(\n       \"FLR::Options(step_id=\", step_id, \" rendezvous=\", IsSet(rendezvous),\n       \" cancellation_manager=\", IsSet(cancellation_manager),\n@@ -2060,8 +2070,8 @@ void FunctionDefHelper::AttrValueWrapper::InitFromString(\n }\n \n FunctionDefHelper::AttrValueWrapper FunctionDefHelper::FunctionRef(\n-    const string& name,\n-    absl::Span<const std::pair<string, AttrValueWrapper>> attrs) {\n+    const std::string& name,\n+    absl::Span<const std::pair<std::string, AttrValueWrapper>> attrs) {\n   AttrValueWrapper ret;\n   ret.proto.mutable_func()->set_name(name);\n   for (const auto& a : attrs) {\n@@ -2077,10 +2087,10 @@ NodeDef FunctionDefHelper::Node::ToNodeDef() const {\n   for (const auto& a : this->attr) {\n     n.mutable_attr()->insert({a.first, a.second.proto});\n   }\n-  for (const string& a : this->arg) {\n+  for (const std::string& a : this->arg) {\n     n.add_input(a);\n   }\n-  for (const string& d : this->dep) {\n+  for (const std::string& d : this->dep) {\n     n.add_input(absl::StrCat(\"^\", d));\n   }\n   if (!this->device.empty()) {\n@@ -2099,11 +2109,11 @@ NodeDef FunctionDefHelper::Node::ToNodeDef() const {\n \n /* static */\n FunctionDef FunctionDefHelper::Create(\n-    const string& function_name, absl::Span<const string> in_def,\n-    absl::Span<const string> out_def, absl::Span<const string> attr_def,\n-    absl::Span<const Node> node_def,\n-    absl::Span<const std::pair<string, string>> ret_def,\n-    absl::Span<const std::pair<string, string>> control_ret_def) {\n+    const std::string& function_name, absl::Span<const std::string> in_def,\n+    absl::Span<const std::string> out_def,\n+    absl::Span<const std::string> attr_def, absl::Span<const Node> node_def,\n+    absl::Span<const std::pair<std::string, std::string>> ret_def,\n+    absl::Span<const std::pair<std::string, std::string>> control_ret_def) {\n   FunctionDef fdef;\n \n   // Signature\n@@ -2149,19 +2159,19 @@ FunctionDef FunctionDefHelper::Create(\n \n /* static */\n FunctionDef FunctionDefHelper::Create(\n-    const string& function_name, absl::Span<const string> in_def,\n-    absl::Span<const string> out_def, absl::Span<const string> attr_def,\n-    absl::Span<const Node> node_def,\n-    absl::Span<const std::pair<string, string>> ret_def) {\n+    const std::string& function_name, absl::Span<const std::string> in_def,\n+    absl::Span<const std::string> out_def,\n+    absl::Span<const std::string> attr_def, absl::Span<const Node> node_def,\n+    absl::Span<const std::pair<std::string, std::string>> ret_def) {\n   return Create(function_name, in_def, out_def, attr_def, node_def, ret_def,\n                 /*control_ret_def=*/{});\n }\n \n /* static */\n-FunctionDef FunctionDefHelper::Define(const string& name,\n-                                      absl::Span<const string> arg_def,\n-                                      absl::Span<const string> ret_def,\n-                                      absl::Span<const string> attr_def,\n+FunctionDef FunctionDefHelper::Define(const std::string& name,\n+                                      absl::Span<const std::string> arg_def,\n+                                      absl::Span<const std::string> ret_def,\n+                                      absl::Span<const std::string> attr_def,\n                                       absl::Span<const Node> node_def) {\n   FunctionDef fdef;\n   OpDefBuilder b(name);\n@@ -2174,7 +2184,7 @@ FunctionDef FunctionDefHelper::Define(const string& name,\n   fdef.mutable_signature()->Swap(&op_reg_data.op_def);\n \n   // Mapping from legacy output names to NodeDef outputs.\n-  std::unordered_map<string, string> ret_index;\n+  std::unordered_map<std::string, std::string> ret_index;\n   for (const auto& a : fdef.signature().input_arg()) {\n     ret_index[a.name()] = a.name();\n   }\n@@ -2190,13 +2200,13 @@ FunctionDef FunctionDefHelper::Define(const string& name,\n     for (const auto& a : src.attr) {\n       n->mutable_attr()->insert({a.first, a.second.proto});\n     }\n-    for (const string& a : src.arg) {\n+    for (const std::string& a : src.arg) {\n       const auto iter = ret_index.find(a);\n       CHECK(iter != ret_index.end())\n           << \"Node input '\" << a << \"' in '\" << n->name() << \"' of \" << name;\n       n->add_input(iter->second);\n     }\n-    for (const string& d : src.dep) {\n+    for (const std::string& d : src.dep) {\n       n->add_input(absl::StrCat(\"^\", d));\n     }\n \n@@ -2227,29 +2237,29 @@ FunctionDef FunctionDefHelper::Define(const string& name,\n   return fdef;\n }\n \n-FunctionDef FunctionDefHelper::Define(absl::Span<const string> arg_def,\n-                                      absl::Span<const string> ret_def,\n-                                      absl::Span<const string> attr_def,\n+FunctionDef FunctionDefHelper::Define(absl::Span<const std::string> arg_def,\n+                                      absl::Span<const std::string> ret_def,\n+                                      absl::Span<const std::string> attr_def,\n                                       absl::Span<const Node> node_def) {\n   return Define(\"_\", arg_def, ret_def, attr_def, node_def);\n }\n \n namespace gradient {\n \n-typedef std::unordered_map<string, Creator> OpGradFactory;\n+typedef std::unordered_map<std::string, Creator> OpGradFactory;\n \n OpGradFactory* GetOpGradFactory() {\n   static OpGradFactory* factory = new OpGradFactory;\n   return factory;\n }\n \n-bool RegisterOp(const string& op, Creator func) {\n+bool RegisterOp(const std::string& op, Creator func) {\n   CHECK(GetOpGradFactory()->insert({op, func}).second)\n       << \"Duplicated gradient for \" << op;\n   return true;\n }\n \n-absl::Status GetOpGradientCreator(const string& op, Creator* creator) {\n+absl::Status GetOpGradientCreator(const std::string& op, Creator* creator) {\n   auto fac = GetOpGradFactory();\n   auto iter = fac->find(op);\n   if (iter == fac->end()) {"
        },
        {
            "sha": "ed2ec8c075db0893d7d967a7235f17249cfea2c9",
            "filename": "tensorflow/core/framework/function.h",
            "status": "modified",
            "additions": 43,
            "deletions": 40,
            "changes": 83,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffunction.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffunction.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ffunction.h?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -125,7 +125,7 @@ class FunctionDefHelper {\n   // Constructs an AttrValue.func given the \"name\" and \"attrs\".\n   static AttrValueWrapper FunctionRef(\n       const std::string& name,\n-      absl::Span<const std::pair<string, AttrValueWrapper>> attrs);\n+      absl::Span<const std::pair<std::string, AttrValueWrapper>> attrs);\n   static AttrValueWrapper FunctionRef(const std::string& name) {\n     return FunctionRef(name, {});\n   }\n@@ -141,11 +141,11 @@ class FunctionDefHelper {\n   struct Node {\n     // When constructing a NodeDef, the first entry in ret is used as\n     // the node name, the remaining values are ignored.\n-    std::vector<string> ret;\n+    std::vector<std::string> ret;\n     std::string op;\n-    std::vector<string> arg;\n-    std::vector<std::pair<string, AttrValueWrapper>> attr;\n-    std::vector<string> dep;\n+    std::vector<std::string> arg;\n+    std::vector<std::pair<std::string, AttrValueWrapper>> attr;\n+    std::vector<std::string> dep;\n     std::string device;\n \n     // Required if the op has zero outputs. Otherwise, ret[0] used as name if\n@@ -157,8 +157,8 @@ class FunctionDefHelper {\n       CHECK(!ret.empty());\n       return ret[0];\n     }\n-    std::vector<string> original_node_names;\n-    std::vector<string> original_func_names;\n+    std::vector<std::string> original_node_names;\n+    std::vector<std::string> original_func_names;\n \n     NodeDef ToNodeDef() const;\n   };\n@@ -170,33 +170,33 @@ class FunctionDefHelper {\n   // - `control_ret_def` holds a mapping from the function control\n   //   output names to the nodes from `node_def`.\n   static FunctionDef Create(\n-      const std::string& function_name, absl::Span<const string> in_def,\n-      absl::Span<const string> out_def, absl::Span<const string> attr_def,\n-      absl::Span<const Node> node_def,\n-      absl::Span<const std::pair<string, string>> ret_def,\n-      absl::Span<const std::pair<string, string>> control_ret_def);\n+      const std::string& function_name, absl::Span<const std::string> in_def,\n+      absl::Span<const std::string> out_def,\n+      absl::Span<const std::string> attr_def, absl::Span<const Node> node_def,\n+      absl::Span<const std::pair<std::string, std::string>> ret_def,\n+      absl::Span<const std::pair<std::string, std::string>> control_ret_def);\n \n   // Creates a FunctionDef from the given parameters. Node inputs must use\n   // function encoding (node_name:output_name[:output_index]).\n   // - `ret_def` holds a mapping from the function output names from `out_def`\n   //   to the node outputs from `node_def`.\n   static FunctionDef Create(\n-      const std::string& function_name, absl::Span<const string> in_def,\n-      absl::Span<const string> out_def, absl::Span<const string> attr_def,\n-      absl::Span<const Node> node_def,\n-      absl::Span<const std::pair<string, string>> ret_def);\n+      const std::string& function_name, absl::Span<const std::string> in_def,\n+      absl::Span<const std::string> out_def,\n+      absl::Span<const std::string> attr_def, absl::Span<const Node> node_def,\n+      absl::Span<const std::pair<std::string, std::string>> ret_def);\n \n   // TODO(josh11b): Get rid of these and transition to the one above.\n   static FunctionDef Define(const std::string& function_name,\n-                            absl::Span<const string> arg_def,\n-                            absl::Span<const string> ret_def,\n-                            absl::Span<const string> attr_def,\n+                            absl::Span<const std::string> arg_def,\n+                            absl::Span<const std::string> ret_def,\n+                            absl::Span<const std::string> attr_def,\n                             absl::Span<const Node> node_def);\n \n   // Defines an anonymous function. I.e., its name is not relevant.\n-  static FunctionDef Define(absl::Span<const string> arg_def,\n-                            absl::Span<const string> ret_def,\n-                            absl::Span<const string> attr_def,\n+  static FunctionDef Define(absl::Span<const std::string> arg_def,\n+                            absl::Span<const std::string> ret_def,\n+                            absl::Span<const std::string> attr_def,\n                             absl::Span<const Node> node_def);\n \n   // Helpers to construct a constant scalar.\n@@ -258,7 +258,7 @@ inline FunctionDefHelper::AttrValueWrapper::AttrValueWrapper(\n // GetFunctionSignature(func name, opdef) returns OK if the func name is found\n // and opdef is filled with a pointer to the corresponding signature\n // (a OpDef proto). Otherwise, returns an error.\n-typedef std::function<absl::Status(const string&, const OpDef**)>\n+typedef std::function<absl::Status(const std::string&, const OpDef**)>\n     GetFunctionSignature;\n \n struct InstantiationResult {\n@@ -293,7 +293,7 @@ bool FunctionDefsEqual(const FunctionDef& f1, const FunctionDef& f2);\n // Return a hash of `fdef` that is consistent with FunctionDefsEqual method.\n // In other words, if two fdefs compare equal, their hash values will be the\n // same.\n-uint64 FunctionDefHash(const FunctionDef& fdef);\n+uint64_t FunctionDefHash(const FunctionDef& fdef);\n \n class CallFrameInterface {\n  public:\n@@ -566,7 +566,7 @@ class FunctionLibraryDefinition : public OpRegistryInterface {\n   }\n \n   // Returns all the function names in the FunctionLibraryDefinition.\n-  std::vector<string> ListFunctionNames() const TF_LOCKS_EXCLUDED(mu_);\n+  std::vector<std::string> ListFunctionNames() const TF_LOCKS_EXCLUDED(mu_);\n \n   const OpRegistryInterface* default_registry() const {\n     return default_registry_;\n@@ -658,7 +658,7 @@ class FunctionLibraryDefinition : public OpRegistryInterface {\n   void Initialize(const FunctionDefLibrary& library,\n                   const FunctionDefLibraryStackTraces& library_traces);\n \n-  core::RefCountPtr<FunctionRecord> FindHelper(const string& func) const\n+  core::RefCountPtr<FunctionRecord> FindHelper(const std::string& func) const\n       TF_SHARED_LOCKS_REQUIRED(mu_);\n   std::string FindGradientHelper(const std::string& func) const\n       TF_SHARED_LOCKS_REQUIRED(mu_);\n@@ -681,8 +681,8 @@ class FunctionLibraryDefinition : public OpRegistryInterface {\n \n   // Remove all functions in `funcs` and all gradients of functions in\n   // `funcs_with_grads` from this library.\n-  absl::Status Remove(const std::vector<string>& funcs,\n-                      const std::vector<string>& funcs_with_grads)\n+  absl::Status Remove(const std::vector<std::string>& funcs,\n+                      const std::vector<std::string>& funcs_with_grads)\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n \n   // Remove `func` from the library. Returns non-OK Status unless `func` is in\n@@ -698,10 +698,11 @@ class FunctionLibraryDefinition : public OpRegistryInterface {\n \n   mutable mutex mu_;\n   const OpRegistryInterface* default_registry_;\n-  gtl::FlatMap<string, FunctionRecord*> records_ TF_GUARDED_BY(mu_);\n-  gtl::FlatMap<string, string> func_grad_ TF_GUARDED_BY(mu_);\n+  gtl::FlatMap<std::string, FunctionRecord*> records_ TF_GUARDED_BY(mu_);\n+  gtl::FlatMap<std::string, std::string> func_grad_ TF_GUARDED_BY(mu_);\n   // Maps from function name to optimized function graph.\n-  gtl::FlatMap<string, std::function<absl::StatusOr<OptimizedFunctionGraph>()>>\n+  gtl::FlatMap<std::string,\n+               std::function<absl::StatusOr<OptimizedFunctionGraph>()>>\n       optimized_function_graph_creator_map_ TF_GUARDED_BY(mu_);\n };\n \n@@ -752,7 +753,7 @@ class FunctionLibraryRuntime : public core::WeakRefCounted {\n     // function's inputs. The device of resource inputs must be the device\n     // backing the resource, not the CPU device backing the resource handle.\n     // Must have the same length as number of inputs to the function.\n-    std::vector<string> input_devices;\n+    std::vector<std::string> input_devices;\n \n     // For multi-device functions, a vector of canonical device names for\n     // function's outputs.\n@@ -780,14 +781,15 @@ class FunctionLibraryRuntime : public core::WeakRefCounted {\n     // resource output, and node producing that resource is a function call,\n     // runtime will leave device specification empty and will rely on Placer to\n     // infer correct device.\n-    std::vector<string> output_devices;\n+    std::vector<std::string> output_devices;\n \n     // If set, it indicates the original output indices of a component function.\n     absl::optional<std::vector<int>> ret_indices = absl::nullopt;\n \n     // Maps from a CompositeDevice name to a list of underlying physical\n     // devices.\n-    absl::flat_hash_map<string, const std::vector<string>*> composite_devices;\n+    absl::flat_hash_map<std::string, const std::vector<std::string>*>\n+        composite_devices;\n \n     // This interface is EXPERIMENTAL and subject to change.\n     //\n@@ -836,8 +838,8 @@ class FunctionLibraryRuntime : public core::WeakRefCounted {\n \n     // If provided, this optimization function will be invoked before\n     // the placer for multi-device functions.\n-    std::function<absl::Status(std::vector<string> /*ret_node_names*/,\n-                               std::vector<string> /*keep_node_names*/,\n+    std::function<absl::Status(std::vector<std::string> /*ret_node_names*/,\n+                               std::vector<std::string> /*keep_node_names*/,\n                                FunctionLibraryDefinition*, const DeviceSet&,\n                                Device* /*cpu_device*/, std::unique_ptr<Graph>*)>\n         optimize_graph_fn;\n@@ -899,7 +901,7 @@ class FunctionLibraryRuntime : public core::WeakRefCounted {\n     // Instantiates the function enabling soft placement or outside compilation.\n     bool allow_soft_placement = false;\n   };\n-  typedef uint64 Handle;\n+  typedef uint64_t Handle;\n   virtual absl::Status Instantiate(const std::string& function_name,\n                                    AttrSlice attrs,\n                                    const InstantiateOptions& options,\n@@ -1055,7 +1057,7 @@ class FunctionLibraryRuntime : public core::WeakRefCounted {\n   // Returns the graph version number.\n   virtual int graph_def_version() const = 0;\n \n-  typedef uint64 LocalHandle;\n+  typedef uint64_t LocalHandle;\n \n   // Creates a copy of ProcessFunctionLibraryRuntime (transferring ownership to\n   // the caller), FunctionLibraryRuntime (owned by the returned\n@@ -1088,7 +1090,8 @@ class FunctionLibraryRuntime : public core::WeakRefCounted {\n // `composite_devices` if the input device is a composite device.\n std::string GetFunctionResourceInputDevice(\n     const Tensor& input, const int arg_index, const FunctionDef& function_def,\n-    absl::flat_hash_map<string, std::vector<string>>* composite_devices);\n+    absl::flat_hash_map<std::string, std::vector<std::string>>*\n+        composite_devices);\n \n // Returns a canonicalized string for the instantiation of the function of the\n // given \"name\", attributes \"attrs\", and \"options\".\n@@ -1173,7 +1176,7 @@ class DistributedFunctionLibraryRuntime {\n                    FunctionLibraryRuntime::DoneCallback done) = 0;\n \n   // Clean up a previously instantiated function on remote worker.\n-  virtual void CleanUp(uint64 step_id,\n+  virtual void CleanUp(uint64_t step_id,\n                        FunctionLibraryRuntime::LocalHandle handle,\n                        FunctionLibraryRuntime::DoneCallback done) = 0;\n "
        },
        {
            "sha": "d0d995cbcc3712b47309b7b552bbbc854b7c059a",
            "filename": "tensorflow/core/framework/function_handle_cache.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffunction_handle_cache.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffunction_handle_cache.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ffunction_handle_cache.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -33,10 +33,10 @@ FunctionHandleCache::~FunctionHandleCache() {\n }\n \n absl::Status FunctionHandleCache::Instantiate(\n-    const string& function_name, AttrSlice attrs,\n+    const std::string& function_name, AttrSlice attrs,\n     FunctionLibraryRuntime::InstantiateOptions options,\n     FunctionLibraryRuntime::Handle* handle) {\n-  string key = Canonicalize(function_name, attrs, options);\n+  std::string key = Canonicalize(function_name, attrs, options);\n   FunctionLibraryRuntime::Handle h;\n   {\n     tf_shared_lock l(mu_);"
        },
        {
            "sha": "317c53823c1685727739d533e8853e0b7896d598",
            "filename": "tensorflow/core/framework/function_handle_cache.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffunction_handle_cache.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffunction_handle_cache.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ffunction_handle_cache.h?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -34,7 +34,7 @@ class FunctionHandleCache {\n   //\n   // The cache retains the ownership of the handle. In particular, the caller\n   // should not invoke `ReleaseHandle`.\n-  absl::Status Instantiate(const string& function_name, AttrSlice attrs,\n+  absl::Status Instantiate(const std::string& function_name, AttrSlice attrs,\n                            FunctionLibraryRuntime::InstantiateOptions options,\n                            FunctionLibraryRuntime::Handle* handle);\n \n@@ -45,8 +45,8 @@ class FunctionHandleCache {\n  private:\n   mutex mu_;\n   FunctionLibraryRuntime* lib_ = nullptr;  // not owned\n-  const string state_handle_;\n-  std::unordered_map<string, FunctionLibraryRuntime::Handle> handles_\n+  const std::string state_handle_;\n+  std::unordered_map<std::string, FunctionLibraryRuntime::Handle> handles_\n       TF_GUARDED_BY(mu_);\n };\n "
        },
        {
            "sha": "fcae39d0277bab52fe10b2ef145c4840dc6e2206",
            "filename": "tensorflow/core/framework/function_test.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 23,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffunction_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffunction_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ffunction_test.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -54,7 +54,7 @@ using ::testing::UnorderedElementsAreArray;\n class Attrs {\n  public:\n   Attrs(const std::initializer_list<  // NOLINT(runtime/explicit)\n-        std::pair<string, FunctionDefHelper::AttrValueWrapper>>\n+        std::pair<std::string, FunctionDefHelper::AttrValueWrapper>>\n             attrs) {\n     for (const auto& aval : attrs) {\n       map_.insert({aval.first, aval.second.proto});\n@@ -69,7 +69,7 @@ class Attrs {\n \n typedef FunctionDefHelper FDH;\n \n-absl::Status GetOpSig(const string& op, const OpDef** sig) {\n+absl::Status GetOpSig(const std::string& op, const OpDef** sig) {\n   return OpRegistry::Global()->LookUpOpDef(op, sig);\n }\n \n@@ -490,7 +490,7 @@ WXPlusB[T:{float, double}](w:T, x:T, b:T) -> (y:T) {\n }\n \n TEST(TFunc, Body_TypeList) {\n-  const Tensor kZero = test::AsScalar<int32>(0);\n+  const Tensor kZero = test::AsScalar<int32_t>(0);\n   auto fdef = FDH::Create(\n       // Name\n       \"Test\",\n@@ -633,7 +633,7 @@ TEST(TFunc, IntsOnDeviceArgSet) {\n   EXPECT_EQ(\"_DeviceRetval\", result.nodes[4].op());\n }\n \n-static void HasError(const absl::Status& s, const string& substr) {\n+static void HasError(const absl::Status& s, const std::string& substr) {\n   EXPECT_TRUE(absl::StrContains(s.ToString(), substr))\n       << \">>\" << s << \"<<, expected substring >>\" << substr << \"<<\";\n }\n@@ -1229,7 +1229,7 @@ TEST(FunctionLibraryDefinitionTest, AddLibrary) {\n   TF_EXPECT_OK(lib_def.AddLibrary(lib_def));\n }\n \n-GradientDef MakeGradDef(const string& f, const string& g) {\n+GradientDef MakeGradDef(const std::string& f, const std::string& g) {\n   GradientDef grad;\n   grad.set_function_name(f);\n   grad.set_gradient_func(g);\n@@ -1239,8 +1239,8 @@ GradientDef MakeGradDef(const string& f, const string& g) {\n TEST(FunctionLibraryDefinitionTest, AddLibrary_Atomic) {\n   // Create lib def containing two functions with equal names\n   FunctionDefLibrary proto;\n-  const string x2_name = test::function::XTimesTwo().signature().name();\n-  const string x4_name = test::function::XTimesFour().signature().name();\n+  const std::string x2_name = test::function::XTimesTwo().signature().name();\n+  const std::string x4_name = test::function::XTimesFour().signature().name();\n   *proto.add_function() = test::function::XTimesTwo();\n   FunctionDef fdef = test::function::XTimesFour();\n   fdef.mutable_signature()->set_name(x2_name);\n@@ -1275,9 +1275,9 @@ TEST(FunctionLibraryDefinitionTest, AddLibrary_Atomic) {\n }\n \n TEST(FunctionLibraryDefinitionTest, AddLibraryDefinition_Atomic_FuncConflict) {\n-  const string x2_name = test::function::XTimesTwo().signature().name();\n-  const string x4_name = test::function::XTimesFour().signature().name();\n-  const string wx_name = test::function::WXPlusB().signature().name();\n+  const std::string x2_name = test::function::XTimesTwo().signature().name();\n+  const std::string x4_name = test::function::XTimesFour().signature().name();\n+  const std::string wx_name = test::function::WXPlusB().signature().name();\n \n   // Create FunctionLibraryDefinition with\n   // (func = XTimesTwo, grad = XTimesFour)\n@@ -1311,9 +1311,9 @@ TEST(FunctionLibraryDefinitionTest, AddLibraryDefinition_Atomic_FuncConflict) {\n }\n \n TEST(FunctionLibraryDefinitionTest, AddLibraryDefinition_Atomic_GradConflict) {\n-  const string x2_name = test::function::XTimesTwo().signature().name();\n-  const string x4_name = test::function::XTimesFour().signature().name();\n-  const string wx_name = test::function::WXPlusB().signature().name();\n+  const std::string x2_name = test::function::XTimesTwo().signature().name();\n+  const std::string x4_name = test::function::XTimesFour().signature().name();\n+  const std::string wx_name = test::function::WXPlusB().signature().name();\n \n   // Create FunctionLibraryDefinition with\n   // (func = XTimesTwo, grad = XTimesFour)\n@@ -1372,8 +1372,8 @@ TEST(FunctionLibraryDefinitionTest, ListFunctionNames) {\n   TF_CHECK_OK(lib_def.AddFunctionDef(test::function::XTimesTwo()));\n   TF_CHECK_OK(lib_def.AddFunctionDef(test::function::WXPlusB()));\n \n-  const std::vector<string> function_names = lib_def.ListFunctionNames();\n-  const std::vector<string> expected = {\"XTimesTwo\", \"WXPlusB\"};\n+  const std::vector<std::string> function_names = lib_def.ListFunctionNames();\n+  const std::vector<std::string> expected = {\"XTimesTwo\", \"WXPlusB\"};\n   EXPECT_EQ(function_names, expected);\n }\n \n@@ -1399,7 +1399,7 @@ TEST(FunctionLibraryDefinitionTest, GetAttr_FuncNoAttr) {\n }\n \n template <typename T>\n-void SetAttrValue(FunctionDef* fdef, const string& attr, const T& value) {\n+void SetAttrValue(FunctionDef* fdef, const std::string& attr, const T& value) {\n   AttrValue attr_value;\n   SetAttrValue(value, &attr_value);\n   fdef->mutable_attr()->insert({attr, attr_value});\n@@ -1421,7 +1421,7 @@ TEST(FunctionLibraryDefinitionTest, GetAttr_FuncWithAttr) {\n   TF_EXPECT_OK(lib.GetAttr(ndef, \"annotation\", &annotation));\n   EXPECT_EQ(annotation, true);\n \n-  string str;\n+  std::string str;\n   TF_EXPECT_OK(lib.GetAttr(ndef, \"options\", &str));\n   EXPECT_EQ(str, \"some string data\");\n }\n@@ -1462,8 +1462,8 @@ TEST(FunctionLibraryDefinitionTest, ReachableDefinitions) {\n   using ::tensorflow::test::function::NDef;\n   using FDH = ::tensorflow::FunctionDefHelper;\n \n-  const auto make_simple_fdef = [](const string& name,\n-                                   const string& interface_name) {\n+  const auto make_simple_fdef = [](const std::string& name,\n+                                   const std::string& interface_name) {\n     auto func_def = FDH::Create(\n         name, {\"x:T\", \"y:T\"}, {\"z:T\"}, {\"T: {float, double}\"},\n         {{{\"output\"}, \"Mul\", {\"x\", \"y\"}, {{\"T\", \"$T\"}}}},\n@@ -1616,7 +1616,7 @@ TEST(FunctionDefsEqualTest, TestFunctionDefsEqual) {\n   // Equal functions\n   const FunctionDef fdef1 = test::function::XTimesTwo();\n   FunctionDef fdef2 = test::function::XTimesTwo();\n-  uint64 hash1 = FunctionDefHash(fdef1);\n+  uint64_t hash1 = FunctionDefHash(fdef1);\n   EXPECT_TRUE(FunctionDefsEqual(fdef1, fdef2));\n   EXPECT_EQ(hash1, FunctionDefHash(fdef2));\n \n@@ -1760,7 +1760,7 @@ TEST(InstantiateFunctionTest, ResourceInputDevice) {\n   *(*arg_attrs.mutable_attr())[\"_composite_device\"].mutable_s() =\n       \"/device:COMPOSITE:0\";\n   (*fdef.mutable_arg_attr())[0] = arg_attrs;\n-  absl::flat_hash_map<string, std::vector<string>> composite_devices;\n+  absl::flat_hash_map<std::string, std::vector<std::string>> composite_devices;\n \n   Tensor arg0(DT_RESOURCE, TensorShape({2}));\n   ResourceHandle resource_handle0;\n@@ -1773,9 +1773,9 @@ TEST(InstantiateFunctionTest, ResourceInputDevice) {\n   Tensor arg1(DT_RESOURCE, TensorShape({}));\n   arg1.scalar<ResourceHandle>()() = resource_handle0;\n \n-  const string device0 = GetFunctionResourceInputDevice(\n+  const std::string device0 = GetFunctionResourceInputDevice(\n       arg0, /*arg_index=*/0, fdef, &composite_devices);\n-  const string device1 = GetFunctionResourceInputDevice(\n+  const std::string device1 = GetFunctionResourceInputDevice(\n       arg1, /*arg_index=*/1, fdef, &composite_devices);\n \n   EXPECT_EQ(device0, \"/device:COMPOSITE:0\");"
        },
        {
            "sha": "1b968b939365a7f01008c1a5c25f22fc152efddb",
            "filename": "tensorflow/core/framework/function_testlib.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffunction_testlib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffunction_testlib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ffunction_testlib.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -48,13 +48,14 @@ GraphDef GDef(absl::Span<const NodeDef> nodes,\n }\n \n // Helper to construct a NodeDef.\n-NodeDef NDef(absl::string_view name, absl::string_view op,\n-             absl::Span<const string> inputs,\n-             absl::Span<const std::pair<string, FDH::AttrValueWrapper>> attrs,\n-             const string& device) {\n+NodeDef NDef(\n+    absl::string_view name, absl::string_view op,\n+    absl::Span<const std::string> inputs,\n+    absl::Span<const std::pair<std::string, FDH::AttrValueWrapper>> attrs,\n+    const std::string& device) {\n   NodeDef n;\n-  n.set_name(string(name));\n-  n.set_op(string(op));\n+  n.set_name(name);\n+  n.set_op(op);\n   for (const auto& in : inputs) n.add_input(in);\n   n.set_device(device);\n   for (const auto& na : attrs)\n@@ -609,8 +610,8 @@ FunctionDef XYXLessThanOrEqualToN(int64_t N) {\n }\n \n FunctionDef RandomUniformLess() {\n-  const Tensor kZero = test::AsScalar<int32>(0);\n-  const Tensor kOne = test::AsScalar<int32>(1);\n+  const Tensor kZero = test::AsScalar<int32_t>(0);\n+  const Tensor kOne = test::AsScalar<int32_t>(1);\n   const Tensor k005 = test::AsScalar<float>(0.05);\n \n   return FDH::Define(\n@@ -703,8 +704,8 @@ FunctionDef MakeBatchDataset() {\n }\n \n FunctionDef MakeMapDataset(bool has_other_args) {\n-  std::vector<string> args = {\"input_dataset: variant\"};\n-  std::vector<string> inputs = {\"input_dataset\"};\n+  std::vector<std::string> args = {\"input_dataset: variant\"};\n+  std::vector<std::string> inputs = {\"input_dataset\"};\n   if (has_other_args) {\n     args.emplace_back(\"other_arguments: Targuments\");\n     inputs.emplace_back(\"other_arguments\");"
        },
        {
            "sha": "b4cbf057cbe0a831d0dbce48ac19ab62d753b391",
            "filename": "tensorflow/core/framework/function_testlib.h",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffunction_testlib.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Ffunction_testlib.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ffunction_testlib.h?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -34,15 +34,14 @@ namespace function {\n class Attrs {\n  public:\n   Attrs(const std::initializer_list<  // NOLINT(runtime/explicit)\n-        std::pair<string, FunctionDefHelper::AttrValueWrapper>>& attrs) {\n+        std::pair<std::string, FunctionDefHelper::AttrValueWrapper>>& attrs) {\n     for (const auto& aval : attrs) {\n       map_.insert({aval.first, aval.second.proto});\n     }\n   }\n \n-  Attrs(\n-      const std::vector<std::pair<string, FunctionDefHelper::AttrValueWrapper>>&\n-          attrs) {\n+  Attrs(const std::vector<\n+        std::pair<std::string, FunctionDefHelper::AttrValueWrapper>>& attrs) {\n     for (const auto& aval : attrs) {\n       map_.insert({aval.first, aval.second.proto});\n     }\n@@ -55,12 +54,12 @@ class Attrs {\n };\n \n // Helper to construct a NodeDef.\n-NodeDef NDef(\n-    absl::string_view name, absl::string_view op,\n-    absl::Span<const string> inputs,\n-    absl::Span<const std::pair<string, FunctionDefHelper::AttrValueWrapper>>\n-        attrs = {},\n-    const string& device = \"\");\n+NodeDef NDef(absl::string_view name, absl::string_view op,\n+             absl::Span<const std::string> inputs,\n+             absl::Span<const std::pair<std::string,\n+                                        FunctionDefHelper::AttrValueWrapper>>\n+                 attrs = {},\n+             const std::string& device = \"\");\n \n // Helper to construct a GraphDef proto.\n GraphDef GDef(absl::Span<const NodeDef> nodes,"
        },
        {
            "sha": "9f54e3eecfdccd6321746ce42c3065fe8f2850e4",
            "filename": "tensorflow/core/framework/graph_def_util.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fgraph_def_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fgraph_def_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fgraph_def_util.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -35,8 +35,8 @@ limitations under the License.\n \n namespace tensorflow {\n \n-string SummarizeGraphDef(const GraphDef& graph_def) {\n-  string ret;\n+std::string SummarizeGraphDef(const GraphDef& graph_def) {\n+  std::string ret;\n   absl::StrAppend(&ret, \"versions = \", graph_def.versions().ShortDebugString(),\n                   \";\\n\");\n   for (const NodeDef& node : graph_def.node()) {\n@@ -85,15 +85,15 @@ absl::Status AddDefaultAttrsToGraphDef(GraphDef* graph_def,\n static absl::Status RemoveNewDefaultAttrsFromNodeDef(\n     NodeDef* node_def, const OpRegistryInterface& consumer_op_registry,\n     const OpRegistryInterface& producer_op_registry,\n-    std::set<std::pair<string, string>>* op_attr_removed) {\n+    std::set<std::pair<std::string, std::string>>* op_attr_removed) {\n   const OpDef* producer_op_def;\n   const OpDef* consumer_op_def;\n   TF_RETURN_IF_ERROR(\n       producer_op_registry.LookUpOpDef(node_def->op(), &producer_op_def));\n   TF_RETURN_IF_ERROR(\n       consumer_op_registry.LookUpOpDef(node_def->op(), &consumer_op_def));\n \n-  std::vector<string> to_remove;\n+  std::vector<std::string> to_remove;\n   for (const auto& attr : node_def->attr()) {\n     // If the attr is not in consumer_op_def and doesn't start with '_'...\n     if (!absl::StartsWith(attr.first, \"_\") &&\n@@ -117,7 +117,7 @@ static absl::Status RemoveNewDefaultAttrsFromNodeDef(\n   // We separate identifying which attrs should be removed from\n   // actually removing them to avoid invalidating the loop iterators\n   // above.\n-  for (const string& attr_name : to_remove) {\n+  for (const std::string& attr_name : to_remove) {\n     node_def->mutable_attr()->erase(attr_name);\n     if (op_attr_removed != nullptr) {\n       op_attr_removed->insert(std::make_pair(node_def->op(), attr_name));\n@@ -127,7 +127,7 @@ static absl::Status RemoveNewDefaultAttrsFromNodeDef(\n   return absl::OkStatus();\n }\n \n-static bool IsFunction(const GraphDef& graph_def, const string& op_name) {\n+static bool IsFunction(const GraphDef& graph_def, const std::string& op_name) {\n   for (const auto& func_def : graph_def.library().function()) {\n     if (op_name == func_def.signature().name()) return true;\n   }\n@@ -137,7 +137,7 @@ static bool IsFunction(const GraphDef& graph_def, const string& op_name) {\n absl::Status RemoveNewDefaultAttrsFromGraphDef(\n     GraphDef* graph_def, const OpRegistryInterface& consumer_op_registry,\n     const OpRegistryInterface& producer_op_registry,\n-    std::set<std::pair<string, string>>* op_attr_removed) {\n+    std::set<std::pair<std::string, std::string>>* op_attr_removed) {\n   // TODO(joshL): Make IsFunction() faster by collecting the names of\n   // all functions as a preprocessing step.\n   for (int n = 0; n < graph_def->node_size(); ++n) {\n@@ -184,7 +184,7 @@ void StripDefaultAttributes(const OpRegistryInterface& op_registry,\n     for (const OpDef::AttrDef& attr_def : op_def->attr()) {\n       if (attr_def.has_default_value()) {\n         AttrValueMap* attrs = node->mutable_attr();\n-        const string& name = attr_def.name();\n+        const std::string& name = attr_def.name();\n         auto iter = attrs->find(name);\n         if (iter != attrs->end()) {\n           const AttrValue& default_value = attr_def.default_value();\n@@ -202,21 +202,21 @@ void StripDefaultAttributes(const OpRegistryInterface& op_registry,\n }\n \n void OpsUsedByGraph(const GraphDef& graph_def,\n-                    std::set<string>* ops_used_in_graph) {\n+                    std::set<std::string>* ops_used_in_graph) {\n   // Map function names to definitions.\n-  std::unordered_map<string, const FunctionDef*> name_to_function;\n+  std::unordered_map<std::string, const FunctionDef*> name_to_function;\n   for (const auto& function : graph_def.library().function()) {\n     name_to_function.insert(\n         std::make_pair(function.signature().name(), &function));\n   }\n \n   // Collect the sorted list of op names.  Since functions can reference\n   // functions, we need a recursive traversal.\n-  std::set<string> used_ops;  // Includes both primitive ops and functions\n+  std::set<std::string> used_ops;  // Includes both primitive ops and functions\n   std::vector<const FunctionDef*> functions_to_process;  // A subset of used_ops\n   // Collect the logic to mark an op in a lambda; it'll be used twice below.\n   const auto mark_op_as_used = [&used_ops, &functions_to_process,\n-                                &name_to_function](const string& op) {\n+                                &name_to_function](const std::string& op) {\n     if (used_ops.insert(op).second) {\n       // If it's a function, we'll need to process further\n       const auto it = name_to_function.find(op);\n@@ -239,7 +239,7 @@ void OpsUsedByGraph(const GraphDef& graph_def,\n   // Filter out function names to produce output.\n   // TODO(josh11b): Change the above code to produce this directly.\n   ops_used_in_graph->clear();\n-  for (const string& op_name : used_ops) {\n+  for (const std::string& op_name : used_ops) {\n     if (name_to_function.find(op_name) == name_to_function.end()) {\n       ops_used_in_graph->insert(op_name);\n     }\n@@ -249,12 +249,12 @@ void OpsUsedByGraph(const GraphDef& graph_def,\n absl::Status StrippedOpListForGraph(const GraphDef& graph_def,\n                                     const OpRegistryInterface& op_registry,\n                                     OpList* stripped_op_list) {\n-  std::set<string> used_ops;\n+  std::set<std::string> used_ops;\n   OpsUsedByGraph(graph_def, &used_ops);\n \n   // Build the stripped op list in sorted order, ignoring functions.\n   stripped_op_list->clear_op();\n-  for (const string& op_name : used_ops) {\n+  for (const std::string& op_name : used_ops) {\n     const OpDef* op_def;\n     TF_RETURN_IF_ERROR(op_registry.LookUpOpDef(op_name, &op_def));\n     OpDef* stripped_op = stripped_op_list->add_op();"
        },
        {
            "sha": "b3e335e776f3f6e5b5b5bc748ecf81c871ce2810",
            "filename": "tensorflow/core/framework/graph_def_util.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fgraph_def_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fgraph_def_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fgraph_def_util.h?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -29,7 +29,7 @@ class NodeDef;\n \n // Produce a human-readable version of a GraphDef that is more concise\n // than a text-format proto.\n-string SummarizeGraphDef(const GraphDef& graph_def);\n+std::string SummarizeGraphDef(const GraphDef& graph_def);\n \n // Validates the syntax of a GraphDef provided externally.\n //\n@@ -97,7 +97,7 @@ absl::Status AddDefaultAttrsToGraphDef(GraphDef* graph_def,\n absl::Status RemoveNewDefaultAttrsFromGraphDef(\n     GraphDef* graph_def, const OpRegistryInterface& consumer_op_registry,\n     const OpRegistryInterface& producer_op_registry,\n-    std::set<std::pair<string, string>>* op_attr_removed);\n+    std::set<std::pair<std::string, std::string>>* op_attr_removed);\n \n // Goes over the `nodes` and removes attributes that are set to their\n // default values according to op_registry.\n@@ -115,7 +115,7 @@ void StripDefaultAttributes(const OpRegistryInterface& op_registry,\n //\n // This returns the ops used as a set of strings.\n void OpsUsedByGraph(const GraphDef& graph_def,\n-                    std::set<string>* ops_used_in_graph);\n+                    std::set<std::string>* ops_used_in_graph);\n \n // This function computes the stripped_op_list field of MetaGraphDef\n // and similar protos.  The op_registry should contain the ops used to"
        },
        {
            "sha": "503f2cc93af194ec6159b8953181e0dac078c708",
            "filename": "tensorflow/core/framework/graph_def_util_test.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 17,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fgraph_def_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fgraph_def_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fgraph_def_util_test.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -59,7 +59,7 @@ TEST(RemoveNewDefaultAttrsFromGraphDefTest, NoChangeWithDefault) {\n                    .Finalize(graph_def.add_node()));\n   GraphDef expected_graph_def = graph_def;\n \n-  std::set<std::pair<string, string>> op_attr_removed;\n+  std::set<std::pair<std::string, std::string>> op_attr_removed;\n   TF_ASSERT_OK(RemoveNewDefaultAttrsFromGraphDef(&graph_def, registry, registry,\n                                                  &op_attr_removed));\n \n@@ -80,7 +80,7 @@ TEST(RemoveNewDefaultAttrsFromGraphDefTest, NoChangeNoDefault) {\n                    .Finalize(graph_def.add_node()));\n   GraphDef expected_graph_def = graph_def;\n \n-  std::set<std::pair<string, string>> op_attr_removed;\n+  std::set<std::pair<std::string, std::string>> op_attr_removed;\n   TF_ASSERT_OK(RemoveNewDefaultAttrsFromGraphDef(&graph_def, registry, registry,\n                                                  &op_attr_removed));\n \n@@ -106,7 +106,7 @@ TEST(RemoveNewDefaultAttrsFromGraphDefTest, UsesDefault) {\n   TF_ASSERT_OK(NodeDefBuilder(\"uses_default\", \"UsesDefault\", &producer_registry)\n                    .Finalize(produced_graph_def.add_node()));\n \n-  std::set<std::pair<string, string>> op_attr_removed;\n+  std::set<std::pair<std::string, std::string>> op_attr_removed;\n   TF_ASSERT_OK(\n       RemoveNewDefaultAttrsFromGraphDef(&produced_graph_def, consumer_registry,\n                                         producer_registry, &op_attr_removed));\n@@ -116,7 +116,8 @@ TEST(RemoveNewDefaultAttrsFromGraphDefTest, UsesDefault) {\n                    .Finalize(expected_graph_def.add_node()));\n   TF_EXPECT_GRAPH_EQ(expected_graph_def, produced_graph_def);\n \n-  std::set<std::pair<string, string>> expected_removed({{\"UsesDefault\", \"a\"}});\n+  std::set<std::pair<std::string, std::string>> expected_removed(\n+      {{\"UsesDefault\", \"a\"}});\n   EXPECT_EQ(expected_removed, op_attr_removed);\n }\n \n@@ -142,7 +143,7 @@ TEST(RemoveNewDefaultAttrsFromGraphDefTest, ChangedFromDefault) {\n                    .Finalize(produced_graph_def.add_node()));\n   GraphDef expected_graph_def = produced_graph_def;\n \n-  std::set<std::pair<string, string>> op_attr_removed;\n+  std::set<std::pair<std::string, std::string>> op_attr_removed;\n   TF_ASSERT_OK(\n       RemoveNewDefaultAttrsFromGraphDef(&produced_graph_def, consumer_registry,\n                                         producer_registry, &op_attr_removed));\n@@ -174,7 +175,7 @@ TEST(RemoveNewDefaultAttrsFromGraphDefTest, UnderscoreAttrs) {\n                    .Finalize(produced_graph_def.add_node()));\n   GraphDef expected_graph_def = produced_graph_def;\n \n-  std::set<std::pair<string, string>> op_attr_removed;\n+  std::set<std::pair<std::string, std::string>> op_attr_removed;\n   TF_ASSERT_OK(\n       RemoveNewDefaultAttrsFromGraphDef(&produced_graph_def, consumer_registry,\n                                         producer_registry, &op_attr_removed));\n@@ -213,7 +214,7 @@ TEST(RemoveNewDefaultAttrsFromGraphDefTest, HasFunction) {\n   TF_ASSERT_OK(NodeDefBuilder(\"call_func\", \"my_func\", &function_registry)\n                    .Finalize(produced_graph_def.add_node()));\n \n-  std::set<std::pair<string, string>> op_attr_removed;\n+  std::set<std::pair<std::string, std::string>> op_attr_removed;\n   TF_ASSERT_OK(\n       RemoveNewDefaultAttrsFromGraphDef(&produced_graph_def, consumer_registry,\n                                         producer_registry, &op_attr_removed));\n@@ -231,7 +232,8 @@ TEST(RemoveNewDefaultAttrsFromGraphDefTest, HasFunction) {\n   EXPECT_EQ(expected_graph_def.library().DebugString(),\n             produced_graph_def.library().DebugString());\n \n-  std::set<std::pair<string, string>> expected_removed({{\"UsesDefault\", \"a\"}});\n+  std::set<std::pair<std::string, std::string>> expected_removed(\n+      {{\"UsesDefault\", \"a\"}});\n   EXPECT_EQ(expected_removed, op_attr_removed);\n }\n \n@@ -272,7 +274,7 @@ TEST(StripDefaultAttributesTest, NonDefaultNotStripped) {\n TEST(StrippedOpListForGraphTest, FlatTest) {\n   // Make four ops\n   OpList op_list;\n-  for (const string& op : {\"A\", \"B\", \"C\", \"D\"}) {\n+  for (const std::string& op : {\"A\", \"B\", \"C\", \"D\"}) {\n     OpDef* op_def = op_list.add_op();\n     op_def->set_name(op);\n     op_def->set_summary(\"summary\");\n@@ -282,21 +284,21 @@ TEST(StrippedOpListForGraphTest, FlatTest) {\n \n   // Make a graph which uses two ops once and twice, respectively.\n   // The result should be independent of the ordering.\n-  const string graph_ops[4][3] = {\n+  const std::string graph_ops[4][3] = {\n       {\"C\", \"B\", \"B\"}, {\"B\", \"C\", \"B\"}, {\"B\", \"B\", \"C\"}, {\"C\", \"C\", \"B\"}};\n   for (const bool use_function : {false, true}) {\n     for (int order = 0; order < 4; order++) {\n       GraphDef graph_def;\n       if (use_function) {\n         FunctionDef* function_def = graph_def.mutable_library()->add_function();\n         function_def->mutable_signature()->set_name(\"F\");\n-        for (const string& op : graph_ops[order]) {\n+        for (const std::string& op : graph_ops[order]) {\n           function_def->add_node_def()->set_op(op);\n         }\n         graph_def.add_node()->set_op(\"F\");\n       } else {\n-        for (const string& op : graph_ops[order]) {\n-          string name = absl::StrCat(\"name\", graph_def.node_size());\n+        for (const std::string& op : graph_ops[order]) {\n+          std::string name = absl::StrCat(\"name\", graph_def.node_size());\n           NodeDef* node = graph_def.add_node();\n           node->set_name(name);\n           node->set_op(op);\n@@ -319,9 +321,9 @@ TEST(StrippedOpListForGraphTest, FlatTest) {\n       }\n \n       // Should get the same result using OpsUsedByGraph().\n-      std::set<string> used_ops;\n+      std::set<std::string> used_ops;\n       OpsUsedByGraph(graph_def, &used_ops);\n-      ASSERT_EQ(std::set<string>({\"B\", \"C\"}), used_ops);\n+      ASSERT_EQ(std::set<std::string>({\"B\", \"C\"}), used_ops);\n     }\n   }\n }\n@@ -356,9 +358,9 @@ TEST(StrippedOpListForGraphTest, NestedFunctionTest) {\n     ASSERT_EQ(stripped_op_list.op(0).name(), \"A\");\n \n     // Should get the same result using OpsUsedByGraph().\n-    std::set<string> used_ops;\n+    std::set<std::string> used_ops;\n     OpsUsedByGraph(graph_def, &used_ops);\n-    ASSERT_EQ(std::set<string>({\"A\"}), used_ops);\n+    ASSERT_EQ(std::set<std::string>({\"A\"}), used_ops);\n   }\n }\n "
        },
        {
            "sha": "b3226c6fac490bfa99bef445a1413b717bcaa48d",
            "filename": "tensorflow/core/framework/graph_to_functiondef.cc",
            "status": "modified",
            "additions": 66,
            "deletions": 65,
            "changes": 131,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fgraph_to_functiondef.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fgraph_to_functiondef.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fgraph_to_functiondef.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -51,37 +51,37 @@ class NodeNameMapping {\n \n   // Normalize the input name and make it unique. This is the same as the\n   // function for output, expect that it adds a name mapping for the name.\n-  string GetInputName(const string& name);\n+  std::string GetInputName(const std::string& name);\n \n   // Normalize the output name and make it unique.\n-  string GetOutputName(const string& name);\n+  std::string GetOutputName(const std::string& name);\n \n   // Make the node name unique.\n-  string Uniquify(const string& name);\n+  std::string Uniquify(const std::string& name);\n \n   // Records name as a used name. If this name is already used,\n   // returns an error status.\n-  absl::Status UseOutputName(const string& name);\n+  absl::Status UseOutputName(const std::string& name);\n \n   // Look up how a node name was previously normalized/uniquified.\n   // Returns empty if name was never seen.\n-  string Lookup(const string& name) const;\n+  std::string Lookup(const std::string& name) const;\n \n  private:\n-  string UniquifyHelper(const string& name);\n-  static string Normalize(string name);\n+  std::string UniquifyHelper(const std::string& name);\n+  static std::string Normalize(std::string name);\n \n   // The normalized/uniquified names already used as\n   // input names (in signature), output names (in signature), and node names\n   // (in node_def).\n   // This is a superset of values in name_mapping_.\n-  absl::flat_hash_map<string, uint64> used_names_;\n+  absl::flat_hash_map<std::string, uint64_t> used_names_;\n   // Mapping from original node name from the graph to the normalized\n   // and uniquified version of it.\n-  absl::flat_hash_map<string, string> name_mapping_;\n+  absl::flat_hash_map<std::string, std::string> name_mapping_;\n };\n \n-string NodeNameMapping::Normalize(string name) {\n+std::string NodeNameMapping::Normalize(std::string name) {\n   // Convert letters to lowercase and non-alphanumeric characters to '_'.\n   if (name.empty()) return \"unknown\";\n   const int n = name.size();\n@@ -106,38 +106,38 @@ string NodeNameMapping::Normalize(string name) {\n   return i == n ? \"unknown\" : name.substr(i);\n }\n \n-string NodeNameMapping::UniquifyHelper(const string& name) {\n+std::string NodeNameMapping::UniquifyHelper(const std::string& name) {\n   auto it = used_names_.emplace(name, 0);\n   // If the name hasn't been used yet, use it as-is.\n   if (it.second) return name;\n \n   // Add a suffix to name to make it unique.\n   while (true) {\n-    const string candidate = absl::StrCat(name, \"_\", it.first->second);\n+    const std::string candidate = absl::StrCat(name, \"_\", it.first->second);\n     it.first->second++;\n     if (used_names_.emplace(candidate, 0).second) return candidate;\n   }\n }\n \n-string NodeNameMapping::GetInputName(const string& name) {\n-  const string& input_name = UniquifyHelper(Normalize(name));\n+std::string NodeNameMapping::GetInputName(const std::string& name) {\n+  const std::string& input_name = UniquifyHelper(Normalize(name));\n   name_mapping_[name] = input_name;\n   return input_name;\n }\n \n-string NodeNameMapping::GetOutputName(const string& name) {\n-  const string& input_name = UniquifyHelper(Normalize(name));\n+std::string NodeNameMapping::GetOutputName(const std::string& name) {\n+  const std::string& input_name = UniquifyHelper(Normalize(name));\n   // Don't add it to name_mapping_ since this name is not for a node.\n   return input_name;\n }\n \n-string NodeNameMapping::Uniquify(const string& name) {\n-  const string uniqued = UniquifyHelper(name);\n+std::string NodeNameMapping::Uniquify(const std::string& name) {\n+  const std::string uniqued = UniquifyHelper(name);\n   name_mapping_[name] = uniqued;\n   return uniqued;\n }\n \n-absl::Status NodeNameMapping::UseOutputName(const string& name) {\n+absl::Status NodeNameMapping::UseOutputName(const std::string& name) {\n   const auto& iter = used_names_.find(name);\n   if (iter != used_names_.end()) {\n     return errors::InvalidArgument(\n@@ -148,19 +148,19 @@ absl::Status NodeNameMapping::UseOutputName(const string& name) {\n   return absl::OkStatus();\n }\n \n-string NodeNameMapping::Lookup(const string& name) const {\n+std::string NodeNameMapping::Lookup(const std::string& name) const {\n   const auto iter = name_mapping_.find(name);\n-  if (iter == name_mapping_.end()) return string();\n+  if (iter == name_mapping_.end()) return std::string();\n   return iter->second;\n }\n \n absl::Status FillFunctionBody(\n-    const string& fn_name, const NodeNameMapping& node_names,\n+    const std::string& fn_name, const NodeNameMapping& node_names,\n     const std::vector<const Node*>& body_nodes,\n-    const absl::flat_hash_map<string, string>& tensor_renaming,\n+    const absl::flat_hash_map<std::string, std::string>& tensor_renaming,\n     bool set_stateful_from_nodes, bool copy_placeholder_attrs_from_nodes,\n     bool allow_destructive_reads, FunctionDef* fdef) {\n-  absl::flat_hash_set<string> func_attr_names;\n+  absl::flat_hash_set<std::string> func_attr_names;\n   for (const auto& func_attr : fdef->signature().attr()) {\n     func_attr_names.insert(func_attr.name());\n   }\n@@ -263,7 +263,7 @@ absl::Status FillFunctionBody(\n     for (const Edge* edge : control_edges) {\n       // Add this control input only if the src node is in the body or a part of\n       // the inputs.\n-      const string normalized = node_names.Lookup(edge->src()->name());\n+      const std::string normalized = node_names.Lookup(edge->src()->name());\n       // If we did not find a name for the source of control edge, this\n       // source must be outside of the body, and not an input. Raise an error.\n       if (normalized.empty()) {\n@@ -322,15 +322,16 @@ absl::Status FillFunctionBody(\n }\n \n absl::Status GraphToFunctionDefHelper(\n-    const Graph& fn_body, const string& fn_name, bool append_hash_to_fn_name,\n-    bool set_stateful_from_nodes, bool copy_placeholder_attrs_from_nodes,\n+    const Graph& fn_body, const std::string& fn_name,\n+    bool append_hash_to_fn_name, bool set_stateful_from_nodes,\n+    bool copy_placeholder_attrs_from_nodes,\n     const std::vector<const Node*>& body_nodes,\n     const std::vector<OutputTensor>& inputs,\n     const std::vector<OutputTensor>& outputs,\n-    const std::vector<string>& output_names,\n+    const std::vector<std::string>& output_names,\n     const std::vector<const Node*>& control_outputs,\n-    const std::vector<string>& control_output_names, const char* description,\n-    bool allow_destructive_reads, FunctionDef* fdef) {\n+    const std::vector<std::string>& control_output_names,\n+    const char* description, bool allow_destructive_reads, FunctionDef* fdef) {\n   if (!output_names.empty()) {\n     DCHECK_EQ(output_names.size(), outputs.size());\n   }\n@@ -350,7 +351,7 @@ absl::Status GraphToFunctionDefHelper(\n   //  - For tensors produced by nodes in function's body:\n   //    {flat_tensor_name -> nested_tensor_name}\n   //    e.g. {Add:3 -> add_0:z:1}\n-  absl::flat_hash_map<string, string> tensor_renaming;\n+  absl::flat_hash_map<std::string, std::string> tensor_renaming;\n \n   // Fill outputs in function's signature.\n   // We fill the outputs first to prevent output_names from colliding\n@@ -380,7 +381,7 @@ absl::Status GraphToFunctionDefHelper(\n     int idx = inputs[i].index;\n     OpDef::ArgDef* argdef = fdef->mutable_signature()->add_input_arg();\n     argdef->set_type(node->output_type(idx));\n-    const string& input_name = node_names.GetInputName(node->name());\n+    const std::string& input_name = node_names.GetInputName(node->name());\n     argdef->set_name(input_name);\n     FunctionDef::ArgAttrs arg_attrs;\n     int64_t resource_arg_unique_id = -1;\n@@ -431,7 +432,7 @@ absl::Status GraphToFunctionDefHelper(\n   // in tensor_renaming.\n   for (const Node* node : body_nodes) {\n     // Make sure node_name does not collide with an input or output name.\n-    const string& node_name = node_names.Uniquify(node->name());\n+    const std::string& node_name = node_names.Uniquify(node->name());\n     // For each output_arg in the op_def, the output_ranges\n     // map will have [start, end] range of indices that this arg produces\n     // among all the output tensors of this op.\n@@ -443,8 +444,8 @@ absl::Status GraphToFunctionDefHelper(\n       int index_start = output.second.first;\n       int index_end = output.second.second;\n       for (int i = index_start; i < index_end; ++i) {\n-        const string& original_name = absl::StrCat(node->name(), \":\", i);\n-        const string& new_name =\n+        const std::string& original_name = absl::StrCat(node->name(), \":\", i);\n+        const std::string& new_name =\n             strings::StrCat(node_name, \":\", output_name, \":\", i - index_start);\n         // Record the mapping if this tensor is not already mapped.\n         // Tensor can be already mapped if it is used as an input.\n@@ -461,10 +462,10 @@ absl::Status GraphToFunctionDefHelper(\n \n   // Remap return values.\n   for (int r = 0; r < fdef->signature().output_arg_size(); ++r) {\n-    const string& ret_name = fdef->signature().output_arg(r).name();\n+    const std::string& ret_name = fdef->signature().output_arg(r).name();\n     // We convert this flat tensor name to the nested value\n     // (e.g. `add:z:1`) that we stored in tensor_renaming.\n-    string return_value;\n+    std::string return_value;\n     if (outputs[r].node->IsRetval()) {\n       Edge const* edge;\n       TF_RETURN_IF_ERROR(outputs[r].node->input_edge(0, &edge));\n@@ -484,8 +485,8 @@ absl::Status GraphToFunctionDefHelper(\n   }\n \n   if (append_hash_to_fn_name) {\n-    const uint64 hash = FunctionDefHash(*fdef);\n-    string encoded;\n+    const uint64_t hash = FunctionDefHash(*fdef);\n+    std::string encoded;\n     TF_RETURN_IF_ERROR(Base64Encode(\n         absl::string_view(reinterpret_cast<const char*>(&hash), sizeof(hash)),\n         &encoded));\n@@ -508,9 +509,9 @@ absl::Status GraphToFunctionDefHelper(\n         \") and the number of control output names (\",\n         control_output_names.size(), \") to match but they do not.\");\n   }\n-  std::set<string> control_output_names_set;\n+  std::set<std::string> control_output_names_set;\n   for (int i = 0; i < control_outputs.size(); ++i) {\n-    string signature_name;\n+    std::string signature_name;\n     if (!control_output_names.empty()) {\n       signature_name = control_output_names[i];\n     } else {\n@@ -523,25 +524,25 @@ absl::Status GraphToFunctionDefHelper(\n       return errors::InvalidArgument(\"Repeated control output name: \",\n                                      signature_name);\n     }\n-    const string control_output_node =\n+    const std::string control_output_node =\n         node_names.Lookup(control_outputs[i]->name());\n     if (control_output_node.empty()) {\n       return errors::InvalidArgument(\n           \"Control output node name must be not empty\");\n     }\n     (*fdef->mutable_control_ret())[signature_name] = control_output_node;\n   }\n-  for (const string& control_output : control_output_names_set) {\n+  for (const std::string& control_output : control_output_names_set) {\n     fdef->mutable_signature()->add_control_output(control_output);\n   }\n \n   return absl::OkStatus();\n }\n \n absl::Status GraphToFunctionDefHelper(\n-    const Graph& graph, const string& name,\n-    const std::function<absl::optional<string>(const Node*)>& control_ret,\n-    const std::vector<string>& output_names, bool allow_destructive_reads,\n+    const Graph& graph, const std::string& name,\n+    const std::function<absl::optional<std::string>(const Node*)>& control_ret,\n+    const std::vector<std::string>& output_names, bool allow_destructive_reads,\n     FunctionDef* fdef) {\n   auto add_arg_or_retval = [](Node* node,\n                               std::vector<OutputTensor>* args_or_retvals) {\n@@ -566,7 +567,7 @@ absl::Status GraphToFunctionDefHelper(\n   std::vector<OutputTensor> inputs;\n   std::vector<OutputTensor> outputs;\n   std::vector<const Node*> control_outputs;\n-  std::vector<string> control_output_names;\n+  std::vector<std::string> control_output_names;\n   for (Node* node : graph.op_nodes()) {\n     if (node->IsArg()) {\n       TF_RETURN_IF_ERROR(add_arg_or_retval(node, &inputs));\n@@ -591,7 +592,7 @@ absl::Status GraphToFunctionDefHelper(\n \n   auto validate_args_retvals =\n       [](const std::vector<OutputTensor>& args_or_retvals,\n-         const string& op_type) {\n+         const std::string& op_type) {\n         for (int i = 0, e = args_or_retvals.size(); i < e; ++i) {\n           if (args_or_retvals[i].node == nullptr) {\n             return errors::InvalidArgument(\"Missing '\", op_type,\n@@ -614,17 +615,17 @@ absl::Status GraphToFunctionDefHelper(\n \n }  // anonymous namespace\n \n-absl::Status GraphToFunctionDef(const Graph& fn_body, const string& fn_name,\n-                                bool append_hash_to_fn_name,\n-                                bool set_stateful_from_nodes,\n-                                bool copy_placeholder_attrs_from_nodes,\n-                                const std::vector<const Node*>& body_nodes,\n-                                const std::vector<OutputTensor>& inputs,\n-                                const std::vector<OutputTensor>& outputs,\n-                                const std::vector<string>& output_names,\n-                                const std::vector<const Node*>& control_outputs,\n-                                const std::vector<string>& control_output_names,\n-                                const char* description, FunctionDef* fdef) {\n+absl::Status GraphToFunctionDef(\n+    const Graph& fn_body, const std::string& fn_name,\n+    bool append_hash_to_fn_name, bool set_stateful_from_nodes,\n+    bool copy_placeholder_attrs_from_nodes,\n+    const std::vector<const Node*>& body_nodes,\n+    const std::vector<OutputTensor>& inputs,\n+    const std::vector<OutputTensor>& outputs,\n+    const std::vector<std::string>& output_names,\n+    const std::vector<const Node*>& control_outputs,\n+    const std::vector<std::string>& control_output_names,\n+    const char* description, FunctionDef* fdef) {\n   return GraphToFunctionDefHelper(\n       fn_body, fn_name, append_hash_to_fn_name, set_stateful_from_nodes,\n       copy_placeholder_attrs_from_nodes, body_nodes, inputs, outputs,\n@@ -634,20 +635,20 @@ absl::Status GraphToFunctionDef(const Graph& fn_body, const string& fn_name,\n }\n \n absl::Status GraphToFunctionDef(\n-    const Graph& graph, const string& name,\n-    const std::function<absl::optional<string>(const Node*)>& control_ret,\n+    const Graph& graph, const std::string& name,\n+    const std::function<absl::optional<std::string>(const Node*)>& control_ret,\n     FunctionDef* fdef) {\n   return GraphToFunctionDefHelper(graph, name, control_ret,\n                                   /*output_names=*/{},\n                                   /*allow_destructive_reads=*/false, fdef);\n }\n \n-absl::Status GraphToFunctionDef(const Graph& graph, const string& name,\n+absl::Status GraphToFunctionDef(const Graph& graph, const std::string& name,\n                                 FunctionDef* fdef) {\n   return GraphToFunctionDef(graph, name, /*control_ret=*/nullptr, fdef);\n }\n \n-absl::Status GraphToFunctionDef(const Graph& graph, const string& name,\n+absl::Status GraphToFunctionDef(const Graph& graph, const std::string& name,\n                                 const std::vector<std::string>& output_names,\n                                 FunctionDef* fdef) {\n   return GraphToFunctionDefHelper(graph, name, /*control_ret=*/nullptr,\n@@ -656,8 +657,8 @@ absl::Status GraphToFunctionDef(const Graph& graph, const string& name,\n }\n \n absl::Status GraphToFunctionDef(\n-    std::unique_ptr<Graph> graph, const string& name,\n-    const std::function<std::optional<string>(const Node*)>& control_ret,\n+    std::unique_ptr<Graph> graph, const std::string& name,\n+    const std::function<std::optional<std::string>(const Node*)>& control_ret,\n     FunctionDef* fdef) {\n   return GraphToFunctionDefHelper(*graph, name, control_ret,\n                                   /*output_names=*/{},"
        },
        {
            "sha": "4558af7938f3129c25859b56fd25a34d9328abad",
            "filename": "tensorflow/core/framework/graph_to_functiondef.h",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fgraph_to_functiondef.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fgraph_to_functiondef.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fgraph_to_functiondef.h?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -29,17 +29,17 @@ namespace tensorflow {\n // Graph to FunctionDef conversion. This code is closely modeled on the Python\n // function graph_to_function_def(), which is located in\n // tensorflow/python/framework/graph_to_function_def.py.\n-absl::Status GraphToFunctionDef(const Graph& fn_body, const string& fn_name,\n-                                bool append_hash_to_fn_name,\n-                                bool set_stateful_from_nodes,\n-                                bool copy_placeholder_attrs_from_nodes,\n-                                const std::vector<const Node*>& body_nodes,\n-                                const std::vector<OutputTensor>& inputs,\n-                                const std::vector<OutputTensor>& outputs,\n-                                const std::vector<string>& output_names,\n-                                const std::vector<const Node*>& control_outputs,\n-                                const std::vector<string>& control_output_names,\n-                                const char* description, FunctionDef* fdef);\n+absl::Status GraphToFunctionDef(\n+    const Graph& fn_body, const std::string& fn_name,\n+    bool append_hash_to_fn_name, bool set_stateful_from_nodes,\n+    bool copy_placeholder_attrs_from_nodes,\n+    const std::vector<const Node*>& body_nodes,\n+    const std::vector<OutputTensor>& inputs,\n+    const std::vector<OutputTensor>& outputs,\n+    const std::vector<std::string>& output_names,\n+    const std::vector<const Node*>& control_outputs,\n+    const std::vector<std::string>& control_output_names,\n+    const char* description, FunctionDef* fdef);\n \n // Converts 'graph' to a FunctionDef 'fdef', with name 'name':\n //\n@@ -50,20 +50,20 @@ absl::Status GraphToFunctionDef(const Graph& fn_body, const string& fn_name,\n //     `control_output` in Op definition (see OpDef). Control output name must\n //     be unique for all control output nodes.\n absl::Status GraphToFunctionDef(\n-    const Graph& graph, const string& name,\n-    const std::function<absl::optional<string>(const Node*)>& control_ret,\n+    const Graph& graph, const std::string& name,\n+    const std::function<absl::optional<std::string>(const Node*)>& control_ret,\n     FunctionDef* fdef);\n \n-absl::Status GraphToFunctionDef(const Graph& graph, const string& name,\n+absl::Status GraphToFunctionDef(const Graph& graph, const std::string& name,\n                                 FunctionDef* fdef);\n \n-absl::Status GraphToFunctionDef(const Graph& graph, const string& name,\n+absl::Status GraphToFunctionDef(const Graph& graph, const std::string& name,\n                                 const std::vector<std::string>& output_names,\n                                 FunctionDef* fdef);\n \n absl::Status GraphToFunctionDef(\n-    std::unique_ptr<Graph> graph, const string& name,\n-    const std::function<std::optional<string>(const Node*)>& control_ret,\n+    std::unique_ptr<Graph> graph, const std::string& name,\n+    const std::function<std::optional<std::string>(const Node*)>& control_ret,\n     FunctionDef* fdef);\n \n }  // namespace tensorflow"
        },
        {
            "sha": "719f9af233758e161f5be4f899222690098ea1a0",
            "filename": "tensorflow/core/framework/graph_to_functiondef_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fgraph_to_functiondef_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fgraph_to_functiondef_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fgraph_to_functiondef_test.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -47,7 +47,7 @@ FunctionDef RemoveDebugInfo(const FunctionDef& def) {\n }\n \n bool EqualFunctionDef(const FunctionDef& a, const FunctionDef& b,\n-                      string* diff) {\n+                      std::string* diff) {\n   // TODO(phawkins) use a more sophisticated equality test.\n   if (a.DebugString() != b.DebugString()) {\n     if (diff) {\n@@ -95,7 +95,7 @@ TEST(GraphToFunctionDefTest, Basics) {\n       },\n       {{\"h\", \"G:sum:0\"}});  // return values\n \n-  string diff;\n+  std::string diff;\n   bool fdefs_equal =\n       EqualFunctionDef(fdef_expected, RemoveDebugInfo(fdef), &diff);\n \n@@ -119,7 +119,7 @@ TEST(GraphToFunctionDefTest, OverrideOutputNames) {\n                                 {},             // body\n                                 {{\"b\", \"a\"}});  // return values\n \n-  string diff;\n+  std::string diff;\n   bool fdefs_equal =\n       EqualFunctionDef(fdef_expected, RemoveDebugInfo(fdef), &diff);\n \n@@ -168,7 +168,7 @@ TEST(GraphToFunctionDefTest, ArgAttrShape) {\n   attrs.mutable_attr()->insert({\"_output_shapes\", output_shapes});\n   (*fdef_expected.mutable_arg_attr())[0] = std::move(attrs);\n \n-  string diff;\n+  std::string diff;\n   bool fdefs_equal =\n       EqualFunctionDef(fdef_expected, RemoveDebugInfo(fdef), &diff);\n \n@@ -199,7 +199,7 @@ TEST(GraphToFunctionDefTest, ArgAttrPrivateAttr) {\n   attrs.mutable_attr()->insert({\"_name\", private_attr});\n   (*fdef_expected.mutable_arg_attr())[0] = std::move(attrs);\n \n-  string diff;\n+  std::string diff;\n   bool fdefs_equal =\n       EqualFunctionDef(fdef_expected, RemoveDebugInfo(fdef), &diff);\n \n@@ -266,7 +266,7 @@ TEST(GraphToFunctionDefTest, ArgAttrConstInput) {\n   (*fdef_expected.mutable_signature()->mutable_description()) =\n       \"ArgAttrConstInput\";\n \n-  string diff;\n+  std::string diff;\n   bool fdefs_equal =\n       EqualFunctionDef(fdef_expected, RemoveDebugInfo(fdef), &diff);\n \n@@ -374,7 +374,7 @@ TEST(GraphToFunctionDefTest, ControlDependencies) {\n       },\n       {{\"c\", \"b:y:0\"}});  // return values\n \n-  string diff;\n+  std::string diff;\n   bool fdefs_equal =\n       EqualFunctionDef(fdef_expected, RemoveDebugInfo(fdef), &diff);\n \n@@ -395,8 +395,9 @@ TEST(GraphToFunctionDefTest, ControlOutputs) {\n   TF_EXPECT_OK(ConvertGraphDefToGraph(options, graph_def, graph.get()));\n \n   // Add a 'b' node to the control return set.\n-  const auto control_ret = [](const Node* n) -> absl::optional<string> {\n-    if (n->name() == \"b\") return absl::make_optional<string>(\"must_execute\");\n+  const auto control_ret = [](const Node* n) -> absl::optional<std::string> {\n+    if (n->name() == \"b\")\n+      return absl::make_optional<std::string>(\"must_execute\");\n     return absl::nullopt;\n   };\n \n@@ -415,7 +416,7 @@ TEST(GraphToFunctionDefTest, ControlOutputs) {\n                                 {{\"c\", \"b:y:0\"}},          // return values\n                                 {{\"must_execute\", \"b\"}});  // control returns\n \n-  string diff;\n+  std::string diff;\n   bool fdefs_equal =\n       EqualFunctionDef(fdef_expected, RemoveDebugInfo(fdef), &diff);\n "
        },
        {
            "sha": "7b7e90df8bab2a7a9dd4a3764070be19c6be0528",
            "filename": "tensorflow/core/framework/kernel_def_builder.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fkernel_def_builder.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fkernel_def_builder.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fkernel_def_builder.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -55,8 +55,8 @@ KernelDefBuilder& KernelDefBuilder::AttrConstraint<int64_t>(\n }\n \n template <>\n-KernelDefBuilder& KernelDefBuilder::AttrConstraint<string>(\n-    const char* attr_name, absl::Span<const string> allowed) {\n+KernelDefBuilder& KernelDefBuilder::AttrConstraint<std::string>(\n+    const char* attr_name, absl::Span<const std::string> allowed) {\n   auto* constraint = kernel_def_->add_constraint();\n   constraint->set_name(attr_name);\n   auto* allowed_values = constraint->mutable_allowed_values()->mutable_list();\n@@ -67,11 +67,11 @@ KernelDefBuilder& KernelDefBuilder::AttrConstraint<string>(\n }\n \n template <>\n-KernelDefBuilder& KernelDefBuilder::AttrConstraint<string>(\n-    const char* attr_name, string allowed) {\n-  return AttrConstraint(\n-      attr_name,\n-      absl::Span<const string>(std::initializer_list<string>({allowed})));\n+KernelDefBuilder& KernelDefBuilder::AttrConstraint<std::string>(\n+    const char* attr_name, std::string allowed) {\n+  return AttrConstraint(attr_name,\n+                        absl::Span<const std::string>(\n+                            std::initializer_list<std::string>({allowed})));\n }\n \n template <>"
        },
        {
            "sha": "eefa454beb763ecf38d13d0685e6d1d49ca88f21",
            "filename": "tensorflow/core/framework/kernel_def_builder_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fkernel_def_builder_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fkernel_def_builder_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fkernel_def_builder_test.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -48,7 +48,7 @@ TEST(KernelDefBuilderTest, TypeConstraint) {\n \n   def = KernelDefBuilder(\"C\")\n             .Device(DEVICE_GPU)\n-            .TypeConstraint<int32>(\"U\")\n+            .TypeConstraint<int32_t>(\"U\")\n             .TypeConstraint<bool>(\"V\")\n             .Build();\n \n@@ -95,7 +95,7 @@ TEST(KernelDefBuilderTest, Int64Constraint) {\n             .Device(DEVICE_GPU)\n             .AttrConstraint(\"U\",\n                             absl::Span<const int64_t>{int64_t{5}, int64_t{17}})\n-            .AttrConstraint(\"V\", string(\"proto\"))\n+            .AttrConstraint(\"V\", std::string(\"proto\"))\n             .Build();\n \n   protobuf::TextFormat::ParseFromString(\n@@ -136,7 +136,7 @@ TEST(KernelDefBuilderTest, StringConstraint) {\n   def = KernelDefBuilder(\"C\")\n             .Device(DEVICE_GPU)\n             .AttrConstraint(\"U\", absl::Span<const char* const>{\"boo\", \"ya\"})\n-            .AttrConstraint(\"V\", string(\"proto\"))\n+            .AttrConstraint(\"V\", std::string(\"proto\"))\n             .Build();\n \n   protobuf::TextFormat::ParseFromString("
        },
        {
            "sha": "a15fa7b0cfbe0f1a2ef51d0e574d49d0e72c085e",
            "filename": "tensorflow/core/framework/kernel_def_util_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fkernel_def_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fkernel_def_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fkernel_def_util_test.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -24,22 +24,22 @@ namespace tensorflow {\n \n namespace {\n \n-NodeDef NodeDefFromText(const string& text) {\n+NodeDef NodeDefFromText(const std::string& text) {\n   NodeDef node_def;\n   EXPECT_TRUE(protobuf::TextFormat::MergeFromString(text, &node_def));\n   return node_def;\n }\n \n-KernelDef KernelDefFromText(const string& text) {\n+KernelDef KernelDefFromText(const std::string& text) {\n   KernelDef kernel_def;\n   EXPECT_TRUE(protobuf::TextFormat::MergeFromString(text, &kernel_def));\n   return kernel_def;\n }\n \n class AttrsMatchTest : public ::testing::Test {\n  protected:\n-  void ExpectStatus(const string& node_def_str, const string& kernel_def_str,\n-                    error::Code code) {\n+  void ExpectStatus(const std::string& node_def_str,\n+                    const std::string& kernel_def_str, error::Code code) {\n     bool match;\n     auto status = KernelAttrsMatch(KernelDefFromText(kernel_def_str),\n                                    NodeDefFromText(node_def_str), &match);\n@@ -53,7 +53,7 @@ class AttrsMatchTest : public ::testing::Test {\n };\n \n TEST_F(AttrsMatchTest, ValidConstraint) {\n-  string node_def_str = R\"(\n+  std::string node_def_str = R\"(\n     name: \"ValidConstraint-op\"\n     op: \"ValidConstraint\"\n     attr {\n@@ -63,7 +63,7 @@ TEST_F(AttrsMatchTest, ValidConstraint) {\n       }\n     }\n   )\";\n-  string kernel_def_str = R\"(\n+  std::string kernel_def_str = R\"(\n     op: \"ValidConstraint\"\n     device_type: \"CPU\"\n     constraint {\n@@ -79,7 +79,7 @@ TEST_F(AttrsMatchTest, ValidConstraint) {\n }\n \n TEST_F(AttrsMatchTest, BadConstraint) {\n-  string node_def_str = R\"(\n+  std::string node_def_str = R\"(\n     name: \"BadConstraint-op\"\n     op: \"BadConstraint\"\n     attr {\n@@ -89,7 +89,7 @@ TEST_F(AttrsMatchTest, BadConstraint) {\n       }\n     }\n   )\";\n-  string kernel_def_str = R\"(\n+  std::string kernel_def_str = R\"(\n     op: \"BadConstraint\"\n     device_type: \"CPU\"\n     constraint {\n@@ -105,7 +105,7 @@ TEST_F(AttrsMatchTest, BadConstraint) {\n }\n \n TEST_F(AttrsMatchTest, Unimplemented) {\n-  string node_def_str = R\"(\n+  std::string node_def_str = R\"(\n     name: \"BadConstraint-op\"\n     op: \"BadConstraint\"\n     attr {\n@@ -115,7 +115,7 @@ TEST_F(AttrsMatchTest, Unimplemented) {\n       }\n     }\n   )\";\n-  string kernel_def_str = R\"(\n+  std::string kernel_def_str = R\"(\n     op: \"BadConstraint\"\n     device_type: \"CPU\"\n     constraint {"
        },
        {
            "sha": "df63471f59dff38bcfd14716b89a4deab79c9ca5",
            "filename": "tensorflow/core/framework/load_library.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fload_library.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fload_library.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fload_library.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -46,10 +46,10 @@ struct Library {\n absl::Status LoadDynamicLibrary(const char* library_filename, void** result,\n                                 const void** buf, size_t* len) {\n   static mutex mu(LINKER_INITIALIZED);\n-  static std::unordered_map<string, Library> loaded_libs;\n+  static std::unordered_map<std::string, Library> loaded_libs;\n   Env* env = Env::Default();\n   Library library;\n-  std::unordered_set<string> seen_op_names;\n+  std::unordered_set<std::string> seen_op_names;\n   {\n     mutex_lock lock(mu);\n     if (loaded_libs.find(library_filename) != loaded_libs.end()) {\n@@ -90,7 +90,7 @@ absl::Status LoadDynamicLibrary(const char* library_filename, void** result,\n       loaded_libs[library_filename] = library;\n     }\n   }\n-  string str;\n+  std::string str;\n   library.op_list.SerializeToString(&str);\n   char* str_buf = reinterpret_cast<char*>(port::Malloc(str.length()));\n   memcpy(str_buf, str.data(), str.length());"
        },
        {
            "sha": "6a56c1695d35b9b8766aa24ed11fe572a7900683",
            "filename": "tensorflow/core/framework/local_rendezvous.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Flocal_rendezvous.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Flocal_rendezvous.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Flocal_rendezvous.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -141,15 +141,15 @@ LocalRendezvous::~LocalRendezvous() {\n }\n \n namespace {\n-uint64 KeyHash(const absl::string_view& k) {\n+uint64_t KeyHash(const absl::string_view& k) {\n   return Hash64(k.data(), k.size());\n }\n }  // namespace\n \n absl::Status LocalRendezvous::Send(const Rendezvous::ParsedKey& key,\n                                    const Rendezvous::Args& send_args,\n                                    const Tensor& val, const bool is_dead) {\n-  uint64 key_hash = KeyHash(key.FullKey());\n+  uint64_t key_hash = KeyHash(key.FullKey());\n   DVLOG(2) << \"Send \" << this << \" \" << key_hash << \" \" << key.FullKey();\n \n   if (is_dead) {\n@@ -158,7 +158,7 @@ absl::Status LocalRendezvous::Send(const Rendezvous::ParsedKey& key,\n         \"The number of dead values sent between a pair of devices.\",\n         \"send_device\", \"recv_device\");\n     rendezvous_dead_values_sent\n-        ->GetCell(string(key.src_device), string(key.dst_device))\n+        ->GetCell(std::string(key.src_device), std::string(key.dst_device))\n         ->IncrementBy(1);\n   }\n \n@@ -229,7 +229,7 @@ absl::Status LocalRendezvous::Send(const Rendezvous::ParsedKey& key,\n void LocalRendezvous::RecvAsync(const Rendezvous::ParsedKey& key,\n                                 const Rendezvous::Args& recv_args,\n                                 Rendezvous::DoneCallback done) {\n-  uint64 key_hash = KeyHash(key.FullKey());\n+  uint64_t key_hash = KeyHash(key.FullKey());\n   DVLOG(2) << \"Recv \" << this << \" \" << key_hash << \" \" << key.FullKey();\n   tsl::core::RefCountPtr<Rendezvous> rc_keep_alive;\n "
        },
        {
            "sha": "628bd4642f4762e65bfc4e5b18929ba38fce7c7b",
            "filename": "tensorflow/core/framework/local_rendezvous.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Flocal_rendezvous.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Flocal_rendezvous.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Flocal_rendezvous.h?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -82,7 +82,7 @@ class LocalRendezvous {\n     Item* tail = nullptr;\n   };\n \n-  typedef gtl::FlatMap<uint64, ItemQueue> Table;\n+  typedef gtl::FlatMap<uint64_t, ItemQueue> Table;\n \n   const int num_buckets_;\n   // Pointer to the owner class of this LocalRendezvous if it is refcounted,"
        },
        {
            "sha": "4fc2b86e18f156744324b436a037c5e3c0cdaaa4",
            "filename": "tensorflow/core/framework/log_memory.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Flog_memory.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Flog_memory.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Flog_memory.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -19,7 +19,7 @@ limitations under the License.\n \n namespace tensorflow {\n \n-const string LogMemory::kLogMemoryLabel = \"__LOG_MEMORY__\";\n+const std::string LogMemory::kLogMemoryLabel = \"__LOG_MEMORY__\";\n \n bool LogMemory::IsEnabled() { return VLOG_IS_ON(2); }\n \n@@ -28,23 +28,23 @@ namespace {\n // Write the proto entry to LOG(INFO).\n template <typename T>\n void OutputToLog(const T& proto) {\n-  string type_name(proto.GetTypeName());\n+  std::string type_name(proto.GetTypeName());\n   const size_t index = type_name.find_last_of('.');\n-  if (index != string::npos) type_name = type_name.substr(index + 1);\n+  if (index != std::string::npos) type_name = type_name.substr(index + 1);\n   LOG(INFO) << LogMemory::kLogMemoryLabel << \" \" << type_name << \" { \"\n             << proto.ShortDebugString() << \" }\";\n }\n \n }  // namespace\n \n-void LogMemory::RecordStep(const int64_t step_id, const string& handle) {\n+void LogMemory::RecordStep(const int64_t step_id, const std::string& handle) {\n   MemoryLogStep step;\n   step.set_step_id(step_id);\n   step.set_handle(handle);\n   OutputToLog(step);\n }\n \n-void LogMemory::RecordTensorAllocation(const string& kernel_name,\n+void LogMemory::RecordTensorAllocation(const std::string& kernel_name,\n                                        const int64_t step_id,\n                                        const Tensor& tensor) {\n   MemoryLogTensorAllocation allocation;\n@@ -55,14 +55,14 @@ void LogMemory::RecordTensorAllocation(const string& kernel_name,\n }\n \n void LogMemory::RecordTensorDeallocation(const int64_t allocation_id,\n-                                         const string& allocator_name) {\n+                                         const std::string& allocator_name) {\n   MemoryLogTensorDeallocation deallocation;\n   deallocation.set_allocation_id(allocation_id);\n   deallocation.set_allocator_name(allocator_name);\n   OutputToLog(deallocation);\n }\n \n-void LogMemory::RecordTensorOutput(const string& kernel_name,\n+void LogMemory::RecordTensorOutput(const std::string& kernel_name,\n                                    const int64_t step_id, const int index,\n                                    const Tensor& tensor) {\n   MemoryLogTensorOutput output;\n@@ -73,7 +73,7 @@ void LogMemory::RecordTensorOutput(const string& kernel_name,\n   OutputToLog(output);\n }\n \n-void LogMemory::RecordRawAllocation(const string& operation,\n+void LogMemory::RecordRawAllocation(const std::string& operation,\n                                     const int64_t step_id, size_t num_bytes,\n                                     void* ptr, Allocator* allocator) {\n   MemoryLogRawAllocation allocation;\n@@ -86,7 +86,7 @@ void LogMemory::RecordRawAllocation(const string& operation,\n   OutputToLog(allocation);\n }\n \n-void LogMemory::RecordRawDeallocation(const string& operation,\n+void LogMemory::RecordRawDeallocation(const std::string& operation,\n                                       const int64_t step_id, void* ptr,\n                                       Allocator* allocator, bool deferred) {\n   MemoryLogRawDeallocation deallocation;"
        },
        {
            "sha": "d10b4d555fd00f76f70654e2b539451af173f97a",
            "filename": "tensorflow/core/framework/logging.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Flogging.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Flogging.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Flogging.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -36,13 +36,13 @@ bool RegisterListener(void (*listener)(const char*)) {\n   return true;\n }\n \n-bool LogToListeners(string msg, string end) {\n+bool LogToListeners(std::string msg, std::string end) {\n   auto listeners = logging::GetListeners();\n   if (listeners->empty()) {\n     return false;\n   }\n \n-  string ended_msg = absl::StrCat(msg, end);\n+  std::string ended_msg = absl::StrCat(msg, end);\n \n   for (auto& listener : *listeners) {\n     listener(ended_msg.c_str());"
        },
        {
            "sha": "ccc167ca91474ecf1aa882a40ce803400dc523a1",
            "filename": "tensorflow/core/framework/lookup_interface.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Flookup_interface.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Flookup_interface.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Flookup_interface.h?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -133,7 +133,7 @@ class LookupInterface : public ResourceBase {\n   absl::Status CheckFindArguments(const Tensor& keys,\n                                   const Tensor& default_value);\n \n-  string DebugString() const override {\n+  std::string DebugString() const override {\n     return absl::StrCat(\"A lookup table of size: \", size());\n   }\n "
        },
        {
            "sha": "b08d16866e1a16a18210b5a7c78a22570241939f",
            "filename": "tensorflow/core/framework/memory_types.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fmemory_types.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fmemory_types.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fmemory_types.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -43,7 +43,7 @@ int GetTotal(const NameRangeMap& name_map) {\n // to DEVICE_MEMORY except those args in host_memory_args.  Removes\n // elements of host_memory_args that were used.\n void MemoryTypesHelper(const NameRangeMap& name_map,\n-                       std::vector<string>* host_memory_args,\n+                       std::vector<std::string>* host_memory_args,\n                        MemoryTypeVector* memory_types) {\n   // Update args that have been marked as in \"HOST_MEMORY\".\n   size_t keep = 0;\n@@ -62,7 +62,7 @@ void MemoryTypesHelper(const NameRangeMap& name_map,\n   host_memory_args->resize(keep);\n }\n \n-bool IsFunctionCallOp(const string& op_type) {\n+bool IsFunctionCallOp(const std::string& op_type) {\n   return op_type == \"SymbolicGradient\" || op_type == \"PartitionedCall\" ||\n          op_type == \"StatefulPartitionedCall\" || op_type == \"While\" ||\n          op_type == \"StatelessWhile\";\n@@ -126,7 +126,8 @@ absl::Status MemoryTypesForNode(const OpRegistryInterface* op_registry,\n \n     // Fills in host memory types based on the kernel def.\n     const auto& from_proto = kdef->host_memory_arg();\n-    std::vector<string> host_memory_args(from_proto.begin(), from_proto.end());\n+    std::vector<std::string> host_memory_args(from_proto.begin(),\n+                                              from_proto.end());\n     MemoryTypesHelper(inp_names, &host_memory_args, inp_mtypes);\n     MemoryTypesHelper(out_names, &host_memory_args, out_mtypes);\n     if (!host_memory_args.empty()) {\n@@ -155,7 +156,7 @@ absl::Status MemoryTypesForNode(const OpRegistryInterface* op_registry,\n     }\n   }\n \n-  std::vector<int32> hostmem_attr;\n+  std::vector<int32_t> hostmem_attr;\n   if (TryGetNodeAttr(ndef, \"_input_hostmem\", &hostmem_attr)) {\n     for (int32_t i : hostmem_attr) {\n       if (0 <= i && i < inp_mtypes->size()) {"
        },
        {
            "sha": "c55d7e46a89140dcdf558738fdd2ae1c0ca188b3",
            "filename": "tensorflow/core/framework/metrics.cc",
            "status": "modified",
            "additions": 61,
            "deletions": 56,
            "changes": 117,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fmetrics.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fmetrics.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fmetrics.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -305,7 +305,7 @@ auto* tf_data_pipeline_processing_time = tsl::monitoring::Gauge<double, 1>::New(\n     \"in microseconds\",\n     \"id\");\n \n-auto* tf_data_auto_shard = tsl::monitoring::Gauge<int64, 2>::New(\n+auto* tf_data_auto_shard = tsl::monitoring::Gauge<int64_t, 2>::New(\n     \"/tensorflow/data/autoshard\", \"tf.data autoshard statistics.\", \"id\",\n     \"name\");\n \n@@ -490,63 +490,65 @@ std::string GraphOptimizationSourceMapping(GraphOptimizationSource source) {\n   }\n }\n \n-void RecordTFDataFetchOp(const string& name) {\n+void RecordTFDataFetchOp(const std::string& name) {\n   tf_data_fetch_op_counter->GetCell(name)->IncrementBy(1);\n }\n \n-void RecordTFDataAutotune(const string& name) {\n+void RecordTFDataAutotune(const std::string& name) {\n   tf_data_autotune_counter->GetCell(name)->IncrementBy(1);\n }\n \n tsl::monitoring::CounterCell* GetTFDataBytesConsumedCounter(\n-    const string& name) {\n+    const std::string& name) {\n   return tf_data_bytes_consumed_counter->GetCell(name);\n }\n \n tsl::monitoring::CounterCell* GetTFDataBytesProducedCounter(\n-    const string& name) {\n+    const std::string& name) {\n   return tf_data_bytes_produced_counter->GetCell(name);\n }\n \n-tsl::monitoring::CounterCell* GetTFDataBytesReadCounter(const string& name) {\n+tsl::monitoring::CounterCell* GetTFDataBytesReadCounter(\n+    const std::string& name) {\n   return tf_data_bytes_read_counter->GetCell(name);\n }\n \n-tsl::monitoring::CounterCell* GetTFDataElementsCounter(const string& name) {\n+tsl::monitoring::CounterCell* GetTFDataElementsCounter(\n+    const std::string& name) {\n   return tf_data_elements_counter->GetCell(name);\n }\n \n tsl::monitoring::GaugeCell<std::function<std::string()>>* GetTFDataModelGauge(\n-    const string& id) {\n+    const std::string& id) {\n   return tf_data_model_gauge->GetCell(id);\n }\n \n tsl::monitoring::GaugeCell<double>* GetTFDataPipelineProcessingTimeGauge(\n-    const string& id) {\n+    const std::string& id) {\n   return tf_data_pipeline_processing_time->GetCell(id);\n }\n \n void RecordTFDataBytesFetched(int64_t num_bytes) {\n   tf_data_bytes_fetched_counter->GetCell()->IncrementBy(num_bytes);\n }\n \n-void RecordTFDataExperiment(const string& name) {\n+void RecordTFDataExperiment(const std::string& name) {\n   tf_data_experiment_counter->GetCell(name)->IncrementBy(1);\n }\n \n-void RecordTFDataExperimentLive(const string& name) {\n+void RecordTFDataExperimentLive(const std::string& name) {\n   tf_data_experiment_live_counter->GetCell(name)->IncrementBy(1);\n }\n \n-void RecordTFDataExperimentOptIn(const string& name) {\n+void RecordTFDataExperimentOptIn(const std::string& name) {\n   tf_data_experiment_opt_in_counter->GetCell(name)->IncrementBy(1);\n }\n \n-void RecordTFDataExperimentOptOut(const string& name) {\n+void RecordTFDataExperimentOptOut(const std::string& name) {\n   tf_data_experiment_opt_out_counter->GetCell(name)->IncrementBy(1);\n }\n \n-void RecordTFDataFingerprint(const string& name) {\n+void RecordTFDataFingerprint(const std::string& name) {\n   tf_data_fingerprint_counter->GetCell(name)->IncrementBy(1);\n }\n \n@@ -557,18 +559,18 @@ void RecordTFDataServiceRuntimeCompressionDecision(bool compression_disabled) {\n       ->IncrementBy(1);\n }\n \n-void RecordTFDataServiceCompressionAction(const string& action) {\n+void RecordTFDataServiceCompressionAction(const std::string& action) {\n   tf_data_service_compression->GetCell(action)->IncrementBy(1);\n }\n \n-void RecordTFDataServiceGetElementDuration(const string& data_transfer_protocol,\n-                                           uint64 duration_us) {\n+void RecordTFDataServiceGetElementDuration(\n+    const std::string& data_transfer_protocol, uint64_t duration_us) {\n   tf_data_service_get_element_duration_usecs_histogram\n       ->GetCell(data_transfer_protocol)\n       ->Add(duration_us);\n }\n \n-void RecordTFDataGetNextDuration(uint64 duration_us) {\n+void RecordTFDataGetNextDuration(uint64_t duration_us) {\n   static auto* tf_data_get_next_duration_cell =\n       tf_data_get_next_duration_usecs_histogram->GetCell();\n   tf_data_get_next_duration_cell->Add(duration_us);\n@@ -586,25 +588,25 @@ void RecordTFDataAutotuneMaxBufferBudgetRatio(const double ratio) {\n   tf_data_buffered_vs_budget_ratio_histogram_cell->Add(ratio);\n }\n \n-void RecordTFDataIteratorBusy(uint64 duration_us) {\n+void RecordTFDataIteratorBusy(uint64_t duration_us) {\n   static auto* tf_data_iterator_busy_cell =\n       tf_data_iterator_busy_counter->GetCell();\n   tf_data_iterator_busy_cell->IncrementBy(duration_us);\n }\n \n-void RecordTFDataIteratorLifetime(uint64 duration_us) {\n+void RecordTFDataIteratorLifetime(uint64_t duration_us) {\n   static auto* tf_data_iterator_lifetime_cell =\n       tf_data_iterator_lifetime_counter->GetCell();\n   tf_data_iterator_lifetime_cell->IncrementBy(duration_us);\n }\n \n-void RecordTFDataIteratorGap(uint64 duration_us) {\n+void RecordTFDataIteratorGap(uint64_t duration_us) {\n   static auto* tf_data_iterator_gap_msec_histogram_cell =\n       tf_data_iterator_gap_msec_histogram->GetCell();\n   tf_data_iterator_gap_msec_histogram_cell->Add(duration_us * 0.001);\n }\n \n-void RecordTFDataOptimization(const string& name, int64_t num_changes) {\n+void RecordTFDataOptimization(const std::string& name, int64_t num_changes) {\n   tf_data_optimization_counter->GetCell(name)->IncrementBy(num_changes);\n }\n \n@@ -641,24 +643,24 @@ void RecordTFDataServiceClientIterators(\n }\n \n void RecordTFDataServiceDataTransferProtocolUsed(\n-    const string& data_transfer_protocol, bool user_specified) {\n+    const std::string& data_transfer_protocol, bool user_specified) {\n   std::string nature = user_specified ? \"specified\" : \"default\";\n   tf_data_service_data_transfer_protocol_used_by_nature\n       ->GetCell(data_transfer_protocol, nature)\n       ->IncrementBy(1);\n }\n \n void RecordTFDataServiceDataTransferProtocolFallback(\n-    const string& data_transfer_protocol, error::Code code,\n-    const string& error_message) {\n+    const std::string& data_transfer_protocol, error::Code code,\n+    const std::string& error_message) {\n   tf_data_service_data_transfer_protocol_fallback\n       ->GetCell(data_transfer_protocol, error::Code_Name(code), error_message)\n       ->IncrementBy(1);\n }\n \n void RecordTFDataServiceDataTransferProtocolError(\n-    const string& data_transfer_protocol, error::Code code,\n-    const string& error_message) {\n+    const std::string& data_transfer_protocol, error::Code code,\n+    const std::string& error_message) {\n   tf_data_service_data_transfer_protocol_error\n       ->GetCell(data_transfer_protocol, error::Code_Name(code), error_message)\n       ->IncrementBy(1);\n@@ -688,7 +690,8 @@ void RecordTFDataServiceOptimalNumberOfWorkers(int64_t number_of_workers) {\n   tf_data_service_optimal_number_of_workers->GetCell()->Set(number_of_workers);\n }\n \n-void RecordTFDataFilename(const string& name, const string& filename) {\n+void RecordTFDataFilename(const std::string& name,\n+                          const std::string& filename) {\n   tf_data_filename_counter->GetCell(name, filename)->IncrementBy(1);\n }\n \n@@ -697,7 +700,7 @@ void RecordTFDataFileLoggerAttempts() {\n }\n \n void RecordTFDataFileLoggerErrors(error::Code error_code,\n-                                  const string& error_message) {\n+                                  const std::string& error_message) {\n   tf_data_file_logger_errors_counter\n       ->GetCell(error::Code_Name(error_code), error_message)\n       ->IncrementBy(1);\n@@ -710,47 +713,48 @@ void RecordTFDataFileLoggerAttemptedNumFiles(size_t num_files) {\n \n void RecordTFDataFileLoggerErrorsNumFiles(size_t num_files,\n                                           error::Code error_code,\n-                                          const string& error_message) {\n+                                          const std::string& error_message) {\n   tf_data_file_logger_errors_num_files_counter\n       ->GetCell(error::Code_Name(error_code), error_message)\n       ->IncrementBy(num_files);\n }\n \n-void RecordTFDataAutoShard(const string& id, data::AutoShardPolicy policy,\n-                           int64 num_workers, int64 num_replicas) {\n+void RecordTFDataAutoShard(const std::string& id, data::AutoShardPolicy policy,\n+                           int64_t num_workers, int64_t num_replicas) {\n   tf_data_auto_shard->GetCell(id, \"policy\")->Set(static_cast<int64_t>(policy));\n   tf_data_auto_shard->GetCell(id, \"num_workers\")->Set(num_workers);\n   tf_data_auto_shard->GetCell(id, \"num_replicas\")->Set(num_replicas);\n }\n \n void RecordTFDataAutoShardRewriteBatchSize(\n-    bool eligible, const std::vector<string>& ineligible_reason) {\n+    bool eligible, const std::vector<std::string>& ineligible_reason) {\n   tf_data_auto_shard_rewrite_batch_size_eligible\n       ->GetCell(eligible ? \"true\" : \"false\")\n       ->IncrementBy(1);\n-  for (const string& reason : ineligible_reason) {\n+  for (const std::string& reason : ineligible_reason) {\n     tf_data_auto_shard_rewrite_batch_size_reason->GetCell(reason)->IncrementBy(\n         1);\n   }\n }\n \n-void RecordTFDataAutotuneStoppingCriteria(const string& name) {\n+void RecordTFDataAutotuneStoppingCriteria(const std::string& name) {\n   tf_data_autotune_stopping_criteria_counter->GetCell(name)->IncrementBy(1);\n }\n \n-void RecordTFDataDebug(const string& event) {\n+void RecordTFDataDebug(const std::string& event) {\n   tf_data_debug->GetCell(event)->IncrementBy(1);\n }\n \n-void RecordTFDataError(const string& error_type, const string& status_code) {\n+void RecordTFDataError(const std::string& error_type,\n+                       const std::string& status_code) {\n   tf_data_error->GetCell(error_type, status_code)->IncrementBy(1);\n }\n \n void RecordTFDataFrameworkType(const std::string& framework_type) {\n   tf_data_framework_type->GetCell(framework_type)->IncrementBy(1);\n }\n \n-void RecordParseDenseFeature(int64 num_features) {\n+void RecordParseDenseFeature(int64_t num_features) {\n   static auto* parse_dense_feature_counter_cell =\n       parse_dense_feature_counter->GetCell();\n   parse_dense_feature_counter_cell->IncrementBy(num_features);\n@@ -797,7 +801,7 @@ void UpdateAotBefMlirLoadCount() {\n   aot_bef_mlir_load_count_cell->IncrementBy(1);\n }\n \n-void UpdateGraphExecTime(const uint64 running_time_usecs) {\n+void UpdateGraphExecTime(const uint64_t running_time_usecs) {\n   if (running_time_usecs > 0) {\n     static auto* graph_runs_cell = graph_runs->GetCell();\n     static auto* graph_run_time_usecs_cell = graph_run_time_usecs->GetCell();\n@@ -809,13 +813,13 @@ void UpdateGraphExecTime(const uint64 running_time_usecs) {\n   }\n }\n \n-void UpdateGraphPendingQueueLength(uint64 len) {\n+void UpdateGraphPendingQueueLength(uint64_t len) {\n   static auto* graph_pending_queue_length_cell =\n       graph_pending_queue_length_histogram->GetCell();\n   graph_pending_queue_length_cell->Add(len);\n }\n \n-void UpdateGraphBuildTime(const uint64 running_time_usecs) {\n+void UpdateGraphBuildTime(const uint64_t running_time_usecs) {\n   if (running_time_usecs > 0) {\n     static auto* build_graph_calls_cell = build_graph_calls->GetCell();\n     static auto* build_graph_time_usecs_cell =\n@@ -825,7 +829,7 @@ void UpdateGraphBuildTime(const uint64 running_time_usecs) {\n   }\n }\n \n-void UpdateFunctionGraphOptimizationTime(const uint64 running_time_usecs) {\n+void UpdateFunctionGraphOptimizationTime(const uint64_t running_time_usecs) {\n   if (running_time_usecs > 0) {\n     static auto* function_graph_optimization_time_usecs_cell =\n         function_graph_optimization_time_usecs->GetCell();\n@@ -834,7 +838,7 @@ void UpdateFunctionGraphOptimizationTime(const uint64 running_time_usecs) {\n   }\n }\n \n-void UpdateFunctionGraphOptimizationSavingTime(const uint64 saving_time_usecs,\n+void UpdateFunctionGraphOptimizationSavingTime(const uint64_t saving_time_usecs,\n                                                GraphOptimizationSource source) {\n   if (saving_time_usecs > 0) {\n     std::string mapped_source = GraphOptimizationSourceMapping(source);\n@@ -845,7 +849,7 @@ void UpdateFunctionGraphOptimizationSavingTime(const uint64 saving_time_usecs,\n   }\n }\n \n-uint64 GetFunctionGraphOptimizationSavingTimeUsecs(\n+uint64_t GetFunctionGraphOptimizationSavingTimeUsecs(\n     GraphOptimizationSource source) {\n   std::string mapped_source = GraphOptimizationSourceMapping(source);\n   return graph_optimization_saving_time_usecs->GetCell(mapped_source)->value();\n@@ -904,14 +908,14 @@ int64_t GetFunctionGraphOptimizationCacheLoadCount(\n   return graph_optimization_cache_load_count->GetCell(mapped_source)->value();\n }\n \n-void UpdateTpuVariableDistributionTime(const uint64 distribution_time_usecs) {\n+void UpdateTpuVariableDistributionTime(const uint64_t distribution_time_usecs) {\n   if (distribution_time_usecs > 0) {\n     tpu_variable_distribution_time_usecs->GetCell()->IncrementBy(\n         distribution_time_usecs);\n   }\n }\n \n-void UpdateXlaCompilationTime(const uint64 compilation_time_usecs) {\n+void UpdateXlaCompilationTime(const uint64_t compilation_time_usecs) {\n   if (compilation_time_usecs > 0) {\n     static auto* xla_compilations_cell = xla_compilations->GetCell();\n     static auto* xla_compilation_time_usecs_cell =\n@@ -921,32 +925,32 @@ void UpdateXlaCompilationTime(const uint64 compilation_time_usecs) {\n   }\n }\n \n-void RecordUnusedOutput(const string& op_name) {\n+void RecordUnusedOutput(const std::string& op_name) {\n   graph_unused_outputs->GetCell(op_name)->IncrementBy(1);\n }\n \n-void RecordPipelineProcessingTime(const string& id,\n+void RecordPipelineProcessingTime(const std::string& id,\n                                   double pipeline_processing_time_usec) {\n   GetTFDataPipelineProcessingTimeGauge(id)->Set(pipeline_processing_time_usec);\n }\n \n-void IncrementTestCounter(const string& name, const string& label) {\n+void IncrementTestCounter(const std::string& name, const std::string& label) {\n   test_counters->GetCell(name, label)->IncrementBy(1);\n }\n \n-const tsl::monitoring::CounterCell* TestCounter(const string& name,\n-                                                const string& label) {\n+const tsl::monitoring::CounterCell* TestCounter(const std::string& name,\n+                                                const std::string& label) {\n   return test_counters->GetCell(name, label);\n }\n \n-TestDelta::TestDelta(const string& name, const string& label)\n+TestDelta::TestDelta(const std::string& name, const std::string& label)\n     : cell_(TestCounter(name, label)) {\n   Reset();\n }\n \n void TestDelta::Reset() { last_value_ = cell_->value(); }\n \n-int64 TestDelta::Get() { return cell_->value() - last_value_; }\n+int64_t TestDelta::Get() { return cell_->value() - last_value_; }\n \n void UpdateTfMlirBridgeFirstPhaseCounter(const std::string& bridge_type,\n                                          const std::string& bridge_version,\n@@ -1020,12 +1024,13 @@ void IncrementPhase2XlaCompilerCounter(Phase2XlaCompilerMetric metric) {\n       ->IncrementBy(1);\n }\n \n-void UpdateTpuErrorCounter(const string& op, const string& error_type) {\n+void UpdateTpuErrorCounter(const std::string& op,\n+                           const std::string& error_type) {\n   tpu_op_error_counter->GetCell(op, error_type)->IncrementBy(1);\n }\n \n-void UpdateEagerClientErrorCounter(const string& error_source,\n-                                   const string& error_type) {\n+void UpdateEagerClientErrorCounter(const std::string& error_source,\n+                                   const std::string& error_type) {\n   eager_client_error_counter->GetCell(error_source, error_type)->IncrementBy(1);\n }\n "
        },
        {
            "sha": "4d84c1f615adae2c19d9d9da9cd4436a10653713",
            "filename": "tensorflow/core/framework/metrics.h",
            "status": "modified",
            "additions": 64,
            "deletions": 62,
            "changes": 126,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fmetrics.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fmetrics.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fmetrics.h?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -39,68 +39,68 @@ enum class GraphOptimizationSource {\n // Records when a data-fetching tf.data operation is executed.\n //\n // The `name` argument identifies the operation type (e.g. \"ToSingleElementOp\").\n-void RecordTFDataFetchOp(const string& name);\n+void RecordTFDataFetchOp(const std::string& name);\n \n // Records that a tf.data.Dataset executed by the program used autotuning.\n //\n // The `name` argument identifies the Dataset type (e.g. \"ParallelMap\").\n-void RecordTFDataAutotune(const string& name);\n+void RecordTFDataAutotune(const std::string& name);\n \n // Returns a counter that can be used to record the number of bytes produced by\n // a tf.data.Dataset.\n //\n // The `name` argument identifies the Dataset type (e.g. \"Batch\" or \"Map\").\n-monitoring::CounterCell* GetTFDataBytesConsumedCounter(const string& name);\n+monitoring::CounterCell* GetTFDataBytesConsumedCounter(const std::string& name);\n \n // Returns a counter that can be used to record the number of bytes produced by\n // a tf.data.Dataset.\n //\n // The `name` argument identifies the Dataset type (e.g. \"Batch\" or \"Map\").\n-monitoring::CounterCell* GetTFDataBytesProducedCounter(const string& name);\n+monitoring::CounterCell* GetTFDataBytesProducedCounter(const std::string& name);\n \n // Returns a counter than can be used to record the number of bytes read from\n // the filesystem by a tf.data.Dataset source.\n //\n // The `name` argument identifies the Dataset type (e.g. \"TFRecordDataset\").\n //\n // TODO(jsimsa): Remove this now that we have GetTFDataBytesConsumedCounter?\n-monitoring::CounterCell* GetTFDataBytesReadCounter(const string& name);\n+monitoring::CounterCell* GetTFDataBytesReadCounter(const std::string& name);\n \n // Returns a counter than can be used to record the number of elements produced\n // by a tf.data.Dataset.\n //\n // The `name` argument identifies the Dataset type (e.g. \"Batch\" or \"Map\").\n-monitoring::CounterCell* GetTFDataElementsCounter(const string& name);\n+monitoring::CounterCell* GetTFDataElementsCounter(const std::string& name);\n \n // Returns a gauge than can be used to record the performance model information.\n //\n // The `id` argument represents the (unique) model ID.\n monitoring::GaugeCell<std::function<std::string()>>* GetTFDataModelGauge(\n-    const string& id);\n+    const std::string& id);\n \n // Records the number of bytes fetched from tf.data.Dataset iterator.\n void RecordTFDataBytesFetched(int64_t num_bytes);\n \n // Records the number of times a tf.data experiment was applied.\n-void RecordTFDataExperiment(const string& name);\n+void RecordTFDataExperiment(const std::string& name);\n \n // Records the number of times a tf.data experiment could have been applied.\n-void RecordTFDataExperimentLive(const string& name);\n+void RecordTFDataExperimentLive(const std::string& name);\n \n // Records the number of times a tf.data experiment was opted into.\n-void RecordTFDataExperimentOptIn(const string& experiment_name);\n+void RecordTFDataExperimentOptIn(const std::string& experiment_name);\n \n // Records the number of times a tf.data experiment was opted out of.\n-void RecordTFDataExperimentOptOut(const string& experiment_name);\n+void RecordTFDataExperimentOptOut(const std::string& experiment_name);\n \n // Records the time (in microseconds) spent generating an element and\n // transferring it over the network for the given protocol.\n-void RecordTFDataServiceGetElementDuration(const string& data_transfer_protocol,\n-                                           uint64 duration_us);\n+void RecordTFDataServiceGetElementDuration(\n+    const std::string& data_transfer_protocol, uint64_t duration_us);\n \n // Records the time (in microseconds) spent in a single invocation of\n // `ItertatorResource::GetNext()`.\n-void RecordTFDataGetNextDuration(uint64 duration_us);\n+void RecordTFDataGetNextDuration(uint64_t duration_us);\n \n // Records the histogram of ratios of tf.data autotune algorithm used RAM over\n // the ram budget.\n@@ -115,34 +115,34 @@ void RecordTFDataAutotuneMaxBufferBudgetRatio(const double ratio);\n //\n // The `name` argument identifies the Dataset graph fingerprint,\n // created using GraphHash().\n-void RecordTFDataFingerprint(const string& name);\n+void RecordTFDataFingerprint(const std::string& name);\n \n // Records the event of a tf.data service pipeline getting a runtime\n // compression decision.\n void RecordTFDataServiceRuntimeCompressionDecision(bool compression_decision);\n \n // Records the event of a tf.data service pipeline making the compression\n // related action.\n-void RecordTFDataServiceCompressionAction(const string& action);\n+void RecordTFDataServiceCompressionAction(const std::string& action);\n \n // Records the time (in microseconds) during which `IteratorResource` was busy\n // processing at least one `GetNext()` request.\n-void RecordTFDataIteratorBusy(uint64 duration_us);\n+void RecordTFDataIteratorBusy(uint64_t duration_us);\n \n // Records the time (in microseconds) between `IteratorResource` receiving the\n // first `GetNext()` request and responding to the last `GetNext()` request.\n-void RecordTFDataIteratorLifetime(uint64 duration_us);\n+void RecordTFDataIteratorLifetime(uint64_t duration_us);\n \n // Records the time histogram (in microseconds) between `IteratorResource`\n // responding to a `GetNext()` request and receiving the next `GetNext()`\n // request.\n-void RecordTFDataIteratorGap(uint64 duration_us);\n+void RecordTFDataIteratorGap(uint64_t duration_us);\n \n // Records the number of independent graph changes resulting from the\n // application of a tf.data optimization.\n //\n // The `name` argument identifies the optimization (e.g. \"noop_elimination\").\n-void RecordTFDataOptimization(const string& name, int64_t num_changes);\n+void RecordTFDataOptimization(const std::string& name, int64_t num_changes);\n \n // Records that a tf.data service worker has been created.\n void RecordTFDataServiceWorkerCreated();\n@@ -160,21 +160,21 @@ void RecordTFDataServiceClientIterators(\n // `data_transfer_protocol` to get data from the worker server and whether or\n // not the user explicitly specified the protocol.\n void RecordTFDataServiceDataTransferProtocolUsed(\n-    const string& data_transfer_protocol, bool user_specified);\n+    const std::string& data_transfer_protocol, bool user_specified);\n \n // Records that a tf.data service worker client fell back to gRPC rather than\n // use `data_transfer_protocol` because of an error of type `code` with message\n // `error_message`.\n void RecordTFDataServiceDataTransferProtocolFallback(\n-    const string& data_transfer_protocol, error::Code code,\n-    const string& error_message);\n+    const std::string& data_transfer_protocol, error::Code code,\n+    const std::string& error_message);\n \n // Records that a tf.data service worker client got an error of non-retriable\n // type `code` with message `error_message` when trying to transfer data over\n // `data_transfer_protocol`.\n void RecordTFDataServiceDataTransferProtocolError(\n-    const string& data_transfer_protocol, error::Code code,\n-    const string& error_message);\n+    const std::string& data_transfer_protocol, error::Code code,\n+    const std::string& error_message);\n \n // Records tf.data service cross-trainer cache queries.\n void RecordTFDataServiceCrossTrainerCacheQuery(bool cache_hit);\n@@ -195,15 +195,15 @@ void RecordTFDataServiceOptimalNumberOfWorkers(int64_t number_of_workers);\n // Records the file name read by a tf.data Dataset.\n //\n // The `name` argument identifies the Dataset type (e.g. \"TFRecordDataset\").\n-void RecordTFDataFilename(const string& name, const string& filename);\n+void RecordTFDataFilename(const std::string& name, const std::string& filename);\n \n // Records the total attempts made by file logger.\n void RecordTFDataFileLoggerAttempts();\n \n // Records an error of type `code` with message `error_message` encountered by\n // file logger.\n void RecordTFDataFileLoggerErrors(error::Code code,\n-                                  const string& error_message);\n+                                  const std::string& error_message);\n \n // Records the total number of files attempted to be logged by file logger.\n void RecordTFDataFileLoggerAttemptedNumFiles(size_t num_files);\n@@ -212,15 +212,15 @@ void RecordTFDataFileLoggerAttemptedNumFiles(size_t num_files);\n // `code` with message `error_message` during logging by file logger with this\n // error code.\n void RecordTFDataFileLoggerErrorsNumFiles(size_t num_files, error::Code code,\n-                                          const string& error_message);\n+                                          const std::string& error_message);\n \n // Records statistics of tf.data auto sharding.\n //\n // The `id` is a unique identifier of the input pipeline. The `policy`\n // identifies the auto-sharding policy used, the `num_workers` identifies the\n // number of workers, and `num_replicas` identifies the number of replicas.\n-void RecordTFDataAutoShard(const string& id, data::AutoShardPolicy policy,\n-                           int64 num_workers, int64 num_replicas);\n+void RecordTFDataAutoShard(const std::string& id, data::AutoShardPolicy policy,\n+                           int64_t num_workers, int64_t num_replicas);\n \n // Records statistics of whether we can rewrite batch size in tf.data auto\n // sharding.\n@@ -229,26 +229,27 @@ void RecordTFDataAutoShard(const string& id, data::AutoShardPolicy policy,\n // indicates whether the input pipeline is eligible for the rewrite. The\n // `ineligible_reason` is the reason if the input pipeline is ineligible.\n void RecordTFDataAutoShardRewriteBatchSize(\n-    bool eligible, const std::vector<string>& ineligible_reason);\n+    bool eligible, const std::vector<std::string>& ineligible_reason);\n \n // Records the number of times each tf.data autotuning algorithm stopping\n // criterion is met.\n-void RecordTFDataAutotuneStoppingCriteria(const string& name);\n+void RecordTFDataAutotuneStoppingCriteria(const std::string& name);\n \n // Records the number of times this event occured, for debugging.\n-void RecordTFDataDebug(const string& event);\n+void RecordTFDataDebug(const std::string& event);\n \n // Records the number of times an error of this type occurred with this status\n // code.\n-void RecordTFDataError(const string& error_type, const string& error_code);\n+void RecordTFDataError(const std::string& error_type,\n+                       const std::string& error_code);\n \n // Records the framework type used to build the tf.data.Dataset.\n void RecordTFDataFrameworkType(const std::string& framework_type);\n \n // Records the number of times tf.data file logger encountered an error of this\n // type occurred with this status code.\n-void RecordTFDataFileLoggerError(const string& error_type,\n-                                 const string& error_code);\n+void RecordTFDataFileLoggerError(const std::string& error_type,\n+                                 const std::string& error_code);\n \n // Records parsing of dense tensor features.\n void RecordParseDenseFeature(int64_t num_features);\n@@ -266,14 +267,14 @@ void RecordGraphOutputTensors(const size_t size);\n // Records the number of cores requested by graphs with XLA SPMD enabled.\n void RecordTPUXlaSpmdCoresPerReplica(int64_t cores_per_replica);\n \n-void UpdateGraphExecTime(const uint64 running_time_usecs);\n-void UpdateGraphPendingQueueLength(uint64 len);\n+void UpdateGraphExecTime(const uint64_t running_time_usecs);\n+void UpdateGraphPendingQueueLength(uint64_t len);\n \n // Records that one output of an op of type `op_name` was unused.\n-void RecordUnusedOutput(const string& op_name);\n+void RecordUnusedOutput(const std::string& op_name);\n \n // Records the pipeline processing time in microseconds\n-void RecordPipelineProcessingTime(const string& id,\n+void RecordPipelineProcessingTime(const std::string& id,\n                                   double pipeline_processing_time_usec);\n \n // Increments the count of binaries loaded from the persistent cache.\n@@ -295,17 +296,17 @@ void UpdateAotBefMlirLoadCount();\n // When executing eagerly, this will not record any activity.\n //\n // TODO(jtkeeling): Should we record building/optimizing tf.functions?\n-void UpdateGraphBuildTime(const uint64 running_time_usecs);\n+void UpdateGraphBuildTime(const uint64_t running_time_usecs);\n \n // Updates the metric stored for time spent optimizing function graphs.\n-void UpdateFunctionGraphOptimizationTime(const uint64 running_time_usecs);\n+void UpdateFunctionGraphOptimizationTime(const uint64_t running_time_usecs);\n \n // Updates the metric stored for time saved by caching graph optimization.\n-void UpdateFunctionGraphOptimizationSavingTime(uint64 saving_time_usec,\n+void UpdateFunctionGraphOptimizationSavingTime(uint64_t saving_time_usec,\n                                                GraphOptimizationSource source);\n \n // Retrieves the total time saved by the graph optimization caching.\n-uint64 GetFunctionGraphOptimizationSavingTimeUsecs(\n+uint64_t GetFunctionGraphOptimizationSavingTimeUsecs(\n     GraphOptimizationSource source);\n \n // Increments the hit count for the graph optimization cache.\n@@ -463,10 +464,10 @@ class ScopedCounter final {\n \n   // Returns duration of the current interval in case the timer has started.\n   // Returns nullopt otherwise.\n-  std::optional<uint64> DurationMicroSec() const {\n-    return started_ ? std::optional<uint64>(accumulated_time_ +\n-                                            Env::Default()->NowMicros() -\n-                                            start_time_)\n+  std::optional<uint64_t> DurationMicroSec() const {\n+    return started_ ? std::optional<uint64_t>(accumulated_time_ +\n+                                              Env::Default()->NowMicros() -\n+                                              start_time_)\n                     : std::nullopt;\n   }\n \n@@ -492,7 +493,7 @@ class ScopedCounter final {\n  private:\n   template <std::size_t... S>\n   void ReportInternal(std::index_sequence<S...>) {\n-    uint64 time_interval = Env::Default()->NowMicros() - start_time_;\n+    uint64_t time_interval = Env::Default()->NowMicros() - start_time_;\n     time_interval += accumulated_time_;\n     if (time_interval > 0) {\n       counter_->GetCell(labels_[S]...)->IncrementBy(time_interval);\n@@ -508,41 +509,42 @@ class ScopedCounter final {\n   monitoring::Counter<NumLabels>* counter_;\n   std::array<std::string, NumLabels> labels_;\n   bool started_{false};\n-  uint64 start_time_;\n-  uint64 accumulated_time_;\n+  uint64_t start_time_;\n+  uint64_t accumulated_time_;\n };\n \n // Returns a counter used to capture timing metrics for graph optimization\n // passes.\n monitoring::Counter<2>* GetGraphOptimizationCounter();\n \n // Updates metrics for time to distribute variables to all TPU hosts.\n-void UpdateTpuVariableDistributionTime(const uint64 distribution_time_usecs);\n+void UpdateTpuVariableDistributionTime(const uint64_t distribution_time_usecs);\n \n // Updates the metrics stored about time XLA spents compiling graphs.\n-void UpdateXlaCompilationTime(const uint64 compilation_time_usecs);\n+void UpdateXlaCompilationTime(const uint64_t compilation_time_usecs);\n \n // Increments (by 1) a simple integer counter that is exposed for testing.\n-void IncrementTestCounter(const string& name, const string& label);\n+void IncrementTestCounter(const std::string& name, const std::string& label);\n \n // Read-only access to a counter for testing.\n-const monitoring::CounterCell* TestCounter(const string& name,\n-                                           const string& label);\n+const monitoring::CounterCell* TestCounter(const std::string& name,\n+                                           const std::string& label);\n \n // Read-only wrapper for a TestCounter to track increments between calls.\n class TestDelta {\n  public:\n-  TestDelta(const string& name, const string& label);\n+  TestDelta(const std::string& name, const std::string& label);\n   void Reset();\n-  int64 Get();\n+  int64_t Get();\n \n  private:\n   const monitoring::CounterCell* cell_;\n-  int64 last_value_;\n+  int64_t last_value_;\n };\n-void UpdateTpuErrorCounter(const string& op, const string& error_type);\n-void UpdateEagerClientErrorCounter(const string& error_source,\n-                                   const string& error_type);\n+void UpdateTpuErrorCounter(const std::string& op,\n+                           const std::string& error_type);\n+void UpdateEagerClientErrorCounter(const std::string& error_source,\n+                                   const std::string& error_type);\n \n }  // namespace metrics\n }  // namespace tensorflow"
        },
        {
            "sha": "0d05c8d72b69d7ba762608bb848344463e43500b",
            "filename": "tensorflow/core/framework/model.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 18,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fmodel.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fmodel.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fmodel.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -240,7 +240,7 @@ bool AreAllParametersMax(const Model::ModelParameters& parameters) {\n }\n \n // Records the ram usage of hill climbing algorithm.\n-void RecordAutotuneRamUsage(int64 ram_budget, double max_buffered_bytes) {\n+void RecordAutotuneRamUsage(int64_t ram_budget, double max_buffered_bytes) {\n   if (ram_budget == 0) {\n     return;\n   }\n@@ -1227,8 +1227,8 @@ class UnknownRatio : public Node {\n   // The processing time is the sum of the self processing time and the product\n   // of the ratio estimate and the sum of processing times of inputs.\n   void TotalProcessingTimeLocked(\n-      absl::flat_hash_map<string, double>* processing_times,\n-      absl::flat_hash_map<string, double>* total_processing_times) override\n+      absl::flat_hash_map<std::string, double>* processing_times,\n+      absl::flat_hash_map<std::string, double>* total_processing_times) override\n       TF_SHARED_LOCKS_REQUIRED(mu_) {\n     double self_processing_time = SelfProcessingTimeLocked();\n     if (processing_times) {\n@@ -1400,13 +1400,13 @@ class AsyncUnknownRatio : public AsyncRatio {\n \n thread_local int64_t Node::work_start_;\n \n-std::shared_ptr<Parameter> MakeParameter(const string& name,\n+std::shared_ptr<Parameter> MakeParameter(const std::string& name,\n                                          std::shared_ptr<SharedState> state,\n                                          double min, double max) {\n   return std::make_shared<Parameter>(name, state, min, max);\n }\n \n-std::shared_ptr<Parameter> MakeParameter(const string& name,\n+std::shared_ptr<Parameter> MakeParameter(const std::string& name,\n                                          std::shared_ptr<SharedState> state,\n                                          double min, double max, double value) {\n   std::shared_ptr<Parameter> parameter =\n@@ -1415,7 +1415,7 @@ std::shared_ptr<Parameter> MakeParameter(const string& name,\n   return parameter;\n }\n \n-std::shared_ptr<Parameter> MakeNonTunableParameter(const string& name,\n+std::shared_ptr<Parameter> MakeNonTunableParameter(const std::string& name,\n                                                    double value) {\n   return std::make_shared<Parameter>(name, nullptr, /*min=*/value,\n                                      /*max=*/value);\n@@ -1649,8 +1649,8 @@ Node::ModelParameters Node::CollectNodeTunableParameters() const {\n   return parameters;\n }\n \n-string Node::DebugString() const {\n-  absl::flat_hash_map<string, string> debug_strings;\n+std::string Node::DebugString() const {\n+  absl::flat_hash_map<std::string, std::string> debug_strings;\n   tf_shared_lock l(mu_);\n   // Build up the debug string from the leaves of the nodes tree to the root.\n   for (const auto& node :\n@@ -2035,9 +2035,10 @@ void Node::CollectTunableParametersHelper(\n   }\n }\n \n-void Node::DebugStringHelper(absl::flat_hash_map<string, string>* debug_strings)\n-    const TF_SHARED_LOCKS_REQUIRED(mu_) {\n-  string result;\n+void Node::DebugStringHelper(\n+    absl::flat_hash_map<std::string, std::string>* debug_strings) const\n+    TF_SHARED_LOCKS_REQUIRED(mu_) {\n+  std::string result;\n   absl::StrAppend(&result, long_name(), \":\\n\");\n   absl::StrAppend(&result, \"  autotune=\", autotune_.load(), \"\\n\");\n   absl::StrAppend(&result, \"  buffered_bytes=\", buffered_bytes_.load(), \"\\n\");\n@@ -2047,7 +2048,7 @@ void Node::DebugStringHelper(absl::flat_hash_map<string, string>* debug_strings)\n   absl::StrAppend(&result, \"  bytes_produced=\", bytes_produced_.load(), \"\\n\");\n   absl::StrAppend(&result, \"  processing_time=\", processing_time_.load(), \"\\n\");\n   absl::StrAppend(&result, \"  num_elements=\", num_elements_.load(), \"\\n\");\n-  string inputs;\n+  std::string inputs;\n   for (auto& input : inputs_) {\n     absl::StrAppend(&inputs, input->long_name(), \",\");\n   }\n@@ -2080,7 +2081,7 @@ std::shared_ptr<Node> Node::SnapshotHelper(\n     {\n       mutex_lock l2(cloned_current->mu_);\n       cloned_current->parameters_ =\n-          absl::flat_hash_map<string, std::shared_ptr<Parameter>>();\n+          absl::flat_hash_map<std::string, std::shared_ptr<Parameter>>();\n       for (const auto& [parameter_name, parameter_ptr] : parameters_) {\n         cloned_current->parameters_[parameter_name] =\n             std::make_shared<Parameter>(parameter_ptr);\n@@ -2257,7 +2258,7 @@ Model::Model(std::optional<std::string> dataset_name)\n     : dataset_name_(std::move(dataset_name)),\n       optimization_period_ms_(kOptimizationPeriodMinMs),\n       safe_to_collect_metrics_(std::make_shared<GuardedBool>(true)) {\n-  model_id_ = absl::StrCat(reinterpret_cast<uint64>(this));\n+  model_id_ = absl::StrCat(reinterpret_cast<uint64_t>(this));\n   model_gauge_cell_ = metrics::GetTFDataModelGauge(model_id_);\n \n   // Capture `safe_to_collect_metrics_` by value to avoid use-after-free issues\n@@ -2297,7 +2298,7 @@ Model::~Model() {\n   metrics::RecordPipelineProcessingTime(model_id_, 0);\n }\n \n-void Model::AddNode(Node::Factory factory, const string& name,\n+void Model::AddNode(Node::Factory factory, const std::string& name,\n                     std::shared_ptr<Node> parent,\n                     std::shared_ptr<Node>* out_node) {\n   // The name captures the sequence of iterators joined by `::`. We only use the\n@@ -2935,7 +2936,7 @@ void Model::OptimizeStageBasedNonAsyncInterleaveManyNodes(\n                               node_tunable_parameters.end());\n   }\n   // Initialize the parallelism parameter values to minimal before tuning.\n-  for (std::pair<string, std::shared_ptr<Parameter>>& pair :\n+  for (std::pair<std::string, std::shared_ptr<Parameter>>& pair :\n        tunable_parameters) {\n     if (pair.second->name != kParallelism) {\n       continue;\n@@ -3206,7 +3207,8 @@ absl::Status Model::FromProto(ModelProto model_proto,\n   return absl::OkStatus();\n }\n \n-absl::Status Model::Save(const string& fname, std::shared_ptr<Node> snapshot,\n+absl::Status Model::Save(const std::string& fname,\n+                         std::shared_ptr<Node> snapshot,\n                          const OptimizationParams& optimization_params) {\n   ModelProto model_proto;\n   std::unique_ptr<Model> model_snapshot = std::make_unique<Model>();\n@@ -3222,7 +3224,8 @@ absl::Status Model::Save(const string& fname, std::shared_ptr<Node> snapshot,\n   return WriteBinaryProto(Env::Default(), fname, model_proto);\n }\n \n-absl::Status Model::Load(const string& fname, std::unique_ptr<Model>* model,\n+absl::Status Model::Load(const std::string& fname,\n+                         std::unique_ptr<Model>* model,\n                          OptimizationParams* optimization_params) {\n   ModelProto model_proto;\n   TF_RETURN_IF_ERROR("
        },
        {
            "sha": "c8c39768dc2e6a8f68eea93926237d84a3c3b939",
            "filename": "tensorflow/core/framework/model.h",
            "status": "modified",
            "additions": 26,
            "deletions": 22,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fmodel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fmodel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fmodel.h?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -97,8 +97,8 @@ struct SharedState {\n \n // Represents a parameter.\n struct Parameter {\n-  Parameter(const string& name, std::shared_ptr<SharedState> state, double min,\n-            double max)\n+  Parameter(const std::string& name, std::shared_ptr<SharedState> state,\n+            double min, double max)\n       : name(name),\n         // Sometimes non-autotune nodes (with `autotune_=false`) may contain\n         // parameters (for example inputs of parallel interleave dataset which\n@@ -121,7 +121,7 @@ struct Parameter {\n         state(parameter->state) {}\n \n   // Human-readable name of the parameter.\n-  const string name;\n+  const std::string name;\n \n   // Identifies the model value of the parameter. This can be different from\n   // the actual value (e.g. during optimization search).\n@@ -138,18 +138,18 @@ struct Parameter {\n };\n \n // Returns a new tunable parameter with the value set to `min`.\n-std::shared_ptr<Parameter> MakeParameter(const string& name,\n+std::shared_ptr<Parameter> MakeParameter(const std::string& name,\n                                          std::shared_ptr<SharedState> state,\n                                          double min, double max);\n \n // Returns a new tunable parameter with the value set to `value` instead\n // of `min`.\n-std::shared_ptr<Parameter> MakeParameter(const string& name,\n+std::shared_ptr<Parameter> MakeParameter(const std::string& name,\n                                          std::shared_ptr<SharedState> state,\n                                          double min, double max, double value);\n \n // Returns a new non-tunable parameter.\n-std::shared_ptr<Parameter> MakeNonTunableParameter(const string& name,\n+std::shared_ptr<Parameter> MakeNonTunableParameter(const std::string& name,\n                                                    double value);\n \n // Class for managing the ram budget of an iterator. This is necessary for\n@@ -283,7 +283,7 @@ class Node {\n   // Arguments for `Node` constructor.\n   struct Args {\n     int64_t id;\n-    string name;\n+    std::string name;\n     std::shared_ptr<Node> output;\n   };\n \n@@ -292,10 +292,10 @@ class Node {\n   using NodePairList =\n       std::list<std::pair<std::shared_ptr<Node>, std::shared_ptr<Node>>>;\n   using ModelParameters =\n-      std::vector<std::pair<string, std::shared_ptr<Parameter>>>;\n-  using NodeValues = absl::flat_hash_map<string, double>;\n+      std::vector<std::pair<std::string, std::shared_ptr<Parameter>>>;\n+  using NodeValues = absl::flat_hash_map<std::string, double>;\n   using ParameterGradients =\n-      absl::flat_hash_map<std::pair<string, string>, double>;\n+      absl::flat_hash_map<std::pair<std::string, std::string>, double>;\n \n   explicit Node(Args args)\n       : id_(args.id),\n@@ -413,10 +413,12 @@ class Node {\n   }\n \n   // Returns a longer node name that is guaranteed to be unique.\n-  string long_name() const { return absl::StrCat(name_, \"(id:\", id_, \")\"); }\n+  std::string long_name() const {\n+    return absl::StrCat(name_, \"(id:\", id_, \")\");\n+  }\n \n   // Returns the node name.\n-  const string& name() const { return name_; }\n+  const std::string& name() const { return name_; }\n \n   // Returns the number of elements produced by the node.\n   int64_t num_elements() const TF_LOCKS_EXCLUDED(mu_) { return num_elements_; }\n@@ -426,7 +428,7 @@ class Node {\n   std::shared_ptr<Node> output_shared() { return output_weak_ptr_.lock(); }\n \n   // Returns the parameter value.\n-  double parameter_value(const string& name) const TF_LOCKS_EXCLUDED(mu_) {\n+  double parameter_value(const std::string& name) const TF_LOCKS_EXCLUDED(mu_) {\n     tf_shared_lock l(mu_);\n     return parameters_.at(name)->state->value;\n   }\n@@ -564,7 +566,7 @@ class Node {\n   ModelParameters CollectNodeTunableParameters() const TF_LOCKS_EXCLUDED(mu_);\n \n   // Returns a human-readable representation of this node.\n-  string DebugString() const TF_LOCKS_EXCLUDED(mu_);\n+  std::string DebugString() const TF_LOCKS_EXCLUDED(mu_);\n \n   // Flushes the metrics recorded by this node.\n   void FlushMetrics() TF_LOCKS_EXCLUDED(mu_);\n@@ -645,7 +647,7 @@ class Node {\n   // Used for (incrementally) recording metrics. The class is thread-safe.\n   class Metrics {\n    public:\n-    explicit Metrics(const string& name)\n+    explicit Metrics(const std::string& name)\n         : bytes_consumed_counter_(metrics::GetTFDataBytesConsumedCounter(name)),\n           bytes_produced_counter_(metrics::GetTFDataBytesProducedCounter(name)),\n           num_elements_counter_(metrics::GetTFDataElementsCounter(name)),\n@@ -787,8 +789,9 @@ class Node {\n       TF_SHARED_LOCKS_REQUIRED(mu_);\n \n   // Build up debug string for the node and store in the debug strings map.\n-  void DebugStringHelper(absl::flat_hash_map<string, string>* debug_strings)\n-      const TF_SHARED_LOCKS_REQUIRED(mu_);\n+  void DebugStringHelper(\n+      absl::flat_hash_map<std::string, std::string>* debug_strings) const\n+      TF_SHARED_LOCKS_REQUIRED(mu_);\n \n   // Copy the node and add the (input, copy) pairs to the NodePairList.\n   std::shared_ptr<Node> SnapshotHelper(std::shared_ptr<Node> cloned_output,\n@@ -827,7 +830,7 @@ class Node {\n \n   mutable mutex mu_;\n   const int64_t id_;\n-  const string name_;\n+  const std::string name_;\n \n   // Indicates whether the subtree rooted in this node should be included in\n   // autotuning. In particular, if this is `false`, then the subtree is excluded\n@@ -844,7 +847,7 @@ class Node {\n   std::atomic<int64_t> processing_time_;\n   std::atomic<bool> record_metrics_;\n   Metrics metrics_;\n-  absl::flat_hash_map<string, std::shared_ptr<Parameter>> parameters_\n+  absl::flat_hash_map<std::string, std::shared_ptr<Parameter>> parameters_\n       TF_GUARDED_BY(mu_);\n \n   // Statistic of inputs processing time history.\n@@ -952,7 +955,7 @@ class Model {\n   }\n \n   // Adds a node with the given name and given parent.\n-  void AddNode(Node::Factory factory, const string& name,\n+  void AddNode(Node::Factory factory, const std::string& name,\n                std::shared_ptr<Node> parent, std::shared_ptr<Node>* out_node)\n       TF_LOCKS_EXCLUDED(mu_);\n \n@@ -1014,12 +1017,13 @@ class Model {\n \n   // Saves this model with a given snapshot and its optimization parameters to a\n   // file. Note that the file directory must already exist.\n-  absl::Status Save(const string& fname, std::shared_ptr<Node> snapshot,\n+  absl::Status Save(const std::string& fname, std::shared_ptr<Node> snapshot,\n                     const OptimizationParams& optimization_params);\n \n   // Loads a model and its optimization parameters from a file with the given\n   // name.\n-  static absl::Status Load(const string& fname, std::unique_ptr<Model>* model,\n+  static absl::Status Load(const std::string& fname,\n+                           std::unique_ptr<Model>* model,\n                            OptimizationParams* optimization_params);\n \n   // Records gap time between consecutive `GetNext()` calls."
        },
        {
            "sha": "6ad728f1a0de2c7adb6f372ecfef1c1e1a47318e",
            "filename": "tensorflow/core/framework/model_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fmodel_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a02032e1660aa8d1901f70912c3af6e2de4213f/tensorflow%2Fcore%2Fframework%2Fmodel_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fmodel_test.cc?ref=3a02032e1660aa8d1901f70912c3af6e2de4213f",
            "patch": "@@ -54,7 +54,7 @@ std::function<int64_t(int64_t)> RamBudgetFunc(int64_t budget) {\n   return [budget](int64_t) { return budget; };\n }\n \n-int64_t CountParametersOnNode(const string& node_name,\n+int64_t CountParametersOnNode(const std::string& node_name,\n                               const Model::ModelParameters& parameters) {\n   int64_t cnt = 0;\n   for (const auto& pair : parameters) {\n@@ -865,10 +865,11 @@ TEST(AsyncInterleaveManyGradientTest, Model) {\n       (new_output_time - output_time) / kParameterStep, kComparisonPrecision);\n }\n \n-class AsyncKnownRatioGradientTest : public ::testing::TestWithParam<string> {};\n+class AsyncKnownRatioGradientTest\n+    : public ::testing::TestWithParam<std::string> {};\n \n TEST_P(AsyncKnownRatioGradientTest, Model) {\n-  const string parameter_name = GetParam();\n+  const std::string parameter_name = GetParam();\n   const double input_time = 100;\n   const int64_t num_inputs_per_output = 2;\n \n@@ -1165,7 +1166,7 @@ TEST(SaveModelTest, Model) {\n \n   // Make Save->Load roundtrip.\n   Env* env = Env::Default();\n-  string tmpFile;\n+  std::string tmpFile;\n   EXPECT_TRUE(env->LocalTempFilename(&tmpFile));\n   tmpFile += \"_autotune_model_test\";\n "
        }
    ],
    "stats": {
        "total": 1859,
        "additions": 960,
        "deletions": 899
    }
}