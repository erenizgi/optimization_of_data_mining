{
    "author": "tensorflower-gardener",
    "message": "[Autotuner] Implement xla_gpu_cublas_fallback flag using a temporary autotune option.\n\nPiperOrigin-RevId: 810793418",
    "sha": "9ce1712cb0028334456de491c06c5ba51bfa18df",
    "files": [
        {
            "sha": "afddc2eef0064563a0daec9053694e42763e73a4",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9ce1712cb0028334456de491c06c5ba51bfa18df/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9ce1712cb0028334456de491c06c5ba51bfa18df/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc?ref=9ce1712cb0028334456de491c06c5ba51bfa18df",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/backends/autotuner/autotuner.h\"\n \n+#include <algorithm>\n #include <cstdint>\n #include <limits>\n #include <memory>\n@@ -309,6 +310,16 @@ absl::StatusOr<std::vector<Autotuner::ConfigResult>> Autotuner::ProfileAll(\n \n absl::StatusOr<Autotuner::ConfigResult> Autotuner::PickBestConfig(\n     std::vector<ConfigResult>& results) {\n+  if (autotune_config_.exclude_cublas_config) {\n+    results.erase(\n+        std::remove_if(results.begin(), results.end(),\n+                       [](const ConfigResult& result) {\n+                         return result.config.codegen_backend->name() ==\n+                                \"cublas\";\n+                       }),\n+        results.end());\n+  }\n+\n   absl::Duration min_duration = absl::InfiniteDuration();\n   ConfigResult* best_result = nullptr;\n   for (ConfigResult& result : results) {"
        },
        {
            "sha": "6eb6ed7e3207601ad5243e5249d448d3e5aba92e",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner.h",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9ce1712cb0028334456de491c06c5ba51bfa18df/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9ce1712cb0028334456de491c06c5ba51bfa18df/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.h?ref=9ce1712cb0028334456de491c06c5ba51bfa18df",
            "patch": "@@ -64,6 +64,12 @@ struct AutotuneConfig {\n   // If not empty, detailed logs will be written to the specified file path\n   // as a textproto representation of an `AutotuningLogs` proto message.\n   std::string dump_logs_to = \"\";\n+  // TODO b/446618161 - Remove this when old triton emitter is\n+  // deprecated.\n+  // If true, autotuner will not select cublas configs. We still try cublas\n+  // configs as they can be used to check numerical issues with triton but they\n+  // are not considered for selection.\n+  bool exclude_cublas_config = false;\n };\n \n class Autotuner {"
        },
        {
            "sha": "749d1779ea16013af2ab47baf2379f55e6e3b73e",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner_test.cc",
            "status": "modified",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9ce1712cb0028334456de491c06c5ba51bfa18df/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9ce1712cb0028334456de491c06c5ba51bfa18df/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner_test.cc?ref=9ce1712cb0028334456de491c06c5ba51bfa18df",
            "patch": "@@ -582,5 +582,34 @@ TEST_F(AutotunerTest, DumpLogsToFile) {\n   EXPECT_THAT(actual_logs, tsl::proto_testing::EqualsProto(expected_logs));\n }\n \n+TEST_F(AutotunerTest, ExcludeCublasConfig) {\n+  config_.exclude_cublas_config = true;\n+  std::vector<std::unique_ptr<BackendConfig>> configs;\n+  configs.push_back(GetTestConfig(\"test_config_1\"));\n+\n+  auto backend = std::make_unique<MockCodegenBackend>();\n+  EXPECT_CALL(*backend, GetSupportedConfigs(_))\n+      .WillOnce(Return(std::move(configs)));\n+  EXPECT_CALL(*backend, Compile(_, _))\n+      .WillOnce(Return(std::unique_ptr<Executable>()));\n+  EXPECT_CALL(*backend, name()).WillRepeatedly(Return(\"cublas\"));\n+  std::vector<std::unique_ptr<CodegenBackend>> backends;\n+  backends.push_back(std::move(backend));\n+\n+  auto profiler = std::make_unique<MockProfiler>();\n+  EXPECT_CALL(*profiler, CreateInputBuffers(_))\n+      .WillOnce(Return(std::make_unique<InputBuffers>()));\n+  EXPECT_CALL(*profiler, Profile(_, _))\n+      .WillOnce(Return(ProfileResult({absl::Seconds(1)})));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto autotuner, Autotuner::Create(std::move(backends),\n+                                        std::move(profiler), config_, nullptr));\n+  auto module = ParseAndReturnVerifiedModule(kHlo).value();\n+  auto dummy_instr = module->entry_computation()->root_instruction();\n+  EXPECT_THAT(autotuner->Autotune(dummy_instr),\n+              StatusIs(absl::StatusCode::kInternal));\n+}\n+\n }  // namespace\n }  // namespace xla"
        },
        {
            "sha": "492080a5e98bdd9c76e1f67515d87b2debff5c05",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9ce1712cb0028334456de491c06c5ba51bfa18df/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9ce1712cb0028334456de491c06c5ba51bfa18df/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc?ref=9ce1712cb0028334456de491c06c5ba51bfa18df",
            "patch": "@@ -56,6 +56,8 @@ AutotuneConfig GetAutotuneConfig(const DebugOptions& debug_options) {\n   autotune_config.expect_all_instructions_in_cache =\n       debug_options.xla_gpu_require_complete_aot_autotune_results();\n   autotune_config.dump_logs_to = debug_options.xla_gpu_dump_autotune_logs_to();\n+  autotune_config.exclude_cublas_config =\n+      !debug_options.xla_gpu_cublas_fallback();\n   return autotune_config;\n }\n "
        }
    ],
    "stats": {
        "total": 48,
        "additions": 48,
        "deletions": 0
    }
}