{
    "author": "WillFroom",
    "message": "[XLA:CPU/GPU] Split out DUS utilities to common target.\n\nPiperOrigin-RevId: 800792533",
    "sha": "78fafade60f30554347066396af35f62422db0d3",
    "files": [
        {
            "sha": "3191d19b39c60761b4462b4cc805d141d1e1b048",
            "filename": "third_party/xla/xla/backends/gpu/codegen/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD?ref=78fafade60f30554347066396af35f62422db0d3",
            "patch": "@@ -284,13 +284,13 @@ cc_library(\n         \"//xla/backends/gpu/codegen/emitters:scatter\",\n         \"//xla/backends/gpu/codegen/emitters:transpose\",\n         \"//xla/backends/gpu/codegen/triton:fusion\",\n+        \"//xla/codegen:ir_emission_utils\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu:ir_emission_utils\",\n-        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n     ],"
        },
        {
            "sha": "380fe8ef0de9119b16f3eff8744a5f1b9903139e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusions.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 22,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.cc?ref=78fafade60f30554347066396af35f62422db0d3",
            "patch": "@@ -18,7 +18,6 @@ limitations under the License.\n #include <optional>\n #include <utility>\n \n-#include \"absl/algorithm/container.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/backends/gpu/codegen/copy.h\"\n #include \"xla/backends/gpu/codegen/cudnn.h\"\n@@ -31,6 +30,7 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/emitters/transpose.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion.h\"\n+#include \"xla/codegen/ir_emission_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n@@ -39,28 +39,15 @@ limitations under the License.\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/shape.h\"\n-#include \"xla/shape_util.h\"\n \n namespace xla {\n namespace gpu {\n-namespace {\n-\n-bool IsDynamicUpdateSliceFusion(const HloFusionAnalysis& analysis) {\n-  return absl::c_all_of(\n-      analysis.fusion_roots(), [](const HloInstructionAdaptor& root) {\n-        return root.opcode() == HloOpcode::kDynamicUpdateSlice ||\n-               (root.opcode() == HloOpcode::kBitcast &&\n-                root.GetOperand(0).opcode() == HloOpcode::kDynamicUpdateSlice);\n-      });\n-}\n-\n-}  // namespace\n \n std::optional<std::unique_ptr<FusionInterface>> HloFusionInfo::GetCopyFusion()\n     const {\n   if (analysis().emitter_fusion_kind() ==\n       HloFusionAnalysis::EmitterFusionKind::kDynamicMemcpy) {\n-    if (IsDynamicUpdateSliceFusion(analysis()) &&\n+    if (IsDynamicUpdateSliceFusion(analysis().fusion_spec()) &&\n         !CanEmitDynamicUpdateSliceInPlace()) {\n       // We currently only implement in-place DUSes as memcpys.\n       return std::nullopt;\n@@ -84,12 +71,8 @@ std::optional<std::unique_ptr<FusionInterface>> HloFusionInfo::GetCopyFusion()\n }\n \n bool HloFusionInfo::CanEmitDynamicUpdateSliceInPlace() const {\n-  auto ret = CanEmitFusedDynamicUpdateSliceInPlaceForGpu(\n-      analysis().fusion(),\n-      [this](const HloInstruction* instruction, const ShapeIndex& index) {\n-        return GetAllocationSlice(*buffer_assignment_, instruction, index);\n-      },\n-      instr_);\n+  auto ret = CanEmitFusedDynamicUpdateSliceInPlace(analysis().fusion(),\n+                                                   buffer_assignment_, instr_);\n   return ret.ok() && *ret;\n }\n \n@@ -121,7 +104,7 @@ std::unique_ptr<FusionInterface> GetFusionEmitter(\n       if (auto copy_fusion = fusion_info.GetCopyFusion()) {\n         return *std::move(copy_fusion);\n       }\n-      if (IsDynamicUpdateSliceFusion(analysis) &&\n+      if (IsDynamicUpdateSliceFusion(analysis.fusion_spec()) &&\n           fusion_info.CanEmitDynamicUpdateSliceInPlace()) {\n         return std::make_unique<InPlaceDynamicUpdateSliceFusion>(analysis);\n       }"
        },
        {
            "sha": "430477fe546d2ea3497d61e6fb078f2153766849",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusions.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusions.h?ref=78fafade60f30554347066396af35f62422db0d3",
            "patch": "@@ -78,8 +78,8 @@ class PreBufferAssignmentFusionInfo : public FusionInfo {\n       : FusionInfo(analysis) {}\n \n   bool CanEmitDynamicUpdateSliceInPlace() const override {\n-    auto ret = CanEmitFusedDynamicUpdateSliceInPlaceForGpu(\n-        analysis().fusion(), /*get_allocation_slice=*/{});\n+    auto ret = CanEmitFusedDynamicUpdateSliceInPlace(\n+        analysis().fusion(), /*get_allocation_slice=*/nullptr);\n     return ret.value_or(false);\n   }\n "
        },
        {
            "sha": "f5b7073d1b05303e1a0e6c755ab66c94c8f56cf8",
            "filename": "third_party/xla/xla/codegen/BUILD",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2FBUILD?ref=78fafade60f30554347066396af35f62422db0d3",
            "patch": "@@ -169,12 +169,38 @@ cc_library(\n     srcs = [\"ir_emission_utils.cc\"],\n     hdrs = [\"ir_emission_utils.h\"],\n     deps = [\n+        \":hlo_fusion_spec\",\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/utils:hlo_traversal\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/functional:any_invocable\",\n+        \"@com_google_absl//absl/functional:function_ref\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"ir_emission_utils_test\",\n+    srcs = [\"ir_emission_utils_test.cc\"],\n+    deps = [\n+        \":ir_emission_utils\",\n+        \"//xla:shape_util\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/hlo/utils:hlo_traversal\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@local_tsl//tsl/platform:status_matchers\",\n+        \"@local_tsl//tsl/platform:test\",\n     ],\n )\n "
        },
        {
            "sha": "5021929753c8c90896e5fb4f598cf6e9bef03209",
            "filename": "third_party/xla/xla/codegen/ir_emission_utils.cc",
            "status": "modified",
            "additions": 203,
            "deletions": 0,
            "changes": 203,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fcodegen%2Fir_emission_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fcodegen%2Fir_emission_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fir_emission_utils.cc?ref=78fafade60f30554347066396af35f62422db0d3",
            "patch": "@@ -15,14 +15,28 @@ limitations under the License.\n \n #include \"xla/codegen/ir_emission_utils.h\"\n \n+#include <functional>\n #include <optional>\n+#include <queue>\n+#include <vector>\n \n+#include \"absl/algorithm/container.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/container/inlined_vector.h\"\n #include \"absl/functional/any_invocable.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/codegen/hlo_fusion_spec.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/primitive_util.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla {\n@@ -106,4 +120,193 @@ std::optional<HloInstructionAdaptor> FindHero(\n   return hero;\n }\n \n+bool IsDynamicUpdateSliceFusion(const HloFusionSpec& fusion_spec) {\n+  return absl::c_all_of(\n+      fusion_spec.fusion_roots(), [](const HloInstructionAdaptor& root) {\n+        return root.opcode() == HloOpcode::kDynamicUpdateSlice ||\n+               (root.opcode() == HloOpcode::kBitcast &&\n+                root.GetOperand(0).opcode() == HloOpcode::kDynamicUpdateSlice);\n+      });\n+}\n+\n+std::vector<HloInstructionAdaptor> GetOutputDefiningDynamicUpdateSlices(\n+    absl::Span<HloInstructionAdaptor const> roots) {\n+  std::vector<HloInstructionAdaptor> dus_ops;\n+  for (HloInstructionAdaptor root : roots) {\n+    while (root.opcode() == HloOpcode::kBitcast) {\n+      root = root.GetOperand(0);\n+    }\n+\n+    if (root.opcode() == HloOpcode::kDynamicUpdateSlice) {\n+      dus_ops.push_back(root);\n+    }\n+  }\n+  return dus_ops;\n+}\n+\n+template <typename T>\n+static absl::InlinedVector<const HloInstruction*, 4> GetStartIndices(T instr) {\n+  absl::InlinedVector<const HloInstruction*, 4> result;\n+  for (int i = instr->first_index_operand_number(); i < instr->operand_count();\n+       i++) {\n+    const HloInstruction* index = instr->operand(i);\n+    result.push_back(index);\n+  }\n+  return result;\n+}\n+\n+absl::StatusOr<bool> CanEmitFusedDynamicUpdateSliceInPlace(\n+    const HloFusionAdaptor& fusion_adaptor,\n+    std::function<absl::StatusOr<BufferAllocation::Slice>(\n+        const HloInstruction* instr, const ShapeIndex& index)>\n+        get_allocation_slice,\n+    const HloInstruction* fusion) {\n+  std::vector<HloInstructionAdaptor> dus_instrs =\n+      GetOutputDefiningDynamicUpdateSlices(fusion_adaptor.GetRoots());\n+\n+  // This check could probably be relaxed: if code generation is made to use a\n+  // separate parallel loop for each dynamic slice update, then it shouldn't be\n+  // necessary for every output to be a dynamic slice update, nor to have the\n+  // same shape.\n+  if (dus_instrs.size() != fusion_adaptor.GetRoots().size()) {\n+    return false;\n+  }\n+\n+  Shape update_shape = dus_instrs[0].GetOperand(1).shape();\n+\n+  for (int i = 0; i < dus_instrs.size(); ++i) {\n+    const auto& dus = dus_instrs[i];\n+\n+    // DynamicUpdateSlice ops should have a single path to the root to avoid\n+    // allowing a dynamic slice update to depend on another, as this would not\n+    // be guaranteed to work with the current codegen.\n+    // We follow DUS users until we find an instruction without users. We\n+    // support only few patterns:\n+    //\n+    //   (1) ROOT dynamic-update-slice\n+    //   (2) ROOT tuple(dynamic-update-slice)\n+    //   (3) ROOT bitcast(dynamic-update-slice)\n+    //   (4) ROOT tuple(bitcast(dynamic-update-slice))\n+    //\n+    // In case there is a root tuple, the search will stop at the tuple operand,\n+    // as the root tuple is not considered a real user by HloInstructionAdaptor.\n+    // Note that due to AlgebraicSimplifier we will never have a chain of\n+    // bitcasts.\n+    HloInstructionAdaptor real_root = dus;\n+    auto users = real_root.GetUsers();\n+    while (!users.empty()) {\n+      if (users.size() > 1) {\n+        return false;\n+      }\n+      real_root = users.front();\n+      if (real_root.opcode() != HloOpcode::kBitcast) {\n+        return false;\n+      }\n+      users = real_root.GetUsers();\n+    }\n+\n+    // Find \"real\" DUS operand by skipping bitcasted operands.\n+    HloInstructionAdaptor operand = dus.GetOperand(0);\n+    if (fusion_adaptor.ContainsInstruction(operand) &&\n+        operand.opcode() == HloOpcode::kBitcast) {\n+      operand = operand.GetOperand(0);\n+    }\n+\n+    // Operand to a DUS (or Bitcast) must be a fusion parameter.\n+    // HloInstructionAdaptor skips parameters, so we need to check whether\n+    // 'operand' is outside of the fusion.\n+    if (fusion_adaptor.ContainsInstruction(operand)) {\n+      return false;\n+    }\n+\n+    // We require that the parameter being updated is only read at the same\n+    // index positions by all users, since we otherwise risk a race condition\n+    // when updating the parameter inplace.\n+    std::queue<HloInstructionAdaptor> q;\n+    absl::flat_hash_set<const HloInstruction*> visited;\n+    q.push(operand);\n+    visited.insert(&operand.instruction());\n+    // We have already checked above that the DUS only has one user. So we don't\n+    // need to visit it during the breadth-first search.\n+    visited.insert(&dus.instruction());\n+    while (!q.empty()) {\n+      HloInstructionAdaptor instr = q.front();\n+      q.pop();\n+      for (const HloInstructionAdaptor& user : instr.GetUsers()) {\n+        if (user.opcode() == HloOpcode::kDynamicSlice &&\n+            dus.GetOperand(0) == user.GetOperand(0) &&\n+            update_shape == user.shape()) {\n+          // We can still emit in-place in this case if the same slice is\n+          // accessed by the DUS and the DS. If they don't access the same\n+          // slice, the two slices might partially overlap and read/write the\n+          // same index at different times, and then we cannot guarantee that we\n+          // read before it is overwritten. However if both access only a single\n+          // element, there also can be no race condition.\n+          absl::InlinedVector<const HloInstruction*, 4> user_start_indices =\n+              GetStartIndices(\n+                  Cast<HloDynamicSliceInstruction>(&user.instruction()));\n+          absl::InlinedVector<const HloInstruction*, 4> dus_start_indices =\n+              GetStartIndices(\n+                  Cast<HloDynamicUpdateSliceInstruction>(&dus.instruction()));\n+          if (ShapeUtil::ElementsIn(update_shape) != 1 &&\n+              user_start_indices != dus_start_indices) {\n+            return false;\n+          }\n+        } else if (user != dus &&\n+                   user.opcode() == HloOpcode::kDynamicUpdateSlice) {\n+          return false;\n+        } else if (user != dus && !user.instruction().IsElementwise() &&\n+                   user.opcode() != HloOpcode::kBitcast &&\n+                   user.opcode() != HloOpcode::kTuple) {\n+          return false;\n+        }\n+        if (visited.insert(&user.instruction()).second) {\n+          q.push(user);\n+        }\n+      }\n+    }\n+\n+    // This check could probably be relaxed: if code generation is made to use a\n+    // separate parallel loop for each dynamic slice update, then it shouldn't\n+    // be necessary for the shape to be the same for all the dynamic slice\n+    // updates. Note that this equality check purposefully ignores the element\n+    // type.\n+    if (Cast<HloDynamicUpdateSliceInstruction>(&dus.instruction())\n+            ->update()\n+            ->shape() != update_shape) {\n+      return false;\n+    }\n+\n+    if (fusion != nullptr) {\n+      ShapeIndex root_index = {};\n+      if (fusion->IsMultiOutputFusion()) {\n+        root_index = {i};\n+      }\n+      // Get output buffer for the fusion root.\n+      TF_ASSIGN_OR_RETURN(BufferAllocation::Slice output_buffer,\n+                          get_allocation_slice(fusion, root_index));\n+\n+      TF_ASSIGN_OR_RETURN(BufferAllocation::Slice lhs_buffer,\n+                          get_allocation_slice(&operand.instruction(), {}));\n+      if (lhs_buffer != output_buffer) {\n+        return false;\n+      }\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+absl::StatusOr<bool> CanEmitFusedDynamicUpdateSliceInPlace(\n+    const HloFusionAdaptor& fusion_adaptor,\n+    const BufferAssignment* buffer_assignment, const HloInstruction* fusion) {\n+  return CanEmitFusedDynamicUpdateSliceInPlace(\n+      fusion_adaptor,\n+      [buffer_assignment](const HloInstruction* instr,\n+                          const ShapeIndex& index) {\n+        return buffer_assignment->GetUniqueSlice(instr, index);\n+      },\n+      buffer_assignment ? fusion : nullptr);\n+}\n+\n }  // namespace xla"
        },
        {
            "sha": "8245e2f0b056a1ecccee72f599e218cc1df3abf5",
            "filename": "third_party/xla/xla/codegen/ir_emission_utils.h",
            "status": "modified",
            "additions": 40,
            "deletions": 4,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fcodegen%2Fir_emission_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fcodegen%2Fir_emission_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fir_emission_utils.h?ref=78fafade60f30554347066396af35f62422db0d3",
            "patch": "@@ -1,7 +1,3 @@\n-#include <optional>\n-\n-#include \"absl/functional/any_invocable.h\"\n-#include \"xla/hlo/utils/hlo_traversal.h\"\n /* Copyright 2025 The OpenXLA Authors.\n \n Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -21,10 +17,20 @@ limitations under the License.\n #define XLA_CODEGEN_IR_EMISSION_UTILS_H_\n \n #include <cstdint>\n+#include <functional>\n+#include <optional>\n+#include <vector>\n \n #include \"absl/container/inlined_vector.h\"\n+#include \"absl/functional/any_invocable.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/codegen/hlo_fusion_spec.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/utils/hlo_traversal.h\"\n+#include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla {\n@@ -76,6 +82,36 @@ std::optional<HloInstructionAdaptor> FindHero(\n     const HloInstructionAdaptor& root,\n     absl::AnyInvocable<bool(const HloInstruction&)> predicate);\n \n+// Should the given fusion be emitted using the DUS emitter.\n+bool IsDynamicUpdateSliceFusion(const HloFusionSpec& fusion_spec);\n+\n+// Returns the dynamic-update-slice instructions defining the results of a\n+// fusion node. A dynamic slice update is said to be \"defining\" of a result if\n+// that result is the output of a dynamic slice update, or if that result is the\n+// output of a bitcast of a dynamic slice update---since such bitcast may be\n+// handled as a no-op.\n+std::vector<HloInstructionAdaptor> GetOutputDefiningDynamicUpdateSlices(\n+    absl::Span<HloInstructionAdaptor const> roots);\n+\n+// Returns whether the fusion represented by 'fusion_adaptor' can be emitted\n+// with the dynamic update slice in-place emitter. If 'fusion_adaptor'\n+// represents a single fusion computation, 'fusion' should provide the fusion\n+// instruction corresponding to that fusion computation. 'get_allocation_slice'\n+// is a callback for getting the allocated buffer slice, given an instruction\n+// and a shape index. This is ignored in case 'fusion' is a nullptr.\n+absl::StatusOr<bool> CanEmitFusedDynamicUpdateSliceInPlace(\n+    const HloFusionAdaptor& fusion_adaptor,\n+    std::function<absl::StatusOr<BufferAllocation::Slice>(\n+        const HloInstruction* instr, const ShapeIndex& index)>\n+        get_allocation_slice,\n+    const HloInstruction* fusion = nullptr);\n+\n+// Same as above, but uses the buffer assignment to get the allocated buffer\n+// slices.\n+absl::StatusOr<bool> CanEmitFusedDynamicUpdateSliceInPlace(\n+    const HloFusionAdaptor& fusion_adaptor,\n+    const BufferAssignment* buffer_assignment, const HloInstruction* fusion);\n+\n }  // namespace xla\n \n #endif  // XLA_CODEGEN_IR_EMISSION_UTILS_H_"
        },
        {
            "sha": "fca569e34d24bebf69b74c812c6dea5286efc679",
            "filename": "third_party/xla/xla/codegen/ir_emission_utils_test.cc",
            "status": "added",
            "additions": 463,
            "deletions": 0,
            "changes": 463,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fcodegen%2Fir_emission_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fcodegen%2Fir_emission_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fir_emission_utils_test.cc?ref=78fafade60f30554347066396af35f62422db0d3",
            "patch": "@@ -0,0 +1,463 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/codegen/ir_emission_utils.h\"\n+\n+#include <memory>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/status/status_matchers.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/hlo/utils/hlo_traversal.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+\n+namespace {\n+\n+class IrEmissionUtilsTest : public HloHardwareIndependentTestBase {};\n+\n+TEST_F(IrEmissionUtilsTest,\n+       CanEmitFusedDynamicUpdateSliceInPlace_HandlesBitcasts) {\n+  const char* hlo = R\"(\n+HloModule fusion, is_scheduled=true\n+\n+fused_computation {\n+  param_0.1 = s32[6]{0} parameter(0)\n+  bitcast = s32[2,3]{1,0} bitcast(param_0.1)\n+  zero = s32[] constant(0)\n+  param_1.1 = s32[] parameter(1)\n+  dynamic-slice = s32[1,1]{1,0} dynamic-slice(bitcast, param_1.1, zero), dynamic_slice_sizes={1,1}\n+  one = s32[] constant(1)\n+  bitcasted_one = s32[1,1]{1,0} bitcast(one)\n+  add = s32[1,1] add(dynamic-slice, bitcasted_one)\n+  dynamic-update-slice = s32[2,3]{1,0} dynamic-update-slice(bitcast, add, param_1.1, zero)\n+  ROOT bitcast.1 = s32[6]{0} bitcast(dynamic-update-slice)\n+}\n+\n+ENTRY main {\n+  param_0 = s32[6]{0} parameter(0)\n+  param_1 = s32[] parameter(1)\n+  ROOT fusion = s32[6]{0} fusion(param_0, param_1), kind=kInput, calls=fused_computation\n+}\n+)\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo));\n+  auto fusion = module->entry_computation()->root_instruction();\n+  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n+  BufferAllocation::Slice slice0(&alloc, 0, 10);\n+  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n+  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlace(\n+                  *adaptor,\n+                  [&slice0](const HloInstruction*, const ShapeIndex&) {\n+                    return slice0;\n+                  },\n+                  fusion),\n+              absl_testing::IsOkAndHolds(true));\n+}\n+\n+TEST_F(IrEmissionUtilsTest,\n+       CanEmitFusedDynamicUpdateSliceInPlace_ElementwiseOnPathToParameter) {\n+  const char* hlo = R\"(\n+HloModule fusion, is_scheduled=true\n+\n+fused_computation {\n+  param_0.1 = s32[2,3]{1,0} parameter(0)\n+  bitcast = s32[2,3]{1,0} negate(param_0.1)\n+  zero = s32[] constant(0)\n+  param_1.1 = s32[] parameter(1)\n+  dynamic-slice = s32[1,1]{1,0} dynamic-slice(bitcast, param_1.1, zero), dynamic_slice_sizes={1,1}\n+  one = s32[] constant(1)\n+  bitcasted_one = s32[1,1]{1,0} bitcast(one)\n+  add = s32[1,1] add(dynamic-slice, bitcasted_one)\n+  dynamic-update-slice = s32[2,3]{1,0} dynamic-update-slice(bitcast, add, param_1.1, zero)\n+  ROOT bitcast.1 = s32[6]{0} bitcast(dynamic-update-slice)\n+}\n+\n+ENTRY main {\n+  param_0 = s32[2,3]{1,0} parameter(0)\n+  param_1 = s32[] parameter(1)\n+  ROOT fusion = s32[6]{0} fusion(param_0, param_1), kind=kInput, calls=fused_computation\n+}\n+)\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo));\n+  auto fusion = module->entry_computation()->root_instruction();\n+  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n+  BufferAllocation::Slice slice0(&alloc, 0, 10);\n+  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n+  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlace(\n+                  *adaptor,\n+                  [&slice0](const HloInstruction*, const ShapeIndex&) {\n+                    return slice0;\n+                  },\n+                  fusion),\n+              absl_testing::IsOkAndHolds(false));\n+}\n+\n+// Same test as above, but different allocation slices for parameter and output.\n+TEST_F(IrEmissionUtilsTest,\n+       CanEmitFusedDynamicUpdateSliceInPlace_SlicesDifferent) {\n+  const char* hlo = R\"(\n+HloModule fusion, is_scheduled=true\n+\n+fused_computation {\n+  param_0.1 = s32[6]{0} parameter(0)\n+  bitcast = s32[2,3]{1,0} bitcast(param_0.1)\n+  zero = s32[] constant(0)\n+  param_1.1 = s32[] parameter(1)\n+  dynamic-slice = s32[1,1]{1,0} dynamic-slice(bitcast, param_1.1, zero), dynamic_slice_sizes={1,1}\n+  one = s32[] constant(1)\n+  bitcasted_one = s32[1,1]{1,0} bitcast(one)\n+  add = s32[1,1] add(dynamic-slice, bitcasted_one)\n+  dynamic-update-slice = s32[2,3]{1,0} dynamic-update-slice(bitcast, add, param_1.1, zero)\n+  ROOT bitcast.1 = s32[6]{0} bitcast(dynamic-update-slice)\n+}\n+\n+ENTRY main {\n+  param_0 = s32[6]{0} parameter(0)\n+  param_1 = s32[] parameter(1)\n+  ROOT fusion = s32[6]{0} fusion(param_0, param_1), kind=kInput, calls=fused_computation\n+}\n+)\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo));\n+  auto fusion = module->entry_computation()->root_instruction();\n+  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n+  BufferAllocation::Slice slice0(&alloc, 0, 10);\n+  BufferAllocation::Slice slice1(&alloc, 10, 20);\n+  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n+  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlace(\n+                  *adaptor,\n+                  [fusion, &slice0, &slice1](const HloInstruction* instr,\n+                                             const ShapeIndex&) {\n+                    if (instr == fusion) {\n+                      return slice0;\n+                    }\n+                    return slice1;\n+                  },\n+                  fusion),\n+              absl_testing::IsOkAndHolds(false));\n+}\n+\n+TEST_F(\n+    IrEmissionUtilsTest,\n+    CanEmitFusedDynamicUpdateSliceInPlace_DynamicUpdateSliceWithDifferentDynamicSliceAccess) {  // NOLINT\n+  const char* hlo = R\"(\n+HloModule fusion, input_output_alias={ {}: (0, {}) }\n+\n+fused_computation {\n+  param_0.1 = s32[6]{0} parameter(0)\n+  bitcast = s32[2,3]{1,0} bitcast(param_0.1)\n+  zero = s32[] constant(0)\n+  one = s32[] constant(1)\n+  param_1.1 = s32[] parameter(1)\n+  dynamic-slice = s32[2,2]{1,0} dynamic-slice(bitcast, param_1.1, one), dynamic_slice_sizes={2,2}\n+  broadcasted_one = s32[2,2]{1,0} broadcast(one), dimensions={}\n+  add = s32[2,2] add(dynamic-slice, broadcasted_one)\n+  dynamic-update-slice = s32[2,3]{1,0} dynamic-update-slice(bitcast, add, param_1.1, zero)\n+  ROOT bitcast.1 = s32[6]{0} bitcast(dynamic-update-slice)\n+}\n+\n+ENTRY main {\n+  param_0 = s32[6]{0} parameter(0)\n+  param_1 = s32[] parameter(1)\n+  ROOT fusion = s32[6]{0} fusion(param_0, param_1), kind=kInput, calls=fused_computation\n+}\n+)\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo));\n+  auto fusion = module->entry_computation()->root_instruction();\n+  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n+  BufferAllocation::Slice slice0(&alloc, 0, 10);\n+  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n+  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlace(\n+                  *adaptor,\n+                  [&slice0](const HloInstruction*, const ShapeIndex&) {\n+                    return slice0;\n+                  },\n+                  fusion),\n+              absl_testing::IsOkAndHolds(false));\n+}\n+\n+TEST_F(IrEmissionUtilsTest,\n+       CanEmitFusedDynamicUpdateSliceInPlace_HandlesMultiOutputFusion) {\n+  const char* hlo = R\"(\n+HloModule MultipleInplaceDus, is_scheduled=true, input_output_alias={ {0}: (0, {}), {1}: (2, {}) }\n+\n+fused_computation {\n+  p0 = bf16[10,11,12] parameter(0)\n+  p1 = bf16[1,11,12] parameter(1)\n+  p2 = bf16[8,11,12] parameter(2)\n+  p3 = bf16[1,11,12] parameter(3)\n+  p4 = s32[] parameter(4)\n+  c0 = s32[] constant(0)\n+  cmp = pred[] compare(p4, c0), direction=EQ\n+  broadcast = pred[1,11,12] broadcast(cmp), dimensions={}\n+  select = bf16[1,11,12] select(broadcast, p1, p3)\n+  dus0 = bf16[10,11,12] dynamic-update-slice(p0, select, c0, c0, c0)\n+  dus1 = bf16[8,11,12] dynamic-update-slice(p2, select, c0, c0, c0)\n+  ROOT tuple = (bf16[10,11,12], bf16[8,11,12]) tuple(dus0, dus1)\n+}\n+\n+ENTRY main {\n+  p0 = bf16[10,11,12] parameter(0)\n+  p1 = bf16[1,11,12] parameter(1)\n+  p2 = bf16[8,11,12] parameter(2)\n+  p3 = bf16[1,11,12] parameter(3)\n+  p4 = s32[] parameter(4)\n+  ROOT fusion_root_multiple = (bf16[10,11,12], bf16[8,11,12]) fusion(p0, p1, p2, p3, p4), kind=kLoop, calls=fused_computation\n+}\n+)\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo));\n+  auto fusion = module->entry_computation()->root_instruction();\n+  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n+  BufferAllocation::Slice slice0(&alloc, 0, 10);\n+  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n+  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlace(\n+                  *adaptor,\n+                  [&slice0](const HloInstruction*, const ShapeIndex&) {\n+                    return slice0;\n+                  },\n+                  fusion),\n+              absl_testing::IsOkAndHolds(true));\n+}\n+\n+TEST_F(\n+    IrEmissionUtilsTest,\n+    CanEmitFusedDynamicUpdateSliceInPlace_HandlesMultiOutputFusionSharedParameter) {  // NOLINT\n+  const char* hlo = R\"(\n+HloModule MultipleInplaceDus, is_scheduled=true, input_output_alias={ {0}: (0, {}), {1}: (2, {}) }\n+\n+fused_computation {\n+  p0 = bf16[10,11,12] parameter(0)\n+  p1 = bf16[1,11,12] parameter(1)\n+  p2 = bf16[1,11,12] parameter(2)\n+  p3 = s32[] parameter(3)\n+  c0 = s32[] constant(0)\n+  cmp = pred[] compare(p3, c0), direction=EQ\n+  broadcast = pred[1,11,12] broadcast(cmp), dimensions={}\n+  select = bf16[1,11,12] select(broadcast, p1, p2)\n+  dus0 = bf16[10,11,12] dynamic-update-slice(p0, select, c0, c0, c0)\n+  dus1 = bf16[10,11,12] dynamic-update-slice(p0, select, c0, c0, c0)\n+  ROOT tuple = (bf16[10,11,12], bf16[10,11,12]) tuple(dus0, dus1)\n+}\n+\n+ENTRY main {\n+  p0 = bf16[10,11,12] parameter(0)\n+  p1 = bf16[1,11,12] parameter(1)\n+  p2 = bf16[1,11,12] parameter(2)\n+  p3 = s32[] parameter(3)\n+  ROOT fusion_root_multiple = (bf16[10,11,12], bf16[10,11,12]) fusion(p0, p1, p2, p3), kind=kLoop, calls=fused_computation\n+}\n+)\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo));\n+  auto fusion = module->entry_computation()->root_instruction();\n+  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n+  BufferAllocation::Slice slice0(&alloc, 0, 10);\n+  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n+  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlace(\n+                  *adaptor,\n+                  [&slice0](const HloInstruction*, const ShapeIndex&) {\n+                    return slice0;\n+                  },\n+                  fusion),\n+              absl_testing::IsOkAndHolds(false));\n+}\n+\n+TEST_F(\n+    IrEmissionUtilsTest,\n+    CanEmitFusedDynamicUpdateSliceInPlace_HandlesMultiOutputFusionWithTransposeBitcasts) {  // NOLINT\n+  const char* hlo = R\"(\n+HloModule MultipleInplaceDusWithTransposeBitcastToTheRoot, is_scheduled=true, input_output_alias={ {0}: (0, {}), {1}: (2, {}) }\n+\n+fused_computation {\n+  p0 = bf16[10,11,12] parameter(0)\n+  p1 = bf16[1,11,12] parameter(1)\n+  p2 = bf16[8,11,12] parameter(2)\n+  p3 = bf16[1,11,12] parameter(3)\n+  p4 = s32[] parameter(4)\n+  c0 = s32[] constant(0)\n+  cmp = pred[] compare(p4, c0), direction=EQ\n+  broadcast = pred[1,11,12] broadcast(cmp), dimensions={}\n+  select = bf16[1,11,12] select(broadcast, p1, p3)\n+  dus0 = bf16[10,11,12] dynamic-update-slice(p0, select, c0, c0, c0)\n+  bitcasted_dus0 = bf16[11,10,12] bitcast(dus0)\n+  dus1 = bf16[8,11,12] dynamic-update-slice(p2, select, c0, c0, c0)\n+  ROOT tuple = (bf16[11,10,12], bf16[8,11,12]) tuple(bitcasted_dus0, dus1)\n+}\n+\n+ENTRY main {\n+  p0 = bf16[10,11,12] parameter(0)\n+  p1 = bf16[1,11,12] parameter(1)\n+  p2 = bf16[8,11,12] parameter(2)\n+  p3 = bf16[1,11,12] parameter(3)\n+  p4 = s32[] parameter(4)\n+  ROOT fusion_root_multiple_transpose_bitcast = (bf16[11,10,12], bf16[8,11,12]) fusion(p0, p1, p2, p3, p4), kind=kLoop, calls=fused_computation\n+}\n+)\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo));\n+  auto fusion = module->entry_computation()->root_instruction();\n+  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n+  BufferAllocation::Slice slice0(&alloc, 0, 10);\n+  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n+  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlace(\n+                  *adaptor,\n+                  [&slice0](const HloInstruction*, const ShapeIndex&) {\n+                    return slice0;\n+                  },\n+                  fusion),\n+              absl_testing::IsOkAndHolds(true));\n+}\n+\n+TEST_F(\n+    IrEmissionUtilsTest,\n+    CanEmitFusedDynamicUpdateSliceInPlace_HandlesTransposeBitcastToTheRoot) {  // NOLINT\n+  const char* hlo = R\"(\n+HloModule SingleInplaceDusWithTransposeBitcastToTheRoot, is_scheduled=true, input_output_alias={ {}: (0, {}) }\n+\n+single_inplace_dus_with_transpose_bitcast {\n+  p0 = bf16[10,11,12] parameter(0)\n+  p1 = bf16[1,11,12] parameter(1)\n+  p2 = bf16[1,11,12] parameter(2)\n+  p3 = s32[] parameter(3)\n+  c0 = s32[] constant(0)\n+  cmp = pred[] compare(p3, c0), direction=EQ\n+  broadcast = pred[1,11,12] broadcast(cmp), dimensions={}\n+  select = bf16[1,11,12] select(broadcast, p1, p2)\n+  dus0 = bf16[10,11,12] dynamic-update-slice(p0, select, c0, c0, c0)\n+  ROOT bitcasted_dus0 = bf16[11,10,12] bitcast(dus0)\n+}\n+\n+ENTRY main {\n+  p0 = bf16[10,11,12] parameter(0)\n+  p1 = bf16[1,11,12] parameter(1)\n+  p2 = bf16[1,11,12] parameter(2)\n+  p3 = s32[] parameter(3)\n+  ROOT fusion_root_transpose_bitcast = bf16[11,10,12] fusion(p0, p1, p2, p3), kind=kLoop, calls=single_inplace_dus_with_transpose_bitcast\n+}\n+)\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo));\n+  auto fusion = module->entry_computation()->root_instruction();\n+  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n+  BufferAllocation::Slice slice0(&alloc, 0, 10);\n+  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n+  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlace(\n+                  *adaptor,\n+                  [&slice0](const HloInstruction*, const ShapeIndex&) {\n+                    return slice0;\n+                  },\n+                  fusion),\n+              absl_testing::IsOkAndHolds(true));\n+}\n+\n+TEST_F(\n+    IrEmissionUtilsTest,\n+    CanEmitFusedDynamicUpdateSliceInPlace_HandlesReshapeBitcastToTheRoot) {  // NOLINT\n+  const char* hlo = R\"(\n+HloModule SingleInplaceDusWithReshapeBitcastToTheRoot, is_scheduled=true, input_output_alias={ {}: (0, {}) }\n+\n+single_inplace_dus_with_reshape_bitcast {\n+  p0 = bf16[10,11,12] parameter(0)\n+  p1 = bf16[1,11,12] parameter(1)\n+  p2 = bf16[1,11,12] parameter(2)\n+  p3 = s32[] parameter(3)\n+  c0 = s32[] constant(0)\n+  cmp = pred[] compare(p3, c0), direction=EQ\n+  broadcast = pred[1,11,12] broadcast(cmp), dimensions={}\n+  select = bf16[1,11,12] select(broadcast, p1, p2)\n+  dus0 = bf16[10,11,12] dynamic-update-slice(p0, select, c0, c0, c0)\n+  ROOT bitcasted_dus0 = bf16[10,11,6,2] bitcast(dus0)\n+}\n+\n+ENTRY main {\n+  p0 = bf16[10,11,12] parameter(0)\n+  p1 = bf16[1,11,12] parameter(1)\n+  p2 = bf16[1,11,12] parameter(2)\n+  p3 = s32[] parameter(3)\n+  ROOT fusion_root_reshape_bitcast = bf16[10,11,6,2] fusion(p0, p1, p2, p3), kind=kLoop, calls=single_inplace_dus_with_reshape_bitcast\n+}\n+)\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo));\n+  auto fusion = module->entry_computation()->root_instruction();\n+  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n+  BufferAllocation::Slice slice0(&alloc, 0, 10);\n+  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n+  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlace(\n+                  *adaptor,\n+                  [&slice0](const HloInstruction*, const ShapeIndex&) {\n+                    return slice0;\n+                  },\n+                  fusion),\n+              absl_testing::IsOkAndHolds(true));\n+}\n+\n+TEST_F(\n+    IrEmissionUtilsTest,\n+    CanEmitFusedDynamicUpdateSliceInPlace_HandlesBitcastToTheRootAndFromParameter) {  // NOLINT\n+  const char* hlo = R\"(\n+HloModule SingleInplaceDusWithBitcastToTheRootAndFromTheParameter, is_scheduled=true, input_output_alias={ {}: (0, {}) }\n+\n+single_inplace_dus_with_bitcast_to_the_root_and_from_the_parameter {\n+  p0 = bf16[10,11,12] parameter(0)\n+  p1 = bf16[1,11,12] parameter(1)\n+  p2 = bf16[1,11,12] parameter(2)\n+  p3 = s32[] parameter(3)\n+  c0 = s32[] constant(0)\n+  cmp = pred[] compare(p3, c0), direction=EQ\n+  broadcast = pred[1,11,12] broadcast(cmp), dimensions={}\n+  select = bf16[1,11,12] select(broadcast, p1, p2)\n+  bitcasted_p0 = bf16[10,6,2,11] bitcast(p0)\n+  bitcasted_select = bf16[1,6,2,11] bitcast(select)\n+  dus0 = bf16[10,6,2,11] dynamic-update-slice(bitcasted_p0, bitcasted_select, c0, c0, c0, c0)\n+  ROOT bitcasted_dus0 = bf16[10,11,6,2] bitcast(dus0)\n+}\n+\n+ENTRY main {\n+  p0 = bf16[10,11,12] parameter(0)\n+  p1 = bf16[1,11,12] parameter(1)\n+  p2 = bf16[1,11,12] parameter(2)\n+  p3 = s32[] parameter(3)\n+  ROOT fusion_root_bitcast_both_ways = bf16[10,11,6,2] fusion(p0, p1, p2, p3),\n+    kind=kLoop, calls=single_inplace_dus_with_bitcast_to_the_root_and_from_the_parameter\n+}\n+)\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo));\n+  auto fusion = module->entry_computation()->root_instruction();\n+  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n+  BufferAllocation::Slice slice0(&alloc, 0, 10);\n+  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n+  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlace(\n+                  *adaptor,\n+                  [&slice0](const HloInstruction*, const ShapeIndex&) {\n+                    return slice0;\n+                  },\n+                  fusion),\n+              absl_testing::IsOkAndHolds(true));\n+}\n+\n+}  // namespace\n+}  // namespace xla"
        },
        {
            "sha": "e95bcd96658745acb5b5df742f7c55a23f800e35",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 168,
            "changes": 168,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc?ref=78fafade60f30554347066396af35f62422db0d3",
            "patch": "@@ -206,174 +206,6 @@ absl::StatusOr<BufferAllocation::Slice> GetAllocationSlice(\n   return buffer_assignment.GetUniqueSlice(instr, index);\n }\n \n-std::vector<HloInstructionAdaptor> GetOutputDefiningDynamicUpdateSlices(\n-    absl::Span<HloInstructionAdaptor const> roots) {\n-  std::vector<HloInstructionAdaptor> dus_ops;\n-  for (HloInstructionAdaptor root : roots) {\n-    while (root.opcode() == HloOpcode::kBitcast) {\n-      root = root.GetOperand(0);\n-    }\n-\n-    if (root.opcode() == HloOpcode::kDynamicUpdateSlice) {\n-      dus_ops.push_back(root);\n-    }\n-  }\n-  return dus_ops;\n-}\n-\n-template <typename T>\n-absl::InlinedVector<const HloInstruction*, 4> GetStartIndices(T instr) {\n-  absl::InlinedVector<const HloInstruction*, 4> result;\n-  for (int i = instr->first_index_operand_number(); i < instr->operand_count();\n-       i++) {\n-    const HloInstruction* index = instr->operand(i);\n-    result.push_back(index);\n-  }\n-  return result;\n-}\n-\n-absl::StatusOr<bool> CanEmitFusedDynamicUpdateSliceInPlaceForGpu(\n-    const HloFusionAdaptor& fusion_adaptor,\n-    std::function<absl::StatusOr<BufferAllocation::Slice>(\n-        const HloInstruction* instr, const ShapeIndex& index)>\n-        get_allocation_slice,\n-    const HloInstruction* fusion) {\n-  std::vector<HloInstructionAdaptor> dus_instrs =\n-      GetOutputDefiningDynamicUpdateSlices(fusion_adaptor.GetRoots());\n-\n-  // This check could probably be relaxed: if code generation is made to use a\n-  // separate parallel loop for each dynamic slice update, then it shouldn't be\n-  // necessary for every output to be a dynamic slice update, nor to have the\n-  // same shape.\n-  if (dus_instrs.size() != fusion_adaptor.GetRoots().size()) {\n-    return false;\n-  }\n-\n-  Shape update_shape = dus_instrs[0].GetOperand(1).shape();\n-\n-  for (int i = 0; i < dus_instrs.size(); ++i) {\n-    const auto& dus = dus_instrs[i];\n-\n-    // DynamicUpdateSlice ops should have a single path to the root to avoid\n-    // allowing a dynamic slice update to depend on another, as this would not\n-    // be guaranteed to work with the current codegen.\n-    // We follow DUS users until we find an instruction without users. We\n-    // support only few patterns:\n-    //\n-    //   (1) ROOT dynamic-update-slice\n-    //   (2) ROOT tuple(dynamic-update-slice)\n-    //   (3) ROOT bitcast(dynamic-update-slice)\n-    //   (4) ROOT tuple(bitcast(dynamic-update-slice))\n-    //\n-    // In case there is a root tuple, the search will stop at the tuple operand,\n-    // as the root tuple is not considered a real user by HloInstructionAdaptor.\n-    // Note that due to AlgebraicSimplifier we will never have a chain of\n-    // bitcasts.\n-    HloInstructionAdaptor real_root = dus;\n-    auto users = real_root.GetUsers();\n-    while (!users.empty()) {\n-      if (users.size() > 1) {\n-        return false;\n-      }\n-      real_root = users.front();\n-      if (real_root.opcode() != HloOpcode::kBitcast) {\n-        return false;\n-      }\n-      users = real_root.GetUsers();\n-    }\n-\n-    // Find \"real\" DUS operand by skipping bitcasted operands.\n-    HloInstructionAdaptor operand = dus.GetOperand(0);\n-    if (fusion_adaptor.ContainsInstruction(operand) &&\n-        operand.opcode() == HloOpcode::kBitcast) {\n-      operand = operand.GetOperand(0);\n-    }\n-\n-    // Operand to a DUS (or Bitcast) must be a fusion parameter.\n-    // HloInstructionAdaptor skips parameters, so we need to check whether\n-    // 'operand' is outside of the fusion.\n-    if (fusion_adaptor.ContainsInstruction(operand)) {\n-      return false;\n-    }\n-\n-    // We require that the parameter being updated is only read at the same\n-    // index positions by all users, since we otherwise risk a race condition\n-    // when updating the parameter inplace.\n-    std::queue<HloInstructionAdaptor> q;\n-    absl::flat_hash_set<const HloInstruction*> visited;\n-    q.push(operand);\n-    visited.insert(&operand.instruction());\n-    // We have already checked above that the DUS only has one user. So we don't\n-    // need to visit it during the breadth-first search.\n-    visited.insert(&dus.instruction());\n-    while (!q.empty()) {\n-      HloInstructionAdaptor instr = q.front();\n-      q.pop();\n-      for (const HloInstructionAdaptor& user : instr.GetUsers()) {\n-        if (user.opcode() == HloOpcode::kDynamicSlice &&\n-            dus.GetOperand(0) == user.GetOperand(0) &&\n-            update_shape == user.shape()) {\n-          // We can still emit in-place in this case if the same slice is\n-          // accessed by the DUS and the DS. If they don't access the same\n-          // slice, the two slices might partially overlap and read/write the\n-          // same index at different times, and then we cannot guarantee that we\n-          // read before it is overwritten. However if both access only a single\n-          // element, there also can be no race condition.\n-          absl::InlinedVector<const HloInstruction*, 4> user_start_indices =\n-              GetStartIndices(\n-                  Cast<HloDynamicSliceInstruction>(&user.instruction()));\n-          absl::InlinedVector<const HloInstruction*, 4> dus_start_indices =\n-              GetStartIndices(\n-                  Cast<HloDynamicUpdateSliceInstruction>(&dus.instruction()));\n-          if (ShapeUtil::ElementsIn(update_shape) != 1 &&\n-              user_start_indices != dus_start_indices) {\n-            return false;\n-          }\n-        } else if (user != dus &&\n-                   user.opcode() == HloOpcode::kDynamicUpdateSlice) {\n-          return false;\n-        } else if (user != dus && !user.instruction().IsElementwise() &&\n-                   user.opcode() != HloOpcode::kBitcast &&\n-                   user.opcode() != HloOpcode::kTuple) {\n-          return false;\n-        }\n-        if (visited.insert(&user.instruction()).second) {\n-          q.push(user);\n-        }\n-      }\n-    }\n-\n-    // This check could probably be relaxed: if code generation is made to use a\n-    // separate parallel loop for each dynamic slice update, then it shouldn't\n-    // be necessary for the shape to be the same for all the dynamic slice\n-    // updates. Note that this equality check purposefully ignores the element\n-    // type.\n-    if (Cast<HloDynamicUpdateSliceInstruction>(&dus.instruction())\n-            ->update()\n-            ->shape() != update_shape) {\n-      return false;\n-    }\n-\n-    if (fusion != nullptr) {\n-      ShapeIndex root_index = {};\n-      if (fusion->IsMultiOutputFusion()) {\n-        root_index = {i};\n-      }\n-      // Get output buffer for the fusion root.\n-      TF_ASSIGN_OR_RETURN(BufferAllocation::Slice output_buffer,\n-                          get_allocation_slice(fusion, root_index));\n-\n-      TF_ASSIGN_OR_RETURN(BufferAllocation::Slice lhs_buffer,\n-                          get_allocation_slice(&operand.instruction(), {}));\n-      if (lhs_buffer != output_buffer) {\n-        return false;\n-      }\n-    }\n-  }\n-\n-  return true;\n-}\n-\n bool IsNormalized(const HloTransposeInstruction& transpose) {\n   const auto& permutation = transpose.dimensions();\n   for (int i = 0; i < permutation.size() - 1; ++i) {"
        },
        {
            "sha": "9d7b4b83a1cf666e904d3ab0008913a0bd70080a",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils.h",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.h?ref=78fafade60f30554347066396af35f62422db0d3",
            "patch": "@@ -173,27 +173,6 @@ absl::StatusOr<BufferAllocation::Slice> GetAllocationSlice(\n     const BufferAssignment& buffer_assignment, const HloInstruction* instr,\n     const ShapeIndex& index);\n \n-// Returns whether the fusion represented by 'fusion_adaptor' can be emitted\n-// with the dynamic update slice in-place emitter. If 'fusion_adaptor'\n-// represents a single fusion computation, 'fusion' should provide the fusion\n-// instruction corresponding to that fusion computation. 'get_allocation_slice'\n-// is a callback for getting the allocated buffer slice, given an instruction\n-// and a shape index. This is ignored in case 'fusion' is a nullptr.\n-absl::StatusOr<bool> CanEmitFusedDynamicUpdateSliceInPlaceForGpu(\n-    const HloFusionAdaptor& fusion_adaptor,\n-    std::function<absl::StatusOr<BufferAllocation::Slice>(\n-        const HloInstruction* instr, const ShapeIndex& index)>\n-        get_allocation_slice,\n-    const HloInstruction* fusion = nullptr);\n-\n-// Returns the dynamic-update-slice instructions defining the results of a\n-// fusion node. A dynamic slice update is said to be \"defining\" of a result if\n-// that result is the output of a dynamic slice update, or if that result is the\n-// output of a bitcast of a dynamic slice update---since such bitcast may be\n-// handled as a no-op.\n-std::vector<HloInstructionAdaptor> GetOutputDefiningDynamicUpdateSlices(\n-    absl::Span<HloInstructionAdaptor const> roots);\n-\n // Returns the first hero instruction reachable from `instr` as root. Hero\n // instruction can be in a different computation if the parent HloFusionAdaptor\n // is a producer-consumer fusion."
        },
        {
            "sha": "baba99b4b8e0d9468993bb397b68d2498b892bac",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 425,
            "changes": 425,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/78fafade60f30554347066396af35f62422db0d3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils_test.cc?ref=78fafade60f30554347066396af35f62422db0d3",
            "patch": "@@ -679,431 +679,6 @@ TEST_F(IrEmissionUtilsTest, LiteralToAttrToXlaFormat) {\n   }\n }\n \n-TEST_F(IrEmissionUtilsTest,\n-       CanEmitFusedDynamicUpdateSliceInPlaceForGpu_HandlesBitcasts) {\n-  const char* hlo = R\"(\n-HloModule fusion, is_scheduled=true\n-\n-fused_computation {\n-  param_0.1 = s32[6]{0} parameter(0)\n-  bitcast = s32[2,3]{1,0} bitcast(param_0.1)\n-  zero = s32[] constant(0)\n-  param_1.1 = s32[] parameter(1)\n-  dynamic-slice = s32[1,1]{1,0} dynamic-slice(bitcast, param_1.1, zero), dynamic_slice_sizes={1,1}\n-  one = s32[] constant(1)\n-  bitcasted_one = s32[1,1]{1,0} bitcast(one)\n-  add = s32[1,1] add(dynamic-slice, bitcasted_one)\n-  dynamic-update-slice = s32[2,3]{1,0} dynamic-update-slice(bitcast, add, param_1.1, zero)\n-  ROOT bitcast.1 = s32[6]{0} bitcast(dynamic-update-slice)\n-}\n-\n-ENTRY main {\n-  param_0 = s32[6]{0} parameter(0)\n-  param_1 = s32[] parameter(1)\n-  ROOT fusion = s32[6]{0} fusion(param_0, param_1), kind=kInput, calls=fused_computation\n-}\n-)\";\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(hlo));\n-  auto fusion = module->entry_computation()->root_instruction();\n-  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n-  BufferAllocation::Slice slice0(&alloc, 0, 10);\n-  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n-  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlaceForGpu(\n-                  *adaptor,\n-                  [&slice0](const HloInstruction*, const ShapeIndex&) {\n-                    return slice0;\n-                  },\n-                  fusion),\n-              absl_testing::IsOkAndHolds(true));\n-}\n-\n-TEST_F(\n-    IrEmissionUtilsTest,\n-    CanEmitFusedDynamicUpdateSliceInPlaceForGpu_ElementwiseOnPathToParameter) {\n-  const char* hlo = R\"(\n-HloModule fusion, is_scheduled=true\n-\n-fused_computation {\n-  param_0.1 = s32[2,3]{1,0} parameter(0)\n-  bitcast = s32[2,3]{1,0} negate(param_0.1)\n-  zero = s32[] constant(0)\n-  param_1.1 = s32[] parameter(1)\n-  dynamic-slice = s32[1,1]{1,0} dynamic-slice(bitcast, param_1.1, zero), dynamic_slice_sizes={1,1}\n-  one = s32[] constant(1)\n-  bitcasted_one = s32[1,1]{1,0} bitcast(one)\n-  add = s32[1,1] add(dynamic-slice, bitcasted_one)\n-  dynamic-update-slice = s32[2,3]{1,0} dynamic-update-slice(bitcast, add, param_1.1, zero)\n-  ROOT bitcast.1 = s32[6]{0} bitcast(dynamic-update-slice)\n-}\n-\n-ENTRY main {\n-  param_0 = s32[2,3]{1,0} parameter(0)\n-  param_1 = s32[] parameter(1)\n-  ROOT fusion = s32[6]{0} fusion(param_0, param_1), kind=kInput, calls=fused_computation\n-}\n-)\";\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(hlo));\n-  auto fusion = module->entry_computation()->root_instruction();\n-  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n-  BufferAllocation::Slice slice0(&alloc, 0, 10);\n-  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n-  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlaceForGpu(\n-                  *adaptor,\n-                  [&slice0](const HloInstruction*, const ShapeIndex&) {\n-                    return slice0;\n-                  },\n-                  fusion),\n-              absl_testing::IsOkAndHolds(false));\n-}\n-\n-// Same test as above, but different allocation slices for parameter and output.\n-TEST_F(IrEmissionUtilsTest,\n-       CanEmitFusedDynamicUpdateSliceInPlaceForGpu_SlicesDifferent) {\n-  const char* hlo = R\"(\n-HloModule fusion, is_scheduled=true\n-\n-fused_computation {\n-  param_0.1 = s32[6]{0} parameter(0)\n-  bitcast = s32[2,3]{1,0} bitcast(param_0.1)\n-  zero = s32[] constant(0)\n-  param_1.1 = s32[] parameter(1)\n-  dynamic-slice = s32[1,1]{1,0} dynamic-slice(bitcast, param_1.1, zero), dynamic_slice_sizes={1,1}\n-  one = s32[] constant(1)\n-  bitcasted_one = s32[1,1]{1,0} bitcast(one)\n-  add = s32[1,1] add(dynamic-slice, bitcasted_one)\n-  dynamic-update-slice = s32[2,3]{1,0} dynamic-update-slice(bitcast, add, param_1.1, zero)\n-  ROOT bitcast.1 = s32[6]{0} bitcast(dynamic-update-slice)\n-}\n-\n-ENTRY main {\n-  param_0 = s32[6]{0} parameter(0)\n-  param_1 = s32[] parameter(1)\n-  ROOT fusion = s32[6]{0} fusion(param_0, param_1), kind=kInput, calls=fused_computation\n-}\n-)\";\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(hlo));\n-  auto fusion = module->entry_computation()->root_instruction();\n-  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n-  BufferAllocation::Slice slice0(&alloc, 0, 10);\n-  BufferAllocation::Slice slice1(&alloc, 10, 20);\n-  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n-  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlaceForGpu(\n-                  *adaptor,\n-                  [fusion, &slice0, &slice1](const HloInstruction* instr,\n-                                             const ShapeIndex&) {\n-                    if (instr == fusion) {\n-                      return slice0;\n-                    }\n-                    return slice1;\n-                  },\n-                  fusion),\n-              absl_testing::IsOkAndHolds(false));\n-}\n-\n-TEST_F(\n-    IrEmissionUtilsTest,\n-    CanEmitFusedDynamicUpdateSliceInPlaceForGpu_DynamicUpdateSliceWithDifferentDynamicSliceAccess) {  // NOLINT\n-  const char* hlo = R\"(\n-HloModule fusion, input_output_alias={ {}: (0, {}) }\n-\n-fused_computation {\n-  param_0.1 = s32[6]{0} parameter(0)\n-  bitcast = s32[2,3]{1,0} bitcast(param_0.1)\n-  zero = s32[] constant(0)\n-  one = s32[] constant(1)\n-  param_1.1 = s32[] parameter(1)\n-  dynamic-slice = s32[2,2]{1,0} dynamic-slice(bitcast, param_1.1, one), dynamic_slice_sizes={2,2}\n-  broadcasted_one = s32[2,2]{1,0} broadcast(one), dimensions={}\n-  add = s32[2,2] add(dynamic-slice, broadcasted_one)\n-  dynamic-update-slice = s32[2,3]{1,0} dynamic-update-slice(bitcast, add, param_1.1, zero)\n-  ROOT bitcast.1 = s32[6]{0} bitcast(dynamic-update-slice)\n-}\n-\n-ENTRY main {\n-  param_0 = s32[6]{0} parameter(0)\n-  param_1 = s32[] parameter(1)\n-  ROOT fusion = s32[6]{0} fusion(param_0, param_1), kind=kInput, calls=fused_computation\n-}\n-)\";\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(hlo));\n-  auto fusion = module->entry_computation()->root_instruction();\n-  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n-  BufferAllocation::Slice slice0(&alloc, 0, 10);\n-  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n-  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlaceForGpu(\n-                  *adaptor,\n-                  [&slice0](const HloInstruction*, const ShapeIndex&) {\n-                    return slice0;\n-                  },\n-                  fusion),\n-              absl_testing::IsOkAndHolds(false));\n-}\n-\n-TEST_F(IrEmissionUtilsTest,\n-       CanEmitFusedDynamicUpdateSliceInPlaceForGpu_HandlesMultiOutputFusion) {\n-  const char* hlo = R\"(\n-HloModule MultipleInplaceDus, is_scheduled=true, input_output_alias={ {0}: (0, {}), {1}: (2, {}) }\n-\n-fused_computation {\n-  p0 = bf16[10,11,12] parameter(0)\n-  p1 = bf16[1,11,12] parameter(1)\n-  p2 = bf16[8,11,12] parameter(2)\n-  p3 = bf16[1,11,12] parameter(3)\n-  p4 = s32[] parameter(4)\n-  c0 = s32[] constant(0)\n-  cmp = pred[] compare(p4, c0), direction=EQ\n-  broadcast = pred[1,11,12] broadcast(cmp), dimensions={}\n-  select = bf16[1,11,12] select(broadcast, p1, p3)\n-  dus0 = bf16[10,11,12] dynamic-update-slice(p0, select, c0, c0, c0)\n-  dus1 = bf16[8,11,12] dynamic-update-slice(p2, select, c0, c0, c0)\n-  ROOT tuple = (bf16[10,11,12], bf16[8,11,12]) tuple(dus0, dus1)\n-}\n-\n-ENTRY main {\n-  p0 = bf16[10,11,12] parameter(0)\n-  p1 = bf16[1,11,12] parameter(1)\n-  p2 = bf16[8,11,12] parameter(2)\n-  p3 = bf16[1,11,12] parameter(3)\n-  p4 = s32[] parameter(4)\n-  ROOT fusion_root_multiple = (bf16[10,11,12], bf16[8,11,12]) fusion(p0, p1, p2, p3, p4), kind=kLoop, calls=fused_computation\n-}\n-)\";\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(hlo));\n-  auto fusion = module->entry_computation()->root_instruction();\n-  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n-  BufferAllocation::Slice slice0(&alloc, 0, 10);\n-  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n-  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlaceForGpu(\n-                  *adaptor,\n-                  [&slice0](const HloInstruction*, const ShapeIndex&) {\n-                    return slice0;\n-                  },\n-                  fusion),\n-              absl_testing::IsOkAndHolds(true));\n-}\n-\n-TEST_F(\n-    IrEmissionUtilsTest,\n-    CanEmitFusedDynamicUpdateSliceInPlaceForGpu_HandlesMultiOutputFusionSharedParameter) {  // NOLINT\n-  const char* hlo = R\"(\n-HloModule MultipleInplaceDus, is_scheduled=true, input_output_alias={ {0}: (0, {}), {1}: (2, {}) }\n-\n-fused_computation {\n-  p0 = bf16[10,11,12] parameter(0)\n-  p1 = bf16[1,11,12] parameter(1)\n-  p2 = bf16[1,11,12] parameter(2)\n-  p3 = s32[] parameter(3)\n-  c0 = s32[] constant(0)\n-  cmp = pred[] compare(p3, c0), direction=EQ\n-  broadcast = pred[1,11,12] broadcast(cmp), dimensions={}\n-  select = bf16[1,11,12] select(broadcast, p1, p2)\n-  dus0 = bf16[10,11,12] dynamic-update-slice(p0, select, c0, c0, c0)\n-  dus1 = bf16[10,11,12] dynamic-update-slice(p0, select, c0, c0, c0)\n-  ROOT tuple = (bf16[10,11,12], bf16[10,11,12]) tuple(dus0, dus1)\n-}\n-\n-ENTRY main {\n-  p0 = bf16[10,11,12] parameter(0)\n-  p1 = bf16[1,11,12] parameter(1)\n-  p2 = bf16[1,11,12] parameter(2)\n-  p3 = s32[] parameter(3)\n-  ROOT fusion_root_multiple = (bf16[10,11,12], bf16[10,11,12]) fusion(p0, p1, p2, p3), kind=kLoop, calls=fused_computation\n-}\n-)\";\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(hlo));\n-  auto fusion = module->entry_computation()->root_instruction();\n-  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n-  BufferAllocation::Slice slice0(&alloc, 0, 10);\n-  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n-  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlaceForGpu(\n-                  *adaptor,\n-                  [&slice0](const HloInstruction*, const ShapeIndex&) {\n-                    return slice0;\n-                  },\n-                  fusion),\n-              absl_testing::IsOkAndHolds(false));\n-}\n-\n-TEST_F(\n-    IrEmissionUtilsTest,\n-    CanEmitFusedDynamicUpdateSliceInPlaceForGpu_HandlesMultiOutputFusionWithTransposeBitcasts) {  // NOLINT\n-  const char* hlo = R\"(\n-HloModule MultipleInplaceDusWithTransposeBitcastToTheRoot, is_scheduled=true, input_output_alias={ {0}: (0, {}), {1}: (2, {}) }\n-\n-fused_computation {\n-  p0 = bf16[10,11,12] parameter(0)\n-  p1 = bf16[1,11,12] parameter(1)\n-  p2 = bf16[8,11,12] parameter(2)\n-  p3 = bf16[1,11,12] parameter(3)\n-  p4 = s32[] parameter(4)\n-  c0 = s32[] constant(0)\n-  cmp = pred[] compare(p4, c0), direction=EQ\n-  broadcast = pred[1,11,12] broadcast(cmp), dimensions={}\n-  select = bf16[1,11,12] select(broadcast, p1, p3)\n-  dus0 = bf16[10,11,12] dynamic-update-slice(p0, select, c0, c0, c0)\n-  bitcasted_dus0 = bf16[11,10,12] bitcast(dus0)\n-  dus1 = bf16[8,11,12] dynamic-update-slice(p2, select, c0, c0, c0)\n-  ROOT tuple = (bf16[11,10,12], bf16[8,11,12]) tuple(bitcasted_dus0, dus1)\n-}\n-\n-ENTRY main {\n-  p0 = bf16[10,11,12] parameter(0)\n-  p1 = bf16[1,11,12] parameter(1)\n-  p2 = bf16[8,11,12] parameter(2)\n-  p3 = bf16[1,11,12] parameter(3)\n-  p4 = s32[] parameter(4)\n-  ROOT fusion_root_multiple_transpose_bitcast = (bf16[11,10,12], bf16[8,11,12]) fusion(p0, p1, p2, p3, p4), kind=kLoop, calls=fused_computation\n-}\n-)\";\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(hlo));\n-  auto fusion = module->entry_computation()->root_instruction();\n-  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n-  BufferAllocation::Slice slice0(&alloc, 0, 10);\n-  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n-  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlaceForGpu(\n-                  *adaptor,\n-                  [&slice0](const HloInstruction*, const ShapeIndex&) {\n-                    return slice0;\n-                  },\n-                  fusion),\n-              absl_testing::IsOkAndHolds(true));\n-}\n-\n-TEST_F(\n-    IrEmissionUtilsTest,\n-    CanEmitFusedDynamicUpdateSliceInPlaceForGpu_HandlesTransposeBitcastToTheRoot) {  // NOLINT\n-  const char* hlo = R\"(\n-HloModule SingleInplaceDusWithTransposeBitcastToTheRoot, is_scheduled=true, input_output_alias={ {}: (0, {}) }\n-\n-single_inplace_dus_with_transpose_bitcast {\n-  p0 = bf16[10,11,12] parameter(0)\n-  p1 = bf16[1,11,12] parameter(1)\n-  p2 = bf16[1,11,12] parameter(2)\n-  p3 = s32[] parameter(3)\n-  c0 = s32[] constant(0)\n-  cmp = pred[] compare(p3, c0), direction=EQ\n-  broadcast = pred[1,11,12] broadcast(cmp), dimensions={}\n-  select = bf16[1,11,12] select(broadcast, p1, p2)\n-  dus0 = bf16[10,11,12] dynamic-update-slice(p0, select, c0, c0, c0)\n-  ROOT bitcasted_dus0 = bf16[11,10,12] bitcast(dus0)\n-}\n-\n-ENTRY main {\n-  p0 = bf16[10,11,12] parameter(0)\n-  p1 = bf16[1,11,12] parameter(1)\n-  p2 = bf16[1,11,12] parameter(2)\n-  p3 = s32[] parameter(3)\n-  ROOT fusion_root_transpose_bitcast = bf16[11,10,12] fusion(p0, p1, p2, p3), kind=kLoop, calls=single_inplace_dus_with_transpose_bitcast\n-}\n-)\";\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(hlo));\n-  auto fusion = module->entry_computation()->root_instruction();\n-  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n-  BufferAllocation::Slice slice0(&alloc, 0, 10);\n-  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n-  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlaceForGpu(\n-                  *adaptor,\n-                  [&slice0](const HloInstruction*, const ShapeIndex&) {\n-                    return slice0;\n-                  },\n-                  fusion),\n-              absl_testing::IsOkAndHolds(true));\n-}\n-\n-TEST_F(\n-    IrEmissionUtilsTest,\n-    CanEmitFusedDynamicUpdateSliceInPlaceForGpu_HandlesReshapeBitcastToTheRoot) {  // NOLINT\n-  const char* hlo = R\"(\n-HloModule SingleInplaceDusWithReshapeBitcastToTheRoot, is_scheduled=true, input_output_alias={ {}: (0, {}) }\n-\n-single_inplace_dus_with_reshape_bitcast {\n-  p0 = bf16[10,11,12] parameter(0)\n-  p1 = bf16[1,11,12] parameter(1)\n-  p2 = bf16[1,11,12] parameter(2)\n-  p3 = s32[] parameter(3)\n-  c0 = s32[] constant(0)\n-  cmp = pred[] compare(p3, c0), direction=EQ\n-  broadcast = pred[1,11,12] broadcast(cmp), dimensions={}\n-  select = bf16[1,11,12] select(broadcast, p1, p2)\n-  dus0 = bf16[10,11,12] dynamic-update-slice(p0, select, c0, c0, c0)\n-  ROOT bitcasted_dus0 = bf16[10,11,6,2] bitcast(dus0)\n-}\n-\n-ENTRY main {\n-  p0 = bf16[10,11,12] parameter(0)\n-  p1 = bf16[1,11,12] parameter(1)\n-  p2 = bf16[1,11,12] parameter(2)\n-  p3 = s32[] parameter(3)\n-  ROOT fusion_root_reshape_bitcast = bf16[10,11,6,2] fusion(p0, p1, p2, p3), kind=kLoop, calls=single_inplace_dus_with_reshape_bitcast\n-}\n-)\";\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(hlo));\n-  auto fusion = module->entry_computation()->root_instruction();\n-  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n-  BufferAllocation::Slice slice0(&alloc, 0, 10);\n-  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n-  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlaceForGpu(\n-                  *adaptor,\n-                  [&slice0](const HloInstruction*, const ShapeIndex&) {\n-                    return slice0;\n-                  },\n-                  fusion),\n-              absl_testing::IsOkAndHolds(true));\n-}\n-\n-TEST_F(\n-    IrEmissionUtilsTest,\n-    CanEmitFusedDynamicUpdateSliceInPlaceForGpu_HandlesBitcastToTheRootAndFromParameter) {  // NOLINT\n-  const char* hlo = R\"(\n-HloModule SingleInplaceDusWithBitcastToTheRootAndFromTheParameter, is_scheduled=true, input_output_alias={ {}: (0, {}) }\n-\n-single_inplace_dus_with_bitcast_to_the_root_and_from_the_parameter {\n-  p0 = bf16[10,11,12] parameter(0)\n-  p1 = bf16[1,11,12] parameter(1)\n-  p2 = bf16[1,11,12] parameter(2)\n-  p3 = s32[] parameter(3)\n-  c0 = s32[] constant(0)\n-  cmp = pred[] compare(p3, c0), direction=EQ\n-  broadcast = pred[1,11,12] broadcast(cmp), dimensions={}\n-  select = bf16[1,11,12] select(broadcast, p1, p2)\n-  bitcasted_p0 = bf16[10,6,2,11] bitcast(p0)\n-  bitcasted_select = bf16[1,6,2,11] bitcast(select)\n-  dus0 = bf16[10,6,2,11] dynamic-update-slice(bitcasted_p0, bitcasted_select, c0, c0, c0, c0)\n-  ROOT bitcasted_dus0 = bf16[10,11,6,2] bitcast(dus0)\n-}\n-\n-ENTRY main {\n-  p0 = bf16[10,11,12] parameter(0)\n-  p1 = bf16[1,11,12] parameter(1)\n-  p2 = bf16[1,11,12] parameter(2)\n-  p3 = s32[] parameter(3)\n-  ROOT fusion_root_bitcast_both_ways = bf16[10,11,6,2] fusion(p0, p1, p2, p3), kind=kLoop, calls=single_inplace_dus_with_bitcast_to_the_root_and_from_the_parameter\n-}\n-)\";\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(hlo));\n-  auto fusion = module->entry_computation()->root_instruction();\n-  BufferAllocation alloc(/*index=*/0, /*size=*/1024, /*color=*/0);\n-  BufferAllocation::Slice slice0(&alloc, 0, 10);\n-  auto adaptor = HloFusionAdaptor::ForInstruction(fusion);\n-  EXPECT_THAT(CanEmitFusedDynamicUpdateSliceInPlaceForGpu(\n-                  *adaptor,\n-                  [&slice0](const HloInstruction*, const ShapeIndex&) {\n-                    return slice0;\n-                  },\n-                  fusion),\n-              absl_testing::IsOkAndHolds(true));\n-}\n-\n gpu::GpuBackendConfig CreateTestProto() {\n   gpu::GpuBackendConfig proto;\n   auto& knobs = *proto.mutable_cudnn_fmha_backend_config()"
        }
    ],
    "stats": {
        "total": 1383,
        "additions": 740,
        "deletions": 643
    }
}