{
    "author": "beckerhe",
    "message": "Remove unnecessary if_gpu_is_configured from Triton tests.\n\nThe tests in xla/backends/gpu/codegen/triton/BUILD are already configured to run only on specific GPU backends, making the if_gpu_is_configured check on the srcs redundant.\n\nPiperOrigin-RevId: 847738574",
    "sha": "12502acbf569675fbdec61b305a1f4e169decdf9",
    "files": [
        {
            "sha": "ea9866b1695c7494f18ed59ad9c4670c6124e5f7",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12502acbf569675fbdec61b305a1f4e169decdf9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12502acbf569675fbdec61b305a1f4e169decdf9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=12502acbf569675fbdec61b305a1f4e169decdf9",
            "patch": "@@ -3,7 +3,6 @@ load(\"//xla:xla.default.bzl\", \"xla_cc_test\")\n load(\n     \"//xla/stream_executor:build_defs.bzl\",\n     \"if_cuda_or_rocm_is_configured\",\n-    \"if_gpu_is_configured\",\n )\n load(\"//xla/tests:build_defs.bzl\", \"xla_test\")\n load(\"//xla/tsl:tsl.bzl\", \"if_google\")\n@@ -541,7 +540,7 @@ xla_cc_test(\n \n xla_test(\n     name = \"triton_gemm_fusion_test\",\n-    srcs = if_gpu_is_configured([\"triton_gemm_fusion_test.cc\"]),\n+    srcs = [\"triton_gemm_fusion_test.cc\"],\n     backends = [\n         \"a100\",\n         \"h100\",\n@@ -595,7 +594,7 @@ xla_test(\n xla_test(\n     name = \"fusion_emitter_int4_device_test\",\n     size = \"large\",\n-    srcs = if_gpu_is_configured([\"fusion_emitter_int4_device_test.cc\"]),\n+    srcs = [\"fusion_emitter_int4_device_test.cc\"],\n     backends = [\n         \"a100\",\n         \"h100\",\n@@ -692,7 +691,7 @@ xla_test(\n \n xla_test(\n     name = \"fusion_emitter_device_test\",\n-    srcs = if_gpu_is_configured([\"fusion_emitter_device_test.cc\"]),\n+    srcs = [\"fusion_emitter_device_test.cc\"],\n     backends = [\n         \"a100\",\n         \"h100\",\n@@ -799,7 +798,7 @@ cc_library(\n xla_test(\n     name = \"fusion_emitter_large_test\",\n     size = \"large\",\n-    srcs = if_gpu_is_configured([\"fusion_emitter_large_test.cc\"]),\n+    srcs = [\"fusion_emitter_large_test.cc\"],\n     backends = [\n         \"a100\",\n         \"h100\",\n@@ -828,7 +827,7 @@ xla_test(\n \n xla_test(\n     name = \"fusion_emitter_parametrized_test\",\n-    srcs = if_gpu_is_configured([\"fusion_emitter_parametrized_test.cc\"]),\n+    srcs = [\"fusion_emitter_parametrized_test.cc\"],\n     backends = [\n         \"a100\",\n         \"h100\",\n@@ -857,7 +856,7 @@ xla_test(\n \n xla_cc_test(\n     name = \"fusion_emitter_shared_dialect_test\",\n-    srcs = if_gpu_is_configured([\"fusion_emitter_shared_dialect_test.cc\"]),\n+    srcs = [\"fusion_emitter_shared_dialect_test.cc\"],\n     # TODO(b/353912594): this test does not need to run on GPU, but it is broken on CPU in OSS.\n     # Force it to run on GPU temporarily in order to get important OSS coverage.\n     tags = [\n@@ -948,7 +947,7 @@ xla_cc_test(\n \n xla_test(\n     name = \"support_legacy_test\",\n-    srcs = if_gpu_is_configured([\"support_legacy_test.cc\"]),\n+    srcs = [\"support_legacy_test.cc\"],\n     backends = [\n         \"a100\",\n         \"h100\","
        }
    ],
    "stats": {
        "total": 15,
        "additions": 7,
        "deletions": 8
    }
}