{
    "author": "chsigg",
    "message": "[xla:gpu] Streamline Triton compilation pipeline setup.\n\n- Combine `:triton_compilation` target back into one, drop `if_cuda/rocm`.\n- Implement target-agnostic `CreateTritonPipeline()`, which forwards to cuda/rocm specific implementations based on `GpuComputeCapability` argument.\n- Split those implementations into 3 separate functions each to mimic upstream Triton.\n- Wrap the creation of the XLA-specific passes into a function.\n- Move `GetDeviceLib()` implementation to `:fusion_emitter` target, where it belongs.\n- Move the logic for enabling `bf16x2` in `int4-rewrite` to the pipeline creation.\n- Register the entire compilation pipeline in `xla-opt` for easier testing and debugging.\n\nPiperOrigin-RevId: 807220491",
    "sha": "8a441cd80b438bca7c94e060ae5b5096f95543c4",
    "files": [
        {
            "sha": "e8e459146162f28e2c72e30c7e193707c7034718",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 61,
            "deletions": 133,
            "changes": 194,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=8a441cd80b438bca7c94e060ae5b5096f95543c4",
            "patch": "@@ -1,4 +1,3 @@\n-load(\"@local_config_rocm//rocm:build_defs.bzl\", \"if_rocm_is_configured\")\n load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n load(\"//xla:xla.default.bzl\", \"xla_cc_test\")\n load(\n@@ -8,7 +7,6 @@ load(\n )\n load(\"//xla/tests:build_defs.bzl\", \"xla_test\")\n load(\"//xla/tsl:tsl.bzl\", \"if_google\")\n-load(\"//xla/tsl/platform/default:cuda_build_defs.bzl\", \"if_cuda_is_configured\")\n \n package(\n     # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n@@ -129,70 +127,28 @@ cc_library(\n )\n \n cc_library(\n-    name = \"compilation_pipeline_cuda\",\n+    name = \"compilation_pipeline\",\n     srcs = [\n+        \"compilation_pipeline.cc\",\n         \"compilation_pipeline_cuda.cc\",\n-    ],\n-    hdrs = [\"compilation_pipeline.h\"],\n-    tags = [\n-        \"cuda-only\",\n-        \"gpu\",\n-    ],\n-    deps = [\n-        \"//xla/backends/gpu/codegen/triton/transforms:passes\",\n-        \"//xla/service:hlo_module_config\",\n-        \"//xla/service/gpu:matmul_utils\",\n-        \"//xla/service/gpu/llvm_gpu_backend:nvptx_backend\",\n-        \"//xla/service/gpu/llvm_gpu_backend:nvptx_libdevice_path\",\n-        \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/strings:str_format\",\n-        \"@llvm-project//mlir:ArithToLLVM\",\n-        \"@llvm-project//mlir:ControlFlowToLLVM\",\n-        \"@llvm-project//mlir:IndexToLLVM\",\n-        \"@llvm-project//mlir:NVVMToLLVM\",\n-        \"@llvm-project//mlir:Pass\",\n-        \"@llvm-project//mlir:SCFToControlFlow\",\n-        \"@llvm-project//mlir:Transforms\",\n-        \"@triton//:TritonDialects\",\n-        \"@triton//:TritonGPUToLLVM\",\n-        \"@triton//:TritonGPUTransforms\",\n-        \"@triton//:TritonInstrumentTransforms\",\n-        \"@triton//:TritonLLVMIR\",\n-        \"@triton//:TritonNvidiaGPUTransforms\",\n-        \"@triton//:TritonToTritonGPU\",\n-        \"@triton//:TritonToTritonGPUPasses\",\n-        \"@triton//:TritonTransforms\",\n-        \"@triton//:WarpSpecialization\",\n-        \"@triton//third_party/nvidia:NVGPUToLLVM\",\n-        \"@triton//third_party/nvidia:NVHopperTransforms\",\n-        \"@triton//third_party/nvidia:TritonNVIDIAGPUToLLVM\",\n-    ],\n-)\n-\n-cc_library(\n-    name = \"compilation_pipeline_rocm\",\n-    srcs = [\n         \"compilation_pipeline_rocm.cc\",\n     ],\n     hdrs = [\"compilation_pipeline.h\"],\n-    tags = [\n-        \"gpu\",\n-        \"rocm-only\",\n-    ],\n+    tags = [\"gpu\"],\n     deps = [\n+        \"//xla/backends/gpu/codegen/emitters/transforms:passes\",\n         \"//xla/backends/gpu/codegen/triton/transforms:passes\",\n+        \"//xla/codegen/emitters/transforms:convert_pure_call_ops_pass\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/service/gpu:matmul_utils\",\n-        \"//xla/service/gpu/llvm_gpu_backend:amdgpu_backend\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n-        \"//xla/tsl/platform:rocm_rocdl_path\",\n+        \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n+        \"@llvm-project//mlir:AffineToStandard\",\n         \"@llvm-project//mlir:ArithToLLVM\",\n         \"@llvm-project//mlir:ControlFlowToLLVM\",\n         \"@llvm-project//mlir:IndexToLLVM\",\n@@ -212,18 +168,9 @@ cc_library(\n         \"@triton//:WarpSpecialization\",\n         \"@triton//third_party/amd:TritonAMDGPUToLLVM\",\n         \"@triton//third_party/amd:TritonAMDGPUTransforms\",\n-    ],\n-)\n-\n-cc_library(\n-    name = \"compilation_pipeline_stub\",\n-    srcs = [\"compilation_pipeline_stub.cc\"],\n-    hdrs = [\"compilation_pipeline.h\"],\n-    deps = [\n-        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/status\",\n-        \"@llvm-project//mlir:Pass\",\n+        \"@triton//third_party/nvidia:NVGPUToLLVM\",\n+        \"@triton//third_party/nvidia:NVHopperTransforms\",\n+        \"@triton//third_party/nvidia:TritonNVIDIAGPUToLLVM\",\n     ],\n )\n \n@@ -236,65 +183,40 @@ cc_library(\n     ),\n     hdrs = [\"fusion_emitter.h\"],\n     deps = [\n-        \":dot_algorithms\",\n-        \":emitter_helpers\",\n-        \":fusion_emitter_legacy_matmul\",\n-        \":support\",\n-        \":tma_utils\",\n         \"//xla:autotuning_proto_cc\",\n-        \"//xla:permutation_util\",\n-        \"//xla:shape_util\",\n-        \"//xla:status_macros\",\n-        \"//xla:util\",\n-        \"//xla:xla_data_proto_cc\",\n-        \"//xla:xla_proto_cc\",\n-        \"//xla/backends/gpu/codegen/emitters/ir:xla_gpu\",\n-        \"//xla/backends/gpu/codegen/emitters/transforms:passes\",\n         \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n-        \"//xla/backends/gpu/codegen/triton/transforms:passes\",\n         \"//xla/codegen:emitter_loc_op_builder\",\n-        \"//xla/codegen/emitters:elemental_hlo_to_mlir\",\n-        \"//xla/codegen/emitters/ir:xla\",\n-        \"//xla/codegen/emitters/transforms:passes\",\n-        \"//xla/hlo/analysis:indexing_analysis\",\n-        \"//xla/hlo/builder:xla_builder\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/translate/hlo_to_mhlo:hlo_function_importer\",\n-        \"//xla/hlo/utils:hlo_traversal\",\n-        \"//xla/mlir_hlo\",\n-        \"//xla/service:dump\",\n         \"//xla/service:hlo_module_config\",\n-        \"//xla/service:instruction_fusion\",\n-        \"//xla/service/gpu:backend_configs_cc\",\n-        \"//xla/service/gpu:ir_emission_utils\",\n-        \"//xla/service/gpu:launch_dimensions\",\n-        \"//xla/service/gpu:matmul_utils\",\n-        \"//xla/service/gpu:triton_fusion_analysis\",\n         \"//xla/service/gpu/model:symbolic_tile_analysis\",\n         \"//xla/service/gpu/model:tiled_hlo_instruction_or_computation\",\n-        \"//xla/service/gpu/model:triton_emitter_constraints\",\n-        \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:launch_dim\",\n-        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n-        \"//xla/tools:hlo_decomposer_lib\",\n-        \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//llvm:ir_headers\",\n+        \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:Pass\",\n+        \"@triton//:TritonDialects\",\n+    ] + if_cuda_or_rocm_is_configured([\n+        \":compilation_pipeline\",\n+        \":dot_algorithms\",\n+        \":emitter_helpers\",\n+        \":fusion_emitter_legacy_matmul\",\n+        \":support\",\n+        \":tma_utils\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n-        \"@com_google_absl//absl/strings:string_view\",\n-        \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Linker\",\n-        \"@llvm-project//llvm:Support\",\n-        \"@llvm-project//llvm:ir_headers\",\n         \"@llvm-project//mlir:AffineDialect\",\n         \"@llvm-project//mlir:AffineToStandard\",\n         \"@llvm-project//mlir:ArithDialect\",\n@@ -305,49 +227,57 @@ cc_library(\n         \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:FuncExtensions\",\n         \"@llvm-project//mlir:FunctionInterfaces\",\n-        \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:IndexToLLVM\",\n         \"@llvm-project//mlir:LLVMDialect\",\n         \"@llvm-project//mlir:LLVMIRTransforms\",\n         \"@llvm-project//mlir:LLVMToLLVMIRTranslation\",\n         \"@llvm-project//mlir:NVVMDialect\",\n         \"@llvm-project//mlir:NVVMToLLVMIRTranslation\",\n-        \"@llvm-project//mlir:Pass\",\n         \"@llvm-project//mlir:ROCDLToLLVMIRTranslation\",\n         \"@llvm-project//mlir:SCFDialect\",\n         \"@llvm-project//mlir:SCFToControlFlow\",\n         \"@llvm-project//mlir:Support\",\n         \"@llvm-project//mlir:TensorDialect\",\n         \"@llvm-project//mlir:ToLLVMIRTranslation\",\n         \"@llvm-project//mlir:Transforms\",\n+        \"//xla:permutation_util\",\n+        \"//xla:shape_util\",\n+        \"//xla:status_macros\",\n+        \"//xla:util\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla:xla_proto_cc\",\n+        \"//xla/backends/gpu/codegen/emitters/ir:xla_gpu\",\n+        \"//xla/backends/gpu/codegen/emitters/transforms:passes\",\n+        \"//xla/backends/gpu/codegen/triton/transforms:passes\",\n+        \"//xla/codegen/emitters:elemental_hlo_to_mlir\",\n+        \"//xla/codegen/emitters/ir:xla\",\n+        \"//xla/codegen/emitters/transforms:passes\",\n+        \"//xla/hlo/analysis:indexing_analysis\",\n+        \"//xla/hlo/builder:xla_builder\",\n+        \"//xla/hlo/translate/hlo_to_mhlo:hlo_function_importer\",\n+        \"//xla/hlo/utils:hlo_traversal\",\n+        \"//xla/mlir_hlo\",\n+        \"//xla/service:dump\",\n+        \"//xla/service:instruction_fusion\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:ir_emission_utils\",\n+        \"//xla/service/gpu:launch_dimensions\",\n+        \"//xla/service/gpu/llvm_gpu_backend:amdgpu_backend\",\n+        \"//xla/service/gpu/llvm_gpu_backend:nvptx_backend\",\n+        \"//xla/service/gpu/llvm_gpu_backend:nvptx_libdevice_path\",\n+        \"//xla/service/gpu:matmul_utils\",\n+        \"//xla/service/gpu:triton_fusion_analysis\",\n+        \"//xla/service/gpu/model:triton_emitter_constraints\",\n+        \"//xla/service/llvm_ir:llvm_util\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/tools:hlo_decomposer_lib\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:rocm_rocdl_path\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@local_tsl//tsl/platform:errors\",\n         \"@local_tsl//tsl/platform:path\",\n         \"@local_tsl//tsl/platform:statusor\",\n-        \"@triton//:TritonDialects\",\n         \"@triton//:TritonTransforms\",\n-    ] + if_gpu_is_configured(\n-        [\n-            \"@triton//:TritonNvidiaGPUTransforms\",\n-            \"@triton//:TritonGPUToLLVM\",\n-            \"@triton//:TritonToTritonGPU\",\n-            \"@triton//:TritonGPUTransforms\",\n-            \"@triton//:TritonLLVMIR\",\n-        ],\n-        [\n-            \":compilation_pipeline_stub\",\n-        ],\n-    ) + if_cuda_is_configured([\n-        \":compilation_pipeline_cuda\",\n-        \"@triton//third_party/nvidia:NVGPUToLLVM\",\n-        \"//xla/service/gpu/llvm_gpu_backend:nvptx_libdevice_path\",\n-        \"//xla/service/gpu/llvm_gpu_backend:nvptx_backend\",\n-        \"@triton//third_party/nvidia:TritonNVIDIAGPUToLLVM\",\n-    ]) + if_rocm_is_configured([\n-        \":compilation_pipeline_rocm\",\n-        \"//xla/tsl/platform:rocm_rocdl_path\",\n-        \"//xla/service/gpu/llvm_gpu_backend:amdgpu_backend\",\n-        \"@triton//third_party/amd:TritonAMDGPUToLLVM\",\n-        \"@triton//third_party/amd:TritonAMDGPUTransforms\",\n     ]),\n )\n \n@@ -446,12 +376,10 @@ cc_library(\n cc_library(\n     name = \"fusion_emitter_stub_for_testing\",\n     srcs = [\n-        \"compilation_pipeline_stub.cc\",\n         \"fusion_emitter_legacy_matmul_stub.cc\",\n         \"fusion_emitter_stub.cc\",\n     ],\n     hdrs = [\n-        \"compilation_pipeline.h\",\n         \"fusion_emitter.h\",\n         \"fusion_emitter_legacy_matmul.h\",\n     ],"
        },
        {
            "sha": "6cd4989eb3e4ec8f6294af022935ff9f26310677",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline.cc",
            "status": "added",
            "additions": 92,
            "deletions": 0,
            "changes": 92,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc?ref=8a441cd80b438bca7c94e060ae5b5096f95543c4",
            "patch": "@@ -0,0 +1,92 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/codegen/triton/compilation_pipeline.h\"\n+\n+#include <variant>\n+\n+#include \"mlir/Conversion/AffineToStandard/AffineToStandard.h\"\n+#include \"mlir/Pass/PassManager.h\"\n+#include \"xla/backends/gpu/codegen/emitters/transforms/passes.h\"\n+#include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n+#include \"xla/codegen/emitters/transforms/passes.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n+\n+namespace xla::gpu {\n+\n+void CreateTritonXlaPipeline(\n+    mlir::OpPassManager* pm,\n+    const stream_executor::GpuComputeCapability& gpu_cc, bool rewrite_int4,\n+    bool allow_tma, bool convert_unsupported_types) {\n+  pm->addPass(mlir::triton::xla::CreateTritonXLASqueezeDimsPass());\n+  pm->addPass(mlir::triton::xla::CreateTritonXLAFoldTransposePass());\n+\n+  auto* cuda_cc = std::get_if<stream_executor::CudaComputeCapability>(&gpu_cc);\n+  bool is_at_least_hopper = cuda_cc != nullptr && cuda_cc->IsAtLeastHopper();\n+\n+  if (rewrite_int4) {\n+    pm->addPass(mlir::triton::xla::CreateInt4ToPackedInt4RewritePass(\n+        /*enable_bf16x2=*/is_at_least_hopper));\n+  }\n+  if (convert_unsupported_types) {\n+    pm->addPass(\n+        mlir::triton::xla::CreateTritonXLAConvertUnsupportedTypesPass());\n+  }\n+\n+  pm->addPass(mlir::triton::xla::CreateTritonXLAExtractInsertToTritonPass(\n+      /*allow_tma=*/allow_tma && is_at_least_hopper));\n+\n+  // Lower affine expressions into arithmetic ops.\n+  pm->addPass(mlir::createLowerAffinePass());\n+\n+  // Lower xla_gpu.apply_indexing into arithmetic ops.\n+  pm->addPass(emitters::CreateSimplifyAffinePass());\n+  pm->addPass(CreateConvertIndexTypePass());\n+}\n+\n+void CreateTritonCudaPipeline(\n+    mlir::OpPassManager* pm,\n+    const stream_executor::CudaComputeCapability& cuda_cc, int num_warps,\n+    int num_ctas, int num_stages,\n+    mlir::triton::nvidia_gpu::ClusterInfo& out_cluster_info);\n+\n+void CreateTritonRocmPipeline(\n+    mlir::OpPassManager* pm,\n+    const stream_executor::RocmComputeCapability& rocm_cc, int num_warps,\n+    int num_ctas, int num_stages);\n+\n+void CreateTritonPipeline(\n+    mlir::OpPassManager* pm,\n+    const stream_executor::GpuComputeCapability& gpu_cc, int num_warps,\n+    int num_ctas, int num_stages,\n+    mlir::triton::nvidia_gpu::ClusterInfo& out_cluster_info) {\n+  if (auto* cuda_cc =\n+          std::get_if<stream_executor::CudaComputeCapability>(&gpu_cc)) {\n+    return CreateTritonCudaPipeline(pm, *cuda_cc, num_warps, num_ctas,\n+                                    num_stages, out_cluster_info);\n+  }\n+\n+  CreateTritonRocmPipeline(\n+      pm, std::get<stream_executor::RocmComputeCapability>(gpu_cc), num_warps,\n+      num_ctas, num_stages);\n+  // There is no clusters in ROCm for now.\n+  out_cluster_info.clusterDimX = 1;\n+  out_cluster_info.clusterDimY = 1;\n+  out_cluster_info.clusterDimZ = 1;\n+}\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "99b63ee8550457cf2d69029eb2a65842e5c8c622",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline.h",
            "status": "modified",
            "additions": 14,
            "deletions": 16,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.h?ref=8a441cd80b438bca7c94e060ae5b5096f95543c4",
            "patch": "@@ -16,20 +16,17 @@ limitations under the License.\n #ifndef XLA_BACKENDS_GPU_CODEGEN_TRITON_COMPILATION_PIPELINE_H_\n #define XLA_BACKENDS_GPU_CODEGEN_TRITON_COMPILATION_PIPELINE_H_\n \n-#include <string>\n-\n-#include \"absl/status/status.h\"\n #include \"mlir/Pass/PassManager.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n \n-namespace mlir::triton::nvidia_gpu {\n-\n-// Forward declaration to avoid including a GPU-only header.\n-struct ClusterInfo;\n-\n-}  // namespace mlir::triton::nvidia_gpu\n+namespace xla::gpu {\n \n-namespace xla {\n-namespace gpu {\n+// Adds TritonXLA passes to the pipeline.\n+void CreateTritonXlaPipeline(\n+    mlir::OpPassManager* pm,\n+    const stream_executor::GpuComputeCapability& gpu_cc, bool rewrite_int4,\n+    bool allow_tma, bool convert_unsupported_types);\n \n // Creates a Triton compilation pipeline.\n //\n@@ -40,11 +37,12 @@ namespace gpu {\n // are some signs that show that this was intended to be used as an in-out\n // parameter which would give a hint to Triton which cluster dims we prefer to\n // use, but that's not the case currently.\n-absl::Status CreateTritonPipeline(\n-    mlir::OpPassManager* pm, std::string arch_name, int num_warps, int num_ctas,\n-    int num_stages, mlir::triton::nvidia_gpu::ClusterInfo& out_cluster_info);\n+void CreateTritonPipeline(\n+    mlir::OpPassManager* pm,\n+    const stream_executor::GpuComputeCapability& gpu_cc, int num_warps,\n+    int num_ctas, int num_stages,\n+    mlir::triton::nvidia_gpu::ClusterInfo& out_cluster_info);\n \n-}  // namespace gpu\n-}  // namespace xla\n+}  // namespace xla::gpu\n \n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_COMPILATION_PIPELINE_H_"
        },
        {
            "sha": "0cf9871d726538f73ad45cfc439a64f30ba64586",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline_cuda.cc",
            "status": "modified",
            "additions": 41,
            "deletions": 41,
            "changes": 82,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc?ref=8a441cd80b438bca7c94e060ae5b5096f95543c4",
            "patch": "@@ -18,19 +18,14 @@ limitations under the License.\n #include \"nvidia/hopper/include/Transforms/Passes.h\"\n #include \"nvidia/include/NVGPUToLLVM/Passes.h\"\n #include \"nvidia/include/TritonNVIDIAGPUToLLVM/Passes.h\"\n-#include \"absl/status/status.h\"\n #include \"absl/strings/str_format.h\"\n #include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n #include \"mlir/Conversion/NVVMToLLVM/NVVMToLLVM.h\"\n #include \"mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Transforms/Passes.h\"\n #include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n-#include \"xla/service/gpu/llvm_gpu_backend/nvptx_libdevice_path.h\"\n-#include \"xla/service/hlo_module_config.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n-#include \"xla/stream_executor/device_description.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/Passes.h\"\n #include \"triton/Conversion/TritonToTritonGPU/Passes.h\"\n #include \"triton/Dialect/Triton/Transforms/Passes.h\"\n@@ -44,22 +39,14 @@ namespace mt = ::mlir::triton;\n namespace mt_xla = ::mlir::triton::xla;\n namespace ttng = mlir::triton::nvidia_gpu;\n \n-absl::Status CreateTritonPipeline(\n-    mlir::OpPassManager* pm, std::string arch_name, int num_warps, int num_ctas,\n-    int num_stages, mt::nvidia_gpu::ClusterInfo& out_cluster_info) {\n-  TF_ASSIGN_OR_RETURN(\n-      const stream_executor::CudaComputeCapability cc,\n-      stream_executor::CudaComputeCapability::FromString(arch_name));\n-  const int ccAsInt = cc.major * 10 + cc.minor;\n-  const int threadsPerWarp = 32;\n-\n+// Based on make_ttir() in\n+// @triton//:third_party/nvidia/backend/compiler.py\n+static void MakeTTIR(mlir::OpPassManager* pm,\n+                     const stream_executor::CudaComputeCapability& cuda_cc) {\n   pm->addPass(mt_xla::CreateRoundF32ToTF32ForTf32DotRewritePass());\n-\n-  // Based on make_ttir() in\n-  // @triton//:third_party/nvidia/backend/compiler.py\n   pm->addPass(mlir::createInlinerPass());\n   pm->addPass(mt::createTritonRewriteTensorPointer());\n-  if (!cc.IsAtLeastHopper()) {\n+  if (!cuda_cc.IsAtLeastHopper()) {\n     pm->addPass(mt::createTritonRewriteTensorDescriptorToPointer());\n   }\n   pm->addPass(mlir::createCanonicalizerPass());\n@@ -68,14 +55,20 @@ absl::Status CreateTritonPipeline(\n   pm->addPass(mlir::createCSEPass());\n   pm->addPass(mlir::createSymbolDCEPass());\n   pm->addPass(mt::createTritonLoopUnroll());\n+}\n \n-  // Based on make_ttgir() in\n-  // @triton//:third_party/nvidia/backend/compiler.py\n+// Based on make_ttgir() in\n+// @triton//:third_party/nvidia/backend/compiler.py\n+static void MakeTTGIR(mlir::OpPassManager* pm,\n+                      const stream_executor::CudaComputeCapability& cuda_cc,\n+                      int num_warps, int num_ctas, int num_stages,\n+                      mlir::triton::nvidia_gpu::ClusterInfo& out_cluster_info) {\n+  const int cuda_cc_as_int = cuda_cc.major * 10 + cuda_cc.minor;\n   pm->addPass(mt::createConvertTritonToTritonGPU(\n-      {absl::StrFormat(\"cuda:%u\", ccAsInt), num_warps, threadsPerWarp,\n-       num_ctas}));\n+      {absl::StrFormat(\"cuda:%u\", cuda_cc_as_int), num_warps,\n+       /*threads_per_warp=*/32, num_ctas}));\n   pm->addPass(mt::gpu::createTritonGPUCoalesce());\n-  if (cc.IsAtLeastAmpere()) {\n+  if (cuda_cc.IsAtLeastAmpere()) {\n     pm->addPass(mt::gpu::createTritonGPUF32DotTC());\n   }\n   pm->addPass(ttng::createTritonNvidiaGPUPlanCTAPass(&out_cluster_info));\n@@ -84,10 +77,10 @@ absl::Status CreateTritonPipeline(\n   pm->addPass(mt::gpu::createTritonGPUAccelerateMatmul());\n   pm->addPass(mt::gpu::createTritonGPURemoveLayoutConversions());\n   pm->addPass(\n-      mt::gpu::createTritonGPUOptimizeDotOperands({cc.IsAtLeastAmpere()}));\n+      mt::gpu::createTritonGPUOptimizeDotOperands({cuda_cc.IsAtLeastAmpere()}));\n   pm->addPass(ttng::createTritonNvidiaGPUOptimizeDescriptorEncodingPass());\n   pm->addPass(mt::createTritonLoopAwareCSE());\n-  if (cc.IsAmpere() || cc.IsHopper()) {\n+  if (cuda_cc.IsAmpere() || cuda_cc.IsHopper()) {\n     pm->addPass(mt::gpu::createTritonGPUFuseNestedLoops());\n     pm->addPass(mlir::createCanonicalizerPass());\n     pm->addPass(mlir::createLoopInvariantCodeMotionPass());\n@@ -97,7 +90,7 @@ absl::Status CreateTritonPipeline(\n     pm->addPass(mt::gpu::createTritonGPUAssignLatencies({num_stages}));\n     pm->addPass(mt::gpu::createTritonGPUScheduleLoops());\n     pm->addPass(mt::gpu::createTritonGPUPipeline({num_stages}));\n-  } else if (cc.IsAtLeastBlackwell()) {\n+  } else if (cuda_cc.IsAtLeastBlackwell()) {\n     pm->addPass(mt::gpu::createTritonGPUFuseNestedLoops());\n     pm->addPass(mlir::createCanonicalizerPass());\n     pm->addPass(mlir::createLoopInvariantCodeMotionPass());\n@@ -119,7 +112,7 @@ absl::Status CreateTritonPipeline(\n   pm->addPass(mt::createTritonLoopAwareCSE());\n   pm->addPass(mt::gpu::createTritonGPUPrefetch());\n   pm->addPass(\n-      mt::gpu::createTritonGPUOptimizeDotOperands({cc.IsAtLeastAmpere()}));\n+      mt::gpu::createTritonGPUOptimizeDotOperands({cuda_cc.IsAtLeastAmpere()}));\n   pm->addPass(mt::gpu::createTritonGPUCoalesceAsyncCopy());\n   pm->addPass(ttng::createTritonNvidiaGPUOptimizeTMemLayoutsPass());\n   pm->addPass(mt::gpu::createTritonGPURemoveLayoutConversions());\n@@ -128,32 +121,37 @@ absl::Status CreateTritonPipeline(\n   pm->addPass(mt::gpu::createTritonGPUReorderInstructions());\n   pm->addPass(mt::createTritonLoopAwareCSE());\n   pm->addPass(mlir::createSymbolDCEPass());\n-  if (cc.IsAtLeastHopper()) {\n+  if (cuda_cc.IsAtLeastHopper()) {\n     pm->addPass(ttng::createTritonNvidiaGPUTMALoweringPass());\n   }\n-  pm->addPass(ttng::createTritonGPUFenceInsertion({ccAsInt}));\n+  pm->addPass(ttng::createTritonGPUFenceInsertion({cuda_cc_as_int}));\n   pm->addPass(ttng::createTritonNvidiaGPUMMALoweringPass());\n   pm->addPass(mlir::createSCCPPass());\n   pm->addPass(mlir::createCanonicalizerPass());\n-\n   // Corresponds to \"mod.get_tensordesc_metadata()\"\n   // in @triton//:third_party/nvidia/backend/compiler.py\n   pm->addPass(mt_xla::CreateExtractTmaInfoPass());\n+}\n \n-  // Based on make_llir() in\n-  // @triton//:third_party/nvidia/backend/compiler.py\n+// Based on make_llir() in\n+// @triton//:third_party/nvidia/backend/compiler.py\n+static void MakeLLIR(mlir::OpPassManager* pm,\n+                     const stream_executor::CudaComputeCapability& cuda_cc) {\n+  const int cuda_cc_as_int = cuda_cc.major * 10 + cuda_cc.minor;\n   pm->addPass(mt::gpu::createTritonGPUCombineTensorSelectAndIf());\n   pm->addPass(mt::gpu::createTritonGPUAllocateWarpGroups());\n   pm->addPass(mlir::createSCFToControlFlowPass());\n   pm->addPass(mt::createAllocateSharedMemoryNvPass(\n-      ccAsInt, mlir::triton::AllocateSharedMemoryNvOptions{}.ptxVersion));\n+      cuda_cc_as_int,\n+      mlir::triton::AllocateSharedMemoryNvOptions{}.ptxVersion));\n   pm->addPass(ttng::createTritonTensorMemoryAllocationPass());\n   // We could add a flag to XLA to optionally enable the following pass:\n   // pm->addPass(mt::instrument::createTritonInstrumentConcurrencySanitizer());\n   pm->addPass(mt::gpu::createTritonGPUGlobalScratchAllocationPass());\n-  pm->addPass(ttng::createTritonGPUProxyFenceInsertion({ccAsInt}));\n+  pm->addPass(ttng::createTritonGPUProxyFenceInsertion({cuda_cc_as_int}));\n   pm->addPass(mt::createConvertTritonGPUToLLVMPass(\n-      ccAsInt, mlir::triton::ConvertTritonGPUToLLVMOptions{}.ptxVersion));\n+      cuda_cc_as_int,\n+      mlir::triton::ConvertTritonGPUToLLVMOptions{}.ptxVersion));\n   pm->addPass(mlir::createCanonicalizerPass());\n   pm->addPass(mlir::createCSEPass());\n   pm->addPass(mt::createConvertNVGPUToLLVM());\n@@ -164,14 +162,16 @@ absl::Status CreateTritonPipeline(\n   pm->addPass(mlir::createSymbolDCEPass());\n   pm->addPass(mlir::createConvertNVVMToLLVMPass());\n   // Note: translateTritonGPUToLLVMIR adds line info with LLVMDIScopePass.\n-\n-  return absl::OkStatus();\n }\n \n-std::string GetLibdevicePath(const HloModuleConfig& hlo_config,\n-                             const se::DeviceDescription& device_info) {\n-  return nvptx::LibDevicePath(\n-      hlo_config.debug_options().xla_gpu_cuda_data_dir());\n+void CreateTritonCudaPipeline(\n+    mlir::OpPassManager* pm,\n+    const stream_executor::CudaComputeCapability& cuda_cc, int num_warps,\n+    int num_ctas, int num_stages,\n+    mlir::triton::nvidia_gpu::ClusterInfo& out_cluster_info) {\n+  MakeTTIR(pm, cuda_cc);\n+  MakeTTGIR(pm, cuda_cc, num_warps, num_ctas, num_stages, out_cluster_info);\n+  MakeLLIR(pm, cuda_cc);\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "4bc90d19f7f4d8d37c2efc83ae46b4dde56c056f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline_rocm.cc",
            "status": "modified",
            "additions": 35,
            "deletions": 51,
            "changes": 86,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_rocm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_rocm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_rocm.cc?ref=8a441cd80b438bca7c94e060ae5b5096f95543c4",
            "patch": "@@ -14,9 +14,10 @@ limitations under the License.\n ==============================================================================*/\n // TODO(ROCm): Enable and include ROCm Triton passes when ROCm Triton is\n // included in build.\n+\n #include <string>\n-#include <utility>\n \n+#include \"absl/strings/str_cat.h\"\n #include \"third_party/amd/include/TritonAMDGPUToLLVM/Passes.h\"\n #include \"third_party/amd/include/TritonAMDGPUTransforms/Passes.h\"\n #include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n@@ -25,42 +26,20 @@ limitations under the License.\n #include \"mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Transforms/Passes.h\"\n-#include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n-#include \"xla/service/gpu/llvm_gpu_backend/amdgpu_backend.h\"\n-#include \"xla/service/gpu/matmul_utils.h\"\n-#include \"xla/service/hlo_module_config.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/tsl/platform/rocm_rocdl_path.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/Passes.h\"\n #include \"triton/Conversion/TritonToTritonGPU/Passes.h\"\n #include \"triton/Dialect/Triton/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n-#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n \n namespace xla {\n namespace gpu {\n \n-namespace ma = ::mlir::arith;\n-namespace mm = ::mlir::math;\n-namespace ml = ::mlir::LLVM;\n namespace mt = ::mlir::triton;\n-namespace mt_xla = ::mlir::triton::xla;\n-\n-using ::llvm::SmallVector;\n-using mlir::ArrayRef;\n-using ::mlir::ShapedType;\n-using ::mlir::Type;\n-using ::mlir::Value;\n-using mlir::ValueRange;\n-\n-absl::Status CreateTritonPipeline(\n-    mlir::OpPassManager* pm, std::string arch_name, int num_warps, int num_ctas,\n-    int num_stages, mt::nvidia_gpu::ClusterInfo& out_cluster_info) {\n-  const int threadsPerWarp = (arch_name[3] == '9') ? 64 : 32;\n-  auto cc = se::RocmComputeCapability(std::move(arch_name));\n \n-  // Based on make_ttir() in\n-  // @triton//:third_party/amd/backend/compiler.py\n+// Based on make_ttir() in\n+// @triton//:third_party/amd/backend/compiler.py\n+static void MakeTTIR(mlir::OpPassManager* pm) {\n   pm->addPass(mlir::createInlinerPass());\n   pm->addPass(mt::createTritonRewriteTensorPointer());\n   pm->addPass(mt::createTritonRewriteTensorDescriptorToPointer());\n@@ -71,16 +50,22 @@ absl::Status CreateTritonPipeline(\n   pm->addPass(mlir::createLoopInvariantCodeMotionPass());\n   pm->addPass(mlir::createSymbolDCEPass());\n   pm->addPass(mt::createTritonLoopUnroll());\n+}\n \n-  // Based on make_ttgir() in\n-  // @triton//:third_party/amd/backend/compiler.py\n+// Based on make_ttgir() in\n+// @triton//:third_party/amd/backend/compiler.py\n+static void MakeTTGIR(mlir::OpPassManager* pm,\n+                      const stream_executor::RocmComputeCapability& rocm_cc,\n+                      int num_warps, int num_ctas, int num_stages) {\n+  const std::string& arch_name = rocm_cc.gcn_arch_name();\n+  const int threadsPerWarp = (arch_name[3] == '9') ? 64 : 32;\n   pm->addPass(mt::createConvertTritonToTritonGPU(\n-      {absl::StrCat(\"hip:\", cc.gfx_version()), num_warps, threadsPerWarp,\n+      {absl::StrCat(\"hip:\", rocm_cc.gfx_version()), num_warps, threadsPerWarp,\n        num_ctas}));\n   pm->addPass(mt::gpu::createTritonGPUCoalesce());\n   pm->addPass(mt::gpu::createTritonGPURemoveLayoutConversions());\n   pm->addPass(mt::gpu::createTritonGPUOptimizeThreadLocality());\n-  // TODO ROCm Pass cc.gfx_version() after fixing issue with fmfa\n+  // TODO ROCm Pass rocm_cc.gfx_version() after fixing issue with fmfa\n   pm->addPass(mlir::createTritonAMDGPUAccelerateMatmul({arch_name}));\n   pm->addPass(mt::gpu::createTritonGPURemoveLayoutConversions());\n   // TODO ROCm Check if we want to compare MI100 and greater\n@@ -94,7 +79,7 @@ absl::Status CreateTritonPipeline(\n   pm->addPass(mlir::createLoopInvariantCodeMotionPass());\n   pm->addPass(mlir::createCanonicalizerPass());\n \n-  if (cc.has_amd_matrix_core()) {\n+  if (rocm_cc.has_amd_matrix_core()) {\n     pm->addPass(mlir::createTritonAMDGPUStreamPipeline(\n         {num_stages, /*global_prefetch=*/0, /*local_prefetch=*/0,\n          /*use_async_copy=*/false, /*use_block_pingpong=*/false}));\n@@ -114,7 +99,7 @@ absl::Status CreateTritonPipeline(\n     pm->addPass(mlir::createTritonAMDGPUInThreadTranspose());\n     pm->addPass(mt::gpu::createTritonGPURemoveLayoutConversions());\n   }\n-  if (cc.has_amd_matrix_core()) {\n+  if (rocm_cc.has_amd_matrix_core()) {\n     pm->addPass(mt::gpu::createTritonGPUReorderInstructions());\n   }\n   if (/*use_block_pingpong=*/false) {\n@@ -129,16 +114,21 @@ absl::Status CreateTritonPipeline(\n   pm->addPass(mlir::createCanonicalizerPass());\n   pm->addPass(mlir::createCSEPass());\n   pm->addPass(mlir::createSymbolDCEPass());\n+}\n \n-  // Based on make_llir() in\n-  // @triton//:third_party/amd/backend/compiler.py\n+// Based on make_llir() in\n+// @triton//:third_party/amd/backend/compiler.py\n+static void MakeLLIR(mlir::OpPassManager* pm,\n+                     const stream_executor::RocmComputeCapability& rocm_cc,\n+                     int num_stages) {\n   const int custom_lds_size = 0;\n-  pm->addPass(mlir::triton::AMD::createOptimizeLDSUsagePass(cc.gfx_version(),\n-                                                            custom_lds_size));\n+  pm->addPass(mlir::triton::AMD::createOptimizeLDSUsagePass(\n+      rocm_cc.gfx_version(), custom_lds_size));\n   pm->addPass(mlir::createSCFToControlFlowPass());\n   pm->addPass(mlir::createConvertIndexToLLVMPass());\n   pm->addPass(mt::gpu::createAllocateSharedMemory());\n-  pm->addPass(mt::createConvertTritonAMDGPUToLLVMPass(cc.gfx_version(), true));\n+  pm->addPass(\n+      mt::createConvertTritonAMDGPUToLLVMPass(rocm_cc.gfx_version(), true));\n   pm->addPass(mlir::createCanonicalizerPass());\n   pm->addPass(mlir::createCSEPass());\n   // Note: translateTritonGPUToLLVMIR adds line info with LLVMDIScopePass.\n@@ -149,24 +139,18 @@ absl::Status CreateTritonPipeline(\n   pm->addPass(mlir::createSymbolDCEPass());\n   if (/*(instruction_sched_variant==\"none\") == */ false) {\n     pm->addPass(mt::createTritonAMDGPULowerInstructionSchedHintsPass(\n-        cc.gfx_version(), num_stages));\n+        rocm_cc.gfx_version(), num_stages));\n   }\n   pm->addPass(mt::createConvertBuiltinFuncToLLVMPass(/*ftz=*/true));\n-  // There is no clusters in ROCm for now.\n-  out_cluster_info.clusterDimX = 1;\n-  out_cluster_info.clusterDimY = 1;\n-  out_cluster_info.clusterDimZ = 1;\n-\n-  return absl::OkStatus();\n }\n \n-std::string GetLibdevicePath(const HloModuleConfig& hlo_config,\n-                             const se::DeviceDescription& device_info) {\n-  std::string libdevice_dir = tsl::RocdlRoot();\n-  auto compute_capability = device_info.rocm_compute_capability();\n-  const std::string libdevice_path =\n-      amdgpu::LibDevicePath(compute_capability.gcn_arch_name(), libdevice_dir);\n-  return libdevice_path;\n+void CreateTritonRocmPipeline(\n+    mlir::OpPassManager* pm,\n+    const stream_executor::RocmComputeCapability& rocm_cc, int num_warps,\n+    int num_ctas, int num_stages) {\n+  MakeTTIR(pm);\n+  MakeTTGIR(pm, rocm_cc, num_warps, num_ctas, num_stages);\n+  MakeLLIR(pm, rocm_cc, num_stages);\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "5590401ba9a8709e2d6cd1d974b68251e8466be9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline_stub.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b2410072f13a2b523f27c8aacf52788c2aebcbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_stub.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b2410072f13a2b523f27c8aacf52788c2aebcbb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_stub.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_stub.cc?ref=1b2410072f13a2b523f27c8aacf52788c2aebcbb",
            "patch": "@@ -1,32 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include <string>\n-\n-#include \"absl/status/status.h\"\n-#include \"mlir/Pass/PassManager.h\"\n-#include \"xla/backends/gpu/codegen/triton/compilation_pipeline.h\"\n-\n-namespace xla {\n-namespace gpu {\n-\n-absl::Status CreateTritonPipeline(\n-    mlir::OpPassManager* pm, std::string arch_name, int num_warps, int num_ctas,\n-    int num_stages, mlir::triton::nvidia_gpu::ClusterInfo& out_cluster_info) {\n-  return absl::UnimplementedError(\"not supported for this build configuration\");\n-}\n-\n-}  // namespace gpu\n-}  // namespace xla"
        },
        {
            "sha": "371b35b4845946607c1860c8efa61a833d1818b0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 44,
            "deletions": 61,
            "changes": 105,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=8a441cd80b438bca7c94e060ae5b5096f95543c4",
            "patch": "@@ -43,6 +43,7 @@ limitations under the License.\n #include \"llvm/IR/Module.h\"\n #include \"llvm/Linker/Linker.h\"\n #include \"llvm/Support/FileSystem.h\"\n+#include \"llvm/Support/LogicalResult.h\"\n #include \"llvm/Support/raw_ostream.h\"\n #include \"mlir/Conversion/AffineToStandard/AffineToStandard.h\"\n #include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n@@ -86,9 +87,7 @@ limitations under the License.\n #include \"mlir/Target/LLVMIR/Dialect/ROCDL/ROCDLToLLVMIRTranslation.h\"\n #include \"mlir/Target/LLVMIR/Export.h\"\n #include \"mlir/Transforms/Passes.h\"\n-#include \"xla/autotuning.pb.h\"\n #include \"xla/backends/gpu/codegen/emitters/ir/xla_gpu_ops.h\"\n-#include \"xla/backends/gpu/codegen/emitters/transforms/passes.h\"\n #include \"xla/backends/gpu/codegen/triton/compilation_pipeline.h\"\n #include \"xla/backends/gpu/codegen/triton/dot_algorithms.h\"\n #include \"xla/backends/gpu/codegen/triton/emitter_helpers.h\"\n@@ -118,6 +117,8 @@ limitations under the License.\n #include \"xla/service/dump.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/llvm_gpu_backend/amdgpu_backend.h\"\n+#include \"xla/service/gpu/llvm_gpu_backend/nvptx_libdevice_path.h\"\n #include \"xla/service/gpu/model/symbolic_tile_analysis.h\"\n #include \"xla/service/gpu/model/tiled_hlo_computation.h\"\n #include \"xla/service/gpu/model/tiled_hlo_instruction.h\"\n@@ -135,16 +136,15 @@ limitations under the License.\n #include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/tools/hlo_decomposer.h\"\n #include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/rocm_rocdl_path.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/path.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/Passes.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n \n namespace xla {\n namespace gpu {\n@@ -2074,21 +2074,24 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n   return std::move(triton_module);\n }\n \n+absl::Status CheckAtLeastAmpere(const se::GpuComputeCapability& gpu_cc) {\n+  if (auto* cuda_cc = std::get_if<se::CudaComputeCapability>(&gpu_cc);\n+      cuda_cc != nullptr && !cuda_cc->IsAtLeastAmpere()) {\n+    return absl::FailedPreconditionError(\n+        absl::StrCat(\"Triton support is only enabled for Ampere GPUs (compute \",\n+                     \"capability 8.0) and up, but got compute capability \",\n+                     cuda_cc->ToString(), \".\"));\n+  }\n+  return absl::OkStatus();\n+}\n+\n absl::StatusOr<TritonWrapperResult> TritonWrapper(\n     absl::string_view fn_name, const HloFusionInstruction* fusion,\n-    const se::GpuComputeCapability& cc,\n+    const se::GpuComputeCapability& gpu_cc,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n     llvm::Module* llvm_module, mlir::MLIRContext& mlir_context) {\n-  if (std::holds_alternative<se::CudaComputeCapability>(cc)) {\n-    auto ccCuda = std::get<se::CudaComputeCapability>(cc);\n-    if (!ccCuda.IsAtLeastAmpere()) {\n-      return absl::FailedPreconditionError(absl::StrCat(\n-          \"Triton support is only enabled for Ampere GPUs (compute \",\n-          \"capability 8.0) and up, but got compute capability \", ccCuda.major,\n-          \".\", ccCuda.minor, \".\"));\n-    }\n-  }\n+  TF_RETURN_IF_ERROR(CheckAtLeastAmpere(gpu_cc));\n \n   TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> triton_module,\n                       CreateTritonModule(fn_name, fusion, device_info,\n@@ -2112,18 +2115,10 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n     const BlockLevelParameters& block_level_parameters,\n     mlir::ModuleOp triton_module, llvm::Module* llvm_module,\n     mlir::MLIRContext& mlir_context, bool is_xla_fusion, bool emit_kernel) {\n-  const auto& cc = device_info.gpu_compute_capability();\n+  const auto& gpu_cc = device_info.gpu_compute_capability();\n+  TF_RETURN_IF_ERROR(CheckAtLeastAmpere(gpu_cc));\n   std::string arch_name =\n-      std::visit([](auto& cc) { return cc.ToString(); }, cc);\n-  if (std::holds_alternative<se::CudaComputeCapability>(cc)) {\n-    auto ccCuda = std::get<se::CudaComputeCapability>(cc);\n-    if (!ccCuda.IsAtLeastAmpere()) {\n-      return absl::FailedPreconditionError(absl::StrCat(\n-          \"Triton support is only enabled for Ampere GPUs (compute \",\n-          \"capability 8.0) and up, but got compute capability \", ccCuda.major,\n-          \".\", ccCuda.minor, \".\"));\n-    }\n-  }\n+      std::visit([](auto& cc) { return cc.ToString(); }, gpu_cc);\n \n   const HloModuleConfig& hlo_config = hlo_module.config();\n \n@@ -2180,55 +2175,32 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n     }\n   }\n \n-  pm.addPass(mlir::triton::xla::CreateTritonXLASqueezeDimsPass());\n-  pm.addPass(mlir::triton::xla::CreateTritonXLAFoldTransposePass());\n-\n-  if (is_xla_fusion) {\n-    pm.addPass(\n-        mlir::triton::xla::CreateInt4ToPackedInt4RewritePass(device_info));\n-  }\n-  if (hlo_module.config()\n-          .debug_options()\n-          .xla_gpu_experimental_scaled_dot_with_triton()) {\n-    pm.addPass(mlir::triton::xla::CreateTritonXLAConvertUnsupportedTypesPass());\n-  }\n-\n-  pm.addPass(mlir::triton::xla::CreateTritonXLAExtractInsertToTritonPass(\n-      block_level_parameters.is_tma_allowed &&\n-      stream_executor::gpu::IsTmaAvailableForDevice(device_info)));\n-\n-  // Lower affine expressions into arithmetic ops.\n-  pm.addPass(mlir::createLowerAffinePass());\n-\n-  // Lower xla_gpu.apply_indexing into arithmetic ops.\n-  pm.addPass(emitters::CreateSimplifyAffinePass());\n-  pm.addPass(CreateConvertIndexTypePass());\n+  CreateTritonXlaPipeline(&pm, gpu_cc, /*rewrite_int4=*/is_xla_fusion,\n+                          block_level_parameters.is_tma_allowed,\n+                          /*convert_unsupported_types=*/\n+                          hlo_module.config()\n+                              .debug_options()\n+                              .xla_gpu_experimental_scaled_dot_with_triton());\n \n-  int64_t num_warps = block_level_parameters.num_warps;\n+  int num_warps = block_level_parameters.num_warps;\n   int num_ctas = block_level_parameters.num_ctas;\n   int num_stages = block_level_parameters.num_stages;\n-\n   if (num_warps <= 0 || num_ctas <= 0 || num_stages <= 0) {\n     return absl::FailedPreconditionError(absl::StrCat(\n         \"(num_warps, num_ctas, num_stages) must be positive, but got: (\",\n         num_warps, \", \", num_ctas, \", \", num_stages, \")\"));\n   }\n-\n   mlir::triton::nvidia_gpu::ClusterInfo cluster_info;\n-  if (!CreateTritonPipeline(&pm, arch_name, num_warps, num_ctas, num_stages,\n-                            cluster_info)\n-           .ok()) {\n-    return Internal(\"Failed to create Triton pipeline.\");\n-  }\n+  CreateTritonPipeline(&pm, gpu_cc, num_warps, num_ctas, num_stages,\n+                       cluster_info);\n+\n   // Triton generates pointers to the global address space, while XLA needs a\n   // kernel signature with pointers to the generic address space.\n   pm.addPass(mlir::triton::xla::CreateGeneralizeKernelSignaturePass());\n   // llvm::Linker::linkModules() segfaults if we don't strip locations.\n   pm.addPass(mlir::createStripDebugInfoPass());\n \n-  bool succeeded = mlir::succeeded(pm.run(triton_module));\n-\n-  if (!succeeded) {\n+  if (failed(pm.run(triton_module))) {\n     return Internal(\"Failed to compile Triton kernel.\");\n   }\n \n@@ -2241,8 +2213,8 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n         shared_mem_bytes, device_info.shared_memory_per_block_optin()));\n   }\n \n-  if (std::holds_alternative<se::CudaComputeCapability>(cc) &&\n-      std::get<se::CudaComputeCapability>(cc).IsBlackwell()) {\n+  if (auto* cuda_cc = std::get_if<se::CudaComputeCapability>(&gpu_cc);\n+      cuda_cc != nullptr && cuda_cc->IsBlackwell()) {\n     // https://docs.nvidia.com/cuda/parallel-thread-execution/#tensor-memory\n     constexpr int kTensorMemoryColumns = 512;\n     const int tensor_mem_columns =\n@@ -2319,5 +2291,16 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n       {shared_mem_bytes, cluster_dim, tma_metadata, captured_nvvm_annotations}};\n }\n \n+std::string GetLibdevicePath(const HloModuleConfig& hlo_config,\n+                             const se::DeviceDescription& device_info) {\n+  if (std::holds_alternative<se::CudaComputeCapability>(\n+          device_info.gpu_compute_capability())) {\n+    return nvptx::LibDevicePath(\n+        hlo_config.debug_options().xla_gpu_cuda_data_dir());\n+  }\n+  return amdgpu::LibDevicePath(\n+      device_info.rocm_compute_capability().gcn_arch_name(), tsl::RocdlRoot());\n+}\n+\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "2076821ea9621c751a20d04af55870951640c4c0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.h",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h?ref=8a441cd80b438bca7c94e060ae5b5096f95543c4",
            "patch": "@@ -18,7 +18,6 @@ limitations under the License.\n \n #include <cstdint>\n #include <optional>\n-#include <vector>\n \n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n@@ -27,19 +26,17 @@ limitations under the License.\n #include \"llvm/IR/Metadata.h\"\n #include \"llvm/IR/Module.h\"\n #include \"llvm/Support/raw_ostream.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n #include \"mlir/IR/Value.h\"\n-#include \"mlir/IR/ValueRange.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"xla/autotuning.pb.h\"\n-#include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/gpu/model/symbolic_tile_analysis.h\"\n #include \"xla/service/gpu/model/tiled_hlo_computation.h\"\n-#include \"xla/service/gpu/model/tiled_hlo_instruction.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\""
        },
        {
            "sha": "c1e959bdd31a72bfb652dffee83a9a932b4a9007",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_stub.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub.cc?ref=8a441cd80b438bca7c94e060ae5b5096f95543c4",
            "patch": "@@ -18,17 +18,13 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/IR/Module.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n-#include \"mlir/IR/Value.h\"\n #include \"mlir/Pass/PassManager.h\"\n-#include \"xla/autotuning.pb.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter.h\"\n-#include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/hlo/ir/hlo_clone_context.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/gpu/model/tiled_hlo_computation.h\""
        },
        {
            "sha": "58be24d5e8bc8c82c10ff2056d5b04a95cdb66f3",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_stub_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_stub_test.cc?ref=8a441cd80b438bca7c94e060ae5b5096f95543c4",
            "patch": "@@ -17,7 +17,6 @@ limitations under the License.\n #include \"mlir/IR/Location.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/Pass/PassManager.h\"\n-#include \"xla/backends/gpu/codegen/triton/compilation_pipeline.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter_legacy_matmul.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n@@ -29,13 +28,6 @@ limitations under the License.\n #include \"xla/service/gpu/model/tiled_hlo_instruction.h\"\n #include \"xla/service/hlo_module_config.h\"\n \n-namespace mlir::triton::nvidia_gpu {\n-// We define ClusterInfo here in order to avoid having to import a GPU-only\n-// header.\n-struct ClusterInfo {};\n-\n-}  // namespace mlir::triton::nvidia_gpu\n-\n namespace xla::gpu {\n namespace {\n \n@@ -51,9 +43,7 @@ TEST(TritonStub, CallStubApi) {\n                    .ok());\n \n   mlir::OpPassManager pm;\n-  ::mlir::triton::nvidia_gpu::ClusterInfo cluster_info;\n \n-  EXPECT_FALSE(CreateTritonPipeline(&pm, \"\", 1, 1, 1, cluster_info).ok());\n   EXPECT_EQ(GetLibdevicePath({}, {}), \"\");\n \n   HloConstantInstruction constant(LiteralUtil::CreateR1<int>({1, 1}));"
        },
        {
            "sha": "0c3b6d03f32b8efff6b8ccd25b07a883b6c203a5",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/int4_passes.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 30,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fint4_passes.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fint4_passes.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fint4_passes.cc?ref=8a441cd80b438bca7c94e060ae5b5096f95543c4",
            "patch": "@@ -13,15 +13,13 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n #include <algorithm>\n-#include <cmath>\n #include <cstdint>\n #include <functional>\n #include <iterator>\n #include <memory>\n #include <optional>\n #include <string>\n #include <utility>\n-#include <variant>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n@@ -54,9 +52,8 @@ limitations under the License.\n #include \"mlir/Transforms/DialectConversion.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n+#include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n-#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n-#include \"xla/stream_executor/device_description.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n \n@@ -743,25 +740,12 @@ LogicalResult SitofpToExtFpSitofpRewrite(ma::SIToFPOp sitofp_op,\n   return success();\n }\n \n-class PlainInt4ToPackedInt4RewritePass\n-    : public impl::LoadInt4RewritePassBase<PlainInt4ToPackedInt4RewritePass> {\n+class LoadInt4RewritePass\n+    : public impl::LoadInt4RewritePassBase<LoadInt4RewritePass> {\n  public:\n   using Base::Base;\n-  PlainInt4ToPackedInt4RewritePass(\n-      const PlainInt4ToPackedInt4RewritePass &other) = default;\n-  explicit PlainInt4ToPackedInt4RewritePass(\n-      const stream_executor::DeviceDescription &device_description)\n-      : bf16x2_enabled_(IsBF16x2Enabled(device_description)) {}\n \n  private:\n-  static bool IsBF16x2Enabled(\n-      const stream_executor::DeviceDescription &device_description) {\n-    bool is_cuda =\n-        std::holds_alternative<stream_executor::CudaComputeCapability>(\n-            device_description.gpu_compute_capability());\n-    return is_cuda &&\n-           device_description.cuda_compute_capability().IsAtLeastHopper();\n-  }\n   // The pass converts the types like tensor<AxBxi4> to tensor<AxB/2xi8>\n   // (assuming B is the packed dimension) in the Triton dialect and replaces\n   // the ExtSIOp with the unpack sequence that accepts twice smaller i8 tensor\n@@ -815,7 +799,7 @@ class PlainInt4ToPackedInt4RewritePass\n     });\n     RewritePatternSet patterns(ctx);\n     scf::populateSCFStructuralTypeConversions(converter, patterns);\n-    patterns.add<ExtSIInt4ToInt8Pattern>(converter, ctx, bf16x2_enabled_);\n+    patterns.add<ExtSIInt4ToInt8Pattern>(converter, ctx, enable_bf16x2_);\n \n     // TODO(b/393299275): LoadOp, AdvanceOp, AddPtrOp, and MakeTensorPtrOp will\n     // not be emitted by the generic Triton emitter. Remove these once the\n@@ -839,18 +823,10 @@ class PlainInt4ToPackedInt4RewritePass\n       return signalPassFailure();\n     }\n   }\n-  // The default value is true, which means that bf16x2 instructions are used\n-  // when the device supports them. We need this for the mlir lit tests to pass.\n-  const bool bf16x2_enabled_ = true;\n };\n \n-std::unique_ptr<mlir::Pass> CreateInt4ToPackedInt4RewritePass() {\n-  return std::make_unique<PlainInt4ToPackedInt4RewritePass>();\n-}\n-\n-std::unique_ptr<Pass> CreateInt4ToPackedInt4RewritePass(\n-    const stream_executor::DeviceDescription &device_description) {\n-  return std::make_unique<PlainInt4ToPackedInt4RewritePass>(device_description);\n+std::unique_ptr<Pass> CreateInt4ToPackedInt4RewritePass(bool enable_bf16x2) {\n+  return createLoadInt4RewritePass(LoadInt4RewritePassOptions{enable_bf16x2});\n }\n \n }  // namespace mlir::triton::xla"
        },
        {
            "sha": "c89cca8cea2b0629ca598c361b21c92e93f583b6",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h?ref=8a441cd80b438bca7c94e060ae5b5096f95543c4",
            "patch": "@@ -22,7 +22,6 @@ limitations under the License.\n #include \"mlir/IR/Operation.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"  // IWYU pragma: keep\n-#include \"xla/stream_executor/device_description.h\"\n \n namespace mlir::triton::xla {\n \n@@ -37,7 +36,7 @@ std::unique_ptr<mlir::Pass> CreateTritonXLAFoldTransposePass();\n std::unique_ptr<mlir::Pass> CreateGeneralizeKernelSignaturePass();\n \n std::unique_ptr<mlir::Pass> CreateInt4ToPackedInt4RewritePass(\n-    const stream_executor::DeviceDescription& device_description);\n+    bool enable_bf16x2);\n std::unique_ptr<mlir::Pass> CreateRoundF32ToTF32ForTf32DotRewritePass();\n std::unique_ptr<mlir::Pass> CreateExtractTmaInfoPass();\n std::unique_ptr<mlir::Pass> CreateTritonXLALowerGetTidPass();"
        },
        {
            "sha": "47873944b6f0f6402ef2cb8d4b507805a5bb8b6f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.td",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td?ref=8a441cd80b438bca7c94e060ae5b5096f95543c4",
            "patch": "@@ -95,6 +95,10 @@ def LoadInt4RewritePass\n   let dependentDialects = [\n     \"triton::TritonDialect\"\n   ];\n+  let options = [\n+    Option<\"enable_bf16x2_\", \"enable_bf16x2\", \"bool\", \"true\",\n+           \"Whether to enable bf16x2.\">,\n+  ];\n }\n \n def RoundF32ToTF32ForTf32DotRewritePass"
        },
        {
            "sha": "50bc651e4e4993f9e18860b8c13d43a118fd35b1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_pipeline.mlir",
            "status": "added",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_pipeline.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_pipeline.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_pipeline.mlir?ref=8a441cd80b438bca7c94e060ae5b5096f95543c4",
            "patch": "@@ -0,0 +1,16 @@\n+// RUN: xla-opt %s --triton-xla-pipeline='target=9.0' \\\n+// RUN:   | FileCheck %s --check-prefix=CHECK --check-prefix=CUDA\n+//\n+// RUN: xla-opt %s --triton-xla-pipeline='target=gfx950' \\\n+// RUN:   | FileCheck %s --check-prefix=CHECK --check-prefix=ROCM\n+\n+// CHECK: module attributes\n+// CUDA: ttg.target = \"cuda:90\"\n+// ROCM: ttg.target = \"hip:gfx950\"\n+\n+// CHECK: llvm.func @func\n+tt.func @func() {\n+  // CHECK: llvm.return\n+  tt.return\n+}\n+"
        },
        {
            "sha": "bbf024511dd856d5749c7bb61288c25ffeb63ce5",
            "filename": "third_party/xla/xla/pjrt/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fpjrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fpjrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2FBUILD?ref=8a441cd80b438bca7c94e060ae5b5096f95543c4",
            "patch": "@@ -1217,7 +1217,9 @@ cc_library(\n             \"@llvm-project//mlir:SCFToControlFlow\",\n             \"@llvm-project//mlir:ToLLVMIRTranslation\",\n             \"@llvm-project//mlir:Transforms\",\n-            \"//xla/backends/gpu/codegen/triton:compilation_pipeline_cuda\",\n+            \"//xla/backends/gpu/codegen/triton:compilation_pipeline\",\n+            \"//xla/stream_executor:device_description\",\n+            \"//xla/stream_executor/cuda:cuda_compute_capability\",\n             \"//xla/tsl/platform:errors\",\n             \"//xla/tsl/platform:statusor\",\n             \"@triton//:TritonDialects\","
        },
        {
            "sha": "d26327842522b700776fd8be338366f36fbb48f5",
            "filename": "third_party/xla/xla/pjrt/triton_cuda.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 16,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fpjrt%2Ftriton_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fpjrt%2Ftriton_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Ftriton_cuda.cc?ref=8a441cd80b438bca7c94e060ae5b5096f95543c4",
            "patch": "@@ -36,6 +36,7 @@ limitations under the License.\n #include \"llvm/Linker/Linker.h\"\n #include \"llvm/MC/TargetRegistry.h\"\n #include \"llvm/Support/CodeGen.h\"\n+#include \"llvm/Support/LogicalResult.h\"\n #include \"llvm/Support/SourceMgr.h\"\n #include \"llvm/Support/TargetSelect.h\"\n #include \"llvm/Support/raw_ostream.h\"\n@@ -60,6 +61,8 @@ limitations under the License.\n #include \"mlir/Target/LLVMIR/Export.h\"\n #include \"xla/backends/gpu/codegen/triton/compilation_pipeline.h\"\n #include \"xla/pjrt/triton.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n@@ -74,20 +77,6 @@ namespace xla::triton {\n \n namespace {\n \n-absl::Status TritonToLLVM(\n-    mlir::ModuleOp module, absl::string_view arch_name, int num_warps,\n-    int num_ctas, int num_stages,\n-    mlir::triton::nvidia_gpu::ClusterInfo* out_cluster_info) {\n-  mlir::PassManager pm(module.getContext());\n-  pm.enableVerifier();\n-  TF_RETURN_IF_ERROR(\n-      xla::gpu::CreateTritonPipeline(&pm, std::string(arch_name), num_warps,\n-                                     num_ctas, num_stages, *out_cluster_info));\n-  return pm.run(module).succeeded()\n-             ? absl::OkStatus()\n-             : absl::InternalError(\"Failed to compile Triton IR to LLVM IR\");\n-}\n-\n absl::StatusOr<std::unique_ptr<llvm::TargetMachine>> CreateTargetMachine(\n     llvm::Module* module, absl::string_view arch_name, bool enable_fp_fusion,\n     absl::string_view features) {\n@@ -237,9 +226,18 @@ absl::StatusOr<CompilationResult> Compile(absl::string_view module,\n   if (!module_op) {\n     return absl::InvalidArgumentError(\"Failed to parse Triton module\");\n   }\n+\n+  mlir::PassManager pm(&context);\n+  pm.enableVerifier();\n   mlir::triton::nvidia_gpu::ClusterInfo cluster_info;\n-  TF_RETURN_IF_ERROR(TritonToLLVM(*module_op, arch_name, num_warps, num_ctas,\n-                                  num_stages, &cluster_info));\n+  TF_ASSIGN_OR_RETURN(\n+      auto cuda_cc,\n+      stream_executor::CudaComputeCapability::FromString(arch_name));\n+  xla::gpu::CreateTritonPipeline(&pm, cuda_cc, num_warps, num_ctas, num_stages,\n+                                 cluster_info);\n+  if (failed(pm.run(*module_op))) {\n+    return absl::InternalError(\"Failed to compile Triton IR to LLVM IR\");\n+  }\n \n   auto shared_mem_bytes =\n       (*module_op)->getAttrOfType<mlir::IntegerAttr>(\"ttg.shared\").getInt();"
        },
        {
            "sha": "16bf352b687594e86ce771de5721b850070f030b",
            "filename": "third_party/xla/xla/service/gpu/tests/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD?ref=8a441cd80b438bca7c94e060ae5b5096f95543c4",
            "patch": "@@ -662,21 +662,28 @@ lit_test_suite(\n #     name = \"xla-opt\",\n #     srcs = [\"xla-opt.cc\"],\n #     deps = [\n+#         \"@llvm-project//llvm:Support\",\n #         \"@llvm-project//mlir:AllExtensions\",\n #         \"@llvm-project//mlir:BuiltinToLLVMIRTranslation\",\n #         \"@llvm-project//mlir:FuncDialect\",\n #         \"@llvm-project//mlir:FuncExtensions\",\n #         \"@llvm-project//mlir:LLVMIRTransforms\",\n #         \"@llvm-project//mlir:LLVMToLLVMIRTranslation\",\n #         \"@llvm-project//mlir:MlirOptLib\",\n+#         \"@llvm-project//mlir:Pass\",\n #         \"@llvm-project//mlir:RegisterAllExtensions\",\n+#         \"@llvm-project//mlir:Support\",\n #         \"@llvm-project//mlir:TensorDialect\",\n #         \"//xla/backends/gpu/codegen/emitters/transforms:passes\",\n+#         \"//xla/backends/gpu/codegen/triton:compilation_pipeline\",\n #         \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n #         \"//xla/backends/gpu/codegen/triton/transforms:passes\",\n #         \"//xla/codegen/emitters/ir:xla\",\n #         \"//xla/codegen/emitters/transforms:passes\",\n+#         \"//xla/stream_executor:device_description\",\n+#         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n #         \"@triton//:AllPassesAndDialects\",\n+#         \"@triton//:TritonNvidiaGPUTransforms\",\n #         \"@triton//third_party/amd:TestAMDAnalysis\",\n #     ],\n # )"
        },
        {
            "sha": "d2e5a95816bdc586628ac79cd27e472d8a5f334d",
            "filename": "third_party/xla/xla/service/gpu/tests/xla-opt.cc",
            "status": "modified",
            "additions": 52,
            "deletions": 0,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fxla-opt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8a441cd80b438bca7c94e060ae5b5096f95543c4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fxla-opt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fxla-opt.cc?ref=8a441cd80b438bca7c94e060ae5b5096f95543c4",
            "patch": "@@ -13,20 +13,72 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include <string>\n+\n+#include \"llvm/Support/CommandLine.h\"\n #include \"mlir/Dialect/Func/Extensions/InlinerExtension.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/Dialect/LLVMIR/Transforms/InlinerInterfaceImpl.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/InitAllExtensions.h\"\n+#include \"mlir/Pass/PassOptions.h\"\n+#include \"mlir/Pass/PassRegistry.h\"\n+#include \"mlir/Support/LLVM.h\"\n #include \"mlir/Target/LLVMIR/Dialect/Builtin/BuiltinToLLVMIRTranslation.h\"\n #include \"mlir/Target/LLVMIR/Dialect/LLVMIR/LLVMToLLVMIRTranslation.h\"\n #include \"mlir/Tools/mlir-opt/MlirOptMain.h\"\n #include \"xla/backends/gpu/codegen/emitters/transforms/passes.h\"\n+#include \"xla/backends/gpu/codegen/triton/compilation_pipeline.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n #include \"xla/codegen/emitters/ir/xla_dialect.h\"\n #include \"xla/codegen/emitters/transforms/passes.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/device_description.h\"\n #include \"third_party/triton/bin/RegisterTritonDialects.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n+\n+namespace {\n+\n+mlir::triton::nvidia_gpu::ClusterInfo cluster_info;\n+\n+struct TritonPipelineOptions\n+    : public mlir::PassPipelineOptions<TritonPipelineOptions> {\n+  Option<std::string> target{*this, \"target\", llvm::cl::init(\"8.0\")};\n+  Option<bool> rewrite_int4{*this, \"rewrite-int4\", llvm::cl::init(true)};\n+  Option<bool> allow_tma{*this, \"allow-tma\", llvm::cl::init(false)};\n+  Option<bool> convert_unsupported_types{*this, \"convert-unsupported-types\",\n+                                         llvm::cl::init(true)};\n+  Option<int> num_warps{*this, \"num-warps\", llvm::cl::init(4)};\n+  Option<int> num_ctas{*this, \"num-ctas\", llvm::cl::init(1)};\n+  Option<int> num_stages{*this, \"num-stages\", llvm::cl::init(3)};\n+};\n+\n+mlir::PassPipelineRegistration<TritonPipelineOptions>\n+    register_triton_xla_pipeline(\n+        \"triton-xla-pipeline\",\n+        \"Runs all Triton passes, including the ones from XLA.\",\n+        [](mlir::OpPassManager& pm, const TritonPipelineOptions& options) {\n+          stream_executor::GpuComputeCapability gpu_cc;\n+          if (auto cuda_cc =\n+                  stream_executor::CudaComputeCapability().FromString(\n+                      options.target);\n+              cuda_cc.ok()) {\n+            gpu_cc = *cuda_cc;\n+          }\n+          if (stream_executor::RocmComputeCapability rocm_cc(options.target);\n+              rocm_cc.is_supported_gfx_version()) {\n+            gpu_cc = rocm_cc;\n+          }\n+          xla::gpu::CreateTritonXlaPipeline(&pm, gpu_cc, options.rewrite_int4,\n+                                            options.allow_tma,\n+                                            options.convert_unsupported_types);\n+          xla::gpu::CreateTritonPipeline(&pm, gpu_cc, options.num_warps,\n+                                         options.num_ctas, options.num_stages,\n+                                         cluster_info);\n+        });\n+\n+}  // namespace\n \n int main(int argc, char **argv) {\n   mlir::DialectRegistry registry;"
        }
    ],
    "stats": {
        "total": 792,
        "additions": 391,
        "deletions": 401
    }
}