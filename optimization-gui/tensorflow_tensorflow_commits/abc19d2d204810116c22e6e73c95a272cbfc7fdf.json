{
    "author": "WillFroom",
    "message": "[XLA:CPU] Combine optimization & lowering pass managers by using callback pass.\n\nPiperOrigin-RevId: 820610316",
    "sha": "abc19d2d204810116c22e6e73c95a272cbfc7fdf",
    "files": [
        {
            "sha": "787dc8f19915235f47f269795c4e13e7f78d9712",
            "filename": "third_party/xla/xla/backends/cpu/codegen/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abc19d2d204810116c22e6e73c95a272cbfc7fdf/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abc19d2d204810116c22e6e73c95a272cbfc7fdf/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2FBUILD?ref=abc19d2d204810116c22e6e73c95a272cbfc7fdf",
            "patch": "@@ -157,11 +157,11 @@ cc_library(\n         \"//xla/codegen/xtile/ir:xtile\",\n         \"//xla/mlir/tools/mlir_replay/public:compiler_trace_proto_cc\",\n         \"//xla/mlir_hlo\",\n-        \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tsl/framework/mlir:status_scoped_diagnostic_handler\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/functional:any_invocable\",\n+        \"@com_google_absl//absl/functional:function_ref\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\","
        },
        {
            "sha": "766d9169dc47ae36c7355e946db158a085a7a28b",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_compiler.cc",
            "status": "modified",
            "additions": 34,
            "deletions": 33,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abc19d2d204810116c22e6e73c95a272cbfc7fdf/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abc19d2d204810116c22e6e73c95a272cbfc7fdf/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc?ref=abc19d2d204810116c22e6e73c95a272cbfc7fdf",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <string>\n #include <utility>\n \n+#include \"absl/functional/function_ref.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n@@ -65,6 +66,7 @@ limitations under the License.\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/Operation.h\"\n #include \"mlir/IR/Visitors.h\"\n+#include \"mlir/Pass/Pass.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"mlir/Support/WalkResult.h\"\n@@ -99,6 +101,19 @@ limitations under the License.\n \n namespace xla::cpu {\n \n+class ModuleCallbackPass\n+    : public mlir::PassWrapper<ModuleCallbackPass,\n+                               mlir::OperationPass<mlir::ModuleOp>> {\n+ public:\n+  explicit ModuleCallbackPass(absl::FunctionRef<void(mlir::ModuleOp)> callback)\n+      : callback_(callback) {}\n+\n+  void runOnOperation() override { callback_(getOperation()); }\n+\n+ private:\n+  absl::FunctionRef<void(mlir::ModuleOp)> callback_;\n+};\n+\n static absl::Status RunPassPipeline(\n     mlir::ModuleOp module, mlir::PassManager& pm,\n     mlir::interpreter::MlirCompilationTrace* trace,\n@@ -273,31 +288,28 @@ FusionCompiler::FusionCompiler(mlir::MLIRContext* context, Options options,\n                                CompilationHooks hooks)\n     : options_(std::move(options)),\n       hooks_(std::move(hooks)),\n-      scalar_optimization_pass_manager_(\n-          mlir::PassManager::on<mlir::ModuleOp>(context)),\n-      tiled_optimization_pass_manager_(\n-          mlir::PassManager::on<mlir::ModuleOp>(context)),\n-      scalar_lowering_pass_manager_(\n-          mlir::PassManager::on<mlir::ModuleOp>(context)),\n-      tiled_lowering_pass_manager_(\n-          mlir::PassManager::on<mlir::ModuleOp>(context)) {\n+      scalar_pass_manager_(mlir::PassManager::on<mlir::ModuleOp>(context)),\n+      tiled_pass_manager_(mlir::PassManager::on<mlir::ModuleOp>(context)) {\n   // Scalar passes.\n-  AddScalarOptimizationPasses(scalar_optimization_pass_manager_,\n-                              options_.vector_width);\n-  AddScalarLoweringPasses(scalar_lowering_pass_manager_, options_.vector_width,\n+  AddScalarOptimizationPasses(scalar_pass_manager_, options_.vector_width);\n+  if (hooks_.post_optimization) {\n+    scalar_pass_manager_.addPass(\n+        std::make_unique<ModuleCallbackPass>(hooks_.post_optimization));\n+  }\n+  AddScalarLoweringPasses(scalar_pass_manager_, options_.vector_width,\n                           options_.fast_min_max);\n \n   // Tiled passes.\n-  AddTiledOptimizationPasses(tiled_optimization_pass_manager_);\n-  AddTiledLoweringPasses(tiled_lowering_pass_manager_);\n+  AddTiledOptimizationPasses(tiled_pass_manager_);\n+  if (hooks_.post_optimization) {\n+    tiled_pass_manager_.addPass(\n+        std::make_unique<ModuleCallbackPass>(hooks_.post_optimization));\n+  }\n+  AddTiledLoweringPasses(tiled_pass_manager_);\n \n-  scalar_optimization_pass_manager_.addInstrumentation(\n-      std::make_unique<TraceInstrumentation>());\n-  scalar_lowering_pass_manager_.addInstrumentation(\n+  scalar_pass_manager_.addInstrumentation(\n       std::make_unique<TraceInstrumentation>());\n-  tiled_optimization_pass_manager_.addInstrumentation(\n-      std::make_unique<TraceInstrumentation>());\n-  tiled_lowering_pass_manager_.addInstrumentation(\n+  tiled_pass_manager_.addInstrumentation(\n       std::make_unique<TraceInstrumentation>());\n }\n \n@@ -317,11 +329,7 @@ absl::StatusOr<std::unique_ptr<llvm::Module>> FusionCompiler::Compile(\n   };\n \n   bool is_tiled = !mlir_module.getBody()->getOps<xtile::EntryFuncOp>().empty();\n-  mlir::PassManager& optimization_pm = is_tiled\n-                                           ? tiled_optimization_pass_manager_\n-                                           : scalar_optimization_pass_manager_;\n-  mlir::PassManager& lowering_pm =\n-      is_tiled ? tiled_lowering_pass_manager_ : scalar_lowering_pass_manager_;\n+  mlir::PassManager& pm = is_tiled ? tiled_pass_manager_ : scalar_pass_manager_;\n \n   VLOG(1) << \"Compiling MLIR module: \" << module_name << \", with \"\n           << get_module_op_count() << \" operations.\";\n@@ -337,15 +345,8 @@ absl::StatusOr<std::unique_ptr<llvm::Module>> FusionCompiler::Compile(\n   if (hooks_.pre_optimization) {\n     hooks_.pre_optimization(mlir_module);\n   }\n-  TF_RETURN_IF_ERROR(RunPassPipeline(mlir_module, optimization_pm, nullptr,\n-                                     options_.verification_level));\n-\n-  if (hooks_.post_optimization) {\n-    hooks_.post_optimization(mlir_module);\n-  }\n-\n-  TF_RETURN_IF_ERROR(RunPassPipeline(mlir_module, lowering_pm, nullptr,\n-                                     options_.verification_level));\n+  TF_RETURN_IF_ERROR(\n+      RunPassPipeline(mlir_module, pm, nullptr, options_.verification_level));\n \n   if (hooks_.post_lowering) {\n     hooks_.post_lowering(mlir_module);"
        },
        {
            "sha": "bd575a40ee50c17eb20775724abae9a114d52182",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_compiler.h",
            "status": "modified",
            "additions": 5,
            "deletions": 13,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abc19d2d204810116c22e6e73c95a272cbfc7fdf/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abc19d2d204810116c22e6e73c95a272cbfc7fdf/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.h?ref=abc19d2d204810116c22e6e73c95a272cbfc7fdf",
            "patch": "@@ -67,19 +67,11 @@ class FusionCompiler {\n  private:\n   Options options_;\n   CompilationHooks hooks_;\n-  // The reason we have 4 distinct pass managers is because:\n-  //   - We have 2 stages: optimization and lowering, this is to enable dumping\n-  //     of the intermediate optimized MLIR.\n-  //   - We have 2 distinct pipelines for scalar and tiled kernels, this is\n-  //     because they differ slightly in their semantics, ideally these would be\n-  //     unified but this is a larger change.\n-  // Pass manager that holds the optimization & loop transformation passes.\n-  mlir::PassManager scalar_optimization_pass_manager_;\n-  mlir::PassManager tiled_optimization_pass_manager_;\n-  // Pass manager that holds the passes responsible for lowering the module from\n-  // MLIR to LLVM.\n-  mlir::PassManager scalar_lowering_pass_manager_;\n-  mlir::PassManager tiled_lowering_pass_manager_;\n+  // We have 2 distinct pipelines for scalar and tiled kernels, this is\n+  // because they differ slightly in their semantics, ideally these would be\n+  // unified but this is a larger change.\n+  mlir::PassManager scalar_pass_manager_;\n+  mlir::PassManager tiled_pass_manager_;\n };\n \n }  // namespace xla::cpu"
        }
    ],
    "stats": {
        "total": 87,
        "additions": 40,
        "deletions": 47
    }
}