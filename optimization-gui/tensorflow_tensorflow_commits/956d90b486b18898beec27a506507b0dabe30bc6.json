{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 850284371",
    "sha": "956d90b486b18898beec27a506507b0dabe30bc6",
    "files": [
        {
            "sha": "686c8e4d504233ca3fedad0cbf71cf63cf4c9486",
            "filename": "tensorflow/core/kernels/collective_nccl_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/956d90b486b18898beec27a506507b0dabe30bc6/tensorflow%2Fcore%2Fkernels%2Fcollective_nccl_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/956d90b486b18898beec27a506507b0dabe30bc6/tensorflow%2Fcore%2Fkernels%2Fcollective_nccl_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fcollective_nccl_test.cc?ref=956d90b486b18898beec27a506507b0dabe30bc6",
            "patch": "@@ -45,7 +45,7 @@ namespace tensorflow {\n static constexpr int kStepId = 10;\n \n std::unique_ptr<OpKernel> GetKernel(const NodeDef& node, DeviceBase* device) {\n-  Status status;\n+  absl::Status status;\n   std::unique_ptr<OpKernel> k = CreateOpKernel(\n       DEVICE_GPU, device, device->GetAllocator(AllocatorAttributes()), node,\n       TF_GRAPH_DEF_VERSION, &status);\n@@ -77,7 +77,8 @@ class NcclTestBase : public ::testing::Test {\n  protected:\n   class DeviceInstance;\n \n-  NcclTestBase(CollectiveType collective_type, const string& collective_name)\n+  NcclTestBase(CollectiveType collective_type,\n+               const std::string& collective_name)\n       : collective_type_(collective_type), collective_name_(collective_name) {}\n \n   void Init(const int num_ranks) {\n@@ -139,7 +140,7 @@ class NcclTestBase : public ::testing::Test {\n       std::vector<float> expected;\n       InitExpected(&expected, input_length, rank, num_ranks);\n       if (VLOG_IS_ON(3)) {\n-        string str_buf;\n+        std::string str_buf;\n         for (const auto& x : expected) {\n           absl::StrAppend(&str_buf, \" \", x);\n         }\n@@ -159,13 +160,13 @@ class NcclTestBase : public ::testing::Test {\n \n   class DeviceInstance {\n    public:\n-    DeviceInstance(int rank, const string& collective_name,\n+    DeviceInstance(int rank, const std::string& collective_name,\n                    CollectiveType collective_type, CollectiveTestEnv* test_env)\n         : test_env_(test_env) {  // TODO(tmorris): tensor_?\n       col_params_ =\n           CreateCollectiveParams(*test_env_, rank, collective_name,\n                                  collective_type, DT_FLOAT, TensorShape());\n-      string device_name = col_params_->group.members[rank].device.name();\n+      std::string device_name = col_params_->group.members[rank].device.name();\n       TF_CHECK_OK(test_env_->device_mgr->LookupDevice(device_name, &device_))\n           << \"Could not find device \" << device_name << \" existing devices \"\n           << test_env_->device_mgr->DebugString();\n@@ -233,14 +234,14 @@ class NcclTestBase : public ::testing::Test {\n     core::RefCountPtr<CollectiveParams> col_params_;\n     std::unique_ptr<OpKernel> merge_op_;\n     std::unique_ptr<OpKernel> final_op_;\n-    Status status_;\n+    absl::Status status_;\n   };\n \n   CollectiveType collective_type_;\n-  const string collective_name_;\n+  const std::string collective_name_;\n   std::vector<std::unique_ptr<DeviceInstance>> instances_;\n   mutex mu_;\n-  int32 op_counter_ TF_GUARDED_BY(mu_) = 0;\n+  int32_t op_counter_ TF_GUARDED_BY(mu_) = 0;\n   std::unique_ptr<CollectiveTestEnv> test_env_;\n };\n "
        },
        {
            "sha": "3c9ccb5e6330f8468b5cc3200951dbf3f046c4c1",
            "filename": "tensorflow/core/kernels/conv_2d_gpu.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/956d90b486b18898beec27a506507b0dabe30bc6/tensorflow%2Fcore%2Fkernels%2Fconv_2d_gpu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/956d90b486b18898beec27a506507b0dabe30bc6/tensorflow%2Fcore%2Fkernels%2Fconv_2d_gpu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fconv_2d_gpu.h?ref=956d90b486b18898beec27a506507b0dabe30bc6",
            "patch": "@@ -887,15 +887,15 @@ template <int ElemBytes>\n struct TransposeElemType;\n template <>\n struct TransposeElemType<1> {\n-  using type = uint8;\n+  using type = uint8_t;\n };\n template <>\n struct TransposeElemType<2> {\n-  using type = uint16;\n+  using type = uint16_t;\n };\n template <>\n struct TransposeElemType<4> {\n-  using type = uint32;\n+  using type = uint32_t;\n };\n template <>\n struct TransposeElemType<8> {"
        },
        {
            "sha": "45c82449b2b579e4fdff13d3440ab690adad3405",
            "filename": "tensorflow/core/kernels/conv_ops_gpu.h",
            "status": "modified",
            "additions": 18,
            "deletions": 20,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/956d90b486b18898beec27a506507b0dabe30bc6/tensorflow%2Fcore%2Fkernels%2Fconv_ops_gpu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/956d90b486b18898beec27a506507b0dabe30bc6/tensorflow%2Fcore%2Fkernels%2Fconv_ops_gpu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fconv_ops_gpu.h?ref=956d90b486b18898beec27a506507b0dabe30bc6",
            "patch": "@@ -53,11 +53,11 @@ bool ComputeInNhwcEnabled(DataType data_type, se::Stream* stream,\n // Get the Dnn workspace limit from the environment variable, which is in MB.\n // Return the workspace memory limit in bytes. If no value is set, return the\n // default value.\n-int64 GetDnnWorkspaceLimit(const string& envvar_in_mb,\n-                           int64_t default_value_in_bytes);\n+int64_t GetDnnWorkspaceLimit(const std::string& envvar_in_mb,\n+                             int64_t default_value_in_bytes);\n \n // Call the Dnn workspace limit from TF_CUDNN_WORKSPACE_LIMIT_IN_MB or default.\n-int64 GetDnnWorkspaceLimitOrDefault();\n+int64_t GetDnnWorkspaceLimitOrDefault();\n \n // A class to provide scratch-space allocator for Stream-Executor Cudnn\n // callback. TensorFlow is responsible for releasing the temporary buffers after\n@@ -67,7 +67,7 @@ class DnnScratchAllocator : public se::ScratchAllocator {\n   virtual ~DnnScratchAllocator() {}\n   DnnScratchAllocator(int64_t memory_limit, OpKernelContext* context)\n       : memory_limit_(memory_limit), total_byte_size_(0), context_(context) {}\n-  int64 GetMemoryLimitInBytes() override { return memory_limit_; }\n+  int64_t GetMemoryLimitInBytes() override { return memory_limit_; }\n   absl::StatusOr<stream_executor::DeviceMemory<uint8>> AllocateBytes(\n       int64_t byte_size) override {\n     Tensor temporary_memory;\n@@ -83,7 +83,7 @@ class DnnScratchAllocator : public se::ScratchAllocator {\n     }\n     AllocationAttributes allocation_attr;\n     allocation_attr.retry_on_failure = false;\n-    Status allocation_status(context_->allocate_temp(\n+    absl::Status allocation_status(context_->allocate_temp(\n         DT_UINT8, TensorShape({byte_size}), &temporary_memory,\n         AllocatorAttributes(), allocation_attr));\n     if (!allocation_status.ok()) {\n@@ -97,14 +97,14 @@ class DnnScratchAllocator : public se::ScratchAllocator {\n     allocated_tensors_.push_back(temporary_memory);\n     total_byte_size_ += byte_size;\n     return absl::StatusOr<stream_executor::DeviceMemory<uint8>>(\n-        AsDeviceMemory(temporary_memory.flat<uint8>().data(),\n-                       temporary_memory.flat<uint8>().size()));\n+        AsDeviceMemory(temporary_memory.flat<uint8_t>().data(),\n+                       temporary_memory.flat<uint8_t>().size()));\n   }\n-  int64 TotalByteSize() { return total_byte_size_; }\n+  int64_t TotalByteSize() { return total_byte_size_; }\n \n  private:\n-  int64 memory_limit_;\n-  int64 total_byte_size_;\n+  int64_t memory_limit_;\n+  int64_t total_byte_size_;\n   OpKernelContext* context_;\n   std::vector<Tensor> allocated_tensors_;\n };\n@@ -177,16 +177,14 @@ AllocateScratchOrFallback(se::ScratchAllocator* scratch_allocator,\n }\n \n template <typename T>\n-Status LaunchAutotunedConv(const AutotuneEntry<se::dnn::ConvOp>& autotune_entry,\n-                           DnnScratchAllocator* scratch_allocator,\n-                           se::dnn::ConvolutionKind kind, se::Stream* stream,\n-                           const se::dnn::BatchDescriptor& input_desc,\n-                           se::DeviceMemory<T> in_ptr,\n-                           const se::dnn::FilterDescriptor& filter_desc,\n-                           se::DeviceMemory<T> filter_ptr,\n-                           const se::dnn::ConvolutionDescriptor& conv_desc,\n-                           const se::dnn::BatchDescriptor& output_desc,\n-                           se::DeviceMemory<T> out_ptr) {\n+absl::Status LaunchAutotunedConv(\n+    const AutotuneEntry<se::dnn::ConvOp>& autotune_entry,\n+    DnnScratchAllocator* scratch_allocator, se::dnn::ConvolutionKind kind,\n+    se::Stream* stream, const se::dnn::BatchDescriptor& input_desc,\n+    se::DeviceMemory<T> in_ptr, const se::dnn::FilterDescriptor& filter_desc,\n+    se::DeviceMemory<T> filter_ptr,\n+    const se::dnn::ConvolutionDescriptor& conv_desc,\n+    const se::dnn::BatchDescriptor& output_desc, se::DeviceMemory<T> out_ptr) {\n   if (!autotune_entry.is_algorithm_config()) {\n     const auto& runners = autotune_entry.GetOpRunners();\n     se::dnn::DataType element_type = se::dnn::ToDataType<T>::value;"
        }
    ],
    "stats": {
        "total": 61,
        "additions": 30,
        "deletions": 31
    }
}