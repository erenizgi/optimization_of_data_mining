{
    "author": "pifon2a",
    "message": "[XLA:GPU] Emit Triton into a local module.\n\nPiperOrigin-RevId: 838127701",
    "sha": "b5451f0a6db628409bb02b5d1673e0f697f3c1f1",
    "files": [
        {
            "sha": "cbd148dc920c562033de5afe36324944c50e0c38",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b5451f0a6db628409bb02b5d1673e0f697f3c1f1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b5451f0a6db628409bb02b5d1673e0f697f3c1f1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc?ref=b5451f0a6db628409bb02b5d1673e0f697f3c1f1",
            "patch": "@@ -221,22 +221,20 @@ absl::StatusOr<llvm::Function*> BuildKernelPrototype(\n // TODO: b/381242007 - Allocate a proper buffer if we want to use\n // device-side TMA APIs.\n absl::StatusOr<llvm::Function*> RemoveUnusedTritonAbiArguments(\n-    IrEmitterContext& ir_emitter_context,\n+    llvm::Module* llvm_module, IrEmitterContext& ir_emitter_context,\n     const std::string& sanitized_kernel_name,\n     LaunchDimensions& launch_dimensions,\n     const emitters::KernelArguments& kernel_arguments) {\n-  llvm::Function* impl_fn =\n-      ir_emitter_context.llvm_module()->getFunction(sanitized_kernel_name);\n+  llvm::Function* impl_fn = llvm_module->getFunction(sanitized_kernel_name);\n   TF_RET_CHECK(impl_fn);\n   impl_fn->setName(ir_emitter_context.name_uniquer()->GetUniqueName(\n       sanitized_kernel_name + \"_impl\"));\n \n-  llvm::IRBuilder builder(ir_emitter_context.llvm_module()->getContext());\n+  llvm::IRBuilder builder(llvm_module->getContext());\n \n   TF_ASSIGN_OR_RETURN(llvm::Function * kernel,\n                       BuildKernelPrototypeFromUniqueName(\n-                          ir_emitter_context.llvm_module(),\n-                          ir_emitter_context.gpu_device_info(),\n+                          llvm_module, ir_emitter_context.gpu_device_info(),\n                           impl_fn->getName().str(), sanitized_kernel_name,\n                           kernel_arguments, launch_dimensions, &builder));\n "
        },
        {
            "sha": "2d5f6bcdce5a1e8beb10dfae33a048642fdcf574",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b5451f0a6db628409bb02b5d1673e0f697f3c1f1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b5451f0a6db628409bb02b5d1673e0f697f3c1f1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h?ref=b5451f0a6db628409bb02b5d1673e0f697f3c1f1",
            "patch": "@@ -107,7 +107,7 @@ absl::StatusOr<llvm::Function*> BuildKernelPrototype(\n     const LaunchDimensions& launch_dimensions, llvm::IRBuilderBase* builder);\n \n absl::StatusOr<llvm::Function*> RemoveUnusedTritonAbiArguments(\n-    IrEmitterContext& ir_emitter_context,\n+    llvm::Module* llvm_module, IrEmitterContext& ir_emitter_context,\n     const std::string& sanitized_kernel_name,\n     LaunchDimensions& launch_dimensions,\n     const emitters::KernelArguments& arguments);"
        },
        {
            "sha": "e8ed11cc83883f9c3ccb441869fd348306f9c35d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 11,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b5451f0a6db628409bb02b5d1673e0f697f3c1f1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b5451f0a6db628409bb02b5d1673e0f697f3c1f1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=b5451f0a6db628409bb02b5d1673e0f697f3c1f1",
            "patch": "@@ -117,9 +117,11 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n     IrEmitterContext& ir_emitter_context, const HloFusionInstruction& fusion,\n     const HloInstruction* instr_override,\n     absl::Span<const Shape> unmanaged_arguments) const {\n-  llvm::IRBuilder builder(ir_emitter_context.llvm_module()->getContext());\n-  VLOG(3) << fusion.ToString();\n   std::string suggested_kernel_name = std::string(fusion.name());\n+  auto local_module =\n+      ir_emitter_context.CreateLocalLLVMModule(suggested_kernel_name);\n+  llvm::IRBuilder builder(local_module->getContext());\n+  VLOG(3) << fusion.ToString();\n   TF_ASSIGN_OR_RETURN(\n       auto kernel_arguments,\n       emitters::KernelArguments::Create(\n@@ -139,10 +141,9 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n \n     TF_ASSIGN_OR_RETURN(\n         TritonWrapperResult triton_wrapper_result,\n-        GenerateTritonKernelAndWrapper(fusion, sanitized_kernel_name,\n-                                       ir_emitter_context.gpu_device_info(),\n-                                       ir_emitter_context.llvm_module(),\n-                                       ir_emitter_context.mlir_context()));\n+        GenerateTritonKernelAndWrapper(\n+            fusion, sanitized_kernel_name, ir_emitter_context.gpu_device_info(),\n+            local_module.get(), ir_emitter_context.mlir_context()));\n \n     auto backend_config =\n         fusion.backend_config<GpuBackendConfig>()->fusion_backend_config();\n@@ -180,10 +181,11 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n     CHECK(launch_config.has_value());\n     launch_dimensions = std::move(launch_config->launch_dimensions);\n \n-    TF_ASSIGN_OR_RETURN(llvm::Function * kernel,\n-                        RemoveUnusedTritonAbiArguments(\n-                            ir_emitter_context, sanitized_kernel_name,\n-                            launch_dimensions, kernel_arguments));\n+    TF_ASSIGN_OR_RETURN(\n+        llvm::Function * kernel,\n+        RemoveUnusedTritonAbiArguments(local_module.get(), ir_emitter_context,\n+                                       sanitized_kernel_name, launch_dimensions,\n+                                       kernel_arguments));\n \n     PopulateNvvmAnnotations(ir_emitter_context.llvm_module(), kernel,\n                             triton_wrapper_result);\n@@ -207,7 +209,9 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n           &fusion, ir_emitter_context.GetNextThunkId()),\n       entry->kernel_name, kernel_arguments, entry->launch_dimensions,\n       entry->cluster_dim, entry->shmem_bytes, entry->tma_metadata));\n-\n+  if (!was_cached) {\n+    result.module = std::move(local_module);\n+  }\n   return result;\n }\n "
        },
        {
            "sha": "e318ab1d9d387002d8db8bd661a3fc7e09e5850a",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_context.h",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b5451f0a6db628409bb02b5d1673e0f697f3c1f1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b5451f0a6db628409bb02b5d1673e0f697f3c1f1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h?ref=b5451f0a6db628409bb02b5d1673e0f697f3c1f1",
            "patch": "@@ -124,6 +124,15 @@ class IrEmitterContext {\n \n   std::vector<GpuExecutable::ConstantInfo>& constants() { return constants_; }\n \n+  std::unique_ptr<llvm::Module> CreateLocalLLVMModule(\n+      const std::string& module_name) {\n+    auto llvm_module =\n+        std::make_unique<llvm::Module>(module_name, llvm_module_->getContext());\n+    llvm_module->setTargetTriple(llvm::Triple(llvm_module_->getTargetTriple()));\n+    llvm_module->setDataLayout(llvm_module_->getDataLayout());\n+    return llvm_module;\n+  }\n+\n   const DebugOptions& debug_options() const {\n     return hlo_module_->config().debug_options();\n   }\n@@ -154,7 +163,6 @@ class IrEmitterContext {\n   std::string platform_name_;\n   const se::DeviceDescription& gpu_device_info_;\n   mlir::MLIRContext* mlir_context_;\n-  mlir::MLIRContext expr_context_;\n   llvm::Module* llvm_module_;\n   llvm::Module* llvm_module_constants_;\n   NameUniquer name_uniquer_;"
        },
        {
            "sha": "fbd0abc96d227484aa23fc57ac7858707793c99f",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 24,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b5451f0a6db628409bb02b5d1673e0f697f3c1f1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b5451f0a6db628409bb02b5d1673e0f697f3c1f1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc?ref=b5451f0a6db628409bb02b5d1673e0f697f3c1f1",
            "patch": "@@ -213,16 +213,6 @@ EmitCollectiveKernelThunk(IrEmitterContext* ir_emitter_context,\n           .xla_gpu_unsupported_use_all_reduce_one_shot_kernel());\n }\n \n-std::unique_ptr<llvm::Module> CreateLocalLLVMModule(\n-    const std::string& module_name, llvm::Module* global_llvm_module) {\n-  auto llvm_module = std::make_unique<llvm::Module>(\n-      module_name, global_llvm_module->getContext());\n-  llvm_module->setTargetTriple(\n-      llvm::Triple(global_llvm_module->getTargetTriple()));\n-  llvm_module->setDataLayout(global_llvm_module->getDataLayout());\n-  return llvm_module;\n-}\n-\n }  // namespace\n \n ThunkEmitter::ThunkEmitter(IrEmitterContext* ir_emitter_context)\n@@ -300,8 +290,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitConditional(\n absl::StatusOr<ThunkSequence> ThunkEmitter::EmitPadToStatic(\n     const HloCustomCallInstruction* instr) {\n   std::string ir_name = std::string(instr->name());\n-  auto local_llvm_module =\n-      CreateLocalLLVMModule(ir_name, ir_emitter_context_->llvm_module());\n+  auto local_llvm_module = ir_emitter_context_->CreateLocalLLVMModule(ir_name);\n \n   TF_ASSIGN_OR_RETURN(auto thunk_sequence,\n                       EmitPadToStaticLLVMIR(instr, local_llvm_module.get(),\n@@ -317,8 +306,8 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitPadToStatic(\n absl::StatusOr<ThunkSequence> ThunkEmitter::EmitSliceToDynamic(\n     const HloCustomCallInstruction* instr) {\n   std::string ir_name = std::string(instr->name());\n-  auto local_llvm_module =\n-      CreateLocalLLVMModule(ir_name, ir_emitter_context_->llvm_module());\n+  auto local_llvm_module = ir_emitter_context_->CreateLocalLLVMModule(ir_name);\n+\n   TF_ASSIGN_OR_RETURN(auto thunk_sequence,\n                       EmitSliceToDynamicLLVMIR(instr, local_llvm_module.get(),\n                                                ir_emitter_context_));\n@@ -1137,6 +1126,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTritonCustomCall(\n         TritonCall::Parse(instr->raw_backend_config_string(), &mlir_context);\n     auto kernel_name = GetSanitizedUniqueName(*ir_emitter_context_, call.name);\n     VLOG(3) << \"Generating: \" << kernel_name;\n+    auto local_module = ir_emitter_context_->CreateLocalLLVMModule(kernel_name);\n \n     mlir::OwningOpRef<mlir::ModuleOp> triton_module;\n     {\n@@ -1176,7 +1166,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTritonCustomCall(\n         CompileTritonToLLVM(kernel_name, *hlo_module,\n                             ir_emitter_context_->gpu_device_info(),\n                             block_level_parameters, triton_module.get(),\n-                            ir_emitter_context_->llvm_module(), mlir_context,\n+                            local_module.get(), mlir_context,\n                             /*is_xla_fusion=*/false, emit_kernels));\n \n     TF_ASSIGN_OR_RETURN(auto kernel_arguments,\n@@ -1190,12 +1180,14 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTritonCustomCall(\n             ir_emitter_context_->gpu_device_info().threads_per_warp()));\n \n     if (emit_kernels) {\n-      TF_RETURN_IF_ERROR(\n-          RemoveUnusedTritonAbiArguments(*ir_emitter_context_, kernel_name,\n-                                         launch_dimensions, kernel_arguments)\n-              .status());\n+      TF_RETURN_IF_ERROR(RemoveUnusedTritonAbiArguments(\n+                             local_module.get(), *ir_emitter_context_,\n+                             kernel_name, launch_dimensions, kernel_arguments)\n+                             .status());\n     }\n-\n+    TF_RET_CHECK(!llvm::Linker::linkModules(\n+        *ir_emitter_context_->llvm_module(), std::move(local_module),\n+        llvm::Linker::Flags::OverrideFromSrc));\n     return {{kernel_name, launch_dimensions, result.cluster_dim,\n              result.shmem_bytes}};\n   };\n@@ -1381,8 +1373,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitWhile(\n absl::StatusOr<ThunkSequence> ThunkEmitter::EmitRngGetAndUpdateState(\n     const HloRngGetAndUpdateStateInstruction* instr) {\n   std::string ir_name = std::string(instr->name());\n-  auto local_llvm_module =\n-      CreateLocalLLVMModule(ir_name, ir_emitter_context_->llvm_module());\n+  auto local_llvm_module = ir_emitter_context_->CreateLocalLLVMModule(ir_name);\n \n   TF_ASSIGN_OR_RETURN(auto thunk_sequence,\n                       EmitRngGetAndUpdateStateLLVMIR(\n@@ -1435,8 +1426,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitSort(\n     }\n   }\n \n-  auto local_llvm_module =\n-      CreateLocalLLVMModule(op_name, ir_emitter_context_->llvm_module());\n+  auto local_llvm_module = ir_emitter_context_->CreateLocalLLVMModule(op_name);\n \n   TF_ASSIGN_OR_RETURN(ThunkSequence sort_thunks,\n                       EmitBitonicSortLLVMIR(sort, local_llvm_module.get(),"
        }
    ],
    "stats": {
        "total": 86,
        "additions": 43,
        "deletions": 43
    }
}