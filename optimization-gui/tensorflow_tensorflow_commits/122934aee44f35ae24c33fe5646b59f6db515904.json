{
    "author": "nputikhin",
    "message": "[XLA:GPU] Make GetAllocations return a span of ptrs instead of objects\n\nPreviously it returned a span of BufferAllocation objects, which was built from a container owning these allocations (std::vector<BufferAllocation>).\n\nChanging the return type to a span of pointers would allow using allocations from several owning containers by combining them into a single vector of pointers.\n\nPiperOrigin-RevId: 814052970",
    "sha": "122934aee44f35ae24c33fe5646b59f6db515904",
    "files": [
        {
            "sha": "00301d7c65c63a13c7ad5d0ae5c1cc5f12b3d4fd",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_profiler.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_profiler.cc?ref=122934aee44f35ae24c33fe5646b59f6db515904",
            "patch": "@@ -69,9 +69,9 @@ std::vector<ExecutionInput> CreateExecutionInputsFromBuffers(\n \n int GetScratchBytes(const Executable* executable) {\n   int scratch_bytes = 0;\n-  for (const auto& allocation : executable->GetAllocations()) {\n-    if (allocation.IsPreallocatedTempBuffer()) {\n-      for (const auto& [buffer, offset] : allocation.assigned_buffers()) {\n+  for (const auto* allocation : executable->GetAllocations()) {\n+    if (allocation->IsPreallocatedTempBuffer()) {\n+      for (const auto& [buffer, offset] : allocation->assigned_buffers()) {\n         // Scratch space is allocated as the second element in the output tuple\n         // of the instruction.\n         const auto& shape_index = buffer->positions().front().index;"
        },
        {
            "sha": "6fa3d76039381e729df9c294f33885dac713d508",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=122934aee44f35ae24c33fe5646b59f6db515904",
            "patch": "@@ -2532,8 +2532,7 @@ cc_library(\n     srcs = [\"thunk_pass_pipeline.cc\"],\n     hdrs = [\"thunk_pass_pipeline.h\"],\n     deps = [\n-        \"//xla/backends/gpu/runtime:sequential_thunk\",\n-        \"//xla/service:hlo_module_config\",\n+        \":sequential_thunk\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n@@ -2563,17 +2562,17 @@ cc_library(\n     srcs = [\"command_buffer_conversion_pass.cc\"],\n     hdrs = [\"command_buffer_conversion_pass.h\"],\n     deps = [\n+        \":command_buffer_cmd\",\n+        \":command_buffer_cmd_emitter\",\n+        \":command_buffer_thunk\",\n+        \":conditional_thunk\",\n+        \":copy_thunk\",\n+        \":custom_call_thunk\",\n+        \":sequential_thunk\",\n+        \":thunk\",\n         \":thunk_pass_pipeline\",\n+        \":while_thunk\",\n         \"//xla:util\",\n-        \"//xla/backends/gpu/runtime:command_buffer_cmd\",\n-        \"//xla/backends/gpu/runtime:command_buffer_cmd_emitter\",\n-        \"//xla/backends/gpu/runtime:command_buffer_thunk\",\n-        \"//xla/backends/gpu/runtime:conditional_thunk\",\n-        \"//xla/backends/gpu/runtime:copy_thunk\",\n-        \"//xla/backends/gpu/runtime:custom_call_thunk\",\n-        \"//xla/backends/gpu/runtime:sequential_thunk\",\n-        \"//xla/backends/gpu/runtime:thunk\",\n-        \"//xla/backends/gpu/runtime:while_thunk\",\n         \"//xla/ffi:ffi_api\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:semantic_version\","
        },
        {
            "sha": "2c45eae7524fd5e96b1d133394309fb142cce57f",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=122934aee44f35ae24c33fe5646b59f6db515904",
            "patch": "@@ -1826,7 +1826,8 @@ StreamExecutorGpuClient::RunAsync(\n         globals, gpu_exec->ResolveConstantGlobals(run_options->stream()));\n   }\n \n-  absl::Span<const BufferAllocation> allocations = gpu_exec->GetAllocations();\n+  absl::Span<const BufferAllocation* const> allocations =\n+      gpu_exec->GetAllocations();\n \n   std::vector<se::DeviceMemoryBase> buffers(allocations.size());\n   {\n@@ -1835,7 +1836,7 @@ StreamExecutorGpuClient::RunAsync(\n         tsl::profiler::TraceMeLevel::kInfo);\n     const int64_t num_buffers = allocations.size();\n     for (int64_t i = 0; i < num_buffers; ++i) {\n-      const BufferAllocation& allocation = allocations[i];\n+      const BufferAllocation& allocation = *allocations[i];\n       se::DeviceMemoryBase& buffer = buffers[i];\n       if (allocation.is_thread_local()) {\n         // buffer = se::DeviceMemoryBase{};\n@@ -1893,7 +1894,7 @@ StreamExecutorGpuClient::RunAsync(\n     const gpu::GpuExecutable::OutputInfo& output_info =\n         gpu_exec->output_info().at(index);\n     const BufferAllocation* allocation =\n-        &allocations[output_info.allocation_index];\n+        allocations[output_info.allocation_index];\n     se::DeviceMemoryBase result_buffer;\n \n     VLOG(4) << \"[\" << device_ordinal << \"] Looking at: allocation \""
        },
        {
            "sha": "7870743faad147f547e8a87472c1f76305a8d3dc",
            "filename": "third_party/xla/xla/pjrt/pjrt_executable.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable.cc?ref=122934aee44f35ae24c33fe5646b59f6db515904",
            "patch": "@@ -267,7 +267,7 @@ CompiledMemoryStats CompiledMemoryStats::FromProto(\n // want, but does not distinguish between device and host memory, and does\n // not account for aliased memory.\n void CompiledMemoryStats::PopulateBufferStatsFromAllocations(\n-    absl::Span<const BufferAllocation> allocs) {\n+    absl::Span<const BufferAllocation* const> allocs) {\n   argument_size_in_bytes = 0;\n   output_size_in_bytes = 0;\n   temp_size_in_bytes = 0;\n@@ -277,7 +277,7 @@ void CompiledMemoryStats::PopulateBufferStatsFromAllocations(\n   host_temp_size_in_bytes = 0;\n   host_alias_size_in_bytes = 0;\n \n-  for (auto& alloc : allocs) {\n+  for (const BufferAllocation* alloc : allocs) {\n     // All logical buffers assigned to a buffer allocation share a color.\n     // With buffer assigner's default colorer the color happens to be the\n     // memory space of the underlying HLO value. Callers may choose other\n@@ -286,7 +286,7 @@ void CompiledMemoryStats::PopulateBufferStatsFromAllocations(\n     // Until buffer allocations provide a stronger guarantee about colors,\n     // we sanity-check that the default coloring behavior was used.\n     int64_t alloc_memory_space = -1;\n-    for (const auto& [value, _] : alloc.assigned_buffers()) {\n+    for (const auto& [value, _] : alloc->assigned_buffers()) {\n       const HloPosition& defining_position = value->defining_position();\n       int64_t memory_space = Layout::kDefaultMemorySpace;\n       if (defining_position.shape().has_layout()) {\n@@ -301,29 +301,29 @@ void CompiledMemoryStats::PopulateBufferStatsFromAllocations(\n     }\n \n     bool is_host = alloc_memory_space == Layout::kHostMemorySpace;\n-    int64_t size = alloc.size();\n-    if (alloc.is_entry_computation_parameter()) {\n+    int64_t size = alloc->size();\n+    if (alloc->is_entry_computation_parameter()) {\n       if (is_host) {\n         host_argument_size_in_bytes += size;\n       } else {\n         argument_size_in_bytes += size;\n       }\n-      if (alloc.is_parameter_aliased_with_output()) {\n+      if (alloc->is_parameter_aliased_with_output()) {\n         if (is_host) {\n           host_alias_size_in_bytes += size;\n         } else {\n           alias_size_in_bytes += size;\n         }\n       }\n     }\n-    if (alloc.maybe_live_out()) {\n+    if (alloc->maybe_live_out()) {\n       if (is_host) {\n         host_output_size_in_bytes += size;\n       } else {\n         output_size_in_bytes += size;\n       }\n     }\n-    if (alloc.IsPreallocatedTempBuffer()) {\n+    if (alloc->IsPreallocatedTempBuffer()) {\n       if (is_host) {\n         host_temp_size_in_bytes += size;\n       } else {"
        },
        {
            "sha": "aa7f89d46bcd099aa2a269c3506751bcbc0f748d",
            "filename": "third_party/xla/xla/pjrt/pjrt_executable.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_executable.h?ref=122934aee44f35ae24c33fe5646b59f6db515904",
            "patch": "@@ -332,7 +332,7 @@ struct CompiledMemoryStats {\n   static CompiledMemoryStats FromProto(const CompiledMemoryStatsProto& proto);\n \n   void PopulateBufferStatsFromAllocations(\n-      absl::Span<const BufferAllocation> allocs);\n+      absl::Span<const BufferAllocation* const> allocs);\n };\n \n class PjRtExecutable {"
        },
        {
            "sha": "84857f81354285da6fb179f3a056366e51161239",
            "filename": "third_party/xla/xla/pjrt/stream_executor_executable.h",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fpjrt%2Fstream_executor_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fpjrt%2Fstream_executor_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fstream_executor_executable.h?ref=122934aee44f35ae24c33fe5646b59f6db515904",
            "patch": "@@ -109,7 +109,12 @@ class StreamExecutorExecutable : public PjRtExecutable {\n \n       BufferAssignmentProto proto = buffers->ToProto();\n       memory_stats.serialized_buffer_assignment = proto.SerializeAsString();\n-      memory_stats.PopulateBufferStatsFromAllocations(buffers->Allocations());\n+      std::vector<const BufferAllocation*> alloc_ptrs;\n+      alloc_ptrs.reserve(buffers->Allocations().size());\n+      for (const BufferAllocation& alloc : buffers->Allocations()) {\n+        alloc_ptrs.push_back(&alloc);\n+      }\n+      memory_stats.PopulateBufferStatsFromAllocations(alloc_ptrs);\n       TF_ASSIGN_OR_RETURN(int64_t peak_memory, ComputePeakMemory(proto));\n       memory_stats.peak_memory_in_bytes = peak_memory;\n       return memory_stats;"
        },
        {
            "sha": "d9e25a200da06c4c12553c27e7bd15be83f85c69",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=122934aee44f35ae24c33fe5646b59f6db515904",
            "patch": "@@ -1506,32 +1506,22 @@ cc_library(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream\",\n-        \"//xla/stream_executor:stream_executor_h\",\n-        \"//xla/tsl/lib/strings:proto_serialization\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/base:nullability\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n-        \"@com_google_absl//absl/memory\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@com_google_absl//absl/types:variant\",\n-        \"@local_tsl//tsl/platform:env\",\n-        \"@local_tsl//tsl/platform:errors\",\n-        \"@local_tsl//tsl/platform:logging\",\n-        \"@local_tsl//tsl/platform:status\",\n-        \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n "
        },
        {
            "sha": "5c57bac34589521aa86687c0c4eba5c32df64689",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=122934aee44f35ae24c33fe5646b59f6db515904",
            "patch": "@@ -656,7 +656,6 @@ cc_library(\n         \":cpu_runtime\",\n         \":executable_proto_cc\",\n         \"//xla:executable_run_options\",\n-        \"//xla:literal\",\n         \"//xla:shape_tree\",\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n@@ -682,14 +681,13 @@ cc_library(\n         \"//xla/service:xla_debug_info_manager\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:device_memory_allocator\",\n-        \"//xla/stream_executor/host:host_stream\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:logging\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/base:dynamic_annotations\",\n+        \"@com_google_absl//absl/base:nullability\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\","
        },
        {
            "sha": "47848f1ecd10b60d174adbd0abb034c1eea1277b",
            "filename": "third_party/xla/xla/service/cpu/cpu_executable.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.cc?ref=122934aee44f35ae24c33fe5646b59f6db515904",
            "patch": "@@ -161,6 +161,13 @@ CpuExecutable::CpuExecutable(\n     XlaDebugInfoManager::Get()->RegisterModule(shared_module(), assignment_);\n   }\n \n+  if (assignment_) {\n+    alloc_ptrs_.reserve(assignment_->Allocations().size());\n+    for (const BufferAllocation& alloc : assignment_->Allocations()) {\n+      alloc_ptrs_.push_back(&alloc);\n+    }\n+  }\n+\n   // Once we compiled HLO module to CPU executable, we don't need to keep the\n   // HLO module metadata around.\n   if (has_module()) {"
        },
        {
            "sha": "9f573cb1d675c32d4620711e6841237f1e5fb5ba",
            "filename": "third_party/xla/xla/service/cpu/cpu_executable.h",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_executable.h?ref=122934aee44f35ae24c33fe5646b59f6db515904",
            "patch": "@@ -23,6 +23,7 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"absl/base/nullability.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n@@ -37,7 +38,6 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/cpu/executable.pb.h\"\n #include \"xla/service/custom_call_status.h\"\n-#include \"xla/service/custom_call_status_internal.h\"\n #include \"xla/service/executable.h\"\n #include \"xla/service/hlo_execution_profile.h\"\n #include \"xla/service/hlo_profile_printer_data.pb.h\"\n@@ -159,8 +159,9 @@ class CpuExecutable : public Executable {\n \n   int64_t SizeOfGeneratedCodeInBytes() const override;\n \n-  absl::Span<const BufferAllocation> GetAllocations() const override {\n-    return assignment_->Allocations();\n+  absl::Span<const BufferAllocation* absl_nonnull const> GetAllocations()\n+      const override {\n+    return alloc_ptrs_;\n   }\n \n   FunctionLibrary* function_library() const { return function_library_.get(); }\n@@ -225,6 +226,7 @@ class CpuExecutable : public Executable {\n \n   // Buffer assignment for the buffers we need to allocate.\n   std::shared_ptr<BufferAssignment> assignment_;\n+  std::vector<const BufferAllocation*> alloc_ptrs_;\n \n   // The LLVM IR, in string format, of the unoptimized module generated for this\n   // CpuExecutable. We save a string instead of an llvm::Module* because leaving"
        },
        {
            "sha": "f5bbde775227233f0fcad2ace1ed4a974076b79f",
            "filename": "third_party/xla/xla/service/executable.h",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fexecutable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fexecutable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fexecutable.h?ref=122934aee44f35ae24c33fe5646b59f6db515904",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"absl/base/nullability.h\"\n #include \"absl/base/thread_annotations.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/log/check.h\"\n@@ -438,7 +439,8 @@ class Executable {\n \n   // Returns the allocations resulting from buffer assignment, or an empty span\n   // if unimplemented.\n-  virtual absl::Span<const BufferAllocation> GetAllocations() const {\n+  virtual absl::Span<const BufferAllocation* absl_nonnull const>\n+  GetAllocations() const {\n     return {};\n   }\n "
        },
        {
            "sha": "055d6b115ac218e7320bc1a680c9db998e92e63c",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=122934aee44f35ae24c33fe5646b59f6db515904",
            "patch": "@@ -598,7 +598,6 @@ cc_library(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@local_tsl//tsl/platform:logging\",\n     ],\n )\n \n@@ -667,6 +666,7 @@ cc_library(\n         \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/base:nullability\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n@@ -714,18 +714,23 @@ xla_cc_test(\n         \"//xla/backends/gpu/runtime:sequential_thunk\",\n         \"//xla/backends/gpu/runtime:thunk\",\n         \"//xla/codegen/emitters:kernel_arguments\",\n+        \"//xla/hlo/analysis:alias_info\",\n+        \"//xla/hlo/analysis:hlo_ordering\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/parser:hlo_parser\",\n         \"//xla/service:buffer_assignment\",\n+        \"//xla/service:buffer_value\",\n         \"//xla/service:hlo_module_config\",\n+        \"//xla/service:logical_buffer\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/platform:env\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n         \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@local_tsl//tsl/platform:path\","
        },
        {
            "sha": "cf5fc69d33e382068a1ce13158e69c96fb5e7992",
            "filename": "third_party/xla/xla/service/gpu/buffer_allocations.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbuffer_allocations.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbuffer_allocations.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbuffer_allocations.cc?ref=122934aee44f35ae24c33fe5646b59f6db515904",
            "patch": "@@ -22,20 +22,19 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/stream_executor/device_memory.h\"\n-#include \"tsl/platform/logging.h\"\n \n namespace xla {\n namespace gpu {\n \n absl::Status BufferAllocations::TearDown(\n     const std::set<se::DeviceMemoryBase>& live_addresses,\n-    absl::Span<const BufferAllocation> allocations) {\n+    absl::Span<const BufferAllocation* const> allocations) {\n   // Deallocate temporary buffers, taking care to try to deallocate all of them\n   // even if one of the deallocations fails.\n   absl::Status status;\n   const int64_t num_buffers = allocations.size();\n   for (BufferAllocation::Index i = 0; i < num_buffers; ++i) {\n-    const BufferAllocation& allocation = allocations[i];\n+    const BufferAllocation& allocation = *allocations[i];\n     se::DeviceMemoryBase buffer_address = GetDeviceAddress(allocation.index());\n     // Deallocate buffers marked \"maybe_live_out\" but aren't actually live out,\n     // and temp buffers."
        },
        {
            "sha": "ffa31e9fabb378251900ea4f9e7cf75c64721053",
            "filename": "third_party/xla/xla/service/gpu/buffer_allocations.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbuffer_allocations.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbuffer_allocations.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbuffer_allocations.h?ref=122934aee44f35ae24c33fe5646b59f6db515904",
            "patch": "@@ -70,7 +70,7 @@ class BufferAllocations {\n   // Tears down all buffers allocated by this object that are not in\n   // `live_addresses`.\n   absl::Status TearDown(const std::set<se::DeviceMemoryBase>& live_addresses,\n-                        absl::Span<const BufferAllocation> allocations);\n+                        absl::Span<const BufferAllocation* const> allocations);\n \n   std::string ToString() const {\n     std::string out;"
        },
        {
            "sha": "bf75f512170233ee76169046d17e1ef275470ae4",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 37,
            "deletions": 9,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=122934aee44f35ae24c33fe5646b59f6db515904",
            "patch": "@@ -95,6 +95,33 @@ limitations under the License.\n namespace xla {\n namespace gpu {\n \n+namespace {\n+\n+// Chooses the correct allocations to be used within the GpuExecutable code.\n+std::vector<const BufferAllocation*> GatherAllocationPtrs(\n+    const GpuExecutable::Params& params) {\n+  const std::vector<BufferAllocation>* allocation_vec = nullptr;\n+  if (params.mlir_allocations.has_value()) {\n+    allocation_vec = &params.mlir_allocations.value();\n+  } else if (params.buffer_assignment != nullptr) {\n+    allocation_vec = &params.buffer_assignment->Allocations();\n+  }\n+\n+  if (allocation_vec == nullptr) {\n+    return {};\n+  }\n+\n+  std::vector<const BufferAllocation*> alloc_ptrs;\n+  alloc_ptrs.reserve(allocation_vec->size());\n+  for (const BufferAllocation& alloc : *allocation_vec) {\n+    alloc_ptrs.push_back(&alloc);\n+  }\n+\n+  return alloc_ptrs;\n+}\n+\n+}  // namespace\n+\n using ::tsl::profiler::ScopedAnnotation;\n \n // Returns the set of `ExecutionStreamIds` requested by all `Thunks` in the\n@@ -153,6 +180,7 @@ GpuExecutable::GpuExecutable(GpuExecutable::Params params)\n       execution_stream_ids_(GetExecutionStreamIds(*thunks_)),\n       module_name_(params.module_name),\n       program_shape_(params.program_shape),\n+      allocation_ptrs_(GatherAllocationPtrs(params)),\n       allocations_(std::move(params.mlir_allocations)),\n       buffer_assignment_(std::move(params.buffer_assignment)),\n       alias_info_(std::move(params.alias_info)),\n@@ -675,16 +703,16 @@ absl::StatusOr<BufferAllocations> GpuExecutable::GenerateBufferAllocations(\n       [&] { return std::string(\"Build buffer allocations\"); },\n       tsl::profiler::TraceMeLevel::kInfo);\n \n-  absl::Span<const BufferAllocation> allocations = GetAllocations();\n+  absl::Span<const BufferAllocation* const> allocations = GetAllocations();\n   const int64_t num_buffers = allocations.size();\n   std::vector<se::DeviceMemoryBase> buffers;\n   buffers.reserve(num_buffers);\n   for (int64_t i = 0; i < num_buffers; ++i) {\n-    const BufferAllocation& allocation = allocations[i];\n+    const BufferAllocation& allocation = *allocations[i];\n     TF_ASSIGN_OR_RETURN(\n         buffers.emplace_back(),\n-        BufferForAllocation(arguments, globals, allocations[i],\n-                            memory_allocator, device_ordinal, i));\n+        BufferForAllocation(arguments, globals, allocation, memory_allocator,\n+                            device_ordinal, i));\n     TF_RETURN_IF_ERROR(CheckAlignment(allocation, buffers.back(), i));\n   }\n   return {{buffers, device_ordinal, memory_allocator}};\n@@ -753,7 +781,7 @@ absl::StatusOr<ExecutionOutput> GpuExecutable::ExecuteAsyncOnStreamImpl(\n       GenerateBufferAllocations(arguments, globals, memory_allocator,\n                                 device_ordinal));\n   VLOG(3) << buffer_allocations.ToString();\n-  absl::Span<const BufferAllocation> allocations = GetAllocations();\n+  absl::Span<const BufferAllocation* const> allocations = GetAllocations();\n \n   std::set<se::DeviceMemoryBase> buffers_in_result;\n \n@@ -777,7 +805,7 @@ absl::StatusOr<ExecutionOutput> GpuExecutable::ExecuteAsyncOnStreamImpl(\n     }\n     const OutputInfo& output_info = output_info_.at(index);\n     const BufferAllocation* allocation =\n-        &allocations[output_info.allocation_index];\n+        allocations[output_info.allocation_index];\n     se::DeviceMemoryBase& result_buffer = p.second;\n \n     VLOG(4) << \"Looking at: allocation \" << output_info.allocation_index\n@@ -950,9 +978,9 @@ int64_t GpuExecutable::SizeOfGeneratedCodeInBytes() const {\n     return -1;\n   }\n   int64_t size = binary().size();\n-  for (const auto& allocation : GetAllocations()) {\n-    if (allocation.is_constant()) {\n-      size += allocation.size();\n+  for (const BufferAllocation* allocation : GetAllocations()) {\n+    if (allocation->is_constant()) {\n+      size += allocation->size();\n     }\n   }\n   return size;"
        },
        {
            "sha": "b50b5536406aa75aabb4fcab5f871f9971a3330b",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.h",
            "status": "modified",
            "additions": 21,
            "deletions": 16,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h?ref=122934aee44f35ae24c33fe5646b59f6db515904",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include <variant>\n #include <vector>\n \n+#include \"absl/base/nullability.h\"\n #include \"absl/base/thread_annotations.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n@@ -172,20 +173,9 @@ class GpuExecutable : public Executable {\n       const ServiceExecutableRunOptions* run_options,\n       VariantArguments arguments);\n \n-  absl::Span<const BufferAllocation> GetAllocations() const override {\n-    // A GpuExecutable can get its allocations in three ways:\n-    // 1 - From a regular compilation that uses allocations from MLIR.\n-    // 2 - From a regular compilation that uses the original allocations from\n-    //     the buffer assignment.\n-    // 3 - From loading the executable from an object file.\n-    //\n-    // In cases 1 and 3, the allocations are stored in allocations_ and in\n-    // case 2, they are part of the buffer_assignment.\n-    //\n-    // This function chooses the correct allocations to be used within the\n-    // GpuExecutable code.\n-    return allocations_.has_value() ? *allocations_\n-                                    : buffer_assignment_->Allocations();\n+  absl::Span<const BufferAllocation* absl_nonnull const> GetAllocations()\n+      const override {\n+    return allocation_ptrs_;\n   }\n \n   const std::vector<ConstantInfo>& constants() const { return constants_; }\n@@ -273,14 +263,29 @@ class GpuExecutable : public Executable {\n \n   ProgramShape program_shape_;\n \n+  // Provides information for allocating memory for every output/temp buffer.\n+  //\n+  // Non-owning pointers - allocation objects reside either in allocations_\n+  // or buffer_assignment_.\n+  //\n+  // A GpuExecutable can get its allocations in three ways:\n+  // 1 - From a regular compilation that uses allocations from MLIR.\n+  // 2 - From a regular compilation that uses the original allocations from\n+  //     the buffer assignment.\n+  // 3 - From loading the executable from an object file.\n+  //\n+  // In cases 1 and 3, the allocations are stored in allocations_ and in\n+  // case 2, they are part of the buffer_assignment.\n+  const std::vector<const BufferAllocation*> allocation_ptrs_;\n+\n   // The allocations_ object contains allocations that **may** be used to\n   // provide information for allocating memory for every output/temp buffer.\n-  // See the comment on GetAllocations().\n+  // See the comment on allocation_ptrs_.\n   std::optional<const std::vector<BufferAllocation>> allocations_;\n \n   // The buffer_assignment_ object contains allocations that **may** be used to\n   // provide information for allocating memory for every output/temp buffer.\n-  // See the comment on GetAllocations().\n+  // See the comment on allocation_ptrs_.\n   //\n   // This object is also used for dumping debug info.\n   std::shared_ptr<const xla::BufferAssignment> buffer_assignment_;"
        },
        {
            "sha": "1b8c70a367ddf2452c0035794e7ecbcf44c6e746",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable_test.cc",
            "status": "modified",
            "additions": 106,
            "deletions": 2,
            "changes": 108,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc?ref=122934aee44f35ae24c33fe5646b59f6db515904",
            "patch": "@@ -24,36 +24,41 @@ limitations under the License.\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"absl/status/status_matchers.h\"\n+#include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n #include \"xla/debug_options_flags.h\"\n+#include \"xla/hlo/analysis/alias_info.h\"\n+#include \"xla/hlo/analysis/hlo_ordering.h\"\n #include \"xla/hlo/ir/hlo_input_output_alias_config.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/buffer_value.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/service/hlo_module_config.h\"\n+#include \"xla/service/logical_buffer.h\"\n #include \"xla/shape_layout.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n #include \"xla/tsl/platform/env.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/util/proto/proto_matchers.h\"\n #include \"tsl/platform/path.h\"\n \n namespace xla::gpu {\n namespace {\n using ::testing::ElementsAre;\n+using ::testing::ElementsAreArray;\n using ::testing::Property;\n using ::tsl::proto_testing::EqualsProto;\n-using ::tsl::testing::IsOkAndHolds;\n \n TEST(GpuExecutableTest, OuputInfoToAndFromProto) {\n   const GpuExecutable::OutputInfo output_info0{/*allocation_index=*/42,\n@@ -192,5 +197,104 @@ TEST(GpuExecutableTest, ExecutableName) {\n   EXPECT_THAT(executable->name(), \"test_module\");\n }\n \n+TEST(GpuExecutableTest, GetMlirAllocations) {\n+  GpuExecutable::Params params;\n+  params.module_name = \"test_module\";\n+  params.executable =\n+      std::make_unique<SequentialThunk>(Thunk::ThunkInfo{}, ThunkSequence{});\n+\n+  std::vector<BufferAllocation> allocations;\n+  allocations.emplace_back(0, 1024, 0);\n+  allocations.emplace_back(1, 2048, 0);\n+\n+  const BufferAllocation* expected_ptr0 = &allocations[0];\n+  const BufferAllocation* expected_ptr1 = &allocations[1];\n+\n+  params.mlir_allocations = std::move(allocations);\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<GpuExecutable> executable,\n+                          GpuExecutable::Create(std::move(params)));\n+\n+  // The pointers must match exactly because the allocations may have Slice\n+  // objects which hold pointers to the parent allocations.\n+  EXPECT_THAT(executable->GetAllocations(),\n+              ElementsAre(expected_ptr0, expected_ptr1));\n+}\n+\n+absl::StatusOr<std::unique_ptr<BufferAssignment>>\n+MakeNonEmptyBufferAssignment() {\n+  const char* hlo_text = R\"(\n+    HloModule m\n+    ENTRY main {\n+      a = f32[128] parameter(0)\n+      b = f32[128] parameter(1)\n+      ROOT c = f32[128] add(a, b)\n+    })\";\n+  TF_ASSIGN_OR_RETURN(auto hlo, ParseAndReturnUnverifiedModule(hlo_text));\n+\n+  AliasInfo alias_info;\n+  TF_ASSIGN_OR_RETURN(\n+      auto buffer_assignment,\n+      BufferAssigner::Run(\n+          hlo.get(), std::make_unique<DependencyHloOrdering>(hlo.get()),\n+          [](const BufferValue& buffer) {\n+            return ShapeUtil::ByteSizeOf(buffer.shape(), sizeof(void*));\n+          },\n+          &alias_info, [](LogicalBuffer::Color) { return /*alignment=*/1; }));\n+  EXPECT_FALSE(buffer_assignment->Allocations().empty());\n+  return buffer_assignment;\n+}\n+\n+TEST(GpuExecutableTest, GetBufferAssignmentAllocations) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<BufferAssignment> buffer_assignment,\n+                          MakeNonEmptyBufferAssignment());\n+\n+  GpuExecutable::Params params;\n+  params.module_name = \"test_module\";\n+  params.executable =\n+      std::make_unique<SequentialThunk>(Thunk::ThunkInfo{}, ThunkSequence{});\n+\n+  std::vector<const BufferAllocation*> expected_allocs;\n+  expected_allocs.reserve(buffer_assignment->Allocations().size());\n+  for (const auto& alloc : buffer_assignment->Allocations()) {\n+    expected_allocs.push_back(&alloc);\n+  }\n+\n+  params.buffer_assignment = std::move(buffer_assignment);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<GpuExecutable> executable,\n+                          GpuExecutable::Create(std::move(params)));\n+\n+  // The pointers must match exactly because the allocations may have Slice\n+  // objects which hold pointers to the parent allocations.\n+  EXPECT_THAT(executable->GetAllocations(), ElementsAreArray(expected_allocs));\n+}\n+\n+TEST(GpuExecutableTest, MlirAllocationsArePreferred) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<BufferAssignment> buffer_assignment,\n+                          MakeNonEmptyBufferAssignment());\n+\n+  GpuExecutable::Params params;\n+  params.module_name = \"test_module\";\n+  params.executable =\n+      std::make_unique<SequentialThunk>(Thunk::ThunkInfo{}, ThunkSequence{});\n+\n+  std::vector<BufferAllocation> allocations;\n+  allocations.emplace_back(0, 1024, 0);\n+  allocations.emplace_back(1, 2048, 0);\n+\n+  const BufferAllocation* expected_ptr0 = &allocations[0];\n+  const BufferAllocation* expected_ptr1 = &allocations[1];\n+\n+  params.buffer_assignment = std::move(buffer_assignment);\n+  params.mlir_allocations = std::move(allocations);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<GpuExecutable> executable,\n+                          GpuExecutable::Create(std::move(params)));\n+\n+  // Expect that the allocations from mlir_allocations are returned.\n+  EXPECT_THAT(executable->GetAllocations(),\n+              ElementsAre(expected_ptr0, expected_ptr1));\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        },
        {
            "sha": "8c308438863b8b2a7b6eb230e93f13df3eb744d3",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_rewriter_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/122934aee44f35ae24c33fe5646b59f6db515904/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test.cc?ref=122934aee44f35ae24c33fe5646b59f6db515904",
            "patch": "@@ -1474,9 +1474,8 @@ class GemmRewriteAllocationTest : public GpuCodegenTest {\n                                          allocator_.get()));\n     GpuExecutable* gpu_executable =\n         static_cast<GpuExecutable*>(executable.get());\n-    absl::Span<const BufferAllocation> allocations =\n-        gpu_executable->GetAllocations();\n-    ASSERT_EQ(allocations.size(), expected_number_of_allocations);\n+    ASSERT_EQ(gpu_executable->GetAllocations().size(),\n+              expected_number_of_allocations);\n   }\n \n   DebugOptions GetDebugOptionsForTest() const override {"
        }
    ],
    "stats": {
        "total": 306,
        "additions": 225,
        "deletions": 81
    }
}