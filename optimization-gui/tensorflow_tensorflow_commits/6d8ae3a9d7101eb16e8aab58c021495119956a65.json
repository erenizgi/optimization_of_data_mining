{
    "author": "Moerafaat",
    "message": "Add warp specialization to Triton autotuning.\n\nThis change introduces `is_warp_specialization_allowed` to `TritonGemmConfig` and `BlockLevelFusionConfig`. The autotuner now explores configurations with warp specialization enabled, but only on Blackwell+ devices and when TMA is also enabled. The fusion emitter uses this new parameter to set the `tt.warp_specialize` attribute.\n\nPiperOrigin-RevId: 824601781",
    "sha": "6d8ae3a9d7101eb16e8aab58c021495119956a65",
    "files": [
        {
            "sha": "a35fb42b55265d7ce2a8bf536a8a1d1ab7d8b1d2",
            "filename": "third_party/xla/xla/autotuning.proto",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fautotuning.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fautotuning.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fautotuning.proto?ref=6d8ae3a9d7101eb16e8aab58c021495119956a65",
            "patch": "@@ -82,6 +82,7 @@ message AutotuneResult {\n     int64 num_warps = 6;\n     int64 num_ctas = 7;\n     bool is_tma_allowed = 8;\n+    bool is_warp_specialization_allowed = 9;\n   }\n \n   message CustomKernelFusionKey {"
        },
        {
            "sha": "1b393b9e8a6dee7c38efb2393250eb7536f48f3c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 36,
            "deletions": 29,
            "changes": 65,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=6d8ae3a9d7101eb16e8aab58c021495119956a65",
            "patch": "@@ -750,8 +750,9 @@ absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n     EmitterLocOpBuilder b, absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n-    const TiledHloComputation& tiled_computation, mlir::FunctionOpInterface fn,\n-    Value pid,\n+    const TiledHloComputation& tiled_computation,\n+    const BlockLevelParameters& block_level_parameters,\n+    mlir::FunctionOpInterface fn, Value pid,\n     absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values);\n \n // Returns the number of iterations of the loop over the contracting\n@@ -938,8 +939,9 @@ absl::StatusOr<ScalarOrTensor> EmitDot(\n     EmitterLocOpBuilder b, absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n-    const TiledHloInstruction& tiled_hlo_dot, mlir::FunctionOpInterface fn,\n-    Value pid,\n+    const TiledHloInstruction& tiled_hlo_dot,\n+    const BlockLevelParameters& block_level_parameters,\n+    mlir::FunctionOpInterface fn, Value pid,\n     absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values) {\n   // We expect to get a tiled HLO in form:\n   //\n@@ -1021,10 +1023,7 @@ absl::StatusOr<ScalarOrTensor> EmitDot(\n       /*upperBound=*/MakeIndex(b, loop_iteration_count),\n       /*step=*/MakeIndex(b, 1), accumulator);\n \n-  if (fusion->GetModule()\n-          ->config()\n-          .debug_options()\n-          .xla_gpu_experimental_enable_triton_warp_specialization()) {\n+  if (block_level_parameters.is_warp_specialization_allowed) {\n     for_op->setAttr(\"tt.warp_specialize\", b.getBoolAttr(true));\n   }\n \n@@ -1045,8 +1044,8 @@ absl::StatusOr<ScalarOrTensor> EmitDot(\n           EmitTiledComputation(\n               b, libdevice_path, device_info,\n               ::xla::Cast<HloFusionInstruction>(tiled_fusion_operand->hlo()),\n-              *tiled_fusion_operand->called_computation(), fn,\n-              computation_index, values));\n+              *tiled_fusion_operand->called_computation(),\n+              block_level_parameters, fn, computation_index, values));\n       if (result.size() != 1) {\n         return absl::InternalError(absl::StrCat(\n             \"Expected nested fusion computation to emit a single value, got \",\n@@ -1109,8 +1108,9 @@ absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n     EmitterLocOpBuilder b, absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n-    const TiledHloInstruction& tiled_hlo_dot, mlir::FunctionOpInterface fn,\n-    Value pid,\n+    const TiledHloInstruction& tiled_hlo_dot,\n+    const BlockLevelParameters& block_level_parameters,\n+    mlir::FunctionOpInterface fn, Value pid,\n     absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values) {\n   VLOG(2) << \"EmitScaledDot: \" << tiled_hlo_dot.ToString();\n   const HloScaledDotInstruction& scaled_dot =\n@@ -1155,6 +1155,8 @@ absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n       {IndexingMap::Variable{{0, loop_iteration_count - 1}, \"k\"}},\n       /*rt_vars=*/{}};\n \n+  // TODO(b/449668102): Consider adding warp specialization support for scaled\n+  // dot. At the moment, there are no benchmarks that use scaled dot.\n   auto for_op = b.create<mlir::scf::ForOp>(\n       /*lowerBound=*/MakeIndex(b, 0),\n       /*upperBound=*/MakeIndex(b, loop_iteration_count),\n@@ -1176,8 +1178,8 @@ absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n           EmitTiledComputation(\n               b, libdevice_path, device_info,\n               ::xla::Cast<HloFusionInstruction>(tiled_fusion_operand->hlo()),\n-              *tiled_fusion_operand->called_computation(), fn,\n-              computation_index, values));\n+              *tiled_fusion_operand->called_computation(),\n+              block_level_parameters, fn, computation_index, values));\n       if (result.size() != 1) {\n         return absl::InternalError(absl::StrCat(\n             \"Expected nested fusion computation to emit a single value, got \",\n@@ -1261,8 +1263,9 @@ absl::StatusOr<ScalarOrTensor> EmitConcatenate(\n     EmitterLocOpBuilder b, absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n-    const TiledHloInstruction& tiled_concatenate, mlir::FunctionOpInterface fn,\n-    Value pid,\n+    const TiledHloInstruction& tiled_concatenate,\n+    const BlockLevelParameters& block_level_parameters,\n+    mlir::FunctionOpInterface fn, Value pid,\n     absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values) {\n   const int64_t concatenate_dimension =\n       tiled_concatenate.hlo()->concatenate_dimension();\n@@ -1355,7 +1358,8 @@ absl::StatusOr<ScalarOrTensor> EmitConcatenate(\n         EmitTiledComputation(\n             b, libdevice_path, device_info,\n             ::xla::Cast<HloFusionInstruction>(tiled_fusion_operand->hlo()),\n-            *tiled_fusion_operand->called_computation(), fn, pid, values));\n+            *tiled_fusion_operand->called_computation(), block_level_parameters,\n+            fn, pid, values));\n     CHECK_EQ(result.size(), 1);\n     b.create<mlir::scf::YieldOp>(result.front().UnwrapTensor());\n   }\n@@ -1435,6 +1439,7 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n     EmitterLocOpBuilder b, absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion, const TiledHloInstruction& tiled_hlo,\n+    const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n     absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values) {\n   const HloInstruction* hlo = tiled_hlo.hlo();\n@@ -1484,21 +1489,21 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n \n   if (hlo->opcode() == HloOpcode::kConcatenate) {\n     return EmitConcatenate(b, libdevice_path, device_info, fusion, tiled_hlo,\n-                           fn, pid, values);\n+                           block_level_parameters, fn, pid, values);\n   }\n \n   if (hlo->opcode() == HloOpcode::kPad) {\n     return EmitPad(b, device_info, tiled_hlo, values, pid);\n   }\n \n   if (hlo->opcode() == HloOpcode::kDot) {\n-    return EmitDot(b, libdevice_path, device_info, fusion, tiled_hlo, fn, pid,\n-                   values);\n+    return EmitDot(b, libdevice_path, device_info, fusion, tiled_hlo,\n+                   block_level_parameters, fn, pid, values);\n   }\n \n   if (hlo->opcode() == HloOpcode::kScaledDot) {\n-    return EmitScaledDot(b, libdevice_path, device_info, fusion, tiled_hlo, fn,\n-                         pid, values);\n+    return EmitScaledDot(b, libdevice_path, device_info, fusion, tiled_hlo,\n+                         block_level_parameters, fn, pid, values);\n   }\n \n   if (hlo->opcode() == HloOpcode::kConstant) {\n@@ -1575,8 +1580,9 @@ absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n     EmitterLocOpBuilder b, absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n-    const TiledHloComputation& tiled_computation, mlir::FunctionOpInterface fn,\n-    Value pid,\n+    const TiledHloComputation& tiled_computation,\n+    const BlockLevelParameters& block_level_parameters,\n+    mlir::FunctionOpInterface fn, Value pid,\n     absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values) {\n   VLOG(2) << \"EmitTiledComputation: \" << tiled_computation.ToString();\n   for (const TiledHloInstruction* tiled_hlo :\n@@ -1601,10 +1607,10 @@ absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n       VLOG(1) << \"Skipping nested fusion: \" << hlo->ToString();\n       continue;\n     }\n-    TF_ASSIGN_OR_RETURN(\n-        ScalarOrTensor result,\n-        EmitTiledHloInstruction(b, libdevice_path, device_info, fusion,\n-                                *tiled_hlo, fn, pid, values));\n+    TF_ASSIGN_OR_RETURN(ScalarOrTensor result,\n+                        EmitTiledHloInstruction(\n+                            b, libdevice_path, device_info, fusion, *tiled_hlo,\n+                            block_level_parameters, fn, pid, values));\n     TF_RET_CHECK(values.insert({tiled_hlo, result}).second) << hlo->ToString();\n     VLOG(8) << \"Emitted \" << hlo->ToString(HloPrintOptions::ShortParsable());\n   }\n@@ -1840,7 +1846,8 @@ absl::Status EmitGeneric(mlir::OpBuilder builder,\n   TF_ASSIGN_OR_RETURN(\n       auto results,\n       EmitTiledComputation(b, libdevice_path, device_info, fusion,\n-                           tiled_hlo_computation, fn, tile_id, values));\n+                           tiled_hlo_computation, block_level_parameters, fn,\n+                           tile_id, values));\n \n   for (auto [root, result, arg] :\n        llvm::zip(tiled_hlo_computation.GetRoots(), results,"
        },
        {
            "sha": "706f375a1e0cb089cbe287d0047a2a57097e4a9d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=6d8ae3a9d7101eb16e8aab58c021495119956a65",
            "patch": "@@ -3284,15 +3284,17 @@ fdot {\n     \"fusion_backend_config\":{\n       \"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\n         \"output_tiles\":[{\"sizes\":[\"128\", \"64\"]}],\n-        \"is_tma_allowed\":\"1\"\n+        \"is_tma_allowed\":\"1\",\n+        \"is_warp_specialization_allowed\":\"1\"\n       }\n     }\n   }\n   fdot.rhs = f16[256,256]{1,0} fusion(fdot.p1), kind=kCustom, calls=frhs, backend_config={\n     \"fusion_backend_config\":{\n       \"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\n         \"output_tiles\":[{\"sizes\":[\"64\", \"128\"]}],\n-        \"is_tma_allowed\":\"1\"\n+        \"is_tma_allowed\":\"1\",\n+        \"is_warp_specialization_allowed\":\"1\"\n       }\n     }\n   }\n@@ -3313,7 +3315,8 @@ ENTRY entry {\n           \"num_warps\":\"8\",\n           \"num_ctas\":\"1\",\n           \"num_stages\":\"1\",\n-          \"is_tma_allowed\":\"1\"}}}\n+          \"is_tma_allowed\":\"1\",\n+          \"is_warp_specialization_allowed\":\"1\"}}}\n })\";\n \n   // Check that the IR attribute is set correctly."
        },
        {
            "sha": "f2f81ad26051d0ff56de6729516b6c252ddd6191",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_deviceless_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc?ref=6d8ae3a9d7101eb16e8aab58c021495119956a65",
            "patch": "@@ -263,15 +263,17 @@ fdot {\n     \"fusion_backend_config\":{\n       \"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\n         \"output_tiles\":[{\"sizes\":[\"128\", \"64\"]}],\n-        \"is_tma_allowed\":\"1\"\n+        \"is_tma_allowed\":\"1\",\n+        \"is_warp_specialization_allowed\":\"1\"\n       }\n     }\n   }\n   fdot.rhs = f16[256,256]{1,0} fusion(fdot.p1), kind=kCustom, calls=frhs, backend_config={\n     \"fusion_backend_config\":{\n       \"kind\":\"__triton_nested_gemm_fusion\", \"block_level_fusion_config\":{\n         \"output_tiles\":[{\"sizes\":[\"64\", \"128\"]}],\n-        \"is_tma_allowed\":\"1\"\n+        \"is_tma_allowed\":\"1\",\n+        \"is_warp_specialization_allowed\":\"1\"\n       }\n     }\n   }\n@@ -292,7 +294,8 @@ ENTRY entry {\n           \"num_warps\":\"8\",\n           \"num_ctas\":\"1\",\n           \"num_stages\":\"1\",\n-          \"is_tma_allowed\":\"1\"}}}\n+          \"is_tma_allowed\":\"1\",\n+          \"is_warp_specialization_allowed\":\"1\"}}}\n })\";\n \n   // Check that we extract the launch configuration correctly when warp"
        },
        {
            "sha": "6e47a558c2d567370ac73002ef42cfdf311abb38",
            "filename": "third_party/xla/xla/hlo/ir/backend_config_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fbackend_config_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fbackend_config_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fbackend_config_test.cc?ref=6d8ae3a9d7101eb16e8aab58c021495119956a65",
            "patch": "@@ -37,7 +37,7 @@ const int kNumRepetitions = 100;\n // since the == operator does not canonicalize the raw strings before comparing\n // them.\n constexpr absl::string_view kRawString =\n-    R\"({\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\"triton_gemm_config\":{\"block_m\":\"256\",\"block_n\":\"256\",\"block_k\":\"32\",\"split_k\":\"1\",\"num_stages\":\"1\",\"num_warps\":\"16\",\"num_ctas\":\"1\",\"is_tma_allowed\":false}},\"force_earliest_schedule\":false,\"reification_cost\":[],\"device_type\":\"DEVICE_TYPE_INVALID\"})\";\n+    R\"({\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\"triton_gemm_config\":{\"block_m\":\"256\",\"block_n\":\"256\",\"block_k\":\"32\",\"split_k\":\"1\",\"num_stages\":\"1\",\"num_warps\":\"16\",\"num_ctas\":\"1\",\"is_tma_allowed\":false,\"is_warp_specialization_allowed\":false}},\"force_earliest_schedule\":false,\"reification_cost\":[],\"device_type\":\"DEVICE_TYPE_INVALID\"})\";\n \n template <typename Input, typename CheckFn>\n void RunThreaded(Input input, CheckFn check_fn) {"
        },
        {
            "sha": "3c975f91d199396c91026a5fb06776aa26985a89",
            "filename": "third_party/xla/xla/service/gpu/autotuning/dot_search_space.cc",
            "status": "modified",
            "additions": 39,
            "deletions": 5,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space.cc?ref=6d8ae3a9d7101eb16e8aab58c021495119956a65",
            "patch": "@@ -123,7 +123,8 @@ TritonDotFusionSearchSpace::TritonDotFusionSearchSpace(\n }\n \n std::vector<TritonGemmConfig> TritonDotFusionSearchSpace::GenerateConfigs(\n-    std::optional<int64_t> force_contracting_split, bool autotune_tma) const {\n+    std::optional<int64_t> force_contracting_split, bool autotune_tma,\n+    bool autotune_warp_specialization) const {\n   std::vector<ConfigWithNotes> configs;\n   if (force_contracting_split.has_value()) {\n     ConfigWithNotes config;\n@@ -151,8 +152,22 @@ std::vector<TritonGemmConfig> TritonDotFusionSearchSpace::GenerateConfigs(\n   ExtendConfigs(configs, &TritonDotFusionSearchSpace::AddCtaSizeParameter);\n   ExtendConfigs(configs, &TritonDotFusionSearchSpace::AddContractingTiling);\n   ExtendConfigs(configs, &TritonDotFusionSearchSpace::AddPipeliningParameter);\n+\n+  if (autotune_warp_specialization && !autotune_tma) {\n+    LOG(WARNING)\n+        << \"Warp specialization is requested, but TMA is not enabled, hence \"\n+           \"warp specialization will be ignored. Set both \"\n+           \"`is_warp_specialization_allowed` and `is_tma_allowed` \"\n+           \"to true on the configuration to enable warp specialization.\";\n+  }\n   if (autotune_tma) {\n+    VLOG(10) << \"Parameterizing all currently constructed configs with \"\n+                \"TMA.\";\n     ExtendConfigs(configs, &TritonDotFusionSearchSpace::AddTmaParameter);\n+    if (autotune_warp_specialization) {\n+      ExtendConfigs(\n+          configs, &TritonDotFusionSearchSpace::AddWarpSpecializationParameter);\n+    }\n   }\n \n   std::vector<TritonGemmConfig> result;\n@@ -629,15 +644,34 @@ void TritonDotFusionSearchSpace::AddTmaParameter(\n     std::vector<ConfigWithNotes>& updated_configs) const {\n   ConfigWithNotes new_config = config;\n   new_config.config.is_tma_allowed = false;\n-  VLOG(10) << \"Adding TMA (disabled) parameter: config = \"\n-           << new_config.ToString();\n   updated_configs.push_back(new_config);\n   new_config.config.is_tma_allowed = true;\n-  VLOG(10) << \"Adding TMA (enabled) parameter: config = \"\n-           << new_config.ToString();\n   updated_configs.push_back(new_config);\n }\n \n+void TritonDotFusionSearchSpace::AddWarpSpecializationParameter(\n+    const ConfigWithNotes& config,\n+    std::vector<ConfigWithNotes>& updated_configs) const {\n+  ConfigWithNotes new_config = config;\n+  new_config.config.is_warp_specialization_allowed = false;\n+  updated_configs.push_back(new_config);\n+\n+  // Warp specialization probably only makes sense if TMA is enabled. Other\n+  // restrictions are required for compatibility with Triton, including:\n+  // - num_warps must be a multiple of 4.\n+  // - num_warps must be <= 16. This is because the next step for num_warps is\n+  // 32, which would break with auto warp specialization, because the feature\n+  // will employ `worker warps` that will mean we exceed the maximum block size\n+  // of 1024 threads.\n+  if (config.config.is_tma_allowed && config.config.num_warps <= 16 &&\n+      config.config.num_warps % 4 == 0) {\n+    new_config.config.is_warp_specialization_allowed = true;\n+    VLOG(10) << \"Adding warp specialization parameter: config = \"\n+             << new_config.ToString();\n+    updated_configs.push_back(new_config);\n+  }\n+}\n+\n void TritonDotFusionSearchSpace::EliminateLowOccupancyConfigs(\n     std::vector<ConfigWithNotes>& configs) const {\n   CHECK(!configs.empty());"
        },
        {
            "sha": "3cbae90aa0a9c7e7977a99450883f823442e8cf0",
            "filename": "third_party/xla/xla/service/gpu/autotuning/dot_search_space.h",
            "status": "modified",
            "additions": 15,
            "deletions": 4,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space.h?ref=6d8ae3a9d7101eb16e8aab58c021495119956a65",
            "patch": "@@ -42,13 +42,19 @@ class TritonDotFusionSearchSpace {\n                              const HloDotInstruction* dot);\n \n   // Generates the list of promising configs in the search space for the\n-  // autotuner to try. If `force_contracting_split` is set, the search space\n+  // autotuner to try.\n+  // If `force_contracting_split` is set, the search space\n   // will be restricted to only include configs with the given split_k factor.\n-  // If `autotune_tma` is set, the search space will be extended with TMA\n-  // parameterization.\n+  //\n+  // If true, `autotune_tma` and `autotune_warp_specialization` extend the\n+  // search space with TMA parameterization and warp specialization\n+  // respectively. Setting 'autotune_warp_specialization' to true also requires\n+  // `autotune_tma` to be true, given that warp specialization is probably not\n+  // useful without TMA.\n   std::vector<TritonGemmConfig> GenerateConfigs(\n       std::optional<int64_t> force_contracting_split = std::nullopt,\n-      bool autotune_tma = false) const;\n+      bool autotune_tma = false,\n+      bool autotune_warp_specialization = false) const;\n \n   // Restrict the set of configs to the ones compatible with the hints list.\n   // Generally, this will mean that configs are restricted to the ones that\n@@ -214,6 +220,11 @@ class TritonDotFusionSearchSpace {\n   void AddTmaParameter(const ConfigWithNotes& config,\n                        std::vector<ConfigWithNotes>& updated_configs) const;\n \n+  // Extend the passed configs with automatic warp specialization.\n+  void AddWarpSpecializationParameter(\n+      const ConfigWithNotes& config,\n+      std::vector<ConfigWithNotes>& updated_configs) const;\n+\n   // The order of these fields is important: the values of those defined earlier\n   // are used to compute the values of later ones.\n   se::DeviceDescription device_description_;"
        },
        {
            "sha": "94a2764eadf694ec504c7218ee01578c036683f4",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 1,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=6d8ae3a9d7101eb16e8aab58c021495119956a65",
            "patch": "@@ -779,6 +779,12 @@ bool GemmFusionAutotunerImpl::IsAutotuningEnabled() const {\n          !debug_options_.xla_gpu_deterministic_ops();\n }\n \n+bool GemmFusionAutotunerImpl::IsWarpSpecializationAvailable() const {\n+  auto compute_capability = config_.GetGpuComputeCapability();\n+  return compute_capability.IsCuda() &&\n+         compute_capability.cuda_compute_capability()->IsAtLeastBlackwell();\n+}\n+\n static std::vector<BackendConfig> GenerateCustomKernelFusionConfigs(\n     const HloFusionInstruction& fusion,\n     se::DeviceDescription device_description) {\n@@ -974,6 +980,16 @@ GemmFusionAutotunerImpl::GenerateTritonConfigs(const HloDotInstruction& dot) {\n   bool autotune_tma = debug_options_.xla_gpu_experimental_enable_triton_tma() &&\n                       stream_executor::gpu::IsTmaAvailableForDevice(\n                           config_.GetDeviceDescription());\n+  bool autotune_warp_specialization =\n+      debug_options_.xla_gpu_experimental_enable_triton_warp_specialization() &&\n+      IsWarpSpecializationAvailable();\n+  if (autotune_warp_specialization && !autotune_tma) {\n+    return absl::InvalidArgumentError(\n+        \"Warp specialization is requested, but TMA is not enabled. If you wish \"\n+        \"to enable warp specialization, set both \"\n+        \"`xla_gpu_experimental_enable_triton_tma` and \"\n+        \"`xla_gpu_experimental_enable_triton_warp_specialization` to true.\");\n+  }\n   TritonDotFusionSearchSpace search_space(config_.GetDeviceDescription(), &dot);\n   VLOG(1) << \"Generating configs from search space: \"\n           << search_space.ToString();\n@@ -983,7 +999,8 @@ GemmFusionAutotunerImpl::GenerateTritonConfigs(const HloDotInstruction& dot) {\n       /*force_contracting_split=*/autotune_contracting_split\n           ? std::nullopt\n           : std::make_optional(1),\n-      /*autotune_tma=*/autotune_tma);\n+      /*autotune_tma=*/autotune_tma,\n+      /*autotune_warp_specialization=*/autotune_warp_specialization);\n \n   // TODO(b/421858850): Restricting configs for dots from broadcasts is a\n   // temporary solution. We should remove this once we have a fix for the error."
        },
        {
            "sha": "b0e5e3e89657b0ef5235877ce1faab5f9f8376e2",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h?ref=6d8ae3a9d7101eb16e8aab58c021495119956a65",
            "patch": "@@ -164,6 +164,7 @@ class GemmFusionAutotunerImpl {\n   // Helper methods.\n   const AutotuneConfig& GetConfig() const { return config_; }\n   bool IsAutotuningEnabled() const;\n+  bool IsWarpSpecializationAvailable() const;\n \n   static const int64_t BLAS_GEMM_DEFAULT;\n "
        },
        {
            "sha": "8203df4df1604efa7445dc2da69cfe1d721cf7b3",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_cuda.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 3,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc?ref=6d8ae3a9d7101eb16e8aab58c021495119956a65",
            "patch": "@@ -94,12 +94,11 @@ std::vector<TritonGemmConfig> GemmFusionAutotunerImpl::GetDefaultTritonConfigs()\n     configs = *kDefaultCudaConfigs;\n   }\n \n+  // Hopper+ devices support TMA. Add TMA parameterized configs.\n   if (!debug_options_.xla_gpu_experimental_enable_triton_tma() ||\n       !compute_capability.IsAtLeastHopper()) {\n     return configs;\n   }\n-\n-  // Hopper+ devices support TMA. Add TMA parameterized configs.\n   std::vector<TritonGemmConfig> tma_parameterized_configs;\n   for (auto& config : configs) {\n     config.is_tma_allowed = false;\n@@ -108,7 +107,25 @@ std::vector<TritonGemmConfig> GemmFusionAutotunerImpl::GetDefaultTritonConfigs()\n     config.is_tma_allowed = true;\n     tma_parameterized_configs.push_back(config);\n   }\n-  return tma_parameterized_configs;\n+\n+  // TODO(b/449668102): Currently only supporting warp specialization on\n+  // Blackwell+. Potentially extend support to Hopper.\n+  if (!compute_capability.IsAtLeastBlackwell()) {\n+    return tma_parameterized_configs;\n+  }\n+  std::vector<TritonGemmConfig> warp_specialized_configs;\n+  for (auto& config : tma_parameterized_configs) {\n+    config.is_warp_specialization_allowed = false;\n+    warp_specialized_configs.push_back(config);\n+\n+    if (config.is_tma_allowed && config.num_warps <= 16 &&\n+        config.num_warps % 4 == 0) {\n+      config.is_warp_specialization_allowed = true;\n+      warp_specialized_configs.push_back(config);\n+    }\n+  }\n+\n+  return warp_specialized_configs;\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "2a66c03a0f062dc40258a11671d529c2204f02ce",
            "filename": "third_party/xla/xla/service/gpu/backend_configs.proto",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbackend_configs.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbackend_configs.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fbackend_configs.proto?ref=6d8ae3a9d7101eb16e8aab58c021495119956a65",
            "patch": "@@ -205,6 +205,9 @@ message BlockLevelFusionConfig {\n \n   // Allow/disallow TMA usage for all arguments of the kernel (where possible).\n   bool is_tma_allowed = 6;\n+\n+  // Allow/disallow automatic warp specialization.\n+  bool is_warp_specialization_allowed = 7;\n }\n \n message DynamicMemcpyConfig {"
        },
        {
            "sha": "5541b251c5c51e1109846d0e9c88784233d901f2",
            "filename": "third_party/xla/xla/service/gpu/matmul_utils.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmatmul_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmatmul_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmatmul_utils.cc?ref=6d8ae3a9d7101eb16e8aab58c021495119956a65",
            "patch": "@@ -806,10 +806,10 @@ absl::StatusOr<se::gpu::BlasLt::Epilogue> AsBlasLtEpilogue(\n   TF_RET_CHECK(proto.num_warps() > 0);\n   TF_RET_CHECK(proto.num_ctas() > 0);\n \n-  return TritonGemmConfig(proto.block_m(), proto.block_n(), proto.block_k(),\n-                          proto.split_k(), proto.num_stages(),\n-                          proto.num_warps(), proto.num_ctas(),\n-                          proto.is_tma_allowed());\n+  return TritonGemmConfig(\n+      proto.block_m(), proto.block_n(), proto.block_k(), proto.split_k(),\n+      proto.num_stages(), proto.num_warps(), proto.num_ctas(),\n+      proto.is_tma_allowed(), proto.is_warp_specialization_allowed());\n }\n \n AutotuneResult::TritonGemmKey TritonGemmConfig::ToProto() const {\n@@ -822,15 +822,17 @@ AutotuneResult::TritonGemmKey TritonGemmConfig::ToProto() const {\n   key.set_num_warps(num_warps);\n   key.set_num_ctas(num_ctas);\n   key.set_is_tma_allowed(is_tma_allowed);\n+  key.set_is_warp_specialization_allowed(is_warp_specialization_allowed);\n   return key;\n }\n \n std::string TritonGemmConfig::ToString() const {\n-  return absl::StrCat(\"{block_m:\", block_m, \",block_n:\", block_n,\n-                      \",block_k:\", block_k, \",split_k:\", split_k,\n-                      \",num_stages:\", num_stages, \",num_warps:\", num_warps,\n-                      \",num_ctas:\", num_ctas,\n-                      \",is_tma_allowed:\", is_tma_allowed, \"}\");\n+  return absl::StrCat(\n+      \"{block_m:\", block_m, \",block_n:\", block_n, \",block_k:\", block_k,\n+      \",split_k:\", split_k, \",num_stages:\", num_stages,\n+      \",num_warps:\", num_warps, \",num_ctas:\", num_ctas,\n+      \",is_tma_allowed:\", is_tma_allowed,\n+      \",is_warp_specialization_allowed:\", is_warp_specialization_allowed, \"}\");\n }\n \n absl::StatusOr<bool> IsMatrixMultiplicationTooSmallForRewriting("
        },
        {
            "sha": "ce0509f3ad1c27b8e598e1f6517cce8a3f46023f",
            "filename": "third_party/xla/xla/service/gpu/matmul_utils.h",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmatmul_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmatmul_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmatmul_utils.h?ref=6d8ae3a9d7101eb16e8aab58c021495119956a65",
            "patch": "@@ -179,15 +179,17 @@ struct TritonGemmConfig {\n   constexpr TritonGemmConfig() = default;\n   constexpr TritonGemmConfig(int block_m, int block_n, int block_k, int split_k,\n                              int num_stages, int num_warps, int num_ctas = 1,\n-                             bool is_tma_allowed = false)\n+                             bool is_tma_allowed = false,\n+                             bool is_warp_specialization_allowed = false)\n       : block_m(block_m),\n         block_n(block_n),\n         block_k(block_k),\n         split_k(split_k),\n         num_stages(num_stages),\n         num_warps(num_warps),\n         num_ctas(num_ctas),\n-        is_tma_allowed(is_tma_allowed) {}\n+        is_tma_allowed(is_tma_allowed),\n+        is_warp_specialization_allowed(is_warp_specialization_allowed) {}\n   int block_m = 0;\n   int block_n = 0;\n   int block_k = 0;\n@@ -198,6 +200,8 @@ struct TritonGemmConfig {\n   int num_ctas = 0;\n   // Allow/disallow TMA usage for all arguments of the kernel (where possible).\n   bool is_tma_allowed = false;\n+  // Allow/disallow automatic warp specialization.\n+  bool is_warp_specialization_allowed = false;\n \n   // When adding new members, please update all methods, such as ToTuple,\n   // FromProto, ToProto, ToString, etc. Updating ToTuple is not enough.\n@@ -209,7 +213,8 @@ struct TritonGemmConfig {\n  private:\n   auto ToTuple() const {\n     return std::make_tuple(block_m, block_n, block_k, split_k, num_stages,\n-                           num_warps, num_ctas, is_tma_allowed);\n+                           num_warps, num_ctas, is_tma_allowed,\n+                           is_warp_specialization_allowed);\n   }\n \n  public:"
        },
        {
            "sha": "380f857b2d9892f3e7bc68ad8b2690a665fe085e",
            "filename": "third_party/xla/xla/service/gpu/model/block_level_parameters.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fblock_level_parameters.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fblock_level_parameters.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fblock_level_parameters.h?ref=6d8ae3a9d7101eb16e8aab58c021495119956a65",
            "patch": "@@ -36,6 +36,7 @@ struct BlockLevelParameters {\n   int num_ctas = 1;\n   int num_stages = 1;\n   bool is_tma_allowed = false;\n+  bool is_warp_specialization_allowed = false;\n \n   // Returns a BlockLevelParameters struct from a BlockLevelFusionConfig proto.\n   static BlockLevelParameters FromBlockLevelFusionConfig(\n@@ -45,6 +46,8 @@ struct BlockLevelParameters {\n     result.num_ctas = config.num_ctas();\n     result.num_stages = config.num_stages();\n     result.is_tma_allowed = config.is_tma_allowed();\n+    result.is_warp_specialization_allowed =\n+        config.is_warp_specialization_allowed();\n     result.output_tile_sizes.reserve(config.output_tiles_size());\n     for (const auto& tile : config.output_tiles()) {\n       result.output_tile_sizes.push_back(\n@@ -65,6 +68,7 @@ struct BlockLevelParameters {\n     config.set_num_ctas(num_ctas);\n     config.set_num_stages(num_stages);\n     config.set_is_tma_allowed(is_tma_allowed);\n+    config.set_is_warp_specialization_allowed(is_warp_specialization_allowed);\n     return config;\n   }\n };"
        },
        {
            "sha": "9452e82afed4ca1d8a3df76ca797a8a830f69782",
            "filename": "third_party/xla/xla/service/gpu/model/block_level_parameters_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fblock_level_parameters_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fblock_level_parameters_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fblock_level_parameters_test.cc?ref=6d8ae3a9d7101eb16e8aab58c021495119956a65",
            "patch": "@@ -36,6 +36,7 @@ TEST(BlockLevelParametersTest,\n   block_level_fusion_config.set_num_ctas(13);\n   block_level_fusion_config.set_num_stages(14);\n   block_level_fusion_config.set_is_tma_allowed(true);\n+  block_level_fusion_config.set_is_warp_specialization_allowed(true);\n \n   BlockLevelParameters block_level_parameters =\n       BlockLevelParameters::FromBlockLevelFusionConfig(\n@@ -46,6 +47,7 @@ TEST(BlockLevelParametersTest,\n   EXPECT_THAT(block_level_parameters.num_ctas, 13);\n   EXPECT_THAT(block_level_parameters.num_stages, 14);\n   EXPECT_THAT(block_level_parameters.is_tma_allowed, true);\n+  EXPECT_THAT(block_level_parameters.is_warp_specialization_allowed, true);\n }\n \n TEST(BlockLevelParametersTest,\n@@ -56,6 +58,7 @@ TEST(BlockLevelParametersTest,\n   block_level_parameters.num_ctas = 13;\n   block_level_parameters.num_stages = 14;\n   block_level_parameters.is_tma_allowed = true;\n+  block_level_parameters.is_warp_specialization_allowed = true;\n \n   BlockLevelFusionConfig block_level_fusion_config =\n       block_level_parameters.ToBlockLevelFusionConfig();\n@@ -67,6 +70,7 @@ TEST(BlockLevelParametersTest,\n   EXPECT_THAT(block_level_fusion_config.num_ctas(), 13);\n   EXPECT_THAT(block_level_fusion_config.num_stages(), 14);\n   EXPECT_THAT(block_level_fusion_config.is_tma_allowed(), true);\n+  EXPECT_THAT(block_level_fusion_config.is_warp_specialization_allowed(), true);\n }\n \n }  // namespace"
        },
        {
            "sha": "cc42043564f0611a34562b614e8b34e0e6803821",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6d8ae3a9d7101eb16e8aab58c021495119956a65/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc?ref=6d8ae3a9d7101eb16e8aab58c021495119956a65",
            "patch": "@@ -202,6 +202,8 @@ absl::Status AnnotateDotOperandNestedFusionImpl(\n   block_level_parameters.num_ctas = config.num_ctas;\n   block_level_parameters.num_stages = config.num_stages;\n   block_level_parameters.is_tma_allowed = config.is_tma_allowed;\n+  block_level_parameters.is_warp_specialization_allowed =\n+      config.is_warp_specialization_allowed;\n \n   TF_ASSIGN_OR_RETURN(auto gpu_config,\n                       nested_fusion.backend_config<GpuBackendConfig>());\n@@ -1443,6 +1445,8 @@ absl::StatusOr<BlockLevelParameters> FindBlockLevelParameters(\n       params.num_ctas = config.num_ctas;\n       params.num_stages = config.num_stages;\n       params.is_tma_allowed = config.is_tma_allowed;\n+      params.is_warp_specialization_allowed =\n+          config.is_warp_specialization_allowed;\n       return params;\n     }\n     VLOG(4) << \"mapped_dot_tile_sizes: \""
        }
    ],
    "stats": {
        "total": 238,
        "additions": 177,
        "deletions": 61
    }
}