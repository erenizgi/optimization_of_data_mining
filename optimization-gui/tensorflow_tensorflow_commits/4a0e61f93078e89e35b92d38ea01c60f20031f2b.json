{
    "author": "apivovarov",
    "message": "Add raft_select_k executor using RAFT select_k\n\n- Introduces `raft_select_k_exec<T>` for batched Top-K selection on GPU\n  using RAFT.\n  - Supports float and nv_bfloat16 types.\n  - Manages device memory via StreamExecutor allocator.\n  - Selects the appropriate RAFT algorithm based on input shape and k.\n- Adds RAII wrapper `RaftStreamResource` for RAFT resources bound to a CUDA stream.\n- Implements unit tests verifying correctness of values and indices for both data types.\n- Test data uses deterministic bit-pattern generation for reproducibility.\n\nNext PR will add `RaftSelectKThunk` that leverages this executor.\n\nTesting\n\n```bash\nbazel test --repo_env=HERMETIC_PYTHON_VERSION=3.12 \\\n--config=cuda --define=xnn_enable_avxvnniint8=false \\\n--copt=-mavx --host_copt=-mavx --repo_env TF_NEED_CUDA=1 \\\n--repo_env TF_NCCL_USE_STUB=1 \\\n--action_env CLANG_COMPILER_PATH=/usr/lib/llvm-19/bin/clang \\\n--repo_env CC=/usr/lib/llvm-19/bin/clang \\\n--repo_env BAZEL_COMPILER=/usr/lib/llvm-19/bin/clang \\\n--action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-19/bin/clang \\\n--repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=9.0 \\\n--@local_config_cuda//:enable_cuda \\\n--@local_config_cuda//cuda:include_cuda_libs=false \\\n--linkopt=-Wl,--disable-new-dtags \\\n--action_env=TF_NVCC_CLANG=\"1\" \\\n--@local_config_cuda//:cuda_compiler=nvcc \\\n--verbose_failures=true --color=yes \\\n--define=tensorflow_mkldnn_contraction_kernel=1 \\\n--copt=-Wno-gnu-offsetof-extensions \\\n--copt=-Qunused-arguments --copt=-Werror=mismatched-tags \\\n--copt=-Wno-error=c23-extensions \\\n@spdlog//:all \\\n@fmt//:all \\\n@rapids_logger//:all \\\n@kokkos//:all \\\n@rmm//:all \\\n@raft//:all \\\n@raft//:raft_matrix \\\n//xla/backends/gpu/runtime:raft_select_k_exec_test\n\nINFO: Build completed successfully, 13197 total actions\n@fmt//:fmt_smoke_test                                           (cached) PASSED in 0.0s\n@kokkos//:smoke_test                                            (cached) PASSED in 0.0s\n@raft//:select_k_smoke_test                                     (cached) PASSED in 2.8s\n@rapids_logger//:smoke_test                                     (cached) PASSED in 0.0s\n@rmm//:cuda_stream_tests                                        (cached) PASSED in 2.9s\n@spdlog//:smoke_test                                            (cached) PASSED in 0.0s\n//xla/backends/gpu/runtime:raft_select_k_exec_test_a100                  PASSED in 8.5s\n//xla/backends/gpu/runtime:raft_select_k_exec_test_b200                  PASSED in 8.1s\n//xla/backends/gpu/runtime:raft_select_k_exec_test_h100                  PASSED in 8.0s\n//xla/backends/gpu/runtime:raft_select_k_exec_test_nvgpu_any             PASSED in 7.9s\n```\nPiperOrigin-RevId: 799247333",
    "sha": "4a0e61f93078e89e35b92d38ea01c60f20031f2b",
    "files": [
        {
            "sha": "b40d86d97a60dd659899aa90e9ea6001b2d7aace",
            "filename": "third_party/xla/third_party/raft/raft.BUILD",
            "status": "modified",
            "additions": 11,
            "deletions": 20,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a0e61f93078e89e35b92d38ea01c60f20031f2b/third_party%2Fxla%2Fthird_party%2Fraft%2Fraft.BUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a0e61f93078e89e35b92d38ea01c60f20031f2b/third_party%2Fxla%2Fthird_party%2Fraft%2Fraft.BUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fraft%2Fraft.BUILD?ref=4a0e61f93078e89e35b92d38ea01c60f20031f2b",
            "patch": "@@ -18,28 +18,19 @@ BASE_COPTS = [\n \n cuda_library(\n     name = \"raft_matrix\",\n-    srcs = glob([\n-        \"cpp/include/raft/core/detail/*.hpp\",\n-        \"cpp/include/raft/core/resource/detail/*.hpp\",\n-        \"cpp/include/raft/linalg/detail/*.hpp\",\n-        \"cpp/include/raft/matrix/detail/*.hpp\",\n-        \"cpp/include/raft/util/detail/*.hpp\",\n-    ]),\n-    hdrs = glob([\n-        \"cpp/include/raft/core/*.hpp\",\n-        \"cpp/include/raft/core/resource/*.hpp\",\n-        \"cpp/include/raft/linalg/*.hpp\",\n-        \"cpp/include/raft/matrix/*.hpp\",\n-        \"cpp/include/raft/util/*.hpp\",\n-    ]),\n     copts = BASE_COPTS,\n-    includes = [\"cpp/include\"],\n+    includes = [\n+        \"cpp/include\",\n+        \"cpp/internal\",\n+    ],\n     textual_hdrs = glob([\n-        \"cpp/include/raft/core/**/*.cuh\",\n-        \"cpp/include/raft/linalg/**/*.cuh\",\n-        \"cpp/include/raft/matrix/**/*.cuh\",\n-        \"cpp/include/raft/util/**/*.cuh\",\n-    ]),\n+        \"cpp/include/**/*.cuh\",\n+        \"cpp/include/**/*.hpp\",\n+        \"cpp/internal/**/*.cuh\",\n+    ]) + [\n+        \"cpp/include/raft/thirdparty/mdspan/include/experimental/mdarray\",\n+        \"cpp/include/raft/thirdparty/mdspan/include/experimental/mdspan\",\n+    ],\n     visibility = [\"//visibility:public\"],\n     deps = [\n         \"@kokkos//:mdspan\","
        },
        {
            "sha": "aa1b49989cf32ba7eb8ab60280dd94c083127b65",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 59,
            "deletions": 0,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a0e61f93078e89e35b92d38ea01c60f20031f2b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a0e61f93078e89e35b92d38ea01c60f20031f2b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=4a0e61f93078e89e35b92d38ea01c60f20031f2b",
            "patch": "@@ -1,3 +1,4 @@\n+load(\"@local_config_cuda//cuda:build_defs.bzl\", \"cuda_library\")\n load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n load(\"//xla:xla.default.bzl\", \"xla_cc_test\", \"xla_internal\")\n load(\"//xla/tests:build_defs.bzl\", \"xla_test\")\n@@ -861,6 +862,64 @@ xla_cc_test(\n     ],\n )\n \n+cuda_library(\n+    name = \"raft_select_k_exec\",\n+    srcs = [\"raft_select_k_exec.cc\"],\n+    hdrs = [\"raft_select_k_exec.h\"],\n+    copts = [\n+        \"-fexceptions\",\n+        \"-DLIBCUDACXX_ENABLE_EXPERIMENTAL_MEMORY_RESOURCE\",\n+    ],\n+    tags = [\"cuda-only\"],\n+    textual_hdrs = [\"raft_vectorized_bf16.h\"],\n+    deps = [\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:device_memory_allocator\",\n+        \"//xla/stream_executor:scratch_allocator\",\n+        \"//xla/stream_executor:stream\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+        \"@raft//:raft_matrix\",\n+    ],\n+)\n+\n+xla_test(\n+    name = \"raft_select_k_exec_test\",\n+    srcs = [\"raft_select_k_exec_test.cc\"],\n+    backends = [\n+        \"gpu\",\n+    ],\n+    tags = [\n+        \"cuda-only\",\n+    ],\n+    deps = [\n+        \":raft_select_k_exec\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla:xla_proto_cc\",\n+        \"//xla/service:platform_util\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:platform\",\n+        \"//xla/stream_executor:platform_manager\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor:stream_executor_memory_allocator\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/base\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/random\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+    ],\n+)\n+\n cc_library(\n     name = \"memset_thunk\",\n     srcs = [\"memset_thunk.cc\"],"
        },
        {
            "sha": "88f714ecd285a894739a037dbab7e3da48d32f4c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/raft_select_k_exec.cc",
            "status": "added",
            "additions": 316,
            "deletions": 0,
            "changes": 316,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a0e61f93078e89e35b92d38ea01c60f20031f2b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_select_k_exec.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a0e61f93078e89e35b92d38ea01c60f20031f2b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_select_k_exec.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_select_k_exec.cc?ref=4a0e61f93078e89e35b92d38ea01c60f20031f2b",
            "patch": "@@ -0,0 +1,316 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/raft_select_k_exec.h\"\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <exception>\n+#include <memory>\n+#include <optional>\n+#include <utility>\n+\n+#include \"absl/base/optimization.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"third_party/gpus/cuda/include/cuda_bf16.h\"\n+#include \"raft/core/device_mdspan.hpp\"\n+#include \"raft/core/mdspan_types.hpp\"\n+#include \"raft/core/resource/cuda_stream.hpp\"\n+#include \"raft/core/resource/device_memory_resource.hpp\"\n+#include \"raft/core/resources.hpp\"\n+#include \"raft/matrix/select_k.cuh\"\n+#include \"raft/matrix/select_k_types.hpp\"\n+// NOTE: This include is required for vectorized BF16 GPU runtime support.\n+// It will no longer be needed after upgrading to raft v25.10.00.\n+#include \"xla/backends/gpu/runtime/raft_vectorized_bf16.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/scratch_allocator.h\"\n+#include \"xla/stream_executor/stream.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla::gpu {\n+namespace se = ::stream_executor;\n+using raft::matrix::SelectAlgo;\n+\n+namespace {\n+\n+// Simple RAII wrapper to manage temporary device memory allocations\n+class OwningScratchAllocator {\n+ public:\n+  OwningScratchAllocator(int device_ordinal,\n+                         se::DeviceMemoryAllocator* allocator)\n+      : device_ordinal_(device_ordinal), allocator_(allocator) {}\n+\n+  OwningScratchAllocator(OwningScratchAllocator&&) = default;\n+  OwningScratchAllocator& operator=(OwningScratchAllocator&&) = default;\n+\n+  // Allocate memory and track ownership\n+  absl::StatusOr<se::DeviceMemory<uint8_t>> AllocateBytes(int64_t byte_size) {\n+    TF_ASSIGN_OR_RETURN(se::OwningDeviceMemory buffer,\n+                        allocator_->Allocate(device_ordinal_, byte_size,\n+                                             /*retry_on_failure=*/false));\n+\n+    se::DeviceMemory<uint8_t> res = *buffer;\n+    void* raw_ptr = res.opaque();\n+    buffers_.emplace(raw_ptr, std::move(buffer));\n+    return res;\n+  }\n+\n+  // Deallocate tracked memory; safe no-op if pointer not found\n+  absl::Status DeallocateBytes(void* ptr) {\n+    auto it = buffers_.find(ptr);\n+    if (it != buffers_.end()) {\n+      buffers_.erase(it);  // RAII frees memory\n+      return absl::OkStatus();\n+    }\n+    return absl::NotFoundError(\"Pointer not found\");\n+  }\n+\n+ private:\n+  int device_ordinal_;\n+  se::DeviceMemoryAllocator* allocator_;\n+  // key = raw device pointer, value = owning memory object\n+  absl::flat_hash_map<void*, se::OwningDeviceMemory> buffers_;\n+};\n+\n+// Custom RMM memory resource backed by StreamExecutor allocator\n+class XlaDeviceMemoryResource : public rmm::mr::device_memory_resource {\n+ public:\n+  XlaDeviceMemoryResource(int device_ordinal,\n+                          se::DeviceMemoryAllocator* allocator)\n+      : scratch_allocator_(device_ordinal, allocator) {}\n+\n+ protected:\n+  void* do_allocate(std::size_t bytes, rmm::cuda_stream_view stream) override {\n+    auto mem = scratch_allocator_.AllocateBytes(bytes);\n+    if (!mem.ok()) {\n+      // RMM expects exceptions\n+      throw rmm::bad_alloc(std::string(mem.status().ToString()));\n+    }\n+    return mem->opaque();\n+  }\n+\n+  void do_deallocate(void* ptr, std::size_t bytes,\n+                     rmm::cuda_stream_view stream) override {\n+    auto status = scratch_allocator_.DeallocateBytes(ptr);\n+    if (!status.ok()) {\n+      // do_deallocate should be noexcept. Don’t throw; just log.\n+      LOG(ERROR) << \"Scratch Deallocation failed: \" << status;\n+    }\n+  }\n+\n+ private:\n+  OwningScratchAllocator scratch_allocator_;\n+};\n+\n+// RAII wrapper for RAFT resources bound to a CUDA stream\n+struct RaftStreamResource : public se::Stream::Resource {\n+  raft::resources res;\n+\n+  // Factory to create a RaftStreamResource tied to a CUDA stream.\n+  // Sets up `raft::resources` with a custom XlaDeviceMemoryResource\n+  // using the given allocator and binds it to the provided stream.\n+  //\n+  // Args:\n+  //   device_ordinal: Device index.\n+  //   allocator: StreamExecutor memory allocator.\n+  //   cuda_stream: CUDA stream to bind.\n+  // Returns:\n+  //   Unique pointer to an initialized RaftStreamResource.\n+  static std::unique_ptr<RaftStreamResource> Create(\n+      int device_ordinal, se::DeviceMemoryAllocator* allocator,\n+      cudaStream_t cuda_stream) {\n+    // Assign our custom AllocatorForRaft for this device\n+    auto handle = std::make_unique<RaftStreamResource>();\n+    raft::resource::set_workspace_resource(\n+        handle->res,\n+        std::make_shared<XlaDeviceMemoryResource>(device_ordinal, allocator));\n+    // Set Cuda Stream\n+    raft::resource::set_cuda_stream(handle->res,\n+                                    rmm::cuda_stream_view{cuda_stream});\n+    return handle;\n+  }\n+};\n+\n+// ============================================================================\n+// choose_select_k_algorithm\n+//\n+// Purpose:\n+//   Heuristic-based selection of the optimal \"select k\" algorithm depending on\n+//   problem shape (rows, cols, k). The decision is based on benchmark data.\n+//\n+// How the heuristic is generated:\n+//\n+//   1. Build the benchmark module:\n+//        raft/cpp/bench/prims/matrix\n+//\n+//   2. Collect performance data by running microbenchmarks:\n+//\n+//        From the RAFT project root:\n+//          ./cpp/build/bench/prims/MATRIX_BENCH \\\n+//            --benchmark_filter=Select \\\n+//            --benchmark_out_format=json \\\n+//            --benchmark_out=select_k_times.json\n+//\n+//        Output:\n+//          - Benchmark results are written to `select_k_times.json`\n+//\n+//   3. Generate the heuristic using the provided notebook:\n+//\n+//        ./cpp/scripts/heuristics/select_k/generate_heuristic.ipynb\n+//\n+//        The notebook consumes `select_k_times.json`, analyzes performance\n+//        trade-offs, and produces the decision tree implemented here.\n+//\n+// Notes:\n+//   - To generate performance data for BFloat16,\n+//     modify cpp/bench/prims/matrix/select_k.cu  and register nv_bfloat16 type\n+//     using SELECTION_REGISTER mactos.\n+// ============================================================================\n+\n+template <typename T>\n+SelectAlgo choose_select_k_algorithm(uint32_t rows, uint32_t cols, uint32_t k) {\n+  static_assert(sizeof(T) == 0,\n+                \"choose_select_k_algorithm<T>: Unsupported type\");\n+  ABSL_UNREACHABLE();\n+}\n+\n+template <>\n+SelectAlgo choose_select_k_algorithm<float>(uint32_t rows, uint32_t cols,\n+                                            uint32_t k) {\n+  if (k > 256) {\n+    return SelectAlgo::kRadix11bits;\n+  } else if (k > 3) {\n+    if (cols > 55000) {\n+      return SelectAlgo::kWarpDistributedShm;\n+    } else {\n+      if (cols > 5250) {\n+        if (k > 192) {\n+          return SelectAlgo::kRadix11bits;\n+        } else {\n+          return SelectAlgo::kWarpDistributedShm;\n+        }\n+      } else {\n+        return SelectAlgo::kWarpDistributedShm;\n+      }\n+    }\n+  } else {\n+    return SelectAlgo::kWarpImmediate;\n+  }\n+}\n+\n+template <>\n+SelectAlgo choose_select_k_algorithm<nv_bfloat16>(uint32_t rows, uint32_t cols,\n+                                                  uint32_t k) {\n+  if (k > 256) {\n+    return SelectAlgo::kRadix11bits;\n+  } else if (k > 3) {\n+    if (cols > 5250 && k > 192) {\n+      return SelectAlgo::kRadix11bits;\n+    } else {\n+      return SelectAlgo::kWarpDistributedShm;\n+    }\n+  } else {\n+    return SelectAlgo::kWarpImmediate;\n+  }\n+}\n+\n+}  // namespace\n+\n+// Host-side entry point for raft select_k\n+template <typename T>\n+absl::Status raft_select_k_exec(\n+    int device_ordinal, se::DeviceMemoryAllocator* allocator,\n+    se::Stream* stream, se::DeviceMemoryBase data_in,\n+    se::DeviceMemoryBase data_out, se::DeviceMemoryBase indices_out,\n+    std::uint32_t batch, std::uint32_t n, std::uint32_t k) {\n+  // Validate input sizes\n+  DCHECK_EQ(data_in.size(), static_cast<uint64_t>(batch) * n * sizeof(T));\n+  DCHECK_EQ(data_out.size(), static_cast<uint64_t>(batch) * k * sizeof(T));\n+  DCHECK_EQ(indices_out.size(),\n+            static_cast<uint64_t>(batch) * k * sizeof(uint32_t));\n+  DCHECK_GE(n, k);\n+\n+  // Pick the most suitable algorithm\n+  SelectAlgo algo = choose_select_k_algorithm<T>(batch, n, k);\n+  VLOG(3) << \"raft_select_k_exec: \"\n+          << \"device_ordinal: \" << device_ordinal << \", \"\n+          << \"data_in: \" << data_in.opaque() << \" (\" << data_in.size() << \"B)\"\n+          << \", data_out: \" << data_out.opaque() << \" (\" << data_out.size()\n+          << \"B)\"\n+          << \", indices_out: \" << indices_out.opaque() << \" (\"\n+          << indices_out.size() << \"B)\"\n+          << \", batch: \" << batch << \", n: \" << n << \", k: \" << k\n+          << \", algo: \" << algo;\n+\n+  // Retrieve or create RAFT resource for this stream\n+  cudaStream_t cuda_stream =\n+      reinterpret_cast<cudaStream_t>(stream->platform_specific_handle().stream);\n+  DCHECK(cuda_stream != nullptr);\n+  RaftStreamResource* resContainer =\n+      stream->GetOrCreateResource<RaftStreamResource>(\n+          [device_ordinal, allocator, cuda_stream] {\n+            return RaftStreamResource::Create(device_ordinal, allocator,\n+                                              cuda_stream);\n+          });\n+\n+  try {\n+    // Wrap raw device pointers in RAFT matrix views\n+    auto input_view =\n+        raft::make_device_matrix_view<const T, uint32_t, raft::row_major>(\n+            reinterpret_cast<const T*>(data_in.opaque()), batch, n);\n+\n+    auto output_values_view =\n+        raft::make_device_matrix_view<T, uint32_t, raft::row_major>(\n+            reinterpret_cast<T*>(data_out.opaque()), batch, k);\n+\n+    auto output_indices_view =\n+        raft::make_device_matrix_view<uint32_t, uint32_t, raft::row_major>(\n+            reinterpret_cast<uint32_t*>(indices_out.opaque()), batch, k);\n+\n+    // Call RAFT select_k kernel\n+    raft::matrix::select_k<T, uint32_t>(\n+        resContainer->res, input_view,\n+        std::nullopt,  // d_input_indices can be omitted\n+        output_values_view, output_indices_view,\n+        /*select_min=*/false,\n+        /*sorted=*/true,\n+        /*algo=*/algo);\n+\n+    return absl::OkStatus();\n+  } catch (const std::exception& e) {\n+    return absl::InternalError(absl::StrCat(\"select_k failed: \", e.what()));\n+  } catch (...) {\n+    return absl::InternalError(\"select_k failed with unknown exception\");\n+  }\n+}\n+\n+// Explicit instantiations for supported types\n+template absl::Status raft_select_k_exec<float>(\n+    int, se::DeviceMemoryAllocator*, se::Stream*, se::DeviceMemoryBase,\n+    se::DeviceMemoryBase, se::DeviceMemoryBase, std::uint32_t, std::uint32_t,\n+    std::uint32_t);\n+\n+template absl::Status raft_select_k_exec<nv_bfloat16>(\n+    int, se::DeviceMemoryAllocator*, se::Stream*, se::DeviceMemoryBase,\n+    se::DeviceMemoryBase, se::DeviceMemoryBase, std::uint32_t, std::uint32_t,\n+    std::uint32_t);\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "d0831ec3a5f5f0d28273e1ed582949bd37cc0b4a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/raft_select_k_exec.h",
            "status": "added",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a0e61f93078e89e35b92d38ea01c60f20031f2b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_select_k_exec.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a0e61f93078e89e35b92d38ea01c60f20031f2b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_select_k_exec.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_select_k_exec.h?ref=4a0e61f93078e89e35b92d38ea01c60f20031f2b",
            "patch": "@@ -0,0 +1,54 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_RAFT_SELECT_K_EXEC_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_RAFT_SELECT_K_EXEC_H_\n+\n+#include <cstdint>\n+\n+#include \"absl/status/status.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/device_memory_allocator.h\"\n+#include \"xla/stream_executor/stream.h\"\n+\n+namespace xla::gpu {\n+\n+// Launches a RAFT Top-K selection on GPU for a batch of matrices.\n+//\n+// Args:\n+//   device_ordinal: GPU device index to run the operation on.\n+//   allocator: StreamExecutor memory allocator for device buffers.\n+//   stream: StreamExecutor stream for the GPU operations.\n+//   data_in: Device memory containing input matrices (batch × n).\n+//   data_out: Device memory to store top-k values (batch × k).\n+//   indices_out: Device memory to store top-k indices (batch × k).\n+//   batch: Number of rows (matrices) in the batch.\n+//   n: Number of columns (elements per row) in input matrices.\n+//   k: Number of top elements to select per row.\n+//\n+// Returns:\n+//   absl::Status indicating success or failure of the operation.\n+template <typename T>\n+absl::Status raft_select_k_exec(\n+    int device_ordinal, ::stream_executor::DeviceMemoryAllocator* allocator,\n+    ::stream_executor::Stream* stream,\n+    ::stream_executor::DeviceMemoryBase data_in,\n+    ::stream_executor::DeviceMemoryBase data_out,\n+    ::stream_executor::DeviceMemoryBase indices_out, std::uint32_t batch,\n+    std::uint32_t n, std::uint32_t k);\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_RAFT_SELECT_K_EXEC_H_"
        },
        {
            "sha": "2f86c42f18c105ebd49d6027e8b272fcaeef22ed",
            "filename": "third_party/xla/xla/backends/gpu/runtime/raft_select_k_exec_test.cc",
            "status": "added",
            "additions": 155,
            "deletions": 0,
            "changes": 155,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a0e61f93078e89e35b92d38ea01c60f20031f2b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_select_k_exec_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a0e61f93078e89e35b92d38ea01c60f20031f2b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_select_k_exec_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_select_k_exec_test.cc?ref=4a0e61f93078e89e35b92d38ea01c60f20031f2b",
            "patch": "@@ -0,0 +1,155 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/raft_select_k_exec.h\"\n+\n+#include <algorithm>\n+#include <cstdint>\n+#include <cstring>\n+#include <functional>\n+#include <memory>\n+#include <vector>\n+\n+#include <gtest/gtest.h>\n+#include \"absl/base/casts.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/random/random.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/ascii.h\"\n+#include \"absl/types/span.h\"\n+#include \"third_party/gpus/cuda/include/cuda_bf16.h\"\n+#include \"xla/service/platform_util.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/platform.h\"\n+#include \"xla/stream_executor/platform_manager.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/xla.pb.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+// Returns the first GPU StreamExecutor\n+se::StreamExecutor* GpuExecutor() {\n+  auto name =\n+      absl::AsciiStrToUpper(PlatformUtil::CanonicalPlatformName(\"gpu\").value());\n+  auto* platform = se::PlatformManager::PlatformWithName(name).value();\n+  CHECK(platform != nullptr);\n+  CHECK_OK(platform->ExecutorForDevice(0));\n+  return platform->ExecutorForDevice(0).value();\n+}\n+\n+// Trait: map Data type to Mask type and a default kStartBits\n+template <typename T>\n+struct MaskFor;\n+\n+template <>\n+struct MaskFor<float> {\n+  using type = uint32_t;\n+  static constexpr type kStartBits = 0x3C000000;  // float32: 1/128\n+};\n+\n+template <>\n+struct MaskFor<nv_bfloat16> {\n+  using type = uint16_t;\n+  static constexpr type kStartBits = 0x3C00;  // bfloat16: 1/128\n+};\n+\n+// Fills vector with unique values using bit patterns starting from kStartBits\n+template <typename T>\n+void append_unique_numbers(size_t count, std::vector<T>& arr) {\n+  using Traits = MaskFor<T>;\n+  using MaskT = typename Traits::type;\n+  MaskT bits = Traits::kStartBits;\n+\n+  for (size_t i = 0; i < count; ++i, ++bits) {\n+    T val = absl::bit_cast<T>(bits);\n+    arr.push_back(val);\n+  }\n+}\n+\n+}  // namespace\n+\n+// Template test function for raft select_k\n+template <typename T>\n+void RunSelectKTest() {\n+  se::StreamExecutor* stream_executor = GpuExecutor();\n+  TF_ASSERT_OK_AND_ASSIGN(auto stream, stream_executor->CreateStream());\n+  int device_ordinal = stream_executor->device_ordinal();\n+  se::StreamExecutorMemoryAllocator allocator(stream_executor);\n+\n+  std::uint32_t batch = 4;\n+  std::uint32_t n = 4096;\n+  std::uint32_t k = 32;\n+  absl::BitGen gen;\n+\n+  // Prepare unique values for Top-K testing\n+  std::vector<T> topk;\n+  topk.reserve(n);\n+  append_unique_numbers<T>(n, topk);\n+\n+  // Populate input matrix (batch x n) with shuffled topk values\n+  std::vector<T> h_data_in(batch * n);\n+  for (int j = 0; j < batch; ++j) {\n+    std::shuffle(topk.begin(), topk.end(), gen);\n+    std::copy(topk.begin(), topk.end(), h_data_in.begin() + j * n);\n+  }\n+\n+  // Compute golden Top-K values for verification\n+  std::sort(topk.begin(), topk.end(), std::greater<T>());\n+  topk.resize(k);\n+\n+  // Allocate device memory for input and outputs\n+  se::DeviceMemory<T> d_data_in =\n+      stream_executor->AllocateArray<T>(batch * n, 0);\n+  se::DeviceMemory<T> d_data_out =\n+      stream_executor->AllocateArray<T>(batch * k, 0);\n+  se::DeviceMemory<uint32_t> d_indices_out =\n+      stream_executor->AllocateArray<uint32_t>(batch * k, 0);\n+\n+  // Copy host to device\n+  TF_ASSERT_OK(stream->MemcpyH2D(absl::Span<const T>(h_data_in), &d_data_in));\n+\n+  // Run raft select_k\n+  TF_ASSERT_OK(raft_select_k_exec<T>(device_ordinal, &allocator, stream.get(),\n+                                     d_data_in, d_data_out, d_indices_out,\n+                                     batch, n, k));\n+\n+  // Copy results back to host\n+  std::vector<T> h_data_out(batch * k);\n+  std::vector<uint32_t> h_indices_out(batch * k);\n+  TF_ASSERT_OK(stream->MemcpyD2H(d_data_out, absl::Span<T>(h_data_out)));\n+  TF_ASSERT_OK(\n+      stream->MemcpyD2H(d_indices_out, absl::Span<uint32_t>(h_indices_out)));\n+  TF_ASSERT_OK(stream->BlockHostUntilDone());\n+\n+  // Verify Top-K values and corresponding indices\n+  for (int j = 0; j < batch; ++j) {\n+    for (int i = 0; i < k; ++i) {\n+      EXPECT_EQ(h_data_out[j * k + i], topk[i]) << \"batch=\" << j << \" i=\" << i;\n+      auto idx = h_indices_out[j * k + i];\n+      EXPECT_EQ(h_data_in[j * n + idx], topk[i]) << \"batch=\" << j << \" i=\" << i;\n+    }\n+  }\n+}\n+\n+TEST(RaftSelectKExecTest, SelectKFloat) { RunSelectKTest<float>(); }\n+\n+TEST(RaftSelectKExecTest, SelectKBFloat16) { RunSelectKTest<nv_bfloat16>(); }\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "13920c71beef38d9a18ebc82ac4c8267ed515188",
            "filename": "third_party/xla/xla/backends/gpu/runtime/raft_vectorized_bf16.h",
            "status": "added",
            "additions": 56,
            "deletions": 0,
            "changes": 56,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4a0e61f93078e89e35b92d38ea01c60f20031f2b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_vectorized_bf16.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4a0e61f93078e89e35b92d38ea01c60f20031f2b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_vectorized_bf16.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fraft_vectorized_bf16.h?ref=4a0e61f93078e89e35b92d38ea01c60f20031f2b",
            "patch": "@@ -0,0 +1,56 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_RAFT_VECTORIZED_BF16_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_RAFT_VECTORIZED_BF16_H_\n+\n+#pragma once\n+#include \"third_party/gpus/cuda/include/cuda_bf16.h\"\n+#include \"raft/util/vectorized.cuh\"\n+\n+namespace raft {\n+\n+template <>\n+struct IOType<__nv_bfloat16, 1> {\n+  typedef __nv_bfloat16 Type;\n+};\n+template <>\n+struct IOType<__nv_bfloat16, 2> {\n+  typedef __nv_bfloat162 Type;\n+};\n+template <>\n+struct IOType<__nv_bfloat16, 4> {\n+  typedef uint2 Type;\n+};\n+template <>\n+struct IOType<__nv_bfloat16, 8> {\n+  typedef uint4 Type;\n+};\n+template <>\n+struct IOType<__nv_bfloat162, 1> {\n+  typedef __nv_bfloat162 Type;\n+};\n+template <>\n+struct IOType<__nv_bfloat162, 2> {\n+  typedef uint2 Type;\n+};\n+template <>\n+struct IOType<__nv_bfloat162, 4> {\n+  typedef uint4 Type;\n+};\n+\n+}  // namespace raft\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_RAFT_VECTORIZED_BF16_H_"
        }
    ],
    "stats": {
        "total": 671,
        "additions": 651,
        "deletions": 20
    }
}