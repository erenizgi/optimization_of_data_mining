{
    "author": "tensorflower-gardener",
    "message": "Deduplicate functions on the one with largest number of call sites.\n\nInstead of picking arbitrarily.\n\nPiperOrigin-RevId: 822566069",
    "sha": "39506ad1cd333de28aa9c46b7c134d88c83d8861",
    "files": [
        {
            "sha": "1f39f572ceef1c3b876d4f143a0a515af5378c17",
            "filename": "third_party/xla/xla/service/spmd/shardy/round_trip_common/export_named_computations.cc",
            "status": "modified",
            "additions": 53,
            "deletions": 4,
            "changes": 57,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/39506ad1cd333de28aa9c46b7c134d88c83d8861/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fexport_named_computations.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/39506ad1cd333de28aa9c46b7c134d88c83d8861/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fexport_named_computations.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fexport_named_computations.cc?ref=39506ad1cd333de28aa9c46b7c134d88c83d8861",
            "patch": "@@ -15,9 +15,11 @@ limitations under the License.\n \n #include \"xla/service/spmd/shardy/round_trip_common/export_named_computations.h\"\n \n+#include <cstdint>\n #include <memory>\n #include <optional>\n #include <tuple>\n+#include <utility>\n \n #include \"absl/log/check.h\"\n #include \"llvm/ADT/DenseMap.h\"\n@@ -151,6 +153,53 @@ class ExportNamedComputationsPass\n     ModuleOp moduleOp = getOperation();\n     SymbolTable symbolTable(moduleOp);\n     mlir::Block& moduleBlock = moduleOp.getRegion().front();\n+\n+    if (dedupFunctionsFully) {\n+      llvm::SmallDenseMap<ComputationKey, int64_t> funcCallSiteCounts;\n+      llvm::SmallDenseMap<std::pair<StringRef, ManualAxesAttr>,\n+                          std::pair<NamedComputationOp, int64_t>>\n+          funcToNamedComputations;\n+      moduleOp.walk([&](NamedComputationOp namedComputationOp) {\n+        ManualAxesAttr manualAxesAttr =\n+            namedComputationOp->getAttrOfType<ManualAxesAttr>(kManualAxes);\n+        auto key =\n+            std::make_tuple(namedComputationOp.getName(),\n+                            namedComputationOp.getInShardings().value_or(\n+                                TensorShardingPerValueAttr()),\n+                            namedComputationOp.getOutShardings().value_or(\n+                                TensorShardingPerValueAttr()),\n+                            manualAxesAttr);\n+        const int64_t callSiteCount = funcCallSiteCounts[key]++;\n+        if (auto [it, inserted] = funcToNamedComputations.try_emplace(\n+                std::pair(namedComputationOp.getName(), manualAxesAttr),\n+                namedComputationOp, callSiteCount);\n+            !inserted) {\n+          auto& [cachedNamedComputationOp, cachedCallSiteCount] = it->second;\n+          if (callSiteCount > cachedCallSiteCount) {\n+            cachedNamedComputationOp = namedComputationOp;\n+            cachedCallSiteCount = callSiteCount;\n+          }\n+        }\n+      });\n+\n+      for (auto& [_, namedComputationCountPair] : funcToNamedComputations) {\n+        auto& [namedComputationOp, callSiteCount] = namedComputationCountPair;\n+        mlir::IRRewriter rewriter(namedComputationOp);\n+        rewriter.setInsertionPointToEnd(&moduleBlock);\n+        ManualAxesAttr manualAxesAttr =\n+            namedComputationOp->getAttrOfType<ManualAxesAttr>(kManualAxes);\n+        StringAttr funcSymName =\n+            createFuncOp(namedComputationOp, rewriter, symbolTable,\n+                         namedComputationOp.getInShardings(),\n+                         namedComputationOp.getOutShardings(), manualAxesAttr);\n+        funcCache.try_emplace(\n+            std::make_tuple(namedComputationOp.getName(),\n+                            TensorShardingPerValueAttr(),\n+                            TensorShardingPerValueAttr(), manualAxesAttr),\n+            funcSymName);\n+      }\n+    }\n+\n     // NOTE: The walk needs to be in post order, which is the default order, to\n     // account for nested named computations.\n     moduleOp.walk([&](NamedComputationOp namedComputationOp) {\n@@ -210,10 +259,10 @@ class ExportNamedComputationsPass\n   Option<bool> dedupFunctionsFully{\n       *this, \"dedup-functions-fully\",\n       llvm::cl::desc(\n-          \"Whether to deduplicate functions fully, regardless of the input and \"\n-          \"output shardings of functions, and it keeps one callee function for \"\n-          \"each caller function. The default is false, meaning it will \"\n-          \"deduplicate only if the input and output shardings are the same.\"),\n+          \"If true, regardless of the input and output shardings of functions, \"\n+          \"it keeps one callee function for each caller function. The default \"\n+          \"is false, meaning it will deduplicate only if the input and output \"\n+          \"shardings are the same.\"),\n       llvm::cl::init(false)};\n };\n "
        },
        {
            "sha": "58d6475c43a11321c5497262e541ff5067ac6923",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/export_named_computations_deduplicate_functions_fully.mlir",
            "status": "modified",
            "additions": 51,
            "deletions": 7,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/39506ad1cd333de28aa9c46b7c134d88c83d8861/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fexport_named_computations_deduplicate_functions_fully.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/39506ad1cd333de28aa9c46b7c134d88c83d8861/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fexport_named_computations_deduplicate_functions_fully.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fexport_named_computations_deduplicate_functions_fully.mlir?ref=39506ad1cd333de28aa9c46b7c134d88c83d8861",
            "patch": "@@ -8,7 +8,7 @@ func.func @multiple_same_named_computations_different_shardings(%arg0: tensor<8x\n   // CHECK-NEXT: %1 = call @baz(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {}]>]>} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n   // CHECK-NEXT: return %1 : tensor<8x2xi32>\n   %0 = sdy.named_computation<\"baz\">(%arg0) in_shardings=[<@mesh, [{}, {\"y\"}]>] out_shardings=[<@mesh, [{\"x\"}, {}]>] (%arg1: tensor<8x2xi32>) {\n-    %2 = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\", ?}, {\"y\", ?}]>]>} : tensor<8x2xi32>\n+    %2 = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\", ?}, {?}]>]>} : tensor<8x2xi32>\n     sdy.return %2 : tensor<8x2xi32>\n   } : (tensor<8x2xi32>) -> tensor<8x2xi32>\n   %1 = sdy.named_computation<\"baz\">(%arg0) in_shardings=[<@mesh, [{}, {\"y\"}]>] out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n@@ -21,6 +21,39 @@ func.func @multiple_same_named_computations_different_shardings(%arg0: tensor<8x\n // CHECK-LABEL: func private @baz(\n // CHECK-SAME:    %arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {\"y\"}]>})\n // CHECK-SAME:    -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {}]>})\n+// CHECK-NEXT:  stablehlo.multiply %arg0, %arg0\n+// CHECK-SAME:  sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\", ?}, {?}]>]>}\n+\n+// -----\n+\n+sdy.mesh @mesh = <[\"x\"=2, \"y\"=2]>\n+\n+// CHECK-LABEL: func @multiple_same_named_computations_different_shardings_different_number_of_call_sites(\n+func.func @multiple_same_named_computations_different_shardings_different_number_of_call_sites(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"y\"}, {\"x\"}]>}) -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) {\n+  // CHECK-NEXT: %0 = call @baz(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {\"y\"}]>]>} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n+  // CHECK-NEXT: %1 = call @baz(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {\"y\"}]>]>} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n+  // CHECK-NEXT: %2 = call @baz(%0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {\"y\"}]>]>} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n+  // CHECK-NEXT: return %2 : tensor<8x2xi32>\n+  %0 = sdy.named_computation<\"baz\">(%arg0) in_shardings=[<@mesh, [{}, {\"y\"}]>] out_shardings=[<@mesh, [{\"x\"}, {}]>] (%arg1: tensor<8x2xi32>) {\n+    %2 = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\", ?}, {?}]>]>} : tensor<8x2xi32>\n+    sdy.return %2 : tensor<8x2xi32>\n+  } : (tensor<8x2xi32>) -> tensor<8x2xi32>\n+  %1 = sdy.named_computation<\"baz\">(%arg0) in_shardings=[<@mesh, [{}, {\"y\"}]>] out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n+    %3 = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\", ?}, {\"y\", ?}]>]>} : tensor<8x2xi32>\n+    sdy.return %3 : tensor<8x2xi32>\n+  } : (tensor<8x2xi32>) -> tensor<8x2xi32>\n+  %2 = sdy.named_computation<\"baz\">(%0) in_shardings=[<@mesh, [{}, {\"y\"}]>] out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n+    %3 = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\", ?}, {\"y\", ?}]>]>} : tensor<8x2xi32>\n+    sdy.return %3 : tensor<8x2xi32>\n+  } : (tensor<8x2xi32>) -> tensor<8x2xi32>\n+  return %2 : tensor<8x2xi32>\n+}\n+\n+// CHECK-LABEL: func private @baz(\n+// CHECK-SAME:    %arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {\"y\"}]>})\n+// CHECK-SAME:    -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>})\n+// CHECK-NEXT:  stablehlo.multiply %arg0, %arg0\n+// CHECK-SAME:  sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\", ?}, {\"y\", ?}]>]>}\n \n // -----\n \n@@ -29,13 +62,17 @@ sdy.mesh @mesh = <[\"x\"=2, \"y\"=2]>\n // CHECK-LABEL: func @named_computations_same_funcs_two_same_manual_axes_different_shardings_one_without_manual_axes(\n // CHECK-SAME:      %arg0: tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\", \"y\"}]>}\n // CHECK-SAME:      -> (tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\", \"y\"}]>}) {\n-// CHECK:       %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{\"x\", \"y\"}]>] out_shardings=[<@mesh, [{\"x\", \"y\"}]>] manual_axes={\"x\"} (%arg1: tensor<4xf32>) {\n-// CHECK-NEXT:    %2 = func.call @foo(%arg1) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>} : (tensor<4xf32>) -> tensor<4xf32>\n-// CHECK-NEXT:    %3 = func.call @foo(%2) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>} : (tensor<4xf32>) -> tensor<4xf32>\n-// CHECK-NEXT:    sdy.return %3 : tensor<4xf32>\n+// CHECK-NEXT:  %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{\"x\", \"y\"}]>] out_shardings=[<@mesh, [{\"x\", \"y\"}]>] manual_axes={\"x\"} (%arg1: tensor<4xf32>) {\n+// CHECK-NEXT:    %3 = func.call @foo(%arg1) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>} : (tensor<4xf32>) -> tensor<4xf32>\n+// CHECK-NEXT:    %4 = func.call @foo(%3) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>} : (tensor<4xf32>) -> tensor<4xf32>\n+// CHECK-NEXT:    sdy.return %4 : tensor<4xf32>\n // CHECK-NEXT:  } : (tensor<8xf32>) -> tensor<8xf32>\n // CHECK-NEXT:  %1 = call @foo_0(%0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>} : (tensor<8xf32>) -> tensor<8xf32>\n-// CHECK-NEXT:  return %1 : tensor<8xf32>\n+// CHECK-NEXT:  %2 = sdy.manual_computation(%1) in_shardings=[<@mesh, [{\"x\", \"y\"}]>] out_shardings=[<@mesh, [{\"x\", \"y\"}]>] manual_axes={\"x\"} (%arg1: tensor<4xf32>) {\n+// CHECK-NEXT:    %3 = func.call @foo(%arg1) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>} : (tensor<4xf32>) -> tensor<4xf32>\n+// CHECK-NEXT:    sdy.return %3 : tensor<4xf32>\n+// CHECK-NEXT:  } : (tensor<8xf32>) -> tensor<8xf32>\n+// CHECK-NEXT:  return %2 : tensor<8xf32>\n func.func @named_computations_same_funcs_two_same_manual_axes_different_shardings_one_without_manual_axes(%arg0: tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\", \"y\"}]>}) -> (tensor<8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\", \"y\"}]>}) {\n   %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{\"x\", \"y\"}]>] out_shardings=[<@mesh, [{\"x\", \"y\"}]>] manual_axes={\"x\"} (%arg1: tensor<4xf32>) {\n     %1 = sdy.named_computation<\"foo\">(%arg1) in_shardings=[<@mesh, [{\"y\"}]>] out_shardings=[<@mesh, [{\"y\"}]>] (%arg2: tensor<4xf32>) {\n@@ -52,7 +89,14 @@ func.func @named_computations_same_funcs_two_same_manual_axes_different_sharding\n     %6 = stablehlo.abs %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>} : tensor<8xf32>\n     sdy.return %6 : tensor<8xf32>\n   } : (tensor<8xf32>) -> tensor<8xf32>\n-  return %5 : tensor<8xf32>\n+  %7 = sdy.manual_computation(%5) in_shardings=[<@mesh, [{\"x\", \"y\"}]>] out_shardings=[<@mesh, [{\"x\", \"y\"}]>] manual_axes={\"x\"} (%arg1: tensor<4xf32>) {\n+    %1 = sdy.named_computation<\"foo\">(%arg1) in_shardings=[<@mesh, [{\"y\"}]>] out_shardings=[<@mesh, [{\"y\"}]>] (%arg2: tensor<4xf32>) {\n+      %2 = stablehlo.abs %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}]>]>} : tensor<4xf32>\n+      sdy.return %2 : tensor<4xf32>\n+    } {xla.sdy.manual_axes = #sdy<manual_axes{\"x\"}>} : (tensor<4xf32>) -> tensor<4xf32>\n+    sdy.return %1 : tensor<4xf32>\n+  } : (tensor<8xf32>) -> tensor<8xf32>\n+  return %7 : tensor<8xf32>\n }\n \n // CHECK-LABEL: func private @foo("
        }
    ],
    "stats": {
        "total": 115,
        "additions": 104,
        "deletions": 11
    }
}