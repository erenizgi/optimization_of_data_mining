{
    "author": "dimitar-asenov",
    "message": "Reverts c28d80ae666abf6ac0f1ec9e72c9bfab4adee1b9\n\nPiperOrigin-RevId: 822586242",
    "sha": "bbea04967a1b5f435c632b62f9a16d6d754834f4",
    "files": [
        {
            "sha": "41ca82bbfb19e7ead45332e93e68ed269f36929f",
            "filename": "third_party/xla/xla/hlo/ir/hlo_computation.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bbea04967a1b5f435c632b62f9a16d6d754834f4/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bbea04967a1b5f435c632b62f9a16d6d754834f4/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.cc?ref=bbea04967a1b5f435c632b62f9a16d6d754834f4",
            "patch": "@@ -239,6 +239,28 @@ void HloComputation::ClearCalledComputations() {\n   CHECK(callee_computations_.empty());\n }\n \n+void HloComputation::SetInstruction(HloInstruction* instruction,\n+                                    InstructionType type) {\n+  static_assert(alignof(HloInstruction) == kInstructionTypeMask + 1,\n+                \"HloInstruction should be aligned as a QWORD\");\n+\n+  DCHECK(type != InstructionType::kUnset)\n+      << \"Set instruction must be called with a valid type, not kUnset.\";\n+  DCHECK(instruction_type() == InstructionType::kUnset ||\n+         instruction_type() == type)\n+      << \"Unexpected instruction type. Current type is \"\n+      << static_cast<int>(instruction_type()) << \" and it cannot be reset to \"\n+      << static_cast<int>(type);\n+\n+  // If `instruction` is nullptr, we need to preserve the existing type.\n+  if (instruction == nullptr) {\n+    type = instruction_type();\n+  }\n+\n+  instruction_and_type_ =\n+      reinterpret_cast<uintptr_t>(instruction) | static_cast<uintptr_t>(type);\n+}\n+\n HloInstruction* HloComputation::AddInstruction(\n     std::unique_ptr<HloInstruction> instruction, absl::string_view new_name) {\n   CHECK(instruction->opcode() != HloOpcode::kParameter)\n@@ -1422,6 +1444,10 @@ HloComputation::CreateFromProto(\n       new HloComputation(proto.name(), parameter_count, &instructions, root,\n                          /*preserve_instruction_ids=*/true));\n   computation->SetUniqueIdHelper(proto.id());\n+  if (proto.is_fusion_computation()) {\n+    computation->instruction_and_type_ =\n+        static_cast<uintptr_t>(InstructionType::kFusion);\n+  }\n   if (!proto.execution_thread().empty()) {\n     computation->SetExecutionThread(proto.execution_thread());\n   }"
        },
        {
            "sha": "7ce2e9d84981e3d849267ddcb051e90a5a9a4bce",
            "filename": "third_party/xla/xla/hlo/ir/hlo_computation.h",
            "status": "modified",
            "additions": 39,
            "deletions": 16,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bbea04967a1b5f435c632b62f9a16d6d754834f4/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bbea04967a1b5f435c632b62f9a16d6d754834f4/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_computation.h?ref=bbea04967a1b5f435c632b62f9a16d6d754834f4",
            "patch": "@@ -208,6 +208,30 @@ class HloComputation {\n \n   ~HloComputation();\n \n+  enum class InstructionType : uint8_t {\n+    kUnset,\n+    // This computation is a fusion computation. A fusion computation ordinarily\n+    // also has a non-null instruction. However, if a fusion instruction\n+    // is removed during compilation, the fusion computation becomes\n+    // unreachable, and its instruction is set to null. We still need to regard\n+    // such computations as fusion computations for HLO scheduling purposes.\n+    kFusion,\n+    // Last Value for range checking.\n+    kLast = kFusion,\n+  };\n+  static_assert(static_cast<int>(InstructionType::kUnset) == 0,\n+                \"kUnset must be 0.\");\n+\n+  InstructionType instruction_type() const {\n+    return static_cast<InstructionType>(instruction_and_type_ &\n+                                        kInstructionTypeMask);\n+  }\n+\n+  HloInstruction* instruction() const {\n+    DCHECK(instruction_type() <= InstructionType::kLast);\n+    return reinterpret_cast<HloInstruction*>(instruction_and_type_ &\n+                                             ~kInstructionTypeMask);\n+  }\n   // Add an instruction to the computation. The computation takes ownership of\n   // the instruction.\n   HloInstruction* AddInstruction(std::unique_ptr<HloInstruction> instruction,\n@@ -789,30 +813,23 @@ class HloComputation {\n   bool HasSideEffect() const;\n \n   // Returns if this computation is a fusion computation.\n+  // Do not use this method to determine if fusion_instruction_ != nullptr.\n+  // Instead, directly do: FusionInstruction() != nullptr\n   bool IsFusionComputation() const {\n-    // TODO(b/418034360): There should be at most one fusion instruction calling\n-    // a fusion computation. Assert this and fix all related tests.\n-    return !caller_instructions(HloOpcode::kFusion).empty();\n+    return instruction_type() == InstructionType::kFusion;\n   }\n \n   // Returns if this computation is the entry computation of the module.\n   bool IsEntryComputation() const;\n \n-  // Returns if this computation is dead. A computation is dead if it is not\n-  // the entry computation and it is not called by any other computation.\n-  bool IsDeadComputation() const {\n-    return !IsEntryComputation() && caller_computations().empty();\n-  }\n-\n   // Returns the owning fusion instruction, or nullptr if this is not a fusion\n-  // computation. Note that this is just one of the fusion instructions that\n-  // calls this computation, there may be more than one callers.\n-  //\n-  // TODO(b/418034360): There should be at most one fusion instruction calling\n-  // a fusion computation. Assert this and fix all related tests.\n+  // computation.\n   HloInstruction* FusionInstruction() const {\n-    auto callers = caller_instructions(HloOpcode::kFusion);\n-    return callers.empty() ? nullptr : callers.front();\n+    return instruction_type() == InstructionType::kFusion ? instruction()\n+                                                          : nullptr;\n+  }\n+  void SetFusionInstruction(HloInstruction* fusion_instruction) {\n+    SetInstruction(fusion_instruction, InstructionType::kFusion);\n   }\n \n   // Returns if this computation is an async computation.\n@@ -1005,6 +1022,8 @@ class HloComputation {\n   absl::Status RemoveInstructionImpl(HloInstruction* instruction,\n                                      bool ignore_safety_check);\n \n+  void SetInstruction(HloInstruction* instruction, InstructionType type);\n+\n   // Private, because only HloModule should be able to set the parent.\n   // We maintain the invariant that a computation has a parent() if and only if\n   // the computation has been added to a module. Accordingly, the only way to\n@@ -1039,6 +1058,10 @@ class HloComputation {\n   // Module containing this computation.\n   HloModule* parent_ = nullptr;\n \n+  // Contains HloInstruction* and its type.\n+  // The respective type in the least significant three bits.\n+  uintptr_t instruction_and_type_ = 0;\n+\n   // Contains an HloInstruction* or an absl::flat_hash_map<HloInstruction*,\n   // /*count=*/int> in the high bits and a CallersType in the least significant\n   // bit."
        },
        {
            "sha": "81214a8cccc0e6391857a980382eca15ee629f25",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instruction.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bbea04967a1b5f435c632b62f9a16d6d754834f4/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bbea04967a1b5f435c632b62f9a16d6d754834f4/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instruction.h?ref=bbea04967a1b5f435c632b62f9a16d6d754834f4",
            "patch": "@@ -222,7 +222,8 @@ static constexpr uintptr_t kInstructionTypeMask = 0b111;\n // HLO is pure (mostly).  It has no concept of mutable state.  Instead, data\n // values are produced by one HLO and flow into consumers across dependency\n // edges.\n-class HloInstruction {\n+// Alignment must be explicitly specified due to ARM 32 platforms.\n+class alignas(kInstructionTypeMask + 1) HloInstruction {\n  public:\n   // A fusion node computes the same value a call to its fusion computation\n   // would compute.  However, the choice of fusion kind dictates codegen"
        },
        {
            "sha": "8c9b3e9ab0b8282d53987cb1cf96a76ef82d6c93",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instructions.cc",
            "status": "modified",
            "additions": 34,
            "deletions": 1,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bbea04967a1b5f435c632b62f9a16d6d754834f4/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bbea04967a1b5f435c632b62f9a16d6d754834f4/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.cc?ref=bbea04967a1b5f435c632b62f9a16d6d754834f4",
            "patch": "@@ -1979,6 +1979,10 @@ HloCallableInstruction::CloneAndAppendInstructionIntoCalledComputation(\n     auto* new_computation = CHECK_NOTNULL(instruction_to_append->GetModule())\n                                 ->AddEmbeddedComputation(builder.Build());\n     AppendComputation(new_computation);\n+    if (opcode() == HloOpcode::kFusion) {\n+      new_computation->SetFusionInstruction(this);\n+    }\n+\n     clone = called_computation_root();\n   } else {\n     // When add_output is false, instruction_to_append is necessarily an\n@@ -2209,6 +2213,31 @@ HloFusionInstruction::HloFusionInstruction(\n     : HloCallableInstruction(HloOpcode::kFusion, shape, operands,\n                              fusion_computation, prefix),\n       fusion_kind_(fusion_kind) {\n+  fusion_computation->SetFusionInstruction(this);\n+}\n+\n+HloFusionInstruction::~HloFusionInstruction() {\n+  ClearFusionComputationInstruction();\n+}\n+\n+void HloFusionInstruction::ClearFusionComputationInstruction() {\n+  // Each fusion calls a single computation, but we use called_computations()\n+  // instead of fused_instructions_computation(), because the order in which\n+  // things get destructed can vary; the fusion computation's back-pointer may\n+  // already be null, which violates a check in\n+  // fused_instructions_computation.\n+  for (HloComputation* computation : called_computations()) {\n+    // Some passes that rewrite fusions may reassign a fusion computation to a\n+    // different fusion instruction as this instruction gets destructed.\n+    if (computation->FusionInstruction() == this) {\n+      computation->SetFusionInstruction(nullptr);\n+    }\n+  }\n+}\n+\n+void HloFusionInstruction::ClearCalledComputations() {\n+  ClearFusionComputationInstruction();\n+  HloInstruction::ClearCalledComputations();\n }\n \n HloInstruction*\n@@ -2464,7 +2493,11 @@ void HloFusionInstruction::MergeFusionInstructionIntoMultiOutput(\n \n HloComputation* HloFusionInstruction::fused_instructions_computation() const {\n   CHECK_EQ(called_computations().size(), 1);\n-  return called_computations().front();\n+  auto* fused_instructions_computation = called_computations().front();\n+  CHECK(fused_instructions_computation->IsFusionComputation())\n+      << \"Computation \" << fused_instructions_computation->name()\n+      << \" is not a fusion kind\";\n+  return fused_instructions_computation;\n }\n \n HloInstruction* HloFusionInstruction::fused_expression_root() const {"
        },
        {
            "sha": "9435d5e5869f0e25dcc652ee0a73288d5ecd5184",
            "filename": "third_party/xla/xla/hlo/ir/hlo_instructions.h",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bbea04967a1b5f435c632b62f9a16d6d754834f4/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bbea04967a1b5f435c632b62f9a16d6d754834f4/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_instructions.h?ref=bbea04967a1b5f435c632b62f9a16d6d754834f4",
            "patch": "@@ -1495,6 +1495,14 @@ class HloFusionInstruction : public HloCallableInstruction {\n                                 HloComputation* fusion_computation,\n                                 absl::string_view prefix = \"\");\n \n+  ~HloFusionInstruction() override;\n+\n+  void ClearCalledComputations() override;\n+\n+  // When a fusion instruction is being destructed, clear the back pointer of\n+  // its fusion computation, to avoid referencing freed memory.\n+  void ClearFusionComputationInstruction();\n+\n   // Clones the given instruction_to_append and inserts the clone into this\n   // callable instruction.\n   HloInstruction* CloneAndAppendInstructionIntoCalledComputation("
        },
        {
            "sha": "f0eb6dfc6a1ab195f7bd41a1f2d8897a277a82da",
            "filename": "third_party/xla/xla/hlo/ir/hlo_schedule.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bbea04967a1b5f435c632b62f9a16d6d754834f4/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_schedule.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bbea04967a1b5f435c632b62f9a16d6d754834f4/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_schedule.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_schedule.cc?ref=bbea04967a1b5f435c632b62f9a16d6d754834f4",
            "patch": "@@ -15,7 +15,6 @@ limitations under the License.\n \n #include \"xla/hlo/ir/hlo_schedule.h\"\n \n-#include <algorithm>\n #include <cstdint>\n #include <ostream>\n #include <queue>\n@@ -343,25 +342,7 @@ absl::Status HloSchedule::Verify() const {\n        sequence_num_by_execution_threads) {\n     std::vector<HloComputation*> nonfusion_computations =\n         module_->MakeNonfusionComputations({thread_name});\n-\n-    // TODO(dasenov): Replace with std::erase_if after XLA uses C++20.\n-    auto remove_it = std::remove_if(nonfusion_computations.begin(),\n-                                    nonfusion_computations.end(),\n-                                    [](const HloComputation* computation) {\n-                                      return computation->IsDeadComputation();\n-                                    });\n-    nonfusion_computations.erase(remove_it, nonfusion_computations.end());\n-\n-    // It's possible to have more sequences than non_fusion_computations.\n-    // This is because in some cases computations that have schedules are\n-    // actually dead. The important thing to check is that each live non-fusion\n-    // computation has a sequence.\n-    //\n-    // TODO(b/418034360): Consider strenghtening this check to equality. That\n-    // would require cleaning up dead computations and/or recomputing the\n-    // schedule in a number of tests. In its present state (using less or equal)\n-    // this check is subsumed by the next one.\n-    TF_RET_CHECK(nonfusion_computations.size() <= sequence_size)\n+    TF_RET_CHECK(nonfusion_computations.size() == sequence_size)\n         << \"For thread \" << thread_name << \", schedule has \" << sequence_size\n         << \" sequences, but module has \" << nonfusion_computations.size()\n         << \" non-fusion computations for thread \" << thread_name;"
        },
        {
            "sha": "e52c9dade29f6bf0fd98070a81686faafd8a8291",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/flatten_call_graph.cc",
            "status": "modified",
            "additions": 43,
            "deletions": 6,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/bbea04967a1b5f435c632b62f9a16d6d754834f4/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fflatten_call_graph.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/bbea04967a1b5f435c632b62f9a16d6d754834f4/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fflatten_call_graph.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fflatten_call_graph.cc?ref=bbea04967a1b5f435c632b62f9a16d6d754834f4",
            "patch": "@@ -88,21 +88,58 @@ absl::StatusOr<bool> FlattenNode(const CallGraphNode& node) {\n   return changed;\n }\n \n+// Annotates flatten computations with callee instruction types.\n+absl::Status AnnotateNode(const CallGraphNode& node) {\n+  for (auto& callsite : node.callsites()) {\n+    HloInstruction* instruction = callsite.instruction();\n+\n+    if (instruction->opcode() == HloOpcode::kFusion) {\n+      for (HloComputation* computation : instruction->called_computations()) {\n+        computation->SetFusionInstruction(instruction);\n+      }\n+    }\n+  }\n+\n+  // Correctly handle dead code: if a fusion computation is no longer used, it\n+  // should not have a fusion instruction set.\n+  if (node.callers().empty() &&\n+      node.computation()->FusionInstruction() != nullptr) {\n+    node.computation()->SetFusionInstruction(nullptr);\n+  }\n+\n+  return absl::OkStatus();\n+}\n+\n }  // namespace\n \n absl::StatusOr<bool> FlattenCallGraph::Run(\n     HloModule* module,\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n   XLA_VLOG_LINES(3, \"Before flatten call graph:\\n\" + module->ToString());\n \n-  // Flatten original call graph.\n-  std::unique_ptr<CallGraph> call_graph =\n-      CallGraph::Build(module, execution_threads);\n-  TF_ASSIGN_OR_RETURN(bool changed,\n-                      call_graph->VisitNodesWithReturn(FlattenNode));\n+  bool changed = false;\n+  {  // Flatten original call graph.\n+    std::unique_ptr<CallGraph> call_graph =\n+        CallGraph::Build(module, execution_threads);\n+    TF_ASSIGN_OR_RETURN(bool flattened,\n+                        call_graph->VisitNodesWithReturn(FlattenNode));\n+    changed |= flattened;\n+  }\n+\n+  if (!changed) {\n+    return false;\n+  }\n+\n+  // TODO(b/418034360): Remove this step once the fusion instruction is\n+  // automatically maintained.\n+  {  // Annotate flattened computations with callee types.\n+    std::unique_ptr<CallGraph> call_graph =\n+        CallGraph::Build(module, execution_threads);\n+    TF_RETURN_IF_ERROR(call_graph->VisitNodes(AnnotateNode));\n+  }\n \n   XLA_VLOG_LINES(3, \"After flatten call graph:\\n\" + module->ToString());\n-  return changed;\n+  return true;\n }\n \n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 197,
        "additions": 153,
        "deletions": 44
    }
}