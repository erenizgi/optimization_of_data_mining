{
    "author": "ScXfjiang",
    "message": "PR #30965: [ROCm] Enable custom all_reduce\n\nImported from GitHub PR https://github.com/openxla/xla/pull/30965\n\nIn this PR, we enable custom all_reduce (one-shot / two shot) for the ROCm platform.\n\nIn the kernel implementation, we need to introduce fine-grained memory to avoid the cache coherence issue, which happens in some AMD archs in the GPU synchronization.\n\nReference:\nhttps://rocm.docs.amd.com/en/docs-6.2.1/conceptual/gpu-memory.html?utm_source=chatgpt.com#coherence\nCopybara import of the project:\n\n--\n5d0307767c2a5517d2279b79fe5f392575cf3e90 by scxfjiang <xuefei.jiang@amd.com>:\n\nenable custom all_reduce for ROCm\n\n--\nd1289e23f7fa11c3eb6a34a07e853b5a23528bac by scxfjiang <xuefei.jiang@amd.com>:\n\nadd test to multi-gpu test suite\n\n--\nbc43160a8d9d45d5bd0d87193ed20feefffa4f65 by scxfjiang <xuefei.jiang@amd.com>:\n\nrename\n\n--\na712ef6c46040fec8abbeafea372bf8cd4b145dd by scxfjiang <xuefei.jiang@amd.com>:\n\nset default value and add comments\n\n--\ncce32d7308de9e70c94bfca37b7ac83e4d6d81f2 by scxfjiang <xuefei.jiang@amd.com>:\n\nmore comments\n\nMerging this change closes #30965\n\nPiperOrigin-RevId: 809927933",
    "sha": "04cf2c2a11939e5c91671a1a37b9f4f3040a3baa",
    "files": [
        {
            "sha": "20daaec57bc2ad5230444773016976a8bcaeaf73",
            "filename": "third_party/xla/build_tools/rocm/run_xla_multi_gpu.sh",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/04cf2c2a11939e5c91671a1a37b9f4f3040a3baa/third_party%2Fxla%2Fbuild_tools%2Frocm%2Frun_xla_multi_gpu.sh",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/04cf2c2a11939e5c91671a1a37b9f4f3040a3baa/third_party%2Fxla%2Fbuild_tools%2Frocm%2Frun_xla_multi_gpu.sh",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Frocm%2Frun_xla_multi_gpu.sh?ref=04cf2c2a11939e5c91671a1a37b9f4f3040a3baa",
            "patch": "@@ -96,4 +96,5 @@ bazel \\\n        //xla/tests:replicated_io_feed_test \\\n        //xla/tools/multihost_hlo_runner:functional_hlo_runner_test \\\n        //xla/pjrt/distributed:topology_util_test \\\n-       //xla/pjrt/distributed:client_server_test\n+       //xla/pjrt/distributed:client_server_test \\\n+       //xla/backends/gpu/runtime:all_reduce_test"
        },
        {
            "sha": "a286a2e9f9a710308a8dc706e341b75b2921ac60",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/04cf2c2a11939e5c91671a1a37b9f4f3040a3baa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/04cf2c2a11939e5c91671a1a37b9f4f3040a3baa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=04cf2c2a11939e5c91671a1a37b9f4f3040a3baa",
            "patch": "@@ -1985,6 +1985,8 @@ xla_test(\n     backend_tags = {\n         \"gpu\": [\n             \"multi_gpu_h100\",\n+            \"multi_gpu\",\n+            \"xla_amdgpu_any\",\n             \"no_oss\",\n         ],\n     },"
        },
        {
            "sha": "04c54fe55532a62e4b46675757109bb0c022970a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/04cf2c2a11939e5c91671a1a37b9f4f3040a3baa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/04cf2c2a11939e5c91671a1a37b9f4f3040a3baa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc?ref=04cf2c2a11939e5c91671a1a37b9f4f3040a3baa",
            "patch": "@@ -59,7 +59,10 @@ static constexpr int64_t kMaxTwoShotAllReduceSizeBytes =\n absl::StatusOr<se::DeviceMemoryHandle> AllocateMemory(\n     se::StreamExecutor* executor, int64_t size,\n     absl::string_view debug_buffer_name) {\n-  se::DeviceMemoryHandle local_buffer_alloc(executor, executor->Allocate(size));\n+  se::DeviceMemoryHandle local_buffer_alloc(\n+      executor,\n+      executor->Allocate(\n+          size, static_cast<int64_t>(stream_executor::MemoryType::kP2P)));\n   if (local_buffer_alloc.memory().is_null()) {\n     return absl::InternalError(absl::StrFormat(\n         \"Failed to allocate %s for all-reduce.\", debug_buffer_name));"
        },
        {
            "sha": "97e60484a7594e772e327a3c0cb135d5ee482185",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_driver_wrapper.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/04cf2c2a11939e5c91671a1a37b9f4f3040a3baa/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_driver_wrapper.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/04cf2c2a11939e5c91671a1a37b9f4f3040a3baa/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_driver_wrapper.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_driver_wrapper.h?ref=04cf2c2a11939e5c91671a1a37b9f4f3040a3baa",
            "patch": "@@ -132,6 +132,7 @@ namespace wrap {\n   __macro(hipLaunchKernel)                          \\\n   __macro(hipMalloc)                                \\\n   __macro(hipMallocManaged)                         \\\n+  __macro(hipExtMallocWithFlags)                    \\\n   __macro(hipMemGetAddressRange)                    \\\n   __macro(hipMemGetInfo)                            \\\n   __macro(hipMemcpyDtoD)                            \\"
        },
        {
            "sha": "64dfba21a1d5a18eb8c362e6722abe2195b733ec",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_executor.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 5,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/04cf2c2a11939e5c91671a1a37b9f4f3040a3baa/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/04cf2c2a11939e5c91671a1a37b9f4f3040a3baa/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.cc?ref=04cf2c2a11939e5c91671a1a37b9f4f3040a3baa",
            "patch": "@@ -413,14 +413,25 @@ bool GetDeviceProperties(hipDeviceProp_t* device_properties,\n }\n \n // Allocates memory on the GPU device.\n-void* DeviceAllocate(Context* context, uint64_t bytes) {\n+void* DeviceAllocate(Context* context, uint64_t bytes,\n+                     bool is_fine_grained = false) {\n   if (bytes == 0) {\n     return nullptr;\n   }\n \n   ScopedActivateContext activated(context);\n-  hipDeviceptr_t result = nullptr;\n-  hipError_t res = wrap::hipMalloc(&result, bytes);\n+  hipDeviceptr_t device_mem = nullptr;\n+  hipError_t res;\n+  if (is_fine_grained) {\n+    // Fine-grained memory, which has better coherence during the kernel\n+    // execution. This type of memory is only used in P2P communication to solve\n+    // the cache coherence issue for some archs (e.g., MI200); most of the time,\n+    // you don't have to use it.\n+    res = wrap::hipExtMallocWithFlags(&device_mem, bytes,\n+                                      hipDeviceMallocFinegrained);\n+  } else {\n+    res = wrap::hipMalloc(&device_mem, bytes);\n+  }\n   if (res != hipSuccess) {\n     // LOG(INFO) because this isn't always important to users (e.g. BFCAllocator\n     // implements a retry if the first allocation fails).\n@@ -429,7 +440,7 @@ void* DeviceAllocate(Context* context, uint64_t bytes) {\n               << \" bytes) from device: \" << ToString(res);\n     return nullptr;\n   }\n-  void* ptr = reinterpret_cast<void*>(result);\n+  void* ptr = reinterpret_cast<void*>(device_mem);\n   VLOG(2) << \"allocated \" << ptr << \" for device \" << context->device_ordinal()\n           << \" of \" << bytes << \" bytes\";\n   return ptr;\n@@ -733,7 +744,16 @@ DeviceMemoryBase RocmExecutor::Allocate(uint64_t size, int64_t memory_space) {\n   switch (static_cast<MemoryType>(memory_space)) {\n     case MemoryType::kCollective:\n     case MemoryType::kDevice:\n-      return DeviceMemoryBase(DeviceAllocate(rocm_context_, size), size);\n+      return DeviceMemoryBase(\n+          DeviceAllocate(rocm_context_, size, /*is_fine_grained*/ false), size);\n+    case MemoryType::kP2P:\n+      // On the ROCm platform, differences in cache design (e.g., coherence\n+      // protocol) can cause cache coherence issues for some archs (e.g., MI200)\n+      // when using normal device memory. To avoid these problems, we use\n+      // fine-grained memory in P2P communication for all archs to make sure of\n+      // the correctness.\n+      return DeviceMemoryBase(\n+          DeviceAllocate(rocm_context_, size, /*is_fine_grained*/ true), size);\n     case MemoryType::kHost:\n       if (auto result = HostAllocate(rocm_context_, size); result.ok()) {\n         return DeviceMemoryBase(*result, size);"
        },
        {
            "sha": "1795a2afd86ecd0ab8df3c6256db421aefabd06d",
            "filename": "third_party/xla/xla/stream_executor/stream_executor.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/04cf2c2a11939e5c91671a1a37b9f4f3040a3baa/third_party%2Fxla%2Fxla%2Fstream_executor%2Fstream_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/04cf2c2a11939e5c91671a1a37b9f4f3040a3baa/third_party%2Fxla%2Fxla%2Fstream_executor%2Fstream_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fstream_executor.h?ref=04cf2c2a11939e5c91671a1a37b9f4f3040a3baa",
            "patch": "@@ -54,7 +54,7 @@ limitations under the License.\n namespace stream_executor {\n \n // Identifies the memory space where an allocation resides.\n-enum class MemoryType { kDevice = 0, kUnified, kCollective, kHost = 5 };\n+enum class MemoryType { kDevice = 0, kUnified, kCollective, kP2P, kHost = 5 };\n \n /// The StreamExecutor is a single-device abstraction for:\n //"
        }
    ],
    "stats": {
        "total": 43,
        "additions": 35,
        "deletions": 8
    }
}