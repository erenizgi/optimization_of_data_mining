{
    "author": "terryysun",
    "message": "PR #32836: [GPU] Dispatch S-curve model to single-partition multi-host topology\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32836\n\nüìù Summary of Changes\nUpdated SINGLE_HOST communication type to SINGLE_PARTITION (fast-interconnect domain) to meet the need of multi-node NVLink (MNNVL) topology. Piped auto-detected partition size for communication type determination, also exposed partition size in SolGPUCostModel::Config for AOT compilation.\n\nüéØ Justification\nS-curve model cannot handle NVLink latency, single fast-interconnect domain including MNNVL topology should use latency table model. This PR updates the routing mechanism so that MNNVL will be treated as a single partition, while previously host is assumed equivalent to partition.\n\nüöÄ Kind of Contribution\n‚ú® New Feature\n\nüìä Benchmark (for Performance Improvements)\nN/A\n\nüß™ Unit Tests:\nAdded unit tests for model dispatching mechanism.\n\nüß™ Execution Tests:\nBehavior unchanged for non-MNNVL topology, N/A.\n\nCopybara import of the project:\n\n--\na9544375934873f7b888fdb5ff6c9dc6ee8b0e6c by Terry Sun <tesun@nvidia.com>:\n\nuse partition size for static model dispatching\n\n--\ne3445a5deb8da10146e90c50da5598f91cfe0a69 by Terry Sun <tesun@nvidia.com>:\n\nexpose partition size to config\n\n--\n212535ce891b8eb96ebb3c1e215a91d2b5035594 by Terry Sun <tesun@nvidia.com>:\n\nbetter modularity\n\n--\na9fe8a0f89dea9e2811d76a3570c7398df8dd756 by Terry Sun <tesun@nvidia.com>:\n\nbetter code structure and doc string\n\n--\na64a2b5ed1d45d815c6a2c47628b4d9ebb8368bd by Terry Sun <tesun@nvidia.com>:\n\nupdate naming\n\nMerging this change closes #32836\n\nPiperOrigin-RevId: 826697791",
    "sha": "81341174769fb5756592ed0c620db8d3b0561285",
    "files": [
        {
            "sha": "b2f55f29870fe584aec06ec3c4d42783844a8f99",
            "filename": "third_party/xla/xla/service/collective_utils.h",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_utils.h?ref=81341174769fb5756592ed0c620db8d3b0561285",
            "patch": "@@ -70,6 +70,11 @@ constexpr char kSolChunkSizeBytes[] = \"chunk_size_bytes\";\n // cost model.\n constexpr char kSolGpusPerNode[] = \"gpus_per_node\";\n \n+// Defines the partition size (number of devices per fast-interconnect domain)\n+// used by the SoL cost model. This is necessary for AOT compilation when the\n+// partition is larger than a node.\n+constexpr char kSolPartitionSize[] = \"partition_size\";\n+\n }  // namespace xla\n \n #endif  // XLA_SERVICE_COLLECTIVE_UTILS_H_"
        },
        {
            "sha": "0bd2b1ec293e92ec50229ddc03fa13aa92269de5",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=81341174769fb5756592ed0c620db8d3b0561285",
            "patch": "@@ -2012,6 +2012,11 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n absl::StatusOr<std::unique_ptr<HloModule>> GpuCompiler::RunHloPasses(\n     std::unique_ptr<HloModule> module, se::StreamExecutor* stream_exec,\n     const CompileOptions& options) {\n+  // TODO rename slice_size to partition_size in CompileOptions\n+  if (options.slice_size > 0) {\n+    module->mutable_config().set_partition_size(options.slice_size);\n+  }\n+\n   const DebugOptions debug_opts = module->config().debug_options();\n   TF_RETURN_IF_ERROR(LoadAutotuneResultsFromFile(debug_opts));\n   bool is_deviceless = options.target_config.has_value() ||"
        },
        {
            "sha": "f692cf4a1eb14713b2446581521703a78d9b3be3",
            "filename": "third_party/xla/xla/service/gpu/model/collective_interpolator_test.cc",
            "status": "modified",
            "additions": 46,
            "deletions": 44,
            "changes": 90,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcollective_interpolator_test.cc?ref=81341174769fb5756592ed0c620db8d3b0561285",
            "patch": "@@ -148,7 +148,7 @@ class CollectiveInterpolationTest : public TestWithParam<ParametrizedTestCase> {\n                                         int num_hosts) {\n     IotaReplicaGroupList iota(1, 1);\n     switch (comm) {\n-      case GPUCommunicationType::SINGLE_HOST:\n+      case GPUCommunicationType::SINGLE_PARTITION:\n         iota = IotaReplicaGroupList(num_hosts, kNumGpusPerHost);\n         break;\n       case GPUCommunicationType::MULTI_HOST_WORLD_LEVEL:\n@@ -225,28 +225,28 @@ class CollectiveInterpolationTest : public TestWithParam<ParametrizedTestCase> {\n       },\n       {\n           /*opcode=*/HloOpcode::kAllReduce,\n-          /*comm=*/GPUCommunicationType::SINGLE_HOST,\n+          /*comm=*/GPUCommunicationType::SINGLE_PARTITION,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/2048,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllReduce,\n-          /*comm=*/GPUCommunicationType::SINGLE_HOST,\n+          /*comm=*/GPUCommunicationType::SINGLE_PARTITION,\n           /*tensor_size=*/2 * 1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/2 * 2048,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllReduce,\n-          /*comm=*/GPUCommunicationType::SINGLE_HOST,\n+          /*comm=*/GPUCommunicationType::SINGLE_PARTITION,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/4,\n           /*network_througput_bytes=*/2048,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllReduce,\n-          /*comm=*/GPUCommunicationType::SINGLE_HOST,\n+          /*comm=*/GPUCommunicationType::SINGLE_PARTITION,\n           /*tensor_size=*/2 * 1024,\n           /*num_nodes=*/4,\n           /*network_througput_bytes=*/2 * 2048,\n@@ -309,28 +309,28 @@ class CollectiveInterpolationTest : public TestWithParam<ParametrizedTestCase> {\n       },\n       {\n           /*opcode=*/HloOpcode::kReduceScatter,\n-          /*comm=*/GPUCommunicationType::SINGLE_HOST,\n+          /*comm=*/GPUCommunicationType::SINGLE_PARTITION,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/2048,\n       },\n       {\n           /*opcode=*/HloOpcode::kReduceScatter,\n-          /*comm=*/GPUCommunicationType::SINGLE_HOST,\n+          /*comm=*/GPUCommunicationType::SINGLE_PARTITION,\n           /*tensor_size=*/2 * 1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/2 * 2048,\n       },\n       {\n           /*opcode=*/HloOpcode::kReduceScatter,\n-          /*comm=*/GPUCommunicationType::SINGLE_HOST,\n+          /*comm=*/GPUCommunicationType::SINGLE_PARTITION,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/4,\n           /*network_througput_bytes=*/2048,\n       },\n       {\n           /*opcode=*/HloOpcode::kReduceScatter,\n-          /*comm=*/GPUCommunicationType::SINGLE_HOST,\n+          /*comm=*/GPUCommunicationType::SINGLE_PARTITION,\n           /*tensor_size=*/2 * 1024,\n           /*num_nodes=*/4,\n           /*network_througput_bytes=*/2 * 2048,\n@@ -393,35 +393,35 @@ class CollectiveInterpolationTest : public TestWithParam<ParametrizedTestCase> {\n       },\n       {\n           /*opcode=*/HloOpcode::kAllGather,\n-          /*comm=*/GPUCommunicationType::SINGLE_HOST,\n+          /*comm=*/GPUCommunicationType::SINGLE_PARTITION,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/2048,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllGather,\n-          /*comm=*/GPUCommunicationType::SINGLE_HOST,\n+          /*comm=*/GPUCommunicationType::SINGLE_PARTITION,\n           /*tensor_size=*/2 * 1024,\n           /*num_nodes=*/2,\n           /*network_througput_bytes=*/2 * 2048,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllGather,\n-          /*comm=*/GPUCommunicationType::SINGLE_HOST,\n+          /*comm=*/GPUCommunicationType::SINGLE_PARTITION,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/4,\n           /*network_througput_bytes=*/2048,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllGather,\n-          /*comm=*/GPUCommunicationType::SINGLE_HOST,\n+          /*comm=*/GPUCommunicationType::SINGLE_PARTITION,\n           /*tensor_size=*/2 * 1024,\n           /*num_nodes=*/4,\n           /*network_througput_bytes=*/2 * 2048,\n       },\n       {\n           /*opcode=*/HloOpcode::kAllToAll,\n-          /*comm=*/GPUCommunicationType::SINGLE_HOST,\n+          /*comm=*/GPUCommunicationType::SINGLE_PARTITION,\n           /*tensor_size=*/1024,\n           /*num_nodes=*/1,\n           /*network_througput_bytes=*/1024,\n@@ -574,60 +574,61 @@ INSTANTIATE_TEST_SUITE_P(\n             /*expected_duration=*/absl::Milliseconds(2500),\n         },\n         {\n-            /*test_name=*/\"AR_single_host_aligned_extrapolate_nodes\",\n+            /*test_name=*/\"AR_SINGLE_PARTITION_aligned_extrapolate_nodes\",\n             /*spec=*/\n             {\n                 /*opcode=*/HloOpcode::kAllReduce,\n                 /*comm=*/\n-                GPUCommunicationType::SINGLE_HOST,\n+                GPUCommunicationType::SINGLE_PARTITION,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/8,\n             },\n             /*expected_duration=*/absl::Milliseconds(500),\n         },\n         {\n-            /*test_name=*/\"AR_single_host_aligned_extrapolate_tensor_size\",\n+            /*test_name=*/\"AR_SINGLE_PARTITION_aligned_extrapolate_tensor_size\",\n             /*spec=*/\n             {\n                 /*opcode=*/HloOpcode::kAllReduce,\n                 /*comm=*/\n-                GPUCommunicationType::SINGLE_HOST,\n+                GPUCommunicationType::SINGLE_PARTITION,\n                 /*tensor_size=*/4 * 1024,\n                 /*num_nodes=*/2,\n             },\n             /*expected_duration=*/absl::Seconds(1),\n         },\n         {\n-            /*test_name=*/\"AR_single_host_aligned_interpolate_nodes\",\n+            /*test_name=*/\"AR_SINGLE_PARTITION_aligned_interpolate_nodes\",\n             /*spec=*/\n             {\n                 /*opcode=*/HloOpcode::kAllReduce,\n                 /*comm=*/\n-                GPUCommunicationType::SINGLE_HOST,\n+                GPUCommunicationType::SINGLE_PARTITION,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/3,\n             },\n             /*expected_duration=*/absl::Milliseconds(500),\n         },\n         {\n-            /*test_name=*/\"AR_single_host_aligned_interpolate_tensor_size\",\n+            /*test_name=*/\"AR_SINGLE_PARTITION_aligned_interpolate_tensor_size\",\n             /*spec=*/\n             {\n                 /*opcode=*/HloOpcode::kAllReduce,\n                 /*comm=*/\n-                GPUCommunicationType::SINGLE_HOST,\n+                GPUCommunicationType::SINGLE_PARTITION,\n                 /*tensor_size=*/1024 + 256,\n                 /*num_nodes=*/2,\n             },\n             /*expected_duration=*/absl::Milliseconds(625),\n         },\n         {\n-            /*test_name=*/\"ARS_single_host_aligned_interpolate_tensor_size\",\n+            /*test_name=*/\"ARS_SINGLE_PARTITION_aligned_interpolate_tensor_\"\n+                          \"size\",\n             /*spec=*/\n             {\n                 /*opcode=*/HloOpcode::kAllReduceStart,\n                 /*comm=*/\n-                GPUCommunicationType::SINGLE_HOST,\n+                GPUCommunicationType::SINGLE_PARTITION,\n                 /*tensor_size=*/1024 + 256,\n                 /*num_nodes=*/2,\n             },\n@@ -754,48 +755,48 @@ INSTANTIATE_TEST_SUITE_P(\n             /*expected_duration=*/absl::Milliseconds(2500),\n         },\n         {\n-            /*test_name=*/\"RS_single_host_aligned_extrapolate_nodes\",\n+            /*test_name=*/\"RS_SINGLE_PARTITION_aligned_extrapolate_nodes\",\n             /*spec=*/\n             {\n                 /*opcode=*/HloOpcode::kReduceScatter,\n                 /*comm=*/\n-                GPUCommunicationType::SINGLE_HOST,\n+                GPUCommunicationType::SINGLE_PARTITION,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/8,\n             },\n             /*expected_duration=*/absl::Milliseconds(500),\n         },\n         {\n-            /*test_name=*/\"RS_single_host_aligned_extrapolate_tensor_size\",\n+            /*test_name=*/\"RS_SINGLE_PARTITION_aligned_extrapolate_tensor_size\",\n             /*spec=*/\n             {\n                 /*opcode=*/HloOpcode::kReduceScatter,\n                 /*comm=*/\n-                GPUCommunicationType::SINGLE_HOST,\n+                GPUCommunicationType::SINGLE_PARTITION,\n                 /*tensor_size=*/4 * 1024,\n                 /*num_nodes=*/2,\n             },\n             /*expected_duration=*/absl::Seconds(1),\n         },\n         {\n-            /*test_name=*/\"RS_single_host_aligned_interpolate_nodes\",\n+            /*test_name=*/\"RS_SINGLE_PARTITION_aligned_interpolate_nodes\",\n             /*spec=*/\n             {\n                 /*opcode=*/HloOpcode::kReduceScatter,\n                 /*comm=*/\n-                GPUCommunicationType::SINGLE_HOST,\n+                GPUCommunicationType::SINGLE_PARTITION,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/3,\n             },\n             /*expected_duration=*/absl::Milliseconds(500),\n         },\n         {\n-            /*test_name=*/\"RS_single_host_aligned_interpolate_tensor_size\",\n+            /*test_name=*/\"RS_SINGLE_PARTITION_aligned_interpolate_tensor_size\",\n             /*spec=*/\n             {\n                 /*opcode=*/HloOpcode::kReduceScatter,\n                 /*comm=*/\n-                GPUCommunicationType::SINGLE_HOST,\n+                GPUCommunicationType::SINGLE_PARTITION,\n                 /*tensor_size=*/1024 + 256,\n                 /*num_nodes=*/2,\n             },\n@@ -922,60 +923,61 @@ INSTANTIATE_TEST_SUITE_P(\n             /*expected_duration=*/absl::Milliseconds(2500),\n         },\n         {\n-            /*test_name=*/\"AG_single_host_aligned_extrapolate_nodes\",\n+            /*test_name=*/\"AG_SINGLE_PARTITION_aligned_extrapolate_nodes\",\n             /*spec=*/\n             {\n                 /*opcode=*/HloOpcode::kAllGather,\n                 /*comm=*/\n-                GPUCommunicationType::SINGLE_HOST,\n+                GPUCommunicationType::SINGLE_PARTITION,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/8,\n             },\n             /*expected_duration=*/absl::Milliseconds(500),\n         },\n         {\n-            /*test_name=*/\"AG_single_host_aligned_extrapolate_tensor_size\",\n+            /*test_name=*/\"AG_SINGLE_PARTITION_aligned_extrapolate_tensor_size\",\n             /*spec=*/\n             {\n                 /*opcode=*/HloOpcode::kAllGather,\n                 /*comm=*/\n-                GPUCommunicationType::SINGLE_HOST,\n+                GPUCommunicationType::SINGLE_PARTITION,\n                 /*tensor_size=*/4 * 1024,\n                 /*num_nodes=*/2,\n             },\n             /*expected_duration=*/absl::Seconds(1),\n         },\n         {\n-            /*test_name=*/\"AG_single_host_aligned_interpolate_nodes\",\n+            /*test_name=*/\"AG_SINGLE_PARTITION_aligned_interpolate_nodes\",\n             /*spec=*/\n             {\n                 /*opcode=*/HloOpcode::kAllGather,\n                 /*comm=*/\n-                GPUCommunicationType::SINGLE_HOST,\n+                GPUCommunicationType::SINGLE_PARTITION,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/3,\n             },\n             /*expected_duration=*/absl::Milliseconds(500),\n         },\n         {\n-            /*test_name=*/\"AG_single_host_aligned_interpolate_tensor_size\",\n+            /*test_name=*/\"AG_SINGLE_PARTITION_aligned_interpolate_tensor_size\",\n             /*spec=*/\n             {\n                 /*opcode=*/HloOpcode::kAllGather,\n                 /*comm=*/\n-                GPUCommunicationType::SINGLE_HOST,\n+                GPUCommunicationType::SINGLE_PARTITION,\n                 /*tensor_size=*/1024 + 256,\n                 /*num_nodes=*/2,\n             },\n             /*expected_duration=*/absl::Milliseconds(625),\n         },\n         {\n-            /*test_name=*/\"AGS_single_host_aligned_interpolate_tensor_size\",\n+            /*test_name=*/\"AGS_SINGLE_PARTITION_aligned_interpolate_tensor_\"\n+                          \"size\",\n             /*spec=*/\n             {\n                 /*opcode=*/HloOpcode::kAllGatherStart,\n                 /*comm=*/\n-                GPUCommunicationType::SINGLE_HOST,\n+                GPUCommunicationType::SINGLE_PARTITION,\n                 /*tensor_size=*/1024 + 256,\n                 /*num_nodes=*/2,\n             },\n@@ -1004,11 +1006,11 @@ INSTANTIATE_TEST_SUITE_P(\n             /*expected_duration=*/absl::Milliseconds(250),\n         },\n         {\n-            /*test_name=*/\"A2A_single_host_exact_match\",\n+            /*test_name=*/\"A2A_SINGLE_PARTITION_exact_match\",\n             {\n                 /*opcode=*/HloOpcode::kAllToAll,\n                 /*comm=*/\n-                GPUCommunicationType::SINGLE_HOST,\n+                GPUCommunicationType::SINGLE_PARTITION,\n                 /*tensor_size=*/1024,\n                 /*num_nodes=*/1,\n             },"
        },
        {
            "sha": "05417dbb997dc37971744d5ddc3c8bafd55fa00f",
            "filename": "third_party/xla/xla/service/gpu/model/sol_gpu_cost_model.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model.cc?ref=81341174769fb5756592ed0c620db8d3b0561285",
            "patch": "@@ -138,6 +138,9 @@ SolGPUCostModel::Config GetPlatformConfig(\n     } else if (option_name == kSolChunkSizeBytes &&\n                absl::SimpleAtoi(option_value, &value) && value > 0) {\n       config.chunk_size_bytes = value;\n+    } else if (option_name == kSolPartitionSize &&\n+               absl::SimpleAtoi(option_value, &value) && value > 0) {\n+      config.partition_size = value;\n     }\n   }\n   return config;"
        },
        {
            "sha": "0634f118c528cd22622a28fe2ada4780c6239ddd",
            "filename": "third_party/xla/xla/service/gpu/model/sol_gpu_cost_model.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model.h?ref=81341174769fb5756592ed0c620db8d3b0561285",
            "patch": "@@ -40,6 +40,8 @@ class SolGPUCostModel {\n     absl::Duration rtt;\n     int64_t gpus_per_node;\n     int64_t chunk_size_bytes;\n+    // Partition size (devices per fast-interconnect domain). 0 means unset.\n+    int64_t partition_size;\n   };\n \n   enum CollectiveAlgorithmType {"
        },
        {
            "sha": "923d9190b149ff4390cde34bbfd25ec549921d0a",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 4,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator.cc?ref=81341174769fb5756592ed0c620db8d3b0561285",
            "patch": "@@ -189,6 +189,17 @@ absl::StatusOr<absl::Duration> DCNCollectiveDuration(\n   return result;\n }\n \n+int64_t GetPartitionSize(const HloInstruction& instr,\n+                         const SolGPUCostModel::Config& sol_flags) {\n+  if (sol_flags.partition_size > 0) {\n+    return sol_flags.partition_size;\n+  }\n+  if (instr.GetModule()->config().partition_size() > 0) {\n+    return instr.GetModule()->config().partition_size();\n+  }\n+  return sol_flags.gpus_per_node;\n+}\n+\n absl::StatusOr<absl::Duration> DispatchEstimation(\n     const absl::StatusOr<GPUCommunicationType>& communication_type,\n     const HloCollectiveInstruction& instr,\n@@ -202,11 +213,12 @@ absl::StatusOr<absl::Duration> DispatchEstimation(\n   GPUCommunicationType comm = *communication_type;\n   TF_ASSIGN_OR_RETURN(auto num_groups_and_devices,\n                       GetReplicaGroupCountAndSize(&instr));\n+  int64_t partition_size = GetPartitionSize(instr, sol_flags);\n \n   switch (comm) {\n     case GPUCommunicationType::MULTI_HOST_WORLD_LEVEL: {\n       return DCNCollectiveDuration(\n-          num_groups_and_devices->second / sol_flags.gpus_per_node,\n+          num_groups_and_devices->second / partition_size,\n           /*num_communicators=*/num_groups_and_devices->first, instr,\n           gpu_device_info, sol_flags, analysis, symbolic_expr_context);\n     }\n@@ -216,10 +228,11 @@ absl::StatusOr<absl::Duration> DispatchEstimation(\n           /*num_communicators=*/num_groups_and_devices->first, instr,\n           gpu_device_info, sol_flags, analysis, symbolic_expr_context);\n     }\n-    case GPUCommunicationType::SINGLE_HOST: {\n+    case GPUCommunicationType::SINGLE_PARTITION: {\n       if (collective_interpolator == nullptr) {\n         return absl::InvalidArgumentError(\n-            \"Collective interpolator is required for single host collectives\");\n+            \"Collective interpolator is required for single partition \"\n+            \"collectives\");\n       }\n       return collective_interpolator->EstimatedRuntime(instr);\n     }\n@@ -309,9 +322,10 @@ SolLatencyEstimator::ComputeCollectiveTime(\n         absl::StrCat(\"Unsupported collective instruction: \", instr.ToString()));\n   }\n \n+  int64_t partition_size = GetPartitionSize(*collective_instr, sol_flags);\n   TF_ASSIGN_OR_RETURN(\n       GPUCommunicationType communication_type,\n-      CommunicationType(sol_flags.gpus_per_node, *collective_instr,\n+      CommunicationType(partition_size, *collective_instr,\n                         gpu_device_info.gpu_compute_capability()));\n   TF_ASSIGN_OR_RETURN(\n       absl::Duration result,"
        },
        {
            "sha": "f7dcb15a3778732e1d6a3afed970b5055a5bb37a",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_backend_assigner.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_backend_assigner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_backend_assigner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_backend_assigner.cc?ref=81341174769fb5756592ed0c620db8d3b0561285",
            "patch": "@@ -99,7 +99,7 @@ absl::StatusOr<bool> CollectiveBackendAssigner::Run(\n               << \" slice_size_=\" << slice_size_;\n       bool use_nvshmem =\n           (num_visible_devices_per_process_ == 1 ||\n-           comm_type == GPUCommunicationType::SINGLE_HOST ||\n+           comm_type == GPUCommunicationType::SINGLE_PARTITION ||\n            (slice_size_ > 0 &&\n             IsIntraNVLinkDomain(module->config(), slice_size_))) &&\n           (!IsAllReduceOp(instr) || shape_size < threshold_in_bytes_);"
        },
        {
            "sha": "ad381f35c709a1c3b2e7f7bb1f4827374ca81f5d",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 38,
            "changes": 78,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc?ref=81341174769fb5756592ed0c620db8d3b0561285",
            "patch": "@@ -44,26 +44,26 @@ namespace xla {\n namespace gpu {\n namespace {\n \n-// Computes a map from source node ID to a set of target node IDs for a\n-// collective-permute instruction. A node ID is computed by dividing the device\n-// (replica) ID by the number of devices per host.\n+// Computes a map from source partition ID to a set of target partition IDs for\n+// a collective-permute instruction. A partition ID is computed by dividing the\n+// device (replica) ID by the number of devices per host.\n absl::flat_hash_map<int64_t, absl::flat_hash_set<int64_t>>\n GetSourceToTargetsNodeMap(const HloCollectivePermuteInstruction& instr,\n-                          int num_devices_per_host) {\n+                          int num_devices_per_partition) {\n   absl::flat_hash_map<int64_t, absl::flat_hash_set<int64_t>>\n-      source_to_targets_node_map;\n+      source_to_targets_partition_map;\n   for (const auto& [source, target] : instr.source_target_pairs()) {\n-    int64_t source_node = source / num_devices_per_host;\n-    int64_t target_node = target / num_devices_per_host;\n-    source_to_targets_node_map[source_node].insert(target_node);\n+    int64_t source_partition = source / num_devices_per_partition;\n+    int64_t target_partition = target / num_devices_per_partition;\n+    source_to_targets_partition_map[source_partition].insert(target_partition);\n   }\n-  return source_to_targets_node_map;\n+  return source_to_targets_partition_map;\n }\n \n struct CollectiveMetadata {\n   // map for ops with `replica_groups`, e.g. all-gather.\n-  absl::flat_hash_map<int64_t, size_t> node_to_participant_count;\n-  int num_devices_per_host;\n+  absl::flat_hash_map<int64_t, size_t> partition_to_participant_count;\n+  int num_devices_per_partition;\n   int64_t replica_count;\n };\n \n@@ -85,46 +85,48 @@ bool SameParticipantCounts(const absl::flat_hash_map<int64_t, size_t>& lhs,\n }\n \n absl::StatusOr<CollectiveMetadata> CommunicationContext(\n-    const HloCollectiveInstruction& instr, int num_devices_per_host) {\n-  absl::flat_hash_map<int64_t, size_t> node_to_participant_count;\n+    const HloCollectiveInstruction& instr, int num_devices_per_partition) {\n+  absl::flat_hash_map<int64_t, size_t> partition_to_participant_count;\n \n   for (const ReplicaGroup& replica_group :\n        instr.device_list().replica_groups()) {\n     absl::flat_hash_map<int64_t, size_t> buffer;\n     for (int64_t rank : replica_group.replica_ids()) {\n-      int64_t node_id = rank / num_devices_per_host;\n-      buffer[node_id]++;\n+      int64_t partition_id = rank / num_devices_per_partition;\n+      buffer[partition_id]++;\n     }\n-    if (!node_to_participant_count.empty() &&\n-        !SameParticipantCounts(buffer, node_to_participant_count)) {\n+    if (!partition_to_participant_count.empty() &&\n+        !SameParticipantCounts(buffer, partition_to_participant_count)) {\n       return absl::FailedPreconditionError(absl::StrCat(\n           \"Non homogenous replica group: \", instr.device_list().ToString()));\n     }\n-    if (node_to_participant_count.empty()) {\n-      node_to_participant_count = buffer;\n+    if (partition_to_participant_count.empty()) {\n+      partition_to_participant_count = buffer;\n     }\n   }\n-  return CollectiveMetadata{node_to_participant_count, num_devices_per_host,\n+  return CollectiveMetadata{partition_to_participant_count,\n+                            num_devices_per_partition,\n                             instr.GetModule()->config().replica_count()};\n }\n \n bool IsSingleHost(const CollectiveMetadata& pattern) {\n-  if (pattern.node_to_participant_count.size() == 1) {\n+  if (pattern.partition_to_participant_count.size() == 1) {\n     return true;\n   }\n   return pattern.replica_count > 0 &&\n-         pattern.node_to_participant_count.empty() &&\n-         pattern.replica_count <= pattern.num_devices_per_host;\n+         pattern.partition_to_participant_count.empty() &&\n+         pattern.replica_count <= pattern.num_devices_per_partition;\n }\n \n bool IsWorldLevelCommunication(const CollectiveMetadata& pattern) {\n-  if (!IsSingleHost(pattern) && pattern.node_to_participant_count.empty()) {\n+  if (!IsSingleHost(pattern) &&\n+      pattern.partition_to_participant_count.empty()) {\n     return true;\n   }\n   return absl::c_all_of(\n-      pattern.node_to_participant_count, [&pattern](const auto& elem) {\n-        const auto& [node_id, participant_count] = elem;\n-        return participant_count == pattern.num_devices_per_host;\n+      pattern.partition_to_participant_count, [&pattern](const auto& elem) {\n+        const auto& [partition_id, participant_count] = elem;\n+        return participant_count == pattern.num_devices_per_partition;\n       });\n }\n \n@@ -143,7 +145,7 @@ bool IsGPUSyncCollective(const HloInstruction& instr) {\n }\n \n absl::StatusOr<GPUCommunicationType> CommunicationType(\n-    int num_devices_per_host, const HloChannelInstruction& instr,\n+    int num_devices_per_partition, const HloChannelInstruction& instr,\n     const se::GpuComputeCapability& gpu_version) {\n   if (!gpu_version.IsCuda()) {\n     return absl::FailedPreconditionError(\"Only CUDA is supported.\");\n@@ -152,9 +154,9 @@ absl::StatusOr<GPUCommunicationType> CommunicationType(\n   if (const auto* collective = DynCast<HloCollectiveInstruction>(&instr)) {\n     TF_ASSIGN_OR_RETURN(\n         CollectiveMetadata comm,\n-        CommunicationContext(*collective, num_devices_per_host));\n+        CommunicationContext(*collective, num_devices_per_partition));\n     if (IsSingleHost(comm)) {\n-      return GPUCommunicationType::SINGLE_HOST;\n+      return GPUCommunicationType::SINGLE_PARTITION;\n     }\n     if (IsWorldLevelCommunication(comm)) {\n       return GPUCommunicationType::MULTI_HOST_WORLD_LEVEL;\n@@ -164,19 +166,19 @@ absl::StatusOr<GPUCommunicationType> CommunicationType(\n     }\n   } else if (const auto* collective_permute =\n                  DynCast<HloCollectivePermuteInstruction>(&instr)) {\n-    const auto source_to_targets_node_map =\n-        GetSourceToTargetsNodeMap(*collective_permute, num_devices_per_host);\n-    for (const auto& [source_node, target_node_set] :\n-         source_to_targets_node_map) {\n-      if (target_node_set.size() > 1) {\n+    const auto source_to_targets_partition_map = GetSourceToTargetsNodeMap(\n+        *collective_permute, num_devices_per_partition);\n+    for (const auto& [source_partition, target_partition_set] :\n+         source_to_targets_partition_map) {\n+      if (target_partition_set.size() > 1) {\n         return GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL;\n       }\n-      CHECK_EQ(target_node_set.size(), 1);\n-      if (source_node != *target_node_set.begin()) {\n+      CHECK_EQ(target_partition_set.size(), 1);\n+      if (source_partition != *target_partition_set.begin()) {\n         return GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL;\n       }\n     }\n-    return GPUCommunicationType::SINGLE_HOST;\n+    return GPUCommunicationType::SINGLE_PARTITION;\n   } else {\n     return absl::FailedPreconditionError(\n         \"Cannot determine communication type for non-collective channel \""
        },
        {
            "sha": "e3b7491a153726ec9df1f1f292c74a8efb0c5d52",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h?ref=81341174769fb5756592ed0c620db8d3b0561285",
            "patch": "@@ -35,13 +35,13 @@ enum class GPUCommunicationType {\n   // the involved hosts has only a subset of its devices participating.\n   MULTI_HOST_NON_WORLD_LEVEL = 2,\n   // All devices participating in the collective operation reside on the same\n-  // host machine.\n-  SINGLE_HOST = 3\n+  // fast-interconnect domain.\n+  SINGLE_PARTITION = 3\n };\n \n // Returns the type of communication pattern for a channel instruction.\n absl::StatusOr<GPUCommunicationType> CommunicationType(\n-    int num_devices_per_host, const HloChannelInstruction& instr,\n+    int partition_size, const HloChannelInstruction& instr,\n     const se::GpuComputeCapability& gpu_version);\n \n // Returns true if instruction is a synchronous collective op."
        },
        {
            "sha": "8dfaf8d7849218899384fe7c865de854f34e652c",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils_test.cc",
            "status": "modified",
            "additions": 132,
            "deletions": 7,
            "changes": 139,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc?ref=81341174769fb5756592ed0c620db8d3b0561285",
            "patch": "@@ -62,7 +62,7 @@ TEST_F(CommunicationTypeTest, DetectsSingleHost8Devices) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n+              IsOkAndHolds(GPUCommunicationType::SINGLE_PARTITION));\n }\n \n TEST_F(CommunicationTypeTest, DetectsSingleHost4Devices) {\n@@ -85,7 +85,7 @@ TEST_F(CommunicationTypeTest, DetectsSingleHost4Devices) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n+              IsOkAndHolds(GPUCommunicationType::SINGLE_PARTITION));\n }\n \n TEST_F(CommunicationTypeTest, DetectsSingleHost16Devices) {\n@@ -106,9 +106,9 @@ TEST_F(CommunicationTypeTest, DetectsSingleHost16Devices) {\n \n   HloCollectiveInstruction* instr = Cast<HloCollectiveInstruction>(\n       module->entry_computation()->root_instruction());\n-  EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n+  EXPECT_THAT(CommunicationType(/*partition_size=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n+              IsOkAndHolds(GPUCommunicationType::SINGLE_PARTITION));\n }\n \n TEST_F(CommunicationTypeTest, DetectWorldLevelAllDevices) {\n@@ -201,7 +201,7 @@ TEST_F(CommunicationTypeTest, DetectsSingleHost16DevicesForEmptyReplicaGroups) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/16, *instr,\n                                 device_info().gpu_compute_capability()),\n-              IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n+              IsOkAndHolds(GPUCommunicationType::SINGLE_PARTITION));\n }\n \n TEST_F(CommunicationTypeTest, DetectWorldLevel8DevicesForEmptyReplicaGroups) {\n@@ -263,7 +263,7 @@ TEST_F(CommunicationTypeTest, DetectsSingleHostCollectivePermute) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n+              IsOkAndHolds(GPUCommunicationType::SINGLE_PARTITION));\n }\n \n TEST_F(CommunicationTypeTest, DetectsSingleHostCollectivePermuteSinglePair) {\n@@ -283,7 +283,7 @@ TEST_F(CommunicationTypeTest, DetectsSingleHostCollectivePermuteSinglePair) {\n       module->entry_computation()->root_instruction());\n   EXPECT_THAT(CommunicationType(/*num_devices_per_host=*/8, *instr,\n                                 device_info().gpu_compute_capability()),\n-              IsOkAndHolds(GPUCommunicationType::SINGLE_HOST));\n+              IsOkAndHolds(GPUCommunicationType::SINGLE_PARTITION));\n }\n \n TEST_F(CommunicationTypeTest, DetectNonWorldLevelCollectivePermute) {\n@@ -355,5 +355,130 @@ TEST_F(CommunicationTypeTest, DetectsCrossHostCollectivePermuteMixed) {\n               IsOkAndHolds(GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL));\n }\n \n+TEST_F(CommunicationTypeTest, DetectsSinglePartitionMultiHost) {\n+  // 16 devices across 2 hosts with partition_size=16 (single partition spanning\n+  // 2 hosts)\n+  absl::string_view kHlo = R\"(\n+    HloModule m, num_partitions=16\n+\n+    ENTRY e {\n+      p = f32[128] parameter(0)\n+      ROOT _ = f32[2048] all-gather(p),\n+        dimensions={0},\n+        use_global_device_ids=true,\n+        channel_id=1,\n+        replica_groups=[1,16]<=[16]\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(kHlo));\n+\n+  HloCollectiveInstruction* instr = Cast<HloCollectiveInstruction>(\n+      module->entry_computation()->root_instruction());\n+  EXPECT_THAT(CommunicationType(/*partition_size=*/16, *instr,\n+                                device_info().gpu_compute_capability()),\n+              IsOkAndHolds(GPUCommunicationType::SINGLE_PARTITION));\n+}\n+\n+TEST_F(CommunicationTypeTest, DetectsMultiPartitionWith8DevicePartitions) {\n+  // 64 devices across 2 partitions with partition_size=32\n+  absl::string_view kHlo = R\"(\n+    HloModule m, num_partitions=16\n+\n+    ENTRY e {\n+      p = f32[128] parameter(0)\n+      ROOT _ = f32[2048] all-gather(p),\n+        dimensions={0},\n+        use_global_device_ids=true,\n+        channel_id=1,\n+        replica_groups=[1, 64]<=[64]\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(kHlo));\n+\n+  HloCollectiveInstruction* instr = Cast<HloCollectiveInstruction>(\n+      module->entry_computation()->root_instruction());\n+  EXPECT_THAT(CommunicationType(/*partition_size=*/32, *instr,\n+                                device_info().gpu_compute_capability()),\n+              IsOkAndHolds(GPUCommunicationType::MULTI_HOST_WORLD_LEVEL));\n+}\n+\n+TEST_F(CommunicationTypeTest, DetectsMultiPartitionNonRailAligned) {\n+  // 64 devices with partition_size=36: partition 0 has 36 devices, partition 1\n+  // has 28 devices\n+  absl::string_view kHlo = R\"(\n+    HloModule m, num_partitions=12\n+\n+    ENTRY e {\n+      p = f32[128] parameter(0)\n+      ROOT _ = f32[1536] all-gather(p),\n+        dimensions={0},\n+        use_global_device_ids=true,\n+        channel_id=1,\n+        replica_groups=[1, 64]<=[64]\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(kHlo));\n+\n+  HloCollectiveInstruction* instr = Cast<HloCollectiveInstruction>(\n+      module->entry_computation()->root_instruction());\n+  // With partition_size=8, spans 2 partitions but not rail-aligned (8 and 4\n+  // devices)\n+  EXPECT_THAT(CommunicationType(/*partition_size=*/36, *instr,\n+                                device_info().gpu_compute_capability()),\n+              IsOkAndHolds(GPUCommunicationType::MULTI_HOST_NON_WORLD_LEVEL));\n+}\n+\n+TEST_F(CommunicationTypeTest, DetectsSinglePartitionSubset) {\n+  // 6 devices within a single partition (partition_size=36)\n+  absl::string_view kHlo = R\"(\n+    HloModule m, num_partitions=4\n+\n+    ENTRY e {\n+      p = f32[128] parameter(0)\n+      ROOT _ = f32[512] all-gather(p),\n+        dimensions={0},\n+        use_global_device_ids=true,\n+        channel_id=1,\n+        replica_groups={{0,1,2,3,4,5}}\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(kHlo));\n+\n+  HloCollectiveInstruction* instr = Cast<HloCollectiveInstruction>(\n+      module->entry_computation()->root_instruction());\n+  EXPECT_THAT(CommunicationType(/*partition_size=*/36, *instr,\n+                                device_info().gpu_compute_capability()),\n+              IsOkAndHolds(GPUCommunicationType::SINGLE_PARTITION));\n+}\n+\n+TEST_F(CommunicationTypeTest, DetectsRailAlignedMultiPartition) {\n+  // 128 devices across 2 partitions with partition_size=8 (rail-aligned: 64\n+  // devices per partition)\n+  absl::string_view kHlo = R\"(\n+    HloModule m, num_partitions=32\n+\n+    ENTRY e {\n+      p = f32[128] parameter(0)\n+      ROOT _ = f32[4096] all-gather(p),\n+        dimensions={0},\n+        use_global_device_ids=true,\n+        channel_id=1,\n+        replica_groups=[1,128]<=[128]\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(kHlo));\n+\n+  HloCollectiveInstruction* instr = Cast<HloCollectiveInstruction>(\n+      module->entry_computation()->root_instruction());\n+  EXPECT_THAT(CommunicationType(/*partition_size=*/64, *instr,\n+                                device_info().gpu_compute_capability()),\n+              IsOkAndHolds(GPUCommunicationType::MULTI_HOST_WORLD_LEVEL));\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        },
        {
            "sha": "78644b55eda325cd1608abbd85e51836ab3d8735",
            "filename": "third_party/xla/xla/service/hlo_module_config.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_config.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_config.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_config.cc?ref=81341174769fb5756592ed0c620db8d3b0561285",
            "patch": "@@ -110,6 +110,9 @@ std::string HloModuleConfig::compilation_cache_key() const {\n     StrAppend(&key, \"::device_memory_size=\", device_memory_size());\n   }\n   StrAppend(&key, \"::use_shardy_partitioner=\", use_shardy_partitioner());\n+  if (partition_size() != 0) {\n+    StrAppend(&key, \"::partition_size=\", partition_size());\n+  }\n   return key;\n }\n \n@@ -339,6 +342,7 @@ HloModuleConfigProto HloModuleConfig::ToProto() const {\n   proto.set_fdo_profile(fdo_profile_);\n   proto.set_device_memory_size(device_memory_size_);\n   proto.set_use_shardy_partitioner(use_shardy_partitioner_);\n+  proto.set_partition_size(partition_size_);\n   *proto.mutable_sharding_config() = ShardingConfig::ToProto(sharding_config_);\n   *proto.mutable_schedule_config() = ScheduleConfig::ToProto(schedule_config_);\n   return proto;\n@@ -418,6 +422,7 @@ HloModuleConfig::CreateFromProto(const HloModuleConfigProto& proto) {\n   config->fdo_profile_ = proto.fdo_profile();\n   config->device_memory_size_ = proto.device_memory_size();\n   config->use_shardy_partitioner_ = proto.use_shardy_partitioner();\n+  config->partition_size_ = proto.partition_size();\n   config->sharding_config_ = ShardingConfig::FromProto(proto.sharding_config());\n   config->schedule_config_ = ScheduleConfig::FromProto(proto.schedule_config());\n   return std::move(config);"
        },
        {
            "sha": "19949ce76c08c00c70ceaee160e047ef19fa81f3",
            "filename": "third_party/xla/xla/service/hlo_module_config.h",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_config.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_config.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_module_config.h?ref=81341174769fb5756592ed0c620db8d3b0561285",
            "patch": "@@ -451,6 +451,12 @@ class HloModuleConfig {\n     use_shardy_partitioner_ = use_shardy_partitioner;\n   }\n \n+  // Number of devices in a fast-interconnect domain.\n+  int64_t partition_size() const { return partition_size_; }\n+  void set_partition_size(int64_t partition_size) {\n+    partition_size_ = partition_size;\n+  }\n+\n   // Do channel IDs in this module carry semantic information.\n   bool ChannelIdSensitive() const {\n     // TODO(b/430952564): Base this on num_partitions / num_replicas instead\n@@ -625,6 +631,9 @@ class HloModuleConfig {\n \n   bool use_shardy_partitioner_ = false;\n \n+  // Number of devices in a fast-interconnect domain.\n+  int64_t partition_size_ = 0;\n+\n   // Sharding configuration, where sharding_config_.nodes[v] controls the\n   // sharding of operation v.\n   ShardingConfig sharding_config_;"
        },
        {
            "sha": "bb7c7c3784de12fc847b263d841d8a747ea79a69",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/81341174769fb5756592ed0c620db8d3b0561285/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=81341174769fb5756592ed0c620db8d3b0561285",
            "patch": "@@ -1606,7 +1606,7 @@ message ExecutionOptions {\n // Serialization of HloModuleConfig. See the C++ class definition for\n // descriptions of each field.\n // There are no guarantees of backwards or forwards compatibility.\n-// Next id: 42.\n+// Next id: 43.\n message HloModuleConfigProto {\n   enum FusionConfigCollection {\n     OFF = 0;       // Do not collect configuration.\n@@ -1671,6 +1671,8 @@ message HloModuleConfigProto {\n   bool use_shardy_partitioner = 34;\n   ShardingConfigProto sharding_config = 38;\n   ScheduleConfigProto schedule_config = 41;\n+  // Number of devices in a fast-interconnect domain.\n+  int64 partition_size = 42;\n }\n \n message HloModuleProtoWithConfig {"
        }
    ],
    "stats": {
        "total": 370,
        "additions": 272,
        "deletions": 98
    }
}