{
    "author": "beckerhe",
    "message": "Add accelerated feature flag to CudaComputeCapability\n\nThis makes CudaComputeCapability able to handle CUDA feature extensions like sm_90a and sm_100f.\n\n- The `a` feature extension (`a` for accelerated I believe) enables features that can only run on all GPUs with the exact same compute capability number (same major and same minor number).\n- The `f` feature extension (`f` for forward compatible I believe) enables feature that can run on all GPUs with the exact same major compute capability number and a higher or equal minor compute capability number.\n- For comparison GPU programs targeting no feature extensions can run on any higher or equal compute capability.\n\nDue to the introduction of feature extensions the type `CudaComputeCapability` is not a total order - in particular comparisons are not symmetric anymore. Therefore I'm doing the following changes:\n\n1. Ordering comparison operators got removed since it's not a total order anymore.\n2. The member function `IsAtLeast(CudaComputeCapability)` got removed because it's unclear what `IsAtLeast(sm_100a)` means. (`IsAtLeast(int major, int minor)` can stay, since this is still well defined.)\n3. New functions `SupportsAllFeaturesOf(CudaComputeCapability)` and `CanRunOn(CudaComputeCapability)` are being introduced. Both do the same comparison but with left and right hand side swapped, i.e. `gpu.SupportsAllFeaturesOf(kernel)` is equivalent to `kernel.CanRunOn(gpu)`.\n4. The change preserves XLA's current behaviour, meaning we will still target the most narrow feature set, i.e. sm_90a and sm_100a on H100 and B200 GPUs respectively. But with this change it will be possible to properly target sm_90 or sm_100f if the user chooses to do so.\n5. The PTX compilation functions now support targeting sm_90 and sm_100 (and sm_100f for that matter). Before for Hopper and Blackwell we were hardcoding sm_90a and sm_100a.\n\nPiperOrigin-RevId: 797707629",
    "sha": "f7327b23ca8de6ca8d24e335edac70145f0d6d39",
    "files": [
        {
            "sha": "96ccfd68c8670e30bbc560c183b9aef441b494c8",
            "filename": "tensorflow/core/common_runtime/gpu/gpu_device.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_device.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_device.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_device.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -2374,7 +2374,11 @@ Status BaseGPUDeviceFactory::GetValidDeviceIds(\n         \"No supported cuda capabilities in binary.\");\n   }\n   se::CudaComputeCapability min_supported_capability = *std::min_element(\n-      cuda_supported_capabilities.begin(), cuda_supported_capabilities.end());\n+      cuda_supported_capabilities.begin(), cuda_supported_capabilities.end(),\n+      [](const stream_executor::CudaComputeCapability& a,\n+         const stream_executor::CudaComputeCapability& b) {\n+        return std::tie(a.major, a.minor) < std::tie(b.major, b.minor);\n+      });\n #endif\n \n   int min_gpu_core_count =\n@@ -2397,7 +2401,8 @@ Status BaseGPUDeviceFactory::GetValidDeviceIds(\n #if GOOGLE_CUDA\n     // Only GPUs with no less than the minimum supported compute capability is\n     // accepted.\n-    if (desc->cuda_compute_capability() < min_supported_capability) {\n+    if (!desc->cuda_compute_capability().SupportsAllFeaturesOf(\n+            min_supported_capability)) {\n       LOG(INFO) << \"Ignoring visible gpu device \" << \"(\"\n                 << GetShortDeviceDescription(visible_gpu_id, *desc) << \") \"\n                 << \"with Cuda compute capability \""
        },
        {
            "sha": "8c3127bc643c15d2ed1c78a61e42d191d5893492",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -421,8 +421,8 @@ xla_test(\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/stream_executor/cuda:cudnn_plugin\",\n         \"//xla/tsl/lib/core:status_test_util\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\","
        },
        {
            "sha": "1677b6416be29ae352c20aabfc0172ad50ce930c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/cuda_command_buffer_thunk_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcuda_command_buffer_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcuda_command_buffer_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcuda_command_buffer_thunk_test.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/ascii.h\"\n #include \"absl/types/span.h\"\n@@ -52,7 +53,6 @@ limitations under the License.\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/types.h\"  // IWYU pragma: keep\n #include \"xla/xla_data.pb.h\"\n@@ -87,8 +87,9 @@ TEST(CommandBufferThunkTest, CuDnnCmd) {\n     GTEST_SKIP() << \"Requires cuDNN 9.7.0 or later.\";\n   }\n \n-  if (stream_executor->GetDeviceDescription().cuda_compute_capability() <\n-      stream_executor::CudaComputeCapability::Ampere()) {\n+  if (!stream_executor->GetDeviceDescription()\n+           .cuda_compute_capability()\n+           .IsAtLeastAmpere()) {\n     GTEST_SKIP() << \"Requires at least an Ampere GPU.\";\n   }\n "
        },
        {
            "sha": "9f6a55726516f9ee226615064c9ff9441d97de7b",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -1155,9 +1155,10 @@ cc_library(\n         \"//xla:window_util\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status:statusor\",\n-        \"@local_tsl//tsl/platform:logging\",\n-        \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n \n@@ -1194,6 +1195,7 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"@com_google_absl//absl/functional:overload\",\n     ],\n )\n@@ -1266,6 +1268,7 @@ cc_library(\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service:float_support\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n@@ -2003,18 +2006,19 @@ xla_test(\n         \"//xla/service:executable\",\n         \"//xla/service:hlo_module_config\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/stream_executor/cuda:nvjitlink_support\",\n         \"//xla/stream_executor/cuda:ptx_compilation_method\",\n         \"//xla/stream_executor/cuda:ptx_compiler_support\",\n         \"//xla/stream_executor/cuda:ptx_linking_method\",\n         \"//xla/tests:hlo_test_base\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/platform:env\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:btree\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n@@ -2845,6 +2849,7 @@ xla_test(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:semantic_version\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tests:hlo_test_base\",\n         \"//xla/tests:xla_internal_test_main\",  # fixdeps: keep\n         \"@com_google_absl//absl/strings\","
        },
        {
            "sha": "4eb2c900e454c231aaf08f43b6af6159cd6314dd",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -278,6 +278,7 @@ cc_library(\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/lib/core:bits\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n@@ -646,6 +647,7 @@ xla_test(\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:platform\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tests:hlo_test_base\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\","
        },
        {
            "sha": "b32b8b50f28249d9bfb74b31990dcba3832a0500",
            "filename": "third_party/xla/xla/service/gpu/autotuning/dot_search_space_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fdot_search_space_test.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -146,7 +146,7 @@ class DotSearchSpaceTest : public DefaultDeviceDotSearchSpaceTest {\n     device_description_.set_threads_per_warp(32);\n     device_description_.set_shared_memory_per_block_optin(227 * 1024);\n     device_description_.set_gpu_compute_capability(\n-        se::CudaComputeCapability::Hopper());\n+        se::CudaComputeCapability::H100Family());\n   }\n };\n "
        },
        {
            "sha": "5e7800be91789edeb4fcb9e8169ecfcc223488f7",
            "filename": "third_party/xla/xla/service/gpu/cublas_padding_requirements.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcublas_padding_requirements.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcublas_padding_requirements.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcublas_padding_requirements.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/shape.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/util.h\"\n \n@@ -36,7 +37,7 @@ bool DimensionRequiresPadding(const int64_t size, const PrimitiveType data_type,\n       absl::Overload(\n           [&](const se::CudaComputeCapability& cc) {\n             for (const auto& req : CublasPaddingRequirements) {\n-              if (cc.IsAtLeast(req.min_compute_capability) &&\n+              if (cc.SupportsAllFeaturesOf(req.min_compute_capability) &&\n                   data_type == req.data_type && size % req.multiple_of != 0) {\n                 return true;\n               }"
        },
        {
            "sha": "746e8953de56773d56a89e77ea038e33f8e86a6d",
            "filename": "third_party/xla/xla/service/gpu/cublas_padding_requirements.h",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcublas_padding_requirements.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcublas_padding_requirements.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcublas_padding_requirements.h?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -19,13 +19,14 @@ limitations under the License.\n #include <array>\n \n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n \n namespace xla {\n namespace gpu {\n \n struct CublasPaddingRequirement {\n-  int min_compute_capability;\n+  se::CudaComputeCapability min_compute_capability;\n   PrimitiveType data_type;\n   int multiple_of;\n };\n@@ -37,9 +38,9 @@ struct HipblasPaddingRequirement {\n \n // List of padding requirements per compute capability and data type.\n constexpr std::array<CublasPaddingRequirement, 3> CublasPaddingRequirements{\n-    {{se::CudaComputeCapability::kVolta, S8, 4},\n-     {se::CudaComputeCapability::kVolta, F16, 8},\n-     {se::CudaComputeCapability::kAmpere, BF16, 8}}};\n+    {{se::CudaComputeCapability::Volta(), S8, 4},\n+     {se::CudaComputeCapability::Volta(), F16, 8},\n+     {se::CudaComputeCapability::Ampere(), BF16, 8}}};\n \n constexpr std::array<HipblasPaddingRequirement, 2> HipblasPaddingRequirements{\n     {{/*rocm gpu arch,*/ F16, 8}, {/*rocm gpu arch,*/ BF16, 8}}};"
        },
        {
            "sha": "ec11b0ba9c3e9b5030b0fceaa0c41eb6879b0b25",
            "filename": "third_party/xla/xla/service/gpu/cudnn_support_utils.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcudnn_support_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcudnn_support_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcudnn_support_utils.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -18,16 +18,18 @@ limitations under the License.\n #include <cstdint>\n #include <vector>\n \n+#include \"absl/log/log.h\"\n+#include \"absl/status/statusor.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/window_util.h\"\n-#include \"tsl/platform/logging.h\"\n-#include \"tsl/platform/statusor.h\"\n \n namespace xla {\n namespace gpu {\n@@ -50,8 +52,10 @@ absl::StatusOr<bool> CudnnSupportsOptimizedIntegerConvolution(\n \n   // Require cc6.1+ for any vectorized integer convolutions\n   // Require cc7.5+ for any IMMA convolutions\n-  if ((vector_size == 32 && !compute_capability.IsAtLeast(7, 5)) ||\n-      !compute_capability.IsAtLeast(6, 1)) {\n+  if ((vector_size == 32 && !compute_capability.SupportsAllFeaturesOf(\n+                                se::CudaComputeCapability(7, 5))) ||\n+      !compute_capability.SupportsAllFeaturesOf(\n+          se::CudaComputeCapability(6, 1))) {\n     VLOG(3) << \"Compute capability \" << compute_capability.ToString()\n             << \" is not sufficent for int8x\" << vector_size\n             << \" vectorization.\";"
        },
        {
            "sha": "861189cbdcdb28acecf7ed4c22fca4701c8914f6",
            "filename": "third_party/xla/xla/service/gpu/dot_algorithm_support_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdot_algorithm_support_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdot_algorithm_support_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fdot_algorithm_support_test.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"absl/strings/substitute.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/primitive_util.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n #include \"xla/tests/hlo_test_base.h\"\n@@ -153,8 +154,8 @@ TEST_P(DotAlgorithmSupportTest, AlgorithmIsSupportedFromCudaCapability) {\n   auto gpu_cc = GetGpuComputeCapability();\n \n   if (const auto *ccc = std::get_if<se::CudaComputeCapability>(&gpu_cc)) {\n-    is_algorithm_supported = ccc->IsAtLeast(params.min_cuda_capability.major,\n-                                            params.min_cuda_capability.minor);\n+    is_algorithm_supported =\n+        ccc->SupportsAllFeaturesOf(params.min_cuda_capability);\n   } else if (const auto *rcc =\n                  std::get_if<se::RocmComputeCapability>(&gpu_cc)) {\n     is_algorithm_supported = rcc->gfx9_mi100_or_later();"
        },
        {
            "sha": "b6df01d30a5d90a21fd32069cabb7100fa365aaa",
            "filename": "third_party/xla/xla/service/gpu/gpu_float_support.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"xla/primitive_util.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/float_support.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/xla_data.pb.h\"\n \n@@ -64,18 +65,17 @@ bool GpuFloatSupport::IsSupported(const HloInstruction& hlo) const {\n     case HloOpcode::kReduceScatter:\n     // Handled by Triton GEMM.\n     case HloOpcode::kDot:\n-      using TypeAndCC = std::pair<\n-          PrimitiveType,\n-          stream_executor::CudaComputeCapability::CudaComputeCapabilities>;\n+      using TypeAndCC =\n+          std::pair<PrimitiveType, stream_executor::CudaComputeCapability>;\n       for (auto [type, cc] :\n-           {TypeAndCC(F8E4M3FN, se::CudaComputeCapability::kAmpere),\n-            TypeAndCC(F8E5M2, se::CudaComputeCapability::kHopper)}) {\n+           {TypeAndCC(F8E4M3FN, se::CudaComputeCapability::Ampere()),\n+            TypeAndCC(F8E5M2, se::CudaComputeCapability::Hopper())}) {\n         if (LowPrecisionType() == type) {\n           auto* cuda_compute_capability =\n               std::get_if<se::CudaComputeCapability>(&compute_capability_);\n           // Do not normalize supported types inside Triton fused computations.\n           return cuda_compute_capability &&\n-                 cuda_compute_capability->IsAtLeast(cc) &&\n+                 cuda_compute_capability->SupportsAllFeaturesOf(cc) &&\n                  IsTritonFusedComputation(*hlo.parent());\n         }\n       }"
        },
        {
            "sha": "aeebbd63e94cdb582b7d09bf21bdbfc05eb81501",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/BUILD",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2FBUILD?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -100,9 +100,12 @@ cc_library(\n         \"//xla/service/llvm_ir:llvm_command_line_options\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:semantic_version\",\n-        \"//xla/stream_executor/cuda:ptx_compiler_helpers\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/stream_executor/cuda:subprocess_compilation\",\n+        \"//xla/tsl/platform:env\",\n+        \"//xla/tsl/platform:errors\",\n         \"@com_google_absl//absl/base\",\n+        \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n@@ -122,9 +125,6 @@ cc_library(\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//llvm:Target\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n-        \"@local_tsl//tsl/platform:env\",\n-        \"@local_tsl//tsl/platform:errors\",\n-        \"@local_tsl//tsl/platform:logging\",\n         \"@local_tsl//tsl/platform:path\",\n         \"@local_tsl//tsl/profiler/lib:scoped_annotation\",\n         \"@local_tsl//tsl/profiler/lib:traceme\",\n@@ -239,10 +239,11 @@ xla_cc_test(\n     ],\n     deps = [\n         \":nvptx_backend\",\n-        \"//xla/stream_executor:device_description\",\n+        \":ptx_version_util\",\n         \"//xla/stream_executor:semantic_version\",\n-        \"//xla/tests:xla_internal_test_main\",\n-        \"@local_tsl//tsl/platform:test\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_googletest//:gtest_main\",\n     ],\n )\n "
        },
        {
            "sha": "d0a7501030a3bfb3a63f54c611dc5b8610c3bfa2",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc",
            "status": "modified",
            "additions": 52,
            "deletions": 27,
            "changes": 79,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fnvptx_backend.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fnvptx_backend.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fnvptx_backend.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/service/gpu/llvm_gpu_backend/nvptx_backend.h\"\n \n+#include <algorithm>\n #include <cstdint>\n #include <functional>\n #include <memory>\n@@ -23,10 +24,11 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/base/call_once.h\"\n+#include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n-#include \"absl/strings/string_view.h\"\n #include \"third_party/gpus/cuda/include/cuda.h\"\n #include \"llvm/Analysis/CGSCCPassManager.h\"\n #include \"llvm/Analysis/LazyCallGraph.h\"\n@@ -58,17 +60,17 @@ limitations under the License.\n #include \"xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h\"\n #include \"xla/service/gpu/llvm_gpu_backend/load_ir_module.h\"\n #include \"xla/service/gpu/llvm_gpu_backend/nvptx_libdevice_path.h\"\n+#include \"xla/service/gpu/llvm_gpu_backend/ptx_version_util.h\"\n #include \"xla/service/gpu/metrics.h\"\n #include \"xla/service/llvm_ir/llvm_command_line_options.h\"\n-#include \"xla/stream_executor/cuda/ptx_compiler_helpers.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/subprocess_compilation.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n+#include \"xla/tsl/platform/env.h\"\n+#include \"xla/tsl/platform/errors.h\"\n #include \"xla/util.h\"\n #include \"xla/xla.pb.h\"\n-#include \"tsl/platform/env.h\"\n-#include \"tsl/platform/errors.h\"\n-#include \"tsl/platform/logging.h\"\n #include \"tsl/profiler/lib/scoped_annotation.h\"\n #include \"tsl/profiler/lib/traceme.h\"\n \n@@ -233,39 +235,62 @@ std::vector<std::string> GetNVPTXBackendOptions(\n }\n \n std::string GetSmName(se::CudaComputeCapability compute_capability) {\n-  int compute_capability_version =\n-      compute_capability.major * 10 + compute_capability.minor;\n-  int sm_version = 30;\n+  using CudaComputeCapabilities =\n+      se::CudaComputeCapability::CudaComputeCapabilities;\n+\n+  auto gpu_compute_capability = compute_capability;\n+  gpu_compute_capability.feature_extension =\n+      se::CudaComputeCapability::FeatureExtension::kNone;\n   // If the current compute capability isn't known, fallback to the\n   // most recent version before it.\n-  int supported_versions[] = {121, 120, 103, 101, 100, 90, 89, 87,\n-                              86,  80,  75,  72,  70,  62, 61, 60,\n-                              53,  52,  50,  37,  35,  32, 30};\n-  for (int v : supported_versions) {\n-    if (v <= compute_capability_version) {\n-      sm_version = v;\n+  constexpr stream_executor::CudaComputeCapability kSupportedVersions[] = {\n+      {12, 1}, {12, 0}, {10, 3}, {10, 1}, {10, 0}, {9, 0}, {8, 9}, {8, 7},\n+      {8, 6},  {8, 0},  {7, 5},  {7, 2},  {7, 0},  {6, 2}, {6, 1}, {6, 0},\n+      {5, 3},  {5, 2},  {5, 0},  {3, 7},  {3, 5},  {3, 2}, {3, 0}};\n+  auto target_compute_capability = kSupportedVersions[0];\n+\n+  for (const auto& v : kSupportedVersions) {\n+    if (!gpu_compute_capability.CanRunOn(v)) {\n       break;\n     }\n+    target_compute_capability = v;\n+  }\n+\n+  if (target_compute_capability.major == gpu_compute_capability.major &&\n+      target_compute_capability.minor == gpu_compute_capability.minor) {\n+    // If we support the requested compute capability, then we can also enable\n+    // the requested feature extension.\n+    target_compute_capability.feature_extension =\n+        compute_capability.feature_extension;\n+  } else if (target_compute_capability.major >=\n+                 CudaComputeCapabilities::kBlackwell &&\n+             target_compute_capability.major <= kSupportedVersions[0].major &&\n+             target_compute_capability.major == compute_capability.major &&\n+             target_compute_capability.minor <= gpu_compute_capability.minor) {\n+    // If we don't support the requested compute capability, but an\n+    // earlier one with the same major version, then we can enable\n+    // the forward compatible feature extension - if the particular\n+    // major version supports the forward compatible feature\n+    // extension.\n+    target_compute_capability.feature_extension =\n+        se::CudaComputeCapability::FeatureExtension::kForwardCompatibleFeatures;\n   }\n \n   // If the current CC isn't supported by LLVM and it is newer then\n   // the max supported LLVM version, do not warn about it. The end\n   // user can't do anything about this. E.g., PTX compiled for SM75 will\n   // run on SM80 too.\n-  if (sm_version != compute_capability_version &&\n-      compute_capability_version < supported_versions[0]) {\n-    LOG(WARNING) << \"Unknown compute capability \"\n-                 << compute_capability.ToString()\n-                 << \". Defaulting to telling LLVM that we're compiling for sm_\"\n-                 << sm_version;\n+  if (target_compute_capability != compute_capability &&\n+      target_compute_capability.major != kSupportedVersions[0].major &&\n+      target_compute_capability.minor != kSupportedVersions[0].minor) {\n+    LOG(WARNING)\n+        << \"Unknown compute capability \" << compute_capability.ToString()\n+        << \". Defaulting to telling LLVM that we're compiling for \"\n+        << target_compute_capability.GetPtxAsTargetName(\n+               stream_executor::CudaComputeCapability::CompileMode::kSass);\n   }\n-  // On Hopper, default to sm_90a so that all instructions can be used. But\n-  // only sm_90 is forward compatible, so don't use sm_90a with newer hardware:\n-  // https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#ptx-compatibility\n-  // Similarly for sm_10#a and sm_12#a (Blackwell).\n-  absl::string_view extension =\n-      stream_executor::ShouldUsePtxExtension(compute_capability) ? \"a\" : \"\";\n-  return absl::StrCat(\"sm_\", sm_version, extension);\n+  return target_compute_capability.GetPtxAsTargetName(\n+      stream_executor::CudaComputeCapability::CompileMode::kSass);\n }\n \n absl::StatusOr<std::string> CompileToPtx("
        },
        {
            "sha": "31d08c2572e389762cdd6c65bdbc8add34b2e71b",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend_test.cc",
            "status": "modified",
            "additions": 31,
            "deletions": 9,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fnvptx_backend_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fnvptx_backend_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fnvptx_backend_test.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -17,25 +17,47 @@ limitations under the License.\n \n #include <utility>\n \n-#include \"xla/stream_executor/device_description.h\"\n+#include <gtest/gtest.h>\n+#include \"absl/strings/str_cat.h\"\n+#include \"xla/service/gpu/llvm_gpu_backend/ptx_version_util.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n-#include \"tsl/platform/test.h\"\n \n namespace xla {\n namespace gpu {\n namespace {\n namespace se = ::stream_executor;\n \n TEST(UtilsTest, TestGetSmName) {\n-  ASSERT_EQ(nvptx::GetSmName(se::CudaComputeCapability{9, 0}), \"sm_90a\");\n-  ASSERT_EQ(nvptx::GetSmName(se::CudaComputeCapability{10, 0}), \"sm_100a\");\n-  ASSERT_EQ(nvptx::GetSmName(se::CudaComputeCapability{10, 1}), \"sm_101a\");\n-  ASSERT_EQ(nvptx::GetSmName(se::CudaComputeCapability{10, 3}), \"sm_103a\");\n-  ASSERT_EQ(nvptx::GetSmName(se::CudaComputeCapability{12, 0}), \"sm_120a\");\n-  ASSERT_EQ(nvptx::GetSmName(se::CudaComputeCapability{12, 1}), \"sm_121a\");\n+  using FeatureExtension = se::CudaComputeCapability::FeatureExtension;\n+  ASSERT_EQ(nvptx::GetSmName(\n+                se::CudaComputeCapability{9, 0, FeatureExtension::kNone}),\n+            \"sm_90\");\n+  ASSERT_EQ(nvptx::GetSmName(se::CudaComputeCapability{\n+                9, 0, FeatureExtension::kAcceleratedFeatures}),\n+            \"sm_90a\");\n+  ASSERT_EQ(nvptx::GetSmName(se::CudaComputeCapability{\n+                10, 0, FeatureExtension::kAcceleratedFeatures}),\n+            \"sm_100a\");\n+  ASSERT_EQ(nvptx::GetSmName(se::CudaComputeCapability{\n+                10, 0, FeatureExtension::kForwardCompatibleFeatures}),\n+            \"sm_100f\");\n+  ASSERT_EQ(nvptx::GetSmName(se::CudaComputeCapability{\n+                10, 1, FeatureExtension::kAcceleratedFeatures}),\n+            \"sm_101a\");\n+  ASSERT_EQ(nvptx::GetSmName(se::CudaComputeCapability{\n+                10, 3, FeatureExtension::kAcceleratedFeatures}),\n+            \"sm_103a\");\n+  ASSERT_EQ(nvptx::GetSmName(se::CudaComputeCapability{\n+                12, 0, FeatureExtension::kAcceleratedFeatures}),\n+            \"sm_120a\");\n+  ASSERT_EQ(nvptx::GetSmName(se::CudaComputeCapability{\n+                12, 1, FeatureExtension::kAcceleratedFeatures}),\n+            \"sm_121a\");\n   // Do not use the extension for a yet-unknown compute capability.\n   // https://docs.nvidia.com/cuda/parallel-thread-execution/#release-notes-ptx-release-history\n-  ASSERT_EQ(nvptx::GetSmName(se::CudaComputeCapability{12, 9}), \"sm_121\");\n+  ASSERT_EQ(nvptx::GetSmName(se::CudaComputeCapability{12, 9}), \"sm_121f\");\n+  ASSERT_EQ(nvptx::GetSmName(se::CudaComputeCapability{13, 0}), \"sm_121\");\n }\n \n using VersionPair = std::pair<se::SemanticVersion, se::SemanticVersion>;"
        },
        {
            "sha": "9e4137c7da66d7acf5c3642bd41da5afb270bde2",
            "filename": "third_party/xla/xla/service/gpu/model/sol_latency_estimator_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_latency_estimator_test.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -431,7 +431,8 @@ TEST_F(IsSolLatencyEstimatorEnabledTest, EnabledBySolEstimatorFlagOnHopper) {\n   HloModuleConfig config;\n   config.mutable_debug_options()\n       .set_xla_gpu_enable_analytical_sol_latency_estimator(true);\n-  gpu_device_info_.set_cuda_compute_capability(9, 0);  // Hopper\n+  gpu_device_info_.set_cuda_compute_capability(\n+      stream_executor::CudaComputeCapability::Hopper());\n \n   auto module = CreateTestModule(config);\n   EXPECT_TRUE(\n@@ -441,7 +442,8 @@ TEST_F(IsSolLatencyEstimatorEnabledTest, EnabledBySolEstimatorFlagOnHopper) {\n TEST_F(IsSolLatencyEstimatorEnabledTest, DisabledIfFlagIsOffOnHopper) {\n   HloModuleConfig config;\n \n-  gpu_device_info_.set_cuda_compute_capability(9, 0);  // Hopper\n+  gpu_device_info_.set_cuda_compute_capability(\n+      stream_executor::CudaComputeCapability::Hopper());\n \n   auto module = CreateTestModule(config);\n \n@@ -455,7 +457,8 @@ TEST_F(IsSolLatencyEstimatorEnabledTest,\n   config.mutable_debug_options()\n       .set_xla_gpu_enable_analytical_sol_latency_estimator(true);\n \n-  gpu_device_info_.set_cuda_compute_capability(9, 0);  // Hopper\n+  gpu_device_info_.set_cuda_compute_capability(\n+      stream_executor::CudaComputeCapability::Hopper());\n \n   auto module = CreateTestModule(config);\n   AddCollectivePermute(module.get());  // Unsupported collective\n@@ -470,7 +473,8 @@ TEST_F(IsSolLatencyEstimatorEnabledTest,\n   config.mutable_debug_options()\n       .set_xla_gpu_enable_analytical_sol_latency_estimator(true);\n \n-  gpu_device_info_.set_cuda_compute_capability(9, 0);  // Hopper\n+  gpu_device_info_.set_cuda_compute_capability(\n+      stream_executor::CudaComputeCapability::Hopper());\n \n   auto module = CreateTestModule(config);\n   AddAllReduce(module.get());          // Supported\n@@ -485,7 +489,8 @@ TEST_F(IsSolLatencyEstimatorEnabledTest, DisabledIfNotHopper) {\n   config.mutable_debug_options()\n       .set_xla_gpu_enable_analytical_sol_latency_estimator(true);\n \n-  gpu_device_info_.set_cuda_compute_capability(8, 0);  // Not Hopper\n+  gpu_device_info_.set_cuda_compute_capability(\n+      stream_executor::CudaComputeCapability::Ampere());  // Not Hopper\n \n   auto module = CreateTestModule(config);\n   AddAllReduce(module.get());  // Supported collective"
        },
        {
            "sha": "1fadd2e6a0d901e15498ee4508c24095f15d157e",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -295,7 +295,7 @@ absl::Status NVPTXCompiler::OptimizeHloPostLayoutAssignment(\n            .xla_gpu_experimental_disable_binary_libraries()) {\n     for (const CublasPaddingRequirement& requirement :\n          CublasPaddingRequirements) {\n-      if (cuda_compute_capability.IsAtLeast(\n+      if (cuda_compute_capability.SupportsAllFeaturesOf(\n               requirement.min_compute_capability)) {\n         pre_pipeline.AddPass<CublasPadForGemms>(cuda_compute_capability,\n                                                 requirement.data_type,"
        },
        {
            "sha": "8a3ea1e714e8579d8d9e7c6d0606e0e00cc7918c",
            "filename": "third_party/xla/xla/service/gpu/ptx_compilation_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fptx_compilation_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fptx_compilation_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fptx_compilation_test.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"absl/container/btree_map.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/match.h\"\n #include \"absl/strings/str_cat.h\"\n@@ -41,14 +42,14 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_executable.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n #include \"xla/service/hlo_module_config.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/nvjitlink_support.h\"\n #include \"xla/stream_executor/cuda/ptx_compilation_method.h\"\n #include \"xla/stream_executor/cuda/ptx_compiler_support.h\"\n #include \"xla/stream_executor/cuda/ptx_linking_method.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tests/hlo_test_base.h\"\n #include \"xla/tsl/platform/env.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla.pb.h\"\n #include \"tsl/platform/path.h\"\n@@ -148,12 +149,15 @@ class NVPTXCompilationTests\n                              PtxCompilationMethod compilation_method,\n                              PtxLinkingMethod linking_method) {\n     using CudaComputeCapability = stream_executor::CudaComputeCapability;\n-    if (!::testing::Value(backend()\n-                              .default_stream_executor()\n-                              ->GetDeviceDescription()\n-                              .gpu_compute_capability(),\n-                          ::testing::VariantWith<CudaComputeCapability>(\n-                              CudaComputeCapability{9, 0})) &&\n+    if (!::testing::Value(\n+            backend()\n+                .default_stream_executor()\n+                ->GetDeviceDescription()\n+                .gpu_compute_capability(),\n+            ::testing::VariantWith<CudaComputeCapability>(\n+                CudaComputeCapability{9, 0,\n+                                      CudaComputeCapability::FeatureExtension::\n+                                          kAcceleratedFeatures})) &&\n         name == \"requires_sm90a\") {\n       GTEST_SKIP() << \"This test requires SM 9.0a\";\n     }"
        },
        {
            "sha": "83fe34863a7154aad506b33f720788b615e7f266",
            "filename": "third_party/xla/xla/service/gpu/tests/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -501,8 +501,8 @@ xla_test(\n     backends = [\"gpu\"],\n     deps = [\n         \":gpu_codegen_test\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"@com_google_googletest//:gtest_main\",\n-        \"@local_tsl//tsl/platform:test\",\n     ],\n )\n \n@@ -570,6 +570,7 @@ xla_test(\n         \"//xla:types\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings\",\n@@ -797,11 +798,11 @@ xla_test(\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/stream_executor/cuda:cuda_platform_id\",\n         \"//xla/tests:hlo_test_base\",\n-        \"//xla/tests:xla_internal_test_main\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_googletest//:gtest_main\",\n     ],\n )\n \n@@ -896,6 +897,7 @@ xla_test(\n     backends = [\"gpu\"],\n     deps = [\n         \"//xla:xla_proto_cc\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tests:hlo_test_base\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\","
        },
        {
            "sha": "225b6e3c83aa40ec8a3990ba699ec11406f64ea8",
            "filename": "third_party/xla/xla/service/gpu/tests/gpu_atomic_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_atomic_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_atomic_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_atomic_test.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -13,9 +13,9 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-\n+#include <gtest/gtest.h>\n #include \"xla/service/gpu/tests/gpu_codegen_test.h\"\n-#include \"tsl/platform/test.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n \n namespace xla {\n namespace gpu {\n@@ -114,7 +114,8 @@ TEST_F(GpuAtomicTest, TestAddAtomicF64) {\n            .default_stream_executor()\n            ->GetDeviceDescription()\n            .cuda_compute_capability()\n-           .IsAtLeast(6)) {\n+           .SupportsAllFeaturesOf(\n+               stream_executor::CudaComputeCapability::Pascal())) {\n     return;\n   }\n "
        },
        {
            "sha": "54dc1f81b4f5b348a6879c89a1cd786e87e5165d",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -474,7 +474,6 @@ cc_library(\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@local_tsl//tsl/platform:errors\",\n         \"@local_tsl//tsl/platform:logging\",\n@@ -1653,6 +1652,7 @@ cc_library(\n         \"//xla/service/gpu/tests:gpu_codegen_test\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:semantic_version\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest_for_library\","
        },
        {
            "sha": "e49ecd923c2102adf9e96d922b038b283f214215",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_rewriter_test_lib.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter_test_lib.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include \"absl/strings/str_replace.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/service/gpu/tests/gpu_codegen_test.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n #include \"xla/xla.pb.h\"\n@@ -100,8 +101,8 @@ bool GemmRewriteTestBase::HasFp8Support() const {\n \n bool GemmRewriteTestBase::HasCudaComputeCapability(\n     const stream_executor::CudaComputeCapability& cc) const {\n-  return IsCuda() &&\n-         std::get<se::CudaComputeCapability>(Capability()).IsAtLeast(cc);\n+  return IsCuda() && std::get<se::CudaComputeCapability>(Capability())\n+                         .SupportsAllFeaturesOf(cc);\n }\n \n ParameterizedGemmRewriteTestBase::ParameterizedGemmRewriteTestBase() {"
        },
        {
            "sha": "0739ae6b447db5a92574c906f1a72db545a2bd99",
            "filename": "third_party/xla/xla/service/platform_util.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fplatform_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fservice%2Fplatform_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fplatform_util.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -46,8 +46,8 @@ limitations under the License.\n namespace xla {\n \n // Minimum supported CUDA compute capability is 3.5.\n-constexpr int kMinCudaComputeCapabilityMajor = 3;\n-constexpr int kMinCudaComputeCapabilityMinor = 5;\n+constexpr se::CudaComputeCapability kMinCudaComputeCapability(\n+    3, 5, se::CudaComputeCapability::FeatureExtension::kNone);\n \n // The name of the interpreter platform.\n constexpr char kInterpreter[] = \"interpreter\";\n@@ -160,12 +160,10 @@ static bool IsDeviceSupported(se::StreamExecutor* executor) {\n   if (executor->GetPlatform()->id() == se::cuda::kCudaPlatformId) {\n     // CUDA devices must have a minimum compute capability.\n     se::CudaComputeCapability cc = description.cuda_compute_capability();\n-    if (!cc.IsAtLeast(kMinCudaComputeCapabilityMajor,\n-                      kMinCudaComputeCapabilityMinor)) {\n+    if (!cc.SupportsAllFeaturesOf(kMinCudaComputeCapability)) {\n       LOG(INFO) << \"StreamExecutor cuda device (\" << executor->device_ordinal()\n                 << \") is of insufficient compute capability: \"\n-                << kMinCudaComputeCapabilityMajor << \".\"\n-                << kMinCudaComputeCapabilityMinor << \" required, \"\n+                << kMinCudaComputeCapability.ToString() << \" required, \"\n                 << \"device is \" << cc.ToString();\n       return false;\n     }"
        },
        {
            "sha": "d08136ae7b2df738de2e993c1913f448f2505680",
            "filename": "third_party/xla/xla/stream_executor/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2FBUILD?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -64,6 +64,7 @@ cc_library(\n         \":semantic_version\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/lib/math:math_util\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\","
        },
        {
            "sha": "1cf4dce3dcd33ec01aa6849cde7f9b19e8e53b3b",
            "filename": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -691,10 +691,9 @@ xla_cc_test(\n     deps = [\n         \":ptx_compiler_helpers\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n-        \"@local_tsl//tsl/platform:status_matchers\",\n-        \"@local_tsl//tsl/platform:test\",\n     ],\n )\n \n@@ -998,6 +997,7 @@ cc_library(\n     ],\n     deps = [\n         \":cuda_command_buffer\",\n+        \":cuda_compute_capability\",\n         \":cuda_context\",\n         \":cuda_event\",\n         \":cuda_kernel\",\n@@ -1990,6 +1990,7 @@ cc_library(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n     ],\n )\n \n@@ -2005,9 +2006,10 @@ xla_cc_test(\n     deps = [\n         \":cuda_compute_capability\",\n         \":cuda_compute_capability_proto_cc\",\n-        \"//xla/tsl/platform:status_matchers\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/hash:hash_testing\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n )"
        },
        {
            "sha": "535f4c64cb205376fac16faa2f61420132717946",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_command_buffer_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_command_buffer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_command_buffer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_command_buffer_test.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -67,8 +67,9 @@ TEST(CudaCommandBufferTest, CuDnnExplicitConstructionAndUpdateWork) {\n     GTEST_SKIP() << \"Requires cuDNN 9.7.0 or later.\";\n   }\n \n-  if (executor->GetDeviceDescription().cuda_compute_capability() <\n-      CudaComputeCapability::Ampere()) {\n+  if (!executor->GetDeviceDescription()\n+           .cuda_compute_capability()\n+           .IsAtLeastAmpere()) {\n     GTEST_SKIP() << \"Requires at least an Ampere GPU.\";\n   }\n "
        },
        {
            "sha": "5e4dde45964396ff04284bafe94c98f7f1564f09",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_compute_capability.cc",
            "status": "modified",
            "additions": 104,
            "deletions": 5,
            "changes": 109,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -15,12 +15,14 @@ limitations under the License.\n \n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n \n+#include <string>\n #include <vector>\n \n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/numbers.h\"\n #include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_format.h\"\n #include \"absl/strings/str_split.h\"\n #include \"absl/strings/string_view.h\"\n \n@@ -34,21 +36,118 @@ absl::StatusOr<CudaComputeCapability> CudaComputeCapability::FromString(\n         absl::StrCat(\"Invalid CUDA architecture name: \", cuda_arch_name));\n   }\n \n+  FeatureExtension feature_extension = FeatureExtension::kNone;\n+  if (!split[1].empty() && (split[1].back() == 'a' || split[1].back() == 'A')) {\n+    feature_extension = FeatureExtension::kAcceleratedFeatures;\n+    split[1].remove_suffix(1);\n+  }\n+\n+  if (!split[1].empty() && (split[1].back() == 'f' || split[1].back() == 'F')) {\n+    feature_extension = FeatureExtension::kForwardCompatibleFeatures;\n+    split[1].remove_suffix(1);\n+  }\n+\n   int major, minor;\n   if (!absl::SimpleAtoi(split[0], &major) ||\n       !absl::SimpleAtoi(split[1], &minor)) {\n     return absl::InvalidArgumentError(\n         absl::StrCat(\"Invalid CUDA architecture name: \", cuda_arch_name));\n   }\n-  return CudaComputeCapability(major, minor);\n+  return CudaComputeCapability{major, minor, feature_extension};\n+}\n+\n+static std::string FeatureExtensionToString(\n+    CudaComputeCapability::FeatureExtension feature_extension) {\n+  switch (feature_extension) {\n+    case CudaComputeCapability::FeatureExtension::kNone:\n+      return \"\";\n+    case CudaComputeCapability::FeatureExtension::kAcceleratedFeatures:\n+      return \"a\";\n+    case CudaComputeCapability::FeatureExtension::kForwardCompatibleFeatures:\n+      return \"f\";\n+  }\n+}\n+\n+std::string CudaComputeCapability::ToString() const {\n+  return absl::StrCat(major, \".\", minor,\n+                      FeatureExtensionToString(feature_extension));\n+}\n+\n+std::string CudaComputeCapability::GetPtxAsTargetName(\n+    CompileMode compile_mode) const {\n+  absl::string_view prefix = [&]() {\n+    switch (compile_mode) {\n+      case CompileMode::kPtx:\n+        return \"compute\";\n+      case CompileMode::kLto:\n+        return \"lto\";\n+      case CompileMode::kSass:\n+        return \"sm\";\n+    }\n+  }();\n+  return absl::StrFormat(\"%s_%d%d%s\", prefix, major, minor,\n+                         FeatureExtensionToString(feature_extension));\n+}\n+\n+absl::StatusOr<CudaComputeCapability> CudaComputeCapability::FromProto(\n+    const CudaComputeCapabilityProto& proto) {\n+  CudaComputeCapability cc;\n+  cc.major = proto.major();\n+  cc.minor = proto.minor();\n+  switch (proto.feature_extension()) {\n+    case CudaComputeCapabilityProto::UNSPECIFIED:\n+      // For backward compatibility we assume sm_90a and sm_100a for Hopper and\n+      // Blackwell generation GPUs.\n+      if (cc.major == 9 || cc.major == 10) {\n+        cc.feature_extension = FeatureExtension::kAcceleratedFeatures;\n+      } else {\n+        cc.feature_extension = FeatureExtension::kNone;\n+      }\n+      break;\n+    case CudaComputeCapabilityProto::NONE:\n+      cc.feature_extension = FeatureExtension::kNone;\n+      break;\n+    case CudaComputeCapabilityProto::ACCELERATED_FEATURES:\n+      cc.feature_extension = FeatureExtension::kAcceleratedFeatures;\n+      break;\n+    case CudaComputeCapabilityProto::FORWARD_COMPATIBLE_FEATURES:\n+      cc.feature_extension = FeatureExtension::kForwardCompatibleFeatures;\n+      break;\n+    default:\n+      return absl::InvalidArgumentError(absl::StrCat(\n+          \"Invalid feature extension: \", proto.feature_extension()));\n+  }\n+  return cc;\n+}\n+\n+CudaComputeCapabilityProto CudaComputeCapability::ToProto() const {\n+  CudaComputeCapabilityProto proto;\n+  proto.set_major(major);\n+  proto.set_minor(minor);\n+\n+  switch (feature_extension) {\n+    case FeatureExtension::kNone:\n+      proto.set_feature_extension(CudaComputeCapabilityProto::NONE);\n+      break;\n+    case FeatureExtension::kAcceleratedFeatures:\n+      proto.set_feature_extension(\n+          CudaComputeCapabilityProto::ACCELERATED_FEATURES);\n+      break;\n+    case FeatureExtension::kForwardCompatibleFeatures:\n+      proto.set_feature_extension(\n+          CudaComputeCapabilityProto::FORWARD_COMPATIBLE_FEATURES);\n+      break;\n+  }\n+  return proto;\n }\n \n CudaComputeCapability CudaComputeCapability::FromIntWithAutoFeatureExtension(\n     int major, int minor) {\n-  // We don't do anything special here, as the extensions are hardcoded in\n-  // `ShouldUsePtxExtension` anyway. This implementation will change with the\n-  // integration of extensions into `CudaComputeCapability`.\n-  return CudaComputeCapability(major, minor);\n+  if (major == 9 || major == 10) {\n+    return CudaComputeCapability{major, minor,\n+                                 FeatureExtension::kAcceleratedFeatures};\n+  }\n+  return CudaComputeCapability{major, minor, FeatureExtension::kNone};\n }\n \n }  // namespace stream_executor"
        },
        {
            "sha": "7153e49b8411e23563be26aef088ed3bc64195ec",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_compute_capability.h",
            "status": "modified",
            "additions": 144,
            "deletions": 50,
            "changes": 194,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability.h?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -17,21 +17,52 @@ limitations under the License.\n #define XLA_STREAM_EXECUTOR_CUDA_CUDA_COMPUTE_CAPABILITY_H_\n \n #include <cassert>\n+#include <cstdint>\n #include <string>\n+#include <tuple>\n #include <utility>\n \n #include \"absl/status/statusor.h\"\n-#include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.pb.h\"\n \n namespace stream_executor {\n \n-// CUDA compute capability, as reported by the device description.\n+// This type represents either\n+// - a compute capability of a NVIDIA GPU.\n+// - a compilation target compute capability.\n+//\n+// A CUDA compute capability is a pair of major and minor versions, e.g. 8.0,\n+// 9.2, 10.1 plus potentially a feature extension, e.g. 9.0a, 10.0f.\n+//\n+// Feature extensions only make sense when talking about a compilation target\n+// compute capability since they restrict on which GPUs the generated code can\n+// be run.\n+//\n+// 1. The accelerated feature extension \"a\":\n+//   Kernels compiled with this extension can only be run on GPUs with the same\n+//   compute capability. For example, a sm_90a kernel can run on a sm_90 GPU,\n+//   but not on a sm_91 GPU.\n+//\n+// 2. The forward compatible feature extension \"f\":\n+//   Kernels compiled with this extension can only be run on GPUs with the same\n+//   major version and a later or same minor version. For example, a sm_100f\n+//   kernel can run on a sm_100 or sm_103 GPUs, but not on a sm_120 GPU.\n struct CudaComputeCapability {\n   int major = 0;\n   int minor = 0;\n \n+  enum class FeatureExtension : uint8_t {\n+    kNone,  // No additional features - Generated PTX will run on all GPUs with\n+            // a higher compute capability. Example: sm_90\n+    kAcceleratedFeatures,  // Enables features that only work on GPUs with the\n+                           // same compute capability. Example: sm_90a\n+    kForwardCompatibleFeatures  // Enables features that only work on GPUs\n+                                // within the same major version and a later\n+                                // minor version. Example: sm_100f\n+  };\n+  FeatureExtension feature_extension = FeatureExtension::kNone;\n+\n   // MSVC does not like \"PASCAL\" symbol.\n   enum CudaComputeCapabilities {\n     kPascal = 6,\n@@ -42,53 +73,89 @@ struct CudaComputeCapability {\n   };\n \n   constexpr CudaComputeCapability() = default;\n-  constexpr CudaComputeCapability(int major, int minor) {\n-    this->major = major;\n-    this->minor = minor;\n-  }\n+  constexpr CudaComputeCapability(int major, int minor)\n+      : CudaComputeCapability(major, minor, FeatureExtension::kNone) {}\n \n-  // Parses the architecture name in the format \"major.minor\", example: \"8.6\".\n+  constexpr CudaComputeCapability(int major, int minor,\n+                                  FeatureExtension feature_extension)\n+      : major{major}, minor{minor}, feature_extension{feature_extension} {}\n+\n+  static absl::StatusOr<CudaComputeCapability> FromProto(\n+      const CudaComputeCapabilityProto& proto);\n+\n+  // Parses the architecture name in the format\n+  // \"major.minor<feature_extension>\", example: \"8.6\" or \"9.0a\" or \"10.0f\".\n   static absl::StatusOr<CudaComputeCapability> FromString(\n       absl::string_view cuda_arch_name);\n \n   // Returns a CudaComputeCapability with the given major and minor versions\n   // and the accelerated feature extension enabled if supported.\n-  // This function only exists for forward compatibility reasons.\n-  // It will preserve the behaviour of automatically enabling the accelerated\n-  // feature extension for newer compute capabilities, even once\n-  // `CudaComputeCapability` supports extensions natively.\n+  // This function only exists for backwards compatibility reasons.\n   // TODO(hebecker): Remove this function once extensions are supported\n   // natively and all users have been migrated.\n   static CudaComputeCapability FromIntWithAutoFeatureExtension(int major,\n                                                                int minor);\n \n-  explicit CudaComputeCapability(const CudaComputeCapabilityProto &proto) {\n-    this->major = proto.major();\n-    this->minor = proto.minor();\n+  constexpr static CudaComputeCapability Pascal() {\n+    return CudaComputeCapability{kPascal, 0};\n   }\n \n-  static CudaComputeCapability Volta() {\n+  constexpr static CudaComputeCapability Volta() {\n     return CudaComputeCapability{kVolta, 0};\n   }\n \n-  static CudaComputeCapability Ampere() {\n+  constexpr static CudaComputeCapability Ampere() {\n     return CudaComputeCapability{kAmpere, 0};\n   }\n \n-  static CudaComputeCapability Hopper() {\n-    return CudaComputeCapability{kHopper, 0};\n+  // Includes all GPUs with compute capability 9.0, notably H100, H200, and\n+  // GH200. When comparing with `IsAtLeast` this will only be true for GPUs with\n+  // compute capability 9.0.\n+  constexpr static CudaComputeCapability H100Family() {\n+    return CudaComputeCapability{kHopper, 0,\n+                                 FeatureExtension::kAcceleratedFeatures};\n+  }\n+\n+  // Includes all GPUs with compute capability 9.x. When comparing with\n+  // `IsAtLeast` this will return true for all compute capabilities of at\n+  // least 9.0.\n+  constexpr static CudaComputeCapability Hopper() {\n+    return CudaComputeCapability{kHopper, 0, FeatureExtension::kNone};\n+  }\n+\n+  // Includes all GPUs with compute capability 10.0, notably B200 and GB200.\n+  // When comparing with `IsAtLeast` this will only be true for GPUs with\n+  // compute capability 10.0.\n+  constexpr static CudaComputeCapability B200Family() {\n+    return CudaComputeCapability{kBlackwell, 0,\n+                                 FeatureExtension::kAcceleratedFeatures};\n   }\n \n-  static CudaComputeCapability Blackwell() {\n-    return CudaComputeCapability{kBlackwell, 0};\n+  // Includes all GPUs with compute capability 10.x. When comparing with\n+  // `IsAtLeast` this will true for all compute capabilities of 10.0 or higher.\n+  constexpr static CudaComputeCapability Blackwell() {\n+    return CudaComputeCapability{kBlackwell, 0, FeatureExtension::kNone};\n   }\n \n+  // Includes all GPUs with compute capability 10.x. When comparing with\n+  // `IsAtLeast` this will true for all 10.x compute capabilities but not for\n+  // compute capabilities with a higher major version.\n+  constexpr static CudaComputeCapability BlackwellGenerationOnly() {\n+    return CudaComputeCapability{kBlackwell, 0,\n+                                 FeatureExtension::kForwardCompatibleFeatures};\n+  }\n+\n+  // Returns true if the compute capability is at least\n+  // `other_major.other_minor`. It is equivalent to\n+  // this->SupportsAllFeaturesOf(CudaComputeCapability{other_major,\n+  // other_minor}).\n   bool IsAtLeast(int other_major, int other_minor = 0) const {\n-    return IsAtLeast(CudaComputeCapability{other_major, other_minor});\n+    return SupportsAllFeaturesOf(\n+        CudaComputeCapability{other_major, other_minor});\n   }\n \n-  bool IsAtLeast(const CudaComputeCapability &cc) const {\n-    return !(*this < cc);\n+  bool IsAtLeastPascal() const {\n+    return major >= CudaComputeCapabilities::kPascal;\n   }\n \n   bool IsAtLeastVolta() const {\n@@ -109,53 +176,80 @@ struct CudaComputeCapability {\n     return major >= CudaComputeCapabilities::kBlackwell;\n   }\n \n+  bool IsPascal() const { return major == CudaComputeCapabilities::kPascal; }\n+\n+  bool IsVolta() const { return major == CudaComputeCapabilities::kVolta; }\n+\n   bool IsAmpere() const { return major == CudaComputeCapabilities::kAmpere; }\n \n+  bool IsAda() const {\n+    constexpr int kAdaMinor = 9;\n+    return major == CudaComputeCapabilities::kAmpere && minor == kAdaMinor;\n+  }\n+\n   bool IsHopper() const { return major == CudaComputeCapabilities::kHopper; }\n \n   bool IsBlackwell() const {\n     return major == CudaComputeCapabilities::kBlackwell;\n   }\n \n-  bool operator<(const CudaComputeCapability &other) const {\n-    return ToPair() < other.ToPair();\n+  // Returns true if a kernel compiled for compute capability `other` can be run\n+  // on a GPU with compute capability `this`.\n+  bool SupportsAllFeaturesOf(const CudaComputeCapability& other) const {\n+    switch (other.feature_extension) {\n+      case FeatureExtension::kNone:\n+        return std::tie(major, minor) >= std::tie(other.major, other.minor);\n+      case FeatureExtension::kAcceleratedFeatures:\n+        return std::tie(major, minor) == std::tie(other.major, other.minor);\n+      case FeatureExtension::kForwardCompatibleFeatures:\n+        return major == other.major && minor >= other.minor;\n+    }\n   }\n \n-  bool operator==(const CudaComputeCapability &other) const {\n-    return ToPair() == other.ToPair();\n+  // Returns true if a kernel compiled for compute capability `this` can be run\n+  // on a GPU with compute capability `other`.\n+  bool CanRunOn(const CudaComputeCapability& other) const {\n+    return other.SupportsAllFeaturesOf(*this);\n   }\n \n-  bool operator!=(const CudaComputeCapability &other) const {\n-    return !(*this == other);\n-  }\n-\n-  bool operator>(const CudaComputeCapability &other) const {\n-    return ToPair() > other.ToPair();\n-  }\n+  // Returns a string representation of the compute capability. The format is\n+  // not guaranteed to follow any standard and should only be used for logging.\n+  std::string ToString() const;\n \n-  bool operator>=(const CudaComputeCapability &other) const {\n-    return ToPair() >= other.ToPair();\n+  friend bool operator==(const CudaComputeCapability& lhs,\n+                         const CudaComputeCapability& rhs) {\n+    return std::tie(lhs.major, lhs.minor, lhs.feature_extension) ==\n+           std::tie(rhs.major, rhs.minor, rhs.feature_extension);\n   }\n \n-  bool operator<=(const CudaComputeCapability &other) const {\n-    return ToPair() <= other.ToPair();\n+  friend bool operator!=(const CudaComputeCapability& lhs,\n+                         const CudaComputeCapability& rhs) {\n+    return !(lhs == rhs);\n   }\n \n-  std::string ToString() const { return absl::StrCat(major, \".\", minor); }\n-\n-  std::pair<int, int> ToPair() const { return std::make_pair(major, minor); }\n-\n-  CudaComputeCapabilityProto ToProto() const {\n-    CudaComputeCapabilityProto proto;\n-    proto.set_major(major);\n-    proto.set_minor(minor);\n-    return proto;\n-  }\n+  CudaComputeCapabilityProto ToProto() const;\n \n   template <typename H>\n-  friend H AbslHashValue(H state, const CudaComputeCapability &cc) {\n-    return H::combine(std::move(state), cc.major, cc.minor);\n+  friend H AbslHashValue(H state, const CudaComputeCapability& cc) {\n+    return H::combine(std::move(state), cc.major, cc.minor,\n+                      cc.feature_extension);\n   }\n+\n+  // Represents the compile mode as it can be passed to tools like NVCC,\n+  // ptxas, or nvprune.\n+  enum class CompileMode : uint8_t {\n+    kPtx,  // This means nvcc/ptxas will generate PTX for the given compute\n+           // architecture.\n+    kLto,  // This means nvcc/ptxas will generate NVVM-IR for Link Time\n+           // Optimization.\n+    kSass  // This means nvcc/ptxas will generate SASS for the given compute\n+           // architecture.\n+  };\n+\n+  // Returns the target identifier that can be passed to ptxas's `--gpu-name`\n+  // option.\n+  std::string GetPtxAsTargetName(\n+      CompileMode compile_mode = CompileMode::kSass) const;\n };\n \n }  // namespace stream_executor"
        },
        {
            "sha": "9c5a6446f843fba670727ee20933e4d4e615db25",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_compute_capability.proto",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability.proto?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -20,4 +20,13 @@ package stream_executor;\n message CudaComputeCapabilityProto {\n   int32 major = 1;\n   int32 minor = 2;\n+\n+  enum FeatureExtension {\n+    UNSPECIFIED = 0;\n+    NONE = 1;\n+    ACCELERATED_FEATURES = 2;\n+    FORWARD_COMPATIBLE_FEATURES = 3;\n+  }\n+\n+  FeatureExtension feature_extension = 3;\n }"
        },
        {
            "sha": "3c07b22e464b244db2c222239d0f5fa55cc76707",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_compute_capability_test.cc",
            "status": "modified",
            "additions": 214,
            "deletions": 44,
            "changes": 258,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_compute_capability_test.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -19,104 +19,274 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/hash/hash_testing.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.pb.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n \n namespace stream_executor {\n namespace {\n-using tsl::testing::IsOkAndHolds;\n-using tsl::testing::StatusIs;\n+using absl_testing::IsOkAndHolds;\n+using absl_testing::StatusIs;\n \n TEST(CudaComputeCapabilityTest, ToString) {\n+  EXPECT_EQ(CudaComputeCapability(\n+                100, 52, CudaComputeCapability::FeatureExtension::kNone)\n+                .ToString(),\n+            \"100.52\");\n+  // For all compute capabilities of at least 9.x we expect an accelerated\n+  // feature set to exist\n   EXPECT_EQ(CudaComputeCapability(100, 52).ToString(), \"100.52\");\n+  EXPECT_EQ(CudaComputeCapability(\n+                100, 52,\n+                CudaComputeCapability::FeatureExtension::kAcceleratedFeatures)\n+                .ToString(),\n+            \"100.52a\");\n+  EXPECT_EQ(\n+      CudaComputeCapability(\n+          100, 52,\n+          CudaComputeCapability::FeatureExtension::kForwardCompatibleFeatures)\n+          .ToString(),\n+      \"100.52f\");\n }\n \n TEST(CudaComputeCapabilityTest, FromString) {\n+  using FeatureExtension = CudaComputeCapability::FeatureExtension;\n+\n   EXPECT_THAT(CudaComputeCapability::FromString(\"100.52\"),\n-              absl_testing::IsOkAndHolds(CudaComputeCapability(100, 52)));\n+              IsOkAndHolds(CudaComputeCapability(100, 52)));\n+  EXPECT_THAT(CudaComputeCapability::FromString(\"100.52a\"),\n+              IsOkAndHolds(CudaComputeCapability(\n+                  100, 52, FeatureExtension::kAcceleratedFeatures)));\n+  EXPECT_THAT(CudaComputeCapability::FromString(\"100.52A\"),\n+              IsOkAndHolds(CudaComputeCapability(\n+                  100, 52, FeatureExtension::kAcceleratedFeatures)));\n+  EXPECT_THAT(CudaComputeCapability::FromString(\"100.52 a\"),\n+              IsOkAndHolds(CudaComputeCapability(\n+                  100, 52, FeatureExtension::kAcceleratedFeatures)));\n+  EXPECT_THAT(CudaComputeCapability::FromString(\"100.52f\"),\n+              IsOkAndHolds(CudaComputeCapability(\n+                  100, 52, FeatureExtension::kForwardCompatibleFeatures)));\n+  EXPECT_THAT(CudaComputeCapability::FromString(\"100.52F\"),\n+              IsOkAndHolds(CudaComputeCapability(\n+                  100, 52, FeatureExtension::kForwardCompatibleFeatures)));\n+  EXPECT_THAT(CudaComputeCapability::FromString(\"100.52 f\"),\n+              IsOkAndHolds(CudaComputeCapability(\n+                  100, 52, FeatureExtension::kForwardCompatibleFeatures)));\n   EXPECT_THAT(CudaComputeCapability::FromString(\"1\"),\n-              absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n+              StatusIs(absl::StatusCode::kInvalidArgument));\n   EXPECT_THAT(CudaComputeCapability::FromString(\"12\"),\n-              absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n+              StatusIs(absl::StatusCode::kInvalidArgument));\n   EXPECT_THAT(CudaComputeCapability::FromString(\"x\"),\n-              absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n+              StatusIs(absl::StatusCode::kInvalidArgument));\n   EXPECT_THAT(CudaComputeCapability::FromString(\"1.\"),\n-              absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n+              StatusIs(absl::StatusCode::kInvalidArgument));\n   EXPECT_THAT(CudaComputeCapability::FromString(\"1.x\"),\n-              absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n+              StatusIs(absl::StatusCode::kInvalidArgument));\n }\n \n TEST(CudaComputeCapabilityTest, FromIntWithAutoFeatureExtension) {\n+  EXPECT_EQ(CudaComputeCapability::FromIntWithAutoFeatureExtension(8, 0),\n+            CudaComputeCapability(8, 0));\n+  EXPECT_EQ(\n+      CudaComputeCapability::FromIntWithAutoFeatureExtension(9, 0),\n+      CudaComputeCapability(\n+          9, 0, CudaComputeCapability::FeatureExtension::kAcceleratedFeatures));\n   EXPECT_EQ(CudaComputeCapability::FromIntWithAutoFeatureExtension(100, 52),\n             CudaComputeCapability(100, 52));\n }\n \n TEST(CudaComputeCapabilityTest, ToProto) {\n-  CudaComputeCapabilityProto proto = CudaComputeCapability(100, 5).ToProto();\n-  EXPECT_EQ(proto.major(), 100);\n-  EXPECT_EQ(proto.minor(), 5);\n+  CudaComputeCapabilityProto proto0 =\n+      CudaComputeCapability(100, 5,\n+                            CudaComputeCapability::FeatureExtension::kNone)\n+          .ToProto();\n+  EXPECT_EQ(proto0.major(), 100);\n+  EXPECT_EQ(proto0.minor(), 5);\n+  EXPECT_EQ(proto0.feature_extension(), CudaComputeCapabilityProto::NONE);\n+  CudaComputeCapabilityProto proto1 =\n+      CudaComputeCapability(\n+          100, 5, CudaComputeCapability::FeatureExtension::kAcceleratedFeatures)\n+          .ToProto();\n+  EXPECT_EQ(proto1.major(), 100);\n+  EXPECT_EQ(proto1.minor(), 5);\n+  EXPECT_EQ(proto1.feature_extension(),\n+            CudaComputeCapabilityProto::ACCELERATED_FEATURES);\n+  CudaComputeCapabilityProto proto2 =\n+      CudaComputeCapability(\n+          100, 5,\n+          CudaComputeCapability::FeatureExtension::kForwardCompatibleFeatures)\n+          .ToProto();\n+  EXPECT_EQ(proto2.major(), 100);\n+  EXPECT_EQ(proto2.minor(), 5);\n+  EXPECT_EQ(proto2.feature_extension(),\n+            CudaComputeCapabilityProto::FORWARD_COMPATIBLE_FEATURES);\n }\n \n-TEST(CudaComputeCapabilityTest, FromProto) {\n+TEST(CudaComputeCapabilityTest, FromProtoWithFeatureExtensionUnspecified) {\n+  using FeatureExtension = CudaComputeCapability::FeatureExtension;\n+\n+  // An unspecified feature extension field should be interpreted as NONE - no\n+  // feature extension enabled.\n   CudaComputeCapabilityProto proto;\n   proto.set_major(100);\n   proto.set_minor(5);\n-  CudaComputeCapability cc(proto);\n+  TF_ASSERT_OK_AND_ASSIGN(auto cc, CudaComputeCapability::FromProto(proto));\n   EXPECT_EQ(cc.major, 100);\n   EXPECT_EQ(cc.minor, 5);\n+  EXPECT_EQ(cc.feature_extension, FeatureExtension::kNone);\n+\n+  // On Hopper we expect accelerated features to be the default as this is how\n+  // XLA treated Hopper GPUs before we could handle feature extensions\n+  // explicitly.\n+  proto.set_major(9);\n+  proto.set_minor(5);\n+  TF_ASSERT_OK_AND_ASSIGN(cc, CudaComputeCapability::FromProto(proto));\n+  EXPECT_EQ(cc.major, 9);\n+  EXPECT_EQ(cc.minor, 5);\n+  EXPECT_EQ(cc.feature_extension, FeatureExtension::kAcceleratedFeatures);\n+\n+  // On Blackwell we expect accelerated features to be the default as this is\n+  // how XLA treated Blackwell GPUs before we could handle feature extensions\n+  // explicitly.\n+  proto.set_major(10);\n+  proto.set_minor(2);\n+  TF_ASSERT_OK_AND_ASSIGN(cc, CudaComputeCapability::FromProto(proto));\n+  EXPECT_EQ(cc.major, 10);\n+  EXPECT_EQ(cc.minor, 2);\n+  EXPECT_EQ(cc.feature_extension, FeatureExtension::kAcceleratedFeatures);\n+}\n+\n+TEST(CudaComputeCapabilityTest, FromProtoWithFeatureExtensionSpecified) {\n+  using FeatureExtension = CudaComputeCapability::FeatureExtension;\n+\n+  CudaComputeCapabilityProto proto;\n+  proto.set_major(100);\n+  proto.set_minor(5);\n+  proto.set_feature_extension(CudaComputeCapabilityProto::ACCELERATED_FEATURES);\n+  TF_ASSERT_OK_AND_ASSIGN(auto cc, CudaComputeCapability::FromProto(proto));\n+  EXPECT_EQ(cc.major, 100);\n+  EXPECT_EQ(cc.minor, 5);\n+  EXPECT_EQ(cc.feature_extension, FeatureExtension::kAcceleratedFeatures);\n }\n \n TEST(CudaComputeCapabilityTest, Hash) {\n+  using FeatureExtension = CudaComputeCapability::FeatureExtension;\n+\n   EXPECT_TRUE(absl::VerifyTypeImplementsAbslHashCorrectly({\n       CudaComputeCapability(0, 0),\n+      CudaComputeCapability(0, 0, FeatureExtension::kAcceleratedFeatures),\n+      CudaComputeCapability(0, 0, FeatureExtension::kForwardCompatibleFeatures),\n       CudaComputeCapability(0, 1),\n+      CudaComputeCapability(0, 1, FeatureExtension::kAcceleratedFeatures),\n+      CudaComputeCapability(0, 1, FeatureExtension::kForwardCompatibleFeatures),\n       CudaComputeCapability(1, 0),\n+      CudaComputeCapability(1, 0, FeatureExtension::kAcceleratedFeatures),\n+      CudaComputeCapability(1, 0, FeatureExtension::kForwardCompatibleFeatures),\n       CudaComputeCapability(1, 1),\n+      CudaComputeCapability(1, 1, FeatureExtension::kAcceleratedFeatures),\n+      CudaComputeCapability(1, 1, FeatureExtension::kForwardCompatibleFeatures),\n   }));\n }\n \n TEST(CudaComputeCapabilityTest, GenerationNumericTest) {\n   EXPECT_TRUE(CudaComputeCapability(7, 5).IsAtLeastVolta());\n   EXPECT_TRUE(CudaComputeCapability(8, 0).IsAtLeastAmpere());\n-  EXPECT_TRUE(CudaComputeCapability(9, 0).IsAtLeastHopper());\n-  EXPECT_TRUE(CudaComputeCapability(10, 0).IsAtLeastBlackwell());\n-}\n-\n-TEST(CudaComputeCapabilityTest, GenerationLiteralTest) {\n-  EXPECT_TRUE(CudaComputeCapability::Volta().IsAtLeast(7));\n-  EXPECT_TRUE(CudaComputeCapability::Ampere().IsAtLeast(8));\n-  EXPECT_TRUE(CudaComputeCapability::Hopper().IsAtLeast(9));\n-  EXPECT_TRUE(CudaComputeCapability::Blackwell().IsAtLeast(10));\n }\n \n TEST(CudaComputeCapabilityTest, ComparisonTest) {\n-  CudaComputeCapability lower{1, 0};\n-  CudaComputeCapability slightly_higher{1, 1};\n-  CudaComputeCapability higher{2, 0};\n+  using FeatureExtension = CudaComputeCapability::FeatureExtension;\n \n-  EXPECT_TRUE(lower == lower);\n-  EXPECT_FALSE(lower == slightly_higher);\n-  EXPECT_FALSE(lower == higher);\n+  CudaComputeCapability base{1, 0};\n+  CudaComputeCapability base_but_accelerated{\n+      1, 0, FeatureExtension::kAcceleratedFeatures};\n+  CudaComputeCapability base_but_forward_compatible{\n+      1, 0, FeatureExtension::kForwardCompatibleFeatures};\n+  CudaComputeCapability newer_but_same_generation{1, 1};\n+  CudaComputeCapability newer_but_same_generation_accelerated{\n+      1, 1, FeatureExtension::kAcceleratedFeatures};\n+  CudaComputeCapability newer_but_same_generation_compatible{\n+      1, 1, FeatureExtension::kForwardCompatibleFeatures};\n+  CudaComputeCapability next_generation{2, 0};\n \n-  EXPECT_TRUE(lower <= lower);\n-  EXPECT_TRUE(lower < slightly_higher);\n-  EXPECT_TRUE(lower <= slightly_higher);\n+  EXPECT_TRUE(base == base);\n+  EXPECT_TRUE(base_but_accelerated == base_but_accelerated);\n+  EXPECT_FALSE(base == base_but_accelerated);\n+  EXPECT_FALSE(base == newer_but_same_generation);\n+  EXPECT_FALSE(base == next_generation);\n \n-  EXPECT_FALSE(lower < lower);\n-  EXPECT_FALSE(slightly_higher <= lower);\n-  EXPECT_FALSE(slightly_higher < lower);\n+  // sm_10 kernels can run sm_10, sm_11, and sm_20 GPUs.\n+  // But sm_10a kernels can only run on sm_10 GPUs.\n+  // sm_10f kernels can run on any sm_10 and sm_11 GPUs.\n+  EXPECT_TRUE(base.CanRunOn(base));\n+  EXPECT_TRUE(base.SupportsAllFeaturesOf(base));\n+  EXPECT_TRUE(base.CanRunOn(newer_but_same_generation));\n+  EXPECT_TRUE(base.CanRunOn(newer_but_same_generation_accelerated));\n+  EXPECT_TRUE(base.CanRunOn(newer_but_same_generation_compatible));\n+  EXPECT_FALSE(base.SupportsAllFeaturesOf(newer_but_same_generation));\n+  EXPECT_FALSE(\n+      base.SupportsAllFeaturesOf(newer_but_same_generation_accelerated));\n+  EXPECT_FALSE(\n+      base.SupportsAllFeaturesOf(newer_but_same_generation_compatible));\n+  EXPECT_TRUE(base.CanRunOn(next_generation));\n+  EXPECT_FALSE(base.SupportsAllFeaturesOf(next_generation));\n \n-  EXPECT_TRUE(slightly_higher >= slightly_higher);\n-  EXPECT_TRUE(slightly_higher > lower);\n-  EXPECT_TRUE(slightly_higher >= lower);\n+  EXPECT_TRUE(base_but_accelerated.CanRunOn(base));\n+  EXPECT_TRUE(base_but_accelerated.SupportsAllFeaturesOf(base));\n+  EXPECT_FALSE(base_but_accelerated.CanRunOn(newer_but_same_generation));\n+  EXPECT_FALSE(\n+      base_but_accelerated.SupportsAllFeaturesOf(newer_but_same_generation));\n+  EXPECT_FALSE(\n+      base_but_accelerated.CanRunOn(newer_but_same_generation_accelerated));\n+  EXPECT_FALSE(base_but_accelerated.SupportsAllFeaturesOf(\n+      newer_but_same_generation_accelerated));\n+  EXPECT_FALSE(\n+      base_but_accelerated.CanRunOn(newer_but_same_generation_compatible));\n+  EXPECT_FALSE(base_but_accelerated.SupportsAllFeaturesOf(\n+      newer_but_same_generation_compatible));\n+  EXPECT_FALSE(base_but_accelerated.CanRunOn(next_generation));\n+  EXPECT_FALSE(base_but_accelerated.SupportsAllFeaturesOf(next_generation));\n+\n+  EXPECT_TRUE(base_but_forward_compatible.CanRunOn(base));\n+  EXPECT_TRUE(base_but_forward_compatible.SupportsAllFeaturesOf(base));\n+  EXPECT_TRUE(base_but_forward_compatible.CanRunOn(newer_but_same_generation));\n+  EXPECT_FALSE(base_but_forward_compatible.SupportsAllFeaturesOf(\n+      newer_but_same_generation));\n+  EXPECT_TRUE(base_but_forward_compatible.CanRunOn(\n+      newer_but_same_generation_accelerated));\n+  EXPECT_FALSE(base_but_forward_compatible.SupportsAllFeaturesOf(\n+      newer_but_same_generation_accelerated));\n+  EXPECT_TRUE(base_but_forward_compatible.CanRunOn(\n+      newer_but_same_generation_compatible));\n+  EXPECT_FALSE(base_but_forward_compatible.SupportsAllFeaturesOf(\n+      newer_but_same_generation_compatible));\n+  EXPECT_FALSE(base_but_forward_compatible.CanRunOn(next_generation));\n+  EXPECT_FALSE(\n+      base_but_forward_compatible.SupportsAllFeaturesOf(next_generation));\n+}\n \n-  EXPECT_FALSE(slightly_higher > slightly_higher);\n-  EXPECT_FALSE(lower > slightly_higher);\n-  EXPECT_FALSE(lower >= slightly_higher);\n+TEST(CudaComputeCapabilityTest, GetPtxAsTargetName) {\n+  EXPECT_EQ(CudaComputeCapability::Ampere().GetPtxAsTargetName(\n+                CudaComputeCapability::CompileMode::kPtx),\n+            \"compute_80\");\n+  EXPECT_EQ(CudaComputeCapability::Ampere().GetPtxAsTargetName(\n+                CudaComputeCapability::CompileMode::kLto),\n+            \"lto_80\");\n+  EXPECT_EQ(CudaComputeCapability::Ampere().GetPtxAsTargetName(\n+                CudaComputeCapability::CompileMode::kSass),\n+            \"sm_80\");\n \n-  EXPECT_TRUE(higher > slightly_higher);\n-  EXPECT_TRUE(higher >= slightly_higher);\n-  EXPECT_TRUE(higher >= higher);\n+  EXPECT_EQ(CudaComputeCapability::Hopper().GetPtxAsTargetName(), \"sm_90\");\n+  EXPECT_EQ(\n+      CudaComputeCapability(\n+          9, 0, CudaComputeCapability::FeatureExtension::kAcceleratedFeatures)\n+          .GetPtxAsTargetName(),\n+      \"sm_90a\");\n+  EXPECT_EQ(\n+      CudaComputeCapability(\n+          10, 0,\n+          CudaComputeCapability::FeatureExtension::kForwardCompatibleFeatures)\n+          .GetPtxAsTargetName(),\n+      \"sm_100f\");\n }\n \n }  // namespace"
        },
        {
            "sha": "be937f4b31b854897561d4c089f583e54fd8057f",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 26,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -53,6 +53,7 @@ limitations under the License.\n #include \"xla/stream_executor/blas.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n #include \"xla/stream_executor/cuda/cuda_command_buffer.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/cuda/cuda_context.h\"\n #include \"xla/stream_executor/cuda/cuda_event.h\"\n #include \"xla/stream_executor/cuda/cuda_kernel.h\"\n@@ -274,16 +275,21 @@ absl::StatusOr<std::string> GetDeviceName(CUdevice device) {\n }\n \n // Returns the compute capability for the device; i.e (3, 5).\n-absl::Status GetComputeCapability(int* cc_major, int* cc_minor,\n-                                  CUdevice device) {\n-  *cc_major = 0;\n-  *cc_minor = 0;\n+absl::StatusOr<CudaComputeCapability> GetComputeCapability(CUdevice device) {\n+  int cc_major = 0;\n+  TF_RETURN_IF_ERROR(cuda::ToStatus(cuDeviceGetAttribute(\n+      &cc_major, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, device)));\n \n+  int cc_minor = 0;\n   TF_RETURN_IF_ERROR(cuda::ToStatus(cuDeviceGetAttribute(\n-      cc_major, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, device)));\n+      &cc_minor, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR, device)));\n \n-  return cuda::ToStatus(cuDeviceGetAttribute(\n-      cc_minor, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR, device));\n+  bool has_accelerated_features = cc_major >= 9;\n+  return CudaComputeCapability(\n+      cc_major, cc_minor,\n+      has_accelerated_features\n+          ? CudaComputeCapability::FeatureExtension::kAcceleratedFeatures\n+          : CudaComputeCapability::FeatureExtension::kNone);\n }\n \n // Helper function that turns the integer output of cuDeviceGetAttribute to type\n@@ -723,7 +729,6 @@ absl::Status CudaExecutor::Init() {\n   TF_ASSIGN_OR_RETURN(CudaContext * context,\n                       CudaContext::Create(device_ordinal(), device_));\n   cuda_context_ = context;\n-  TF_RETURN_IF_ERROR(GetComputeCapability(&cc_major_, &cc_minor_, device_));\n   TF_ASSIGN_OR_RETURN(delay_kernels_supported_, DelayKernelIsSupported());\n   numa_node_ = ReadNumaNode(GetPCIBusID(device_), device_ordinal())\n                    .value_or(tsl::port::kNUMANoAffinity);\n@@ -805,10 +810,6 @@ absl::StatusOr<std::unique_ptr<Kernel>> CudaExecutor::LoadKernel(\n     cuda_kernel->set_gpu_function(function);\n \n   } else if (spec.has_cuda_ptx_in_memory()) {\n-    if (cc_major_ == 0 && cc_minor_ == 0) {\n-      return absl::InternalError(\"Compute capability not set\");\n-    }\n-\n     const char* ptx = spec.cuda_ptx_in_memory()->ptx.data();\n     if (ptx == nullptr) {\n       LOG(FATAL) << \"Loader spec has no ptx for kernel \" << kernel_name;\n@@ -919,10 +920,6 @@ absl::StatusOr<ModuleHandle> CudaExecutor::LoadModule(\n     return LoadModuleFromCuBin(\n         reinterpret_cast<const char*>(spec.cuda_cubin_in_memory().data()));\n   } else if (spec.has_cuda_ptx_in_memory()) {\n-    if (cc_major_ == 0 && cc_minor_ == 0) {\n-      return absl::InternalError(\"Compute capability not set\");\n-    }\n-\n     if (!spec.cuda_ptx_in_memory()) {\n       return absl::InternalError(\"PTX not found in spec\");\n     }\n@@ -1279,10 +1276,7 @@ CudaExecutor::CreateCommandBuffer(CommandBuffer::Mode mode) {\n absl::StatusOr<std::unique_ptr<DeviceDescription>>\n CudaExecutor::CreateDeviceDescription(int device_ordinal) {\n   TF_ASSIGN_OR_RETURN(CUdevice device, GetDevice(device_ordinal));\n-\n-  int cc_major;\n-  int cc_minor;\n-  TF_RETURN_IF_ERROR(GetComputeCapability(&cc_major, &cc_minor, device));\n+  TF_ASSIGN_OR_RETURN(CudaComputeCapability cc, GetComputeCapability(device));\n \n   DeviceDescription desc;\n   int32_t driver_version{};\n@@ -1381,22 +1375,21 @@ CudaExecutor::CreateDeviceDescription(int device_ordinal) {\n     desc.set_name(device_name);\n   }\n \n-  desc.set_platform_version(\n-      absl::StrCat(\"Compute Capability \", cc_major, \".\", cc_minor));\n+  desc.set_platform_version(absl::StrCat(\"Compute Capability \", cc.ToString()));\n \n   // TODO(leary) should be a way to query this from the driver, but this is\n   // unlikely to change for us any time soon.\n   desc.set_device_address_bits(64);\n \n   desc.set_device_vendor(\"NVIDIA Corporation\");\n-  desc.set_cuda_compute_capability(cc_major, cc_minor);\n+  desc.set_cuda_compute_capability(cc);\n   desc.set_shared_memory_per_core(GetMaxSharedMemoryPerCore(device).value());\n   desc.set_shared_memory_per_block(GetMaxSharedMemoryPerBlock(device).value());\n   desc.set_shared_memory_per_block_optin(\n       GetMaxSharedMemoryPerBlockOptin(device).value());\n   int core_count = GetMultiprocessorCount(device).value();\n   desc.set_core_count(core_count);\n-  desc.set_fpus_per_core(fpus_per_core(cc_major, cc_minor));\n+  desc.set_fpus_per_core(fpus_per_core(cc.major, cc.minor));\n   desc.set_threads_per_core_limit(\n       GetMaxThreadsPerMultiprocessor(device).value());\n   desc.set_registers_per_block_limit(GetMaxRegistersPerBlock(device).value());\n@@ -1420,8 +1413,8 @@ CudaExecutor::CreateDeviceDescription(int device_ordinal) {\n   //\n   // For now, this identifier is good enough.\n   desc.set_model_str(absl::StrFormat(\n-      \"sm_%d.%d with %dB RAM, %d cores, %dKHz clock, %dKHz mem clock, %dB L2$\",\n-      cc_major, cc_minor, device_memory_size, core_count, sm_clock_khz,\n+      \"sm_%s with %dB RAM, %d cores, %dKHz clock, %dKHz mem clock, %dB L2$\",\n+      cc.ToString(), device_memory_size, core_count, sm_clock_khz,\n       value_or(mem_clock_khz, 0), l2_cache_bytes));\n \n   return std::make_unique<DeviceDescription>(std::move(desc));"
        },
        {
            "sha": "501563f04d8a6b83502f10fc64e58a1a851f0015",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.h",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -100,8 +100,6 @@ class CudaExecutor : public GpuExecutor {\n       std::optional<std::variant<StreamPriority, int>> priority) override;\n   absl::StatusOr<std::unique_ptr<CommandBuffer>> CreateCommandBuffer(\n       CommandBuffer::Mode mode) override;\n-  int cc_major() const { return cc_major_; }\n-  int cc_minor() const { return cc_minor_; }\n \n   absl::StatusOr<std::unique_ptr<DeviceDescription>> CreateDeviceDescription()\n       const override {\n@@ -184,12 +182,6 @@ class CudaExecutor : public GpuExecutor {\n   // True if delay kernels are supported.\n   bool delay_kernels_supported_ = false;\n \n-  // The major version of the compute capability for device_.\n-  int cc_major_;\n-\n-  // The minor version of the compute capability for device_.\n-  int cc_minor_;\n-\n   // The NUMA node of the CPU closest to device_\n   int numa_node_;\n "
        },
        {
            "sha": "cd5d9d6dfc7c4e1206e5e59f5c7ec7ca787b323b",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_test.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -67,9 +67,9 @@ TEST(CudaExecutorTest, CreateDeviceDescription) {\n   EXPECT_THAT(result->model_str(), Not(IsEmpty()));\n   EXPECT_THAT(result->device_vendor(), \"NVIDIA Corporation\");\n \n-  EXPECT_THAT(\n-      result->gpu_compute_capability(),\n-      VariantWith<CudaComputeCapability>(Ge(CudaComputeCapability{1, 0})));\n+  EXPECT_THAT(result->gpu_compute_capability(),\n+              VariantWith<CudaComputeCapability>(::testing::Field(\n+                  \"major\", &CudaComputeCapability::major, Ge(1))));\n }\n \n TEST(CudaExecutorTest, GetCudaKernel) {"
        },
        {
            "sha": "b2a938bf65e48bc924a8b553fc2d86fbd769543f",
            "filename": "third_party/xla/xla/stream_executor/cuda/driver_compilation_provider.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 3,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fdriver_compilation_provider.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fdriver_compilation_provider.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fdriver_compilation_provider.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -87,11 +87,19 @@ absl::StatusOr<Assembly> DriverCompilationProvider::CompileAndLink(\n #if CUDA_VERSION >= 12000\n   // Even though CUDA 11.8 has Hopper support, SM 9.0a and most Hopper features\n   // (WGMMA, TMA, and more) are only supported in CUDA 12+.\n-  if (cc.major == 9 && cc.minor == 0) {\n+  if (cc.feature_extension ==\n+      CudaComputeCapability::FeatureExtension::kAcceleratedFeatures) {\n     target =\n         static_cast<CUjit_target>(target + CU_COMPUTE_ACCELERATED_TARGET_BASE);\n   }\n #endif\n+\n+  if (cc.feature_extension ==\n+      CudaComputeCapability::FeatureExtension::kForwardCompatibleFeatures) {\n+    return absl::UnimplementedError(\n+        \"Compiling forward compatible kernels is not implemented yet.\");\n+  }\n+\n   constexpr size_t kErrorLogBufferSize = 512 * 1024;  // 4 KiB\n   std::string error_log_buffer(kErrorLogBufferSize, '\\0');\n \n@@ -166,8 +174,8 @@ absl::StatusOr<Assembly> DriverCompilationProvider::CompileAndLink(\n   CHECK(info_log_buffer_size() <= kInfoLogBufferSize);\n   info_log_buffer.resize(info_log_buffer_size());\n \n-  absl::string_view extension = ShouldUsePtxExtension(cc) ? \"a\" : \"\";\n-  std::string architecture = absl::StrCat(\"sm_\", cc.major, cc.minor, extension);\n+  std::string architecture =\n+      cc.GetPtxAsTargetName(CudaComputeCapability::CompileMode::kSass);\n \n   if (result != CUDA_SUCCESS) {\n     VLOG(3) << \"Driver compilation error log output: \" << error_log_buffer;"
        },
        {
            "sha": "8e2df8eb332d7f9f7dc65923b0fb6b49a70d6bde",
            "filename": "third_party/xla/xla/stream_executor/cuda/nvjitlink_impl.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvjitlink_impl.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvjitlink_impl.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fnvjitlink_impl.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -139,11 +139,7 @@ absl::StatusOr<cuda::Assembly> CompileAndLinkUsingLibNvJitLink(\n   WarnIfBadPtxasVersion(\"nvJitLink\", cc, {version_major, version_minor, 0});\n \n   std::vector<std::string> cli_args;\n-  // On Hopper, default to sm_90a so that all instructions can be used. But\n-  // only sm_90 is forward compatible, so don't use sm_90a with newer hardware:\n-  // https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#ptx-compatibility\n-  absl::string_view extension = ShouldUsePtxExtension(cc) ? \"a\" : \"\";\n-  std::string architecture = absl::StrCat(\"sm_\", cc.major, cc.minor, extension);\n+  const std::string architecture = cc.GetPtxAsTargetName();\n   cli_args.emplace_back(absl::StrCat(\"-arch=\", architecture));\n \n   if (VLOG_IS_ON(2) || dump_compilation_log) {"
        },
        {
            "sha": "1ce308b2634f4c61d6d9a1760ca19700ffe309ef",
            "filename": "third_party/xla/xla/stream_executor/cuda/ptx_compiler_helpers.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_helpers.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -145,13 +145,4 @@ void WarnIfBadPtxasVersion(absl::string_view method,\n   });\n }\n \n-// Extension is used for compute capabilities 9.0, 10.0/10.1/10.3 and 12.0/12.1\n-// https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#ptx-compatibility\n-bool ShouldUsePtxExtension(const CudaComputeCapability& cc) {\n-  return (cc.major == 9 && cc.minor == 0) ||\n-         (cc.major == 10 &&\n-          (cc.minor == 0 || cc.minor == 1 || cc.minor == 3)) ||\n-         (cc.major == 12 && (cc.minor == 0 || cc.minor == 1));\n-}\n-\n }  // namespace stream_executor"
        },
        {
            "sha": "b69eb015e1263930cc3f7160e2fd84ef9dd54313",
            "filename": "third_party/xla/xla/stream_executor/cuda/ptx_compiler_helpers.h",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_helpers.h?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -46,14 +46,6 @@ void WarnIfBadPtxasVersion(absl::string_view method,\n                            const CudaComputeCapability& cc,\n                            SemanticVersion compiler_version);\n \n-// Determines whether the PTX extension for a compute capability should be used.\n-//\n-// Returns true if the argument compute capability has PTX extensions that are\n-// only valid for that compute capability. For example, \"sm_90\" only includes\n-// features that are forward compatible, whereas \"sm_90a\" (the extension) also\n-// includes Hopper-specific features, such as WGMMA. We want to use the latter.\n-bool ShouldUsePtxExtension(const CudaComputeCapability& cc);\n-\n // Determines the latest supported PTX ISA from an \"unsupported version\" error\n // log issued by ptxas.\n //"
        },
        {
            "sha": "c421902f1ed2d3f00971db822f603c8f46918950",
            "filename": "third_party/xla/xla/stream_executor/cuda/ptx_compiler_helpers_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_helpers_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_helpers_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_helpers_test.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -18,14 +18,11 @@ limitations under the License.\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"tsl/platform/status_matchers.h\"\n-#include \"tsl/platform/test.h\"\n \n namespace stream_executor {\n namespace {\n-using ::tsl::testing::IsOk;\n-using ::tsl::testing::StatusIs;\n \n // When the compilation succeeds, then the error log is empty.\n constexpr absl::string_view kPtxasLogSuccessfulCompilation = R\"("
        },
        {
            "sha": "50c735064ca0edbe44ae1134b6ba7896862b40aa",
            "filename": "third_party/xla/xla/stream_executor/cuda/ptx_compiler_impl.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_impl.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_impl.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fptx_compiler_impl.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -97,13 +97,9 @@ absl::StatusOr<cuda::Assembly> CompileGpuAsmUsingLibNvPtxCompiler(\n   absl::Cleanup compiler_cleaner = [&compiler_handle] {\n     nvPTXCompilerDestroy(&compiler_handle);\n   };\n-  // On Hopper, default to sm_90a so that all instructions can be used. But\n-  // only sm_90 is forward compatible, so don't use sm_90a with newer hardware:\n-  // https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#ptx-compatibility\n-  absl::string_view extension = ShouldUsePtxExtension(cc) ? \"a\" : \"\";\n-  std::string architecture = absl::StrCat(\"sm_\", cc.major, cc.minor, extension);\n \n-  options.extra_flags.emplace_back(absl::StrCat(\"-arch=\", architecture));\n+  options.extra_flags.emplace_back(\n+      absl::StrCat(\"-arch=\", cc.GetPtxAsTargetName()));\n   options.extra_flags.emplace_back(\"--warn-on-spills\");\n \n   if (VLOG_IS_ON(2) || dump_compilation_log) {\n@@ -143,8 +139,9 @@ absl::StatusOr<cuda::Assembly> CompileGpuAsmUsingLibNvPtxCompiler(\n     //      ptxas fatal   : Value 'sm_80' is not defined for option 'gpu-name'\n     if (absl::StrContains(*error_log, \"ptxas fatal   : Value '\") &&\n         absl::StrContains(*error_log, \"is not defined for option 'gpu-name'\")) {\n-      return absl::UnimplementedError(absl::StrFormat(\n-          \"Linked libnvptxcompiler is too old for %s.\", architecture));\n+      return absl::UnimplementedError(\n+          absl::StrFormat(\"Linked libnvptxcompiler is too old for %s.\",\n+                          cc.GetPtxAsTargetName()));\n     }\n     if (IsPtxRegisterAllocationError(*error_log)) {\n       return PtxRegisterAllocationError(*error_log);"
        },
        {
            "sha": "85532dc922730860c9d713c9b0583f0637b1618d",
            "filename": "third_party/xla/xla/stream_executor/cuda/subprocess_compilation.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fsubprocess_compilation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fsubprocess_compilation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fsubprocess_compilation.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -294,16 +294,13 @@ absl::StatusOr<cuda::Assembly> CompileGpuAsmUsingPtxAs(\n     tsl::Env::Default()->DeleteFile(cubin_path).IgnoreError();\n   };\n   tsl::SubProcess ptxas_info_dumper;\n-  // On Hopper, default to sm_90a so that all instructions can be used. But\n-  // only sm_90 is forward compatible, so don't use sm_90a with newer hardware:\n-  // https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#ptx-compatibility\n-  std::string extension = ShouldUsePtxExtension(cc) ? \"a\" : \"\";\n   std::vector<std::string> ptxas_args = {\n       std::string{ptxas_path},\n       ptx_path,\n       \"-o\",\n       cubin_path,\n-      absl::StrCat(\"-arch=sm_\", cc.major, cc.minor, extension),\n+      absl::StrCat(\"-arch=\", cc.GetPtxAsTargetName(\n+                                 CudaComputeCapability::CompileMode::kSass)),\n       \"--warn-on-spills\"};\n   if (VLOG_IS_ON(2) || dump_compilation_log) {\n     ptxas_args.push_back(\"-v\");\n@@ -522,8 +519,7 @@ absl::StatusOr<std::vector<uint8_t>> LinkUsingNvlink(\n   };\n   std::vector<std::string> args;\n   args.push_back(std::string{nvlink_path});\n-  absl::string_view extension = ShouldUsePtxExtension(cc) ? \"a\" : \"\";\n-  args.push_back(absl::StrCat(\"-arch=sm_\", cc.major, cc.minor, extension));\n+  args.push_back(absl::StrCat(\"-arch=\", cc.GetPtxAsTargetName()));\n   for (int i = 0; i < images.size(); i++) {\n     args.push_back(temp_files[i]);\n   }"
        },
        {
            "sha": "a63bc439834e07e519572a4696ba6baa39461687",
            "filename": "third_party/xla/xla/stream_executor/device_description.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_description.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_description.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_description.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"xla/stream_executor/device_description.pb.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/tsl/lib/math/math_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n \n namespace stream_executor {\n \n@@ -51,8 +52,9 @@ absl::StatusOr<DeviceDescription> DeviceDescription::FromProto(\n   device_description.clock_rate_ghz_ = proto.clock_rate_ghz();\n \n   if (proto.has_cuda_compute_capability()) {\n-    device_description.gpu_compute_capability_ =\n-        CudaComputeCapability(proto.cuda_compute_capability());\n+    TF_ASSIGN_OR_RETURN(\n+        device_description.gpu_compute_capability_,\n+        CudaComputeCapability::FromProto(proto.cuda_compute_capability()));\n   }\n   if (proto.has_rocm_compute_capability()) {\n     device_description.gpu_compute_capability_ ="
        },
        {
            "sha": "cb8163d494c0a30991eec534c30866bc281e2728",
            "filename": "third_party/xla/xla/stream_executor/device_description.h",
            "status": "modified",
            "additions": 32,
            "deletions": 32,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_description.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_description.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_description.h?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -48,7 +48,7 @@ class RocmComputeCapability {\n   explicit RocmComputeCapability(std::string gcn_arch_name)\n       : gcn_arch_name_(std::move(gcn_arch_name)) {}\n \n-  explicit RocmComputeCapability(const RocmComputeCapabilityProto &proto)\n+  explicit RocmComputeCapability(const RocmComputeCapabilityProto& proto)\n       : gcn_arch_name_(proto.gcn_arch_name()) {}\n \n   RocmComputeCapability() = default;\n@@ -144,7 +144,7 @@ class RocmComputeCapability {\n     return proto;\n   }\n \n-  bool operator==(const RocmComputeCapability &other) const {\n+  bool operator==(const RocmComputeCapability& other) const {\n     return gcn_arch_name_ == other.gcn_arch_name_;\n   }\n \n@@ -178,7 +178,7 @@ class DeviceDescription {\n   // Returns the platform being run on; this value is primarily intended for\n   // printing, and comes out something like \"OpenCL 1.2\" or \"Compute Capability\n   // 3.5\".\n-  const std::string &platform_version() const { return platform_version_; }\n+  const std::string& platform_version() const { return platform_version_; }\n \n   // Returns the driver version interfacing with the underlying platform.\n   // Note for CUDA this returns the CUDA Toolkit version the driver ships with.\n@@ -193,7 +193,7 @@ class DeviceDescription {\n   }\n \n   // Returns the name that the device reports. Vendor dependent.\n-  const std::string &name() const { return name_; }\n+  const std::string& name() const { return name_; }\n \n   // Gets a human-readable description of the device, e.g. \"nvidia GPU\n   // supporting sm75 with 32GB RAM, 80 SMs, ...\".  This is intended to be the\n@@ -203,11 +203,11 @@ class DeviceDescription {\n   // This string is not guaranteed to be stable between versions.  Please DO NOT\n   // rely on it never changing.  (Within one version of the code, it won't\n   // change, don't worry.)\n-  const std::string &model_str() const { return model_str_; }\n+  const std::string& model_str() const { return model_str_; }\n \n   // Returns the PCI bus identifier for this device, of the form\n   // [domain]:[bus]:[device].[function]\n-  const std::string &pci_bus_id() const { return pci_bus_id_; }\n+  const std::string& pci_bus_id() const { return pci_bus_id_; }\n \n   // Returns the NUMA node associated with this device, for use in\n   // determining socket locality. If the NUMA node could not be determined, -1\n@@ -225,44 +225,44 @@ class DeviceDescription {\n   // Returns the limit on the thread dimensionality values in each of the\n   // respective dimensions. These limits affect what constitutes a legitimate\n   // kernel launch request.\n-  const ThreadDim &thread_dim_limit() const { return thread_dim_limit_; }\n+  const ThreadDim& thread_dim_limit() const { return thread_dim_limit_; }\n \n   // Returns the limit on the block dimensionality values in each of the\n   // respective dimensions. These limits may affect what constitutes a\n   // legitimate kernel launch request.\n-  const BlockDim &block_dim_limit() const { return block_dim_limit_; }\n+  const BlockDim& block_dim_limit() const { return block_dim_limit_; }\n \n   // Returns the limit on the total number of threads that can be launched in a\n   // single block; i.e. the limit on x * y * z dimensions of a ThreadDim.\n   // This limit affects what constitutes a legitimate kernel launch request.\n-  const int64_t &threads_per_block_limit() const {\n+  const int64_t& threads_per_block_limit() const {\n     return threads_per_block_limit_;\n   }\n \n   // Returns the limit on the total number of threads that can be simultaneously\n   // launched on a given multiprocessor.\n-  const int64_t &threads_per_core_limit() const {\n+  const int64_t& threads_per_core_limit() const {\n     return threads_per_core_limit_;\n   }\n \n   // Returns the number of threads per warp/wavefront.\n   constexpr int64_t threads_per_warp() const { return threads_per_warp_; }\n \n   // Returns the limit on the total number of registers per core.\n-  const int64_t &registers_per_core_limit() const {\n+  const int64_t& registers_per_core_limit() const {\n     return registers_per_core_limit_;\n   }\n \n   // Returns the limit on the total number of registers that can be\n   // simultaneously used by a block.\n-  const int64_t &registers_per_block_limit() const {\n+  const int64_t& registers_per_block_limit() const {\n     return registers_per_block_limit_;\n   }\n \n   // Returns the number of address bits available to kernel code running on the\n   // platform. This affects things like the maximum allocation size and perhaps\n   // types used in kernel code such as size_t.\n-  const int64_t &device_address_bits() const { return device_address_bits_; }\n+  const int64_t& device_address_bits() const { return device_address_bits_; }\n \n   // Returns the device memory size in bytes.\n   int64_t device_memory_size() const { return device_memory_size_; }\n@@ -283,7 +283,7 @@ class DeviceDescription {\n \n   // Returns the device vendor string, e.g., \"NVIDIA Corporation\", \"Advanced\n   // Micro Devices, Inc.\", or \"GenuineIntel\".\n-  const std::string &device_vendor() const { return device_vendor_; }\n+  const std::string& device_vendor() const { return device_vendor_; }\n \n   // Returns the CUDA compute capability if we're running on the CUDA platform.\n   // If a CUDA compute capability is not available, the major version will be\n@@ -295,7 +295,7 @@ class DeviceDescription {\n   // be \"gfx000\" (which is an invalid gfx arch).\n   RocmComputeCapability rocm_compute_capability() const;\n \n-  const GpuComputeCapability &gpu_compute_capability() const;\n+  const GpuComputeCapability& gpu_compute_capability() const;\n \n   // Returns the maximum amount of shared memory present on a single core\n   // (i.e. Streaming Multiprocessor on NVIDIA GPUs; Compute Unit for OpenCL\n@@ -319,7 +319,7 @@ class DeviceDescription {\n   // much smaller than the cache size will likely stay in it.\n   constexpr int64_t l1_cache_size_per_SM() const {\n     return std::visit(\n-        [](const auto &capability) -> int64_t {\n+        [](const auto& capability) -> int64_t {\n           if constexpr (std::is_same_v<std::decay_t<decltype(capability)>,\n                                        RocmComputeCapability>) {\n             // MI100 and MI200 has 16KB L1 cache per CU.\n@@ -339,7 +339,7 @@ class DeviceDescription {\n \n   constexpr int64_t dram_to_l2_transaction_size_bytes() const {\n     return std::visit(\n-        [](const auto &capability) -> int {\n+        [](const auto& capability) -> int {\n           if constexpr (std::is_same_v<std::decay_t<decltype(capability)>,\n                                        RocmComputeCapability>) {\n             // DRAM->L2 bus is 128 Byte width for MI300.\n@@ -360,7 +360,7 @@ class DeviceDescription {\n \n   constexpr int64_t memory_transactions_per_clock() const {\n     return std::visit(\n-        [](const auto &capability) -> int {\n+        [](const auto& capability) -> int {\n           if constexpr (std::is_same_v<std::decay_t<decltype(capability)>,\n                                        RocmComputeCapability>) {\n             // 16 works well on MI300.\n@@ -384,9 +384,9 @@ class DeviceDescription {\n \n   // For string values that are not available via the underlying platform, this\n   // value will be provided.\n-  static inline const char *const kUndefinedString = \"<undefined>\";\n+  static inline const char* const kUndefinedString = \"<undefined>\";\n \n-  void set_gpu_compute_capability(const GpuComputeCapability &c) {\n+  void set_gpu_compute_capability(const GpuComputeCapability& c) {\n     gpu_compute_capability_ = c;\n   }\n \n@@ -402,23 +402,23 @@ class DeviceDescription {\n   void set_platform_version(std::string value) {\n     platform_version_ = std::move(value);\n   }\n-  void set_driver_version(const SemanticVersion &value) {\n+  void set_driver_version(const SemanticVersion& value) {\n     driver_version_ = value;\n   }\n-  void set_runtime_version(const SemanticVersion &value) {\n+  void set_runtime_version(const SemanticVersion& value) {\n     runtime_version_ = value;\n   }\n-  void set_compile_time_toolkit_version(const SemanticVersion &value) {\n+  void set_compile_time_toolkit_version(const SemanticVersion& value) {\n     compile_time_toolkit_version_ = value;\n   }\n   void set_pci_bus_id(std::string value) { pci_bus_id_ = std::move(value); }\n   void set_name(std::string value) { name_ = std::move(value); }\n   void set_model_str(std::string value) { model_str_ = std::move(value); }\n \n-  void set_thread_dim_limit(const ThreadDim &value) {\n+  void set_thread_dim_limit(const ThreadDim& value) {\n     thread_dim_limit_ = value;\n   }\n-  void set_block_dim_limit(const BlockDim &value) { block_dim_limit_ = value; }\n+  void set_block_dim_limit(const BlockDim& value) { block_dim_limit_ = value; }\n \n   void set_threads_per_core_limit(int64_t value) {\n     threads_per_core_limit_ = value;\n@@ -452,8 +452,8 @@ class DeviceDescription {\n \n   void set_clock_rate_ghz(float value) { clock_rate_ghz_ = value; }\n \n-  void set_cuda_compute_capability(int major, int minor) {\n-    gpu_compute_capability_ = CudaComputeCapability{major, minor};\n+  void set_cuda_compute_capability(const CudaComputeCapability& cc) {\n+    gpu_compute_capability_ = cc;\n   }\n \n   void set_rocm_compute_capability(std::string gcn_arch_name) {\n@@ -519,16 +519,16 @@ class DeviceDescription {\n // Returns whether the given thread_dim is acceptable given the limits described\n // in device_description. For detailed reasons for failing the predicate, enable\n // VLOG(2) for this module.\n-bool ThreadDimOk(const DeviceDescription &device_description,\n-                 const ThreadDim &thread_dim);\n+bool ThreadDimOk(const DeviceDescription& device_description,\n+                 const ThreadDim& thread_dim);\n \n // Calculate the number of threads/blocks required to process element_count\n // elements. Note that you can still end up with more threads than\n // element_count due to rounding, so kernels often start with an \"is this\n // thread id in the element_count range?\" test.\n-void CalculateDimensionality(const DeviceDescription &device_description,\n-                             int64_t element_count, int64_t *threads_per_block,\n-                             int64_t *block_count);\n+void CalculateDimensionality(const DeviceDescription& device_description,\n+                             int64_t element_count, int64_t* threads_per_block,\n+                             int64_t* block_count);\n \n }  // namespace stream_executor\n "
        },
        {
            "sha": "b2ba4fa5d65eccdf66e37df36ea6afe8958443d5",
            "filename": "third_party/xla/xla/tests/exhaustive/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftests%2Fexhaustive%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftests%2Fexhaustive%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fexhaustive%2FBUILD?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -62,6 +62,7 @@ cc_library(\n         \"//xla/service:shaped_buffer\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:platform\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tests:client_library_test_base\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/util:command_line_flags\","
        },
        {
            "sha": "72251a9dc1c1b31e92f442803e779a53c54b81cb",
            "filename": "third_party/xla/xla/tests/exhaustive/platform.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftests%2Fexhaustive%2Fplatform.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftests%2Fexhaustive%2Fplatform.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fexhaustive%2Fplatform.cc?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n \n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/platform.h\"\n \n@@ -71,8 +72,8 @@ Platform::Value GetPlatformValue(const stream_executor::Platform& platform) {\n bool Platform::IsNvidiaP100() const {\n   return std::holds_alternative<stream_executor::CudaComputeCapability>(\n              value_) &&\n-         !std::get<stream_executor::CudaComputeCapability>(value_).IsAtLeast(\n-             stream_executor::CudaComputeCapability::Volta());\n+         std::get<stream_executor::CudaComputeCapability>(value_) ==\n+             stream_executor::CudaComputeCapability::Pascal();\n }\n \n bool Platform::IsNvidiaV100() const {"
        },
        {
            "sha": "3652ace77f781db3bfc6aeefd548ce72f7bb70f0",
            "filename": "third_party/xla/xla/tools/hlo_opt/gpu_specs/a100_pcie_80.txtpb",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fa100_pcie_80.txtpb",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fa100_pcie_80.txtpb",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fa100_pcie_80.txtpb?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -30,6 +30,7 @@ gpu_device_info {\n   shared_memory_per_block_optin: 166912\n   cuda_compute_capability {\n     major: 8\n+    feature_extension: NONE\n   }\n   registers_per_core_limit: 65536\n   registers_per_block_limit: 65536"
        },
        {
            "sha": "e46139481fc8b4731c6782ede14ca21038b43730",
            "filename": "third_party/xla/xla/tools/hlo_opt/gpu_specs/a100_sxm_40.txtpb",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fa100_sxm_40.txtpb",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fa100_sxm_40.txtpb",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fa100_sxm_40.txtpb?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -30,6 +30,7 @@ gpu_device_info {\n   shared_memory_per_block_optin: 166912\n   cuda_compute_capability {\n     major: 8\n+    feature_extension: NONE\n   }\n   registers_per_core_limit: 65536\n   registers_per_block_limit: 65536"
        },
        {
            "sha": "abe87b63d6bcafbf920e5f8b5165a8e8f03a9dbe",
            "filename": "third_party/xla/xla/tools/hlo_opt/gpu_specs/a100_sxm_80.txtpb",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fa100_sxm_80.txtpb",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fa100_sxm_80.txtpb",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fa100_sxm_80.txtpb?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -30,6 +30,7 @@ gpu_device_info {\n   shared_memory_per_block_optin: 166912\n   cuda_compute_capability {\n     major: 8\n+    feature_extension: NONE\n   }\n   registers_per_core_limit: 65536\n   registers_per_block_limit: 65536"
        },
        {
            "sha": "b864cee30fef2f59cc22a82f227d0483a5b7ae4b",
            "filename": "third_party/xla/xla/tools/hlo_opt/gpu_specs/a6000.txtpb",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fa6000.txtpb",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fa6000.txtpb",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fa6000.txtpb?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -31,6 +31,7 @@ gpu_device_info {\n   cuda_compute_capability {\n     major: 8\n     minor: 6\n+    feature_extension: NONE\n   }\n   registers_per_core_limit: 65536\n   registers_per_block_limit: 65536"
        },
        {
            "sha": "e00f759fe6c6b75d4234562467e80bd73c72f101",
            "filename": "third_party/xla/xla/tools/hlo_opt/gpu_specs/b200.txtpb",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fb200.txtpb",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fb200.txtpb",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fb200.txtpb?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -30,6 +30,7 @@ gpu_device_info {\n   shared_memory_per_block_optin: 232448\n   cuda_compute_capability {\n     major: 10\n+    feature_extension: ACCELERATED_FEATURES\n   }\n   registers_per_core_limit: 65536\n   registers_per_block_limit: 65536"
        },
        {
            "sha": "6daee1e45ff81ac92eac070ffae49396590ad38a",
            "filename": "third_party/xla/xla/tools/hlo_opt/gpu_specs/h100_pcie.txtpb",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fh100_pcie.txtpb",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fh100_pcie.txtpb",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fh100_pcie.txtpb?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -15,7 +15,7 @@\n gpu_device_info {\n   cuda_compute_capability {\n     major: 9\n-    minor: 0\n+    feature_extension: ACCELERATED_FEATURES\n   }\n   block_dim_limit_x: 2147483647\n   block_dim_limit_y: 65535"
        },
        {
            "sha": "7760c5634d41aa3720fa96774c0517b5cf1e7103",
            "filename": "third_party/xla/xla/tools/hlo_opt/gpu_specs/h100_sxm.txtpb",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fh100_sxm.txtpb",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fh100_sxm.txtpb",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fh100_sxm.txtpb?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -30,6 +30,7 @@ gpu_device_info {\n   shared_memory_per_block_optin: 232448\n   cuda_compute_capability {\n     major: 9\n+    feature_extension: ACCELERATED_FEATURES\n   }\n   registers_per_core_limit: 65536\n   registers_per_block_limit: 65536"
        },
        {
            "sha": "3aa8ce9352ebb1d8800628fe80b7cce3bcea63c3",
            "filename": "third_party/xla/xla/tools/hlo_opt/gpu_specs/p100.txtpb",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fp100.txtpb",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fp100.txtpb",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fp100.txtpb?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -30,6 +30,7 @@ gpu_device_info {\n   shared_memory_per_block_optin: 49152\n   cuda_compute_capability {\n     major: 6\n+    feature_extension: NONE\n   }\n   registers_per_core_limit: 65536\n   registers_per_block_limit: 65536"
        },
        {
            "sha": "8474e435a882e2888e562564c860de1335d67aa1",
            "filename": "third_party/xla/xla/tools/hlo_opt/gpu_specs/v100.txtpb",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fv100.txtpb",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7327b23ca8de6ca8d24e335edac70145f0d6d39/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fv100.txtpb",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fhlo_opt%2Fgpu_specs%2Fv100.txtpb?ref=f7327b23ca8de6ca8d24e335edac70145f0d6d39",
            "patch": "@@ -30,6 +30,7 @@ gpu_device_info {\n   shared_memory_per_block_optin: 98304\n   cuda_compute_capability {\n     major: 7\n+    feature_extension: NONE\n   }\n   registers_per_core_limit: 65536\n   registers_per_block_limit: 65536"
        }
    ],
    "stats": {
        "total": 1069,
        "additions": 749,
        "deletions": 320
    }
}