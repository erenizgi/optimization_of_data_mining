{
    "author": "chsigg",
    "message": "Migrate Triton dot algorithm tests to use nested GEMM fusion.\n\nThe generic emitter uses this input and is the new default path. The legacy emitter will be removed soon.\n\nThis change updates several tests to use the `__triton_nested_gemm_fusion` backend config instead of the legacy `__triton_gemm`. This involves restructuring the HLO to include nested fusions for the LHS and RHS operands. Additionally, several generic Triton emitter features are enabled in the test debug options to support the new nested fusion structure. The test utility for dot fusions is also updated to correctly parse block-level parameters from the backend config.\n\nPiperOrigin-RevId: 828943705",
    "sha": "4ebd9c2860579d2d6162bfe187ab15bc62aa8f9b",
    "files": [
        {
            "sha": "8a6850c8562fcaeda509f6a3023f722d5a567e56",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms_test.cc",
            "status": "modified",
            "additions": 129,
            "deletions": 44,
            "changes": 173,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4ebd9c2860579d2d6162bfe187ab15bc62aa8f9b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4ebd9c2860579d2d6162bfe187ab15bc62aa8f9b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms_test.cc?ref=4ebd9c2860579d2d6162bfe187ab15bc62aa8f9b",
            "patch": "@@ -81,10 +81,16 @@ class AlgorithmTest : public GpuCodegenTest {\n   DebugOptions GetDebugOptionsForTest() const override {\n     DebugOptions debug_options = GpuCodegenTest::GetDebugOptionsForTest();\n     debug_options.set_xla_gpu_autotune_level(0);\n-    // TODO(b/393299275): remove when the flag is enabled by default.\n+    // TODO(b/393299275): remove when these flags are on by default.\n     debug_options.clear_xla_gpu_unsupported_generic_triton_emitter_features();\n     debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n         DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n+    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n+        DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM);\n+    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n+        DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION);\n+    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n+        DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_GEMM_SHAPES);\n     return debug_options;\n   }\n \n@@ -485,10 +491,26 @@ TEST_F(Triton6xBF16GemmTest, Emit6xBF16GemmWhenBothInputsAreF32) {\n   constexpr absl::string_view kHloText = R\"(\n     HloModule Emit6xBF16GemmWhenBothInputsAreF32\n \n+    lhs {\n+      ROOT p0 = f32[5,7] parameter(0)\n+    }\n+\n+    rhs {\n+      ROOT p0 = f32[7,33] parameter(0)\n+    }\n+\n     triton_dot {\n       p0 = f32[5,7] parameter(0)\n       p1 = f32[7,33] parameter(1)\n-      ROOT dot = f32[5,33] dot(p0, p1),\n+      lhs = f32[5,7] fusion(p0), kind=kCustom, calls=lhs,\n+       backend_config={\"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"32\",\"32\"]}]}}}\n+      rhs = f32[7,33] fusion(p1), kind=kCustom, calls=rhs,\n+       backend_config={\"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"32\",\"32\"]}]}}}\n+      ROOT dot = f32[5,33] dot(lhs, rhs),\n         lhs_contracting_dims={1}, rhs_contracting_dims={0},\n         algorithm=dot_bf16_bf16_f32_x6\n     }\n@@ -497,9 +519,11 @@ TEST_F(Triton6xBF16GemmTest, Emit6xBF16GemmWhenBothInputsAreF32) {\n       p0 = f32[5,7]{1,0} parameter(0)\n       p1 = f32[7,33]{1,0} parameter(1)\n       ROOT _ = f32[5,33] fusion(p0, p1), kind=kCustom, calls=triton_dot,\n-        backend_config={\"fusion_backend_config\": {kind: \"__triton_gemm\",\n-        triton_gemm_config:\n-        {\"block_m\":32,\"block_n\":32,\"block_k\":32,\"split_k\":1,\"num_stages\":1,\"num_warps\":1,\"num_ctas\":1}}}\n+        backend_config={\"fusion_backend_config\": {\n+          kind: \"__triton_nested_gemm_fusion\",\n+          block_level_fusion_config: {\n+            \"output_tiles\":[{\"sizes\":[\"32\",\"32\"]}],\n+            \"num_stages\":1,\"num_warps\":1,\"num_ctas\":1}}}\n     }\n   )\";\n   TF_ASSERT_OK(\n@@ -536,10 +560,26 @@ TEST_F(Triton6xBF16GemmTest, Triton6xBF16GemmWorksForLongContractingDimension) {\n   constexpr absl::string_view kHloText = R\"(\n     HloModule Triton6xBF16GemmWorksForLongContractingDimension\n \n+    lhs {\n+      ROOT p0 = f32[5,2048] parameter(0)\n+    }\n+\n+    rhs {\n+      ROOT p0 = f32[2048,33] parameter(0)\n+    }\n+\n     triton_dot {\n       p0 = f32[5,2048] parameter(0)\n       p1 = f32[2048,33] parameter(1)\n-      ROOT dot = f32[5,33] dot(p0, p1),\n+      lhs = f32[5,2048] fusion(p0), kind=kCustom, calls=lhs,\n+       backend_config={\"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"64\",\"32\"]}]}}}\n+      rhs = f32[2048,33] fusion(p1), kind=kCustom, calls=rhs,\n+       backend_config={\"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"32\",\"32\"]}]}}}\n+      ROOT dot = f32[5,33] dot(lhs, rhs),\n         lhs_contracting_dims={1}, rhs_contracting_dims={0},\n         algorithm=dot_bf16_bf16_f32_x6\n     }\n@@ -548,9 +588,11 @@ TEST_F(Triton6xBF16GemmTest, Triton6xBF16GemmWorksForLongContractingDimension) {\n       p0 = f32[5,2048]{1,0} parameter(0)\n       p1 = f32[2048,33]{1,0} parameter(1)\n       ROOT _ = f32[5,33] fusion(p0, p1), kind=kCustom, calls=triton_dot,\n-        backend_config={\"fusion_backend_config\": {kind: \"__triton_gemm\",\n-        triton_gemm_config:\n-        {\"block_m\":64,\"block_n\":32,\"block_k\":32,\"split_k\":1,\"num_stages\":1,\"num_warps\":4, \"num_ctas\":1}}}\n+        backend_config={\"fusion_backend_config\": {\n+          kind: \"__triton_nested_gemm_fusion\",\n+          block_level_fusion_config: {\n+            \"output_tiles\":[{\"sizes\":[\"64\",\"32\"]}],\n+            \"num_stages\":1,\"num_warps\":4, \"num_ctas\":1}}}\n     }\n   )\";\n   TF_ASSERT_OK(\n@@ -595,10 +637,26 @@ TEST_F(Triton3xBF16GemmTest, Emit3xBF16GemmWhenBothInputsAreF32) {\n   constexpr absl::string_view kHloText = R\"(\n     HloModule Emit3xBF16GemmWhenBothInputsAreF32\n \n+    lhs {\n+      ROOT p0 = f32[5,7] parameter(0)\n+    }\n+\n+    rhs {\n+      ROOT p0 = f32[7,33] parameter(0)\n+    }\n+\n     triton_dot {\n       p0 = f32[5,7] parameter(0)\n       p1 = f32[7,33] parameter(1)\n-      ROOT dot = f32[5,33] dot(p0, p1),\n+      lhs = f32[5,7] fusion(p0), kind=kCustom, calls=lhs,\n+       backend_config={\"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"32\",\"32\"]}]}}}\n+      rhs = f32[7,33] fusion(p1), kind=kCustom, calls=rhs,\n+       backend_config={\"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"32\",\"32\"]}]}}}\n+      ROOT dot = f32[5,33] dot(lhs, rhs),\n         lhs_contracting_dims={1}, rhs_contracting_dims={0},\n         algorithm=dot_bf16_bf16_f32_x3\n     }\n@@ -607,9 +665,11 @@ TEST_F(Triton3xBF16GemmTest, Emit3xBF16GemmWhenBothInputsAreF32) {\n       p0 = f32[5,7]{1,0} parameter(0)\n       p1 = f32[7,33]{1,0} parameter(1)\n       ROOT _ = f32[5,33] fusion(p0, p1), kind=kCustom, calls=triton_dot,\n-        backend_config={\"fusion_backend_config\": {kind: \"__triton_gemm\",\n-        triton_gemm_config:\n-        {\"block_m\":32,\"block_n\":32,\"block_k\":32,\"split_k\":1,\"num_stages\":1,\"num_warps\":1,\"num_ctas\":1}}}\n+        backend_config={\"fusion_backend_config\": {\n+          kind: \"__triton_nested_gemm_fusion\",\n+          block_level_fusion_config: {\n+            \"output_tiles\":[{\"sizes\":[\"32\",\"32\"]}],\n+            \"num_stages\":1,\"num_warps\":1,\"num_ctas\":1}}}\n     }\n   )\";\n   TF_ASSERT_OK(\n@@ -640,10 +700,26 @@ TEST_F(Triton3xBF16GemmTest, Triton3xBF16GemmWorksForLongContractingDimension) {\n   constexpr absl::string_view kHloText = R\"(\n     HloModule Triton3xBF16GemmWorksForLongContractingDimension\n \n+    lhs {\n+      ROOT p0 = f32[5,2048] parameter(0)\n+    }\n+\n+    rhs {\n+      ROOT p0 = f32[2048,33] parameter(0)\n+    }\n+\n     triton_dot {\n       p0 = f32[5,2048] parameter(0)\n       p1 = f32[2048,33] parameter(1)\n-      ROOT dot = f32[5,33] dot(p0, p1),\n+      lhs = f32[5,2048] fusion(p0), kind=kCustom, calls=lhs,\n+       backend_config={\"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"64\",\"32\"]}]}}}\n+      rhs = f32[2048,33] fusion(p1), kind=kCustom, calls=rhs,\n+       backend_config={\"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"32\",\"32\"]}]}}}\n+      ROOT dot = f32[5,33] dot(lhs, rhs),\n         lhs_contracting_dims={1}, rhs_contracting_dims={0},\n         algorithm=dot_bf16_bf16_f32_x3\n     }\n@@ -652,9 +728,11 @@ TEST_F(Triton3xBF16GemmTest, Triton3xBF16GemmWorksForLongContractingDimension) {\n       p0 = f32[5,2048]{1,0} parameter(0)\n       p1 = f32[2048,33]{1,0} parameter(1)\n       ROOT _ = f32[5,33] fusion(p0, p1), kind=kCustom, calls=triton_dot,\n-        backend_config={\"fusion_backend_config\": {kind: \"__triton_gemm\",\n-        triton_gemm_config:\n-        {\"block_m\":64,\"block_n\":32,\"block_k\":32,\"split_k\":1,\"num_stages\":1,\"num_warps\":4, \"num_ctas\":1}}}\n+        backend_config={\"fusion_backend_config\": {\n+          kind: \"__triton_nested_gemm_fusion\",\n+          block_level_fusion_config: {\n+            \"output_tiles\":[{\"sizes\":[\"64\",\"32\"]}],\n+            \"num_stages\":1,\"num_warps\":4, \"num_ctas\":1}}}\n     }\n   )\";\n   TF_ASSERT_OK(\n@@ -815,43 +893,50 @@ TEST_F(TritonAlgorithmTest, Dot_BF16_X6_WithConst) {\n   constexpr absl::string_view kHloText = R\"(\n     HloModule Dot_BF16_X6_WithConst\n \n-    %triton_fusion_dot (p_0: f32[1,258]) -> f32[258] {\n-      %c_1 = f32[] constant(-1.22474492)\n-      %r_1 = f32[1]{0} reshape(f32[] %c_1)\n-      %r_2 = f32[1,1]{1,0} reshape(f32[1]{0} %r_1)\n-      %p_0 = f32[1,258]{1,0} parameter(0)\n-      %r_3 = f32[258]{0} reshape(f32[1,258]{1,0} %p_0)\n-      %r_4 = f32[258,1]{1,0} reshape(f32[258]{0} %r_3)\n-      %dot_0 = f32[1,258]{1,0} dot(f32[1,1]{1,0} %r_2, f32[258,1]{1,0} %r_4),\n+    lhs {\n+      constant = f32[] constant(-1.22474492)\n+      ROOT broadcast = f32[1,1] broadcast(constant)\n+    }\n+\n+    rhs {\n+      ROOT p0 = f32[258,1] parameter(0)\n+    }\n+\n+    triton_fusion_dot {\n+      p0 = f32[258,1] parameter(0)\n+      lhs = f32[1,1] fusion(), kind=kCustom, calls=lhs,\n+       backend_config={\"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"16\",\"16\"]}]}}}\n+      rhs = f32[258,1] fusion(p0), kind=kCustom, calls=rhs,\n+       backend_config={\"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"16\",\"256\"]}]}}}\n+      dot = f32[1,258] dot(lhs, rhs),\n           lhs_contracting_dims={0},\n           rhs_contracting_dims={1},\n           algorithm=dot_bf16_bf16_f32_x6\n-      %r_5 = f32[258]{0} reshape(f32[1,258]{1,0} %dot_0)\n-      %c_2 = f32[] constant(0.282094777)\n-      %b_0 = f32[258]{0} broadcast(f32[] %c_2), dimensions={}\n-      ROOT %m_0 = f32[258]{0} multiply(f32[258]{0} %r_5, f32[258]{0} %b_0)\n+      constant = f32[] constant(0.282094777)\n+      broadcast = f32[1,258] broadcast(constant), dimensions={}\n+      ROOT root = f32[1,258] multiply(dot, broadcast)\n     }\n \n-    ENTRY %entry_computation {\n-      %p_0 = f32[1,258]{1,0} parameter(0)\n-      ROOT %dot = f32[258]{0} fusion(f32[1,258]{1,0} %p_0),\n+    ENTRY entry_computation {\n+      p0 = f32[258,1] parameter(0)\n+      ROOT root = f32[1,258] fusion(p0),\n         kind=kCustom,\n-        calls=%triton_fusion_dot,\n+        calls=triton_fusion_dot,\n         backend_config={\n           \"operation_queue_id\":\"0\",\n           \"wait_on_operation_queues\":[],\n           \"fusion_backend_config\":{\n-            \"kind\":\"__triton_gemm\",\n-            \"triton_gemm_config\":{\n-              \"block_m\":\"16\",\n-              \"block_n\":\"256\",\n-              \"block_k\":\"16\",\n-              \"split_k\":\"1\",\n-              \"num_stages\":\"4\",\n-              \"num_warps\":\"4\",\n-              \"num_ctas\":\"1\"\n-            }\n-          },\n+            \"kind\":\"__triton_nested_gemm_fusion\",\n+            \"block_level_fusion_config\":{\n+              \"output_tiles\": [{\"sizes\": [\"16\",\"256\"]}],\n+              \"num_stages\":4,\n+              \"num_warps\":4,\n+              \"num_ctas\":1\n+            }},\n           \"force_earliest_schedule\":false\n         }\n     }"
        },
        {
            "sha": "57b1734ee29856ffb97c13ca8f0c5cf2ed5e64ac",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_port_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4ebd9c2860579d2d6162bfe187ab15bc62aa8f9b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4ebd9c2860579d2d6162bfe187ab15bc62aa8f9b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc?ref=4ebd9c2860579d2d6162bfe187ab15bc62aa8f9b",
            "patch": "@@ -93,6 +93,7 @@ class TritonTest : public GpuCodegenTest {\n         debug_options\n             .mutable_xla_gpu_unsupported_generic_triton_emitter_features();\n     emitter_opts->Add(DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n+    emitter_opts->Add(DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM);\n     emitter_opts->Add(\n         DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION);\n     emitter_opts->Add("
        },
        {
            "sha": "9555b136f7027909a063383886ccdd5bff17080f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_int4_device_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4ebd9c2860579d2d6162bfe187ab15bc62aa8f9b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_int4_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4ebd9c2860579d2d6162bfe187ab15bc62aa8f9b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_int4_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_int4_device_test.cc?ref=4ebd9c2860579d2d6162bfe187ab15bc62aa8f9b",
            "patch": "@@ -107,6 +107,7 @@ class TritonTest : public GpuCodegenTest {\n             .mutable_debug_options()\n             .mutable_xla_gpu_unsupported_generic_triton_emitter_features();\n     emitter_opts->Add(DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n+    emitter_opts->Add(DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM);\n     emitter_opts->Add(\n         DebugOptions::GENERIC_TRITON_EMITTER_ALLOW_ALL_OPS_IN_GEMM_FUSION);\n     emitter_opts->Add("
        },
        {
            "sha": "8f8d6d6267a36ddb9cc8aa78f0a0d59b9a69ce21",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/test_utils.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 3,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4ebd9c2860579d2d6162bfe187ab15bc62aa8f9b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4ebd9c2860579d2d6162bfe187ab15bc62aa8f9b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc?ref=4ebd9c2860579d2d6162bfe187ab15bc62aa8f9b",
            "patch": "@@ -52,6 +52,7 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/gpu_float_support.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -206,13 +207,20 @@ absl::Status CreateTritonIrAndFileCheckForDot(\n                       test->ParseAndReturnVerifiedModule(hlo_text));\n   auto* comp = verified_module->GetComputationWithName(triton_fusion_name);\n   TF_RET_CHECK(comp != nullptr);\n-  return CreateTritonIrAndFileCheck(*comp, /*block_level_parameters=*/{},\n-                                    filecheck_pattern);\n+  return CreateTritonIrAndFileCheckForDot(*comp, filecheck_pattern);\n }\n \n absl::Status CreateTritonIrAndFileCheckForDot(\n     const HloComputation& computation, absl::string_view filecheck_pattern) {\n-  return CreateTritonIrAndFileCheck(computation, /*block_level_parameters=*/{},\n+  BlockLevelParameters block_level_parameters;\n+  if (auto gpu_config =\n+          computation.FusionInstruction()->backend_config<GpuBackendConfig>();\n+      gpu_config.ok() && gpu_config->has_fusion_backend_config() &&\n+      gpu_config->fusion_backend_config().has_block_level_fusion_config()) {\n+    block_level_parameters = BlockLevelParameters::FromBlockLevelFusionConfig(\n+        gpu_config->fusion_backend_config().block_level_fusion_config());\n+  }\n+  return CreateTritonIrAndFileCheck(computation, block_level_parameters,\n                                     filecheck_pattern);\n }\n "
        }
    ],
    "stats": {
        "total": 189,
        "additions": 142,
        "deletions": 47
    }
}