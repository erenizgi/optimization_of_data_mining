{
    "author": "ezhulenev",
    "message": "[xla:gpu] Add support for passing collective communicators to XLA:FFI handlers\n\nPlumb XLA:GPU collective execution details through XLA FFI internal APIs to allow writing FFI handlers with access to GPU cliques and communicators.\n\nFor now it requires \"internal\" FFI which relies on static linking, as a lot of collective runtime implementation details are still in flux and it's too early to write C API for them, as XLA must guarantee backward compatibility.\n\nPiperOrigin-RevId: 839408531",
    "sha": "142aa13be42122fb709abbe8d620eaf2ee423099",
    "files": [
        {
            "sha": "d0ddd0edec0d4bde0920d849303bb953fda17025",
            "filename": "third_party/xla/xla/backends/gpu/BUILD",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2FBUILD?ref=142aa13be42122fb709abbe8d620eaf2ee423099",
            "patch": "@@ -1,4 +1,5 @@\n-load(\"//xla/tsl/platform:rules_cc.bzl\", \"cc_library\")\n+load(\"@rules_cc//cc:cc_library.bzl\", \"cc_library\")\n+load(\"//xla/tsl:tsl.default.bzl\", \"get_compatible_with_portable\")\n \n package(\n     # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n@@ -16,8 +17,12 @@ package_group(\n cc_library(\n     name = \"ffi\",\n     hdrs = [\"ffi.h\"],\n+    compatible_with = get_compatible_with_portable(),\n     visibility = [\"//visibility:public\"],\n     deps = [\n+        \"//xla/backends/gpu/runtime:collective_clique_requests\",\n+        \"//xla/backends/gpu/runtime:collective_cliques\",\n+        \"//xla/backends/gpu/runtime:collective_params\",\n         \"//xla/ffi:api\",\n         \"//xla/ffi/api:c_api\",\n         \"//xla/ffi/api:c_api_internal\","
        },
        {
            "sha": "09c6ac25ade95ba2779163a6bec265103edab131",
            "filename": "third_party/xla/xla/backends/gpu/ffi.h",
            "status": "modified",
            "additions": 76,
            "deletions": 5,
            "changes": 81,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fffi.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fffi.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fffi.h?ref=142aa13be42122fb709abbe8d620eaf2ee423099",
            "patch": "@@ -20,9 +20,12 @@ limitations under the License.\n #include <optional>\n \n #include \"absl/base/optimization.h\"\n+#include \"xla/backends/gpu/runtime/collective_clique_requests.h\"\n+#include \"xla/backends/gpu/runtime/collective_cliques.h\"\n+#include \"xla/backends/gpu/runtime/collective_params.h\"\n+#include \"xla/ffi/api/api.h\"  // IWYU pragma: export\n #include \"xla/ffi/api/c_api.h\"\n #include \"xla/ffi/api/c_api_internal.h\"  // IWYU pragma: keep\n-#include \"xla/ffi/api/api.h\"  // IWYU pragma: export\n #include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/scratch_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -33,12 +36,17 @@ namespace xla::ffi {\n // Type tags to bind parameters passed via execution context to FFI handler\n //===----------------------------------------------------------------------===//\n \n-struct Stream {};            // binds `se::Stream*`\n-struct Allocator {};         // binds `se::DeviceMemoryAllocator*`\n-struct ScratchAllocator {};  // binds `se::OwningScratchAllocator`\n+// Type tag binds to one of the following types defined by XLA:GPU runtime:\n+struct Stream {};                    //  `se::Stream*`\n+struct Allocator {};                 //  `se::DeviceMemoryAllocator*`\n+struct ScratchAllocator {};          //  `se::OwningScratchAllocator`\n+struct CollectiveParams {};          //  `const xla::gpu::CollectiveParams*`\n+struct CollectiveCliqueRequests {};  //  `xla::gpu::CollectiveCliqueRequests*`\n+struct CollectiveCliques {};         //  `const xla::gpu::CollectiveCliques*`\n \n+// Parametrized type tag for platform stream, e.g. `cudaStream_t`\n template <typename T>\n-struct PlatformStream {};  // binds a platform stream, e.g. `cudaStream_t`\n+struct PlatformStream {};\n \n //===----------------------------------------------------------------------===//\n // Context decoding\n@@ -106,6 +114,69 @@ struct CtxDecoding<ScratchAllocator> {\n   }\n };\n \n+template <>\n+struct CtxDecoding<CollectiveParams> {\n+  using Type = const xla::gpu::CollectiveParams*;\n+\n+  static std::optional<Type> Decode(const XLA_FFI_Api* api,\n+                                    XLA_FFI_ExecutionContext* ctx,\n+                                    DiagnosticEngine& diagnostic) {\n+    void* collective_params = nullptr;\n+    if (XLA_FFI_Error* error =\n+            api->internal_api->XLA_FFI_INTERNAL_CollectiveParams_Get(\n+                ctx, &collective_params);\n+        ABSL_PREDICT_FALSE(error)) {\n+      diagnostic.Emit(\"Failed to get collective params: \")\n+          << internal::GetErrorMessage(api, error);\n+      internal::DestroyError(api, error);\n+      return std::nullopt;\n+    }\n+    return reinterpret_cast<Type>(collective_params);\n+  }\n+};\n+\n+template <>\n+struct CtxDecoding<CollectiveCliqueRequests> {\n+  using Type = xla::gpu::CollectiveCliqueRequests*;\n+\n+  static std::optional<Type> Decode(const XLA_FFI_Api* api,\n+                                    XLA_FFI_ExecutionContext* ctx,\n+                                    DiagnosticEngine& diagnostic) {\n+    void* collective_clique_requests = nullptr;\n+    if (XLA_FFI_Error* error =\n+            api->internal_api->XLA_FFI_INTERNAL_CollectiveCliqueRequests_Get(\n+                ctx, &collective_clique_requests);\n+        ABSL_PREDICT_FALSE(error)) {\n+      diagnostic.Emit(\"Failed to get collective clique requests: \")\n+          << internal::GetErrorMessage(api, error);\n+      internal::DestroyError(api, error);\n+      return std::nullopt;\n+    }\n+    return reinterpret_cast<Type>(collective_clique_requests);\n+  }\n+};\n+\n+template <>\n+struct CtxDecoding<CollectiveCliques> {\n+  using Type = const xla::gpu::CollectiveCliques*;\n+\n+  static std::optional<Type> Decode(const XLA_FFI_Api* api,\n+                                    XLA_FFI_ExecutionContext* ctx,\n+                                    DiagnosticEngine& diagnostic) {\n+    void* collective_cliques = nullptr;\n+    if (XLA_FFI_Error* error =\n+            api->internal_api->XLA_FFI_INTERNAL_CollectiveCliques_Get(\n+                ctx, &collective_cliques);\n+        ABSL_PREDICT_FALSE(error)) {\n+      diagnostic.Emit(\"Failed to get collective cliques: \")\n+          << internal::GetErrorMessage(api, error);\n+      internal::DestroyError(api, error);\n+      return std::nullopt;\n+    }\n+    return reinterpret_cast<Type>(collective_cliques);\n+  }\n+};\n+\n template <typename T>\n struct CtxDecoding<PlatformStream<T>> {\n   using Type = T;"
        },
        {
            "sha": "6105b3ab9682e0cceba16f4048e9d8622ced75f2",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=142aa13be42122fb709abbe8d620eaf2ee423099",
            "patch": "@@ -688,6 +688,9 @@ cc_library(\n     srcs = [\"custom_call_thunk.cc\"],\n     hdrs = [\"custom_call_thunk.h\"],\n     deps = [\n+        \":collective_clique_requests\",\n+        \":collective_cliques\",\n+        \":collective_params\",\n         \":custom_call_target\",\n         \":shaped_slice\",\n         \":thunk\",\n@@ -1590,6 +1593,7 @@ cc_library(\n     name = \"collective_clique_requests\",\n     srcs = [\"collective_clique_requests.cc\"],\n     hdrs = [\"collective_clique_requests.h\"],\n+    compatible_with = get_compatible_with_portable(),\n     deps = [\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -1617,6 +1621,7 @@ cc_library(\n     name = \"collective_params\",\n     srcs = [\"collective_params.cc\"],\n     hdrs = [\"collective_params.h\"],\n+    compatible_with = get_compatible_with_portable(),\n     deps = [\n         \":collective_clique_requests\",\n         \"//xla:executable_run_options\",\n@@ -1643,6 +1648,7 @@ cc_library(\n     name = \"collective_cliques\",\n     srcs = [\"collective_cliques.cc\"],\n     hdrs = [\"collective_cliques.h\"],\n+    compatible_with = get_compatible_with_portable(),\n     deps = [\n         \":collective_clique_requests\",\n         \":collective_params\","
        },
        {
            "sha": "42640649c1f936b6a2bf06194914d426d8b17197",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk.cc",
            "status": "modified",
            "additions": 61,
            "deletions": 32,
            "changes": 93,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc?ref=142aa13be42122fb709abbe8d620eaf2ee423099",
            "patch": "@@ -37,6 +37,9 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/runtime/collective_clique_requests.h\"\n+#include \"xla/backends/gpu/runtime/collective_cliques.h\"\n+#include \"xla/backends/gpu/runtime/collective_params.h\"\n #include \"xla/backends/gpu/runtime/custom_call_target.h\"\n #include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n@@ -410,6 +413,9 @@ CustomCallThunk::BuildCallFrame(\n CallOptions CustomCallThunk::BuildCallOptions(\n     RunId run_id, se::Stream* absl_nullable stream,\n     const BufferAllocations* absl_nullable buffer_allocations,\n+    const CollectiveParams* absl_nullable collective_params,\n+    CollectiveCliqueRequests* absl_nullable collective_clique_requests,\n+    const CollectiveCliques* absl_nullable collective_cliques,\n     const ffi::ExecutionContext* absl_nullable execution_context) {\n   int32_t device_ordinal = -1;\n   se::DeviceMemoryAllocator* allocator = nullptr;\n@@ -418,18 +424,23 @@ CallOptions CustomCallThunk::BuildCallOptions(\n     allocator = buffer_allocations->memory_allocator();\n   }\n \n-  return CallOptions{run_id,\n-                     device_ordinal,\n-                     CallOptions::GpuOptions{stream, allocator},\n-                     called_computation_,\n-                     execution_context,\n-                     execution_state_.get()};\n+  return CallOptions{\n+      run_id,\n+      device_ordinal,\n+      CallOptions::GpuOptions{stream, allocator, collective_params,\n+                              collective_clique_requests, collective_cliques},\n+      called_computation_,\n+      execution_context,\n+      execution_state_.get()};\n }\n \n absl::Status CustomCallThunk::ExecuteFfiHandler(\n     RunId run_id, XLA_FFI_Handler* handler, XLA_FFI_ExecutionStage stage,\n     se::Stream* stream, const ffi::ExecutionContext* execution_context,\n-    const BufferAllocations* buffer_allocations) {\n+    const BufferAllocations* buffer_allocations,\n+    const CollectiveParams* absl_nullable collective_params,\n+    CollectiveCliqueRequests* absl_nullable collective_clique_requests,\n+    const CollectiveCliques* absl_nullable collective_cliques) {\n   if (handler == nullptr) {\n     return absl::InternalError(\"FFI execute handler is not set\");\n   }\n@@ -439,23 +450,28 @@ absl::Status CustomCallThunk::ExecuteFfiHandler(\n   }\n \n   TF_ASSIGN_OR_RETURN(auto call_frame, BuildCallFrame(buffer_allocations));\n-  CallOptions options =\n-      BuildCallOptions(run_id, stream, buffer_allocations, execution_context);\n+  CallOptions options = BuildCallOptions(\n+      run_id, stream, buffer_allocations, collective_params,\n+      collective_clique_requests, collective_cliques, execution_context);\n   return Call(handler, *call_frame, options, stage);\n }\n \n absl::Status CustomCallThunk::ExecuteFfiHandler(\n     RunId run_id, xla::ffi::Ffi& handler, xla::ffi::ExecutionStage stage,\n     se::Stream* stream, const ffi::ExecutionContext* execution_context,\n-    const BufferAllocations* buffer_allocations) {\n+    const BufferAllocations* buffer_allocations,\n+    const CollectiveParams* absl_nullable collective_params,\n+    CollectiveCliqueRequests* absl_nullable collective_clique_requests,\n+    const CollectiveCliques* absl_nullable collective_cliques) {\n   if (stage != xla::ffi::ExecutionStage::kPrepare &&\n       !(buffer_allocations && stream)) {\n     return absl::InternalError(\"buffer allocations and stream are required\");\n   }\n \n   TF_ASSIGN_OR_RETURN(auto call_frame, BuildCallFrame(buffer_allocations));\n-  CallOptions options =\n-      BuildCallOptions(run_id, stream, buffer_allocations, execution_context);\n+  CallOptions options = BuildCallOptions(\n+      run_id, stream, buffer_allocations, collective_params,\n+      collective_clique_requests, collective_cliques, execution_context);\n   return Call(handler, *call_frame, options, stage);\n }\n \n@@ -467,20 +483,26 @@ absl::Status CustomCallThunk::Prepare(const PrepareParams& params) {\n     if (const auto* c_bundle =\n             std::get_if<XLA_FFI_Handler_Bundle>(&bundle_.value());\n         c_bundle && c_bundle->prepare) {\n-      return ExecuteFfiHandler(run_id, c_bundle->prepare,\n-                               XLA_FFI_ExecutionStage_PREPARE,\n-                               /*stream=*/nullptr,\n-                               /*execution_context=*/nullptr,\n-                               /*buffer_allocations=*/nullptr);\n+      return ExecuteFfiHandler(\n+          run_id, c_bundle->prepare, XLA_FFI_ExecutionStage_PREPARE,\n+          /*stream=*/nullptr,\n+          /*execution_context=*/nullptr,\n+          /*buffer_allocations=*/nullptr,\n+          /*collective_params=*/params.collective_params,\n+          /*collective_clique_requests=*/params.clique_requests,\n+          /*collective_cliques=*/nullptr);\n     }\n     if (const auto* owned_bundle =\n             std::get_if<OwnedHandlerBundle>(&bundle_.value());\n         owned_bundle && owned_bundle->prepare) {\n-      return ExecuteFfiHandler(run_id, *owned_bundle->prepare,\n-                               xla::ffi::ExecutionStage::kPrepare,\n-                               /*stream=*/nullptr,\n-                               /*execution_context=*/nullptr,\n-                               /*buffer_allocations=*/nullptr);\n+      return ExecuteFfiHandler(\n+          run_id, *owned_bundle->prepare, xla::ffi::ExecutionStage::kPrepare,\n+          /*stream=*/nullptr,\n+          /*execution_context=*/nullptr,\n+          /*buffer_allocations=*/nullptr,\n+          /*collective_params=*/params.collective_params,\n+          /*collective_clique_requests=*/params.clique_requests,\n+          /*collective_cliques=*/nullptr);\n     }\n   }\n \n@@ -495,18 +517,21 @@ absl::Status CustomCallThunk::Initialize(const InitializeParams& params) {\n     if (const auto* c_bundle =\n             std::get_if<XLA_FFI_Handler_Bundle>(&bundle_.value());\n         c_bundle && c_bundle->initialize) {\n-      return ExecuteFfiHandler(run_id, *c_bundle->initialize,\n-                               XLA_FFI_ExecutionStage_INITIALIZE, params.stream,\n-                               params.ffi_execution_context,\n-                               params.buffer_allocations);\n+      return ExecuteFfiHandler(\n+          run_id, *c_bundle->initialize, XLA_FFI_ExecutionStage_INITIALIZE,\n+          params.stream, params.ffi_execution_context,\n+          params.buffer_allocations, params.collective_params,\n+          /*collective_clique_requests=*/nullptr, params.collective_cliques);\n     }\n     if (const auto* owned_bundle =\n             std::get_if<OwnedHandlerBundle>(&bundle_.value());\n         owned_bundle && owned_bundle->initialize) {\n-      return ExecuteFfiHandler(run_id, *owned_bundle->initialize,\n-                               xla::ffi::ExecutionStage::kInitialize,\n-                               params.stream, params.ffi_execution_context,\n-                               params.buffer_allocations);\n+      return ExecuteFfiHandler(\n+          run_id, *owned_bundle->initialize,\n+          xla::ffi::ExecutionStage::kInitialize, params.stream,\n+          params.ffi_execution_context, params.buffer_allocations,\n+          params.collective_params, /*collective_clique_requests=*/nullptr,\n+          params.collective_cliques);\n     }\n   }\n   return absl::OkStatus();\n@@ -525,7 +550,9 @@ absl::Status CustomCallThunk::ExecuteOnStream(const ExecuteParams& params) {\n         c_bundle) {\n       return ExecuteFfiHandler(\n           run_id, c_bundle->execute, XLA_FFI_ExecutionStage_EXECUTE, stream,\n-          params.ffi_execution_context, params.buffer_allocations);\n+          params.ffi_execution_context, params.buffer_allocations,\n+          params.collective_params, /*collective_clique_requests=*/nullptr,\n+          params.collective_cliques);\n     }\n     if (const auto* owned_bundle =\n             std::get_if<OwnedHandlerBundle>(&bundle_.value());\n@@ -535,7 +562,9 @@ absl::Status CustomCallThunk::ExecuteOnStream(const ExecuteParams& params) {\n       }\n       return ExecuteFfiHandler(\n           run_id, *owned_bundle->execute, xla::ffi::ExecutionStage::kExecute,\n-          stream, params.ffi_execution_context, params.buffer_allocations);\n+          stream, params.ffi_execution_context, params.buffer_allocations,\n+          params.collective_params, /*collective_clique_requests=*/nullptr,\n+          params.collective_cliques);\n     }\n   }\n "
        },
        {
            "sha": "4bec3588c60e8c26422d7df789b92a57f244b76a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk.h",
            "status": "modified",
            "additions": 19,
            "deletions": 11,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h?ref=142aa13be42122fb709abbe8d620eaf2ee423099",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/runtime/collective_cliques.h\"\n #include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/executable_run_options.h\"\n@@ -185,19 +186,26 @@ class CustomCallThunk : public Thunk {\n   xla::ffi::CallOptions BuildCallOptions(\n       RunId run_id, se::Stream* absl_nullable stream,\n       const BufferAllocations* absl_nullable buffer_allocations,\n+      const CollectiveParams* absl_nullable collective_params,\n+      CollectiveCliqueRequests* absl_nullable collective_clique_requests,\n+      const CollectiveCliques* absl_nullable collective_cliques,\n       const ffi::ExecutionContext* absl_nullable execution_context);\n \n-  absl::Status ExecuteFfiHandler(RunId run_id, XLA_FFI_Handler* handler,\n-                                 XLA_FFI_ExecutionStage stage,\n-                                 se::Stream* stream,\n-                                 const ffi::ExecutionContext* execution_context,\n-                                 const BufferAllocations* buffer_allocations);\n-\n-  absl::Status ExecuteFfiHandler(RunId run_id, xla::ffi::Ffi& handler,\n-                                 xla::ffi::ExecutionStage stage,\n-                                 se::Stream* stream,\n-                                 const ffi::ExecutionContext* execution_context,\n-                                 const BufferAllocations* buffer_allocations);\n+  absl::Status ExecuteFfiHandler(\n+      RunId run_id, XLA_FFI_Handler* handler, XLA_FFI_ExecutionStage stage,\n+      se::Stream* stream, const ffi::ExecutionContext* execution_context,\n+      const BufferAllocations* buffer_allocations,\n+      const CollectiveParams* absl_nullable collective_params,\n+      CollectiveCliqueRequests* absl_nullable collective_clique_requests,\n+      const CollectiveCliques* absl_nullable collective_cliques);\n+\n+  absl::Status ExecuteFfiHandler(\n+      RunId run_id, xla::ffi::Ffi& handler, xla::ffi::ExecutionStage stage,\n+      se::Stream* stream, const ffi::ExecutionContext* execution_context,\n+      const BufferAllocations* buffer_allocations,\n+      const CollectiveParams* absl_nullable collective_params,\n+      CollectiveCliqueRequests* absl_nullable collective_clique_requests,\n+      const CollectiveCliques* absl_nullable collective_cliques);\n \n   // API version of the custom call. If not set, it means the custom call thunk\n   // was initialized from a non-registered function pointer and can't be"
        },
        {
            "sha": "6ba68b78d321454456ac428265097aa12e8bf3e3",
            "filename": "third_party/xla/xla/ffi/api/api.h",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fffi%2Fapi%2Fapi.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fffi%2Fapi%2Fapi.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fapi%2Fapi.h?ref=142aa13be42122fb709abbe8d620eaf2ee423099",
            "patch": "@@ -287,6 +287,12 @@ class Ffi {\n   // Creates an empty binding for the instantiate stage.\n   static Binding<ExecutionStage::kInstantiate> BindInstantiate();\n \n+  // Creates an empty binding for the prepare stage.\n+  static Binding<ExecutionStage::kPrepare> BindPrepare();\n+\n+  // Creates an empty binding for the initialize stage.\n+  static Binding<ExecutionStage::kInitialize> BindInitialize();\n+\n   // Automatic FFI binding that does binding specification inference from the\n   // `fn` type signature and binds `fn` to it. This enables a more concise FFI\n   // handler registration with fully automatic type inference at the cost of\n@@ -754,6 +760,14 @@ inline Binding<ExecutionStage::kInstantiate> Ffi::BindInstantiate() {\n   return Bind<ExecutionStage::kInstantiate>();\n }\n \n+inline Binding<ExecutionStage::kPrepare> Ffi::BindPrepare() {\n+  return Bind<ExecutionStage::kPrepare>();\n+}\n+\n+inline Binding<ExecutionStage::kInitialize> Ffi::BindInitialize() {\n+  return Bind<ExecutionStage::kInitialize>();\n+}\n+\n //===----------------------------------------------------------------------===//\n // Template metaprogramming to automatically infer Binding from invocable\n // object."
        },
        {
            "sha": "d0baf4fc3b7bb086c184fae2c35dc19aae01fb42",
            "filename": "third_party/xla/xla/ffi/api/c_api_internal.h",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fffi%2Fapi%2Fc_api_internal.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fffi%2Fapi%2Fc_api_internal.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fapi%2Fc_api_internal.h?ref=142aa13be42122fb709abbe8d620eaf2ee423099",
            "patch": "@@ -99,6 +99,23 @@ typedef XLA_FFI_Error* XLA_FFI_INTERNAL_Stream_Get(\n typedef XLA_FFI_Error* XLA_FFI_INTERNAL_DeviceMemoryAllocator_Get(\n     XLA_FFI_ExecutionContext* ctx, void** allocator);\n \n+// Returns a pointer to `xla::gpu::CollectiveParams` which allows FFI\n+// handlers to access collective execution parameters at run time.\n+typedef XLA_FFI_Error* XLA_FFI_INTERNAL_CollectiveParams_Get(\n+    XLA_FFI_ExecutionContext* ctx, void** collective_cliques);\n+\n+// Returns a pointer to `xla::gpu::CollectiveCliqueRequests` which allows\n+// FFI handlers to request GPU cliques at run time. Available only for FFI\n+// handlers executing at prepare stage.\n+typedef XLA_FFI_Error* XLA_FFI_INTERNAL_CollectiveCliqueRequests_Get(\n+    XLA_FFI_ExecutionContext* ctx, void** collective_clique_requests);\n+\n+// Returns a pointer to `xla::gpu::CollectiveClique` which allows FFI handlers\n+// to get access to requested and acquired GPU cliques. Available only for FFI\n+// handlers executing at execute stage.\n+typedef XLA_FFI_Error* XLA_FFI_INTERNAL_CollectiveCliques_Get(\n+    XLA_FFI_ExecutionContext* ctx, void** collective_clique);\n+\n //===----------------------------------------------------------------------===//\n // API access\n //===----------------------------------------------------------------------===//\n@@ -122,6 +139,10 @@ struct XLA_FFI_InternalApi {\n   _XLA_FFI_INTERNAL_API_STRUCT_FIELD(XLA_FFI_INTERNAL_Stream_Get);\n   _XLA_FFI_INTERNAL_API_STRUCT_FIELD(\n       XLA_FFI_INTERNAL_DeviceMemoryAllocator_Get);\n+  _XLA_FFI_INTERNAL_API_STRUCT_FIELD(XLA_FFI_INTERNAL_CollectiveParams_Get);\n+  _XLA_FFI_INTERNAL_API_STRUCT_FIELD(\n+      XLA_FFI_INTERNAL_CollectiveCliqueRequests_Get);\n+  _XLA_FFI_INTERNAL_API_STRUCT_FIELD(XLA_FFI_INTERNAL_CollectiveCliques_Get);\n };\n \n #undef _XLA_FFI_INTERNAL_API_STRUCT_FIELD"
        },
        {
            "sha": "fec2490662791f19d248a573f04549ed5eaeb2da",
            "filename": "third_party/xla/xla/ffi/ffi_api.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fffi%2Fffi_api.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fffi%2Fffi_api.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fffi_api.cc?ref=142aa13be42122fb709abbe8d620eaf2ee423099",
            "patch": "@@ -99,8 +99,9 @@ static XLA_FFI_ExecutionContext CreateExecutionContext(\n     }\n \n     BackendContext operator()(const CallOptions::GpuOptions& options) const {\n-      return XLA_FFI_ExecutionContext::GpuContext{options.stream,\n-                                                  options.allocator};\n+      return XLA_FFI_ExecutionContext::GpuContext{\n+          options.stream, options.allocator, options.collective_params,\n+          options.collective_clique_requests, options.collective_cliques};\n     }\n   };\n "
        },
        {
            "sha": "109ecc397b4d13409a63389b3d9d1a775108f8c4",
            "filename": "third_party/xla/xla/ffi/ffi_api.h",
            "status": "modified",
            "additions": 24,
            "deletions": 2,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fffi%2Fffi_api.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fffi%2Fffi_api.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fffi_api.h?ref=142aa13be42122fb709abbe8d620eaf2ee423099",
            "patch": "@@ -38,8 +38,6 @@ limitations under the License.\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/concurrency/chain.h\"\n \n-namespace xla::ffi {\n-\n // This is an implementation of XLA FFI API defined in `api/c_api.h` header. It\n // should be linked statically into the \"main\" XLA binary, and third party FFI\n // handlers can be linked and registered dynamically.\n@@ -48,6 +46,27 @@ namespace xla::ffi {\n // the same toolchain) can also use `api/c_api_internal.h` to get access to\n // various internal data structures.\n \n+//===----------------------------------------------------------------------===//\n+// Forward declare backend-specific types.\n+//===----------------------------------------------------------------------===//\n+\n+namespace Eigen {\n+struct ThreadPoolDevice;\n+}  // namespace Eigen\n+\n+namespace stream_executor {\n+class Stream;\n+class DeviceMemoryAllocator;\n+}  // namespace stream_executor\n+\n+namespace xla::gpu {\n+struct CollectiveParams;\n+class CollectiveCliqueRequests;\n+class CollectiveCliques;\n+}  // namespace xla::gpu\n+\n+namespace xla::ffi {\n+\n //===----------------------------------------------------------------------===//\n // Calling XLA FFI handlers\n //===----------------------------------------------------------------------===//\n@@ -63,6 +82,9 @@ struct CallOptions {\n   struct GpuOptions {\n     se::Stream* stream = nullptr;\n     se::DeviceMemoryAllocator* allocator = nullptr;\n+    const xla::gpu::CollectiveParams* collective_params = nullptr;\n+    xla::gpu::CollectiveCliqueRequests* collective_clique_requests = nullptr;\n+    const xla::gpu::CollectiveCliques* collective_cliques = nullptr;\n   };\n \n   using BackendOptions = std::variant<std::monostate, CpuOptions, GpuOptions>;"
        },
        {
            "sha": "6d2fd9fd533ae592a2d5206184660e3eada14c82",
            "filename": "third_party/xla/xla/ffi/ffi_internal_api.cc",
            "status": "modified",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fffi%2Fffi_internal_api.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fffi%2Fffi_internal_api.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fffi_internal_api.cc?ref=142aa13be42122fb709abbe8d620eaf2ee423099",
            "patch": "@@ -131,6 +131,44 @@ static XLA_FFI_Error* XLA_FFI_INTERNAL_DeviceMemoryAllocator_Get(\n       InvalidArgument(\"XLA FFI GPU context is not available\")};\n }\n \n+static XLA_FFI_Error* XLA_FFI_INTERNAL_CollectiveParams_Get(\n+    XLA_FFI_ExecutionContext* ctx, void** collective_params) {\n+  if (auto* gpu = std::get_if<XLA_FFI_ExecutionContext::GpuContext>(\n+          &ctx->backend_context)) {\n+    *collective_params = const_cast<xla::gpu::CollectiveParams*>(  // NOLINT\n+        gpu->collective_params);\n+    return nullptr;\n+  }\n+\n+  return new XLA_FFI_Error{\n+      InvalidArgument(\"XLA FFI GPU context is not available\")};\n+}\n+\n+static XLA_FFI_Error* XLA_FFI_INTERNAL_CollectiveCliqueRequests_Get(\n+    XLA_FFI_ExecutionContext* ctx, void** collective_clique_requests) {\n+  if (auto* gpu = std::get_if<XLA_FFI_ExecutionContext::GpuContext>(\n+          &ctx->backend_context)) {\n+    *collective_clique_requests = gpu->collective_clique_requests;\n+    return nullptr;\n+  }\n+\n+  return new XLA_FFI_Error{\n+      InvalidArgument(\"XLA FFI GPU context is not available\")};\n+}\n+\n+static XLA_FFI_Error* XLA_FFI_INTERNAL_CollectiveCliques_Get(\n+    XLA_FFI_ExecutionContext* ctx, void** collective_clique) {\n+  if (auto* gpu = std::get_if<XLA_FFI_ExecutionContext::GpuContext>(\n+          &ctx->backend_context)) {\n+    *collective_clique = const_cast<xla::gpu::CollectiveCliques*>(  // NOLINT\n+        gpu->collective_cliques);\n+    return nullptr;\n+  }\n+\n+  return new XLA_FFI_Error{\n+      InvalidArgument(\"XLA FFI GPU context is not available\")};\n+}\n+\n const XLA_FFI_InternalApi* GetInternalApi() {\n   static XLA_FFI_InternalApi internal_api = {\n       // Generic XLA APIs available on all XLA backends.\n@@ -148,6 +186,9 @@ const XLA_FFI_InternalApi* GetInternalApi() {\n       // XLA:GPU specific APIs.\n       XLA_FFI_INTERNAL_Stream_Get,\n       XLA_FFI_INTERNAL_DeviceMemoryAllocator_Get,\n+      XLA_FFI_INTERNAL_CollectiveParams_Get,\n+      XLA_FFI_INTERNAL_CollectiveCliqueRequests_Get,\n+      XLA_FFI_INTERNAL_CollectiveCliques_Get,\n   };\n \n   return &internal_api;"
        },
        {
            "sha": "7faa963ca0b250e53eafe1b67cc357d18a893c6a",
            "filename": "third_party/xla/xla/ffi/ffi_structs.h",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fffi%2Fffi_structs.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Fffi%2Fffi_structs.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fffi%2Fffi_structs.h?ref=142aa13be42122fb709abbe8d620eaf2ee423099",
            "patch": "@@ -40,6 +40,12 @@ class Stream;\n class DeviceMemoryAllocator;\n }  // namespace stream_executor\n \n+namespace xla::gpu {\n+struct CollectiveParams;\n+class CollectiveCliqueRequests;\n+class CollectiveCliques;\n+}  // namespace xla::gpu\n+\n //===----------------------------------------------------------------------===//\n // XLA FFI C structs definition\n //===----------------------------------------------------------------------===//\n@@ -60,6 +66,9 @@ struct XLA_FFI_ExecutionContext {\n   struct GpuContext {\n     stream_executor::Stream* stream = nullptr;\n     stream_executor::DeviceMemoryAllocator* allocator = nullptr;\n+    const xla::gpu::CollectiveParams* collective_params = nullptr;\n+    xla::gpu::CollectiveCliqueRequests* collective_clique_requests = nullptr;\n+    const xla::gpu::CollectiveCliques* collective_cliques = nullptr;\n   };\n \n   using BackendContext = std::variant<std::monostate, CpuContext, GpuContext>;"
        },
        {
            "sha": "7be83f51fabf78fb2ec417656d90341844a8723e",
            "filename": "third_party/xla/xla/tests/BUILD",
            "status": "modified",
            "additions": 45,
            "deletions": 0,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2FBUILD?ref=142aa13be42122fb709abbe8d620eaf2ee423099",
            "patch": "@@ -2960,6 +2960,51 @@ xla_test(\n     ],\n )\n \n+xla_test(\n+    name = \"collective_ops_ffi_test\",\n+    srcs = [\"collective_ops_ffi_test.cc\"],\n+    backend_tags = {\n+        \"gpu\": [\n+            \"multi_gpu\",\n+        ],\n+        \"nvgpu_any\": [\n+            \"no_oss\",\n+        ],\n+    },\n+    backends = [\"gpu\"],\n+    deps = [\n+        \":collective_ops_e2e_test_base\",\n+        \":literal_test_util\",\n+        \":test_utils\",\n+        \":xla_internal_test_main\",\n+        \"//xla:future\",\n+        \"//xla:literal\",\n+        \"//xla:status_macros\",\n+        \"//xla/backends/gpu:ffi\",\n+        \"//xla/backends/gpu/collectives:gpu_clique_key\",\n+        \"//xla/backends/gpu/collectives:gpu_collectives\",\n+        \"//xla/backends/gpu/runtime:collective_clique_requests\",\n+        \"//xla/backends/gpu/runtime:collective_cliques\",\n+        \"//xla/backends/gpu/runtime:collective_execution\",\n+        \"//xla/backends/gpu/runtime:collective_params\",\n+        \"//xla/core/collectives:communicator\",\n+        \"//xla/ffi\",\n+        \"//xla/ffi:ffi_api\",\n+        \"//xla/ffi/api:c_api\",\n+        \"//xla/service:collective_ops_utils\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:stream\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/platform:test\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n xla_test(\n     name = \"collective_metadata_test\",\n     srcs = [\"collective_metadata_test.cc\"],"
        },
        {
            "sha": "e852a9cfaebf16fd59dd2fbf01ea4392ca2cd686",
            "filename": "third_party/xla/xla/tests/collective_ops_ffi_test.cc",
            "status": "added",
            "additions": 167,
            "deletions": 0,
            "changes": 167,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_ffi_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/142aa13be42122fb709abbe8d620eaf2ee423099/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_ffi_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_ffi_test.cc?ref=142aa13be42122fb709abbe8d620eaf2ee423099",
            "patch": "@@ -0,0 +1,167 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstdint>\n+#include <utility>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n+#include \"xla/backends/gpu/ffi.h\"\n+#include \"xla/backends/gpu/runtime/collective_clique_requests.h\"\n+#include \"xla/backends/gpu/runtime/collective_execution.h\"\n+#include \"xla/backends/gpu/runtime/collective_params.h\"\n+#include \"xla/core/collectives/communicator.h\"\n+#include \"xla/ffi/api/c_api.h\"\n+#include \"xla/ffi/ffi.h\"\n+#include \"xla/ffi/ffi_api.h\"\n+#include \"xla/future.h\"\n+#include \"xla/literal.h\"\n+#include \"xla/service/collective_ops_utils.h\"\n+#include \"xla/status_macros.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/stream.h\"\n+#include \"xla/tests/collective_ops_e2e_test_base.h\"\n+#include \"xla/tests/literal_test_util.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/platform/test.h\"\n+\n+namespace xla::gpu {\n+\n+class CollectiveOpsTestFFI : public CollectiveOpsE2ETestBase {};\n+\n+static constexpr int64_t kNumReplicas = 2;\n+\n+// In this test we execute all collective operations across all devices.\n+static ReplicaGroup AllDevices() {\n+  ReplicaGroup group;\n+  for (int64_t i = 0; i < kNumReplicas; ++i) {\n+    group.add_replica_ids(i);\n+  }\n+  return group;\n+}\n+\n+// This is a prepare handler that tells XLA:GPU runtime what collective cliques\n+// should be acquired before the execution starts. All collective operations\n+// must let XLA:GPU runtime know what cliques they need ahead of time.\n+static absl::Status PrepareAllReduce(\n+    ffi::RemainingArgs, ffi::RemainingRets,\n+    const CollectiveParams* collective_params,\n+    CollectiveCliqueRequests* clique_requests) {\n+  TF_RET_CHECK(collective_params && clique_requests);\n+\n+  // Request a clique that covers all devices (this test runs on 2 gpus).\n+  TF_ASSIGN_OR_RETURN(\n+      GpuCliqueKey clique_key,\n+      GetGpuCliqueKey(\n+          *collective_params, {AllDevices()},\n+          CollectiveOpGroupMode::COLLECTIVE_OP_GROUP_MODE_FLATTENED_ID,\n+          AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE));\n+\n+  // Ask XLA:GPU runtime to acquire a clique for this key. Later we will be able\n+  // to get access to it from the execute handler.\n+  TF_RETURN_IF_ERROR(clique_requests->RequestClique(clique_key));\n+\n+  return absl::OkStatus();\n+}\n+\n+// FFI handler that uses XLA:GPU collectives API to perform an all reduce. This\n+// is just a test that demonstrates how to use XLA:GPU collectives API in an FFI\n+// handler, builtin all-reduce is a much better option.\n+static absl::Status AllReduce(se::Stream* stream, ffi::BufferR0<U32> src,\n+                              ffi::Result<ffi::BufferR0<U32>> dst,\n+                              const CollectiveParams* collective_params,\n+                              const CollectiveCliques* collective_cliques) {\n+  TF_RET_CHECK(collective_params && collective_cliques);\n+\n+  TF_ASSIGN_OR_RETURN(\n+      GpuCliqueKey clique_key,\n+      GetGpuCliqueKey(\n+          *collective_params, {AllDevices()},\n+          CollectiveOpGroupMode::COLLECTIVE_OP_GROUP_MODE_FLATTENED_ID,\n+          AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE));\n+\n+  // Get the communicator for the requested clique.\n+  TF_ASSIGN_OR_RETURN(Communicator * comm,\n+                      collective_cliques->GetComm(\n+                          clique_key, collective_params->global_device_id));\n+\n+  Future<> future = comm->AllReduce(\n+      src.device_memory(), dst->device_memory(), src.element_type(),\n+      src.element_count(), ReductionKind::SUM, GpuCollectives::On(*stream));\n+  return future.Await();\n+}\n+\n+XLA_FFI_DEFINE_HANDLER(kPrepareAllReduce, PrepareAllReduce,\n+                       ffi::Ffi::BindPrepare()\n+                           .RemainingArgs()\n+                           .RemainingRets()\n+                           .Ctx<ffi::CollectiveParams>()\n+                           .Ctx<ffi::CollectiveCliqueRequests>());\n+\n+XLA_FFI_DEFINE_HANDLER(kAllReduce, AllReduce,\n+                       ffi::Ffi::Bind()\n+                           .Ctx<ffi::Stream>()\n+                           .Arg<ffi::BufferR0<U32>>()  // src\n+                           .Ret<ffi::BufferR0<U32>>()  // dst\n+                           .Ctx<ffi::CollectiveParams>()\n+                           .Ctx<ffi::CollectiveCliques>());\n+\n+// Register handler bundle for the custom all-reduce operation.\n+XLA_FFI_REGISTER_HANDLER(ffi::GetXlaFfiApi(), \"__xla_test$$all_reduce\", \"gpu\",\n+                         XLA_FFI_Handler_Bundle{\n+                             /*instantiate=*/nullptr,\n+                             /*prepare=*/kPrepareAllReduce,\n+                             /*initialize=*/nullptr,\n+                             /*execute=*/kAllReduce,\n+                         });\n+\n+TEST_F(CollectiveOpsTestFFI, AllReduce) {\n+  if (hlo_runner_->device_count() < kNumReplicas) {\n+    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+                 << hlo_runner_->device_count() << \" available)\";\n+  }\n+\n+  constexpr absl::string_view hlo_string = R\"(\n+      HloModule m\n+\n+      ENTRY test_computation {\n+        id = u32[] replica-id()\n+        ROOT all-reduce = u32[] custom-call(id),\n+          custom_call_target=\"__xla_test$$all_reduce\",\n+          api_version=API_VERSION_TYPED_FFI\n+      }\n+    )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(hlo_string, kNumReplicas));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(ExecutionResult execution_result,\n+                          ExecuteReplicated(std::move(module)));\n+\n+  absl::Span<const Literal> results = execution_result.results;\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+\n+  // sum [0, num_devices)\n+  const uint32_t expected = kNumReplicas * (kNumReplicas - 1) / 2;\n+  for (int i = 0; i < kNumReplicas; ++i) {\n+    LiteralTestUtil::ExpectR0Equal<uint32_t>(expected, results[i]);\n+  }\n+}\n+\n+}  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 545,
        "additions": 492,
        "deletions": 53
    }
}