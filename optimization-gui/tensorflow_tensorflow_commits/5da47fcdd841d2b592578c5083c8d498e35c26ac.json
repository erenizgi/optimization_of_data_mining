{
    "author": "basioli-k",
    "message": "[XLA:GPU][codegen] Emit shlo for broadcast_in_dim and lower to equivalent triton op.\n\nPiperOrigin-RevId: 820598440",
    "sha": "5da47fcdd841d2b592578c5083c8d498e35c26ac",
    "files": [
        {
            "sha": "219439315681881041520de58152adb5bc94c8d0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5da47fcdd841d2b592578c5083c8d498e35c26ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5da47fcdd841d2b592578c5083c8d498e35c26ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=5da47fcdd841d2b592578c5083c8d498e35c26ac",
            "patch": "@@ -360,13 +360,6 @@ Value Minimum(EmitterLocOpBuilder& b, const se::DeviceDescription& device_info,\n       values[0], values[1]);\n }\n \n-ScalarOrTensor Splat(EmitterLocOpBuilder& b, ScalarOrTensor value,\n-                     ArrayRef<int64_t> shape) {\n-  CHECK(!shape.empty());\n-  auto type = mlir::RankedTensorType::get(shape, value.getType());\n-  return ScalarOrTensor(b.create<mt::SplatOp>(type, value.UnwrapUnsafe()));\n-}\n-\n bool IsSupportedElementwiseLibdeviceFunction(const HloInstruction& hlo) {\n   auto dev_fn_id = GetTargetDeviceFunctionID(hlo.opcode());\n   if (!dev_fn_id.has_value()) {"
        },
        {
            "sha": "27853eedecbc6c2d654ad0296b2ec2bb1185c37a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.h",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5da47fcdd841d2b592578c5083c8d498e35c26ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5da47fcdd841d2b592578c5083c8d498e35c26ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h?ref=5da47fcdd841d2b592578c5083c8d498e35c26ac",
            "patch": "@@ -193,9 +193,6 @@ inline mlir::Value OnesLike(EmitterLocOpBuilder& b, mlir::Value x) {\n \n bool IsFp8Type(mlir::Type t);\n \n-ScalarOrTensor Splat(EmitterLocOpBuilder& b, ScalarOrTensor value,\n-                     llvm::ArrayRef<int64_t> shape);\n-\n // Triton type conversions.\n mlir::Value Cast(EmitterLocOpBuilder& b, mlir::Value value,\n                  mlir::Type dst_element_ty);"
        },
        {
            "sha": "681486d275bb9783f89f2826c848190b7818c6f7",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 23,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5da47fcdd841d2b592578c5083c8d498e35c26ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5da47fcdd841d2b592578c5083c8d498e35c26ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=5da47fcdd841d2b592578c5083c8d498e35c26ac",
            "patch": "@@ -300,38 +300,37 @@ absl::StatusOr<TileInfo> TileInfo::Construct(\n \n using TensorValue = mlir::TypedValue<mlir::RankedTensorType>;\n \n-ScalarOrTensor Broadcast(EmitterLocOpBuilder b, TensorValue value,\n-                         ArrayRef<int64_t> shape) {\n-  return ScalarOrTensor(\n-      b.create<ttir::BroadcastOp>(value.getType().clone(shape), value));\n-}\n-\n // Same as HLO BroadcastInDims. The sorted indices in `dims` specify the mapping\n // of the input dimensions to the output dimensions.\n ScalarOrTensor BroadcastInDims(EmitterLocOpBuilder b, ScalarOrTensor value,\n                                ArrayRef<int64_t> output_shape,\n                                ArrayRef<int64_t> dims) {\n   CHECK(llvm::is_sorted(dims)) << \"broadcast dims must be sorted\";\n \n+  mlir::TypedValue<mlir::RankedTensorType> broadcast_in_dim_input;\n+\n   if (value.IsScalar()) {\n-    return Splat(b, value, output_shape);\n-  }\n-  TensorValue input_tensor = value.UnwrapTensor();\n-  auto input_shape = input_tensor.getType().getShape();\n-  int64_t axis = 0;\n-  int64_t input_dim_id = 0;\n-  for (int output_dim_id = 0; output_dim_id < output_shape.size();\n-       output_dim_id++) {\n-    if (input_dim_id < dims.size() && output_dim_id == dims[input_dim_id]) {\n-      // The dim is not broadcasted. Validate matching dim sizes.\n-      CHECK_EQ(input_shape[input_dim_id], output_shape[output_dim_id]);\n-      ++input_dim_id;\n-      axis = output_dim_id + 1;\n-      continue;\n-    }\n-    input_tensor = b.create<ttir::ExpandDimsOp>(input_tensor, axis);\n+    CHECK(dims.empty()) << \"scalar broadcast must have empty dims\";\n+    auto scalar_tensor_type =\n+        mlir::RankedTensorType::get(/*shape=*/{}, value.getType());\n+\n+    broadcast_in_dim_input = b.create<mlir::tensor::FromElementsOp>(\n+                                  scalar_tensor_type, value.UnwrapScalar())\n+                                 .getResult();\n+  } else {\n+    broadcast_in_dim_input = value.UnwrapTensor();\n   }\n-  return Broadcast(b, input_tensor, output_shape);\n+\n+  auto result_type = mlir::RankedTensorType::get(\n+      output_shape, broadcast_in_dim_input.getType().getElementType());\n+\n+  return ScalarOrTensor(b.create<stablehlo::BroadcastInDimOp>(\n+      result_type, broadcast_in_dim_input, dims));\n+}\n+\n+ScalarOrTensor Splat(EmitterLocOpBuilder b, ScalarOrTensor value,\n+                     ArrayRef<int64_t> output_shape) {\n+  return BroadcastInDims(b, value, output_shape, /*dims=*/{});\n }\n \n ScalarOrTensor Iota(EmitterLocOpBuilder b, int32_t limit) {"
        },
        {
            "sha": "43bd4ef1f215b1df671990041f9119a61218956c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 14,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5da47fcdd841d2b592578c5083c8d498e35c26ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5da47fcdd841d2b592578c5083c8d498e35c26ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=5da47fcdd841d2b592578c5083c8d498e35c26ac",
            "patch": "@@ -876,7 +876,7 @@ ENTRY main {\n       CreateXTileIrAndFileCheck(this, kHloText, \"triton_reduction_computation\",\n                                 R\"(\n CHECK:  stablehlo.iota\n-CHECK-COUNT-4:  tt.expand_dims\n+CHECK:  stablehlo.broadcast_in_dim\n CHECK:  \"tt.reduce\"(%[[SELECT:.*]]) <{axis = 2 : i32}>\n           )\"));\n \n@@ -928,8 +928,7 @@ ENTRY main {\n ; Make sure input reduction tile is padded with a neutral value.\n CHECK:  %[[LOAD:.*]] = triton_xla.extract\n CHECK:  %[[RANGE:.*]] = stablehlo.iota\n-CHECK:  %[[EXPAND:.*]] = tt.expand_dims %[[RANGE]]\n-CHECK:  %[[BROADCAST:.*]] = tt.broadcast %[[EXPAND]]\n+CHECK:  %[[BROADCAST:.*]] = stablehlo.broadcast_in_dim %[[RANGE]]\n CHECK:  %[[CMPI:.*]] = arith.cmpi slt, %[[BROADCAST]]\n CHECK:  %[[SELECT:.*]] = arith.select %[[CMPI]], %[[LOAD]]\n CHECK:  \"tt.reduce\"(%[[SELECT]]) <{axis = 0 : i32}>\n@@ -1842,13 +1841,15 @@ ENTRY main {\n // CHECK: %[[PAD_VALUE:.*]] = arith.constant 1.000000e+00 : f32\n // CHECK: %[[TILE_OFFSET:.*]] = xla.apply_indexing\n // CHECK: %[[IOTA_VAL:.*]] = stablehlo.iota dim = 0 : tensor<32xi32>\n-// CHECK: %[[IOTA:.*]] = tt.broadcast %[[IOTA_VAL]] : tensor<32xi32> -> tensor<32xi32>\n+// CHECK: %[[IOTA:.*]] = stablehlo.broadcast_in_dim %[[IOTA_VAL]], dims = [0] : (tensor<32xi32>) -> tensor<32xi32>\n // CHECK: %[[TILE_OFFSET_I32:.*]] = arith.index_cast %[[TILE_OFFSET]]\n // CHECK: %[[C17:.*]] = arith.constant 17 : i32\n // CHECK: %[[THRESHOLD:.*]] = arith.subi %[[C17]], %[[TILE_OFFSET_I32]]\n-// CHECK: %[[THRESHOLD_SPLAT:.*]] = tt.splat %[[THRESHOLD]]\n+// CHECK: %[[FROM_ELEMENTS_THRESHOLD:.*]] = tensor.from_elements %[[THRESHOLD]]\n+// CHECK: %[[THRESHOLD_SPLAT:.*]] = stablehlo.broadcast_in_dim %[[FROM_ELEMENTS_THRESHOLD]], dims = []\n // CHECK: %[[MASK:.*]] = arith.cmpi slt, %[[IOTA]], %[[THRESHOLD_SPLAT]]\n-// CHECK: %[[PAD_SPLAT:.*]] = tt.splat %[[PAD_VALUE]] : f32 -> tensor<32xf32>\n+// CHECK: %[[FROM_ELEMENTS_PAD_VALUE:.*]] = tensor.from_elements %[[PAD_VALUE]]\n+// CHECK: %[[PAD_SPLAT:.*]] = stablehlo.broadcast_in_dim %[[FROM_ELEMENTS_PAD_VALUE]], dims = []\n // CHECK: %[[SELECT:.*]] = arith.select %[[MASK]], %[[EXTRACT]], %[[PAD_SPLAT]]\n \n // CHECK:   triton_xla.insert %[[SELECT]] into %[[OUT]]\n@@ -1908,12 +1909,10 @@ ENTRY main {\n       CreateXTileIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n // CHECK: triton_xla.extract {{.*}} : tensor<32x16xf32>\n // CHECK: stablehlo.iota dim = 0 : tensor<32xi32>\n-// CHECK: tt.expand_dims\n-// CHECK: tt.broadcast\n+// CHECK: stablehlo.broadcast_in_dim\n // CHECK: arith.cmpi\n // CHECK: stablehlo.iota dim = 0 : tensor<16xi32>\n-// CHECK: tt.expand_dims\n-// CHECK: tt.broadcast\n+// CHECK: stablehlo.broadcast_in_dim\n // CHECK: arith.cmpi slt\n // CHECK: arith.andi\n // CHECK: arith.select\n@@ -2401,10 +2400,20 @@ ENTRY main {\n           \"num_ctas\":\"1\",\n           \"num_stages\":\"1\"}}}\n })\";\n-  TF_EXPECT_OK(\n-      CreateTritonIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto xtile_module_and_hlo_module,\n+      CreateXTileIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n+CHECK:       %[[RES_FROM_ELEMENTS:.*]] = tensor.from_elements %[[ARG:.*]] : tensor<f32>\n+CHECK:       stablehlo.broadcast_in_dim %[[RES_FROM_ELEMENTS]], dims = []\n+          )\"));\n+\n+  TF_EXPECT_OK(LowerXTileIrToTritonAndFileCheck(\n+      this, xtile_module_and_hlo_module.first.get(), R\"(\n CHECK:       tt.splat {{.*}} f32 -> tensor<8x4xf32>\n-)\"));\n+)\",\n+      GetFusionInstruction(*xtile_module_and_hlo_module.second,\n+                           \"triton_computation\")));\n }\n \n TEST_F(TritonEmitterTest, PredOutputIsStoredCorrectly) {\n@@ -2669,7 +2678,7 @@ CHECK:      %[[MUL:.*]] = arith.muli %[[RANGE]], {{.*}} : tensor<64xi32>\n CHECK:      arith.addi{{.*}} %[[MUL]]\n             // Omit the data type below, since it depends on a test parameter\n             // and is not abbreviated the same as in HLO.\n-CHECK:      tt.broadcast {{.*}} -> tensor<1x2x64x8x\n+CHECK:      stablehlo.broadcast_in_dim {{.*}}, dims = [2] : {{.*}}\n           )\"));\n \n   TF_ASSERT_OK(LowerXTileIrToTritonAndFileCheck("
        },
        {
            "sha": "529534d0667d01efa4f13f251149bd9f009ad8fe",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_shared_dialect_test.cc",
            "status": "modified",
            "additions": 63,
            "deletions": 3,
            "changes": 66,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5da47fcdd841d2b592578c5083c8d498e35c26ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5da47fcdd841d2b592578c5083c8d498e35c26ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_shared_dialect_test.cc?ref=5da47fcdd841d2b592578c5083c8d498e35c26ac",
            "patch": "@@ -39,7 +39,7 @@ namespace {\n \n using XTileDialectTest = HloTestBaseWithSymbolicExprContext;\n \n-TEST_F(XTileDialectTest, TestEmittingStableHloTranspose) {\n+TEST_F(XTileDialectTest, HloTransposeIsLoweredToStableHloTranspose) {\n   constexpr absl::string_view kHloText = R\"(\n HloModule t\n \n@@ -68,7 +68,7 @@ CHECK: %[[RES:.*]] = stablehlo.transpose %[[ARG:.*]], dims = [1, 0] : (tensor<32\n )\"));\n }\n \n-TEST_F(XTileDialectTest, TestEmittingTensorBitcast) {\n+TEST_F(XTileDialectTest, HloBitcastIsLoweredToTensorBitcast) {\n   constexpr absl::string_view kHloText = R\"(\n HloModule t, is_scheduled=true\n \n@@ -97,7 +97,7 @@ CHECK: %[[RES:.*]] = tensor.bitcast %[[ARG:.*]] : tensor<16x32xf32> to tensor<16\n )\"));\n }\n \n-TEST_F(XTileDialectTest, TestEmittingStableHloIota) {\n+TEST_F(XTileDialectTest, HloIotaIsLoweredToStableHloIota) {\n   constexpr absl::string_view kHloText = R\"(\n HloModule t, is_scheduled=true\n \n@@ -124,6 +124,66 @@ CHECK: %[[RES:.*]] = stablehlo.iota dim = 0 : tensor<16xi32>\n )\"));\n }\n \n+TEST_F(XTileDialectTest, HloBroadcastInDimIsLoweredToStableHloBroadcastInDim) {\n+  constexpr absl::string_view kHloText = R\"(\n+HloModule t\n+\n+broadcast_in_dim_fusion {\n+  p0 = f32[150,160] parameter(0)\n+  ROOT broadcast = f32[150,160,31] broadcast(p0), dimensions={0,1}\n+}\n+\n+ENTRY e {\n+  p0 = f32[150,160] parameter(0)\n+  ROOT custom-call = f32[150,160,31] fusion(p0), kind=kCustom,\n+    calls=broadcast_in_dim_fusion,\n+    backend_config={\"fusion_backend_config\": {kind: \"__triton\"}}\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloText));\n+\n+  BlockLevelParameters block_level_parameters;\n+  block_level_parameters.output_tile_sizes = {{16, 32, 8}};\n+\n+  TF_EXPECT_OK(CreateXTileIrAndFileCheck(\n+      this, *module->GetComputationWithName(\"broadcast_in_dim_fusion\"),\n+      block_level_parameters,\n+      R\"(\n+CHECK: %[[RES:.*]] = stablehlo.broadcast_in_dim %[[ARG:.*]], dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x8xf32>\n+)\"));\n+}\n+\n+TEST_F(XTileDialectTest,\n+       HloZeroDimensionalBroadcastIsLoweredToStableHloBroadcastInDim) {\n+  constexpr absl::string_view kHloText = R\"(\n+HloModule t\n+\n+broadcast_in_dim_fusion {\n+  p0 = f32[] parameter(0)\n+  ROOT broadcast = f32[150,160,31] broadcast(p0), dimensions={}\n+}\n+\n+ENTRY e {\n+  p0 = f32[] parameter(0)\n+  ROOT custom-call = f32[150,160,31] fusion(p0), kind=kCustom,\n+    calls=broadcast_in_dim_fusion,\n+    backend_config={\"fusion_backend_config\": {kind: \"__triton\"}}\n+})\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloText));\n+\n+  BlockLevelParameters block_level_parameters;\n+  block_level_parameters.output_tile_sizes = {{16, 32, 8}};\n+\n+  TF_EXPECT_OK(CreateXTileIrAndFileCheck(\n+      this, *module->GetComputationWithName(\"broadcast_in_dim_fusion\"),\n+      block_level_parameters,\n+      R\"(\n+CHECK: %[[RES_FROM_ELEMENTS:.*]] = tensor.from_elements %[[ARG:.*]] : tensor<f32>\n+CHECK: %[[RES:.*]] = stablehlo.broadcast_in_dim %[[RES_FROM_ELEMENTS]], dims = [] : (tensor<f32>) -> tensor<16x32x8xf32>\n+)\"));\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "42e0fb33043462ef0ccafde71afacf69490dd521",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/stablehlo_lower_to_triton.cc",
            "status": "modified",
            "additions": 53,
            "deletions": 2,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5da47fcdd841d2b592578c5083c8d498e35c26ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fstablehlo_lower_to_triton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5da47fcdd841d2b592578c5083c8d498e35c26ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fstablehlo_lower_to_triton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fstablehlo_lower_to_triton.cc?ref=5da47fcdd841d2b592578c5083c8d498e35c26ac",
            "patch": "@@ -17,8 +17,10 @@ limitations under the License.\n #include <memory>\n #include <utility>\n \n+#include \"absl/log/check.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/Diagnostics.h\"\n@@ -30,6 +32,7 @@ limitations under the License.\n #include \"mlir/Transforms/DialectConversion.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"stablehlo/dialect/StablehloOps.h\"\n+#include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n \n namespace mlir::triton::xla {\n@@ -91,14 +94,62 @@ class LowerIotaToMakeRange : public mlir::OpRewritePattern<stablehlo::IotaOp> {\n   }\n };\n \n+class LowerBroadcastInDim\n+    : public mlir::OpRewritePattern<stablehlo::BroadcastInDimOp> {\n+ public:\n+  using OpRewritePattern::OpRewritePattern;\n+\n+ private:\n+  mlir::LogicalResult matchAndRewrite(\n+      stablehlo::BroadcastInDimOp op,\n+      mlir::PatternRewriter& rewriter) const override {\n+    ::xla::EmitterLocOpBuilder builder(op.getLoc(), rewriter);\n+    auto input_tensor = op.getOperand();\n+    auto input_shape = input_tensor.getType().getShape();\n+    auto output_shape = op.getResult().getType().getShape();\n+    auto broadcast_dims = op.getBroadcastDimensions();\n+\n+    if (input_shape.empty()) {\n+      auto broadcast_dim_input = op.getOperand();\n+      auto broadcast_dim_input_element_type =\n+          broadcast_dim_input.getType().getElementType();\n+\n+      auto extracted = rewriter.create<mlir::tensor::ExtractOp>(\n+          op.getLoc(), broadcast_dim_input_element_type, broadcast_dim_input);\n+\n+      rewriter.replaceOpWithNewOp<ttir::SplatOp>(op, op.getResult().getType(),\n+                                                 extracted);\n+      return mlir::success();\n+    }\n+    int64_t axis = 0;\n+    int64_t input_dim_id = 0;\n+    for (int output_dim_id = 0; output_dim_id < output_shape.size();\n+         output_dim_id++) {\n+      if (input_dim_id < broadcast_dims.size() &&\n+          output_dim_id == broadcast_dims[input_dim_id]) {\n+        // The dim is not broadcasted. Validate matching dim sizes.\n+        CHECK_EQ(input_shape[input_dim_id], output_shape[output_dim_id]);\n+        ++input_dim_id;\n+        axis = output_dim_id + 1;\n+        continue;\n+      }\n+      input_tensor = builder.create<ttir::ExpandDimsOp>(input_tensor, axis);\n+    }\n+    rewriter.replaceOpWithNewOp<ttir::BroadcastOp>(op, op.getResult().getType(),\n+                                                   input_tensor);\n+\n+    return mlir::success();\n+  }\n+};\n+\n class StableHLOLowerToTritonPass\n     : public impl::StableHLOLowerToTritonPassBase<StableHLOLowerToTritonPass> {\n  public:\n   void runOnOperation() override {\n     mlir::MLIRContext* mlir_context = &getContext();\n     mlir::RewritePatternSet patterns(mlir_context);\n-    patterns.add<LowerTranspose>(mlir_context);\n-    patterns.add<LowerTranspose, LowerIotaToMakeRange>(mlir_context);\n+    patterns.add<LowerTranspose, LowerIotaToMakeRange, LowerBroadcastInDim>(\n+        mlir_context);\n \n     if (mlir::failed(\n             mlir::applyPatternsGreedily(getOperation(), std::move(patterns)))) {"
        },
        {
            "sha": "8cfdc6bef16e50f15d8771788ce32568b3933925",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/stable_hlo_to_triton_lowering.mlir",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5da47fcdd841d2b592578c5083c8d498e35c26ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fstable_hlo_to_triton_lowering.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5da47fcdd841d2b592578c5083c8d498e35c26ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fstable_hlo_to_triton_lowering.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Fstable_hlo_to_triton_lowering.mlir?ref=5da47fcdd841d2b592578c5083c8d498e35c26ac",
            "patch": "@@ -33,3 +33,23 @@ func.func @lower_iota_on_non_signed_32_bit_tensor_falls_back_to_stablehlo() -> t\n   // CHECK: return %[[RES]] : tensor<8xui32>\n   return %0 : tensor<8xui32>\n }\n+\n+// CHECK: func @lower_broadcast_in_dim(%[[ARG0:.*]]: tensor<2x4xf32>) -> tensor<8x2x4x16xf32>\n+func.func @lower_broadcast_in_dim(%arg0: tensor<2x4xf32>) -> tensor<8x2x4x16xf32> {\n+  // CHECK: %[[RES_EXPAND_DIMS_0:.*]] = tt.expand_dims %[[ARG0]] {axis = 0 : i32} : tensor<2x4xf32> -> tensor<1x2x4xf32>\n+  // CHECK: %[[RES_EXPAND_DIMS_1:.*]] = tt.expand_dims %[[RES_EXPAND_DIMS_0]] {axis = 3 : i32} : tensor<1x2x4xf32> -> tensor<1x2x4x1xf32>\n+  // CHECK: %[[RES:.*]] = tt.broadcast %[[RES_EXPAND_DIMS_1]] : tensor<1x2x4x1xf32> -> tensor<8x2x4x16xf32>\n+  %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<2x4xf32>) -> tensor<8x2x4x16xf32>\n+  // CHECK: return %[[RES]] : tensor<8x2x4x16xf32>\n+  return %0 : tensor<8x2x4x16xf32>\n+}\n+\n+// CHECK: func @lower_broadcast_in_dim_on_0d_tensor_produced_by_from_elements_to_splat(%[[ARG0:.*]]: f32) -> tensor<4x2xf32>\n+func.func @lower_broadcast_in_dim_on_0d_tensor_produced_by_from_elements_to_splat(%arg0: f32) -> tensor<4x2xf32> {\n+  // CHECK-NOT: tensor.from_elements\n+  // CHECK: %[[RES:.*]] = tt.splat %[[ARG0]] : f32 -> tensor<4x2xf32>\n+  %from_elements = tensor.from_elements %arg0 : tensor<f32>\n+  %0 = stablehlo.broadcast_in_dim %from_elements, dims = [] : (tensor<f32>) -> tensor<4x2xf32>\n+  // CHECK: return %[[RES]] : tensor<4x2xf32>\n+  return %0 : tensor<4x2xf32>\n+}"
        }
    ],
    "stats": {
        "total": 233,
        "additions": 181,
        "deletions": 52
    }
}