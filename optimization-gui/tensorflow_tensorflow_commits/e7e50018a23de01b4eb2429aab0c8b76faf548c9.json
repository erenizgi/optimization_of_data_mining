{
    "author": "tensorflower-gardener",
    "message": "Reverts 7144ba7d809c97e0953415cdc1582324e110fb69\n\nPiperOrigin-RevId: 821972512",
    "sha": "e7e50018a23de01b4eb2429aab0c8b76faf548c9",
    "files": [
        {
            "sha": "701b607575e3b40c948fff75820319268d07c63c",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7e50018a23de01b4eb2429aab0c8b76faf548c9/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7e50018a23de01b4eb2429aab0c8b76faf548c9/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2FBUILD?ref=e7e50018a23de01b4eb2429aab0c8b76faf548c9",
            "patch": "@@ -323,11 +323,12 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/hlo/analysis:alias_info\",\n         \"//xla/hlo/analysis:hlo_alias_analysis\",\n-        \"//xla/hlo/analysis:hlo_dataflow_analysis\",\n+        \"//xla/hlo/analysis:tuple_points_to_analysis\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/service:buffer_value\",\n         \"//xla/service:hlo_value\",\n+        \"//xla/service:logical_buffer\",\n         \"//xla/service/heap_simulator\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\","
        },
        {
            "sha": "f10e5a8c7e832bdb757005ba49d6fe1472da6cf1",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/hlo_memory_scheduler.cc",
            "status": "modified",
            "additions": 81,
            "deletions": 72,
            "changes": 153,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7e50018a23de01b4eb2429aab0c8b76faf548c9/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fhlo_memory_scheduler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7e50018a23de01b4eb2429aab0c8b76faf548c9/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fhlo_memory_scheduler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fhlo_memory_scheduler.cc?ref=e7e50018a23de01b4eb2429aab0c8b76faf548c9",
            "patch": "@@ -38,7 +38,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"xla/hlo/analysis/alias_info.h\"\n #include \"xla/hlo/analysis/hlo_alias_analysis.h\"\n-#include \"xla/hlo/analysis/hlo_dataflow_analysis.h\"\n+#include \"xla/hlo/analysis/tuple_points_to_analysis.h\"\n #include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -47,6 +47,7 @@ limitations under the License.\n #include \"xla/service/buffer_value.h\"\n #include \"xla/service/heap_simulator/heap_simulator.h\"\n #include \"xla/service/hlo_value.h\"\n+#include \"xla/service/logical_buffer.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n@@ -94,9 +95,10 @@ class ListScheduler {\n   // Construct and return a memory-minimizing sequence of HLO instructions\n   // containing the given HLO computation.\n   static absl::StatusOr<HloInstructionSequence> Run(\n-      HloComputation* computation, const HloAliasAnalysis& alias_analysis,\n+      HloComputation* computation,\n+      const TuplePointsToAnalysis& points_to_analysis,\n       const BufferValue::SizeFunction* absl_nonnull size_function) {\n-    ListScheduler scheduler(computation, alias_analysis, size_function);\n+    ListScheduler scheduler(computation, points_to_analysis, size_function);\n     return scheduler.CreateSchedule();\n   }\n \n@@ -116,57 +118,56 @@ class ListScheduler {\n   using Priority = std::pair<int64_t, int64_t>;\n \n   ListScheduler(HloComputation* computation,\n-                const HloAliasAnalysis& alias_analysis,\n+                const TuplePointsToAnalysis& points_to_analysis,\n                 const BufferValue::SizeFunction* absl_nonnull size_function)\n       : computation_(computation),\n-        alias_analysis_(alias_analysis),\n+        points_to_analysis_(points_to_analysis),\n         size_function_(ABSL_DIE_IF_NULL(size_function)) {\n-    // Create a map containing the HloValue uses for each HLO instruction. An\n-    // HLO instruction \"uses\" a HloValue if the HloValue is in an operand of the\n-    // instruction as indicated by HloDataflowAnalysis.\n-    const HloDataflowAnalysis& dataflow_analysis =\n-        alias_analysis.dataflow_analysis();\n-    absl::flat_hash_set<const HloValue*> computation_values;\n+    // Create a map containing the LogicalBuffer uses for each HLO\n+    // instruction. An HLO instruction \"uses\" a LogicalBuffer if the\n+    // LogicalBuffer is in an operand of the instruction as indicated by\n+    // points-to analysis.\n     for (auto* instruction : computation->instructions()) {\n-      dataflow_analysis.GetInstructionValueSet(instruction)\n-          .ForEachElement(\n-              [&](const ShapeIndex& /*index*/, const HloValueSet& value_set) {\n-                computation_values.insert(value_set.values().begin(),\n-                                          value_set.values().end());\n-              });\n-      absl::flat_hash_set<const HloValue*> instr_uses;\n+      absl::flat_hash_set<const LogicalBuffer*> instr_uses;\n       for (auto* operand : instruction->operands()) {\n-        dataflow_analysis.GetInstructionValueSet(operand).ForEachElement(\n-            [&](const ShapeIndex& /*index*/, const HloValueSet& value_set) {\n-              instr_uses.insert(value_set.values().begin(),\n-                                value_set.values().end());\n+        points_to_analysis.GetPointsToSet(operand).ForEachElement(\n+            [&](const ShapeIndex& /*index*/,\n+                const PointsToSet::BufferList& buffers) {\n+              instr_uses.insert(buffers.begin(), buffers.end());\n             });\n       }\n-      value_uses_[instruction] =\n-          std::vector<const HloValue*>(instr_uses.begin(), instr_uses.end());\n+      buffer_uses_[instruction] = std::vector<const LogicalBuffer*>(\n+          instr_uses.begin(), instr_uses.end());\n     }\n \n     // Create map containing the number of unscheduled uses (hlo instructions)\n-    // of each HloValue.\n-    unscheduled_use_count_.reserve(computation_values.size());\n-    for (const HloValue* value : computation_values) {\n-      // HloValues live out of the computation have an implicit use at the end\n-      // of the computation. Therefore we initialize `unscheduled_use_count_` to\n-      // 1 in such cases.\n-      unscheduled_use_count_[value] =\n-          alias_analysis.ValueLivesOut(*value) ? 1 : 0;\n+    // of each logical buffer.\n+    unscheduled_use_count_.reserve(points_to_analysis.num_logical_buffers());\n+    for (auto* instruction : computation->instructions()) {\n+      for (auto* buffer :\n+           points_to_analysis.GetBuffersDefinedByInstruction(instruction)) {\n+        unscheduled_use_count_[buffer] = 0;\n+      }\n     }\n     for (auto* instruction : computation->instructions()) {\n-      for (const HloValue* value : value_uses_.at(instruction)) {\n-        ++unscheduled_use_count_[value];\n+      for (const LogicalBuffer* buffer : buffer_uses_.at(instruction)) {\n+        ++unscheduled_use_count_[buffer];\n       }\n     }\n+\n+    // Buffers live out of the computation have an implicit use at the end of\n+    // the computation.\n+    for (const LogicalBuffer* live_out_buffer :\n+         points_to_analysis.GetPointsToSet(computation->root_instruction())\n+             .CreateFlattenedSet()) {\n+      ++unscheduled_use_count_[live_out_buffer];\n+    }\n   }\n \n-  // Returns whether the memory used by the given HloValue should be ignored by\n+  // Returns whether the memory used by the given buffer should be ignored by\n   // the scheduling heuristic.\n-  static bool IgnoreValue(const HloValue& value) {\n-    return IgnoreInstruction(*value.instruction());\n+  static bool IgnoreBuffer(const LogicalBuffer& buffer) {\n+    return IgnoreInstruction(*buffer.instruction());\n   }\n \n   // An entry in the worklist used by CreateSchedule.  Corresponds to one\n@@ -182,7 +183,7 @@ class ListScheduler {\n     // U is the number of uses of B that have not yet been scheduled. This pair\n     // is a pointer into the unscheduled_use_count_ map, so it gets updated for\n     // free when we update counts in the map.\n-    std::vector<const std::pair<const HloValue* const, int64_t>*>\n+    std::vector<const std::pair<const LogicalBuffer* const, int64_t>*>\n         used_buffer_unscheduled_use_counts;\n   };\n \n@@ -192,20 +193,18 @@ class ListScheduler {\n     entry.instruction = instruction;\n \n     entry.bytes_defined = 0;\n-    HloValueSet value_set =\n-        alias_analysis_.dataflow_analysis().GetFlattenedValueSet(instruction);\n-    for (const HloValue* value : value_set.values()) {\n-      if (!IgnoreInstruction(*instruction) &&\n-          value->instruction() == instruction) {\n-        entry.bytes_defined += (*size_function_)(*value);\n+    for (auto* buffer :\n+         points_to_analysis_.GetBuffersDefinedByInstruction(instruction)) {\n+      if (!IgnoreBuffer(*buffer)) {\n+        entry.bytes_defined += (*size_function_)(*buffer);\n       }\n     }\n \n-    for (auto* value : value_uses_.at(instruction)) {\n-      if (IgnoreValue(*value)) {\n+    for (auto* buffer : buffer_uses_.at(instruction)) {\n+      if (IgnoreBuffer(*buffer)) {\n         continue;\n       }\n-      auto unscheduled_use_count_it = unscheduled_use_count_.find(value);\n+      auto unscheduled_use_count_it = unscheduled_use_count_.find(buffer);\n       CHECK(unscheduled_use_count_it != unscheduled_use_count_.end());\n       entry.used_buffer_unscheduled_use_counts.push_back(\n           &*unscheduled_use_count_it);\n@@ -311,9 +310,9 @@ class ListScheduler {\n       scheduled_instructions_.insert(best);\n \n       bool adjust_ready_queue = false;\n-      // Update the unscheduled uses of the HloValues.\n-      for (const HloValue* value : value_uses_.at(best)) {\n-        int64_t& count = unscheduled_use_count_[value];\n+      // Update the unscheduled uses of the logical buffers.\n+      for (const LogicalBuffer* buffer : buffer_uses_.at(best)) {\n+        int64_t& count = unscheduled_use_count_[buffer];\n         CHECK_GT(count, 0);\n         --count;\n         if (count == 1) {\n@@ -337,7 +336,7 @@ class ListScheduler {\n       for (HloInstruction* succ : best->control_successors()) {\n         update_pred_count(succ);\n       }\n-      // The unscheduled use count for a HloValue has changed to 1, so the\n+      // The unscheduled use count for a buffer has changed to 1, so the\n       // priorities of some ready instructions may go up. We update them in the\n       // ready queue, so that they can appear earlier.\n       if (adjust_ready_queue) {\n@@ -363,24 +362,23 @@ class ListScheduler {\n         }\n       }\n     }\n-    CHECK_EQ(schedule.size(), computation_->instruction_count())\n-        << \"There could be a cycle in the HLO graph\";\n+    CHECK_EQ(schedule.size(), computation_->instruction_count());\n     CHECK_EQ(scheduled_instructions_.size(), computation_->instruction_count());\n \n     return schedule;\n   }\n \n   HloComputation* computation_;\n-  const HloAliasAnalysis& alias_analysis_;\n+  const TuplePointsToAnalysis& points_to_analysis_;\n   const BufferValue::SizeFunction* absl_nonnull size_function_;\n \n-  // A map containing the HloValue that each instruction uses.\n-  absl::flat_hash_map<const HloInstruction*, std::vector<const HloValue*>>\n-      value_uses_;\n+  // A map containing the LogicalBuffers that each instruction uses.\n+  absl::flat_hash_map<const HloInstruction*, std::vector<const LogicalBuffer*>>\n+      buffer_uses_;\n \n   // A map containing the count of unscheduled HLOs which using a particular\n-  // HloValue.\n-  absl::flat_hash_map<const HloValue*, int64_t> unscheduled_use_count_;\n+  // LogicalBuffer.\n+  absl::flat_hash_map<const LogicalBuffer*, int64_t> unscheduled_use_count_;\n \n   // Set of instructions which have been scheduled.\n   absl::flat_hash_set<const HloInstruction*> scheduled_instructions_;\n@@ -400,15 +398,16 @@ int64_t SumBufferSizes(const HloInstruction* hlo, const HloValueSet& value_set,\n }  // namespace\n \n absl::StatusOr<HloSchedule> ComputationSchedulerAlgorithm::Run(\n-    const HloModule* module, const HloAliasAnalysis& alias_analysis,\n+    const HloModule* module, const TuplePointsToAnalysis& points_to_analysis,\n+    const HloAliasAnalysis& alias_analysis,\n     const absl::flat_hash_set<absl::string_view>& execution_threads,\n     int64_t* peak_memory) const {\n   HloSchedule schedule(module);\n   for (HloComputation* computation :\n        module->MakeComputationPostOrder(execution_threads)) {\n     if (!computation->IsFusionComputation()) {\n       TF_ASSIGN_OR_RETURN(HloInstructionSequence computation_sequence,\n-                          Run(computation, alias_analysis));\n+                          Run(computation, points_to_analysis, alias_analysis));\n       if (postprocessor_) {\n         computation_sequence = postprocessor_(computation_sequence);\n       }\n@@ -425,6 +424,7 @@ absl::StatusOr<HloSchedule> ComputationSchedulerAlgorithm::Run(\n \n absl::StatusOr<HloInstructionSequence> DFSMemoryScheduler::Run(\n     HloComputation* computation,\n+    const TuplePointsToAnalysis& points_to_analysis,\n     const HloAliasAnalysis& alias_analysis) const {\n   // These variables are a hack to prevent overflows.\n   int64_t cumulative_total_size = 0;\n@@ -503,6 +503,7 @@ absl::StatusOr<HloInstructionSequence> DFSMemoryScheduler::Run(\n \n absl::StatusOr<HloInstructionSequence> BFScheduler::Run(\n     HloComputation* computation,\n+    const TuplePointsToAnalysis& points_to_analysis,\n     const HloAliasAnalysis& alias_analysis) const {\n   // Index of HloInstruction in the `computation`.\n   absl::flat_hash_map<const HloInstruction*, int64_t> inst_index;\n@@ -556,18 +557,21 @@ absl::StatusOr<HloInstructionSequence> BFScheduler::Run(\n \n absl::StatusOr<HloInstructionSequence> ListMemoryScheduler::Run(\n     HloComputation* computation,\n+    const TuplePointsToAnalysis& points_to_analysis,\n     const HloAliasAnalysis& alias_analysis) const {\n-  return ListScheduler::Run(computation, alias_analysis, size_function_);\n+  return ListScheduler::Run(computation, points_to_analysis, size_function_);\n }\n \n absl::StatusOr<HloInstructionSequence> PostOrderScheduler::Run(\n     HloComputation* computation,\n+    const TuplePointsToAnalysis& points_to_analysis,\n     const HloAliasAnalysis& alias_analysis) const {\n   return HloInstructionSequence(computation->MakeInstructionPostOrder());\n }\n \n absl::StatusOr<HloSchedule> DefaultMemoryScheduler::Run(\n-    const HloModule* module, const HloAliasAnalysis& alias_analysis,\n+    const HloModule* module, const TuplePointsToAnalysis& points_to_analysis,\n+    const HloAliasAnalysis& alias_analysis,\n     const absl::flat_hash_set<absl::string_view>& execution_threads,\n     int64_t* peak_memory) const {\n   // We try a few schedulers and choose whichever returns a lower min-memory,\n@@ -579,22 +583,24 @@ absl::StatusOr<HloSchedule> DefaultMemoryScheduler::Run(\n   // List wins for most of our benchmarks; postorder-based schedulers win for\n   // some RNNs.\n   int64_t list_memory;\n-  TF_ASSIGN_OR_RETURN(HloSchedule list_sequence,\n-                      list_scheduler_.Run(module, alias_analysis,\n-                                          execution_threads, &list_memory));\n+  TF_ASSIGN_OR_RETURN(\n+      HloSchedule list_sequence,\n+      list_scheduler_.Run(module, points_to_analysis, alias_analysis,\n+                          execution_threads, &list_memory));\n   VLOG(2) << \"Min-memory list sequence: \" << HumanReadableNumBytes(list_memory);\n \n   int64_t dfs_memory;\n-  TF_ASSIGN_OR_RETURN(HloSchedule dfs_sequence,\n-                      dfs_scheduler_.Run(module, alias_analysis,\n-                                         execution_threads, &dfs_memory));\n+  TF_ASSIGN_OR_RETURN(\n+      HloSchedule dfs_sequence,\n+      dfs_scheduler_.Run(module, points_to_analysis, alias_analysis,\n+                         execution_threads, &dfs_memory));\n   VLOG(2) << \"Min-memory dfs sequence: \" << HumanReadableNumBytes(dfs_memory);\n \n   int64_t post_order_memory;\n   TF_ASSIGN_OR_RETURN(\n       HloSchedule post_order_sequence,\n-      post_order_scheduler_.Run(module, alias_analysis, execution_threads,\n-                                &post_order_memory));\n+      post_order_scheduler_.Run(module, points_to_analysis, alias_analysis,\n+                                execution_threads, &post_order_memory));\n   VLOG(2) << \"Min-memory post order sequence: \"\n           << HumanReadableNumBytes(post_order_memory);\n \n@@ -626,12 +632,15 @@ absl::StatusOr<HloSchedule> ScheduleModule(\n     return absl::StrFormat(\"XlaMemoryScheduler:#module=%s,program_id=%d#\",\n                            module->name(), module->unique_id());\n   });\n+  TF_ASSIGN_OR_RETURN(std::unique_ptr<TuplePointsToAnalysis> points_to_analysis,\n+                      TuplePointsToAnalysis::Run(module));\n   TF_ASSIGN_OR_RETURN(std::unique_ptr<HloAliasAnalysis> alias_analysis,\n                       HloAliasAnalysis::Run(module, algorithm.alias_info()));\n \n   TF_ASSIGN_OR_RETURN(\n       HloSchedule schedule,\n-      algorithm.Run(module, *alias_analysis, execution_threads, peak_memory));\n+      algorithm.Run(module, *points_to_analysis, *alias_analysis,\n+                    execution_threads, peak_memory));\n \n   TF_RETURN_IF_ERROR(schedule.Verify());\n "
        },
        {
            "sha": "f06330b53ff0ff2d8ba4e4c77171e8d7cea19051",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/hlo_memory_scheduler.h",
            "status": "modified",
            "additions": 14,
            "deletions": 3,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e7e50018a23de01b4eb2429aab0c8b76faf548c9/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fhlo_memory_scheduler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e7e50018a23de01b4eb2429aab0c8b76faf548c9/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fhlo_memory_scheduler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fhlo_memory_scheduler.h?ref=e7e50018a23de01b4eb2429aab0c8b76faf548c9",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"xla/hlo/analysis/alias_info.h\"\n #include \"xla/hlo/analysis/hlo_alias_analysis.h\"\n+#include \"xla/hlo/analysis/tuple_points_to_analysis.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_schedule.h\"\n@@ -41,13 +42,16 @@ namespace xla {\n // 'module' given a points-to analysis result that describes buffer aliasing.\n // peak_memory (may be nullptr) is set to the peak memory of the resulting\n // schedule according to the HeapSimulator.\n+//\n+// TODO(yunxing): Cleanup usage of TuplePointsToAnalysis.\n class ModuleSchedulerAlgorithm {\n  public:\n   explicit ModuleSchedulerAlgorithm(const AliasInfo* alias_info)\n       : alias_info_(alias_info) {}\n   virtual ~ModuleSchedulerAlgorithm() = default;\n   virtual absl::StatusOr<HloSchedule> Run(\n-      const HloModule* module, const HloAliasAnalysis& alias_analysis,\n+      const HloModule* module, const TuplePointsToAnalysis& points_to_analysis,\n+      const HloAliasAnalysis& alias_analysis,\n       const absl::flat_hash_set<absl::string_view>& execution_threads,\n       int64_t* peak_memory) const = 0;\n \n@@ -71,9 +75,11 @@ class ComputationSchedulerAlgorithm : public ModuleSchedulerAlgorithm {\n  public:\n   virtual absl::StatusOr<HloInstructionSequence> Run(\n       HloComputation* computation,\n+      const TuplePointsToAnalysis& points_to_analysis,\n       const HloAliasAnalysis& alias_analysis) const = 0;\n   absl::StatusOr<HloSchedule> Run(\n-      const HloModule* module, const HloAliasAnalysis& alias_analysis,\n+      const HloModule* module, const TuplePointsToAnalysis& points_to_analysis,\n+      const HloAliasAnalysis& alias_analysis,\n       const absl::flat_hash_set<absl::string_view>& execution_threads,\n       int64_t* peak_memory) const override;\n \n@@ -118,6 +124,7 @@ class ListMemoryScheduler : public ComputationSchedulerAlgorithm {\n   using ModuleSchedulerAlgorithm::Run;\n   absl::StatusOr<HloInstructionSequence> Run(\n       HloComputation* computation,\n+      const TuplePointsToAnalysis& points_to_analysis,\n       const HloAliasAnalysis& alias_analysis) const override;\n };\n \n@@ -138,6 +145,7 @@ class DFSMemoryScheduler : public ComputationSchedulerAlgorithm {\n   using ModuleSchedulerAlgorithm::Run;\n   absl::StatusOr<HloInstructionSequence> Run(\n       HloComputation* computation,\n+      const TuplePointsToAnalysis& points_to_analysis,\n       const HloAliasAnalysis& alias_analysis) const override;\n };\n \n@@ -165,6 +173,7 @@ class BFScheduler : public ComputationSchedulerAlgorithm {\n                                       std::move(postprocessor)) {}\n   absl::StatusOr<HloInstructionSequence> Run(\n       HloComputation* computation,\n+      const TuplePointsToAnalysis& points_to_analysis,\n       const HloAliasAnalysis& alias_analysis) const override;\n };\n \n@@ -185,6 +194,7 @@ class PostOrderScheduler : public ComputationSchedulerAlgorithm {\n   using ModuleSchedulerAlgorithm::Run;\n   absl::StatusOr<HloInstructionSequence> Run(\n       HloComputation* computation,\n+      const TuplePointsToAnalysis& points_to_analysis,\n       const HloAliasAnalysis& alias_analysis) const override;\n };\n \n@@ -214,7 +224,8 @@ class DefaultMemoryScheduler : public ModuleSchedulerAlgorithm {\n         dfs_scheduler_(alias_info, size_function_, postprocessor),\n         post_order_scheduler_(alias_info, size_function_, postprocessor) {}\n   absl::StatusOr<HloSchedule> Run(\n-      const HloModule* module, const HloAliasAnalysis& alias_analysis,\n+      const HloModule* module, const TuplePointsToAnalysis& points_to_analysis,\n+      const HloAliasAnalysis& alias_analysis,\n       const absl::flat_hash_set<absl::string_view>& execution_threads,\n       int64_t* peak_memory) const override;\n "
        }
    ],
    "stats": {
        "total": 173,
        "additions": 97,
        "deletions": 76
    }
}