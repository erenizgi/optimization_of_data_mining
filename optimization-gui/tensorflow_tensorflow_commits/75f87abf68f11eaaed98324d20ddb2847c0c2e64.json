{
    "author": "majnemer",
    "message": "Remove `xla::AlignedAlloc` and migrate callers to `tsl::port::AlignedMalloc`.\n\nThis change deprecates and removes `xla::AlignedAlloc` and its associated `FreeDeleter`, replacing usages with `tsl::port::AlignedMalloc` function in `tsl/platform/mem.h`. The `xla::IsPowerOf2` utility is also updated to use `absl::has_single_bit`.\n\nPiperOrigin-RevId: 832484510",
    "sha": "75f87abf68f11eaaed98324d20ddb2847c0c2e64",
    "files": [
        {
            "sha": "e52aa03efdb2548ea9b607c99905e2301b270655",
            "filename": "third_party/xla/xla/pjrt/c/pjrt_c_api_gpu_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/75f87abf68f11eaaed98324d20ddb2847c0c2e64/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_gpu_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/75f87abf68f11eaaed98324d20ddb2847c0c2e64/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_gpu_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fc%2Fpjrt_c_api_gpu_test.cc?ref=75f87abf68f11eaaed98324d20ddb2847c0c2e64",
            "patch": "@@ -379,13 +379,17 @@ TEST_F(PjrtCApiGpuTest, CreateAndDestroyExecuteContext) {\n TEST_F(PjrtCApiGpuTest, DmaMapAndUnmap) {\n   size_t dma_size = 1024 * 1024;\n   size_t alignment = 1024 * 1024;\n-  auto host_dma_ptr = xla::AlignedAlloc(alignment, dma_size);\n+  void* host_dma_ptr = tsl::port::AlignedMalloc(dma_size, alignment);\n+  auto host_dma_ptr_deleter =\n+      absl::Cleanup([host_dma_ptr, dma_size, alignment] {\n+        tsl::port::AlignedSizedFree(host_dma_ptr, alignment, dma_size);\n+      });\n \n   PJRT_Client_DmaMap_Args dma_args;\n   dma_args.struct_size = PJRT_Client_DmaMap_Args_STRUCT_SIZE;\n   dma_args.extension_start = nullptr;\n   dma_args.client = client_;\n-  dma_args.data = host_dma_ptr.get();\n+  dma_args.data = host_dma_ptr;\n   dma_args.size = dma_size;\n   PJRT_Error* dma_error = api_->PJRT_Client_DmaMap(&dma_args);\n   ASSERT_EQ(dma_error, nullptr);\n@@ -395,7 +399,7 @@ TEST_F(PjrtCApiGpuTest, DmaMapAndUnmap) {\n   unmap_args.struct_size = PJRT_Client_DmaUnmap_Args_STRUCT_SIZE;\n   unmap_args.extension_start = nullptr;\n   unmap_args.client = client_;\n-  unmap_args.data = host_dma_ptr.get();\n+  unmap_args.data = host_dma_ptr;\n   PJRT_Error* unmap_error = api_->PJRT_Client_DmaUnmap(&unmap_args);\n   ASSERT_EQ(unmap_error, nullptr);\n   MakeErrorDeleter(api_)(unmap_error);"
        },
        {
            "sha": "eb1bd6ce67a36719fbf03abc07039549d2959de7",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client_test.cc",
            "status": "modified",
            "additions": 31,
            "deletions": 19,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/75f87abf68f11eaaed98324d20ddb2847c0c2e64/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/75f87abf68f11eaaed98324d20ddb2847c0c2e64/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc?ref=75f87abf68f11eaaed98324d20ddb2847c0c2e64",
            "patch": "@@ -33,6 +33,7 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/cleanup/cleanup.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n@@ -2599,13 +2600,17 @@ TEST(StreamExecutorGpuClientTest, DmaMapUnmap) {\n       tensorflow::down_cast<PjRtStreamExecutorClient*>(gpu_client.get());\n   size_t dma_size = 1024;\n   size_t alignment = 4096;\n-  auto host_dma_ptr = xla::AlignedAlloc(alignment, dma_size);\n-  TF_EXPECT_OK(client->DmaMap(host_dma_ptr.get(), dma_size));\n-  EXPECT_TRUE(client->IsDmaMapped(host_dma_ptr.get(), dma_size));\n-  EXPECT_FALSE(client->IsDmaMapped(\n-      reinterpret_cast<char*>(host_dma_ptr.get()) + 5, dma_size));\n-  TF_EXPECT_OK(client->DmaUnmap(host_dma_ptr.get()));\n-  EXPECT_FALSE(client->IsDmaMapped(host_dma_ptr.get(), dma_size));\n+  auto host_dma_ptr = tsl::port::AlignedMalloc(dma_size, alignment);\n+  auto host_dma_ptr_cleanup =\n+      absl::Cleanup([host_dma_ptr, dma_size, alignment] {\n+        tsl::port::AlignedSizedFree(host_dma_ptr, alignment, dma_size);\n+      });\n+  TF_EXPECT_OK(client->DmaMap(host_dma_ptr, dma_size));\n+  EXPECT_TRUE(client->IsDmaMapped(host_dma_ptr, dma_size));\n+  EXPECT_FALSE(\n+      client->IsDmaMapped(reinterpret_cast<char*>(host_dma_ptr) + 5, dma_size));\n+  TF_EXPECT_OK(client->DmaUnmap(host_dma_ptr));\n+  EXPECT_FALSE(client->IsDmaMapped(host_dma_ptr, dma_size));\n }\n \n TEST(StreamExecutorGpuClientTest, MultipleDeviceShareDmaMapping) {\n@@ -2633,10 +2638,14 @@ TEST(StreamExecutorGpuClientTest, MultipleDeviceShareDmaMapping) {\n \n   size_t dma_size = 2 * 1024 * 1024;\n   size_t alignment = 1024;\n-  auto host_dma_ptr = xla::AlignedAlloc(alignment, dma_size);\n-  TF_EXPECT_OK(client->DmaMap(host_dma_ptr.get(), dma_size));\n+  auto host_dma_ptr = tsl::port::AlignedMalloc(dma_size, alignment);\n+  auto host_dma_ptr_cleanup =\n+      absl::Cleanup([host_dma_ptr, dma_size, alignment] {\n+        tsl::port::AlignedSizedFree(host_dma_ptr, alignment, dma_size);\n+      });\n+  TF_EXPECT_OK(client->DmaMap(host_dma_ptr, dma_size));\n \n-  auto result = first_buffer->CopyRawToHost(host_dma_ptr.get(), 0, size);\n+  auto result = first_buffer->CopyRawToHost(host_dma_ptr, 0, size);\n   TF_EXPECT_OK(result.Await());\n \n   PjRtDevice* const second_device = client->addressable_devices()[1];\n@@ -2647,12 +2656,12 @@ TEST(StreamExecutorGpuClientTest, MultipleDeviceShareDmaMapping) {\n   auto second_buffer = transfer_manager->RetrieveBuffer(0);\n \n   TF_EXPECT_OK(transfer_manager->TransferRawDataToSubBuffer(\n-      0, host_dma_ptr.get(), 0, size, true, []() {}));\n+      0, host_dma_ptr, 0, size, true, []() {}));\n   TF_ASSERT_OK_AND_ASSIGN(auto literal, second_buffer->ToLiteralSync());\n   EXPECT_EQ(literal->element_count(), test_length);\n   EXPECT_THAT(literal->data<int32_t>(), ElementsAreArray(data));\n \n-  TF_EXPECT_OK(client->DmaUnmap(host_dma_ptr.get()));\n+  TF_EXPECT_OK(client->DmaUnmap(host_dma_ptr));\n }\n \n TEST(StreamExecutorGpuClientTest, RawBuffer) {\n@@ -2741,9 +2750,13 @@ ENTRY main.5 {\n \n   size_t dma_size = 4 * 1024;\n   size_t alignment = 1024;\n-  auto host_dma_ptr = xla::AlignedAlloc(alignment, dma_size);\n-  TF_EXPECT_OK(client->DmaMap(host_dma_ptr.get(), dma_size));\n-  memset(host_dma_ptr.get(), 0, dma_size);\n+  auto host_dma_ptr = tsl::port::AlignedMalloc(dma_size, alignment);\n+  auto host_dma_ptr_deleter =\n+      absl::Cleanup([host_dma_ptr, dma_size, alignment] {\n+        tsl::port::AlignedSizedFree(host_dma_ptr, alignment, dma_size);\n+      });\n+  TF_EXPECT_OK(client->DmaMap(host_dma_ptr, dma_size));\n+  memset(host_dma_ptr, 0, dma_size);\n   Shape shape =\n       ShapeUtil::MakeShape(S32, {static_cast<int64_t>(dma_size * 1024)});\n \n@@ -2767,11 +2780,10 @@ ENTRY main.5 {\n     }\n     last_opaque_ptr = opaque_ptr;\n \n-    memcpy(host_dma_ptr.get(), &i, sizeof(int32_t));\n+    memcpy(host_dma_ptr, &i, sizeof(int32_t));\n     absl::Notification done;\n     TF_EXPECT_OK(transfer_manager->TransferRawDataToSubBuffer(\n-        0, host_dma_ptr.get(), 0, dma_size, true,\n-        [&done]() { done.Notify(); }));\n+        0, host_dma_ptr, 0, dma_size, true, [&done]() { done.Notify(); }));\n     done.WaitForNotification();\n \n     std::vector<std::vector<xla::PjRtBuffer*>> input_ptrs = {\n@@ -2798,7 +2810,7 @@ ENTRY main.5 {\n \n   EXPECT_TRUE(clobbered);\n \n-  TF_EXPECT_OK(client->DmaUnmap(host_dma_ptr.get()));\n+  TF_EXPECT_OK(client->DmaUnmap(host_dma_ptr));\n }\n \n TEST(StreamExecutorGpuClientTest, EventCaching) {"
        },
        {
            "sha": "4b22c201565c40239770276f0c70ab6e274406c4",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/75f87abf68f11eaaed98324d20ddb2847c0c2e64/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/75f87abf68f11eaaed98324d20ddb2847c0c2e64/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD?ref=75f87abf68f11eaaed98324d20ddb2847c0c2e64",
            "patch": "@@ -175,6 +175,7 @@ xla_test(\n         \":tracked_gpu_device_buffer\",\n         \"@com_google_googletest//:gtest\",\n         \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_absl//absl/cleanup\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\","
        },
        {
            "sha": "fb578be47792cd7e5d8fc88fffc66c8867eee95f",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_client_test.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 7,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/75f87abf68f11eaaed98324d20ddb2847c0c2e64/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/75f87abf68f11eaaed98324d20ddb2847c0c2e64/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client_test.cc?ref=75f87abf68f11eaaed98324d20ddb2847c0c2e64",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/cleanup/cleanup.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n@@ -1738,11 +1739,15 @@ TEST(TfrtGpuClientTest, DmaMapUnmap) {\n   auto client = tensorflow::down_cast<TfrtGpuClient*>(gpu_client.get());\n   size_t dma_size = 8192;\n   size_t alignment = 4096;\n-  auto host_dma_ptr = xla::AlignedAlloc(alignment, dma_size);\n+  auto host_dma_ptr = tsl::port::AlignedMalloc(dma_size, alignment);\n+  auto host_dma_ptr_deleter =\n+      absl::Cleanup([host_dma_ptr, dma_size, alignment] {\n+        tsl::port::AlignedSizedFree(host_dma_ptr, alignment, dma_size);\n+      });\n \n   // DmaMap the first half of the buffer.\n   size_t dma_map_size = dma_size / 2;\n-  char* first_half_ptr = static_cast<char*>(host_dma_ptr.get());\n+  char* first_half_ptr = static_cast<char*>(host_dma_ptr);\n   char* second_half_ptr = first_half_ptr + dma_map_size;\n   int offset = 5;\n   TF_EXPECT_OK(client->DmaMap(first_half_ptr, dma_map_size));\n@@ -1811,10 +1816,15 @@ TEST(TfrtGpuClientTest, MultipleDeviceShareDmaMapping) {\n \n   size_t dma_size = 2 * 1024 * 1024;\n   size_t alignment = 1024;\n-  auto host_dma_ptr = xla::AlignedAlloc(alignment, dma_size);\n-  TF_EXPECT_OK(client->DmaMap(host_dma_ptr.get(), dma_size));\n+  auto host_dma_ptr = tsl::port::AlignedMalloc(dma_size, alignment);\n+  auto host_dma_ptr_deleter =\n+      absl::Cleanup([host_dma_ptr, dma_size, alignment] {\n+        tsl::port::AlignedSizedFree(host_dma_ptr, alignment, dma_size);\n+      });\n+\n+  TF_EXPECT_OK(client->DmaMap(host_dma_ptr, dma_size));\n \n-  auto result = first_buffer->CopyRawToHost(host_dma_ptr.get(), 0, size);\n+  auto result = first_buffer->CopyRawToHost(host_dma_ptr, 0, size);\n   TF_EXPECT_OK(result.Await());\n \n   PjRtDevice* const second_device = client->addressable_devices()[1];\n@@ -1825,12 +1835,12 @@ TEST(TfrtGpuClientTest, MultipleDeviceShareDmaMapping) {\n   auto second_buffer = transfer_manager->RetrieveBuffer(0);\n \n   TF_EXPECT_OK(transfer_manager->TransferRawDataToSubBuffer(\n-      0, host_dma_ptr.get(), 0, size, true, []() {}));\n+      0, host_dma_ptr, 0, size, true, []() {}));\n   TF_ASSERT_OK_AND_ASSIGN(auto literal, second_buffer->ToLiteralSync());\n   EXPECT_EQ(literal->element_count(), test_length);\n   EXPECT_THAT(literal->data<int32_t>(), ElementsAreArray(data));\n \n-  TF_EXPECT_OK(client->DmaUnmap(host_dma_ptr.get()));\n+  TF_EXPECT_OK(client->DmaUnmap(host_dma_ptr));\n }\n \n TEST(TfrtGpuClientTest, HostExecuteRuntimeTest) {"
        },
        {
            "sha": "a9f3b8f26d853f3f42460702dc95331cfd86e756",
            "filename": "third_party/xla/xla/util.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/75f87abf68f11eaaed98324d20ddb2847c0c2e64/third_party%2Fxla%2Fxla%2Futil.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/75f87abf68f11eaaed98324d20ddb2847c0c2e64/third_party%2Fxla%2Fxla%2Futil.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Futil.cc?ref=75f87abf68f11eaaed98324d20ddb2847c0c2e64",
            "patch": "@@ -538,26 +538,5 @@ std::string PrintAllFields(const tsl::protobuf::Message& message) {\n   return result.str();\n }\n \n-std::unique_ptr<void, FreeDeleter> AlignedAlloc(std::size_t alignment,\n-                                                std::size_t size) {\n-  CHECK_GT(alignment, 0) << \"alignment must be positive\";\n-  CHECK(IsPowerOf2(alignment))\n-      << \"alignment must be a power of 2, but got \" << alignment;\n-  CHECK_GT(size, 0) << \"size must be positive\";\n-#ifdef _WIN32\n-  void* raw_ptr = _aligned_malloc(size, alignment);  // Note argument order\n-#elif defined(__ANDROID__) && __ANDROID_API__ < 28\n-  // Use posix_memalign as a fallback for older Android APIs\n-  void* raw_ptr;\n-  int result = posix_memalign(&raw_ptr, alignment, size);\n-  CHECK_EQ(result, 0) << \"posix_memalign failed with error code: \" << result;\n-#else\n-  void* raw_ptr = std::aligned_alloc(alignment, size);\n-#endif\n-  CHECK_NE(raw_ptr, nullptr) << \"aligned_alloc failed\";\n-  // Return unique_ptr managing the memory.\n-  return std::unique_ptr<void, FreeDeleter>(raw_ptr, FreeDeleter());\n-}\n-\n int64_t Product(absl::Span<const int64_t> xs) { return Product<int64_t>(xs); }\n }  // namespace xla"
        },
        {
            "sha": "7bb69650e3f85489bd793a2537acfbca95f442be",
            "filename": "third_party/xla/xla/util.h",
            "status": "modified",
            "additions": 3,
            "deletions": 22,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/75f87abf68f11eaaed98324d20ddb2847c0c2e64/third_party%2Fxla%2Fxla%2Futil.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/75f87abf68f11eaaed98324d20ddb2847c0c2e64/third_party%2Fxla%2Fxla%2Futil.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Futil.h?ref=75f87abf68f11eaaed98324d20ddb2847c0c2e64",
            "patch": "@@ -1005,31 +1005,12 @@ using Vector3 = std::array<int64_t, 3>;\n std::string PrintAllFields(const tsl::protobuf::Message& message);\n \n // Returns true if x is a power of 2.\n-constexpr bool IsPowerOf2(size_t x) noexcept {\n+ABSL_DEPRECATE_AND_INLINE()\n+constexpr bool IsPowerOf2(size_t x) {\n   // Checks that x is non-zero and has only a single bit set.\n-  return x != 0 && (x & (x - 1)) == 0;\n+  return absl::has_single_bit(x);\n }\n \n-// A custom deleter that frees the pointer via std::free().\n-struct FreeDeleter {\n-  void operator()(void* ptr) {\n-#if defined(_WIN32)\n-    _aligned_free(ptr);\n-#else\n-    std::free(ptr);\n-#endif\n-  }\n-};\n-\n-/**\n- * @brief Allocates memory with specified alignment.\n- * @param alignment Specifies the alignment. Power of two.\n- * @param size The number of bytes to allocate. Integral multiple of alignment\n- * @return A unique_ptr managing the allocated memory.\n- */\n-std::unique_ptr<void, FreeDeleter> AlignedAlloc(std::size_t alignment,\n-                                                std::size_t size);\n-\n // Note that STRING is evaluated regardless of whether it will be logged.\n #define XLA_LOG_LINES(SEV, STRING) \\\n   ::xla::LogLines##SEV(STRING, __FILE__, __LINE__)"
        }
    ],
    "stats": {
        "total": 131,
        "additions": 59,
        "deletions": 72
    }
}