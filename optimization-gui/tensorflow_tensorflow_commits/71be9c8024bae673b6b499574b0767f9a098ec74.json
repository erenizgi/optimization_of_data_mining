{
    "author": "WillFroom",
    "message": "[XLA:CPU][XTile] Expand single element vector ops before lowering to intrinsics.\n\nPiperOrigin-RevId: 834684558",
    "sha": "71be9c8024bae673b6b499574b0767f9a098ec74",
    "files": [
        {
            "sha": "80d6f4444b536a10b7e0e40ab417157f79e91079",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_compiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/71be9c8024bae673b6b499574b0767f9a098ec74/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/71be9c8024bae673b6b499574b0767f9a098ec74/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc?ref=71be9c8024bae673b6b499574b0767f9a098ec74",
            "patch": "@@ -337,6 +337,7 @@ static void AddTiledOptimizationPasses(mlir::OpPassManager& pm) {\n // The input IR is from the xtile dialect which uses tensors that are converted\n // first to the vector dialect and then to LLVM.\n static void AddTiledLoweringPasses(mlir::OpPassManager& pm, bool fast_min_max) {\n+  pm.addPass(CreateVectorToScalarPass());\n   pm.addPass(cpu::CreateMemrefCopyToLoopsPass());\n   pm.addPass(cpu::createLowerToLLVMPass());\n   pm.addPass(mlir::createConvertVectorToSCFPass("
        },
        {
            "sha": "6aaea220050ae5311cb772b0532f57a79d2f10bf",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/71be9c8024bae673b6b499574b0767f9a098ec74/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/71be9c8024bae673b6b499574b0767f9a098ec74/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD?ref=71be9c8024bae673b6b499574b0767f9a098ec74",
            "patch": "@@ -54,6 +54,7 @@ cc_library(\n         \"memref_copy_to_loops.cc\",\n         \"shlo_to_vector.cc\",\n         \"tensor_ops_to_bufferizable.cc\",\n+        \"vector_to_scalar_pass.cc\",\n     ],\n     hdrs = [\"passes.h\"],\n     deps = ["
        },
        {
            "sha": "b086a33d018ae4ffeba098ebc9ccd1b13c1e70e1",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/passes.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/71be9c8024bae673b6b499574b0767f9a098ec74/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/71be9c8024bae673b6b499574b0767f9a098ec74/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.h?ref=71be9c8024bae673b6b499574b0767f9a098ec74",
            "patch": "@@ -40,6 +40,7 @@ std::unique_ptr<mlir::Pass> CreateShloToVectorPass();\n std::unique_ptr<mlir::Pass> CreateTensorOpsToBufferizablePass();\n std::unique_ptr<mlir::Pass> CreateMemrefCopyToLoopsPass();\n std::unique_ptr<mlir::Pass> CreateFuseElementwisePass();\n+std::unique_ptr<mlir::Pass> CreateVectorToScalarPass();\n \n #define GEN_PASS_REGISTRATION\n #include \"xla/backends/cpu/codegen/tiled/transforms/passes.h.inc\""
        },
        {
            "sha": "522094c7f10d7c93699d3712ca346a186ec641f9",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/passes.td",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/71be9c8024bae673b6b499574b0767f9a098ec74/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/71be9c8024bae673b6b499574b0767f9a098ec74/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td?ref=71be9c8024bae673b6b499574b0767f9a098ec74",
            "patch": "@@ -92,3 +92,16 @@ def FuseElementwisePass : Pass<\"xtile-cpu-fuse-elementwise\"> {\n     and fewer temporary allocations in bufferization.\n   }];\n }\n+\n+def VectorToScalarPass : Pass<\"xtile-cpu-vector-to-scalar\"> {\n+  let summary = \"Convert vector ops to scalar ops where possible.\";\n+\n+  let description = [{\n+    This pass converts elementwise vector ops to scalar ops if the operation\n+    acts on a single element.\n+  }];\n+\n+  let dependentDialects = [\n+    \"::mlir::vector::VectorDialect\",\n+  ];\n+}"
        },
        {
            "sha": "2b4705ddc4abcc2b153a0bafa6e668ec80fd0a67",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/vector_to_scalar.mlir",
            "status": "added",
            "additions": 53,
            "deletions": 0,
            "changes": 53,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/71be9c8024bae673b6b499574b0767f9a098ec74/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fvector_to_scalar.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/71be9c8024bae673b6b499574b0767f9a098ec74/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fvector_to_scalar.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fvector_to_scalar.mlir?ref=71be9c8024bae673b6b499574b0767f9a098ec74",
            "patch": "@@ -0,0 +1,53 @@\n+// RUN: fusion_compiler_opt %s -xtile-cpu-vector-to-scalar -split-input-file | FileCheck %s\n+\n+func.func @vector_to_scalar_0d(%arg0 : vector<f32>, %arg1 : vector<f32>) -> vector<f32> {\n+  // CHECK-DAG: %[[SCALAR0:.*]] = vector.extract %arg0[]\n+  // CHECK-DAG: %[[SCALAR1:.*]] = vector.extract %arg1[]\n+  // CHECK: %[[SCALAR_ADD:.*]] = arith.addf %[[SCALAR0]], %[[SCALAR1]] : f32\n+  // CHECK: %[[VECTOR_ADD:.*]] = vector.from_elements %[[SCALAR_ADD]] : vector<f32>\n+  %add = arith.addf %arg0, %arg1 : vector<f32>\n+  // CHECK: return %[[VECTOR_ADD]] : vector<f32>\n+  return %add : vector<f32>\n+}\n+\n+//-----\n+\n+func.func @vector_to_scalar_1d(%arg0 : vector<1xf32>, %arg1 : vector<1xf32>) -> vector<1xf32> {\n+  // CHECK-DAG: %[[SCALAR0:.*]] = vector.extract %arg0[0]\n+  // CHECK-DAG: %[[SCALAR1:.*]] = vector.extract %arg1[0]\n+  // CHECK: %[[SCALAR_MUL:.*]] = arith.mulf %[[SCALAR0]], %[[SCALAR1]] : f32\n+  // CHECK: %[[VECTOR_MUL:.*]] = vector.from_elements %[[SCALAR_MUL]] : vector<1xf32>\n+  %mul = arith.mulf %arg0, %arg1 : vector<1xf32>\n+  // CHECK: return %[[VECTOR_MUL]] : vector<1xf32>\n+  return %mul : vector<1xf32>\n+}\n+\n+//-----\n+\n+func.func @vector_to_scalar_2d(%arg0 : vector<1x1xf32>) -> vector<1x1xf32> {\n+  // CHECK: %[[SCALAR0:.*]] = vector.extract %arg0[0, 0]\n+  // CHECK: %[[SCALAR_COS:.*]] = math.cos %[[SCALAR0]] : f32\n+  // CHECK: %[[VECTOR_COS:.*]] = vector.from_elements %[[SCALAR_COS]] : vector<1x1xf32>\n+  %cos = math.cos %arg0 : vector<1x1xf32>\n+  // CHECK: return %[[VECTOR_COS]] : vector<1x1xf32>\n+  return %cos : vector<1x1xf32>\n+}\n+\n+//-----\n+\n+func.func @vector_to_scalar_constant() -> vector<1x1xf32> {\n+  // CHECK: %[[SCALAR:.*]] = arith.constant 1.000000e+00 : f32\n+  // CHECK: %[[VECTOR:.*]] = vector.from_elements %[[SCALAR]] : vector<1x1xf32>\n+  %cos = arith.constant dense<1.0> : vector<1x1xf32>\n+  // CHECK: return %[[VECTOR]] : vector<1x1xf32>\n+  return %cos : vector<1x1xf32>\n+}\n+\n+//-----\n+\n+func.func @skips_multi_element(%arg0 : vector<2xf32>) -> vector<2xf32> {\n+  // CHECK: %[[RES:.*]] = math.sin %arg0 : vector<2xf32>\n+  %sin = math.sin %arg0 : vector<2xf32>\n+  // CHECK: return %[[RES]] : vector<2xf32>\n+  return %sin : vector<2xf32>\n+}"
        },
        {
            "sha": "5a4dc5addcf83d77983a38008af7c4137effa109",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/vector_to_scalar_pass.cc",
            "status": "added",
            "additions": 170,
            "deletions": 0,
            "changes": 170,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/71be9c8024bae673b6b499574b0767f9a098ec74/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvector_to_scalar_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/71be9c8024bae673b6b499574b0767f9a098ec74/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvector_to_scalar_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvector_to_scalar_pass.cc?ref=71be9c8024bae673b6b499574b0767f9a098ec74",
            "patch": "@@ -0,0 +1,170 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cassert>\n+#include <cstdint>\n+#include <memory>\n+#include <optional>\n+#include <utility>\n+\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinAttributeInterfaces.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/OpDefinition.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/ValueRange.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h\"\n+\n+namespace xla::cpu {\n+\n+#define GEN_PASS_DEF_VECTORTOSCALARPASS\n+#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h.inc\"\n+\n+namespace {\n+\n+mlir::Type TypeConverter(mlir::Type type) {\n+  auto maybe_vector_type = mlir::dyn_cast<mlir::VectorType>(type);\n+  if (!maybe_vector_type) {\n+    return type;\n+  }\n+  if (maybe_vector_type.getNumElements() != 1) {\n+    return type;\n+  }\n+\n+  return maybe_vector_type.getElementType();\n+}\n+\n+mlir::Value SourceMaterialization(mlir::OpBuilder& builder,\n+                                  mlir::Type result_type,\n+                                  mlir::ValueRange inputs, mlir::Location loc) {\n+  if (inputs.size() != 1) {\n+    return nullptr;\n+  }\n+  return mlir::vector::FromElementsOp::create(builder, loc, result_type,\n+                                              inputs.front());\n+}\n+\n+mlir::Value TargetMaterialization(mlir::OpBuilder& builder,\n+                                  mlir::Type result_type,\n+                                  mlir::ValueRange inputs, mlir::Location loc) {\n+  if (inputs.size() != 1) {\n+    return nullptr;\n+  }\n+  auto input_vector_type =\n+      mlir::cast<mlir::VectorType>(inputs.front().getType());\n+  llvm::SmallVector<int64_t> indices(input_vector_type.getRank(), 0);\n+  return mlir::vector::ExtractOp::create(builder, loc, inputs.front(), indices);\n+}\n+\n+struct ElementwiseConverter\n+    : public mlir::OpTraitConversionPattern<mlir::OpTrait::Elementwise> {\n+ public:\n+  using OpTraitConversionPattern::OpTraitConversionPattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      mlir::Operation* op, mlir::ArrayRef<mlir::Value> operands,\n+      mlir::ConversionPatternRewriter& rewriter) const override {\n+    llvm::SmallVector<mlir::Type> new_result_types;\n+    if (mlir::failed(getTypeConverter()->convertTypes(op->getResultTypes(),\n+                                                      new_result_types))) {\n+      return rewriter.notifyMatchFailure(op, \"failed to convert type\");\n+    }\n+\n+    mlir::IRMapping mapping;\n+    mapping.map(op->getOperands(), operands);\n+    mlir::Operation* new_op = rewriter.clone(*op, mapping);\n+\n+    for (auto [results, new_type] :\n+         llvm::zip(new_op->getResults(), new_result_types)) {\n+      results.setType(new_type);\n+    }\n+\n+    rewriter.replaceOp(op, new_op);\n+    return mlir::success();\n+  }\n+};\n+\n+struct ConstantConversionPattern\n+    : public mlir::OpConversionPattern<mlir::arith::ConstantOp> {\n+  using OpConversionPattern<mlir::arith::ConstantOp>::OpConversionPattern;\n+\n+  mlir::LogicalResult matchAndRewrite(\n+      mlir::arith::ConstantOp op, OpAdaptor adaptor,\n+      mlir::ConversionPatternRewriter& rewriter) const override {\n+    auto dense_attr = mlir::cast<mlir::DenseElementsAttr>(op.getValueAttr());\n+    auto scalar_attr = dense_attr.getValues<mlir::TypedAttr>()[0];\n+    rewriter.replaceOpWithNewOp<mlir::arith::ConstantOp>(op, scalar_attr);\n+\n+    return mlir::success();\n+  }\n+};\n+\n+class VectorToScalarPass\n+    : public impl::VectorToScalarPassBase<VectorToScalarPass> {\n+ public:\n+  using VectorToScalarPassBase::VectorToScalarPassBase;\n+\n+  void runOnOperation() override {\n+    mlir::TypeConverter type_converter;\n+    type_converter.addConversion(&TypeConverter);\n+\n+    type_converter.addSourceMaterialization(&SourceMaterialization);\n+    type_converter.addTargetMaterialization(&TargetMaterialization);\n+\n+    mlir::ConversionTarget target(getContext());\n+\n+    target.markUnknownOpDynamicallyLegal(\n+        [&](mlir::Operation* op) -> std::optional<bool> {\n+          if (op->hasTrait<mlir::OpTrait::Elementwise>()) {\n+            return type_converter.isLegal(op);\n+          }\n+          return std::nullopt;\n+        });\n+\n+    target.addDynamicallyLegalOp<mlir::arith::ConstantOp>(\n+        [&](mlir::arith::ConstantOp op) {\n+          return type_converter.isLegal(op.getOperation());\n+        });\n+\n+    mlir::RewritePatternSet patterns(&getContext());\n+\n+    patterns.add<ElementwiseConverter, ConstantConversionPattern>(\n+        type_converter, &getContext());\n+\n+    if (mlir::failed(mlir::applyPartialConversion(getOperation(), target,\n+                                                  std::move(patterns)))) {\n+      signalPassFailure();\n+      return;\n+    }\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<mlir::Pass> CreateVectorToScalarPass() {\n+  return std::make_unique<VectorToScalarPass>();\n+}\n+\n+}  // namespace xla::cpu"
        }
    ],
    "stats": {
        "total": 239,
        "additions": 239,
        "deletions": 0
    }
}