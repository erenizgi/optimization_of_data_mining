{
    "author": "basioli-k",
    "message": "[XLA:GPU] Run GPU compiler passes only on the main execution thread.\n\nThis is a preparation for host offloading as we don't want GPU passes to alter the host thread computations.\n\nPiperOrigin-RevId: 797795191",
    "sha": "5f5a585450ded29ca6187f9272ed61d56d837184",
    "files": [
        {
            "sha": "6396a3ab4061f00491c1bc511bcd17f2f55c0974",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 46,
            "deletions": 22,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f5a585450ded29ca6187f9272ed61d56d837184/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f5a585450ded29ca6187f9272ed61d56d837184/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=5f5a585450ded29ca6187f9272ed61d56d837184",
            "patch": "@@ -624,7 +624,9 @@ absl::Status RunPreSPMDPartitionerPasses(HloModule* hlo_module) {\n   pre_spmd_pipeline.AddPass<TopkRewriter>(\n       [](const HloSortInstruction*, int64_t) { return true; });\n \n-  return pre_spmd_pipeline.Run(hlo_module).status();\n+  return pre_spmd_pipeline\n+      .Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+      .status();\n }\n \n absl::Status RunSPMDPasses(\n@@ -664,7 +666,8 @@ absl::Status RunSPMDPasses(\n         std::nullopt,\n #endif  // PLATFORM_GOOGLE\n         max_windowed_einsum_iteration);\n-    return spmd_pipeline.Run(hlo_module).status();\n+    return spmd_pipeline.Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+        .status();\n   } else {\n     HloPassPipeline sharding_removal_pipeline(\"sharding-removal\");\n     // Remove redundant sharding ops when partition_count == 1.\n@@ -675,7 +678,9 @@ absl::Status RunSPMDPasses(\n           /*runSdyShardingPropagation=*/false);\n     }\n     sharding_removal_pipeline.AddPass<HloDCE>();\n-    return sharding_removal_pipeline.Run(hlo_module).status();\n+    return sharding_removal_pipeline\n+        .Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+        .status();\n   }\n }\n \n@@ -908,7 +913,8 @@ absl::Status RunOptimizationPasses(\n   pipeline.AddPass<SplitkRewriter>(gpu_target_config.device_description);\n   pipeline.AddPass<HloComputationDeduplicator>(\n       /*mark_fusion_duplications=*/false);\n-  return pipeline.Run(hlo_module).status();\n+  return pipeline.Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+      .status();\n }\n \n absl::Status RunCollectiveOptimizationPasses(\n@@ -1078,7 +1084,9 @@ absl::Status RunCollectiveOptimizationPasses(\n   // modifications.\n   collectives_pipeline.AddPass<WhileLoopTripCountAnnotator>();\n \n-  return collectives_pipeline.Run(hlo_module).status();\n+  return collectives_pipeline\n+      .Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+      .status();\n }\n \n absl::Status RunLayoutAssignmentPasses(\n@@ -1108,7 +1116,8 @@ absl::Status RunLayoutAssignmentPasses(\n   // the creation of invalid transpose/bitcast operations within\n   // host memory offloading segments.\n   pipeline.AddPass<HostOffloadLegalize>();\n-  return pipeline.Run(hlo_module).status();\n+  return pipeline.Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+      .status();\n }\n \n absl::Status RunFusionPasses(HloModule* hlo_module,\n@@ -1122,10 +1131,11 @@ absl::Status RunFusionPasses(HloModule* hlo_module,\n   pre_fusion.AddPass<AddTrackingSuffixToInstructionNames>();\n   TF_RETURN_IF_ERROR(pre_fusion.Run(hlo_module).status());\n \n-  TF_RETURN_IF_ERROR(FusionPipeline(hlo_module->config().debug_options(),\n-                                    shape_size_fn, thread_pool, gpu_device_info)\n-                         .Run(hlo_module)\n-                         .status());\n+  TF_RETURN_IF_ERROR(\n+      FusionPipeline(hlo_module->config().debug_options(), shape_size_fn,\n+                     thread_pool, gpu_device_info)\n+          .Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+          .status());\n \n   if (VLOG_IS_ON(2)) {\n     HloFusionStatsVisitor stats;\n@@ -1233,7 +1243,8 @@ absl::Status RunPostFusionPasses(\n \n   AddDoubleBufferingPasses(*hlo_module, pipeline);\n \n-  return pipeline.Run(hlo_module).status();\n+  return pipeline.Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+      .status();\n }\n \n absl::Status RunPostFusionSimplificationPasses(\n@@ -1262,7 +1273,8 @@ absl::Status RunPostFusionSimplificationPasses(\n     pipeline.AddPass<ExplicitStreamAnnotationAsyncWrapper>();\n   }\n   pipeline.AddPass<ExplicitCollectivesGroupAsyncWrapper>();\n-  return pipeline.Run(hlo_module).status();\n+  return pipeline.Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+      .status();\n }\n \n absl::Status RunPostFusionVerificationPasses(\n@@ -1281,7 +1293,8 @@ absl::Status RunPostFusionVerificationPasses(\n     }\n   }\n \n-  return pipeline.Run(hlo_module).status();\n+  return pipeline.Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+      .status();\n }\n \n absl::Status RunLayoutNormalizationPasses(\n@@ -1301,7 +1314,9 @@ absl::Status RunLayoutNormalizationPasses(\n   // Layout normalization will create scatters that are not simplified and\n   // also have unsorted update_window_dims.\n   layout_normalization_pipeline.AddPass<ScatterSimplifier>();\n-  return layout_normalization_pipeline.Run(hlo_module).status();\n+  return layout_normalization_pipeline\n+      .Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+      .status();\n }\n \n absl::Status RunAsyncDotPasses(HloModule* hlo_module) {\n@@ -1323,7 +1338,8 @@ absl::Status RunAsyncDotPasses(HloModule* hlo_module) {\n       return false;\n     });\n   }\n-  return pipeline.Run(hlo_module).status();\n+  return pipeline.Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+      .status();\n }\n \n absl::Status RunDynamicSliceFusionPasses(HloModule* hlo_module,\n@@ -1350,7 +1366,9 @@ absl::Status RunDynamicSliceFusionPasses(HloModule* hlo_module,\n           });\n       return hero_op.has_value();\n     });\n-    TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());\n+    TF_RETURN_IF_ERROR(\n+        pipeline.Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+            .status());\n   }\n \n   return absl::OkStatus();\n@@ -1420,7 +1438,8 @@ absl::Status GpuCompiler::RunCollectiveScheduleLinearizerPasses(\n       [this, stream_exec](const HloModule* module) {\n         return RequiresCollectiveScheduleLinearizer(module, stream_exec);\n       });\n-  return pipeline.Run(hlo_module).status();\n+  return pipeline.Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+      .status();\n }\n \n // Runs optimization passes on the given HLO module.\n@@ -1533,7 +1552,7 @@ absl::Status GpuCompiler::RunPreSchedulingCopyInsertion(\n     const GpuAliasInfo* alias_info) {\n   return PreSchedulingCopyInsertionPipeline(hlo_module.config(), alias_info,\n                                             device_description)\n-      .Run(&hlo_module)\n+      .Run(&hlo_module, {HloInstruction::kMainExecutionThread})\n       .status();\n }\n \n@@ -1731,7 +1750,9 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n     // annotations, this pass will add the annotations.\n     pipeline.AddPass<SubByteNormalization>(\n         SubByteNormalization::SET_ELEMENT_SIZE);\n-    TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());\n+    TF_RETURN_IF_ERROR(\n+        pipeline.Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+            .status());\n   }\n \n   HloPassPipeline pipeline(\"post-layout_assignment\");\n@@ -1846,7 +1867,9 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n       \"end-of-post-layout_assignment\");\n #endif  // NDEBUG\n \n-  TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());\n+  TF_RETURN_IF_ERROR(\n+      pipeline.Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+          .status());\n   return absl::OkStatus();\n }\n \n@@ -2757,7 +2780,7 @@ absl::Status GpuCompiler::RunPreSchedulingPasses(\n                                                        gpu_device_info);\n     }\n   }\n-  return pipeline.Run(module).status();\n+  return pipeline.Run(module, {HloInstruction::kMainExecutionThread}).status();\n }\n \n HloCostAnalysis::Options CreateHloAnalysisOpts(\n@@ -2899,7 +2922,8 @@ absl::Status GpuCompiler::RunPostSchedulingPipelines(\n                    module->config().debug_options().xla_ignore_channel_id(),\n                    HloVerifierOpts{}.VerifyInstructionNameUnchanged());\n   }\n-  return main_pipeline.Run(module).status();\n+  return main_pipeline.Run(module, {HloInstruction::kMainExecutionThread})\n+      .status();\n }\n \n absl::Status GpuCompiler::LoadAutotuneResultsFromFile("
        },
        {
            "sha": "2383487fb6a23cad9c9a95909d8dce0a24f1a25e",
            "filename": "third_party/xla/xla/service/gpu/gpu_hlo_schedule.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f5a585450ded29ca6187f9272ed61d56d837184/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f5a585450ded29ca6187f9272ed61d56d837184/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc?ref=5f5a585450ded29ca6187f9272ed61d56d837184",
            "patch": "@@ -646,7 +646,8 @@ absl::StatusOr<HloSchedule> ScheduleGpuModuleWithMemoryScheduler(\n   return ScheduleModule(\n       module,\n       DefaultMemoryScheduler(alias_info, size_func, PostProcessSchedule),\n-      /*execution_threads=*/{}, peak_memory_bytes);\n+      /*execution_threads=*/{HloInstruction::kMainExecutionThread},\n+      peak_memory_bytes);\n }\n \n }  // end namespace"
        },
        {
            "sha": "adb28fddea0038ddb716f504b6218378d5761ac9",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 5,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5f5a585450ded29ca6187f9272ed61d56d837184/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5f5a585450ded29ca6187f9272ed61d56d837184/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=5f5a585450ded29ca6187f9272ed61d56d837184",
            "patch": "@@ -262,7 +262,9 @@ absl::Status NVPTXCompiler::OptimizeHloConvolutionCanonicalization(\n   // CudnnConvPadForTensorCores may add instructions which can be simplified\n   // by constant folding.\n   pipeline.AddPass<HloConstantFolding>();\n-  TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());\n+  TF_RETURN_IF_ERROR(\n+      pipeline.Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+          .status());\n \n   return absl::OkStatus();\n }\n@@ -306,7 +308,9 @@ absl::Status NVPTXCompiler::OptimizeHloPostLayoutAssignment(\n   // Padding a gemm operand that's a constant results in pad(constant).  Run\n   // constant-folding to simplify this into a new constant.\n   pre_pipeline.AddPass<HloConstantFolding>();\n-  TF_RETURN_IF_ERROR(pre_pipeline.Run(hlo_module).status());\n+  TF_RETURN_IF_ERROR(\n+      pre_pipeline.Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+          .status());\n \n   TF_RETURN_IF_ERROR(GpuCompiler::OptimizeHloPostLayoutAssignment(\n       hlo_module, stream_exec, options, gpu_target_config, alias_info,\n@@ -317,7 +321,9 @@ absl::Status NVPTXCompiler::OptimizeHloPostLayoutAssignment(\n   // Transform TriangularSolve ops into custom-calls, so we can add temp\n   // memory.\n   post_pipeline.AddPass<TriangularSolveRewriter>();\n-  TF_RETURN_IF_ERROR(post_pipeline.Run(hlo_module).status());\n+  TF_RETURN_IF_ERROR(\n+      post_pipeline.Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+          .status());\n \n   return absl::OkStatus();\n }\n@@ -408,9 +414,12 @@ absl::Status NVPTXCompiler::RunCudnnCompilerPasses(\n                            module->name(), module->unique_id());\n   });\n   CuDnnFusionCompiler fusion_compiler(*stream_exec, *dnn_compiled_graphs);\n-  TF_RETURN_IF_ERROR(fusion_compiler.Run(module).status());\n+  TF_RETURN_IF_ERROR(\n+      fusion_compiler.Run(module, {HloInstruction::kMainExecutionThread})\n+          .status());\n   CuDnnCustomCallCompiler call_compiler(*stream_exec, *dnn_compiled_graphs);\n-  return call_compiler.Run(module).status();\n+  return call_compiler.Run(module, {HloInstruction::kMainExecutionThread})\n+      .status();\n }\n \n namespace {"
        }
    ],
    "stats": {
        "total": 90,
        "additions": 62,
        "deletions": 28
    }
}