{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Introduce `StreamState` to manage per-stream resources in RaggedAllToAllThunk.\n\nThis way it will be easier to manage all resource and later move heavy part of the rendezvous to `Initialize` phase.\n\nPiperOrigin-RevId: 804446982",
    "sha": "4ecfc78ec274a4338fd446a2979c959b888bc7ef",
    "files": [
        {
            "sha": "507070baf0719a1d3d2d41632792d2263873f057",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4ecfc78ec274a4338fd446a2979c959b888bc7ef/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4ecfc78ec274a4338fd446a2979c959b888bc7ef/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=4ecfc78ec274a4338fd446a2979c959b888bc7ef",
            "patch": "@@ -1127,8 +1127,8 @@ cc_library(\n         \"//xla/backends/gpu/collectives:gpu_communicator\",\n         \"//xla/core/collectives:communicator\",\n         \"//xla/core/collectives:rank_id\",\n+        \"//xla/hlo/ir:collective_op_group_mode\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/service:collective_ops_utils\",\n         \"//xla/service:rendezvous\",\n         \"//xla/service/gpu/transforms/collectives:collective_ops_utils\",\n         \"//xla/stream_executor:device_memory\","
        },
        {
            "sha": "3df64a8c9ca7891a343636390a587ab5531d0a61",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all_thunk.cc",
            "status": "modified",
            "additions": 51,
            "deletions": 41,
            "changes": 92,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4ecfc78ec274a4338fd446a2979c959b888bc7ef/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4ecfc78ec274a4338fd446a2979c959b888bc7ef/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc?ref=4ecfc78ec274a4338fd446a2979c959b888bc7ef",
            "patch": "@@ -42,9 +42,9 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/hlo/ir/collective_op_group_mode.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n #include \"xla/service/rendezvous.h\"\n #include \"xla/shape.h\"\n@@ -466,48 +466,58 @@ absl::Status RaggedAllToAllStartThunk::Initialize(\n   TF_RETURN_IF_ERROR(CollectiveThunk::Initialize(params));\n   device_count_ = params.local_device_count;\n \n-  // Allocate temp buffers in the host memory to load the sizes and offsets of\n-  // ragged tensors from device memory.\n-  absl::MutexLock lock(&mutex_);\n-  if (!host_buffer_allocs_.contains(params.executor)) {\n-    std::vector<std::unique_ptr<se::MemoryAllocation>> allocs;\n-    for (int64_t i = 0; i < kNumRaggedMetadataOperands; ++i) {\n-      TF_ASSIGN_OR_RETURN(std::unique_ptr<se::MemoryAllocation> alloc,\n-                          params.executor->HostMemoryAllocate(\n-                              config_.num_total_updates * sizeof(int64_t)));\n-      allocs.push_back(std::move(alloc));\n+  {\n+    // Allocate temp buffers in the host memory to load the sizes and offsets of\n+    // ragged tensors from device memory.\n+    absl::MutexLock lock(&mutex_);\n+    if (!host_buffer_allocs_.contains(params.executor)) {\n+      std::vector<std::unique_ptr<se::MemoryAllocation>> allocs;\n+      for (int64_t i = 0; i < kNumRaggedMetadataOperands; ++i) {\n+        TF_ASSIGN_OR_RETURN(std::unique_ptr<se::MemoryAllocation> alloc,\n+                            params.executor->HostMemoryAllocate(\n+                                config_.num_total_updates * sizeof(int64_t)));\n+        allocs.push_back(std::move(alloc));\n+      }\n+      host_buffer_allocs_.emplace(params.executor, std::move(allocs));\n     }\n-    host_buffer_allocs_.emplace(params.executor, std::move(allocs));\n-  }\n \n-  if (!device_buffer_allocs_.contains(params.executor)) {\n-    se::DeviceMemoryHandle output_offsets_device_buffer{\n-        params.executor,\n-        params.executor->Allocate(config_.num_total_updates * sizeof(int64_t))};\n+    if (!device_buffer_allocs_.contains(params.executor)) {\n+      se::DeviceMemoryHandle output_offsets_device_buffer{\n+          params.executor, params.executor->Allocate(config_.num_total_updates *\n+                                                     sizeof(int64_t))};\n \n-    if (output_offsets_device_buffer.memory().is_null()) {\n-      return absl::InternalError(\"Failed to allocate output offsets buffer.\");\n-    }\n+      if (output_offsets_device_buffer.memory().is_null()) {\n+        return absl::InternalError(\"Failed to allocate output offsets buffer.\");\n+      }\n \n-    device_buffer_allocs_.emplace(params.executor,\n-                                  std::move(output_offsets_device_buffer));\n+      device_buffer_allocs_.emplace(params.executor,\n+                                    std::move(output_offsets_device_buffer));\n+    }\n   }\n \n   if (is_local()) {\n+    absl::MutexLock lock(&mutex_);\n+\n     se::StreamExecutor* executor = params.executor;\n-    {\n-      absl::MutexLock lock(&events_mutex_);\n-      if (!start_events_.count(executor)) {\n-        TF_ASSIGN_OR_RETURN(std::unique_ptr<se::Event> event,\n-                            executor->CreateEvent());\n-        start_events_.insert({executor, std::move(event)});\n-      }\n \n-      if (!end_events_.count(executor)) {\n-        TF_ASSIGN_OR_RETURN(std::unique_ptr<se::Event> event,\n-                            executor->CreateEvent());\n-        end_events_.insert({executor, std::move(event)});\n-      }\n+    if (!per_stream_states_.contains(executor)) {\n+      TF_ASSIGN_OR_RETURN(\n+          const GpuCliqueKey clique_key,\n+          GetCollectiveGpuCliqueKey(*params.collective_params, config_.config));\n+\n+      const std::optional<RankId> rank =\n+          clique_key.rank(params.collective_params->global_device_id);\n+\n+      TF_ASSIGN_OR_RETURN(std::unique_ptr<se::Event> start_event,\n+                          executor->CreateEvent());\n+      TF_ASSIGN_OR_RETURN(std::unique_ptr<se::Event> end_event,\n+                          executor->CreateEvent());\n+\n+      auto state = std::make_unique<StreamState>(\n+          executor->device_ordinal(), rank.value(), std::move(start_event),\n+          std::move(end_event));\n+\n+      per_stream_states_.emplace(executor, std::move(state));\n     }\n   }\n   return absl::OkStatus();\n@@ -562,12 +572,10 @@ absl::StatusOr<bool> RaggedAllToAllStartThunk::RunCollective(\n       bool peer_access_enabled,\n       params.collective_cliques->peer_access_enabled(comm_handle.clique_key));\n \n-  se::Event* start_event = nullptr;\n-  se::Event* end_event = nullptr;\n+  StreamState* state = nullptr;\n   {\n-    absl::MutexLock lock(&events_mutex_);\n-    start_event = start_events_[stream.parent()].get();\n-    end_event = end_events_[stream.parent()].get();\n+    absl::MutexLock lock(&mutex_);\n+    state = per_stream_states_[stream.parent()].get();\n   }\n \n   bool should_use_one_shot_kernel =\n@@ -579,15 +587,17 @@ absl::StatusOr<bool> RaggedAllToAllStartThunk::RunCollective(\n     TF_RETURN_IF_ERROR(RunOneShotRaggedAllToAll(\n         comm_handle.clique_key, config_.num_input_rows,\n         config_.num_row_elements, config_.num_total_updates, device_buffers,\n-        stream, *rank, comm_handle.comm, start_event, end_event));\n+        stream, *rank, comm_handle.comm, state->start_event.get(),\n+        state->end_event.get()));\n     return false;\n   }\n \n   if (should_use_memcpy()) {\n     TF_RETURN_IF_ERROR(RunMemCpyRaggedAllToAll(\n         comm_handle.clique_key, *rank, config_.num_row_elements,\n         config_.num_total_updates, device_buffers, stream, comm_handle.comm,\n-        ragged_metadata_allocs, start_event, end_event));\n+        ragged_metadata_allocs, state->start_event.get(),\n+        state->end_event.get()));\n     return false;\n   }\n "
        },
        {
            "sha": "e9aadb5804a0c4390b2e2e5ed37d02300f9d3bfb",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all_thunk.h",
            "status": "modified",
            "additions": 26,
            "deletions": 9,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4ecfc78ec274a4338fd446a2979c959b888bc7ef/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4ecfc78ec274a4338fd446a2979c959b888bc7ef/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.h?ref=4ecfc78ec274a4338fd446a2979c959b888bc7ef",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n \n #include <cstdint>\n #include <memory>\n+#include <utility>\n #include <vector>\n \n #include \"absl/base/thread_annotations.h\"\n@@ -27,8 +28,9 @@ limitations under the License.\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n+#include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/hlo/ir/collective_op_group_mode.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/service/collective_ops_utils.h\"\n #include \"xla/stream_executor/device_memory_handle.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/memory_allocation.h\"\n@@ -75,6 +77,27 @@ class RaggedAllToAllStartThunk : public CollectiveThunk {\n                                      CommunicatorHandle comm) override;\n \n  private:\n+  struct StreamState {\n+    int device_ordinal;\n+    RankId rank;\n+\n+    // Event to synchronize streams on different devices at the start of the\n+    // kernel.\n+    std::unique_ptr<se::Event> start_event;\n+\n+    // Event to synchronize streams on different devices at the end of the\n+    // kernel.\n+    std::unique_ptr<se::Event> end_event;\n+\n+    StreamState(int device_ordinal, RankId rank,\n+                std::unique_ptr<se::Event> start_event,\n+                std::unique_ptr<se::Event> end_event)\n+        : device_ordinal(device_ordinal),\n+          rank(rank),\n+          start_event(std::move(start_event)),\n+          end_event(std::move(end_event)) {}\n+  };\n+\n   bool is_local() const;\n   bool should_use_memcpy() const { return p2p_memcpy_enabled_ && is_local(); }\n \n@@ -92,14 +115,8 @@ class RaggedAllToAllStartThunk : public CollectiveThunk {\n   absl::flat_hash_map<se::StreamExecutor*, se::DeviceMemoryHandle>\n       device_buffer_allocs_ ABSL_GUARDED_BY(mutex_);\n \n-  absl::Mutex events_mutex_;\n-  // Events to synchronize steams on different devices at the start of the\n-  // kernel.\n-  absl::flat_hash_map<se::StreamExecutor*, std::unique_ptr<se::Event>>\n-      start_events_ ABSL_GUARDED_BY(events_mutex_);\n-  // Events to synchronize steams on different devices at the end of the kernel.\n-  absl::flat_hash_map<se::StreamExecutor*, std::unique_ptr<se::Event>>\n-      end_events_ ABSL_GUARDED_BY(events_mutex_);\n+  absl::flat_hash_map<se::StreamExecutor*, std::unique_ptr<StreamState>>\n+      per_stream_states_ ABSL_GUARDED_BY(mutex_);\n };\n \n }  // namespace gpu"
        }
    ],
    "stats": {
        "total": 129,
        "additions": 78,
        "deletions": 51
    }
}