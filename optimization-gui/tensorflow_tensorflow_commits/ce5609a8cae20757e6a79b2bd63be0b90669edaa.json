{
    "author": "tensorflower-gardener",
    "message": "Take in shardings of named computations from the argument shardings of the function.\n\nPiperOrigin-RevId: 798206295",
    "sha": "ce5609a8cae20757e6a79b2bd63be0b90669edaa",
    "files": [
        {
            "sha": "2ce8d7921b9f5c4006a7f70bb42a8ccf2f67a430",
            "filename": "third_party/xla/xla/service/spmd/shardy/round_trip_common/import_func_calls.cc",
            "status": "modified",
            "additions": 48,
            "deletions": 3,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce5609a8cae20757e6a79b2bd63be0b90669edaa/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fimport_func_calls.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce5609a8cae20757e6a79b2bd63be0b90669edaa/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fimport_func_calls.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fimport_func_calls.cc?ref=ce5609a8cae20757e6a79b2bd63be0b90669edaa",
            "patch": "@@ -15,8 +15,10 @@ limitations under the License.\n \n #include \"xla/service/spmd/shardy/round_trip_common/import_func_calls.h\"\n \n+#include <cstdint>\n #include <iterator>\n #include <memory>\n+#include <optional>\n \n #include \"absl/log/check.h\"\n #include \"llvm/ADT/DenseMap.h\"\n@@ -57,8 +59,11 @@ using ::mlir::StringRef;\n using ::mlir::SymbolTable;\n using ::mlir::func::CallOp;\n using ::mlir::func::FuncOp;\n+using ::mlir::sdy::getTensorRank;\n using ::mlir::sdy::kShardingAttr;\n using ::mlir::sdy::NamedComputationOp;\n+using ::mlir::sdy::TensorShardingAttr;\n+using ::mlir::sdy::TensorShardingPerValueAttr;\n \n bool isInlineableCallOp(CallOp callOp) {\n   if (hasFrontendAttr(callOp, kXlaBackendConfigAttr)) {\n@@ -69,6 +74,42 @@ bool isInlineableCallOp(CallOp callOp) {\n   return !inlineableAttr || inlineableAttr->getValue();\n }\n \n+// Returns the first non-maximal mesh on the argument shardings, if there is\n+// one. Otherwise returns `std::nullopt`.\n+std::optional<mlir::Attribute> getMeshOrRefOnArguments(\n+    FuncOp funcOp, const SymbolTable& symbolTable) {\n+  for (int64_t argNum = 0; argNum < funcOp.getNumArguments(); ++argNum) {\n+    if (TensorShardingAttr sdySharding =\n+            funcOp.getArgAttrOfType<TensorShardingAttr>(argNum, kShardingAttr);\n+        sdySharding && !sdySharding.getMesh(symbolTable).isMaximal()) {\n+      return std::make_optional(sdySharding.getMeshOrRef());\n+    }\n+  }\n+  return std::nullopt;\n+}\n+\n+TensorShardingPerValueAttr getFuncArgShardings(CallOp callOp, FuncOp funcOp,\n+                                               const SymbolTable& symbolTable) {\n+  std::optional<mlir::Attribute> meshOrRef =\n+      getMeshOrRefOnArguments(funcOp, symbolTable);\n+  if (!meshOrRef) {\n+    return nullptr;\n+  }\n+  mlir::SmallVector<TensorShardingAttr> argShardings;\n+  argShardings.reserve(funcOp.getNumArguments());\n+  for (int64_t argNum = 0; argNum < funcOp.getNumArguments(); ++argNum) {\n+    TensorShardingAttr sdySharding =\n+        funcOp.getArgAttrOfType<TensorShardingAttr>(argNum, kShardingAttr);\n+    argShardings.push_back(sdySharding\n+                               ? sdySharding\n+                               : TensorShardingAttr::getFullyOpen(\n+                                     funcOp.getContext(),\n+                                     getTensorRank(callOp.getOperand(argNum)),\n+                                     *meshOrRef));\n+  }\n+  return TensorShardingPerValueAttr::get(funcOp.getContext(), argShardings);\n+}\n+\n void importCallOp(\n     CallOp callOp,\n     llvm::SmallDenseMap<StringRef, mlir::Region*>& calleeNameToMovedRegion,\n@@ -81,11 +122,17 @@ void importCallOp(\n                 });\n \n   StringRef calleeName = callOp.getCallee();\n+  FuncOp funcOp = symbolTable.lookup<FuncOp>(calleeName);\n+  CHECK(funcOp) << \"Failed to lookup function: \" << calleeName.str();\n+\n   rewriter.setInsertionPoint(callOp);\n   auto namedCompOp = rewriter.create<NamedComputationOp>(\n       callOp->getLoc(), callOp->getResultTypes(), calleeName,\n       callOp.getOperands(),\n-      /*inShardings=*/nullptr,\n+      /*inShardings=*/\n+      getFuncArgShardings(callOp, funcOp, symbolTable),\n+      // TODO(b/439018088): Take func result shardings if call op result\n+      // shardings are empty.\n       /*outShardings=*/mlir::sdy::getShardingPerValue(callOp));\n   namedCompOp->setAttrs(namedCompAttrs);\n \n@@ -102,8 +149,6 @@ void importCallOp(\n     rewriter.cloneRegionBefore(*movedRegionIt->second, namedCompRegion,\n                                namedCompRegion.begin());\n   } else {\n-    FuncOp funcOp = symbolTable.lookup<FuncOp>(calleeName);\n-    CHECK(funcOp) << \"Failed to lookup function: \" << calleeName.str();\n     mlir::sdy::inlineRegionAndConvertTerminatorOp<mlir::sdy::ReturnOp>(\n         funcOp.getBody(), namedCompRegion);\n     calleeNameToMovedRegion[calleeName] = &namedCompRegion;"
        },
        {
            "sha": "8907452cdd65c037b65799004022a6a982aad476",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/import_func_calls.mlir",
            "status": "modified",
            "additions": 177,
            "deletions": 3,
            "changes": 180,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce5609a8cae20757e6a79b2bd63be0b90669edaa/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fimport_func_calls.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce5609a8cae20757e6a79b2bd63be0b90669edaa/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fimport_func_calls.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fimport_func_calls.mlir?ref=ce5609a8cae20757e6a79b2bd63be0b90669edaa",
            "patch": "@@ -26,7 +26,7 @@ func.func private @foo(%arg0: tensor<8x2xi32>) -> tensor<8x2xi32> {\n \n // CHECK-LABEL: func @backend_config_out_shardings\n func.func @backend_config_out_shardings(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) {\n-  // CHECK-NEXT: %[[NC:.*]] = sdy.named_computation<\"bar\">(%arg0) out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n+  // CHECK-NEXT: %[[NC:.*]] = sdy.named_computation<\"bar\">(%arg0) in_shardings=[<@mesh, [{\"x\"}, {}]>] out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n   // CHECK-NEXT:   %[[MULT:.*]] = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}} : tensor<8x2xi32>\n   // CHECK-NEXT:   sdy.return %[[MULT]] : tensor<8x2xi32>\n   // CHECK-NEXT: } {mhlo.frontend_attributes = {backend_config = \"{\\22flag_configs\\22:[],\\22scoped_memory_configs\\22:[],\\22device_type\\22:\\22DEVICE_TYPE_HOST\\22,\\22used_scoped_memory_configs\\22:[]}\"},\n@@ -103,14 +103,14 @@ sdy.mesh @mesh = #sdy.mesh<[\"x\"=2, \"y\"=2]>\n \n // CHECK-LABEL: func @multiple_call_ops_same_name\n func.func @multiple_call_ops_same_name(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) {\n-  // CHECK-NEXT: %[[NC_0:.*]] = sdy.named_computation<\"foobar\">(%arg0) out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n+  // CHECK-NEXT: %[[NC_0:.*]] = sdy.named_computation<\"foobar\">(%arg0) in_shardings=[<@mesh, [{\"x\"}, {}]>] out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n   // CHECK-NEXT:   %[[MULT_0:.*]] = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}} : tensor<8x2xi32>\n   // CHECK-NEXT:   sdy.return %[[MULT_0]] : tensor<8x2xi32>\n   // CHECK-NEXT: } {mhlo.frontend_attributes = {backend_config = \"{\\22flag_configs\\22:[],\\22scoped_memory_configs\\22:[],\\22device_type\\22:\\22DEVICE_TYPE_HOST\\22,\\22used_scoped_memory_configs\\22:[]}\"},\n   // CHECK-SAME:    random_attr = \"random_value\"}\n   // CHECK-SAME: (tensor<8x2xi32>) -> tensor<8x2xi32>\n \n-  // CHECK-NEXT: %[[NC_1:.*]] = sdy.named_computation<\"foobar\">(%[[NC_0]]) out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n+  // CHECK-NEXT: %[[NC_1:.*]] = sdy.named_computation<\"foobar\">(%[[NC_0]]) in_shardings=[<@mesh, [{\"x\"}, {}]>] out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n   // CHECK-NEXT:   %[[MULT_1:.*]] = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}} : tensor<8x2xi32>\n   // CHECK-NEXT:   sdy.return %[[MULT_1]] : tensor<8x2xi32>\n   // CHECK-NEXT: } {mhlo.frontend_attributes = {backend_config = \"{\\22flag_configs\\22:[],\\22scoped_memory_configs\\22:[],\\22device_type\\22:\\22DEVICE_TYPE_HOST\\22,\\22used_scoped_memory_configs\\22:[]}\"},\n@@ -134,6 +134,40 @@ func.func private @foobar(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@\n \n // -----\n \n+sdy.mesh @mesh = #sdy.mesh<[\"x\"=2, \"y\"=2]>\n+\n+// CHECK-LABEL: func @multiple_call_ops_same_name_func_no_input_output_shardings\n+func.func @multiple_call_ops_same_name_func_no_input_output_shardings(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) {\n+  // CHECK-NEXT: %[[NC_0:.*]] = sdy.named_computation<\"foobar\">(%arg0) out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n+  // CHECK-NEXT:   %[[MULT_0:.*]] = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}} : tensor<8x2xi32>\n+  // CHECK-NEXT:   sdy.return %[[MULT_0]] : tensor<8x2xi32>\n+  // CHECK-NEXT: } {mhlo.frontend_attributes = {backend_config = \"{\\22flag_configs\\22:[],\\22scoped_memory_configs\\22:[],\\22device_type\\22:\\22DEVICE_TYPE_HOST\\22,\\22used_scoped_memory_configs\\22:[]}\"},\n+  // CHECK-SAME:    random_attr = \"random_value\"}\n+  // CHECK-SAME: (tensor<8x2xi32>) -> tensor<8x2xi32>\n+\n+  // CHECK-NEXT: %[[NC_1:.*]] = sdy.named_computation<\"foobar\">(%[[NC_0]]) out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n+  // CHECK-NEXT:   %[[MULT_1:.*]] = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}} : tensor<8x2xi32>\n+  // CHECK-NEXT:   sdy.return %[[MULT_1]] : tensor<8x2xi32>\n+  // CHECK-NEXT: } {mhlo.frontend_attributes = {backend_config = \"{\\22flag_configs\\22:[],\\22scoped_memory_configs\\22:[],\\22device_type\\22:\\22DEVICE_TYPE_HOST\\22,\\22used_scoped_memory_configs\\22:[]}\"},\n+  // CHECK-SAME:    random_attr = \"random_value\"}\n+  // CHECK-SAME: (tensor<8x2xi32>) -> tensor<8x2xi32>\n+\n+  // CHECK-NEXT: %[[MOVE_TO_HOST:.*]] = stablehlo.custom_call @MoveToHost(%[[NC_1]]) {backend_config = \"\"} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n+  // CHECK-NEXT: return %[[MOVE_TO_HOST]] : tensor<8x2xi32>\n+  %0 = call @foobar(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {\"y\"}]>]>, random_attr = \"random_value\", mhlo.frontend_attributes = {backend_config = \"{\\22flag_configs\\22:[],\\22scoped_memory_configs\\22:[],\\22device_type\\22:\\22DEVICE_TYPE_HOST\\22,\\22used_scoped_memory_configs\\22:[]}\"}} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n+  %1 = call @foobar(%0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {\"y\"}]>]>, random_attr = \"random_value\", mhlo.frontend_attributes = {backend_config = \"{\\22flag_configs\\22:[],\\22scoped_memory_configs\\22:[],\\22device_type\\22:\\22DEVICE_TYPE_HOST\\22,\\22used_scoped_memory_configs\\22:[]}\"}} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n+  %2 = stablehlo.custom_call @MoveToHost(%1) {backend_config = \"\"} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n+  return %2 : tensor<8x2xi32>\n+}\n+\n+// CHECK-NOT: func private @foobar\n+func.func private @foobar(%arg0: tensor<8x2xi32>) -> tensor<8x2xi32> {\n+  %0 = stablehlo.multiply %arg0, %arg0 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}} : tensor<8x2xi32>\n+  return %0 : tensor<8x2xi32>\n+}\n+\n+// -----\n+\n // CHECK-LABEL: func @non_flat_call_graph_all_uninlineable\n func.func @non_flat_call_graph_all_uninlineable(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n   // CHECK-NEXT: %[[NC1:.*]] = sdy.named_computation<\"foo\">(%arg0) (%arg1: tensor<8xf32>) {\n@@ -218,3 +252,143 @@ func.func private @baz(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n   %0 = stablehlo.abs %arg0 : tensor<8xf32>\n   return %0 : tensor<8xf32>\n }\n+\n+// -----\n+\n+sdy.mesh @mesh = #sdy.mesh<[\"x\"=2, \"y\"=2]>\n+\n+// CHECK-LABEL: func @single_call_multiple_args_func_no_input_sharding\n+func.func @single_call_multiple_args_func_no_input_sharding(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> tensor<8x2xi32> {\n+  // CHECK-NEXT: %[[NC:.*]] = sdy.named_computation<\"foo\">(%arg0, %arg0) out_shardings=[<@mesh, [{\"y\"}, {\"x\"}]>] (%arg1: tensor<8x2xi32>, %arg2: tensor<8x2xi32>) {\n+  // CHECK-NEXT:   %[[MULTIPLY:.*]] = stablehlo.multiply %arg1, %arg2 : tensor<8x2xi32>\n+  // CHECK-NEXT:   sdy.return %[[MULTIPLY]] : tensor<8x2xi32>\n+  // CHECK-NEXT: } {mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>) -> tensor<8x2xi32>\n+  // CHECK-NEXT: %[[NEGATE:.*]] = stablehlo.negate %[[NC]] : tensor<8x2xi32>\n+  // CHECK-NEXT: return %[[NEGATE]] : tensor<8x2xi32>\n+  %0 = call @foo(%arg0, %arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}, {\"x\"}]>]>, mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>) -> tensor<8x2xi32>\n+  %1 = stablehlo.negate %0 : tensor<8x2xi32>\n+  return %1 : tensor<8x2xi32>\n+}\n+\n+// CHECK-NOT: func private @foo\n+func.func private @foo(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32>) -> tensor<8x2xi32> {\n+  %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x2xi32>\n+  return %0 : tensor<8x2xi32>\n+}\n+\n+// -----\n+\n+sdy.mesh @mesh = #sdy.mesh<[\"x\"=2, \"y\"=2]>\n+\n+// CHECK-LABEL: func @single_call_multiple_args_func_all_arguments_with_input_sharding\n+func.func @single_call_multiple_args_func_all_arguments_with_input_sharding(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> tensor<8x2xi32> {\n+  // CHECK-NEXT: %[[NC:.*]] = sdy.named_computation<\"foo\">(%arg0, %arg0) in_shardings=[<@mesh, [{\"x\"}, {}]>, <@mesh, [{}, {\"y\"}]>] out_shardings=[<@mesh, [{\"y\"}, {\"x\"}]>] (%arg1: tensor<8x2xi32>, %arg2: tensor<8x2xi32>) {\n+  // CHECK-NEXT:   %[[MULTIPLY:.*]] = stablehlo.multiply %arg1, %arg2 : tensor<8x2xi32>\n+  // CHECK-NEXT:   sdy.return %[[MULTIPLY]] : tensor<8x2xi32>\n+  // CHECK-NEXT: } {mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>) -> tensor<8x2xi32>\n+  // CHECK-NEXT: %[[NEGATE:.*]] = stablehlo.negate %[[NC]] : tensor<8x2xi32>\n+  // CHECK-NEXT: return %[[NEGATE]] : tensor<8x2xi32>\n+  %0 = call @foo(%arg0, %arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}, {\"x\"}]>]>, mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>) -> tensor<8x2xi32>\n+  %1 = stablehlo.negate %0 : tensor<8x2xi32>\n+  return %1 : tensor<8x2xi32>\n+}\n+\n+// CHECK-NOT: func private @foo\n+func.func private @foo(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {}]>}, %arg1: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {\"y\"}]>}) -> tensor<8x2xi32> {\n+  %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x2xi32>\n+  return %0 : tensor<8x2xi32>\n+}\n+\n+// -----\n+\n+sdy.mesh @mesh = #sdy.mesh<[\"x\"=2, \"y\"=2]>\n+\n+// CHECK-LABEL: func @single_call_multiple_args_func_some_arguments_with_input_sharding_some_arguments_without\n+func.func @single_call_multiple_args_func_some_arguments_with_input_sharding_some_arguments_without(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> tensor<8x2xi32> {\n+  // CHECK-NEXT: %[[NC:.*]] = sdy.named_computation<\"foo\">(%arg0, %arg0) in_shardings=[<@mesh, [{?}, {?}]>, <@mesh, [{}, {\"y\"}]>] out_shardings=[<@mesh, [{\"y\"}, {\"x\"}]>] (%arg1: tensor<8x2xi32>, %arg2: tensor<8x2xi32>) {\n+  // CHECK-NEXT:   %[[MULTIPLY:.*]] = stablehlo.multiply %arg1, %arg2 : tensor<8x2xi32>\n+  // CHECK-NEXT:   sdy.return %[[MULTIPLY]] : tensor<8x2xi32>\n+  // CHECK-NEXT: } {mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>) -> tensor<8x2xi32>\n+  // CHECK-NEXT: %[[NEGATE:.*]] = stablehlo.negate %[[NC]] : tensor<8x2xi32>\n+  // CHECK-NEXT: return %[[NEGATE]] : tensor<8x2xi32>\n+  %0 = call @foo(%arg0, %arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}, {\"x\"}]>]>, mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>) -> tensor<8x2xi32>\n+  %1 = stablehlo.negate %0 : tensor<8x2xi32>\n+  return %1 : tensor<8x2xi32>\n+}\n+\n+// CHECK-NOT: func private @foo\n+func.func private @foo(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {\"y\"}]>}) -> tensor<8x2xi32> {\n+  %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x2xi32>\n+  return %0 : tensor<8x2xi32>\n+}\n+\n+// -----\n+\n+sdy.mesh @mesh = #sdy.mesh<[\"x\"=2, \"y\"=2]>\n+\n+// CHECK-LABEL: func @single_call_multiple_args_with_different_ranks_func_some_arguments_with_input_sharding_some_arguments_without\n+func.func @single_call_multiple_args_with_different_ranks_func_some_arguments_with_input_sharding_some_arguments_without(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}, %arg1: tensor<8xi32>) -> tensor<8x2xi32> {\n+  // CHECK-NEXT: %[[NC:.*]] = sdy.named_computation<\"foo\">(%arg0, %arg1) in_shardings=[<@mesh, [{?}, {?}]>, <@mesh, [{\"y\"}]>] out_shardings=[<@mesh, [{\"y\"}, {\"x\"}]>] (%arg2: tensor<8x2xi32>, %arg3: tensor<8xi32>) {\n+  // CHECK-NEXT:   %[[MULTIPLY:.*]] = stablehlo.multiply %arg2, %arg2 : tensor<8x2xi32>\n+  // CHECK-NEXT:   sdy.return %[[MULTIPLY]] : tensor<8x2xi32>\n+  // CHECK-NEXT: } {mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8xi32>) -> tensor<8x2xi32>\n+  // CHECK-NEXT: %[[NEGATE:.*]] = stablehlo.negate %[[NC]] : tensor<8x2xi32>\n+  // CHECK-NEXT: return %[[NEGATE]] : tensor<8x2xi32>\n+  %0 = call @foo(%arg0, %arg1) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}, {\"x\"}]>]>, mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8xi32>) -> tensor<8x2xi32>\n+  %1 = stablehlo.negate %0 : tensor<8x2xi32>\n+  return %1 : tensor<8x2xi32>\n+}\n+\n+// CHECK-NOT: func private @foo\n+func.func private @foo(%arg0: tensor<8x2xi32>, %arg1: tensor<8xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"y\"}]>}) -> tensor<8x2xi32> {\n+  %0 = stablehlo.multiply %arg0, %arg0 : tensor<8x2xi32>\n+  return %0 : tensor<8x2xi32>\n+}\n+\n+// -----\n+\n+sdy.mesh @mesh = #sdy.mesh<[\"x\"=2, \"y\"=2]>\n+sdy.mesh @mesh_maximal = #sdy.mesh<[], device_ids=[0]>\n+\n+// CHECK-LABEL: func @single_call_multiple_args_func_some_arguments_with_input_sharding_on_maximal_mesh_some_arguments_without\n+func.func @single_call_multiple_args_func_some_arguments_with_input_sharding_on_maximal_mesh_some_arguments_without(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> tensor<8x2xi32> {\n+  // CHECK-NEXT: %[[NC:.*]] = sdy.named_computation<\"foo\">(%arg0, %arg0) out_shardings=[<@mesh, [{\"y\"}, {\"x\"}]>] (%arg1: tensor<8x2xi32>, %arg2: tensor<8x2xi32>) {\n+  // CHECK-NEXT:   %[[MULTIPLY:.*]] = stablehlo.multiply %arg1, %arg2 : tensor<8x2xi32>\n+  // CHECK-NEXT:   sdy.return %[[MULTIPLY]] : tensor<8x2xi32>\n+  // CHECK-NEXT: } {mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>) -> tensor<8x2xi32>\n+  // CHECK-NEXT: %[[NEGATE:.*]] = stablehlo.negate %[[NC]] : tensor<8x2xi32>\n+  // CHECK-NEXT: return %[[NEGATE]] : tensor<8x2xi32>\n+  %0 = call @foo(%arg0, %arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}, {\"x\"}]>]>, mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>) -> tensor<8x2xi32>\n+  %1 = stablehlo.negate %0 : tensor<8x2xi32>\n+  return %1 : tensor<8x2xi32>\n+}\n+\n+// CHECK-NOT: func private @foo\n+func.func private @foo(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh_maximal, []>}) -> tensor<8x2xi32> {\n+  %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x2xi32>\n+  return %0 : tensor<8x2xi32>\n+}\n+\n+// -----\n+\n+sdy.mesh @mesh = #sdy.mesh<[\"x\"=2, \"y\"=2]>\n+sdy.mesh @mesh_maximal = #sdy.mesh<[], device_ids=[0]>\n+\n+// CHECK-LABEL: func @single_call_multiple_args_func_some_arguments_with_input_sharding_on_maximal_and_on_non_maximal_mesh_some_arguments_without\n+func.func @single_call_multiple_args_func_some_arguments_with_input_sharding_on_maximal_and_on_non_maximal_mesh_some_arguments_without(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> tensor<8x2xi32> {\n+  // CHECK-NEXT: %[[NC:.*]] = sdy.named_computation<\"foo\">(%arg0, %arg0, %arg0) in_shardings=[<@mesh, [{?}, {?}]>, <@mesh_maximal, []>, <@mesh, [{}, {\"y\"}]>] out_shardings=[<@mesh, [{\"y\"}, {\"x\"}]>] (%arg1: tensor<8x2xi32>, %arg2: tensor<8x2xi32>, %arg3: tensor<8x2xi32>) {\n+  // CHECK-NEXT:   %[[MULTIPLY:.*]] = stablehlo.multiply %arg1, %arg2 : tensor<8x2xi32>\n+  // CHECK-NEXT:   sdy.return %[[MULTIPLY]] : tensor<8x2xi32>\n+  // CHECK-NEXT: } {mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>, tensor<8x2xi32>) -> tensor<8x2xi32>\n+  // CHECK-NEXT: %[[NEGATE:.*]] = stablehlo.negate %[[NC]] : tensor<8x2xi32>\n+  // CHECK-NEXT: return %[[NEGATE]] : tensor<8x2xi32>\n+  %0 = call @foo(%arg0, %arg0, %arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"y\"}, {\"x\"}]>]>, mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8x2xi32>, tensor<8x2xi32>, tensor<8x2xi32>) -> tensor<8x2xi32>\n+  %1 = stablehlo.negate %0 : tensor<8x2xi32>\n+  return %1 : tensor<8x2xi32>\n+}\n+\n+// CHECK-NOT: func private @foo\n+func.func private @foo(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh_maximal, []>}, %arg2: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {\"y\"}]>}) -> tensor<8x2xi32> {\n+  %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x2xi32>\n+  return %0 : tensor<8x2xi32>\n+}"
        },
        {
            "sha": "3aba8e5d5f4c882f792829048bb706d071953940",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/import_func_calls_only_unlineable_false.mlir",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce5609a8cae20757e6a79b2bd63be0b90669edaa/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fimport_func_calls_only_unlineable_false.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce5609a8cae20757e6a79b2bd63be0b90669edaa/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fimport_func_calls_only_unlineable_false.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fimport_func_calls_only_unlineable_false.mlir?ref=ce5609a8cae20757e6a79b2bd63be0b90669edaa",
            "patch": "@@ -26,7 +26,7 @@ func.func private @foo(%arg0: tensor<8x2xi32>) -> tensor<8x2xi32> {\n \n // CHECK-LABEL: func @backend_config_out_shardings\n func.func @backend_config_out_shardings(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) {\n-  // CHECK-NEXT: %[[NC:.*]] = sdy.named_computation<\"bar\">(%arg0) out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n+  // CHECK-NEXT: %[[NC:.*]] = sdy.named_computation<\"bar\">(%arg0) in_shardings=[<@mesh, [{\"x\"}, {}]>] out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n   // CHECK-NEXT:   %[[MULT:.*]] = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}} : tensor<8x2xi32>\n   // CHECK-NEXT:   sdy.return %[[MULT]] : tensor<8x2xi32>\n   // CHECK-NEXT: } {mhlo.frontend_attributes = {backend_config = \"{\\22flag_configs\\22:[],\\22scoped_memory_configs\\22:[],\\22device_type\\22:\\22DEVICE_TYPE_HOST\\22,\\22used_scoped_memory_configs\\22:[]}\"},\n@@ -112,14 +112,14 @@ sdy.mesh @mesh = #sdy.mesh<[\"x\"=2, \"y\"=2]>\n \n // CHECK-LABEL: func @multiple_call_ops_same_name\n func.func @multiple_call_ops_same_name(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) {\n-  // CHECK-NEXT: %[[NC_0:.*]] = sdy.named_computation<\"foobar\">(%arg0) out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n+  // CHECK-NEXT: %[[NC_0:.*]] = sdy.named_computation<\"foobar\">(%arg0) in_shardings=[<@mesh, [{\"x\"}, {}]>] out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n   // CHECK-NEXT:   %[[MULT_0:.*]] = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}} : tensor<8x2xi32>\n   // CHECK-NEXT:   sdy.return %[[MULT_0]] : tensor<8x2xi32>\n   // CHECK-NEXT: } {mhlo.frontend_attributes = {backend_config = \"{\\22flag_configs\\22:[],\\22scoped_memory_configs\\22:[],\\22device_type\\22:\\22DEVICE_TYPE_HOST\\22,\\22used_scoped_memory_configs\\22:[]}\"},\n   // CHECK-SAME:    random_attr = \"random_value\"}\n   // CHECK-SAME: (tensor<8x2xi32>) -> tensor<8x2xi32>\n \n-  // CHECK-NEXT: %[[NC_1:.*]] = sdy.named_computation<\"foobar\">(%[[NC_0]]) out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n+  // CHECK-NEXT: %[[NC_1:.*]] = sdy.named_computation<\"foobar\">(%[[NC_0]]) in_shardings=[<@mesh, [{\"x\"}, {}]>] out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n   // CHECK-NEXT:   %[[MULT_1:.*]] = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}} : tensor<8x2xi32>\n   // CHECK-NEXT:   sdy.return %[[MULT_1]] : tensor<8x2xi32>\n   // CHECK-NEXT: } {mhlo.frontend_attributes = {backend_config = \"{\\22flag_configs\\22:[],\\22scoped_memory_configs\\22:[],\\22device_type\\22:\\22DEVICE_TYPE_HOST\\22,\\22used_scoped_memory_configs\\22:[]}\"},"
        }
    ],
    "stats": {
        "total": 237,
        "additions": 228,
        "deletions": 9
    }
}