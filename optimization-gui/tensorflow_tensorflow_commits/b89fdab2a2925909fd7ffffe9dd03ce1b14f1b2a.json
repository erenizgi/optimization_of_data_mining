{
    "author": "AleksaArsic",
    "message": "PR #32773: [ROCm] Fix convolution fp16 performance drop on gfx11xx, gfx12xx\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32773\n\nüìù Summary of Changes\nRemove hardcoded NHWC convolution layout for fp16 precision.\n\nüéØ Justification\nPerformance drops for fp16 precision on gfx11xx and gfx12xx GPUs were observed internally, as well as by the [community](https://github.com/jax-ml/jax/issues/30548).\n\nüöÄ Kind of Contribution\nüêõ Bug Fix\n\nüìä Benchmark\nCommunity member provided the script with whom the [profiling can be done](https://github.com/jax-ml/jax/issues/30548#issue-3270872993).\nSignificant performance improvement for fp16 on gfx12xx:\n```\nRunning on: rocm:0\n\nTesting float32...\nAvg time: 0.092307 s, Throughput: 1.68 TFLOP/s\n\nTesting float16...\nAvg time: 0.011742 s, Throughput: 13.17 TFLOP/s\n\nTesting bfloat16...\nAvg time: 0.011989 s, Throughput: 12.90 TFLOP/s\n```\nResults of the profiling before the fix:\n```\nRunning on: rocm:0\n\nTesting float32...\nAvg time: 0.092312 s, Throughput: 1.67 TFLOP/s\n\nTesting float16...\nAvg time: 0.775142 s, Throughput: 0.20 TFLOP/s\n\nTesting bfloat16...\nAvg time: 0.011990 s, Throughput: 12.90 TFLOP/s\n```\n\n@xla-rotation can you please review this PR?\n\nCopybara import of the project:\n\n--\nc9fdba79e32c13d9cbf640e61d941d071fabba9d by Aleksa Arsic <Aleksa.Arsic@amd.com>:\n\nRemove hardcoded convolution NCHW layout assignment for fp16 precision.\n\n--\n69660d19999a14b24d63b52e6dae310cfbdcbb6b by Aleksa Arsic <Aleksa.Arsic@amd.com>:\n\nAdd unit tests for ROCm layout assignment.\n\nMerging this change closes #32773\n\nPiperOrigin-RevId: 822022522",
    "sha": "b89fdab2a2925909fd7ffffe9dd03ce1b14f1b2a",
    "files": [
        {
            "sha": "c122bd7afb22e3704e4aa05b0b8c61dda38b2f91",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b89fdab2a2925909fd7ffffe9dd03ce1b14f1b2a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b89fdab2a2925909fd7ffffe9dd03ce1b14f1b2a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc?ref=b89fdab2a2925909fd7ffffe9dd03ce1b14f1b2a",
            "patch": "@@ -162,16 +162,9 @@ HeuristicLayoutAssignment(const HloInstruction* instr,\n     }\n   }\n \n-  const auto* rocm_compute_capability =\n-      std::get_if<se::RocmComputeCapability>(&gpu_version);\n-  if (rocm_compute_capability && input_ty == F16) {\n-    return kAllNHWC;\n-  }\n-\n-  // If we're not Volta or not fp16/bfloat16, or not conv2D, the decision is\n-  // easy: Use NCHW.\n   const bool isFloat16 = (input_ty == F16) || (input_ty == BF16);\n   if (std::holds_alternative<se::CudaComputeCapability>(gpu_version)) {\n+    // CUDA:\n     // If we're not Volta or not fp16/bfloat16, or not conv2D, the decision is\n     // easy: Use NCHW.\n     const auto* cuda_compute_capability =\n@@ -184,6 +177,9 @@ HeuristicLayoutAssignment(const HloInstruction* instr,\n       return kAllNCHW;\n     }\n   } else if (std::holds_alternative<se::RocmComputeCapability>(gpu_version)) {\n+    // ROCm:\n+    // If we do not have NHWC layout support or not fp16/bfloat16, or not\n+    // conv2D, or ROCm NHWC is disabled the decision is to use NCHW.\n     bool is_enabled = false;\n     TF_CHECK_OK(tsl::ReadBoolFromEnvVar(\"TF_USE_ROCM_NHWC\",\n                                         /*default_val=*/false, &is_enabled));\n@@ -198,7 +194,7 @@ HeuristicLayoutAssignment(const HloInstruction* instr,\n \n   VLOG(2) << \"Using heuristic to figure out layouts for \" << instr->ToString();\n \n-  // For other Volta f16 convolutions, use NHWC.\n+  // For other f16 convolutions, use NHWC.\n   return kAllNHWC;\n }\n "
        },
        {
            "sha": "8ca9f42234c2bb87d4ece02c52d7141774599258",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment_test.cc",
            "status": "modified",
            "additions": 158,
            "deletions": 0,
            "changes": 158,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b89fdab2a2925909fd7ffffe9dd03ce1b14f1b2a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b89fdab2a2925909fd7ffffe9dd03ce1b14f1b2a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_test.cc?ref=b89fdab2a2925909fd7ffffe9dd03ce1b14f1b2a",
            "patch": "@@ -60,6 +60,10 @@ class LayoutAssignmentTest : public HloTestBase {\n     return backend().default_stream_executor()->GetDeviceDescription();\n   }\n \n+  se::RocmComputeCapability GetRocmComputeCapability() {\n+    return GetDeviceDescription().rocm_compute_capability();\n+  }\n+\n   se::CudaComputeCapability GetCudaComputeCapability() {\n     return GetDeviceDescription().cuda_compute_capability();\n   }\n@@ -626,6 +630,160 @@ ENTRY entry {\n   EXPECT_EQ(output_layout, LayoutUtil::GetDefaultLayoutForR3());\n }\n \n+TEST_F(LayoutAssignmentTest, FP16ROCmConvolutionHasNCHWLayoutRDNA) {\n+  const char* hlo = R\"(\n+ENTRY entry {\n+  p0 = f16[2,64,64,16]{3,2,1,0} parameter(0)\n+  p1 = f16[6,16,3,32]{3,2,1,0} parameter(1)\n+  ROOT conv = (f64[2,64,64,32]{3,2,1,0}, u8[0]{0}) custom-call(p0, p1),\n+    window={size=3x3 pad=1_1x1_1}, dim_labels=b10f_o10i->b10f,\n+    custom_call_target=\"__cudnn$convForward\"\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> hlo_module,\n+                          ParseAndReturnVerifiedModule(hlo));\n+  ComputationLayout computation_layout(\n+      hlo_module->entry_computation()->ComputeProgramShape());\n+\n+  GpuLayoutAssignment layout_assignment(\n+      &computation_layout, se::RocmComputeCapability::EarliestRDNASupport(),\n+      GetDnnVersion(), GetDeviceDescription());\n+\n+  EXPECT_THAT(layout_assignment.Run(hlo_module.get()),\n+              absl_testing::IsOkAndHolds(true));\n+\n+  // We start from b10f_o10i->b10f, meaning that the inputs start out as\n+  // NWHC_OWHI->NWHC. Layout assignment should yield layouts of the form\n+  // {1,2,3,0} for both inputs and for the output, therefore, in order to get to\n+  // the desired NCHW_OIHW->NCHW layout.\n+  EXPECT_THAT(\n+      RunFileCheck(hlo_module->ToString(HloPrintOptions::ShortParsable()), R\"(\n+      // CHECK-DAG: [[P0:[^ ]+]] = {{.*}} parameter(0)\n+      // CHECK-DAG: [[P1:[^ ]+]] = {{.*}} parameter(1)\n+      // CHECK-DAG: [[COPY_P0:[^ ]+]] = {{.*}}{1,2,3,0} copy([[P0]])\n+      // CHECK-DAG: [[COPY_P1:[^ ]+]] = {{.*}}{1,2,3,0} copy([[P1]])\n+      // CHECK:     [[CONV:[^ ]+]] = {{.*}}{1,2,3,0}, {{.*}} custom-call([[COPY_P0]], [[COPY_P1]])\n+      )\"),\n+      absl_testing::IsOkAndHolds(true));\n+}\n+\n+TEST_F(LayoutAssignmentTest, FP32ROCmConvolutionHasNCHWLayoutRDNA) {\n+  const char* hlo = R\"(\n+ENTRY entry {\n+  p0 = f32[2,64,64,16]{3,2,1,0} parameter(0)\n+  p1 = f32[6,16,3,32]{3,2,1,0} parameter(1)\n+  ROOT conv = (f64[2,64,64,32]{3,2,1,0}, u8[0]{0}) custom-call(p0, p1),\n+    window={size=3x3 pad=1_1x1_1}, dim_labels=b10f_o10i->b10f,\n+    custom_call_target=\"__cudnn$convForward\"\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> hlo_module,\n+                          ParseAndReturnVerifiedModule(hlo));\n+  ComputationLayout computation_layout(\n+      hlo_module->entry_computation()->ComputeProgramShape());\n+\n+  GpuLayoutAssignment layout_assignment(\n+      &computation_layout, se::RocmComputeCapability::EarliestRDNASupport(),\n+      GetDnnVersion(), GetDeviceDescription());\n+\n+  EXPECT_THAT(layout_assignment.Run(hlo_module.get()),\n+              absl_testing::IsOkAndHolds(true));\n+\n+  // We start from b10f_o10i->b10f, meaning that the inputs start out as\n+  // NWHC_OWHI->NWHC. Layout assignment should yield layouts of the form\n+  // {1,2,3,0} for both inputs and for the output, therefore, in order to get to\n+  // the desired NCHW_OIHW->NCHW layout.\n+  EXPECT_THAT(\n+      RunFileCheck(hlo_module->ToString(HloPrintOptions::ShortParsable()), R\"(\n+      // CHECK-DAG: [[P0:[^ ]+]] = {{.*}} parameter(0)\n+      // CHECK-DAG: [[P1:[^ ]+]] = {{.*}} parameter(1)\n+      // CHECK-DAG: [[COPY_P0:[^ ]+]] = {{.*}}{1,2,3,0} copy([[P0]])\n+      // CHECK-DAG: [[COPY_P1:[^ ]+]] = {{.*}}{1,2,3,0} copy([[P1]])\n+      // CHECK:     [[CONV:[^ ]+]] = {{.*}}{1,2,3,0}, {{.*}} custom-call([[COPY_P0]], [[COPY_P1]])\n+      )\"),\n+      absl_testing::IsOkAndHolds(true));\n+}\n+\n+TEST_F(LayoutAssignmentTest, FP16ROCmConvolutionHasNHWCLayoutCDNA) {\n+  // Enable ROCm NHWC for this test\n+  setenv(\"TF_USE_ROCM_NHWC\", \"true\", 1);\n+\n+  const char* hlo = R\"(\n+ENTRY entry {\n+  p0 = f16[2,64,64,16]{3,2,1,0} parameter(0)\n+  p1 = f16[6,16,3,32]{3,2,1,0} parameter(1)\n+  ROOT conv = (f64[2,64,64,32]{3,2,1,0}, u8[0]{0}) custom-call(p0, p1),\n+    window={size=3x3 pad=1_1x1_1}, dim_labels=b10f_o10i->b10f,\n+    custom_call_target=\"__cudnn$convForward\"\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> hlo_module,\n+                          ParseAndReturnVerifiedModule(hlo));\n+  ComputationLayout computation_layout(\n+      hlo_module->entry_computation()->ComputeProgramShape());\n+\n+  GpuLayoutAssignment layout_assignment(\n+      &computation_layout, se::RocmComputeCapability::EarliestCDNASupport(),\n+      GetDnnVersion(), GetDeviceDescription());\n+\n+  EXPECT_THAT(layout_assignment.Run(hlo_module.get()),\n+              absl_testing::IsOkAndHolds(true));\n+\n+  // We start from b10f_o10i->b10f, meaning that the inputs start out as\n+  // NWHC_OWHI->NWHC. Layout assignment should yield layouts of the form\n+  // {3,1,2,0} (transpose the middle dimensions) for both inputs and for the\n+  // output, therefore, in order to get to the desired NHWC_OHWI->NHWC layout.\n+  EXPECT_THAT(\n+      RunFileCheck(hlo_module->ToString(HloPrintOptions::ShortParsable()), R\"(\n+      // CHECK-DAG: [[P0:[^ ]+]] = {{.*}} parameter(0)\n+      // CHECK-DAG: [[P1:[^ ]+]] = {{.*}} parameter(1)\n+      // CHECK-DAG: [[COPY_P0:[^ ]+]] = {{.*}}{3,1,2,0} copy([[P0]])\n+      // CHECK-DAG: [[COPY_P1:[^ ]+]] = {{.*}}{3,1,2,0} copy([[P1]])\n+      // CHECK:     [[CONV:[^ ]+]] = {{.*}}{3,1,2,0}, {{.*}} custom-call([[COPY_P0]], [[COPY_P1]])\n+      )\"),\n+      absl_testing::IsOkAndHolds(true));\n+\n+  // Clean up after the test\n+  unsetenv(\"TF_USE_ROCM_NHWC\");\n+}\n+\n+TEST_F(LayoutAssignmentTest, FP32ROCmConvolutionHasNCHWLayoutCDNA) {\n+  const char* hlo = R\"(\n+ENTRY entry {\n+  p0 = f32[2,64,64,16]{3,2,1,0} parameter(0)\n+  p1 = f32[6,16,3,32]{3,2,1,0} parameter(1)\n+  ROOT conv = (f64[2,64,64,32]{3,2,1,0}, u8[0]{0}) custom-call(p0, p1),\n+    window={size=3x3 pad=1_1x1_1}, dim_labels=b10f_o10i->b10f,\n+    custom_call_target=\"__cudnn$convForward\"\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> hlo_module,\n+                          ParseAndReturnVerifiedModule(hlo));\n+  ComputationLayout computation_layout(\n+      hlo_module->entry_computation()->ComputeProgramShape());\n+\n+  GpuLayoutAssignment layout_assignment(\n+      &computation_layout, se::RocmComputeCapability::EarliestCDNASupport(),\n+      GetDnnVersion(), GetDeviceDescription());\n+\n+  EXPECT_THAT(layout_assignment.Run(hlo_module.get()),\n+              absl_testing::IsOkAndHolds(true));\n+\n+  // We start from b10f_o10i->b10f, meaning that the inputs start out as\n+  // NWHC_OWHI->NWHC. Layout assignment should yield layouts of the form\n+  // {1,2,3,0} for both inputs and for the output, therefore, in order to get to\n+  // the desired NCHW_OIHW->NCHW layout.\n+  EXPECT_THAT(\n+      RunFileCheck(hlo_module->ToString(HloPrintOptions::ShortParsable()), R\"(\n+      // CHECK-DAG: [[P0:[^ ]+]] = {{.*}} parameter(0)\n+      // CHECK-DAG: [[P1:[^ ]+]] = {{.*}} parameter(1)\n+      // CHECK-DAG: [[COPY_P0:[^ ]+]] = {{.*}}{1,2,3,0} copy([[P0]])\n+      // CHECK-DAG: [[COPY_P1:[^ ]+]] = {{.*}}{1,2,3,0} copy([[P1]])\n+      // CHECK:     [[CONV:[^ ]+]] = {{.*}}{1,2,3,0}, {{.*}} custom-call([[COPY_P0]], [[COPY_P1]])\n+      )\"),\n+      absl_testing::IsOkAndHolds(true));\n+}\n+\n TEST_F(LayoutAssignmentTest, CuDNNConvolutionHasNHWCLayoutPostHopper) {\n   const char* hlo = R\"(\n ENTRY entry {"
        },
        {
            "sha": "3dce2854ed9a58eeefa898d52219aa05cf163eb7",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_compute_capability.h",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b89fdab2a2925909fd7ffffe9dd03ce1b14f1b2a/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_compute_capability.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b89fdab2a2925909fd7ffffe9dd03ce1b14f1b2a/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_compute_capability.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_compute_capability.h?ref=b89fdab2a2925909fd7ffffe9dd03ce1b14f1b2a",
            "patch": "@@ -43,6 +43,14 @@ class RocmComputeCapability {\n \n   RocmComputeCapability() = default;\n \n+  static RocmComputeCapability EarliestCDNASupport() {\n+    return RocmComputeCapability{\"gfx908\"};\n+  }\n+\n+  static RocmComputeCapability EarliestRDNASupport() {\n+    return RocmComputeCapability{\"gfx1030\"};\n+  }\n+\n   std::string gcn_arch_name() const { return gcn_arch_name_; }\n \n   std::string ToString() const { return gcn_arch_name(); }\n@@ -83,8 +91,7 @@ class RocmComputeCapability {\n       \"gfx1030\",  // RX68xx / RX69xx\n       \"gfx1100\",  // RX7900\n       \"gfx1101\",  // RX7700 / RX7800\n-      \"gfx1103\", \"gfx1150\", \"gfx1151\", \"gfx1200\", \"gfx1201\",\n-  };\n+      \"gfx1103\", \"gfx1150\", \"gfx1151\", \"gfx1200\", \"gfx1201\"};\n \n   bool is_supported_gfx_version() const {\n     return IsThisGfxInAnyList(kSupportedGfxVersions);"
        }
    ],
    "stats": {
        "total": 183,
        "additions": 172,
        "deletions": 11
    }
}