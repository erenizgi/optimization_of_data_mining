{
    "author": "ermilovmaxim",
    "message": "argument removal without building prototype\n\nPiperOrigin-RevId: 839014575",
    "sha": "a57f99ab2cc78ef9af662255f054f8d40cbafbd4",
    "files": [
        {
            "sha": "2dcc248a5c0e4c7c34bd2d1de1a92d69e025c33c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a57f99ab2cc78ef9af662255f054f8d40cbafbd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a57f99ab2cc78ef9af662255f054f8d40cbafbd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD?ref=a57f99ab2cc78ef9af662255f054f8d40cbafbd4",
            "patch": "@@ -258,7 +258,6 @@ cc_library(\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\","
        },
        {
            "sha": "ad12a2d092394848832a4272af180f58aee8585d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.cc",
            "status": "modified",
            "additions": 89,
            "deletions": 59,
            "changes": 148,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a57f99ab2cc78ef9af662255f054f8d40cbafbd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a57f99ab2cc78ef9af662255f054f8d40cbafbd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc?ref=a57f99ab2cc78ef9af662255f054f8d40cbafbd4",
            "patch": "@@ -14,6 +14,7 @@ limitations under the License.\n ==============================================================================*/\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n \n+#include <cstdint>\n #include <string>\n #include <utility>\n #include <vector>\n@@ -51,11 +52,59 @@ limitations under the License.\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n namespace gpu {\n \n+void CopySelectAttrs(const llvm::Function& src, llvm::Function& dst) {\n+  for (uint32_t arg_idx = 0; arg_idx < src.arg_size(); arg_idx++) {\n+    // Get the original argument to extract attributes from if they exist.\n+    llvm::Argument* src_arg = src.getArg(arg_idx);\n+    llvm::Argument& dst_arg = *dst.getArg(arg_idx);\n+    dst_arg.setName(absl::StrCat(\"arg\", arg_idx));\n+\n+    if (src_arg->hasByValAttr()) {\n+      dst.addParamAttr(arg_idx, src_arg->getAttribute(llvm::Attribute::ByVal));\n+    }\n+\n+    // If the alignment has been specified in the original function, use it.\n+    // Otherwise, use the alignment from the kernel argument.\n+    if (src_arg->hasAttribute(llvm::Attribute::Alignment)) {\n+      dst.addParamAttr(arg_idx,\n+                       src_arg->getAttribute(llvm::Attribute::Alignment));\n+    }\n+    if (src_arg->hasAttribute(\"nvvm.grid_constant\")) {\n+      dst.addParamAttr(arg_idx, llvm::Attribute::get(dst_arg.getContext(),\n+                                                     \"nvvm.grid_constant\"));\n+    }\n+  }\n+}\n+\n+void AnnotateAttrsIfUnset(const emitters::KernelArguments& arguments,\n+                          llvm::Function& dst) {\n+  for (auto&& [arg_idx, kernel_argument] : llvm::enumerate(arguments.args())) {\n+    llvm::Argument& dst_arg = *dst.getArg(arg_idx);\n+    dst_arg.setName(absl::StrCat(\"arg\", arg_idx));\n+\n+    if (!dst_arg.hasByValAttr()) {\n+      dst.addDereferenceableParamAttr(arg_idx, kernel_argument.slice().size());\n+    }\n+    // If the alignment has been specified in the original function, use it.\n+    // Otherwise, use the alignment from the kernel argument.\n+    if (!dst_arg.hasAttribute(llvm::Attribute::Alignment) &&\n+        kernel_argument.alignment()) {\n+      dst.addParamAttr(\n+          arg_idx,\n+          llvm::Attribute::get(dst_arg.getContext(), llvm::Attribute::Alignment,\n+                               kernel_argument.alignment()));\n+    }\n+    if (!kernel_argument.aliased()) {\n+      dst.addParamAttr(arg_idx, llvm::Attribute::get(dst_arg.getContext(),\n+                                                     llvm::Attribute::NoAlias));\n+    }\n+  }\n+}\n+\n // Annotates the launch dimensions of the corresponding IR kernel in\n // `llvm_module`.\n absl::Status AnnotateKernelLaunchDimensions(\n@@ -157,43 +206,12 @@ absl::StatusOr<llvm::Function*> BuildKernelPrototypeFromUniqueName(\n   // that return instruction.\n   builder->SetInsertPoint(llvm::ReturnInst::Create(context, entry_bb));\n   // Get the original function to extract attributes.\n-  auto impl_func = llvm_module->getFunction(impl_fn_name);\n+  llvm::Function* impl_func = llvm_module->getFunction(impl_fn_name);\n \n-  for (auto&& [arg_idx, kernel_argument] : llvm::enumerate(arguments.args())) {\n-    // Get the original argument to extract attributes from if they exist.\n-    llvm::Argument* impl_arg = impl_func ? impl_func->getArg(arg_idx) : nullptr;\n-    llvm::Argument& llvm_arg = *kernel->getArg(arg_idx);\n-    llvm_arg.setName(absl::StrCat(\"arg\", arg_idx));\n-\n-    if (impl_arg && impl_arg->hasByValAttr()) {\n-      kernel->addParamAttr(arg_idx,\n-                           impl_arg->getAttribute(llvm::Attribute::ByVal));\n-    } else {\n-      kernel->addDereferenceableParamAttr(arg_idx,\n-                                          kernel_argument.slice().size());\n-    }\n-    // If the alignment has been specified in the original function, use it.\n-    // Otherwise, use the alignment from the kernel argument.\n-    if (impl_arg && impl_arg->hasAttribute(llvm::Attribute::Alignment)) {\n-      kernel->addParamAttr(arg_idx,\n-                           impl_arg->getAttribute(llvm::Attribute::Alignment));\n-    } else if (kernel_argument.alignment() > 0) {\n-      // Scalars don't need an alignment attribute.\n-      kernel->addParamAttr(arg_idx,\n-                           llvm::Attribute::get(llvm_arg.getContext(),\n-                                                llvm::Attribute::Alignment,\n-                                                kernel_argument.alignment()));\n-    }\n-    if (!kernel_argument.aliased()) {\n-      kernel->addParamAttr(arg_idx,\n-                           llvm::Attribute::get(llvm_arg.getContext(),\n-                                                llvm::Attribute::NoAlias));\n-    }\n-    if (impl_arg && impl_arg->hasAttribute(\"nvvm.grid_constant\")) {\n-      kernel->addParamAttr(arg_idx, llvm::Attribute::get(llvm_arg.getContext(),\n-                                                         \"nvvm.grid_constant\"));\n-    }\n+  if (impl_func) {\n+    CopySelectAttrs(*impl_func, *kernel);\n   }\n+  AnnotateAttrsIfUnset(arguments, *kernel);\n   return kernel;\n }\n \n@@ -216,41 +234,53 @@ absl::StatusOr<llvm::Function*> BuildKernelPrototype(\n // device-side TMA APIs.\n absl::StatusOr<llvm::Function*> RemoveUnusedTritonAbiArguments(\n     llvm::Module* llvm_module, IrEmitterContext& ir_emitter_context,\n-    const std::string& sanitized_kernel_name,\n-    LaunchDimensions& launch_dimensions,\n-    const emitters::KernelArguments& kernel_arguments) {\n+    const std::string& sanitized_kernel_name) {\n   llvm::Function* impl_fn = llvm_module->getFunction(sanitized_kernel_name);\n   TF_RET_CHECK(impl_fn);\n   impl_fn->setName(ir_emitter_context.GetSanitizedUniqueName(\n-      absl::StrCat(sanitized_kernel_name, \"_impl\")));\n+      sanitized_kernel_name + \"_impl\"));\n \n-  llvm::IRBuilder builder(llvm_module->getContext());\n+  constexpr int arg_to_remove = 2;\n \n-  TF_ASSIGN_OR_RETURN(llvm::Function * kernel,\n-                      BuildKernelPrototypeFromUniqueName(\n-                          llvm_module, ir_emitter_context.gpu_device_info(),\n-                          impl_fn->getName().str(), sanitized_kernel_name,\n-                          kernel_arguments, launch_dimensions, &builder));\n+  auto fn_attrs = impl_fn->getAttributes();\n+  llvm::SmallVector<llvm::Type*, 8> arg_types;\n+\n+  for (uint32_t i = 0; i < impl_fn->arg_size(); i++) {\n+    if (i < impl_fn->arg_size() - arg_to_remove) {\n+      arg_types.push_back(impl_fn->getArg(i)->getType());\n+    } else {\n+      fn_attrs = fn_attrs.removeParamAttributes(llvm_module->getContext(), i);\n+\n+      auto arg = impl_fn->getArg(i);\n+      arg->replaceAllUsesWith(llvm::ConstantPointerNull::get(\n+          llvm::cast<llvm::PointerType>(arg->getType())));\n+    }\n+  }\n+\n+  llvm::FunctionType* new_type =\n+      llvm::FunctionType::get(impl_fn->getReturnType(), arg_types, false);\n+\n+  auto inserted =\n+      llvm_module\n+          ->getOrInsertFunction(sanitized_kernel_name, new_type, fn_attrs)\n+          .getCallee();\n+  llvm::Function* new_function = static_cast<llvm::Function*>(inserted);\n+\n+  new_function->setCallingConv(impl_fn->getCallingConv());\n+  new_function->copyMetadata(impl_fn, 0);\n+  new_function->setAttributes(impl_fn->getAttributes());\n+\n+  new_function->splice(new_function->begin(), impl_fn);\n \n-  // Move function body into kernel prototype.\n-  llvm::Function* prototype_func = builder.GetInsertBlock()->getParent();\n-  prototype_func->splice(prototype_func->begin(), impl_fn);\n   for (const auto& [impl_fn_arg, kernel_arg] :\n-       llvm::zip(impl_fn->args(), kernel->args())) {\n+       llvm::zip(impl_fn->args(), new_function->args())) {\n+    kernel_arg.setName(impl_fn_arg.getName());\n     impl_fn_arg.replaceAllUsesWith(&kernel_arg);\n   }\n-  CHECK_EQ(impl_fn->arg_size(), kernel->arg_size() + 2);\n-\n-  auto tma_scratchpad_arg = impl_fn->getArg(impl_fn->arg_size() - 2);\n-  tma_scratchpad_arg->replaceAllUsesWith(llvm::ConstantPointerNull::get(\n-      llvm::cast<llvm::PointerType>(tma_scratchpad_arg->getType())));\n-  auto profiling_scratchpad_arg = impl_fn->getArg(impl_fn->arg_size() - 1);\n-  profiling_scratchpad_arg->replaceAllUsesWith(llvm::ConstantPointerNull::get(\n-      llvm::cast<llvm::PointerType>(profiling_scratchpad_arg->getType())));\n \n   impl_fn->eraseFromParent();\n \n-  return kernel;\n+  return new_function;\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "3c5c1329221cce016b009731921d556abe732443",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.h",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a57f99ab2cc78ef9af662255f054f8d40cbafbd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a57f99ab2cc78ef9af662255f054f8d40cbafbd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h?ref=a57f99ab2cc78ef9af662255f054f8d40cbafbd4",
            "patch": "@@ -100,6 +100,10 @@ class KernelFusionInterface : public FusionInterface {\n       const Shape& shape, mlir::MLIRContext* ctx);\n };\n \n+void CopySelectAttrs(const llvm::Function& src, llvm::Function& dst);\n+void AnnotateAttrsIfUnset(const emitters::KernelArguments& arguments,\n+                          llvm::Function& dst);\n+\n absl::StatusOr<llvm::Function*> BuildKernelPrototype(\n     llvm::Module* llvm_module, const se::DeviceDescription& gpu_device_info,\n     const std::string& impl_fn_name, const std::string& unique_kernel_name,\n@@ -108,9 +112,7 @@ absl::StatusOr<llvm::Function*> BuildKernelPrototype(\n \n absl::StatusOr<llvm::Function*> RemoveUnusedTritonAbiArguments(\n     llvm::Module* llvm_module, IrEmitterContext& ir_emitter_context,\n-    const std::string& sanitized_kernel_name,\n-    LaunchDimensions& launch_dimensions,\n-    const emitters::KernelArguments& arguments);\n+    const std::string& sanitized_kernel_name);\n \n absl::Status AnnotateKernelLaunchDimensions(\n     const se::DeviceDescription& device_info,"
        },
        {
            "sha": "39dbfc66c41cd51ec19da4ba85ccdc068a954ba4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a57f99ab2cc78ef9af662255f054f8d40cbafbd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a57f99ab2cc78ef9af662255f054f8d40cbafbd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=a57f99ab2cc78ef9af662255f054f8d40cbafbd4",
            "patch": "@@ -56,6 +56,7 @@ cc_library(\n         \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:launch_dim\",\n+        \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/log\","
        },
        {
            "sha": "366ebcb6d216c25e07f87f128b77d4d3c9d562cd",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a57f99ab2cc78ef9af662255f054f8d40cbafbd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a57f99ab2cc78ef9af662255f054f8d40cbafbd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=a57f99ab2cc78ef9af662255f054f8d40cbafbd4",
            "patch": "@@ -58,6 +58,7 @@ limitations under the License.\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n+#include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n \n@@ -189,10 +190,15 @@ absl::StatusOr<TritonFusion::EmitResult> TritonFusion::Emit(\n     TF_ASSIGN_OR_RETURN(\n         llvm::Function * kernel,\n         RemoveUnusedTritonAbiArguments(local_module.get(), ir_emitter_context,\n-                                       sanitized_kernel_name, launch_dimensions,\n-                                       kernel_arguments));\n+                                       sanitized_kernel_name));\n+\n+    AnnotateAttrsIfUnset(kernel_arguments, *kernel);\n     PopulateNvvmAnnotations(local_module.get(), kernel, triton_wrapper_result);\n \n+    TF_RETURN_IF_ERROR(AnnotateKernelLaunchDimensions(\n+        ir_emitter_context.gpu_device_info(), launch_dimensions, kernel,\n+        local_module.get()));\n+\n     return {{kernel->getName().str(), launch_dimensions,\n              triton_wrapper_result.cluster_dim,\n              triton_wrapper_result.shmem_bytes, /*binary=*/\"\","
        },
        {
            "sha": "ea1e289b3965337d49cdf08e24a97ab09eb78188",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a57f99ab2cc78ef9af662255f054f8d40cbafbd4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a57f99ab2cc78ef9af662255f054f8d40cbafbd4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc?ref=a57f99ab2cc78ef9af662255f054f8d40cbafbd4",
            "patch": "@@ -1327,10 +1327,15 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTritonCustomCall(\n             ir_emitter_context_->gpu_device_info().threads_per_warp()));\n \n     if (emit_kernels) {\n-      TF_RETURN_IF_ERROR(RemoveUnusedTritonAbiArguments(\n-                             local_module.get(), *ir_emitter_context_,\n-                             kernel_name, launch_dimensions, kernel_arguments)\n-                             .status());\n+      TF_ASSIGN_OR_RETURN(\n+          llvm::Function * kernel,\n+          RemoveUnusedTritonAbiArguments(local_module.get(),\n+                                         *ir_emitter_context_, kernel_name));\n+\n+      AnnotateAttrsIfUnset(kernel_arguments, *kernel);\n+      TF_RETURN_IF_ERROR(AnnotateKernelLaunchDimensions(\n+          ir_emitter_context_->gpu_device_info(), launch_dimensions, kernel,\n+          local_module.get()));\n     }\n \n     kernel_modules_.push_back(std::move(local_module));"
        }
    ],
    "stats": {
        "total": 181,
        "additions": 112,
        "deletions": 69
    }
}