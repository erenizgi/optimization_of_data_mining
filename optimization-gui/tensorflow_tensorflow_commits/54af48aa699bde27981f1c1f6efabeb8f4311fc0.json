{
    "author": "sohaibiftikhar",
    "message": "[XLA:GPU]: Add triton_xla.atomic_write operation.\n\natomic_write allows to do an atomic write to a given memory location,\nor a tensor of memory locations, with the given memory scope and semantic.\n\nPiperOrigin-RevId: 804795320",
    "sha": "54af48aa699bde27981f1c1f6efabeb8f4311fc0",
    "files": [
        {
            "sha": "d8d1a1617fdd3010189df8681a2db4a356c5a8e0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/triton_xla_ops.td",
            "status": "modified",
            "additions": 58,
            "deletions": 0,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/54af48aa699bde27981f1c1f6efabeb8f4311fc0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/54af48aa699bde27981f1c1f6efabeb8f4311fc0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td?ref=54af48aa699bde27981f1c1f6efabeb8f4311fc0",
            "patch": "@@ -23,11 +23,19 @@ include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n include \"mlir/Interfaces/ViewLikeInterface.td\" // OffsetSizeAndStrideOpInterface\n include \"xla/backends/gpu/codegen/triton/ir/triton_xla_dialect.td\"\n+include \"triton/Dialect/Triton/IR/TritonAttrDefs.td\" // TT_MemSemanticAttr, TT_MemSyncScopeAttr\n include \"triton/Dialect/Triton/IR/TritonInterfaces.td\"\n include \"triton/Dialect/Triton/IR/TritonOpInterfaces.td\"\n include \"triton/Dialect/Triton/IR/TritonTypes.td\"\n include \"triton/Dialect/TritonGPU/IR/TritonGPUTypeInterfaces.td\"\n \n+\n+// -----------------------------------------------------------------------------\n+// Interfaces\n+// -----------------------------------------------------------------------------\n+// Copied over from TritonOps.td which cannot be directly included here.\n+def GlobalMemory : Resource<\"::mlir::triton::GlobalMemory\">;\n+\n // -----------------------------------------------------------------------------\n // Triton XLA Ops\n // -----------------------------------------------------------------------------\n@@ -210,4 +218,54 @@ def TTXLA_GetTidOp : TTXLA_Op<\"get_tid\", [Pure]> {\n   let assemblyFormat = \"attr-dict `:` functional-type(operands, $result)\";\n }\n \n+def TTXLA_AtomicWriteOp : TTXLA_Op<\"atomic_write\", []> {\n+  let summary = \"Atomically write u32 value(s) to memory location(s).\";\n+\n+  let description = [{\n+      Atomically write u32 value(s) to the memory location(s) specified by\n+      $ptr. The memory ordering guarantees are specified by the\n+      $mem_sync_(scope|semantic) attribute.\n+\n+      An optional $mask argument can be provided to specify which of the\n+      sublanes/threads should be updated. The mask must be the same shape as\n+      the $ptr.\n+\n+      Memory Synchronization Scope:\n+        - cta: The write is visible to all threads within the same compute\n+          thread-block.\n+        - gpu: The write is visible to all threads within the same GPU.\n+        - sys: The write is visible to all threads within the system.\n+\n+      The following memory synchronization semantics are allowed:\n+        - relaxed: No memory ordering guarantees.\n+        - release: All writes that are issued before this one will be\n+          completed before the store is visible to other threads.\n+\n+      Internally this operation expands into st.global.<scope>.<semantic>.u32\n+  }];\n+\n+  let arguments = (ins\n+    Arg<TT_PtrLike, \"\",\n+      [MemWrite<GlobalMemory>]>:$ptr,\n+    TT_I32Like:$value,\n+    Optional<TT_BoolLike>:$mask,\n+    TT_MemSyncScopeAttr:$mem_sync_scope,\n+    TT_MemSemanticAttr:$mem_sync_semantic\n+  );\n+\n+  // An atomic write has no results.\n+  let results = (outs);\n+\n+  // We keep the mem_sync_scope and mem_sync_semantic attributes in the\n+  // assembly format to allow parsing them from strings.\n+  // The alternative through attr-dict makes the string form look like:\n+  // { mem_sync_scope = 1, mem_sync_semantic = 3 } which is not very readable.\n+  let assemblyFormat = [{\n+    $mem_sync_scope `,` $mem_sync_semantic `,`\n+    $ptr `,` $value (`,` $mask^) ? attr-dict `:`\n+    functional-type(operands, results)\n+  }];\n+}\n+\n #endif // XLA_BACKENDS_GPU_CODEGEN_TRITON_IR_TRITON_XLA_OPS_TD_\n+"
        },
        {
            "sha": "998b04ce6891645baa74a1e0d397021e5f417be1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/54af48aa699bde27981f1c1f6efabeb8f4311fc0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/54af48aa699bde27981f1c1f6efabeb8f4311fc0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD?ref=54af48aa699bde27981f1c1f6efabeb8f4311fc0",
            "patch": "@@ -37,6 +37,7 @@ cc_library(\n         \"round_f32_to_tf32_for_tf32_dot_pass.cc\",\n         \"triton_xla_extract_insert_to_triton_pass.cc\",\n         \"triton_xla_fold_transpose_pass.cc\",\n+        \"triton_xla_lower_atomics_pass.cc\",\n         \"triton_xla_lower_get_tid_pass.cc\",\n         \"triton_xla_squeeze_dims_pass.cc\",\n     ],"
        },
        {
            "sha": "56966be7e98717c7c46fe92ccc1d6e6f067e2fef",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/54af48aa699bde27981f1c1f6efabeb8f4311fc0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/54af48aa699bde27981f1c1f6efabeb8f4311fc0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h?ref=54af48aa699bde27981f1c1f6efabeb8f4311fc0",
            "patch": "@@ -40,6 +40,7 @@ std::unique_ptr<mlir::Pass> CreateInt4ToPackedInt4RewritePass(\n std::unique_ptr<mlir::Pass> CreateRoundF32ToTF32ForTf32DotRewritePass();\n std::unique_ptr<mlir::Pass> CreateExtractTmaInfoPass();\n std::unique_ptr<mlir::Pass> CreateTritonXLALowerGetTidPass();\n+std::unique_ptr<mlir::Pass> CreateTritonXLALowerAtomicsPass();\n \n // Returns true if the `op` contains an operation in it's regions that satisfies\n // the `fn`."
        },
        {
            "sha": "7c4a9d2c5e77d4e4335608c07524bc0602c04b4b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.td",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/54af48aa699bde27981f1c1f6efabeb8f4311fc0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/54af48aa699bde27981f1c1f6efabeb8f4311fc0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td?ref=54af48aa699bde27981f1c1f6efabeb8f4311fc0",
            "patch": "@@ -119,4 +119,14 @@ def TritonXLALowerGetTidPass\n   let constructor = \"CreateTritonXLALowerGetTidPass()\";\n }\n \n+def TritonXLALowerAtomicsPass\n+    : Pass<\"triton-xla-atomics\", \"mlir::ModuleOp\"> {\n+  let summary = \"Lower triton_xla.atomic operations.\";\n+  let description = [{\n+    This pass lowers atomic operations to the PTX intrinsic with the given\n+    memory semantics.\n+  }];\n+  let constructor = \"CreateTritonXLALowerAtomicsPass()\";\n+}\n+\n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_PASSES_TD_"
        },
        {
            "sha": "5ecc1d18be87a816550efe5bf593ca6c95407bbe",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_atomic_write.mlir",
            "status": "added",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/54af48aa699bde27981f1c1f6efabeb8f4311fc0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_atomic_write.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/54af48aa699bde27981f1c1f6efabeb8f4311fc0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_atomic_write.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_atomic_write.mlir?ref=54af48aa699bde27981f1c1f6efabeb8f4311fc0",
            "patch": "@@ -0,0 +1,41 @@\n+// RUN: xla-opt %s --triton-xla-atomics | FileCheck %s\n+\n+// CHECK-LABEL: tt.func @nomask_kernel\n+// CHECK-SAME: (%[[ARG0:.+]]: !tt.ptr<i32>, %[[ARG1:.+]]: i32)\n+tt.func @nomask_kernel(%ptr : !tt.ptr<i32>, %value : i32) {\n+  // CHECK-NEXT: %[[RES:.+]] = tt.elementwise_inline_asm\n+  // CHECK-SAME: st.global.gpu.relaxed.u32 [$1], $2;\n+  // CHECK-SAME: {constraints = \"=r,l,r\", packed_element = 1 : i32, pure = false}\n+  // CHECK-SAME: %[[ARG0]], %[[ARG1]] : !tt.ptr<i32>, i32 -> i32\n+  triton_xla.atomic_write gpu, relaxed, %ptr,  %value: (!tt.ptr<i32>, i32) -> ()\n+  // CHECK-NEXT: tt.return\n+  tt.return\n+}\n+\n+// CHECK-LABEL: tt.func @nomask_vector_kernel\n+// CHECK-SAME: (%[[ARG0:.+]]: tensor<8x!tt.ptr<i32>>, %[[ARG1:.+]]: i32)\n+tt.func @nomask_vector_kernel(%ptr : tensor<8x!tt.ptr<i32>>, %value : i32) {\n+  // CHECK-NEXT: %[[RES:.+]] = tt.elementwise_inline_asm\n+  // CHECK-SAME: st.global.gpu.relaxed.u32 [$1], $2;\n+  // CHECK-SAME: {constraints = \"=r,l,r\", packed_element = 1 : i32, pure = false}\n+  // CHECK-SAME: %[[ARG0]], %[[ARG1]] : tensor<8x!tt.ptr<i32>>, i32 -> tensor<8xi32>\n+  triton_xla.atomic_write gpu, relaxed, %ptr,  %value:\n+    (tensor<8x!tt.ptr<i32>>, i32) -> ()\n+  // CHECK-NEXT: tt.return\n+  tt.return\n+}\n+\n+// CHECK-LABEL: tt.func @mask_kernel\n+// CHECK-SAME: (%[[ARG0:.+]]: tensor<4x!tt.ptr<i32>>, %[[ARG1:.+]]: i32, %[[ARG2:.+]]: tensor<4xi1>)\n+tt.func @mask_kernel(%ptr : tensor<4x!tt.ptr<i32>>, %value : i32, %mask : tensor<4xi1>) {\n+  // CHECK-NEXT: %[[RES:.+]] = tt.elementwise_inline_asm\n+  // CHECK-SAME: .reg .pred %p<>;\n+  // CHECK-SAME: setp.ne.u32 %p<>, $3, 0;\n+  // CHECK-SAME: @%p st.global.sys.release.u32 [$1], $2;\n+  // CHECK-SAME: {constraints = \"=r,l,r,r\", packed_element = 1 : i32, pure = false}\n+  // CHECK-SAME: %[[ARG0]], %[[ARG1]], %[[ARG2]] : tensor<4x!tt.ptr<i32>>, i32, tensor<4xi1> -> tensor<4xi32>\n+  triton_xla.atomic_write sys, release, %ptr,  %value, %mask:\n+    (tensor<4x!tt.ptr<i32>>, i32, tensor<4xi1>) -> ()\n+  // CHECK-NEXT: tt.return\n+  tt.return\n+}"
        },
        {
            "sha": "60befd63fc1bde083ce0f32dbb92c7ed45f22755",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_lower_atomics_pass.cc",
            "status": "added",
            "additions": 151,
            "deletions": 0,
            "changes": 151,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/54af48aa699bde27981f1c1f6efabeb8f4311fc0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_atomics_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/54af48aa699bde27981f1c1f6efabeb8f4311fc0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_atomics_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_atomics_pass.cc?ref=54af48aa699bde27981f1c1f6efabeb8f4311fc0",
            "patch": "@@ -0,0 +1,151 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <memory>\n+#include <string>\n+#include <utility>\n+\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/OpDefinition.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/ValueRange.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+\n+namespace mlir::triton::xla {\n+\n+#define GEN_PASS_DEF_TRITONXLALOWERATOMICSPASS\n+#include \"xla/backends/gpu/codegen/triton/transforms/passes.h.inc\"\n+\n+namespace {\n+\n+absl::string_view GetMemorySemanticStr(triton::MemSemantic semantic) {\n+  switch (semantic) {\n+    case triton::MemSemantic::RELAXED:\n+      return \"relaxed\";\n+    case triton::MemSemantic::ACQUIRE:\n+      return \"acquire\";\n+    case triton::MemSemantic::RELEASE:\n+      return \"release\";\n+    case triton::MemSemantic::ACQUIRE_RELEASE:\n+      return \"acq_rel\";\n+  }\n+}\n+\n+absl::string_view GetMemSyncScopeStr(triton::MemSyncScope scope) {\n+  switch (scope) {\n+    case triton::MemSyncScope::GPU:\n+      return \"gpu\";\n+    case triton::MemSyncScope::SYSTEM:\n+      return \"sys\";\n+    case triton::MemSyncScope::CTA:\n+      return \"cta\";\n+  }\n+}\n+\n+LogicalResult LowerAtomicWriteOp(AtomicWriteOp atomic_write,\n+                                 PatternRewriter& rewriter) {\n+  mlir::ImplicitLocOpBuilder builder(atomic_write.getLoc(), rewriter);\n+\n+  mlir::Value addr = atomic_write.getPtr();\n+  mlir::Value value = atomic_write.getValue();\n+  triton::MemSemantic semantic = atomic_write.getMemSyncSemantic();\n+  if (semantic != triton::MemSemantic::RELAXED &&\n+      semantic != triton::MemSemantic::RELEASE) {\n+    return rewriter.notifyMatchFailure(\n+        atomic_write, absl::StrFormat(\"Unsupported memory semantic: %s\",\n+                                      stringifyMemSemantic(semantic)));\n+  }\n+  absl::string_view memory_semantic = GetMemorySemanticStr(semantic);\n+  absl::string_view scope = GetMemSyncScopeStr(atomic_write.getMemSyncScope());\n+\n+  // Predicate ASM to check if the mask is set.\n+  // NB: The arguments start from 1 because 0 is reserved to be the output.\n+  // Even though we don't care about result($0) in this case it must be there\n+  // for ElementwiseInlineAsmOp verifiers to work\n+  constexpr absl::string_view kAtomicWriteAsmWithMaskTemplate = R\"(\n+    .reg .pred %%p<>;\n+    setp.ne.u32 %%p<>, $3, 0;\n+    @%%p st.global.%s.%s.u32 [$1], $2;\n+  )\";\n+  constexpr absl::string_view kAtomicWriteAsmTemplate = R\"(\n+    st.global.%s.%s.u32 [$1], $2;\n+  )\";\n+\n+  const auto i32_type = rewriter.getI32Type();\n+  mlir::Type result_type = i32_type;\n+  auto ranked_tensor_type =\n+      mlir::dyn_cast<mlir::RankedTensorType>(addr.getType());\n+  if (ranked_tensor_type) {\n+    // Tensor arguments must have tensor result type.\n+    result_type =\n+        mlir::RankedTensorType::get(ranked_tensor_type.getShape(), i32_type);\n+  }\n+  mlir::Value mask = atomic_write.getMask();\n+  if (mask) {\n+    const std::string atomic_write_asm_with_mask = absl::StrFormat(\n+        kAtomicWriteAsmWithMaskTemplate, scope, memory_semantic);\n+    builder.create<triton::ElementwiseInlineAsmOp>(\n+        /*result_types=*/result_type,\n+        /*asm_string=*/rewriter.getStringAttr(atomic_write_asm_with_mask),\n+        /*constraints=*/rewriter.getStringAttr(\"=r,l,r,r\"),\n+        /*pure=*/rewriter.getBoolAttr(false),\n+        /*packed_element=*/rewriter.getI32IntegerAttr(1),\n+        /*args=*/mlir::ValueRange{addr, value, mask});\n+  } else {\n+    const std::string atomic_write_asm =\n+        absl::StrFormat(kAtomicWriteAsmTemplate, scope, memory_semantic);\n+    builder.create<triton::ElementwiseInlineAsmOp>(\n+        /*result_types=*/result_type,\n+        /*asm_string=*/rewriter.getStringAttr(atomic_write_asm),\n+        /*constraints=*/rewriter.getStringAttr(\"=r,l,r\"),\n+        /*pure=*/rewriter.getBoolAttr(false),\n+        /*packed_element=*/rewriter.getI32IntegerAttr(1),\n+        /*args=*/mlir::ValueRange{addr, value});\n+  }\n+  // No results to replace; just erase the op.\n+  rewriter.eraseOp(atomic_write);\n+  return success();\n+}\n+\n+class TritonXLALowerAtomicsPass\n+    : public impl::TritonXLALowerAtomicsPassBase<TritonXLALowerAtomicsPass> {\n+ public:\n+  using Base::Base;\n+\n+ private:\n+  void runOnOperation() override {\n+    RewritePatternSet patterns(&getContext());\n+    patterns.add(LowerAtomicWriteOp);\n+    if (failed(applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n+      return signalPassFailure();\n+    }\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<Pass> CreateTritonXLALowerAtomicsPass() {\n+  return std::make_unique<TritonXLALowerAtomicsPass>();\n+}\n+\n+}  // namespace mlir::triton::xla"
        }
    ],
    "stats": {
        "total": 262,
        "additions": 262,
        "deletions": 0
    }
}