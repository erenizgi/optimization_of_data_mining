{
    "author": "ZixuanJiang",
    "message": "Remove unused functions for HloSharding.\n\nPiperOrigin-RevId: 840420089",
    "sha": "2280356f7daab94fdf789c28986bb9d7876df65d",
    "files": [
        {
            "sha": "b6376ad4f88aca7c7422a8c40365fe95a9b407ac",
            "filename": "third_party/xla/xla/hlo/ir/hlo_sharding.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 52,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2280356f7daab94fdf789c28986bb9d7876df65d/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2280356f7daab94fdf789c28986bb9d7876df65d/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc?ref=2280356f7daab94fdf789c28986bb9d7876df65d",
            "patch": "@@ -148,15 +148,6 @@ HloSharding HloSharding::AssignDevice(int64_t device_id,\n   return HloSharding(device_id, metadata);\n }\n \n-HloSharding HloSharding::Tile1D(const Shape& input_shape, int64_t num_tiles,\n-                                absl::Span<const OpMetadata> metadata) {\n-  CHECK_EQ(1, input_shape.dimensions().size());\n-  CHECK_GT(num_tiles, 1);\n-  absl::Span<const int64_t> dimensions(&num_tiles, 1);\n-  return HloSharding(TileAssignment(dimensions, dimensions, {0}),\n-                     /*replicate_on_last_tile_dim=*/false, metadata);\n-}\n-\n HloSharding HloSharding::PartialTile(\n     const TileAssignment& tile_assignment_last_dim_replicate,\n     absl::Span<const OpMetadata> metadata) {\n@@ -532,29 +523,6 @@ bool HloSharding::UsesDevice(int64_t device) const {\n          TileAgnosticDeviceAssignment().UsesDevice(device);\n }\n \n-std::map<int64_t, int64_t> HloSharding::UsedDevices(int64_t* count) const {\n-  int64_t element_count = 1;\n-  std::map<int64_t, int64_t> device_map;\n-  if (IsTuple()) {\n-    for (auto& tuple_element_sharding : tuple_elements()) {\n-      auto unique_device = tuple_element_sharding.UniqueDevice();\n-      if (unique_device) {\n-        device_map[*unique_device] += 1;\n-      }\n-    }\n-    element_count = tuple_elements().size();\n-  } else {\n-    auto unique_device = UniqueDevice();\n-    if (unique_device) {\n-      device_map[*unique_device] += 1;\n-    }\n-  }\n-  if (count != nullptr) {\n-    *count = element_count;\n-  }\n-  return device_map;\n-}\n-\n std::vector<int64_t> HloSharding::TileIndexForDevice(int64_t device) const {\n   CHECK(!maximal_);\n   CHECK(!IsManual());\n@@ -571,26 +539,6 @@ std::vector<int64_t> HloSharding::TileIndexForDevice(int64_t device) const {\n   return ret_index;\n }\n \n-int64_t HloSharding::DeviceForTileIndex(absl::Span<const int64_t> index) const {\n-  CHECK(!replicated_);\n-  CHECK(!IsManual());\n-  CHECK(!IsUnknown());\n-  CHECK(!IsTuple());\n-  if (maximal_) {\n-    return *tile_assignment_.array().begin();\n-  }\n-  if (index.size() == TiledDataRank() &&\n-      index.size() < tile_assignment_.num_dimensions()) {\n-    std::vector<int64_t> first_subgroup_index(index.begin(), index.end());\n-    for (int64_t i = 0; i < tile_assignment_.num_dimensions() - index.size();\n-         ++i) {\n-      first_subgroup_index.push_back(0);\n-    }\n-    return tile_assignment_(first_subgroup_index);\n-  }\n-  return tile_assignment_(index);\n-}\n-\n std::vector<int64_t> HloSharding::TileOffsetForDevice(const Shape& shape,\n                                                       int64_t device) const {\n   CHECK(!IsTuple());"
        },
        {
            "sha": "d4a0515e9311468617e9c099746291b8fef00127",
            "filename": "third_party/xla/xla/hlo/ir/hlo_sharding.h",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2280356f7daab94fdf789c28986bb9d7876df65d/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2280356f7daab94fdf789c28986bb9d7876df65d/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h?ref=2280356f7daab94fdf789c28986bb9d7876df65d",
            "patch": "@@ -154,11 +154,6 @@ class HloSharding {\n         subgroup_types, metadata);\n   }\n \n-  // Creates a new sharding which splits a one-dimensional input shape into\n-  // `num_tiles` tiles.\n-  static HloSharding Tile1D(const Shape& input_shape, int64_t num_tiles,\n-                            absl::Span<const OpMetadata> metadata = {});\n-\n   // Creates a new sharding for a tuple type. The given ShapeTree must have\n   // elements for every leaf shape contained in the tuple.\n   static HloSharding Tuple(const ShapeTree<HloSharding>& sub_shardings);\n@@ -380,30 +375,10 @@ class HloSharding {\n   // Returns true if the sharding defines an operation on the given device.\n   bool UsesDevice(int64_t device) const;\n \n-  // Retrieves a histogram of the devices used by the sharding. The returned\n-  // map has the device number as key, and the occurrence count as value.\n-  // If a sharding does not have a device, it will not be included in the\n-  // histogram. The count argument, if not nullptr, will receive the total\n-  // number of elements this sharding is made of (one for array, N leaves for\n-  // tuples).\n-  std::map<int64_t, int64_t> UsedDevices(int64_t* count) const;\n-\n   // Returns the tile that should be executed on the given device.\n   // REQUIRES: !IsTuple()\n   std::vector<int64_t> TileIndexForDevice(int64_t device) const;\n \n-  // Returns the device that should execute the given tile.\n-  // It is an error to call this if is_replicated() is true.\n-  // When ReplicateOnLastTileDim() == true, if index.size() == data rank, it\n-  // returns the first device in that replicated subgroup; otherwise,\n-  // index.size() should be the same as tile_assignment()'s rank and specifies\n-  // the member of the replication subgroup.\n-  // REQUIRES: !IsTuple()\n-  // REQUIRES: !IsManual()\n-  // REQUIRES: !IsUnknown()\n-  // REQUIRES: !maximal_\n-  int64_t DeviceForTileIndex(absl::Span<const int64_t> index) const;\n-\n   // Given a device ID, returns the offset within the specified shape of the\n   // tile that should be executed on the given core. This returns the lower\n   // extent of the tile in the input space."
        },
        {
            "sha": "1644a384510159d210ef0375e7a85a6f7a523fb7",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 97,
            "changes": 97,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2280356f7daab94fdf789c28986bb9d7876df65d/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2280356f7daab94fdf789c28986bb9d7876df65d/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc?ref=2280356f7daab94fdf789c28986bb9d7876df65d",
            "patch": "@@ -712,22 +712,6 @@ bool MergeShardingIfCompatible(const HloSharding& to_merge,\n   return true;\n }\n \n-std::optional<int64_t> SelectDominantDevice(\n-    const std::map<int64_t, int64_t>& device_map, int64_t* top_count) {\n-  int64_t device = 0;\n-  int64_t count = 0;\n-  for (auto& it : device_map) {\n-    if (it.second > count) {\n-      count = it.second;\n-      device = it.first;\n-    }\n-  }\n-  if (top_count != nullptr) {\n-    *top_count = count;\n-  }\n-  return count > 0 ? std::optional<int64_t>(device) : std::optional<int64_t>();\n-}\n-\n HloSharding FindCommonSharding(absl::Span<const HloSharding> shardings,\n                                std::optional<HloSharding> default_sharding) {\n   CHECK(!shardings.empty());\n@@ -750,46 +734,6 @@ HloSharding FindCommonSharding(absl::Span<const HloSharding> shardings,\n   return default_sharding.has_value() ? default_sharding.value() : shardings[0];\n }\n \n-void AssignComputationDevice(HloComputation* computation, int64_t device) {\n-  VLOG(4) << \"Assigning device \" << device << \" to \" << computation->name()\n-          << \" computation\";\n-  for (HloInstruction* instruction : computation->instructions()) {\n-    if (!instruction->has_sharding()) {\n-      VLOG(4) << \"Assigning device \" << device << \" to \" << instruction->name();\n-      instruction->set_device_sharding(device);\n-    }\n-  }\n-}\n-\n-std::optional<int64_t> GetDominantDevice(\n-    absl::Span<HloComputation* const> computations, double dominant_factor) {\n-  int64_t instruction_count = 0;\n-  std::map<int64_t, int64_t> device_map;\n-  for (HloComputation* computation : computations) {\n-    for (HloInstruction* instruction : computation->instructions()) {\n-      int64_t count = 1;\n-      if (instruction->has_sharding()) {\n-        for (auto& it : instruction->sharding().UsedDevices(&count)) {\n-          // The UsedDevices() API returns a map<device, occurrence_count>.\n-          device_map[it.first] += it.second;\n-        }\n-      }\n-      instruction_count += count;\n-    }\n-  }\n-  int64_t count;\n-  std::optional<int64_t> device = SelectDominantDevice(device_map, &count);\n-  std::optional<int64_t> dominant_device;\n-  if (device) {\n-    double factor =\n-        static_cast<double>(count) / static_cast<double>(instruction_count);\n-    if (factor >= dominant_factor) {\n-      dominant_device = device;\n-    }\n-  }\n-  return dominant_device;\n-}\n-\n HloSharding MoveAndMergeShardingTiles(const HloSharding& sharding,\n                                       int64_t source_dim, int64_t target_dim) {\n   CHECK(sharding.IsTiled());\n@@ -1544,47 +1488,6 @@ std::optional<HloSharding> ScatterUpdateShardingFromOutputParallelDimensions(\n       scatter.scatter_updates()[0]->shape().dimensions().size());\n }\n \n-absl::StatusOr<std::pair<std::unique_ptr<HloInstruction>, HloOpcode>>\n-IdentityValueAndHloOpcodeForScatterReduceComputation(\n-    const HloScatterInstruction& scatter) {\n-  auto computation = scatter.to_apply();\n-  // We only handle computations with 2 parameters and only 1 calculation.\n-  if (computation->instruction_count() != 3) {\n-    return absl::Status(\n-        absl::StatusCode::kInvalidArgument,\n-        \"Expected scatter reduce computation with 2 parameters and only 1 \"\n-        \"calculation\");\n-  }\n-\n-  auto root_instruction = computation->root_instruction();\n-  if (root_instruction->opcode() == HloOpcode::kAdd ||\n-      root_instruction->opcode() == HloOpcode::kOr) {\n-    return std::make_pair(HloInstruction::CreateConstant(LiteralUtil::Zero(\n-                              scatter.shape().element_type())),\n-                          root_instruction->opcode());\n-  }\n-  if (root_instruction->opcode() == HloOpcode::kMultiply ||\n-      root_instruction->opcode() == HloOpcode::kAnd) {\n-    return std::make_pair(HloInstruction::CreateConstant(\n-                              LiteralUtil::One(scatter.shape().element_type())),\n-                          root_instruction->opcode());\n-  }\n-  if (root_instruction->opcode() == HloOpcode::kMaximum) {\n-    return std::make_pair(HloInstruction::CreateConstant(LiteralUtil::MinValue(\n-                              scatter.shape().element_type())),\n-                          root_instruction->opcode());\n-  }\n-  if (root_instruction->opcode() == HloOpcode::kMinimum) {\n-    return std::make_pair(HloInstruction::CreateConstant(LiteralUtil::MaxValue(\n-                              scatter.shape().element_type())),\n-                          root_instruction->opcode());\n-  }\n-\n-  return absl::Status(absl::StatusCode::kInvalidArgument,\n-                      \"Expected scatter reduce computation which is \"\n-                      \"add/or/multiply/add/min/max\");\n-}\n-\n HloSharding PartiallyReplicateTiledShardingOnDims(\n     const HloSharding& sharding, absl::Span<const int64_t> dims_to_replicate) {\n   if (sharding.IsTileMaximal() || sharding.IsManual()) {"
        },
        {
            "sha": "c5164f0be6e26f118a7e579f7e2ef2c873e7dfad",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util.h",
            "status": "modified",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2280356f7daab94fdf789c28986bb9d7876df65d/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2280356f7daab94fdf789c28986bb9d7876df65d/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.h?ref=2280356f7daab94fdf789c28986bb9d7876df65d",
            "patch": "@@ -119,27 +119,6 @@ HloSharding FindCommonSharding(\n     absl::Span<const HloSharding> shardings,\n     std::optional<HloSharding> default_sharding = std::nullopt);\n \n-// Given a map<device, occurrence_count>, selects the device with higher\n-// occurrence count (if any). If top_count in not nullptr, it will receive the\n-// count of the dominant device returned.\n-std::optional<int64_t> SelectDominantDevice(\n-    const std::map<int64_t, int64_t>& device_map, int64_t* top_count);\n-\n-// Assigns all the instructions of a computation, to a given device.\n-// This API does not recurse into called computations, and does not assign\n-// instructions which already have sharding.\n-void AssignComputationDevice(HloComputation* computation, int64_t device);\n-\n-// Given a set of computations, tries to extract the dominant device. A device\n-// is dominant if the combined occurrence among all the instructions of the\n-// input computations, is greater/equal than/to dominant_factor (real number\n-// from 0 to 1).\n-// This API does not recurse into called computations.\n-// If no device exists that satisfies the condition, the returned optional will\n-// hold no value.\n-std::optional<int64_t> GetDominantDevice(\n-    absl::Span<HloComputation* const> computations, double dominant_factor);\n-\n // Given a tiled sharding, move the tiles from source_dim and merge it into\n // target_dim. For example, given a sharding with tile assignment [a, b, c, d,\n // e], source_dim = 1, target_dim = 3, the function will return a sharding with\n@@ -256,17 +235,6 @@ std::optional<HloSharding> ScatterUpdateShardingFromOutputParallelDimensions(\n     const HloSharding& output_sharding, const HloScatterInstruction& scatter,\n     const CallGraph& call_graph);\n \n-// Returns an identity value and an HloOpcode for reduce computation of scatter\n-// instruction.\n-// - If computation is add/or, return 0/false with corresponding op code;\n-// - If computation is multiply/and, return 1/true with corresponding op code.\n-// - If computation is min/max, return max value/min value with corresponding op\n-//   code.\n-// - Otherwise, return error status.\n-absl::StatusOr<std::pair<std::unique_ptr<HloInstruction>, HloOpcode>>\n-IdentityValueAndHloOpcodeForScatterReduceComputation(\n-    const HloScatterInstruction& scatter);\n-\n // Returns a sharding that replicates data across devices along the given\n // dimensions in the original sharding.\n HloSharding PartiallyReplicateTiledShardingOnDims("
        },
        {
            "sha": "f0cbb03fadfaaf7cfef768fe4b3af4c89c592c2b",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2280356f7daab94fdf789c28986bb9d7876df65d/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2280356f7daab94fdf789c28986bb9d7876df65d/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=2280356f7daab94fdf789c28986bb9d7876df65d",
            "patch": "@@ -6178,7 +6178,6 @@ xla_cc_test(\n     srcs = [\"sharding_config_test.cc\"],\n     deps = [\n         \":sharding_config\",\n-        \"//xla:shape_util\",\n         \"//xla/hlo/ir:hlo_sharding\",\n         \"@com_google_googletest//:gtest_main\",\n     ],"
        },
        {
            "sha": "ee87360d9c2c2ce25f4642efddcd81a95b3338f3",
            "filename": "third_party/xla/xla/service/hlo_sharding_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2280356f7daab94fdf789c28986bb9d7876df65d/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_sharding_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2280356f7daab94fdf789c28986bb9d7876df65d/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_sharding_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_sharding_test.cc?ref=2280356f7daab94fdf789c28986bb9d7876df65d",
            "patch": "@@ -182,11 +182,6 @@ TEST_F(HloShardingTest, Tile) {\n     EXPECT_IS_OK(sharding.Validate(ShapeUtil::MakeShape(F32, {3, 5}),\n                                    /*num_devices=*/4));\n \n-    EXPECT_EQ(0, sharding.DeviceForTileIndex({0, 0}));\n-    EXPECT_EQ(3, sharding.DeviceForTileIndex({0, 1}));\n-    EXPECT_EQ(2, sharding.DeviceForTileIndex({1, 0}));\n-    EXPECT_EQ(1, sharding.DeviceForTileIndex({1, 1}));\n-\n     EXPECT_EQ(sharding.TileOffsetForDevice(shape, 0),\n               (std::vector<int64_t>{0, 0}));\n     EXPECT_EQ(sharding.TileOffsetForDevice(shape, 3),"
        },
        {
            "sha": "a12e1e88afd6048db72e49dd43f65be9014691d2",
            "filename": "third_party/xla/xla/service/sharding_config_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2280356f7daab94fdf789c28986bb9d7876df65d/third_party%2Fxla%2Fxla%2Fservice%2Fsharding_config_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2280356f7daab94fdf789c28986bb9d7876df65d/third_party%2Fxla%2Fxla%2Fservice%2Fsharding_config_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fsharding_config_test.cc?ref=2280356f7daab94fdf789c28986bb9d7876df65d",
            "patch": "@@ -19,20 +19,16 @@ limitations under the License.\n \n #include <gtest/gtest.h>\n #include \"xla/hlo/ir/hlo_sharding.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/shape_util.h\"\n \n namespace xla {\n namespace {\n \n class ShardingConfigTest : public ::testing::Test {\n  protected:\n-  const Shape kShape = ShapeUtil::MakeShape(F32, {1024});\n   const ShardingConfig kTestConfig{\n       {{HloSharding::Manual()},\n        {HloSharding::Replicate()},\n-       {{},\n-        {{HloSharding::Tile1D(kShape, 2)}, {HloSharding::Tile1D(kShape, 4)}}}}};\n+       {{}, {{HloSharding::IotaTile({2})}, {HloSharding::IotaTile({4})}}}}};\n };\n \n TEST_F(ShardingConfigTest, ConfigToProtoToConfigMatchesOriginal) {"
        },
        {
            "sha": "df9bc03cc21c6154ddb89c5c691772ab7cc03979",
            "filename": "third_party/xla/xla/service/while_loop_invariant_code_motion_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2280356f7daab94fdf789c28986bb9d7876df65d/third_party%2Fxla%2Fxla%2Fservice%2Fwhile_loop_invariant_code_motion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2280356f7daab94fdf789c28986bb9d7876df65d/third_party%2Fxla%2Fxla%2Fservice%2Fwhile_loop_invariant_code_motion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fwhile_loop_invariant_code_motion_test.cc?ref=2280356f7daab94fdf789c28986bb9d7876df65d",
            "patch": "@@ -649,7 +649,7 @@ TEST_F(WhileLoopInvariantCodeMotionTest, DoesNotHoistSPMDFullToShardShape) {\n         HloInstruction::CreateGetTupleElement(array_s32, param, 1));\n     HloInstruction* sharded_gte_1 = builder.AddInstruction(\n         HloInstruction::CreateCustomCall(array_s32, {gte_1}, \"Sharding\"));\n-    sharded_gte_1->set_sharding(HloSharding::Tile1D(array_s32, 4));\n+    sharded_gte_1->set_sharding(HloSharding::IotaTile({4}));\n     HloInstruction* manually_sharded_gte_1 =\n         builder.AddInstruction(HloInstruction::CreateCustomCall(\n             array_s32, {sharded_gte_1}, \"SPMDFullToShardShape\"));\n@@ -664,7 +664,7 @@ TEST_F(WhileLoopInvariantCodeMotionTest, DoesNotHoistSPMDFullToShardShape) {\n     HloInstruction* sharded_add_result =\n         builder.AddInstruction(HloInstruction::CreateCustomCall(\n             array_s32, {manually_sharded_add_result}, \"SPMDShardShapeToFull\"));\n-    sharded_add_result->set_sharding(HloSharding::Tile1D(array_s32, 4));\n+    sharded_add_result->set_sharding(HloSharding::IotaTile({4}));\n     builder.AddInstruction(\n         HloInstruction::CreateTuple({gte_0, gte_1, sharded_add_result}));\n "
        }
    ],
    "stats": {
        "total": 222,
        "additions": 3,
        "deletions": 219
    }
}