{
    "author": "ezhulenev",
    "message": "[xla:gpu] Remove mentions of NCCL from CollectiveThunk\n\nGive a more descriptive name to `use_nccl` argument.\n\nPiperOrigin-RevId: 838782393",
    "sha": "7614a37b30ef0b6864f7405d748a0a2a537ede40",
    "files": [
        {
            "sha": "508cc159d8041631a9ee32776a85b9c26483a22b",
            "filename": "third_party/xla/xla/backends/gpu/collectives/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD?ref=7614a37b30ef0b6864f7405d748a0a2a537ede40",
            "patch": "@@ -111,7 +111,7 @@ cc_library(\n     deps = [\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/core/collectives:clique_key\",\n-        \"//xla/service:global_device_id\",\n+        \"//xla/runtime:device_id\",\n         \"//xla/tsl/lib/gtl:int_type\",\n         \"//xla/tsl/platform:logging\",\n         \"@com_google_absl//absl/algorithm:container\","
        },
        {
            "sha": "d588bff13c6f8ee2a781af337c911a0678526043",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_clique_key.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_key.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_key.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_key.cc?ref=7614a37b30ef0b6864f7405d748a0a2a537ede40",
            "patch": "@@ -29,15 +29,21 @@ limitations under the License.\n #include \"absl/strings/str_join.h\"\n #include \"absl/types/span.h\"\n #include \"xla/core/collectives/clique_key.h\"\n-#include \"xla/service/global_device_id.h\"\n+#include \"xla/runtime/device_id.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/casts.h\"\n \n namespace xla::gpu {\n \n bool IsP2PStreamKind(AsyncStreamKind stream_kind) {\n-  return stream_kind != AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE;\n+  switch (stream_kind) {\n+    case AsyncStreamKind::ASYNC_STREAM_KIND_P2P0:\n+    case AsyncStreamKind::ASYNC_STREAM_KIND_P2P1:\n+      return true;\n+    default:\n+      return false;\n+  }\n }\n \n CollectiveStreamId GetCollectiveStreamId(bool is_async,\n@@ -118,14 +124,14 @@ std::string GpuCliqueKey::ToString() const {\n     std::vector<std::string> values;\n     values.reserve(participant_groups_.size());\n     for (const auto& group : participant_groups_) {\n-      values.push_back(\"[\" + GlobalDeviceIdsToString(group) + \"]\");\n+      values.push_back(absl::StrFormat(\"[%s]\", absl::StrJoin(group, \",\")));\n     }\n     group_string = absl::StrFormat(\"; groups=[%s]\", absl::StrJoin(values, \",\"));\n   }\n   return absl::StrFormat(\n       \"devices=[%s]; is_p2p=%d%s; root_device=%lld; \"\n       \"num_local_participants=%lld; incarnations=[%s]\",\n-      GlobalDeviceIdsToString(devices()), is_p2p_, group_string,\n+      absl::StrJoin(devices(), \",\"), is_p2p_, group_string,\n       root_device_.value(), num_local_participants_,\n       absl::StrJoin(incarnations_, \", \",\n                     [](std::string* out, IncarnationId id) {"
        },
        {
            "sha": "a687ede7bcda2c2a1eb5dce9e320b08d11cf8cd4",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_clique_key.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_key.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_key.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_key.h?ref=7614a37b30ef0b6864f7405d748a0a2a537ede40",
            "patch": "@@ -23,7 +23,7 @@ limitations under the License.\n #include \"absl/hash/hash.h\"\n #include \"absl/types/span.h\"\n #include \"xla/core/collectives/clique_key.h\"\n-#include \"xla/service/global_device_id.h\"\n+#include \"xla/runtime/device_id.h\"\n #include \"xla/tsl/lib/gtl/int_type.h\"\n #include \"xla/xla_data.pb.h\"\n "
        },
        {
            "sha": "b94c5f9668605b5925ddcc605011c92a47da7118",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=7614a37b30ef0b6864f7405d748a0a2a537ede40",
            "patch": "@@ -2786,6 +2786,7 @@ cc_library(\n         \":collective_thunk\",\n         \":thunk\",\n         \"//xla:shape_util\",\n+        \"//xla:status_macros\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n         \"//xla/backends/gpu/collectives:gpu_collectives\","
        },
        {
            "sha": "c55faf55a3b6dd449844fa7f40a9e52ea1a0db06",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc?ref=7614a37b30ef0b6864f7405d748a0a2a537ede40",
            "patch": "@@ -125,7 +125,7 @@ absl::Status CollectiveKernelThunk::Prepare(const PrepareParams& params) {\n   TF_ASSIGN_OR_RETURN(\n       GpuCliqueKey clique_key,\n       GetCollectiveGpuCliqueKey(*params.collective_params, collective_config_,\n-                                /*use_nccl=*/false));\n+                                /*include_participant_groups=*/false));\n   return params.clique_requests->RequestClique(clique_key);\n }\n \n@@ -163,7 +163,7 @@ absl::Status CollectiveKernelThunk::Initialize(const InitializeParams& params) {\n   TF_ASSIGN_OR_RETURN(\n       const GpuCliqueKey clique_key,\n       GetCollectiveGpuCliqueKey(*params.collective_params, collective_config_,\n-                                /*use_nccl=*/false));\n+                                /*include_participant_groups=*/false));\n   const std::optional<RankId> rank =\n       clique_key.rank(params.collective_params->global_device_id);\n   TF_RET_CHECK(rank.has_value())\n@@ -273,7 +273,7 @@ absl::Status CollectiveKernelThunk::ExecuteOnStream(\n   TF_ASSIGN_OR_RETURN(\n       const GpuCliqueKey clique_key,\n       GetCollectiveGpuCliqueKey(*params.collective_params, collective_config_,\n-                                /*use_nccl=*/false));\n+                                /*include_participant_groups=*/false));\n   const int32_t num_devices = clique_key.num_devices();\n \n   // TODO(b/407736956): Support variadic all-reduce."
        },
        {
            "sha": "c3f1260385b71a662ec2f9239ad4212ea856df94",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_metadata_thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc?ref=7614a37b30ef0b6864f7405d748a0a2a537ede40",
            "patch": "@@ -183,7 +183,7 @@ absl::Status CollectiveMetadataThunk::Initialize(\n   TF_ASSIGN_OR_RETURN(\n       const GpuCliqueKey clique_key,\n       GetCollectiveGpuCliqueKey(*params.collective_params, collective_config_,\n-                                /*use_nccl=*/false));\n+                                /*include_participant_groups=*/false));\n   const int64_t num_ranks = clique_key.num_devices();\n   TF_RET_CHECK(result_.size() ==\n                sizeof(CollectiveKernelMetadata) +"
        },
        {
            "sha": "029daa31a2da219883ddbcf51d22ae1e8b0673ed",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc?ref=7614a37b30ef0b6864f7405d748a0a2a537ede40",
            "patch": "@@ -201,7 +201,7 @@ absl::StatusOr<GpuCliqueKey> GetGpuCliqueKey(\n     const CollectiveParams& params,\n     absl::Span<const ReplicaGroup> replica_groups,\n     CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind,\n-    bool use_nccl) {\n+    bool include_participant_groups) {\n   TF_RET_CHECK(params.collectives) << \"Collectives API is not provided\";\n \n   GlobalDeviceId global_device_id = params.global_device_id;\n@@ -222,7 +222,7 @@ absl::StatusOr<GpuCliqueKey> GetGpuCliqueKey(\n \n   // Get grouping of participating devices.\n   std::vector<std::vector<GlobalDeviceId>> participant_groups;\n-  if (use_nccl) {\n+  if (include_participant_groups) {\n     // If splitting is enabled, participating groups must match in order for a\n     // clique to be reused from the cache. We can ignore the participating\n     // groups otherwise.\n@@ -274,15 +274,16 @@ absl::StatusOr<GpuCliqueKey> GetGpuCliqueKey(\n \n absl::StatusOr<GpuCliqueKey> GetCollectiveGpuCliqueKey(\n     const CollectiveParams& params, const CollectiveConfig& collective_config,\n-    bool use_nccl) {\n-  return GetGpuCliqueKey(\n-      params, collective_config.replica_groups, collective_config.group_mode,\n-      AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE, use_nccl);\n+    bool include_participant_groups) {\n+  return GetGpuCliqueKey(params, collective_config.replica_groups,\n+                         collective_config.group_mode,\n+                         AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE,\n+                         include_participant_groups);\n }\n \n absl::StatusOr<CommunicatorHandle> GetComm(\n     const CollectiveParams& params, const CollectiveCliques& collective_cliques,\n-    const std::vector<ReplicaGroup>& replica_groups,\n+    absl::Span<const ReplicaGroup> replica_groups,\n     CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind) {\n   TF_ASSIGN_OR_RETURN(\n       GpuCliqueKey clique_key,"
        },
        {
            "sha": "166cb9d4e4a999305b0f56723c4bd53f50f96ad1",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h?ref=7614a37b30ef0b6864f7405d748a0a2a537ede40",
            "patch": "@@ -277,17 +277,17 @@ absl::StatusOr<GpuCliqueKey> GetGpuCliqueKey(\n     const CollectiveParams& params,\n     absl::Span<const ReplicaGroup> replica_groups,\n     CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind,\n-    bool use_nccl = true);\n+    bool include_participant_groups = true);\n \n // Helper over GetGpuCliqueKey that builds key for AsyncStreamKind::kCollective.\n absl::StatusOr<GpuCliqueKey> GetCollectiveGpuCliqueKey(\n     const CollectiveParams& params, const CollectiveConfig& collective_config,\n-    bool use_nccl = true);\n+    bool include_participant_groups = true);\n \n // Returns a communicator and additional information about the clique.\n absl::StatusOr<CommunicatorHandle> GetComm(\n     const CollectiveParams& params, const CollectiveCliques& collective_cliques,\n-    const std::vector<ReplicaGroup>& replica_groups,\n+    absl::Span<const ReplicaGroup> replica_groups,\n     CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind);\n \n struct DeviceBufferPair {"
        },
        {
            "sha": "e8759685bfd95777a61339beb4d09d1b900dbb20",
            "filename": "third_party/xla/xla/backends/gpu/runtime/nvshmem_collective_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_thunk.cc?ref=7614a37b30ef0b6864f7405d748a0a2a537ede40",
            "patch": "@@ -33,6 +33,7 @@ limitations under the License.\n #include \"xla/core/collectives/collectives_registry.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/shape.h\"\n+#include \"xla/status_macros.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -45,7 +46,6 @@ namespace xla {\n namespace gpu {\n \n namespace {\n-static constexpr CollectiveStreamId kNoStreamId = CollectiveStreamId(0);\n \n bool IsTypeSupportedByNvshmem(PrimitiveType element_type,\n                               Thunk::Kind reduction_op) {\n@@ -95,7 +95,7 @@ absl::Status NvshmemCollectiveThunk::Prepare(const PrepareParams& params) {\n       GpuCliqueKey clique_key,\n       GetGpuCliqueKey(*params.collective_params, config().replica_groups,\n                       config().group_mode, GetAsyncStreamKind(),\n-                      /*use_nccl= */ false));\n+                      /*include_participant_groups=*/false));\n   return params.clique_requests->RequestClique(clique_key);\n }\n "
        },
        {
            "sha": "1bb5d6cc0155e3a6c1a870f453e0dad949b14ff4",
            "filename": "third_party/xla/xla/runtime/device_id.h",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fruntime%2Fdevice_id.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7614a37b30ef0b6864f7405d748a0a2a537ede40/third_party%2Fxla%2Fxla%2Fruntime%2Fdevice_id.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fruntime%2Fdevice_id.h?ref=7614a37b30ef0b6864f7405d748a0a2a537ede40",
            "patch": "@@ -32,6 +32,16 @@ TSL_LIB_GTL_DEFINE_INT_TYPE(LocalDeviceId, int64_t);\n \n using ::tsl::IncarnationId;  // NOLINT(misc-unused-using-decls)\n \n+template <typename Sink>\n+void AbslStringify(Sink& sink, GlobalDeviceId id) {\n+  absl::Format(&sink, \"%d\", id.value());\n+}\n+\n+template <typename Sink>\n+void AbslStringify(Sink& sink, LocalDeviceId id) {\n+  absl::Format(&sink, \"%d\", id.value());\n+}\n+\n }  // namespace xla\n \n #endif  // XLA_RUNTIME_DEVICE_ID_H_"
        }
    ],
    "stats": {
        "total": 62,
        "additions": 40,
        "deletions": 22
    }
}