{
    "author": "Moerafaat",
    "message": "[XLA:GPU/TMA] Centralize TMA enablement control in the autotuner. This one was missed in previous consolidations.\n\nPiperOrigin-RevId: 837391515",
    "sha": "77ba53c883f8bc59bd6c3da20e535a4de7c81a5c",
    "files": [
        {
            "sha": "ca65b91a6a30c4cf4f32e5036497a7ec42d81ca8",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/77ba53c883f8bc59bd6c3da20e535a4de7c81a5c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/77ba53c883f8bc59bd6c3da20e535a4de7c81a5c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=77ba53c883f8bc59bd6c3da20e535a4de7c81a5c",
            "patch": "@@ -66,6 +66,7 @@ cc_library(\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/platform:env\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\","
        },
        {
            "sha": "aea76b846fc2ca88392d5ec6e905c8556788336d",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_cuda.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/77ba53c883f8bc59bd6c3da20e535a4de7c81a5c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/77ba53c883f8bc59bd6c3da20e535a4de7c81a5c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc?ref=77ba53c883f8bc59bd6c3da20e535a4de7c81a5c",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"xla/service/gpu/transforms/block_scaling_rewriter.h\"\n #include \"xla/service/gpu/transforms/cudnn_fusion_compiler.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/gpu/tma_metadata.h\"\n \n namespace xla {\n namespace gpu {\n@@ -107,9 +108,9 @@ std::vector<TritonGemmConfig> GemmFusionAutotunerImpl::GetDefaultTritonConfigs()\n     configs = *kDefaultCudaConfigs;\n   }\n \n-  // Hopper+ devices support TMA. Add TMA parameterized configs.\n   if (!debug_options_.xla_gpu_experimental_enable_triton_tma() ||\n-      !compute_capability.IsAtLeastHopper()) {\n+      !stream_executor::gpu::IsTmaAvailableForDevice(\n+          config_.GetDeviceDescription())) {\n     return configs;\n   }\n   std::vector<TritonGemmConfig> tma_parameterized_configs;"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 4,
        "deletions": 2
    }
}