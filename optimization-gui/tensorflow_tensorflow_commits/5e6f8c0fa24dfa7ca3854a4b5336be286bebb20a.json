{
    "author": "dimvar",
    "message": "PR #34228: Refactor the heuristic for max unroll factor.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/34228\n\nA recent change introduced max unroll factor = 8 on Blackwell under certain conditions. But the function MaxUnrollFactor is still hardcoded to return 4, which can be confusing to the reader. This patch moves the heuristic into MaxUnrollFactor. There is no functional change.\nCopybara import of the project:\n\n--\n3bc07c5f88261dbc683b6c7488036039a0caba00 by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:\n\nRefactor the heuristic for max unroll factor.\n\nMerging this change closes #34228\n\nPiperOrigin-RevId: 836167603",
    "sha": "5e6f8c0fa24dfa7ca3854a4b5336be286bebb20a",
    "files": [
        {
            "sha": "c48680b2e11861a89b5361024533adfc534d7f0c",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e6f8c0fa24dfa7ca3854a4b5336be286bebb20a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e6f8c0fa24dfa7ca3854a4b5336be286bebb20a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=5e6f8c0fa24dfa7ca3854a4b5336be286bebb20a",
            "patch": "@@ -2921,6 +2921,7 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:side_effect_util\",\n         \"//xla:util\",\n+        \"//xla:xla_proto_cc\",\n         \"//xla/hlo/analysis:hlo_dataflow_analysis\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/ir:hlo_instruction_utils\","
        },
        {
            "sha": "2b18027754e5cde70f7af5d64c033ef459909102",
            "filename": "third_party/xla/xla/service/gpu/gpu_fusible.cc",
            "status": "modified",
            "additions": 71,
            "deletions": 65,
            "changes": 136,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e6f8c0fa24dfa7ca3854a4b5336be286bebb20a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e6f8c0fa24dfa7ca3854a4b5336be286bebb20a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc?ref=5e6f8c0fa24dfa7ca3854a4b5336be286bebb20a",
            "patch": "@@ -50,11 +50,25 @@ limitations under the License.\n #include \"xla/side_effect_util.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/util.h\"\n+#include \"xla/xla.pb.h\"\n \n namespace xla {\n namespace gpu {\n namespace {\n \n+bool ContainsTransposeWithSmallMostMinorDim(const HloFusionAdaptor& fusion,\n+                                            int64_t unroll_factor) {\n+  return HloAnyOf(fusion, [unroll_factor](HloInstructionAdaptor instr) {\n+    if (instr.opcode() != HloOpcode::kTranspose) {\n+      return false;\n+    }\n+    const HloInstruction& transpose = instr.instruction();\n+    // We can assume that TransposeDimensionGrouper pass has run, so no need\n+    // to try to combine adjacent dimensions.\n+    return transpose.shape().dimensions().back() < unroll_factor;\n+  });\n+}\n+\n bool HasAnyTiledTransposeRoot(const HloComputation& computation) {\n   return absl::c_any_of(GetFusionRoots(computation),\n                         [&](const HloInstruction* instr) {\n@@ -84,6 +98,61 @@ int ComputeMaxUnrollFactor(int64_t num_elements, int64_t max_unroll) {\n \n }  // namespace\n \n+int64_t MaxUnrollFactor(const HloFusionAnalysis* analysis) {\n+  if (analysis == nullptr) {\n+    return 4;\n+  }\n+\n+  // On Blackwell we would like to increase the maximum unroll factor to 8, as\n+  // we need more vectorization for full performance.\n+  // However we need to check additional conditions:\n+  //   - Unrolling is potentially bad for fusions with reductions, where one\n+  //     thread will handle the full reduction dimension, so more unrolling\n+  //     can hurt parallelism.\n+  //   - Unrolling is potentially bad for fusions with many outputs, as that\n+  //     might increase register pressure. A thread needs to compute all the\n+  //     outputs first before it can write them due to potential in-place\n+  //     buffers. More unrolling will increase the number of values that need\n+  //     to be computed before writing.\n+  //   - Unrolling is potentially bad for transposes if the most minor\n+  //     dimension of transpose is smaller than the unroll factor. This could\n+  //     potentially be checked with indexing analysis as well, but it is\n+  //     tricky to get the conditions right when bad or unknown indexing\n+  //     should block more unrolling or not. For now, let's keep it simple and\n+  //     only check for transpose.\n+\n+  // For now, don't allow any multi-output fusions. However register pressure\n+  // also does not only depend on the number of outputs, so we might hit it\n+  // also for single fusions, or there could be multi-output fusions that\n+  // don't face register pressure. This part of the heuristic may need\n+  // improvements.\n+  constexpr int kMaxNumOutputsForFullUnrolling = 1;\n+  // On PTX level, we can vectorize with v4.b32, but not with v8.b32. So\n+  // higher unroll factor does not make sense with 32 bit or more.\n+  constexpr int kMaxBitsToVectorizeWithVectorSize4 = 32;\n+  DebugOptions debug_options = analysis->fusion_root(0)\n+                                   .instruction()\n+                                   .GetModule()\n+                                   ->config()\n+                                   .debug_options();\n+  if (analysis->device_info().cuda_compute_capability().IsBlackwell() &&\n+      analysis->emitter_fusion_kind() ==\n+          HloFusionAnalysis::EmitterFusionKind::kLoop &&\n+      analysis->input_output_info().smallest_output_dtype_bits <\n+          kMaxBitsToVectorizeWithVectorSize4 &&\n+      analysis->fusion_root_count() <= kMaxNumOutputsForFullUnrolling &&\n+      debug_options.xla_gpu_experimental_allow_unroll_factor_eight() &&\n+      !HloAnyOf(\n+          analysis->fusion(),\n+          [](HloInstructionAdaptor node) {\n+            return node.opcode() == HloOpcode::kReduce;\n+          }) &&\n+      !ContainsTransposeWithSmallMostMinorDim(analysis->fusion(), 8)) {\n+    return 8;\n+  }\n+  return 4;\n+}\n+\n bool IsPhysicallyTransposing(const HloInstruction& instr) {\n   if (instr.opcode() == HloOpcode::kFusion) {\n     for (const HloInstruction* fused_instr : instr.fused_instructions()) {\n@@ -930,21 +999,6 @@ LaunchDimensionsConfig ComputeLoopFusionConfig(\n   return ComputeLoopFusionConfig(analysis, GetElementShape(analysis));\n }\n \n-namespace {\n-bool ContainsTransposeWithSmallMostMinorDim(const HloFusionAdaptor& fusion,\n-                                            int64_t unroll_factor) {\n-  return HloAnyOf(fusion, [unroll_factor](HloInstructionAdaptor instr) {\n-    if (instr.opcode() != HloOpcode::kTranspose) {\n-      return false;\n-    }\n-    const HloInstruction& transpose = instr.instruction();\n-    // We can assume that TransposeDimensionGrouper pass has run, so no need\n-    // to try to combine adjacent dimensions.\n-    return transpose.shape().dimensions().back() < unroll_factor;\n-  });\n-}\n-}  // namespace\n-\n LaunchDimensionsConfig ComputeLoopFusionConfig(\n     const HloFusionAnalysis& analysis, const Shape& element_shape) {\n   int unroll_factor = 1;\n@@ -959,56 +1013,8 @@ LaunchDimensionsConfig ComputeLoopFusionConfig(\n                           analysis.device_info().core_count();\n   if (num_elements >= n_threads_max &&\n       !MayCausePerformanceDropIfUnrolled(analysis.fusion())) {\n-    int64_t max_unroll = MaxUnrollFactor();\n-    // On Blackwell we would like to increase the maximum unroll factor to 8, as\n-    // we need more vectorization for full performance.\n-    // However we need to check additional conditions:\n-    //   - Unrolling is potentially bad for fusions with reductions, where one\n-    //     thread will handle the full reduction dimension, so more unrolling\n-    //     can hurt parallelism.\n-    //   - Unrolling is potentially bad for fusions with many outputs, as that\n-    //     might increase register pressure. A thread needs to compute all the\n-    //     outputs first before it can write them due to potential in-place\n-    //     buffers. More unrolling will increase the number of values that need\n-    //     to be computed before writing.\n-    //   - Unrolling is potentially bad for transposes if the most minor\n-    //     dimension of transpose is smaller than the unroll factor. This could\n-    //     potentially be checked with indexing analysis as well, but it is\n-    //     tricky to get the conditions right when bad or unknown indexing\n-    //     should block more unrolling or not. For now, let's keep it simple and\n-    //     only check for transpose.\n-\n-    // For now, don't allow any multi-output fusions. However register pressure\n-    // also does not only depend on the number of outputs, so we might hit it\n-    // also for single fusions, or there could be multi-output fusions that\n-    // don't face register pressure. This part of the heuristic may need\n-    // improvements.\n-    constexpr int kMaxNumOutputsForFullUnrolling = 1;\n-    // On PTX level, we can vectorize with v4.b32, but not with v8.b32. So\n-    // higher unroll factor does not make sense with 32 bit or more.\n-    constexpr int kMaxBitsToVectorizeWithVectorSize4 = 32;\n-    if (analysis.device_info().cuda_compute_capability().IsBlackwell() &&\n-        analysis.emitter_fusion_kind() ==\n-            HloFusionAnalysis::EmitterFusionKind::kLoop &&\n-        analysis.input_output_info().smallest_output_dtype_bits <\n-            kMaxBitsToVectorizeWithVectorSize4 &&\n-        analysis.fusion_root_count() <= kMaxNumOutputsForFullUnrolling &&\n-        analysis.fusion_root(0)\n-            .instruction()\n-            .GetModule()\n-            ->config()\n-            .debug_options()\n-            .xla_gpu_experimental_allow_unroll_factor_eight() &&\n-        !HloAnyOf(\n-            analysis.fusion(),\n-            [](HloInstructionAdaptor node) {\n-              return node.opcode() == HloOpcode::kReduce;\n-            }) &&\n-        !ContainsTransposeWithSmallMostMinorDim(analysis.fusion(),\n-                                                max_unroll * 2)) {\n-      max_unroll *= 2;\n-    }\n-    unroll_factor = ComputeMaxUnrollFactor(num_elements, max_unroll);\n+    unroll_factor =\n+        ComputeMaxUnrollFactor(num_elements, MaxUnrollFactor(&analysis));\n   }\n   // CHECK that unroll_factor is a power-of-2, as needed by the logic below.\n   CHECK(absl::has_single_bit(static_cast<uint64_t>(unroll_factor)));"
        },
        {
            "sha": "448ef957e6dc5a60bf132a6703ad64d50210f70b",
            "filename": "third_party/xla/xla/service/gpu/gpu_fusible.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5e6f8c0fa24dfa7ca3854a4b5336be286bebb20a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5e6f8c0fa24dfa7ca3854a4b5336be286bebb20a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.h?ref=5e6f8c0fa24dfa7ca3854a4b5336be286bebb20a",
            "patch": "@@ -234,7 +234,7 @@ bool IsGenericTritonFusion(const HloInstruction& instr);\n bool MayCausePerformanceDropIfUnrolled(const HloFusionAdaptor& fusion);\n \n // Returns the max loop unroll factor.\n-inline constexpr int64_t MaxUnrollFactor() { return 4; }\n+int64_t MaxUnrollFactor(const HloFusionAnalysis* analysis = nullptr);\n \n LaunchDimensionsConfig ComputeLoopFusionConfig(\n     const HloFusionAnalysis& analysis);"
        }
    ],
    "stats": {
        "total": 139,
        "additions": 73,
        "deletions": 66
    }
}