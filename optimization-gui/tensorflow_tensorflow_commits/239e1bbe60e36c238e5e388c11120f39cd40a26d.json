{
    "author": "ezhulenev",
    "message": "[xla:gpu] Move MulticastMemory to se::gpu namespace\n\nAlso fix couple of clang warnings in cuda_executor.\n\nPiperOrigin-RevId: 840291167",
    "sha": "239e1bbe60e36c238e5e388c11120f39cd40a26d",
    "files": [
        {
            "sha": "9fd9b82cd107e38fad47dbd9adf67d17415fef5a",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=239e1bbe60e36c238e5e388c11120f39cd40a26d",
            "patch": "@@ -1996,6 +1996,7 @@ cc_library(\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/gpu:collective_kernel_metadata\",\n         \"//xla/stream_executor/gpu:gpu_executor_header\",\n+        \"//xla/stream_executor/gpu:multicast_memory\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -2578,6 +2579,7 @@ xla_test(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/core/collectives:rank_id\",\n+        \"//xla/core/collectives:reduction_kind\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/service:collective_ops_utils\",\n@@ -2595,6 +2597,7 @@ xla_test(\n         \"//xla/stream_executor/gpu:collective_kernel_metadata\",\n         \"//xla/stream_executor/gpu:gpu_executor_header\",\n         \"//xla/stream_executor/gpu:gpu_init\",\n+        \"//xla/stream_executor/gpu:multicast_memory\",\n         \"//xla/stream_executor/host:host_platform\",\n         \"//xla/tests:literal_test_util\",\n         \"//xla/tsl/platform:errors\","
        },
        {
            "sha": "888d2d89cbedac86db2be228eaad5f4cd26c9f98",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc?ref=239e1bbe60e36c238e5e388c11120f39cd40a26d",
            "patch": "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/array.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/core/collectives/reduction_kind.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/literal.h\"\n@@ -48,6 +49,7 @@ limitations under the License.\n #include \"xla/stream_executor/gpu/collective_kernel_metadata.h\"\n #include \"xla/stream_executor/gpu/gpu_executor.h\"\n #include \"xla/stream_executor/gpu/gpu_init.h\"\n+#include \"xla/stream_executor/gpu/multicast_memory.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -95,12 +97,11 @@ class AllReduceKernelTest : public ::testing::Test,\n     TF_RETURN_IF_ERROR(executors[0]->EnablePeerAccessTo(executors[1]));\n     TF_RETURN_IF_ERROR(executors[1]->EnablePeerAccessTo(executors[0]));\n \n-    std::unique_ptr<stream_executor::gpu::GpuExecutor::MulticastMemory>\n-        multicast_memory;\n+    std::unique_ptr<se::gpu::MulticastMemory> multicast_memory;\n     if (params_.all_reduce_strategy == AllReduceStrategy::kMultimem) {\n       TF_ASSIGN_OR_RETURN(\n           multicast_memory,\n-          dynamic_cast<stream_executor::gpu::GpuExecutor*>(executors[0])\n+          dynamic_cast<se::gpu::GpuExecutor*>(executors[0])\n               ->CreateMulticastMemory(num_elements * sizeof(T), num_ranks));\n \n       for (int i = 0; i < num_ranks; ++i) {\n@@ -130,7 +131,7 @@ class AllReduceKernelTest : public ::testing::Test,\n           /*data_buffer_size=*/aligned_input_size +\n           /*signal_buffer_size=*/aligned_signal_size;\n       allocated_buffers.emplace_back(executor->AllocateArray<T>(\n-          total_size, static_cast<int64_t>(stream_executor::MemoryType::kP2P)));\n+          total_size, static_cast<int64_t>(se::MemoryType::kP2P)));\n       local_input_buffers.emplace_back(\n           allocated_buffers[i].GetByteSlice(0, aligned_input_size));\n       TF_RET_CHECK(!local_input_buffers[i].is_null());\n@@ -153,11 +154,10 @@ class AllReduceKernelTest : public ::testing::Test,\n     constexpr int kNumPeerParameters = 2;\n     size_t param_to_peers_size = sizeof(void*) * kNumPeerParameters * num_ranks;\n     std::vector<void*> param_to_peers_ptrs;\n-    for (const stream_executor::DeviceMemoryBase& local_input_buffer :\n-         local_input_buffers) {\n+    for (const se::DeviceMemoryBase& local_input_buffer : local_input_buffers) {\n       param_to_peers_ptrs.push_back(local_input_buffer.opaque());\n     }\n-    for (const stream_executor::DeviceMemoryBase& signal_flags_buffer :\n+    for (const se::DeviceMemoryBase& signal_flags_buffer :\n          signal_flags_buffers) {\n       param_to_peers_ptrs.push_back(signal_flags_buffer.opaque());\n     }\n@@ -167,8 +167,8 @@ class AllReduceKernelTest : public ::testing::Test,\n       metadata.rank = i;\n \n       if (params_.all_reduce_strategy == AllReduceStrategy::kMultimem) {\n-        stream_executor::gpu::GpuExecutor* gpu_executor =\n-            dynamic_cast<stream_executor::gpu::GpuExecutor*>(executors[i]);\n+        se::gpu::GpuExecutor* gpu_executor =\n+            dynamic_cast<se::gpu::GpuExecutor*>(executors[i]);\n         TF_RET_CHECK(gpu_executor != nullptr);\n         TF_ASSIGN_OR_RETURN(\n             void* mapped_memory,\n@@ -250,7 +250,7 @@ TEST_P(AllReduceKernelTest, KernelTestAddF32) {\n   std::vector<se::StreamExecutor*> executors = {GetGpuExecutor(0),\n                                                 GetGpuExecutor(1)};\n   if (strategy() == AllReduceStrategy::kMultimem &&\n-      !dynamic_cast<stream_executor::gpu::GpuExecutor*>(executors[0])\n+      !dynamic_cast<se::gpu::GpuExecutor*>(executors[0])\n            ->is_multicast_supported()) {\n     GTEST_SKIP() << \"Multimem not supported on this device.\";\n   }"
        },
        {
            "sha": "9004370672e6ccc97bddc2ea02d34390252ffa32",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_metadata_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.cc?ref=239e1bbe60e36c238e5e388c11120f39cd40a26d",
            "patch": "@@ -41,6 +41,7 @@ limitations under the License.\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/collective_kernel_metadata.h\"\n #include \"xla/stream_executor/gpu/gpu_executor.h\"\n+#include \"xla/stream_executor/gpu/multicast_memory.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -283,7 +284,7 @@ absl::StatusOr<void*> CollectiveMetadataThunk::MultimemAddressSpaceProvider::\n                               const se::StreamExecutor* stream_executor,\n                               se::DeviceMemoryBase mapped_memory) {\n   const auto* gpu_executor =\n-      dynamic_cast<const stream_executor::gpu::GpuExecutor*>(stream_executor);\n+      dynamic_cast<const se::gpu::GpuExecutor*>(stream_executor);\n   if (gpu_executor == nullptr) {\n     return absl::UnimplementedError(\"Multicast is not supported on device.\");\n   }\n@@ -294,8 +295,7 @@ absl::StatusOr<void*> CollectiveMetadataThunk::MultimemAddressSpaceProvider::\n \n   if (device_number == first_device) {\n     TF_ASSIGN_OR_RETURN(\n-        std::unique_ptr<stream_executor::gpu::GpuExecutor::MulticastMemory>\n-            multicast_memory,\n+        std::unique_ptr<se::gpu::MulticastMemory> multicast_memory,\n         gpu_executor->CreateMulticastMemory(\n             mapped_memory.size(), clique_key.num_local_participants()));\n     first_device_to_multicast_memory_.emplace(device_number,"
        },
        {
            "sha": "23445ad9b3e8f7c3be797afed8d4f85b4812b5e6",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_metadata_thunk.h",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_metadata_thunk.h?ref=239e1bbe60e36c238e5e388c11120f39cd40a26d",
            "patch": "@@ -30,7 +30,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/gpu/gpu_executor.h\"\n+#include \"xla/stream_executor/gpu/multicast_memory.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n \n@@ -54,9 +54,8 @@ class CollectiveMetadataThunk : public Thunk {\n         se::DeviceMemoryBase mapped_memory);\n \n    private:\n-    absl::flat_hash_map<\n-        int,\n-        std::unique_ptr<stream_executor::gpu::GpuExecutor::MulticastMemory>>\n+    absl::flat_hash_map<int,\n+                        std::unique_ptr<stream_executor::gpu::MulticastMemory>>\n         first_device_to_multicast_memory_;\n   };\n "
        },
        {
            "sha": "c63c37b0c3b6eaef729343d79e7e4754b3797a1d",
            "filename": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD?ref=239e1bbe60e36c238e5e388c11120f39cd40a26d",
            "patch": "@@ -1167,6 +1167,7 @@ cc_library(\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/gpu:context\",\n         \"//xla/stream_executor/gpu:gpu_executor_header\",\n+        \"//xla/stream_executor/gpu:multicast_memory\",\n         \"//xla/stream_executor/gpu:read_numa_node\",\n         \"//xla/stream_executor/gpu:scoped_activate_context\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n@@ -1264,6 +1265,7 @@ xla_test(\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/gpu:gpu_init\",\n+        \"//xla/stream_executor/gpu:multicast_memory\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\","
        },
        {
            "sha": "c0dc6ee3b691c8f3d9ab07f2c2885962849d385e",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.cc",
            "status": "modified",
            "additions": 50,
            "deletions": 50,
            "changes": 100,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc?ref=239e1bbe60e36c238e5e388c11120f39cd40a26d",
            "patch": "@@ -78,6 +78,7 @@ limitations under the License.\n #include \"xla/stream_executor/generic_memory_allocator.h\"\n #include \"xla/stream_executor/gpu/context.h\"\n #include \"xla/stream_executor/gpu/gpu_executor.h\"\n+#include \"xla/stream_executor/gpu/multicast_memory.h\"\n #include \"xla/stream_executor/gpu/read_numa_node.h\"\n #include \"xla/stream_executor/gpu/scoped_activate_context.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n@@ -583,20 +584,20 @@ absl::StatusOr<void*> HostAllocate(Context* context, int numa_node,\n           xla::XlaFormatDevice(context->device_ordinal()), size, numa_node));\n     }\n     return buffer;\n-  } else {\n-    ScopedActivateContext activation(context);\n-    void* buffer = nullptr;\n-    // \"Portable\" memory is visible to all CUDA contexts. Safe for our use\n-    // model.\n-    TF_RETURN_IF_ERROR(cuda::ToStatus(\n-        cuMemHostAlloc(&buffer, size, CU_MEMHOSTALLOC_PORTABLE)));\n-    if (!buffer && size > 0) {\n-      return absl::InternalError(absl::StrFormat(\n-          \"%sFailed to allocate pinned host memory of size %d\",\n-          xla::XlaFormatDevice(context->device_ordinal()), size));\n-    }\n-    return buffer;\n   }\n+\n+  ScopedActivateContext activation(context);\n+  void* buffer = nullptr;\n+  // \"Portable\" memory is visible to all CUDA contexts. Safe for our use\n+  // model.\n+  TF_RETURN_IF_ERROR(\n+      cuda::ToStatus(cuMemHostAlloc(&buffer, size, CU_MEMHOSTALLOC_PORTABLE)));\n+  if (!buffer && size > 0) {\n+    return absl::InternalError(\n+        absl::StrFormat(\"%sFailed to allocate pinned host memory of size %d\",\n+                        xla::XlaFormatDevice(context->device_ordinal()), size));\n+  }\n+  return buffer;\n }\n \n // Deallocates memory allocated via HostAllocate.\n@@ -651,7 +652,7 @@ absl::StatusOr<bool> IsRdmaSupported(CUdevice device) {\n \n absl::StatusOr<bool> IsMulticastSupported(CUdevice device) {\n   int is_multicast_supported = 0;\n-  TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(\n+  TF_RETURN_IF_ERROR(cuda::ToStatus(\n       cuDeviceGetAttribute(&is_multicast_supported,\n                            CU_DEVICE_ATTRIBUTE_MULTICAST_SUPPORTED, device)));\n   return is_multicast_supported;\n@@ -687,7 +688,7 @@ absl::StatusOr<CUmulticastObjectProp> CreateMulticastObjectProperties(\n   multicast_properties.flags = 0;\n \n   size_t multicast_granularity = 0;\n-  TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(\n+  TF_RETURN_IF_ERROR(cuda::ToStatus(\n       cuMulticastGetGranularity(&multicast_granularity, &multicast_properties,\n                                 CU_MULTICAST_GRANULARITY_RECOMMENDED)));\n \n@@ -994,7 +995,9 @@ absl::StatusOr<bool> CudaExecutor::VmmDeallocateMemory(void* ptr) {\n \n absl::StatusOr<void*> CollectiveMemoryAllocate(StreamExecutor* executor,\n                                                uint64_t bytes) {\n-  if (bytes == 0) return nullptr;\n+  if (bytes == 0) {\n+    return nullptr;\n+  }\n \n   std::unique_ptr<ActivateContext> activation = executor->Activate();\n   TF_ASSIGN_OR_RETURN(xla::gpu::GpuCollectives * gpu_collectives,\n@@ -1234,14 +1237,12 @@ absl::StatusOr<std::unique_ptr<Kernel>> CudaExecutor::LoadKernel(\n   } else {\n     const auto& packing_spec =\n         std::get<KernelArgumentsPackingSpec>(spec.kernel_args_packing());\n-    cuda_kernel->set_args_packing([packing_spec](const Kernel& kernel,\n-                                                 const KernelArgs& args) {\n-      const auto& mem_args =\n-          stream_executor::Cast<stream_executor::KernelArgsDeviceMemoryArray>(\n-              &args);\n-      return packing_spec.BuildArguments(mem_args->device_memory_args(),\n-                                         args.number_of_shared_bytes());\n-    });\n+    cuda_kernel->set_args_packing(\n+        [packing_spec](const Kernel& kernel, const KernelArgs& args) {\n+          const auto& mem_args = Cast<KernelArgsDeviceMemoryArray>(&args);\n+          return packing_spec.BuildArguments(mem_args->device_memory_args(),\n+                                             args.number_of_shared_bytes());\n+        });\n   }\n   return std::move(cuda_kernel);\n }\n@@ -1309,7 +1310,8 @@ absl::StatusOr<ModuleHandle> CudaExecutor::LoadModule(\n     absl::MutexLock lock{in_memory_modules_mu_};\n     return LoadModuleFromCuBin(\n         reinterpret_cast<const char*>(spec.cuda_cubin_in_memory().data()));\n-  } else if (spec.has_cuda_ptx_in_memory()) {\n+  }\n+  if (spec.has_cuda_ptx_in_memory()) {\n     if (!spec.cuda_ptx_in_memory()) {\n       return absl::InternalError(\"PTX not found in spec\");\n     }\n@@ -1416,8 +1418,7 @@ DeviceMemoryBase CudaExecutor::Allocate(uint64_t size, int64_t memory_space) {\n     return DeviceMemoryBase(result.value(), size);\n   }\n \n-  if (memory_space ==\n-      static_cast<int64_t>(stream_executor::MemoryType::kHost)) {\n+  if (memory_space == static_cast<int64_t>(MemoryType::kHost)) {\n     auto result = HostAllocate(cuda_context_, numa_node_, size);\n     if (!result.ok()) {\n       XLA_LOG_DEVICE(ERROR, device_ordinal())\n@@ -1878,7 +1879,9 @@ CudaExecutor::CreateDeviceDescription(int device_ordinal) {\n           .value());\n \n   auto value_or = [](const auto& status_or, auto default_val) {\n-    if (status_or.ok()) return *status_or;\n+    if (status_or.ok()) {\n+      return *status_or;\n+    }\n     return default_val;\n   };\n \n@@ -1990,7 +1993,7 @@ absl::StatusOr<TensorMap> CudaExecutor::CreateTensorMap(\n   return absl::bit_cast<TensorMap>(tensor_map);\n }\n \n-absl::StatusOr<std::unique_ptr<GpuExecutor::MulticastMemory>>\n+absl::StatusOr<std::unique_ptr<MulticastMemory>>\n CudaExecutor::CreateMulticastMemory(uint64_t size, int num_devices) const {\n   if (!is_multicast_supported_) {\n     return absl::FailedPreconditionError(\n@@ -2012,18 +2015,17 @@ CudaExecutor::CudaMulticastMemory::~CudaMulticastMemory() {\n   if (handle_ != 0) {\n     for (auto const& [device_ordinal, mapped_memory_ptr] : mapped_devices_) {\n       XLA_VLOG_DEVICE(3, device_ordinal) << \"Unbind multicast: \" << handle_;\n-      CHECK_OK(stream_executor::cuda::ToStatus(cuMulticastUnbind(\n-          handle_, device_ordinal, /*mcOffset=*/0, padded_size_)));\n+      CHECK_OK(cuda::ToStatus(cuMulticastUnbind(handle_, device_ordinal,\n+                                                /*mcOffset=*/0, padded_size_)));\n \n       XLA_VLOG_DEVICE(3, device_ordinal) << \"Unmap ptr: \" << mapped_memory_ptr;\n-      CHECK_OK(stream_executor::cuda::ToStatus(\n-          cuMemUnmap(mapped_memory_ptr, padded_size_)));\n+      CHECK_OK(cuda::ToStatus(cuMemUnmap(mapped_memory_ptr, padded_size_)));\n       XLA_VLOG_DEVICE(3, device_ordinal)\n           << \"Release address space: \" << mapped_memory_ptr;\n-      CHECK_OK(stream_executor::cuda::ToStatus(\n-          cuMemAddressFree(mapped_memory_ptr, padded_size_)));\n+      CHECK_OK(\n+          cuda::ToStatus(cuMemAddressFree(mapped_memory_ptr, padded_size_)));\n     }\n-    CHECK_OK(stream_executor::cuda::ToStatus(\n+    CHECK_OK(cuda::ToStatus(\n         cuMemRelease(static_cast<CUmemGenericAllocationHandle>(handle_))));\n   }\n }\n@@ -2049,17 +2051,16 @@ absl::Status CudaExecutor::CudaMulticastMemory::Initialize(\n \n   CUmemAllocationProp properties = GetVmmAllocationProperties(\n       cuda_executor->device_, cuda_executor->is_rdma_supported_);\n-  TF_RETURN_IF_ERROR(\n-      stream_executor::cuda::ToStatus(cuMemGetAllocationGranularity(\n-          &granularity_, &properties, CU_MEM_ALLOC_GRANULARITY_RECOMMENDED)));\n+  TF_RETURN_IF_ERROR(cuda::ToStatus(cuMemGetAllocationGranularity(\n+      &granularity_, &properties, CU_MEM_ALLOC_GRANULARITY_RECOMMENDED)));\n \n   padded_size_ = xla::RoundUpTo<size_t>(size, granularity_);\n   num_devices_ = num_devices;\n   TF_ASSIGN_OR_RETURN(CUmulticastObjectProp multicast_properties,\n                       CreateMulticastObjectProperties(num_devices_, size));\n \n-  TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(\n-      cuMulticastCreate(&handle_, &multicast_properties)));\n+  TF_RETURN_IF_ERROR(\n+      cuda::ToStatus(cuMulticastCreate(&handle_, &multicast_properties)));\n   XLA_VLOG_DEVICE(3, cuda_executor->device_ordinal())\n       << \"Created multicast memory: \" << static_cast<uint64_t>(handle_)\n       << \" size: \" << padded_size_ << \" with granularity: \" << granularity_\n@@ -2079,8 +2080,8 @@ absl::Status CudaExecutor::CudaMulticastMemory::SubscribeDevice(\n   }\n \n   XLA_VLOG_DEVICE(3, device_number) << \"Subscribe to multicast: \" << handle_;\n-  TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(\n-      cuMulticastAddDevice(handle_, device_number)));\n+  TF_RETURN_IF_ERROR(\n+      cuda::ToStatus(cuMulticastAddDevice(handle_, device_number)));\n   subscribed_devices_++;\n   return absl::OkStatus();\n }\n@@ -2106,9 +2107,8 @@ absl::StatusOr<void*> CudaExecutor::CudaMulticastMemory::MapMemory(\n     return absl::FailedPreconditionError(\"All devices should be subscribed.\");\n   }\n \n-  TF_ASSIGN_OR_RETURN(\n-      stream_executor::gpu::CudaExecutor::VmmMemoryHandle memory_handle,\n-      cuda_executor->RetainVmmMemoryHandle(location.opaque()));\n+  TF_ASSIGN_OR_RETURN(CudaExecutor::VmmMemoryHandle memory_handle,\n+                      cuda_executor->RetainVmmMemoryHandle(location.opaque()));\n \n   CUmemGenericAllocationHandle retained_memory_handle =\n       static_cast<CUmemGenericAllocationHandle>(memory_handle.handle());\n@@ -2119,7 +2119,7 @@ absl::StatusOr<void*> CudaExecutor::CudaMulticastMemory::MapMemory(\n                     reinterpret_cast<uint64_t>(base_address.opaque());\n \n   // Bind the memory to the multicast object.\n-  TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(\n+  TF_RETURN_IF_ERROR(cuda::ToStatus(\n       cuMulticastBindMem(handle_, /*mcOffset=*/0, retained_memory_handle,\n                          /*memOffset=*/offset, padded_size_, /*flags=*/0)));\n \n@@ -2132,14 +2132,14 @@ absl::StatusOr<void*> CudaExecutor::CudaMulticastMemory::MapMemory(\n   // Map a virtual address range for the multicast memory. Multicast\n   // memory is used to reduce the data stored in the multicast object.\n   CUdeviceptr multicast_device_ptr;\n-  TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(cuMemAddressReserve(\n+  TF_RETURN_IF_ERROR(cuda::ToStatus(cuMemAddressReserve(\n       &multicast_device_ptr, padded_size_, granularity_, 0, 0)));\n \n-  TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(\n+  TF_RETURN_IF_ERROR(cuda::ToStatus(\n       cuMemMap(multicast_device_ptr, padded_size_, 0, handle_, 0)));\n \n   CUmemAccessDesc accessDesc = GetVmmAccessDescriptor(cuda_executor->device_);\n-  TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(\n+  TF_RETURN_IF_ERROR(cuda::ToStatus(\n       cuMemSetAccess(multicast_device_ptr, padded_size_, &accessDesc, 1)));\n \n   absl::MutexLock subscription_lock(mapped_devices_mu_);"
        },
        {
            "sha": "2c42a87ac3e1a77306820da6188a7ac9f24720ef",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h?ref=239e1bbe60e36c238e5e388c11120f39cd40a26d",
            "patch": "@@ -47,6 +47,7 @@ limitations under the License.\n #include \"xla/stream_executor/event_based_timer.h\"\n #include \"xla/stream_executor/fft.h\"\n #include \"xla/stream_executor/gpu/gpu_executor.h\"\n+#include \"xla/stream_executor/gpu/multicast_memory.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/kernel.h\"\n #include \"xla/stream_executor/kernel_spec.h\""
        },
        {
            "sha": "29c1c8967579fc382afedb0dcb10c22ac36a5fb9",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor_multigpu_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test.cc?ref=239e1bbe60e36c238e5e388c11120f39cd40a26d",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_executor_multigpu_test_kernels.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/gpu_init.h\"\n+#include \"xla/stream_executor/gpu/multicast_memory.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n@@ -87,7 +88,7 @@ TEST(CudaExecutorMultiGpuTest, CudaMulticastMemoryResubscriptionFails) {\n   if (!executors[0]->is_multicast_supported()) {\n     GTEST_SKIP() << \"Test requires multicast support.\";\n   }\n-  std::unique_ptr<CudaExecutor::MulticastMemory> multicast_memory;\n+  std::unique_ptr<MulticastMemory> multicast_memory;\n   TF_ASSERT_OK_AND_ASSIGN(multicast_memory,\n                           executors[0]->CreateMulticastMemory(1024, 2));\n   EXPECT_THAT(multicast_memory->SubscribeDevice(0), IsOk());\n@@ -103,7 +104,7 @@ TEST(CudaExecutorMultiGpuTest, AllDevicesMustBeSubscribedBeforeMapping) {\n   if (!executors[0]->is_multicast_supported()) {\n     GTEST_SKIP() << \"Test requires multicast support.\";\n   }\n-  std::unique_ptr<CudaExecutor::MulticastMemory> multicast_memory;\n+  std::unique_ptr<MulticastMemory> multicast_memory;\n   TF_ASSERT_OK_AND_ASSIGN(multicast_memory,\n                           executors[0]->CreateMulticastMemory(1024, 2));\n   EXPECT_THAT(multicast_memory->SubscribeDevice(0), IsOk());\n@@ -121,7 +122,7 @@ TEST(CudaExecutorMultiGpuTest, CudaMulticastMemorySubscribeMoreDevices) {\n   if (!executors[0]->is_multicast_supported()) {\n     GTEST_SKIP() << \"Test requires multicast support.\";\n   }\n-  std::unique_ptr<CudaExecutor::MulticastMemory> multicast_memory;\n+  std::unique_ptr<MulticastMemory> multicast_memory;\n   TF_ASSERT_OK_AND_ASSIGN(multicast_memory,\n                           executors[0]->CreateMulticastMemory(1024, 2));\n   EXPECT_THAT(multicast_memory->SubscribeDevice(0), IsOk());\n@@ -140,7 +141,7 @@ TEST(CudaExecutorMultiGpuTest, CudaMulticastMemoryUsingNonVmmMemory) {\n     GTEST_SKIP() << \"Test requires multicast support.\";\n   }\n   const int64_t kNumDevices = 2;\n-  std::unique_ptr<CudaExecutor::MulticastMemory> multicast_memory;\n+  std::unique_ptr<MulticastMemory> multicast_memory;\n   TF_ASSERT_OK_AND_ASSIGN(\n       multicast_memory, executors[0]->CreateMulticastMemory(1024, kNumDevices));\n   EXPECT_THAT(multicast_memory->SubscribeDevice(0), IsOk());\n@@ -164,7 +165,7 @@ TEST(CudaExecutorMultiGpuTest, CudaMulticastMemoryUsingVmmMemory) {\n   const int kNumElements = 8;\n   const size_t kMemorySize = kNumElements * sizeof(int);\n   const int kValue = 2;\n-  std::unique_ptr<CudaExecutor::MulticastMemory> multicast_memory;\n+  std::unique_ptr<MulticastMemory> multicast_memory;\n   TF_ASSERT_OK_AND_ASSIGN(multicast_memory, executors[0]->CreateMulticastMemory(\n                                                 kMemorySize, kNumDevices));\n   EXPECT_THAT(multicast_memory->SubscribeDevice(0), IsOk());\n@@ -207,7 +208,7 @@ TEST(CudaExecutorMultiGpuTest, CudaMulticastMemoryMapDifferentSlicesUnaligned) {\n   const int64_t kNumElements = 8;\n   const int64_t kMappedMemorySize = kNumElements * sizeof(int);\n   const int kValue = 2;\n-  std::unique_ptr<CudaExecutor::MulticastMemory> multicast_memory;\n+  std::unique_ptr<MulticastMemory> multicast_memory;\n   TF_ASSERT_OK_AND_ASSIGN(\n       multicast_memory,\n       executors[0]->CreateMulticastMemory(kMappedMemorySize, kNumDevices));\n@@ -242,7 +243,7 @@ TEST(CudaExecutorMultiGpuTest, CudaMulticastMemoryMapDifferentSlices) {\n   const int64_t kNumElements = 8;\n   const int64_t kMappedMemorySize = kNumElements * sizeof(int);\n   const int kValue = 2;\n-  std::unique_ptr<CudaExecutor::MulticastMemory> multicast_memory;\n+  std::unique_ptr<MulticastMemory> multicast_memory;\n   TF_ASSERT_OK_AND_ASSIGN(\n       multicast_memory,\n       executors[0]->CreateMulticastMemory(kMappedMemorySize, kNumDevices));"
        },
        {
            "sha": "e560b77255f4e3dd03511c0a979e6ca06eaf5c39",
            "filename": "third_party/xla/xla/stream_executor/gpu/BUILD",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD?ref=239e1bbe60e36c238e5e388c11120f39cd40a26d",
            "patch": "@@ -170,6 +170,7 @@ cc_library(\n     name = \"gpu_executor_header\",\n     hdrs = [\"gpu_executor.h\"],\n     deps = [\n+        \":multicast_memory\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream_executor_common\",\n@@ -181,6 +182,16 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"multicast_memory\",\n+    hdrs = [\"multicast_memory.h\"],\n+    deps = [\n+        \"//xla/stream_executor:device_memory\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+    ],\n+)\n+\n cc_library(\n     name = \"gpu_helpers_header\",\n     hdrs = [\"gpu_helpers.h\"],"
        },
        {
            "sha": "b49ffec2055a710bf3e7e3b83618f0ad97722194",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_executor.h",
            "status": "modified",
            "additions": 3,
            "deletions": 23,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_executor.h?ref=239e1bbe60e36c238e5e388c11120f39cd40a26d",
            "patch": "@@ -26,14 +26,12 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/synchronization/mutex.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/gpu/multicast_memory.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/stream_executor/stream_executor_common.h\"\n \n-namespace stream_executor {\n-\n-namespace gpu {\n+namespace stream_executor::gpu {\n \n class GpuStream;\n \n@@ -68,23 +66,6 @@ class GpuExecutor : public StreamExecutorCommon {\n \n   uint64_t GetArgumentLoggingMode() const { return argument_logging_mode_; }\n \n-  // Abstract class for multicast memory.\n-  class MulticastMemory {\n-   public:\n-    virtual ~MulticastMemory() = default;\n-\n-    MulticastMemory() = default;\n-\n-    virtual absl::Status SubscribeDevice(int device_number) {\n-      return absl::UnimplementedError(\"SubscribeDevice is not implemented.\");\n-    }\n-\n-    virtual absl::StatusOr<void*> MapMemory(const DeviceMemoryBase& location,\n-                                            const GpuExecutor* gpu_executor) {\n-      return absl::UnimplementedError(\"MapMemory is not implemented.\");\n-    }\n-  };\n-\n   virtual absl::StatusOr<std::unique_ptr<MulticastMemory>>\n   CreateMulticastMemory(uint64_t size, int num_devices) const {\n     return absl::UnimplementedError(\n@@ -108,7 +89,6 @@ class GpuExecutor : public StreamExecutorCommon {\n   void operator=(const GpuExecutor&) = delete;\n };\n \n-}  // namespace gpu\n-}  // namespace stream_executor\n+}  // namespace stream_executor::gpu\n \n #endif  // XLA_STREAM_EXECUTOR_GPU_GPU_EXECUTOR_H_"
        },
        {
            "sha": "2424ac768e35bfb254b78fa534fb29125b2a6362",
            "filename": "third_party/xla/xla/stream_executor/gpu/multicast_memory.h",
            "status": "added",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fmulticast_memory.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/239e1bbe60e36c238e5e388c11120f39cd40a26d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fmulticast_memory.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fmulticast_memory.h?ref=239e1bbe60e36c238e5e388c11120f39cd40a26d",
            "patch": "@@ -0,0 +1,44 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_STREAM_EXECUTOR_GPU_MULTICAST_MEMORY_H_\n+#define XLA_STREAM_EXECUTOR_GPU_MULTICAST_MEMORY_H_\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+\n+namespace stream_executor::gpu {\n+\n+class GpuExecutor;\n+\n+// Abstract class for multicast memory.\n+class MulticastMemory {\n+ public:\n+  virtual ~MulticastMemory() = default;\n+\n+  virtual absl::Status SubscribeDevice(int device_number) {\n+    return absl::UnimplementedError(\"SubscribeDevice is not implemented.\");\n+  }\n+\n+  virtual absl::StatusOr<void*> MapMemory(const DeviceMemoryBase& location,\n+                                          const GpuExecutor* gpu_executor) {\n+    return absl::UnimplementedError(\"MapMemory is not implemented.\");\n+  }\n+};\n+\n+}  // namespace stream_executor::gpu\n+\n+#endif  // XLA_STREAM_EXECUTOR_GPU_MULTICAST_MEMORY_H_"
        }
    ],
    "stats": {
        "total": 235,
        "additions": 138,
        "deletions": 97
    }
}