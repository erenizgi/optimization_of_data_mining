{
    "author": "Cjkkkk",
    "message": "PR #32985: [XLA:GPU] add cuDNN SDPA fp32 bias support\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32985\n\nüìù Summary of Changes\nAdd fp32 bias support for cuDNN SDPA, previously it assumes to have same data type as QKV (fp16/bf16).\n\nüéØ Justification\nfp32 bias is used in certain workloads, cuDNN will produces NaNs if data type is not set correctly in cudnn frontend. This PR will fix this issue: https://github.com/jax-ml/jax/issues/32242.\n\nüöÄ Kind of Contribution\nüêõ Bug Fix\n\nüìä Benchmark (for Performance Improvements)\nNone\n\nüß™ Unit Tests:\nNone\n\nüß™ Execution Tests:\nmake bias test as template and test both bf16 and fp32.\n\nCopybara import of the project:\n\n--\n2d147d67ceb426b89fe643378db4b4d567086ff9 by Cjkkkk <ske@nvidia.com>:\n\nadd fp32 bias support\n\n--\n35927259453ad70309bc887dc4206013fd439b82 by Cjkkkk <ske@nvidia.com>:\n\nadd dependency for str_replace.h\n\n--\n9c57e545ac62c0e24aafd7b3420f2b6d5b9a1e0a by Cjkkkk <ske@nvidia.com>:\n\nguard fp32 bias test with cudnn 9.13+\n\nMerging this change closes #32985\n\nPiperOrigin-RevId: 827393277",
    "sha": "abd77445699ae805c6c7710b4e0bc8d83613cdf4",
    "files": [
        {
            "sha": "10ebc98531642c11aec94e6b1e8c409a2d6d8df2",
            "filename": "third_party/xla/xla/service/gpu/tests/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abd77445699ae805c6c7710b4e0bc8d83613cdf4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abd77445699ae805c6c7710b4e0bc8d83613cdf4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD?ref=abd77445699ae805c6c7710b4e0bc8d83613cdf4",
            "patch": "@@ -855,6 +855,7 @@ xla_test(\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n+        \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n     ],"
        },
        {
            "sha": "db0e8ccd7740cf74afbb1dbbb9c6302e5cdf9778",
            "filename": "third_party/xla/xla/service/gpu/tests/gpu_fused_mha_test.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 8,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abd77445699ae805c6c7710b4e0bc8d83613cdf4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_fused_mha_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abd77445699ae805c6c7710b4e0bc8d83613cdf4/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_fused_mha_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_fused_mha_test.cc?ref=abd77445699ae805c6c7710b4e0bc8d83613cdf4",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"absl/strings/str_replace.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/array4d.h\"\n #include \"xla/error_spec.h\"\n@@ -358,7 +359,7 @@ class FlashAttentionBMMScaleBiasSoftmaxBMM : public MultiHeadedAttentionTest {\n   const std::string                                                   // NOLINT\n   GetModuleFlash_Attention_BMM1_Bias_Softmax_BMM2_HloString_BF16() {  // NOLINT\n     const std::string hlo_text = R\"(\n-    HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[2,6,2048,128]{3,2,1,0},bf16[2,6,128,2048]{3,2,1,0},bf16[2,6,2048,128]{3,2,1,0},bf16[2,6,2048,2048]{3,2,1,0})->bf16[2,6,2048,128]{3,2,1,0}}, allow_spmd_sharding_propagation_to_output={true}\n+    HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[2,6,2048,128]{3,2,1,0},bf16[2,6,128,2048]{3,2,1,0},bf16[2,6,2048,128]{3,2,1,0},$bias_type[2,6,2048,2048]{3,2,1,0})->bf16[2,6,2048,128]{3,2,1,0}}, allow_spmd_sharding_propagation_to_output={true}\n \n     region_0.28 {\n       Arg_0.29 = bf16[] parameter(0)\n@@ -379,8 +380,9 @@ class FlashAttentionBMMScaleBiasSoftmaxBMM : public MultiHeadedAttentionTest {\n       constant.6 = bf16[] constant(2)\n       broadcast.7 = bf16[2,6,2048,2048]{3,2,1,0} broadcast(constant.6), dimensions={}\n       multiply.11 = bf16[2,6,2048,2048]{3,2,1,0} multiply(dot.10, broadcast.7)\n-      Arg_3.4 = bf16[2,6,2048,2048]{3,2,1,0} parameter(3), sharding={replicated}\n-      add.27 = bf16[2,6,2048,2048]{3,2,1,0} add(multiply.11, Arg_3.4)\n+      Arg_3.4 = $bias_type[2,6,2048,2048]{3,2,1,0} parameter(3), sharding={replicated}\n+      convert.1 = bf16[2,6,2048,2048]{3,2,1,0} convert(Arg_3.4)\n+      add.27 = bf16[2,6,2048,2048]{3,2,1,0} add(multiply.11, convert.1)\n       constant.9 = bf16[] constant(-inf)\n       reduce.32 = bf16[2,6,2048]{2,1,0} reduce(add.27, constant.9), dimensions={3}, to_apply=region_0.28\n       reshape.33 = bf16[2,6,2048,1]{3,2,1,0} reshape(reduce.32)\n@@ -408,15 +410,15 @@ class FlashAttentionBMMScaleBiasSoftmaxBMM : public MultiHeadedAttentionTest {\n   const std::string  // NOLINT\n   GetModuleFlash_Attention_CuDNN_BMM1_Bias_Softmax_BMM2_HloString_BF16() {  // NOLINT\n     const std::string hlo_text = R\"(\n-    HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,128,2048]{3,2,1,0}, bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,2048,2048]{3,2,1,0})->bf16[2,6,2048,128]{3,2,1,0}}, allow_spmd_sharding_propagation_to_output={true}\n+    HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,128,2048]{3,2,1,0}, bf16[2,6,2048,128]{3,2,1,0}, $bias_type[2,6,2048,2048]{3,2,1,0})->bf16[2,6,2048,128]{3,2,1,0}}, allow_spmd_sharding_propagation_to_output={true}\n \n     ENTRY main.52 {\n       Arg_0.1 = bf16[2,6,2048,128]{3,2,1,0} parameter(0), sharding={replicated}\n       Arg_1.2 = bf16[2,6,128,2048]{3,2,1,0} parameter(1), sharding={replicated}\n       transpose = bf16[2,6,2048,128]{3,2,1,0} transpose(Arg_1.2), dimensions={0,1,3,2}\n       Arg_2.3 = bf16[2,6,2048,128]{3,2,1,0} parameter(2), sharding={replicated}\n-      Arg_3.4 = bf16[2,6,2048,2048]{3,2,1,0} parameter(3), sharding={replicated}\n-      fmha-bmm-scale-bias-softmax-bmm = (bf16[2,6,2048,128]{3,2,1,0}, u8[0]{0}) custom-call(Arg_0.1, transpose, Arg_2.3, Arg_3.4), custom_call_target=\"__cudnn$fmhaScaleBiasSoftmax\", operand_layout_constraints={bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,2048,2048]{3,2,1,0}}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_fmha_backend_config\":{\"algorithm\":{\"algo_id\":\"0\",\"math_type\":\"TENSOR_OP_MATH\",\"tuning_knobs\":{\"24\":\"0\",\"17\":\"1\"},\"workspace_size\":\"0\"},\"fmha_scale\":2,\"dropout_rate\":0,\"bmm1_dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"3\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"bmm2_dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"intermediate_tensor_shape\":{\"element_type\":\"BF16\",\"dimensions\":[\"2\",\"6\",\"2048\",\"2048\"],\"tuple_shapes\":[],\"layout\":{\"dim_level_types\":[],\"dim_unique\":[],\"dim_ordered\":[],\"minor_to_major\":[\"3\",\"2\",\"1\",\"0\"],\"tiles\":[],\"tail_padding_alignment_in_elements\":\"1\",\"element_size_in_bits\":\"0\",\"memory_space\":\"0\",\"index_primitive_type\":\"PRIMITIVE_TYPE_INVALID\",\"pointer_primitive_type\":\"PRIMITIVE_TYPE_INVALID\",\"dynamic_shape_metadata_prefix_bytes\":\"0\",\"split_configs\":[]},\"is_dynamic_dimension\":[false,false,false,false]},\"seed\":\"42\",\"is_flash_attention\":false,\"is_causal_mask\":false,\"mask_type\":\"NO_MASK\",\"force_deterministic\":false,\"sliding_window_length\":0},\"force_earliest_schedule\":false}\n+      Arg_3.4 = $bias_type[2,6,2048,2048]{3,2,1,0} parameter(3), sharding={replicated}\n+      fmha-bmm-scale-bias-softmax-bmm = (bf16[2,6,2048,128]{3,2,1,0}, u8[0]{0}) custom-call(Arg_0.1, transpose, Arg_2.3, Arg_3.4), custom_call_target=\"__cudnn$fmhaScaleBiasSoftmax\", operand_layout_constraints={bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,2048,128]{3,2,1,0}, $bias_type[2,6,2048,2048]{3,2,1,0}}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_fmha_backend_config\":{\"algorithm\":{\"algo_id\":\"0\",\"math_type\":\"TENSOR_OP_MATH\",\"tuning_knobs\":{\"24\":\"0\",\"17\":\"1\"},\"workspace_size\":\"0\"},\"fmha_scale\":2,\"dropout_rate\":0,\"bmm1_dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"3\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"bmm2_dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"intermediate_tensor_shape\":{\"element_type\":\"BF16\",\"dimensions\":[\"2\",\"6\",\"2048\",\"2048\"],\"tuple_shapes\":[],\"layout\":{\"dim_level_types\":[],\"dim_unique\":[],\"dim_ordered\":[],\"minor_to_major\":[\"3\",\"2\",\"1\",\"0\"],\"tiles\":[],\"tail_padding_alignment_in_elements\":\"1\",\"element_size_in_bits\":\"0\",\"memory_space\":\"0\",\"index_primitive_type\":\"PRIMITIVE_TYPE_INVALID\",\"pointer_primitive_type\":\"PRIMITIVE_TYPE_INVALID\",\"dynamic_shape_metadata_prefix_bytes\":\"0\",\"split_configs\":[]},\"is_dynamic_dimension\":[false,false,false,false]},\"seed\":\"42\",\"is_flash_attention\":false,\"is_causal_mask\":false,\"mask_type\":\"NO_MASK\",\"force_deterministic\":false,\"sliding_window_length\":0},\"force_earliest_schedule\":false}\n       ROOT get-tuple-element = bf16[2,6,2048,128]{3,2,1,0} get-tuple-element(fmha-bmm-scale-bias-softmax-bmm), index=0\n     } // main.52\n   )\";\n@@ -742,8 +744,24 @@ class FlashAttentionBMMScaleBiasSoftmaxBMM : public MultiHeadedAttentionTest {\n         GetModuleFlash_Attention_BMM1_Bias_Softmax_BMM2_HloString_BF16();\n     std::string hlo_string_ref =\n         GetModuleFlash_Attention_CuDNN_BMM1_Bias_Softmax_BMM2_HloString_BF16();\n-    EXPECT_TRUE(RunAndCompareTwoModules(hlo_string, hlo_string_ref,\n-                                        ErrorSpec{1e-3, 1e-5}));\n+\n+    if (GetDnnVersionInfoOrDefault(backend().default_stream_executor()) >=\n+        se::dnn::VersionInfo(9, 13, 0)) {\n+      // fp32 bias is supported to cudnn 9.13 and above\n+      std::string f32_bias_hlo_string =\n+          absl::StrReplaceAll(hlo_string, {{\"$bias_type\", \"f32\"}});\n+      std::string f32_bias_hlo_string_ref =\n+          absl::StrReplaceAll(hlo_string_ref, {{\"$bias_type\", \"f32\"}});\n+      EXPECT_TRUE(RunAndCompareTwoModules(\n+          f32_bias_hlo_string, f32_bias_hlo_string_ref, ErrorSpec{1e-3, 1e-5}));\n+    }\n+\n+    std::string bf16_bias_hlo_string =\n+        absl::StrReplaceAll(hlo_string, {{\"$bias_type\", \"bf16\"}});\n+    std::string bf16_bias_hlo_string_ref =\n+        absl::StrReplaceAll(hlo_string_ref, {{\"$bias_type\", \"bf16\"}});\n+    EXPECT_TRUE(RunAndCompareTwoModules(\n+        bf16_bias_hlo_string, bf16_bias_hlo_string_ref, ErrorSpec{1e-3, 1e-5}));\n   }\n \n   void TestImpl_Flash_Attention_Training_BMM1_Bias_Softmax_BMM2() {"
        },
        {
            "sha": "39f282fc900f40feb622d1e577b9fdfde01db064",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_dnn.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abd77445699ae805c6c7710b4e0bc8d83613cdf4/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abd77445699ae805c6c7710b4e0bc8d83613cdf4/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc?ref=abd77445699ae805c6c7710b4e0bc8d83613cdf4",
            "patch": "@@ -4091,11 +4091,14 @@ absl::StatusOr<CudnnGraph> GetCudnnFlashAttentionOperationGraph(\n \n   // Setting bias\n   if (bias_descriptor.has_value()) {\n+    cudnn_frontend::DataType_t dataType =\n+        ToCudnnFrontendDataType(bias_descriptor->type());\n     auto bias_tensor =\n         graph.tensor(Tensor_attributes()\n                          .set_name(\"bias\")\n                          .set_dim(bias_descriptor->dimensions())\n                          .set_stride(bias_descriptor->GetLogicalStrides())\n+                         .set_data_type(dataType)\n                          .set_uid(next_uid()));\n     sdpa_options.set_bias(bias_tensor);\n   }\n@@ -4813,12 +4816,15 @@ absl::StatusOr<CudnnGraph> GetCudnnFlashAttentionBackwardOperationGraph(\n   std::shared_ptr<Tensor_attributes> d_bias_tensor;\n   if (use_bias) {\n     DCHECK(bias_descriptor != std::nullopt);\n+    cudnn_frontend::DataType_t dataType =\n+        ToCudnnFrontendDataType(bias_descriptor->type());\n     auto bias_dims = bias_descriptor->dimensions();\n     auto bias_strides = bias_descriptor->GetLogicalStrides();\n     auto bias_tensor = graph.tensor(Tensor_attributes()\n                                         .set_name(\"bias\")\n                                         .set_dim(bias_dims)\n                                         .set_stride(bias_strides)\n+                                        .set_data_type(dataType)\n                                         .set_uid(next_uid()));\n     sdpa_backward_options.set_bias(bias_tensor);\n "
        }
    ],
    "stats": {
        "total": 41,
        "additions": 33,
        "deletions": 8
    }
}