{
    "author": "vwbaker",
    "message": "Update tests to work with `--xla_gpu_experimental_enable_fusion_autotuner`.\n\nThe new flag adds an autotuner pass which autotunes between block_level_emitters and native emitters and therefore the config is unpredictable. These tests are trying to test the heuristics used when there is no autotuner, so we should turn the autotuner off to preserve the tests logic.\n\nPiperOrigin-RevId: 812783680",
    "sha": "92a3b52a3f27341f7ef571ac4ea2f33d7810cce0",
    "files": [
        {
            "sha": "fc2c60ed1167d05d4e964c2f5f75d6cef402fb07",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/92a3b52a3f27341f7ef571ac4ea2f33d7810cce0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/92a3b52a3f27341f7ef571ac4ea2f33d7810cce0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=92a3b52a3f27341f7ef571ac4ea2f33d7810cce0",
            "patch": "@@ -1136,10 +1136,13 @@ ENTRY main {\n   ROOT fusion = f32[1024,1024,1024] fusion(p0), kind=kLoop, calls=transpose\n })\";\n \n+  // Disable autotuning as this test is attempting to test a heuristic, but\n+  // autotuning tests both cases, and is not guaranteed to be deterministic.\n+  auto config = GetModuleConfigForTest();\n+  config.mutable_debug_options().set_xla_gpu_autotune_level(0);\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto module_and_executable,\n-      GetOptimizedModuleForExecutable(transpose_fusion_module,\n-                                      GetModuleConfigForTest()));\n+      GetOptimizedModuleForExecutable(transpose_fusion_module, config));\n   const HloModule* optimized_module = module_and_executable.first;\n \n   if (cc.IsAtLeastAmpere()) {\n@@ -1174,11 +1177,13 @@ ENTRY main {\n   reshape = f32[1024,1024,4]{2,1,0} reshape(p0)\n   ROOT transpose = f32[4,1024,1024]{2,1,0} transpose(reshape), dimensions={2,1,0}\n })\";\n-\n+  // Disable autotuning as this test is attempting to test a heuristic, but\n+  // autotuning tests both cases, and is not guaranteed to be deterministic.\n+  auto config = GetModuleConfigForTest();\n+  config.mutable_debug_options().set_xla_gpu_autotune_level(0);\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto rewritable_transpose_module_and_executable,\n-      GetOptimizedModuleForExecutable(rewritable_transpose_string,\n-                                      GetModuleConfigForTest()));\n+      GetOptimizedModuleForExecutable(rewritable_transpose_string, config));\n   const HloModule* rewritable_transpose_optimized_module =\n       rewritable_transpose_module_and_executable.first;\n   EXPECT_TRUE(HasBlockLevelFusionConfig(\n@@ -1200,8 +1205,7 @@ ENTRY main {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto unrewritable_transpose_module_and_executable,\n-      GetOptimizedModuleForExecutable(unrewritable_transpose_string,\n-                                      GetModuleConfigForTest()));\n+      GetOptimizedModuleForExecutable(unrewritable_transpose_string, config));\n   const HloModule* unrewritable_transpose_optimized_module =\n       unrewritable_transpose_module_and_executable.first;\n   EXPECT_FALSE(HasBlockLevelFusionConfig("
        }
    ],
    "stats": {
        "total": 18,
        "additions": 11,
        "deletions": 7
    }
}