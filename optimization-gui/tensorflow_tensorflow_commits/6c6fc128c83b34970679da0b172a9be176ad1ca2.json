{
    "author": "GleasonK",
    "message": "[StableHLO->HLO] Address direct lowering diffs\n\nPiperOrigin-RevId: 805601454",
    "sha": "6c6fc128c83b34970679da0b172a9be176ad1ca2",
    "files": [
        {
            "sha": "3079fad3ec3cd9fff6d341f362e9d62843781505",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 35,
            "deletions": 3,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c6fc128c83b34970679da0b172a9be176ad1ca2/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c6fc128c83b34970679da0b172a9be176ad1ca2/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=6c6fc128c83b34970679da0b172a9be176ad1ca2",
            "patch": "@@ -5001,6 +5001,26 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.ml\n    %0 = stablehlo.divide %cst, %cst : tensor<i32>\n    %1 = stablehlo.divide %cst_1, %cst_1 : tensor<ui32>\n    %2 = stablehlo.divide %cst_2, %cst_2 : tensor<f32>\n+@@ -485,6 +748,19 @@\n+   // CHECK-NEXT: return [[RESULT0]]\n+   %0 = stablehlo.set_dimension_size %arg0, %c, dim = 0 : (tensor<10xf32>, tensor<i32>) -> tensor<?xf32, #stablehlo.bounds<10>>\n+   return %0 : tensor<?xf32, #stablehlo.bounds<10>>\n++}\n++\n++// -----\n++\n++// Don't fold when washing away a bounded dimension, not safe to replace with\n++// operand when types mismatch.\n++// CHECK-LABEL: func.func @no_fold_set_dimension_size_bounded_input\n++func.func @no_fold_set_dimension_size_bounded_input(%arg0: tensor<?x4xf32, #stablehlo.bounds<8, ?>>) -> tensor<8x4xf32> {\n++  %c = stablehlo.constant dense<8> : tensor<i32>\n++  // CHECK: [[RESULT0:%.+]] = stablehlo.set_dimension_size\n++  // CHECK-NEXT: return [[RESULT0]]\n++  %0 = stablehlo.set_dimension_size %arg0, %c, dim = 0 : (tensor<?x4xf32, #stablehlo.bounds<8, ?>>, tensor<i32>) -> tensor<8x4xf32>\n++  return %0 : tensor<8x4xf32>\n+ }\n+ \n+ // -----\n diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n --- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n +++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n@@ -5536,7 +5556,19 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold\n      }\n      APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }\n      static APInt foldUint(APInt lhs, APInt rhs) { return lhs.urem(rhs); }\n-@@ -963,8 +1051,16 @@\n+@@ -925,6 +1013,11 @@\n+     auto resultType = op.getType();\n+     // No need to verify static shape or dtype here since we aren't evaluating\n+     // dtype, just folding set_dim_size ops with no semantic meaning.\n++\n++    // Don't fold if the input is dynamic and we're washing away the bound.\n++    if (op.getOperand().getType() != op.getType())\n++      return rewriter.notifyMatchFailure(\n++          op, \"operand and result type must be the same\");\n+ \n+     SplatElementsAttr cstSplatAttr;\n+     matchPattern(op.getSize(), m_Constant(&cstSplatAttr));\n+@@ -963,8 +1056,16 @@\n    struct FoldSign {\n      FoldSign(Type elementType) : elementType(elementType) {}\n      Type elementType;\n@@ -5555,7 +5587,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold\n  \n      APInt operator()(APInt operand) {\n        // SignOp only supports signed integers.\n-@@ -1220,13 +1316,9 @@\n+@@ -1220,13 +1321,9 @@\n  \n      for (auto [inputValue, bodyArg] :\n           llvm::zip_equal(op.getOperands(), body.getArguments())) {\n@@ -5572,7 +5604,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold\n          return rewriter.notifyMatchFailure(op,\n                                             \"Input must be a splat constant.\");\n  \n-@@ -1236,7 +1328,7 @@\n+@@ -1236,7 +1333,7 @@\n              op, \"Could not get the shape of the body argument.\");\n  \n        bodyArgConstantAttrs.push_back(DenseElementsAttr::get("
        },
        {
            "sha": "118464ddd37a6b42a7daf474699e4016002489cb",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/mlir_hlo_to_hlo.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 5,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c6fc128c83b34970679da0b172a9be176ad1ca2/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c6fc128c83b34970679da0b172a9be176ad1ca2/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc?ref=6c6fc128c83b34970679da0b172a9be176ad1ca2",
            "patch": "@@ -217,14 +217,24 @@ bool IsBoundedOrStatic(mlir::Type ty) {\n \n   if (ranked_ty.hasStaticShape()) return true;\n \n-  auto encoding = mlir::dyn_cast_or_null<mlir::mhlo::TypeExtensionsAttr>(\n-      ranked_ty.getEncoding());\n-  if (!encoding || encoding.getBounds().empty()) return false;\n-\n+  // Allow both StableHLO and MHLO type extensions.\n+  std::optional<mlir::ArrayRef<int64_t>> bounds;\n+  if (auto encoding =\n+          mlir::dyn_cast_or_null<mlir::stablehlo::TypeExtensionsAttr>(\n+              ranked_ty.getEncoding())) {\n+    bounds = encoding.getBounds();\n+  } else if (auto encoding =\n+                 mlir::dyn_cast_or_null<mlir::mhlo::TypeExtensionsAttr>(\n+                     ranked_ty.getEncoding())) {\n+    bounds = encoding.getBounds();\n+  }\n+\n+  if (!bounds.has_value() || bounds->empty()) return false;\n+  mlir::ArrayRef<int64_t> bounds_ref = *bounds;\n   int64_t rank = ranked_ty.getRank();\n   for (int64_t dim = 0; dim < rank; ++dim) {\n     if (ranked_ty.isDynamicDim(dim) &&\n-        encoding.getBounds()[dim] == mlir::ShapedType::kDynamic)\n+        bounds_ref[dim] == mlir::ShapedType::kDynamic)\n       return false;\n   }\n   return true;"
        },
        {
            "sha": "4ac8db04196a0deda11eef4e30e7a132c5623eae",
            "filename": "third_party/xla/xla/hlo/translate/tests/stablehlo.mlir",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c6fc128c83b34970679da0b172a9be176ad1ca2/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fstablehlo.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c6fc128c83b34970679da0b172a9be176ad1ca2/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fstablehlo.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fstablehlo.mlir?ref=6c6fc128c83b34970679da0b172a9be176ad1ca2",
            "patch": "@@ -2086,3 +2086,14 @@ func.func @main(%arg0: tensor<i1>, %arg1: memref<2xf32>) -> memref<2xf32> {\n     }\n   func.return %0#1: memref<2xf32>\n }\n+\n+// -----\n+\n+// CHECK-LABEL: HloModule main\n+// CHECK: set-dimension-size\n+// CHECK-NOT: cast\n+func.func @main(%arg0: tensor<4xf32>, %arg1: tensor<i32>) -> tensor<?xf32> {\n+  %0 = stablehlo.set_dimension_size %arg0, %arg1, dim = 0 : (tensor<4xf32>, tensor<i32>) -> tensor<?xf32, #stablehlo.bounds<4>>\n+  %cast = tensor.cast %0 : tensor<?xf32, #stablehlo.bounds<4>> to tensor<?xf32>\n+  return %cast : tensor<?xf32>\n+}"
        },
        {
            "sha": "1fde1efde3b5b5ba11721f89b69f8a9ff7016c2f",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/IR/hlo_ops.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c6fc128c83b34970679da0b172a9be176ad1ca2/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c6fc128c83b34970679da0b172a9be176ad1ca2/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_ops.cc?ref=6c6fc128c83b34970679da0b172a9be176ad1ca2",
            "patch": "@@ -4441,6 +4441,9 @@ OpFoldResult SetDimensionSizeOp::fold(FoldAdaptor adaptor) {\n   auto ty = dyn_cast<RankedTensorType>(getType());\n   if (!ty) return {};\n \n+  // If input is dynamic and output is not, we can't fold.\n+  if (getOperand().getType() != getType()) return {};\n+\n   int64_t dimSize = ty.getDimSize(getDimension());\n   if (dimSize == size.getSplatValue<IntegerAttr>().getInt())\n     return getOperand();"
        },
        {
            "sha": "b44edcd56ae36523d652bc7a8deba1758d5d51ab",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/hlo_legalize_to_stablehlo/hlo_legalize_to_stablehlo_pass.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c6fc128c83b34970679da0b172a9be176ad1ca2/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_stablehlo%2Fhlo_legalize_to_stablehlo_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c6fc128c83b34970679da0b172a9be176ad1ca2/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_stablehlo%2Fhlo_legalize_to_stablehlo_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_stablehlo%2Fhlo_legalize_to_stablehlo_pass.cc?ref=6c6fc128c83b34970679da0b172a9be176ad1ca2",
            "patch": "@@ -128,6 +128,13 @@ struct HloLegalizeToStablehloPass\n                         mhlo::XlaRngGetAndUpdateStateOp>();\n       target.addDynamicallyLegalOp<mhlo::AddDependencyOp>(\n           [](mhlo::AddDependencyOp op) { return !hasMhloOperand(op); });\n+      target.addDynamicallyLegalOp<mhlo::CustomCallOp>(\n+          [](mhlo::CustomCallOp op) {\n+            return !!op.getCustomCallScheduleAttr();\n+          });\n+      // TODO: StableHLO AllToAll has different semantics than MHLO AllToAll.\n+      target.addDynamicallyLegalOp<mhlo::AllToAllOp>(\n+          [](mhlo::AllToAllOp op) { return op.getNumOperands() > 1; });\n       patterns.add<AddDependencyOpToStablehloTokenConverter>(&getContext());\n     }\n "
        },
        {
            "sha": "dfdc5dfea25383ab62729ce9eb12942dce95161b",
            "filename": "third_party/xla/xla/mlir_hlo/tests/Dialect/mhlo/hlo-legalize-to-stablehlo-partial.mlir",
            "status": "modified",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c6fc128c83b34970679da0b172a9be176ad1ca2/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fhlo-legalize-to-stablehlo-partial.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c6fc128c83b34970679da0b172a9be176ad1ca2/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fhlo-legalize-to-stablehlo-partial.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fhlo-legalize-to-stablehlo-partial.mlir?ref=6c6fc128c83b34970679da0b172a9be176ad1ca2",
            "patch": "@@ -35,6 +35,30 @@ func.func @copy() -> tensor<2x1xi32> {\n \n // -----\n \n+// CHECK-LABEL: func @all_to_all_tuple\n+func.func @all_to_all_tuple(%arg0: tensor<128x4xf32>, %arg1: tensor<128x4xf32>) -> (tensor<128x4xf32>, tensor<128x4xf32>) {\n+  // CHECK: mhlo.all_to_all\n+  %0:2 = \"mhlo.all_to_all\"(%arg0, %arg1) {\n+    replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>,\n+    channel_handle = #mhlo.channel_handle<handle = 1, type = 1>\n+  } : (tensor<128x4xf32>, tensor<128x4xf32>) -> (tensor<128x4xf32>, tensor<128x4xf32>)\n+  return %0#0, %0#1 : tensor<128x4xf32>, tensor<128x4xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: func @custom_call_schedule\n+func.func @custom_call_schedule(%arg0: tensor<f32>) -> tensor<f32> {\n+  // CHECK: mhlo.custom_call\n+  %0 = \"mhlo.custom_call\"(%arg0) {\n+    call_target_name = \"foo\",\n+    custom_call_schedule = #mhlo<custom_call_schedule EARLIEST>\n+  } : (tensor<f32>) -> tensor<f32>\n+  func.return %0 : tensor<f32>\n+}\n+\n+// -----\n+\n // Tokens flow between StableHLO and MHLO ops, so need to have special converson\n // logic. AddDependencyOp is the only op that doesn't exist in StableHLO but\n // uses token types, so it can have either StableHLO or MHLO token types as"
        }
    ],
    "stats": {
        "total": 103,
        "additions": 95,
        "deletions": 8
    }
}