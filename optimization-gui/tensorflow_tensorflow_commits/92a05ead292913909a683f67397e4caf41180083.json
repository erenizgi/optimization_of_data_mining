{
    "author": "ImanHosseini",
    "message": "Handle AllocateBuffer feeding into multiple indices of a tuple\n\nPiperOrigin-RevId: 816128774",
    "sha": "92a05ead292913909a683f67397e4caf41180083",
    "files": [
        {
            "sha": "f21fee8383bfcf30cc3a6673397c4da84c289268",
            "filename": "third_party/xla/xla/hlo/transforms/host_offloader.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 8,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/92a05ead292913909a683f67397e4caf41180083/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fhost_offloader.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/92a05ead292913909a683f67397e4caf41180083/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fhost_offloader.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fhost_offloader.cc?ref=92a05ead292913909a683f67397e4caf41180083",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <cstdint>\n #include <iomanip>\n #include <memory>\n+#include <optional>\n #include <queue>\n #include <vector>\n \n@@ -700,7 +701,8 @@ absl::Status HostOffloader::CreateAllocateBufferForDynamicUpdateSlice(\n   // and the DynamicUpdateSlice.\n   std::queue<InstructionAndShapeIndex> queue;\n   queue.push(InstructionAndShapeIndex(dynamic_update_slice));\n-  HloInstruction* previous_instruction = nullptr;\n+  std::optional<InstructionAndShapeIndex> previous_instruction_and_shape =\n+      std::nullopt;\n   bool found_broadcast = false;\n   while (!queue.empty()) {\n     InstructionAndShapeIndex instruction_and_shape = queue.front();\n@@ -792,7 +794,7 @@ absl::Status HostOffloader::CreateAllocateBufferForDynamicUpdateSlice(\n       // non-host memory space user, we need to restore it to its original\n       // memory space and create a new AllocateBuffer on host just for the\n       // instruction that we're walking up the graph from.\n-      CHECK_NE(previous_instruction, nullptr)\n+      CHECK(previous_instruction_and_shape.has_value())\n           << \"We expect to have a previous instruction at this point.\";\n       TF_ASSIGN_OR_RETURN(\n           std::vector<InstructionAndShapeIndex> successors,\n@@ -810,26 +812,34 @@ absl::Status HostOffloader::CreateAllocateBufferForDynamicUpdateSlice(\n                              instruction->mutable_shape(), shape_index),\n                          previous_memory_space);\n           std::vector<int64_t> operand_indices =\n-              previous_instruction->operand_indices(instruction);\n-          if (operand_indices.size() > 1) {\n+              previous_instruction_and_shape->instruction->operand_indices(\n+                  instruction);\n+          if (operand_indices.size() > 1 &&\n+              previous_instruction_and_shape->instruction->opcode() !=\n+                  HloOpcode::kTuple) {\n             return absl::UnimplementedError(\n                 \"We do not yet support adjusting AllocateBuffer when it \"\n-                \"appears in multiple operand indices.\");\n+                \"appears in multiple operand indices unless in a tuple.\");\n+          }\n+          int operand_index = 0;\n+          if (instruction->opcode() == HloOpcode::kTuple) {\n+            operand_index = previous_instruction_and_shape->shape_index.front();\n           }\n           HloInstruction* new_allocate_buffer =\n               instruction->parent()->AddInstruction(\n                   HloInstruction::CreateCustomCall(instruction->shape(), {},\n                                                    \"AllocateBuffer\"));\n           SetMemorySpace(new_allocate_buffer->mutable_shape(),\n                          Layout::kHostMemorySpace);\n-          TF_RETURN_IF_ERROR(previous_instruction->ReplaceOperandWith(\n-              operand_indices[0], new_allocate_buffer));\n+          TF_RETURN_IF_ERROR(\n+              previous_instruction_and_shape->instruction->ReplaceOperandWith(\n+                  operand_indices[operand_index], new_allocate_buffer));\n           break;\n         }\n       }\n       return absl::OkStatus();\n     }\n-    previous_instruction = instruction;\n+    previous_instruction_and_shape = instruction_and_shape;\n     const std::vector<InstructionAndShapeIndex> predecessors =\n         host_offload_utils::GetPredecessors(instruction_and_shape);\n     for (const InstructionAndShapeIndex& predecessor : predecessors) {"
        },
        {
            "sha": "90a8af4d5145f1782c07e457e4a77c58b6982e1f",
            "filename": "third_party/xla/xla/hlo/transforms/host_offloader_test.cc",
            "status": "modified",
            "additions": 48,
            "deletions": 0,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/92a05ead292913909a683f67397e4caf41180083/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fhost_offloader_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/92a05ead292913909a683f67397e4caf41180083/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fhost_offloader_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fhost_offloader_test.cc?ref=92a05ead292913909a683f67397e4caf41180083",
            "patch": "@@ -4536,6 +4536,54 @@ ENTRY main.39_spmd (param.2: f32[16,16,16]) -> (f32[16,16,16], f32[16,16,16]) {\n   EXPECT_EQ(default_memory_space_count, 1);\n }\n \n+TEST_F(HostOffloaderTest, PreExistingAllocateBufferMultipleUsersDuplicated) {\n+  const absl::string_view hlo_string = R\"(\n+HloModule module, entry_computation_layout={(f32[1,10], s32[])->(f32[1,10], f32[2,10])}\n+\n+ENTRY main {\n+  p0 = f32[1,10] parameter(0)\n+  p1 = s32[] parameter(1)\n+  c0 = s32[] constant(0)\n+  alloc = f32[2,10] custom-call(), custom_call_target=\"AllocateBuffer\"\n+  mth = f32[1,10] custom-call(p0), custom_call_target=\"MoveToHost\"\n+  tuple = (f32[2,10], f32[2,10]) tuple(alloc, alloc)\n+  gte0 = f32[2,10] get-tuple-element(tuple), index=0\n+  dus = f32[2,10] dynamic-update-slice(gte0, mth, p1, c0)\n+  ds = f32[1,10] dynamic-slice(dus, p1, c0), dynamic_slice_sizes={1,10}\n+  mtd = f32[1,10] custom-call(ds), custom_call_target=\"MoveToDevice\"\n+  gte1 = f32[2,10] get-tuple-element(tuple), index=1\n+  add = f32[2,10] add(gte1, gte1)\n+  ROOT root_tuple = (f32[1,10], f32[2,10]) tuple(mtd, add)\n+}\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_string));\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, RunHostOffloader(module.get()));\n+  EXPECT_TRUE(changed);\n+  VLOG(1) << module->ToString();\n+\n+  // We expect there to be two AllocateBuffer instructions, one in host memory\n+  // and one in default memory.\n+  int host_memory_space_count = 0;\n+  int default_memory_space_count = 0;\n+  for (HloComputation* computation : module->computations()) {\n+    for (HloInstruction* instruction : computation->instructions()) {\n+      if (instruction->IsCustomCall(\"AllocateBuffer\")) {\n+        if (instruction->shape().layout().memory_space() ==\n+            Layout::kHostMemorySpace) {\n+          host_memory_space_count++;\n+        } else if (instruction->shape().layout().memory_space() ==\n+                   Layout::kDefaultMemorySpace) {\n+          default_memory_space_count++;\n+        }\n+      }\n+    }\n+  }\n+  EXPECT_EQ(host_memory_space_count, 1);\n+  EXPECT_EQ(default_memory_space_count, 1);\n+}\n+\n TEST_F(HostOffloaderTest, AutomaticHostComputeOffloadDisabled) {\n   const absl::string_view hlo_string = R\"(\n     HloModule module, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}"
        }
    ],
    "stats": {
        "total": 74,
        "additions": 66,
        "deletions": 8
    }
}