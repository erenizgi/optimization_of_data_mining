{
    "author": "derdrdirk",
    "message": "Removes GemmAlgorithmPicker in favor of new Autotuner.\n\nPiperOrigin-RevId: 803021012",
    "sha": "c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9",
    "files": [
        {
            "sha": "09bdaaaa65dfb3649bc719291e47b9a5e086f68b",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9",
            "patch": "@@ -1898,7 +1898,6 @@ cc_library(\n         \"//xla/service/gpu/autotuning:autotuner_pass\",\n         \"//xla/service/gpu/autotuning:autotuner_util\",\n         \"//xla/service/gpu/autotuning:conv_algorithm_picker\",\n-        \"//xla/service/gpu/autotuning:gemm_algorithm_picker\",\n         \"//xla/service/gpu/autotuning:gemm_fusion_autotuner\",\n         \"//xla/service/gpu/llvm_gpu_backend:nvptx_backend\",\n         \"//xla/service/gpu/llvm_gpu_backend:nvptx_utils\",\n@@ -1918,9 +1917,11 @@ cc_library(\n         \"//xla/service/gpu/transforms:triangular_solve_rewriter\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor:stream_executor_memory_allocator\",\n         \"//xla/stream_executor/cuda:assemble_compilation_provider\",\n         \"//xla/stream_executor/cuda:compilation_options\",\n         \"//xla/stream_executor/cuda:compilation_provider\",\n@@ -2141,7 +2142,6 @@ cc_library(\n         \"//xla/service/gpu/autotuning:autotuner_pass\",\n         \"//xla/service/gpu/autotuning:autotuner_util\",\n         \"//xla/service/gpu/autotuning:conv_algorithm_picker\",\n-        \"//xla/service/gpu/autotuning:gemm_algorithm_picker\",\n         \"//xla/service/gpu/autotuning:gemm_fusion_autotuner\",\n         \"//xla/service/gpu/llvm_gpu_backend:amdgpu_backend\",\n         \"//xla/service/gpu/transforms:algebraic_simplifier\",\n@@ -2152,9 +2152,11 @@ cc_library(\n         \"//xla/service/gpu/transforms:gpusolver_rewriter\",\n         \"//xla/service/gpu/transforms:triangular_solve_rewriter\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor:device_memory_allocator\",\n         \"//xla/stream_executor:dnn\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor:stream_executor_memory_allocator\",\n         \"//xla/stream_executor/rocm:rocm_platform_id\",\n         \"//xla/stream_executor/rocm:rocm_solver_context\",\n         \"//xla/tsl/platform:env\","
        },
        {
            "sha": "b16d1e508631e2a55946f8552487947311927dfe",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 18,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc?ref=c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9",
            "patch": "@@ -47,7 +47,6 @@ limitations under the License.\n #include \"xla/service/gpu/autotuning/autotuner_pass.h\"\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n #include \"xla/service/gpu/autotuning/conv_algorithm_picker.h\"\n-#include \"xla/service/gpu/autotuning/gemm_algorithm_picker.h\"\n #include \"xla/service/gpu/autotuning/gemm_fusion_autotuner.h\"\n #include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/cublas_padding_requirements.h\"\n@@ -253,7 +252,8 @@ absl::Status AMDGPUCompiler::AddConvAndGemmAutotuningPasses(\n           .debug_options()\n           .xla_gpu_experimental_disable_binary_libraries() ||\n       debug_options.xla_gpu_autotune_level() == 0 ||\n-      debug_options.xla_gpu_exclude_nondeterministic_ops()) {\n+      debug_options.xla_gpu_exclude_nondeterministic_ops() ||\n+      stream_exec == nullptr) {\n     return absl::OkStatus();\n   }\n \n@@ -264,22 +264,18 @@ absl::Status AMDGPUCompiler::AddConvAndGemmAutotuningPasses(\n   std::vector<std::unique_ptr<CodegenBackend>> backends;\n   // TODO: b/407494793 - Add proper support for ROCM. Currently the Cublas\n   // backend uses the same API as rocBLAS.\n-  if (debug_options.xla_gpu_experimental_use_autotuner_pass()) {\n-    backends.push_back(\n-        std::make_unique<CublasBackend>(stream_exec, &debug_options, this));\n-    auto should_autotune = [](const HloInstruction& instruction) -> bool {\n-      return instruction.opcode() == HloOpcode::kCustomCall &&\n-             IsCublasGemm(instruction);\n-    };\n-    TF_ASSIGN_OR_RETURN(\n-        std::unique_ptr<AutotunerPass> autotuner_pass,\n-        AutotunerPass::Create(std::move(backends), debug_options,\n-                              options.device_allocator, stream_exec,\n-                              thread_pool, should_autotune));\n-    pipeline->AddPass(std::move(autotuner_pass));\n-  } else {\n-    pipeline->AddPass<GemmAlgorithmPicker>(autotune_config);\n-  }\n+  backends.push_back(\n+      std::make_unique<CublasBackend>(stream_exec, &debug_options, this));\n+  auto should_autotune = [](const HloInstruction& instruction) -> bool {\n+    return instruction.opcode() == HloOpcode::kCustomCall &&\n+           IsCublasGemm(instruction);\n+  };\n+  TF_ASSIGN_OR_RETURN(\n+      std::unique_ptr<AutotunerPass> autotuner_pass,\n+      AutotunerPass::Create(std::move(backends), debug_options, stream_exec,\n+                            thread_pool, should_autotune,\n+                            options.device_allocator));\n+  pipeline->AddPass(std::move(autotuner_pass));\n \n   return absl::OkStatus();\n }"
        },
        {
            "sha": "e80950768e0586af0319bbaa65318fec225fcb17",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 76,
            "changes": 76,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9",
            "patch": "@@ -346,50 +346,6 @@ xla_test(\n     ],\n )\n \n-cc_library(\n-    name = \"gemm_algorithm_picker\",\n-    srcs = [\"gemm_algorithm_picker.cc\"],\n-    hdrs = [\"gemm_algorithm_picker.h\"],\n-    tags = [\"gpu\"],\n-    deps = [\n-        \":autotuner_util\",\n-        \":redzone_buffers\",\n-        \"//xla:autotune_results_proto_cc\",\n-        \"//xla:autotuning_proto_cc\",\n-        \"//xla:shape_util\",\n-        \"//xla:util\",\n-        \"//xla:xla_proto_cc\",\n-        \"//xla/backends/gpu/runtime:buffer_comparator\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/pass:hlo_pass\",\n-        \"//xla/service:hlo_module_config\",\n-        \"//xla/service/gpu:backend_configs_cc\",\n-        \"//xla/service/gpu:cublas_cudnn\",\n-        \"//xla/service/gpu:matmul_utils\",\n-        \"//xla/service/gpu:stream_executor_util\",\n-        \"//xla/stream_executor:blas\",\n-        \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:device_memory\",\n-        \"//xla/stream_executor:device_memory_allocator\",\n-        \"//xla/stream_executor:stream_executor_h\",\n-        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n-        \"//xla/stream_executor/gpu:redzone_allocator\",\n-        \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:logging\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"//xla/tsl/util/proto:proto_utils\",\n-        \"@com_google_absl//absl/container:flat_hash_set\",\n-        \"@com_google_absl//absl/functional:overload\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/strings:str_format\",\n-        \"@com_google_absl//absl/synchronization\",\n-        \"@com_google_absl//absl/types:span\",\n-        \"@local_tsl//tsl/profiler/lib:scoped_annotation\",\n-    ],\n-)\n-\n cc_library(\n     name = \"autotuner_status_key\",\n     srcs = [\"autotuner_status_key.cc\"],\n@@ -565,38 +521,6 @@ xla_test(\n     ],\n )\n \n-xla_test(\n-    name = \"gemm_algorithm_picker_test\",\n-    srcs = [\"gemm_algorithm_picker_test.cc\"],\n-    backends = [\n-        \"v100\",\n-        \"amdgpu_any\",\n-    ],\n-    deps = [\n-        \":autotuner_util\",\n-        \":gemm_algorithm_picker\",\n-        \"//xla:autotune_results_proto_cc\",\n-        \"//xla:xla_proto_cc\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/testlib:pattern_matcher_gmock\",\n-        \"//xla/service:pattern_matcher\",\n-        \"//xla/service/gpu:backend_configs_cc\",\n-        \"//xla/service/gpu/transforms:gemm_rewriter\",\n-        \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:platform\",\n-        \"//xla/stream_executor:semantic_version\",\n-        \"//xla/tests:hlo_test_base\",\n-        \"//xla/tsl/lib/core:status_test_util\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"//xla/tsl/platform:test\",\n-        \"//xla/tsl/platform:test_main\",\n-        \"//xla/tsl/protobuf:dnn_proto_cc\",\n-        \"@com_google_absl//absl/functional:overload\",\n-        \"@com_google_absl//absl/log\",\n-        \"@com_google_absl//absl/strings:string_view\",\n-    ],\n-)\n-\n cc_library(\n     name = \"conv_algorithm_picker\",\n     srcs = [\"conv_algorithm_picker.cc\"],"
        },
        {
            "sha": "93b2b5618ecd7b0fceb58138c6d6d186a5951fca",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc?ref=c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9",
            "patch": "@@ -45,9 +45,13 @@ namespace gpu {\n \n absl::StatusOr<std::unique_ptr<AutotunerPass>> AutotunerPass::Create(\n     std::vector<std::unique_ptr<CodegenBackend>> backends,\n-    const DebugOptions& debug_options, se::DeviceMemoryAllocator* allocator,\n+    const DebugOptions& debug_options,\n     stream_executor::StreamExecutor* stream_executor,\n-    tsl::thread::ThreadPool* thread_pool, InstructionFilterFn should_autotune) {\n+    tsl::thread::ThreadPool* thread_pool, InstructionFilterFn should_autotune,\n+    se::DeviceMemoryAllocator* allocator) {\n+  // At least one of stream_executor or allocator must be provided.\n+  CHECK(stream_executor != nullptr || allocator != nullptr);\n+\n   std::unique_ptr<GpuProfiler> profiler =\n       GpuProfiler::Create(stream_executor, ProfileOptions(), allocator);\n "
        },
        {
            "sha": "fa0c6535e0e9bc018e7415d3c2bf2bfb2a1d595f",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.h?ref=c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9",
            "patch": "@@ -39,9 +39,9 @@ class AutotunerPass : public HloModulePass {\n  public:\n   static absl::StatusOr<std::unique_ptr<AutotunerPass>> Create(\n       std::vector<std::unique_ptr<CodegenBackend>> backends,\n-      const DebugOptions& debug_options, se::DeviceMemoryAllocator* allocator,\n-      se::StreamExecutor* stream_executor, tsl::thread::ThreadPool* thread_pool,\n-      InstructionFilterFn should_autotune);\n+      const DebugOptions& debug_options, se::StreamExecutor* stream_executor,\n+      tsl::thread::ThreadPool* thread_pool, InstructionFilterFn should_autotune,\n+      se::DeviceMemoryAllocator* allocator = nullptr);\n \n   absl::string_view name() const override { return \"autotuner\"; }\n "
        },
        {
            "sha": "1cb3dfb96129143aa90177e60e83610bdb42f1d1",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass_test.cc?ref=c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9",
            "patch": "@@ -114,9 +114,9 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotuned) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<AutotunerPass> pass,\n       AutotunerPass::Create(std::move(backends),\n-                            module->config().debug_options(), allocator_.get(),\n-                            stream_executor_, &thread_pool,\n-                            IsCublasGemmInstruction));\n+                            module->config().debug_options(), stream_executor_,\n+                            &thread_pool, IsCublasGemmInstruction,\n+                            allocator_.get()));\n   EXPECT_THAT(pass->Run(module.get(), /*execution_threads=*/{}),\n               tsl::testing::IsOkAndHolds(true));\n   // Verify that the backend config has been updated in the HLO.\n@@ -144,8 +144,8 @@ TEST_F(AutotunerPassTest, CublasGemmIsNotAutotunedWhenFilterReturnsFalse) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<AutotunerPass> pass,\n       AutotunerPass::Create(std::move(backends),\n-                            module->config().debug_options(), allocator_.get(),\n-                            stream_executor_, &thread_pool, should_autotune));\n+                            module->config().debug_options(), stream_executor_,\n+                            &thread_pool, should_autotune, allocator_.get()));\n   EXPECT_THAT(pass->Run(module.get(), /*execution_threads=*/{}),\n               tsl::testing::IsOkAndHolds(true));\n   // Verify that the backend config has *not* been updated in the HLO.\n@@ -180,8 +180,8 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotunedAndCached) {\n         std::unique_ptr<AutotunerPass> pass,\n         AutotunerPass::Create(std::move(backends),\n                               module->config().debug_options(),\n-                              allocator_.get(), stream_executor_, &thread_pool,\n-                              IsCublasGemmInstruction));\n+                              stream_executor_, &thread_pool,\n+                              IsCublasGemmInstruction, allocator_.get()));\n     EXPECT_THAT(pass->Run(module.get(), /*execution_threads=*/{}),\n                 tsl::testing::IsOkAndHolds(true));\n   }\n@@ -236,8 +236,8 @@ TEST_F(AutotunerPassTest, CublasGemmIsAutotunedAndCached) {\n         std::unique_ptr<AutotunerPass> pass2,\n         AutotunerPass::Create(std::move(backends2),\n                               module->config().debug_options(),\n-                              allocator_.get(), stream_executor_, &thread_pool,\n-                              IsCublasGemmInstruction));\n+                              stream_executor_, &thread_pool,\n+                              IsCublasGemmInstruction, allocator_.get()));\n     EXPECT_THAT(pass2->Run(module.get(), /*execution_threads=*/{}),\n                 tsl::testing::IsOkAndHolds(true));\n   }"
        },
        {
            "sha": "664123e3cc3df52c33c4c7512c9ffc66484e6ef5",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 530,
            "changes": 530,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/571197f99d65143834525d6b2c3ecf4e3a9466f7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_algorithm_picker.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/571197f99d65143834525d6b2c3ecf4e3a9466f7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_algorithm_picker.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_algorithm_picker.cc?ref=571197f99d65143834525d6b2c3ecf4e3a9466f7",
            "patch": "@@ -1,530 +0,0 @@\n-/* Copyright 2019 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/service/gpu/autotuning/gemm_algorithm_picker.h\"\n-\n-#include <algorithm>\n-#include <cstddef>\n-#include <cstdint>\n-#include <optional>\n-#include <string>\n-#include <utility>\n-#include <variant>\n-#include <vector>\n-\n-#include \"absl/container/flat_hash_set.h\"\n-#include \"absl/functional/overload.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/str_cat.h\"\n-#include \"absl/strings/str_format.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"absl/synchronization/mutex.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/autotuning.pb.h\"\n-#include \"xla/backends/gpu/runtime/buffer_comparator.h\"\n-#include \"xla/hlo/ir/hlo_computation.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/service/gpu/autotuning/autotuner_util.h\"\n-#include \"xla/service/gpu/autotuning/redzone_buffers.h\"\n-#include \"xla/service/gpu/backend_configs.pb.h\"\n-#include \"xla/service/gpu/cublas_cudnn.h\"\n-#include \"xla/service/gpu/matmul_utils.h\"\n-#include \"xla/service/gpu/stream_executor_util.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/blas.h\"\n-#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n-#include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n-#include \"xla/stream_executor/gpu/redzone_allocator.h\"\n-#include \"xla/stream_executor/stream_executor.h\"\n-#include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/logging.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/tsl/util/proto/proto_utils.h\"\n-#include \"xla/util.h\"\n-#include \"xla/xla.pb.h\"\n-#include \"tsl/profiler/lib/scoped_annotation.h\"\n-\n-namespace xla {\n-namespace gpu {\n-namespace {\n-\n-using se::gpu::BlasLt;\n-\n-absl::StatusOr<BlasLt::Epilogue> AsBlasLtEpilogue(\n-    GemmBackendConfig_Epilogue epilogue) {\n-  switch (epilogue) {\n-    case GemmBackendConfig::DEFAULT:\n-      return BlasLt::Epilogue::kDefault;\n-    case GemmBackendConfig::RELU:\n-      return BlasLt::Epilogue::kReLU;\n-    case GemmBackendConfig::GELU:\n-      return BlasLt::Epilogue::kGELU;\n-    case GemmBackendConfig::GELU_AUX:\n-      return BlasLt::Epilogue::kGELUWithAux;\n-    case GemmBackendConfig::BIAS:\n-      return BlasLt::Epilogue::kBias;\n-    case GemmBackendConfig::BIAS_RELU:\n-      return BlasLt::Epilogue::kBiasThenReLU;\n-    case GemmBackendConfig::BIAS_GELU:\n-      return BlasLt::Epilogue::kBiasThenGELU;\n-    case GemmBackendConfig::BIAS_GELU_AUX:\n-      return BlasLt::Epilogue::kBiasThenGELUWithAux;\n-    case GemmBackendConfig::SILU:\n-      return BlasLt::Epilogue::kSILU;\n-    case GemmBackendConfig::BIAS_SILU:\n-      return BlasLt::Epilogue::kBiasThenSILU;\n-    default:\n-      return Internal(\"Unsupported Epilogue.\");\n-  }\n-}\n-\n-class GemmAutotuner {\n-  const AutotuneConfig& autotune_config_;\n-  RedzoneBuffers rz_buffers_;\n-  se::Stream* stream_ = nullptr;\n-  bool deterministic_ops_ = false;\n-  size_t solutions_limit_ = 0;\n-  size_t num_algorithms_left_ = 0;\n-\n- public:\n-  explicit GemmAutotuner(const AutotuneConfig& autotune_config)\n-      : autotune_config_(autotune_config) {}\n-\n-  const AutotuneConfig& config() const { return autotune_config_; }\n-\n-  size_t num_algorithms_left() const { return num_algorithms_left_; }\n-\n-  absl::StatusOr<AutotuneResult> operator()(const HloInstruction* gemm,\n-                                            const AutotuneCacheKey& key) {\n-    num_algorithms_left_ = 0;\n-    if (autotune_config_.IsDeviceless()) {\n-      // Return empty result, will tune at runtime.\n-      return AutotuneResult{};\n-    }\n-    VLOG(3) << \"Starting autotune of GemmThunk \" << gemm->ToString();\n-\n-    TF_ASSIGN_OR_RETURN(stream_, autotune_config_.GetStream());\n-    const DebugOptions& debug_options =\n-        gemm->GetModule()->config().debug_options();\n-    deterministic_ops_ = RequireDeterminism(gemm->GetModule()->config());\n-    solutions_limit_ = debug_options.xla_gpu_autotune_max_solutions();\n-\n-    TF_ASSIGN_OR_RETURN(auto gemm_config,\n-                        GemmConfig::For(gemm, stream_->parent()\n-                                                  ->GetDeviceDescription()\n-                                                  .gpu_compute_capability()));\n-\n-    // Don't run autotuning concurrently on the same GPU.\n-    absl::MutexLock gpu_lock(&GetGpuMutex(stream_->parent()));\n-\n-    bool should_init_buffers = autotune_config_.should_init_buffers();\n-    bool should_check_correctness = autotune_config_.should_check_correctness();\n-    int redzone_padding_bytes = debug_options.xla_gpu_redzone_padding_bytes();\n-    TF_ASSIGN_OR_RETURN(se::Stream * stream, autotune_config_.GetStream());\n-    TF_ASSIGN_OR_RETURN(\n-        rz_buffers_,\n-        RedzoneBuffers::FromInstruction(\n-            *gemm, autotune_config_.GetAllocator(), stream,\n-            RedzoneBuffers::kAllInputsAllOutputs, should_init_buffers,\n-            should_check_correctness, redzone_padding_bytes));\n-\n-    return IsCublasLtMatmul(*gemm) || IsCublasLtMatmulF8(*gemm)\n-               ? TuneGpuBlasLt(gemm, gemm_config)\n-               : TuneGpuBlas(gemm, gemm_config);\n-  }\n-\n- private:\n-  se::DeviceMemoryBase LhsBuffer() { return rz_buffers_.input_buffers().at(0); }\n-  se::DeviceMemoryBase RhsBuffer() { return rz_buffers_.input_buffers().at(1); }\n-  se::DeviceMemoryBase OutputBuffer() {\n-    return rz_buffers_.output_buffers().at(0);\n-  }\n-\n-  const Shape& GetOutputShape(const HloInstruction* gemm) {\n-    return gemm->shape().IsTuple() ? gemm->shape().tuple_shapes(0)\n-                                   : gemm->shape();\n-  }\n-\n-  absl::StatusOr<AutotuneResult> TuneGpuBlasLt(const HloInstruction* gemm,\n-                                               const GemmConfig& gemm_config) {\n-    auto workspace_buffer = rz_buffers_.output_buffers().at(\n-        gemm->shape().tuple_shapes().size() - 1);\n-\n-    GpuBackendConfig gpu_config =\n-        gemm->backend_config<GpuBackendConfig>().value();\n-    const GemmBackendConfig& backend_config = gpu_config.gemm_backend_config();\n-\n-    bool has_matrix_bias = gemm_config.beta != 0.;\n-\n-    TF_ASSIGN_OR_RETURN(\n-        bool has_vector_bias,\n-        gpublas_lt::EpilogueAddsVectorBias(backend_config.epilogue()));\n-\n-    TF_ASSIGN_OR_RETURN(\n-        bool has_aux_output,\n-        gpublas_lt::EpilogueHasAuxiliaryOutput(backend_config.epilogue()));\n-\n-    TF_ASSIGN_OR_RETURN(auto epilogue,\n-                        AsBlasLtEpilogue(backend_config.epilogue()));\n-\n-    se::DeviceMemoryBase a_scale_buffer, b_scale_buffer, c_scale_buffer,\n-        d_scale_buffer, d_amax_buffer, bias_buffer, aux_buffer;\n-\n-    int64_t input_buffer_idx = 2;  // lhs is at 0, rhs is at 1\n-    if (has_vector_bias) {\n-      if (has_matrix_bias) {\n-        input_buffer_idx++;\n-      }\n-      bias_buffer = rz_buffers_.input_buffers().at(input_buffer_idx++);\n-    }\n-    // In the current GemmRewriter design for FP8, the a/b scales remain active\n-    // even when they are not used. Consequently, we must inform the autotuner\n-    // so it can choose algorithms that properly support a/b scales.\n-    if (xla::primitive_util::IsF8Type(\n-            gemm->operand(0)->shape().element_type()) &&\n-        xla::primitive_util::IsF8Type(\n-            gemm->operand(1)->shape().element_type())) {\n-      a_scale_buffer = rz_buffers_.input_buffers().at(input_buffer_idx++);\n-      b_scale_buffer = rz_buffers_.input_buffers().at(input_buffer_idx++);\n-    }\n-\n-    if (has_aux_output) {\n-      aux_buffer = rz_buffers_.output_buffers().at(1);\n-    }\n-\n-    TF_ASSIGN_OR_RETURN(auto plan,\n-                        BlasLt::GetMatmulPlan(stream_, gemm_config, epilogue));\n-\n-    TF_ASSIGN_OR_RETURN(\n-        auto algorithms,\n-        plan->GetAlgorithms(stream_, GemmConfig::kNumAlgorithms,\n-                            /*max_workspace_size*/ workspace_buffer.size()));\n-\n-    auto tuned_func = [&](const BlasLt::MatmulAlgorithm& algorithm)\n-        -> absl::StatusOr<se::blas::ProfileResult> {\n-      // Run a warmup iteration without the profiler active.\n-      TF_RETURN_IF_ERROR(plan->SetAlgorithm(algorithm));\n-      TF_RETURN_IF_ERROR(plan->ExecuteOnStream(\n-          stream_, LhsBuffer(), RhsBuffer(), OutputBuffer(), OutputBuffer(),\n-          bias_buffer, aux_buffer, a_scale_buffer, b_scale_buffer,\n-          c_scale_buffer, d_scale_buffer, d_amax_buffer, workspace_buffer));\n-      se::blas::ProfileResult profile_result;\n-      profile_result.set_warmup_run_executed(true);\n-      TF_RETURN_IF_ERROR(plan->ExecuteOnStream(\n-          stream_, LhsBuffer(), RhsBuffer(), OutputBuffer(), OutputBuffer(),\n-          bias_buffer, aux_buffer, a_scale_buffer, b_scale_buffer,\n-          c_scale_buffer, d_scale_buffer, d_amax_buffer, workspace_buffer,\n-          &profile_result));\n-      return std::move(profile_result);\n-    };\n-\n-    return GetBestAlgorithm<BlasLt::MatmulAlgorithm>(\n-        gemm, algorithms, gemm_config.beta, /*return_algo_index*/ true,\n-        tuned_func);\n-  }\n-\n-  absl::StatusOr<AutotuneResult> TuneGpuBlas(const HloInstruction* gemm,\n-                                             const GemmConfig& gemm_config) {\n-    auto workspace_buffer = rz_buffers_.output_buffers().at(1);\n-\n-    std::vector<se::blas::AlgorithmType> algorithms;\n-    TF_ASSIGN_OR_RETURN(GemmConfig::DescriptorsTuple desc,\n-                        gemm_config.GetMatrixDescriptors(\n-                            LhsBuffer(), RhsBuffer(), OutputBuffer()));\n-\n-    auto blas = stream_->parent()->AsBlas();\n-    if (blas == nullptr) {\n-      return absl::InternalError(\"No BLAS support for stream\");\n-    }\n-    blas->GetBlasGemmAlgorithms(stream_, desc.lhs, desc.rhs, &desc.output,\n-                                &gemm_config.alpha, &gemm_config.beta,\n-                                &algorithms);\n-\n-    auto tuned_func = [&](const se::blas::AlgorithmType& algorithm)\n-        -> absl::StatusOr<se::blas::ProfileResult> {\n-      // Do a warm-up run first, without a profile result. RunGemm swallows\n-      // error codes when profile_result is passed, as it is in the measurement\n-      // below, but not otherwise. It is, therefore, consistent to ignore the\n-      // error code here.\n-      static_cast<void>(RunGemm(gemm_config, LhsBuffer(), RhsBuffer(),\n-                                OutputBuffer(), workspace_buffer,\n-                                deterministic_ops_, stream_, algorithm));\n-      se::blas::ProfileResult profile_result;\n-      // Allow GpuTimer to use its delay kernel implementation to improve\n-      // accuracy.\n-      profile_result.set_warmup_run_executed(true);\n-      // We expect GemmWithAlgorithm to fail sometimes -- in fact, it will fail\n-      // for all algorithms if we're targeting < sm_50. But because we pass a\n-      // non-null ProfileResult, DoGemmWithAlgorithm should always return true,\n-      // and the actual success-ness is returned in ProfileResult::is_valid.\n-      TF_RETURN_IF_ERROR(RunGemm(gemm_config, LhsBuffer(), RhsBuffer(),\n-                                 OutputBuffer(), workspace_buffer,\n-                                 deterministic_ops_, stream_, algorithm,\n-                                 &profile_result));\n-      return std::move(profile_result);\n-    };\n-\n-    return GetBestAlgorithm<se::blas::AlgorithmType>(\n-        gemm, algorithms, gemm_config.beta, /*return_algo_index*/ false,\n-        tuned_func);\n-  }\n-\n-  // Returns the index (into `algorithms`) of the fastest algorithm.\n-  template <typename AlgoT, typename TunedFunc>\n-  absl::StatusOr<AutotuneResult> GetBestAlgorithm(\n-      const HloInstruction* gemm, absl::Span<const AlgoT> algorithms,\n-      double beta, bool return_algo_index, TunedFunc&& run_benchmark) {\n-    static_assert(std::is_invocable_r_v<absl::StatusOr<se::blas::ProfileResult>,\n-                                        TunedFunc, const AlgoT&>,\n-                  \"Tuned function has incorrect prototype!\");\n-\n-    if (!stream_->parent()->SynchronizeAllActivity()) {\n-      return Internal(\"Failed to synchronize GPU for autotuning.\");\n-    }\n-    tsl::profiler::ScopedAnnotation annotation([&] {\n-      return absl::StrFormat(\"XlaAutotunerMeasurement:#hlo_op=%s#\",\n-                             gemm->name());\n-    });\n-\n-    auto& hlo_module_config = gemm->GetModule()->mutable_config();\n-    const auto& output_shape = GetOutputShape(gemm);\n-\n-    se::DeviceMemoryBase reference_buffer;\n-    if (autotune_config_.should_check_correctness()) {\n-      TF_ASSIGN_OR_RETURN(reference_buffer,\n-                          rz_buffers_.RedzoneAllocator().AllocateBytes(\n-                              ShapeUtil::ByteSizeOf(output_shape)));\n-    }\n-\n-    // Do not print error messages if should_skip_wrong_results() is ON.\n-    BufferComparator comparator(\n-        output_shape,\n-        hlo_module_config.debug_options().xla_gpu_autotune_gemm_rtol(),\n-        /* verbose */ !autotune_config_.should_skip_wrong_results());\n-    std::vector<AutotuneResult> results;\n-    results.reserve(algorithms.size());\n-    std::optional<int64_t> reference_algorithm;\n-\n-    auto num = algorithms.size();\n-    if (solutions_limit_ > 0) num = std::min(num, solutions_limit_);\n-    for (size_t i = 0; i < num; i++) {\n-      const AlgoT& algorithm = algorithms[i];\n-      // Make sure the output buffer always has the same value if we use\n-      // the bias parameter.\n-      if (autotune_config_.should_reinit_output_buffer() && beta != 0) {\n-        int64_t rng_state = 0;\n-        InitializeBuffer(stream_, output_shape.element_type(), &rng_state,\n-                         OutputBuffer());\n-      }\n-      TF_ASSIGN_OR_RETURN(auto profile_result, run_benchmark(algorithm));\n-\n-      AutotuneResult& result = results.emplace_back();\n-      result.mutable_gemm()->set_algorithm(profile_result.algorithm());\n-\n-      if (!profile_result.is_valid()) {  // Unsupported algorithm.\n-        result.mutable_failure()->set_kind(AutotuneResult::DISQUALIFIED);\n-        continue;\n-      }\n-\n-      VLOG(2) << \"gemm algorithm \" << profile_result.algorithm() << \" took \"\n-              << profile_result.elapsed_time_in_ms() << \"ms\";\n-\n-      *result.mutable_run_time() = tsl::proto_utils::ToDurationProto(\n-          absl::Milliseconds(profile_result.elapsed_time_in_ms()));\n-\n-      if (!autotune_config_.should_check_correctness()) {\n-        num_algorithms_left_++;\n-        continue;\n-      }\n-      TF_ASSIGN_OR_RETURN(\n-          se::RedzoneAllocator::RedzoneCheckStatus rz_check_status,\n-          rz_buffers_.RedzoneAllocator().CheckRedzones());\n-\n-      if (!rz_check_status.ok()) {\n-        result.mutable_failure()->set_kind(AutotuneResult::REDZONE_MODIFIED);\n-        *result.mutable_failure()->mutable_msg() =\n-            rz_check_status.RedzoneFailureMsg();\n-        LOG(ERROR) << \"Detected out-of-bounds write in gemm buffer\";\n-        CHECK(!autotune_config_.should_crash_on_check_failure());\n-        continue;\n-      }\n-\n-      num_algorithms_left_++;\n-      if (!reference_algorithm) {\n-        TF_RETURN_IF_ERROR(stream_->Memcpy(&reference_buffer, OutputBuffer(),\n-                                           OutputBuffer().size()));\n-        reference_algorithm = profile_result.algorithm();\n-        continue;\n-      }\n-      // Perform the comparison versus the reference algorithm.\n-      TF_ASSIGN_OR_RETURN(\n-          bool outputs_match,\n-          comparator.CompareEqual(stream_, /*current=*/OutputBuffer(),\n-                                  /*expected=*/reference_buffer));\n-      if (!outputs_match) {\n-        LOG(ERROR) << \"Results mismatch between different GEMM algorithms. \"\n-                   << \"This is likely a bug/unexpected loss of precision.\";\n-        CHECK(!autotune_config_.should_crash_on_check_failure());\n-\n-        // By default, autotuner does NOT really skip wrong results, but\n-        // merely prints out the above error message: this may lead to a\n-        // great confusion. When should_skip_wrong_results() is set to true,\n-        // solutions with accuracy problems will be disqualified.\n-        auto kind = AutotuneResult::WRONG_RESULT;\n-        if (autotune_config_.should_skip_wrong_results()) {\n-          kind = AutotuneResult::DISQUALIFIED;\n-          num_algorithms_left_--;  // Decrement again since we disqualified it.\n-        }\n-        result.mutable_failure()->set_kind(kind);\n-        result.mutable_failure()->mutable_reference_gemm()->set_algorithm(\n-            *reference_algorithm);\n-      }\n-    }  // for algorithms\n-\n-    absl::StatusOr<AutotuneResult> best =\n-        PickBestResult(results, gemm->ToString(), hlo_module_config);\n-    if (best.ok()) {\n-      // Note that, cublas-lt returns an opaque object as an algorithm ID,\n-      // therefore we need to convert it to the index from the algorithms list\n-      // (otherwise, we cannot store this ID inside a gemm_backend_config).\n-      // In contrast, legacy cublas returns a 32-bit integer algorithm ID which\n-      // can be readily stored inside an HLO (hence return_algo_index is false\n-      // for cublas case).\n-      if (!return_algo_index) return best;\n-      // Otherwise, map a real algorithm ID to its index among the results.\n-      for (size_t i = 0; i < results.size(); ++i) {\n-        if (best->gemm().algorithm() == results[i].gemm().algorithm()) {\n-          best->mutable_gemm()->set_algorithm(i);\n-          return best;\n-        }\n-      }\n-      return Internal(\"unknown best algorithm\");\n-    }\n-    LOG(WARNING) << \"Failed to find best cuBLAS algorithm, GEMM performance \"\n-                    \"might be suboptimal: \"\n-                 << best.status();\n-    return AutotuneResult{};\n-  }  // GetBestAlgorithm\n-};  // class GemmAutotuner\n-\n-// Do Gemm Autotune without stream executor. Use results from autotune cache\n-// only.\n-absl::StatusOr<bool> RunOnInstruction(HloInstruction* gemm,\n-                                      GemmAutotuner& autotuner) {\n-  VLOG(3) << \"Loading the autotune result of GemmThunk \" << gemm->ToString();\n-\n-  GpuBackendConfig gpu_config =\n-      gemm->backend_config<GpuBackendConfig>().value();\n-  GemmBackendConfig& backend_config = *gpu_config.mutable_gemm_backend_config();\n-\n-  // Degenerate gemms replaced with memzero operation, no need to auto tune it.\n-  if (backend_config.alpha_real() == 0.0 &&\n-      backend_config.alpha_imag() == 0.0 && backend_config.beta() == 0.0) {\n-    VLOG(3) << \"Skip degenerate gemm instruction auto tuning\";\n-    return false;\n-  }\n-\n-  const AutotuneConfig& config = autotuner.config();\n-  AutotuneCacheKey key(config.GetDeviceDescription(), *gemm);\n-  TF_ASSIGN_OR_RETURN(AutotuneResult algorithm,\n-                      AutotunerUtil::Autotune(\n-                          gemm, config, [&] { return autotuner(gemm, key); }));\n-\n-  auto old_algorithm = backend_config.selected_algorithm();\n-  bool update_algorithm =\n-      IsCublasLtMatmulF8(*gemm) ||\n-      std::visit(absl::Overload(\n-                     [](const se::CudaComputeCapability& cc) {\n-                       // We only set the 'algorithm' field on\n-                       // non-Ampere architectures, as for Ampere\n-                       // it's ignored in any case.\n-                       return !cc.IsAtLeast(se::CudaComputeCapability::kAmpere);\n-                     },\n-                     [](const se::RocmComputeCapability&) {\n-                       return true;  // TODO: not decided yet\n-                     }),\n-                 config.GetGpuComputeCapability());\n-\n-  if (update_algorithm) {\n-    int64_t new_algorithm{};\n-    if (algorithm.has_gemm()) {\n-      new_algorithm = algorithm.gemm().algorithm();\n-    } else {\n-      // NOTE: runtime autotuning is no longer available => set to default\n-      new_algorithm = se::blas::kDefaultAlgorithm;\n-    }\n-\n-    if (new_algorithm == old_algorithm &&\n-        backend_config.has_selected_algorithm()) {\n-      // We don't need to update the backend config if the algorithm was not\n-      // changed unless previously the algorithm wasn't set explicitly.\n-      return false;\n-    }\n-\n-    backend_config.set_selected_algorithm(new_algorithm);\n-    TF_RETURN_IF_ERROR(gemm->set_backend_config(gpu_config));\n-    return true;  // We changed `gemm`\n-  }\n-\n-  return false;  // No change to `gemm`\n-}\n-\n-absl::StatusOr<bool> RunOnComputation(HloComputation* computation,\n-                                      GemmAutotuner& autotuner,\n-                                      size_t* num_algorithms_left) {\n-  bool changed = false;\n-\n-  for (HloInstruction* instr : computation->instructions()) {\n-    if (IsCublasGemm(*instr)) {\n-      TF_ASSIGN_OR_RETURN(bool result, RunOnInstruction(instr, autotuner));\n-      // Gathering statistics on the algorithms left after tuning (for testing)\n-      *num_algorithms_left =\n-          std::max(*num_algorithms_left, autotuner.num_algorithms_left());\n-      changed |= result;\n-    }\n-  }\n-  return changed;\n-}\n-\n-}  // namespace\n-\n-absl::StatusOr<bool> GemmAlgorithmPicker::Run(\n-    HloModule* module,\n-    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n-  XLA_SCOPED_LOGGING_TIMER(\n-      absl::StrCat(\"GemmAlgorithmPicker for \", module->name()));\n-\n-  num_algorithms_left_ = 0;\n-  if (module->config().debug_options().xla_gpu_autotune_level() == 0) {\n-    VLOG(2) << \"GEMM auto-tuning disabled, GemmAlgorithmPicker returning early\";\n-    return false;\n-  }\n-  GemmAutotuner autotuner(config_);\n-  bool changed = false;\n-  for (HloComputation* computation :\n-       module->MakeNonfusionComputations(execution_threads)) {\n-    TF_ASSIGN_OR_RETURN(bool result, RunOnComputation(computation, autotuner,\n-                                                      &num_algorithms_left_));\n-    changed |= result;\n-  }\n-  return changed;\n-}\n-\n-}  // namespace gpu\n-}  // namespace xla"
        },
        {
            "sha": "8f74c23456d66417f16fbe4e198b714edb7a6931",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.h",
            "status": "removed",
            "additions": 0,
            "deletions": 71,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/571197f99d65143834525d6b2c3ecf4e3a9466f7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_algorithm_picker.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/571197f99d65143834525d6b2c3ecf4e3a9466f7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_algorithm_picker.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_algorithm_picker.h?ref=571197f99d65143834525d6b2c3ecf4e3a9466f7",
            "patch": "@@ -1,71 +0,0 @@\n-/* Copyright 2019 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-#ifndef XLA_SERVICE_GPU_AUTOTUNING_GEMM_ALGORITHM_PICKER_H_\n-#define XLA_SERVICE_GPU_AUTOTUNING_GEMM_ALGORITHM_PICKER_H_\n-\n-#include <cstddef>\n-#include <functional>\n-#include <optional>\n-\n-#include \"absl/container/flat_hash_set.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/autotune_results.pb.h\"\n-#include \"xla/autotuning.pb.h\"\n-#include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/pass/hlo_pass_interface.h\"\n-#include \"xla/service/gpu/autotuning/autotuner_util.h\"\n-#include \"xla/service/gpu/autotuning/redzone_buffers.h\"\n-#include \"xla/service/hlo_module_config.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/stream_executor/blas.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n-#include \"xla/stream_executor/device_memory_allocator.h\"\n-#include \"xla/stream_executor/gpu/redzone_allocator.h\"\n-#include \"xla/stream_executor/stream_executor.h\"\n-\n-namespace xla {\n-namespace gpu {\n-\n-// GemmAlgorithmPicker supports two modes: device and deviceless.\n-// In device mode, we run autotuning on the device and store autotune results.\n-// In deviceless mode, we pass in some information related to the device and\n-// use stored autotune results to rewrite Gemm instructions. If the required\n-// autotune result is not stored, then algorithm is set to kRuntimeAutotuning.\n-class GemmAlgorithmPicker : public HloModulePass {\n- public:\n-  explicit GemmAlgorithmPicker(AutotuneConfig config) : config_(config) {}\n-\n-  absl::string_view name() const override { return \"gemm-algorithm-picker\"; }\n-\n-  size_t num_algorithms_left() const { return num_algorithms_left_; }\n-\n-  using HloPassInterface::Run;\n-  absl::StatusOr<bool> Run(\n-      HloModule* module,\n-      const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n-\n- private:\n-  AutotuneConfig config_;\n-  // The number of valid algorithms used for autotuning (from the last call),\n-  // to be used for testing purposes.\n-  size_t num_algorithms_left_ = 0;\n-};\n-\n-}  // namespace gpu\n-}  // namespace xla\n-\n-#endif  // XLA_SERVICE_GPU_AUTOTUNING_GEMM_ALGORITHM_PICKER_H_"
        },
        {
            "sha": "881995f6fef7ca3ec97810a4a253b5047297be3b",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_algorithm_picker_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 337,
            "changes": 337,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/571197f99d65143834525d6b2c3ecf4e3a9466f7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_algorithm_picker_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/571197f99d65143834525d6b2c3ecf4e3a9466f7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_algorithm_picker_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_algorithm_picker_test.cc?ref=571197f99d65143834525d6b2c3ecf4e3a9466f7",
            "patch": "@@ -1,337 +0,0 @@\n-/* Copyright 2022 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/service/gpu/autotuning/gemm_algorithm_picker.h\"\n-\n-#include <cstddef>\n-#include <cstdint>\n-#include <string>\n-#include <variant>\n-\n-#include \"absl/functional/overload.h\"\n-#include \"absl/log/log.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"xla/autotune_results.pb.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/testlib/pattern_matcher_gmock.h\"\n-#include \"xla/service/gpu/autotuning/autotuner_util.h\"\n-#include \"xla/service/gpu/backend_configs.pb.h\"\n-#include \"xla/service/gpu/transforms/gemm_rewriter.h\"\n-#include \"xla/service/pattern_matcher.h\"\n-#include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/platform.h\"\n-#include \"xla/stream_executor/semantic_version.h\"\n-#include \"xla/tests/hlo_test_base.h\"\n-#include \"xla/tsl/lib/core/status_test_util.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/tsl/platform/test.h\"\n-#include \"xla/tsl/protobuf/dnn.pb.h\"\n-#include \"xla/xla.pb.h\"\n-\n-namespace xla::gpu {\n-namespace {\n-\n-namespace m = ::xla::match;\n-\n-class GemmAlgorithmPickerTest : public HloTestBase,\n-                                public ::testing::WithParamInterface<bool> {\n- public:\n-  GemmAlgorithmPickerTest() { AutotunerUtil::ClearAutotuneResults(); }\n-\n-  DebugOptions GetDebugOptionsForTest() const override {\n-    DebugOptions debug_options = HloTestBase::GetDebugOptionsForTest();\n-    debug_options.set_xla_gpu_enable_cublaslt(GetParam());\n-    debug_options.set_xla_gpu_enable_triton_gemm(false);\n-    return debug_options;\n-  }\n-\n-  se::StreamExecutor* stream_exec() {\n-    return backend().default_stream_executor();\n-  }\n-  const se::DeviceDescription& device_desc() {\n-    return stream_exec()->GetDeviceDescription();\n-  }\n-  const se::GpuComputeCapability& gpu_comp() {\n-    return device_desc().gpu_compute_capability();\n-  }\n-\n-  void SetUp() override {\n-    absl::string_view name =\n-        ::testing::UnitTest::GetInstance()->current_test_info()->name();\n-    // We need special handling for BlasGetVersion test.\n-    bool blas_get_version = name.rfind(\"BlasGetVersion\") == 0;\n-\n-    std::visit(\n-        absl::Overload(\n-            [&](const se::CudaComputeCapability& cc) {\n-              if (!blas_get_version && cc.IsAtLeastAmpere()) {\n-                GTEST_SKIP()\n-                    << \"Skipping this test for Ampere+ as it is supported \"\n-                       \"and recommended with the Nvidia Volta+ GPUs.\";\n-              }\n-            },\n-            [&](const se::RocmComputeCapability& cc) {\n-              if (blas_get_version) {\n-                if (device_desc().runtime_version() <\n-                    stream_executor::SemanticVersion{6, 2, 0}) {\n-                  GTEST_SKIP()\n-                      << \"This API is not available on ROCM 6.1 and below.\";\n-                }\n-              } else if (GetDebugOptionsForTest().xla_gpu_enable_cublaslt() &&\n-                         !cc.has_hipblaslt()) {\n-                GTEST_SKIP() << \"No gpublas-lt support on this architecture!\";\n-              }\n-            }),\n-        gpu_comp());\n-  }\n-};\n-\n-TEST_P(GemmAlgorithmPickerTest, BlasGetVersion) {\n-  auto* blas = stream_exec()->AsBlas();\n-  ASSERT_TRUE(blas != nullptr);\n-  std::string version;\n-  ASSERT_TRUE(blas->GetVersion(&version).ok());\n-  VLOG(0) << \"Blas version: \" << version;\n-  ASSERT_TRUE(!version.empty());\n-}\n-\n-TEST_P(GemmAlgorithmPickerTest, SkipAlgorithmsWithAccuracyCheck) {\n-  constexpr absl::string_view kHlo = R\"(\n-HloModule module\n-\n-ENTRY main {\n-  %arg0 = f32[100,100]{1,0} parameter(0)\n-  %arg1 = f32[100,100]{1,0} parameter(1)\n-  ROOT %dot = f32[100,100]{1,0} dot(arg0, arg1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-})\";\n-\n-  auto module_cfg = GetModuleConfigForTest();\n-  auto debug_opts = module_cfg.debug_options();\n-  size_t num_left1 = 0, num_left2 = 0;\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(kHlo, module_cfg));\n-\n-  {\n-    // Run first with default settings (autotune level = 4), keep the number of\n-    // algorithms left after autotuning\n-    TF_ASSERT_OK_AND_ASSIGN(\n-        bool changed,\n-        RunHloPass(\n-            GemmRewriter(\n-                gpu_comp(),\n-                /*toolkit_version=*/stream_executor::SemanticVersion{12, 4, 0}),\n-            module.get()));\n-\n-    AutotuneConfig cfg = AutotuneConfig::FromDebugOptions(\n-        DeviceOrDevicelessConfig{DeviceConfig{stream_exec(), nullptr}},\n-        debug_opts);\n-    GemmAlgorithmPicker gpicker(cfg);\n-    // Note that, we do not care if the algorithm index has been changed:\n-    // the thing matters is the # of algorithms left after sorting out.\n-    TF_ASSERT_OK_AND_ASSIGN(changed, RunHloPass(gpicker, module.get()));\n-    num_left1 = gpicker.num_algorithms_left();\n-    if (num_left1 < 2) {\n-      GTEST_SKIP() << \"Too few algorithms left after the first step\";\n-    }\n-\n-    // Test that the function to get current stream value works fine:\n-    auto* blas = stream_exec()->AsBlas();\n-    ASSERT_TRUE(blas != nullptr);\n-    TF_ASSERT_OK_AND_ASSIGN(bool is_main_stream, blas->IsMainStreamSet());\n-    // ROCM only: CUDA blas API does not reset stream after each blas call.\n-    if (std::holds_alternative<se::RocmComputeCapability>(gpu_comp())) {\n-      ASSERT_TRUE(is_main_stream);\n-    }\n-  }\n-\n-  // Clear cache before the second run!\n-  AutotunerUtil::ClearAutotuneResults();\n-  {\n-    // Run once again but now with autotune level 5 and embarrassingly tight\n-    // rtol which shall disqualify most of the algorithms.\n-\n-    // Note that, we have \"two sources of truth\" for GemmAlgorithmPicker: i.e.,\n-    // debug_options are used to initialize both 'HloModuleConfig' and also\n-    // 'AutotuneConfig'.\n-    debug_opts.set_xla_gpu_autotune_gemm_rtol(1e-12);\n-    debug_opts.set_xla_gpu_autotune_level(5);\n-    module->mutable_config().set_debug_options(debug_opts);\n-    TF_ASSERT_OK_AND_ASSIGN(\n-        bool changed,\n-        RunHloPass(\n-            GemmRewriter(\n-                gpu_comp(),\n-                /*toolkit_version=*/stream_executor::SemanticVersion{12, 4, 0}),\n-            module.get()));\n-\n-    AutotuneConfig cfg = AutotuneConfig::FromDebugOptions(\n-        DeviceOrDevicelessConfig{DeviceConfig{stream_exec(), nullptr}},\n-        debug_opts);\n-    GemmAlgorithmPicker gpicker(cfg);\n-    TF_ASSERT_OK_AND_ASSIGN(changed, RunHloPass(gpicker, module.get()));\n-    num_left2 = gpicker.num_algorithms_left();\n-  }\n-  // Assert that we have fewer algorithms left after the second run.\n-  ASSERT_TRUE(num_left1 > num_left2);\n-}\n-\n-TEST_P(GemmAlgorithmPickerTest, SetAlgorithm) {\n-  constexpr absl::string_view kHlo = R\"(\n-HloModule module\n-\n-ENTRY main {\n-  %arg0 = f32[100,100]{1,0} parameter(0)\n-  %arg1 = f32[100,100]{1,0} parameter(1)\n-  ROOT %dot = f32[100,100]{1,0} dot(arg0, arg1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-})\";\n-\n-  auto module_cfg = GetModuleConfigForTest();\n-  TF_ASSERT_OK_AND_ASSIGN(auto m,\n-                          ParseAndReturnVerifiedModule(kHlo, module_cfg));\n-\n-  bool changed = false;\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      changed,\n-      RunHloPass(\n-          GemmRewriter(\n-              gpu_comp(),\n-              /*toolkit_version=*/stream_executor::SemanticVersion{12, 4, 0}),\n-          m.get()));\n-  changed = false;\n-  DebugOptions opts;\n-  AutotuneConfig cfg = AutotuneConfig::FromDebugOptions(\n-      DeviceOrDevicelessConfig{DeviceConfig{stream_exec(), nullptr}}, opts);\n-  TF_ASSERT_OK_AND_ASSIGN(changed,\n-                          RunHloPass(GemmAlgorithmPicker(cfg), m.get()));\n-  ASSERT_TRUE(changed);\n-\n-  AutotuneResults results;\n-  TF_ASSERT_OK(AutotunerUtil::SerializeAutotuneResults(&results));\n-  ASSERT_EQ(results.results_size(), 1);\n-  auto& result = *results.mutable_results(0)->mutable_result();\n-  int64_t old_algo_id = result.algorithm().algo_id();\n-  int64_t new_algo_id = old_algo_id + 1;\n-  result.mutable_gemm()->set_algorithm(new_algo_id);\n-\n-  AutotunerUtil::ClearAutotuneResults();\n-  TF_ASSERT_OK(AutotunerUtil::LoadAutotuneResults(results));\n-\n-  // Now send the same module through GemmAlgorithmPicker again.  The dot should\n-  // have the new algorithm.\n-  TF_ASSERT_OK_AND_ASSIGN(m, ParseAndReturnVerifiedModule(kHlo, module_cfg));\n-  changed = false;\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      changed,\n-      RunHloPass(\n-          GemmRewriter(gpu_comp(),\n-                       /*toolkit_version=*/se::SemanticVersion{12, 4, 0}),\n-          m.get()));\n-  changed = false;\n-  TF_ASSERT_OK_AND_ASSIGN(changed,\n-                          RunHloPass(GemmAlgorithmPicker(cfg), m.get()));\n-  ASSERT_TRUE(changed);\n-\n-  SCOPED_TRACE(m->ToString());\n-  HloInstruction* dot;\n-  ASSERT_THAT(m->entry_computation()->root_instruction(),\n-              GmockMatch(m::GetTupleElement(m::CustomCall(&dot), 0)));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(GpuBackendConfig gpu_config,\n-                          dot->backend_config<GpuBackendConfig>());\n-  const GemmBackendConfig& config = gpu_config.gemm_backend_config();\n-  EXPECT_EQ(config.selected_algorithm(), new_algo_id);\n-}\n-\n-TEST_P(GemmAlgorithmPickerTest, GetAlgorithmWithoutDevice) {\n-  constexpr absl::string_view kHlo = R\"(\n-HloModule module\n-\n-ENTRY main {\n-  %arg0 = f32[100,100]{1,0} parameter(0)\n-  %arg1 = f32[100,100]{1,0} parameter(1)\n-  ROOT %dot = f32[100,100]{1,0} dot(arg0, arg1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-})\";\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto m, ParseAndReturnVerifiedModule(kHlo, GetModuleConfigForTest()));\n-\n-  bool changed = false;\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      changed,\n-      RunHloPass(\n-          GemmRewriter(\n-              gpu_comp(),\n-              /*toolkit_version=*/stream_executor::SemanticVersion{12, 4, 0}),\n-          m.get()));\n-  changed = false;\n-\n-  DebugOptions opts;\n-  AutotuneConfig cfg = AutotuneConfig::FromDebugOptions(\n-      DeviceOrDevicelessConfig{DeviceConfig{stream_exec(), nullptr}}, opts);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(changed,\n-                          RunHloPass(GemmAlgorithmPicker(cfg), m.get()));\n-  ASSERT_TRUE(changed);\n-\n-  AutotuneResults results;\n-  TF_ASSERT_OK(AutotunerUtil::SerializeAutotuneResults(&results));\n-  ASSERT_EQ(results.results_size(), 1);\n-  auto& result = *results.mutable_results(0)->mutable_result();\n-  int64_t old_algo_id = result.algorithm().algo_id();\n-  int64_t new_algo_id = old_algo_id + 1;\n-  result.mutable_gemm()->set_algorithm(new_algo_id);\n-\n-  AutotunerUtil::ClearAutotuneResults();\n-  TF_ASSERT_OK(AutotunerUtil::LoadAutotuneResults(results));\n-\n-  auto module_cfg = GetModuleConfigForTest();\n-  // Now send the same module through GemmAlgorithmPicker again.  The dot should\n-  // have the new algorithm.\n-  TF_ASSERT_OK_AND_ASSIGN(m, ParseAndReturnVerifiedModule(kHlo, module_cfg));\n-  changed = false;\n-\n-  DevicelessConfig deviceless_config{device_desc()};\n-  AutotuneConfig deviceless_cfg = AutotuneConfig::FromDebugOptions(\n-      DeviceOrDevicelessConfig{deviceless_config}, opts);\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      changed,\n-      RunHloPass(\n-          GemmRewriter(\n-              gpu_comp(),\n-              /*toolkit_version=*/stream_executor::SemanticVersion{12, 4, 0}),\n-          m.get()));\n-  changed = false;\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      changed, RunHloPass(GemmAlgorithmPicker(deviceless_cfg), m.get()))\n-  ASSERT_TRUE(changed);\n-\n-  SCOPED_TRACE(m->ToString());\n-  HloInstruction* dot;\n-\n-  ASSERT_THAT(m->entry_computation()->root_instruction(),\n-              GmockMatch(m::GetTupleElement(m::CustomCall(&dot), 0)));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(GpuBackendConfig gpu_config,\n-                          dot->backend_config<GpuBackendConfig>());\n-  const GemmBackendConfig& config = gpu_config.gemm_backend_config();\n-\n-  EXPECT_EQ(config.selected_algorithm(), new_algo_id);\n-}\n-\n-INSTANTIATE_TEST_SUITE_P(GemmAlgorithmPickerTestSuite, GemmAlgorithmPickerTest,\n-                         ::testing::Bool());\n-\n-}  // namespace\n-}  // namespace xla::gpu"
        },
        {
            "sha": "58ffd3532876b9faa1cd5b5152ed0f3ffb92903f",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 30,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=c1d3c1bf632f628e8cd2eaa234c2cf29ce702ca9",
            "patch": "@@ -62,7 +62,6 @@ limitations under the License.\n #include \"xla/service/gpu/autotuning/autotuner_pass.h\"\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n #include \"xla/service/gpu/autotuning/conv_algorithm_picker.h\"\n-#include \"xla/service/gpu/autotuning/gemm_algorithm_picker.h\"\n #include \"xla/service/gpu/autotuning/gemm_fusion_autotuner.h\"\n #include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/cublas_padding_requirements.h\"\n@@ -356,41 +355,30 @@ absl::Status NVPTXCompiler::AddConvAndGemmAutotuningPasses(\n           .debug_options()\n           .xla_gpu_experimental_disable_binary_libraries() ||\n       debug_options.xla_gpu_autotune_level() == 0 ||\n-      debug_options.xla_gpu_exclude_nondeterministic_ops()) {\n+      debug_options.xla_gpu_exclude_nondeterministic_ops() ||\n+      stream_exec == nullptr) {\n     return absl::OkStatus();\n   }\n \n   // TODO(b/407495801): Cached Gemm as well as Conv autotuning results are\n   // loaded in the GpuConvAlgorithmPicker but should be loaded in the autotuner.\n   pipeline->AddPass<GpuConvAlgorithmPicker>(autotune_config);\n \n-  if (debug_options.xla_gpu_experimental_use_autotuner_pass()) {\n-    std::vector<std::unique_ptr<CodegenBackend>> backends;\n-    backends.push_back(\n-        std::make_unique<CublasBackend>(stream_exec, &debug_options, this));\n-    backends.push_back(\n-        std::make_unique<CublasLtBackend>(stream_exec, &debug_options, this));\n-    auto should_autotune = [](const HloInstruction& instruction) -> bool {\n-      return instruction.opcode() == HloOpcode::kCustomCall &&\n-             IsCublasGemm(instruction);\n-    };\n-    TF_ASSIGN_OR_RETURN(\n-        std::unique_ptr<AutotunerPass> autotuner_pass,\n-        AutotunerPass::Create(std::move(backends), debug_options,\n-                              options.device_allocator, stream_exec,\n-                              thread_pool, should_autotune));\n-    pipeline->AddPass(std::move(autotuner_pass));\n-  } else {\n-    // On Ampere or later, GemmAlgorithmPicker just provides a way to \"warmup\"\n-    // the\n-    // execution. But we already do that during GemmFusionAutotuner pass. In\n-    // that case, we do a recursive compilation call that has\n-    // 'is_autotuning_compilation' set to true.\n-    if (!std::get<se::CudaComputeCapability>(gpu_version).IsAtLeastAmpere() ||\n-        options.is_autotuning_compilation) {\n-      pipeline->AddPass<GemmAlgorithmPicker>(autotune_config);\n-    }\n-  }\n+  std::vector<std::unique_ptr<CodegenBackend>> backends;\n+  backends.push_back(\n+      std::make_unique<CublasBackend>(stream_exec, &debug_options, this));\n+  backends.push_back(\n+      std::make_unique<CublasLtBackend>(stream_exec, &debug_options, this));\n+  auto should_autotune = [](const HloInstruction& instruction) -> bool {\n+    return instruction.opcode() == HloOpcode::kCustomCall &&\n+           IsCublasGemm(instruction);\n+  };\n+  TF_ASSIGN_OR_RETURN(\n+      std::unique_ptr<AutotunerPass> autotuner_pass,\n+      AutotunerPass::Create(std::move(backends), debug_options, stream_exec,\n+                            thread_pool, should_autotune,\n+                            options.device_allocator));\n+  pipeline->AddPass(std::move(autotuner_pass));\n   return absl::OkStatus();\n }\n \n@@ -693,7 +681,9 @@ absl::StatusOr<std::vector<uint8_t>> NVPTXCompiler::LinkModules(\n     const stream_executor::DeviceDescription& device_description,\n     se::StreamExecutor* stream_exec, std::vector<std::vector<uint8_t>> modules,\n     const DebugOptions& debug_options) {\n-  if (modules.empty()) return std::vector<uint8_t>{};\n+  if (modules.empty()) {\n+    return std::vector<uint8_t>{};\n+  }\n \n   auto cc = std::get<stream_executor::CudaComputeCapability>(\n       device_description.gpu_compute_capability());"
        }
    ],
    "stats": {
        "total": 1134,
        "additions": 56,
        "deletions": 1078
    }
}