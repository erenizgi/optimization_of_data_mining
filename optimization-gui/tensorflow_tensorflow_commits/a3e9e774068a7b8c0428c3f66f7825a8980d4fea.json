{
    "author": "rao-ashish",
    "message": "PR #33910: Implement cross host transfer optimizations in StreamExecutorGpuClient\n\nImported from GitHub PR https://github.com/openxla/xla/pull/33910\n\nüìù Summary of Changes\nThis PR adds three optimizations to the `CrossHost{Send,Receive}Buffers` methods in `StreamExecutorGpuClient`:\n1. Communicators are cached for re-use across transfers with `AcquireGpuClique`.\n2. Transfers of multiple arrays are aggregated with NCCL group calls using `GpuCommunicator::GroupExecute`.\n3. Usage / definition events and promises that signal the completion of cross-host transfers are fulfilled on `StreamExecutorGpuClient::thread_pool()` instead of blocking the XLA execute thread.\n\nThis PR follows up on #33284 and is based on discussions with @emilyfertig, @mwhittaker, and @pschuh.\n\nüéØ Justification\n#33284 added `CrossHost{Send,Receive}Buffers` to the PjRt Client API to enable the optimizations implemented in this PR. The old implementation does not cache communicators, aggregate transfers, or asynchronously fulfill events / promises, leading to reduced performance.\n\nüöÄ Kind of Contribution\n‚ö°Ô∏è Performance Improvement\n\nüìä Benchmark (for Performance Improvements)\nBenchmarks were ran on these [three toy programs](https://gist.github.com/rao-ashish/28155ffee98406b7c041037c20fc75e6).\n\nWorkload | Implementation | Time\n-- | -- | --\nexample_basic_overlap | Unoptimized | 26.968 sec\n¬† | Optimized | 43.124 ms\nexample_overlap | Unoptimized | 369.541 sec\n¬† | Optimized | 733.071 ms\nexample_pp | Unoptimized | 369.0924 sec\n¬† | Optimized | 2.433 sec\n\nüß™ Unit Tests:\nThe unit tests added in #33284 continue to pass. The new implementation requires that cross-host transfers are always between a local device and a remote device; a test has been added to make sure that the implementation throws an InvalidArgument error if this is not the case.\n\nüß™ Execution Tests:\nWhen building XLA with the code in this PR along with [a patch for pjrt_ifrt/pjrt_client.cc](https://gist.github.com/rao-ashish/ba5cd773906c8e428147bb80a3568912) so that it uses the new cross-host transfers API, I verified that [these 4 correctness checks](https://gist.github.com/rao-ashish/24ac0df0cb18243c649ac535964b31b8) pass. These IFRT changes will be included in a follow-up PR once other PjRt clients have implemented the new API.\nCopybara import of the project:\n\n--\n15611c927526f4731231fc6d71c79264672b9cbf by Ashish Rao <asrao@nvidia.com>:\n\nImplement cross host transfer optimizations in StreamExecutorGpuClient\n\n--\n69dc9ba808a4c6ba5ae37011d097e3d0ad37fdda by Ashish Rao <asrao@nvidia.com>:\n\nRemove distributed_client_ from StreamExecutorGpuClient and first attempt to plumb incarnations for cross host transfers\n\n--\nc03db313b46eaf9185965e44a7222992c6237c14 by Ashish Rao <asrao@nvidia.com>:\n\nCode cleanup + make SuccessfulCrossHostTransferTest use different arrays in a group\n\n--\n66fb558bf4732dc22335bb409ec415fadd33ac2c by Ashish Rao <asrao@nvidia.com>:\n\nUse a single usage/definition event for a group of sends/receives\n\n--\nc2a7dbd8c75e15d166615991d4aa9bcb94d5ebff by Ashish Rao <asrao@nvidia.com>:\n\nSupport new DefineBuffer method signature\n\n--\n5de4aec41349e63d241dcbbbef7373d43ff32a31 by Ashish Rao <asrao@nvidia.com>:\n\nAdd gpu_communicator as a dependency of se_gpu_pjrt_client in BUILD file\n\n--\nf4040201452f9052d3215ef1ef2e34d2b2f64f73 by Ashish Rao <asrao@nvidia.com>:\n\nFix bug in how send promises are fulfilled\n\n--\n4c2cfda13d6afd1911743f92c3ef95b79c443058 by Ashish Rao <asrao@nvidia.com>:\n\nChange Prepared{Send,Receive} to be simple structs\n\n--\nd4001e93baf20ec33190087259ad0a3a123a8e3b by Ashish Rao <asrao@nvidia.com>:\n\nCode cleanup\n\n--\nd89e3f8a1047f68e0a1a094b19d94759bf75c7e9 by Ashish Rao <asrao@nvidia.com>:\n\nChange Prepared{Send,Receive} to full classes\n\nMerging this change closes #33910\n\nPiperOrigin-RevId: 839179377",
    "sha": "a3e9e774068a7b8c0428c3f66f7825a8980d4fea",
    "files": [
        {
            "sha": "78c5c4003254aa4fc7f4637b3328fc54b0c673c7",
            "filename": "third_party/xla/xla/pjrt/gpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a3e9e774068a7b8c0428c3f66f7825a8980d4fea/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a3e9e774068a7b8c0428c3f66f7825a8980d4fea/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD?ref=a3e9e774068a7b8c0428c3f66f7825a8980d4fea",
            "patch": "@@ -68,9 +68,11 @@ cc_library(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n+        \"//xla/backends/gpu/collectives:gpu_clique\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n         \"//xla/backends/gpu/collectives:gpu_cliques\",\n         \"//xla/backends/gpu/collectives:gpu_collectives\",\n+        \"//xla/backends/gpu/collectives:gpu_communicator\",\n         \"//xla/client:client_library\",\n         \"//xla/client:local_client\",\n         \"//xla/core/collectives\","
        },
        {
            "sha": "82cbcf932e6dda556ca6d989ef6a92a57ab8b984",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 592,
            "deletions": 190,
            "changes": 782,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a3e9e774068a7b8c0428c3f66f7825a8980d4fea/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a3e9e774068a7b8c0428c3f66f7825a8980d4fea/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=a3e9e774068a7b8c0428c3f66f7825a8980d4fea",
            "patch": "@@ -47,9 +47,11 @@ limitations under the License.\n #include \"absl/time/time.h\"\n #include \"absl/types/span.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n+#include \"xla/backends/gpu/collectives/gpu_clique.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/collectives/gpu_cliques.h\"\n #include \"xla/backends/gpu/collectives/gpu_collectives.h\"\n+#include \"xla/backends/gpu/collectives/gpu_communicator.h\"\n #include \"xla/client/local_client.h\"\n #include \"xla/core/collectives/clique_id.h\"\n #include \"xla/core/collectives/collectives.h\"\n@@ -354,14 +356,21 @@ absl::Status StreamExecutorGpuClient::UpdateCompileOptionsInternal(\n   return absl::OkStatus();\n }\n \n-std::string CrossHostTransferName(PjRtGlobalDeviceId src_global_device_id,\n-                                  PjRtGlobalDeviceId dst_global_device_id,\n-                                  RunId transfer_run_id) {\n-  return absl::StrCat(\"cross_host_transfer-\", src_global_device_id.value(),\n-                      \"_to_\", dst_global_device_id.value(), \"-run_\",\n-                      transfer_run_id.ToInt());\n+// ==== Start cross-host transfer implementations ==== //\n+\n+// Anonymous namespace for se_gpu_pjrt_client.cc internal cross-host transfer\n+// helpers.\n+namespace {\n+\n+// Get the local device state for a given PjRtDevice.\n+absl::StatusOr<LocalDeviceState*> GetLocalDeviceState(PjRtDevice* device) {\n+  PjRtStreamExecutorDevice* pjrt_se_device =\n+      tensorflow::down_cast<PjRtStreamExecutorDevice*>(device);\n+  return pjrt_se_device->GetLocalDeviceState();\n }\n \n+// Creates a communicator for a cross-host transfer; used by the original\n+// cross-host transfers API.\n absl::StatusOr<std::unique_ptr<Communicator>> CreateTransferCommunicator(\n     LocalDeviceState* local_device, gpu::GpuCollectives* gpu_collectives,\n     CliqueId clique_id, bool is_sender) {\n@@ -393,6 +402,271 @@ absl::StatusOr<std::unique_ptr<Communicator>> CreateTransferCommunicator(\n   return std::move(communicators[0]);\n }\n \n+// Helper structs / classes for second cross-host transfers API.\n+using AcquiredCliqueAndCommunicator =\n+    std::pair<std::shared_ptr<gpu::LockableGpuClique::Lock>,\n+              gpu::GpuCommunicator*>;\n+\n+class PreparedSend {\n+ public:\n+  StreamExecutorGpuClient* client_;\n+  gpu::GpuCliqueKey clique_key_;\n+  tsl::RCReference<CommonPjRtRawBuffer> raw_buffer_;\n+  std::vector<tsl::RCReference<tsl::AsyncValue>> definition_events_;\n+  tsl::RCReference<PjRtStreamExecutorDeviceEvent> usage_event_;\n+  AcquiredCliqueAndCommunicator clique_and_communicator_;\n+  std::shared_ptr<Future<>::Promise> promise_;\n+\n+  PreparedSend(StreamExecutorGpuClient* client, gpu::GpuCliqueKey clique_key,\n+               tsl::RCReference<CommonPjRtRawBuffer> raw_buffer,\n+               std::vector<tsl::RCReference<tsl::AsyncValue>> definition_events,\n+               tsl::RCReference<PjRtStreamExecutorDeviceEvent> usage_event,\n+               AcquiredCliqueAndCommunicator clique_and_communicator,\n+               std::shared_ptr<Future<>::Promise> promise)\n+      : client_(client),\n+        clique_key_(std::move(clique_key)),\n+        raw_buffer_(std::move(raw_buffer)),\n+        definition_events_(std::move(definition_events)),\n+        usage_event_(std::move(usage_event)),\n+        clique_and_communicator_(std::move(clique_and_communicator)),\n+        promise_(std::move(promise)) {}\n+\n+  PreparedSend(PreparedSend&&) = default;\n+  PreparedSend& operator=(PreparedSend&&) = default;\n+\n+  ~PreparedSend() {\n+    if (!usage_event_ || usage_event_->event()->IsDefined()) {\n+      return;\n+    }\n+    LOG(WARNING) << \"PreparedSend destroyed with unfulfilled usage_event\";\n+    client_->SetEventAsError(\n+        usage_event_->event(),\n+        absl::InternalError(\"PreparedSend destroyed without fulfilling \"\n+                            \"usage_event\"));\n+  }\n+};\n+\n+class PreparedReceive {\n+ public:\n+  StreamExecutorGpuClient* client_;\n+  gpu::GpuCliqueKey clique_key_;\n+  std::unique_ptr<PjRtBuffer> buffer_;\n+  tsl::RCReference<CommonPjRtRawBuffer> raw_buffer_;\n+  tsl::RCReference<PjRtStreamExecutorDeviceEvent> definition_event_;\n+  AcquiredCliqueAndCommunicator clique_and_communicator_;\n+\n+  PreparedReceive(\n+      StreamExecutorGpuClient* client, gpu::GpuCliqueKey clique_key,\n+      std::unique_ptr<PjRtBuffer> buffer,\n+      tsl::RCReference<CommonPjRtRawBuffer> raw_buffer,\n+      tsl::RCReference<PjRtStreamExecutorDeviceEvent> definition_event,\n+      AcquiredCliqueAndCommunicator clique_and_communicator)\n+      : client_(client),\n+        clique_key_(std::move(clique_key)),\n+        buffer_(std::move(buffer)),\n+        raw_buffer_(std::move(raw_buffer)),\n+        definition_event_(std::move(definition_event)),\n+        clique_and_communicator_(std::move(clique_and_communicator)) {}\n+\n+  PreparedReceive(PreparedReceive&&) = default;\n+  PreparedReceive& operator=(PreparedReceive&&) = default;\n+\n+  ~PreparedReceive() {\n+    if (!definition_event_ || definition_event_->event()->IsDefined()) {\n+      return;\n+    }\n+    LOG(WARNING)\n+        << \"PreparedReceive destroyed with unfulfilled definition_event\";\n+    client_->SetEventAsError(\n+        definition_event_->event(),\n+        absl::InternalError(\"PreparedReceive destroyed without fulfilling \"\n+                            \"definition_event\"));\n+  }\n+};\n+\n+// Acquire the GPU clique and communicator for a given clique key.\n+absl::StatusOr<AcquiredCliqueAndCommunicator> AcquireCliqueAndCommunicator(\n+    StreamExecutorGpuClient* client, gpu::GpuCollectives* gpu_collectives,\n+    const gpu::GpuCliqueKey& clique_key,\n+    gpu::AcquiredCliquesMap& acquired_cliques_map, RankId rank_id,\n+    se::Stream* stream) {\n+  // Get the clique ID callback.\n+  const ExecuteOptions dummy_execute_options;\n+  gpu::GpuExecutableRunOptions* dummy_gpu_run_options =\n+      client->gpu_run_options(dummy_execute_options);\n+  gpu::CliqueIdCallback clique_id_callback =\n+      dummy_gpu_run_options->clique_id_callback();\n+\n+  // Acquire the GPU clique for this receive.\n+  if (!acquired_cliques_map.contains(clique_key)) {\n+    TF_ASSIGN_OR_RETURN(\n+        acquired_cliques_map[clique_key],\n+        AcquireGpuClique(gpu_collectives,\n+                         /*device=*/stream->parent(), RunId(0), clique_key,\n+                         clique_id_callback, rank_id, acquired_cliques_map,\n+                         /*max_nchannels=*/0));\n+  }\n+  std::shared_ptr<gpu::LockableGpuClique::Lock> clique =\n+      acquired_cliques_map[clique_key];\n+\n+  // Get the communicator to use for this receive.\n+  std::optional<Communicator*> maybe_communicator = (*clique)->comm(rank_id);\n+  if (!maybe_communicator.has_value()) {\n+    return absl::InternalError(\n+        \"AcquireCliqueAndCommunicator: Unable to get communicator from \"\n+        \"acquired GPU clique.\");\n+  }\n+\n+  return AcquiredCliqueAndCommunicator{\n+      std::move(clique),\n+      tsl::down_cast<gpu::GpuCommunicator*>(*maybe_communicator)};\n+}\n+\n+// Create a `PreparedSend` object bundling together state needed to perform a\n+// send.\n+absl::StatusOr<PreparedSend> PrepareSend(\n+    StreamExecutorGpuClient* client, gpu::GpuCollectives* gpu_collectives,\n+    se::Stream* stream, PjRtBuffer* buffer,\n+    PjRtGlobalDeviceId dst_global_device_id, CrossHostTransferKey transfer_key,\n+    gpu::AcquiredCliquesMap& acquired_cliques_map,\n+    std::shared_ptr<Future<>::Promise> promise,\n+    tsl::RCReference<PjRtStreamExecutorDeviceEvent> usage_event) {\n+  // Form the GPU clique key.\n+  // TODO(asrao, mwhittaker): Supply correct incarnations when creating the\n+  // clique key.\n+  gpu::GpuCliqueKey clique_key = gpu::GpuCliqueKey(\n+      /*devices=*/{GlobalDeviceId(buffer->device()->global_device_id().value()),\n+                   GlobalDeviceId(dst_global_device_id.value())},\n+      /*num_local_participants=*/1);\n+\n+  // Get the clique and communicator for the send.\n+  TF_ASSIGN_OR_RETURN(\n+      AcquiredCliqueAndCommunicator clique_and_communicator,\n+      AcquireCliqueAndCommunicator(client, gpu_collectives, clique_key,\n+                                   acquired_cliques_map, RankId(0), stream));\n+\n+  // Acquire a hold on this buffer. The hold is held as long as usage_event is\n+  // not fulfilled; this behavior is achieved by registering a 'dummy' closure\n+  // capturing raw_buffer that executes after usage_event is fulfilled.\n+  // definition_events can be used to track when the buffer data is ready.\n+  tsl::RCReference<CommonPjRtRawBuffer> raw_buffer;\n+  std::vector<tsl::RCReference<tsl::AsyncValue>> definition_events;\n+\n+  TF_RETURN_IF_ERROR(\n+      tensorflow::down_cast<CommonPjRtBufferImpl*>(buffer)\n+          ->AcquireScopedRawBuffer(\n+              [&](tsl::RCReference<CommonPjRtRawBuffer> buf_raw_buffer,\n+                  std::vector<tsl::RCReference<tsl::AsyncValue>>\n+                      buf_definition_events) mutable\n+                  -> absl::StatusOr<tsl::RCReference<PjRtDeviceEvent>> {\n+                raw_buffer = std::move(buf_raw_buffer);\n+                usage_event->AndThen([raw_buffer]() {});\n+                definition_events = std::move(buf_definition_events);\n+                return usage_event;\n+              },\n+              \"PrepareSend\"));\n+\n+  // Return the result.\n+  return PreparedSend(client, std::move(clique_key), std::move(raw_buffer),\n+                      std::move(definition_events), std::move(usage_event),\n+                      std::move(clique_and_communicator), std::move(promise));\n+}\n+\n+// Create a `PreparedReceive` object bundling together state needed to perform a\n+// receive.\n+absl::StatusOr<PreparedReceive> PrepareReceive(\n+    StreamExecutorGpuClient* client, gpu::GpuCollectives* gpu_collectives,\n+    se::Stream* stream, PjRtDevice* device, PjRtMemorySpace* memory_space,\n+    PjRtGlobalDeviceId src_global_device_id, CrossHostTransferKey transfer_key,\n+    Shape shape, gpu::AcquiredCliquesMap& acquired_cliques_map,\n+    tsl::RCReference<PjRtStreamExecutorDeviceEvent> definition_event) {\n+  // Form the GPU clique key.\n+  // TODO(asrao, mwhittaker): Supply correct incarnations when creating the\n+  // clique key.\n+  gpu::GpuCliqueKey clique_key = gpu::GpuCliqueKey(\n+      /*devices=*/{GlobalDeviceId(src_global_device_id.value()),\n+                   GlobalDeviceId(device->global_device_id().value())},\n+      /*num_local_participants=*/1);\n+\n+  // Get the clique and communicator for the receive.\n+  TF_ASSIGN_OR_RETURN(\n+      AcquiredCliqueAndCommunicator clique_and_communicator,\n+      AcquireCliqueAndCommunicator(client, gpu_collectives, clique_key,\n+                                   acquired_cliques_map, RankId(1), stream));\n+\n+  // Allocate an uninitialized buffer. The buffer will be populated with data\n+  // received from the sending process.\n+  TF_ASSIGN_OR_RETURN(\n+      Shape on_device_shape,\n+      client->MakeDefaultShapeForMemorySpace(\n+          memory_space, shape, shape.has_layout() ? &shape.layout() : nullptr));\n+  TF_ASSIGN_OR_RETURN(\n+      size_t on_device_bytes_count,\n+      client->GetOnDeviceBytesCount(memory_space, on_device_shape));\n+  TF_ASSIGN_OR_RETURN(\n+      tsl::RCReference<CommonPjRtRawBuffer> raw_buffer,\n+      client->AllocateRawBuffer(memory_space, on_device_bytes_count,\n+                                /*retry_on_oom=*/true,\n+                                /*allocate_after=*/{}));\n+\n+  TF_ASSIGN_OR_RETURN(std::unique_ptr<PjRtBuffer> buffer,\n+                      client->DefineBuffer(on_device_shape, memory_space,\n+                                           raw_buffer, {definition_event},\n+                                           /*raw_buffer_is_mutable=*/true));\n+  definition_event->AndThen([raw_buffer]() {});\n+\n+  return PreparedReceive(client, std::move(clique_key), std::move(buffer),\n+                         std::move(raw_buffer), std::move(definition_event),\n+                         std::move(clique_and_communicator));\n+}\n+\n+absl::flat_hash_map<gpu::GpuCliqueKey, std::vector<PreparedSend>>\n+GroupSendsByCliqueKey(std::vector<PreparedSend>&& prepared_sends) {\n+  absl::flat_hash_map<gpu::GpuCliqueKey, std::vector<PreparedSend>> grouped;\n+  grouped.reserve(prepared_sends.size());\n+  for (auto&& prepared_send : prepared_sends) {\n+    grouped[prepared_send.clique_key_].push_back(std::move(prepared_send));\n+  }\n+  return grouped;\n+}\n+\n+absl::flat_hash_map<gpu::GpuCliqueKey, std::vector<PreparedReceive>>\n+GroupReceivesByCliqueKey(std::vector<PreparedReceive>&& prepared_receives) {\n+  absl::flat_hash_map<gpu::GpuCliqueKey, std::vector<PreparedReceive>> grouped;\n+  grouped.reserve(prepared_receives.size());\n+  for (auto&& prepared_receive : prepared_receives) {\n+    grouped[prepared_receive.clique_key_].push_back(\n+        std::move(prepared_receive));\n+  }\n+  return grouped;\n+}\n+\n+absl::Status FulfillDeviceEvent(\n+    PjRtStreamExecutorClient* client, LocalDeviceState* local_device_state,\n+    se::Stream* stream,\n+    tsl::RCReference<PjRtStreamExecutorDeviceEvent> device_event,\n+    const absl::Status& status) {\n+  if (!status.ok()) {\n+    client->SetEventAsError(device_event->event(), status);\n+    return absl::OkStatus();\n+  }\n+  absl::Status s = client->AllocateAndRecordEvent(device_event->event(),\n+                                                  local_device_state, stream);\n+  if (!s.ok()) {\n+    client->SetEventAsError(device_event->event(), s);\n+  }\n+  return s;\n+}\n+\n+void FulfillPromises(std::vector<std::shared_ptr<Future<>::Promise>>& promises,\n+                     absl::Status status) {\n+  for (std::shared_ptr<Future<>::Promise>& promise : promises) {\n+    promise->Set(status);\n+  }\n+}\n+}  // namespace\n+\n+// Send functionality for second cross-host transfers API.\n absl::StatusOr<std::vector<Future<>>>\n StreamExecutorGpuClient::CrossHostSendBuffers(\n     absl::Span<PjRtBuffer* const> buffers,\n@@ -407,140 +681,208 @@ StreamExecutorGpuClient::CrossHostSendBuffers(\n         \"must have the same length, but got %d, %d, and %d.\",\n         buffers.size(), dst_global_device_ids.size(), transfer_keys.size());\n   }\n-\n-  // Perform sends.\n-  std::vector<Future<>> out_futures;\n-  out_futures.reserve(buffers.size());\n   for (int i = 0; i < buffers.size(); ++i) {\n-    TF_ASSIGN_OR_RETURN(\n-        Future<> curr_future,\n-        CrossHostSendBuffer(buffers[i], dst_global_device_ids[i],\n-                            transfer_keys[i]));\n-    out_futures.push_back(std::move(curr_future));\n+    // Each transfer must be between an addressable and a non-addressable\n+    // device. If both devices are addressable, then both a data transfer and a\n+    // 'normal' XLA SPMD executable may try to acquire the same GPU clique,\n+    // causing issues.\n+    PjRtDevice* src_device = buffers[i]->device();\n+    if (!src_device->IsAddressable()) {\n+      return InvalidArgument(\n+          \"CrossHostSendBuffers: buffer %d is on non-addressable device with \"\n+          \"global device id %d.\",\n+          i, src_device->global_device_id().value());\n+    }\n+    TF_ASSIGN_OR_RETURN(PjRtDevice * dst_device,\n+                        LookupDevice(dst_global_device_ids[i]));\n+    if (dst_device->IsAddressable()) {\n+      return InvalidArgument(\n+          \"CrossHostSendBuffers: destination device for buffer %d is \"\n+          \"addressable (global device id %d), but cross-host transfers must \"\n+          \"be between an addressable and a non-addressable device.\",\n+          i, dst_global_device_ids[i].value());\n+    }\n   }\n-  return out_futures;\n-}\n-\n-// Helpers used inside CrossHostSendBuffer to acquire a hold on a send buffer\n-// and get its raw buffer and definition events. This is used to ensure that the\n-// buffer is not deleted while the send is in progress.\n-struct HeldSendBuffer {\n-  tsl::RCReference<CommonPjRtRawBuffer> raw_buffer;\n-  std::vector<tsl::RCReference<tsl::AsyncValue>> definition_events;\n-};\n \n-absl::StatusOr<HeldSendBuffer> AcquireHeldSendBuffer(\n-    tsl::RCReference<PjRtDeviceEvent> usage_event,\n-    CommonPjRtBufferImpl* buffer_impl, const char* caller_name) {\n-  tsl::RCReference<CommonPjRtRawBuffer> raw_buffer;\n-  std::vector<tsl::RCReference<tsl::AsyncValue>> definition_events;\n-\n-  TF_RETURN_IF_ERROR(buffer_impl->AcquireScopedRawBuffer(\n-      [&](tsl::RCReference<CommonPjRtRawBuffer> buf_raw_buffer,\n-          std::vector<tsl::RCReference<tsl::AsyncValue>>\n-              buf_definition_events) mutable\n-          -> absl::StatusOr<tsl::RCReference<PjRtDeviceEvent>> {\n-        raw_buffer = std::move(buf_raw_buffer);\n-        usage_event->AndThen([raw_buffer]() {});\n-        definition_events = std::move(buf_definition_events);\n-        return usage_event;\n-      },\n-      caller_name));\n-\n-  return HeldSendBuffer{std::move(raw_buffer), std::move(definition_events)};\n-}\n+  // Create futures and promises.\n+  std::vector<Future<>> futures;\n+  std::vector<std::shared_ptr<Future<>::Promise>> promises;\n+  futures.reserve(buffers.size());\n+  promises.reserve(buffers.size());\n+  for (int i = 0; i < buffers.size(); ++i) {\n+    auto [promise, future] = Future<>::MakePromise();\n+    futures.push_back(std::move(future));\n+    promises.push_back(std::move(promise).ToShared());\n+  }\n \n-absl::StatusOr<Future<>> StreamExecutorGpuClient::CrossHostSendBuffer(\n-    PjRtBuffer* buffer, PjRtGlobalDeviceId dst_global_device_id,\n-    CrossHostTransferKey transfer_key) {\n-  // Get the default GpuCollectives instance.\n-  TF_ASSIGN_OR_RETURN(Collectives * collectives,\n-                      CollectivesRegistry::Default(\"gpu\"));\n-  gpu::GpuCollectives* gpu_collectives =\n-      tsl::down_cast<gpu::GpuCollectives*>(collectives);\n+  // Group the sends by local device.\n+  absl::flat_hash_map<PjRtDevice*, std::vector<int>> sends_by_device;\n+  for (int i = 0; i < buffers.size(); ++i) {\n+    sends_by_device[buffers[i]->device()].push_back(i);\n+  }\n \n-  // Get the local device and its id.\n-  PjRtStreamExecutorDevice* pjrt_se_device =\n-      tensorflow::down_cast<PjRtStreamExecutorDevice*>(buffer->device());\n-  TF_ASSIGN_OR_RETURN(LocalDeviceState * local_device,\n-                      pjrt_se_device->GetLocalDeviceState());\n-  PjRtGlobalDeviceId src_global_device_id = pjrt_se_device->global_device_id();\n+  // Execute sends for each local device.\n+  for (auto& [device, send_idxs] : sends_by_device) {\n+    // Execute sends.\n+    std::vector<PjRtBuffer*> curr_buffers;\n+    std::vector<PjRtGlobalDeviceId> curr_dst_ids;\n+    std::vector<CrossHostTransferKey> curr_transfer_keys;\n+    std::vector<std::shared_ptr<Future<>::Promise>> curr_promises;\n+    for (int idx : send_idxs) {\n+      curr_buffers.push_back(buffers[idx]);\n+      curr_dst_ids.push_back(dst_global_device_ids[idx]);\n+      curr_transfer_keys.push_back(transfer_keys[idx]);\n+      curr_promises.push_back(promises[idx]);\n+    }\n \n-  // Get the name of the transfer.\n-  std::string cross_host_transfer_name = CrossHostTransferName(\n-      src_global_device_id, dst_global_device_id, RunId(transfer_key.value()));\n+    ScheduleSendsOnLocalDevice(\n+        device, std::move(curr_buffers), std::move(curr_dst_ids),\n+        std::move(curr_transfer_keys), std::move(curr_promises));\n+  }\n \n-  // Get the buffer's shape.\n-  TF_ASSIGN_OR_RETURN(Shape shape, buffer->HostShape());\n+  return futures;\n+}\n \n-  auto [promise, future] = Future<>::MakePromise();\n+void StreamExecutorGpuClient::ScheduleSendsOnLocalDevice(\n+    PjRtDevice* device, std::vector<PjRtBuffer*> buffers,\n+    const std::vector<PjRtGlobalDeviceId> dst_global_device_ids,\n+    const std::vector<CrossHostTransferKey> transfer_keys,\n+    std::vector<std::shared_ptr<Future<>::Promise>> promises) {\n+  // Get the local device state, transfer stream, and prepare the send\n+  // buffers. We associate the group of sends with a single usage_event.\n+  LocalDeviceState* local_device_state;\n+  se::Stream* stream;\n+  std::vector<PreparedSend> prepared_sends;\n+  prepared_sends.reserve(buffers.size());\n+  tsl::RCReference<PjRtStreamExecutorDeviceEvent> usage_event;\n+\n+  auto setup_sends = [&]() -> absl::Status {\n+    TF_ASSIGN_OR_RETURN(local_device_state, GetLocalDeviceState(device));\n+    stream = local_device_state->GetDeviceToDeviceStream();\n+    gpu::GpuCollectives* gpu_collectives = gpu::GpuCollectives::Default();\n+    usage_event = tsl::MakeRef<PjRtStreamExecutorDeviceEvent>(\n+        BufferSequencingEvent::Create(this->thread_pool()));\n+\n+    gpu::AcquiredCliquesMap acquired_cliques_map;\n+    for (int i = 0; i < buffers.size(); ++i) {\n+      absl::StatusOr<PreparedSend> prepared_send = PrepareSend(\n+          this, gpu_collectives, stream, buffers[i], dst_global_device_ids[i],\n+          transfer_keys[i], acquired_cliques_map, promises[i], usage_event);\n+\n+      if (!prepared_send.ok()) {\n+        SetEventAsError(usage_event->event(), prepared_send.status());\n+        return prepared_send.status();\n+      }\n \n-  // Create an event to track when the send is done.\n-  auto usage_event = tsl::MakeRef<PjRtStreamExecutorDeviceEvent>(\n-      BufferSequencingEvent::Create(this->thread_pool()));\n+      prepared_sends.push_back(*std::move(prepared_send));\n+    }\n \n-  // Acquire a hold on the buffer and get some metadata.\n-  TF_ASSIGN_OR_RETURN(\n-      HeldSendBuffer held_send_buffer,\n-      AcquireHeldSendBuffer(\n-          usage_event, tensorflow::down_cast<CommonPjRtBufferImpl*>(buffer),\n-          \"CrossHostSendBuffer\"));\n+    return absl::OkStatus();\n+  };\n \n-  auto send = [this, gpu_collectives, promise = std::move(promise),\n-               usage_event = std::move(usage_event),\n-               held_send_buffer = std::move(held_send_buffer), local_device,\n-               cross_host_transfer_name, shape]() mutable {\n-    se::Stream* stream = local_device->GetDeviceToDeviceStream();\n+  if (absl::Status status = setup_sends(); !status.ok()) {\n+    FulfillPromises(promises, status);\n+    return;\n+  }\n \n-    auto f = [&]() -> absl::Status {\n+  // Form the closure called for each group of sends.\n+  auto launch_send_group = [](gpu::GpuCommunicator* gpu_communicator,\n+                              absl::Span<PreparedSend> prepared_sends,\n+                              se::Stream* stream) -> absl::Status {\n+    for (PreparedSend& prepared_send : prepared_sends) {\n       // Wait until the buffer we want to send is fully materialized.\n-      for (const auto& event : held_send_buffer.definition_events) {\n+      for (const auto& event : prepared_send.definition_events_) {\n         tsl::BlockUntilReady(event.get());\n-        if (auto* status = event->GetErrorIfPresent()) {\n+        if (auto* status = event->GetErrorIfPresent(); status != nullptr) {\n           return *status;\n         }\n       }\n-\n-      // Get the clique ID from the KV store.\n-      TF_ASSIGN_OR_RETURN(std::string descriptor,\n-                          kv_store_->Get(cross_host_transfer_name,\n-                                         cross_host_transfer_timeout_));\n-      CliqueId clique_id(descriptor);\n-\n-      // Create a communicator.\n-      TF_ASSIGN_OR_RETURN(\n-          std::unique_ptr<Communicator> communicator,\n-          CreateTransferCommunicator(local_device, gpu_collectives, clique_id,\n-                                     /*is_sender=*/true));\n-\n-      // Send data to the receiver.\n+      // Launch the send.\n       auto mem = tensorflow::down_cast<PjRtStreamExecutorRawBuffer*>(\n-                     held_send_buffer.raw_buffer.get())\n+                     prepared_send.raw_buffer_.get())\n                      ->device_buffer();\n+      TF_RETURN_IF_ERROR(gpu_communicator->LaunchSend(\n+          /*send_buffer=*/mem->mem(),\n+          /*dtype=*/xla::PrimitiveType::U8,\n+          /*count=*/mem->mem().size(),\n+          /*peer=*/RankId(1),\n+          /*executor=*/gpu::GpuCollectives::On(*stream)));\n+    }\n+    return absl::OkStatus();\n+  };\n \n-      Future<> send_future = communicator->Send(\n-          mem->mem(), shape.element_type(), ShapeUtil::ElementsIn(shape),\n-          RankId(0), gpu::GpuCollectives::On(*stream));\n-      TF_RETURN_IF_ERROR(send_future.Await());\n-\n-      // Mark send as done.\n-      TF_RETURN_IF_ERROR(\n-          AllocateAndRecordEvent(usage_event->event(), local_device, stream));\n-\n-      return absl::OkStatus();\n-    };\n-\n-    absl::Status s = f();\n-    if (!s.ok()) {\n-      SetEventAsError(usage_event->event(), s);\n+  // Form the closure to schedule on the device's execute thread.\n+  auto execute_sends_fn = [this, local_device_state, stream,\n+                           promises = std::move(promises),\n+                           prepared_sends = std::move(prepared_sends),\n+                           launch_send_group = std::move(launch_send_group),\n+                           usage_event = std::move(usage_event)]() mutable {\n+    // Group transfers by GPU clique.\n+    absl::flat_hash_map<gpu::GpuCliqueKey, std::vector<PreparedSend>>\n+        grouped_sends = GroupSendsByCliqueKey(std::move(prepared_sends));\n+\n+    // Transfers for a particular clique are executed as a group. This vector\n+    // holds group futures for each clique_key in grouped_sends.\n+    std::vector<Future<>> group_futures;\n+    group_futures.reserve(grouped_sends.size());\n+\n+    for (auto& [clique_key, curr_sends] : grouped_sends) {\n+      // Get the communicator on which we will execute this group of\n+      // transfers. We assume each clique key is associated with a unique\n+      // communicator, so we just take the communicator of the first\n+      // transfer_idx of this clique key.\n+      gpu::GpuCommunicator* gpu_communicator =\n+          curr_sends[0].clique_and_communicator_.second;\n+\n+      // Launch the group of transfers.\n+      group_futures.push_back(gpu_communicator->GroupExecute(\n+          [&launch_send_group, &curr_sends = curr_sends,\n+           stream](gpu::GpuCommunicator* gpu_comm) -> absl::Status {\n+            return launch_send_group(gpu_comm, absl::MakeSpan(curr_sends),\n+                                     stream);\n+          }));\n     }\n-    promise.Set(s);\n+\n+    // On a separate thread pool, await group futures and fulfill buffer\n+    // sequencing events and promises.\n+    Future<> all_sends_future = JoinFutures(group_futures);\n+\n+    all_sends_future.OnReady(\n+        *this->thread_pool()->AsExecutor(),\n+        [this, local_device_state, stream, promises = std::move(promises),\n+         usage_event, grouped_sends = std::move(grouped_sends)](\n+            const absl::Status& status) mutable {\n+          // Add usage_event onto the stream.\n+          absl::Status fulfill_status = FulfillDeviceEvent(\n+              this, local_device_state, stream, usage_event, status);\n+\n+          // Fail promises early if there was an issue.\n+          if (!status.ok() || !fulfill_status.ok()) {\n+            FulfillPromises(promises, status);\n+            return;\n+          }\n+\n+          // Asynchronously fulfill promises via a host callback, failing them\n+          // early if there is an issue registering the callback.\n+          absl::Status callback_status = RunCallbackOnStream(\n+              stream, this->thread_pool(), [promises]() mutable {\n+                FulfillPromises(promises, absl::OkStatus());\n+              });\n+\n+          if (!callback_status.ok()) {\n+            FulfillPromises(promises, callback_status);\n+          }\n+        });\n   };\n \n-  local_device->execute_thread()->Schedule(std::move(send));\n-  return future;\n+  // Schedule transfers on the execute thread.\n+  local_device_state->execute_thread()->Schedule(std::move(execute_sends_fn));\n }\n \n+// Prepare a receive buffer on a given device for receiving data as part of a\n+// cross-host transfer. This function (and the PrepareReceiveBufferResult helper\n+// struct) is used by the original cross-host transfers API\n+// (MakeCrossHostReceiveBuffers).\n absl::StatusOr<StreamExecutorGpuClient::PrepareReceiveBufferResult>\n StreamExecutorGpuClient::PrepareReceiveBuffer(PjRtDevice* device, Shape shape) {\n   TF_ASSIGN_OR_RETURN(auto* memory_space, device->default_memory_space());\n@@ -577,6 +919,7 @@ StreamExecutorGpuClient::PrepareReceiveBuffer(PjRtDevice* device, Shape shape) {\n                                     std::move(definition_event)};\n }\n \n+// Receive functionality for second cross-host transfers API.\n absl::StatusOr<std::vector<std::unique_ptr<PjRtBuffer>>>\n StreamExecutorGpuClient::CrossHostReceiveBuffers(\n     xla::PjRtDevice* device, absl::Span<const xla::Shape> shapes,\n@@ -593,91 +936,147 @@ StreamExecutorGpuClient::CrossHostReceiveBuffers(\n         \"transfer_keys must have the same length, but got %d, %d, and %d.\",\n         shapes.size(), src_global_device_ids.size(), transfer_keys.size());\n   }\n-\n-  // Perform receives.\n-  std::vector<std::unique_ptr<PjRtBuffer>> receive_buffers;\n-  receive_buffers.reserve(shapes.size());\n-  for (int i = 0; i < shapes.size(); ++i) {\n-    TF_ASSIGN_OR_RETURN(\n-        std::unique_ptr<PjRtBuffer> receive_buffer,\n-        CrossHostReceiveBuffer(shapes[i], device, src_global_device_ids[i],\n-                               transfer_keys[i]));\n-    receive_buffers.push_back(std::move(receive_buffer));\n+  // Each transfer must be between an addressable and a non-addressable\n+  // device. If both devices are addressable, then both a data transfer and a\n+  // 'normal' XLA SPMD executable may try to acquire the same GPU clique,\n+  // causing issues.\n+  if (!device->IsAddressable()) {\n+    return InvalidArgument(\n+        \"CrossHostReceiveBuffers: destination device is non-addressable \"\n+        \"(global device id %d), but cross-host transfers must be between an \"\n+        \"addressable and a non-addressable device.\",\n+        device->global_device_id().value());\n+  }\n+  for (int i = 0; i < src_global_device_ids.size(); ++i) {\n+    TF_ASSIGN_OR_RETURN(PjRtDevice * src_device,\n+                        LookupDevice(src_global_device_ids[i]));\n+    if (src_device->IsAddressable()) {\n+      return InvalidArgument(\n+          \"CrossHostReceiveBuffers: source device for buffer %d is \"\n+          \"addressable (global device id %d), but cross-host transfers must \"\n+          \"be between an addressable and a non-addressable device.\",\n+          i, src_global_device_ids[i].value());\n+    }\n   }\n-  return receive_buffers;\n-}\n-\n-absl::StatusOr<std::unique_ptr<PjRtBuffer>>\n-StreamExecutorGpuClient::CrossHostReceiveBuffer(\n-    xla::Shape shape, xla::PjRtDevice* device,\n-    PjRtGlobalDeviceId src_global_device_id,\n-    CrossHostTransferKey transfer_key) {\n-  // Get the default GpuCollectives instance.\n-  TF_ASSIGN_OR_RETURN(Collectives * collectives,\n-                      CollectivesRegistry::Default(\"gpu\"));\n-  gpu::GpuCollectives* gpu_collectives =\n-      tsl::down_cast<gpu::GpuCollectives*>(collectives);\n-\n-  // Get the name of the transfer.\n-  PjRtGlobalDeviceId dst_global_device_id = device->global_device_id();\n-  std::string cross_host_transfer_name = CrossHostTransferName(\n-      src_global_device_id, dst_global_device_id, RunId(transfer_key.value()));\n-\n-  TF_ASSIGN_OR_RETURN(\n-      StreamExecutorGpuClient::PrepareReceiveBufferResult receive_prep_result,\n-      PrepareReceiveBuffer(device, shape));\n-\n-  auto recv = [this, gpu_collectives, cross_host_transfer_name,\n-               local_device = receive_prep_result.local_device,\n-               definition_event = receive_prep_result.definition_event,\n-               stream = receive_prep_result.stream,\n-               raw_buffer = std::move(receive_prep_result.raw_buffer), shape,\n-               dtype = receive_prep_result.buffer->element_type()]() mutable {\n-    WaitForAllocation(stream, *raw_buffer);\n-    auto f = [&]() -> absl::Status {\n-      auto mem =\n-          tensorflow::down_cast<PjRtStreamExecutorRawBuffer*>(raw_buffer.get())\n-              ->device_buffer();\n-\n-      // Construct the clique ID and set the descriptor in the KV store.\n-      TF_ASSIGN_OR_RETURN(CliqueId clique_id,\n-                          gpu_collectives->CreateUniqueCliqueId());\n-      std::string descriptor = clique_id.ToString();\n-      TF_RETURN_IF_ERROR(kv_store_->Set(cross_host_transfer_name, descriptor));\n \n-      // Create a communicator.\n-      TF_ASSIGN_OR_RETURN(\n-          std::unique_ptr<Communicator> communicator,\n-          CreateTransferCommunicator(local_device, gpu_collectives, clique_id,\n-                                     /*is_sender=*/false));\n+  // Get the local device state, transfer stream, and prepare the receive\n+  // buffers. We associate the group of receives with a single definition_event.\n+  LocalDeviceState* local_device_state;\n+  se::Stream* stream;\n+  std::vector<std::unique_ptr<PjRtBuffer>> buffers;\n+  buffers.reserve(shapes.size());\n+  std::vector<PreparedReceive> prepared_receives;\n+  prepared_receives.reserve(shapes.size());\n+  tsl::RCReference<PjRtStreamExecutorDeviceEvent> definition_event;\n+\n+  auto setup_receives = [&]() -> absl::Status {\n+    TF_ASSIGN_OR_RETURN(local_device_state, GetLocalDeviceState(device));\n+    stream = local_device_state->GetDeviceToDeviceStream();\n+    TF_ASSIGN_OR_RETURN(PjRtMemorySpace * memory_space,\n+                        device->default_memory_space());\n+    gpu::GpuCollectives* gpu_collectives = gpu::GpuCollectives::Default();\n+    definition_event = tsl::MakeRef<PjRtStreamExecutorDeviceEvent>(\n+        BufferSequencingEvent::Create(this->thread_pool()));\n+\n+    gpu::AcquiredCliquesMap acquired_cliques_map;\n+    for (int i = 0; i < shapes.size(); ++i) {\n+      absl::StatusOr<PreparedReceive> prepared_receive =\n+          PrepareReceive(this, gpu_collectives, stream, device, memory_space,\n+                         src_global_device_ids[i], transfer_keys[i], shapes[i],\n+                         acquired_cliques_map, definition_event);\n+\n+      if (!prepared_receive.ok()) {\n+        SetEventAsError(definition_event->event(), prepared_receive.status());\n+        return prepared_receive.status();\n+      }\n \n-      // Receive data from the sender.\n-      Future<> recv_future = communicator->Recv(\n-          mem->mem(), shape.element_type(), ShapeUtil::ElementsIn(shape),\n-          RankId(1), gpu::GpuCollectives::On(*stream));\n-      TF_RETURN_IF_ERROR(recv_future.Await());\n+      buffers.push_back(std::move(prepared_receive->buffer_));\n+      prepared_receives.push_back(*std::move(prepared_receive));\n+    }\n \n-      // Keep mem alive until the Recv has finished executing. Note that\n-      // recv_event is fulfilled when the receive is enqueued, but not\n-      // necessarily executed.\n-      definition_event.AndThen([mem]() {});\n+    return absl::OkStatus();\n+  };\n+  TF_RETURN_IF_ERROR(setup_receives());\n+\n+  // Form the closure called for each group of receives.\n+  auto launch_receive_group = [this](\n+                                  gpu::GpuCommunicator* gpu_communicator,\n+                                  absl::Span<PreparedReceive> prepared_receives,\n+                                  se::Stream* stream) -> absl::Status {\n+    for (PreparedReceive& prepared_receive : prepared_receives) {\n+      // Wait until the receive buffer is allocated.\n+      WaitForAllocation(stream, *prepared_receive.raw_buffer_);\n+\n+      // Launch the receive.\n+      auto mem = tensorflow::down_cast<PjRtStreamExecutorRawBuffer*>(\n+                     prepared_receive.raw_buffer_.get())\n+                     ->device_buffer();\n+      TF_RETURN_IF_ERROR(gpu_communicator->LaunchRecv(\n+          /*recv_buffer=*/mem->mem(),\n+          /*dtype=*/xla::PrimitiveType::U8,\n+          /*count=*/mem->mem().size(),\n+          /*peer=*/RankId(0),\n+          /*executor=*/gpu::GpuCollectives::On(*stream)));\n+    }\n+    return absl::OkStatus();\n+  };\n \n-      // Set definition event.\n-      TF_RETURN_IF_ERROR(\n-          AllocateAndRecordEvent(definition_event, local_device, stream));\n+  // Form the closure to schedule on the device's execute thread.\n+  auto execute_receives_fn =\n+      [this, local_device_state, stream,\n+       prepared_receives = std::move(prepared_receives),\n+       launch_receive_group = std::move(launch_receive_group),\n+       definition_event = std::move(definition_event)]() mutable {\n+        // Group transfers by GPU clique.\n+        absl::flat_hash_map<gpu::GpuCliqueKey, std::vector<PreparedReceive>>\n+            grouped_receives =\n+                GroupReceivesByCliqueKey(std::move(prepared_receives));\n+\n+        // Transfers for a particular clique are executed as a group. This\n+        // vector holds group futures for each clique_key in grouped_receives.\n+        std::vector<Future<>> group_futures;\n+        group_futures.reserve(grouped_receives.size());\n+\n+        for (auto& [clique_key, curr_receives] : grouped_receives) {\n+          // Get the communicator on which we will execute this group of\n+          // transfers. We assume each clique key is associated with a unique\n+          // communicator, so we just take the communicator of the first\n+          // transfer_idx of this clique key.\n+          gpu::GpuCommunicator* gpu_communicator =\n+              curr_receives[0].clique_and_communicator_.second;\n+\n+          // Launch the group of transfers.\n+          group_futures.push_back(gpu_communicator->GroupExecute(\n+              [&launch_receive_group, &curr_receives = curr_receives,\n+               stream](gpu::GpuCommunicator* gpu_comm) -> absl::Status {\n+                return launch_receive_group(\n+                    gpu_comm, absl::MakeSpan(curr_receives), stream);\n+              }));\n+        }\n \n-      return absl::OkStatus();\n-    };\n+        // On a separate thread pool, await group futures and fulfill buffer\n+        // sequencing events and promises.\n+        Future<> all_receives_future = JoinFutures(group_futures);\n+\n+        all_receives_future.OnReady(\n+            *this->thread_pool()->AsExecutor(),\n+            [this, local_device_state, stream,\n+             grouped_receives = std::move(grouped_receives),\n+             definition_event = std::move(definition_event)](\n+                const absl::Status& status) mutable {\n+              CHECK_OK(FulfillDeviceEvent(this, local_device_state, stream,\n+                                          definition_event, status));\n+            });\n+      };\n \n-    if (absl::Status s = f(); !s.ok()) {\n-      SetEventAsError(definition_event, s);\n-    }\n-  };\n-  receive_prep_result.local_device->execute_thread()->Schedule(std::move(recv));\n+  // Schedule transfers on the execute thread.\n+  local_device_state->execute_thread()->Schedule(\n+      std::move(execute_receives_fn));\n \n-  return std::move(receive_prep_result.buffer);\n+  return buffers;\n }\n \n+// Send functionality for original cross-host transfers API.\n void StreamExecutorGpuClient::ScheduleRemoteSend(\n     PjRtMemorySpace* memory_space,\n     tsl::RCReference<CommonPjRtRawBuffer> raw_buffer,\n@@ -769,6 +1168,7 @@ void StreamExecutorGpuClient::ScheduleRemoteSend(\n       tsl::MakeRef<PjRtStreamExecutorDeviceEvent>(std::move(usage_event)));\n }\n \n+// Receive functionality for original cross-host transfers API.\n absl::StatusOr<std::vector<std::unique_ptr<PjRtBuffer>>>\n StreamExecutorGpuClient::MakeCrossHostReceiveBuffers(\n     absl::Span<const Shape> shapes, PjRtDevice* device,\n@@ -861,6 +1261,8 @@ StreamExecutorGpuClient::MakeCrossHostReceiveBuffers(\n   return buffers;\n }\n \n+// ==== End cross-host transfer implementations ==== //\n+\n absl::StatusOr<const xla::PjRtTopologyDescription*>\n StreamExecutorGpuClient::GetTopologyDescription() const {\n   if (!topology_.has_value()) {"
        },
        {
            "sha": "b43592589b9cf3751a97b7d3bc77409ccbd447ed",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.h",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a3e9e774068a7b8c0428c3f66f7825a8980d4fea/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a3e9e774068a7b8c0428c3f66f7825a8980d4fea/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h?ref=a3e9e774068a7b8c0428c3f66f7825a8980d4fea",
            "patch": "@@ -33,6 +33,8 @@ limitations under the License.\n #include \"absl/time/time.h\"\n #include \"absl/types/span.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n+#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/backends/gpu/collectives/gpu_cliques.h\"\n #include \"xla/client/local_client.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/future.h\"\n@@ -49,6 +51,7 @@ limitations under the License.\n #include \"xla/pjrt/pjrt_executable.h\"\n #include \"xla/pjrt/pjrt_stream_executor_client.h\"\n #include \"xla/pjrt/plugin/xla_gpu/xla_gpu_client_options.h\"\n+#include \"xla/pjrt/se_raw_buffer.h\"\n #include \"xla/pjrt/tracked_device_buffer.h\"\n #include \"xla/runtime/device_id.h\"\n #include \"xla/service/computation_placer.h\"\n@@ -205,9 +208,11 @@ class StreamExecutorGpuClient : public xla::PjRtStreamExecutorClient {\n   // Helpers for cross host transfers.\n   absl::Duration cross_host_transfer_timeout_ = absl::Minutes(3);\n \n-  absl::StatusOr<Future<>> CrossHostSendBuffer(\n-      PjRtBuffer* buffer, PjRtGlobalDeviceId dst_global_device_id,\n-      CrossHostTransferKey transfer_key);\n+  void ScheduleSendsOnLocalDevice(\n+      PjRtDevice* device, std::vector<PjRtBuffer*> buffers,\n+      std::vector<PjRtGlobalDeviceId> dst_global_device_ids,\n+      std::vector<CrossHostTransferKey> transfer_keys,\n+      std::vector<std::shared_ptr<Future<>::Promise>> promises);\n \n   struct PrepareReceiveBufferResult {\n     std::unique_ptr<PjRtBuffer> buffer;\n@@ -219,11 +224,6 @@ class StreamExecutorGpuClient : public xla::PjRtStreamExecutorClient {\n \n   absl::StatusOr<PrepareReceiveBufferResult> PrepareReceiveBuffer(\n       PjRtDevice* device, Shape shape);\n-\n-  absl::StatusOr<std::unique_ptr<PjRtBuffer>> CrossHostReceiveBuffer(\n-      xla::Shape shape, xla::PjRtDevice* device,\n-      PjRtGlobalDeviceId src_global_device_ids,\n-      CrossHostTransferKey transfer_keys);\n };\n \n std::vector<std::unique_ptr<PjRtStreamExecutorDevice>> BuildLocalDevices("
        },
        {
            "sha": "d563cccbce6828c4affbedf2d2eb3e58c9f0adb5",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client_test.cc",
            "status": "modified",
            "additions": 53,
            "deletions": 9,
            "changes": 62,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a3e9e774068a7b8c0428c3f66f7825a8980d4fea/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a3e9e774068a7b8c0428c3f66f7825a8980d4fea/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc?ref=a3e9e774068a7b8c0428c3f66f7825a8980d4fea",
            "patch": "@@ -2932,6 +2932,51 @@ TEST(StreamExecutorGpuClientTest, FailedCrossHostSendArgsSizeMismatch) {\n                            \"must have the same length, but got 1, 1, and 2.\")));\n }\n \n+TEST(StreamExecutorGpuClientTest, FailedCrossHostTransferSrcAndDstAddressable) {\n+  // Create the client.\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<PjRtClient> client,\n+                          GetStreamExecutorGpuClient(DefaultOptions()));\n+\n+  // Create a buffer to try to send.\n+  std::vector<int32_t> data(256);\n+  absl::c_iota(data, 1);\n+\n+  Shape shape = ShapeUtil::MakeShape(S32, {256});\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<PjRtBuffer> buffer,\n+      client->BufferFromHostBuffer(\n+          data.data(), shape.element_type(), shape.dimensions(),\n+          /*byte_strides=*/std::nullopt,\n+          PjRtClient::HostBufferSemantics::kImmutableOnlyDuringCall, nullptr,\n+          *client->addressable_devices()[0]->default_memory_space(),\n+          /*device_layout=*/nullptr));\n+\n+  // Try to transfer some data between two addressable devices.\n+  EXPECT_THAT(\n+      client->CrossHostSendBuffers({buffer.get()}, {PjRtGlobalDeviceId(1)},\n+                                   {CrossHostTransferKey(0)}),\n+      absl_testing::StatusIs(\n+          absl::StatusCode::kInvalidArgument,\n+          ::testing::StrEq(\n+              \"CrossHostSendBuffers: destination device for buffer 0 is \"\n+              \"addressable (global device id 1), but cross-host transfers must \"\n+              \"be between an addressable and a non-addressable device.\")));\n+\n+  EXPECT_THAT(\n+      client->CrossHostReceiveBuffers(\n+          /*device=*/client->addressable_devices()[0],\n+          /*shapes=*/{shape},\n+          /*src_global_device_ids=*/{PjRtGlobalDeviceId(1)},\n+          /*transfer_keys=*/{CrossHostTransferKey(0)}),\n+      absl_testing::StatusIs(\n+          absl::StatusCode::kInvalidArgument,\n+          ::testing::StrEq(\n+              \"CrossHostReceiveBuffers: source device for buffer 0 is \"\n+              \"addressable (global device id 1), but cross-host transfers must \"\n+              \"be between an addressable and a non-addressable device.\")));\n+}\n+\n TEST(StreamExecutorGpuClientTest, FailedCrossHostReceiveArgsSizeMismatch) {\n   // Create the client.\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<PjRtClient> client,\n@@ -3076,13 +3121,14 @@ absl::Status SuccessfulCrossHostTransferTestBody(bool is_sender,\n   // Sender logic.\n   if (is_sender) {\n     LOG(INFO) << log_prefix << \": creating buffers\";\n-    std::vector<int32_t> data(256);\n-    absl::c_iota(data, 1);\n-    Shape shape = ShapeUtil::MakeShape(S32, {256});\n \n     // Create the data to send.\n+    Shape shape = ShapeUtil::MakeShape(S32, {256});\n     std::vector<std::unique_ptr<PjRtBuffer>> buffers;\n     for (int i = 0; i < num_arrays; ++i) {\n+      std::vector<int32_t> data(256);\n+      absl::c_iota(data, 1000 * i);\n+\n       TF_ASSIGN_OR_RETURN(\n           std::unique_ptr<PjRtBuffer> buffer,\n           client->BufferFromHostBuffer(\n@@ -3121,12 +3167,6 @@ absl::Status SuccessfulCrossHostTransferTestBody(bool is_sender,\n     }\n   } else {\n     // Receiver logic.\n-    // Expected data to receive.\n-    std::vector<int32_t> expected_data(256);\n-    absl::c_iota(expected_data, 1);\n-    auto expected_literal = LiteralUtil::CreateR1<int32_t>(expected_data);\n-\n-    // Receive some data.\n     std::vector<Shape> shapes;\n     std::vector<PjRtGlobalDeviceId> src_device_ids;\n     std::vector<CrossHostTransferKey> transfer_keys;\n@@ -3149,6 +3189,10 @@ absl::Status SuccessfulCrossHostTransferTestBody(bool is_sender,\n     EXPECT_EQ(receive_buffers.size(), num_arrays);\n \n     for (int i = 0; i < num_arrays; ++i) {\n+      std::vector<int32_t> expected_data(256);\n+      absl::c_iota(expected_data, 1000 * i);\n+      auto expected_literal = LiteralUtil::CreateR1<int32_t>(expected_data);\n+\n       LOG(INFO) << log_prefix << \": waiting for receive \" << i\n                 << \" to complete\";\n       TF_RETURN_IF_ERROR(receive_buffers[i]->GetReadyFuture().Await());"
        }
    ],
    "stats": {
        "total": 862,
        "additions": 655,
        "deletions": 207
    }
}