{
    "author": "seantalts",
    "message": "[XLA:CPU] Make XLA's fast tanh return exactly +/- 1.0 for +/- infinity.\n\nModify the `EmitFastTanhF32` and `EmitFastTanhF64` intrinsics to handle infinite inputs. Instead of relying on the polynomial approximation, which can result in values slightly off from +/- 1.0, explicitly check for +/- infinity and return exactly +/- 1.0. This aligns the behavior for infinite inputs with standard library `tanh` functions.\n\nPiperOrigin-RevId: 797590426",
    "sha": "e5485599a79922aa14f799d2767b5367944d04b0",
    "files": [
        {
            "sha": "b6c77be69298a39dcd0e2d87914fe43af788a781",
            "filename": "third_party/xla/xla/codegen/intrinsic/tanh.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 3,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e5485599a79922aa14f799d2767b5367944d04b0/third_party%2Fxla%2Fxla%2Fcodegen%2Fintrinsic%2Ftanh.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e5485599a79922aa14f799d2767b5367944d04b0/third_party%2Fxla%2Fxla%2Fcodegen%2Fintrinsic%2Ftanh.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fintrinsic%2Ftanh.cc?ref=e5485599a79922aa14f799d2767b5367944d04b0",
            "patch": "@@ -31,6 +31,21 @@ namespace xla::codegen::intrinsics {\n \n namespace {\n \n+// Handles +/- infinity cases for tanh, returning +/- 1.0 respectively.\n+// For any other input, returns the given `result`.\n+llvm::Value* HandleInfinity(llvm::IRBuilderBase* b, llvm::Value* input,\n+                            llvm::Value* result) {\n+  llvm::Type* type = input->getType();\n+  llvm::Value* abs_input =\n+      llvm_ir::EmitCallToIntrinsic(llvm::Intrinsic::fabs, {input}, {type}, b);\n+  llvm::Value* is_inf_mask =\n+      b->CreateFCmpOEQ(abs_input, llvm::ConstantFP::getInfinity(type, false));\n+  llvm::Value* one = llvm::ConstantFP::get(type, 1.0);\n+  llvm::Value* inf_result = llvm_ir::EmitCallToIntrinsic(\n+      llvm::Intrinsic::copysign, {one, input}, {type}, b);\n+  return b->CreateSelect(is_inf_mask, inf_result, result);\n+}\n+\n llvm::Value* EmitFastTanh(llvm::IRBuilderBase* b, llvm::Value* input,\n                           bool with_fma) {\n   llvm::Type* type = input->getType();\n@@ -85,8 +100,10 @@ llvm::Value* EmitFastTanh(llvm::IRBuilderBase* b, llvm::Value* input,\n                       llvm::ConstantFP::get(type, denominator_coeffs[i]));\n   }\n \n-  return b->CreateSelect(use_aprox, input,\n-                         b->CreateFDiv(numerator, denominator));\n+  llvm::Value* result =\n+      b->CreateSelect(use_aprox, input, b->CreateFDiv(numerator, denominator));\n+\n+  return HandleInfinity(b, input, result);\n }\n \n llvm::Value* EmitFastTanhF64(llvm::IRBuilderBase* b, llvm::Value* input,\n@@ -141,7 +158,9 @@ llvm::Value* EmitFastTanhF64(llvm::IRBuilderBase* b, llvm::Value* input,\n   }\n \n   // Divide the numerator by the denominator.\n-  return b->CreateFDiv(numerator, denominator);\n+  llvm::Value* result = b->CreateFDiv(numerator, denominator);\n+\n+  return HandleInfinity(b, input, result);\n }\n \n }  // namespace"
        },
        {
            "sha": "f07da63f49abad88b3baf028ef24d899f83fd7d6",
            "filename": "third_party/xla/xla/codegen/intrinsic/tanh_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e5485599a79922aa14f799d2767b5367944d04b0/third_party%2Fxla%2Fxla%2Fcodegen%2Fintrinsic%2Ftanh_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e5485599a79922aa14f799d2767b5367944d04b0/third_party%2Fxla%2Fxla%2Fcodegen%2Fintrinsic%2Ftanh_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fintrinsic%2Ftanh_test.cc?ref=e5485599a79922aa14f799d2767b5367944d04b0",
            "patch": "@@ -71,6 +71,8 @@ TEST(TanhTest, EmitTanhF32) {\n                   std::numeric_limits<float>::infinity(),\n                   std::numeric_limits<float>::quiet_NaN()};\n   auto* fn = jit.GetScalarFn<float(float)>(Tanh::Name(type));\n+  EXPECT_THAT(fn(std::numeric_limits<float>::infinity()),\n+              NearUlps<float>(1.0, 0));\n   for (float val : vals) {\n     float actual = fn(val);\n     float expected = std::tanh(val);\n@@ -106,6 +108,8 @@ TEST(TanhTest, EmitTanhF64) {\n                    std::numeric_limits<double>::infinity(),\n                    std::numeric_limits<double>::quiet_NaN()};\n   auto* fn = jit.GetScalarFn<double(double)>(Tanh::Name(type));\n+  EXPECT_THAT(fn(std::numeric_limits<double>::infinity()),\n+              NearUlps<double>(1.0, 0));\n   for (double val : vals) {\n     double actual = fn(val);\n     double expected = std::tanh(val);"
        }
    ],
    "stats": {
        "total": 29,
        "additions": 26,
        "deletions": 3
    }
}