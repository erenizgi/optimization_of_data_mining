{
    "author": "pifon2a",
    "message": "[XLA:GPU] Propagate squeeze-dims through scf.if.\n\nPiperOrigin-RevId: 801761596",
    "sha": "8d810287b33d2ab2a68e6027b20239413a5ff8ac",
    "files": [
        {
            "sha": "3f003b38d2c101ebe8601b2a8513d4693a4da8b9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_squeeze_dims.mlir",
            "status": "modified",
            "additions": 69,
            "deletions": 2,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d810287b33d2ab2a68e6027b20239413a5ff8ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_squeeze_dims.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d810287b33d2ab2a68e6027b20239413a5ff8ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_squeeze_dims.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_squeeze_dims.mlir?ref=8d810287b33d2ab2a68e6027b20239413a5ff8ac",
            "patch": "@@ -1,7 +1,7 @@\n // RUN: xla-opt %s --triton-xla-squeeze-dims=\"finalize=false\" \\\n-// RUN: | FileCheck %s\n+// RUN: --split-input-file | FileCheck %s\n \n-// RUN: xla-opt %s --triton-xla-squeeze-dims \\\n+// RUN: xla-opt %s --triton-xla-squeeze-dims --split-input-file \\\n // RUN: | FileCheck %s --check-prefix=FINALIZE\n \n // CHECK-LABEL: func @push_squeeze_dims_up_through_elementwise\n@@ -12,6 +12,8 @@ tt.func @push_squeeze_dims_up_through_elementwise(%arg0: tensor<4x1x8xf32>) -> t\n   tt.return %1 : tensor<4x8xf32>\n }\n \n+// -----\n+\n // CHECK-LABEL: func @push_squeeze_dims_up_through_multiple_results\n tt.func @push_squeeze_dims_up_through_multiple_results(%arg0: tensor<4x1x8xf32>) -> tensor<4x8xf32> {\n   // CHECK: tt.elementwise_inline_asm {{.*}} : tensor<4x8xf32> -> tensor<4x8xf32>, tensor<4x8xf32>\n@@ -20,6 +22,8 @@ tt.func @push_squeeze_dims_up_through_multiple_results(%arg0: tensor<4x1x8xf32>)\n   tt.return %1 : tensor<4x8xf32>\n }\n \n+// -----\n+\n // CHECK-LABEL: func @push_squeeze_dims_up_through_broadcast\n tt.func @push_squeeze_dims_up_through_broadcast(%arg0: tensor<1x4x1x8xf32>) -> tensor<4x16x8xf32> {\n   // CHECK: tt.broadcast {{.*}} : tensor<4x1x8xf32> -> tensor<4x16x8xf32>\n@@ -28,6 +32,8 @@ tt.func @push_squeeze_dims_up_through_broadcast(%arg0: tensor<1x4x1x8xf32>) -> t\n   tt.return %1 : tensor<4x16x8xf32>\n }\n \n+// -----\n+\n // CHECK-LABEL: func @push_squeeze_dims_up_through_trans\n tt.func @push_squeeze_dims_up_through_trans(%arg0: tensor<4x1x8xf32>) -> tensor<8x4xf32> {\n   // CHECK: tt.trans {{.*}} {order = array<i32: 1, 0>} : tensor<4x8xf32> -> tensor<8x4xf32>\n@@ -36,6 +42,8 @@ tt.func @push_squeeze_dims_up_through_trans(%arg0: tensor<4x1x8xf32>) -> tensor<\n   tt.return %1 : tensor<8x4xf32>\n }\n \n+// -----\n+\n // CHECK-LABEL: func @push_squeeze_dims_up_through_join\n tt.func @push_squeeze_dims_up_through_join(%arg0: tensor<1x4xf32>, %arg1: tensor<1x4xf32>) -> tensor<4x2xf32> {\n   // CHECK-DAG: tt.join {{.*}} : tensor<4xf32> -> tensor<4x2xf32>\n@@ -44,6 +52,8 @@ tt.func @push_squeeze_dims_up_through_join(%arg0: tensor<1x4xf32>, %arg1: tensor\n   tt.return %1 : tensor<4x2xf32>\n }\n \n+// -----\n+\n // CHECK-LABEL: func @push_squeeze_dims_up_through_reduce\n tt.func @push_squeeze_dims_up_through_reduce(%arg0: tensor<8x4x1xf32>) -> tensor<8xf32> {\n   // CHECK: \"tt.reduce\"({{.*}}) <{axis = 1 : i32}> ({\n@@ -57,6 +67,8 @@ tt.func @push_squeeze_dims_up_through_reduce(%arg0: tensor<8x4x1xf32>) -> tensor\n   tt.return %2 : tensor<8xf32>\n }\n \n+// -----\n+\n // CHECK-LABEL: func @fold_squeeze_of_expand_cancelling\n tt.func @fold_squeeze_of_expand_cancelling(%arg0: tensor<4x8xf32>) -> tensor<4x8xf32> {\n   // CHECK-NOT: tt.expand_dims\n@@ -66,6 +78,8 @@ tt.func @fold_squeeze_of_expand_cancelling(%arg0: tensor<4x8xf32>) -> tensor<4x8\n   tt.return %1 : tensor<4x8xf32>\n }\n \n+// -----\n+\n // CHECK-LABEL: func @fold_squeeze_of_expand_swapping\n tt.func @fold_squeeze_of_expand_swapping(%arg0: tensor<4x1x8xf32>) -> tensor<1x4x8xf32> {\n   // CHECK: triton_xla.squeeze_dims {{.*}} {axis = 1 : i32} : tensor<4x1x8xf32> -> tensor<4x8xf32>\n@@ -75,6 +89,8 @@ tt.func @fold_squeeze_of_expand_swapping(%arg0: tensor<4x1x8xf32>) -> tensor<1x4\n   tt.return %1 : tensor<1x4x8xf32>\n }\n \n+// -----\n+\n // CHECK-LABEL: func @squeeze_reshape\n tt.func @squeeze_reshape(%arg0: tensor<4x1x1xf32>) -> tensor<4xf32> {\n   // CHECK: triton_xla.squeeze_dims {{.*}} {axis = 1 : i32} : tensor<4x1x1xf32> -> tensor<4x1xf32>\n@@ -83,6 +99,8 @@ tt.func @squeeze_reshape(%arg0: tensor<4x1x1xf32>) -> tensor<4xf32> {\n   tt.return %0 : tensor<4xf32>\n }\n \n+// -----\n+\n // CHECK-LABEL: func @expand_reshape\n tt.func @expand_reshape(%arg0: tensor<4xf32>) -> tensor<4x1x1xf32> {\n   %0 = tt.reshape %arg0 : tensor<4xf32> -> tensor<4x1x1xf32>\n@@ -91,6 +109,8 @@ tt.func @expand_reshape(%arg0: tensor<4xf32>) -> tensor<4x1x1xf32> {\n   tt.return %0 : tensor<4x1x1xf32>\n }\n \n+// -----\n+\n // CHECK-LABEL: func @skip_reshape_with_attr\n tt.func @skip_reshape_with_attr(%arg0: tensor<4x1xf32>) -> tensor<4xf32> {\n   // CHECK-NOT: triton_xla.squeeze_dims\n@@ -99,6 +119,8 @@ tt.func @skip_reshape_with_attr(%arg0: tensor<4x1xf32>) -> tensor<4xf32> {\n   tt.return %0 : tensor<4xf32>\n }\n \n+// -----\n+\n #arg_enc = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>\n #res_enc = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"ttg.num-ctas\" = 1 : i32, \"ttg.num-warps\" = 1 : i32} {\n@@ -112,6 +134,8 @@ tt.func @reshape_with_encoding(%arg0: tensor<1x32xf32, #arg_enc>) -> tensor<32xf\n }\n }\n \n+// -----\n+\n // CHECK-LABEL: func @fold_squeeze_dims_of_load_ptr\n tt.func @fold_squeeze_dims_of_load_ptr(%arg0: !tt.ptr<f32>, %arg1: i32) -> tensor<4x8xf32> {\n   %c0_i32 = arith.constant 0 : i32\n@@ -131,6 +155,8 @@ tt.func @fold_squeeze_dims_of_load_ptr(%arg0: !tt.ptr<f32>, %arg1: i32) -> tenso\n   tt.return %2 : tensor<4x8xf32>\n }\n \n+// -----\n+\n // CHECK-LABEL: func @squeeze_dims_of_load_ptr_with_boundary_check\n tt.func @squeeze_dims_of_load_ptr_with_boundary_check(%arg0: !tt.ptr<f32>) -> tensor<4x8xf32> {\n   %c0_i32 = arith.constant 0 : i32\n@@ -145,6 +171,8 @@ tt.func @squeeze_dims_of_load_ptr_with_boundary_check(%arg0: !tt.ptr<f32>) -> te\n   tt.return %2 : tensor<4x8xf32>\n }\n \n+// -----\n+\n // CHECK-LABEL: func @squeeze_dims_of_load_ptr_with_mask\n tt.func @squeeze_dims_of_load_ptr_with_mask(%arg0: !tt.ptr<f32>, %arg1: tensor<4x1x8xi1>) -> tensor<4x8xf32> {\n   %c0_i32 = arith.constant 0 : i32\n@@ -159,6 +187,8 @@ tt.func @squeeze_dims_of_load_ptr_with_mask(%arg0: !tt.ptr<f32>, %arg1: tensor<4\n   tt.return %2 : tensor<4x8xf32>\n }\n \n+// -----\n+\n // CHECK-LABEL: func @squeeze_store\n tt.func @squeeze_store(%arg0: !tt.ptr<f32>, %arg1: tensor<4x1x8xf32>) {\n   %c0_i32 = arith.constant 0 : i32\n@@ -177,6 +207,8 @@ tt.func @squeeze_store(%arg0: !tt.ptr<f32>, %arg1: tensor<4x1x8xf32>) {\n   tt.return\n }\n \n+// -----\n+\n // CHECK-LABEL: func @squeeze_store_unit_tensor\n tt.func @squeeze_store_unit_tensor(%arg0: !tt.ptr<f32>, %arg1: tensor<1x1xf32>) {\n   %c0_i32 = arith.constant 0 : i32\n@@ -188,6 +220,8 @@ tt.func @squeeze_store_unit_tensor(%arg0: !tt.ptr<f32>, %arg1: tensor<1x1xf32>)\n   tt.return\n }\n \n+// -----\n+\n // CHECK-LABEL: func @squeeze_store_with_mask\n tt.func @squeeze_store_with_mask(%arg0: !tt.ptr<f32>, %arg1: tensor<4x1xf32>, %arg2: tensor<4x1xi1>) {\n   %c0_i32 = arith.constant 0 : i32\n@@ -200,6 +234,8 @@ tt.func @squeeze_store_with_mask(%arg0: !tt.ptr<f32>, %arg1: tensor<4x1xf32>, %a\n   tt.return\n }\n \n+// -----\n+\n // CHECK-LABEL: func @reorder_squeeze_dims\n tt.func @reorder_squeeze_dims(%arg0: tensor<4x1x8x1xf32>) -> tensor<4x8xf32> {\n   // CHECK: triton_xla.squeeze_dims {{.*}} {axis = 1 : i32} : tensor<4x1x8x1xf32> -> tensor<4x8x1xf32>\n@@ -209,6 +245,8 @@ tt.func @reorder_squeeze_dims(%arg0: tensor<4x1x8x1xf32>) -> tensor<4x8xf32> {\n   tt.return %1 : tensor<4x8xf32>\n }\n \n+// -----\n+\n // CHECK-LABEL: func @diamond\n tt.func @diamond(%arg0: tensor<4x1x8xf32>) -> tensor<4x8xf32> {\n   // CHECK-NOT: arith.negf {{.*}} : tensor<4x1x8xf32>\n@@ -218,6 +256,8 @@ tt.func @diamond(%arg0: tensor<4x1x8xf32>) -> tensor<4x8xf32> {\n   tt.return %2 : tensor<4x8xf32>\n }\n \n+// -----\n+\n // CHECK-LABEL: func @insert_expand_dims\n tt.func @insert_expand_dims(%arg0: tensor<4x1xf32>) -> tensor<4xf32> {\n   // CHECK: %[[NEGF:.*]] = arith.negf {{.*}} : tensor<4xf32>\n@@ -235,6 +275,33 @@ tt.func @insert_expand_dims(%arg0: tensor<4x1xf32>) -> tensor<4xf32> {\n   tt.return %5 : tensor<4xf32>\n }\n \n+// -----\n+\n+// CHECK-LABEL: func @push_squeeze_dims_up_through_if\n+tt.func @push_squeeze_dims_up_through_if(%arg0: tensor<16xf32>,\n+    %arg1: tensor<16xf32>, %arg2: tensor<4x1xf32>, %arg3: tensor<4x1xf32>,\n+    %cond: i1) -> (tensor<16xf32>, tensor<4xf32>) {\n+  %if:2 = scf.if %cond -> (tensor<16xf32>, tensor<4x1xf32>) {\n+    scf.yield %arg0, %arg2 : tensor<16xf32>, tensor<4x1xf32>\n+  } else {\n+    scf.yield %arg1, %arg3 : tensor<16xf32>, tensor<4x1xf32>\n+  }\n+  %squeeze = triton_xla.squeeze_dims %if#1 {axis = 1 : i32}\n+    : tensor<4x1xf32> -> tensor<4xf32>\n+  tt.return %if#0, %squeeze : tensor<16xf32>, tensor<4xf32>\n+}\n+// CHECK:        scf.if %{{.*}} -> (tensor<16xf32>, tensor<4xf32>) {\n+// CHECK-NEXT:    %[[SQUEEZE:.*]] = triton_xla.squeeze_dims\n+// CHECK-NEXT:    scf.yield %arg0, %[[SQUEEZE]] : tensor<16xf32>, tensor<4xf32>\n+// CHECK-NEXT:   } else {\n+// CHECK-NEXT:    %[[SQUEEZE:.*]] = triton_xla.squeeze_dims\n+// CHECK-NEXT:    scf.yield %arg1, %[[SQUEEZE]] : tensor<16xf32>, tensor<4xf32>\n+// CHECK-NEXT:  }\n+// CHECK-NOT:   triton_xla.squeeze_dims\n+// CHECK:       tt.return\n+\n+// -----\n+\n // FINALIZE-LABEL: func @squeeze_dims_to_reshape\n tt.func @squeeze_dims_to_reshape(%arg0: tensor<4x1x8xf32>) -> tensor<4x8xf32> {\n   // FINALIZE: tt.reshape {{.*}} : tensor<4x1x8xf32> -> tensor<4x8xf32>"
        },
        {
            "sha": "12fde574097c693b88f6f02dc786e788eb8b5e5b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_squeeze_dims_pass.cc",
            "status": "modified",
            "additions": 56,
            "deletions": 2,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8d810287b33d2ab2a68e6027b20239413a5ff8ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_squeeze_dims_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8d810287b33d2ab2a68e6027b20239413a5ff8ac/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_squeeze_dims_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_squeeze_dims_pass.cc?ref=8d810287b33d2ab2a68e6027b20239413a5ff8ac",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/IR/Attributes.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n@@ -486,6 +487,58 @@ LogicalResult PushSqueezeDimsUpThroughExpandDims(SqueezeDimsOp op,\n   return success();\n }\n \n+// Pushes squeeze_dims up through tt.expand_dims, or folds them.\n+// Example:\n+//   %1 = scf.if %cond -> type1 {\n+//     scf.yield %then : type1\n+//   } else {\n+//     scf.yield %else : type1\n+//   }\n+//   %2 = squeeze_dims %1, axis=0\n+// is rewritten to:\n+//   %1 = scf.if %cond -> type2 {\n+//     %then_ = squeeze_dims %then, axis=0\n+//     scf.yield %2 : type2\n+//   } else {\n+//     %else_ = squeeze_dims %else, axis=0\n+//     scf.yield %else_ : type2\n+//   }\n+LogicalResult PushSqueezeDimsUpThroughIf(SqueezeDimsOp op,\n+                                         PatternRewriter& rewriter) {\n+  Value src = op.getSrc();\n+  auto if_op = src.getDefiningOp<scf::IfOp>();\n+  if (!if_op || !src.hasOneUse()) {\n+    return rewriter.notifyMatchFailure(op, \"Expected scf.if producer.\");\n+  }\n+\n+  // Compute the new types for the if op.\n+  unsigned result_number = cast<OpResult>(op.getSrc()).getResultNumber();\n+  auto new_types = llvm::to_vector(if_op.getResultTypes());\n+  new_types[result_number] = op.getType();\n+\n+  auto new_if_op = rewriter.create<scf::IfOp>(\n+      op.getLoc(), new_types, if_op.getCondition(), /*addThenBlock=*/false,\n+      /*addElseBlock=*/false);\n+\n+  // Update then and else regions.\n+  for (auto [old_region, new_region] :\n+       llvm::zip(if_op.getRegions(), new_if_op.getRegions())) {\n+    rewriter.inlineRegionBefore(*old_region, *new_region, new_region->end());\n+    if (new_region->empty()) {\n+      continue;\n+    }\n+    auto yield_op = new_region->front().getTerminator();\n+    OpBuilder::InsertionGuard guard = SetInsertionPoint(rewriter, yield_op);\n+    auto squeeze_op = rewriter.create<SqueezeDimsOp>(\n+        op.getLoc(), op.getType(), yield_op->getOperand(result_number),\n+        op.getAxis());\n+    yield_op->setOperand(result_number, squeeze_op);\n+  }\n+  rewriter.replaceOp(op, new_if_op.getResult(result_number));\n+  rewriter.replaceOp(if_op, new_if_op);\n+  return success();\n+}\n+\n // Reorders squeeze_dims ops to enforce the invariant that lower-axis ops\n // come first.\n // Example:\n@@ -542,10 +595,11 @@ class TritonXLASqueezeDimsPass\n     patterns.add(ExpandReshapeResult);\n     patterns.add<PushSqueezeDimsUpThroughElementwise>(&getContext());\n     patterns.add(PushSqueezeDimsUpThroughBroadcast);\n-    patterns.add(PushSqueezeDimsUpThroughTrans);\n+    patterns.add(PushSqueezeDimsUpThroughExpandDims);\n+    patterns.add(PushSqueezeDimsUpThroughIf);\n     patterns.add(PushSqueezeDimsUpThroughJoin);\n     patterns.add(PushSqueezeDimsUpThroughReduce);\n-    patterns.add(PushSqueezeDimsUpThroughExpandDims);\n+    patterns.add(PushSqueezeDimsUpThroughTrans);\n     patterns.add(ReorderSqueezeDims);\n     if (failed(applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n       return signalPassFailure();"
        }
    ],
    "stats": {
        "total": 129,
        "additions": 125,
        "deletions": 4
    }
}