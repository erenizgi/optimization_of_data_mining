{
    "author": "hyeontaek",
    "message": "[IFRT Proxy] `Array::pjrt_layout()` uses `nullptr` to indicate a default layout\n\nIFRT Proxy now returns a `nullptr` if it knows that the Array layout represents a default layout. The user code previously has been migrated to handle this new behavior gracefully, obtaining a concrete default layout as before.\n\nCaveat: IFRT Proxy client infers the layout of the output arrays from `LoadedExecutable::GetOutputLayouts()`, which always concrete layouts today. Thus, these output arrays would use concrete layouts for default layouts, even if the arrays on the server side use `nullptr` for default layouts. This behavior is currently acceptable where all users convert the layout into a concrete one before using it, while this behavior will eventually change so that IFRT Proxy client reflects the array layouts on the server side more accurately.\nPiperOrigin-RevId: 821741105",
    "sha": "cc9fd2b254fd843478e0b17d640821627f8d27de",
    "files": [
        {
            "sha": "57ad0dbfd67a056b28d06772ccf3d3f82a60ae70",
            "filename": "third_party/xla/xla/python/ifrt_proxy/client/array.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 15,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cc9fd2b254fd843478e0b17d640821627f8d27de/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Farray.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cc9fd2b254fd843478e0b17d640821627f8d27de/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Farray.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Farray.cc?ref=cc9fd2b254fd843478e0b17d640821627f8d27de",
            "patch": "@@ -589,10 +589,11 @@ absl::StatusOr<xla::ifrt::ArrayRef> Array::AssembleArrayFromSingleDeviceArrays(\n   // We assume that all shards have the same layout.\n   const xla::ifrt::ArrayRef& rcref = arrays[0];\n   Array* array = llvm::cast<Array>(rcref.get());\n-\n+  TF_ASSIGN_OR_RETURN(std::shared_ptr<const xla::PjRtLayout> layout,\n+                      array->pjrt_layout());\n   return xla::ifrt::ArrayRef(tsl::MakeRef<Array>(\n       client, std::move(rpc_helper), dtype, std::move(shape),\n-      std::move(sharding), result_handle, array->custom_layout()));\n+      std::move(sharding), result_handle, std::move(layout)));\n }\n \n absl::StatusOr<std::vector<xla::ifrt::ArrayRef>> Array::RemapArrays(\n@@ -664,7 +665,9 @@ absl::StatusOr<std::vector<xla::ifrt::ArrayRef>> Array::RemapArrays(\n     if (output_layouts[mapping.out_array] == nullptr) {\n       const xla::ifrt::ArrayRef& rcref = arrays[mapping.in_array];\n       Array* array = llvm::cast<Array>(rcref.get());\n-      output_layouts[mapping.out_array] = array->custom_layout();\n+      TF_ASSIGN_OR_RETURN(std::shared_ptr<const xla::PjRtLayout> layout,\n+                          array->pjrt_layout());\n+      output_layouts[mapping.out_array] = std::move(layout);\n     }\n   }\n \n@@ -751,8 +754,7 @@ Array::DisassembleIntoSingleDeviceArrays(\n   for (int i = 0; i < result_handles.size(); ++i) {\n     result.push_back(xla::ifrt::ArrayRef(tsl::MakeRef<Array>(\n         client_, rpc_helper_, dtype_, std::move(shape_and_shardings[i].first),\n-        std::move(shape_and_shardings[i].second), result_handles[i],\n-        this->custom_layout())));\n+        std::move(shape_and_shardings[i].second), result_handles[i], layout_)));\n   }\n \n   return result;\n@@ -792,7 +794,7 @@ absl::StatusOr<xla::ifrt::ArrayRef> Array::FullyReplicatedShard(\n \n   return xla::ifrt::ArrayRef(tsl::MakeRef<Array>(\n       client_, rpc_helper_, dtype_, shape_, std::move(single_device_sharding),\n-      result_handle, this->custom_layout()));\n+      result_handle, layout_));\n }\n \n tsl::Future<> Array::CopyToStringHostBuffer(\n@@ -940,15 +942,7 @@ tsl::Future<> Array::CopyToHostBuffer(\n }\n \n absl::StatusOr<std::shared_ptr<const PjRtLayout>> Array::pjrt_layout() const {\n-  absl::MutexLock l(mu_);\n-  if (custom_layout_ != nullptr) {\n-    return custom_layout_;\n-  }\n-\n-  TF_ASSIGN_OR_RETURN(auto shard_shape, sharding_->GetShardShape(shape_));\n-  return client_->GetDefaultPjRtLayout(dtype_, shard_shape.dims(),\n-                                       sharding_->devices()->devices().front(),\n-                                       sharding_->memory_kind());\n+  return layout_;\n }\n \n xla::ifrt::Client* Array::client() const { return client_; }"
        },
        {
            "sha": "c6240d05e9cdcd908b2218ceac5bd85a7d00dfaa",
            "filename": "third_party/xla/xla/python/ifrt_proxy/client/array.h",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cc9fd2b254fd843478e0b17d640821627f8d27de/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Farray.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cc9fd2b254fd843478e0b17d640821627f8d27de/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Farray.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Farray.h?ref=cc9fd2b254fd843478e0b17d640821627f8d27de",
            "patch": "@@ -108,7 +108,7 @@ class Array final : public llvm::RTTIExtends<Array, xla::ifrt::Array> {\n         dtype_(dtype),\n         shape_(std::move(shape)),\n         sharding_(std::move(sharding)),\n-        custom_layout_(std::move(layout)),\n+        layout_(std::move(layout)),\n         user_context_(UserContextScope::current()),\n         handle_(arr_handle) {}\n \n@@ -140,10 +140,6 @@ class Array final : public llvm::RTTIExtends<Array, xla::ifrt::Array> {\n     return handle_;\n   }\n \n-  std::shared_ptr<const xla::PjRtLayout> custom_layout() const {\n-    return custom_layout_;\n-  }\n-\n   xla::ifrt::Client* client() const override;\n   tsl::Future<> GetReadyFuture() const override;\n   tsl::Future<> Delete() override;\n@@ -191,11 +187,7 @@ class Array final : public llvm::RTTIExtends<Array, xla::ifrt::Array> {\n   const DType dtype_;\n   const Shape shape_;\n   const ShardingRef sharding_;\n-\n-  // This is layout explicitly supplied at creation time. we explicitly\n-  // distinguish it from default layouts since some functions\n-  // behaves differently depending on where the layout came from.\n-  const std::shared_ptr<const xla::PjRtLayout> custom_layout_;\n+  const std::shared_ptr<const xla::PjRtLayout> layout_;\n \n   const UserContextRef user_context_;\n "
        },
        {
            "sha": "ccc805344425457e3625ec8079cb7b28a802abe2",
            "filename": "third_party/xla/xla/python/ifrt_proxy/client/array_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cc9fd2b254fd843478e0b17d640821627f8d27de/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Farray_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cc9fd2b254fd843478e0b17d640821627f8d27de/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Farray_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Farray_test.cc?ref=cc9fd2b254fd843478e0b17d640821627f8d27de",
            "patch": "@@ -161,14 +161,11 @@ TEST_F(ArrayTest, FullyReplicatedShard) {\n }\n \n TEST_F(ArrayTest, GetDefaultPjRtLayoutSuccess) {\n-  ON_CALL(*mock_client_, GetDefaultPjRtLayout).WillByDefault(Return(kLayout1));\n-\n   auto array = tsl::MakeRef<Array>(\n       mock_client_.get(), rpc_helper_, DType(DType::Kind::kBF16), Shape({}),\n       sharding_, ArrayHandle{1234}, /*layout=*/nullptr);\n   TF_ASSERT_OK_AND_ASSIGN(auto layout_1, array->pjrt_layout());\n-  ASSERT_NE(layout_1, nullptr);\n-  EXPECT_EQ(*layout_1, *kLayout1);\n+  EXPECT_EQ(layout_1, nullptr);\n }\n \n TEST_F(ArrayTest, GetCustomLayoutSuccess) {\n@@ -306,8 +303,7 @@ TEST_F(ArrayTest, AssembleArrayFromSingleDeviceArraysDefaultPjRtLayoutSuccess) {\n       SingleDeviceShardSemantics::kAllShards);\n   TF_ASSERT_OK(result.status());\n   TF_ASSERT_OK_AND_ASSIGN(auto layout, result.value()->pjrt_layout());\n-  ASSERT_NE(layout, nullptr);\n-  EXPECT_EQ(*layout, *kLayout1);\n+  EXPECT_EQ(layout, nullptr);\n }\n \n TEST_F(ArrayTest, RemapArraysSuccess) {"
        },
        {
            "sha": "c6871dedb946fa2cd03cebd345abcf6261ed0617",
            "filename": "third_party/xla/xla/python/ifrt_proxy/client/client.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cc9fd2b254fd843478e0b17d640821627f8d27de/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Fclient.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cc9fd2b254fd843478e0b17d640821627f8d27de/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Fclient.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Fclient.cc?ref=cc9fd2b254fd843478e0b17d640821627f8d27de",
            "patch": "@@ -361,10 +361,12 @@ absl::StatusOr<std::vector<xla::ifrt::ArrayRef>> Client::CopyArrays(\n         arrays[i]->sharding().WithDeviceAssignment(devices, memory_kind));\n     auto* proxy_array = llvm::cast<xla::ifrt::proxy::Array>(arrays[i].get());\n     CHECK(proxy_array != nullptr);\n-    new_arrays.push_back(tsl::MakeRef<Array>(\n-        this, rpc_helper_, arrays[i]->dtype(), arrays[i]->shape(),\n-        std::move(new_sharding), ArrayHandle{result_handles[i]},\n-        /*layout=*/proxy_array->custom_layout()));\n+    TF_ASSIGN_OR_RETURN(std::shared_ptr<const xla::PjRtLayout> layout,\n+                        proxy_array->pjrt_layout());\n+    new_arrays.push_back(\n+        tsl::MakeRef<Array>(this, rpc_helper_, arrays[i]->dtype(),\n+                            arrays[i]->shape(), std::move(new_sharding),\n+                            ArrayHandle{result_handles[i]}, std::move(layout)));\n   }\n   return new_arrays;\n }"
        },
        {
            "sha": "e1105b547520d7eec6044521455537e40dbdf7f7",
            "filename": "third_party/xla/xla/python/ifrt_proxy/client/client_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 10,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cc9fd2b254fd843478e0b17d640821627f8d27de/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Fclient_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cc9fd2b254fd843478e0b17d640821627f8d27de/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Fclient_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Fclient_test.cc?ref=cc9fd2b254fd843478e0b17d640821627f8d27de",
            "patch": "@@ -382,10 +382,12 @@ TEST_P(ClientTest, CopyArraysDefaultLayoutSuccess) {\n       client_->CopyArrays(absl::MakeSpan(arrays), std::move(device_list),\n                           MemoryKind(\"mock\"), ArrayCopySemantics::kAlwaysCopy));\n   ASSERT_THAT(copied_arrays, SizeIs(2));\n-  EXPECT_EQ(llvm::cast<Array>(copied_arrays[0].get())->custom_layout(),\n-            nullptr);\n-  EXPECT_EQ(llvm::cast<Array>(copied_arrays[1].get())->custom_layout(),\n-            nullptr);\n+  TF_ASSERT_OK_AND_ASSIGN(std::shared_ptr<const xla::PjRtLayout> layout_1,\n+                          copied_arrays[0].get()->pjrt_layout());\n+  EXPECT_EQ(layout_1, nullptr);\n+  TF_ASSERT_OK_AND_ASSIGN(std::shared_ptr<const xla::PjRtLayout> layout_2,\n+                          copied_arrays[1].get()->pjrt_layout());\n+  EXPECT_EQ(layout_2, nullptr);\n }\n \n TEST_P(ClientTest, CopyArraysCustomLayoutSuccess) {\n@@ -418,12 +420,12 @@ TEST_P(ClientTest, CopyArraysCustomLayoutSuccess) {\n       client_->CopyArrays(absl::MakeSpan(arrays), std::move(device_list),\n                           MemoryKind(\"mock\"), ArrayCopySemantics::kAlwaysCopy));\n   ASSERT_THAT(copied_arrays, SizeIs(2));\n-  EXPECT_EQ(\n-      llvm::cast<Array>(copied_arrays[0].get())->custom_layout()->ToString(),\n-      layout_1_->ToString());\n-  EXPECT_EQ(\n-      llvm::cast<Array>(copied_arrays[1].get())->custom_layout()->ToString(),\n-      layout_2_->ToString());\n+  TF_ASSERT_OK_AND_ASSIGN(std::shared_ptr<const xla::PjRtLayout> layout_1,\n+                          copied_arrays[0].get()->pjrt_layout());\n+  EXPECT_EQ(layout_1->ToString(), layout_1_->ToString());\n+  TF_ASSERT_OK_AND_ASSIGN(std::shared_ptr<const xla::PjRtLayout> layout_2,\n+                          copied_arrays[1].get()->pjrt_layout());\n+  EXPECT_EQ(layout_2->ToString(), layout_2_->ToString());\n }\n \n TEST_P(ClientTest, GetDefaultDeviceAssignmentSuccess) {"
        },
        {
            "sha": "30a8d7cd2e8fe84df951aada90dce40f356fc752",
            "filename": "third_party/xla/xla/python/ifrt_proxy/client/executable.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/cc9fd2b254fd843478e0b17d640821627f8d27de/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Fexecutable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/cc9fd2b254fd843478e0b17d640821627f8d27de/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Fexecutable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt_proxy%2Fclient%2Fexecutable.cc?ref=cc9fd2b254fd843478e0b17d640821627f8d27de",
            "patch": "@@ -744,6 +744,15 @@ LoadedExecutable::Execute(absl::Span<xla::ifrt::ArrayRef> args,\n       output_spec_cache_->Retrieve().has_value();\n \n   xla::ifrt::LoadedExecutable::ExecuteResult result;\n+  // TODO(hyeontaek): `GetOutputLayouts()` uses a concrete layout for a\n+  // default layout. This will change as proper IFRT layout support is fleshed\n+  // out. While the code here using `layouts` will automatically benefit from\n+  // the semantics change for `GetOutputLayouts()`, we would have a slightly\n+  // inconsistent state here until the change happens where output arrays use a\n+  // concrete layout for a default layout. This will not cause an issue for the\n+  // time being when the user always uses concrete layouts, but we would need to\n+  // resolve this issue before the user begins to use `nullptr` default layouts\n+  // without resolving it to a concrete layout.\n   absl::StatusOr<std::vector<std::shared_ptr<const xla::PjRtLayout>>> layouts =\n       GetOutputLayouts();\n "
        }
    ],
    "stats": {
        "total": 85,
        "additions": 40,
        "deletions": 45
    }
}