{
    "author": "tensorflower-gardener",
    "message": "Enable new gemm fusion autotuner behind a flag in the same pass.\n\n- This will helps us leverage gemm_fusion_autotuner_test and gemm microbenchmarks.\n- Once the issues are addresses we will move to autotuner_pass directly and and port necessary tests from gemm_fusion_autotuner_test to autotuner_pass_test.\n\nPiperOrigin-RevId: 827585151",
    "sha": "ec726dd6c84a34c0ae82c2638d80f2c1c5604884",
    "files": [
        {
            "sha": "abcd0fa0e3e581f4d9e417f91a95a74f17378a52",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cudnn.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ec726dd6c84a34c0ae82c2638d80f2c1c5604884/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ec726dd6c84a34c0ae82c2638d80f2c1c5604884/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.cc?ref=ec726dd6c84a34c0ae82c2638d80f2c1c5604884",
            "patch": "@@ -120,25 +120,25 @@ bool IsSupportedCudnnFusion(const HloInstruction& instr,\n       instr.backend_config<GpuBackendConfig>()\n               ->fusion_backend_config()\n               .kind() != kCuDnnFusionKind) {\n-    LOG(ERROR) << \"Instr is not a cudnn fusion.\";\n+    VLOG(1) << \"Instr is not a cudnn fusion.\";\n     return false;\n   }\n \n   HloDotInstruction* dot =\n       Cast<HloDotInstruction>(hlo_query::GetFirstInstructionWithOpcode(\n           *instr.fused_instructions_computation(), HloOpcode::kDot));\n   if (dot == nullptr) {\n-    LOG(ERROR) << \"Fusion does not contain a dot.\";\n+    VLOG(1) << \"Fusion does not contain a dot.\";\n     return false;\n   }\n   if (!algorithm_util::IsSupportedByCudnn(\n           dot->precision_config().algorithm())) {\n-    LOG(ERROR) << \"Fusion contains a precision config not supported by cudnn.\";\n+    VLOG(1) << \"Fusion contains a precision config not supported by cudnn.\";\n     return false;\n   }\n \n   if (GetDnnVersionInfoOrDefault(stream_executor).major_version() < 9) {\n-    LOG(ERROR) << \"Cudnn version is too old.\";\n+    VLOG(1) << \"Cudnn version is too old.\";\n     return false;\n   }\n \n@@ -151,7 +151,7 @@ bool IsSupportedCudnnFusion(const HloInstruction& instr,\n     return true;\n   }\n \n-  LOG(ERROR) << \"Fusion is not supported by cudnn.\";\n+  VLOG(1) << \"Fusion is not supported by cudnn.\";\n   return false;\n }\n "
        },
        {
            "sha": "3081f5a5b8b725f4c584648dfabe70a380dbec08",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ec726dd6c84a34c0ae82c2638d80f2c1c5604884/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ec726dd6c84a34c0ae82c2638d80f2c1c5604884/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=ec726dd6c84a34c0ae82c2638d80f2c1c5604884",
            "patch": "@@ -124,6 +124,7 @@ cc_library(\n         \":gemm_fusion_autotuner_rocm\",\n     ]) + [\n         \":autotuner_compile_util\",\n+        \":autotuner_pass\",\n         \":autotuner_status_key\",\n         \":autotuner_util\",\n         \":dot_search_space\",\n@@ -135,6 +136,10 @@ cc_library(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n+        \"//xla/backends/autotuner:codegen_backend\",\n+        \"//xla/backends/gpu/autotuner:cublas\",\n+        \"//xla/backends/gpu/autotuner:fission_backend\",\n+        \"//xla/backends/gpu/autotuner:triton\",\n         \"//xla/backends/gpu/codegen/triton:tma_utils\",\n         \"//xla/backends/gpu/runtime:buffer_comparator\",\n         \"//xla/hlo/analysis:symbolic_expr\",\n@@ -147,6 +152,7 @@ cc_library(\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n         \"//xla/service:algorithm_util\",\n         \"//xla/service:call_inliner\",\n+        \"//xla/service:compiler\",\n         \"//xla/service:dump\",\n         \"//xla/service:executable\",\n         \"//xla/service:hlo_cost_analysis\","
        },
        {
            "sha": "275fdf5b4dc1c5c896abf23e38d799018a093bec",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 69,
            "deletions": 0,
            "changes": 69,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ec726dd6c84a34c0ae82c2638d80f2c1c5604884/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ec726dd6c84a34c0ae82c2638d80f2c1c5604884/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=ec726dd6c84a34c0ae82c2638d80f2c1c5604884",
            "patch": "@@ -44,6 +44,10 @@ limitations under the License.\n #include \"google/protobuf/text_format.h\"\n #include \"xla/autotune_results.pb.h\"\n #include \"xla/autotuning.pb.h\"\n+#include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/backends/gpu/autotuner/cublas.h\"\n+#include \"xla/backends/gpu/autotuner/fission_backend.h\"\n+#include \"xla/backends/gpu/autotuner/triton.h\"\n #include \"xla/backends/gpu/runtime/buffer_comparator.h\"\n #include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n@@ -61,9 +65,11 @@ limitations under the License.\n #include \"xla/pjrt/distributed/key_value_store_interface.h\"\n #include \"xla/service/algorithm_util.h\"\n #include \"xla/service/call_inliner.h\"\n+#include \"xla/service/compiler.h\"\n #include \"xla/service/dump.h\"\n #include \"xla/service/executable.h\"\n #include \"xla/service/gpu/autotuning/autotuner_compile_util.h\"\n+#include \"xla/service/gpu/autotuning/autotuner_pass.h\"\n #include \"xla/service/gpu/autotuning/autotuner_status_key.h\"\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n #include \"xla/service/gpu/autotuning/dot_search_space.h\"\n@@ -134,6 +140,21 @@ using ProfilingOutput = AutotunerCompileUtil::ProfilingOutput;\n \n namespace {\n \n+std::unique_ptr<HloPassPipeline> GetCublasRewriterPipeline(\n+    const se::DeviceDescription& device_description) {\n+  auto pipeline = std::make_unique<HloPassPipeline>(\"cublas_rewriter_pipeline\");\n+  pipeline->AddPass(std::make_unique<DotAlgorithmRewriter>());\n+  for (GemmRewriterOptions::DType dtype :\n+       {GemmRewriterOptions::DType::kFp8Only,\n+        GemmRewriterOptions::DType::kNonFp8Only}) {\n+    auto gemm_rewriter = std::make_unique<GemmRewriter>(\n+        device_description.gpu_compute_capability(),\n+        device_description.runtime_version(), GemmRewriterOptions{dtype});\n+    pipeline->AddPass(std::move(gemm_rewriter));\n+  }\n+  return pipeline;\n+}\n+\n using AutoTuneCacheKeyCount = absl::flat_hash_map<AutotuneCacheKey, uint64_t>;\n \n using KeysAndInstructions =\n@@ -1546,6 +1567,11 @@ absl::StatusOr<bool> GemmFusionAutotuner::Run(\n   XLA_SCOPED_LOGGING_TIMER(\"GEMM fusion autotuner\");\n \n   const DebugOptions& debug_options = module->config().debug_options();\n+\n+  if (debug_options.xla_gpu_experimental_use_autotuner_pass()) {\n+    return RunViaNewInfra(module, execution_threads);\n+  }\n+\n   GemmFusionAutotunerImpl autotuner(config_, toolkit_version_, debug_options,\n                                     thread_pool_, symbolic_expr_context_);\n   GemmFusionCollector fusion_collector(&autotuner);\n@@ -1666,5 +1692,48 @@ absl::StatusOr<bool> GemmFusionAutotuner::Run(\n       module, execution_threads);\n }\n \n+absl::StatusOr<bool> GemmFusionAutotuner::RunViaNewInfra(\n+    HloModule* module,\n+    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n+  const DebugOptions& debug_options = module->config().debug_options();\n+  std::vector<std::unique_ptr<CodegenBackend>> backends;\n+\n+  se::StreamExecutor* stream_exec = config_.GetExecutor();\n+  TF_ASSIGN_OR_RETURN(std::unique_ptr<Compiler> compiler,\n+                      Compiler::GetForPlatform(stream_exec->GetPlatform()));\n+  se::DeviceMemoryAllocator* device_allocator = config_.GetAllocator();\n+  std::unique_ptr<Compiler::TargetConfig> target_config;\n+  target_config = std::make_unique<Compiler::TargetConfig>(stream_exec);\n+  backends.push_back(std::make_unique<TritonBackend>(\n+      &debug_options, compiler.get(), target_config.get(),\n+      symbolic_expr_context_));\n+  backends.push_back(std::make_unique<FissionBackend>(\n+      &debug_options, compiler.get(), target_config.get(),\n+      std::make_unique<CublasBackend>(stream_exec, &debug_options,\n+                                      compiler.get(), target_config.get()),\n+      GetCublasRewriterPipeline(target_config->device_description),\n+      symbolic_expr_context_));\n+  auto should_autotune = [](const HloInstruction& instruction) -> bool {\n+    if (instruction.opcode() != HloOpcode::kFusion) {\n+      return false;\n+    }\n+    auto gpu_config = instruction.backend_config<GpuBackendConfig>();\n+    const FusionBackendConfig& backend_config =\n+        gpu_config->fusion_backend_config();\n+    if (backend_config.kind() == kTritonGemmFusionKind ||\n+        backend_config.kind() == kCuDnnFusionKind) {\n+      return true;\n+    }\n+    return false;\n+  };\n+\n+  TF_ASSIGN_OR_RETURN(\n+      std::unique_ptr<AutotunerPass> autotuner_pass,\n+      AutotunerPass::Create(std::move(backends), debug_options, stream_exec,\n+                            thread_pool_, should_autotune, target_config.get(),\n+                            device_allocator, false, key_value_store_));\n+  return autotuner_pass->Run(module, execution_threads);\n+}\n+\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "2bd79a64485d2de4744bb5ced734c734041a8bdb",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ec726dd6c84a34c0ae82c2638d80f2c1c5604884/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ec726dd6c84a34c0ae82c2638d80f2c1c5604884/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h?ref=ec726dd6c84a34c0ae82c2638d80f2c1c5604884",
            "patch": "@@ -93,6 +93,10 @@ class GemmFusionAutotuner : public HloModulePass {\n       HloModule* module,\n       const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n \n+  absl::StatusOr<bool> RunViaNewInfra(\n+      HloModule* module,\n+      const absl::flat_hash_set<absl::string_view>& execution_threads);\n+\n  private:\n   AutotuneConfig config_;\n   se::SemanticVersion toolkit_version_;"
        }
    ],
    "stats": {
        "total": 89,
        "additions": 84,
        "deletions": 5
    }
}