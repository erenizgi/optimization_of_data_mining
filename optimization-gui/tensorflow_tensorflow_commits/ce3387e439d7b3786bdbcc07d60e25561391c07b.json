{
    "author": "tensorflower-gardener",
    "message": "[Autotuner] Introduce FissionBackends factory.\n\n- We are not yet using backend factories in the pass, we will switch once all the autotuner passes are ported to new infra.\n\nPiperOrigin-RevId: 827574574",
    "sha": "ce3387e439d7b3786bdbcc07d60e25561391c07b",
    "files": [
        {
            "sha": "03b63dea1fcf6e8a72774d8152efb82d36d35d3e",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce3387e439d7b3786bdbcc07d60e25561391c07b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce3387e439d7b3786bdbcc07d60e25561391c07b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=ce3387e439d7b3786bdbcc07d60e25561391c07b",
            "patch": "@@ -683,10 +683,15 @@ cc_library(\n         \":cublaslt\",\n         \":cudnn\",\n         \":factory\",\n+        \":fission_backend\",\n         \":triton\",\n         \"//xla/backends/autotuner:codegen_backend\",\n         \"//xla/hlo/analysis:symbolic_expr\",\n+        \"//xla/hlo/pass:hlo_pass_pipeline\",\n         \"//xla/service:compiler\",\n+        \"//xla/service/gpu/transforms:dot_algorithm_rewriter\",\n+        \"//xla/service/gpu/transforms:gemm_rewriter\",\n+        \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/cuda:cuda_platform_id\",\n         \"//xla/stream_executor/platform:platform_object_registry\","
        },
        {
            "sha": "9e8fe7c461747de2ce3bb6a54db1d6d75aff86ac",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory.h",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce3387e439d7b3786bdbcc07d60e25561391c07b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce3387e439d7b3786bdbcc07d60e25561391c07b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory.h?ref=ce3387e439d7b3786bdbcc07d60e25561391c07b",
            "patch": "@@ -36,6 +36,13 @@ struct GetCodegenBackends {\n       SymbolicExprContext* symbolic_expr_context)>;\n };\n \n+struct GetFissionBackends {\n+  using Type = std::function<std::vector<std::unique_ptr<CodegenBackend>>(\n+      stream_executor::StreamExecutor*, const DebugOptions*, Compiler*,\n+      const Compiler::TargetConfig*,\n+      SymbolicExprContext* symbolic_expr_context)>;\n+};\n+\n }  // namespace gpu\n }  // namespace xla\n "
        },
        {
            "sha": "199d6467866dd0f7d1c516da6c1f1456b2892c50",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory_cuda.cc",
            "status": "modified",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce3387e439d7b3786bdbcc07d60e25561391c07b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce3387e439d7b3786bdbcc07d60e25561391c07b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_cuda.cc?ref=ce3387e439d7b3786bdbcc07d60e25561391c07b",
            "patch": "@@ -17,23 +17,48 @@ limitations under the License.\n #define TENSORFLOW_COMPILER_XLA_BACKENDS_GPU_AUTOTUNER_CUDA_FACTORY_H_\n \n #include <memory>\n+#include <utility>\n #include <vector>\n \n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/backends/gpu/autotuner/cublas.h\"\n #include \"xla/backends/gpu/autotuner/cublaslt.h\"\n #include \"xla/backends/gpu/autotuner/cudnn.h\"\n #include \"xla/backends/gpu/autotuner/factory.h\"\n+#include \"xla/backends/gpu/autotuner/fission_backend.h\"\n #include \"xla/backends/gpu/autotuner/triton.h\"\n #include \"xla/hlo/analysis/symbolic_expr.h\"\n+#include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n #include \"xla/service/compiler.h\"\n+#include \"xla/service/gpu/transforms/dot_algorithm_rewriter.h\"\n+#include \"xla/service/gpu/transforms/gemm_rewriter.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n+#include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/platform/platform_object_registry.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n \n namespace xla {\n namespace gpu {\n \n+namespace {\n+\n+std::unique_ptr<HloPassPipeline> GetCublasRewriterPipeline(\n+    const se::DeviceDescription& device_description) {\n+  auto pipeline = std::make_unique<HloPassPipeline>(\"cublas_rewriter_pipeline\");\n+  pipeline->AddPass(std::make_unique<DotAlgorithmRewriter>());\n+  for (GemmRewriterOptions::DType dtype :\n+       {GemmRewriterOptions::DType::kFp8Only,\n+        GemmRewriterOptions::DType::kNonFp8Only}) {\n+    auto gemm_rewriter = std::make_unique<GemmRewriter>(\n+        device_description.gpu_compute_capability(),\n+        device_description.runtime_version(), GemmRewriterOptions{dtype});\n+    pipeline->AddPass(std::move(gemm_rewriter));\n+  }\n+  return pipeline;\n+}\n+\n+}  // namespace\n+\n std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForCuda(\n     stream_executor::StreamExecutor* stream_executor,\n     const DebugOptions* debug_options, Compiler* compiler,\n@@ -51,10 +76,29 @@ std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForCuda(\n   return backends;\n }\n \n+std::vector<std::unique_ptr<CodegenBackend>> GetFissionBackendsForCuda(\n+    stream_executor::StreamExecutor* stream_executor,\n+    const DebugOptions* debug_options, Compiler* compiler,\n+    const Compiler::TargetConfig* target_config,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  std::vector<std::unique_ptr<CodegenBackend>> backends;\n+  backends.push_back(std::make_unique<FissionBackend>(\n+      debug_options, compiler, target_config,\n+      std::make_unique<CublasBackend>(stream_executor, debug_options, compiler,\n+                                      target_config),\n+      GetCublasRewriterPipeline(target_config->device_description),\n+      symbolic_expr_context));\n+  return backends;\n+}\n+\n STREAM_EXECUTOR_REGISTER_OBJECT_STATICALLY(GetCodegenBackendsCudaRegistration,\n                                            GetCodegenBackends,\n                                            se::cuda::kCudaPlatformId,\n                                            GetCodegenBackendsForCuda);\n+STREAM_EXECUTOR_REGISTER_OBJECT_STATICALLY(GetFissionBackendsCudaRegistration,\n+                                           GetFissionBackends,\n+                                           se::cuda::kCudaPlatformId,\n+                                           GetFissionBackendsForCuda);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "6f0a549799576cf117e70ac5cdedfc5c1afe5041",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/factory_rocm.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ce3387e439d7b3786bdbcc07d60e25561391c07b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ce3387e439d7b3786bdbcc07d60e25561391c07b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffactory_rocm.cc?ref=ce3387e439d7b3786bdbcc07d60e25561391c07b",
            "patch": "@@ -45,10 +45,22 @@ std::vector<std::unique_ptr<CodegenBackend>> GetCodegenBackendsForROCm(\n   return backends;\n }\n \n+std::vector<std::unique_ptr<CodegenBackend>> GetFissionBackendsForROCm(\n+    stream_executor::StreamExecutor* stream_executor,\n+    const DebugOptions* debug_options, Compiler* compiler,\n+    const Compiler::TargetConfig* target_config,\n+    SymbolicExprContext* symbolic_expr_context) {\n+  return {};\n+}\n+\n STREAM_EXECUTOR_REGISTER_OBJECT_STATICALLY(GetCodegenBackendsROCmRegistration,\n                                            GetCodegenBackends,\n                                            se::rocm::kROCmPlatformId,\n                                            GetCodegenBackendsForROCm);\n+STREAM_EXECUTOR_REGISTER_OBJECT_STATICALLY(GetFissionBackendsROCmRegistration,\n+                                           GetFissionBackends,\n+                                           se::rocm::kROCmPlatformId,\n+                                           GetFissionBackendsForROCm);\n \n }  // namespace gpu\n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 68,
        "additions": 68,
        "deletions": 0
    }
}