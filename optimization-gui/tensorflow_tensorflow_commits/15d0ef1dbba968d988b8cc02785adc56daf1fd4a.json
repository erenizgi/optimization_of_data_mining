{
    "author": "sohaibiftikhar",
    "message": "[XLA:GPU]: Add triton_xla.get_tid operation.\n\nget_tid returns the thread id in the X dimension by using the PTX instrinsic.\n\nPiperOrigin-RevId: 803013913",
    "sha": "15d0ef1dbba968d988b8cc02785adc56daf1fd4a",
    "files": [
        {
            "sha": "f0845e2fbea0bb02b5de3eac83651da0195ff6dc",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/triton_xla_ops.td",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15d0ef1dbba968d988b8cc02785adc56daf1fd4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15d0ef1dbba968d988b8cc02785adc56daf1fd4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td?ref=15d0ef1dbba968d988b8cc02785adc56daf1fd4a",
            "patch": "@@ -198,4 +198,16 @@ def TTXLA_SqueezeDimsOp : TTXLA_Op<\"squeeze_dims\", [\n     let assemblyFormat = \"$src attr-dict `:` type($src) `->` type($result)\";\n }\n \n+def TTXLA_GetTidOp : TTXLA_Op<\"get_tid\", [Pure]> {\n+  let summary = \"Get the thread ID within a triton kernel.\";\n+  let description = [{\n+    This operation returns the thread ID within a kernel.\n+    Only the X dimension is supported. The result is a 32-bit\n+    integer which in CUDA terms is threadIdx.x.\n+  }];\n+  let arguments = (ins);\n+  let results = (outs I32:$result);\n+  let assemblyFormat = \"attr-dict `:` functional-type(operands, $result)\";\n+}\n+\n #endif // XLA_BACKENDS_GPU_CODEGEN_TRITON_IR_TRITON_XLA_OPS_TD_"
        },
        {
            "sha": "d5d8b9f1ed19bc0f2da15da09e5649ccaf26386c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15d0ef1dbba968d988b8cc02785adc56daf1fd4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15d0ef1dbba968d988b8cc02785adc56daf1fd4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD?ref=15d0ef1dbba968d988b8cc02785adc56daf1fd4a",
            "patch": "@@ -37,19 +37,18 @@ cc_library(\n         \"round_f32_to_tf32_for_tf32_dot_pass.cc\",\n         \"triton_xla_extract_insert_to_triton_pass.cc\",\n         \"triton_xla_fold_transpose_pass.cc\",\n+        \"triton_xla_lower_get_tid_pass.cc\",\n         \"triton_xla_squeeze_dims_pass.cc\",\n     ],\n     hdrs = [\"passes.h\"],\n     deps = [\n         \":passes_inc_gen\",\n         \"//xla:permutation_util\",\n-        \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla/backends/gpu/codegen/triton:emitter_helpers\",\n         \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n         \"//xla/codegen:emitter_loc_op_builder\",\n         \"//xla/codegen/emitters/ir:xla\",\n-        \"//xla/hlo/analysis:indexing_analysis\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n@@ -64,7 +63,6 @@ cc_library(\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:Analysis\",\n         \"@llvm-project//mlir:ArithDialect\",\n-        \"@llvm-project//mlir:DialectUtils\",\n         \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:FunctionInterfaces\",\n         \"@llvm-project//mlir:IR\","
        },
        {
            "sha": "2f8efd9cf030a91416fbf6df016be55a9c1c3a4f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15d0ef1dbba968d988b8cc02785adc56daf1fd4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15d0ef1dbba968d988b8cc02785adc56daf1fd4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h?ref=15d0ef1dbba968d988b8cc02785adc56daf1fd4a",
            "patch": "@@ -40,6 +40,7 @@ std::unique_ptr<mlir::Pass> CreateInt4ToPackedInt4RewritePass(\n     const stream_executor::DeviceDescription& device_description);\n std::unique_ptr<mlir::Pass> CreateRoundF32ToTF32ForTf32DotRewritePass();\n std::unique_ptr<mlir::Pass> CreateExtractTmaInfoPass();\n+std::unique_ptr<mlir::Pass> CreateTritonXLALowerGetTidPass();\n \n // Returns true if the `op` contains an operation in it's regions that satisfies\n // the `fn`."
        },
        {
            "sha": "935ad48be46ae3f89d021f22f9c0b4872650c8aa",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.td",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15d0ef1dbba968d988b8cc02785adc56daf1fd4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15d0ef1dbba968d988b8cc02785adc56daf1fd4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td?ref=15d0ef1dbba968d988b8cc02785adc56daf1fd4a",
            "patch": "@@ -105,4 +105,14 @@ def RoundF32ToTF32ForTf32DotRewritePass\n   let constructor = \"CreateRoundF32ToTF32ForTf32DotRewritePass()\";\n }\n \n+def TritonXLALowerGetTidPass\n+    : Pass<\"triton-xla-get-tid\", \"mlir::ModuleOp\"> {\n+  let summary = \"Lower get_tid to the PTX intrinsic.\";\n+  let description = [{\n+    This pass lowers get_tid to the PTX intrinsic by loading the thread ID from\n+    the register and returning it.\n+  }];\n+  let constructor = \"CreateTritonXLALowerGetTidPass()\";\n+}\n+\n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_PASSES_TD_"
        },
        {
            "sha": "9b0f728111a8b836f8c3a4b83f609779cb9b05d3",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_get_tid.mlir",
            "status": "added",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15d0ef1dbba968d988b8cc02785adc56daf1fd4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_get_tid.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15d0ef1dbba968d988b8cc02785adc56daf1fd4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_get_tid.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_get_tid.mlir?ref=15d0ef1dbba968d988b8cc02785adc56daf1fd4a",
            "patch": "@@ -0,0 +1,11 @@\n+// RUN: xla-opt %s --triton-xla-get-tid | FileCheck %s\n+\n+// CHECK-LABEL: tt.func @kernel()\n+tt.func @kernel() -> i32 {\n+  // CHECK-NOT: triton_xla.get_flat_tid\n+  // CHECK: %[[TID:.*]] = tt.elementwise_inline_asm\n+  // CHECK-SAME: mov.u32 $0, %tid.x;\n+  // CHECK: tt.return %[[TID]] : i32\n+  %0 = triton_xla.get_tid : () -> i32\n+  tt.return %0 : i32\n+}\n\\ No newline at end of file"
        },
        {
            "sha": "8ab68caf2cb233b278ae4b628b1ebb84b7800c41",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_lower_get_tid_pass.cc",
            "status": "added",
            "additions": 81,
            "deletions": 0,
            "changes": 81,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/15d0ef1dbba968d988b8cc02785adc56daf1fd4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_get_tid_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/15d0ef1dbba968d988b8cc02785adc56daf1fd4a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_get_tid_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_get_tid_pass.cc?ref=15d0ef1dbba968d988b8cc02785adc56daf1fd4a",
            "patch": "@@ -0,0 +1,81 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/Location.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/ValueRange.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+\n+namespace mlir::triton::xla {\n+\n+#define GEN_PASS_DEF_TRITONXLALOWERGETTIDPASS\n+#include \"xla/backends/gpu/codegen/triton/transforms/passes.h.inc\"\n+\n+namespace {\n+\n+LogicalResult LowerGetTidOp(GetTidOp get_flat_tid, PatternRewriter& rewriter) {\n+  mlir::OpBuilder::InsertionGuard guard(rewriter);\n+  rewriter.setInsertionPoint(get_flat_tid);\n+  const Location loc = get_flat_tid.getLoc();\n+\n+  const mlir::Type i32_type = rewriter.getI32Type();\n+  const absl::string_view get_tid_asm = R\"(\n+    mov.u32 $0, %tid.x;\n+  )\";\n+  auto tid_op = rewriter.create<mlir::triton::ElementwiseInlineAsmOp>(\n+      loc,\n+      /*result_types=*/i32_type,\n+      /*asm_string=*/rewriter.getStringAttr(get_tid_asm),\n+      /*constraints=*/rewriter.getStringAttr(\"=r\"),\n+      /*pure=*/rewriter.getBoolAttr(true),\n+      /*packed_element=*/rewriter.getI32IntegerAttr(1),\n+      /*args*/ mlir::ValueRange{});\n+  rewriter.replaceOp(get_flat_tid, tid_op->getResults());\n+  return success();\n+}\n+\n+class TritonXLALowerGetTidPass\n+    : public impl::TritonXLALowerGetTidPassBase<TritonXLALowerGetTidPass> {\n+ public:\n+  using Base::Base;\n+\n+ private:\n+  void runOnOperation() override {\n+    RewritePatternSet patterns(&getContext());\n+    patterns.add(LowerGetTidOp);\n+    if (failed(applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n+      return signalPassFailure();\n+    }\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<Pass> CreateTritonXLALowerGetTidPass() {\n+  return std::make_unique<TritonXLALowerGetTidPass>();\n+}\n+\n+}  // namespace mlir::triton::xla"
        }
    ],
    "stats": {
        "total": 119,
        "additions": 116,
        "deletions": 3
    }
}