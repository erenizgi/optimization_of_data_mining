{
    "author": "ezhulenev",
    "message": "[xla:gpu] Add support for stateful custom calls to command buffers\n\nPiperOrigin-RevId: 837763476",
    "sha": "ae0bcfbed65c9f3cf97c7714efdf69deff6507e0",
    "files": [
        {
            "sha": "e5a94d1dd28cb62666aa2c71ab1edaf30abd9a93",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=ae0bcfbed65c9f3cf97c7714efdf69deff6507e0",
            "patch": "@@ -89,6 +89,7 @@ cc_library(\n         \"//xla/backends/gpu/collectives:gpu_collectives\",\n         \"//xla/ffi:attribute_map\",\n         \"//xla/ffi:call_frame\",\n+        \"//xla/ffi:execution_state\",\n         \"//xla/ffi:ffi_api\",\n         \"//xla/ffi/api:c_api\",\n         \"//xla/hlo/evaluator:hlo_evaluator\","
        },
        {
            "sha": "39d4af6011a36f4ba152e8e382367eeb8b62742f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc?ref=ae0bcfbed65c9f3cf97c7714efdf69deff6507e0",
            "patch": "@@ -2014,12 +2014,14 @@ CustomCallCmd::RecordXlaFfiCall(const Thunk::ExecuteParams& execute_params,\n           execute_params.stream->parent(),\n           execute_params.command_buffer_trace_stream, [&](se::Stream* stream) {\n             ffi::CallOptions options = {\n-                run_id, execute_params.buffer_allocations->device_ordinal(),\n+                run_id,\n+                execute_params.buffer_allocations->device_ordinal(),\n                 ffi::CallOptions::GpuOptions{\n                     stream,\n                     execute_params.buffer_allocations->memory_allocator()},\n                 /*called_computation=*/nullptr,  // TODO(b/342285364)\n-                execute_params.ffi_execution_context};\n+                execute_params.ffi_execution_context,\n+                execution_state_.get()};\n             return ffi::Call(handler_, *call_frame, options);\n           }));\n "
        },
        {
            "sha": "85bde83f28bfee8ddb5c8709f1923be1386408fe",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.h",
            "status": "modified",
            "additions": 13,
            "deletions": 5,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h?ref=ae0bcfbed65c9f3cf97c7714efdf69deff6507e0",
            "patch": "@@ -49,6 +49,7 @@ limitations under the License.\n #include \"xla/ffi/api/c_api.h\"\n #include \"xla/ffi/attribute_map.h\"\n #include \"xla/ffi/call_frame.h\"\n+#include \"xla/ffi/execution_state.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/runtime/execution_graph.h\"\n@@ -131,8 +132,9 @@ using ResourceUseVector = absl::InlinedVector<ResourceUse, 1>;\n \n class CommandBufferCmd {\n  public:\n-  CommandBufferCmd(CommandBufferCmdType cmd_type,\n-                   se::StreamPriority priority = se::StreamPriority::Default)\n+  explicit CommandBufferCmd(\n+      CommandBufferCmdType cmd_type,\n+      se::StreamPriority priority = se::StreamPriority::Default)\n       : cmd_type_(cmd_type), priority_(priority) {\n     token_ = Resource::Create(Resource::kToken);\n     resources_.push_back(ResourceUse::Write(token_));\n@@ -765,7 +767,7 @@ class MemcpyDeviceToDeviceCmd : public CommandBufferCmd {\n \n class MemzeroCmd : public CommandBufferCmd {\n  public:\n-  MemzeroCmd(BufferAllocation::Slice dst);\n+  explicit MemzeroCmd(BufferAllocation::Slice dst);\n \n   absl::StatusOr<const se::CommandBuffer::Command*> Record(\n       const Thunk::ExecuteParams& execute_params,\n@@ -804,7 +806,7 @@ class Memset32Cmd : public CommandBufferCmd {\n \n class ChildCmd : public CommandBufferCmd {\n  public:\n-  ChildCmd(CommandBufferCmdExecutor child_commands);\n+  explicit ChildCmd(CommandBufferCmdExecutor child_commands);\n \n   absl::Status Initialize(const Thunk::InitializeParams& params,\n                           StateManager& state) override;\n@@ -936,7 +938,7 @@ class GemmCmd : public TracedCommandBufferCmd {\n \n class CublasLtCmd : public TracedCommandBufferCmd, public CublasLtMatmulThunk {\n  public:\n-  CublasLtCmd(const CublasLtMatmulThunk& matmul_thunk);\n+  explicit CublasLtCmd(const CublasLtMatmulThunk& matmul_thunk);\n \n   absl::Status Initialize(const Thunk::InitializeParams& params,\n                           StateManager& state) override;\n@@ -1008,11 +1010,13 @@ class CustomCallCmd : public CommandBufferCmd {\n                 std::vector<NullableShapedSlice> operands,\n                 std::vector<NullableShapedSlice> results,\n                 ffi::CallFrame call_frame,\n+                std::shared_ptr<ffi::ExecutionState> execution_state,\n                 const HloComputation* called_computation)\n       : CommandBufferCmd(CommandBufferCmdType::kCustomCallCmd),\n         target_name_(std::move(target_name)),\n         handler_(handler),\n         call_frame_(std::move(call_frame)),\n+        execution_state_(std::move(execution_state)),\n         call_frames_([this] { return call_frame_->Copy(); }),\n         called_computation_(called_computation),\n         operands_(std::move(operands)),\n@@ -1052,6 +1056,10 @@ class CustomCallCmd : public CommandBufferCmd {\n   // Reference call frame pre-initialized at construction time.\n   std::optional<ffi::CallFrame> call_frame_;\n \n+  // Execution state bound to the FFI handler. It is initialized by the\n+  // corresponding Thunk at construction time.\n+  std::shared_ptr<ffi::ExecutionState> execution_state_;\n+\n   // A pool of call frames used at run time. Newly created call frames are\n   // copied from the reference call frame and updated with buffer addresses.\n   std::optional<ObjectPool<ffi::CallFrame>> call_frames_;"
        },
        {
            "sha": "1af03c8609424774b1b4012ba5f2954c97ab53dc",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd_emitter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc?ref=ae0bcfbed65c9f3cf97c7714efdf69deff6507e0",
            "patch": "@@ -232,7 +232,7 @@ static absl::StatusOr<Command> Convert(const CustomCallThunk& thunk) {\n   if (auto bundle = thunk.bundle(); bundle.has_value()) {\n     return std::make_unique<CustomCallCmd>(\n         thunk.target_name(), bundle->execute, thunk.operands(), thunk.results(),\n-        *thunk.call_frame(),\n+        *thunk.call_frame(), thunk.execution_state(),\n         /*called_computation=*/nullptr);  // TODO(b/342285364)\n   }\n   return std::make_unique<CustomCallCmd>(thunk.target_name(),"
        },
        {
            "sha": "f8f6224419d09161ee0fcd022ad8191c5ee23e47",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc?ref=ae0bcfbed65c9f3cf97c7714efdf69deff6507e0",
            "patch": "@@ -266,6 +266,7 @@ absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n   }\n \n   auto execution_state = std::make_unique<ffi::ExecutionState>();\n+\n   // Initialize FFI handler state if it has an instantiate callback.\n   if (bundle.instantiate) {\n     // At FFI handler instantiation time, we don't have any arguments or"
        },
        {
            "sha": "86b04f07577e4cef55b1b37d85a283fd1bed870d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk.h",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h?ref=ae0bcfbed65c9f3cf97c7714efdf69deff6507e0",
            "patch": "@@ -130,6 +130,7 @@ class CustomCallThunk : public Thunk {\n \n   const std::string& target_name() const { return target_name_; }\n   CustomCallTarget call_target() const { return call_target_; }\n+\n   std::optional<XLA_FFI_Handler_Bundle> bundle() const {\n     if (!bundle_.has_value()) {\n       return std::nullopt;\n@@ -138,10 +139,15 @@ class CustomCallThunk : public Thunk {\n         std::get_if<XLA_FFI_Handler_Bundle>(&bundle_.value());\n     return c_bundle ? std::make_optional(*c_bundle) : std::nullopt;\n   }\n+\n   std::optional<ffi::CallFrame> call_frame() const {\n     return call_frame_ ? std::make_optional(call_frame_->Copy()) : std::nullopt;\n   }\n \n+  std::shared_ptr<ffi::ExecutionState> execution_state() const {\n+    return execution_state_;\n+  }\n+\n   const std::vector<NullableShapedSlice>& operands() const { return operands_; }\n   const std::vector<NullableShapedSlice>& results() const { return results_; }\n \n@@ -223,7 +229,7 @@ class CustomCallThunk : public Thunk {\n   std::optional<ObjectPool<ffi::CallFrame>> call_frames_;\n \n   // Execution state bound to the FFI handler. Optional.\n-  std::unique_ptr<ffi::ExecutionState> execution_state_;\n+  std::shared_ptr<ffi::ExecutionState> execution_state_;\n \n   // TODO(ezhulenev): Currently we assume that HloModule that owns this\n   // computation is owned by a GpuExecutable and stays alive for as long as"
        },
        {
            "sha": "88d6b155492c2eb9ecad45da32e2f1f1555a54c1",
            "filename": "third_party/xla/xla/service/gpu/tests/command_buffer_test.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 2,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ae0bcfbed65c9f3cf97c7714efdf69deff6507e0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fcommand_buffer_test.cc?ref=ae0bcfbed65c9f3cf97c7714efdf69deff6507e0",
            "patch": "@@ -241,22 +241,35 @@ TEST_P(CommandBufferTest, Fusions) {\n                               /*run_hlo_passes=*/false);\n }\n \n-static absl::Status Memcpy(se::Stream* stream, ffi::AnyBuffer src,\n+// Empty memcpy state to test stateful custom calls.\n+struct MemcpyState {};\n+\n+static absl::StatusOr<std::unique_ptr<MemcpyState>> MemcpyInstantiate() {\n+  return std::make_unique<MemcpyState>();\n+}\n+\n+static absl::Status Memcpy(se::Stream* stream, MemcpyState* state,\n+                           ffi::AnyBuffer src,\n                            ffi::Result<ffi::AnyBuffer> dst) {\n+  EXPECT_NE(state, nullptr);\n   se::DeviceMemoryBase dst_mem = dst->device_memory();\n   se::DeviceMemoryBase src_mem = src.device_memory();\n   return stream->MemcpyD2D(&dst_mem, src_mem, src_mem.size());\n }\n \n+XLA_FFI_DEFINE_HANDLER(kMemcpyInstantiate, MemcpyInstantiate,\n+                       ffi::Ffi::BindInstantiate());\n+\n XLA_FFI_DEFINE_HANDLER(kMemcpy, Memcpy,\n                        ffi::Ffi::Bind()\n                            .Ctx<ffi::Stream>()\n+                           .Ctx<ffi::State<MemcpyState>>()\n                            .Arg<ffi::AnyBuffer>()   // src\n                            .Ret<ffi::AnyBuffer>(),  // dst\n                        {ffi::Traits::kCmdBufferCompatible});\n \n XLA_FFI_REGISTER_HANDLER(ffi::GetXlaFfiApi(), \"__xla_test$$memcpy\", \"gpu\",\n-                         kMemcpy);\n+                         {kMemcpyInstantiate, nullptr, nullptr, kMemcpy});\n \n TEST_P(CommandBufferTest, TracedCustomCalls) {\n   constexpr absl::string_view hlo_text = R\"("
        }
    ],
    "stats": {
        "total": 53,
        "additions": 42,
        "deletions": 11
    }
}