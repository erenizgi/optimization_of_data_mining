{
    "author": "vwbaker",
    "message": "Add is_autotuning_compilation to autotuner\n\nThis is necessary to bailout of autotuning when the autotuning pass is added to RunBackend (we should be able to pull this out in the future though).\n\nPiperOrigin-RevId: 799513582",
    "sha": "0fbb870fa35d31fc2a7ddb66b41a2e0e82707466",
    "files": [
        {
            "sha": "abdbb0daf95cc40161643d3ceb4be1cee144633a",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0fbb870fa35d31fc2a7ddb66b41a2e0e82707466/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0fbb870fa35d31fc2a7ddb66b41a2e0e82707466/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=0fbb870fa35d31fc2a7ddb66b41a2e0e82707466",
            "patch": "@@ -91,6 +91,7 @@ xla_test(\n         \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest_main\",\n@@ -610,14 +611,21 @@ xla_test(\n         \":native_emitter\",\n         \"//xla/backends/autotuner:codegen_backend\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/ir:hlo_module_group\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/service:compiler\",\n+        \"//xla/service:executable\",\n+        \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:nvptx_compiler\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n+        \"//xla/stream_executor:platform\",\n+        \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n )"
        },
        {
            "sha": "82cb4ac4c9ebe35a9f76fbc64d3090fe8df27890",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter_test.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 19,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0fbb870fa35d31fc2a7ddb66b41a2e0e82707466/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0fbb870fa35d31fc2a7ddb66b41a2e0e82707466/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc?ref=0fbb870fa35d31fc2a7ddb66b41a2e0e82707466",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/substitute.h\"\n #include \"xla/autotuning.pb.h\"\n@@ -63,20 +64,21 @@ int CountTmaAllowed(\n class TritonBlockLevelFusionEmitterBackendTest\n     : public HloHardwareIndependentTestBase {\n  protected:\n-  TritonBlockLevelFusionEmitterBackendTest()\n-      : backend_(PlatformUtil::GetDefaultPlatform()\n-                     .value()\n-                     ->ExecutorForDevice(0)\n-                     .value(),\n-                 &debug_options_, &compiler_) {\n+  TritonBlockLevelFusionEmitterBackendTest() {\n     // TODO(b/315957220): Remove the experimental flags once TMA is enabled by\n     // default.\n     debug_options_.set_xla_gpu_experimental_enable_triton_tma(true);\n+    backend_ = std::make_unique<BlockLevelEmitterBackend>(\n+        PlatformUtil::GetDefaultPlatform()\n+            .value()\n+            ->ExecutorForDevice(0)\n+            .value(),\n+        &debug_options_, &compiler_);\n   }\n \n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n-  BlockLevelEmitterBackend backend_;\n+  std::unique_ptr<BlockLevelEmitterBackend> backend_;\n };\n \n // Verifies that GetDefaultConfig correctly parses and returns the\n@@ -116,7 +118,7 @@ ENTRY %main {\n   // Call GetDefaultConfig on the root instruction (the fusion op).\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<BackendConfig> config,\n-      backend_.GetDefaultConfig(\n+      backend_->GetDefaultConfig(\n           *(module->entry_computation()->root_instruction())));\n   // Verify that the returned config is indeed a BlockLevelFusionConfig.\n   BlockLevelFusionConfig block_level_fusion_config;\n@@ -160,7 +162,7 @@ ENTRY %main {\n   // Call GetDefaultConfig on the root instruction (the fusion op).\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<BackendConfig> config,\n-      backend_.GetDefaultConfig(\n+      backend_->GetDefaultConfig(\n           *(module->entry_computation()->root_instruction())));\n   // Verify that the returned config is indeed a BlockLevelFusionConfig.\n   BlockLevelFusionConfig block_level_fusion_config;\n@@ -207,7 +209,7 @@ ENTRY %main {\n   // Call GetDefaultConfig on the root instruction (the fusion op).\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<BackendConfig> config,\n-      backend_.GetDefaultConfig(\n+      backend_->GetDefaultConfig(\n           *(module->entry_computation()->root_instruction())));\n   // Verify that the returned config is indeed a BlockLevelFusionConfig.\n   BlockLevelFusionConfig block_level_fusion_config;\n@@ -256,7 +258,7 @@ ENTRY %main {\n   // Call GetDefaultConfig on the root instruction (the fusion op).\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<BackendConfig> config,\n-      backend_.GetDefaultConfig(\n+      backend_->GetDefaultConfig(\n           *(module->entry_computation()->root_instruction())));\n   // Verify that the returned config is indeed a BlockLevelFusionConfig.\n   BlockLevelFusionConfig block_level_fusion_config;\n@@ -301,7 +303,7 @@ ENTRY %main {\n   // Call GetSupportedConfigs on the root instruction (the fusion op).\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::vector<std::unique_ptr<BackendConfig>> configs,\n-      backend_.GetSupportedConfigs(\n+      backend_->GetSupportedConfigs(\n           *(module->entry_computation()->root_instruction())));\n \n   // If device supports TMA, the backend should generate 70 combinations:\n@@ -313,7 +315,7 @@ ENTRY %main {\n   // The middle dimension (d1 = 1) must always have tile size 1.\n   //\n   // If device doesn't support TMA, we currently expect half the number (35).\n-  bool is_tma_supported = backend_.target_config()\n+  bool is_tma_supported = backend_->target_config()\n                               .device_description.cuda_compute_capability()\n                               .IsAtLeastHopper();\n   if (is_tma_supported) {\n@@ -385,7 +387,7 @@ backend_config={\"fusion_backend_config\":{\"kind\":\"__triton\"}}\n   // Call GetSupportedConfigs on the root instruction (the fusion op).\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::vector<std::unique_ptr<BackendConfig>> configs,\n-      backend_.GetSupportedConfigs(\n+      backend_->GetSupportedConfigs(\n           *(module->entry_computation()->root_instruction())));\n \n   // If device supports TMA, expect 40 total configurations:\n@@ -395,7 +397,7 @@ backend_config={\"fusion_backend_config\":{\"kind\":\"__triton\"}}\n   // The middle dimension (d1 = 0) must always have tile size 0.\n   //\n   // If device doesn't support TMA, we currently expect half the number (20).\n-  bool is_tma_supported = backend_.target_config()\n+  bool is_tma_supported = backend_->target_config()\n                               .device_description.cuda_compute_capability()\n                               .IsAtLeastHopper();\n   if (is_tma_supported) {\n@@ -464,7 +466,7 @@ ENTRY %main {\n   // Call GetDefaultConfig on the root instruction (the fusion op).\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<BackendConfig> config,\n-      backend_.GetDefaultConfig(\n+      backend_->GetDefaultConfig(\n           *(module->entry_computation()->root_instruction())));\n   // Verify that the returned config is indeed a BlockLevelFusionConfig.\n   BlockLevelFusionConfig block_level_fusion_config;\n@@ -480,7 +482,7 @@ ENTRY %main {\n               )pb\"));\n \n   // Apply the generated config to the fusion instruction.\n-  EXPECT_THAT(backend_.ApplyConfig(*instr, *config), absl_testing::IsOk());\n+  EXPECT_THAT(backend_->ApplyConfig(*instr, *config), absl_testing::IsOk());\n   TF_ASSERT_OK_AND_ASSIGN(GpuBackendConfig gpu_backend_config,\n                           instr->backend_config<GpuBackendConfig>());\n   // Ensure that the backend config on the instruction matches what was applied.\n@@ -522,10 +524,10 @@ ENTRY %main {\n   // Call GetDefaultConfig on the root instruction (the fusion op).\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<BackendConfig> config,\n-      backend_.GetDefaultConfig(\n+      backend_->GetDefaultConfig(\n           *(module->entry_computation()->root_instruction())));\n   // Attempt to compile the root instruction using the retrieved backend config.\n-  absl::StatusOr<std::unique_ptr<Executable>> executable = backend_.Compile(\n+  absl::StatusOr<std::unique_ptr<Executable>> executable = backend_->Compile(\n       *(module->entry_computation()->root_instruction()), *config);\n   // Verify that compilation succeeded and returned a valid executable.\n   EXPECT_THAT(executable, absl_testing::IsOk());"
        },
        {
            "sha": "ab1e969cfc9c6338445acb7c959b00a66f1e529a",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_codegen_backend.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0fbb870fa35d31fc2a7ddb66b41a2e0e82707466/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0fbb870fa35d31fc2a7ddb66b41a2e0e82707466/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h?ref=0fbb870fa35d31fc2a7ddb66b41a2e0e82707466",
            "patch": "@@ -73,6 +73,7 @@ class GpuCodegenBackend : public CodegenBackend {\n         false);\n \n     Compiler::CompileOptions options;\n+    options.is_autotuning_compilation = true;\n     TF_ASSIGN_OR_RETURN(auto optimized_module,\n                         RunHloPasses(std::move(hlo_module), options));\n     return compiler_->RunBackend(std::move(optimized_module), stream_executor_,\n@@ -95,7 +96,7 @@ class GpuCodegenBackend : public CodegenBackend {\n   std::string name_;\n   stream_executor::StreamExecutor* stream_executor_;\n   Compiler::TargetConfig target_config_;\n-  const DebugOptions& debug_options_;\n+  const DebugOptions debug_options_;\n   // TODO(b/407494653): remove compiler when we don't need to run any HLO passes\n   // and the codegen backend can directly produce an executable without a\n   // compiler instance."
        },
        {
            "sha": "46dbc301dcd6d199467133fe8685c8a9d77ff2e3",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/native_emitter_test.cc",
            "status": "modified",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0fbb870fa35d31fc2a7ddb66b41a2e0e82707466/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0fbb870fa35d31fc2a7ddb66b41a2e0e82707466/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc?ref=0fbb870fa35d31fc2a7ddb66b41a2e0e82707466",
            "patch": "@@ -22,12 +22,19 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n #include \"absl/status/status_matchers.h\"\n+#include \"absl/status/statusor.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module_group.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/service/compiler.h\"\n+#include \"xla/service/executable.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n+#include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/platform_util.h\"\n+#include \"xla/stream_executor/platform.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n@@ -183,6 +190,54 @@ TEST_F(NativeEmitterBackendTest, CompileForDefaultConfig) {\n   EXPECT_THAT(maybe_executable, absl_testing::IsOk());\n }\n \n+class MockCompiler : public Compiler {\n+ public:\n+  MOCK_METHOD(absl::StatusOr<std::unique_ptr<Executable>>, RunBackend,\n+              (std::unique_ptr<HloModule> module, se::StreamExecutor* executor,\n+               const CompileOptions& options),\n+              (override));\n+  MOCK_METHOD(se::Platform::Id, PlatformId, (), (const, override));\n+  MOCK_METHOD(absl::StatusOr<std::unique_ptr<HloModule>>, RunHloPasses,\n+              (std::unique_ptr<HloModule> module, se::StreamExecutor* executor,\n+               const CompileOptions& options),\n+              (override));\n+  MOCK_METHOD(absl::StatusOr<std::vector<std::unique_ptr<Executable>>>, Compile,\n+              (std::unique_ptr<HloModuleGroup> module_group,\n+               std::vector<std::vector<se::StreamExecutor*>> stream_execs,\n+               const CompileOptions& options),\n+              (override));\n+  MOCK_METHOD(\n+      absl::StatusOr<std::vector<std::unique_ptr<AotCompilationResult>>>,\n+      CompileAheadOfTime,\n+      (std::unique_ptr<HloModuleGroup> module_group,\n+       const AotCompilationOptions& options),\n+      (override));\n+  MOCK_METHOD(HloCostAnalysis::ShapeSizeFunction, ShapeSizeBytesFunction, (),\n+              (const, override));\n+};\n+\n+TEST_F(NativeEmitterBackendTest, CompileSetsIsAutotuningCompilationOption) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto reduction_module,\n+                          ParseAndReturnVerifiedModule(kReductionFusionHlo));\n+  auto fusion = reduction_module->entry_computation()->root_instruction();\n+  MockCompiler mock_compiler;\n+  NativeEmitterBackend backend(\n+      PlatformUtil::GetDefaultPlatform().value()->ExecutorForDevice(0).value(),\n+      &debug_options_, &mock_compiler);\n+  // Call GetDefaultConfig on the fusion instruction.\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<BackendConfig> config,\n+                          backend.GetDefaultConfig(*(fusion)));\n+  EXPECT_CALL(\n+      mock_compiler,\n+      RunBackend(\n+          testing::_, testing::_,\n+          testing::Field(&Compiler::CompileOptions::is_autotuning_compilation,\n+                         true)))\n+      .WillOnce(testing::Return(std::unique_ptr<Executable>()));\n+  // Attempt to compile the fusion using the retrieved backend config.\n+  EXPECT_THAT(backend.Compile(*fusion, *config), absl_testing::IsOk());\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "4acad28c200441b0f8a8cc6a3acfa880b9bf97bd",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton_test.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 16,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0fbb870fa35d31fc2a7ddb66b41a2e0e82707466/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0fbb870fa35d31fc2a7ddb66b41a2e0e82707466/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton_test.cc?ref=0fbb870fa35d31fc2a7ddb66b41a2e0e82707466",
            "patch": "@@ -69,33 +69,34 @@ const char kHlo[] = R\"(\n \n class TritonBackendTest : public HloHardwareIndependentTestBase {\n  protected:\n-  TritonBackendTest()\n-      : backend_(PlatformUtil::GetDefaultPlatform()\n-                     .value()\n-                     ->ExecutorForDevice(0)\n-                     .value(),\n-                 &debug_options_, &compiler_) {\n+  TritonBackendTest() {\n     // TODO(b/315957220): Remove the experimental flags once TMA is enabled by\n     // default.\n     debug_options_.set_xla_gpu_experimental_enable_triton_tma(true);\n+    backend_ =\n+        std::make_unique<TritonBackend>(PlatformUtil::GetDefaultPlatform()\n+                                            .value()\n+                                            ->ExecutorForDevice(0)\n+                                            .value(),\n+                                        &debug_options_, &compiler_);\n   }\n \n   DebugOptions debug_options_;\n   NVPTXCompiler compiler_;\n-  TritonBackend backend_;\n+  std::unique_ptr<TritonBackend> backend_;\n };\n \n TEST_F(TritonBackendTest, GetSupportedConfigs) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           ParseAndReturnVerifiedModule(kHlo));\n \n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> configs =\n-      backend_.GetSupportedConfigs(\n+      backend_->GetSupportedConfigs(\n           *(module->entry_computation()->root_instruction()));\n   EXPECT_THAT(configs, absl_testing::IsOk());\n   EXPECT_GT(configs.value().size(), 0);\n \n-  if (backend_.target_config()\n+  if (backend_->target_config()\n           .device_description.cuda_compute_capability()\n           .IsAtLeastHopper()) {\n     auto count_tma_allowed =\n@@ -119,11 +120,11 @@ TEST_F(TritonBackendTest, GetSupportedConfigsRestrictedDefaultSearch) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           ParseAndReturnVerifiedModule(kHlo));\n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> default_configs =\n-      backend_.GetSupportedConfigs(\n+      backend_->GetSupportedConfigs(\n           *(module->entry_computation()->root_instruction()));\n   debug_options_.set_xla_gpu_exhaustive_tiling_search(true);\n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n-      exhaustive_configs = backend_.GetSupportedConfigs(\n+      exhaustive_configs = backend_->GetSupportedConfigs(\n           *(module->entry_computation()->root_instruction()));\n   EXPECT_THAT(default_configs, IsOk());\n   EXPECT_THAT(exhaustive_configs, IsOk());\n@@ -138,7 +139,7 @@ TEST_F(TritonBackendTest, GetSupportedConfigsForUnsupportedInstruction) {\n                                           ->called_computations()[0]\n                                           ->root_instruction();\n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> configs =\n-      backend_.GetSupportedConfigs(*unsupported_instr);\n+      backend_->GetSupportedConfigs(*unsupported_instr);\n   EXPECT_THAT(configs, absl_testing::IsOk());\n   EXPECT_THAT(configs.value(), testing::IsEmpty());\n }\n@@ -150,7 +151,7 @@ TEST_F(TritonBackendTest, GetDefaultConfig) {\n       TritonGemmConfig(64, 64, 64, 1, 1, 2, 1, false).ToProto();\n \n   absl::StatusOr<std::unique_ptr<BackendConfig>> config =\n-      backend_.GetDefaultConfig(\n+      backend_->GetDefaultConfig(\n           *(module->entry_computation()->root_instruction()));\n \n   EXPECT_THAT(config, absl_testing::IsOk());\n@@ -167,7 +168,7 @@ TEST_F(TritonBackendTest, GetDefaultConfigForUnsupportedInstruction) {\n                                           ->called_computations()[0]\n                                           ->root_instruction();\n   absl::StatusOr<std::unique_ptr<BackendConfig>> config =\n-      backend_.GetDefaultConfig(*unsupported_instr);\n+      backend_->GetDefaultConfig(*unsupported_instr);\n   EXPECT_THAT(config.status(), StatusIs(absl::StatusCode::kInvalidArgument));\n }\n \n@@ -176,9 +177,9 @@ TEST_F(TritonBackendTest, Compile) {\n                           ParseAndReturnVerifiedModule(kHlo));\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<BackendConfig> config,\n-      backend_.GetDefaultConfig(\n+      backend_->GetDefaultConfig(\n           *(module->entry_computation()->root_instruction())));\n-  absl::StatusOr<std::unique_ptr<Executable>> executable = backend_.Compile(\n+  absl::StatusOr<std::unique_ptr<Executable>> executable = backend_->Compile(\n       *(module->entry_computation()->root_instruction()), *config);\n   EXPECT_THAT(executable, absl_testing::IsOk());\n }"
        },
        {
            "sha": "bbe73bdf991aaeb021999f21c9359d34288bfdd0",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0fbb870fa35d31fc2a7ddb66b41a2e0e82707466/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0fbb870fa35d31fc2a7ddb66b41a2e0e82707466/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=0fbb870fa35d31fc2a7ddb66b41a2e0e82707466",
            "patch": "@@ -2685,9 +2685,9 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n           /*alias_info=*/std::move(alias_info),\n           /*debug_options=*/std::move(debug_opts),\n           /*device_description=*/gpu_device_info,\n-          /*debug_module=*/options.is_autotuning_compilation\n-              ? std::unique_ptr<HloModule>()\n-              : std::move(module),\n+          // TODO b/407494653: AutotunerPass requires module to compile.\n+          // Remove module on autotuning runs once this is fixed.\n+          /*debug_module=*/std::move(module),\n           /*enable_debug_info_manager=*/!options.is_autotuning_compilation}));\n \n   if (embed_ir_in_executable) {"
        }
    ],
    "stats": {
        "total": 145,
        "additions": 106,
        "deletions": 39
    }
}