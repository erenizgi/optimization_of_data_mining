{
    "author": "WillFroom",
    "message": "[XLA:CPU/GPU] Support i2 in lower tensors pass.\n\nPiperOrigin-RevId: 808497782",
    "sha": "1a336addda937ed0e39313b674c1412ee20960ed",
    "files": [
        {
            "sha": "a3f5dedf78e05077dd3f814f99fc956c9f18b7f3",
            "filename": "third_party/xla/xla/codegen/emitters/tests/loop/s2_to_s8.hlo",
            "status": "added",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1a336addda937ed0e39313b674c1412ee20960ed/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftests%2Floop%2Fs2_to_s8.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1a336addda937ed0e39313b674c1412ee20960ed/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftests%2Floop%2Fs2_to_s8.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftests%2Floop%2Fs2_to_s8.hlo?ref=1a336addda937ed0e39313b674c1412ee20960ed",
            "patch": "@@ -0,0 +1,7 @@\n+// RUN: gpu_test_correctness %s\n+// RUN: cpu_test_correctness %s\n+\n+fusion {\n+  %p0 = s2[1001] parameter(0)\n+  ROOT %add = s8[1001] convert(%p0)\n+}"
        },
        {
            "sha": "debe187cce68f0dcd99d3278e7a8c3e0b9dff0fb",
            "filename": "third_party/xla/xla/codegen/emitters/tests/loop/s8_to_s2.hlo",
            "status": "added",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1a336addda937ed0e39313b674c1412ee20960ed/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftests%2Floop%2Fs8_to_s2.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1a336addda937ed0e39313b674c1412ee20960ed/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftests%2Floop%2Fs8_to_s2.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftests%2Floop%2Fs8_to_s2.hlo?ref=1a336addda937ed0e39313b674c1412ee20960ed",
            "patch": "@@ -0,0 +1,7 @@\n+// RUN: gpu_test_correctness %s\n+// RUN: cpu_test_correctness %s\n+\n+fusion {\n+  %p0 = s8[256] parameter(0)\n+  ROOT %add = s2[256] convert(%p0)\n+}"
        },
        {
            "sha": "1aafdd80ab4fc8cd093df1b0c488453e132af7c4",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1a336addda937ed0e39313b674c1412ee20960ed/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1a336addda937ed0e39313b674c1412ee20960ed/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2FBUILD?ref=1a336addda937ed0e39313b674c1412ee20960ed",
            "patch": "@@ -92,6 +92,7 @@ cc_library(\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/numeric:bits\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\","
        },
        {
            "sha": "04899580c4364b99c61dc905be6f2a9c4e46b142",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/lower_tensors.cc",
            "status": "modified",
            "additions": 89,
            "deletions": 65,
            "changes": 154,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1a336addda937ed0e39313b674c1412ee20960ed/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Flower_tensors.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1a336addda937ed0e39313b674c1412ee20960ed/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Flower_tensors.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Flower_tensors.cc?ref=1a336addda937ed0e39313b674c1412ee20960ed",
            "patch": "@@ -23,6 +23,7 @@ limitations under the License.\n \n #include \"absl/algorithm/container.h\"\n #include \"absl/log/check.h\"\n+#include \"absl/numeric/bits.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/STLExtras.h\"\n@@ -323,27 +324,53 @@ Value GetLinearIndex(ValueRange indices, mlir::ImplicitLocOpBuilder& b) {\n   return b.create<mlir::arith::IndexCastUIOp>(index_ty, index);\n }\n \n-std::tuple<Value, Value> GetI4IndexAndNibble(Value linear_index,\n-                                             mlir::ImplicitLocOpBuilder& b) {\n-  Value zero = b.create<mlir::arith::ConstantIntOp>(linear_index.getType(), 0);\n-  Value one = b.create<mlir::arith::ConstantIntOp>(linear_index.getType(), 1);\n-  Value is_low_nibble = b.create<mlir::arith::CmpIOp>(\n-      mlir::arith::CmpIPredicate::eq, zero,\n-      b.create<mlir::arith::AndIOp>(linear_index, one));\n-  Value i8_index = b.create<mlir::arith::ShRUIOp>(linear_index, one);\n-  return {i8_index, is_low_nibble};\n+// If the provided type is a sub-byte type get its bit-width, else nullopt.\n+std::optional<int> GetSubByteBitWidth(Type element_type) {\n+  if (element_type.isIntOrFloat()) {\n+    int bit_width = element_type.getIntOrFloatBitWidth();\n+    if (bit_width == 4 || bit_width == 2) {\n+      return bit_width;\n+    }\n+  }\n+  return std::nullopt;\n+}\n+\n+// Get the number of bits used to index into a sub-byte type.\n+int SubByteIndexingBits(int bit_width) {\n+  CHECK_LT(bit_width, 8) << \"Passed width is not a sub-byte\";\n+  return absl::bit_width<unsigned int>(8 / bit_width) - 1;\n+}\n+\n+std::tuple<Value, Value> GetSubByteIndex(Value linear_index, int bit_width,\n+                                         mlir::ImplicitLocOpBuilder& b) {\n+  CHECK_LT(bit_width, 8) << \"Passed width is not a sub-byte\";\n+  int sub_byte_indexing_bits = SubByteIndexingBits(bit_width);\n+  // Get the sub-byte index by just masking out the high bits.\n+  Value sub_byte_index = b.create<mlir::arith::TruncIOp>(\n+      b.getI8Type(), b.create<mlir::arith::AndIOp>(\n+                         linear_index, b.create<mlir::arith::ConstantIntOp>(\n+                                           linear_index.getType(),\n+                                           (1 << sub_byte_indexing_bits) - 1)));\n+  //  Calculate the bit shift (assume little-endianness).\n+  Value sub_byte_shift = b.create<mlir::arith::MulIOp>(\n+      b.create<mlir::arith::ConstantIntOp>(b.getI8Type(), bit_width),\n+      sub_byte_index);\n+  Value byte_shift = b.create<mlir::arith::ConstantIntOp>(\n+      linear_index.getType(), sub_byte_indexing_bits);\n+  Value i8_index = b.create<mlir::arith::ShRUIOp>(linear_index, byte_shift);\n+  return {i8_index, sub_byte_shift};\n }\n \n ml::GEPOp CreateGep(TypedValue<mlir::RankedTensorType> tensor,\n                     Value linear_index, mlir::ImplicitLocOpBuilder& b) {\n   mlir::RankedTensorType tensor_type = tensor.getType();\n   Type element_type = tensor_type.getElementType();\n   int64_t num_elements = tensor_type.getNumElements();\n-  if (element_type.isIntOrFloat() &&\n-      element_type.getIntOrFloatBitWidth() == 4) {\n+  std::optional<int> sub_byte_width = GetSubByteBitWidth(element_type);\n+  if (sub_byte_width) {\n     element_type = b.getI8Type();\n     // Elements are packed.\n-    num_elements = CeilOfRatio<int64_t>(num_elements, 2);\n+    num_elements = CeilOfRatio<int64_t>(num_elements, 8 / *sub_byte_width);\n   }\n   auto ptr = ml::LLVMPointerType::get(b.getContext());\n   auto tensor_ptr =\n@@ -373,11 +400,11 @@ struct RewriteTensorExtract : OpRewritePattern<mlir::tensor::ExtractOp> {\n     mlir::ImplicitLocOpBuilder b(op.getLoc(), rewriter);\n     auto linear_index = GetLinearIndex(op.getIndices(), b);\n     Type element_type = op.getTensor().getType().getElementType();\n-    Value is_low_nibble = nullptr;\n-    if (element_type.isIntOrFloat() &&\n-        element_type.getIntOrFloatBitWidth() == 4) {\n-      std::tie(linear_index, is_low_nibble) =\n-          GetI4IndexAndNibble(linear_index, b);\n+    Value sub_byte_shift = nullptr;\n+    std::optional<int> sub_byte_width = GetSubByteBitWidth(element_type);\n+    if (sub_byte_width) {\n+      std::tie(linear_index, sub_byte_shift) =\n+          GetSubByteIndex(linear_index, *sub_byte_width, b);\n     }\n \n     auto gep = CreateGep(op.getTensor(), linear_index, b);\n@@ -390,12 +417,10 @@ struct RewriteTensorExtract : OpRewritePattern<mlir::tensor::ExtractOp> {\n     }\n     auto load = load_op.getResult();\n \n-    if (is_low_nibble) {\n-      auto high_value = b.create<mlir::arith::ShRUIOp>(\n-          load, b.create<mlir::arith::ConstantIntOp>(load.getType(), 4));\n+    if (sub_byte_shift) {\n       load = b.create<mlir::arith::TruncIOp>(\n-          rewriter.getI4Type(),\n-          b.create<mlir::arith::SelectOp>(is_low_nibble, load, high_value));\n+          b.getIntegerType(*sub_byte_width),\n+          b.create<mlir::arith::ShRUIOp>(load, sub_byte_shift));\n     }\n \n     if (op.getType().isIntOrFloat()) {\n@@ -428,11 +453,12 @@ struct RewriteTransferRead : OpRewritePattern<vector::TransferReadOp> {\n       vector_type = vector_type.cloneWith(std::nullopt, b.getI8Type());\n     }\n     mlir::Type gep_element_type = vector_type.getElementType();\n-    if (gep_element_type.isIntOrFloat() &&\n-        gep_element_type.getIntOrFloatBitWidth() == 4) {\n+    std::optional<int> sub_byte_width = GetSubByteBitWidth(gep_element_type);\n+    if (sub_byte_width) {\n       linear_index = b.create<arith::ShRUIOp>(\n           linear_index,\n-          b.create<arith::ConstantIntOp>(linear_index.getType(), 1));\n+          b.create<arith::ConstantIntOp>(linear_index.getType(),\n+                                         SubByteIndexingBits(*sub_byte_width)));\n     }\n     auto gep = CreateGep(source, linear_index, b);\n \n@@ -480,14 +506,16 @@ struct RewriteTensorInsert : OpRewritePattern<mlir::tensor::InsertOp> {\n     auto scalar_value = op.getScalar();\n \n     // For i4 we store 2 values into one byte. This needs special handling here.\n-    if (tensor_dest.getType().getElementType().isIntOrFloat() &&\n-        tensor_dest.getType().getElementType().getIntOrFloatBitWidth() == 4) {\n+    Type element_type = tensor_dest.getType().getElementType();\n+    std::optional<int> sub_byte_width = GetSubByteBitWidth(element_type);\n+    if (sub_byte_width) {\n       // We need to use directly op.getDest() as input, otherwise the following\n       // rewrite might remove the only user of it.\n       tensor_dest = op.getDest();\n-      Value is_low_nibble;\n-      std::tie(linear_index, is_low_nibble) =\n-          GetI4IndexAndNibble(linear_index, b);\n+      // Value is_low_nibble;\n+      Value sub_byte_shift = nullptr;\n+      std::tie(linear_index, sub_byte_shift) =\n+          GetSubByteIndex(linear_index, *sub_byte_width, b);\n \n       // Technically we should half the number of elements when going to i8\n       // element type, but it doesn't really matter because we only actually use\n@@ -499,34 +527,33 @@ struct RewriteTensorInsert : OpRewritePattern<mlir::tensor::InsertOp> {\n       auto tensor_dest_i8 =\n           b.create<UnrealizedConversionCastOp>(tensor_ty, tensor_dest)\n               .getResult(0);\n-      if (scalar_value.getType() != rewriter.getI4Type()) {\n-        scalar_value =\n-            b.create<arith::BitcastOp>(rewriter.getI4Type(), scalar_value);\n+      if (scalar_value.getType() != b.getIntegerType(*sub_byte_width)) {\n+        scalar_value = b.create<arith::BitcastOp>(\n+            b.getIntegerType(*sub_byte_width), scalar_value);\n       }\n       scalar_value = b.create<mlir::arith::ExtUIOp>(ty, scalar_value);\n \n+      // Mask of 1s in the sub-byte position & 0s elsewhere.\n+      Value mask = b.create<mlir::arith::ShLIOp>(\n+          b.create<mlir::arith::ConstantIntOp>(ty, (1 << *sub_byte_width) - 1),\n+          sub_byte_shift);\n+      // Mask of 0s in the sub-byte position & 1s elsewhere.\n+      Value inverse_mask = b.create<mlir::arith::XOrIOp>(\n+          b.create<mlir::arith::ConstantIntOp>(ty, 0xff), mask);\n+\n+      Value shifted_value =\n+          b.create<mlir::arith::ShLIOp>(scalar_value, sub_byte_shift);\n+\n       // We need AtomicRMWOp because it can happen that different threads try to\n       // access the same memory location.\n       auto atomic_rmw = b.create<AtomicRMWOp>(tensor_dest_i8, linear_index);\n       mlir::ImplicitLocOpBuilder body_builder(atomic_rmw.getLoc(),\n                                               atomic_rmw.getBodyBuilder());\n       Value current_value = atomic_rmw.getCurrentValue();\n-      Value low_updated = body_builder.create<mlir::arith::OrIOp>(\n-          body_builder.create<mlir::arith::AndIOp>(\n-              current_value,\n-              body_builder.create<mlir::arith::ConstantIntOp>(ty, 0xf0)),\n-          body_builder.create<mlir::arith::AndIOp>(\n-              scalar_value,\n-              body_builder.create<mlir::arith::ConstantIntOp>(ty, 0x0f)));\n-      Value high_updated = body_builder.create<mlir::arith::OrIOp>(\n-          body_builder.create<mlir::arith::AndIOp>(\n-              current_value,\n-              body_builder.create<mlir::arith::ConstantIntOp>(ty, 0x0f)),\n-          body_builder.create<mlir::arith::ShLIOp>(\n-              scalar_value,\n-              body_builder.create<mlir::arith::ConstantIntOp>(ty, 4)));\n-      Value new_value = body_builder.create<mlir::arith::SelectOp>(\n-          is_low_nibble, low_updated, high_updated);\n+      Value masked_out_current_value =\n+          body_builder.create<mlir::arith::AndIOp>(current_value, inverse_mask);\n+      Value new_value = body_builder.create<mlir::arith::OrIOp>(\n+          masked_out_current_value, shifted_value);\n       body_builder.create<scf::YieldOp>(new_value);\n       Value casted_result = b.create<UnrealizedConversionCastOp>(\n                                  tensor_dest.getType(), atomic_rmw.getResult())\n@@ -578,11 +605,12 @@ struct RewriteTransferWrite : OpRewritePattern<vector::TransferWriteOp> {\n           op.getVectorType().cloneWith(std::nullopt, b.getI8Type()),\n           vector_value);\n     }\n-    if (vector_element_type.isIntOrFloat() &&\n-        vector_element_type.getIntOrFloatBitWidth() == 4) {\n+    std::optional<int> sub_byte_width = GetSubByteBitWidth(vector_element_type);\n+    if (sub_byte_width) {\n       linear_index = b.create<arith::ShRUIOp>(\n           linear_index,\n-          b.create<arith::ConstantIntOp>(linear_index.getType(), 1));\n+          b.create<arith::ConstantIntOp>(linear_index.getType(),\n+                                         SubByteIndexingBits(*sub_byte_width)));\n     }\n     auto gep = CreateGep(tensor_dest, linear_index, b);\n \n@@ -1193,13 +1221,12 @@ class RewriteAtomicRMW : public OpRewritePattern<AtomicRMWOp> {\n     // Calculate load address for the input.\n     mlir::ImplicitLocOpBuilder b(op.getLoc(), rewriter);\n     Value linear_index = GetLinearIndex(op.getIndices(), b);\n-    Value is_low_nibble;\n+    Value sub_byte_shift;\n \n-    bool is_4_bit_wide =\n-        result_ty.isIntOrFloat() && result_ty.getIntOrFloatBitWidth() == 4;\n-    if (is_4_bit_wide) {\n-      std::tie(linear_index, is_low_nibble) =\n-          GetI4IndexAndNibble(linear_index, b);\n+    std::optional<int> sub_byte_bit_width = GetSubByteBitWidth(result_ty);\n+    if (sub_byte_bit_width) {\n+      std::tie(linear_index, sub_byte_shift) =\n+          GetSubByteIndex(linear_index, *sub_byte_bit_width, b);\n     }\n     Value addr = CreateGep(input, linear_index, b);\n     Value shift, mask;\n@@ -1218,17 +1245,14 @@ class RewriteAtomicRMW : public OpRewritePattern<AtomicRMWOp> {\n                                         rewriter.getI8Type(), addr, index,\n                                         mlir::LLVM::GEPNoWrapFlags::inbounds);\n \n-      // Calculate the bit shift (assume little-endianness).\n       Value offset = rewriter.create<ml::TruncOp>(loc, atomic_ty, addr_offset);\n       shift = rewriter.create<ml::MulOp>(\n           loc, offset,\n           rewriter.create<ml::ConstantOp>(loc, offset.getType(), 8));\n-      if (is_4_bit_wide) {\n-        auto c0 = rewriter.create<ml::ConstantOp>(loc, shift.getType(), 0);\n-        auto c4 = rewriter.create<ml::ConstantOp>(loc, shift.getType(), 4);\n-        auto subshift =\n-            rewriter.create<ml::SelectOp>(loc, is_low_nibble, c0, c4);\n-        shift = rewriter.create<ml::AddOp>(loc, shift, subshift);\n+      if (sub_byte_bit_width) {\n+        sub_byte_shift =\n+            rewriter.create<ml::ZExtOp>(loc, shift.getType(), sub_byte_shift);\n+        shift = rewriter.create<ml::AddOp>(loc, shift, sub_byte_shift);\n       }\n \n       // Compose the update mask."
        },
        {
            "sha": "161c879a7afb33a9310746e2d382ce05faad06e8",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/tests/lower_tensors.mlir",
            "status": "modified",
            "additions": 114,
            "deletions": 74,
            "changes": 188,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1a336addda937ed0e39313b674c1412ee20960ed/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Flower_tensors.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1a336addda937ed0e39313b674c1412ee20960ed/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Flower_tensors.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Ftests%2Flower_tensors.mlir?ref=1a336addda937ed0e39313b674c1412ee20960ed",
            "patch": "@@ -326,50 +326,48 @@ func.func @atomic_rmw_i4(%in: tensor<8xi4>, %i: index) -> (tensor<8xi4>) {\n   }\n   return %ret : tensor<8xi4>\n }\n-// CHECK-LABEL: func.func @atomic_rmw_i4(\n-// CHECK-SAME:    %[[VAL_0:.*]]: !llvm.ptr, %[[VAL_1:.*]]: index) {\n-\n-// CHECK-DAG:  %[[LC1_i4:.*]] = arith.constant 1 : i4\n-// CHECK-DAG:  %[[C0:.*]] = arith.constant 0 : i64\n-// CHECK-DAG:  %[[C1:.*]] = arith.constant 1 : i64\n-// CHECK-DAG:  %[[LC3:.*]] = llvm.mlir.constant(3 : i64) : i64\n-// CHECK-DAG:  %[[LCm1:.*]] = llvm.mlir.constant(-1 : i64) : i64\n-// CHECK-DAG:  %[[LC8:.*]] = llvm.mlir.constant(8 : i32) : i32\n-// CHECK-DAG:  %[[LC0:.*]] = llvm.mlir.constant(0 : i32) : i32\n-// CHECK-DAG:  %[[LC4:.*]] = llvm.mlir.constant(4 : i32) : i32\n-// CHECK-DAG:  %[[LCm1_i32:.*]] = llvm.mlir.constant(-1 : i32) : i32\n-// CHECK-DAG:  %[[LC15:.*]] = llvm.mlir.constant(15 : i32) : i32\n-// CHECK-DAG:  %[[LCTRUE:.*]] = llvm.mlir.constant(true) : i1\n-\n-// CHECK:  %[[VAL_13:.*]] = arith.index_castui %[[VAL_1]] : index to i64\n-// CHECK:  %[[VAL_14:.*]] = arith.andi %[[VAL_13]], %[[C1]] : i64\n-// CHECK:  %[[VAL_15:.*]] = arith.cmpi eq, %[[VAL_14]], %[[C0]] : i64\n-// CHECK:  %[[VAL_16:.*]] = arith.shrui %[[VAL_13]], %[[C1]] : i64\n-// CHECK:  %[[VAL_17:.*]] = llvm.getelementptr inbounds %[[VAL_0]][0, %[[VAL_16]]]\n-// CHECK-SAME:   : (!llvm.ptr, i64) -> !llvm.ptr, !llvm.array<4 x i8>\n-\n-// CHECK:  %[[VAL_18:.*]] = llvm.ptrtoint %[[VAL_17]] : !llvm.ptr to i64\n-// CHECK:  %[[VAL_19:.*]] = llvm.and %[[VAL_18]], %[[LC3]] : i64\n-// CHECK:  %[[VAL_20:.*]] = llvm.mul %[[VAL_19]], %[[LCm1]] : i64\n-// CHECK:  %[[VAL_21:.*]] = llvm.getelementptr inbounds %[[VAL_17]][%[[VAL_20]]]\n-// CHECK-SAME:   : (!llvm.ptr, i64) -> !llvm.ptr, i8\n-\n-// CHECK:  %[[VAL_22:.*]] = llvm.trunc %[[VAL_19]] : i64 to i32\n-// CHECK:  %[[VAL_23:.*]] = llvm.mul %[[VAL_22]], %[[LC8]] : i32\n-// CHECK:  %[[VAL_24:.*]] = llvm.select %[[VAL_15]], %[[LC0]], %[[LC4]]\n-// CHECK:  %[[VAL_25:.*]] = llvm.add %[[VAL_23]], %[[VAL_24]] : i32\n-// CHECK:  %[[VAL_26:.*]] = llvm.shl %[[LC15]], %[[VAL_25]] : i32\n-// CHECK:  %[[VAL_27:.*]] = llvm.xor %[[LCm1_i32]], %[[VAL_26]] : i32\n-// CHECK:  %[[LOAD_4BYTES:.*]] = llvm.load %[[VAL_21]] : !llvm.ptr -> i32\n-// CHECK:  scf.while (%[[VAL_30:.*]] = %[[LOAD_4BYTES]]) : (i32) -> i32 {\n-// CHECK:    %[[VAL_31:.*]] = llvm.lshr %[[VAL_30]], %[[VAL_25]] : i32\n-// CHECK:    %[[VAL_32:.*]] = llvm.trunc %[[VAL_31]] : i32 to i4\n-// CHECK:    %[[VAL_33:.*]] = arith.addi %[[VAL_32]], %[[LC1_i4]] : i4\n-// CHECK:    %[[VAL_34:.*]] = llvm.zext %[[VAL_33]] : i4 to i32\n-// CHECK:    %[[VAL_35:.*]] = llvm.and %[[VAL_30]], %[[VAL_27]] : i32\n-// CHECK:    %[[VAL_36:.*]] = llvm.shl %[[VAL_34]], %[[VAL_25]] : i32\n-// CHECK:    %[[VAL_37:.*]] = llvm.or %[[VAL_35]], %[[VAL_36]] : i32\n-// CHECK:    llvm.cmpxchg\n+\n+// CHECK-LABEL: @atomic_rmw_i4(\n+// CHECK-SAME:    %[[ARG0:.*]]: !llvm.ptr, %[[ARG1:.*]]: index) {\n+// CHECK:       %[[C1_I4:.*]] = arith.constant 1 : i4\n+// CHECK:       %[[C1_I64:.*]] = arith.constant 1 : i64\n+// CHECK:       %[[C4_I8:.*]] = arith.constant 4 : i8\n+// CHECK:       %[[C3_I64:.*]] = llvm.mlir.constant(3 : i64) : i64\n+// CHECK:       %[[C_NEG1_I64:.*]] = llvm.mlir.constant(-1 : i64) : i64\n+// CHECK:       %[[C8_I32:.*]] = llvm.mlir.constant(8 : i32) : i32\n+// CHECK:       %[[C_NEG1_I32:.*]] = llvm.mlir.constant(-1 : i32) : i32\n+// CHECK:       %[[C15_I32:.*]] = llvm.mlir.constant(15 : i32) : i32\n+// CHECK:       %[[C_TRUE:.*]] = llvm.mlir.constant(true) : i1\n+// CHECK:       %[[ARG1_I64:.*]] = arith.index_castui %[[ARG1]] : index to i64\n+// CHECK:       %[[AND_1:.*]] = arith.andi %[[ARG1_I64]], %[[C1_I64]] : i64\n+// CHECK:       %[[TRUNC_I8:.*]] = arith.trunci %[[AND_1]] : i64 to i8\n+// CHECK:       %[[MUL_4:.*]] = arith.muli %[[TRUNC_I8]], %[[C4_I8]] : i8\n+// CHECK:       %[[SHR_1:.*]] = arith.shrui %[[ARG1_I64]], %[[C1_I64]] : i64\n+// CHECK:       %[[GEP_BASE:.*]] = llvm.getelementptr inbounds %[[ARG0]][0, %[[SHR_1]]] : (!llvm.ptr, i64) -> !llvm.ptr, !llvm.array<4 x i8>\n+// CHECK:       %[[PTR_INT:.*]] = llvm.ptrtoint %[[GEP_BASE]] : !llvm.ptr to i64\n+// CHECK:       %[[AND_3:.*]] = llvm.and %[[PTR_INT]], %[[C3_I64]] : i64\n+// CHECK:       %[[MUL_NEG1:.*]] = llvm.mul %[[AND_3]], %[[C_NEG1_I64]] : i64\n+// CHECK:       %[[GEP_OFFSET:.*]] = llvm.getelementptr inbounds %[[GEP_BASE]][%[[MUL_NEG1]]] : (!llvm.ptr, i64) -> !llvm.ptr, i8\n+// CHECK:       %[[TRUNC_I32:.*]] = llvm.trunc %[[AND_3]] : i64 to i32\n+// CHECK:       %[[MUL_8:.*]] = llvm.mul %[[TRUNC_I32]], %[[C8_I32]] : i32\n+// CHECK:       %[[ZEXT_I32:.*]] = llvm.zext %[[MUL_4]] : i8 to i32\n+// CHECK:       %[[ADD_19:.*]] = llvm.add %[[MUL_8]], %[[ZEXT_I32]] : i32\n+// CHECK:       %[[SHL_20:.*]] = llvm.shl %[[C15_I32]], %[[ADD_19]] : i32\n+// CHECK:       %[[XOR_21:.*]] = llvm.xor %[[C_NEG1_I32]], %[[SHL_20]] : i32\n+// CHECK:       %[[LOADED_I32:.*]] = llvm.load %[[GEP_OFFSET]] : !llvm.ptr -> i32\n+// CHECK:       %[[WHILE_RES:.*]] = scf.while (%[[ARG2:.*]] = %[[LOADED_I32]]) : (i32) -> i32 {\n+// CHECK:         %[[LSHR:.*]] = llvm.lshr %[[ARG2]], %[[ADD_19]] : i32\n+// CHECK:         %[[TRUNC_I4:.*]] = llvm.trunc %[[LSHR]] : i32 to i4\n+// CHECK:         %[[ADDI:.*]] = arith.addi %[[TRUNC_I4]], %[[C1_I4]] : i4\n+// CHECK:         %[[ZEXT_I32_2:.*]] = llvm.zext %[[ADDI]] : i4 to i32\n+// CHECK:         %[[AND_MASK_I32:.*]] = llvm.and %[[ARG2]], %[[XOR_21]] : i32\n+// CHECK:         %[[SHL_NEW:.*]] = llvm.shl %[[ZEXT_I32_2]], %[[ADD_19]] : i32\n+// CHECK:         %[[OR_NEW:.*]] = llvm.or %[[AND_MASK_I32]], %[[SHL_NEW]] : i32\n+// CHECK:         %[[CMPXCHG:.*]] = llvm.cmpxchg %[[GEP_OFFSET]], %[[ARG2]], %[[OR_NEW]] monotonic monotonic : !llvm.ptr, i32\n+// CHECK:         %[[EXTRACT0:.*]] = llvm.extractvalue %[[CMPXCHG]][0] : !llvm.struct<(i32, i1)>\n+// CHECK:         %[[EXTRACT1:.*]] = llvm.extractvalue %[[CMPXCHG]][1] : !llvm.struct<(i32, i1)>\n+// CHECK:         %[[XOR_34:.*]] = llvm.xor %[[EXTRACT1]], %[[C_TRUE]] : i1\n+// CHECK:         scf.condition\n \n // -----\n \n@@ -439,39 +437,81 @@ func.func @shared_i4() -> tensor<10xi4> {\n \n // -----\n \n-func.func @i4_load_store(%arg: tensor<10xi4>, %i: index, %j: index)\n-    -> tensor<10xi4> {\n+func.func @i4_load(%arg: tensor<10xi4>, %i: index)\n+    -> i4 {\n   %v = tensor.extract %arg[%i] : tensor<10xi4>\n-  %r = tensor.insert %v into %arg[%j] : tensor<10xi4>\n+  return %v : i4\n+}\n+\n+// CHECK-LABEL: @i4_load(\n+// CHECK-SAME:      %[[ARG0:.*]]: !llvm.ptr, %[[ARG1:.*]]: index) -> i4 {\n+// CHECK-DAG:     %[[C1_I64:.*]] = arith.constant 1 : i64\n+// CHECK-DAG:     %[[C4_I8:.*]] = arith.constant 4 : i8\n+// CHECK:         %[[ARG1_I64:.*]] = arith.index_castui %[[ARG1]] : index to i64\n+// CHECK:         %[[AND_1:.*]] = arith.andi %[[ARG1_I64]], %[[C1_I64]] : i64\n+// CHECK:         %[[TRUNC_I8:.*]] = arith.trunci %[[AND_1]] : i64 to i8\n+// CHECK:         %[[MUL_4:.*]] = arith.muli %[[TRUNC_I8]], %[[C4_I8]] : i8\n+// CHECK:         %[[SHR_1:.*]] = arith.shrui %[[ARG1_I64]], %[[C1_I64]] : i64\n+// CHECK:         %[[GEP_BASE:.*]] = llvm.getelementptr inbounds %[[ARG0]][0, %[[SHR_1]]] : (!llvm.ptr, i64) -> !llvm.ptr, !llvm.array<5 x i8>\n+// CHECK:         %[[LOADED_I8:.*]] = llvm.load %[[GEP_BASE]] : !llvm.ptr -> i8\n+// CHECK:         %[[SHR_VAL:.*]] = arith.shrui %[[LOADED_I8]], %[[MUL_4]] : i8\n+// CHECK:         %[[TRUNC_I4:.*]] = arith.trunci %[[SHR_VAL]] : i8 to i4\n+// CHECK:         return %[[TRUNC_I4]] : i4\n+\n+// -----\n+\n+func.func @i4_store(%arg: tensor<10xi4>, %v: i4, %i: index)\n+    -> tensor<10xi4> {\n+  %r = tensor.insert %v into %arg[%i] : tensor<10xi4>\n   return %r : tensor<10xi4>\n }\n-// CHECK-LABEL: @i4_load_store\n-// CHECK-DAG: %[[C4:.*]] = arith.constant 4 : i8\n-// CHECK-DAG: %[[C15:.*]] = arith.constant 15 : i8\n-// CHECK-DAG: %[[C_NEG16:.*]] = arith.constant -16 : i8\n-// CHECK: llvm.getelementptr\n-// CHECK-SAME: -> !llvm.ptr, !llvm.array<5 x i8>\n-// CHECK: %[[VALUE_I8:.*]] = arith.extui {{.*}} : i4 to i8\n-// CHECK: llvm.getelementptr\n-// CHECK-SAME: -> !llvm.ptr, !llvm.array<10 x i8>\n-// CHECK: %[[CURRENT_I32:.*]] = llvm.load\n-// CHECK-SAME: !llvm.ptr -> i32\n-// CHECK: scf.while (%[[INIT:.*]] = %[[CURRENT_I32]])\n-// CHECK: %[[SHIFTED:.*]] = llvm.lshr %[[INIT]]\n-// CHECK: %[[CURRENT:.*]] = llvm.trunc %[[SHIFTED]]\n-// CHECK-DAG: %[[MASKED_VALUE_I8:.*]] = arith.andi %[[VALUE_I8]], %[[C15]] : i8\n-// CHECK-DAG: %[[MASKED_CURRENT_LO:.*]] = arith.andi %[[CURRENT]], %[[C_NEG16]] : i8\n-// CHECK: %[[NEW_LO:.*]] = arith.ori %[[MASKED_CURRENT_LO]], %[[MASKED_VALUE_I8]] : i8\n-// CHECK-DAG: %[[VALUE_HI:.*]] = arith.shli %[[VALUE_I8]], %[[C4]] : i8\n-// CHECK-DAG: %[[MASKED_CURRENT_HI:.*]] = arith.andi %[[CURRENT]], %[[C15]] : i8\n-// CHECK: %[[NEW_HI:.*]] = arith.ori %[[MASKED_CURRENT_HI]], %[[VALUE_HI]] : i8\n-// CHECK: %[[NEW_VALUE:.*]] = arith.select %{{.*}}, %[[NEW_LO]], %[[NEW_HI]] : i8\n-// CHECK: %[[NEW_VALUE_I32:.*]] = llvm.zext %[[NEW_VALUE]]\n-// CHECK-DAG: %[[NEW_VALUE_SHIFTED:.*]] = llvm.shl %[[NEW_VALUE_I32]]\n-// CHECK-DAG: %[[MASKED_INIT:.*]] = llvm.and %[[INIT]]\n-// CHECK: %[[NEW_INIT:.*]] = llvm.or %[[MASKED_INIT]], %[[NEW_VALUE_SHIFTED]]\n-// CHECK: llvm.cmpxchg %{{.*}}, %[[INIT]], %[[NEW_INIT]] monotonic monotonic\n-// CHECK: scf.condition\n+\n+// CHECK-LABEL: @i4_store(\n+// CHECK-SAME:      %[[ARG0:.*]]: !llvm.ptr, %[[ARG1:.*]]: i4, %[[ARG2:.*]]: index) {\n+// CHECK-DAG:     %[[C1_I64:.*]] = arith.constant 1 : i64\n+// CHECK-DAG:     %[[C4_I8:.*]] = arith.constant 4 : i8\n+// CHECK-DAG:     %[[C15_I8:.*]] = arith.constant 15 : i8\n+// CHECK-DAG:     %[[C_NEG1_I8:.*]] = arith.constant -1 : i8\n+// CHECK-DAG:     %[[C3_I64:.*]] = llvm.mlir.constant(3 : i64) : i64\n+// CHECK-DAG:     %[[C_NEG1_I64:.*]] = llvm.mlir.constant(-1 : i64) : i64\n+// CHECK-DAG:     %[[C8_I32:.*]] = llvm.mlir.constant(8 : i32) : i32\n+// CHECK-DAG:     %[[C_NEG1_I32:.*]] = llvm.mlir.constant(-1 : i32) : i32\n+// CHECK-DAG:     %[[C255_I32:.*]] = llvm.mlir.constant(255 : i32) : i32\n+// CHECK-DAG:     %[[C_TRUE:.*]] = llvm.mlir.constant(true) : i1\n+// CHECK:         %[[CAST_ARG0:.*]] = builtin.unrealized_conversion_cast %[[ARG0]] : !llvm.ptr to tensor<10xi4>\n+// CHECK:         %[[ARG2_I64:.*]] = arith.index_castui %[[ARG2]] : index to i64\n+// CHECK:         %[[AND_1:.*]] = arith.andi %[[ARG2_I64]], %[[C1_I64]] : i64\n+// CHECK:         %[[TRUNC_I8:.*]] = arith.trunci %[[AND_1]] : i64 to i8\n+// CHECK:         %[[MUL_4:.*]] = arith.muli %[[TRUNC_I8]], %[[C4_I8]] : i8\n+// CHECK:         %[[SHR_1:.*]] = arith.shrui %[[ARG2_I64]], %[[C1_I64]] : i64\n+// CHECK:         %[[CAST_TENSOR:.*]] = builtin.unrealized_conversion_cast %[[CAST_ARG0]] : tensor<10xi4> to tensor<10xi8>\n+// CHECK:         %[[EXT_ARG1:.*]] = arith.extui %[[ARG1]] : i4 to i8\n+// CHECK:         %[[SHL_15:.*]] = arith.shli %[[C15_I8]], %[[MUL_4]] : i8\n+// CHECK:         %[[XOR_NEG1_I8:.*]] = arith.xori %[[SHL_15]], %[[C_NEG1_I8]] : i8\n+// CHECK:         %[[SHL_EXT:.*]] = arith.shli %[[EXT_ARG1]], %[[MUL_4]] : i8\n+// CHECK:         %[[CAST_TENSOR_PTR:.*]] = builtin.unrealized_conversion_cast %[[CAST_TENSOR]] : tensor<10xi8> to !llvm.ptr\n+// CHECK:         %[[GEP_BASE:.*]] = llvm.getelementptr inbounds %[[CAST_TENSOR_PTR]][0, %[[SHR_1]]] : (!llvm.ptr, i64) -> !llvm.ptr, !llvm.array<10 x i8>\n+// CHECK:         %[[PTR_INT:.*]] = llvm.ptrtoint %[[GEP_BASE]] : !llvm.ptr to i64\n+// CHECK:         %[[AND_3:.*]] = llvm.and %[[PTR_INT]], %[[C3_I64]] : i64\n+// CHECK:         %[[MUL_NEG1:.*]] = llvm.mul %[[AND_3]], %[[C_NEG1_I64]] : i64\n+// CHECK:         %[[GEP_OFFSET:.*]] = llvm.getelementptr inbounds %[[GEP_BASE]][%[[MUL_NEG1]]] : (!llvm.ptr, i64) -> !llvm.ptr, i8\n+// CHECK:         %[[TRUNC_I32:.*]] = llvm.trunc %[[AND_3]] : i64 to i32\n+// CHECK:         %[[MUL_8:.*]] = llvm.mul %[[TRUNC_I32]], %[[C8_I32]] : i32\n+// CHECK:         %[[SHL_255:.*]] = llvm.shl %[[C255_I32]], %[[MUL_8]] : i32\n+// CHECK:         %[[XOR_NEG1_I32:.*]] = llvm.xor %[[C_NEG1_I32]], %[[SHL_255]] : i32\n+// CHECK:         %[[LOADED_I32:.*]] = llvm.load %[[GEP_OFFSET]] : !llvm.ptr -> i32\n+// CHECK:         %[[WHILE_RES:.*]] = scf.while (%[[INIT:.*]] = %[[LOADED_I32]]) : (i32) -> i32 {\n+// CHECK:           %[[LSHR:.*]] = llvm.lshr %[[INIT]], %[[MUL_8]] : i32\n+// CHECK:           %[[TRUNC_I8_2:.*]] = llvm.trunc %[[LSHR]] : i32 to i8\n+// CHECK:           %[[AND_MASK_I8:.*]] = arith.andi %[[TRUNC_I8_2]], %[[XOR_NEG1_I8]] : i8\n+// CHECK:           %[[ORI_NEW_I8:.*]] = arith.ori %[[AND_MASK_I8]], %[[SHL_EXT]] : i8\n+// CHECK:           %[[ZEXT_I32:.*]] = llvm.zext %[[ORI_NEW_I8]] : i8 to i32\n+// CHECK:           %[[AND_MASK_I32:.*]] = llvm.and %[[INIT]], %[[XOR_NEG1_I32]] : i32\n+// CHECK:           %[[SHL_NEW:.*]] = llvm.shl %[[ZEXT_I32]], %[[MUL_8]] : i32\n+// CHECK:           %[[OR_NEW:.*]] = llvm.or %[[AND_MASK_I32]], %[[SHL_NEW]] : i32\n+// CHECK:           llvm.cmpxchg %[[GEP_OFFSET]], %[[INIT]], %[[OR_NEW]] monotonic monotonic : !llvm.ptr, i32\n+// CHECK:           scf.condition\n+\n \n // -----\n "
        }
    ],
    "stats": {
        "total": 357,
        "additions": 218,
        "deletions": 139
    }
}