{
    "author": "basioli-k",
    "message": "[XLA][codegen] Move checking if triton supports an HLO instruction outside of code emission\n\nPiperOrigin-RevId: 829425060",
    "sha": "94b36896014d2bfc899b8437802ccd76b3855cd4",
    "files": [
        {
            "sha": "9eb0e40226340e9b5dc4d2b71088e70ff584eb91",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 37,
            "deletions": 18,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/94b36896014d2bfc899b8437802ccd76b3855cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/94b36896014d2bfc899b8437802ccd76b3855cd4/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=94b36896014d2bfc899b8437802ccd76b3855cd4",
            "patch": "@@ -1286,11 +1286,6 @@ absl::StatusOr<TensorValue> EmitPad(\n     const TiledHloInstruction& tiled_pad,\n     absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values,\n     Value pid) {\n-  if (!IsTritonSupportedInstruction(*tiled_pad.hlo(),\n-                                    device_info.gpu_compute_capability())) {\n-    return absl::FailedPreconditionError(\n-        absl::StrCat(\"Pad is not supported: \", tiled_pad.hlo()->ToString()));\n-  }\n   // TODO(b/393299275): get rid of calls to `GetPaddedTileSizes` once tiling\n   // is using power of twos everywhere, including when propagating into the\n   // prologue of reductions.\n@@ -1495,19 +1490,6 @@ absl::StatusOr<std::vector<TensorValue>> EmitTiledComputation(\n     // Skip generating nested fusions, they are emitted by their consumer.\n     if (hlo->parent()->IsFusionComputation() &&\n         hlo->opcode() == HloOpcode::kFusion) {\n-      if (hlo->GetModule()\n-              ->config()\n-              .debug_options()\n-              .xla_gpu_experimental_scaled_dot_with_triton()) {\n-        continue;\n-      }\n-      CodegenDecision decision = IsTritonSupportedInstruction(\n-          *hlo, device_info.gpu_compute_capability());\n-      if (!decision.CanFuse()) {\n-        return absl::FailedPreconditionError(\n-            absl::StrCat(\"Fusion \", hlo->ToString(),\n-                         \" is not supported: \", decision.Explain()));\n-      }\n       VLOG(1) << \"Skipping nested fusion: \" << hlo->ToString();\n       continue;\n     }\n@@ -1845,11 +1827,48 @@ mlir::MemRefType GetMemRefType(const Shape& shape, mlir::Type element_type) {\n   return mlir::MemRefType::get(shape.dimensions(), storage_type, layout);\n }\n \n+absl::Status IsTritonSupportedFusion(const HloFusionInstruction& fusion,\n+                                     const se::DeviceDescription& device_info) {\n+  const HloComputation* computation = fusion.fused_instructions_computation();\n+  for (const HloInstruction* hlo : computation->instructions()) {\n+    // Skip generating nested fusions, they are emitted by their consumer.\n+    if (hlo->parent()->IsFusionComputation() &&\n+        hlo->opcode() == HloOpcode::kFusion) {\n+      if (hlo->GetModule()\n+              ->config()\n+              .debug_options()\n+              .xla_gpu_experimental_scaled_dot_with_triton()) {\n+        continue;\n+      }\n+      CodegenDecision decision = IsTritonSupportedInstruction(\n+          *hlo, device_info.gpu_compute_capability());\n+      if (!decision.CanFuse()) {\n+        return absl::FailedPreconditionError(\n+            absl::StrCat(\"Fusion \", hlo->ToString(),\n+                         \" is not supported: \", decision.Explain()));\n+      }\n+      VLOG(1) << \"Skipping nested fusion: \" << hlo->ToString();\n+      continue;\n+    }\n+\n+    if (hlo->opcode() == HloOpcode::kPad) {\n+      if (!IsTritonSupportedInstruction(*hlo,\n+                                        device_info.gpu_compute_capability())) {\n+        return absl::FailedPreconditionError(\n+            absl::StrCat(\"Pad is not supported: \", hlo->ToString()));\n+      }\n+    }\n+  }\n+  return absl::OkStatus();\n+}\n+\n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n     absl::string_view fn_name, const HloFusionInstruction* fusion,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n     SymbolicExprContext& symbolic_expr_context) {\n+  TF_RETURN_IF_ERROR(IsTritonSupportedFusion(*fusion, device_info));\n+\n   // TODO: b/451959933 - Use reference or check pointer.\n   mlir::MLIRContext& mlir_context = *symbolic_expr_context.GetMLIRContext();\n   TF_ASSIGN_OR_RETURN(auto triton_module,"
        }
    ],
    "stats": {
        "total": 55,
        "additions": 37,
        "deletions": 18
    }
}