{
    "author": "basioli-k",
    "message": "[XLA][host offloading] Open source HloHostDeviceTypeCallWrapper.\n\nPiperOrigin-RevId: 797731473",
    "sha": "f8c3ce07459878b3a1fc007aaaae5b659bf79fd1",
    "files": [
        {
            "sha": "573ccfa8a9c03de5f242ff7e395240190eda20d8",
            "filename": "third_party/xla/xla/core/host_offloading/BUILD",
            "status": "modified",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f8c3ce07459878b3a1fc007aaaae5b659bf79fd1/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f8c3ce07459878b3a1fc007aaaae5b659bf79fd1/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2FBUILD?ref=f8c3ce07459878b3a1fc007aaaae5b659bf79fd1",
            "patch": "@@ -301,3 +301,44 @@ strict_cc_test(\n         \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n+\n+cc_library(\n+    name = \"hlo_host_device_type_call_wrapper\",\n+    srcs = [\n+        \"hlo_host_device_type_call_wrapper.cc\",\n+    ],\n+    hdrs = [\n+        \"hlo_host_device_type_call_wrapper.h\",\n+    ],\n+    compatible_with = get_compatible_with_libtpu_portable(),\n+    deps = [\n+        \"//xla:shape_tree\",\n+        \"//xla:shape_util\",\n+        \"//xla:side_effect_util\",\n+        \"//xla:status_macros\",\n+        \"//xla:util\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/core/host_offloading:annotate_host_compute_offload\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/pass:hlo_pass\",\n+        \"//xla/hlo/transforms:offloaded_instruction_wrapper\",\n+        \"//xla/hlo/transforms/simplifiers:hlo_dce\",\n+        \"//xla/hlo/transforms/simplifiers:tuple_simplifier\",\n+        \"//xla/service:call_graph\",\n+        \"//xla/service:call_inliner\",\n+        \"//xla/service:host_offload_utils\",\n+        \"//xla/service:memory_annotations_hdr\",\n+        \"//xla/service:tuple_util\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@local_tsl//tsl/platform:casts\",\n+    ],\n+)"
        },
        {
            "sha": "45c861223d89db43d77b0d6434ea9df270b2bff0",
            "filename": "third_party/xla/xla/core/host_offloading/hlo_host_device_type_call_wrapper.cc",
            "status": "added",
            "additions": 357,
            "deletions": 0,
            "changes": 357,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f8c3ce07459878b3a1fc007aaaae5b659bf79fd1/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2Fhlo_host_device_type_call_wrapper.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f8c3ce07459878b3a1fc007aaaae5b659bf79fd1/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2Fhlo_host_device_type_call_wrapper.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2Fhlo_host_device_type_call_wrapper.cc?ref=f8c3ce07459878b3a1fc007aaaae5b659bf79fd1",
            "patch": "@@ -0,0 +1,357 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/core/host_offloading/hlo_host_device_type_call_wrapper.h\"\n+\n+#include <cstdint>\n+#include <memory>\n+#include <vector>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/core/host_offloading/annotate_host_compute_offload.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/transforms/offloaded_instruction_wrapper.h\"\n+#include \"xla/hlo/transforms/simplifiers/hlo_dce.h\"\n+#include \"xla/hlo/transforms/simplifiers/tuple_simplifier.h\"\n+#include \"xla/service/call_graph.h\"\n+#include \"xla/service/call_inliner.h\"\n+#include \"xla/service/host_offload_utils.h\"\n+#include \"xla/service/memory_annotations.h\"\n+#include \"xla/service/tuple_util.h\"\n+#include \"xla/shape_tree.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/side_effect_util.h\"\n+#include \"xla/status_macros.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n+#include \"xla/xla_data.pb.h\"\n+#include \"tsl/platform/casts.h\"\n+\n+namespace xla {\n+\n+namespace {\n+bool IsPassThroughShardingOp(const HloInstruction& instr) {\n+  return instr.IsCustomCall(\"Sharding\") ||\n+         instr.IsCustomCall(\"SPMDShardToFullShape\") ||\n+         instr.IsCustomCall(\"SPMDFullToShardShape\");\n+}\n+\n+void RemoveComputeTypeFrontendAttribute(HloInstruction& instr) {\n+  FrontendAttributes copy_of_frontend_attributes = instr.frontend_attributes();\n+  copy_of_frontend_attributes.mutable_map()->erase(kXlaComputeTypeAttr);\n+  instr.set_frontend_attributes(copy_of_frontend_attributes);\n+}\n+\n+void RemoveComputeTypeFrontendAttribute(HloComputation& computation) {\n+  for (HloInstruction* instr : computation.instructions()) {\n+    RemoveComputeTypeFrontendAttribute(*instr);\n+  }\n+}\n+\n+// Offloads instructions marked as host compute that reside within\n+// `computation`.\n+absl::StatusOr<bool> OffloadHostInstructions(\n+    HloComputation& computation,\n+    const HloHostDeviceTypeCallWrapper::Options& options) {\n+  auto should_offload_to_host_compute =\n+      [&](const HloInstruction* instr) -> bool {\n+    if (host_offload_utils::ComputeTypeIsHost(instr)) {\n+      return true;\n+    }\n+    while (IsPassThroughShardingOp(*instr)) {\n+      instr = instr->operand(0);\n+    }\n+    return host_offload_utils::ComputeTypeIsHost(instr);\n+  };\n+\n+  TF_ASSIGN_OR_RETURN(\n+      auto offloaded_instructions_and_calls,\n+      offloader_util::FindAndWrapOffloadedComputations(\n+          computation,\n+          /*should_offload=*/should_offload_to_host_compute,\n+          /*should_fuse*/\n+          [](const HloInstruction&, const HloInstruction& hlo) {\n+            // If the computation has a schedule, we cannot fuse.\n+            // Otherwise incremental HloSchedule::Update() will\n+            // fail. Before:\n+            //   a = ...\n+            //   copy_a = copy(a) // host-to-host copy\n+            //   c = ds(copy_a)\n+            //   b = ...\n+            //   copy_b = copy(b) // host-to-host copy\n+            //   d = ds(copy_b)\n+            //\n+            // After offloading and fusing:\n+            //   a = ...\n+            //   b = ...\n+            //   host_call = host-call(a, b)\n+            //   copy_a = gte(host_call, 0)\n+            //   copy_b = gte(host_call, 1)\n+            //   c = ds(copy_a)\n+            //   d = ds(copy_b)\n+            //\n+            // Now b has to be scheduled before c, which an\n+            // incremental HloSchedule::Update() cannot do since it\n+            // only schedules new instructions and doesn't change\n+            // the original sequence.\n+            return !hlo.GetModule()->has_schedule();\n+          },\n+          options.clear_backend_config_device_type, \"host-call\"));\n+\n+  bool modified = false;\n+  for (const auto& [_, instr_call] : offloaded_instructions_and_calls) {\n+    CHECK_EQ(instr_call->opcode(), HloOpcode::kCall) << absl::StreamFormat(\n+        \"Host instruction must be a call. %s\", instr_call->ToString());\n+    CHECK_EQ(instr_call->called_computations().size(), 1);\n+    HloComputation* host_computation =\n+        instr_call->called_computations().front();\n+\n+    TF_RET_CHECK(instr_call->operands().size() ==\n+                 host_computation->num_parameters())\n+        << \"Expected the number of operands to match the number of parameters \"\n+           \"of the host called computation.\";\n+    TF_RET_CHECK(ShapeUtil::Equal(host_computation->root_instruction()->shape(),\n+                                  instr_call->shape()))\n+        << \"Shape mismatch between the host computation and the corresponding \"\n+           \"host call.\";\n+\n+    TF_RETURN_IF_ERROR(options.set_backend_config_fn(instr_call));\n+\n+    for (HloComputation* called_computation :\n+         instr_call->called_computations()) {\n+      TF_RETURN_IF_ERROR(\n+          offloader_util::RecursivelyClearComputeTypeFrontendAttribute(\n+              called_computation));\n+    }\n+\n+    modified = true;\n+  }\n+\n+  return modified;\n+}\n+}  // namespace\n+\n+/*static*/ absl::StatusOr<HloCallInstruction*>\n+HloHostDeviceTypeCallWrapper::RemoveTupleParameters(HloCallInstruction* call) {\n+  std::vector<HloInstruction*> new_call_operands;\n+  std::vector<int32_t> tuple_operand_indices;\n+\n+  // Add all non-tuple operands to the new call operands vector.\n+  for (int64_t call_operand_no = 0; call_operand_no < call->operand_count();\n+       ++call_operand_no) {\n+    HloInstruction* operand_instr = call->mutable_operand(call_operand_no);\n+    if (operand_instr->shape().IsTuple()) {\n+      tuple_operand_indices.push_back(call_operand_no);\n+      continue;\n+    }\n+    new_call_operands.push_back(operand_instr);\n+  }\n+\n+  if (tuple_operand_indices.empty()) {\n+    // No tuple operands.\n+    return call;\n+  }\n+\n+  HloComputation* called_computation = call->called_computation();\n+\n+  for (int32_t tuple_operand_index : tuple_operand_indices) {\n+    HloInstruction* tuple_operand_instr =\n+        call->mutable_operand(tuple_operand_index);\n+\n+    ShapeTree<HloInstruction*> operand_tuple_shape_tree =\n+        TupleUtil::DisassembleTupleInstruction(tuple_operand_instr);\n+\n+    HloInstruction* called_comp_parameter =\n+        called_computation->parameter_instruction(tuple_operand_index);\n+\n+    TF_RETURN_IF_ERROR(operand_tuple_shape_tree.ForEachElementWithStatus(\n+        [&](const ShapeIndex& operand_tuple_index,\n+            HloInstruction* operand_tuple_element) -> absl::Status {\n+          HloInstruction* called_comp_leaf_instr =\n+              TupleUtil::GetTupleInstructionAtIndex(*called_comp_parameter,\n+                                                    operand_tuple_index);\n+\n+          if (!operand_tuple_shape_tree.IsLeaf(operand_tuple_index)) {\n+            TF_RET_CHECK(absl::c_all_of(called_comp_leaf_instr->users(),\n+                                        [&](HloInstruction* user) -> bool {\n+                                          return user->opcode() ==\n+                                                 HloOpcode::kGetTupleElement;\n+                                        }))\n+                << \"Unsupported: Expected host tuple parameter to be \"\n+                   \"decomposed into GTEs\"\n+                << called_comp_leaf_instr->ToString();\n+            return absl::OkStatus();\n+          }\n+\n+          if (called_comp_leaf_instr == nullptr) {\n+            // Unused tuple element.\n+            return absl::OkStatus();\n+          }\n+\n+          HloInstruction* leaf_param =\n+              called_computation->AddParameter(HloInstruction::CreateParameter(\n+                  called_computation->num_parameters(),\n+                  operand_tuple_element->shape(), \"param\"));\n+\n+          TF_RETURN_IF_ERROR(\n+              called_comp_leaf_instr->ReplaceAllUsesWith(leaf_param));\n+\n+          new_call_operands.push_back(operand_tuple_element);\n+\n+          return absl::OkStatus();\n+        }));\n+  }\n+\n+  // Remove any unused tuple parameter gte instructions.\n+  TF_RETURN_IF_ERROR(\n+      HloDCE().RunOnComputation(called_computation, false).status());\n+  // Remove the tuple parameters that are no longer used.\n+  TF_RETURN_IF_ERROR(\n+      called_computation->RemoveUnusedParametersFromAnyComputation());\n+\n+  HloInstruction* new_call = call->parent()->AddInstruction(\n+      call->CloneWithNewOperands(call->shape(), new_call_operands));\n+\n+  TF_RETURN_IF_ERROR(call->ReplaceAllUsesWith(new_call));\n+  TF_RETURN_IF_ERROR(call->parent()->RemoveInstruction(call));\n+  return tsl::down_cast<HloCallInstruction*>(new_call);\n+}\n+\n+/*static*/ absl::StatusOr<HloCallInstruction*>\n+HloHostDeviceTypeCallWrapper::MaterializeConstantsOnHostComputation(\n+    HloCallInstruction* call) {\n+  std::vector<HloInstruction*> non_constant_operands;\n+  HloComputation* called_computation = call->called_computation();\n+\n+  absl::flat_hash_set<int64_t> dead_param_indices;\n+  for (HloInstruction* param : called_computation->parameter_instructions()) {\n+    if (param->IsDead()) {\n+      dead_param_indices.insert(param->parameter_number());\n+    }\n+  }\n+  // Remove any existing dead parameters.\n+  TF_RETURN_IF_ERROR(\n+      called_computation->RemoveUnusedParametersFromAnyComputation());\n+\n+  int skipped_params = 0;\n+  for (int64_t operand_no = 0; operand_no < call->operand_count();\n+       ++operand_no) {\n+    if (dead_param_indices.contains(operand_no)) {\n+      ++skipped_params;\n+      continue;\n+    }\n+    HloInstruction* operand = call->mutable_operand(operand_no);\n+    TF_RET_CHECK(!operand->shape().IsTuple())\n+        << \"Tuple inputs for async host computations should be flattened.\";\n+    HloInstruction* constant_operand = nullptr;\n+    if (operand->IsConstant()) {\n+      constant_operand = operand;\n+    } else if (operand->IsCustomCall(\n+                   memory_annotations::kMoveToHostCustomCallTarget) &&\n+               operand->operand(0)->IsConstant()) {\n+      constant_operand = operand->mutable_operand(0);\n+    }\n+\n+    if (constant_operand != nullptr) {\n+      HloInstruction* cloned_constant =\n+          called_computation->AddInstruction(constant_operand->Clone());\n+      HloInstruction* called_computation_parameter =\n+          called_computation->parameter_instruction(operand_no -\n+                                                    skipped_params);\n+      TF_RETURN_IF_ERROR(\n+          called_computation_parameter->ReplaceAllUsesWith(cloned_constant));\n+    } else {\n+      non_constant_operands.push_back(operand);\n+    }\n+  }\n+\n+  if (non_constant_operands.size() == call->operand_count()) {\n+    return call;\n+  }\n+\n+  // Remove any parameter that were constants and are now unused.\n+  TF_RETURN_IF_ERROR(\n+      called_computation->RemoveUnusedParametersFromAnyComputation());\n+\n+  HloInstruction* new_call = call->parent()->AddInstruction(\n+      call->CloneWithNewOperands(call->shape(), non_constant_operands));\n+\n+  TF_RETURN_IF_ERROR(call->ReplaceAllUsesWith(new_call));\n+  TF_RETURN_IF_ERROR(call->parent()->RemoveInstruction(call));\n+  return tsl::down_cast<HloCallInstruction*>(new_call);\n+}\n+\n+absl::StatusOr<bool> HloHostDeviceTypeCallWrapper::Run(\n+    HloModule* module,\n+    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n+  bool has_host_compute_instr = false;\n+  for (HloComputation* computation :\n+       module->MakeNonfusionComputations(execution_threads)) {\n+    if (absl::c_any_of(computation->instructions(), [&](HloInstruction* instr) {\n+          return host_offload_utils::ComputeTypeIsHost(instr);\n+        })) {\n+      has_host_compute_instr = true;\n+      break;\n+    };\n+  }\n+\n+  if (!has_host_compute_instr) {\n+    return false;\n+  }\n+\n+  TF_RETURN_IF_ERROR(\n+      AnnotateHostComputeOffload().Run(module, execution_threads).status());\n+  TF_RETURN_IF_ERROR(CallInliner().Run(module, execution_threads).status());\n+  TF_RETURN_IF_ERROR(TupleSimplifier().Run(module, execution_threads).status());\n+  TF_RETURN_IF_ERROR(HloDCE().Run(module, execution_threads).status());\n+\n+  std::unique_ptr<CallGraph> call_graph = CallGraph::Build(module);\n+  for (HloComputation* computation :\n+       module->MakeNonfusionComputations({execution_threads})) {\n+    std::vector<HloInstruction*> callers =\n+        call_graph->GetComputationCallers(computation);\n+    bool caller_is_single_host_instr =\n+        callers.size() == 1 &&\n+        host_offload_utils::ComputeTypeIsHost(callers.front());\n+    if (caller_is_single_host_instr) {\n+      // Skip offloading instructions inside of computations that are already\n+      // part of an offloaded program (while, conditional, host utility\n+      // function).\n+      RemoveComputeTypeFrontendAttribute(*computation);\n+      continue;\n+    }\n+    TF_RETURN_IF_ERROR(\n+        OffloadHostInstructions(*computation, options_).status());\n+  }\n+\n+  TF_RETURN_IF_ERROR(module->RemoveUnusedComputations());\n+\n+  if (module->has_schedule()) {\n+    TF_RETURN_IF_ERROR(module->schedule().Update());\n+  }\n+\n+  return true;\n+}\n+\n+}  // namespace xla"
        },
        {
            "sha": "99c83653f27619e57047a2d730b4ca2229740177",
            "filename": "third_party/xla/xla/core/host_offloading/hlo_host_device_type_call_wrapper.h",
            "status": "added",
            "additions": 75,
            "deletions": 0,
            "changes": 75,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f8c3ce07459878b3a1fc007aaaae5b659bf79fd1/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2Fhlo_host_device_type_call_wrapper.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f8c3ce07459878b3a1fc007aaaae5b659bf79fd1/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2Fhlo_host_device_type_call_wrapper.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcore%2Fhost_offloading%2Fhlo_host_device_type_call_wrapper.h?ref=f8c3ce07459878b3a1fc007aaaae5b659bf79fd1",
            "patch": "@@ -0,0 +1,75 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_CORE_HOST_OFFLOADING_HLO_HOST_DEVICE_TYPE_CALL_WRAPPER_H_\n+#define XLA_CORE_HOST_OFFLOADING_HLO_HOST_DEVICE_TYPE_CALL_WRAPPER_H_\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/pass/hlo_pass_interface.h\"\n+\n+namespace xla {\n+\n+// Wraps host instructions annotated as host compute into calls and annotates\n+// the device type as host.\n+class HloHostDeviceTypeCallWrapper : public HloModulePass {\n+ public:\n+  struct Options {\n+    // Function that sets the device type of the instruction to host.\n+    // Implementation of the function is device specific.\n+    std::function<absl::Status(HloInstruction*)> set_backend_config_fn;\n+    // Function that clears the device type from the backend config.\n+    std::function<absl::Status(HloInstruction*)>\n+        clear_backend_config_device_type;\n+  };\n+\n+  explicit HloHostDeviceTypeCallWrapper(const Options& options)\n+      : options_(options) {}\n+\n+  absl::string_view name() const override {\n+    return \"hlo_host_device_type_call_wrapper\";\n+  }\n+\n+  absl::StatusOr<bool> Run(\n+      HloModule* module,\n+      const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n+\n+  // Materializes constants on the host computation to avoid unnecessary device\n+  // to host transfers.\n+  //\n+  // Returns an an updated call instruction/computation that does not\n+  // contain constant operands/parameters.\n+  static absl::StatusOr<HloCallInstruction*>\n+  MaterializeConstantsOnHostComputation(HloCallInstruction* call);\n+\n+  // Removes tuple parameter/operands from the call instruction.\n+  //\n+  // Returns an an updated call instruction/computation that does not\n+  // contain tuple parameters/operands.\n+  static absl::StatusOr<HloCallInstruction*> RemoveTupleParameters(\n+      HloCallInstruction* call);\n+\n+ private:\n+  Options options_;\n+};\n+\n+}  // namespace xla\n+\n+#endif  // XLA_CORE_HOST_OFFLOADING_HLO_HOST_DEVICE_TYPE_CALL_WRAPPER_H_"
        }
    ],
    "stats": {
        "total": 473,
        "additions": 473,
        "deletions": 0
    }
}