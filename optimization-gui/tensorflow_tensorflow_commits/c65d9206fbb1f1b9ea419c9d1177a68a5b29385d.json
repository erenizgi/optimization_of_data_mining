{
    "author": "mwhittaker",
    "message": "Support collective cancelling in TFRT GPU client.\n\nPiperOrigin-RevId: 814302211",
    "sha": "c65d9206fbb1f1b9ea419c9d1177a68a5b29385d",
    "files": [
        {
            "sha": "dcb42f1272dddc9f08a2f7b538cb4364a130df7d",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c65d9206fbb1f1b9ea419c9d1177a68a5b29385d/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c65d9206fbb1f1b9ea419c9d1177a68a5b29385d/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=c65d9206fbb1f1b9ea419c9d1177a68a5b29385d",
            "patch": "@@ -671,13 +671,6 @@ StreamExecutorGpuClient::CreateBuffersForAsyncHostToDevice(\n \n absl::StatusOr<absl::flat_hash_map<GlobalDeviceId, IncarnationId>>\n StreamExecutorGpuClient::GetLatestIncarnations(const ExecuteOptions& options) {\n-  // Get the latest incarnation for every task.\n-  if (!num_nodes_.has_value()) {\n-    return FailedPrecondition(\"Unknown number of nodes\");\n-  }\n-  std::vector<int> tasks(*num_nodes_);\n-  std::iota(tasks.begin(), tasks.end(), 0);\n-\n   // Map every device to its incarnation.\n   absl::flat_hash_map<GlobalDeviceId, IncarnationId> device_incarnations;\n   for (const PjRtDevice* device : devices()) {"
        },
        {
            "sha": "aaf106d9a624023017a406d34d262876c173d815",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c65d9206fbb1f1b9ea419c9d1177a68a5b29385d/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c65d9206fbb1f1b9ea419c9d1177a68a5b29385d/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD?ref=c65d9206fbb1f1b9ea419c9d1177a68a5b29385d",
            "patch": "@@ -50,6 +50,7 @@ cc_library(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n+        \"//xla/backends/gpu/collectives:gpu_cliques\",\n         \"//xla/backends/gpu/collectives:gpu_collectives\",\n         \"//xla/client:executable_build_options\",\n         \"//xla/client:local_client\",\n@@ -88,6 +89,7 @@ cc_library(\n         \"//xla/pjrt/plugin/xla_gpu:xla_gpu_allocator_config\",\n         \"//xla/pjrt/plugin/xla_gpu:xla_gpu_client_options\",\n         \"//xla/pjrt/proto:compile_options_proto_cc\",\n+        \"//xla/runtime:device_id\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:compiler\",\n         \"//xla/service:computation_placer_hdr\","
        },
        {
            "sha": "219b80263325c536f74af4d502e8581e11573480",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_client.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 3,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c65d9206fbb1f1b9ea419c9d1177a68a5b29385d/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c65d9206fbb1f1b9ea419c9d1177a68a5b29385d/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.cc?ref=c65d9206fbb1f1b9ea419c9d1177a68a5b29385d",
            "patch": "@@ -42,6 +42,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"unsupported/Eigen/CXX11/Tensor\"\n #include \"mlir/IR/BuiltinOps.h\"\n+#include \"xla/backends/gpu/collectives/gpu_cliques.h\"\n #include \"xla/client/executable_build_options.h\"\n #include \"xla/client/local_client.h\"\n #include \"xla/debug_options_flags.h\"\n@@ -80,6 +81,7 @@ limitations under the License.\n #include \"xla/service/compiler.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/executable.h\"\n+#include \"xla/service/gpu/gpu_executable_run_options.h\"\n #include \"xla/service/hlo.pb.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/service/shaped_buffer.h\"\n@@ -139,6 +141,7 @@ TfrtGpuClient::TfrtGpuClient(\n     std::string platform_name, int process_index, xla::LocalClient* xla_client,\n     std::vector<std::unique_ptr<TfrtGpuDevice>> devices,\n     bool should_stage_host_to_device_transfers,\n+    bool abort_collectives_on_failure,\n     MaybeOwning<se::DeviceMemoryAllocator> allocator,\n     std::unique_ptr<tsl::Allocator> host_memory_allocator,\n     std::unique_ptr<gpu::GpuExecutableRunOptions> gpu_run_options,\n@@ -149,6 +152,7 @@ TfrtGpuClient::TfrtGpuClient(\n       xla_client_(CHECK_NOTNULL(xla_client)),\n       should_stage_host_to_device_transfers_(\n           should_stage_host_to_device_transfers),\n+      abort_collectives_on_failure_(abort_collectives_on_failure),\n       allocator_(std::move(allocator)),\n       host_memory_allocator_(std::make_unique<HostMemoryAllocator>(\n           std::move(host_memory_allocator))),\n@@ -221,6 +225,22 @@ absl::StatusOr<PjRtDevice*> TfrtGpuClient::LookupAddressableDevice(\n                          local_device_id.value());\n }\n \n+void TfrtGpuClient::UpdateGlobalProcessInfo(\n+    absl::Span<tensorflow::CoordinatedTaskStateInfo> infos) {\n+  if (!abort_collectives_on_failure_) {\n+    // If we're not aborting collectives, we don't need to track information\n+    // about other processes. We only track global process info to know when to\n+    // abort.\n+    VLOG(5) << \"Not updating global process info because \"\n+               \"abort_collectives_on_failure_ is false\";\n+    return;\n+  }\n+  absl::Status s = ::xla::gpu::UpdateGlobalProcessInfo(infos);\n+  if (!s.ok()) {\n+    LOG(WARNING) << \"Failed to update global process info: \" << s;\n+  }\n+}\n+\n absl::StatusOr<Layout> TfrtGpuClient::GetDefaultLayout(\n     PrimitiveType element_type, absl::Span<const int64_t> dims) {\n   return topology_.GetDefaultLayout(element_type, dims);\n@@ -1213,9 +1233,9 @@ absl::StatusOr<std::unique_ptr<PjRtClient>> GetTfrtGpuClient(\n   return std::unique_ptr<PjRtClient>(std::make_unique<TfrtGpuClient>(\n       std::move(pjrt_platform_name), options.node_id, xla_client,\n       std::move(devices), options.should_stage_host_to_device_transfers,\n-      std::move(allocator), std::move(host_memory_allocator),\n-      std::move(gpu_run_options), std::move(kv_store),\n-      std::move(gpu_topology)));\n+      options.abort_collectives_on_failure, std::move(allocator),\n+      std::move(host_memory_allocator), std::move(gpu_run_options),\n+      std::move(kv_store), std::move(gpu_topology)));\n }\n \n }  // namespace xla"
        },
        {
            "sha": "41e95484b084a7606a09bc45a4b81eb87af1af7c",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_client.h",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c65d9206fbb1f1b9ea419c9d1177a68a5b29385d/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c65d9206fbb1f1b9ea419c9d1177a68a5b29385d/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.h?ref=c65d9206fbb1f1b9ea419c9d1177a68a5b29385d",
            "patch": "@@ -118,6 +118,7 @@ class TfrtGpuClient final : public PjRtClient {\n                 xla::LocalClient* xla_client,\n                 std::vector<std::unique_ptr<TfrtGpuDevice>> devices,\n                 bool should_stage_host_to_device_transfers,\n+                bool abort_collectives_on_failure,\n                 MaybeOwning<se::DeviceMemoryAllocator> allocator,\n                 std::unique_ptr<tsl::Allocator> host_memory_allocator,\n                 std::unique_ptr<gpu::GpuExecutableRunOptions> gpu_run_options,\n@@ -148,6 +149,9 @@ class TfrtGpuClient final : public PjRtClient {\n   absl::StatusOr<PjRtDevice*> LookupAddressableDevice(\n       PjRtLocalDeviceId local_device_id) const override;\n \n+  void UpdateGlobalProcessInfo(\n+      absl::Span<tensorflow::CoordinatedTaskStateInfo> infos) override;\n+\n   absl::Span<PjRtMemorySpace* const> memory_spaces() const override;\n \n   xla::LocalClient* xla_client() const { return xla_client_; }\n@@ -328,6 +332,7 @@ class TfrtGpuClient final : public PjRtClient {\n   xla::LocalClient* xla_client_;\n \n   bool should_stage_host_to_device_transfers_;\n+  const bool abort_collectives_on_failure_ = false;\n \n   // Device memory allocator. If owned, the allocator must outlive the devices,\n   // because it is the device destructor that waits for any outstanding work to"
        },
        {
            "sha": "6d9c601134d6d287d99af3a6ef09793aa88c3eb2",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_executable.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 4,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c65d9206fbb1f1b9ea419c9d1177a68a5b29385d/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c65d9206fbb1f1b9ea419c9d1177a68a5b29385d/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_executable.cc?ref=c65d9206fbb1f1b9ea419c9d1177a68a5b29385d",
            "patch": "@@ -56,10 +56,13 @@ limitations under the License.\n #include \"xla/pjrt/proto/compile_options.pb.h\"\n #include \"xla/pjrt/semaphore.h\"\n #include \"xla/pjrt/utils.h\"\n+#include \"xla/runtime/device_id.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/compiler.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/executable.h\"\n+#include \"xla/service/global_device_id.h\"\n+#include \"xla/service/gpu/gpu_executable_run_options.h\"\n #include \"xla/service/hlo.pb.h\"\n #include \"xla/service/maybe_owning_device_memory.h\"\n #include \"xla/service/shaped_buffer.h\"\n@@ -98,6 +101,7 @@ limitations under the License.\n #endif\n \n namespace xla {\n+\n class TfrtGpuCopyToDeviceStream : public CopyToDeviceStream {\n  public:\n   TfrtGpuCopyToDeviceStream(int64_t channel_id, se::Stream* stream,\n@@ -602,8 +606,9 @@ absl::StatusOr<PjRtLoadedExecutable::Result> TfrtGpuExecutable::ExecuteHelper(\n        send_device_memory(std::move(send_device_memory)),\n        recv_device_memory(std::move(recv_device_memory)),\n        output_cuda_execute_event(std::move(output_cuda_execute_event)),\n-       compute_reservation(std::move(compute_reservation)),\n-       client = client_](std::vector<ExecutionInput> execution_inputs) mutable {\n+       compute_reservation(std::move(compute_reservation)), client = client_,\n+       task_incarnations = options.incarnations](\n+          std::vector<ExecutionInput> execution_inputs) mutable {\n         VLOG(1) << \"execute_fn for \" << executable_name\n                 << \", launch_id: \" << launch_id << \", replica: \" << replica\n                 << \", device: \" << device->DebugString();\n@@ -633,6 +638,19 @@ absl::StatusOr<PjRtLoadedExecutable::Result> TfrtGpuExecutable::ExecuteHelper(\n           }\n         }\n \n+        // Set the incarnations in gpu_run_options.\n+        gpu::GpuExecutableRunOptions* gpu_run_options =\n+            CHECK_NOTNULL(client->gpu_run_options());\n+        absl::StatusOr<absl::flat_hash_map<GlobalDeviceId, IncarnationId>>\n+            device_incarnations =\n+                GetLatestIncarnations(client->devices(), task_incarnations);\n+        if (!device_incarnations.ok()) {\n+          VLOG(1) << \"Unable to set incarnations in GpuExecutableRunOptions: \"\n+                  << device_incarnations.status();\n+        } else {\n+          gpu_run_options->set_incarnations(*std::move(device_incarnations));\n+        }\n+\n         auto stream = device->stream();\n         ExecutableRunOptions run_options;\n         run_options.set_stream(stream);\n@@ -642,8 +660,7 @@ absl::StatusOr<PjRtLoadedExecutable::Result> TfrtGpuExecutable::ExecuteHelper(\n         run_options.set_device_assignment(device_assignment.get());\n         run_options.set_run_id(RunId(launch_id));\n         run_options.set_rng_seed(device->GetNewPrngSeed());\n-        run_options.set_gpu_executable_run_options(\n-            CHECK_NOTNULL(client->gpu_run_options()));\n+        run_options.set_gpu_executable_run_options(gpu_run_options);\n         run_options.set_launch_id(launch_id);\n         run_options.set_local_device_count(client->device_count());\n         run_options.set_device_ordinal(device->local_device_id().value());"
        },
        {
            "sha": "a2660958486c55f6a13143a077e6bdfe1efdc72b",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/utils.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c65d9206fbb1f1b9ea419c9d1177a68a5b29385d/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c65d9206fbb1f1b9ea419c9d1177a68a5b29385d/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.cc?ref=c65d9206fbb1f1b9ea419c9d1177a68a5b29385d",
            "patch": "@@ -939,4 +939,22 @@ void EnqueueWorkWhenReady(\n   });\n }\n \n+absl::StatusOr<absl::flat_hash_map<GlobalDeviceId, IncarnationId>>\n+GetLatestIncarnations(\n+    absl::Span<PjRtDevice* const> devices,\n+    const absl::flat_hash_map<int, IncarnationId>& incarnations) {\n+  // Map every device to its incarnation.\n+  absl::flat_hash_map<GlobalDeviceId, IncarnationId> device_incarnations;\n+  for (const PjRtDevice* device : devices) {\n+    int task_id = device->process_index();\n+    auto it = incarnations.find(task_id);\n+    if (it == incarnations.end()) {\n+      return FailedPrecondition(\"Incarnation for task %d not found\", task_id);\n+    }\n+    GlobalDeviceId device_id(device->global_device_id().value());\n+    device_incarnations[device_id] = it->second;\n+  }\n+  return device_incarnations;\n+}\n+\n }  // namespace xla"
        },
        {
            "sha": "cb75028d68305ba813192a2097be752c7e098e2f",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/utils.h",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c65d9206fbb1f1b9ea419c9d1177a68a5b29385d/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c65d9206fbb1f1b9ea419c9d1177a68a5b29385d/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Futils.h?ref=c65d9206fbb1f1b9ea419c9d1177a68a5b29385d",
            "patch": "@@ -179,6 +179,11 @@ void EnqueueWorkWhenReady(\n     absl::Span<const tsl::RCReference<tsl::AsyncValue>> values,\n     absl::AnyInvocable<void()> callee);\n \n+absl::StatusOr<absl::flat_hash_map<GlobalDeviceId, IncarnationId>>\n+GetLatestIncarnations(\n+    absl::Span<PjRtDevice* const> devices,\n+    const absl::flat_hash_map<int, IncarnationId>& incarnations);\n+\n }  // namespace xla\n \n #endif  // XLA_PJRT_GPU_TFRT_UTILS_H_"
        }
    ],
    "stats": {
        "total": 88,
        "additions": 74,
        "deletions": 14
    }
}