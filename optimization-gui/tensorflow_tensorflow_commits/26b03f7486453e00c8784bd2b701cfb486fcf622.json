{
    "author": "pifon2a",
    "message": "[XLA:GPU] Refactor the func computing logical -> physical layout indexing map.\n\nPiperOrigin-RevId: 809948759",
    "sha": "26b03f7486453e00c8784bd2b701cfb486fcf622",
    "files": [
        {
            "sha": "9b1aaa2de28852913dad96e3ef7c83578f8121f5",
            "filename": "third_party/xla/xla/service/gpu/model/coalescing_analysis.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 21,
            "changes": 53,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26b03f7486453e00c8784bd2b701cfb486fcf622/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26b03f7486453e00c8784bd2b701cfb486fcf622/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.cc?ref=26b03f7486453e00c8784bd2b701cfb486fcf622",
            "patch": "@@ -50,8 +50,7 @@ limitations under the License.\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n \n-namespace xla {\n-namespace gpu {\n+namespace xla::gpu {\n \n // Returns true if all input reads are coalesced. If consumer is not nullptr,\n // producer and consumer are considered as one fusion, otherwise it's only the\n@@ -231,6 +230,30 @@ Shape GetLinearizedShape(const Shape& shape) {\n   return result;\n }\n \n+llvm::SmallVector<IndexingMap, 4> MapLogicalToLinearizedPhysicalShape(\n+    absl::Span<const HloInstruction* const> operands,\n+    MLIRContext* mlir_context) {\n+  llvm::SmallVector<IndexingMap, 4> indexing_maps;\n+  // For every operand compute thread ID -> physical layout of operand\n+  // indexing map.\n+  for (const HloInstruction* operand : operands) {\n+    const Shape& operand_shape = operand->shape();\n+\n+    IndexingMap operand_logical_to_physical_map =\n+        GetIndexingMapFromLogicalToPhysicalLayout(operand_shape, mlir_context);\n+    IndexingMap operand_physical_to_linearized_shape = GetBitcastMap(\n+        ShapeUtil::MakeShapeWithDescendingLayoutAndSamePhysicalLayout(\n+            operand_shape),\n+        GetLinearizedShape(operand_shape), mlir_context);\n+    IndexingMap operand_logical_to_linearized_physical_shape =\n+        operand_logical_to_physical_map * operand_physical_to_linearized_shape;\n+    operand_logical_to_linearized_physical_shape.Simplify();\n+    indexing_maps.push_back(\n+        std::move(operand_logical_to_linearized_physical_shape));\n+  }\n+  return indexing_maps;\n+}\n+\n // Returns thread ID to linearized physical layout indexing map for each operand\n // of the fusion.\n std::optional<GroupedByOpIndexingMap> GetThreadIdToInputMemoryLayoutsMaps(\n@@ -245,7 +268,9 @@ std::optional<GroupedByOpIndexingMap> GetThreadIdToInputMemoryLayoutsMaps(\n   if (fusion_interface == nullptr) {\n     return std::nullopt;\n   }\n-\n+  llvm::SmallVector<IndexingMap, 4>\n+      operand_logical_to_linearized_physical_maps =\n+          MapLogicalToLinearizedPhysicalShape(operands, mlir_context);\n   GroupedByOpIndexingMap result;\n   for (const auto& [root_index, hero] :\n        llvm::enumerate(fusion_analysis.fusion_heroes())) {\n@@ -267,26 +292,14 @@ std::optional<GroupedByOpIndexingMap> GetThreadIdToInputMemoryLayoutsMaps(\n                                               hero_operand, mlir_context);\n       // For every operand compute thread ID -> physical layout of operand\n       // indexing map.\n-      for (const HloInstruction* operand : operands) {\n+      for (auto&& [operand, operand_linarized_physical_map] :\n+           llvm::zip(operands, operand_logical_to_linearized_physical_maps)) {\n         auto operand_indexing_maps_it =\n             instr_indexing_keyed_by_operands.find(operand);\n         if (operand_indexing_maps_it ==\n             instr_indexing_keyed_by_operands.end()) {\n           continue;\n         }\n-        const Shape& operand_shape = operand->shape();\n-\n-        IndexingMap operand_logical_to_physical_map =\n-            GetIndexingMapFromLogicalToPhysicalLayout(operand_shape,\n-                                                      mlir_context);\n-        IndexingMap operand_physical_to_linearized_shape = GetBitcastMap(\n-            ShapeUtil::MakeShapeWithDescendingLayoutAndSamePhysicalLayout(\n-                operand_shape),\n-            GetLinearizedShape(operand_shape), mlir_context);\n-        IndexingMap operand_logical_to_linearized_physical_shape =\n-            operand_logical_to_physical_map *\n-            operand_physical_to_linearized_shape;\n-        operand_logical_to_linearized_physical_shape.Simplify();\n \n         for (const OperandIndexing& operand_indexing :\n              operand_indexing_maps_it->second) {\n@@ -298,8 +311,7 @@ std::optional<GroupedByOpIndexingMap> GetThreadIdToInputMemoryLayoutsMaps(\n             break;\n           }\n           IndexingMap logical_output_to_linearized_physical_input_map =\n-              operand_indexing_map *\n-              operand_logical_to_linearized_physical_shape;\n+              operand_indexing_map * operand_linarized_physical_map;\n           IndexingMap thread_id_to_linearized_physical_input_map =\n               *thread_id_to_hero_operand_map *\n               logical_output_to_linearized_physical_input_map;\n@@ -708,5 +720,4 @@ bool CoalescingAnalysis::IsReadCoalesced(const HloInstruction* operand) const {\n   return it->second;\n }\n \n-}  // namespace gpu\n-}  // namespace xla\n+}  // namespace xla::gpu"
        },
        {
            "sha": "23f3f8745fb8e41d88e4709b4b82df96e8a9f889",
            "filename": "third_party/xla/xla/service/gpu/model/coalescing_analysis.h",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26b03f7486453e00c8784bd2b701cfb486fcf622/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26b03f7486453e00c8784bd2b701cfb486fcf622/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fcoalescing_analysis.h?ref=26b03f7486453e00c8784bd2b701cfb486fcf622",
            "patch": "@@ -20,14 +20,15 @@ limitations under the License.\n \n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/types/span.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n #include \"mlir/IR/MLIRContext.h\"\n+#include \"xla/hlo/analysis/indexing_map.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/model/tiled_hlo_instruction.h\"\n #include \"xla/stream_executor/device_description.h\"\n \n-namespace xla {\n-namespace gpu {\n+namespace xla::gpu {\n \n using CoalescingMap = absl::flat_hash_map<const HloInstruction*, bool>;\n \n@@ -98,7 +99,12 @@ double BandwidthUtilizationRateHeuristicForTiledMemoryAccess(\n bool IsTiledReadCoalescedHeuristic(const TiledHloInstruction& operand,\n                                    const se::DeviceDescription& device_info);\n \n-}  // namespace gpu\n-}  // namespace xla\n+// Returns the indexing map from logical to linearized physical shape for each\n+// operand.\n+llvm::SmallVector<IndexingMap, 4> MapLogicalToLinearizedPhysicalShape(\n+    absl::Span<const HloInstruction* const> operands,\n+    mlir::MLIRContext* mlir_context);\n+\n+}  // namespace xla::gpu\n \n #endif  // XLA_SERVICE_GPU_MODEL_COALESCING_ANALYSIS_H_"
        }
    ],
    "stats": {
        "total": 67,
        "additions": 42,
        "deletions": 25
    }
}