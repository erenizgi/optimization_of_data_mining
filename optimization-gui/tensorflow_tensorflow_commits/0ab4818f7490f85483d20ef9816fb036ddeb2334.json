{
    "author": "ZixuanJiang",
    "message": "Use all-gather in the spmd_partitioner_test.\n\nBefore this change, we disallowed all-gather such that the partitioner generates `all-reduce(dynamic-update-slice())` pattern. With this change, we allow all-gather for two reasons.\n1. In most cases, all-gather is allowed and preferred.\n2. It is easier to read and match the partitioner result.\n\nPiperOrigin-RevId: 820593767",
    "sha": "0ab4818f7490f85483d20ef9816fb036ddeb2334",
    "files": [
        {
            "sha": "981ffbb16d64620661a2ba5f223438d72d72c03c",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_test.cc",
            "status": "modified",
            "additions": 233,
            "deletions": 398,
            "changes": 631,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0ab4818f7490f85483d20ef9816fb036ddeb2334/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0ab4818f7490f85483d20ef9816fb036ddeb2334/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc?ref=0ab4818f7490f85483d20ef9816fb036ddeb2334",
            "patch": "@@ -38,6 +38,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/ir/hlo_print_options.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n #include \"xla/hlo/ir/replica_group.h\"\n #include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n@@ -84,12 +85,10 @@ class SpmdPartitioningTest\n   absl::StatusOr<std::unique_ptr<HloModule>> PartitionComputation(\n       absl::string_view hlo_module, int64_t num_devices,\n       SpmdPartitionerOptions options = SpmdPartitionerOptions(),\n-      bool use_all_gather = false) {\n+      bool use_all_gather = true) {\n     options.allow_module_signature_change = true;\n     auto collective_ops_creator =\n         GetDefaultCollectiveOpsCreator(num_devices, /*num_replicas=*/1);\n-    // Do not use all-gather for pattern-matching purpose, as the partitioner\n-    // might create reshape/transposes around it.\n     if (!use_all_gather) {\n       collective_ops_creator.create_cross_partition_all_gather = nullptr;\n     }\n@@ -501,15 +500,11 @@ ENTRY entry {\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           PartitionComputation(hlo_string, /*num_devices=*/2));\n-  HloInstruction* root = module->entry_computation()->root_instruction();\n-  EXPECT_THAT(\n-      root,\n-      op::Copy(op::AllReduce(AllOf(\n-          op::DynamicUpdateSlice(\n-              op::Broadcast(), AllOf(op::Constant(), op::Shape(\"s32[1,3]\")),\n-              op::Reshape(op::DynamicSlice(op::Constant(), op::PartitionId())),\n-              op::Constant()),\n-          op::Shape(\"s32[2,3]\")))));\n+  auto sharded_constant = AllOf(op::Constant(), op::Shape(\"s32[1,3]\"));\n+  auto replicated_constant =\n+      AllOf(op::AllGather(op::Constant()), op::Shape(\"s32[2,3]\"));\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              op::Copy(replicated_constant));\n }\n \n TEST_P(SpmdPartitioningTest,\n@@ -530,23 +525,15 @@ ENTRY entry {\n                           PartitionComputation(hlo_string, /*num_devices=*/4));\n   VLOG(1) << module->ToString();\n \n-  // Verify all-reduce instruction is generated.\n-  auto all_reduce_instruction =\n-      std::find_if(module->entry_computation()->instructions().begin(),\n-                   module->entry_computation()->instructions().end(),\n-                   HloPredicateIsOp<HloOpcode::kAllReduce>);\n-  EXPECT_NE(all_reduce_instruction,\n-            module->entry_computation()->instructions().end());\n+  // Verify all-gather instruction is generated.\n+  HloInstruction* all_gather =\n+      FindInstruction(module.get(), HloOpcode::kAllGather);\n+  EXPECT_NE(all_gather, nullptr);\n \n-  // Verify all-reduce instruction contains ReplicaGroupV2.\n-  EXPECT_TRUE((*all_reduce_instruction)\n-                  ->device_list()\n-                  .iota_replica_group_list()\n-                  .has_value());\n-  IotaReplicaGroupList list = (*all_reduce_instruction)\n-                                  ->device_list()\n-                                  .iota_replica_group_list()\n-                                  .value();\n+  // Verify all-gather instruction contains ReplicaGroupV2.\n+  EXPECT_TRUE(all_gather->device_list().iota_replica_group_list().has_value());\n+  IotaReplicaGroupList list =\n+      all_gather->device_list().iota_replica_group_list().value();\n   EXPECT_EQ(list.num_replica_groups(), 1);\n   EXPECT_EQ(list.num_devices_per_group(), 4);\n   EXPECT_THAT(list.reshape_dims(), ::testing::ElementsAre(4));\n@@ -564,15 +551,11 @@ ENTRY entry {\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           PartitionComputation(hlo_string, /*num_devices=*/2));\n+  auto sharded_constant = AllOf(op::Constant(), op::Shape(\"s32[1,3]\"));\n+  auto replicated_constant =\n+      AllOf(op::AllGather(op::Constant()), op::Shape(\"s32[2,3]\"));\n   HloInstruction* root = module->entry_computation()->root_instruction();\n-  EXPECT_THAT(\n-      root,\n-      op::Copy(op::Copy(op::AllReduce(AllOf(\n-          op::DynamicUpdateSlice(\n-              op::Broadcast(), AllOf(op::Constant(), op::Shape(\"s32[1,3]\")),\n-              op::Reshape(op::DynamicSlice(op::Constant(), op::PartitionId())),\n-              op::Constant()),\n-          op::Shape(\"s32[2,3]\"))))));\n+  EXPECT_THAT(root, op::Copy(op::Copy(replicated_constant)));\n }\n \n TEST_P(SpmdPartitioningTest, TiledToTiledEven) {\n@@ -761,13 +744,8 @@ ENTRY entry {\n                           PartitionComputation(hlo_string, /*num_devices=*/2));\n   HloInstruction* root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(\n-      root,\n-      op::Copy(op::AllReduce(op::DynamicUpdateSlice(\n-          op::Broadcast(),\n-          op::GetTupleElement(\n-              AllOf(op::Infeed(), op::Shape(\"(f32[4,2]{1,0}, token[])\"))),\n-          op::Reshape(op::DynamicSlice(op::Constant(), op::PartitionId())),\n-          op::Constant()))));\n+      root, op::Copy(op::AllGather(op::GetTupleElement(\n+                AllOf(op::Infeed(), op::Shape(\"(f32[4,2]{1,0}, token[])\"))))));\n }\n \n TEST_P(SpmdPartitioningTest, UnevenTiledInfeed) {\n@@ -1090,10 +1068,7 @@ ENTRY entry {\n                              op::Constant(), op::Constant(), op::Constant())),\n                          op::Shape(\"f32[16,3,6,16,32]\"));\n   auto resharded_rhs =\n-      AllOf(op::Shape(\"f32[16,6,6,16,32]\"),\n-            op::AllReduce(op::DynamicUpdateSlice(\n-                op::Broadcast(), rhs, op::Constant(), op::Reshape(),\n-                op::Constant(), op::Constant(), op::Constant())));\n+      AllOf(op::Shape(\"f32[16,6,6,16,32]\"), op::AllGather(rhs));\n \n   auto left_halo = AllOf(op::CollectivePermute(op::Slice(lhs)),\n                          op::Shape(\"f32[16,2,12,24,32]\"));\n@@ -2584,17 +2559,11 @@ ENTRY entry {\n   VLOG(1) << module->ToString();\n \n   auto param0 = AllOf(op::Parameter(0), op::Shape(\"f32[7,129]\"));\n-  auto param0_adjusted =\n-      AllOf(op::Select(op::Compare(op::Add(), op::Broadcast(op::Constant())),\n-                       param0, op::Broadcast(op::Constant())),\n-            op::Shape(\"f32[7,129]\"));\n-  auto param0_replicated = AllOf(op::AllReduce(op::DynamicUpdateSlice(\n-                                     op::Broadcast(), param0_adjusted, _, _)),\n-                                 op::Shape(\"f32[7,257]\"));\n+  auto param0_replicated =\n+      AllOf(op::Slice(op::AllGather(param0)), op::Shape(\"f32[7,257]\"));\n   auto param1 = AllOf(op::Parameter(1), op::Shape(\"f32[7,58]\"));\n-  auto param1_replicated = AllOf(\n-      op::AllReduce(op::DynamicUpdateSlice(op::Broadcast(), param1, _, _)),\n-      op::Shape(\"f32[7,116]\"));\n+  auto param1_replicated =\n+      AllOf(op::AllGather(param1), op::Shape(\"f32[7,116]\"));\n \n   auto concatenate =\n       AllOf(op::Concatenate(param0_replicated, param1_replicated),\n@@ -2625,8 +2594,7 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           PartitionComputation(hlo_string, /*num_devices=*/4));\n \n-  auto param0_replicated = AllOf(op::AllReduce(\n-      op::DynamicUpdateSlice(op::Broadcast(), op::Parameter(0), _)));\n+  auto param0_replicated = AllOf(op::AllGather(op::Parameter(0)));\n   auto concatenate_replicated =\n       AllOf(op::Concatenate(param0_replicated, param0_replicated),\n             op::Shape(\"f32[512]\"));\n@@ -2709,37 +2677,31 @@ ENTRY entry {\n \n // Pad is treated as a special case of window operator. When this pad-window has\n // a large edge pad, halo exchange with collective permute is not sufficient.\n-// resharding with collective(AG or AR) is needed.\n+// resharding with collective (AG or AR) is needed.\n // This test case aims to validate the collective insertion behavior when spmd\n // partitioner handles large cross-partition pad in SPMD partitioner.\n TEST_P(SpmdPartitioningTest, LargeEdgePadAlongCrossPartitionDimension) {\n   absl::string_view hlo_string = R\"(\n     HloModule module\n \n     ENTRY entry {\n-      %param0 = f32[14,257] parameter(0), sharding={devices=[2,4]0,1,2,3,4,5,6,7}\n+      %param0 = f32[14,257] parameter(0), sharding={devices=[2,4]<=[8]}\n       %const = f32[] constant(0)\n-      ROOT %pad = f32[14,2257] pad(%param0, %const), padding=0_0x0_2000,\n-    sharding={devices=[2,4]0,1,2,3,4,5,6,7}\n+      ROOT %pad = f32[14,2257] pad(%param0, %const), padding=0_0x0_2000, sharding={devices=[2,4]<=[8]}\n   })\";\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           PartitionComputation(hlo_string, /*num_devices=*/8));\n-  VLOG(1) << module->ToString();\n-\n-  const HloInstruction* root = module->entry_computation()->root_instruction();\n-\n-  // Reshard operand will trigger collective.\n-  // All-gather is intended to be used in production for pad operation,\n-  // dynamic-update-slice + all-reduce is used in test environment for pattern\n-  // matching purpose.\n-  const HloInstruction* all_reduce =\n-      FindInstruction(module.get(), HloOpcode::kAllReduce);\n-  ASSERT_NE(all_reduce, nullptr);\n \n-  EXPECT_THAT(all_reduce->operand(0), op::DynamicUpdateSlice());\n+  auto param0 = AllOf(op::Parameter(), op::Shape(\"f32[7,65]\"));\n+  auto all_gather = AllOf(op::AllGather(param0), op::Shape(\"f32[7,260]\"));\n+  auto remove_padding = AllOf(op::Slice(all_gather), op::Shape(\"f32[7,257]\"));\n+  auto pad =\n+      AllOf(op::Pad(remove_padding, op::Constant()), op::Shape(\"f32[7,2260]\"));\n+  auto dynamic_slice =\n+      AllOf(op::DynamicSlice(pad, _, _), op::Shape(\"f32[7,565]\"));\n \n-  EXPECT_THAT(root, op::DynamicSlice(op::Pad(all_reduce, _), _, _));\n+  EXPECT_THAT(module->entry_computation()->root_instruction(), dynamic_slice);\n }\n \n TEST_P(SpmdPartitioningTest, LargeRightPadOnSliceHaloExchange) {\n@@ -3822,7 +3784,7 @@ ENTRY entry {\n         EXPECT_EQ(operand->shape().dimensions(0), 1);\n         EXPECT_EQ(operand->shape().dimensions(1), 1024);\n       }\n-      EXPECT_THAT(inst, op::Sort(op::AllReduce(), op::AllReduce()));\n+      EXPECT_THAT(inst, op::Sort(op::AllGather(), op::AllGather()));\n     }\n     EXPECT_NE(inst->opcode(), HloOpcode::kAllToAll);\n   }\n@@ -4253,8 +4215,7 @@ ENTRY %reshape {\n       auto module, PartitionComputation(hlo_string, /*num_devices=*/128));\n   VLOG(1) << module->ToString();\n   const auto root = module->entry_computation()->root_instruction();\n-  auto reshape = AllOf(op::Reshape(op::AllReduce(op::DynamicUpdateSlice(\n-                           _, op::Parameter(0), _, _, _, _, _, _, _))),\n+  auto reshape = AllOf(op::Reshape(op::AllGather(op::Parameter(0))),\n                        op::Shape(\"bf16[320,4,8]\"));\n   EXPECT_THAT(root, AllOf(op::DynamicSlice(reshape, _, _, _),\n                           op::Shape(\"bf16[40,4,8]\")));\n@@ -4293,8 +4254,7 @@ ENTRY %reshape {\n   auto param = AllOf(op::Parameter(0), op::Shape(\"bf16[6]\"));\n   // Reshard param from {devices=[4]<=[4]} to {devices=[2,2]<=[4]\n   // last_tile_dim_replicate}\n-  auto reshard_param = AllOf(op::AllReduce(op::DynamicUpdateSlice(_, param, _)),\n-                             op::Shape(\"bf16[12]\"));\n+  auto reshard_param = AllOf(op::AllGather(param), op::Shape(\"bf16[12]\"));\n \n   auto reshape = AllOf(op::Reshape(reshard_param), op::Shape(\"bf16[3,4]\"));\n \n@@ -4345,9 +4305,7 @@ ENTRY %reshape {\n                           PartitionComputation(hlo_string, /*num_devices=*/4));\n \n   auto param = AllOf(op::Shape(\"bf16[2,8]\"), op::Parameter(0));\n-  auto param_replicated = AllOf(\n-      op::Shape(\"bf16[8,8]\"), op::AllReduce(op::DynamicUpdateSlice(\n-                                  op::Broadcast(op::Constant()), param, _, _)));\n+  auto param_replicated = AllOf(op::Shape(\"bf16[8,8]\"), op::AllGather(param));\n   auto reshape = AllOf(op::Shape(\"bf16[64]\"), op::Reshape(param_replicated));\n   auto reshape_resharded =\n       AllOf(op::Shape(\"bf16[16]\"), op::DynamicSlice(reshape, _));\n@@ -4681,12 +4639,8 @@ ENTRY %main {\n                          op::Shape(\"f32[14,1]\"));\n   auto reshape_r = AllOf(op::Reshape(op::GetTupleElement(local_reduce)),\n                          op::Shape(\"s32[14,1]\"));\n-  auto broadcast_l =\n-      AllOf(op::AllReduce(op::DynamicUpdateSlice(_, reshape_l, _, _)),\n-            op::Shape(\"f32[14,4]\"));\n-  auto broadcast_r =\n-      AllOf(op::AllReduce(op::DynamicUpdateSlice(_, reshape_r, _, _)),\n-            op::Shape(\"s32[14,4]\"));\n+  auto broadcast_l = AllOf(op::AllGather(reshape_l), op::Shape(\"f32[14,4]\"));\n+  auto broadcast_r = AllOf(op::AllGather(reshape_r), op::Shape(\"s32[14,4]\"));\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root, AllOf(op::Reduce(broadcast_l, broadcast_r, op::Parameter(2),\n                                      op::Parameter(3)),\n@@ -4736,12 +4690,8 @@ ENTRY %main {\n                          op::Shape(\"f32[28,1]\"));\n   auto reshape_r = AllOf(op::Reshape(op::GetTupleElement(local_reduce)),\n                          op::Shape(\"s32[28,1]\"));\n-  auto broadcast_l =\n-      AllOf(op::AllReduce(op::DynamicUpdateSlice(_, reshape_l, _, _)),\n-            op::Shape(\"f32[28,2]\"));\n-  auto broadcast_r =\n-      AllOf(op::AllReduce(op::DynamicUpdateSlice(_, reshape_r, _, _)),\n-            op::Shape(\"s32[28,2]\"));\n+  auto broadcast_l = AllOf(op::AllGather(reshape_l), op::Shape(\"f32[28,2]\"));\n+  auto broadcast_r = AllOf(op::AllGather(reshape_r), op::Shape(\"s32[28,2]\"));\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root, AllOf(op::Reduce(broadcast_l, broadcast_r, op::Parameter(2),\n                                      op::Parameter(3)),\n@@ -5388,7 +5338,7 @@ ENTRY entry {\n \n   // Check while op.\n   const auto arg0 = AllOf(op::Reshape(), op::Shape(\"bf16[32,1,512]{2,1,0}\"));\n-  const auto arg1 = AllOf(op::AllReduce(), op::Shape(\"bf16[1,512,768]{2,1,0}\"));\n+  const auto arg1 = AllOf(op::AllGather(), op::Shape(\"bf16[1,512,768]{2,1,0}\"));\n \n   const auto while_op =\n       AllOf(op::While(op::Tuple(arg0, arg1, op::Broadcast(), op::Broadcast(),\n@@ -6367,29 +6317,24 @@ TEST_P(SpmdPartitioningTest, EinsumRHSWindowedNonContractingNoDoubleAG) {\n HloModule module\n \n ENTRY entry {\n-  %lhs = f32[32,24,64,128] parameter(0)\n-  %lhs.copy = f32[32,24,64,128] copy(%lhs), sharding={devices=[1,2,1,1]0,1}\n-  %lhs2 = f32[32,24,64,128] parameter(2)\n-  %lhs2.copy = f32[32,24,64,128] copy(%lhs2), sharding={devices=[1,2,1,1]0,1}\n-  %rhs = f32[32,39295,64,128] parameter(1)\n-  %rhs.copy = f32[32,39295,64,128] copy(%rhs), sharding={devices=[1,2,1,1]0,1}\n-  %dot = f32[32,24,39295] dot(%lhs.copy, %rhs.copy),\n+  %lhs = f32[32,24,64,128] parameter(0), sharding={devices=[1,2,1,1]0,1}\n+  %lhs2 = f32[32,24,64,128] parameter(2), sharding={devices=[1,2,1,1]0,1}\n+  %rhs = f32[32,40,64,128] parameter(1), sharding={devices=[1,2,1,1]0,1}\n+  %dot = f32[32,24,40] dot(%lhs, %rhs),\n     lhs_batch_dims={0}, rhs_batch_dims={0},\n     lhs_contracting_dims={2,3}, rhs_contracting_dims={2,3},\n     sharding={devices=[1,2,1]0,1}\n-  %dot2 = f32[32,24,39295] dot(%lhs2.copy, %rhs.copy),\n+  %dot2 = f32[32,24,40] dot(%lhs2, %rhs),\n     lhs_batch_dims={0}, rhs_batch_dims={0},\n     lhs_contracting_dims={2,3}, rhs_contracting_dims={2,3},\n     sharding={devices=[1,2,1]0,1}\n   ROOT %t = tuple(%dot, %dot2)\n })\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(auto module, PartitionComputation(hlo_string,\n-                                                            /*num_devices=*/2));\n-  VLOG(1) << module->ToString();\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          PartitionComputation(hlo_string, /*num_devices=*/2));\n   const auto root = module->entry_computation()->root_instruction();\n-  const auto tuple_element = op::AllReduce(op::DynamicUpdateSlice(\n-      _, op::Dot(_, op::AllReduce(op::DynamicUpdateSlice())), _, _, _));\n+  const auto tuple_element = op::AllGather(op::Dot(_, op::AllGather()));\n   EXPECT_THAT(root, op::Tuple(tuple_element, tuple_element));\n }\n \n@@ -6421,10 +6366,8 @@ ENTRY entry {\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(\n       root,\n-      op::Tuple(op::AllReduce(op::DynamicUpdateSlice(\n-                    _, op::Slice(op::GetTupleElement(op::While(_))), _, _, _)),\n-                op::AllReduce(op::DynamicUpdateSlice(\n-                    _, op::Dot(_, op::Slice(_)), _, _, _))));\n+      op::Tuple(op::AllGather(op::Slice(op::GetTupleElement(op::While(_)))),\n+                op::AllGather(op::Dot(_, op::Slice(_)))));\n }\n \n TEST_P(SpmdPartitioningTest,\n@@ -6459,14 +6402,11 @@ ENTRY entry {\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(\n       root,\n-      op::Tuple(op::AllReduce(op::DynamicUpdateSlice(\n-                    _, op::Slice(op::GetTupleElement(op::While(_))), _, _, _)),\n-                op::AllReduce(op::DynamicUpdateSlice(\n-                    _, op::Dot(_, op::Slice(_)), _, _, _))));\n+      op::Tuple(op::AllGather(op::Slice(op::GetTupleElement(op::While(_)))),\n+                op::AllGather(op::Dot(_, op::Slice(_)))));\n \n-  // Tuple<-AllReduce<-DynamicUpdateSlice<-Slice<-GetTupleElement<-While\n-  const auto while_loop =\n-      root->operand(0)->operand(0)->operand(1)->operand(0)->operand(0);\n+  // Tuple<-AllGather<-Slice<-GetTupleElement<-While\n+  const auto while_loop = root->operand(0)->operand(0)->operand(0)->operand(0);\n   // Check loop condition.\n   EXPECT_THAT(\n       while_loop->while_condition()->root_instruction(),\n@@ -6529,14 +6469,11 @@ ENTRY entry {\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(\n       root,\n-      op::Tuple(op::AllReduce(op::DynamicUpdateSlice(\n-                    _, op::Slice(op::GetTupleElement(op::While(_))), _, _, _)),\n-                op::AllReduce(op::DynamicUpdateSlice(\n-                    _, op::Dot(_, op::Slice(_)), _, _, _))));\n+      op::Tuple(op::AllGather(op::Slice(op::GetTupleElement(op::While(_)))),\n+                op::AllGather(op::Dot(_, op::Slice(_)))));\n \n-  // Tuple<-AllReduce<-DynamicUpdateSlice<-Slice<-GetTupleElement<-While\n-  const auto while_loop =\n-      root->operand(0)->operand(0)->operand(1)->operand(0)->operand(0);\n+  // Tuple<-AllGather<-Slice<-GetTupleElement<-While\n+  const auto while_loop = root->operand(0)->operand(0)->operand(0)->operand(0);\n   // Check loop condition.\n   EXPECT_THAT(\n       while_loop->while_condition()->root_instruction(),\n@@ -7827,10 +7764,7 @@ ENTRY entry {\n       op::Shape(\"bf16[1,1536,256,1]\"));\n \n   const auto partial_replicate_rhs =\n-      AllOf(op::AllReduce(op::DynamicUpdateSlice(\n-                op::Broadcast(), rhs, op::Constant(), op::Constant(),\n-                op::Reshape(), op::Constant())),\n-            op::Shape(\"bf16[1,1536,512,1]\"));\n+      AllOf(op::AllGather(rhs), op::Shape(\"bf16[1,1536,512,1]\"));\n   EXPECT_THAT(\n       root,\n       AllOf(op::DynamicSlice(\n@@ -7869,10 +7803,7 @@ ENTRY entry {\n       op::Shape(\"bf16[1,1536,256,1]\"));\n \n   const auto partial_replicate_rhs =\n-      AllOf(op::AllReduce(op::DynamicUpdateSlice(\n-                op::Broadcast(), rhs, op::Constant(), op::Constant(),\n-                op::Reshape(), op::Constant())),\n-            op::Shape(\"bf16[1,1536,512,1]\"));\n+      AllOf(op::AllGather(rhs), op::Shape(\"bf16[1,1536,512,1]\"));\n   EXPECT_THAT(\n       root,\n       AllOf(op::DynamicSlice(\n@@ -8072,10 +8003,7 @@ ENTRY entry {\n \n   const auto root = module->entry_computation()->root_instruction();\n   auto input = AllOf(op::Parameter(0), op::Shape(\"s32[128,32]\"));\n-  auto update = AllOf(\n-      op::AllReduce(op::DynamicUpdateSlice(op::Broadcast(), op::Parameter(2),\n-                                           op::Constant(), op::Reshape())),\n-      op::Shape(\"s32[128,2]\"));\n+  auto update = AllOf(op::AllGather(op::Parameter(2)), op::Shape(\"s32[128,2]\"));\n \n   EXPECT_THAT(root,\n               AllOf(op::Select(op::Broadcast(),\n@@ -8143,11 +8071,8 @@ ENTRY entry {\n   auto input = AllOf(op::Copy(op::DynamicSlice(op::Parameter(0), op::Reshape(),\n                                                op::Reshape())),\n                      op::Shape(\"s32[64,32]\"));\n-  auto update = AllOf(op::AllReduce(op::DynamicUpdateSlice(\n-                          op::Broadcast(),\n-                          op::Copy(op::DynamicSlice(\n-                              op::Parameter(1), op::Reshape(), op::Reshape())),\n-                          op::Constant(), op::Reshape())),\n+  auto update = AllOf(op::AllGather(op::Copy(op::DynamicSlice(\n+                          op::Parameter(1), op::Reshape(), op::Reshape()))),\n                       op::Shape(\"s32[64,2]\"));\n \n   EXPECT_THAT(root,\n@@ -8298,7 +8223,7 @@ ENTRY entry {\n                           PartitionComputation(hlo_string, /*num_devices=*/8));\n   auto root = module->entry_computation()->root_instruction();\n   auto operand = AllOf(op::Shape(\"f32[2,9,8]\"), op::Parameter(0));\n-  auto indices = AllOf(op::Shape(\"s32[2,2,2]\"), op::AllReduce());\n+  auto indices = AllOf(op::Shape(\"s32[2,2,2]\"), op::AllGather());\n   auto gather = AllOf(op::Shape(\"f32[8,2,2]\"), op::Gather(operand, indices));\n   VLOG(1) << module->ToString();\n   EXPECT_THAT(root, op::CollectivePermute(gather));\n@@ -8548,17 +8473,14 @@ ENTRY entry {\n                              op::Shape(\"f32[3,9]\"));\n   EXPECT_THAT(\n       root,\n-      AllOf(op::Tuple(op::DynamicSlice(\n-                          op::Pad(op::AllReduce(op::DynamicUpdateSlice(\n-                                      _, op::GetTupleElement(scatter), _, _)),\n-                                  _),\n-                          _, _),\n-                      op::DynamicSlice(\n-                          op::Pad(op::AllReduce(op::DynamicUpdateSlice(\n-                                      _, op::GetTupleElement(scatter), _, _)),\n-                                  _),\n-                          _, _)),\n-            op::Shape(\"(f32[2,3],f32[2,3])\")));\n+      AllOf(\n+          op::Tuple(op::DynamicSlice(\n+                        op::Pad(op::AllGather(op::GetTupleElement(scatter)), _),\n+                        _, _),\n+                    op::DynamicSlice(\n+                        op::Pad(op::AllGather(op::GetTupleElement(scatter)), _),\n+                        _, _)),\n+          op::Shape(\"(f32[2,3],f32[2,3])\")));\n }\n \n TEST_P(SpmdPartitioningTest, VariadicScatterSharedOperands) {\n@@ -8838,7 +8760,7 @@ ENTRY entry {\n   VLOG(1) << module->ToString();\n   const auto root = module->entry_computation()->root_instruction();\n   auto operand = AllOf(op::Shape(\"f32[2,9,8]\"), op::Select());\n-  auto indices = AllOf(op::Shape(\"s32[2,2,2]\"), op::AllReduce());\n+  auto indices = AllOf(op::Shape(\"s32[2,2,2]\"), op::AllGather());\n   auto update = AllOf(op::Shape(\"f32[2,2,8]\"), op::CollectivePermute());\n   auto scatter =\n       AllOf(op::Shape(\"f32[2,9,8]\"), op::Scatter(operand, indices, update));\n@@ -9413,12 +9335,10 @@ ENTRY entry {\n \n   const auto lhs = AllOf(op::Shape(\"f32[24,6]\"), op::Parameter(0));\n   auto partial_replicated_lhs =\n-      AllOf(op::Shape(\"f32[24,12]\"),\n-            op::AllReduce(op::DynamicUpdateSlice(_, lhs, _, _)));\n+      AllOf(op::Shape(\"f32[24,12]\"), op::AllGather(lhs));\n   const auto rhs = AllOf(op::Shape(\"f32[16,6]\"), op::Parameter(1));\n   auto partial_replicated_rhs =\n-      AllOf(op::Shape(\"f32[16,12]\"),\n-            op::AllReduce(op::DynamicUpdateSlice(_, rhs, _, _)));\n+      AllOf(op::Shape(\"f32[16,12]\"), op::AllGather(rhs));\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root,\n               AllOf(op::Dot(partial_replicated_lhs, partial_replicated_rhs),\n@@ -9445,8 +9365,7 @@ ENTRY entry {\n   const auto lhs = AllOf(op::Shape(\"f32[24,50]\"), op::Parameter(0));\n   const auto rhs = AllOf(op::Shape(\"f32[16,50]\"), op::Parameter(1));\n   auto partial_replicated_rhs =\n-      AllOf(op::Shape(\"f32[32,50]\"),\n-            op::AllReduce(op::DynamicUpdateSlice(_, rhs, _, _)));\n+      AllOf(op::Shape(\"f32[32,50]\"), op::AllGather(rhs));\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(\n       root, AllOf(op::Shape(\"f32[24,16]\"),\n@@ -9478,8 +9397,7 @@ ENTRY entry {\n       AllOf(op::Shape(\"f32[24,100]\"), op::DynamicSlice(lhs, _, _));\n   const auto rhs = AllOf(op::Shape(\"f32[16,50]\"), op::Parameter(1));\n   auto partial_replicated_rhs = AllOf(\n-      op::Shape(\"f32[16,100]\"), op::AllReduce(op::DynamicUpdateSlice(\n-                                    _, op::CollectivePermute(rhs), _, _)));\n+      op::Shape(\"f32[16,100]\"), op::AllGather(op::CollectivePermute(rhs)));\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root, AllOf(op::Shape(\"f32[24,16]\"),\n                           op::Dot(lhs_slice, partial_replicated_rhs)));\n@@ -9534,8 +9452,7 @@ ENTRY entry {\n   const auto lhs = AllOf(op::Shape(\"f32[2,12,100]\"), op::Parameter(0));\n   const auto rhs = AllOf(op::Shape(\"f32[2,16,100]\"), op::Parameter(1));\n   auto partial_replicated_rhs =\n-      AllOf(op::Shape(\"f32[2,32,100]\"),\n-            op::AllReduce(op::DynamicUpdateSlice(_, rhs, _, _, _)));\n+      AllOf(op::Shape(\"f32[2,32,100]\"), op::AllGather(rhs));\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root, AllOf(op::Shape(\"f32[2,12,32]\"),\n                           op::Dot(lhs, partial_replicated_rhs)));\n@@ -9621,8 +9538,7 @@ ENTRY entry {\n   const auto lhs = AllOf(op::Shape(\"f32[2,24,50]\"), op::Parameter(0));\n   const auto rhs = AllOf(op::Shape(\"f32[2,16,100]\"), op::Parameter(1));\n   auto partial_replicated_lhs =\n-      AllOf(op::Shape(\"f32[2,24,100]\"),\n-            op::AllReduce(op::DynamicUpdateSlice(_, lhs, _, _, _)));\n+      AllOf(op::Shape(\"f32[2,24,100]\"), op::AllGather(lhs));\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root, AllOf(op::Shape(\"f32[2,24,16]\"),\n                           op::Dot(partial_replicated_lhs, rhs)));\n@@ -9648,8 +9564,7 @@ ENTRY entry {\n   const auto lhs = AllOf(op::Shape(\"f32[2,8,12,100]\"), op::Parameter(0));\n   const auto rhs = AllOf(op::Shape(\"f32[2,8,16,100]\"), op::Parameter(1));\n   auto partial_replicated_rhs =\n-      AllOf(op::Shape(\"f32[2,8,32,100]\"),\n-            op::AllReduce(op::DynamicUpdateSlice(_, rhs, _, _, _, _)));\n+      AllOf(op::Shape(\"f32[2,8,32,100]\"), op::AllGather(rhs));\n   auto dot =\n       AllOf(op::Shape(\"f32[2,8,12,32]\"), op::Dot(lhs, partial_replicated_rhs));\n   auto reshape = AllOf(op::Shape(\"f32[2,2,4,12,32]\"), op::Reshape(dot));\n@@ -9814,8 +9729,7 @@ ENTRY entry {\n   const auto lhs = AllOf(op::Shape(\"f32[12,8,100]\"), op::Parameter(0));\n   const auto rhs = AllOf(op::Shape(\"f32[16,50]\"), op::Parameter(1));\n   auto partially_replicated_rhs =\n-      AllOf(op::Shape(\"f32[16,100]\"),\n-            op::AllReduce(op::DynamicUpdateSlice(op::Broadcast(_), rhs, _, _)));\n+      AllOf(op::Shape(\"f32[16,100]\"), op::AllGather(rhs));\n   auto dot =\n       AllOf(op::Shape(\"f32[12,8,16]\"), op::Dot(lhs, partially_replicated_rhs));\n   const auto root = module->entry_computation()->root_instruction();\n@@ -9868,10 +9782,9 @@ ENTRY entry {\n \n   const auto lhs = AllOf(op::Shape(\"f32[12,8,50]\"), op::Parameter(0));\n   const auto rhs = AllOf(op::Shape(\"f32[50,25]\"), op::Parameter(1));\n-  auto dot = AllOf(\n-      op::Shape(\"f32[12,8,50]\"),\n-      op::Dot(lhs, AllOf(op::Shape(\"f32[50,50]\"),\n-                         op::AllReduce(op::DynamicUpdateSlice(_, rhs, _, _)))));\n+  auto dot =\n+      AllOf(op::Shape(\"f32[12,8,50]\"),\n+            op::Dot(lhs, AllOf(op::Shape(\"f32[50,50]\"), op::AllGather(rhs))));\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root, AllOf(op::Shape(\"f32[12,4,50]\"),\n                           op::DynamicSlice(op::AllReduce(dot), _, _, _)))\n@@ -9898,10 +9811,9 @@ ENTRY entry {\n \n   const auto lhs = AllOf(op::Shape(\"f32[12,4,10]\"), op::Parameter(0));\n   const auto rhs = AllOf(op::Shape(\"f32[5,50]\"), op::Parameter(1));\n-  auto dot = AllOf(\n-      op::Shape(\"f32[12,4,50]\"),\n-      op::Dot(lhs, AllOf(op::Shape(\"f32[10,50]\"),\n-                         op::AllReduce(op::DynamicUpdateSlice(_, rhs, _, _)))));\n+  auto dot =\n+      AllOf(op::Shape(\"f32[12,4,50]\"),\n+            op::Dot(lhs, AllOf(op::Shape(\"f32[10,50]\"), op::AllGather(rhs))));\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root, dot) << module->ToString();\n }\n@@ -9969,9 +9881,8 @@ ENTRY main {\n   VLOG(1) << module->ToString();\n \n   const auto lhs = AllOf(op::Shape(\"bf16[1024,256]\"), op::Parameter(0));\n-  const auto rhs = AllOf(op::Shape(\"bf16[1024,256]\"),\n-                         op::AllReduce(op::DynamicUpdateSlice(\n-                             op::Broadcast(), op::Parameter(1), _, _)));\n+  const auto rhs =\n+      AllOf(op::Shape(\"bf16[1024,256]\"), op::AllGather(op::Parameter(1)));\n   auto dot = AllOf(op::Shape(\"bf16[256,256]\"), op::Dot(lhs, rhs));\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root, op::AllReduce(dot));\n@@ -9995,9 +9906,8 @@ ENTRY main {\n   const auto all_to_all_p1 = AllOf(\n       op::Shape(\"bf16[32,64,16]\"),\n       op::Reshape(op::Transpose(op::AllToAll(op::Reshape(op::Parameter(1))))));\n-  const auto rhs = AllOf(op::Shape(\"bf16[32,64,32]\"),\n-                         op::AllReduce(op::DynamicUpdateSlice(\n-                             op::Broadcast(), all_to_all_p1, _, _, _)));\n+  const auto rhs =\n+      AllOf(op::Shape(\"bf16[32,64,32]\"), op::AllGather(all_to_all_p1));\n   auto dot = AllOf(op::Shape(\"bf16[32,32,64]\"), op::Dot(lhs, rhs));\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root, op::AllReduce(dot));\n@@ -10017,9 +9927,8 @@ ENTRY main {\n       auto module, PartitionComputation(hlo_string, /*num_devices=*/128));\n   VLOG(1) << module->ToString();\n \n-  const auto lhs = AllOf(op::Shape(\"bf16[128,8,8,1280]\"),\n-                         op::AllReduce(op::DynamicUpdateSlice(\n-                             op::Broadcast(), op::Parameter(0), _, _, _, _)));\n+  const auto lhs =\n+      AllOf(op::Shape(\"bf16[128,8,8,1280]\"), op::AllGather(op::Parameter(0)));\n   const auto rhs = AllOf(op::Shape(\"bf16[3,3,1280,160]\"), op::Parameter(1));\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root,\n@@ -10060,10 +9969,8 @@ ENTRY entry {\n       AllOf(op::Shape(\"f32[6,2]\"), op::Multiply(multiply_lhs, multiply_rhs));\n   auto add = AllOf(op::Shape(\"f32[6,2]\"), op::Add(multiply, multiply_rhs));\n   const auto root = module->entry_computation()->root_instruction();\n-  EXPECT_THAT(root, AllOf(op::Shape(\"f32[6,3]\"),\n-                          op::AllReduce(op::DynamicUpdateSlice(\n-                              op::Broadcast(), op::Select(_, add, _),\n-                              op::Constant(), op::Reshape()))));\n+  EXPECT_THAT(root,\n+              AllOf(op::Shape(\"f32[6,3]\"), op::Slice(op::AllGather(add))));\n }\n \n TEST_P(SpmdPartitioningTest, ElementwiseTest_SubgroupSharding_ReplicateToTile) {\n@@ -10143,9 +10050,8 @@ ENTRY entry {\n   auto tiled = AllOf(op::Shape(\"f32[4,4]\"),\n                      op::Copy(op::DynamicSlice(op::Parameter(0), op::Reshape(),\n                                                op::Reshape())));\n-  auto partially_replicated = AllOf(\n-      op::Shape(\"f32[4,8]\"), op::Copy(op::AllReduce(op::DynamicUpdateSlice(\n-                                 op::Broadcast(_), tiled, _, _))));\n+  auto partially_replicated =\n+      AllOf(op::Shape(\"f32[4,8]\"), op::Copy(op::AllGather(tiled)));\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root, partially_replicated);\n }\n@@ -10163,19 +10069,19 @@ ENTRY entry {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           PartitionComputation(hlo_string, /*num_devices=*/6));\n   VLOG(1) << module->ToString();\n-  auto tiled = AllOf(op::Shape(\"f32[4,3]\"), op::Select(_, op::Parameter(0), _));\n-  auto partially_replicated = AllOf(\n-      op::Shape(\"f32[8,4]\"),\n-      op::Copy(op::Reshape(op::Transpose(op::AllToAll(op::Reshape(op::AllReduce(\n-          op::DynamicUpdateSlice(op::Broadcast(), tiled, _, _))))))));\n+  auto tiled = AllOf(op::Shape(\"f32[4,3]\"), op::Parameter(0));\n+  auto partially_replicated =\n+      AllOf(op::Shape(\"f32[8,4]\"),\n+            op::Copy(op::Reshape(op::Transpose(\n+                op::AllToAll(op::Reshape(op::Slice(op::AllGather(tiled))))))));\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root, partially_replicated);\n \n-  const HloInstruction* all_reduce =\n-      FindInstruction(module.get(), \"all-reduce\");\n-  EXPECT_NE(all_reduce, nullptr);\n+  const HloInstruction* all_gather =\n+      FindInstruction(module.get(), \"all-gather\");\n+  EXPECT_NE(all_gather, nullptr);\n   EXPECT_TRUE(\n-      absl::StrContains(all_reduce->ToString(), \"replica_groups=[2,3]<=[6]\"));\n+      absl::StrContains(all_gather->ToString(), \"replica_groups=[2,3]<=[6]\"));\n }\n \n TEST_P(SpmdPartitioningTest, PartialReplicateToTileReshardUnevenPartition) {\n@@ -10252,8 +10158,7 @@ ENTRY entry {\n                                       op::Reshape())));\n   auto partially_replicated =\n       AllOf(op::Shape(\"f32[4,8]\"),\n-            op::Copy(op::AllReduce(op::DynamicUpdateSlice(\n-                op::Broadcast(_), partially_replicated_init, _, _))));\n+            op::Copy(op::AllGather(partially_replicated_init)));\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root, partially_replicated);\n }\n@@ -10309,8 +10214,7 @@ ENTRY entry {\n                 op::Parameter(0), op::Reshape(), op::Reshape()))));\n   auto partially_replicated =\n       AllOf(op::Shape(\"f32[8,4]\"),\n-            op::Copy(op::AllReduce(op::DynamicUpdateSlice(\n-                op::Broadcast(_), partially_replicated_init, _, _))));\n+            op::Copy(op::AllGather(partially_replicated_init)));\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root, partially_replicated);\n }\n@@ -10368,9 +10272,7 @@ ENTRY entry {\n   auto concat = op::Concatenate(piece1, piece2);\n   auto partially_replicated =\n       AllOf(op::Shape(\"f32[3,3]\"),\n-            op::AllReduce(op::DynamicUpdateSlice(\n-                op::Broadcast(_),\n-                op::Select(_, op::DynamicSlice(concat, _, _), _), _, _)));\n+            op::Slice(op::AllGather(op::DynamicSlice(concat, _, _))));\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root, op::Copy(partially_replicated));\n }\n@@ -10755,8 +10657,7 @@ ENTRY entry {\n   const auto lhs = AllOf(op::Shape(\"f32[4,275,4]\"));\n   const auto rhs = AllOf(op::Shape(\"f32[1,275,4]\"));\n   auto conv = AllOf(op::Convolution(lhs, rhs), op::Shape(\"f32[5,4,4]\"));\n-  EXPECT_THAT(root, AllOf(op::AllReduce(op::DynamicUpdateSlice(\n-                              _, op::CollectivePermute(conv), _, _, _)),\n+  EXPECT_THAT(root, AllOf(op::AllGather(op::CollectivePermute(conv)),\n                           op::Shape(\"f32[5,4,8]\")));\n }\n \n@@ -10781,8 +10682,7 @@ ENTRY entry {\n   const auto lhs = AllOf(op::Shape(\"f32[4,275,4]\"));\n   const auto rhs = AllOf(op::Shape(\"f32[1,275,4]\"));\n   auto conv = AllOf(op::Convolution(lhs, rhs), op::Shape(\"f32[5,4,4]\"));\n-  EXPECT_THAT(root, AllOf(op::AllReduce(op::DynamicUpdateSlice(\n-                              _, op::CollectivePermute(conv), _, _, _)),\n+  EXPECT_THAT(root, AllOf(op::AllGather(op::CollectivePermute(conv)),\n                           op::Shape(\"f32[5,4,8]\")));\n }\n \n@@ -11397,11 +11297,9 @@ ENTRY entry {\n   const auto root = module->entry_computation()->root_instruction();\n   auto param = AllOf(op::Parameter(), op::Shape(\"f32[2000, 1000]\"));\n   auto resharded_lhs =\n-      AllOf(op::AllReduce(op::DynamicUpdateSlice(_, param, _, _)),\n-            op::Shape(\"f32[2000, 4000]\"));\n+      AllOf(op::AllGather(param), op::Shape(\"f32[2000, 4000]\"));\n   auto resharded_rhs =\n-      AllOf(op::AllReduce(op::DynamicUpdateSlice(_, op::Copy(param), _, _)),\n-            op::Shape(\"f32[4000, 1000]\"));\n+      AllOf(op::AllGather(op::Copy(param)), op::Shape(\"f32[4000, 1000]\"));\n   EXPECT_THAT(root, AllOf(op::Convolution(resharded_lhs, resharded_rhs),\n                           op::Shape(\"f32[2000, 1000]\")));\n }\n@@ -11469,8 +11367,7 @@ ENTRY %module {\n   auto operand = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Reshape());\n   auto indices = AllOf(op::Shape(\"s32[2,1,4]\"), op::Subtract());\n   auto gather = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Gather(operand, indices));\n-  EXPECT_THAT(root,\n-              op::AllReduce(op::DynamicUpdateSlice(_, gather, _, _, _, _)));\n+  EXPECT_THAT(root, op::AllGather(gather));\n }\n \n TEST_P(SpmdPartitioningTest, GatherParallelDimRedistributionIndices) {\n@@ -11499,8 +11396,7 @@ ENTRY %module {\n   auto operand = AllOf(op::Shape(\"s32[2,2,2,2]\"), op::Reshape());\n   auto indices = AllOf(op::Shape(\"s32[2,2,2]\"), op::Subtract());\n   auto gather = AllOf(op::Shape(\"s32[2,2,2,2]\"), op::Gather(operand, indices));\n-  EXPECT_THAT(root, op::AllReduce(op::AllReduce(\n-                        op::DynamicUpdateSlice(_, gather, _, _, _, _))));\n+  EXPECT_THAT(root, op::AllGather(op::AllGather(gather)));\n }\n \n TEST_P(SpmdPartitioningTest, GatherParallelDimReplicatedIndices) {\n@@ -11530,8 +11426,7 @@ ENTRY %module {\n   auto operand = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Parameter());\n   auto indices = AllOf(op::Shape(\"s32[2,1,4]\"), op::Subtract());\n   auto gather = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Gather(operand, indices));\n-  EXPECT_THAT(root,\n-              op::AllReduce(op::DynamicUpdateSlice(_, gather, _, _, _, _)));\n+  EXPECT_THAT(root, op::AllGather(gather));\n }\n \n TEST_P(SpmdPartitioningTest, GatherParallelDimReplicatedOperand) {\n@@ -11560,8 +11455,7 @@ ENTRY %module {\n   auto operand = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::DynamicSlice());\n   auto indices = AllOf(op::Shape(\"s32[2,1,4]\"), op::Subtract());\n   auto gather = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Gather(operand, indices));\n-  EXPECT_THAT(root,\n-              op::AllReduce(op::DynamicUpdateSlice(_, gather, _, _, _, _)));\n+  EXPECT_THAT(root, op::AllGather(gather));\n }\n \n TEST_P(SpmdPartitioningTest, GatherParallelDimPartialReplicatedIndices) {\n@@ -11591,8 +11485,7 @@ ENTRY %module {\n   auto operand = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Parameter());\n   auto indices = AllOf(op::Shape(\"s32[2,1,4]\"), op::Subtract());\n   auto gather = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Gather(operand, indices));\n-  EXPECT_THAT(root,\n-              op::AllReduce(op::DynamicUpdateSlice(_, gather, _, _, _, _)));\n+  EXPECT_THAT(root, op::AllGather(gather));\n }\n \n TEST_P(SpmdPartitioningTest, GatherParallelDimPartialReplicatedOperand) {\n@@ -11622,8 +11515,7 @@ ENTRY %module {\n   auto operand = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::DynamicSlice());\n   auto indices = AllOf(op::Shape(\"s32[2,1,4]\"), op::Subtract());\n   auto gather = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Gather(operand, indices));\n-  EXPECT_THAT(root,\n-              op::AllReduce(op::DynamicUpdateSlice(_, gather, _, _, _, _)));\n+  EXPECT_THAT(root, op::AllGather(gather));\n }\n \n TEST_P(SpmdPartitioningTest, GatherParallelDimSwappedDimensions) {\n@@ -11653,8 +11545,7 @@ ENTRY %module {\n   auto operand = AllOf(op::Shape(\"s32[4,1,2,2]\"), op::CollectivePermute());\n   auto indices = AllOf(op::Shape(\"s32[2,4,1]\"), op::Subtract());\n   auto gather = AllOf(op::Shape(\"s32[4,1,2,2]\"), op::Gather(operand, indices));\n-  EXPECT_THAT(root, op::AllReduce(op::AllReduce(\n-                        op::DynamicUpdateSlice(_, gather, _, _, _, _))));\n+  EXPECT_THAT(root, op::AllGather(op::AllGather(gather)));\n }\n \n TEST_P(SpmdPartitioningTest, GatherParallelDimFromOutsideWhilePositive) {\n@@ -11717,10 +11608,7 @@ ENTRY entry {\n   auto operand = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::DynamicSlice());\n   auto indices = AllOf(op::Shape(\"s32[2,1,4]\"), op::Subtract());\n   auto gather = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Gather(operand, indices));\n-  EXPECT_THAT(\n-      root,\n-      op::Tuple(op::AllReduce(op::DynamicUpdateSlice(_, gather, _, _, _, _)), _,\n-                _));\n+  EXPECT_THAT(root, op::Tuple(op::AllGather(gather), _, _));\n }\n \n TEST_P(SpmdPartitioningTest, GatherParallelDimFromOutsideWhileNegative) {\n@@ -11785,10 +11673,7 @@ ENTRY entry {\n   auto operand = AllOf(op::Shape(\"s32[8,4,2,2]\"), op::GetTupleElement());\n   auto indices = AllOf(op::Shape(\"s32[2,1,4]\"), op::Concatenate());\n   auto gather = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Gather(operand, indices));\n-  EXPECT_THAT(\n-      root,\n-      op::Tuple(op::AllReduce(op::DynamicUpdateSlice(_, gather, _, _, _, _)), _,\n-                _));\n+  EXPECT_THAT(root, op::Tuple(op::AllGather(gather), _, _));\n }\n \n TEST_P(SpmdPartitioningTest, ScatterRepsOnLastTileDimDontDivideGroups) {\n@@ -11921,9 +11806,7 @@ ENTRY entry {\n     auto indices = AllOf(op::Shape(\"s32[2,1,4]\"), op::Subtract());\n     auto gather =\n         AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Gather(operand, indices));\n-    EXPECT_THAT(\n-        partitioned_gather,\n-        op::Copy(op::AllReduce(op::DynamicUpdateSlice(_, gather, _, _, _, _))));\n+    EXPECT_THAT(partitioned_gather, op::Copy(op::AllGather(gather)));\n   }\n \n   // Verify scatter is partitioned properly.\n@@ -11937,9 +11820,7 @@ ENTRY entry {\n     auto update = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::DynamicSlice());\n     auto scatter =\n         AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Scatter(operand, indices, update));\n-    EXPECT_THAT(partitioned_scatter,\n-                op::Copy(op::AllReduce(\n-                    op::DynamicUpdateSlice(_, scatter, _, _, _, _))));\n+    EXPECT_THAT(partitioned_scatter, op::Copy(op::AllGather(scatter)));\n   }\n }\n \n@@ -11970,11 +11851,10 @@ ENTRY %module {\n                           PartitionComputation(hlo_string, /*num_devices=*/4));\n   const auto root = module->entry_computation()->root_instruction();\n   VLOG(1) << module->ToString();\n-  auto operand = AllOf(op::Shape(\"s32[4,4,2,2]\"), op::AllReduce());\n+  auto operand = AllOf(op::Shape(\"s32[4,4,2,2]\"), op::AllGather());\n   auto indices = AllOf(op::Shape(\"s32[2,4,2]\"), op::Subtract());\n   auto gather = AllOf(op::Shape(\"s32[4,2,2,2]\"), op::Gather(operand, indices));\n-  EXPECT_THAT(root, op::AllReduce(op::AllReduce(\n-                        op::DynamicUpdateSlice(_, gather, _, _, _, _))));\n+  EXPECT_THAT(root, op::AllGather(op::AllGather(gather)));\n }\n \n TEST_P(SpmdPartitioningTest, Gather_b303520921) {\n@@ -12026,8 +11906,7 @@ ENTRY %module {\n   auto operand = AllOf(op::Shape(\"s32[2,4,1,2]\"), op::Reshape());\n   auto indices = AllOf(op::Shape(\"s32[2,2,4]\"), op::Subtract());\n   auto gather = AllOf(op::Shape(\"s32[2,4,1,2]\"), op::Gather(operand, indices));\n-  EXPECT_THAT(root, op::AllReduce(op::AllReduce(\n-                        op::DynamicUpdateSlice(_, gather, _, _, _, _))));\n+  EXPECT_THAT(root, op::AllGather(op::AllGather(gather)));\n   auto* all_to_all = FindInstruction(module.get(), \"all-to-all\");\n   EXPECT_TRUE(all_to_all != nullptr);\n   if (GetParam() ==\n@@ -12069,9 +11948,7 @@ ENTRY %module {\n   auto indices = AllOf(op::Shape(\"s32[2,2,1]\"), op::Subtract());\n   auto gather = AllOf(op::Shape(\"s32[2,1,2,2]\"), op::Gather(operand, indices));\n   VLOG(1) << module->ToString();\n-  EXPECT_THAT(root,\n-              op::AllReduce(op::DynamicUpdateSlice(\n-                  _, op::AllReduce(op::Select(_, _, gather)), _, _, _, _)));\n+  EXPECT_THAT(root, op::AllGather(op::AllReduce(op::Select(_, _, gather))));\n }\n \n TEST_P(SpmdPartitioningTest,\n@@ -12160,9 +12037,8 @@ ENTRY %module {\n   auto indices = AllOf(op::Shape(\"s32[2,8,4]\"), op::Subtract());\n   auto gather = AllOf(op::Shape(\"s32[8,4,1,2]\"), op::Gather(operand, indices));\n   EXPECT_THAT(\n-      root, op::AllReduce(op::DynamicUpdateSlice(\n-                _, op::AllReduce(op::AllReduce(op::Select(_, _, gather))), _, _,\n-                _, _)));\n+      root,\n+      op::AllGather(op::AllReduce(op::AllReduce(op::Select(_, _, gather)))));\n }\n \n TEST_P(SpmdPartitioningTest,\n@@ -12188,10 +12064,7 @@ ENTRY %module {\n   auto operand = AllOf(op::Shape(\"s32[8,4,1,2]\"), op::Parameter());\n   auto indices = AllOf(op::Shape(\"s32[2,4,4]\"), op::CollectivePermute());\n   auto gather = AllOf(op::Shape(\"s32[4,4,1,2]\"), op::Gather(operand, indices));\n-  EXPECT_THAT(\n-      root, op::AllReduce(op::DynamicUpdateSlice(\n-                _, op::AllReduce(op::DynamicUpdateSlice(_, gather, _, _, _, _)),\n-                _, _, _, _)));\n+  EXPECT_THAT(root, op::AllGather(op::AllGather(gather)));\n }\n \n TEST_P(SpmdPartitioningTest,\n@@ -12214,13 +12087,10 @@ ENTRY %module {\n                           PartitionComputation(hlo_string, /*num_devices=*/8));\n   VLOG(1) << module->ToString();\n   const auto root = module->entry_computation()->root_instruction();\n-  auto operand = AllOf(op::Shape(\"s32[8,4,1,2]\"), op::AllReduce());\n+  auto operand = AllOf(op::Shape(\"s32[8,4,1,2]\"), op::AllGather());\n   auto indices = AllOf(op::Shape(\"s32[2,4,2]\"), op::Parameter());\n   auto gather = AllOf(op::Shape(\"s32[4,2,1,2]\"), op::Gather(operand, indices));\n-  EXPECT_THAT(\n-      root, op::AllReduce(op::AllReduce(op::DynamicUpdateSlice(\n-                _, op::AllReduce(op::DynamicUpdateSlice(_, gather, _, _, _, _)),\n-                _, _, _, _))));\n+  EXPECT_THAT(root, op::AllGather(op::AllGather(op::AllGather(gather))));\n }\n \n TEST_P(SpmdPartitioningTest,\n@@ -12239,8 +12109,10 @@ ENTRY %module {\n     collapsed_slice_dims={0,1}, start_index_map={0,1}, index_vector_dim=0,\n     slice_sizes={1,1,2,2}, sharding={replicated}\n })\";\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module,\n+      PartitionComputation(hlo_string, /*num_devices=*/8,\n+                           SpmdPartitionerOptions(), /*use_all_gather=*/false));\n   VLOG(1) << module->ToString();\n   const auto root = module->entry_computation()->root_instruction();\n   auto operand = AllOf(op::Shape(\"s32[4,2,2,2]\"), op::Parameter());\n@@ -12272,12 +12144,12 @@ ENTRY %module {\n                           PartitionComputation(hlo_string, /*num_devices=*/8));\n   VLOG(1) << module->ToString();\n   const auto root = module->entry_computation()->root_instruction();\n-  auto operand = AllOf(op::Shape(\"s32[8,2,2,2]\"), op::AllReduce());\n+  auto operand = AllOf(op::Shape(\"s32[8,2,2,2]\"), op::AllGather());\n   auto indices = AllOf(op::Shape(\"s32[2,4,2]\"), op::Subtract());\n   auto gather = AllOf(op::Shape(\"s32[4,2,2,2]\"), op::Gather(operand, indices));\n-  EXPECT_THAT(root,\n-              op::AllReduce(op::AllReduce(op::DynamicUpdateSlice(\n-                  _, op::AllReduce(op::Select(_, _, gather)), _, _, _, _))));\n+  EXPECT_THAT(\n+      root,\n+      op::AllGather(op::AllGather(op::AllReduce(op::Select(_, _, gather)))));\n }\n \n TEST_P(SpmdPartitioningTest, GatherTrivialSlicedOperandPartial) {\n@@ -12295,7 +12167,7 @@ ENTRY main.4 {\n                           PartitionComputation(hlo_string, /*num_devices=*/8));\n   VLOG(1) << module->ToString();\n   const auto root = module->entry_computation()->root_instruction();\n-  auto operand = AllOf(op::Shape(\"s64[8,1]\"), op::AllReduce());\n+  auto operand = AllOf(op::Shape(\"s64[8,1]\"), op::AllGather());\n   auto indices = AllOf(op::Shape(\"s32[2]\"), op::Subtract());\n   auto gather = AllOf(op::Shape(\"s64[2,1]\"), op::Gather(operand, indices));\n   EXPECT_THAT(root, op::AllReduce(op::Select(_, _, gather)));\n@@ -12444,20 +12316,15 @@ ENTRY entry {\n       PartitionComputation(hlo_string, /*num_devices=*/32, options));\n   VLOG(1) << module->ToString();\n   HloInstruction* root = module->entry_computation()->root_instruction();\n-  EXPECT_THAT(\n-      root,\n-      op::AllReduce(op::DynamicUpdateSlice(\n-          _, op::AllReduce(op::Select(_, _, op::Gather(op::AllReduce(_), _))),\n-          _, _, _)));\n+  EXPECT_THAT(root, op::AllGather(op::AllReduce(\n+                        op::Select(_, _, op::Gather(op::AllGather(_), _)))));\n   auto gather = FindInstruction(module.get(), HloOpcode::kGather);\n   EXPECT_THAT(\n       gather->operand(1),\n       op::Subtract(op::Clamp(_, op::Clamp(_, op::Parameter(1), _), _), _));\n   auto collective_permute =\n       FindInstruction(module.get(), HloOpcode::kCollectivePermute);\n   EXPECT_NE(collective_permute, nullptr);\n-  auto all_reduce = FindInstruction(module.get(), HloOpcode::kAllReduce);\n-  EXPECT_THAT(all_reduce->operand(0), op::DynamicUpdateSlice(_, _, _, _));\n   auto dynamic_slice = FindInstruction(module.get(), HloOpcode::kDynamicSlice);\n   EXPECT_THAT(dynamic_slice->operand(1), op::PartitionId());\n }\n@@ -12505,8 +12372,7 @@ ENTRY %module {\n   auto update = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Parameter());\n   auto scatter =\n       AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Scatter(operand, indices, update));\n-  EXPECT_THAT(root,\n-              op::AllReduce(op::DynamicUpdateSlice(_, scatter, _, _, _, _)));\n+  EXPECT_THAT(root, op::AllGather(scatter));\n }\n \n TEST_P(SpmdPartitioningTest, ScatterParallelDimReplicatedIndices) {\n@@ -12550,8 +12416,7 @@ ENTRY %module {\n   auto update = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Parameter());\n   auto scatter =\n       AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Scatter(operand, indices, update));\n-  EXPECT_THAT(root,\n-              op::AllReduce(op::DynamicUpdateSlice(_, scatter, _, _, _, _)));\n+  EXPECT_THAT(root, op::AllGather(scatter));\n }\n \n TEST_P(SpmdPartitioningTest, ScatterParallelDimReplicatedOperand) {\n@@ -12594,8 +12459,7 @@ ENTRY %module {\n   auto update = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Parameter());\n   auto scatter =\n       AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Scatter(operand, indices, update));\n-  EXPECT_THAT(root,\n-              op::AllReduce(op::DynamicUpdateSlice(_, scatter, _, _, _, _)));\n+  EXPECT_THAT(root, op::AllGather(scatter));\n }\n \n TEST_P(SpmdPartitioningTest, ScatterParallelDimReplicatedUpdate) {\n@@ -12638,8 +12502,7 @@ ENTRY %module {\n   auto update = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::DynamicSlice());\n   auto scatter =\n       AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Scatter(operand, indices, update));\n-  EXPECT_THAT(root,\n-              op::AllReduce(op::DynamicUpdateSlice(_, scatter, _, _, _, _)));\n+  EXPECT_THAT(root, op::AllGather(scatter));\n }\n \n TEST_P(SpmdPartitioningTest, ScatterParallelDimPartialReplicatedIndices) {\n@@ -12683,8 +12546,7 @@ ENTRY %module {\n   auto update = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Parameter());\n   auto scatter =\n       AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Scatter(operand, indices, update));\n-  EXPECT_THAT(root,\n-              op::AllReduce(op::DynamicUpdateSlice(_, scatter, _, _, _, _)));\n+  EXPECT_THAT(root, op::AllGather(scatter));\n }\n \n TEST_P(SpmdPartitioningTest, ScatterParallelDimPartialReplicatedOperand) {\n@@ -12728,8 +12590,7 @@ ENTRY %module {\n   auto update = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Parameter());\n   auto scatter =\n       AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Scatter(operand, indices, update));\n-  EXPECT_THAT(root,\n-              op::AllReduce(op::DynamicUpdateSlice(_, scatter, _, _, _, _)));\n+  EXPECT_THAT(root, op::AllGather(scatter));\n }\n \n TEST_P(SpmdPartitioningTest, ScatterParallelDimPartialReplicatedUpdate) {\n@@ -12773,8 +12634,7 @@ ENTRY %module {\n   auto update = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::DynamicSlice());\n   auto scatter =\n       AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Scatter(operand, indices, update));\n-  EXPECT_THAT(root,\n-              op::AllReduce(op::DynamicUpdateSlice(_, scatter, _, _, _, _)));\n+  EXPECT_THAT(root, op::AllGather(scatter));\n }\n \n TEST_P(SpmdPartitioningTest, ScatterParallelDimSwappedDimensions) {\n@@ -12818,8 +12678,7 @@ ENTRY %module {\n   auto update = AllOf(op::Shape(\"s32[4,1,2,2]\"), op::CollectivePermute());\n   auto scatter =\n       AllOf(op::Shape(\"s32[4,1,2,2]\"), op::Scatter(operand, indices, update));\n-  EXPECT_THAT(root, op::AllReduce(op::AllReduce(\n-                        op::DynamicUpdateSlice(_, scatter, _, _, _, _))));\n+  EXPECT_THAT(root, op::AllGather(op::AllGather(scatter)));\n }\n \n TEST_P(SpmdPartitioningTest, ScatterParallelDimFromOutsideWhilePositive) {\n@@ -12897,10 +12756,7 @@ ENTRY entry {\n   auto update = AllOf(op::Shape(\"s32[1,4,2,2]\"), op::DynamicSlice());\n   auto scatter =\n       AllOf(op::Shape(\"s32[1,4,2,2]\"), op::Scatter(operand, indices, update));\n-  EXPECT_THAT(\n-      root,\n-      op::Tuple(op::AllReduce(op::DynamicUpdateSlice(_, scatter, _, _, _, _)),\n-                _, _, _));\n+  EXPECT_THAT(root, op::Tuple(op::AllGather(scatter), _, _, _));\n }\n \n TEST_P(SpmdPartitioningTest, ScatterParallelDimAndNonParallelDimPartitioned) {\n@@ -12948,9 +12804,8 @@ ENTRY %module {\n   auto update = AllOf(op::Shape(\"s32[4,2,2,2]\"));\n   auto scatter =\n       AllOf(op::Shape(\"s32[4,4,2,2]\"), op::Scatter(operand, indices, update));\n-  EXPECT_THAT(root, op::AllReduce(op::AllReduce(op::DynamicUpdateSlice(\n-                        _, op::DynamicSlice(op::AllReduce(scatter), _, _, _, _),\n-                        _, _, _, _))));\n+  EXPECT_THAT(root, op::AllGather(op::AllGather(\n+                        op::DynamicSlice(op::AllReduce(scatter), _, _, _, _))));\n }\n \n TEST_P(SpmdPartitioningTest, b_356877097) {\n@@ -12980,7 +12835,7 @@ ENTRY main.22 {\n   const auto root = module->entry_computation()->root_instruction();\n   auto operand = AllOf(op::Shape(\"f32[16,2]\"), op::Broadcast());\n   auto indices = AllOf(op::Shape(\"s32[8,2]\"), op::Subtract());\n-  auto update = AllOf(op::Shape(\"f32[8]\"), op::AllReduce());\n+  auto update = AllOf(op::Shape(\"f32[8]\"), op::AllGather());\n   EXPECT_THAT(root, AllOf(op::Shape(\"f32[16,2]\"),\n                           op::Scatter(operand, indices, update)));\n }\n@@ -13026,8 +12881,7 @@ ENTRY %module {\n   auto update = AllOf(op::Shape(\"s32[2,4,1,2]\"), op::DynamicSlice());\n   auto scatter =\n       AllOf(op::Shape(\"s32[2,4,1,2]\"), op::Scatter(operand, indices, update));\n-  EXPECT_THAT(root, op::AllReduce(op::AllReduce(\n-                        op::DynamicUpdateSlice(_, scatter, _, _, _, _))));\n+  EXPECT_THAT(root, op::AllGather(op::AllGather(scatter)));\n }\n \n TEST_P(SpmdPartitioningTest,\n@@ -13072,8 +12926,7 @@ ENTRY %module {\n   auto scatter =\n       AllOf(op::Shape(\"s32[2,2,2,2]\"), op::Scatter(operand, indices, update));\n   VLOG(1) << module->ToString();\n-  EXPECT_THAT(root, op::AllReduce(op::AllReduce(\n-                        op::DynamicUpdateSlice(_, scatter, _, _, _, _))));\n+  EXPECT_THAT(root, op::AllGather(op::AllGather(scatter)));\n }\n \n TEST_P(SpmdPartitioningTest, ScatterMergedIndexParallelAndIndexPassthrough) {\n@@ -13124,8 +12977,7 @@ ENTRY %module {\n     auto scatter =\n         AllOf(op::Shape(\"s32[2,4,2,2]\"), op::Scatter(operand, indices, update));\n     EXPECT_THAT(module->entry_computation()->root_instruction(),\n-                op::AllReduce(op::DynamicUpdateSlice(_, op::AllReduce(scatter),\n-                                                     _, _, _, _)));\n+                op::AllGather(op::AllReduce(scatter)));\n   }\n }\n \n@@ -13166,8 +13018,7 @@ ENTRY %module {\n   auto update = AllOf(op::Shape(\"s32[8,4,1,2]\"), op::DynamicSlice());\n   auto scatter =\n       AllOf(op::Shape(\"s32[4,2,1,2]\"), op::Scatter(operand, indices, update));\n-  EXPECT_THAT(root, op::AllReduce(op::AllReduce(op::AllReduce(\n-                        op::DynamicUpdateSlice(_, scatter, _, _, _, _)))));\n+  EXPECT_THAT(root, op::AllGather(op::AllGather(op::AllGather(scatter))));\n }\n \n TEST_P(SpmdPartitioningTest,\n@@ -13207,8 +13058,7 @@ ENTRY %module {\n   auto update = AllOf(op::Shape(\"s32[4,4,1,2]\"), op::DynamicSlice());\n   auto scatter =\n       AllOf(op::Shape(\"s32[8,4,1,2]\"), op::Scatter(operand, indices, update));\n-  EXPECT_THAT(root, op::AllReduce(op::DynamicUpdateSlice(\n-                        _, op::AllReduce(scatter), _, _, _, _)));\n+  EXPECT_THAT(root, op::AllGather(op::AllReduce(scatter)));\n }\n \n TEST_P(SpmdPartitioningTest,\n@@ -13239,8 +13089,10 @@ ENTRY %module {\n     scatter_dims_to_operand_dims={0,1},\n     index_vector_dim=0, sharding={replicated}\n })\";\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          PartitionComputation(hlo_string, /*num_devices=*/8));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module,\n+      PartitionComputation(hlo_string, /*num_devices=*/8,\n+                           SpmdPartitionerOptions(), /*use_all_gather=*/false));\n   VLOG(1) << module->ToString();\n   const auto root = module->entry_computation()->root_instruction();\n   auto operand = AllOf(op::Shape(\"s32[8,4,1,2]\"), op::Select());\n@@ -13289,8 +13141,7 @@ ENTRY %module {\n   auto update = AllOf(op::Shape(\"s32[4,4,2,2]\"), op::DynamicSlice());\n   auto scatter =\n       AllOf(op::Shape(\"s32[4,2,2,2]\"), op::Scatter(operand, indices, update));\n-  EXPECT_THAT(root, op::AllReduce(op::AllReduce(op::DynamicUpdateSlice(\n-                        _, op::AllReduce(scatter), _, _, _, _))));\n+  EXPECT_THAT(root, op::AllGather(op::AllGather(op::AllReduce(scatter))));\n }\n \n TEST_P(SpmdPartitioningTest,\n@@ -13330,8 +13181,7 @@ ENTRY %module {\n   auto update = AllOf(op::Shape(\"s32[4,2,2,2]\"), op::DynamicSlice());\n   auto scatter =\n       AllOf(op::Shape(\"s32[8,2,2,2]\"), op::Scatter(operand, indices, update));\n-  EXPECT_THAT(root, op::AllReduce(op::DynamicUpdateSlice(\n-                        _, op::AllReduce(op::AllReduce(scatter)), _, _, _, _)));\n+  EXPECT_THAT(root, op::AllGather(op::AllReduce(op::AllReduce(scatter))));\n }\n \n TEST_P(SpmdPartitioningTest, ScatterTrivialSlicedOperandPartial) {\n@@ -13359,13 +13209,13 @@ ENTRY main.4 {\n                           PartitionComputation(hlo_string, /*num_devices=*/8));\n   VLOG(1) << module->ToString();\n   const auto root = module->entry_computation()->root_instruction();\n-  auto operand = AllOf(op::Shape(\"s64[8,1]\"), op::AllReduce());\n+  auto operand = AllOf(op::Shape(\"s64[8,1]\"), op::AllGather());\n   auto indices = AllOf(op::Shape(\"s32[2]\"), op::Subtract());\n   auto update = AllOf(op::Shape(\"s64[2,1]\"), op::Parameter());\n   auto scatter =\n       AllOf(op::Shape(\"s64[8,1]\"), op::Scatter(operand, indices, update));\n-  EXPECT_THAT(root, op::AllReduce(op::AllReduce(op::DynamicUpdateSlice(\n-                        _, op::DynamicSlice(scatter, _, _), _, _))));\n+  EXPECT_THAT(root,\n+              op::AllGather(op::AllGather(op::DynamicSlice(scatter, _, _))));\n }\n \n // Tests for scatter partitioning methods with SPMD config option.\n@@ -14122,10 +13972,8 @@ ENTRY entry {\n   auto param0 = AllOf(op::Parameter(0), op::Shape(\"f32[1,1]\"));\n   auto broadcast =\n       AllOf(op::AllReduce(op::Select(_, param0, _)), op::Shape(\"f32[1,1]\"));\n-  EXPECT_THAT(\n-      root,\n-      AllOf(op::Copy(op::AllReduce(op::DynamicUpdateSlice(_, broadcast, _, _))),\n-            op::Shape(\"f32[1,2]\")));\n+  EXPECT_THAT(root,\n+              AllOf(op::Copy(op::AllGather(broadcast)), op::Shape(\"f32[1,2]\")));\n }\n \n TEST_P(SpmdPartitioningTest, BroadcastAsReplicate3) {\n@@ -14336,12 +14184,10 @@ ENTRY entry {\n   VLOG(1) << module->ToString();\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(\n-      root, op::Copy(op::AllReduce(op::DynamicUpdateSlice(\n-                _,\n-                op::CollectivePermute(op::AllReduce(op::Scatter(\n-                    op::Shape(\"bf16[50048,1020]\"), op::Shape(\"s32[512,1024,1]\"),\n-                    op::Shape(\"bf16[512,1024,1020]\")))),\n-                _, _))));\n+      root,\n+      op::Copy(op::AllGather(op::CollectivePermute(op::AllReduce(op::Scatter(\n+          op::Shape(\"bf16[50048,1020]\"), op::Shape(\"s32[512,1024,1]\"),\n+          op::Shape(\"bf16[512,1024,1020]\")))))));\n }\n \n TEST_P(SpmdPartitioningTest, ScatterPreferTrivialIfSmallerThanIndices) {\n@@ -14378,12 +14224,9 @@ ENTRY entry {\n   VLOG(1) << module->ToString();\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root,\n-              op::Copy(op::AllReduce(op::DynamicUpdateSlice(\n-                  _,\n-                  op::AllReduce(op::Scatter(op::Shape(\"bf16[32,128,50001]\"),\n-                                            op::Shape(\"s32[32,256,3]\"),\n-                                            op::Shape(\"bf16[32,256]\"))),\n-                  _, _, _))));\n+              op::Copy(op::AllGather(op::AllReduce(op::Scatter(\n+                  op::Shape(\"bf16[32,128,50001]\"), op::Shape(\"s32[32,256,3]\"),\n+                  op::Shape(\"bf16[32,256]\"))))));\n }\n \n TEST_P(SpmdPartitioningTest, GatherOperandPassthroughIndexPassthrough) {\n@@ -14716,13 +14559,15 @@ ENTRY entry {\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           PartitionComputation(hlo_string, /*num_devices=*/16));\n-  VLOG(1) << module->ToString();\n-  auto dus =\n-      AllOf(op::Shape(\"f32[3,2]\"),\n-            op::DynamicUpdateSlice(op::Broadcast(),\n-                                   op::Select(_, op::Parameter(0), _), _, _));\n+\n+  auto param0 = AllOf(op::Parameter(0), op::Shape(\"f32[1,1]\"));\n+  auto all_gather_dim_1 = AllOf(op::AllGather(param0), op::Shape(\"f32[1,2]\"));\n+  auto all_reduce_dus_dim_0 =\n+      AllOf(op::AllReduce(op::DynamicUpdateSlice(\n+                _, op::Select(_, all_gather_dim_1, _), _, _)),\n+            op::Shape(\"f32[3,2]\"));\n   EXPECT_THAT(module->entry_computation()->root_instruction(),\n-              op::Copy(AllOf(op::AllReduce(op::AllReduce(dus)))));\n+              op::Copy(all_reduce_dus_dim_0));\n }\n \n TEST_P(SpmdPartitioningTest, GatherPassthrough) {\n@@ -14972,10 +14817,8 @@ ENTRY %main.21 (Arg_0.1: f32[4,4,8], Arg_1.2: f32[4,8]) -> (f32[4,4,8], f32[4])\n \n   XLA_VLOG_LINES(1, module->ToString());\n   EXPECT_THAT(module->entry_computation()->root_instruction(),\n-              op::Tuple(op::AllReduce(op::DynamicUpdateSlice(\n-                            _, op::Shape(\"f32[1,4,8]\"), _, _, _)),\n-                        op::AllReduce(op::DynamicUpdateSlice(\n-                            _, op::Shape(\"f32[1]\"), _))));\n+              op::Tuple(op::AllGather(op::Shape(\"f32[1,4,8]\")),\n+                        op::AllGather(op::Shape(\"f32[1]\"))));\n }\n \n TEST_P(SpmdPartitioningTest, UnevenPadAllToAllReshard) {\n@@ -15387,7 +15230,7 @@ ENTRY main {\n                           PartitionComputation(hlo_string, /*num_devices=*/8));\n \n   const HloComputation* entry = module->entry_computation();\n-  EXPECT_EQ(NumOfInstructions(entry, HloOpcode::kAllReduce), 1);\n+  EXPECT_EQ(NumOfInstructions(entry, HloOpcode::kAllGather), 1);\n   EXPECT_EQ(NumOfInstructions(entry, HloOpcode::kAllToAll), 0);\n   EXPECT_EQ(NumOfInstructions(entry, HloOpcode::kCollectivePermute), 0);\n }\n@@ -15437,20 +15280,23 @@ TEST_P(SpmdPartitioningTest, ReshardNoFullRematCompatible) {\n HloModule Test\n \n ENTRY main.6 {\n-  Arg_0.1 = f32[6,32,4] parameter(0), sharding={devices=[4,2,1]0,2,1,3,4,6,5,7}\n-  ROOT copy = copy(Arg_0.1), sharding={devices=[2,2,2]<=[8]}\n+\n+  p0 = f32[6,32,4] parameter(0), sharding={devices=[4,2,1]<=[2,2,2]T(0,2,1)}\n+  ROOT copy = copy(p0), sharding={devices=[2,2,2]<=[8]}\n })\";\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           PartitionComputation(hlo_string, /*num_devices=*/8));\n \n-  XLA_VLOG_LINES(1, module->ToString());\n-  auto* allreduce = FindInstruction(module.get(), HloOpcode::kAllReduce);\n-  EXPECT_NE(allreduce, nullptr);\n-  // It should not touch the middle dim in the [2,2,2] sharding.\n-  EXPECT_EQ(allreduce->replica_groups().size(), 2);\n-  EXPECT_EQ(FindInstruction(module.get(), HloOpcode::kCollectivePermute),\n-            nullptr);\n+  auto param0 = AllOf(op::Parameter(0), op::Shape(\"f32[2,16,4]\"));\n+  auto all_gather =\n+      AllOf(op::AllGather(op::Copy(param0)), op::Shape(\"f32[8,16,4]\"));\n+  auto remove_padding = AllOf(op::Slice(all_gather), op::Shape(\"f32[6,16,4]\"));\n+\n+  auto dynamic_slice = AllOf(op::DynamicSlice(remove_padding, _, _, _),\n+                             op::Shape(\"f32[3,16,2]\"));\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              op::Copy(dynamic_slice));\n }\n \n TEST_P(SpmdPartitioningTest, ReshardNoFullRematIncompatible) {\n@@ -15465,14 +15311,15 @@ ENTRY main.6 {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           PartitionComputation(hlo_string, /*num_devices=*/8));\n \n-  XLA_VLOG_LINES(1, module->ToString());\n-  auto* allreduce = FindInstruction(module.get(), HloOpcode::kAllReduce);\n-  EXPECT_NE(allreduce, nullptr);\n-  // It should not touch the middle dim in the [2,2,2] sharding.\n-  EXPECT_EQ(allreduce->replica_groups().size(), 2);\n-  // Collective permute to resolve different device orders.\n-  EXPECT_NE(FindInstruction(module.get(), HloOpcode::kCollectivePermute),\n-            nullptr);\n+  auto param0 = AllOf(op::Parameter(0), op::Shape(\"f32[2,16,4]\"));\n+  auto all_gather =\n+      AllOf(op::AllGather(op::Copy(param0)), op::Shape(\"f32[8,16,4]\"));\n+  auto remove_padding = AllOf(op::Slice(all_gather), op::Shape(\"f32[6,16,4]\"));\n+\n+  auto dynamic_slice = AllOf(op::DynamicSlice(remove_padding, _, _, _),\n+                             op::Shape(\"f32[3,16,2]\"));\n+  EXPECT_THAT(module->entry_computation()->root_instruction(),\n+              op::Copy(op::CollectivePermute(dynamic_slice)));\n }\n \n TEST_P(SpmdPartitioningTest, OutfeedChainedManualPartitioned) {\n@@ -15748,11 +15595,11 @@ ENTRY %entry {\n       AllOf(op::CollectivePermute(), op::Shape(\"bf16[8,2048,1,5120]\"));\n   const auto broadcast =\n       AllOf(op::Broadcast(), op::Shape(\"bf16[8,2048,16384]\"));\n-  const auto all_reduce =\n-      AllOf(op::AllReduce(), op::Shape(\"bf16[20480,16384]\"));\n+  const auto all_gather =\n+      AllOf(op::AllGather(), op::Shape(\"bf16[20480,16384]\"));\n   const auto root = module->entry_computation()->root_instruction();\n   EXPECT_THAT(root, AllOf(op::GetTupleElement(op::While(op::Tuple(\n-                              op::Reshape(), all_reduce, op::Broadcast(),\n+                              op::Reshape(), all_gather, op::Broadcast(),\n                               collective_permute, op::Constant()))),\n                           op::Shape(\"bf16[8,2048,16384]\")));\n }\n@@ -16101,9 +15948,7 @@ ENTRY entry {\n                           PartitionComputation(hlo_string, /*num_devices=*/2));\n \n   auto param0 = AllOf(op::Parameter(0), op::Shape(\"s32[2]\"));\n-  auto param0_replicated = AllOf(op::AllReduce(op::DynamicUpdateSlice(\n-                                     op::Broadcast(op::Constant()), param0, _)),\n-                                 op::Shape(\"s32[4]\"));\n+  auto param0_replicated = AllOf(op::AllGather(param0), op::Shape(\"s32[4]\"));\n   auto result =\n       AllOf(op::BitcastConvert(param0_replicated), op::Shape(\"f32[4]\"));\n   EXPECT_THAT(module->entry_computation()->root_instruction(), result);\n@@ -16122,9 +15967,7 @@ ENTRY entry {\n                           PartitionComputation(hlo_string, /*num_devices=*/4));\n \n   auto param0 = AllOf(op::Parameter(0), op::Shape(\"s32[2,1]\"));\n-  auto param0_reshard = AllOf(op::AllReduce(op::DynamicUpdateSlice(\n-                                  op::Broadcast(op::Constant()), param0, _, _)),\n-                              op::Shape(\"s32[2,2]\"));\n+  auto param0_reshard = AllOf(op::AllGather(param0), op::Shape(\"s32[2,2]\"));\n   auto result = AllOf(op::BitcastConvert(param0_reshard), op::Shape(\"f64[2]\"));\n   EXPECT_THAT(module->entry_computation()->root_instruction(),\n               op::CollectivePermute(result));\n@@ -16165,16 +16008,14 @@ ENTRY entry {\n   // {devices=[2,2,2]<=[8]} to {devices=[8,1,1]<=[8]}.\n   auto param0 = AllOf(op::Parameter(0), op::Shape(\"f32[16,16,16]\"));\n   auto param0_replicated =\n-      AllOf(op::AllReduce(op::AllReduce(op::AllReduce(\n-                op::DynamicUpdateSlice(op::Broadcast(), param0, _, _, _)))),\n+      AllOf(op::AllGather(op::AllGather(op::AllGather(param0))),\n             op::Shape(\"f32[32,32,32]\"));\n   auto param0_reshard = AllOf(op::Shape(\"f32[4,32,32]\"),\n                               op::DynamicSlice(param0_replicated, _, _, _));\n   auto cholesky =\n       AllOf(op::Cholesky(param0_reshard), op::Shape(\"f32[4,32,32]\"));\n   auto cholesky_partially_replicated =\n-      AllOf(op::AllReduce(op::DynamicUpdateSlice(\n-                op::Broadcast(), op::Copy(op::Reshape(cholesky)), _, _, _, _)),\n+      AllOf(op::AllGather(op::Copy(op::Reshape(cholesky))),\n             op::Shape(\"f32[1,16,32,32]\"));\n   EXPECT_THAT(module->entry_computation()->root_instruction(),\n               AllOf(op::Reshape(op::DynamicSlice(cholesky_partially_replicated,\n@@ -16195,14 +16036,10 @@ ENTRY main {\n \n   auto param0 = AllOf(op::Parameter(0), op::Shape(\"f32[5,16,16]\"));\n   auto param0_reshard =\n-      AllOf(op::Shape(\"f32[5,32,32]\"),\n-            op::AllReduce(op::AllReduce(\n-                op::DynamicUpdateSlice(op::Broadcast(), param0, _, _, _))));\n+      AllOf(op::Shape(\"f32[5,32,32]\"), op::AllGather(op::AllGather(param0)));\n   auto param1 = AllOf(op::Parameter(1), op::Shape(\"f32[5,16,24]\"));\n   auto param1_reshard =\n-      AllOf(op::Shape(\"f32[5,32,48]\"),\n-            op::AllReduce(op::AllReduce(\n-                op::DynamicUpdateSlice(op::Broadcast(), param1, _, _, _))));\n+      AllOf(op::Shape(\"f32[5,32,48]\"), op::AllGather(op::AllGather(param1)));\n \n   auto ts = AllOf(op::TriangularSolve(param0_reshard, param1_reshard),\n                   op::Shape(\"f32[5,32,48]\"));\n@@ -16388,8 +16225,10 @@ ENTRY entry {\n   add = s32[2,4]{1,0} add(constant, a), sharding={unreduced}\n   ROOT copy = s32[2,4]{1,0} copy(%add), sharding={unreduced}\n })\";\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          PartitionComputation(hlo_string, /*num_devices=*/2));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module,\n+      PartitionComputation(hlo_string, /*num_devices=*/2,\n+                           SpmdPartitionerOptions(), /*use_all_gather=*/false));\n   VLOG(1) << module->ToString();\n   // Check that we use all-reduce to reshard the operands of the add in spite\n   // that the `add` has unreduced axes.\n@@ -16475,9 +16314,7 @@ ENTRY entry {\n })\";\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          PartitionComputation(hlo_string, /*num_devices=*/16,\n-                                               SpmdPartitionerOptions(),\n-                                               /*use_all_gather=*/true));\n+                          PartitionComputation(hlo_string, /*num_devices=*/16));\n   const HloComputation* recovery_computation =\n       module->original_value_recovery_table()\n           .begin()\n@@ -16538,9 +16375,7 @@ ENTRY entry {\n })\";\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          PartitionComputation(hlo_string, /*num_devices=*/16,\n-                                               SpmdPartitionerOptions(),\n-                                               /*use_all_gather=*/true));\n+                          PartitionComputation(hlo_string, /*num_devices=*/16));\n   const HloComputation* recovery_computation =\n       module->original_value_recovery_table()\n           .begin()"
        }
    ],
    "stats": {
        "total": 631,
        "additions": 233,
        "deletions": 398
    }
}