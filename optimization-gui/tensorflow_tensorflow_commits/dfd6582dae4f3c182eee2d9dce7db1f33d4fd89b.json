{
    "author": "tensorflower-gardener",
    "message": "[Autotuner] Use Legacy cache in pass.\n\n- Refactored the autotuner to make the cache usage more clear.\n\nPiperOrigin-RevId: 804860868",
    "sha": "dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b",
    "files": [
        {
            "sha": "bb064e1a4694a594d6dce211c10848f6961927cb",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner.cc",
            "status": "modified",
            "additions": 68,
            "deletions": 42,
            "changes": 110,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.cc?ref=dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b",
            "patch": "@@ -67,30 +67,57 @@ absl::StatusOr<std::unique_ptr<Autotuner>> Autotuner::Create(\n                     std::move(autotune_config), std::move(cache), thread_pool));\n }\n \n+absl::Status Autotuner::Autotune(HloModule* module,\n+                                 const InstructionFilterFn& should_autotune) {\n+  InstructionsByFingerprint instrunctions_by_fingerprint =\n+      GetAutotuningCandidates(module, should_autotune);\n+  if (instrunctions_by_fingerprint.empty()) {\n+    VLOG(1) << \"No instructions to autotune.\";\n+    return absl::OkStatus();\n+  }\n+\n+  VLOG(1) << \"Autotuning \" << instrunctions_by_fingerprint.size()\n+          << \" unique instructions.\";\n+  for (auto& [_, instructions] : instrunctions_by_fingerprint) {\n+    CHECK(!instructions.empty());\n+    VLOG(1) << \"Autotuning instruction:\" << instructions[0]->ToString();\n+    TF_ASSIGN_OR_RETURN(Config best_config,\n+                        GetCachedOrTuneBestConfig(instructions[0]));\n+    CodegenBackend* best_codegen_backend = best_config.codegen_backend;\n+    for (auto* instr : instructions) {\n+      TF_RETURN_IF_ERROR(best_codegen_backend->ApplyConfig(\n+          *instr, *best_config.backend_config));\n+    }\n+  }\n+  return absl::OkStatus();\n+}\n+\n absl::Status Autotuner::Autotune(HloInstruction* instr) {\n   VLOG(1) << \"Autotuning HLO: \" << instr->ToString();\n-  TF_ASSIGN_OR_RETURN(auto best_config, GetBestConfig(instr));\n+  TF_ASSIGN_OR_RETURN(Config best_config, GetCachedOrTuneBestConfig(instr));\n   CodegenBackend* best_codegen_backend = best_config.codegen_backend;\n   return best_codegen_backend->ApplyConfig(*instr, *best_config.backend_config);\n }\n \n-absl::StatusOr<Autotuner::Config> Autotuner::GetBestConfig(\n+absl::StatusOr<Autotuner::Config> Autotuner::GetCachedOrTuneBestConfig(\n     HloInstruction* instr) {\n-  if (cache_) {\n-    auto cached_config = cache_->Lookup(instr);\n-    if (cached_config.has_value()) {\n-      VLOG(1) << \"Found cached config for HLO: \" << instr->ToString();\n-      for (auto& codegen_backend : codegen_backends_) {\n-        if (codegen_backend->name() == cached_config->codegen_backend_name) {\n-          auto backend_config = std::make_unique<google::protobuf::Any>(\n-              cached_config->backend_config);\n-          return Config{codegen_backend.get(), std::move(backend_config)};\n-        }\n-      }\n-      return absl::InternalError(\"Cached backend not found!\");\n+  std::optional<Config> cached_config = LookUp(instr);\n+  Config best_config;\n+  if (cached_config.has_value()) {\n+    best_config = std::move(*cached_config);\n+  } else {\n+    if (autotune_config_.expect_all_instructions_in_cache) {\n+      return absl::InternalError(\"No cached config found for HLO instr: \" +\n+                                 instr->ToString());\n     }\n+    TF_ASSIGN_OR_RETURN(best_config, TuneBestConfig(instr));\n+    Insert(instr, best_config);\n   }\n+  return best_config;\n+}\n \n+absl::StatusOr<Autotuner::Config> Autotuner::TuneBestConfig(\n+    HloInstruction* instr) {\n   TF_ASSIGN_OR_RETURN(std::vector<Config> supported_configs,\n                       GetSupportedConfigs(instr));\n   if (supported_configs.empty()) {\n@@ -114,7 +141,7 @@ absl::StatusOr<Autotuner::Config> Autotuner::GetBestConfig(\n   VLOG(1) << \"Successfully compiled \" << executable_candidates.size()\n           << \" configs out of \" << supported_configs.size() << \" configs.\";\n \n-  return ProfileAndPickBest(instr, executable_candidates);\n+  return ProfileAndPickBest(executable_candidates);\n }\n \n Autotuner::InstructionsByFingerprint Autotuner::GetAutotuningCandidates(\n@@ -130,28 +157,34 @@ Autotuner::InstructionsByFingerprint Autotuner::GetAutotuningCandidates(\n   return instrunctions_by_fingerprint;\n }\n \n-absl::Status Autotuner::Autotune(HloModule* module,\n-                                 const InstructionFilterFn& should_autotune) {\n-  InstructionsByFingerprint instrunctions_by_fingerprint =\n-      GetAutotuningCandidates(module, should_autotune);\n-  if (instrunctions_by_fingerprint.empty()) {\n-    VLOG(1) << \"No instructions to autotune.\";\n-    return absl::OkStatus();\n+std::optional<Autotuner::Config> Autotuner::LookUp(\n+    const HloInstruction* instr) {\n+  if (cache_) {\n+    auto cached_config = cache_->Lookup(instr);\n+    if (cached_config.has_value()) {\n+      VLOG(1) << \"Found cached config for HLO: \" << instr->ToString();\n+      for (auto& codegen_backend : codegen_backends_) {\n+        if (codegen_backend->name() == cached_config->codegen_backend_name) {\n+          auto backend_config = std::make_unique<google::protobuf::Any>(\n+              cached_config->backend_config);\n+          return Config{codegen_backend.get(), std::move(backend_config)};\n+        }\n+      }\n+      LOG(WARNING) << \"Cached config for HLO: \" << instr->ToString()\n+                   << \" has unsupported backend \"\n+                   << cached_config->codegen_backend_name;\n+    }\n   }\n+  return std::nullopt;\n+}\n \n-  VLOG(1) << \"Autotuning \" << instrunctions_by_fingerprint.size()\n-          << \" unique instructions.\";\n-  for (auto& [_, instructions] : instrunctions_by_fingerprint) {\n-    CHECK(!instructions.empty());\n-    VLOG(1) << \"Autotuning instruction:\" << instructions[0]->ToString();\n-    TF_ASSIGN_OR_RETURN(Config best_config, GetBestConfig(instructions[0]));\n-    CodegenBackend* best_codegen_backend = best_config.codegen_backend;\n-    for (auto* instr : instructions) {\n-      TF_RETURN_IF_ERROR(best_codegen_backend->ApplyConfig(\n-          *instr, *best_config.backend_config));\n-    }\n+void Autotuner::Insert(const HloInstruction* instr, Autotuner::Config& config) {\n+  if (cache_) {\n+    AutotunerCacheInterface::Config cached_config;\n+    cached_config.codegen_backend_name = config.codegen_backend->name();\n+    cached_config.backend_config = *config.backend_config;\n+    CHECK_OK(cache_->Insert(instr, cached_config));\n   }\n-  return absl::OkStatus();\n }\n \n absl::StatusOr<std::vector<Autotuner::Config>> Autotuner::GetSupportedConfigs(\n@@ -196,7 +229,7 @@ std::vector<absl::StatusOr<std::unique_ptr<Executable>>> Autotuner::CompileAll(\n }\n \n absl::StatusOr<Autotuner::Config> Autotuner::ProfileAndPickBest(\n-    HloInstruction* instr, std::vector<ExecutableCandidate>& candidates) {\n+    std::vector<ExecutableCandidate>& candidates) {\n   if (candidates.empty()) {\n     return absl::InternalError(\"No executables to profile!\");\n   }\n@@ -277,13 +310,6 @@ absl::StatusOr<Autotuner::Config> Autotuner::ProfileAndPickBest(\n \n   VLOG(1) << \"Picked config: \" << best_config->codegen_backend->name() << \" \"\n           << best_config->backend_config->ShortDebugString();\n-\n-  AutotunerCacheInterface::Config config;\n-  config.codegen_backend_name = (best_config->codegen_backend->name());\n-  config.backend_config = *best_config->backend_config;\n-  if (cache_) {\n-    TF_RETURN_IF_ERROR(cache_->Insert(instr, config));\n-  }\n   return std::move(*best_config);\n }\n "
        },
        {
            "sha": "55f92edc2ef2511e12b91897079915596913eb6c",
            "filename": "third_party/xla/xla/backends/autotuner/autotuner.h",
            "status": "modified",
            "additions": 21,
            "deletions": 4,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fautotuner%2Fautotuner.h?ref=dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n #define XLA_BACKENDS_AUTOTUNER_AUTOTUNER_H_\n \n #include <memory>\n+#include <optional>\n #include <utility>\n #include <vector>\n \n@@ -55,6 +56,9 @@ struct AutotuneConfig {\n   bool optimize_scratch_bytes = false;\n   // Window size in microseconds to consider for scratch bytes optimization.\n   int scratch_bytes_window_size_us = 4;\n+  // If true, the autotuner will return an error if the best config for a\n+  // certain instruction is not in the cache.\n+  bool expect_all_instructions_in_cache = false;\n };\n \n class Autotuner {\n@@ -65,8 +69,10 @@ class Autotuner {\n       std::unique_ptr<AutotunerCacheInterface> cache,\n       tsl::thread::ThreadPool* thread_pool = nullptr);\n \n-  // Try all supported configs from the registered codegen backends for the\n-  // given HLO instruction and apply the best one.\n+  // Autotune the given HLO instruction. If a cache is provided, the cached\n+  // config will be used if the instruction is in the cache. Otherwise, the\n+  // autotuner will try all supported configs from the registered codegen\n+  // backends for the given HLO instruction and apply the best one.\n   absl::Status Autotune(HloInstruction* instr);\n \n   // Autotune all instructions in the module for which the filter function\n@@ -102,15 +108,26 @@ class Autotuner {\n   InstructionsByFingerprint GetAutotuningCandidates(\n       const HloModule* module, const InstructionFilterFn& should_autotune);\n \n-  absl::StatusOr<Config> GetBestConfig(HloInstruction* instr);\n+  // Gets the best config for the given instruction either from cache or by\n+  // tuning all supported configs if the instruction is not in the cache.\n+  absl::StatusOr<Config> GetCachedOrTuneBestConfig(HloInstruction* instr);\n+  // Gets the best config for the given instruction by compiling and profiling\n+  // all supported configs.\n+  absl::StatusOr<Config> TuneBestConfig(HloInstruction* instr);\n+\n+  // TODO: b/407494653 - Directly use cache api when the configs are unified.\n+  // Translates from Autotuner::Config to AutotunerCacheInterface::Config and\n+  // the other way around.\n+  std::optional<Autotuner::Config> LookUp(const HloInstruction* instr);\n+  void Insert(const HloInstruction* instr, Autotuner::Config& config);\n \n   absl::StatusOr<std::vector<Config>> GetSupportedConfigs(\n       HloInstruction* instr);\n   std::vector<absl::StatusOr<std::unique_ptr<Executable>>> CompileAll(\n       HloInstruction* instr, std::vector<Config>& configs);\n \n   absl::StatusOr<Config> ProfileAndPickBest(\n-      HloInstruction* instr, std::vector<ExecutableCandidate>& candidates);\n+      std::vector<ExecutableCandidate>& candidates);\n \n   absl::StatusOr<ScopedShapedBuffer> GetReferenceOutput(\n       std::vector<ExecutableCandidate>& candidates,"
        },
        {
            "sha": "d39b48d75fd874ef40c1003c45aa90e437aa475a",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b",
            "patch": "@@ -777,9 +777,11 @@ xla_cc_binary(\n     deps = [\n         \":factory\",\n         \":gpu_profiler\",\n+        \":legacy_cache\",\n         \"//xla:debug_options_flags\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/backends/autotuner\",\n+        \"//xla/backends/autotuner:autotuner_cache_interface\",\n         \"//xla/backends/autotuner:codegen_backend\",\n         \"//xla/backends/autotuner:file_based_autotuner_cache\",\n         \"//xla/backends/autotuner:profiler\","
        },
        {
            "sha": "17ce897671abff8d0abd990df87149a06ff3d761",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/autotuner_main.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 20,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fautotuner_main.cc?ref=dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b",
            "patch": "@@ -28,11 +28,12 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/backends/autotuner/autotuner.h\"\n+#include \"xla/backends/autotuner/autotuner_cache_interface.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n-#include \"xla/backends/autotuner/file_based_autotuner_cache.h\"\n #include \"xla/backends/autotuner/profiler.h\"\n #include \"xla/backends/gpu/autotuner/factory.h\"\n #include \"xla/backends/gpu/autotuner/gpu_profiler.h\"\n+#include \"xla/backends/gpu/autotuner/legacy_cache.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -63,8 +64,8 @@ autotuned module to stdout.\n Usage:\n \n   bazel run autotuner_main -- --hlo_file=path/to/hlo_module \\\n-    [--autotune_cache_dir=path/to/cache_dir] \\\n-    [--autotune_cache_mode=READ|WRITE|READ_WRITE]\n+    [--cache_dir=path/to/cache_dir] \\\n+    [--autotune_cache_mode=READ|READ_WRITE]\n )\";\n }  // namespace\n \n@@ -80,7 +81,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> GetModule(\n   return ParseAndReturnUnverifiedModule(hlo_text);\n }\n \n-absl::Status Autotune(HloModule& module, const std::string& autotune_cache_dir,\n+absl::Status Autotune(HloModule& module, const std::string& cache_dir,\n                       const std::string& autotune_cache_mode_str) {\n   TF_ASSIGN_OR_RETURN(std::string platform_name,\n                       PlatformUtil::CanonicalPlatformName(\"gpu\"));\n@@ -115,25 +116,22 @@ absl::Status Autotune(HloModule& module, const std::string& autotune_cache_dir,\n   tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"autotuner\",\n                                       tsl::port::MaxParallelism());\n \n-  FileBasedCacheConfig cache_config;\n-  cache_config.autotune_cache_dir = autotune_cache_dir;\n-\n-  const absl::flat_hash_map<std::string, FileBasedCacheConfig::CacheMode>\n+  const absl::flat_hash_map<std::string, DebugOptions::AutotuneCacheMode>\n       mode_map = {\n-          {\"READ\", FileBasedCacheConfig::CacheMode::READ},\n-          {\"WRITE\", FileBasedCacheConfig::CacheMode::WRITE},\n-          {\"READ_WRITE\", FileBasedCacheConfig::CacheMode::READ_WRITE},\n+          {\"READ_WRITE\", DebugOptions::AUTOTUNE_CACHE_MODE_UPDATE},\n+          {\"READ\", DebugOptions::AUTOTUNE_CACHE_MODE_READ},\n       };\n   auto it = mode_map.find(autotune_cache_mode_str);\n   if (it == mode_map.end()) {\n     return absl::InvalidArgumentError(\n         absl::StrCat(\"Invalid autotune_cache_mode: \", autotune_cache_mode_str));\n   }\n-  cache_config.autotune_cache_mode = it->second;\n-  cache_config.device_desc = stream_executor->GetDeviceDescription();\n \n-  TF_ASSIGN_OR_RETURN(auto cache,\n-                      FileBasedAutotunerCache::Create(cache_config));\n+  std::unique_ptr<AutotunerCacheInterface> cache;\n+  if (!cache_dir.empty()) {\n+    cache = std::make_unique<LegacyCache>(\n+        cache_dir, it->second, stream_executor->GetDeviceDescription());\n+  }\n \n   AutotuneConfig autotune_config;\n   TF_ASSIGN_OR_RETURN(\n@@ -160,14 +158,14 @@ absl::Status Autotune(HloModule& module, const std::string& autotune_cache_dir,\n \n int main(int argc, char* argv[]) {\n   std::string hlo_file;\n-  std::string autotune_cache_dir;\n+  std::string cache_dir;\n   std::string autotune_cache_mode = \"READ_WRITE\";\n   std::vector<tsl::Flag> flag_list = {\n       tsl::Flag(\"hlo_file\", &hlo_file, \"Path to the HLO file to autotune.\"),\n-      tsl::Flag(\"autotune_cache_dir\", &autotune_cache_dir,\n+      tsl::Flag(\"cache_dir\", &cache_dir,\n                 \"Directory to store/load the autotune cache.\"),\n       tsl::Flag(\"autotune_cache_mode\", &autotune_cache_mode,\n-                \"Autotune cache mode: READ, WRITE, or READ_WRITE.\")};\n+                \"Autotune cache mode: READ or READ_WRITE.\")};\n \n   const std::string usage_string =\n       absl::StrCat(kUsage, \"\\n\\n\", tsl::Flags::Usage(argv[0], flag_list));\n@@ -178,8 +176,7 @@ int main(int argc, char* argv[]) {\n   tsl::port::InitMain(usage_string.c_str(), &argc, &argv);\n   auto module = xla::gpu::GetModule(hlo_file);\n   CHECK_OK(module.status());\n-  CHECK_OK(xla::gpu::Autotune(*module.value(), autotune_cache_dir,\n-                              autotune_cache_mode));\n+  CHECK_OK(xla::gpu::Autotune(*module.value(), cache_dir, autotune_cache_mode));\n   std::cout << module.value()->ToString() << std::endl;\n   return 0;\n }"
        },
        {
            "sha": "7b19376a3fd98e9d2c3eafcdff6868cbcfb75742",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b",
            "patch": "@@ -760,9 +760,9 @@ cc_library(\n         \"//xla/backends/autotuner\",\n         \"//xla/backends/autotuner:autotuner_cache_interface\",\n         \"//xla/backends/autotuner:codegen_backend\",\n-        \"//xla/backends/autotuner:file_based_autotuner_cache\",\n         \"//xla/backends/autotuner:profiler\",\n         \"//xla/backends/gpu/autotuner:gpu_profiler\",\n+        \"//xla/backends/gpu/autotuner:legacy_cache\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/stream_executor:device_memory_allocator\","
        },
        {
            "sha": "a8dcba97be0e6132b2b34ac56400ebe3a4ab085f",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_pass.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 21,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_pass.cc?ref=dfd6582dae4f3c182eee2d9dce7db1f33d4fd89b",
            "patch": "@@ -29,9 +29,9 @@ limitations under the License.\n #include \"xla/backends/autotuner/autotuner.h\"\n #include \"xla/backends/autotuner/autotuner_cache_interface.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n-#include \"xla/backends/autotuner/file_based_autotuner_cache.h\"\n #include \"xla/backends/autotuner/profiler.h\"\n #include \"xla/backends/gpu/autotuner/gpu_profiler.h\"\n+#include \"xla/backends/gpu/autotuner/legacy_cache.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n@@ -60,26 +60,9 @@ absl::StatusOr<std::unique_ptr<AutotunerPass>> AutotunerPass::Create(\n   const std::string& cache_dir =\n       debug_options.xla_gpu_experimental_autotuner_cache_dir();\n   if (!cache_dir.empty()) {\n-    FileBasedCacheConfig cache_config;\n-    cache_config.autotune_cache_dir = cache_dir;\n-    cache_config.device_desc = stream_executor->GetDeviceDescription();\n-    switch (debug_options.xla_gpu_experimental_autotune_cache_mode()) {\n-      case DebugOptions::AUTOTUNE_CACHE_MODE_READ:\n-        cache_config.autotune_cache_mode =\n-            FileBasedCacheConfig::CacheMode::READ;\n-        break;\n-      case DebugOptions::AUTOTUNE_CACHE_MODE_UPDATE:\n-        cache_config.autotune_cache_mode =\n-            FileBasedCacheConfig::CacheMode::READ_WRITE;\n-        break;\n-      default:\n-        // Includes AUTOTUNE_CACHE_MODE_UNSPECIFIED\n-        LOG(WARNING) << \"Unknown autotune cache mode, defaulting to READ_WRITE\";\n-        cache_config.autotune_cache_mode =\n-            FileBasedCacheConfig::CacheMode::READ_WRITE;\n-        break;\n-    }\n-    TF_ASSIGN_OR_RETURN(cache, FileBasedAutotunerCache::Create(cache_config));\n+    cache = std::make_unique<LegacyCache>(\n+        cache_dir, debug_options.xla_gpu_experimental_autotune_cache_mode(),\n+        stream_executor->GetDeviceDescription());\n   }\n \n   AutotuneConfig autotune_config;"
        }
    ],
    "stats": {
        "total": 201,
        "additions": 113,
        "deletions": 88
    }
}