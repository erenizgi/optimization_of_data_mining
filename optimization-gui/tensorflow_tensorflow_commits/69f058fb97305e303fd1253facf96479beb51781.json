{
    "author": "vwbaker",
    "message": "Disable autotuning for transpose-emitter test\n\nThis test is meant to test the native transpose emitter, not the triton\nemitter (which generates a different barrier), so we disable autotuning here. This is failing on a new triton integration which presumably makes triton faster and has the autotuner choose to go the triton patch rather than the native emitter path.\n\nPiperOrigin-RevId: 850632173",
    "sha": "69f058fb97305e303fd1253facf96479beb51781",
    "files": [
        {
            "sha": "ec3a4bde8563a5cc4130752f8724cbbb8bf1b692",
            "filename": "third_party/xla/xla/service/gpu/tests/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/69f058fb97305e303fd1253facf96479beb51781/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/69f058fb97305e303fd1253facf96479beb51781/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2FBUILD?ref=69f058fb97305e303fd1253facf96479beb51781",
            "patch": "@@ -483,7 +483,6 @@ xla_test(\n         \"//xla/tests:hlo_test_base\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_googletest//:gtest_main\",\n-        \"@local_tsl//tsl/platform:test\",\n     ],\n )\n "
        },
        {
            "sha": "740f36d7bc0aa806b4261e816e686417bd0b4bdf",
            "filename": "third_party/xla/xla/service/gpu/tests/gpu_kernel_tiling_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/69f058fb97305e303fd1253facf96479beb51781/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_kernel_tiling_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/69f058fb97305e303fd1253facf96479beb51781/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_kernel_tiling_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_kernel_tiling_test.cc?ref=69f058fb97305e303fd1253facf96479beb51781",
            "patch": "@@ -24,7 +24,6 @@ limitations under the License.\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/tests/hlo_test_base.h\"\n-#include \"tsl/platform/test.h\"\n \n namespace xla {\n namespace gpu {\n@@ -69,6 +68,11 @@ TEST_F(GpuKernelTilingTest, UnnestedTransposeWithProperDimensionsTiled) {\n   auto hlo_module =\n       ParseAndReturnVerifiedModule(kHloString, ConfigWithLayoutAssignment())\n           .value();\n+  // This test is meant to test the native transpose emitter, not the triton\n+  // emitter, so we disable autotuning.\n+  hlo_module->mutable_config()\n+      .mutable_debug_options()\n+      .set_xla_gpu_autotune_level(0);\n \n   auto expected_ir = R\"(\n ; CHECK: call void BARRIER()"
        }
    ],
    "stats": {
        "total": 7,
        "additions": 5,
        "deletions": 2
    }
}