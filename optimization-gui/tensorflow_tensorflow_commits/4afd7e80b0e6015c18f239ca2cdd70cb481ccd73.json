{
    "author": "GleasonK",
    "message": "Partial roll forward of the safe parts of direct HLO lowering\n\nPiperOrigin-RevId: 800940153",
    "sha": "4afd7e80b0e6015c18f239ca2cdd70cb481ccd73",
    "files": [
        {
            "sha": "1fcd9fdceec0cc8264124681813d1f589f7d47e2",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 1496,
            "deletions": 0,
            "changes": 1496,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4afd7e80b0e6015c18f239ca2cdd70cb481ccd73/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4afd7e80b0e6015c18f239ca2cdd70cb481ccd73/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=4afd7e80b0e6015c18f239ca2cdd70cb481ccd73",
            "patch": "@@ -4706,4 +4706,1500 @@ diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.c\n -\n -}  // namespace stablehlo\n -}  // namespace mlir\n+diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n+--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n++++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n+@@ -150,8 +150,8 @@\n+ ////////\n+ // CompareOp\n+ \n+-// CHECK-LABEL: func.func @compare_folds\n+-func.func @compare_folds()\n++// CHECK-LABEL: func.func @compare_fold_int\n++func.func @compare_fold_int()\n+   -> (tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>) {\n+   %cn1 = stablehlo.constant dense<-1> : tensor<i32>\n+   %c0 = stablehlo.constant dense<0> : tensor<i32>\n+@@ -176,6 +176,270 @@\n+          tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>\n+ }\n+ \n++// -----\n++\n++// CHECK-LABEL: func.func @compare_fold_float\n++func.func @compare_fold_float()\n++  -> (tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>) {\n++  %c0 = stablehlo.constant dense<0.0> : tensor<f32>\n++  %c1 = stablehlo.constant dense<0.01> : tensor<f32>\n++  %c2 = stablehlo.constant dense<-0.01> : tensor<f32>\n++  %c3 = stablehlo.constant dense<42.1> : tensor<f32>\n++  %c4 = stablehlo.constant dense<-50.0> : tensor<f32>\n++\n++  %0 = stablehlo.compare EQ, %c0, %c0, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %1 = stablehlo.compare EQ, %c1, %c2, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %2 = stablehlo.compare NE, %c0, %c0, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %3 = stablehlo.compare NE, %c1, %c2, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %4 = stablehlo.compare GT, %c3, %c3, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %5 = stablehlo.compare GT, %c3, %c4, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %6 = stablehlo.compare GE, %c3, %c3, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %7 = stablehlo.compare GE, %c3, %c4, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %8 = stablehlo.compare LT, %c2, %c2, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %9 = stablehlo.compare LT, %c2, %c4, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %10 = stablehlo.compare LE, %c2, %c2, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %11 = stablehlo.compare LE, %c2, %c4, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++\n++  // CHECK-DAG:  [[FALSE:%.+]] = stablehlo.constant dense<false> : tensor<i1>\n++  // CHECK-DAG:  [[TRUE:%.+]] = stablehlo.constant dense<true> : tensor<i1>\n++\n++  // CHECK-NEXT: return [[TRUE]], [[FALSE]], [[FALSE]], [[TRUE]], [[FALSE]], [[TRUE]], [[TRUE]], [[TRUE]], [[FALSE]], [[FALSE]], [[TRUE]], [[FALSE]]\n++  return %0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11 :\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>\n++}\n++\n++// -----\n++\n++// CHECK-LABEL: func.func @compare_fold_float_edge_cases\n++func.func @compare_fold_float_edge_cases()\n++  -> (tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>) {\n++  %zero = stablehlo.constant dense<0.0> : tensor<f32>\n++  %pos_inf = stablehlo.constant dense<0x7F800000> : tensor<f32>\n++  %neg_inf = stablehlo.constant dense<0xFF800000> : tensor<f32>\n++  %nan = stablehlo.constant dense<0x7FC00000> : tensor<f32>\n++\n++  // CHECK-DAG:  [[FALSE:%.+]] = stablehlo.constant dense<false> : tensor<i1>\n++  // CHECK-DAG:  [[TRUE:%.+]] = stablehlo.constant dense<true> : tensor<i1>\n++\n++  %0 = stablehlo.compare EQ, %zero, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %1 = stablehlo.compare EQ, %zero, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %2 = stablehlo.compare EQ, %zero, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %3 = stablehlo.compare EQ, %zero, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %4 = stablehlo.compare EQ, %pos_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %5 = stablehlo.compare EQ, %pos_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %6 = stablehlo.compare EQ, %pos_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %7 = stablehlo.compare EQ, %pos_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %8 = stablehlo.compare EQ, %neg_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %9 = stablehlo.compare EQ, %neg_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %10 = stablehlo.compare EQ, %neg_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %11 = stablehlo.compare EQ, %neg_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %12 = stablehlo.compare EQ, %nan, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %13 = stablehlo.compare EQ, %nan, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %14 = stablehlo.compare EQ, %nan, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %15 = stablehlo.compare EQ, %nan, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++\n++  %16 = stablehlo.compare NE, %zero, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %17 = stablehlo.compare NE, %zero, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %18 = stablehlo.compare NE, %zero, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %19 = stablehlo.compare NE, %zero, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %20 = stablehlo.compare NE, %pos_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %21 = stablehlo.compare NE, %pos_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %22 = stablehlo.compare NE, %pos_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %23 = stablehlo.compare NE, %pos_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %24 = stablehlo.compare NE, %neg_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %25 = stablehlo.compare NE, %neg_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %26 = stablehlo.compare NE, %neg_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %27 = stablehlo.compare NE, %neg_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %28 = stablehlo.compare NE, %nan, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %29 = stablehlo.compare NE, %nan, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %30 = stablehlo.compare NE, %nan, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %31 = stablehlo.compare NE, %nan, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++\n++  %32 = stablehlo.compare GT, %zero, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %33 = stablehlo.compare GT, %zero, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %34 = stablehlo.compare GT, %zero, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %35 = stablehlo.compare GT, %zero, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %36 = stablehlo.compare GT, %pos_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %37 = stablehlo.compare GT, %pos_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %38 = stablehlo.compare GT, %pos_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %39 = stablehlo.compare GT, %pos_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %40 = stablehlo.compare GT, %neg_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %41 = stablehlo.compare GT, %neg_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %42 = stablehlo.compare GT, %neg_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %43 = stablehlo.compare GT, %neg_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %44 = stablehlo.compare GT, %nan, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %45 = stablehlo.compare GT, %nan, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %46 = stablehlo.compare GT, %nan, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %47 = stablehlo.compare GT, %nan, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++\n++  %48 = stablehlo.compare GE, %zero, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %49 = stablehlo.compare GE, %zero, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %50 = stablehlo.compare GE, %zero, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %51 = stablehlo.compare GE, %zero, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %52 = stablehlo.compare GE, %pos_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %53 = stablehlo.compare GE, %pos_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %54 = stablehlo.compare GE, %pos_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %55 = stablehlo.compare GE, %pos_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %56 = stablehlo.compare GE, %neg_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %57 = stablehlo.compare GE, %neg_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %58 = stablehlo.compare GE, %neg_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %59 = stablehlo.compare GE, %neg_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %60 = stablehlo.compare GE, %nan, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %61 = stablehlo.compare GE, %nan, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %62 = stablehlo.compare GE, %nan, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %63 = stablehlo.compare GE, %nan, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++\n++  %64 = stablehlo.compare LT, %zero, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %65 = stablehlo.compare LT, %zero, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %66 = stablehlo.compare LT, %zero, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %67 = stablehlo.compare LT, %zero, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %68 = stablehlo.compare LT, %pos_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %69 = stablehlo.compare LT, %pos_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %70 = stablehlo.compare LT, %pos_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %71 = stablehlo.compare LT, %pos_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %72 = stablehlo.compare LT, %neg_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %73 = stablehlo.compare LT, %neg_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %74 = stablehlo.compare LT, %neg_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %75 = stablehlo.compare LT, %neg_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %76 = stablehlo.compare LT, %nan, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %77 = stablehlo.compare LT, %nan, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %78 = stablehlo.compare LT, %nan, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %79 = stablehlo.compare LT, %nan, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++\n++  %80 = stablehlo.compare LE, %zero, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %81 = stablehlo.compare LE, %zero, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %82 = stablehlo.compare LE, %zero, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %83 = stablehlo.compare LE, %zero, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %84 = stablehlo.compare LE, %pos_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %85 = stablehlo.compare LE, %pos_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %86 = stablehlo.compare LE, %pos_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %87 = stablehlo.compare LE, %pos_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %88 = stablehlo.compare LE, %neg_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %89 = stablehlo.compare LE, %neg_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %90 = stablehlo.compare LE, %neg_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %91 = stablehlo.compare LE, %neg_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %92 = stablehlo.compare LE, %nan, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %93 = stablehlo.compare LE, %nan, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %94 = stablehlo.compare LE, %nan, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++  %95 = stablehlo.compare LE, %nan, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n++\n++  // CHECK: return [[TRUE]],  [[FALSE]], [[FALSE]], [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[TRUE]],  [[FALSE]], [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[TRUE]],  [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]],\n++\n++  // CHECK-SAME:   [[FALSE]], [[TRUE]],  [[TRUE]],  [[TRUE]],\n++  // CHECK-SAME:   [[TRUE]],  [[FALSE]], [[TRUE]],  [[TRUE]],\n++  // CHECK-SAME:   [[TRUE]],  [[TRUE]],  [[FALSE]], [[TRUE]],\n++  // CHECK-SAME:   [[TRUE]],  [[TRUE]],  [[TRUE]],  [[TRUE]],\n++\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[TRUE]],  [[FALSE]],\n++  // CHECK-SAME:   [[TRUE]],  [[FALSE]], [[TRUE]],  [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]],\n++\n++  // CHECK-SAME:   [[TRUE]],  [[FALSE]], [[TRUE]],  [[FALSE]],\n++  // CHECK-SAME:   [[TRUE]],  [[TRUE]],  [[TRUE]],  [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[TRUE]],  [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]],\n++\n++  // CHECK-SAME:   [[FALSE]], [[TRUE]],  [[FALSE]], [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]],\n++  // CHECK-SAME:   [[TRUE]],  [[TRUE]],  [[FALSE]], [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]],\n++\n++  // CHECK-SAME:   [[TRUE]],  [[TRUE]],  [[FALSE]], [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[TRUE]],  [[FALSE]], [[FALSE]],\n++  // CHECK-SAME:   [[TRUE]],  [[TRUE]],  [[TRUE]],  [[FALSE]],\n++  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]]\n++\n++  return  %0,  %1,  %2,  %3,\n++          %4,  %5,  %6,  %7,\n++          %8,  %9, %10, %11,\n++         %12, %13, %14, %15,\n++\n++         %16, %17, %18, %19,\n++         %20, %21, %22, %23,\n++         %24, %25, %26, %27,\n++         %28, %29, %30, %31,\n++\n++         %32, %33, %34, %35,\n++         %36, %37, %38, %39,\n++         %40, %41, %42, %43,\n++         %44, %45, %46, %47,\n++\n++         %48, %49, %50, %51,\n++         %52, %53, %54, %55,\n++         %56, %57, %58, %59,\n++         %60, %61, %62, %63,\n++\n++         %64, %65, %66, %67,\n++         %68, %69, %70, %71,\n++         %72, %73, %74, %75,\n++         %76, %77, %78, %79,\n++\n++         %80, %81, %82, %83,\n++         %84, %85, %86, %87,\n++         %88, %89, %90, %91,\n++         %92, %93, %94, %95 :\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,\n++         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>\n++}\n+ \n+ // -----\n+ \n+@@ -218,8 +482,7 @@\n+   %cst_2 = stablehlo.constant dense<2.0> : tensor<f32>\n+   // CHECK: stablehlo.constant dense<1> : tensor<i32>\n+   // CHECK: stablehlo.constant dense<1> : tensor<ui32>\n+-  // CHECK: stablehlo.divide{{.*}} : tensor<f32>\n+-  // DISABLED-CHECK: stablehlo.constant dense<1.0{{.*}}> : tensor<f32>\n++  // CHECK: stablehlo.constant dense<1.0{{.*}}> : tensor<f32>\n+   %0 = stablehlo.divide %cst, %cst : tensor<i32>\n+   %1 = stablehlo.divide %cst_1, %cst_1 : tensor<ui32>\n+   %2 = stablehlo.divide %cst_2, %cst_2 : tensor<f32>\n+diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n+--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n++++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n+@@ -1633,17 +1633,17 @@\n+   %s4 = stablehlo.select %4, %arg1, %arg2 : (tensor<2xi1>, tensor<2xi32>, tensor<2xi32>) -> tensor<2xi32>\n+   %s5 = stablehlo.select %5, %arg1, %arg3 : (tensor<2xi1>, tensor<2xi32>, tensor<2xi32>) -> tensor<2xi32>\n+ \n+-  // CHECK-DAG:  [[C0:%.+]] = stablehlo.compare EQ, [[ARG0]], [[ARG1]], SIGNED\n+-  // CHECK-DAG:  [[C1:%.+]] = stablehlo.compare NE, [[ARG0]], [[ARG1]], SIGNED\n+-\n+-  // CHECK-DAG:  [[S0:%.+]] = stablehlo.select [[C0]], [[ARG0]], [[ARG1]]\n+-  // CHECK-DAG:  [[S1:%.+]] = stablehlo.select [[C1]], [[ARG0]], [[ARG1]]\n+-  // CHECK-DAG:  [[S2:%.+]] = stablehlo.maximum [[ARG0]], [[ARG1]]\n+-  // CHECK-DAG:  [[S3:%.+]] = stablehlo.maximum [[ARG0]], [[ARG2]]\n+-  // CHECK-DAG:  [[S4:%.+]] = stablehlo.minimum [[ARG1]], [[ARG2]]\n+-  // CHECK-DAG:  [[S5:%.+]] = stablehlo.minimum [[ARG1]], [[ARG3]]\n+-\n+-  // CHECK-NEXT: return [[S0]], [[S1]], [[S2]], [[S3]], [[S4]], [[S5]]\n++  // DISABLED-CHECK-DAG:  [[C0:%.+]] = stablehlo.compare EQ, [[ARG0]], [[ARG1]], SIGNED\n++  // DISABLED-CHECK-DAG:  [[C1:%.+]] = stablehlo.compare NE, [[ARG0]], [[ARG1]], SIGNED\n++\n++  // DISABLED-CHECK-DAG:  [[S0:%.+]] = stablehlo.select [[C0]], [[ARG0]], [[ARG1]]\n++  // DISABLED-CHECK-DAG:  [[S1:%.+]] = stablehlo.select [[C1]], [[ARG0]], [[ARG1]]\n++  // DISABLED-CHECK-DAG:  [[S2:%.+]] = stablehlo.maximum [[ARG0]], [[ARG1]]\n++  // DISABLED-CHECK-DAG:  [[S3:%.+]] = stablehlo.maximum [[ARG0]], [[ARG2]]\n++  // DISABLED-CHECK-DAG:  [[S4:%.+]] = stablehlo.minimum [[ARG1]], [[ARG2]]\n++  // DISABLED-CHECK-DAG:  [[S5:%.+]] = stablehlo.minimum [[ARG1]], [[ARG3]]\n++\n++  // DISABLED-CHECK-NEXT: return [[S0]], [[S1]], [[S2]], [[S3]], [[S4]], [[S5]]\n+   return %s0, %s1, %s2, %s3, %s4, %s5 :\n+          tensor<2xi32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>\n+ }\n+@@ -1674,23 +1674,23 @@\n+   %s6 = stablehlo.select %6, %arg3, %arg2 : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>\n+   %s7 = stablehlo.select %7, %arg2, %arg3 : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>\n+ \n+-  // CHECK-DAG:  [[C1:%.+]] = stablehlo.compare GT, [[ARG1]], [[ARG2]], SIGNED\n+-  // CHECK-DAG:  [[C3:%.+]] = stablehlo.compare GE, [[ARG1]], [[ARG2]], SIGNED\n+-\n+-  // CHECK-DAG:  [[S0:%.+]] = stablehlo.minimum [[ARG0]], [[ARG1]]\n+-  // CHECK-DAG:  [[S1:%.+]] = stablehlo.select [[C1]], [[ARG0]], [[ARG1]]\n+-  // CHECK-DAG:  [[S2:%.+]] = stablehlo.minimum [[ARG3]], [[ARG1]]\n+-  // CHECK-DAG:  [[S3:%.+]] = stablehlo.select [[C3]], [[ARG0]], [[ARG2]]\n+-\n+-  // CHECK-DAG:  [[C5:%.+]] = stablehlo.compare LT, [[ARG0]], [[ARG2]], SIGNED\n+-  // CHECK-DAG:  [[C7:%.+]] = stablehlo.compare LE, [[ARG0]], [[ARG2]], SIGNED\n+-\n+-  // CHECK-DAG:  [[S4:%.+]] = stablehlo.maximum [[ARG2]], [[ARG1]]\n+-  // CHECK-DAG:  [[S5:%.+]] = stablehlo.select [[C5]], [[ARG1]], [[ARG2]]\n+-  // CHECK-DAG:  [[S6:%.+]] = stablehlo.maximum [[ARG3]], [[ARG2]]\n+-  // CHECK-DAG:  [[S7:%.+]] = stablehlo.select [[C7]], [[ARG2]], [[ARG3]]\n+-\n+-  // CHECK-NEXT: return [[S0]], [[S1]], [[S2]], [[S3]], [[S4]], [[S5]], [[S6]], [[S7]]\n++  // DISABLED-CHECK-DAG:  [[C1:%.+]] = stablehlo.compare GT, [[ARG1]], [[ARG2]], SIGNED\n++  // DISABLED-CHECK-DAG:  [[C3:%.+]] = stablehlo.compare GE, [[ARG1]], [[ARG2]], SIGNED\n++\n++  // DISABLED-CHECK-DAG:  [[S0:%.+]] = stablehlo.minimum [[ARG0]], [[ARG1]]\n++  // DISABLED-CHECK-DAG:  [[S1:%.+]] = stablehlo.select [[C1]], [[ARG0]], [[ARG1]]\n++  // DISABLED-CHECK-DAG:  [[S2:%.+]] = stablehlo.minimum [[ARG3]], [[ARG1]]\n++  // DISABLED-CHECK-DAG:  [[S3:%.+]] = stablehlo.select [[C3]], [[ARG0]], [[ARG2]]\n++\n++  // DISABLED-CHECK-DAG:  [[C5:%.+]] = stablehlo.compare LT, [[ARG0]], [[ARG2]], SIGNED\n++  // DISABLED-CHECK-DAG:  [[C7:%.+]] = stablehlo.compare LE, [[ARG0]], [[ARG2]], SIGNED\n++\n++  // DISABLED-CHECK-DAG:  [[S4:%.+]] = stablehlo.maximum [[ARG2]], [[ARG1]]\n++  // DISABLED-CHECK-DAG:  [[S5:%.+]] = stablehlo.select [[C5]], [[ARG1]], [[ARG2]]\n++  // DISABLED-CHECK-DAG:  [[S6:%.+]] = stablehlo.maximum [[ARG3]], [[ARG2]]\n++  // DISABLED-CHECK-DAG:  [[S7:%.+]] = stablehlo.select [[C7]], [[ARG2]], [[ARG3]]\n++\n++  // DISABLED-CHECK-NEXT: return [[S0]], [[S1]], [[S2]], [[S3]], [[S4]], [[S5]], [[S6]], [[S7]]\n+   return %s0, %s1, %s2, %s3, %s4, %s5, %s6, %s7 : tensor<i32>, tensor<i32>, tensor<i32>, tensor<i32>,\n+                                                   tensor<i32>, tensor<i32>, tensor<i32>, tensor<i32>\n+ }\n+@@ -2040,12 +2040,12 @@\n+ \n+ // CHECK-LABEL: @push_shape_ops_to_end\n+ func.func @push_shape_ops_to_end(%arg0 : tensor<12xf32>) -> tensor<3x4x2x1xf32> {\n+-  // CHECK: %[[COS:.+]] = stablehlo.cosine %arg0 : tensor<12xf32>\n+-  // CHECK: %[[ABS:.+]] = stablehlo.abs %[[COS]] : tensor<12xf32>\n+-  // CHECK: %[[RESHAPE:.+]] = stablehlo.reshape %[[ABS]] : (tensor<12xf32>) -> tensor<3x4xf32>\n+-  // CHECK: %[[BROADCAST:.+]] = stablehlo.broadcast %[[RESHAPE]], sizes = [1, 2] : (tensor<3x4xf32>) -> tensor<1x2x3x4xf32>\n+-  // CHECK: %[[TRANSPOSE:.+]] = stablehlo.transpose %[[BROADCAST]], dims = [2, 3, 1, 0] : (tensor<1x2x3x4xf32>) -> tensor<3x4x2x1xf32>\n+-  // CHECK: return %[[TRANSPOSE]]\n++  // DISABLED-CHECK: %[[COS:.+]] = stablehlo.cosine %arg0 : tensor<12xf32>\n++  // DISABLED-CHECK: %[[ABS:.+]] = stablehlo.abs %[[COS]] : tensor<12xf32>\n++  // DISABLED-CHECK: %[[RESHAPE:.+]] = stablehlo.reshape %[[ABS]] : (tensor<12xf32>) -> tensor<3x4xf32>\n++  // DISABLED-CHECK: %[[BROADCAST:.+]] = stablehlo.broadcast %[[RESHAPE]], sizes = [1, 2] : (tensor<3x4xf32>) -> tensor<1x2x3x4xf32>\n++  // DISABLED-CHECK: %[[TRANSPOSE:.+]] = stablehlo.transpose %[[BROADCAST]], dims = [2, 3, 1, 0] : (tensor<1x2x3x4xf32>) -> tensor<3x4x2x1xf32>\n++  // DISABLED-CHECK: return %[[TRANSPOSE]]\n+   %0 = stablehlo.reshape %arg0 : (tensor<12xf32>) -> tensor<3x4xf32>\n+   %1 = stablehlo.broadcast %0, sizes = [1, 2] : (tensor<3x4xf32>) -> tensor<1x2x3x4xf32>\n+   %2 = stablehlo.cosine %1 : (tensor<1x2x3x4xf32>) -> tensor<1x2x3x4xf32>\n+@@ -2059,9 +2059,9 @@\n+ \n+ // CHECK-LABEL: @reorder_with_type_change\n+ func.func @reorder_with_type_change(%arg0 : tensor<3x4xi32>) -> tensor<12xi64> {\n+-  // CHECK: %[[CONVERT:.+]] = stablehlo.convert %arg0 : (tensor<3x4xi32>) -> tensor<3x4xi64>\n+-  // CHECK: %[[RESHAPE:.+]] = stablehlo.reshape %[[CONVERT]] : (tensor<3x4xi64>) -> tensor<12xi64>\n+-  // CHECK: return %[[RESHAPE]]\n++  // DISABLED-CHECK: %[[CONVERT:.+]] = stablehlo.convert %arg0 : (tensor<3x4xi32>) -> tensor<3x4xi64>\n++  // DISABLED-CHECK: %[[RESHAPE:.+]] = stablehlo.reshape %[[CONVERT]] : (tensor<3x4xi64>) -> tensor<12xi64>\n++  // DISABLED-CHECK: return %[[RESHAPE]]\n+   %0 = stablehlo.reshape %arg0 : (tensor<3x4xi32>) -> tensor<12xi32>\n+   %1 = stablehlo.convert %0 : (tensor<12xi32>) -> tensor<12xi64>\n+   return %1 : tensor<12xi64>\n+diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir\n+--- stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir\n++++ stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir\n+@@ -59,3 +59,34 @@\n+   }\n+   return %0 : tensor<i64>\n+ }\n++\n++// -----\n++\n++// Check that we properly handle expressions involving NaN terms or variables\n++// that could potentially be NaN.\n++\n++// CHECK-LABEL: @fold_constant_nan_to_nan\n++func.func @fold_constant_nan_to_nan() -> tensor<f32> {\n++  // CHECK: [[NAN:%.*]] = stablehlo.constant dense<0x7FC00000> : tensor<f32>\n++  // CHECK: return [[NAN]] : tensor<f32>\n++  %zero = stablehlo.constant dense<0.0> : tensor<f32>\n++  %one = stablehlo.constant dense<1.0> : tensor<f32>\n++  %nan = stablehlo.constant dense<0x7FC00000> : tensor<f32>\n++  %nan_times_zero = stablehlo.multiply %nan, %zero : tensor<f32>\n++  %result = stablehlo.add %one, %nan_times_zero : tensor<f32>\n++  return %result : tensor<f32>\n++}\n++\n++// TODO: Consider adding an `--assume-non-nan` pass option to override this.\n++// CHECK-LABEL: @do_not_assume_non_nan\n++func.func @do_not_assume_non_nan(%arg0: tensor<f32>) -> tensor<f32> {\n++  // Note: These two checks are out of order on purpose: [[RESULT]] binds to the\n++  // `return` op first and then looks backward for the corresponding assignment.\n++  // CHECK-DAG: return [[RESULT:.*]] : tensor<f32>\n++  // CHECK-DAG: [[RESULT]] = stablehlo.{{(add|multiply).*}} : tensor<f32>\n++  %zero = stablehlo.constant dense<0.0> : tensor<f32>\n++  %one = stablehlo.constant dense<1.0> : tensor<f32>\n++  %arg_times_zero = stablehlo.multiply %arg0, %zero : tensor<f32>\n++  %result = stablehlo.add %one, %arg_times_zero : tensor<f32>\n++  return %result : tensor<f32>\n++}\n+diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n+--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n++++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n+@@ -23,6 +23,7 @@\n+ #include <string>\n+ #include <utility>\n+ \n++#include \"llvm/ADT/APFloat.h\"\n+ #include \"llvm/ADT/APInt.h\"\n+ #include \"llvm/ADT/APSInt.h\"\n+ #include \"llvm/ADT/FloatingPointMode.h\"\n+@@ -86,6 +87,18 @@\n+       /*isUnsigned=*/isUnsigned);\n+ }\n+ \n++APFloat getAPFloat(\n++    Type type, double value,\n++    llvm::RoundingMode roundingMode = llvm::RoundingMode::NearestTiesToEven) {\n++  auto floatType = dyn_cast<FloatType>(type);\n++  if (!floatType) llvm::report_fatal_error(\"expected float type\");\n++\n++  APFloat result(value);\n++  bool losesInfo = false;\n++  result.convert(floatType.getFloatSemantics(), roundingMode, &losesInfo);\n++  return result;\n++}\n++\n+ LogicalResult validateStaticShapeResult(PatternRewriter& rewriter,\n+                                         Operation* op, ShapedType resultType) {\n+   if (!resultType.hasStaticShape())\n+@@ -94,26 +107,30 @@\n+   return success();\n+ }\n+ \n+-template <typename Fn>\n+-static TypedAttr foldUnaryOpIntOrFloat(Type resultType, TypedAttr operand,\n+-                                       Fn&& folder) {\n++/// Unary constant folder that uses a generic folder function to handle both\n++/// ints and floats.\n++template <typename Fn, typename IntResultType = IntegerAttr,\n++          typename FloatResultType = FloatAttr>\n++TypedAttr foldUnaryOpIntOrFloat(Type resultType, TypedAttr operand,\n++                                Fn&& folder) {\n+   Type elemTy = getElementTypeOrSelf(operand);\n+ \n+   Attribute res;\n+   if (isa<IntegerType>(elemTy))\n+-    res = constFoldUnaryOp<IntegerAttr, IntegerAttr::ValueType, void>(operand,\n+-                                                                      folder);\n++    res = constFoldUnaryOp<IntegerAttr, IntegerAttr::ValueType, void,\n++                           IntResultType>(operand, folder);\n+   if (isa<FloatType>(elemTy))\n+-    res = constFoldUnaryOp<FloatAttr, FloatAttr::ValueType, void>(operand,\n+-                                                                  folder);\n++    res = constFoldUnaryOp<FloatAttr, FloatAttr::ValueType, void,\n++                           FloatResultType>(operand, folder);\n+   if (res) return cast<TypedAttr>(res);\n+ \n+   return nullptr;\n+ }\n+ \n+-/// Binary constant folder that used a generic folder function to handle both\n++/// Unary constant folder that uses a generic folder function to handle both\n+ /// ints and floats.\n+-template <typename Fn>\n++template <typename Fn, typename IntResultType = IntegerAttr,\n++          typename FloatResultType = FloatAttr>\n+ FailureOr<TypedAttr> foldUnaryOpIntOrFloat(PatternRewriter& rewriter,\n+                                            Operation* op, Fn&& folder) {\n+   if (op->getNumOperands() != 1 || op->getNumResults() != 1)\n+@@ -124,35 +141,38 @@\n+ \n+   if (!attr) return rewriter.notifyMatchFailure(op, \"operand not constants\");\n+ \n+-  TypedAttr res = foldUnaryOpIntOrFloat(op->getResultTypes()[0], attr, folder);\n++  TypedAttr res = foldUnaryOpIntOrFloat<Fn, IntResultType, FloatResultType>(\n++      op->getResultTypes()[0], attr, std::forward<Fn>(folder));\n+   if (!res) return rewriter.notifyMatchFailure(op, \"folding failed\");\n+ \n+   return res;\n+ }\n+ \n+-/// Binary constant folder that used a generic folder function to handle both\n++/// Binary constant folder that uses a generic folder function to handle both\n+ /// ints and floats.\n+-template <typename Fn>\n+-static TypedAttr foldBinaryOpIntOrFloat(Type resultType, TypedAttr lhs,\n+-                                        TypedAttr rhs, Fn&& folder) {\n++template <typename Fn, typename IntResultType = IntegerAttr,\n++          typename FloatResultType = FloatAttr>\n++TypedAttr foldBinaryOpIntOrFloat(Type resultType, TypedAttr lhs, TypedAttr rhs,\n++                                 Fn&& folder) {\n+   Attribute operands[2] = {lhs, rhs};\n+   Type elemTy = getElementTypeOrSelf(lhs);\n+ \n+   Attribute res;\n+   if (isa<IntegerType>(elemTy))\n+-    res = constFoldBinaryOp<IntegerAttr, IntegerAttr::ValueType, void>(\n+-        operands, resultType, folder);\n++    res = constFoldBinaryOp<IntegerAttr, IntegerAttr::ValueType, void,\n++                            IntResultType>(operands, resultType, folder);\n+   if (isa<FloatType>(elemTy))\n+-    res = constFoldBinaryOp<FloatAttr, FloatAttr::ValueType, void>(\n+-        operands, resultType, folder);\n++    res = constFoldBinaryOp<FloatAttr, FloatAttr::ValueType, void,\n++                            FloatResultType>(operands, resultType, folder);\n+   if (res) return cast<TypedAttr>(res);\n+ \n+   return nullptr;\n+ }\n+ \n+-/// Binary constant folder that used a generic folder function to handle both\n++/// Binary constant folder that uses a generic folder function to handle both\n+ /// ints and floats.\n+-template <typename Fn>\n++template <typename Fn, typename IntResultType = IntegerAttr,\n++          typename FloatResultType = FloatAttr>\n+ FailureOr<TypedAttr> foldBinaryOpIntOrFloat(PatternRewriter& rewriter,\n+                                             Operation* op, Fn&& folder) {\n+   if (op->getNumOperands() != 2 || op->getNumResults() != 1)\n+@@ -165,8 +185,8 @@\n+   if (!lhsAttr || !rhsAttr)\n+     return rewriter.notifyMatchFailure(op, \"lhs & rhs operands not constants\");\n+ \n+-  TypedAttr res =\n+-      foldBinaryOpIntOrFloat(op->getResultTypes()[0], lhsAttr, rhsAttr, folder);\n++  TypedAttr res = foldBinaryOpIntOrFloat<Fn, IntResultType, FloatResultType>(\n++      op->getResultTypes()[0], lhsAttr, rhsAttr, std::forward<Fn>(folder));\n+   if (!res) return rewriter.notifyMatchFailure(op, \"folding failed\");\n+ \n+   return res;\n+@@ -371,23 +391,38 @@\n+ struct FoldAndOpPattern : public ShapeOpRewritePattern<AndOp> {\n+   using ShapeOpRewritePattern::ShapeOpRewritePattern;\n+ \n+-  LogicalResult matchAndRewrite(mlir::stablehlo::AndOp op,\n+-                                PatternRewriter& rewriter) const override {\n+-    // TODO: Support more int types\n++  LogicalResult matchAndRewrite(AndOp op,\n++                                PatternRewriter& rewriter) const override {\n+     auto resultType = op.getType();\n+-    if (!resultType.getElementType().isInteger(1))\n+-      return rewriter.notifyMatchFailure(op, \"expected boolean element type\");\n+-\n+-    auto res = foldBinaryOpIntOrFloat(rewriter, op, FoldAnd{});\n+-    if (failed(res)) return failure();\n+-    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());\n+-    return success();\n+-  }\n+-\n+-  struct FoldAnd {\n++    auto resultElementType = resultType.getElementType();\n++    FailureOr<TypedAttr> result;\n++\n++    if (resultElementType.isInteger(/*width=*/1)) {\n++      result = foldBinaryOpIntOrFloat(rewriter, op, FoldLogicalAnd{});\n++    } else if (resultElementType.isInteger()) {\n++      result = foldBinaryOpIntOrFloat(rewriter, op, FoldBitwiseAnd{});\n++    } else {\n++      return rewriter.notifyMatchFailure(op, \"Expected integral element type.\");\n++    }\n++\n++    if (failed(result)) return failure();\n++    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op,\n++                                                             result.value());\n++    return success();\n++  }\n++\n++  struct FoldLogicalAnd {\n+     APInt operator()(APInt lhs, APInt rhs) const {\n+       return APInt(lhs.getBitWidth(), !lhs.isZero() && !rhs.isZero());\n+     }\n++    std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) const {\n++      return std::nullopt;\n++    }\n++  };\n++\n++  struct FoldBitwiseAnd {\n++    APInt operator()(APInt lhs, APInt rhs) const { return lhs & rhs; }\n++\n+     std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) const {\n+       return std::nullopt;\n+     }\n+@@ -426,7 +461,7 @@\n+     if (failed(validateShapeFoldDtype(rewriter, op, resultType)))\n+       return failure();\n+ \n+-    auto res = foldBinaryOpIntOrFloat(\n++    auto res = foldBinaryOpIntOrFloat<FoldCompare, IntegerAttr, IntegerAttr>(\n+         rewriter, op,\n+         FoldCompare(op.getComparisonDirection(), op.getCompareType()));\n+     if (failed(res)) return failure();\n+@@ -441,9 +476,29 @@\n+     ComparisonDirection direction;\n+     std::optional<ComparisonType> kind;\n+ \n+-    // TODO: Enable float folding.\n+-    std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {\n+-      return std::nullopt;\n++    APInt operator()(APFloat lhs, APFloat rhs) {\n++      bool result = false;\n++      switch (direction) {\n++        case ComparisonDirection::EQ:\n++          result = lhs == rhs;\n++          break;\n++        case ComparisonDirection::NE:\n++          result = lhs != rhs;\n++          break;\n++        case ComparisonDirection::GE:\n++          result = lhs >= rhs;\n++          break;\n++        case ComparisonDirection::GT:\n++          result = lhs > rhs;\n++          break;\n++        case ComparisonDirection::LE:\n++          result = lhs <= rhs;\n++          break;\n++        case ComparisonDirection::LT:\n++          result = lhs < rhs;\n++          break;\n++      }\n++      return APInt(/*bitwidth=*/1, result);\n+     }\n+     APInt operator()(APInt lhs, APInt rhs) {\n+       bool result = false;\n+@@ -509,6 +564,20 @@\n+     Operation* terminator = blockToInline->getTerminator();\n+     ValueRange results = terminator->getOperands();\n+ \n++    // TODO: Add support for complex, quantized, and token return types.\n++    // Currently, this pattern only supports int and float return types. We'll\n++    // need a more general equivalent of `getZeroAttr` to support other types.\n++    SmallVector<TypedAttr> placeholderAttrs;\n++    for (auto result : op.getResults()) {\n++      TypedAttr placeholderAttr = rewriter.getZeroAttr(result.getType());\n++      if (!placeholderAttr)\n++        return rewriter.notifyMatchFailure(\n++            op,\n++            \"The case op's return type isn't currently supported by this \"\n++            \"optimization pattern.\");\n++      placeholderAttrs.push_back(placeholderAttr);\n++    }\n++\n+     // Inline the active branch of the `case` op.\n+     rewriter.inlineBlockBefore(blockToInline, op, blockArgs);\n+     rewriter.replaceAllOpUsesWith(op, results);\n+@@ -521,9 +590,9 @@\n+     Block& noopBlock = region.emplaceBlock();\n+     SmallVector<Value> placeholderResults;\n+     rewriter.setInsertionPointToEnd(&noopBlock);\n+-    for (auto result : op.getResults()) {\n+-      placeholderResults.push_back(rewriter.create<ConstantOp>(\n+-          region.getLoc(), rewriter.getZeroAttr(result.getType())));\n++    for (auto placeholderAttr : placeholderAttrs) {\n++      placeholderResults.push_back(\n++          rewriter.create<ConstantOp>(region.getLoc(), placeholderAttr));\n+     }\n+     rewriter.create<stablehlo::ReturnOp>(region.getLoc(), placeholderResults);\n+ \n+@@ -628,10 +697,7 @@\n+         : foldIntFn(isUnsignedInt ? foldUint : foldSint) {}\n+     std::function<APInt(APInt, APInt)> foldIntFn;\n+ \n+-    // TODO: Enable float folding.\n+-    std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {\n+-      return std::nullopt;  // return lhs / rhs;\n+-    }\n++    APFloat operator()(APFloat lhs, APFloat rhs) { return lhs / rhs; }\n+     APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }\n+     static APInt foldUint(APInt lhs, APInt rhs) { return lhs.udiv(rhs); }\n+     static APInt foldSint(APInt lhs, APInt rhs) { return lhs.sdiv(rhs); }\n+@@ -669,9 +735,8 @@\n+       : foldIntFn(isUnsignedInt ? foldUint : foldSint) {}\n+   std::function<APInt(APInt, APInt)> foldIntFn;\n+ \n+-  // TODO: Enable float folding.\n+-  std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {\n+-    return std::nullopt;  // return lhs >= rhs ? lhs : rhs;\n++  APFloat operator()(APFloat lhs, APFloat rhs) {\n++    return lhs >= rhs ? lhs : rhs;\n+   }\n+   APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }\n+   static APInt foldUint(APInt lhs, APInt rhs) {\n+@@ -687,9 +752,8 @@\n+       : foldIntFn(isUnsignedInt ? foldUint : foldSint) {}\n+   std::function<APInt(APInt, APInt)> foldIntFn;\n+ \n+-  // TODO: Enable float folding.\n+-  std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {\n+-    return std::nullopt;  // return lhs <= rhs ? lhs : rhs;\n++  APFloat operator()(APFloat lhs, APFloat rhs) {\n++    return lhs <= rhs ? lhs : rhs;\n+   }\n+   APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }\n+   static APInt foldUint(APInt lhs, APInt rhs) {\n+@@ -706,11 +770,14 @@\n+   LogicalResult matchAndRewrite(MaxOp op,\n+                                 PatternRewriter& rewriter) const override {\n+     auto resultType = op.getType();\n++    auto resultElementType = resultType.getElementType();\n+     if (failed(validateShapeFoldDtype(rewriter, op, resultType)))\n+       return failure();\n+ \n+-    bool isUnsignedInt = resultType.getElementType().isUnsignedInteger();\n+-    auto res = foldBinaryOpIntOrFloat(rewriter, op, FoldMax(isUnsignedInt));\n++    bool isUnsignedIntOrBool = resultElementType.isUnsignedInteger() ||\n++                               resultElementType.isInteger(/*width=*/1);\n++    auto res =\n++        foldBinaryOpIntOrFloat(rewriter, op, FoldMax(isUnsignedIntOrBool));\n+     if (failed(res)) return failure();\n+     rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());\n+     return success();\n+@@ -723,11 +790,14 @@\n+   LogicalResult matchAndRewrite(MinOp op,\n+                                 PatternRewriter& rewriter) const override {\n+     auto resultType = op.getType();\n++    auto resultElementType = resultType.getElementType();\n+     if (failed(validateShapeFoldDtype(rewriter, op, resultType)))\n+       return failure();\n+ \n+-    bool isUnsignedInt = resultType.getElementType().isUnsignedInteger();\n+-    auto res = foldBinaryOpIntOrFloat(rewriter, op, FoldMin(isUnsignedInt));\n++    bool isUnsignedIntOrBool = resultElementType.isUnsignedInteger() ||\n++                               resultElementType.isInteger(/*width=*/1);\n++    auto res =\n++        foldBinaryOpIntOrFloat(rewriter, op, FoldMin(isUnsignedIntOrBool));\n+     if (failed(res)) return failure();\n+     rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());\n+     return success();\n+@@ -786,21 +856,36 @@\n+ \n+   LogicalResult matchAndRewrite(OrOp op,\n+                                 PatternRewriter& rewriter) const override {\n+-    // TODO: Support more int types\n+     auto resultType = op.getType();\n+-    if (!resultType.getElementType().isInteger(1))\n+-      return rewriter.notifyMatchFailure(op, \"expected boolean element type\");\n+-\n+-    auto res = foldBinaryOpIntOrFloat(rewriter, op, FoldOr{});\n+-    if (failed(res)) return failure();\n+-    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());\n+-    return success();\n+-  }\n+-\n+-  struct FoldOr {\n++    auto resultElementType = resultType.getElementType();\n++    FailureOr<TypedAttr> result;\n++\n++    if (resultElementType.isInteger(/*width=*/1)) {\n++      result = foldBinaryOpIntOrFloat(rewriter, op, FoldLogicalOr{});\n++    } else if (resultElementType.isInteger()) {\n++      result = foldBinaryOpIntOrFloat(rewriter, op, FoldBitwiseOr{});\n++    } else {\n++      return rewriter.notifyMatchFailure(op, \"Expected integral element type.\");\n++    }\n++\n++    if (failed(result)) return failure();\n++    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op,\n++                                                             result.value());\n++    return success();\n++  }\n++\n++  struct FoldLogicalOr {\n+     APInt operator()(APInt lhs, APInt rhs) const {\n+       return APInt(lhs.getBitWidth(), !lhs.isZero() || !rhs.isZero());\n+     }\n++    std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) const {\n++      return std::nullopt;\n++    }\n++  };\n++\n++  struct FoldBitwiseOr {\n++    APInt operator()(APInt lhs, APInt rhs) const { return lhs | rhs; }\n++\n+     std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) const {\n+       return std::nullopt;\n+     }\n+@@ -828,9 +913,12 @@\n+         : foldIntFn(isUnsignedInt ? foldUint : foldSint) {}\n+     std::function<APInt(APInt, APInt)> foldIntFn;\n+ \n+-    // TODO: Enable float folding.\n+     std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {\n+-      return std::nullopt;  // return lhs.remainder(rhs);\n++      // `APFloat::mod` requires both operands to have identical semantics.\n++      if (&lhs.getSemantics() != &rhs.getSemantics()) return std::nullopt;\n++\n++      lhs.mod(rhs);  // This modifies `lhs` in place.\n++      return lhs;    // `lhs` now holds the result.\n+     }\n+     APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }\n+     static APInt foldUint(APInt lhs, APInt rhs) { return lhs.urem(rhs); }\n+@@ -963,8 +1051,16 @@\n+   struct FoldSign {\n+     FoldSign(Type elementType) : elementType(elementType) {}\n+     Type elementType;\n+-    // TODO: Enable float folding.\n+-    std::optional<APFloat> operator()(APFloat operand) { return std::nullopt; }\n++    double result;\n++    APFloat operator()(APFloat operand) {\n++      if (operand.isNegative())\n++        result = -1.0;\n++      else if (operand.isZero())\n++        result = 0.0;\n++      else\n++        result = 1.0;\n++      return getAPFloat(elementType, result);\n++    }\n+ \n+     APInt operator()(APInt operand) {\n+       // SignOp only supports signed integers.\n+@@ -1220,13 +1316,9 @@\n+ \n+     for (auto [inputValue, bodyArg] :\n+          llvm::zip_equal(op.getOperands(), body.getArguments())) {\n+-      auto inputConstantOp = inputValue.getDefiningOp<ConstantOp>();\n+-      if (!inputConstantOp)\n+-        return rewriter.notifyMatchFailure(op, \"Input must be a constant.\");\n+-\n+-      auto inputConstantAttr =\n+-          dyn_cast_or_null<DenseElementsAttr>(inputConstantOp.getValue());\n+-      if (!inputConstantAttr)\n++      SplatElementsAttr constantSplatAttr;\n++      if (!matchPattern(inputValue, m_Constant(&constantSplatAttr)) ||\n++          !constantSplatAttr)\n+         return rewriter.notifyMatchFailure(op,\n+                                            \"Input must be a splat constant.\");\n+ \n+@@ -1236,7 +1328,7 @@\n+             op, \"Could not get the shape of the body argument.\");\n+ \n+       bodyArgConstantAttrs.push_back(DenseElementsAttr::get(\n+-          bodyArgShapedType, inputConstantAttr.getSplatValue<Attribute>()));\n++          bodyArgShapedType, constantSplatAttr.getSplatValue<Attribute>()));\n+     }\n+ \n+     for (BlockArgument bodyArg : body.getArguments()) {\n+diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n+--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n++++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n+@@ -64,7 +64,7 @@\n+ static constexpr StablehloAggressiveSimplificationPassOptions kDefaultOptions;\n+ \n+ static bool isIotaRange(ArrayRef<int64_t> dims) {\n+-  return llvm::all_of(llvm::enumerate(dims), [](const auto &it) {\n++  return llvm::all_of(llvm::enumerate(dims), [](const auto& it) {\n+     return static_cast<int64_t>(it.index()) == it.value();\n+   });\n+ }\n+@@ -72,20 +72,20 @@\n+ template <typename OpType>\n+ struct SimplifyOpRewritePattern : OpRewritePattern<OpType> {\n+   SimplifyOpRewritePattern(\n+-      MLIRContext *context,\n+-      const StablehloAggressiveSimplificationPassOptions &options,\n++      MLIRContext* context,\n++      const StablehloAggressiveSimplificationPassOptions& options,\n+       PatternBenefit benefit = 1, ArrayRef<StringRef> generatedNames = {})\n+       : OpRewritePattern<OpType>(context, benefit, generatedNames),\n+         options(options) {}\n+ \n+   // Prevent `options` from binding to a temporary.\n+   SimplifyOpRewritePattern(\n+-      MLIRContext *context,\n+-      StablehloAggressiveSimplificationPassOptions &&options,\n++      MLIRContext* context,\n++      StablehloAggressiveSimplificationPassOptions&& options,\n+       PatternBenefit benefit = 1,\n+       ArrayRef<StringRef> generatedNames = {}) = delete;\n+ \n+-  const StablehloAggressiveSimplificationPassOptions &options;\n++  const StablehloAggressiveSimplificationPassOptions& options;\n+ };\n+ \n+ /// Matches when either of the submatchers match.\n+@@ -93,7 +93,7 @@\n+ struct m_AnyOf {\n+   m_AnyOf(MatcherA a, MatcherB b) : matcherA(a), matcherB(b) {}\n+ \n+-  bool match(Operation *op) { return matcherA.match(op) || matcherB.match(op); }\n++  bool match(Operation* op) { return matcherA.match(op) || matcherB.match(op); }\n+ \n+   MatcherA matcherA;\n+   MatcherB matcherB;\n+@@ -146,7 +146,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(CompareOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     RankedTensorType type = op.getType();\n+ \n+     // Bail out on non-integer comparison.\n+@@ -211,7 +211,7 @@\n+  public:\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+   LogicalResult matchAndRewrite(ConcatenateOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     if (op.getInputs().size() != 1 ||\n+         op.getInputs().front().getType() != op.getType())\n+       return rewriter.notifyMatchFailure(op, \"not single operand noop-concat\");\n+@@ -227,7 +227,7 @@\n+  public:\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+   LogicalResult matchAndRewrite(ConcatenateOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     auto axis = op.getDimension();\n+     llvm::SmallVector<Value> newOperands = llvm::to_vector(\n+         llvm::make_filter_range(op.getOperands(), [&](Value operand) {\n+@@ -249,8 +249,8 @@\n+ class ConcatenateOpFlatten : public SimplifyOpRewritePattern<ConcatenateOp> {\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+   LogicalResult matchAndRewrite(ConcatenateOp op,\n+-                                PatternRewriter &rewriter) const override {\n+-    auto getFlattenedOperands = [&](const Value &val) -> ValueRange {\n++                                PatternRewriter& rewriter) const override {\n++    auto getFlattenedOperands = [&](const Value& val) -> ValueRange {\n+       auto definingOp = dyn_cast_or_null<ConcatenateOp>(val.getDefiningOp());\n+       // To avoid inflate the memory footprint, only flatten the\n+       // ConcatenateOp when it has only one use.\n+@@ -293,7 +293,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(CustomCallOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     constexpr StringRef kMhloBackendConfigAttrName = \"mhlo.backend_config\";\n+ \n+     if (op.getApiVersion() != CustomCallApiVersion::API_VERSION_ORIGINAL)\n+@@ -327,7 +327,7 @@\n+ /////////////////////////////////\n+ \n+ // Used in DRR file.\n+-DenseI64ArrayAttr getMergedBroadcastDimensions(OpBuilder &b,\n++DenseI64ArrayAttr getMergedBroadcastDimensions(OpBuilder& b,\n+                                                ArrayRef<int64_t> dims,\n+                                                ArrayRef<int64_t> dimsParent) {\n+   auto mergedDims = llvm::map_to_vector(\n+@@ -350,8 +350,8 @@\n+ /// the op is used outside of the HLO dialect (e.g. in func.return). In these\n+ /// cases, we insert a stablehlo.convert to smooth things out.\n+ template <typename OpTy, typename... Args>\n+-static OpTy refineOpWithNewOp(PatternRewriter &rewriter, Operation *op,\n+-                              Args &&...args) {\n++static OpTy refineOpWithNewOp(PatternRewriter& rewriter, Operation* op,\n++                              Args&&... args) {\n+   auto newOp = rewriter.create<OpTy>(op->getLoc(), std::forward<Args>(args)...);\n+ \n+   llvm::SmallVector<Value> replacementResults;\n+@@ -360,7 +360,7 @@\n+   for (auto [opResult, newOpResult] :\n+        llvm::zip(op->getResults(), newOp->getResults())) {\n+     Value replacementResult = newOpResult;\n+-    if (llvm::any_of(opResult.getUsers(), [&](Operation *user) {\n++    if (llvm::any_of(opResult.getUsers(), [&](Operation* user) {\n+           return user->getDialect() != op->getDialect();\n+         }))\n+       replacementResult = rewriter.create<ConvertOp>(\n+@@ -379,7 +379,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(DynamicBroadcastInDimOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     RankedTensorType operandType = op.getOperand().getType();\n+     if (!operandType.hasStaticShape())\n+       return rewriter.notifyMatchFailure(op, \"requires operand static shape\");\n+@@ -410,7 +410,7 @@\n+ // DynamicGatherOp\n+ /////////////////////////////////\n+ \n+-DenseI64ArrayAttr convertToI64Array(OpBuilder &b, Attribute attr) {\n++DenseI64ArrayAttr convertToI64Array(OpBuilder& b, Attribute attr) {\n+   auto denseAttr = cast<ElementsAttr>(attr);\n+   SmallVector<int64_t> result;\n+   result.reserve(denseAttr.getNumElements());\n+@@ -427,7 +427,7 @@\n+   using SimplifyOpRewritePattern<DynamicIotaOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(DynamicIotaOp iota,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     // Result type has static shape, replace with iota.\n+     auto resultTy = cast<ShapedType>(iota.getType());\n+     if (!resultTy.hasStaticShape())\n+@@ -447,7 +447,7 @@\n+   using SimplifyOpRewritePattern<DynamicIotaOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(DynamicIotaOp iota,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     auto resultType = cast<ShapedType>(iota.getType());\n+     if (resultType.getRank() < 2)\n+       return rewriter.notifyMatchFailure(iota, \"requires rank >= 2\");\n+@@ -496,7 +496,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(DynamicReshapeOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     // This is a noop when the output type is already a static shape.\n+     RankedTensorType type = op.getType();\n+     if (!type.hasStaticShape())\n+@@ -516,8 +516,8 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(DynamicReshapeOp op,\n+-                                PatternRewriter &rewriter) const override {\n+-    Operation *defOp = op.getOperand().getDefiningOp();\n++                                PatternRewriter& rewriter) const override {\n++    Operation* defOp = op.getOperand().getDefiningOp();\n+     if (!defOp ||\n+         !defOp->hasTrait<mlir::OpTrait::SameOperandsAndResultShape>()) {\n+       return rewriter.notifyMatchFailure(\n+@@ -549,7 +549,7 @@\n+   using SimplifyOpRewritePattern<DynamicSliceOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(DynamicSliceOp dynamicSlice,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     Value input = dynamicSlice.getOperand();\n+     auto inputType = cast<ShapedType>(input.getType());\n+     if (!inputType.hasStaticShape())\n+@@ -558,7 +558,7 @@\n+ \n+     auto sliceSizes = dynamicSlice.getSliceSizes();\n+     SmallVector<int64_t, 4> tempStartIndices;\n+-    for (const auto &indexAndSliceStart :\n++    for (const auto& indexAndSliceStart :\n+          llvm::enumerate(dynamicSlice.getStartIndices())) {\n+       APInt val;\n+       Value start = indexAndSliceStart.value();\n+@@ -579,7 +579,7 @@\n+     // pack them into a single tensor.\n+     auto sliceStartIndices = rewriter.getDenseI64ArrayAttr(tempStartIndices);\n+     SmallVector<int64_t, 4> tempSliceLimits;\n+-    for (const auto &[start, size] : llvm::zip(tempStartIndices, sliceSizes)) {\n++    for (const auto& [start, size] : llvm::zip(tempStartIndices, sliceSizes)) {\n+       tempSliceLimits.push_back(start + size);\n+     }\n+     auto sliceLimits = rewriter.getDenseI64ArrayAttr(tempSliceLimits);\n+@@ -605,7 +605,7 @@\n+   using SimplifyOpRewritePattern<RealDynamicSliceOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(RealDynamicSliceOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     // This rewrite only works for unit strides because DynamicSliceOp\n+     // doesn't support strides (i.e. it implicitly has unit strides).\n+     DenseIntElementsAttr stridesAttr;\n+@@ -670,11 +670,11 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(ReduceOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     // If all returned values in the ReduceOp region exists outside the\n+     // region, replace the ReduceOp with those values.\n+     if (auto retOp = dyn_cast<ReturnOp>(op.getBody().front().getTerminator())) {\n+-      Region *retRegion = retOp->getParentRegion();\n++      Region* retRegion = retOp->getParentRegion();\n+       if (llvm::any_of(retOp.getResults(), [retRegion](Value result) {\n+             return result.getParentRegion() == retRegion;\n+           }))\n+@@ -693,7 +693,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(ReduceOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     // We require all reduce shapes to be the same, up to the element types, so\n+     // we can just use the first operand and the first result as\n+     // representatives.\n+@@ -733,7 +733,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(ReduceOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     SmallVector<OpResult, 4> usedResults;\n+     llvm::copy_if(op.getResults(), std::back_inserter(usedResults),\n+                   [](OpResult result) { return !result.use_empty(); });\n+@@ -745,7 +745,7 @@\n+     const auto numOperands = op.getNumOperands();\n+     const auto numOperandPairs = numOperands / pairSize;\n+ \n+-    Block &reducerBlock = op.getBody().front();\n++    Block& reducerBlock = op.getBody().front();\n+     auto retOp = cast<ReturnOp>(reducerBlock.getTerminator());\n+ \n+     assert(numOperandPairs == op.getNumResults() &&\n+@@ -757,10 +757,10 @@\n+       if (v.getParentRegion() == reducerBody) workList.push_back(v);\n+     };\n+ \n+-    SmallPtrSet<Operation *, 16> usedOps;\n++    SmallPtrSet<Operation*, 16> usedOps;\n+     SmallBitVector usedArgs(numOperands);\n+     SmallBitVector usedReturnOperands(numOperandPairs);\n+-    for (const auto &usedResult : usedResults) {\n++    for (const auto& usedResult : usedResults) {\n+       auto resultNo = usedResult.getResultNumber();\n+       usedReturnOperands.set(resultNo);\n+ \n+@@ -774,9 +774,9 @@\n+           const auto pairNo = blockArg.getArgNumber() % numOperandPairs;\n+           usedArgs.set(pairNo);\n+           usedArgs.set(pairNo + numOperandPairs);\n+-        } else if (auto *defOp = definition.getDefiningOp()) {\n++        } else if (auto* defOp = definition.getDefiningOp()) {\n+           usedOps.insert(defOp);\n+-          for (const auto &operand : defOp->getOperands())\n++          for (const auto& operand : defOp->getOperands())\n+             addToWorkList(operand);\n+         }\n+       }\n+@@ -785,7 +785,7 @@\n+     const auto newNumOperandPairs = usedResults.size();\n+     const auto newNumOperands = newNumOperandPairs * pairSize;\n+     if (newNumOperands != usedArgs.count()) {\n+-      return rewriter.notifyMatchFailure(op, [&](Diagnostic &diag) {\n++      return rewriter.notifyMatchFailure(op, [&](Diagnostic& diag) {\n+         diag << \"non-conservative case: \" << newNumOperandPairs\n+              << \" return results should be matched with \" << newNumOperands\n+              << \" operands, but got \" << usedArgs.count();\n+@@ -809,7 +809,7 @@\n+     auto newOp =\n+         rewriter.create<ReduceOp>(op.getLoc(), newInputs, newInitVals,\n+                                   op.getDimensionsAttr(), newElementTypes);\n+-    Block *newReducerBlock = rewriter.createBlock(&newOp.getBody());\n++    Block* newReducerBlock = rewriter.createBlock(&newOp.getBody());\n+ \n+     IRMapping mapper;\n+     for (auto arg : reducerBlock.getArguments())\n+@@ -818,11 +818,11 @@\n+                    newReducerBlock->addArgument(arg.getType(), arg.getLoc()));\n+ \n+     rewriter.setInsertionPointToStart(newReducerBlock);\n+-    for (Operation &op : reducerBlock.getOperations())\n++    for (Operation& op : reducerBlock.getOperations())\n+       if (usedOps.contains(&op)) rewriter.clone(op, mapper);\n+ \n+     SmallVector<Value> newReturnOperands;\n+-    for (const auto &en : llvm::enumerate(retOp.getOperands()))\n++    for (const auto& en : llvm::enumerate(retOp.getOperands()))\n+       if (usedReturnOperands[en.index()])\n+         newReturnOperands.push_back(mapper.lookup(en.value()));\n+ \n+@@ -830,7 +830,7 @@\n+ \n+     // Build new results list (unused entries will be null).\n+     SmallVector<Value> newResults(op.getNumResults());\n+-    for (const auto &[i, result] : llvm::enumerate(usedResults)) {\n++    for (const auto& [i, result] : llvm::enumerate(usedResults)) {\n+       newResults[result.getResultNumber()] = newOp.getResult(i);\n+     }\n+ \n+@@ -851,7 +851,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(GetDimensionSizeOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     // Fold get_dimension_size when the queried dim is statically known.\n+     RankedTensorType operandTy = op.getOperand().getType();\n+ \n+@@ -877,7 +877,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(GatherOp gather,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     DenseIntElementsAttr index;\n+     if (!matchPattern(gather.getStartIndices(), m_Constant(&index)))\n+       return failure();\n+@@ -952,7 +952,7 @@\n+   using SimplifyOpRewritePattern<IotaOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(IotaOp iota,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     auto resultTy = cast<ShapedType>(iota.getType());\n+     if (resultTy.getRank() < 2)\n+       return rewriter.notifyMatchFailure(iota, \"itoa not broadcastable\");\n+@@ -989,7 +989,7 @@\n+   using SimplifyOpRewritePattern<PadOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(PadOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     auto operand = op.getOperand();\n+     auto padVal = op.getPaddingValue();\n+ \n+@@ -1028,7 +1028,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(SelectOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     RankedTensorType type = op.getType();\n+ \n+     Value trueVal = op.getOnTrue();\n+@@ -1079,7 +1079,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(SelectOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     Value pred = op.getPred();\n+     Value trueVal = op.getOnTrue();\n+     Value falseVal = op.getOnFalse();\n+@@ -1133,7 +1133,7 @@\n+   using SimplifyOpRewritePattern<SliceOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(SliceOp slice,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     auto resultTy = cast<ShapedType>(slice.getType());\n+     if (!resultTy.hasStaticShape())\n+       return rewriter.notifyMatchFailure(slice, \"result shape not static\");\n+@@ -1228,12 +1228,12 @@\n+   using SimplifyOpRewritePattern<SortOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(SortOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     DenseSet<unsigned> erasedArgs;\n+     unsigned numOperands = op.getNumOperands();\n+     for (unsigned i = 0; i < numOperands; ++i) {\n+       if (!op.getResult(i).use_empty()) continue;\n+-      Block &block = op.getComparator().front();\n++      Block& block = op.getComparator().front();\n+       if (!block.getArgument(i * 2).use_empty()) continue;\n+       if (!block.getArgument(i * 2 + 1).use_empty()) continue;\n+       erasedArgs.insert(i);\n+@@ -1242,7 +1242,7 @@\n+ \n+     SmallVector<Value> newOperands;\n+     BitVector erasedBlockArgs(op.getNumOperands() * 2);\n+-    for (const auto &en : llvm::enumerate(op.getInputs())) {\n++    for (const auto& en : llvm::enumerate(op.getInputs())) {\n+       if (erasedArgs.contains(en.index())) {\n+         erasedBlockArgs.set(en.index() * 2);\n+         erasedBlockArgs.set(en.index() * 2 + 1);\n+@@ -1253,7 +1253,7 @@\n+ \n+     auto newOp = rewriter.create<SortOp>(op.getLoc(), newOperands,\n+                                          op.getDimension(), op.getIsStable());\n+-    Region &region = newOp.getComparator();\n++    Region& region = newOp.getComparator();\n+     rewriter.inlineRegionBefore(op.getComparator(), region, region.end());\n+     region.front().eraseArguments(erasedBlockArgs);\n+ \n+@@ -1278,7 +1278,7 @@\n+   using SimplifyOpRewritePattern<SortOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(SortOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     if (op.getResults().empty() ||\n+         static_cast<int64_t>(op.getDimension()) != -1)\n+       return rewriter.notifyMatchFailure(op,\n+@@ -1304,7 +1304,7 @@\n+   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(TransposeOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     auto input = op.getOperand();\n+     auto permutation = op.getPermutation();\n+ \n+@@ -1340,7 +1340,7 @@\n+   using SimplifyOpRewritePattern<TupleOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(TupleOp op,\n+-                                PatternRewriter &rewriter) const override {\n++                                PatternRewriter& rewriter) const override {\n+     if (op.getVal().empty())\n+       return rewriter.notifyMatchFailure(op, \"empty tuple\");\n+ \n+@@ -1356,7 +1356,7 @@\n+           op, \"tuple predecessor type does not match\");\n+ \n+     // Check that this is a repacking of the parent tuple.\n+-    for (const auto &elementAndIdx : llvm::enumerate(op.getVal())) {\n++    for (const auto& elementAndIdx : llvm::enumerate(op.getVal())) {\n+       auto elementOp = elementAndIdx.value().getDefiningOp<GetTupleElementOp>();\n+       if (!elementOp ||\n+           elementOp.getIndexAttr().getInt() !=\n+@@ -1385,9 +1385,9 @@\n+   using SimplifyOpRewritePattern<WhileOp>::SimplifyOpRewritePattern;\n+ \n+   LogicalResult matchAndRewrite(WhileOp whileOp,\n+-                                PatternRewriter &rewriter) const override {\n+-    Block *cond = whileOp.SingleBlock::getBody(0);\n+-    Block *body = whileOp.SingleBlock::getBody(1);\n++                                PatternRewriter& rewriter) const override {\n++    Block* cond = whileOp.SingleBlock::getBody(0);\n++    Block* body = whileOp.SingleBlock::getBody(1);\n+     auto bodyReturnOp = cast<ReturnOp>(body->getTerminator());\n+     if (!llvm::any_of(llvm::zip(whileOp->getOperands(), body->getArguments(),\n+                                 bodyReturnOp->getOperands()),\n+@@ -1400,10 +1400,10 @@\n+     SmallVector<Value> newOperands, resultsToReplace;\n+     SmallVector<unsigned> invariantArgIdxs;\n+     BitVector invariantArgIdxBitVector(cond->getNumArguments());\n+-    for (const auto &enumeratedOperands : llvm::enumerate(llvm::zip(\n++    for (const auto& enumeratedOperands : llvm::enumerate(llvm::zip(\n+              whileOp.getOperands(), cond->getArguments(), body->getArguments(),\n+              bodyReturnOp->getOperands(), whileOp->getResults()))) {\n+-      const auto &operands = enumeratedOperands.value();\n++      const auto& operands = enumeratedOperands.value();\n+       Value whileOperand = std::get<0>(operands);\n+       BlockArgument condBlockArg = std::get<1>(operands);\n+       BlockArgument bodyBlockArg = std::get<2>(operands);\n+@@ -1455,14 +1455,14 @@\n+ // Pattern: op(X : zero_extent_tensor) -> constant([])\n+ struct ZeroExtentToEmptyConstant final : RewritePattern {\n+   explicit ZeroExtentToEmptyConstant(\n+-      MLIRContext *context,\n++      MLIRContext* context,\n+       StablehloAggressiveSimplificationPassOptions options,\n+       PatternBenefit benefit = 1)\n+       : RewritePattern(MatchAnyOpTypeTag(), benefit, context),\n+         options(options) {}\n+ \n+-  LogicalResult matchAndRewrite(Operation *op,\n+-                                PatternRewriter &rewriter) const override {\n++  LogicalResult matchAndRewrite(Operation* op,\n++                                PatternRewriter& rewriter) const override {\n+     auto loc = op->getLoc();\n+ \n+     if (!isa_and_present<StablehloDialect>(op->getDialect()))\n+@@ -1492,10 +1492,10 @@\n+ \n+     // If one of the operands is a zero-extent tensor, replace the operand with\n+     // an empty tensor.\n+-    for (OpOperand &operand : op->getOpOperands()) {\n++    for (OpOperand& operand : op->getOpOperands()) {\n+       auto operandType = getMaybeZeroExtentType(operand.get().getType());\n+       if (!operandType || operand.get().getDefiningOp<ConstantOp>()) continue;\n+-      Operation *owner = operand.getOwner();\n++      Operation* owner = operand.getOwner();\n+       int operandNum = operand.getOperandNumber();\n+       auto emptyConstantOp = rewriter.create<ConstantOp>(\n+           loc, operandType.value(),\n+@@ -1514,13 +1514,13 @@\n+ struct ReorderElementwiseAndShapeOp final\n+     : OpTraitRewritePattern<OpTrait::Elementwise> {\n+   explicit ReorderElementwiseAndShapeOp(\n+-      MLIRContext *context,\n++      MLIRContext* context,\n+       StablehloAggressiveSimplificationPassOptions options,\n+       PatternBenefit benefit = 1)\n+       : OpTraitRewritePattern(context, benefit), options(options) {}\n+ \n+-  LogicalResult matchAndRewrite(Operation *op,\n+-                                PatternRewriter &rewriter) const override {\n++  LogicalResult matchAndRewrite(Operation* op,\n++                                PatternRewriter& rewriter) const override {\n+     if (op->getOperands().size() != 1)\n+       return rewriter.notifyMatchFailure(op, \"expected to be unary\");\n+ \n+@@ -1575,7 +1575,7 @@\n+       : StablehloAggressiveSimplificationPassBase() {}\n+ \n+   void runOnOperation() override {\n+-    MLIRContext *context = &getContext();\n++    MLIRContext* context = &getContext();\n+     RewritePatternSet patterns(context);\n+ \n+     StablehloAggressiveSimplificationPassOptions options{\n+@@ -1597,12 +1597,13 @@\n+ }  // namespace\n+ \n+ void populateStablehloCanonicalizationPatterns(\n+-    MLIRContext *context, RewritePatternSet *patterns,\n+-    const StablehloAggressiveSimplificationPassOptions &options,\n++    MLIRContext* context, RewritePatternSet* patterns,\n++    const StablehloAggressiveSimplificationPassOptions& options,\n+     PatternBenefit benefit) {\n+   populateWithGenerated(*patterns);\n++  // TODO: Re-enable `CompareSelectIntoMinMax` after fixing legalization issue.\n+   patterns->add<\n+-      CompareOpCanon, CompareSelectIntoMinMax, ConcatenateOpFlatten,\n++      CompareOpCanon, /*CompareSelectIntoMinMax,*/ ConcatenateOpFlatten,\n+       ConcatenateOpNoop, ConcatenateOpRemoveEmpty,\n+       CustomCallUnregisteredBackendConfigToFfi, DynamicIotaOpToBroadcast,\n+       DynamicReshapeOpSameOperandAndResultShape, DynamicSliceOpToSlice,\n+@@ -1614,8 +1615,11 @@\n+       context, options, benefit);\n+ \n+   // Generic patterns\n+-  patterns->add<ReorderElementwiseAndShapeOp, ZeroExtentToEmptyConstant>(\n+-      context, options, benefit);\n++  // TODO: Re-enable `ReorderElementwiseAndShapeOp` after fixing BF16 precision\n++  // emulation issue in XLA-CPU.\n++  patterns->add<\n++      /*ReorderElementwiseAndShapeOp,*/\n++      ZeroExtentToEmptyConstant>(context, options, benefit);\n+ \n+   // TODO: Dynamism Refinements, consider merging with canonicalize dynamism\n+   patterns\n+@@ -1623,22 +1627,22 @@\n+             DynamicReshapeOpIsStatic, DynamicIotaIsStatic>(context, options);\n+ }\n+ \n+-void populateStablehloCanonicalizationPatterns(MLIRContext *context,\n+-                                               RewritePatternSet *patterns,\n++void populateStablehloCanonicalizationPatterns(MLIRContext* context,\n++                                               RewritePatternSet* patterns,\n+                                                PatternBenefit benefit) {\n+   populateStablehloCanonicalizationPatterns(context, patterns, kDefaultOptions,\n+                                             benefit);\n+ }\n+ \n+ void populateStablehloHloImportCanonicalizationPatterns(\n+-    MLIRContext *context, RewritePatternSet *patterns,\n+-    const StablehloAggressiveSimplificationPassOptions &options) {\n++    MLIRContext* context, RewritePatternSet* patterns,\n++    const StablehloAggressiveSimplificationPassOptions& options) {\n+   patterns->add<ReshapeOp_RemoveNoop, GetTupleElementOp_UnpackTuple>(context);\n+   patterns->add<TupleIsRepacking, WhileOpImplicitCapture>(context, options);\n+ }\n+ \n+ void populateStablehloHloImportCanonicalizationPatterns(\n+-    MLIRContext *context, RewritePatternSet *patterns) {\n++    MLIRContext* context, RewritePatternSet* patterns) {\n+   populateStablehloHloImportCanonicalizationPatterns(context, patterns,\n+                                                      kDefaultOptions);\n+ }\n+diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n+--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n++++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n+@@ -234,7 +234,7 @@\n+ //\n+ // No-op, but wrap in ConvertOp to preserve dynamic output shape. This can be\n+ // important if the result is returned, in which case refining the type would\n+-// require also updating the funciton signature.\n++// require also updating the function signature.\n+ def DynamicBroadcastInDimOp_ReplaceNoopWithConvert\n+   : Pat<(StableHLO_DynamicBroadcastInDimOp:$op\n+             $operand, $shape, IotaDims:$dims, $expanding, $nonexpanding),\n+@@ -387,9 +387,10 @@\n+ \n+ // Pattern: multiply(X, 0i) -> 0i\n+ //\n+-// Multiplication by 0. This fold is not trivial for floats in presence of NaNs.\n++// Multiplication by 0. This fold is not trivial for floats in presence of NaNs,\n++// so we currently only enable it for ints.\n+ def MulOp_FoldToZero\n+-  : Pat<(StableHLO_MulOp $lhs, (StableHLO_ConstantOp:$zero AnyZero:$value)),\n++  : Pat<(StableHLO_MulOp $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n+         (replaceWithValue $zero)>;\n+ \n+ // Pattern: multiply(X, 1i) -> X\n "
        },
        {
            "sha": "199071996a47debd59e0ee863881f55de6f5d969",
            "filename": "third_party/xla/xla/hlo/translate/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4afd7e80b0e6015c18f239ca2cdd70cb481ccd73/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4afd7e80b0e6015c18f239ca2cdd70cb481ccd73/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2FBUILD?ref=4afd7e80b0e6015c18f239ca2cdd70cb481ccd73",
            "patch": "@@ -131,6 +131,7 @@ cc_library(\n         \"@llvm-project//mlir:Transforms\",\n         \"@llvm-project//mlir:UBDialect\",\n         \"@stablehlo//:stablehlo_passes\",\n+        \"@stablehlo//:stablehlo_passes_optimization\",\n     ],\n )\n "
        },
        {
            "sha": "445d7674fb86c949f6ccbcb39681a7dd8c010227",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/mlir_hlo_to_hlo.cc",
            "status": "modified",
            "additions": 30,
            "deletions": 12,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4afd7e80b0e6015c18f239ca2cdd70cb481ccd73/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4afd7e80b0e6015c18f239ca2cdd70cb481ccd73/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc?ref=4afd7e80b0e6015c18f239ca2cdd70cb481ccd73",
            "patch": "@@ -1350,6 +1350,7 @@ void BuildGetTupleElementsForTupleResults(mlir::Operation* op, xla::XlaOp tuple,\n \n namespace mlir {\n \n+// (-- LINT.IfChange(stablehlo_optimization_path) --)\n namespace stablehlo {\n namespace {\n \n@@ -1667,6 +1668,13 @@ LogicalResult ExportXlaOp(RecvOp op, OpLoweringContext ctx) {\n     }\n   }\n \n+  // HLO GetTupleElement needs a single sharding,\n+  std::optional<xla::OpSharding> sharding = ctx.builder->sharding();\n+  if (sharding.has_value() && sharding->type() == xla::OpSharding::TUPLE) {\n+    CHECK_GE(ctx.builder->sharding()->tuple_shardings_size(), 2);\n+    sharding = ctx.builder->sharding()->tuple_shardings(1);\n+  }\n+  xla::XlaScopedShardingAssignment sharding_scope(ctx.builder, sharding);\n   value_map[op.getResult(num_results - 1)] =\n       xla::GetTupleElement(xla_result, 1);\n \n@@ -1900,9 +1908,9 @@ LogicalResult ExportXlaOp(CaseOp op, OpLoweringContext ctx) {\n     if (failed(GetXlaOps(op, implicit_operands, ctx, args))) return failure();\n \n     llvm::SmallVector<std::optional<xla::OpSharding>> arg_shardings;\n-    if (!ret_shardings.empty()) {\n-      // We only add arg shardings if there are result shardings, otherwise it\n-      // means sharding propagation hasn't been done yet.\n+    if (!ret_shardings.empty() || op->getNumResults() == 0) {\n+      // We only add arg shardings if there are result shardings or no results,\n+      // otherwise it means sharding propagation hasn't been done yet.\n       arg_shardings = GetXlaOpShardings(args);\n     }\n \n@@ -2907,10 +2915,14 @@ LogicalResult ExportXlaOp(ReduceWindowOp op, OpLoweringContext ctx) {\n     return failure();\n   }\n \n+  constexpr ArrayRef<int64_t> kEmptyArray = {};\n+\n   xla::XlaOp result = xla::ReduceWindowWithGeneralPadding(\n       operands, init_values, body, op.getWindowDimensions(),\n-      op.getWindowStrides().value(), op.getBaseDilations().value(),\n-      op.getWindowDilations().value(), Convert_padding(op.getPadding()));\n+      op.getWindowStrides().value_or(kEmptyArray),\n+      op.getBaseDilations().value_or(kEmptyArray),\n+      op.getWindowDilations().value_or(kEmptyArray),\n+      Convert_padding(op.getPadding()));\n \n   if (op.getNumResults() == 1) {\n     value_map[op.getResult(0)] = result;\n@@ -3001,10 +3013,12 @@ LogicalResult ExportXlaOp(SelectAndScatterOp op, OpLoweringContext ctx) {\n   if (failed(GetXlaOp(op.getInitValue(), value_map, &init_value, op)))\n     return failure();\n \n+  constexpr ArrayRef<int64_t> kEmptyArray = {};\n+\n   value_map[op] = xla::SelectAndScatterWithGeneralPadding(\n-      operand, select, op.getWindowDimensions().value(),\n-      op.getWindowStrides().value(), Convert_padding(op.getPadding()), source,\n-      init_value, scatter);\n+      operand, select, op.getWindowDimensions().value_or(kEmptyArray),\n+      op.getWindowStrides().value_or(kEmptyArray),\n+      Convert_padding(op.getPadding()), source, init_value, scatter);\n   return success();\n }\n \n@@ -3060,9 +3074,12 @@ LogicalResult ExportXlaOp(UniformDequantizeOp op, OpLoweringContext ctx) {\n \n }  // namespace\n }  // namespace stablehlo\n+// (-- LINT.ThenChange(:mhlo_optimization_path) --)\n \n+// (-- LINT.IfChange(mhlo_optimization_path) --)\n namespace mhlo {\n namespace {\n+\n LogicalResult ExportXlaOp(CollectiveBroadcastOp op, OpLoweringContext ctx) {\n   auto& value_map = *ctx.values;\n   xla::XlaOp operand;\n@@ -3794,9 +3811,9 @@ LogicalResult ExportXlaOp(IfOp op, OpLoweringContext ctx) {\n \n   llvm::SmallVector<std::optional<xla::OpSharding>> true_arg_shardings,\n       false_arg_shardings;\n-  if (!ret_shardings.empty()) {\n-    // We only add arg shardings if there are result shardings, otherwise it\n-    // means sharding propagation hasn't been done yet.\n+  if (!ret_shardings.empty() || op->getNumResults() == 0) {\n+    // We only add arg shardings if there are result shardings or no results,\n+    // otherwise it means sharding propagation hasn't been done yet.\n     true_arg_shardings = GetXlaOpShardings(true_args);\n     false_arg_shardings = GetXlaOpShardings(false_args);\n   }\n@@ -5199,6 +5216,8 @@ LogicalResult ExportXlaOp(MinimumBroadcastShapesOp op, OpLoweringContext ctx) {\n \n }  // namespace\n }  // namespace mhlo\n+// (-- LINT.ThenChange(:stablehlo_optimization_path) --)\n+\n }  // namespace mlir\n \n #include \"xla/hlo/translate/mhlo_to_hlo/hlo_op_writer.inc\"\n@@ -6186,7 +6205,6 @@ LogicalResult ConvertToHloModule::LowerBasicBlockAsFunction(\n     return failure();\n   }\n   computation = computation_or.value();\n-  // LLVM_DEBUG(llvm::dbgs() << \"Created: \" << result->name() << \"\\n\");\n   return success();\n }\n "
        }
    ],
    "stats": {
        "total": 1539,
        "additions": 1527,
        "deletions": 12
    }
}