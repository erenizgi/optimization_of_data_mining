{
    "author": "pemeliya",
    "message": "PR #32748: [ROCM] Adding grid-stride loops for buffer comparator and redzone checker kernels\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32748\n\nüìù Summary of Changes\nAdded grid-stride loops to buffer_comparator and redzone checker kernels,\nextended gpu_kernel_tiling test and buffer_comparator_test, removed rocm-specific hacks\n\nüéØ Justification\nWhen the input shape is too large, buffer_comparator/redzone_checker kernels may fail to launch due to grid size limitations\n\nüöÄ Kind of Contribution\nüêõ Bug Fix\n\nüß™ Unit Tests:\nWhat unit tests were added? For example, a new pass should be tested on minimal\nHLO. The transformation can be tested with FileCheck tests or assertions on the\ntransformed HLO.\n\nüß™ Execution Tests:\nWhat execution tests were added? For example, a new optimization should be\ntested with an end-to-end execution test triggering the optimization and\nasserting correctness. Please provide test cases running with at most 2 GPUs.\n\nThis PR was extracted as part of the original one: https://github.com/openxla/xla/pull/30127\n\n@xla-rotation: can you have a look, please ?\n\nCopybara import of the project:\n\n--\n7797cd1c9154be64af9569f15b12928931015c12 by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\nAdding grid-stride loops for buffer comparator and redzone checker kernels\n\n--\n3f0172e8c8d4ad1812846f14d29f73059427ab24 by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\nchanged buffer comparator test to reduce memory profile\n\nMerging this change closes #32748\n\nPiperOrigin-RevId: 833899988",
    "sha": "12bda18eb7037e360275b29a714255428ddfa6a0",
    "files": [
        {
            "sha": "9c2d45bbe1dffc420eb81c6ef7a2cb1d1c540cdc",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffer_comparator.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 8,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12bda18eb7037e360275b29a714255428ddfa6a0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12bda18eb7037e360275b29a714255428ddfa6a0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.cc?ref=12bda18eb7037e360275b29a714255428ddfa6a0",
            "patch": "@@ -48,6 +48,7 @@ namespace gpu {\n struct ComparisonParams {\n   double relative_tol = 0.1;\n   bool verbose = true;\n+  bool run_host_compare = true;\n   const Shape* shape = nullptr;\n   se::Stream* stream = nullptr;\n   se::DeviceMemoryBase current{};\n@@ -85,6 +86,14 @@ static absl::StatusOr<bool> DeviceCompare(const ComparisonParams& params) {\n \n   LaunchDimensions dim =\n       CalculateLaunchDimensions(*params.shape, gpu_device_info);\n+  // Limit # of blocks to some meaningful number which is large enough to\n+  // occupy all GPU cores if necessary but not too large to reduce # of idle\n+  // blocks\n+  dim = LaunchDimensions(\n+      se::BlockDim(std::min(dim.num_blocks(),\n+                            BufferComparator::kMaxNumThreadBlocksForKernel),\n+                   1, 1),\n+      dim.thread_counts_per_block());\n \n   se::DeviceMemory<uint64_t> as_uint64(out.memory());\n   TF_RETURN_IF_ERROR(comparison_kernel.Launch(\n@@ -161,9 +170,8 @@ static absl::StatusOr<bool> CompareEqualParameterized(\n     const ComparisonParams& params) {\n   XLA_SCOPED_LOGGING_TIMER(\"BufferComparator::CompareEqual\");\n   TF_ASSIGN_OR_RETURN(bool result, DeviceCompare<ElementT>(params));\n-  if (result) {\n-    return true;\n-  }\n+  if (result) return true;\n+  if (!params.run_host_compare) return false;\n \n   TF_ASSIGN_OR_RETURN(bool host_return,\n                       (HostCompare<ElementT, ComparisonT>(params)));\n@@ -173,9 +181,9 @@ static absl::StatusOr<bool> CompareEqualParameterized(\n }\n \n absl::StatusOr<bool> BufferComparator::CompareEqual(\n-    se::Stream* stream, se::DeviceMemoryBase current,\n-    se::DeviceMemoryBase expected) const {\n-  ComparisonParams params{relative_tol_, verbose_, &shape_,\n+    se::Stream* stream, const se::DeviceMemoryBase& current,\n+    const se::DeviceMemoryBase& expected) const {\n+  ComparisonParams params{relative_tol_, verbose_, run_host_compare_, &shape_,\n                           stream,        current,  expected};\n \n   auto do_compare = [&](auto cst_type) {\n@@ -206,8 +214,11 @@ absl::StatusOr<bool> BufferComparator::CompareEqual(\n }\n \n BufferComparator::BufferComparator(const Shape& shape, double tolerance,\n-                                   bool verbose)\n-    : shape_(shape), relative_tol_(tolerance), verbose_(verbose) {\n+                                   bool verbose, bool run_host_compare)\n+    : shape_(shape),\n+      relative_tol_(tolerance),\n+      verbose_(verbose),\n+      run_host_compare_(run_host_compare) {\n   // Normalize complex shapes: since we treat the passed array as a contiguous\n   // storage it does not matter which dimension are we doubling.\n   auto double_dim_size = [&]() {"
        },
        {
            "sha": "041fda06c2b3e7864612aa8173ef39a6228ca961",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffer_comparator.h",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12bda18eb7037e360275b29a714255428ddfa6a0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12bda18eb7037e360275b29a714255428ddfa6a0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator.h?ref=12bda18eb7037e360275b29a714255428ddfa6a0",
            "patch": "@@ -26,11 +26,14 @@ namespace xla::gpu {\n // A device-side comparator that compares buffers.\n class BufferComparator {\n  public:\n+  // Maximum number of thread blocks to be used for comparator kernel\n+  static constexpr uint64_t kMaxNumThreadBlocksForKernel = 32768;\n+\n   BufferComparator(const BufferComparator&) = delete;\n-  BufferComparator(BufferComparator&&) = default;\n+  BufferComparator(BufferComparator&&) noexcept = default;\n \n   explicit BufferComparator(const Shape& shape, double tolerance = 0.1,\n-                            bool verbose = true);\n+                            bool verbose = true, bool run_host_compare = true);\n \n   // Returns true if the two buffers compare equal. The definition of \"equal\"\n   // is:\n@@ -42,12 +45,15 @@ class BufferComparator {\n   //\n   // See the implementation for the tolerance value.\n   absl::StatusOr<bool> CompareEqual(se::Stream* stream,\n-                                    se::DeviceMemoryBase current,\n-                                    se::DeviceMemoryBase expected) const;\n+                                    const se::DeviceMemoryBase& current,\n+                                    const se::DeviceMemoryBase& expected) const;\n+\n  private:\n   Shape shape_;\n   double relative_tol_;  // relative tolerance for comparison\n   bool verbose_;         // whether to print out error message on mismatch\n+  // enable host-side compare if device compare reports a mismatch\n+  bool run_host_compare_;\n };\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "86f2bddf24295ffa210384387433299525a11925",
            "filename": "third_party/xla/xla/backends/gpu/runtime/buffer_comparator_test.cc",
            "status": "modified",
            "additions": 39,
            "deletions": 0,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12bda18eb7037e360275b29a714255428ddfa6a0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12bda18eb7037e360275b29a714255428ddfa6a0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fbuffer_comparator_test.cc?ref=12bda18eb7037e360275b29a714255428ddfa6a0",
            "patch": "@@ -448,6 +448,45 @@ TEST_F(BufferComparatorTest, BF16) {\n                    .value());\n }\n \n+TEST_F(BufferComparatorTest, VeryLargeArray) {\n+  constexpr PrimitiveType number_type = U8;\n+  using NT = primitive_util::PrimitiveTypeToNative<number_type>::type;\n+\n+  // Set non-power-of-two element count on purpose, use aligned byffer siuze\n+  int64_t n_elems = (1LL << 33) - 11,\n+          // Buffer size must be 4-bytes aligned for Memset32\n+      buf_size = (((n_elems + 1) * sizeof(NT)) + 3) & ~3;\n+  auto stream = stream_exec_->CreateStream().value();\n+\n+  // Use host memory here since there is a limitation of 4GB per test on\n+  // device memory alloc\n+  TF_ASSERT_OK_AND_ASSIGN(auto base,\n+                          stream_exec_->HostMemoryAllocate(buf_size));\n+\n+  // We use overlapping lhs and rhs arrays to reduce memory usage, also this\n+  // serves as an extra test for possible pointer aliasing problems\n+  se::DeviceMemoryBase lhs(base->opaque(), n_elems * sizeof(NT)),\n+      rhs(static_cast<NT*>(base->opaque()) + 1, lhs.size());\n+\n+  constexpr uint32_t pattern = 0xABABABAB;\n+  TF_CHECK_OK(stream->Memset32(&lhs, pattern, buf_size));\n+\n+  // First we do \"positive\" test to make sure lhs and rhs are indeed equal:\n+  // disable host comparison here since it could take a while for ~8GB array\n+  BufferComparator comparator(ShapeUtil::MakeShape(number_type, {n_elems}),\n+                              /*tolerance*/ 0.1, /* verbose */ false,\n+                              /*run_host_compare*/ false);\n+  EXPECT_TRUE(comparator.CompareEqual(stream.get(), lhs, rhs).value());\n+\n+  se::DeviceMemoryBase last_word(\n+      static_cast<uint8_t*>(base->opaque()) + (n_elems & ~3), sizeof(uint32_t));\n+  // Change only the very last entry of rhs to verify that the whole arrays are\n+  // compared (if the grid dimensions are not computed correctly, this might\n+  // not be the case).\n+  TF_CHECK_OK(stream->Memset32(&last_word, 0x11223344, last_word.size()));\n+  EXPECT_FALSE(comparator.CompareEqual(stream.get(), lhs, rhs).value());\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "e07eb2bf3b4926451ae24f002b4ded9984a1350d",
            "filename": "third_party/xla/xla/service/gpu/launch_dimensions.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 28,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12bda18eb7037e360275b29a714255428ddfa6a0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flaunch_dimensions.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12bda18eb7037e360275b29a714255428ddfa6a0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flaunch_dimensions.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flaunch_dimensions.cc?ref=12bda18eb7037e360275b29a714255428ddfa6a0",
            "patch": "@@ -54,36 +54,16 @@ LaunchDimensions CalculateLaunchDimensions(\n   num_elements = CeilOfRatio(num_elements, int64_t{dim_config.unroll_factor});\n   const int kWarpSchedulers = 4;\n \n-  if (xla::PlatformUtil::CanonicalPlatformName(\"gpu\").value() == \"rocm\") {\n-    int64_t threads_per_block_x = std::min<int64_t>(\n-        gpu_device_info.threads_per_warp() * kWarpSchedulers, num_elements);\n+  int64_t threads_per_block = std::min<int64_t>(\n+      gpu_device_info.threads_per_warp() * kWarpSchedulers, num_elements);\n \n-    int64_t num_blocks = CeilOfRatio(num_elements, threads_per_block_x);\n-    CHECK(num_blocks < gpu_device_info.block_dim_limit().x);\n+  int64_t num_blocks_total = CeilOfRatio(num_elements, threads_per_block);\n+  int64_t num_blocks_y = CeilOfRatio<uint64_t>(\n+      num_blocks_total, gpu_device_info.block_dim_limit().x);\n+  int64_t num_blocks_x = CeilOfRatio(num_blocks_total, num_blocks_y);\n \n-    int threads_per_block_y = 1;\n-    while ((num_blocks * threads_per_block_x) >\n-           std::numeric_limits<uint32_t>::max()) {\n-      threads_per_block_x /= 2;\n-      threads_per_block_y *= 2;\n-    }\n-\n-    return LaunchDimensions(\n-        se::BlockDim(num_blocks, 1, 1),\n-        se::ThreadDim(threads_per_block_x, threads_per_block_y, 1));\n-\n-  } else {\n-    int64_t threads_per_block = std::min<int64_t>(\n-        gpu_device_info.threads_per_warp() * kWarpSchedulers, num_elements);\n-\n-    int64_t num_blocks_total = CeilOfRatio(num_elements, threads_per_block);\n-    int64_t num_blocks_y = CeilOfRatio<uint64_t>(\n-        num_blocks_total, gpu_device_info.block_dim_limit().x);\n-    int64_t num_blocks_x = CeilOfRatio(num_blocks_total, num_blocks_y);\n-\n-    return LaunchDimensions(se::BlockDim(num_blocks_x, num_blocks_y, 1),\n-                            se::ThreadDim(threads_per_block, 1, 1));\n-  }\n+  return LaunchDimensions(se::BlockDim(num_blocks_x, num_blocks_y, 1),\n+                          se::ThreadDim(threads_per_block, 1, 1));\n }\n \n LaunchDimensionsProto LaunchDimensions::ToProto() const {"
        },
        {
            "sha": "757409950f0acb5a8caae6ae5f28ca4074e56d4c",
            "filename": "third_party/xla/xla/service/gpu/launch_dimensions.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12bda18eb7037e360275b29a714255428ddfa6a0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flaunch_dimensions.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12bda18eb7037e360275b29a714255428ddfa6a0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flaunch_dimensions.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flaunch_dimensions.h?ref=12bda18eb7037e360275b29a714255428ddfa6a0",
            "patch": "@@ -49,9 +49,9 @@ class LaunchDimensions {\n       : block_counts_(block_counts),\n         thread_counts_per_block_(thread_counts_per_block) {}\n \n-  se::BlockDim block_counts() const { return block_counts_; }\n+  const se::BlockDim& block_counts() const { return block_counts_; }\n \n-  se::ThreadDim thread_counts_per_block() const {\n+  const se::ThreadDim& thread_counts_per_block() const {\n     return thread_counts_per_block_;\n   }\n "
        },
        {
            "sha": "f2f8db5c60a832046eeb72aea8624190624b3b74",
            "filename": "third_party/xla/xla/stream_executor/gpu/buffer_comparator_kernel_lib.cu.h",
            "status": "modified",
            "additions": 45,
            "deletions": 41,
            "changes": 86,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12bda18eb7037e360275b29a714255428ddfa6a0/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_comparator_kernel_lib.cu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12bda18eb7037e360275b29a714255428ddfa6a0/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_comparator_kernel_lib.cu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fbuffer_comparator_kernel_lib.cu.h?ref=12bda18eb7037e360275b29a714255428ddfa6a0",
            "patch": "@@ -61,32 +61,32 @@ template <typename T>\n __global__ void xla_fp_comparison(T* buffer_a, T* buffer_b,\n                                   float rel_error_threshold,\n                                   uint64_t buffer_length, int* mismatch_count) {\n-  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n-  if (idx >= buffer_length) {\n-    return;\n-  }\n-\n-  auto elem_a = Canonicalize(buffer_a[idx]);\n-  auto elem_b = Canonicalize(buffer_b[idx]);\n-\n-  // NaN's are considered equal.\n-  if (Eigen::numext::isnan(elem_a) && Eigen::numext::isnan(elem_b)) {\n-    return;\n-  }\n-\n-  // Two infinities are considered equal. Computing relative error would\n-  // otherwise result in NaN.\n-  if (elem_a == elem_b) {\n-    return;\n-  }\n-\n-  float rel_error = Eigen::numext::abs(elem_a - elem_b) /\n-                    (Eigen::numext::maxi(Eigen::numext::abs(elem_a),\n-                                         Eigen::numext::abs(elem_b)) +\n-                     1);\n-\n-  if (rel_error > rel_error_threshold || Eigen::numext::isnan(rel_error))\n-    atomicAdd(mismatch_count, 1);\n+  const uint64_t block_dim_x = static_cast<uint64_t>(blockDim.x),\n+                 stride = block_dim_x * gridDim.x;\n+  for (uint64_t idx = threadIdx.x + blockIdx.x * block_dim_x;\n+       idx < buffer_length; idx += stride) {\n+    auto elem_a = Canonicalize(buffer_a[idx]);\n+    auto elem_b = Canonicalize(buffer_b[idx]);\n+\n+    // NaN's are considered equal.\n+    if (Eigen::numext::isnan(elem_a) && Eigen::numext::isnan(elem_b)) {\n+      continue;\n+    }\n+    // Two infinities are considered equal. Computing relative error would\n+    // otherwise result in NaN.\n+    if (elem_a == elem_b) {\n+      continue;\n+    }\n+\n+    float rel_error = Eigen::numext::abs(elem_a - elem_b) /\n+                      (Eigen::numext::maxi(Eigen::numext::abs(elem_a),\n+                                           Eigen::numext::abs(elem_b)) +\n+                       1);\n+\n+    if (rel_error > rel_error_threshold || Eigen::numext::isnan(rel_error)) {\n+      atomicAdd(mismatch_count, 1);\n+    }\n+  }  // for\n }\n \n // TODO(b/191520348): The comparison below requires exact equality.\n@@ -95,21 +95,25 @@ __global__ void xla_int_comparison(T* buffer_a, T* buffer_b,\n                                    float rel_error_threshold,\n                                    uint64_t buffer_length,\n                                    int* mismatch_count) {\n-  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n-  if (idx >= buffer_length) return;\n-  float elem_a;\n-  float elem_b;\n-  if constexpr (std::numeric_limits<T>::is_signed) {\n-    elem_a = static_cast<int64_t>(buffer_a[idx]);\n-    elem_b = static_cast<int64_t>(buffer_b[idx]);\n-  } else {\n-    elem_a = static_cast<uint64_t>(buffer_a[idx]);\n-    elem_b = static_cast<uint64_t>(buffer_b[idx]);\n-  }\n-  float rel_error =\n-      fabs(elem_a - elem_b) / (fmax(fabs(elem_a), fabs(elem_b)) + 1);\n-  if (rel_error > rel_error_threshold || isnan(rel_error))\n-    atomicAdd(mismatch_count, 1);\n+  const uint64_t block_dim_x = static_cast<uint64_t>(blockDim.x),\n+                 stride = block_dim_x * gridDim.x;\n+  for (uint64_t idx = threadIdx.x + blockIdx.x * block_dim_x;\n+       idx < buffer_length; idx += stride) {\n+    float elem_a;\n+    float elem_b;\n+    if constexpr (std::numeric_limits<T>::is_signed) {\n+      elem_a = static_cast<int64_t>(buffer_a[idx]);\n+      elem_b = static_cast<int64_t>(buffer_b[idx]);\n+    } else {\n+      elem_a = static_cast<uint64_t>(buffer_a[idx]);\n+      elem_b = static_cast<uint64_t>(buffer_b[idx]);\n+    }\n+    float rel_error =\n+        fabs(elem_a - elem_b) / (fmax(fabs(elem_a), fabs(elem_b)) + 1);\n+    if (rel_error > rel_error_threshold || isnan(rel_error)) {\n+      atomicAdd(mismatch_count, 1);\n+    }\n+  }  // for\n }\n \n template <typename NativeT>"
        },
        {
            "sha": "9ad57b9d33a111b5f6b19da4de169683a4ffb35a",
            "filename": "third_party/xla/xla/stream_executor/gpu/redzone_allocator.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12bda18eb7037e360275b29a714255428ddfa6a0/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12bda18eb7037e360275b29a714255428ddfa6a0/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator.cc?ref=12bda18eb7037e360275b29a714255428ddfa6a0",
            "patch": "@@ -178,7 +178,8 @@ static absl::Status RunRedzoneChecker(\n   int64_t threads_per_block = std::min(\n       executor->GetDeviceDescription().threads_per_block_limit(), num_elements);\n   int64_t block_count =\n-      tsl::MathUtil::CeilOfRatio(num_elements, threads_per_block);\n+      std::min(tsl::MathUtil::CeilOfRatio(num_elements, threads_per_block),\n+               RedzoneAllocator::kMaxNumThreadBlocksForKernel);\n \n   TF_RETURN_IF_ERROR(comparison_kernel.Launch(\n       ThreadDim(threads_per_block), BlockDim(block_count), stream, redzone,"
        },
        {
            "sha": "66a2c1b3e1b18d4b01982d8c5b3a9462391fd48b",
            "filename": "third_party/xla/xla/stream_executor/gpu/redzone_allocator.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12bda18eb7037e360275b29a714255428ddfa6a0/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12bda18eb7037e360275b29a714255428ddfa6a0/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator.h?ref=12bda18eb7037e360275b29a714255428ddfa6a0",
            "patch": "@@ -47,6 +47,9 @@ class RedzoneAllocator : public ScratchAllocator {\n   static constexpr int64_t kDefaultRedzoneSize =\n       1LL << 23;  // 8MiB per side, 16MiB total.\n   static constexpr uint8_t kDefaultRedzonePattern = -1;  // NOLINT\n+  // Maximum number of thread blocks to be used for redzone checker kernel\n+  static constexpr int64_t kMaxNumThreadBlocksForKernel = 32768;\n+\n   RedzoneAllocator(Stream* stream, DeviceMemoryAllocator* memory_allocator,\n                    int64_t memory_limit = (1LL << 32),  // 4GB\n                    int64_t redzone_size = kDefaultRedzoneSize,"
        },
        {
            "sha": "1920618f0b34de793f75740c960dc3640886f513",
            "filename": "third_party/xla/xla/stream_executor/gpu/redzone_allocator_kernel_lib.cu.h",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12bda18eb7037e360275b29a714255428ddfa6a0/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator_kernel_lib.cu.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12bda18eb7037e360275b29a714255428ddfa6a0/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator_kernel_lib.cu.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fredzone_allocator_kernel_lib.cu.h?ref=12bda18eb7037e360275b29a714255428ddfa6a0",
            "patch": "@@ -24,12 +24,11 @@ __global__ void RedzoneAllocatorKernelImpl(uint8_t* input_buffer,\n                                            uint8_t redzone_pattern,\n                                            uint64_t buffer_length,\n                                            uint32_t* out_mismatched_ptr) {\n-  uint64_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n-  if (idx >= buffer_length) {\n-    return;\n-  }\n-  if (input_buffer[idx] != redzone_pattern) {\n-    atomicAdd(out_mismatched_ptr, 1);\n+  const uint64_t block_dim_x = static_cast<uint64_t>(blockDim.x),\n+                 stride = block_dim_x * gridDim.x;\n+  for (uint64_t idx = threadIdx.x + blockIdx.x * block_dim_x;\n+       idx < buffer_length; idx += stride) {\n+    if (input_buffer[idx] != redzone_pattern) atomicAdd(out_mismatched_ptr, 1);\n   }\n }\n }  // namespace stream_executor::gpu"
        }
    ],
    "stats": {
        "total": 223,
        "additions": 133,
        "deletions": 90
    }
}