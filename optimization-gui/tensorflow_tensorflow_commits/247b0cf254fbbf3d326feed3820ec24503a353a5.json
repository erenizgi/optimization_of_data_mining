{
    "author": "basioli-k",
    "message": "[XLA][codegen] Migrate FpToFpOp to arith trunc/ext ops.\n\nThe code path that emits FpToFpOp can get triggered by the CPU backend which doesn't support Triton specific ops.\n\nPiperOrigin-RevId: 836177591",
    "sha": "247b0cf254fbbf3d326feed3820ec24503a353a5",
    "files": [
        {
            "sha": "8ac18c14f5d1088b92bedbfda6f4b91808db0106",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/247b0cf254fbbf3d326feed3820ec24503a353a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/247b0cf254fbbf3d326feed3820ec24503a353a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=247b0cf254fbbf3d326feed3820ec24503a353a5",
            "patch": "@@ -285,23 +285,10 @@ Value Cast(EmitterLocOpBuilder& b, Value value, Type dst_element_ty) {\n   auto src_fp_element_ty = mlir::dyn_cast<mlir::FloatType>(src_element_ty);\n   auto dst_fp_element_ty = mlir::dyn_cast<mlir::FloatType>(dst_element_ty);\n   if (src_fp_element_ty && dst_fp_element_ty) {\n-    // F8 <-> FP16, BF16, FP32, FP64 need to be handled via Triton's tt.fp_to_fp\n-    // because LLVM doesn't support casts from/to FP8.\n-    // TODO(b/413272992): Add better test coverage for FpToFpOp.\n-    if (IsFp8Type(src_element_ty) && !IsFp8Type(dst_element_ty)) {\n-      return b.create<mt::FpToFpOp>(dst_ty, value);\n-    }\n-    if (IsFp8Type(dst_element_ty) && !IsFp8Type(src_element_ty)) {\n-      return b.create<mt::FpToFpOp>(\n-          dst_ty, value,\n-          mt::RoundingModeAttr::get(b.getContext(), mt::RoundingMode::RTNE));\n-    }\n     if (IsFp8Type(src_element_ty) && IsFp8Type(dst_element_ty)) {\n       // FP8 <-> FP8 conversion needs to go through FP16\n-      auto fp16_value = b.create<mt::FpToFpOp>(fp16_ty, value);\n-      return b.create<mt::FpToFpOp>(\n-          dst_ty, fp16_value,\n-          mt::RoundingModeAttr::get(b.getContext(), mt::RoundingMode::RTNE));\n+      auto fp16_value = b.create<ma::ExtFOp>(fp16_ty, value);\n+      return b.create<ma::TruncFOp>(dst_ty, fp16_value);\n     }\n \n     if (src_fp_element_ty.getFPMantissaWidth() >"
        },
        {
            "sha": "85807ddad291392fe34176b85a8690fabf4d083b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/247b0cf254fbbf3d326feed3820ec24503a353a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/247b0cf254fbbf3d326feed3820ec24503a353a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=247b0cf254fbbf3d326feed3820ec24503a353a5",
            "patch": "@@ -2045,6 +2045,7 @@ absl::Status LowerXTileToTriton(mlir::ModuleOp xtile_dialect_module,\n     if (fusion_kind != kTritonGemmFusionKind) {\n       pm.addPass(xtile::createConvertElementwise0DTensorToScalarPass());\n     }\n+    pm.addPass(mlir::triton::xla::CreateArithFP8ConversionToTritonPass());\n     pm.addPass(mlir::triton::xla::CreateTensorLowerToTritonPass());\n     pm.addPass(mlir::triton::xla::CreateStableHLOLowerToTritonPass());\n     pm.addPass(mlir::triton::xla::CreateXTileLowerToTritonPass());"
        },
        {
            "sha": "5151d25e189da05ad824ec28447b8e627941c624",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/247b0cf254fbbf3d326feed3820ec24503a353a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/247b0cf254fbbf3d326feed3820ec24503a353a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD?ref=247b0cf254fbbf3d326feed3820ec24503a353a5",
            "patch": "@@ -31,6 +31,7 @@ gentbl_cc_library(\n cc_library(\n     name = \"passes\",\n     srcs = [\n+        \"arith_fp8_conversion_to_triton.cc\",\n         \"extract_tma_info_pass.cc\",\n         \"generalize_kernel_signature.cc\",\n         \"int4_passes.cc\","
        },
        {
            "sha": "730c3df447afe8256940ca529d3562d2a64266d5",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/arith_fp8_conversion_to_triton.cc",
            "status": "added",
            "additions": 145,
            "deletions": 0,
            "changes": 145,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/247b0cf254fbbf3d326feed3820ec24503a353a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Farith_fp8_conversion_to_triton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/247b0cf254fbbf3d326feed3820ec24503a353a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Farith_fp8_conversion_to_triton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Farith_fp8_conversion_to_triton.cc?ref=247b0cf254fbbf3d326feed3820ec24503a353a5",
            "patch": "@@ -0,0 +1,145 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinTypeInterfaces.h\"\n+#include \"mlir/IR/Diagnostics.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/TypeUtilities.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include \"mlir/Transforms/WalkPatternRewriteDriver.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+\n+namespace mlir::triton::xla {\n+\n+namespace ttir = ::mlir::triton;\n+\n+#define GEN_PASS_DEF_ARITHFP8CONVERSIONTOTRITONPASS\n+#include \"xla/backends/gpu/codegen/triton/transforms/passes.h.inc\"\n+\n+namespace {\n+\n+bool IsFp8TypeOrFp8TensorType(mlir::Type t) {\n+  auto element_type = mlir::getElementTypeOrSelf(t);\n+  return mlir::cast<mlir::FloatType>(element_type).getWidth() == 8;\n+}\n+\n+bool IsFp8ToNonFp8Conversion(mlir::Type src_ty, mlir::Type dst_ty) {\n+  return IsFp8TypeOrFp8TensorType(src_ty) && !IsFp8TypeOrFp8TensorType(dst_ty);\n+}\n+\n+bool IsNonFp8ToFp8Conversion(mlir::Type src_ty, mlir::Type dst_ty) {\n+  return !IsFp8TypeOrFp8TensorType(src_ty) && IsFp8TypeOrFp8TensorType(dst_ty);\n+}\n+\n+class LowerExtFOp : public mlir::OpRewritePattern<mlir::arith::ExtFOp> {\n+ public:\n+  using OpRewritePattern::OpRewritePattern;\n+\n+ private:\n+  mlir::LogicalResult matchAndRewrite(\n+      mlir::arith::ExtFOp op, mlir::PatternRewriter& rewriter) const override {\n+    auto src_ty = op.getOperand().getType();\n+    auto dst_ty = op.getType();\n+\n+    if (!IsFp8ToNonFp8Conversion(src_ty, dst_ty)) {\n+      return rewriter.notifyMatchFailure(\n+          op->getLoc(),\n+          \"ExtFOp will be lowered to FpToFpOp only if it converts from FP8 to \"\n+          \"FP16, BF16, FP32, FP64.\");\n+    }\n+    rewriter.replaceOpWithNewOp<ttir::FpToFpOp>(op, op.getType(),\n+                                                op.getOperand());\n+    return mlir::success();\n+  }\n+};\n+\n+class LowerTruncFOp : public mlir::OpRewritePattern<mlir::arith::TruncFOp> {\n+ public:\n+  using OpRewritePattern::OpRewritePattern;\n+\n+ private:\n+  mlir::LogicalResult matchAndRewrite(\n+      mlir::arith::TruncFOp op,\n+      mlir::PatternRewriter& rewriter) const override {\n+    auto src_ty = op.getOperand().getType();\n+    auto dst_ty = op.getType();\n+\n+    if (!IsNonFp8ToFp8Conversion(src_ty, dst_ty)) {\n+      return rewriter.notifyMatchFailure(\n+          op->getLoc(),\n+          \"TruncFOp will be lowered to FpToFpOp only if it converts from FP16, \"\n+          \"BF16, FP32 or FP64 to FP8.\");\n+    }\n+\n+    // TruncFOp default rounding mode is to_nearest_even based on the code in\n+    // ArithOps.cpp\n+    auto rounding_mode = op.getRoundingmode().value_or(\n+        mlir::arith::RoundingMode::to_nearest_even);\n+\n+    ttir::RoundingMode triton_rounding_mode;\n+    switch (rounding_mode) {\n+      case mlir::arith::RoundingMode::to_nearest_even:\n+        triton_rounding_mode = ttir::RoundingMode::RTNE;\n+        break;\n+      case mlir::arith::RoundingMode::toward_zero:\n+        triton_rounding_mode = ttir::RoundingMode::RTZ;\n+        break;\n+      default:\n+        return rewriter.notifyMatchFailure(\n+            op->getLoc(),\n+            \"TruncFOp rounding mode attribute not supported by \"\n+            \"FpToFpOp.\");\n+    }\n+\n+    ttir::RoundingModeAttr triton_rounding_mode_attr =\n+        ttir::RoundingModeAttr::get(rewriter.getContext(),\n+                                    triton_rounding_mode);\n+\n+    rewriter.replaceOpWithNewOp<ttir::FpToFpOp>(\n+        op, op.getType(), op.getOperand(), triton_rounding_mode_attr);\n+    return mlir::success();\n+  }\n+};\n+\n+class ArithFP8ConversionToTritonPass\n+    : public impl::ArithFP8ConversionToTritonPassBase<\n+          ArithFP8ConversionToTritonPass> {\n+ public:\n+  void runOnOperation() override {\n+    mlir::MLIRContext* mlir_context = &getContext();\n+    mlir::RewritePatternSet patterns(mlir_context);\n+    patterns.add<LowerExtFOp, LowerTruncFOp>(mlir_context);\n+    walkAndApplyPatterns(getOperation(), std::move(patterns));\n+  }\n+};\n+}  // namespace\n+\n+// F8 <-> FP16, BF16, FP32, FP64 need to be handled via Triton's tt.fp_to_fp\n+// because LLVM doesn't support casts from/to FP8.\n+// TODO(b/413272992): Add better test coverage for FpToFpOp.\n+std::unique_ptr<Pass> CreateArithFP8ConversionToTritonPass() {\n+  return std::make_unique<ArithFP8ConversionToTritonPass>();\n+}\n+\n+}  // namespace mlir::triton::xla"
        },
        {
            "sha": "2ab433be265408d04bdfc41e4f7a1ff869f9cffb",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/247b0cf254fbbf3d326feed3820ec24503a353a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/247b0cf254fbbf3d326feed3820ec24503a353a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h?ref=247b0cf254fbbf3d326feed3820ec24503a353a5",
            "patch": "@@ -53,6 +53,7 @@ std::unique_ptr<mlir::Pass> CreateTensorLowerToTritonPass();\n std::unique_ptr<mlir::Pass> CreateTritonXLAMathToLibdevicePass(\n     absl::string_view libdevice_path, absl::string_view triple);\n std::unique_ptr<mlir::Pass> CreateXTileLowerToTritonPass();\n+std::unique_ptr<mlir::Pass> CreateArithFP8ConversionToTritonPass();\n \n // Returns true if the `op` contains an operation in it's regions that satisfies\n // the `fn`."
        },
        {
            "sha": "1c4d71feb98e1b174fd0def57b6acccae7bf0f12",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.td",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/247b0cf254fbbf3d326feed3820ec24503a353a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/247b0cf254fbbf3d326feed3820ec24503a353a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td?ref=247b0cf254fbbf3d326feed3820ec24503a353a5",
            "patch": "@@ -246,7 +246,6 @@ def TritonXLAMathToLibdevicePass\n   ];\n }\n \n-\n def XTileLowerToTritonPass\n     : Pass<\"xtile-lower-to-triton\", \"mlir::ModuleOp\"> {\n   let summary = \"Lowers XTile operations to their Triton equivalent.\";\n@@ -257,4 +256,13 @@ def XTileLowerToTritonPass\n   let constructor = \"CreateXTileLowerToTritonPass()\";\n }\n \n+def ArithFP8ConversionToTritonPass\n+    : Pass<\"arith-fp8-conversion-to-triton\", \"mlir::ModuleOp\"> {\n+  let summary =\n+    \"Converts arith extf and truncf operations to their Triton equivalent.\";\n+  let dependentDialects = [\n+    \"::mlir::triton::TritonDialect\",\n+  ];\n+}\n+\n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_PASSES_TD_"
        },
        {
            "sha": "3bdb50de5b4b9f9c55bdc2e116513fc8cea72230",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/arith_fp8_conversion_to_triton.mlir",
            "status": "added",
            "additions": 57,
            "deletions": 0,
            "changes": 57,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/247b0cf254fbbf3d326feed3820ec24503a353a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Farith_fp8_conversion_to_triton.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/247b0cf254fbbf3d326feed3820ec24503a353a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Farith_fp8_conversion_to_triton.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Farith_fp8_conversion_to_triton.mlir?ref=247b0cf254fbbf3d326feed3820ec24503a353a5",
            "patch": "@@ -0,0 +1,57 @@\n+// RUN: xla-opt %s -arith-fp8-conversion-to-triton | FileCheck %s\n+\n+// CHECK-LABEL: @extf_fp8_to_bf16\n+func.func @extf_fp8_to_bf16(%arg0: tensor<16xf8E5M2>) -> tensor<16xbf16> {\n+  // CHECK: tt.fp_to_fp\n+  %0 = arith.extf %arg0 : tensor<16xf8E5M2> to tensor<16xbf16>\n+  return %0 : tensor<16xbf16>\n+}\n+\n+// CHECK-LABEL: @extf_fp8_to_f32\n+func.func @extf_fp8_to_f32(%arg0: tensor<32xf8E4M3FN>) -> tensor<32xf32> {\n+  // CHECK: tt.fp_to_fp\n+  %0 = arith.extf %arg0 : tensor<32xf8E4M3FN> to tensor<32xf32>\n+  return %0 : tensor<32xf32>\n+}\n+\n+// CHECK-LABEL: @truncf_bf16_to_fp8e5m2_round_to_nearest_even\n+func.func @truncf_bf16_to_fp8e5m2_round_to_nearest_even(%arg0: tensor<16xbf16>) -> tensor<16xf8E5M2> {\n+  // CHECK: tt.fp_to_fp %{{.*}}, rounding = rtne\n+  %0 = arith.truncf %arg0 to_nearest_even : tensor<16xbf16> to tensor<16xf8E5M2>\n+  return %0 : tensor<16xf8E5M2>\n+}\n+\n+// CHECK-LABEL: @truncf_f32_to_fp8e4m3fn_round_to_zero\n+func.func @truncf_f32_to_fp8e4m3fn_round_to_zero(%arg0: tensor<32xf32>) -> tensor<32xf8E4M3FN> {\n+  // CHECK: tt.fp_to_fp %{{.*}}, rounding = rtz\n+  %0 = arith.truncf %arg0 toward_zero : tensor<32xf32> to tensor<32xf8E4M3FN>\n+  return %0 : tensor<32xf8E4M3FN>\n+}\n+\n+// CHECK-LABEL: @truncf_f32_to_fp8e4m3fn_no_rounding_mode_uses_nearest_even\n+func.func @truncf_f32_to_fp8e4m3fn_no_rounding_mode_uses_nearest_even(%arg0: tensor<32xf32>) -> tensor<32xf8E4M3FN> {\n+  // CHECK: tt.fp_to_fp %{{.*}}, rounding = rtne\n+  %0 = arith.truncf %arg0 : tensor<32xf32> to tensor<32xf8E4M3FN>\n+  return %0 : tensor<32xf8E4M3FN>\n+}\n+\n+// CHECK-LABEL: @truncf_f32_to_fp8e4m3fn_unsupported_rounding_mode_falls_back_to_arith\n+func.func @truncf_f32_to_fp8e4m3fn_unsupported_rounding_mode_falls_back_to_arith(%arg0: tensor<32xf32>) -> tensor<32xf8E4M3FN> {\n+  // CHECK: arith.truncf\n+  %0 = arith.truncf %arg0 upward : tensor<32xf32> to tensor<32xf8E4M3FN>\n+  return %0 : tensor<32xf8E4M3FN>\n+}\n+\n+// CHECK-LABEL: @truncf_f32_to_f16_falls_back_to_arith\n+func.func @truncf_f32_to_f16_falls_back_to_arith(%arg0: tensor<32xf32>) -> tensor<32xf16> {\n+  // CHECK: arith.truncf\n+  %0 = arith.truncf %arg0 to_nearest_even : tensor<32xf32> to tensor<32xf16>\n+  return %0 : tensor<32xf16>\n+}\n+\n+// CHECK-LABEL: @extf_f16_to_f64_falls_back_to_arith\n+func.func @extf_f16_to_f64_falls_back_to_arith(%arg0: tensor<32xf16>) -> tensor<32xf64> {\n+  // CHECK: arith.extf\n+  %0 = arith.extf %arg0 : tensor<32xf16> to tensor<32xf64>\n+  return %0 : tensor<32xf64>\n+}"
        }
    ],
    "stats": {
        "total": 232,
        "additions": 216,
        "deletions": 16
    }
}