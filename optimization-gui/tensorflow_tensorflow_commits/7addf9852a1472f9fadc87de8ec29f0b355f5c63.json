{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 845646520",
    "sha": "7addf9852a1472f9fadc87de8ec29f0b355f5c63",
    "files": [
        {
            "sha": "98a488d4209a9bb37260969e4ba3a61c5b8851a0",
            "filename": "tensorflow/core/graph/benchmark_testlib.h",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fbenchmark_testlib.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fbenchmark_testlib.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fbenchmark_testlib.h?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -73,7 +73,7 @@ inline GraphDef CreateGraphDef(int num_nodes, int num_edges_per_node) {\n   const int kNumInNodes = 10 * num_edges_per_node;\n   GraphDef graph_def;\n \n-  auto create_node = [](const string& name, const string& op) {\n+  auto create_node = [](const std::string& name, const std::string& op) {\n     NodeDef node;\n     node.set_name(name);\n     node.set_op(op);\n@@ -115,17 +115,17 @@ inline GraphDef CreateRandomGraph(int size) {\n   random::PhiloxRandom philox(0x12345);\n   random::SimplePhilox rnd(&philox);\n \n-  string prefix = \"long_node_name_prefix_to_measure_string_copy_overhead\";\n+  std::string prefix = \"long_node_name_prefix_to_measure_string_copy_overhead\";\n \n   GraphDef graph;\n   for (int i = 0; i < size; ++i) {\n-    const string name = absl::StrCat(prefix, i);\n-    const uint32 num_inputs = rnd.Uniform(std::min(i, 5));\n+    const std::string name = absl::StrCat(prefix, i);\n+    const uint32_t num_inputs = rnd.Uniform(std::min(i, 5));\n \n     NodeDef node;\n     node.set_name(name);\n     for (int n = 0; n < num_inputs; ++n) {\n-      const uint32 input_node = rnd.Uniform(i);\n+      const uint32_t input_node = rnd.Uniform(i);\n       node.add_input(absl::StrCat(prefix, input_node));\n     }\n \n@@ -142,7 +142,7 @@ inline GraphDef CreateFaninFanoutNodeGraph(int num_regular_fanins,\n                                            bool fanout_unique_index) {\n   GraphDef graph;\n \n-  auto create_node = [](const string& name) {\n+  auto create_node = [](const std::string& name) {\n     NodeDef node;\n     node.set_name(name);\n     return node;\n@@ -151,28 +151,28 @@ inline GraphDef CreateFaninFanoutNodeGraph(int num_regular_fanins,\n   NodeDef node = create_node(/*name=*/\"node\");\n \n   for (int i = 0; i < num_regular_fanins; ++i) {\n-    const string input_node_name = absl::StrFormat(\"in%05d\", i);\n+    const std::string input_node_name = absl::StrFormat(\"in%05d\", i);\n     NodeDef input_node = create_node(/*name=*/input_node_name);\n     *graph.add_node() = std::move(input_node);\n     node.add_input(input_node_name);\n   }\n \n   for (int i = 0; i < num_controlling_fanins; ++i) {\n-    const string input_node_name = absl::StrFormat(\"control_in%05d\", i);\n+    const std::string input_node_name = absl::StrFormat(\"control_in%05d\", i);\n     NodeDef input_node = create_node(/*name=*/input_node_name);\n     *graph.add_node() = std::move(input_node);\n     node.add_input(absl::StrCat(\"^\", input_node_name));\n   }\n \n   for (int i = 0; i < num_regular_fanouts; ++i) {\n     NodeDef output_node = create_node(/*name=*/absl::StrFormat(\"out%05d\", i));\n-    const string input_node_index =\n+    const std::string input_node_index =\n         fanout_unique_index ? absl::StrCat(node.name(), \":\", i) : node.name();\n     output_node.add_input(input_node_index);\n     *graph.add_node() = std::move(output_node);\n   }\n \n-  const string controlled_fanout_input = absl::StrCat(\"^\", node.name());\n+  const std::string controlled_fanout_input = absl::StrCat(\"^\", node.name());\n   for (int i = 0; i < num_controlled_fanouts; ++i) {\n     NodeDef output_node =\n         create_node(/*name=*/absl::StrFormat(\"control_out%05d\", i));"
        },
        {
            "sha": "3ca3748eeb18be5f0d8dc59683d2d66e12e6e82a",
            "filename": "tensorflow/core/graph/collective_order.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fcollective_order.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fcollective_order.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fcollective_order.cc?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -25,8 +25,9 @@ namespace {\n // them.\n absl::Status DiscoverDataDependencies(\n     const Graph* graph, std::vector<Node*>* collective_nodes,\n-    std::vector<int32>* instance_keys,\n-    absl::flat_hash_map<Node*, absl::flat_hash_set<int32>>* data_dependencies) {\n+    std::vector<int32_t>* instance_keys,\n+    absl::flat_hash_map<Node*, absl::flat_hash_set<int32_t>>*\n+        data_dependencies) {\n   absl::Status s;\n   // Algorithm: do Reverse DFS starting at sink.  `node_leave` is called when\n   // all parents of `node` have been visited.  At that point,\n@@ -69,8 +70,8 @@ absl::Status DiscoverDataDependencies(\n // If there exists an edge a -> b then `dependency_edges[a]` contains `b`\n absl::Status CreateControlDependencies(\n     const std::vector<Node*>& collective_nodes,\n-    const std::vector<int32>& instance_keys,\n-    absl::flat_hash_map<Node*, absl::flat_hash_set<int32>>* data_dependencies,\n+    const std::vector<int32_t>& instance_keys,\n+    absl::flat_hash_map<Node*, absl::flat_hash_set<int32_t>>* data_dependencies,\n     absl::flat_hash_map<Node*, absl::flat_hash_set<Node*>>* dependency_edges) {\n   // If there exists some path a -> ... -> b then `all_paths[a]` contains `b`\n   absl::flat_hash_map<Node*, absl::flat_hash_set<Node*>> all_paths;\n@@ -158,7 +159,7 @@ absl::Status InsertControlDependencies(\n   } else if (order_type == GraphCollectiveOrder::kAttrs) {\n     // `wait_for` is the inverse of `dependency_edges`, i.e. `wait_for[node]`\n     // contains the list of instance keys for which `node` must wait.\n-    absl::flat_hash_map<Node*, absl::flat_hash_set<int32>> wait_for;\n+    absl::flat_hash_map<Node*, absl::flat_hash_set<int32_t>> wait_for;\n     for (const auto& pair : dependency_edges) {\n       int32_t src_instance;\n       TF_RETURN_IF_ERROR(\n@@ -168,7 +169,8 @@ absl::Status InsertControlDependencies(\n       }\n     }\n     for (const auto& pair : wait_for) {\n-      std::vector<int32> wait_for_list(pair.second.begin(), pair.second.end());\n+      std::vector<int32_t> wait_for_list(pair.second.begin(),\n+                                         pair.second.end());\n       pair.first->ClearAttr(\"wait_for\");\n       pair.first->AddAttr(\"wait_for\", wait_for_list);\n     }\n@@ -184,9 +186,9 @@ absl::Status InsertControlDependencies(\n absl::Status OrderCollectives(Graph* graph, GraphCollectiveOrder order_type) {\n   // `instance_keys[i]` corresponds to `collective_nodes[i]`\n   std::vector<Node*> collective_nodes;\n-  std::vector<int32> instance_keys;\n+  std::vector<int32_t> instance_keys;\n   // node -> set of collectives on which node depends.\n-  absl::flat_hash_map<Node*, absl::flat_hash_set<int32>> data_dependencies;\n+  absl::flat_hash_map<Node*, absl::flat_hash_set<int32_t>> data_dependencies;\n   TF_RETURN_IF_ERROR(DiscoverDataDependencies(\n       graph, &collective_nodes, &instance_keys, &data_dependencies));\n "
        },
        {
            "sha": "e443dadc678c266dc64a0e83d0513062252a54a4",
            "filename": "tensorflow/core/graph/control_flow.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fcontrol_flow.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fcontrol_flow.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fcontrol_flow.cc?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -27,7 +27,7 @@ namespace tensorflow {\n namespace {\n // Information about a loop frame structure.\n struct Frame {\n-  string name;\n+  std::string name;\n \n   // Pointer to the parent frame. The root frame has a pointer to itself.\n   Frame* parent = nullptr;\n@@ -40,7 +40,7 @@ struct Frame {\n // Verify that the ControlFlowInfo of the graph has valid loop structure.\n absl::Status ValidateControlFlowInfo(\n     const Graph* graph, const std::vector<ControlFlowInfo>& cf_info) {\n-  std::unordered_map<string, Frame> frames;\n+  std::unordered_map<std::string, Frame> frames;\n   for (const Node* node : graph->op_nodes()) {\n     const ControlFlowInfo& cf = cf_info[node->id()];\n     if (!cf.frame || !cf.parent_frame) {\n@@ -85,7 +85,7 @@ absl::Status ValidateControlFlowInfo(\n \n absl::Status BuildControlFlowInfo(const Graph* g,\n                                   std::vector<ControlFlowInfo>* info,\n-                                  std::vector<string>* unreachable_nodes) {\n+                                  std::vector<std::string>* unreachable_nodes) {\n   info->clear();\n   info->resize(g->num_node_ids());\n \n@@ -97,7 +97,7 @@ absl::Status BuildControlFlowInfo(const Graph* g,\n   src_info.frame = src_node;\n   src_info.parent_frame = src_node;\n \n-  string frame_name;\n+  std::string frame_name;\n   std::deque<const Node*> ready;\n   ready.push_back(src_node);\n   while (!ready.empty()) {\n@@ -135,7 +135,8 @@ absl::Status BuildControlFlowInfo(const Graph* g,\n       // Process the node 'out'.\n       if (IsEnter(out)) {\n         if (is_visited) {\n-          const string& parent_frame = (*info)[out_parent->id()].frame_name;\n+          const std::string& parent_frame =\n+              (*info)[out_parent->id()].frame_name;\n           if (parent_frame != frame_name) {\n             return errors::InvalidArgument(\n                 FormatNodeForError(*out),"
        },
        {
            "sha": "b15bb671f7e1ceea122ce831cca35e5312cb6e92",
            "filename": "tensorflow/core/graph/control_flow.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fcontrol_flow.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fcontrol_flow.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fcontrol_flow.h?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -36,7 +36,7 @@ struct ControlFlowInfo {\n \n   const Node* frame = nullptr;         // frame of a node\n   const Node* parent_frame = nullptr;  // parent frame of a node\n-  string frame_name;                   // frame name of a node\n+  std::string frame_name;              // frame name of a node\n };\n \n // Clear and populate `info` with each node's frame and the level it belongs to.\n@@ -54,7 +54,7 @@ struct ControlFlowInfo {\n // which all sane front-ends should satisfy.\n absl::Status BuildControlFlowInfo(\n     const Graph* g, std::vector<ControlFlowInfo>* info,\n-    std::vector<string>* unreachable_nodes = nullptr);\n+    std::vector<std::string>* unreachable_nodes = nullptr);\n \n }  // namespace tensorflow\n "
        },
        {
            "sha": "6026522f28cfb0fb55cdc267ca97818daeb10e35",
            "filename": "tensorflow/core/graph/costmodel.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fcostmodel.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fcostmodel.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fcostmodel.cc?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -35,7 +35,7 @@ void CostModel::SuppressInfrequent() {\n   // Find the median of the non-zero counts, and use half of its value\n   // as the cutoff for a \"normal\" execution mode node.\n   if (count_.empty()) return;\n-  std::vector<int32> non_zero;\n+  std::vector<int32_t> non_zero;\n   for (auto v : count_) {\n     if (v > 0) non_zero.push_back(v);\n   }\n@@ -192,7 +192,7 @@ void CostModel::RecordCount(const Node* node, int count) {\n   count_[id] += count;\n }\n \n-int32 CostModel::TotalCount(const Node* node) const {\n+int32_t CostModel::TotalCount(const Node* node) const {\n   const int id = Id(node);\n   if (id < 0) return 0;\n   return (static_cast<size_t>(id) < slot_bytes_.size()) ? count_[id] : 0;\n@@ -419,7 +419,7 @@ Microseconds CostModel::ComputationTimeEstimate(int64_t math_ops) {\n \n void CostModel::IncrementUpdateTimes() { update_times_++; }\n \n-int32 CostModel::GetUpdateTimes() const { return update_times_; }\n+int32_t CostModel::GetUpdateTimes() const { return update_times_; }\n \n // ----------------------------------------------------------------------------\n // InitCostModel"
        },
        {
            "sha": "9bfd9b2a60ce1b018456b08fcf2767e229362f34",
            "filename": "tensorflow/core/graph/costmodel.h",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fcostmodel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fcostmodel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fcostmodel.h?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -31,7 +31,7 @@ limitations under the License.\n #include \"tensorflow/core/platform/protobuf.h\"\n \n namespace tensorflow {\n-typedef std::unordered_map<absl::string_view, int32, StringPieceHasher>\n+typedef std::unordered_map<absl::string_view, int32_t, StringPieceHasher>\n     NodeNameToCostIdMap;\n \n class StepStats;\n@@ -95,7 +95,7 @@ class CostModel {\n   void RecordCount(const Node* node, int num_count);\n \n   // Returns how many times \"node\" has been executed.\n-  int32 TotalCount(const Node* node) const;\n+  int32_t TotalCount(const Node* node) const;\n \n   // Records that \"output_slot\" of \"node\" has produced tensors of\n   // aggregated \"bytes\".\n@@ -184,7 +184,7 @@ class CostModel {\n   void IncrementUpdateTimes();\n \n   // Get the times that the cost model is updated.\n-  int32 GetUpdateTimes() const;\n+  int32_t GetUpdateTimes() const;\n \n  private:\n   static Bytes MinTensorMemoryUsage(const TensorShapeProto& tensor_shape,\n@@ -197,13 +197,13 @@ class CostModel {\n \n   // Nodes and Edges whose count is < this value\n   // get type/byte estimates of 0.\n-  int32 min_count_ = 0;\n+  int32_t min_count_ = 0;\n \n   // The number of times the cost model is updated.\n-  int32 update_times_ = 0;\n+  int32_t update_times_ = 0;\n \n   // Number of times each Node has been executed.\n-  std::vector<int32> count_;\n+  std::vector<int32_t> count_;\n   // Cumulative execution time.\n   std::vector<Microseconds> time_;\n   // Cumulative Bytes output on each channel."
        },
        {
            "sha": "c062f58856523b3b25c91a7ce302e998eb7a5aa3",
            "filename": "tensorflow/core/graph/costmodel_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fcostmodel_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fcostmodel_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fcostmodel_test.cc?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -56,7 +56,7 @@ MATCHER_P(ShapeProtoEquals, other, \"\") {\n   return true;\n }\n \n-static void InitGraph(const string& s, Graph* graph) {\n+static void InitGraph(const std::string& s, Graph* graph) {\n   GraphDef graph_def;\n \n   auto parser = protobuf::TextFormat::Parser();\n@@ -97,8 +97,8 @@ Node* FindNode(const Graph& graph, std::string name) {\n   return nullptr;\n }\n \n-Node* AddNode(Graph& graph, const string& name, const string& node_type,\n-              int num_inputs) {\n+Node* AddNode(Graph& graph, const std::string& name,\n+              const std::string& node_type, int num_inputs) {\n   auto builder = NodeDefBuilder(name, node_type);\n   for (int i = 0; i < num_inputs; ++i) {\n     builder = builder.Input(absl::StrCat(\"node_\", i), i, DT_FLOAT);\n@@ -114,7 +114,7 @@ Node* AddNode(Graph& graph, const string& name, const string& node_type,\n }\n \n static void GenerateStepStats(Graph* graph, StepStats* step_stats,\n-                              const string& device_name) {\n+                              const std::string& device_name) {\n   // Fill RunMetadata's step_stats and partition_graphs fields.\n   DeviceStepStats* device_stepstats = step_stats->add_dev_stats();\n   device_stepstats->set_device(device_name);\n@@ -150,7 +150,7 @@ TEST(CostModelTest, WorksWithManager) {\n   GenerateStepStats(graph1.get(), &step_stats, \"DummyDevice1\");\n   GenerateStepStats(graph2.get(), &step_stats, \"DummyDevice2\");\n   StepStatsCollector collector(&step_stats);\n-  std::unordered_map<string, const Graph*> device_map;\n+  std::unordered_map<std::string, const Graph*> device_map;\n   device_map[\"DummyDevice1\"] = graph1.get();\n   device_map[\"DummyDevice2\"] = graph2.get();\n   CostModelManager cost_model_manager;\n@@ -161,7 +161,7 @@ TEST(CostModelTest, WorksWithManager) {\n   TF_ASSERT_OK(\n       cost_model_manager.AddToCostGraphDef(graph2.get(), &cost_graph_def));\n   ASSERT_EQ(cost_graph_def.node_size(), 12);\n-  absl::flat_hash_map<int32, const CostGraphDef::Node> ids;\n+  absl::flat_hash_map<int32_t, const CostGraphDef::Node> ids;\n   for (auto node : cost_graph_def.node()) {\n     int32_t index = node.id();\n     auto result = ids.insert({index, node});"
        },
        {
            "sha": "e3f50ef59484ea65c8b3e864f50e833650038390",
            "filename": "tensorflow/core/graph/edgeset.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fedgeset.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fedgeset.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fedgeset.h?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -82,7 +82,7 @@ class EdgeSet {\n #ifdef NDEBUG\n   void RegisterMutation() {}\n #else\n-  uint32 mutations_ = 0;\n+  uint32_t mutations_ = 0;\n   void RegisterMutation() { mutations_++; }\n #endif\n \n@@ -127,7 +127,7 @@ class EdgeSet::const_iterator {\n     CHECK_EQ(init_mutations_, owner_->mutations_);\n   }\n   const EdgeSet* owner_ = nullptr;\n-  uint32 init_mutations_ = 0;\n+  uint32_t init_mutations_ = 0;\n #endif\n };\n "
        },
        {
            "sha": "c7acee2bd056ebdf49796bb7e772a109e1c7b79d",
            "filename": "tensorflow/core/graph/graph.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fgraph.cc?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -190,7 +190,7 @@ void Node::ClearTypeInfo() {\n \n absl::Status Node::ShrinkTypeInfo(\n     const absl::flat_hash_map<int, int>& index_mapping,\n-    const string& type_attr_name, bool update_full_type) {\n+    const std::string& type_attr_name, bool update_full_type) {\n   std::vector<DataType> dtypes;\n   TF_RETURN_IF_ERROR(GetNodeAttr(def(), type_attr_name, &dtypes));\n \n@@ -239,11 +239,11 @@ const OpDef& Node::op_def() const { return *props_->op_def; }\n \n NodeDef* Node::mutable_def() { return &props_->node_def; }\n \n-int32 Node::num_inputs() const { return props_->input_types.size(); }\n+int32_t Node::num_inputs() const { return props_->input_types.size(); }\n DataType Node::input_type(int32_t i) const { return props_->input_types[i]; }\n const DataTypeVector& Node::input_types() const { return props_->input_types; }\n \n-int32 Node::num_outputs() const { return props_->output_types.size(); }\n+int32_t Node::num_outputs() const { return props_->output_types.size(); }\n DataType Node::output_type(int32_t o) const { return props_->output_types[o]; }\n const DataTypeVector& Node::output_types() const {\n   return props_->output_types;\n@@ -416,7 +416,7 @@ bool InputTensor::operator==(const InputTensor& other) const {\n   return node == other.node && index == other.index;\n }\n \n-uint64 InputTensor::Hash::operator()(InputTensor const& s) const {\n+uint64_t InputTensor::Hash::operator()(InputTensor const& s) const {\n   return Hash64Combine(std::hash<const Node*>()(s.node),\n                        std::hash<int>()(s.index));\n }\n@@ -427,7 +427,7 @@ bool OutputTensor::operator==(const OutputTensor& other) const {\n   return node == other.node && index == other.index;\n }\n \n-uint64 OutputTensor::Hash::operator()(OutputTensor const& s) const {\n+uint64_t OutputTensor::Hash::operator()(OutputTensor const& s) const {\n   return Hash64Combine(std::hash<const Node*>()(s.node),\n                        std::hash<int>()(s.index));\n }\n@@ -1086,7 +1086,7 @@ GraphDebugInfo Graph::BuildDebugInfo() const {\n std::string Edge::DebugString() const {\n   auto src_name = src_ ? src_->name().c_str() : \"<NULL>\";\n   auto dst_name = dst_ ? dst_->name().c_str() : \"<NULL>\";\n-  return strings::Printf(\"[id=%d %s:%d -> %s:%d]\", id_, src_name, src_output_,\n+  return absl::StrFormat(\"[id=%d %s:%d -> %s:%d]\", id_, src_name, src_output_,\n                          dst_name, dst_input_);\n }\n "
        },
        {
            "sha": "10b29e0975625f03570740a98bf163a98cf5403e",
            "filename": "tensorflow/core/graph/graph.h",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fgraph.h?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -107,11 +107,11 @@ class Node {\n   NodeDef* mutable_def();\n \n   // input and output types\n-  int32 num_inputs() const;\n+  int32_t num_inputs() const;\n   DataType input_type(int32_t i) const;\n   const DataTypeVector& input_types() const;\n \n-  int32 num_outputs() const;\n+  int32_t num_outputs() const;\n   DataType output_type(int32_t o) const;\n   const DataTypeVector& output_types() const;\n \n@@ -139,14 +139,14 @@ class Node {\n \n   // Sets 'original_node_names' field of this node's DebugInfo proto to\n   // 'names'.\n-  void set_original_node_names(const std::vector<string>& names);\n-  void set_original_func_names(const std::vector<string>& names);\n+  void set_original_node_names(const std::vector<std::string>& names);\n+  void set_original_func_names(const std::vector<std::string>& names);\n \n   // Read only access to attributes\n   AttrSlice attrs() const;\n \n   // Inputs requested by the NodeDef.  For the actual inputs, use in_edges.\n-  const protobuf::RepeatedPtrField<string>& requested_inputs() const;\n+  const protobuf::RepeatedPtrField<std::string>& requested_inputs() const;\n \n   // Get the neighboring nodes via edges either in or out of this node.  This\n   // includes control edges.\n@@ -220,7 +220,7 @@ class Node {\n     UpdateProperties();\n   }\n \n-  void AddAttr(const std::string& name, std::vector<string>&& val) {\n+  void AddAttr(const std::string& name, std::vector<std::string>&& val) {\n     MoveAttrValue(std::move(val), AddAttrHelper(name));\n     UpdateProperties();\n   }\n@@ -278,7 +278,7 @@ class Node {\n   // update the node's full type information (if present).\n   absl::Status ShrinkTypeInfo(\n       const absl::flat_hash_map<int, int>& index_mapping,\n-      const string& type_attr_name, bool update_full_type);\n+      const std::string& type_attr_name, bool update_full_type);\n \n   // Called after an incident non-control edge has changed. Does nothing if not\n   // all input edges are defined.\n@@ -383,8 +383,8 @@ class Node {\n // Stores debug information associated with the Node.\n struct NodeDebugInfo {\n   const std::string name;\n-  std::vector<string> original_node_names;\n-  std::vector<string> original_func_names;\n+  std::vector<std::string> original_node_names;\n+  std::vector<std::string> original_func_names;\n \n   NodeDebugInfo(const Node& n);\n   NodeDebugInfo(const NodeDef& ndef);\n@@ -407,7 +407,7 @@ struct InputTensor {\n   // A hash function for InputTensors. Nodes are hashed based on their pointer\n   // value.\n   struct Hash {\n-    uint64 operator()(InputTensor const& s) const;\n+    uint64_t operator()(InputTensor const& s) const;\n   };\n };\n \n@@ -428,7 +428,7 @@ struct OutputTensor {\n   // A hash function for OutputTensors. Nodes are hashed based on their pointer\n   // value.\n   struct Hash {\n-    uint64 operator()(OutputTensor const& s) const;\n+    uint64_t operator()(OutputTensor const& s) const;\n   };\n };\n \n@@ -803,7 +803,7 @@ class Graph {\n                                WhileContext** result);\n \n   // Builds a node name to node pointer index for all nodes in the graph.\n-  std::unordered_map<string, Node*> BuildNodeNameIndex() const;\n+  std::unordered_map<std::string, Node*> BuildNodeNameIndex() const;\n \n   absl::optional<std::vector<bool>>& GetConstArgIndicesCache() const {\n     return const_arg_indices_cache_;\n@@ -906,16 +906,16 @@ class Graph {\n \n   // A table of the unique assigned device names.  Indices do NOT correspond\n   // to node IDs.  Index 0 is always the empty string.\n-  std::vector<string> device_names_;\n+  std::vector<std::string> device_names_;\n \n   // Maps unique device names to indices within device_names_[i].\n-  std::unordered_map<string, int> device_names_map_;\n+  std::unordered_map<std::string, int> device_names_map_;\n \n   // All the while contexts owned by this graph, keyed by frame name,\n   // corresponding to all the while loops contained in this graph (including\n   // nested loops). The stored contexts are usually accessed via\n   // AddWhileContext() or Node::while_ctx(), but this manages the lifetime.\n-  std::map<string, WhileContext> while_ctxs_;\n+  std::map<std::string, WhileContext> while_ctxs_;\n \n   // Cache of the indices of the arguments which need to be constant for the XLA\n   // compilation."
        },
        {
            "sha": "5680800a5592c593dc0b540622ba5793f7bfc9d8",
            "filename": "tensorflow/core/graph/graph_debug_info_builder_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph_debug_info_builder_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph_debug_info_builder_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fgraph_debug_info_builder_test.cc?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -47,7 +47,7 @@ class TestStackTrace : public AbstractStackTrace {\n \n   StackFrame LastUserFrame() const override { return frames_.back(); }\n \n-  string ToString(const TracePrintingOptions& opts) const override {\n+  std::string ToString(const TracePrintingOptions& opts) const override {\n     auto frame = LastUserFrame();\n     return absl::StrCat(frame.file_name, \":\", frame.line_number, \":\",\n                         frame.function_name);"
        },
        {
            "sha": "a4f08eab66b09059a4e7df8680310cd9cbd35bf2",
            "filename": "tensorflow/core/graph/graph_def_builder.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph_def_builder.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph_def_builder.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fgraph_def_builder.cc?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -44,12 +44,12 @@ GraphDefBuilder::Options GraphDefBuilder::Options::WithControlInputs(\n }\n GraphDefBuilder::Options GraphDefBuilder::Options::WithNameImpl(\n     absl::string_view name) {\n-  name_ = string(name);\n+  name_ = std::string(name);\n   return *this;\n }\n GraphDefBuilder::Options GraphDefBuilder::Options::WithDeviceImpl(\n     absl::string_view device) {\n-  device_ = string(device);\n+  device_ = std::string(device);\n   return *this;\n }\n GraphDefBuilder::Options GraphDefBuilder::Options::WithControlInputImpl(\n@@ -72,7 +72,7 @@ absl::Status GraphDefBuilder::ToGraphDef(GraphDef* graph_def) const {\n   return status_;\n }\n \n-string GraphDefBuilder::Options::GetNameForOp(absl::string_view op) const {\n+std::string GraphDefBuilder::Options::GetNameForOp(absl::string_view op) const {\n   if (name_.empty()) return graph_->NewName(op);\n   return name_;\n }\n@@ -99,14 +99,15 @@ void GraphDefBuilder::Options::UpdateStatus(const absl::Status& status) const {\n \n namespace ops {\n \n-Node* SourceOp(const string& op_name, const GraphDefBuilder::Options& opts) {\n+Node* SourceOp(const std::string& op_name,\n+               const GraphDefBuilder::Options& opts) {\n   if (opts.HaveError()) return nullptr;\n   NodeBuilder node_builder(opts.GetNameForOp(op_name), op_name,\n                            opts.op_registry());\n   return opts.FinalizeBuilder(&node_builder);\n }\n \n-Node* UnaryOp(const string& op_name, NodeOut input,\n+Node* UnaryOp(const std::string& op_name, NodeOut input,\n               const GraphDefBuilder::Options& opts) {\n   if (opts.HaveError()) return nullptr;\n   NodeBuilder node_builder(opts.GetNameForOp(op_name), op_name,\n@@ -115,7 +116,7 @@ Node* UnaryOp(const string& op_name, NodeOut input,\n   return opts.FinalizeBuilder(&node_builder);\n }\n \n-Node* BinaryOp(const string& op_name, NodeOut a, NodeOut b,\n+Node* BinaryOp(const std::string& op_name, NodeOut a, NodeOut b,\n                const GraphDefBuilder::Options& opts) {\n   if (opts.HaveError()) return nullptr;\n   NodeBuilder node_builder(opts.GetNameForOp(op_name), op_name,\n@@ -124,7 +125,7 @@ Node* BinaryOp(const string& op_name, NodeOut a, NodeOut b,\n   return opts.FinalizeBuilder(&node_builder);\n }\n \n-Node* TernaryOp(const string& op_name, NodeOut a, NodeOut b, NodeOut c,\n+Node* TernaryOp(const std::string& op_name, NodeOut a, NodeOut b, NodeOut c,\n                 const GraphDefBuilder::Options& opts) {\n   if (opts.HaveError()) return nullptr;\n   NodeBuilder node_builder(opts.GetNameForOp(op_name), op_name,"
        },
        {
            "sha": "afe3aebe55d62ce9c6bfdfe5c668441f222d977a",
            "filename": "tensorflow/core/graph/graph_def_builder.h",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph_def_builder.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph_def_builder.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fgraph_def_builder.h?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -104,14 +104,14 @@ class GraphDefBuilder {\n \n     // Returns a string representation of the status associated with *this.\n     // Returns the string `\"OK\"` if the status doesn't have any error.\n-    string StatusToString() const {\n+    std::string StatusToString() const {\n       return status_->ok() ? \"OK\" : std::string(status_->message());\n     }\n \n     // Given the Op type name, return a name for a node of that type.\n     // Uses the value set in WithName() if that has been called.  Otherwise,\n     // returns a name built out of the Op type name.\n-    string GetNameForOp(absl::string_view op) const;\n+    std::string GetNameForOp(absl::string_view op) const;\n \n     // Sets the device, adds control inputs, adds attrs, and calls Finalize().\n     // If Finalize returns an error, it is saved and this function returns\n@@ -133,17 +133,17 @@ class GraphDefBuilder {\n     Options WithControlInputsImpl(absl::Span<Node* const> control_inputs);\n     template <class T>\n     Options WithAttrImpl(absl::string_view name, T&& value) {\n-      attrs_.emplace_back(string(name), AttrValue());\n+      attrs_.emplace_back(std::string(name), AttrValue());\n       SetAttrValue(std::forward<T>(value), &attrs_.back().second);\n       return *this;\n     }\n \n     Graph* const graph_;\n     absl::Status* const status_;\n-    string name_;\n-    string device_;\n+    std::string name_;\n+    std::string device_;\n     std::vector<Node*> control_inputs_;\n-    std::vector<std::pair<string, AttrValue>> attrs_;\n+    std::vector<std::pair<std::string, AttrValue>> attrs_;\n   };\n \n   // Start building a new graph.\n@@ -176,7 +176,7 @@ class GraphDefBuilder {\n \n   // Returns whether a user-defined function with `name` already exists in the\n   // graph.\n-  bool HasFunction(const string& name) {\n+  bool HasFunction(const std::string& name) {\n     return flib_def_.Find(name) != nullptr;\n   }\n \n@@ -196,18 +196,19 @@ namespace ops {\n typedef NodeBuilder::NodeOut NodeOut;\n \n // For adding an Op with no inputs to a GraphDefBuilder.\n-Node* SourceOp(const string& op_name, const GraphDefBuilder::Options& opts);\n+Node* SourceOp(const std::string& op_name,\n+               const GraphDefBuilder::Options& opts);\n \n // For adding an Op with one input to a GraphDefBuilder.\n-Node* UnaryOp(const string& op_name, NodeOut input,\n+Node* UnaryOp(const std::string& op_name, NodeOut input,\n               const GraphDefBuilder::Options& opts);\n \n // For adding an Op with two inputs to a GraphDefBuilder.\n-Node* BinaryOp(const string& op_name, NodeOut a, NodeOut b,\n+Node* BinaryOp(const std::string& op_name, NodeOut a, NodeOut b,\n                const GraphDefBuilder::Options& opts);\n \n // For adding an Op with three inputs to a GraphDefBuilder.\n-Node* TernaryOp(const string& op_name, NodeOut a, NodeOut b, NodeOut c,\n+Node* TernaryOp(const std::string& op_name, NodeOut a, NodeOut b, NodeOut c,\n                 const GraphDefBuilder::Options& opts);\n \n }  // namespace ops"
        },
        {
            "sha": "ed6a23e3813d802f8caac3b73b461202a46e2a8a",
            "filename": "tensorflow/core/graph/graph_node_util.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph_node_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph_node_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fgraph_node_util.cc?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -25,9 +25,11 @@ limitations under the License.\n \n namespace tensorflow {\n \n-string SummarizeNode(const Node& node) { return SummarizeNodeDef(node.def()); }\n+std::string SummarizeNode(const Node& node) {\n+  return SummarizeNodeDef(node.def());\n+}\n \n-string FormatNodeForError(const Node& node) {\n+std::string FormatNodeForError(const Node& node) {\n   return FormatNodeDefForError(node.def());\n }\n \n@@ -41,9 +43,10 @@ absl::Status AttachDef(const absl::Status& status, const Node& node,\n   return AttachDef(status, node.def(), allow_multiple_formatted_node);\n }\n \n-absl::btree_set<string> GetMergedNames(const std::vector<string>& from_names,\n-                                       const std::vector<string>& to_names) {\n-  absl::btree_set<string> merged_names;\n+absl::btree_set<std::string> GetMergedNames(\n+    const std::vector<std::string>& from_names,\n+    const std::vector<std::string>& to_names) {\n+  absl::btree_set<std::string> merged_names;\n   merged_names.insert(from_names.begin(), from_names.end());\n   merged_names.insert(to_names.begin(), to_names.end());\n   return merged_names;"
        },
        {
            "sha": "8d7a44c5fed2e03d4aa3dcb468be562042dc8246",
            "filename": "tensorflow/core/graph/graph_node_util.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph_node_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph_node_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fgraph_node_util.h?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -29,12 +29,12 @@ class OpDef;\n \n // Produce a human-readable version of a Node or NodeDef that is more concise\n // than a text-format proto.\n-string SummarizeNode(const Node& node);\n+std::string SummarizeNode(const Node& node);\n \n // Produces a formatted string pattern from the node which can uniquely identify\n // this node upstream to produce an informative error message. The pattern\n // followed is: {{node <node_name>}}\n-string FormatNodeForError(const Node& node);\n+std::string FormatNodeForError(const Node& node);\n \n // Merges the original node names from the debug information of 'from' to the\n // debug information of 'to'."
        },
        {
            "sha": "1328c5c8b57b4c2e8439b6744ec40e4d7955b72d",
            "filename": "tensorflow/core/graph/graph_partition.cc",
            "status": "modified",
            "additions": 64,
            "deletions": 56,
            "changes": 120,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph_partition.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph_partition.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fgraph_partition.cc?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -160,16 +160,17 @@ bool IsDstInputOnHost(const Edge* edge, const GraphInfo& info) {\n \n // Add a control edge from each input to each recv.\n void AddReadControl(const std::vector<NodeDef*>& recvs,\n-                    const std::vector<string>& inputs) {\n+                    const std::vector<std::string>& inputs) {\n   for (NodeDef* recv : recvs) {\n-    for (const string& input : inputs) {\n+    for (const std::string& input : inputs) {\n       recv->add_input(absl::StrCat(\"^\", input));\n     }\n   }\n }\n \n void SetSendRecvAttrs(const PartitionOptions& opts, const Edge* edge,\n-                      const string& tensor_name_attr, NodeDefBuilder* builder) {\n+                      const std::string& tensor_name_attr,\n+                      NodeDefBuilder* builder) {\n   builder->Attr(\"tensor_name\", tensor_name_attr);\n   builder->Attr(\"send_device\", edge->src()->assigned_device_name());\n   builder->Attr(\"send_device_incarnation\",\n@@ -184,7 +185,7 @@ void SetSendRecvAttrs(const PartitionOptions& opts, const Edge* edge,\n NodeDef* AddSend(const PartitionOptions& opts, const GraphInfo& g_info,\n                  GraphDef* gdef, const Edge* edge,\n                  NodeDefBuilder::NodeOut send_from, int64_t start_time,\n-                 const string& tensor_name_attr, absl::Status* status) {\n+                 const std::string& tensor_name_attr, absl::Status* status) {\n   const DataType dtype = send_from.data_type;\n   const DataType cast_dtype = opts.should_cast ? opts.should_cast(edge) : dtype;\n   const Node* src = edge->src();\n@@ -201,7 +202,7 @@ NodeDef* AddSend(const PartitionOptions& opts, const GraphInfo& g_info,\n   // Add a cast node that casts dtype to cast_dtype.\n   // NOTE(yuanbyu): Only cast for cross-device send/recv.\n   if (dtype != cast_dtype && !NeedSameDeviceSendRecv(edge, g_info)) {\n-    const string cast_op = (host_memory) ? \"_HostCast\" : \"Cast\";\n+    const std::string cast_op = (host_memory) ? \"_HostCast\" : \"Cast\";\n     NodeDefBuilder cast_builder(opts.new_name(src->name()), cast_op,\n                                 NodeDebugInfo(*src));\n     cast_builder.Device(src->assigned_device_name()).Input(send_from);\n@@ -226,7 +227,7 @@ NodeDef* AddSend(const PartitionOptions& opts, const GraphInfo& g_info,\n   }\n \n   // Add the send node.\n-  const string send_op = (host_memory) ? \"_HostSend\" : \"_Send\";\n+  const std::string send_op = (host_memory) ? \"_HostSend\" : \"_Send\";\n   NodeDefBuilder send_builder(opts.new_name(src->name()), send_op,\n                               NodeDebugInfo(*src));\n   SetSendRecvAttrs(opts, edge, tensor_name_attr, &send_builder);\n@@ -241,7 +242,7 @@ NodeDef* AddSend(const PartitionOptions& opts, const GraphInfo& g_info,\n \n NodeDef* AddRecv(const PartitionOptions& opts, const GraphInfo& g_info,\n                  GraphDef* gdef, const Edge* edge, NodeDef** real_recv,\n-                 const string& tensor_name_attr, absl::Status* status) {\n+                 const std::string& tensor_name_attr, absl::Status* status) {\n   const DataType dtype = EdgeType(edge);\n   const Node* src = edge->src();\n   const Node* dst = edge->dst();\n@@ -285,7 +286,7 @@ NodeDef* AddRecv(const PartitionOptions& opts, const GraphInfo& g_info,\n   }\n \n   // Add the recv node.\n-  const string recv_op = (host_memory) ? \"_HostRecv\" : \"_Recv\";\n+  const std::string recv_op = (host_memory) ? \"_HostRecv\" : \"_Recv\";\n   NodeDefBuilder recv_builder(opts.new_name(src->name()), recv_op,\n                               NodeDebugInfo(*src));\n   SetSendRecvAttrs(opts, edge, tensor_name_attr, &recv_builder);\n@@ -298,7 +299,7 @@ NodeDef* AddRecv(const PartitionOptions& opts, const GraphInfo& g_info,\n \n   // Add the cast node (from cast_dtype to dtype) or an Identity node.\n   if (dtype != cast_dtype) {\n-    const string cast_op = (host_memory) ? \"_HostCast\" : \"Cast\";\n+    const std::string cast_op = (host_memory) ? \"_HostCast\" : \"Cast\";\n     NodeDefBuilder cast_builder(opts.new_name(src->name()), cast_op,\n                                 NodeDebugInfo(*src));\n     cast_builder.Attr(\"DstT\", dtype);\n@@ -339,8 +340,9 @@ NodeDef* AddDummyConst(const PartitionOptions& opts, GraphDef* gdef,\n \n // A dummy node for scheduling.\n NodeDef* AddControlTrigger(const PartitionOptions& opts, GraphDef* gdef,\n-                           const string& assigned_device_name, int64_t epoch,\n-                           int64_t starttime, absl::Status* status) {\n+                           const std::string& assigned_device_name,\n+                           int64_t epoch, int64_t starttime,\n+                           absl::Status* status) {\n   NodeDef* result = gdef->add_node();\n   *status = NodeDefBuilder(opts.new_name(absl::StrCat(\"synch_\", epoch)),\n                            \"ControlTrigger\")\n@@ -398,18 +400,19 @@ void OptimizeControlFlowColocation(Graph* graph) {\n   DFS(*graph, visit, {});\n }\n \n-string ControlLoopName(const string& name) {\n+std::string ControlLoopName(const std::string& name) {\n   return absl::StrCat(\"_cloop\", name);\n }\n \n bool IsControlLoop(const Node* node) {\n-  const string& name = node->name();\n+  const std::string& name = node->name();\n   return absl::StartsWith(name, \"_cloop\");\n }\n \n // An enter node for control flow.\n-Node* AddControlEnter(Graph* g, const string& node_name,\n-                      const string& device_name, const string& frame_name,\n+Node* AddControlEnter(Graph* g, const std::string& node_name,\n+                      const std::string& device_name,\n+                      const std::string& frame_name,\n                       const int parallel_iterations, absl::Status* status) {\n   NodeBuilder node_builder(node_name, \"Enter\", g->op_registry());\n   node_builder.Input({\"dummy\", 0, DT_FLOAT});\n@@ -423,9 +426,9 @@ Node* AddControlEnter(Graph* g, const string& node_name,\n }\n \n // A merge node for control flow.\n-Node* AddControlMerge(const string& in_name1, const string& in_name2, Graph* g,\n-                      const string& node_name, const string& device_name,\n-                      absl::Status* status) {\n+Node* AddControlMerge(const std::string& in_name1, const std::string& in_name2,\n+                      Graph* g, const std::string& node_name,\n+                      const std::string& device_name, absl::Status* status) {\n   NodeBuilder node_builder(node_name, \"Merge\", g->op_registry());\n   node_builder.Input({{in_name1, 0, DT_FLOAT}, {in_name2, 0, DT_FLOAT}});\n   Node* res_node;\n@@ -437,7 +440,7 @@ Node* AddControlMerge(const string& in_name1, const string& in_name2, Graph* g,\n \n // A switch node for control flow.\n Node* AddControlSwitch(NodeBuilder::NodeOut input1, NodeBuilder::NodeOut input2,\n-                       const string& device_name,\n+                       const std::string& device_name,\n                        const GraphDefBuilder::Options& bopts) {\n   Node* res_node =\n       ops::BinaryOp(\"Switch\", std::move(input1), std::move(input2), bopts);\n@@ -447,7 +450,7 @@ Node* AddControlSwitch(NodeBuilder::NodeOut input1, NodeBuilder::NodeOut input2,\n }\n \n // A next_iteration node for control flow.\n-Node* AddControlNext(NodeBuilder::NodeOut input, const string& device_name,\n+Node* AddControlNext(NodeBuilder::NodeOut input, const std::string& device_name,\n                      const GraphDefBuilder::Options& bopts) {\n   Node* res_node = ops::UnaryOp(\"NextIteration\", std::move(input), bopts);\n   if (bopts.HaveError()) return nullptr;\n@@ -469,7 +472,7 @@ Node* EmptyConst(const GraphDefBuilder::Options& options) {\n }\n \n // A dummy const node for control flow.\n-Node* AddControlConst(const string& device_name,\n+Node* AddControlConst(const std::string& device_name,\n                       const GraphDefBuilder::Options& bopts) {\n   Node* res_node = EmptyConst(bopts);\n   if (bopts.HaveError()) return nullptr;\n@@ -513,21 +516,22 @@ absl::Status AddControlLoop(const PartitionOptions& opts, Graph* g,\n   absl::Status status;\n   GraphDefBuilder::Options bopts(g, &status);\n   const ControlFlowInfo& src_info = (*cf_info)[src->id()];\n-  const string& device_name = edge->dst()->assigned_device_name();\n-  const string& frame_name = src_info.frame_name;\n+  const std::string& device_name = edge->dst()->assigned_device_name();\n+  const std::string& frame_name = src_info.frame_name;\n   int parallel_iterations;\n   status = GetNodeAttr(src_info.frame->attrs(), \"parallel_iterations\",\n                        &parallel_iterations);\n   if (!status.ok()) return status;\n \n   // The names of the nodes to be added.\n-  const string& enter_name =\n+  const std::string& enter_name =\n       ControlLoopName(opts.new_name(edge->dst()->name()));\n-  const string& merge_name =\n+  const std::string& merge_name =\n       ControlLoopName(opts.new_name(edge->dst()->name()));\n-  const string& switch_name =\n+  const std::string& switch_name =\n+      ControlLoopName(opts.new_name(edge->dst()->name()));\n+  const std::string& next_name =\n       ControlLoopName(opts.new_name(edge->dst()->name()));\n-  const string& next_name = ControlLoopName(opts.new_name(edge->dst()->name()));\n \n   // Add the nodes to the graph g.\n   Node* enter = AddControlEnter(g, enter_name, device_name, frame_name,\n@@ -634,14 +638,14 @@ absl::Status AddControlFlow(const PartitionOptions& opts, Graph* g,\n   OptimizeControlFlowColocation(g);\n \n   // The map from frames to their LoopCond nodes.\n-  std::unordered_map<string, Node*> frame_cond_map;\n+  std::unordered_map<std::string, Node*> frame_cond_map;\n   int num_node_ids = g->num_node_ids();\n   for (int i = 0; i < num_node_ids; ++i) {\n     Node* node = g->FindNodeId(i);\n     if (node == nullptr) continue;\n \n     if (IsLoopCond(node)) {\n-      const string& frame_name = cf_info[node->id()].frame_name;\n+      const std::string& frame_name = cf_info[node->id()].frame_name;\n       DCHECK(!frame_name.empty());\n       frame_cond_map[frame_name] = node;\n     }\n@@ -655,7 +659,7 @@ absl::Status AddControlFlow(const PartitionOptions& opts, Graph* g,\n   // the merge of the outer loop to the enter of the inner loop.\n   //\n   // A map from <frame_name, device_name> to ControlLoop.\n-  std::unordered_map<string, ControlLoop> control_loops;\n+  std::unordered_map<std::string, ControlLoop> control_loops;\n   int num_edge_ids = g->num_edge_ids();\n   for (int i = 0; i < num_edge_ids; ++i) {\n     const Edge* edge = g->FindEdgeId(i);\n@@ -666,15 +670,15 @@ absl::Status AddControlFlow(const PartitionOptions& opts, Graph* g,\n     // Skip Sink/Source nodes.\n     if (!src->IsOp() || !dst->IsOp()) continue;\n \n-    const string& src_device = src->assigned_device_name();\n-    const string& dst_device = dst->assigned_device_name();\n+    const std::string& src_device = src->assigned_device_name();\n+    const std::string& dst_device = dst->assigned_device_name();\n     // Skip local edges.\n     if (src_device == dst_device) continue;\n \n     const Node* src_frame = OutputFrame(src, cf_info);\n     const Node* dst_frame = InputFrame(dst, cf_info);\n-    const string& src_frame_name = cf_info[src_frame->id()].frame_name;\n-    const string& dst_frame_name = cf_info[dst_frame->id()].frame_name;\n+    const std::string& src_frame_name = cf_info[src_frame->id()].frame_name;\n+    const std::string& dst_frame_name = cf_info[dst_frame->id()].frame_name;\n     // Skip if src and dst are not in the same frame.\n     if (src_frame_name.empty() || src_frame_name != dst_frame_name) {\n       continue;\n@@ -685,12 +689,12 @@ absl::Status AddControlFlow(const PartitionOptions& opts, Graph* g,\n     // for its outer frame when nested.\n     ControlLoop child_loop;\n     while (true) {\n-      const string& curr_frame_name = cf_info[src_frame->id()].frame_name;\n+      const std::string& curr_frame_name = cf_info[src_frame->id()].frame_name;\n       if (curr_frame_name.empty()) {\n         // We have reached the root frame.\n         if (child_loop.merge != nullptr) {\n-          const string& node_name = opts.new_name(edge->dst()->name());\n-          const string& device_name = edge->dst()->assigned_device_name();\n+          const std::string& node_name = opts.new_name(edge->dst()->name());\n+          const std::string& device_name = edge->dst()->assigned_device_name();\n           Node* const_node =\n               AddControlConst(device_name, bopts.WithName(node_name));\n           if (!status.ok()) return status;\n@@ -700,7 +704,8 @@ absl::Status AddControlFlow(const PartitionOptions& opts, Graph* g,\n         break;\n       }\n \n-      const string& cl_key = absl::StrCat(curr_frame_name, \"$$\", dst_device);\n+      const std::string& cl_key =\n+          absl::StrCat(curr_frame_name, \"$$\", dst_device);\n       auto it = control_loops.find(cl_key);\n       if (it != control_loops.end()) {\n         if (child_loop.enter != nullptr) {\n@@ -748,15 +753,16 @@ absl::Status AddControlFlow(const PartitionOptions& opts, Graph* g,\n     // Skip Sink/Source nodes.\n     if (!src->IsOp() || !dst->IsOp()) continue;\n \n-    const string& src_device = src->assigned_device_name();\n-    const string& dst_device = dst->assigned_device_name();\n+    const std::string& src_device = src->assigned_device_name();\n+    const std::string& dst_device = dst->assigned_device_name();\n     if (src_device != dst_device) {\n       const Node* src_frame = OutputFrame(src, cf_info);\n       const Node* dst_frame = InputFrame(dst, cf_info);\n-      const string& src_frame_name = cf_info[src_frame->id()].frame_name;\n-      const string& dst_frame_name = cf_info[dst_frame->id()].frame_name;\n+      const std::string& src_frame_name = cf_info[src_frame->id()].frame_name;\n+      const std::string& dst_frame_name = cf_info[dst_frame->id()].frame_name;\n       if (!src_frame_name.empty() && src_frame_name == dst_frame_name) {\n-        const string& cl_key = absl::StrCat(dst_frame_name, \"$$\", dst_device);\n+        const std::string& cl_key =\n+            absl::StrCat(dst_frame_name, \"$$\", dst_device);\n         ControlLoop loop = control_loops[cl_key];\n         DCHECK(loop.enter != nullptr);\n         // Note that we'll create multiple duplicate edges if dst has multiple\n@@ -812,12 +818,13 @@ absl::Status TopologicalSortNodesWithTimePriority(\n   };\n \n   // Build initial structures, initial contents of queue.\n-  std::unordered_map<string, std::vector<const NodeDef*>> node_to_output_nodes;\n+  std::unordered_map<std::string, std::vector<const NodeDef*>>\n+      node_to_output_nodes;\n   std::unordered_map<const NodeDef*, int> inputs_needed;\n   for (int n = 0; n < gdef->node_size(); ++n) {\n     const NodeDef* ndef = &gdef->node(n);\n     for (int i = 0; i < ndef->input_size(); ++i) {\n-      node_to_output_nodes[string(ParseTensorName(ndef->input(i)).first)]\n+      node_to_output_nodes[std::string(ParseTensorName(ndef->input(i)).first)]\n           .push_back(ndef);\n     }\n     int64_t start_time;\n@@ -872,8 +879,9 @@ absl::Status TopologicalSortNodesWithTimePriority(\n   return absl::OkStatus();\n }\n \n-absl::Status AddControlEdges(const PartitionOptions& opts,\n-                             std::unordered_map<string, GraphDef>* partitions) {\n+absl::Status AddControlEdges(\n+    const PartitionOptions& opts,\n+    std::unordered_map<std::string, GraphDef>* partitions) {\n   absl::Status status;\n   // TODO(yuanbyu): Very naive for now. To be improved.\n   const int num_epochs = 100;\n@@ -891,7 +899,7 @@ absl::Status AddControlEdges(const PartitionOptions& opts,\n \n     // Add a dummy node for every epoch, and add a control edge from the\n     // \"last\" node in the preceding epoch to the dummy node.\n-    string device_name = gdef->node(0).device();\n+    std::string device_name = gdef->node(0).device();\n     int64_t makespan = start_times.back().second;\n     int64_t resolution = (makespan / num_epochs) + 1;\n \n@@ -909,7 +917,7 @@ absl::Status AddControlEdges(const PartitionOptions& opts,\n         }\n         dummys.push_back(dummy);\n         if (j > 0) {\n-          string src_name = start_times[j - 1].first->name();\n+          std::string src_name = start_times[j - 1].first->name();\n           Graph::AddInput(dummy, src_name, Graph::kControlSlot);\n         }\n         i++;\n@@ -940,7 +948,7 @@ void SetIncarnation(const PartitionOptions& opts, NodeDef* ndef) {\n     // Not related to send/recv.\n     return;\n   }\n-  const string& send_device = GetNodeAttrString(*ndef, \"send_device\");\n+  const std::string& send_device = GetNodeAttrString(*ndef, \"send_device\");\n   if (send_device.empty()) {\n     // No known send_device. The runtime will detect it later.\n     return;\n@@ -968,10 +976,10 @@ void SetIncarnation(const PartitionOptions& opts, GraphDef* gdef) {\n }\n \n absl::Status Partition(const PartitionOptions& opts, Graph* g,\n-                       std::unordered_map<string, GraphDef>* partitions) {\n+                       std::unordered_map<std::string, GraphDef>* partitions) {\n   // TODO(b/290689453) Refactor this into smaller functions\n   absl::Status status;\n-  absl::flat_hash_map<string, std::unique_ptr<GraphDebugInfoBuilder>>\n+  absl::flat_hash_map<std::string, std::unique_ptr<GraphDebugInfoBuilder>>\n       debug_info_builders;\n   partitions->clear();\n \n@@ -991,15 +999,15 @@ absl::Status Partition(const PartitionOptions& opts, Graph* g,\n   status = BuildMemoryDeviceInfo(*g, &g_info);\n   if (!status.ok()) return status;\n \n-  string dstp;\n+  std::string dstp;\n   std::vector<const Edge*> inputs;\n   DupRecvTable dup_recv(3);\n   // For a node dst, 'ref_recvs' remembers the recvs introduced by a ref\n   // edge to dst. 'ref_control_inputs' remembers the inputs by a non-ref\n   // edge to dst. We will add a control edge for every pair in\n   // (ref_recvs x ref_control_inputs).\n   std::vector<NodeDef*> ref_recvs;\n-  std::vector<string> ref_control_inputs;\n+  std::vector<std::string> ref_control_inputs;\n \n   int32_t num_data = 0;\n   int32_t num_control = 0;\n@@ -1121,7 +1129,7 @@ absl::Status Partition(const PartitionOptions& opts, Graph* g,\n       auto iter = dup_recv.find(key);\n       if (iter != dup_recv.end()) {\n         // We found one. Reuse the data/control transferred already.\n-        const string& recv_node_name = iter->second.recv->name();\n+        const std::string& recv_node_name = iter->second.recv->name();\n         if (edge->IsControlEdge()) {\n           Graph::AddInput(dst_def, recv_node_name, Graph::kControlSlot);\n         } else {\n@@ -1157,7 +1165,7 @@ absl::Status Partition(const PartitionOptions& opts, Graph* g,\n         send_from.Reset(src->name(), edge->src_output(), EdgeType(edge));\n       }\n \n-      string tensor_name_attr;\n+      std::string tensor_name_attr;\n       if (opts.get_tensor_name_attr) {\n         tensor_name_attr = opts.get_tensor_name_attr(edge);\n       } else {"
        },
        {
            "sha": "c1d9493c76c6b5061a380dd73b72505cf3ee9d2e",
            "filename": "tensorflow/core/graph/graph_partition.h",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph_partition.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph_partition.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fgraph_partition.h?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -31,19 +31,19 @@ namespace tensorflow {\n struct PartitionOptions {\n   // A function that returns a location for the execution of a given\n   // Node.\n-  typedef std::function<string(const Node*)> NodeToLocFunc;\n+  typedef std::function<std::string(const Node*)> NodeToLocFunc;\n   NodeToLocFunc node_to_loc = nullptr;\n \n   // A function that returns a unique graph node name with the given\n   // prefix.\n-  typedef std::function<string(const string&)> NewNameFunc;\n+  typedef std::function<std::string(const std::string&)> NewNameFunc;\n   NewNameFunc new_name = nullptr;\n \n   // A function that returns the incarnation of a device given the\n   // device's fullname. If not found, GetIncarnationFunc should return\n   // kIllegalIncarnation.\n-  static constexpr uint64 kIllegalIncarnation = 0;\n-  typedef std::function<uint64(const string&)> GetIncarnationFunc;\n+  static constexpr uint64_t kIllegalIncarnation = 0;\n+  typedef std::function<uint64_t(const std::string&)> GetIncarnationFunc;\n   GetIncarnationFunc get_incarnation = nullptr;\n \n   // If specified, flib_def defines a function library that should be\n@@ -79,7 +79,7 @@ struct PartitionOptions {\n \n   // Optional customized function to compute the \"tensor_name\" attr value of\n   // Send/Recv ops inserted during partitioning.\n-  std::function<string(const Edge*)> get_tensor_name_attr = nullptr;\n+  std::function<std::string(const Edge*)> get_tensor_name_attr = nullptr;\n \n   // If true, the `Partition()` function can make destructive changes to the\n   // passed-in `Graph`.\n@@ -96,13 +96,14 @@ struct PartitionOptions {\n //\n // Stores the partitions in *partitions.\n absl::Status Partition(const PartitionOptions& opts, Graph* input,\n-                       std::unordered_map<string, GraphDef>* partitions);\n+                       std::unordered_map<std::string, GraphDef>* partitions);\n \n // Add control edges to the partitions to control the ordering\n // and timing of the recv nodes based on the start times calculated\n // using some scheduling algorithm.\n-absl::Status AddControlEdges(const PartitionOptions& opts,\n-                             std::unordered_map<string, GraphDef>* partitions);\n+absl::Status AddControlEdges(\n+    const PartitionOptions& opts,\n+    std::unordered_map<std::string, GraphDef>* partitions);\n \n }  // namespace tensorflow\n "
        },
        {
            "sha": "4f5e431b87df50096c2195f60a7bf4f5b4aba19f",
            "filename": "tensorflow/core/graph/graph_partition_test.cc",
            "status": "modified",
            "additions": 42,
            "deletions": 36,
            "changes": 78,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph_partition_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fgraph_partition_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fgraph_partition_test.cc?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -68,38 +68,42 @@ using ::testing::Ne;\n \n const char gpu_device[] = \"/job:a/replica:0/task:0/device:GPU:0\";\n \n-string SplitByDevice(const Node* node) { return node->assigned_device_name(); }\n+std::string SplitByDevice(const Node* node) {\n+  return node->assigned_device_name();\n+}\n \n-string DeviceName(const Node* node) {\n+std::string DeviceName(const Node* node) {\n   char first = node->name()[0];\n   if (first == 'G') {\n     return gpu_device;\n   } else {\n-    const string cpu_prefix = \"/job:a/replica:0/task:0/cpu:\";\n+    const std::string cpu_prefix = \"/job:a/replica:0/task:0/cpu:\";\n     int index = first - 'A';\n     return absl::StrCat(cpu_prefix, index);\n   }\n }\n \n void Partition(const GraphDef& graph_def,\n-               std::unordered_map<string, GraphDef>* partitions) {\n+               std::unordered_map<std::string, GraphDef>* partitions) {\n   Graph g(OpRegistry::Global());\n   GraphConstructorOptions opts;\n   TF_CHECK_OK(ConvertGraphDefToGraph(opts, graph_def, &g));\n \n   // Assigns devices to each node. Uses 1st letter of the node name as the\n   // device index if no device is specified.\n   for (Node* node : g.nodes()) {\n-    string device_name = !node->requested_device().empty()\n-                             ? node->requested_device()\n-                             : DeviceName(node);\n+    std::string device_name = !node->requested_device().empty()\n+                                  ? node->requested_device()\n+                                  : DeviceName(node);\n     node->set_assigned_device_name(device_name);\n   }\n \n   PartitionOptions popts;\n   popts.node_to_loc = SplitByDevice;\n-  popts.new_name = [&g](const string& prefix) { return g.NewName(prefix); };\n-  popts.get_incarnation = [](const string& name) {\n+  popts.new_name = [&g](const std::string& prefix) {\n+    return g.NewName(prefix);\n+  };\n+  popts.get_incarnation = [](const std::string& name) {\n     return (name[0] - 'A') + 100;\n   };\n   absl::Status s = Partition(popts, &g, partitions);\n@@ -116,7 +120,7 @@ void Partition(const GraphDef& graph_def,\n }\n \n void CheckLoopConstruction(const GraphDef& graph_def) {\n-  std::unordered_map<string, GraphDef> partitions;\n+  std::unordered_map<std::string, GraphDef> partitions;\n   Partition(graph_def, &partitions);\n   for (const auto& kv : partitions) {\n     const GraphDef& gdef = kv.second;\n@@ -128,7 +132,7 @@ void CheckLoopConstruction(const GraphDef& graph_def) {\n       // _recvs must have a control input\n       if (ndef.op() == \"_Recv\") {\n         bool has_control = false;\n-        for (const string& input_name : ndef.input()) {\n+        for (const std::string& input_name : ndef.input()) {\n           if (absl::StartsWith(input_name, \"^\")) {\n             has_control = true;\n             break;\n@@ -171,10 +175,10 @@ REGISTER_OP(\"Combine\")\n     .Output(\"o: float\")\n     .SetShapeFn(shape_inference::UnknownShape);\n \n-Output ConstructOp(const Scope& scope, const string& op_type,\n+Output ConstructOp(const Scope& scope, const std::string& op_type,\n                    const absl::Span<const Input> inputs) {\n   if (!scope.ok()) return Output();\n-  const string unique_name = scope.GetUniqueNameForOp(op_type);\n+  const std::string unique_name = scope.GetUniqueNameForOp(op_type);\n   auto builder =\n       NodeBuilder(unique_name, op_type, scope.graph()->op_registry());\n   for (auto const& input : inputs) {\n@@ -230,20 +234,20 @@ class GraphPartitionTest : public ::testing::Test {\n   void ExpectMatchA() {\n     GraphDef graph_def;\n     TF_EXPECT_OK(scope_a_.ToGraphDef(&graph_def));\n-    string a = \"/job:a/replica:0/task:0/cpu:0\";\n+    std::string a = \"/job:a/replica:0/task:0/cpu:0\";\n     TF_EXPECT_GRAPH_EQ(graph_def, partitions_[a]);\n   }\n \n   void ExpectMatchB() {\n     GraphDef graph_def;\n     TF_EXPECT_OK(scope_b_.ToGraphDef(&graph_def));\n-    string b = \"/job:a/replica:0/task:0/cpu:1\";\n+    std::string b = \"/job:a/replica:0/task:0/cpu:1\";\n     TF_EXPECT_GRAPH_EQ(graph_def, partitions_[b]);\n   }\n \n   void ExpectFunctions(const FunctionDefLibrary& library,\n-                       const std::set<string>& expected_names) {\n-    std::set<string> actual_names;\n+                       const std::set<std::string>& expected_names) {\n+    std::set<std::string> actual_names;\n     for (const FunctionDef& fdef : library.function()) {\n       actual_names.insert(fdef.signature().name());\n     }\n@@ -254,7 +258,7 @@ class GraphPartitionTest : public ::testing::Test {\n   GraphDef in_graph_def_;\n   Scope scope_a_;\n   Scope scope_b_;\n-  std::unordered_map<string, GraphDef> partitions_;\n+  std::unordered_map<std::string, GraphDef> partitions_;\n };\n \n TEST_F(GraphPartitionTest, SingleDevice) {\n@@ -277,8 +281,8 @@ TEST_F(GraphPartitionTest, CrossDeviceData) {\n   Partition(ToGraphDef(), &partitions_);\n   EXPECT_EQ(2, partitions_.size());\n \n-  string a = \"/job:a/replica:0/task:0/cpu:0\";\n-  string b = \"/job:a/replica:0/task:0/cpu:1\";\n+  std::string a = \"/job:a/replica:0/task:0/cpu:0\";\n+  std::string b = \"/job:a/replica:0/task:0/cpu:1\";\n   a1 = FloatInput(scope_a_.WithOpName(\"A1\"));\n   _Send(scope_a_.WithOpName(\"A1/_0\"), a1, \"edge_1_A1\", a, 82, b);\n   ExpectMatchA();\n@@ -298,8 +302,8 @@ TEST_F(GraphPartitionTest, CrossDeviceControl) {\n   Partition(ToGraphDef(), &partitions_);\n   EXPECT_EQ(2, partitions_.size());\n \n-  string a = \"/job:a/replica:0/task:0/cpu:0\";\n-  string b = \"/job:a/replica:0/task:0/cpu:1\";\n+  std::string a = \"/job:a/replica:0/task:0/cpu:0\";\n+  std::string b = \"/job:a/replica:0/task:0/cpu:1\";\n   a1 = FloatInput(scope_a_.WithOpName(\"A1\"));\n   auto c =\n       Const(scope_a_.WithOpName(\"A1/ctrl/_0\").WithControlDependencies(a1), {});\n@@ -323,8 +327,8 @@ TEST_F(GraphPartitionTest, CrossDeviceData_MultiUse) {\n   Partition(ToGraphDef(), &partitions_);\n   EXPECT_EQ(2, partitions_.size());\n \n-  string a = \"/job:a/replica:0/task:0/cpu:0\";\n-  string b = \"/job:a/replica:0/task:0/cpu:1\";\n+  std::string a = \"/job:a/replica:0/task:0/cpu:0\";\n+  std::string b = \"/job:a/replica:0/task:0/cpu:1\";\n   a1 = FloatInput(scope_a_.WithOpName(\"A1\"));\n   _Send(scope_a_.WithOpName(\"A1/_0\"), a1, \"edge_1_A1\", a, 82, b);\n   ExpectMatchA();\n@@ -346,8 +350,8 @@ TEST_F(GraphPartitionTest, CrossDeviceControl_MultiUse) {\n   Partition(ToGraphDef(), &partitions_);\n   EXPECT_EQ(2, partitions_.size());\n \n-  string a = \"/job:a/replica:0/task:0/cpu:0\";\n-  string b = \"/job:a/replica:0/task:0/cpu:1\";\n+  std::string a = \"/job:a/replica:0/task:0/cpu:0\";\n+  std::string b = \"/job:a/replica:0/task:0/cpu:1\";\n   a1 = FloatInput(scope_a_.WithOpName(\"A1\"));\n   auto c =\n       Const(scope_a_.WithOpName(\"A1/ctrl/_0\").WithControlDependencies(a1), {});\n@@ -372,8 +376,8 @@ TEST_F(GraphPartitionTest, CrossDevice_DataControl) {\n   Partition(ToGraphDef(), &partitions_);\n   EXPECT_EQ(2, partitions_.size());\n \n-  string a = \"/job:a/replica:0/task:0/cpu:0\";\n-  string b = \"/job:a/replica:0/task:0/cpu:1\";\n+  std::string a = \"/job:a/replica:0/task:0/cpu:0\";\n+  std::string b = \"/job:a/replica:0/task:0/cpu:1\";\n   a1 = FloatInput(scope_a_.WithOpName(\"A1\"));\n   _Send(scope_a_.WithOpName(\"A1/_0\"), a1, \"edge_1_A1\", a, 82, b);\n   auto c =\n@@ -417,7 +421,7 @@ TEST_F(GraphPartitionTest, CrossDeviceLoopSimple1) {\n   auto b1 = Identity(in_.WithOpName(\"B1\"), a3);\n   NextIteration(in_.WithOpName(\"B5\"), b1);\n \n-  std::unordered_map<string, GraphDef> partitions;\n+  std::unordered_map<std::string, GraphDef> partitions;\n   Partition(ToGraphDef(), &partitions);\n   for (const auto& kv : partitions) {\n     const GraphDef& gdef = kv.second;\n@@ -471,10 +475,12 @@ TEST_F(GraphPartitionTest, PartitionIncompleteGraph) {\n \n   PartitionOptions popts;\n   popts.node_to_loc = SplitByDevice;\n-  popts.new_name = [&g](const string& prefix) { return g.NewName(prefix); };\n-  popts.get_incarnation = [](const string&) { return 1; };\n+  popts.new_name = [&g](const std::string& prefix) {\n+    return g.NewName(prefix);\n+  };\n+  popts.get_incarnation = [](const std::string&) { return 1; };\n \n-  std::unordered_map<string, GraphDef> partitions;\n+  std::unordered_map<std::string, GraphDef> partitions;\n   status = Partition(popts, &g, &partitions);\n   // Partitioning should fail, but not crash like it did before the\n   // changes that accompanied the addition of this test.\n@@ -498,8 +504,8 @@ TEST_F(GraphPartitionTest, Functions) {\n   EXPECT_EQ(2, partitions_.size());\n \n   // Test that partition graphs inherit function library from original graph.\n-  string a = \"/job:a/replica:0/task:0/cpu:0\";\n-  string b = \"/job:a/replica:0/task:0/cpu:1\";\n+  std::string a = \"/job:a/replica:0/task:0/cpu:0\";\n+  std::string b = \"/job:a/replica:0/task:0/cpu:1\";\n \n   // Node \"A2\" is placed in part `a`, and uses only \"XTimesTwo\".\n   ExpectFunctions(partitions_[a].library(), {\"XTimesTwo\"});\n@@ -602,7 +608,7 @@ TEST_F(GraphPartitionTest, GraphDebugInfo) {\n \n   // Expect each partitioned graph to contain the stack traces for its nodes.\n   // A stack trace for A1 should be in the A partition (\".../cpu:0\").\n-  string a = \"/job:a/replica:0/task:0/cpu:0\";\n+  std::string a = \"/job:a/replica:0/task:0/cpu:0\";\n   const GraphDebugInfo& a_debug_info = partitions_[a].debug_info();\n   StackTracesMap traces = LoadTracesFromDebugInfo(a_debug_info);\n   const auto& a_it = traces.find(\"A1\");\n@@ -611,7 +617,7 @@ TEST_F(GraphPartitionTest, GraphDebugInfo) {\n               ::testing::ContainsRegex(\"alpha.cc.*30\"));\n \n   // Stack traces for B1 and B2 should be in the B partition (\".../cpu:1\").\n-  string b = \"/job:a/replica:0/task:0/cpu:1\";\n+  std::string b = \"/job:a/replica:0/task:0/cpu:1\";\n   const GraphDebugInfo& b_debug_info = partitions_[b].debug_info();\n   traces = LoadTracesFromDebugInfo(b_debug_info);\n   const auto& b1_it = traces.find(\"B1\");"
        },
        {
            "sha": "e29d2d92d4c5971c7b4896e5d9bd6d4da33e6dc8",
            "filename": "tensorflow/core/graph/node_builder.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fnode_builder.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fnode_builder.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fnode_builder.cc?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -108,7 +108,7 @@ NodeBuilder& NodeBuilder::Device(absl::string_view device_spec) {\n }\n \n NodeBuilder& NodeBuilder::AssignedDevice(absl::string_view device) {\n-  assigned_device_ = string(device);\n+  assigned_device_ = std::string(device);\n   return *this;\n }\n "
        },
        {
            "sha": "476393cae8166b2eda6474107edd156ca37b5fec",
            "filename": "tensorflow/core/graph/node_builder.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fnode_builder.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fnode_builder.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fnode_builder.h?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -67,8 +67,8 @@ class NodeBuilder {\n     // * a nullptr Node* was passed to the NodeOut constructor, or\n     // * an out-of-range index was passed to the NodeOut constructor.\n     bool error;\n-    string name;\n-    int32 index;\n+    std::string name;\n+    int32_t index;\n     DataType dt;\n   };\n \n@@ -132,7 +132,7 @@ class NodeBuilder {\n   absl::StatusOr<Node*> Finalize(Graph* graph, bool consume = false);\n \n   // Accessors for the values set in the constructor.\n-  const string& node_name() const { return def_builder_.node_name(); }\n+  const std::string& node_name() const { return def_builder_.node_name(); }\n   const OpDef& op_def() const { return def_builder_.op_def(); }\n \n  private:\n@@ -157,8 +157,8 @@ class NodeBuilder {\n   const OpRegistryInterface* op_registry_;\n   std::vector<NodeOut> inputs_;\n   std::vector<Node*> control_inputs_;\n-  std::vector<string> errors_;\n-  string assigned_device_;\n+  std::vector<std::string> errors_;\n+  std::string assigned_device_;\n };\n \n // IMPLEMENTATION -------------------------------------------------------------"
        },
        {
            "sha": "bac15370ae039e4bb86ed6654eed8662502cef96",
            "filename": "tensorflow/core/graph/optimizer_cse_test.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 16,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Foptimizer_cse_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Foptimizer_cse_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Foptimizer_cse_test.cc?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -36,7 +36,7 @@ limitations under the License.\n namespace tensorflow {\n namespace {\n \n-static void InitGraph(const string& s, Graph* graph) {\n+static void InitGraph(const std::string& s, Graph* graph) {\n   GraphDef graph_def;\n \n   auto parser = protobuf::TextFormat::Parser();\n@@ -50,14 +50,14 @@ class OptimizerCSETest : public ::testing::Test {\n  public:\n   OptimizerCSETest() : graph_(OpRegistry::Global()) {}\n \n-  void InitGraph(const string& s) {\n+  void InitGraph(const std::string& s) {\n     ::tensorflow::InitGraph(s, &graph_);\n     original_ = CanonicalGraphString(&graph_);\n   }\n \n   static bool IncludeNode(const Node* n) { return n->IsOp(); }\n \n-  static string EdgeId(const Node* n, int index) {\n+  static std::string EdgeId(const Node* n, int index) {\n     if (index == 0) {\n       return n->name();\n     } else if (index == Graph::kControlSlot) {\n@@ -67,9 +67,9 @@ class OptimizerCSETest : public ::testing::Test {\n     }\n   }\n \n-  string CanonicalGraphString(Graph* g) {\n-    std::vector<string> nodes;\n-    std::vector<string> edges;\n+  std::string CanonicalGraphString(Graph* g) {\n+    std::vector<std::string> nodes;\n+    std::vector<std::string> edges;\n     for (const Node* n : g->nodes()) {\n       if (IncludeNode(n)) {\n         nodes.push_back(absl::StrCat(n->name(), \"(\", n->type_string(), \")\"));\n@@ -88,21 +88,22 @@ class OptimizerCSETest : public ::testing::Test {\n                         absl::StrJoin(edges, \";\"));\n   }\n \n-  string DoCSE(const std::function<bool(const Node*)>& consider_fn = nullptr) {\n-    string before = CanonicalGraphString(&graph_);\n+  std::string DoCSE(\n+      const std::function<bool(const Node*)>& consider_fn = nullptr) {\n+    std::string before = CanonicalGraphString(&graph_);\n     LOG(ERROR) << \"Before rewrites: \" << before;\n \n     OptimizeCSE(&graph_, consider_fn);\n \n-    string result = CanonicalGraphString(&graph_);\n+    std::string result = CanonicalGraphString(&graph_);\n     LOG(ERROR) << \"After rewrites:  \" << result;\n     return result;\n   }\n \n-  const string& OriginalGraph() const { return original_; }\n+  const std::string& OriginalGraph() const { return original_; }\n \n   Graph graph_;\n-  string original_;\n+  std::string original_;\n };\n \n REGISTER_OP(\"Input\").Output(\"o: float\").SetIsStateful();\n@@ -339,8 +340,8 @@ TEST_F(OptimizerCSETest, Constant_Dedup) {\n   EXPECT_EQ(OriginalGraph(),\n             \"n/_0(Const);n/_1(Const);n/_2(Const);n/_3(Const);\"\n             \"n/_4(Const);n/_5(Const);n/_6(Const);n/_7(Const)|\");\n-  std::vector<string> nodes = str_util::Split(DoCSE(), \";|\");\n-  std::set<string> node_set(nodes.begin(), nodes.end());\n+  std::vector<std::string> nodes = str_util::Split(DoCSE(), \";|\");\n+  std::set<std::string> node_set(nodes.begin(), nodes.end());\n   // Expect exactly one of each type of node to be retained after CSE.\n   EXPECT_EQ(node_set.count(\"n/_0(Const)\") + node_set.count(\"n/_7(Const)\"), 1);\n   EXPECT_EQ(node_set.count(\"n/_1(Const)\") + node_set.count(\"n/_6(Const)\"), 1);\n@@ -350,14 +351,14 @@ TEST_F(OptimizerCSETest, Constant_Dedup) {\n \n void BM_CSE(::testing::benchmark::State& state) {\n   const int op_nodes = state.range(0);\n-  string s;\n+  std::string s;\n   for (int in = 0; in < 10; in++) {\n-    s += strings::Printf(\"node { name: 'in%04d' op: 'Input'}\", in);\n+    s += absl::StrFormat(\"node { name: 'in%04d' op: 'Input'}\", in);\n   }\n   random::PhiloxRandom philox(301, 17);\n   random::SimplePhilox rnd(&philox);\n   for (int op = 0; op < op_nodes; op++) {\n-    s += strings::Printf(\n+    s += absl::StrFormat(\n         \"node { name: 'op%04d' op: 'Mul' attr { key: 'T' value { \"\n         \"type: DT_FLOAT } } input: ['in%04d', 'in%04d' ] }\",\n         op, rnd.Uniform(10), rnd.Uniform(10));"
        },
        {
            "sha": "697defb2ef25581c4df68e36224b6504335aab23",
            "filename": "tensorflow/core/graph/subgraph.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 19,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fsubgraph.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fsubgraph.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fsubgraph.cc?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -61,7 +61,7 @@ absl::Status FeedInputs(\n   out_feed_types->clear();\n   out_feed_types->reserve(feed_rewrites.size());\n   for (size_t i = 0; i < feed_rewrites.size(); ++i) {\n-    const string& t = feed_rewrites[i]->endpoint_name();\n+    const std::string& t = feed_rewrites[i]->endpoint_name();\n     TensorId id(ParseTensorName(t));\n \n     auto iter = name_index->find(id.first);\n@@ -127,7 +127,7 @@ absl::Status FetchOutputs(\n   out_fetch_nodes->clear();\n   out_fetch_nodes->reserve(fetch_rewrites.size());\n   for (size_t i = 0; i < fetch_rewrites.size(); ++i) {\n-    const string& t = fetch_rewrites[i]->endpoint_name();\n+    const std::string& t = fetch_rewrites[i]->endpoint_name();\n \n     // Parse t into node_name and output_index.\n     TensorId id(ParseTensorName(t));\n@@ -174,7 +174,7 @@ absl::Status FetchOutputs(\n   return absl::OkStatus();\n }\n \n-bool AddNodeToTargets(const string& node_or_tensor_name,\n+bool AddNodeToTargets(const std::string& node_or_tensor_name,\n                       const NameIndex& name_index,\n                       std::unordered_set<const Node*>* targets) {\n   TensorId id = ParseTensorName(node_or_tensor_name);\n@@ -188,17 +188,18 @@ bool AddNodeToTargets(const string& node_or_tensor_name,\n   return true;\n }\n \n-absl::Status PruneForTargets(Graph* g, const NameIndex& name_index,\n-                             const std::vector<Node*>& fetch_nodes,\n-                             const absl::Span<const string>& target_nodes) {\n-  string not_found;\n+absl::Status PruneForTargets(\n+    Graph* g, const NameIndex& name_index,\n+    const std::vector<Node*>& fetch_nodes,\n+    const absl::Span<const std::string>& target_nodes) {\n+  std::string not_found;\n   std::unordered_set<const Node*> targets;\n   for (Node* n : fetch_nodes) {\n     if (!AddNodeToTargets(n->name(), name_index, &targets)) {\n       absl::StrAppend(&not_found, n->name(), \" \");\n     }\n   }\n-  for (const string& s : target_nodes) {\n+  for (const std::string& s : target_nodes) {\n     if (!AddNodeToTargets(s, name_index, &targets)) {\n       absl::StrAppend(&not_found, s, \" \");\n     }\n@@ -295,20 +296,20 @@ absl::Status SendFetchRewrite::AddNode(Graph* g,\n }\n \n absl::Status RewriteGraphForExecution(\n-    Graph* g, const absl::Span<const string>& fed_outputs,\n-    const absl::Span<const string>& fetch_outputs,\n-    const absl::Span<const string>& target_node_names,\n+    Graph* g, const absl::Span<const std::string>& fed_outputs,\n+    const absl::Span<const std::string>& fetch_outputs,\n+    const absl::Span<const std::string>& target_node_names,\n     const DeviceAttributes& device_info, bool use_function_convention,\n     RewriteGraphMetadata* out_metadata) {\n   std::vector<std::unique_ptr<PruneRewrite>> feed_rewrites;\n   feed_rewrites.reserve(fed_outputs.size());\n   if (use_function_convention) {\n     for (size_t i = 0; i < fed_outputs.size(); ++i) {\n       feed_rewrites.emplace_back(new ArgFeedRewrite(\n-          &fed_outputs[i], &device_info, static_cast<int32>(i)));\n+          &fed_outputs[i], &device_info, static_cast<int32_t>(i)));\n     }\n   } else {\n-    for (const string& fed_output : fed_outputs) {\n+    for (const std::string& fed_output : fed_outputs) {\n       feed_rewrites.emplace_back(\n           new RecvFeedRewrite(&fed_output, &device_info));\n     }\n@@ -319,10 +320,10 @@ absl::Status RewriteGraphForExecution(\n   if (use_function_convention) {\n     for (size_t i = 0; i < fetch_outputs.size(); ++i) {\n       fetch_rewrites.emplace_back(new RetvalFetchRewrite(\n-          &fetch_outputs[i], &device_info, static_cast<int32>(i)));\n+          &fetch_outputs[i], &device_info, static_cast<int32_t>(i)));\n     }\n   } else {\n-    for (const string& fetch_output : fetch_outputs) {\n+    for (const std::string& fetch_output : fetch_outputs) {\n       fetch_rewrites.emplace_back(\n           new SendFetchRewrite(&fetch_output, &device_info));\n     }\n@@ -334,22 +335,22 @@ absl::Status RewriteGraphForExecution(\n \n namespace {\n template <typename StringContainer>\n-std::vector<string> ConvertToVector(StringContainer field) {\n-  return std::vector<string>(field.begin(), field.end());\n+std::vector<std::string> ConvertToVector(StringContainer field) {\n+  return std::vector<std::string>(field.begin(), field.end());\n }\n }  // namespace\n \n absl::Status RewriteGraphForExecution(\n     Graph* g, const std::vector<std::unique_ptr<PruneRewrite>>& feed_rewrites,\n     const std::vector<std::unique_ptr<PruneRewrite>>& fetch_rewrites,\n-    const absl::Span<const string>& target_node_names,\n+    const absl::Span<const std::string>& target_node_names,\n     RewriteGraphMetadata* out_metadata) {\n   if (fetch_rewrites.empty() && target_node_names.empty()) {\n     return errors::InvalidArgument(\n         \"Must specify at least one target to fetch or execute.\");\n   }\n \n-  std::unordered_set<string> endpoints;\n+  std::unordered_set<std::string> endpoints;\n   for (const auto& feed_rewrite : feed_rewrites) {\n     auto result = endpoints.insert(feed_rewrite->endpoint_name());\n     if (!result.second) {"
        },
        {
            "sha": "c8843a37d58fa9dce22eb1a82b8e94389c0f18ae",
            "filename": "tensorflow/core/graph/subgraph.h",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fsubgraph.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fsubgraph.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fsubgraph.h?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -50,7 +50,8 @@ struct RewriteGraphMetadata {\n class PruneRewrite {\n  public:\n   // `endpoint_name` and `device_info` must outlive this object.\n-  PruneRewrite(const string* endpoint_name, const DeviceAttributes* device_info)\n+  PruneRewrite(const std::string* endpoint_name,\n+               const DeviceAttributes* device_info)\n       : endpoint_name_(endpoint_name), device_info_(device_info) {}\n   virtual ~PruneRewrite() {}\n \n@@ -60,14 +61,14 @@ class PruneRewrite {\n                                Node** out_node) = 0;\n \n   // Returns the name of the tensor to which this rewrite applies.\n-  const string& endpoint_name() { return *endpoint_name_; }\n+  const std::string& endpoint_name() { return *endpoint_name_; }\n \n  protected:\n   // The device on which the new node will be created.\n   const DeviceAttributes& device_info() { return *device_info_; }\n \n  private:\n-  const string* const endpoint_name_;          // Not owned.\n+  const std::string* const endpoint_name_;     // Not owned.\n   const DeviceAttributes* const device_info_;  // Not owned.\n };\n \n@@ -98,9 +99,9 @@ class PruneRewrite {\n //    - fetch output \"node:output_index\" does not exist in \"*g\"\n //    - target node \"node\" does not exist in \"*g\"\n absl::Status RewriteGraphForExecution(\n-    Graph* g, const absl::Span<const string>& fed_outputs,\n-    const absl::Span<const string>& fetch_outputs,\n-    const absl::Span<const string>& target_node_names,\n+    Graph* g, const absl::Span<const std::string>& fed_outputs,\n+    const absl::Span<const std::string>& fetch_outputs,\n+    const absl::Span<const std::string>& target_node_names,\n     const DeviceAttributes& device_info, bool use_function_convention,\n     RewriteGraphMetadata* out_metadata);\n \n@@ -109,7 +110,7 @@ absl::Status RewriteGraphForExecution(\n absl::Status RewriteGraphForExecution(\n     Graph* g, const std::vector<std::unique_ptr<PruneRewrite>>& feed_rewrites,\n     const std::vector<std::unique_ptr<PruneRewrite>>& fetch_rewrites,\n-    const absl::Span<const string>& target_node_names,\n+    const absl::Span<const std::string>& target_node_names,\n     RewriteGraphMetadata* out_metadata);\n \n /////////////////////////////////////////////////////////\n@@ -119,14 +120,14 @@ absl::Status RewriteGraphForExecution(\n // A rewrite action that adds an _Arg node for a fed tensor.\n class ArgFeedRewrite : public PruneRewrite {\n  public:\n-  ArgFeedRewrite(const string* endpoint_name,\n+  ArgFeedRewrite(const std::string* endpoint_name,\n                  const DeviceAttributes* device_info, int32_t arg_index)\n       : PruneRewrite(endpoint_name, device_info), arg_index_(arg_index) {}\n   absl::Status AddNode(Graph* g, NodeBuilder::NodeOut feed_tensor,\n                        Node** out_node) override;\n \n  private:\n-  const int32 arg_index_;\n+  const int32_t arg_index_;\n };\n \n // A rewrite action that adds a client-terminated _Recv node for a fed tensor.\n@@ -140,14 +141,14 @@ class RecvFeedRewrite : public PruneRewrite {\n // A rewrite action that adds a _Retval node for a fetched tensor.\n class RetvalFetchRewrite : public PruneRewrite {\n  public:\n-  RetvalFetchRewrite(const string* endpoint_name,\n+  RetvalFetchRewrite(const std::string* endpoint_name,\n                      const DeviceAttributes* device_info, int32_t retval_index)\n       : PruneRewrite(endpoint_name, device_info), retval_index_(retval_index) {}\n   absl::Status AddNode(Graph* g, NodeBuilder::NodeOut fetch_tensor,\n                        Node** out_node) override;\n \n  private:\n-  const int32 retval_index_;\n+  const int32_t retval_index_;\n };\n \n // A rewrite action that adds a client-terminated _Send node for a"
        },
        {
            "sha": "30caf3857e303cdd75b78295cfde1a32b17b0024",
            "filename": "tensorflow/core/graph/tensor_id.h",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Ftensor_id.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Ftensor_id.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Ftensor_id.h?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -44,7 +44,7 @@ struct TensorId : public std::pair<absl::string_view, int> {\n   const absl::string_view node() const { return first; }\n   int index() const { return second; }\n \n-  string ToString() const {\n+  std::string ToString() const {\n     if (second == Graph::kControlSlot) return absl::StrCat(\"^\", first);\n     return absl::StrCat(first, \":\", second);\n   }\n@@ -63,19 +63,19 @@ bool IsTensorIdControl(const TensorId& tensor_id);\n \n // Same as TensorId, except owns the backing storage for the op name. This makes\n // the memory management simpler at the expense of a copy.\n-struct SafeTensorId : public std::pair<string, int> {\n-  typedef std::pair<string, int> Base;\n+struct SafeTensorId : public std::pair<std::string, int> {\n+  typedef std::pair<std::string, int> Base;\n \n   // NOTE(skyewm): this is required on some platforms. I'm not sure why the\n   // using \"using Base::pair;\" isn't always sufficient.\n   SafeTensorId() : Base() {}\n-  SafeTensorId(const string& str, int idx) : Base(str, idx) {}\n+  SafeTensorId(const std::string& str, int idx) : Base(str, idx) {}\n   SafeTensorId(const TensorId& id);\n \n-  const string& node() const { return first; }\n+  const std::string& node() const { return first; }\n   int index() const { return second; }\n \n-  string ToString() const {\n+  std::string ToString() const {\n     if (second == Graph::kControlSlot) return absl::StrCat(\"^\", first);\n     return absl::StrCat(first, \":\", second);\n   }"
        },
        {
            "sha": "4bec9298680b7861d64152446f5a539fa3d4128f",
            "filename": "tensorflow/core/graph/tensor_id_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Ftensor_id_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Ftensor_id_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Ftensor_id_test.cc?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -23,7 +23,9 @@ limitations under the License.\n namespace tensorflow {\n namespace {\n \n-string ParseHelper(const string& n) { return ParseTensorName(n).ToString(); }\n+std::string ParseHelper(const std::string& n) {\n+  return ParseTensorName(n).ToString();\n+}\n \n TEST(TensorIdTest, ParseTensorName) {\n   EXPECT_EQ(ParseHelper(\"W1\"), \"W1:0\");\n@@ -35,18 +37,18 @@ TEST(TensorIdTest, ParseTensorName) {\n   EXPECT_EQ(ParseHelper(\"^foo\"), \"^foo\");\n }\n \n-uint32 Skewed(random::SimplePhilox* rnd, int max_log) {\n-  const uint32 space = 1 << (rnd->Rand32() % (max_log + 1));\n+uint32_t Skewed(random::SimplePhilox* rnd, int max_log) {\n+  const uint32_t space = 1 << (rnd->Rand32() % (max_log + 1));\n   return rnd->Rand32() % space;\n }\n \n void BM_ParseTensorName(::testing::benchmark::State& state) {\n   const int arg = state.range(0);\n   random::PhiloxRandom philox(301, 17);\n   random::SimplePhilox rnd(&philox);\n-  std::vector<string> names;\n+  std::vector<std::string> names;\n   for (int i = 0; i < 100; i++) {\n-    string name;\n+    std::string name;\n     switch (arg) {\n       case 0: {  // Generate random names\n         size_t len = Skewed(&rnd, 4);\n@@ -92,7 +94,7 @@ void BM_ParseTensorName(::testing::benchmark::State& state) {\n BENCHMARK(BM_ParseTensorName)->Arg(0)->Arg(1)->Arg(2)->Arg(3)->Arg(4)->Arg(5);\n \n TEST(TensorIdTest, IsTensorIdControl) {\n-  string input = \"^foo\";\n+  std::string input = \"^foo\";\n   TensorId tensor_id = ParseTensorName(input);\n   EXPECT_TRUE(IsTensorIdControl(tensor_id));\n \n@@ -106,7 +108,7 @@ TEST(TensorIdTest, IsTensorIdControl) {\n }\n \n TEST(TensorIdTest, PortZero) {\n-  for (string input : {\"foo\", \"foo:0\"}) {\n+  for (std::string input : {\"foo\", \"foo:0\"}) {\n     TensorId tensor_id = ParseTensorName(input);\n     EXPECT_EQ(\"foo\", tensor_id.node());\n     EXPECT_EQ(0, tensor_id.index());"
        },
        {
            "sha": "b882361aa8093ee606d5f71b3abe246411acf7f1",
            "filename": "tensorflow/core/graph/testlib.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 19,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Ftestlib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Ftestlib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Ftestlib.cc?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -32,8 +32,9 @@ namespace tensorflow {\n namespace test {\n namespace graph {\n \n-Node* Send(Graph* g, Node* input, const string& tensor, const string& sender,\n-           const uint64 sender_incarnation, const string& receiver) {\n+Node* Send(Graph* g, Node* input, const std::string& tensor,\n+           const std::string& sender, const uint64_t sender_incarnation,\n+           const std::string& receiver) {\n   Node* ret;\n   TF_CHECK_OK(NodeBuilder(g->NewName(\"n\"), \"_Send\")\n                   .Input(input, 0)\n@@ -46,9 +47,9 @@ Node* Send(Graph* g, Node* input, const string& tensor, const string& sender,\n   return ret;\n }\n \n-Node* Recv(Graph* g, const string& tensor, const string& type,\n-           const string& sender, const uint64 sender_incarnation,\n-           const string& receiver) {\n+Node* Recv(Graph* g, const std::string& tensor, const std::string& type,\n+           const std::string& sender, const uint64_t sender_incarnation,\n+           const std::string& receiver) {\n   Node* ret;\n   DataType dtype;\n   CHECK(DataTypeFromString(type, &dtype));\n@@ -72,7 +73,7 @@ Node* Constant(Graph* g, const Tensor& tensor) {\n   return ret;\n }\n \n-Node* Constant(Graph* g, const Tensor& tensor, const string& name) {\n+Node* Constant(Graph* g, const Tensor& tensor, const std::string& name) {\n   Node* ret;\n   TF_CHECK_OK(NodeBuilder(name, \"Const\")\n                   .Attr(\"dtype\", tensor.dtype())\n@@ -85,7 +86,7 @@ Node* HostConstant(Graph* g, const Tensor& tensor) {\n   return HostConstant(g, tensor, g->NewName(\"n\"));\n }\n \n-Node* HostConstant(Graph* g, const Tensor& tensor, const string& name) {\n+Node* HostConstant(Graph* g, const Tensor& tensor, const std::string& name) {\n   Node* ret;\n   TF_CHECK_OK(NodeBuilder(name, \"HostConst\")\n                   .Attr(\"dtype\", tensor.dtype())\n@@ -104,7 +105,7 @@ Node* Var(Graph* g, const DataType dtype, const TensorShape& shape) {\n }\n \n Node* Var(Graph* g, const DataType dtype, const TensorShape& shape,\n-          const string& name) {\n+          const std::string& name) {\n   Node* ret;\n   TF_CHECK_OK(NodeBuilder(name, \"Variable\")\n                   .Attr(\"dtype\", dtype)\n@@ -134,7 +135,7 @@ Node* Cumsum(Graph* g, Node* data, Node* axes, bool exclusive, bool reverse) {\n   return ret;\n }\n \n-Node* Reduce(Graph* g, const string& reduce, Node* data, Node* axes,\n+Node* Reduce(Graph* g, const std::string& reduce, Node* data, Node* axes,\n              bool keep_dims) {\n   Node* ret;\n   TF_CHECK_OK(NodeBuilder(g->NewName(\"n\"), reduce, g->op_registry())\n@@ -179,7 +180,7 @@ Node* BatchMatmul(Graph* g, Node* in0, Node* in1, bool adj_x, bool adj_y) {\n   return ret;\n }\n \n-Node* RandomNumberGenerator(const string& op, Graph* g, Node* input,\n+Node* RandomNumberGenerator(const std::string& op, Graph* g, Node* input,\n                             DataType dtype) {\n   Node* ret;\n   TF_CHECK_OK(NodeBuilder(g->NewName(\"n\"), op, g->op_registry())\n@@ -222,15 +223,15 @@ Node* RandomPoisson(Graph* g, Node* shape, Node* lam) {\n   return ret;\n }\n \n-Node* Unary(Graph* g, const string& func, Node* input, int index) {\n+Node* Unary(Graph* g, const std::string& func, Node* input, int index) {\n   Node* ret;\n   TF_CHECK_OK(NodeBuilder(g->NewName(\"n\"), func, g->op_registry())\n                   .Input(input, index)\n                   .Finalize(g, &ret));\n   return ret;\n }\n \n-Node* Binary(Graph* g, const string& func, Node* in0, Node* in1) {\n+Node* Binary(Graph* g, const std::string& func, Node* in0, Node* in1) {\n   Node* ret;\n   TF_CHECK_OK(NodeBuilder(g->NewName(\"n\"), func, g->op_registry())\n                   .Input(in0)\n@@ -239,7 +240,7 @@ Node* Binary(Graph* g, const string& func, Node* in0, Node* in1) {\n   return ret;\n }\n \n-Node* Multi(Graph* g, const string& func, absl::Span<Node* const> ins) {\n+Node* Multi(Graph* g, const std::string& func, absl::Span<Node* const> ins) {\n   Node* ret;\n   auto b = NodeBuilder(g->NewName(\"n\"), func, g->op_registry());\n   for (Node* n : ins) b = b.Input(n);\n@@ -271,7 +272,7 @@ Node* Roll(Graph* g, Node* input, Node* shift, Node* axis) {\n   return ret;\n }\n \n-Node* Error(Graph* g, Node* input, const string& errmsg, bool log_error) {\n+Node* Error(Graph* g, Node* input, const std::string& errmsg, bool log_error) {\n   Node* ret;\n   TF_CHECK_OK(NodeBuilder(g->NewName(\"n\"), \"Error\")\n                   .Input(input)\n@@ -317,7 +318,7 @@ Node* Switch(Graph* g, Node* in0, Node* in1) {\n   return ret;\n }\n \n-Node* Enter(Graph* g, Node* input, const string& frame_name) {\n+Node* Enter(Graph* g, Node* input, const std::string& frame_name) {\n   Node* ret;\n   TF_CHECK_OK(NodeBuilder(g->NewName(\"n\"), \"Enter\")\n                   .Input(input)\n@@ -341,11 +342,11 @@ Node* Merge(Graph* g, Node* in0, Node* in1) {\n   return ret;\n }\n \n-Node* Merge(Graph* g, Node* in0, absl::Span<const string> remaining_in) {\n+Node* Merge(Graph* g, Node* in0, absl::Span<const std::string> remaining_in) {\n   std::vector<NodeBuilder::NodeOut> inputs;\n   inputs.reserve(remaining_in.size() + 1);\n   inputs.emplace_back(in0);\n-  for (const string& in_name : remaining_in) {\n+  for (const std::string& in_name : remaining_in) {\n     inputs.emplace_back(in_name, 0, inputs[0].dt);\n   }\n \n@@ -383,7 +384,7 @@ Node* ConcatV2(Graph* g, absl::Span<Node* const> tensors, Node* concat_dim) {\n   return ret;\n }\n \n-Node* Next(Graph* g, const string& name, Node* input) {\n+Node* Next(Graph* g, const std::string& name, Node* input) {\n   Node* ret;\n   TF_CHECK_OK(\n       NodeBuilder(name, \"NextIteration\").Input(input).Finalize(g, &ret));\n@@ -497,7 +498,7 @@ Node* DiagPart(Graph* g, Node* in, DataType type) {\n   return ret;\n }\n \n-Node* CheckNumerics(Graph* g, Node* in, const string& message) {\n+Node* CheckNumerics(Graph* g, Node* in, const std::string& message) {\n   Node* ret;\n   TF_CHECK_OK(NodeBuilder(g->NewName(\"n\"), \"CheckNumerics\")\n                   .Input(in)"
        },
        {
            "sha": "f4df5a4ed4d0380e553b5eb94f8a33ff80b66e6d",
            "filename": "tensorflow/core/graph/testlib.h",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Ftestlib.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Ftestlib.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Ftestlib.h?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -39,42 +39,43 @@ void ToGraphDef(Graph* g, GraphDef* def);\n \n // Adds a node in \"g\" producing a constant \"tensor\".\n Node* Constant(Graph* g, const Tensor& tensor);\n-Node* Constant(Graph* g, const Tensor& tensor, const string& name);\n+Node* Constant(Graph* g, const Tensor& tensor, const std::string& name);\n \n // Adds a node in \"g\" producing a constant \"tensor\" on the host.\n // The given node which, unlike the regular Constant above, always\n // stores its output on the host.  This is necessary for use\n // in GPU tests where the test Op in question runs on the device\n // but requires some arguments to be pinned to the host.\n Node* HostConstant(Graph* g, const Tensor& tensor);\n-Node* HostConstant(Graph* g, const Tensor& tensor, const string& name);\n+Node* HostConstant(Graph* g, const Tensor& tensor, const std::string& name);\n \n // Adds a variable in \"g\" of the given \"shape\" and \"dtype\".\n Node* Var(Graph* g, DataType dtype, const TensorShape& shape);\n Node* Var(Graph* g, DataType dtype, const TensorShape& shape,\n-          const string& name);\n+          const std::string& name);\n \n // Adds an assign node in \"g\" which assigns \"val\" into \"var\".\n Node* Assign(Graph* g, Node* var, Node* val);\n \n // Adds a send node \"g\" sending \"input\" as a named \"tensor\" from\n // \"sender\" to \"receiver\".\n-Node* Send(Graph* g, Node* input, const string& tensor, const string& sender,\n-           uint64 sender_incarnation, const string& receiver);\n+Node* Send(Graph* g, Node* input, const std::string& tensor,\n+           const std::string& sender, uint64_t sender_incarnation,\n+           const std::string& receiver);\n \n // Adds a recv node in \"g\" receiving a named \"tensor\" from \"sender\"\n // to \"receiver\".\n-Node* Recv(Graph* g, const string& tensor, const string& type,\n-           const string& sender, uint64 sender_incarnation,\n-           const string& receiver);\n+Node* Recv(Graph* g, const std::string& tensor, const std::string& type,\n+           const std::string& sender, uint64_t sender_incarnation,\n+           const std::string& receiver);\n \n // Adds a cumsum \"node\" in \"g\" doing cumsum(data, axes).\n Node* Cumsum(Graph* g, Node* data, Node* axes, bool exclusive = false,\n              bool reverse = false);\n \n // Adds a reduction \"node\" in \"g\" doing sum(data, axes).  \"reduce\" is\n // a reduction, e.g., Sum, Max, Min, Mean, etc.\n-Node* Reduce(Graph* g, const string& reduce, Node* data, Node* axes,\n+Node* Reduce(Graph* g, const std::string& reduce, Node* data, Node* axes,\n              bool keep_dims = false);\n \n // Adds a Matmul node in g doing in0.contract(in1).\n@@ -89,17 +90,17 @@ Node* BatchMatmul(Graph* g, Node* in0, Node* in1, bool adj_x, bool adj_y);\n Node* QuantizeToUINT8(Graph* g, Node* data);\n \n // Adds a unary function \"func\" \"node\" in \"g\" taking \"input\".\n-Node* Unary(Graph* g, const string& func, Node* input, int index = 0);\n+Node* Unary(Graph* g, const std::string& func, Node* input, int index = 0);\n \n // Adds an identity node in \"g\" taking \"input\" and producing an\n // identity copy.\n Node* Identity(Graph* g, Node* input, int index = 0);\n \n // Adds a binary function \"func\" node in \"g\" taking \"in0\" and \"in1\".\n-Node* Binary(Graph* g, const string& func, Node* in0, Node* in1);\n+Node* Binary(Graph* g, const std::string& func, Node* in0, Node* in1);\n \n // Adds a function \"func\" node in \"g\" taking inputs \"ins\".\n-Node* Multi(Graph* g, const string& func, absl::Span<Node* const> ins);\n+Node* Multi(Graph* g, const std::string& func, absl::Span<Node* const> ins);\n \n // Adds a binary add node in \"g\" doing in0 + in1.\n Node* Add(Graph* g, Node* in0, Node* in1);\n@@ -131,7 +132,7 @@ Node* TruncatedNormal(Graph* g, Node* input, DataType dtype);\n \n // Adds an error node in \"g\". The node's computation always\n // generates an error with the given error message \"errmsg\".\n-Node* Error(Graph* g, Node* input, const string& errmsg,\n+Node* Error(Graph* g, Node* input, const std::string& errmsg,\n             bool log_error = false);\n \n // Adds a node that generates a invalid ref output.\n@@ -150,7 +151,7 @@ Node* NoOp(Graph* g, const std::vector<Node*>& control_inputs);\n Node* Switch(Graph* g, Node* in0, Node* in1);\n \n // Adds an Enter node in \"g\", which enters a new frame.\n-Node* Enter(Graph* g, Node* input, const string& frame_name);\n+Node* Enter(Graph* g, Node* input, const std::string& frame_name);\n \n // Adds an Exit node in \"g\", which exits a frame.\n Node* Exit(Graph* g, Node* input);\n@@ -160,11 +161,11 @@ Node* Merge(Graph* g, Node* in0, Node* in1);\n \n // Adds a Merge node in \"g\". The first input is \"in0\", the remaining\n // inputs are only given by their names in remaining_in.\n-Node* Merge(Graph* g, Node* in0, absl::Span<const string> remaining_in);\n+Node* Merge(Graph* g, Node* in0, absl::Span<const std::string> remaining_in);\n \n // Adds a NextIteration node in \"g\", which makes its input available\n // to the next iteration.\n-Node* Next(Graph* g, const string& name, Node* input);\n+Node* Next(Graph* g, const std::string& name, Node* input);\n \n // Adds a LoopCond node in \"g\", representing the \"pivot\" termination\n // condition of a loop.\n@@ -215,7 +216,7 @@ Node* Diag(Graph* g, Node* in, DataType type);\n Node* DiagPart(Graph* g, Node* in, DataType type);\n \n // Add a CheckNumerics node in \"g\".\n-Node* CheckNumerics(Graph* g, Node* in, const string& message);\n+Node* CheckNumerics(Graph* g, Node* in, const std::string& message);\n \n // Add an _Arg node in \"g\".\n Node* Arg(Graph* g, int64_t index, DataType type);"
        },
        {
            "sha": "4572ceb9de789747874a66155044d43806c2dfec",
            "filename": "tensorflow/core/graph/validate.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fvalidate.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fvalidate.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fvalidate.cc?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -100,7 +100,7 @@ absl::Status ValidateGraphHasNoCycle(const Graph& graph) {\n   }\n \n   if (processed < graph.num_nodes()) {\n-    std::vector<string> nodes_in_cycle;\n+    std::vector<std::string> nodes_in_cycle;\n     for (int i = 0; i < pending_count.size() && nodes_in_cycle.size() < 3;\n          ++i) {\n       if (pending_count[i] != 0) {"
        },
        {
            "sha": "35e7ebb4cff6e0ce09f6b6befbd7b0daaa080137",
            "filename": "tensorflow/core/graph/validate_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fvalidate_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fvalidate_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fvalidate_test.cc?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -38,7 +38,7 @@ REGISTER_OP(\"FloatInput\").Output(\"o: float\");\n REGISTER_OP(\"Int32Input\").Output(\"o: int32\");\n \n TEST(ValidateGraphDefTest, TestValidGraph) {\n-  const string graph_def_str =\n+  const std::string graph_def_str =\n       \"node { name: 'A' op: 'FloatInput' }\"\n       \"node { name: 'B' op: 'FloatInput' }\"\n       \"node { name: 'C' op: 'Mul' attr { key: 'T' value { type: DT_FLOAT } }\"\n@@ -50,7 +50,7 @@ TEST(ValidateGraphDefTest, TestValidGraph) {\n }\n \n TEST(ValidateGraphDefTest, GraphWithUnspecifiedDefaultAttr) {\n-  const string graph_def_str =\n+  const std::string graph_def_str =\n       \"node { name: 'A' op: 'FloatInput' }\"\n       \"node { name: 'B' op: 'Int32Input' }\"\n       \"node { \"\n@@ -74,7 +74,7 @@ TEST(ValidateGraphDefTest, GraphWithUnspecifiedDefaultAttr) {\n \n TEST(ValidateGraphDefTest, GraphWithUnspecifiedRequiredAttr) {\n   // \"DstT\" attribute is missing.\n-  const string graph_def_str =\n+  const std::string graph_def_str =\n       \"node { name: 'A' op: 'FloatInput' }\"\n       \"node { \"\n       \"       name: 'B' op: 'Cast' \"\n@@ -102,7 +102,7 @@ TEST(ValidateGraphDefAgainstOpListTest, GraphWithOpOnlyInOpList) {\n   TF_ASSERT_OK(OpDefBuilder(\"UniqueSnowflake\").Finalize(&op_reg_data));\n   OpList op_list;\n   *op_list.add_op() = op_reg_data.op_def;\n-  const string graph_def_str = \"node { name: 'A' op: 'UniqueSnowflake' }\";\n+  const std::string graph_def_str = \"node { name: 'A' op: 'UniqueSnowflake' }\";\n   GraphDef graph_def;\n   auto parser = protobuf::TextFormat::Parser();\n   CHECK(parser.MergeFromString(graph_def_str, &graph_def)) << graph_def_str;\n@@ -114,7 +114,7 @@ TEST(ValidateGraphDefAgainstOpListTest, GraphWithGlobalOpNotInOpList) {\n   TF_ASSERT_OK(OpDefBuilder(\"NotAnywhere\").Finalize(&op_reg_data));\n   OpList op_list;\n   *op_list.add_op() = op_reg_data.op_def;\n-  const string graph_def_str = \"node { name: 'A' op: 'FloatInput' }\";\n+  const std::string graph_def_str = \"node { name: 'A' op: 'FloatInput' }\";\n   GraphDef graph_def;\n   auto parser = protobuf::TextFormat::Parser();\n   CHECK(parser.MergeFromString(graph_def_str, &graph_def)) << graph_def_str;\n@@ -150,7 +150,7 @@ TEST(GetOpListForValidationTest, ShouldStripDocs) {\n }\n \n TEST(VerifyNoDuplicateNodeNames, NoDuplicateNodeNames) {\n-  const string graph_def_str =\n+  const std::string graph_def_str =\n       \"node { name: 'A' op: 'FloatInput' }\"\n       \"node { name: 'B' op: 'Int32Input' }\"\n       \"node { \"\n@@ -165,7 +165,7 @@ TEST(VerifyNoDuplicateNodeNames, NoDuplicateNodeNames) {\n }\n \n TEST(VerifyNoDuplicateNodeNames, DuplicateNodeNames) {\n-  const string graph_def_str =\n+  const std::string graph_def_str =\n       \"node { name: 'A' op: 'FloatInput' }\"\n       \"node { name: 'A' op: 'Int32Input' }\"\n       \"node { \"\n@@ -181,7 +181,7 @@ TEST(VerifyNoDuplicateNodeNames, DuplicateNodeNames) {\n }\n \n TEST(ValidateGraphHasNoCycleTest, NoCyclePasses) {\n-  const string graph_def_str =\n+  const std::string graph_def_str =\n       \"node { name: 'A' op: 'FloatInput' }\"\n       \"node { name: 'B' op: 'FloatInput' }\"\n       \"node { name: 'C' op: 'Mul' attr { key: 'T' value { type: DT_FLOAT } }\"\n@@ -198,7 +198,7 @@ TEST(ValidateGraphHasNoCycleTest, NoCyclePasses) {\n }\n \n TEST(ValidateGraphHasNoCycleTest, NoCycleWithMergePasses) {\n-  const string graph_def_str =\n+  const std::string graph_def_str =\n       R\"EOF(\n       node { name: 'A' op: 'FloatInput' }\n       node { name: 'merge' op: 'Merge' input: [ 'A:0', 'next:0' ]\n@@ -221,8 +221,8 @@ TEST(ValidateGraphHasNoCycleTest, NoCycleWithMergePasses) {\n   TF_EXPECT_OK(graph::ValidateGraphHasNoCycle(graph));\n }\n \n-Node* AddNodeFromNodeDef(Graph& graph, const string& name,\n-                         const string& node_type, int num_inputs) {\n+Node* AddNodeFromNodeDef(Graph& graph, const std::string& name,\n+                         const std::string& node_type, int num_inputs) {\n   auto builder = NodeDefBuilder(name, node_type);\n   for (int i = 0; i < num_inputs; ++i) {\n     builder = builder.Input(absl::StrCat(\"node_\", i), i, DT_FLOAT);"
        },
        {
            "sha": "4f15b7d37c7b18565e4e8c8340d52e4049df41e5",
            "filename": "tensorflow/core/graph/while_context.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fwhile_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7addf9852a1472f9fadc87de8ec29f0b355f5c63/tensorflow%2Fcore%2Fgraph%2Fwhile_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fgraph%2Fwhile_context.h?ref=7addf9852a1472f9fadc87de8ec29f0b355f5c63",
            "patch": "@@ -39,7 +39,7 @@ class WhileContext {\n                std::vector<OutputTensor> body_inputs,\n                std::vector<OutputTensor> body_outputs);\n \n-  const string& frame_name() const { return frame_name_; }\n+  const std::string& frame_name() const { return frame_name_; }\n   const std::vector<Node*>& enter_nodes() const { return enter_nodes_; }\n   const std::vector<Node*>& exit_nodes() const { return exit_nodes_; }\n   const OutputTensor& cond_output() const { return cond_output_; }\n@@ -53,7 +53,7 @@ class WhileContext {\n   // uniquely identified by its frame name. Frames are used by the executor to\n   // manage the iterations of a loop. See the FrameState comment in\n   // core/common_runtime/executor.cc for more details.\n-  const string frame_name_;\n+  const std::string frame_name_;\n \n   // The enter nodes defining the input loop variables to the while loop. This\n   // vector defines the order of the loop variables."
        }
    ],
    "stats": {
        "total": 638,
        "additions": 334,
        "deletions": 304
    }
}