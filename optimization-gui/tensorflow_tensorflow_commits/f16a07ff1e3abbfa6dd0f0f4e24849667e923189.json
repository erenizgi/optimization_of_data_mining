{
    "author": "WillFroom",
    "message": "[XLA:CPU][XTile] Read directly from the input tensor rather than via intermediate vector in vectorized reduce.\n\nThis removes the need for the dynamic vector extract pass and will also pave the way for tree-reduction and better bufferization.\n\nPiperOrigin-RevId: 831422921",
    "sha": "f16a07ff1e3abbfa6dd0f0f4e24849667e923189",
    "files": [
        {
            "sha": "fa426e7d534a628bf6c09ba775637f5d2068ec8c",
            "filename": "third_party/xla/xla/backends/cpu/codegen/fusion_compiler.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f16a07ff1e3abbfa6dd0f0f4e24849667e923189/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f16a07ff1e3abbfa6dd0f0f4e24849667e923189/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ffusion_compiler.cc?ref=f16a07ff1e3abbfa6dd0f0f4e24849667e923189",
            "patch": "@@ -310,7 +310,6 @@ static void AddTiledOptimizationPasses(mlir::OpPassManager& pm) {\n   pm.addPass(CreateShloToVectorPass());\n   pm.addPass(CreateXTileToVectorPass());\n   pm.addPass(mlir::createCanonicalizerPass());\n-  pm.addPass(CreateRewriteDynamicVectorExtractPass());\n   pm.addPass(CreateElementalTensorToVectorPass());\n   pm.addPass(CreateLowerXTileEntryPass());\n   pm.addNestedPass<mlir::func::FuncOp>("
        },
        {
            "sha": "cca2b909d27cfead88e880c9e12791cee7ddc162",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f16a07ff1e3abbfa6dd0f0f4e24849667e923189/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f16a07ff1e3abbfa6dd0f0f4e24849667e923189/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2FBUILD?ref=f16a07ff1e3abbfa6dd0f0f4e24849667e923189",
            "patch": "@@ -51,7 +51,6 @@ cc_library(\n         \"elemental_tensor_to_vector.cc\",\n         \"lower_xtile_entry.cc\",\n         \"memref_copy_to_loops.cc\",\n-        \"rewrite_dynamic_vector_extract.cc\",\n         \"shlo_to_vector.cc\",\n         \"tensor_ops_to_vector.cc\",\n         \"xtile_to_vector.cc\",\n@@ -68,9 +67,9 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@llvm-project//llvm:Support\",\n-        \"@llvm-project//mlir:Analysis\",\n         \"@llvm-project//mlir:ArithDialect\",\n         \"@llvm-project//mlir:ArithOpsIncGen\",\n+        \"@llvm-project//mlir:BufferizationDialect\",\n         \"@llvm-project//mlir:DataLayoutInterfaces\",\n         \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:FuncTransforms\",\n@@ -104,11 +103,13 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:ArithDialect\",\n+        \"@llvm-project//mlir:BufferizationDialect\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:LinalgTransforms\",\n         \"@llvm-project//mlir:MemRefDialect\",\n         \"@llvm-project//mlir:SCFDialect\",\n         \"@llvm-project//mlir:Support\",\n+        \"@llvm-project//mlir:TensorDialect\",\n         \"@llvm-project//mlir:VectorDialect\",\n     ],\n )"
        },
        {
            "sha": "f81b433897829f981c68679e91a9c247b179ebf7",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/passes.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f16a07ff1e3abbfa6dd0f0f4e24849667e923189/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f16a07ff1e3abbfa6dd0f0f4e24849667e923189/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.h?ref=f16a07ff1e3abbfa6dd0f0f4e24849667e923189",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <memory>\n \n #include \"mlir/Dialect/Arith/IR/Arith.h\"  // IWYU pragma: keep\n+#include \"mlir/Dialect/Bufferization/IR/Bufferization.h\"  // IWYU pragma: keep\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"  // IWYU pragma: keep\n #include \"mlir/Dialect/Math/IR/Math.h\"  // IWYU pragma: keep\n #include \"mlir/Dialect/SCF/IR/SCF.h\"  // IWYU pragma: keep\n@@ -38,7 +39,6 @@ std::unique_ptr<mlir::Pass> CreateLowerXTileEntryPass();\n std::unique_ptr<mlir::Pass> CreateShloToVectorPass();\n std::unique_ptr<mlir::Pass> CreateXTileToVectorPass();\n std::unique_ptr<mlir::Pass> CreateTensorOpsToVectorPass();\n-std::unique_ptr<mlir::Pass> CreateRewriteDynamicVectorExtractPass();\n std::unique_ptr<mlir::Pass> CreateMemrefCopyToLoopsPass();\n \n #define GEN_PASS_REGISTRATION"
        },
        {
            "sha": "4932c36355e93163991d5d8f2aadf11d23039208",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/passes.td",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f16a07ff1e3abbfa6dd0f0f4e24849667e923189/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f16a07ff1e3abbfa6dd0f0f4e24849667e923189/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fpasses.td?ref=f16a07ff1e3abbfa6dd0f0f4e24849667e923189",
            "patch": "@@ -47,6 +47,7 @@ def ShloToVectorPass : Pass<\"xtile-cpu-shlo-to-vector\", \"mlir::ModuleOp\"> {\n \n   let dependentDialects = [\n     \"mlir::arith::ArithDialect\",\n+    \"mlir::bufferization::BufferizationDialect\",\n     \"mlir::memref::MemRefDialect\",\n     \"mlir::scf::SCFDialect\",\n     \"mlir::stablehlo::StablehloDialect\",\n@@ -80,17 +81,6 @@ def TensorOpsToVectorPass : Pass<\"xtile-cpu-tensor-ops-to-vector\",\n   ];\n }\n \n-def RewriteDynamicVectorExtractPass : Pass<\"xtile-cpu-rewrite-dynamic-vector-extract\",\n-                                 \"mlir::ModuleOp\"> {\n-  let summary = \"Rewrite vector.extracts with dynamic indices.\";\n-\n-  let constructor = \"CreateRewriteDynamicVectorExtractPass()\";\n-\n-  let dependentDialects = [\n-    \"::mlir::vector::VectorDialect\",\n-    \"::mlir::memref::MemRefDialect\",\n-  ];\n-}\n \n def MemrefCopyToLoopsPass : Pass<\"xtile-cpu-memref-copy-to-loops\",\n                                  \"mlir::ModuleOp\"> {"
        },
        {
            "sha": "ec5d86f460ade85d5cfefbba2c9e62729468c756",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/rewrite_dynamic_vector_extract.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 329,
            "changes": 329,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e57bfcf087a4dec906b05b8c57abcd82fc09c3f2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Frewrite_dynamic_vector_extract.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e57bfcf087a4dec906b05b8c57abcd82fc09c3f2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Frewrite_dynamic_vector_extract.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Frewrite_dynamic_vector_extract.cc?ref=e57bfcf087a4dec906b05b8c57abcd82fc09c3f2",
            "patch": "@@ -1,329 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include <cassert>\n-#include <cstdint>\n-#include <memory>\n-#include <optional>\n-#include <utility>\n-\n-#include \"llvm/ADT/STLExtras.h\"\n-#include \"llvm/ADT/SmallVector.h\"\n-#include \"mlir/Analysis/SliceAnalysis.h\"\n-#include \"mlir/Dialect/Arith/IR/Arith.h\"\n-#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n-#include \"mlir/Dialect/SCF/IR/SCF.h\"\n-#include \"mlir/Dialect/SCF/Utils/Utils.h\"\n-#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n-#include \"mlir/IR/AffineExpr.h\"\n-#include \"mlir/IR/Attributes.h\"\n-#include \"mlir/IR/BuiltinAttributes.h\"\n-#include \"mlir/IR/BuiltinOps.h\"\n-#include \"mlir/IR/BuiltinTypes.h\"\n-#include \"mlir/IR/MLIRContext.h\"\n-#include \"mlir/IR/OpDefinition.h\"\n-#include \"mlir/IR/PatternMatch.h\"\n-#include \"mlir/IR/TypeUtilities.h\"\n-#include \"mlir/IR/Value.h\"\n-#include \"mlir/IR/Visitors.h\"\n-#include \"mlir/Interfaces/DataLayoutInterfaces.h\"\n-#include \"mlir/Pass/Pass.h\"\n-#include \"mlir/Support/LLVM.h\"\n-#include \"mlir/Support/WalkResult.h\"\n-#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n-#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h\"\n-\n-namespace xla::cpu {\n-\n-#define GEN_PASS_DECL_REWRITEDYNAMICVECTOREXTRACTPASS\n-#define GEN_PASS_DEF_REWRITEDYNAMICVECTOREXTRACTPASS\n-#include \"xla/backends/cpu/codegen/tiled/transforms/passes.h.inc\"\n-\n-namespace {\n-\n-// Check if the given extract operands depends on the given value.\n-bool ExtractDependsOnValue(mlir::vector::ExtractOp extract_op,\n-                           mlir::Value value) {\n-  mlir::BackwardSliceOptions backward_slice_options;\n-  backward_slice_options.omitUsesFromAbove = false;\n-  backward_slice_options.inclusive = true;\n-\n-  for (mlir::Value dynamic_index : extract_op.getDynamicPosition()) {\n-    // We have to explicitly check the index itself as getBackwardSlice only\n-    // starts from the defining operation.\n-    if (dynamic_index == value) {\n-      return true;\n-    }\n-\n-    llvm::SetVector<mlir::Operation*> backwardSlice;\n-    if (mlir::failed(mlir::getBackwardSlice(dynamic_index, &backwardSlice,\n-                                            backward_slice_options))) {\n-      continue;\n-    }\n-\n-    for (mlir::Operation* op : backwardSlice) {\n-      for (mlir::Value operand : op->getOperands()) {\n-        if (operand == value) {\n-          return true;\n-        }\n-      }\n-    }\n-  }\n-\n-  return false;\n-}\n-\n-struct FoldExtractIntoTransferRead\n-    : mlir::OpRewritePattern<mlir::vector::ExtractOp> {\n-  using OpRewritePattern::OpRewritePattern;\n-\n-  mlir::LogicalResult matchAndRewrite(\n-      mlir::vector::ExtractOp op,\n-      mlir::PatternRewriter& rewriter) const override {\n-    if (!op.hasDynamicPosition()) {\n-      return rewriter.notifyMatchFailure(\n-          op, \"extract does not have dynamic position\");\n-    }\n-\n-    auto transfer_read_op =\n-        op.getSource().getDefiningOp<mlir::vector::TransferReadOp>();\n-\n-    if (!transfer_read_op) {\n-      return rewriter.notifyMatchFailure(op,\n-                                         \"source is not a transfer_read op\");\n-    }\n-\n-    mlir::ValueRange transfer_read_indices = transfer_read_op.getIndices();\n-\n-    llvm::SmallVector<mlir::OpFoldResult> extended_positions(\n-        op.getMixedPosition());\n-    for (int64_t idx = extended_positions.size();\n-         idx < transfer_read_indices.size(); ++idx) {\n-      extended_positions.push_back(rewriter.getIndexAttr(0));\n-    }\n-\n-    llvm::SmallVector<mlir::Value> new_offsets;\n-    new_offsets.reserve(transfer_read_indices.size());\n-    for (auto [tile_offset, extract_offset] :\n-         llvm::zip(transfer_read_indices, extended_positions)) {\n-      if (auto static_position =\n-              mlir::dyn_cast<mlir::Attribute>(extract_offset)) {\n-        new_offsets.push_back(mlir::arith::AddIOp::create(\n-            rewriter, op.getLoc(), rewriter.getIndexType(), tile_offset,\n-            mlir::arith::ConstantIndexOp::create(\n-                rewriter, op.getLoc(),\n-                mlir::cast<mlir::IntegerAttr>(static_position).getInt())));\n-      } else {\n-        auto dynamic_position = mlir::dyn_cast<mlir::Value>(extract_offset);\n-        new_offsets.push_back(mlir::arith::AddIOp::create(\n-            rewriter, op.getLoc(), rewriter.getIndexType(), tile_offset,\n-            dynamic_position));\n-      }\n-    }\n-\n-    // Output of extract can be either a vector or a scalar.\n-    auto maybe_vector_type = mlir::dyn_cast<mlir::VectorType>(op.getType());\n-\n-    mlir::Value submask;\n-    if (auto mask = transfer_read_op.getMask()) {\n-      // Transfer read result and mask must be non-0D vectors.\n-      auto sub_mask_type = mlir::VectorType::get(\n-          maybe_vector_type ? maybe_vector_type.getShape() : 1,\n-          rewriter.getI1Type());\n-      submask = mlir::vector::ExtractOp::create(\n-          rewriter, op.getLoc(), sub_mask_type, mask, op.getDynamicPosition(),\n-          op.getStaticPosition());\n-    }\n-\n-    int64_t input_rank = transfer_read_op.getBase().getType().getRank();\n-    int64_t output_rank = maybe_vector_type ? maybe_vector_type.getRank() : 1;\n-\n-    // Drop major dimensions which reflects the behaviour of vector::ExtractOp.\n-    int64_t num_dropped_dims = input_rank - output_rank;\n-    mlir::AffineMap new_permutation_map =\n-        mlir::AffineMap::getFilteredIdentityMap(\n-            rewriter.getContext(), input_rank, [&](mlir::AffineDimExpr expr) {\n-              return expr.getPosition() >= num_dropped_dims;\n-            });\n-\n-    llvm::SmallVector<mlir::Attribute> in_bounds(\n-        transfer_read_op.getInBounds().begin() + num_dropped_dims,\n-        transfer_read_op.getInBounds().end());\n-\n-    auto output_type = mlir::VectorType::get(\n-        maybe_vector_type ? maybe_vector_type.getShape() : 1,\n-        mlir::getElementTypeOrSelf(op.getType()));\n-    auto new_transfer_read = mlir::vector::TransferReadOp::create(\n-        rewriter, op.getLoc(), output_type, transfer_read_op.getBase(),\n-        new_offsets, new_permutation_map, transfer_read_op.getPadding(),\n-        submask, rewriter.getArrayAttr(in_bounds));\n-\n-    if (maybe_vector_type) {\n-      rewriter.replaceOp(op, new_transfer_read);\n-    } else {\n-      rewriter.replaceOpWithNewOp<mlir::vector::ExtractOp>(\n-          op, new_transfer_read, 0);\n-    }\n-\n-    return mlir::success();\n-  }\n-};\n-\n-// FoldExtractIntoTransferRead creates its own dynamic extracts if a mask is\n-// present, so we need to fold these.\n-// We do this by shifting the offset and then extracting with static indices.\n-struct FoldExtractIntoCreateMask\n-    : mlir::OpRewritePattern<mlir::vector::ExtractOp> {\n-  using OpRewritePattern::OpRewritePattern;\n-\n-  mlir::LogicalResult matchAndRewrite(\n-      mlir::vector::ExtractOp op,\n-      mlir::PatternRewriter& rewriter) const override {\n-    if (!op.hasDynamicPosition()) {\n-      return rewriter.notifyMatchFailure(\n-          op, \"extract does not have dynamic position\");\n-    }\n-    auto mask_op = op.getSource().getDefiningOp<mlir::vector::CreateMaskOp>();\n-    if (!mask_op) {\n-      return rewriter.notifyMatchFailure(op, \"source is not a create_mask op\");\n-    }\n-\n-    mlir::ValueRange mask_operands = mask_op.getOperands();\n-\n-    llvm::SmallVector<mlir::OpFoldResult> extended_positions(\n-        op.getMixedPosition());\n-    for (int64_t idx = extended_positions.size(); idx < mask_operands.size();\n-         ++idx) {\n-      extended_positions.push_back(rewriter.getIndexAttr(0));\n-    }\n-\n-    llvm::SmallVector<mlir::Value> new_bounds;\n-    new_bounds.reserve(mask_operands.size());\n-    for (auto [mask_bound, extract_offset] :\n-         llvm::zip(mask_operands, extended_positions)) {\n-      if (auto static_position =\n-              mlir::dyn_cast<mlir::Attribute>(extract_offset)) {\n-        new_bounds.push_back(mlir::arith::SubIOp::create(\n-            rewriter, op.getLoc(), rewriter.getIndexType(), mask_bound,\n-            mlir::arith::ConstantIndexOp::create(\n-                rewriter, op.getLoc(),\n-                mlir::cast<mlir::IntegerAttr>(static_position).getInt())));\n-      } else {\n-        auto dynamic_position = mlir::dyn_cast<mlir::Value>(extract_offset);\n-        new_bounds.push_back(mlir::arith::SubIOp::create(\n-            rewriter, op.getLoc(), rewriter.getIndexType(), mask_bound,\n-            dynamic_position));\n-      }\n-    }\n-\n-    // As we are going to extract only the output shape vector from the minor\n-    // dimensions we only need to create a mask of size 1 for the remaining\n-    // dimensions.\n-    llvm::SmallVector<int64_t> new_mask_shape;\n-    if (auto output_shape = mlir::dyn_cast<mlir::ShapedType>(op.getType())) {\n-      new_mask_shape.assign(\n-          mask_op.getType().getRank() - output_shape.getRank(), 1);\n-      new_mask_shape.append(output_shape.getShape().begin(),\n-                            output_shape.getShape().end());\n-    } else {\n-      new_mask_shape.assign(mask_op.getType().getRank(), 1);\n-    }\n-\n-    auto shifted_mask = mlir::vector::CreateMaskOp::create(\n-        rewriter, op.getLoc(),\n-        mlir::VectorType::get(new_mask_shape, rewriter.getI1Type()),\n-        new_bounds);\n-\n-    llvm::SmallVector<int64_t> zero_index(op.getMixedPosition().size(), 0);\n-    rewriter.replaceOpWithNewOp<mlir::vector::ExtractOp>(\n-        op, op.getType(), shifted_mask, mlir::ValueRange{}, zero_index);\n-\n-    return mlir::success();\n-  }\n-};\n-\n-// Unroll loops that have a vector.extract that depend on the loop induction\n-// variable.\n-struct UnrollExtractLoops : mlir::OpRewritePattern<mlir::scf::ForOp> {\n-  using OpRewritePattern::OpRewritePattern;\n-\n-  mlir::LogicalResult matchAndRewrite(\n-      mlir::scf::ForOp for_op, mlir::PatternRewriter& rewriter) const override {\n-    std::optional<mlir::LogicalResult> unroll_result;\n-    // Walk the body of the loop and unroll if we have a dependent extract.\n-    for_op.getBody()->walk([&](mlir::vector::ExtractOp extract) {\n-      if (!ExtractDependsOnValue(extract, for_op.getInductionVar())) {\n-        return mlir::WalkResult::advance();\n-      }\n-      unroll_result = mlir::loopUnrollFull(for_op);\n-      return mlir::WalkResult::interrupt();\n-    });\n-\n-    if (!unroll_result.has_value()) {\n-      return rewriter.notifyMatchFailure(\n-          for_op, \"loop does not contain a dependent extract\");\n-    }\n-\n-    if (mlir::failed(*unroll_result)) {\n-      return rewriter.notifyMatchFailure(for_op, \"failed to unroll loop\");\n-    }\n-\n-    return mlir::success();\n-  }\n-};\n-\n-class RewriteDynamicVectorExtractPass\n-    : public impl::RewriteDynamicVectorExtractPassBase<\n-          RewriteDynamicVectorExtractPass> {\n- public:\n-  using RewriteDynamicVectorExtractPassBase::\n-      RewriteDynamicVectorExtractPassBase;\n-\n-  void runOnOperation() override {\n-    mlir::ModuleOp module = getOperation();\n-    mlir::MLIRContext* context = &getContext();\n-\n-    {\n-      mlir::RewritePatternSet patterns(context);\n-      patterns.add<FoldExtractIntoTransferRead, FoldExtractIntoCreateMask>(\n-          context);\n-      if (mlir::failed(\n-              mlir::applyPatternsGreedily(module, std::move(patterns)))) {\n-        signalPassFailure();\n-        return;\n-      }\n-    }\n-\n-    // As a final sledge hammer, we can unroll the loops if we have any\n-    // dependent extracts.\n-    {\n-      mlir::RewritePatternSet patterns(context);\n-      patterns.add<UnrollExtractLoops>(context);\n-      if (mlir::failed(\n-              mlir::applyPatternsGreedily(module, std::move(patterns)))) {\n-        signalPassFailure();\n-        return;\n-      }\n-    }\n-  }\n-};\n-\n-}  // namespace\n-\n-std::unique_ptr<mlir::Pass> CreateRewriteDynamicVectorExtractPass() {\n-  return std::make_unique<RewriteDynamicVectorExtractPass>();\n-}\n-\n-}  // namespace xla::cpu"
        },
        {
            "sha": "e92d15797552c4548f6ce1153b5b8f2f7afc847b",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/shlo_to_vector.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f16a07ff1e3abbfa6dd0f0f4e24849667e923189/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fshlo_to_vector.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f16a07ff1e3abbfa6dd0f0f4e24849667e923189/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fshlo_to_vector.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fshlo_to_vector.cc?ref=f16a07ff1e3abbfa6dd0f0f4e24849667e923189",
            "patch": "@@ -236,29 +236,25 @@ struct LowerReduce : mlir::OpRewritePattern<mlir::stablehlo::ReduceOp> {\n           op, \"reduce op with multiple results is not supported\");\n     }\n \n-    mlir::TypedValue<mlir::VectorType> source_vector =\n-        ReadTensorToVector(rewriter, op.getInputs().front());\n-    mlir::VectorType source_vector_type = source_vector.getType();\n-\n-    mlir::Value init_value = rewriter.create<mlir::tensor::ExtractOp>(\n-        op->getLoc(), source_vector_type.getElementType(),\n-        op.getInitValues().front());\n-\n+    auto source_tensor = mlir::cast<mlir::TypedValue<mlir::RankedTensorType>>(\n+        op.getInputs().front());\n     mlir::Value result_tensor = op.getResult(0);\n-    auto result_tensor_type =\n+    auto result_type =\n         mlir::cast<mlir::RankedTensorType>(result_tensor.getType());\n-    auto result_vector_type = GetVectorType(result_tensor_type);\n+\n+    mlir::Value init_value = rewriter.create<mlir::tensor::ExtractOp>(\n+        op->getLoc(), result_type.getElementType(), op.getInitValues().front());\n \n     // Ensure the reduction dimensions are sorted so we can easily check if the\n     // minor dimension is reduced.\n     llvm::SmallVector<int64_t> reduction_dims(op.getDimensions());\n     absl::c_sort(reduction_dims);\n \n     mlir::Value reduced_vector = EmitVectorizedReduction(\n-        rewriter, op->getLoc(), result_vector_type, source_vector, init_value,\n+        rewriter, op->getLoc(), result_type, source_tensor, init_value,\n         reduction_dims, op.getBody().front());\n \n-    rewriter.replaceOp(op, WriteVectorToTensor(rewriter, reduced_vector));\n+    rewriter.replaceOp(op, reduced_vector);\n \n     return mlir::success();\n   }"
        },
        {
            "sha": "5b799d02781394767fe5b99aa718f04af68e6ebe",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/rewrite_dynamic_vector_extract.mlir",
            "status": "removed",
            "additions": 0,
            "deletions": 172,
            "changes": 172,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e57bfcf087a4dec906b05b8c57abcd82fc09c3f2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Frewrite_dynamic_vector_extract.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e57bfcf087a4dec906b05b8c57abcd82fc09c3f2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Frewrite_dynamic_vector_extract.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Frewrite_dynamic_vector_extract.mlir?ref=e57bfcf087a4dec906b05b8c57abcd82fc09c3f2",
            "patch": "@@ -1,172 +0,0 @@\n-// RUN: fusion_compiler_opt %s \\\n-// RUN: -xtile-cpu-rewrite-dynamic-vector-extract -canonicalize \\\n-// RUN: -split-input-file | FileCheck %s\n-\n-func.func @fold_vector_extract_into_transfer_read(\n-  %buffer: memref<8x4x2xf32>,\n-  %idx0: index,\n-  %idx1: index) -> vector<2xf32> {\n-  %c0 = arith.constant 0 : index\n-  %c0_f32 = arith.constant 0.0 : f32\n-  %c1 = arith.constant 1 : index\n-  %c3 = arith.constant 3 : index\n-  %c7 = arith.constant 7 : index\n-  %mask = vector.create_mask %c7, %c3, %c1 : vector<8x4x2xi1>\n-  %original_vector = vector.transfer_read %buffer[%c0, %c0, %c0],\n-    %c0_f32, %mask : memref<8x4x2xf32>, vector<8x4x2xf32>\n-  %subvector = vector.extract %original_vector[%idx0, %idx1]\n-    : vector<2xf32> from vector<8x4x2xf32>\n-  return %subvector : vector<2xf32>\n-}\n-\n-// CHECK:      func.func @fold_vector_extract_into_transfer_read(\n-// CHECK-SAME:   %[[BUFFER:.*]]: memref<8x4x2xf32>,\n-// CHECK-SAME:   %[[IDX0:.*]]: index,\n-// CHECK-SAME:   %[[IDX1:.*]]: index) -> vector<2xf32> {\n-// CHECK-DAG:    %[[PAD:.*]] = arith.constant 0.000000e+00 : f32\n-// CHECK-DAG:    %[[C0:.*]] = arith.constant 0 : index\n-// CHECK-DAG:    %[[C1:.*]] = arith.constant 1 : index\n-// CHECK-DAG:    %[[C3:.*]] = arith.constant 3 : index\n-// CHECK-DAG:    %[[C7:.*]] = arith.constant 7 : index\n-// CHECK:        %[[SHIFT_IDX0:.*]] = arith.subi %[[C7]], %[[IDX0]] : index\n-// CHECK:        %[[SHIFT_SUBIDX1:.*]] = arith.subi %[[C3]], %[[IDX1]] : index\n-// CHECK:        %[[SHIFT_MASK:.*]] = vector.create_mask\n-// CHECK-SAME:     %[[SHIFT_IDX0]], %[[SHIFT_SUBIDX1]], %[[C1]] : vector<1x1x2xi1>\n-// CHECK:        %[[SUBMASK:.*]] = vector.extract %[[SHIFT_MASK]][0, 0]\n-// CHECK-SAME:     : vector<2xi1> from vector<1x1x2xi1>\n-// CHECK:        %[[SUBVECTOR:.*]] = vector.transfer_read\n-// CHECK-SAME:     %[[BUFFER]][%[[IDX0]], %[[IDX1]], %[[C0]]], %[[PAD]], %[[SUBMASK]]\n-// CHECK-SAME:     {in_bounds = [true]} : memref<8x4x2xf32>, vector<2xf32>\n-// CHECK:        return %[[SUBVECTOR]] : vector<2xf32>\n-// CHECK:      }\n-\n-\n-// -----\n-\n-func.func @extract_scalar_folds_correctly(\n-  %buffer: memref<8x4x2xf32>,\n-  %idx0: index,\n-  %idx1: index,\n-  %idx2: index) -> f32 {\n-  %c0 = arith.constant 0 : index\n-  %c0_f32 = arith.constant 0.0 : f32\n-  %c1 = arith.constant 1 : index\n-  %c3 = arith.constant 3 : index\n-  %c7 = arith.constant 7 : index\n-  %mask = vector.create_mask %c7, %c3, %c1 : vector<8x4x2xi1>\n-  %original_vector = vector.transfer_read %buffer[%c0, %c0, %c0],\n-    %c0_f32, %mask : memref<8x4x2xf32>, vector<8x4x2xf32>\n-  %scalar = vector.extract %original_vector[%idx0, %idx1, %idx2]\n-    : f32 from vector<8x4x2xf32>\n-  return %scalar : f32\n-}\n-\n-// CHECK: func.func @extract_scalar_folds_correctly(\n-// CHECK-SAME: %[[BUFFER:[^ ]*]]: memref<8x4x2xf32>,\n-// CHECK-SAME: %[[IDX0:[^ ]*]]: index,\n-// CHECK-SAME: %[[IDX1:[^ ]*]]: index,\n-// CHECK-SAME: %[[IDX2:[^ ]*]]: index) -> f32 {\n-// CHECK:   %[[PAD:.*]] = arith.constant 0.000000e+00 : f32\n-// CHECK:   %[[C1:.*]] = arith.constant 1 : index\n-// CHECK:   %[[C3:.*]] = arith.constant 3 : index\n-// CHECK:   %[[C7:.*]] = arith.constant 7 : index\n-// CHECK:   %[[SHIFT_IDX0:.*]] = arith.subi %[[C7]], %[[IDX0]] : index\n-// CHECK:   %[[SHIFT_IDX1:.*]] = arith.subi %[[C3]], %[[IDX1]] : index\n-// CHECK:   %[[SHIFT_IDX2:.*]] = arith.subi %[[C1]], %[[IDX2]] : index\n-// CHECK:   %[[MASK:.*]] = vector.create_mask %0, %1, %2 : vector<1x1x1xi1>\n-// CHECK:   %[[SUBMASK:.*]] = vector.extract %[[MASK]][0, 0, 0]\n-// CHECK-SAME: : vector<1xi1> from vector<1x1x1xi1>\n-// CHECK:   %[[READ:.*]] = vector.transfer_read %[[BUFFER]]\n-// CHECK-SAME: [%[[IDX0]], %[[IDX1]], %[[IDX2]]], %[[PAD]], %[[SUBMASK]]\n-// CHECK-SAME: : memref<8x4x2xf32>, vector<1xf32>\n-// CHECK:   %[[SCALAR:.*]] = vector.extract %[[READ]][0] : f32 from vector<1xf32>\n-// CHECK:   return %[[SCALAR]] : f32\n-// CHECK: }\n-\n-\n-// -----\n-\n-func.func @unroll_dependent_vector_extract(%input: vector<8x2xf32>) -> vector<2xf32> {\n-  %c0 = arith.constant 0 : index\n-  %c1 = arith.constant 1 : index\n-  %c8 = arith.constant 8 : index\n-  %c0_f32 = arith.constant 0. : f32\n-  %init = vector.broadcast %c0_f32 : f32 to vector<2xf32>\n-  %result = scf.for %index = %c0 to %c8 step %c1 iter_args(%carry = %init) -> vector<2xf32> {\n-    %extract = vector.extract %input[%index] : vector<2xf32> from vector<8x2xf32>\n-    %add = arith.addf %carry, %extract : vector<2xf32>\n-    scf.yield %add : vector<2xf32>\n-  }\n-  return %result : vector<2xf32>\n-}\n-\n-// CHECK-LABEL:    func.func @unroll_dependent_vector_extract(\n-// CHECK-NOT:       scf.for\n-// CHECK-COUNT-8:     vector.extract\n-\n-// -----\n-\n-func.func @unroll_indirect_dependent_vector_extract(%input: vector<8x2xf32>) -> vector<2xf32> {\n-  %c0 = arith.constant 0 : index\n-  %c1 = arith.constant 1 : index\n-  %c2 = arith.constant 2 : index\n-  %c4 = arith.constant 4 : index\n-  %c0_f32 = arith.constant 0. : f32\n-  %init = vector.broadcast %c0_f32 : f32 to vector<2xf32>\n-  %result = scf.for %index = %c0 to %c4 step %c1 iter_args(%carry = %init) -> vector<2xf32> {\n-    %strided_index = arith.muli %index, %c2 : index\n-    %extract = vector.extract %input[%strided_index] : vector<2xf32> from vector<8x2xf32>\n-    %add = arith.addf %carry, %extract : vector<2xf32>\n-    scf.yield %add : vector<2xf32>\n-  }\n-  return %result : vector<2xf32>\n-}\n-\n-// CHECK-LABEL:    func.func @unroll_indirect_dependent_vector_extract(\n-// CHECK-NOT:        scf.for\n-// CHECK-COUNT-4:     vector.extract\n-\n-// -----\n-\n-func.func @does_not_unroll_independent_vector_extract(%input: vector<8x2xf32>, %arg_index: index) -> vector<2xf32> {\n-  %c0 = arith.constant 0 : index\n-  %c1 = arith.constant 1 : index\n-  %c8 = arith.constant 8 : index\n-  %c0_f32 = arith.constant 0. : f32\n-  %init = vector.broadcast %c0_f32 : f32 to vector<2xf32>\n-  %result = scf.for %index = %c0 to %c8 step %c1 iter_args(%carry = %init) -> vector<2xf32> {\n-    %extract = vector.extract %input[%arg_index] : vector<2xf32> from vector<8x2xf32>\n-    %add = arith.addf %carry, %extract : vector<2xf32>\n-    scf.yield %add : vector<2xf32>\n-  }\n-  return %result : vector<2xf32>\n-}\n-\n-// CHECK-LABEL:    func.func @does_not_unroll_independent_vector_extract(\n-// CHECK:            scf.for\n-// CHECK-COUNT-1:     vector.extract\n-\n-// -----\n-\n-// Ensure that we don't unroll loops that have a vector that depends on the for\n-// loop but the index itself does not.\n-func.func @does_not_unroll_only_dependent_vector(\n-    %input: memref<8x2xf32>, %arg_index: index) -> vector<2xf32> {\n-  %c0 = arith.constant 0 : index\n-  %c2 = arith.constant 2 : index\n-  %c8 = arith.constant 8 : index\n-  %c0_f32 = arith.constant 0. : f32\n-  %init = vector.broadcast %c0_f32 : f32 to vector<2xf32>\n-  %result = scf.for %index = %c0 to %c8 step %c2 iter_args(%carry = %init) -> vector<2xf32> {\n-    %input_vector = vector.transfer_read %input[%index, %c0], %c0_f32  : memref<8x2xf32>, vector<2x2xf32>\n-    %extract = vector.extract %input_vector[0] : vector<2xf32> from vector<2x2xf32>\n-    %add = arith.addf %carry, %extract : vector<2xf32>\n-    scf.yield %add : vector<2xf32>\n-  }\n-  return %result : vector<2xf32>\n-}\n-\n-\n-// CHECK-LABEL:    func.func @does_not_unroll_only_dependent_vector(\n-// CHECK:            scf.for\n-// CHECK-COUNT-1:     vector.extract"
        },
        {
            "sha": "cf6e3fa2f2b09f4d9f1f42a204c7ba47bba20bc7",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/tests/shlo_to_vector.mlir",
            "status": "modified",
            "additions": 9,
            "deletions": 17,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f16a07ff1e3abbfa6dd0f0f4e24849667e923189/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fshlo_to_vector.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f16a07ff1e3abbfa6dd0f0f4e24849667e923189/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fshlo_to_vector.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Ftests%2Fshlo_to_vector.mlir?ref=f16a07ff1e3abbfa6dd0f0f4e24849667e923189",
            "patch": "@@ -52,16 +52,13 @@ func.func @reduce_outer(%input : tensor<1024x32xf32>, %init : tensor<f32>) -> te\n \n // CHECK: func.func @reduce_outer\n // CHECK:   memref.alloca() : memref<32xf32>\n-// CHECK:   vector.extract %{{.*}} : vector<32xf32> from vector<1024x32xf32>\n+// CHECK:   vector.transfer_read %{{.*}} : tensor<1024x32xf32>, vector<32xf32>\n // CHECK:   scf.for\n-// CHECK:     vector.extract %{{.*}} : vector<32xf32> from vector<1024x32xf32>\n+// CHECK:     vector.transfer_read %{{.*}} : tensor<1024x32xf32>, vector<32xf32>\n // CHECK:     arith.addf {{.*}} : vector<32xf32>\n // CHECK:     scf.yield %{{.*}} : vector<32xf32>\n // CHECK:   }\n // CHECK:   vector.transfer_write %{{.*}} : vector<32xf32>, memref<32xf32>\n-// CHECK:   vector.transfer_read %{{.*}} : memref<32xf32>, vector<32xf32>\n-// CHECK:   vector.broadcast %{{.*}} : f32 to vector<32xf32>\n-// CHECK:   arith.addf {{.*}} : vector<32xf32>\n \n // -----\n \n@@ -78,11 +75,10 @@ func.func @reduce_inner(%input : tensor<1024x32xf32>, %init : tensor<f32>) -> te\n // CHECK: func.func @reduce_inner\n // CHECK:   memref.alloca() : memref<1024xf32>\n // CHECK:   scf.for\n-// CHECK:     vector.extract {{.*}} : vector<32xf32> from vector<1024x32xf32>\n+// CHECK:     vector.transfer_read {{.*}} : tensor<1024x32xf32>, vector<32xf32>\n // CHECK:     vector.reduction <add>, {{.*}} : vector<32xf32> into f32\n // CHECK:     memref.store {{.*}} : memref<1024xf32>\n // CHECK:   }\n-// CHECK:   vector.transfer_read {{.*}} : memref<1024xf32>, vector<1024xf32>\n \n // -----\n \n@@ -98,17 +94,14 @@ func.func @reduce_middle(%input : tensor<1024x32x8xf32>, %init : tensor<f32>) ->\n // CHECK: func.func @reduce_middle\n // CHECK:   memref.alloca() : memref<1024x8xf32>\n // CHECK:   scf.for\n-// CHECK:     vector.extract {{.*}} : vector<8xf32> from vector<1024x32x8xf32>\n+// CHECK:     vector.transfer_read {{.*}} : tensor<1024x32x8xf32>, vector<8xf32>\n // CHECK:     scf.for\n-// CHECK:       vector.extract %{{.*}} : vector<8xf32> from vector<1024x32x8xf32>\n+// CHECK:       vector.transfer_read %{{.*}} : tensor<1024x32x8xf32>, vector<8xf32>\n // CHECK:       arith.addf {{.*}} : vector<8xf32>\n // CHECK:       scf.yield {{.*}} : vector<8xf32>\n // CHECK:     }\n // CHECK:     vector.transfer_write {{.*}} : vector<8xf32>, memref<1024x8xf32>\n // CHECK:   }\n-// CHECK:   vector.transfer_read {{.*}} : memref<1024x8xf32>, vector<1024x8xf32>\n-// CHECK:   vector.broadcast {{.*}} : f32 to vector<1024x8xf32>\n-// CHECK:   arith.addf {{.*}} : vector<1024x8xf32>\n // CHECK: }\n \n // -----\n@@ -125,21 +118,20 @@ func.func @reduce_outer_and_inner(%input : tensor<1024x32x8xf32>, %init : tensor\n // CHECK: func.func @reduce_outer_and_inner\n // CHECK:   %[[BUFFER0:.*]] = memref.alloca() : memref<32x8xf32>\n // CHECK:   scf.for\n-// CHECK:     vector.extract %{{.*}} : vector<8xf32> from vector<1024x32x8xf32>\n+// CHECK:     vector.transfer_read {{.*}} : tensor<1024x32x8xf32>, vector<8xf32>\n // CHECK:     scf.for\n-// CHECK:       vector.extract %{{.*}} : vector<8xf32> from vector<1024x32x8xf32>\n+// CHECK:       vector.transfer_read %{{.*}} : tensor<1024x32x8xf32>, vector<8xf32>\n // CHECK:       arith.addf %{{.*}} : vector<8xf32>\n // CHECK:       scf.yield {{.*}} : vector<8xf32>\n // CHECK:     }\n-// CHECK:     vector.transfer_write {{.*}} : vector<8xf32>, memref<32x8xf32>\n+// CHECK:     vector.transfer_write {{.*}}, %[[BUFFER0]]{{.*}} : vector<8xf32>, memref<32x8xf32>\n // CHECK:   }\n // CHECK:   %[[BUFFER1:.*]] = memref.alloca() : memref<32xf32>\n // CHECK:   scf.for\n // CHECK:     vector.transfer_read %[[BUFFER0]]{{.*}} : memref<32x8xf32>, vector<8xf32>\n // CHECK:     vector.reduction <add>, {{.*}} : vector<8xf32> into f32\n-// CHECK:     memref.store %{{.*}}, %[[BUFFER1]]{{.*}} : memref<32xf32>\n+// CHECK:     memref.store {{.*}} %[[BUFFER1]]{{.*}} : memref<32xf32>\n // CHECK:   }\n-// CHECK:   vector.transfer_read %[[BUFFER1]]{{.*}} : memref<32xf32>, vector<32xf32>\n // CHECK: }\n \n // -----"
        },
        {
            "sha": "53e9af8bf694a31c2ccb1cf017e5f23b100aa1cb",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/vectorized_reduce_emitter.cc",
            "status": "modified",
            "additions": 64,
            "deletions": 89,
            "changes": 153,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f16a07ff1e3abbfa6dd0f0f4e24849667e923189/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvectorized_reduce_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f16a07ff1e3abbfa6dd0f0f4e24849667e923189/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvectorized_reduce_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvectorized_reduce_emitter.cc?ref=f16a07ff1e3abbfa6dd0f0f4e24849667e923189",
            "patch": "@@ -28,12 +28,14 @@ limitations under the License.\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/ADT/SmallVectorExtras.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/Bufferization/IR/Bufferization.h\"\n #include \"mlir/Dialect/Linalg/Transforms/Transforms.h\"\n #include \"mlir/Dialect/MemRef/IR/MemRef.h\"\n #include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/Location.h\"\n #include \"mlir/IR/OpDefinition.h\"\n #include \"mlir/IR/Value.h\"\n@@ -64,18 +66,10 @@ static absl::StatusOr<mlir::vector::CombiningKind> GetCombiningKind(\n   return absl::InternalError(\"Unsupported reduction combiner\");\n }\n \n-static mlir::Value ExtractVector(mlir::OpBuilder& builder, mlir::Location loc,\n-                                 mlir::Value source, mlir::ValueRange indices) {\n-  return mlir::vector::ExtractOp::create(\n-      builder, loc, source, llvm::map_to_vector(indices, [](mlir::Value idx) {\n-        return mlir::OpFoldResult(idx);\n-      }));\n-}\n-\n-static void InsertVectorIntoBuffer(mlir::OpBuilder& builder, mlir::Location loc,\n-                                   mlir::Value value,\n-                                   mlir::TypedValue<mlir::MemRefType> buffer,\n-                                   mlir::ValueRange indices) {\n+static void InsertValue(mlir::OpBuilder& builder, mlir::Location loc,\n+                        mlir::Value value,\n+                        mlir::TypedValue<mlir::MemRefType> buffer,\n+                        mlir::ValueRange indices) {\n   llvm::SmallVector<mlir::Value> padded_indices(indices);\n   while (padded_indices.size() < buffer.getType().getRank()) {\n     padded_indices.push_back(\n@@ -90,18 +84,18 @@ static void InsertVectorIntoBuffer(mlir::OpBuilder& builder, mlir::Location loc,\n   }\n }\n \n-static mlir::TypedValue<mlir::VectorType> ExtractVectorFromBuffer(\n+static mlir::TypedValue<mlir::VectorType> ExtractVector(\n     mlir::OpBuilder& builder, mlir::Location loc,\n-    mlir::TypedValue<mlir::MemRefType> buffer, mlir::ValueRange indices = {}) {\n+    mlir::TypedValue<mlir::ShapedType> input, mlir::ValueRange indices = {}) {\n   llvm::SmallVector<mlir::Value> padded_indices(indices);\n-  while (padded_indices.size() < buffer.getType().getRank()) {\n+  while (padded_indices.size() < input.getType().getRank()) {\n     padded_indices.push_back(\n-        builder.create<mlir::arith::ConstantIndexOp>(loc, 0));\n+        mlir::arith::ConstantIndexOp::create(builder, loc, 0));\n   }\n   mlir::VectorType vector_type = mlir::VectorType::get(\n-      buffer.getType().getShape().drop_front(indices.size()),\n-      buffer.getType().getElementType());\n-  return mlir::vector::TransferReadOp::create(builder, loc, vector_type, buffer,\n+      input.getType().getShape().drop_front(indices.size()),\n+      input.getType().getElementType());\n+  return mlir::vector::TransferReadOp::create(builder, loc, vector_type, input,\n                                               padded_indices,\n                                               /*padding=*/std::nullopt);\n }\n@@ -149,15 +143,16 @@ mlir::Value VectorizeBody(mlir::OpBuilder& builder, mlir::Location loc,\n   return mapping.lookup(old_body.getTerminator()->getOperand(0));\n }\n \n-mlir::Value EmitNonMinorReduction(\n-    mlir::OpBuilder& builder, mlir::Location loc, mlir::VectorType result_type,\n-    mlir::TypedValue<mlir::VectorType> source_vector,\n+mlir::TypedValue<mlir::MemRefType> EmitNonMinorReduction(\n+    mlir::OpBuilder& builder, mlir::Location loc,\n+    mlir::RankedTensorType result_type,\n+    mlir::TypedValue<mlir::RankedTensorType> source_tensor,\n     llvm::ArrayRef<int64_t> reduction_dims, mlir::Block& body,\n     bool minor_dim_reduced) {\n-  mlir::VectorType source_vector_type = source_vector.getType();\n-  int64_t rank = source_vector_type.getRank();\n+  mlir::RankedTensorType source_tensor_type = source_tensor.getType();\n+  int64_t rank = source_tensor_type.getRank();\n   int64_t minor_dim = rank - 1;\n-  int64_t minor_dim_size = source_vector_type.getDimSize(minor_dim);\n+  int64_t minor_dim_size = source_tensor_type.getDimSize(minor_dim);\n   llvm::SmallVector<int64_t> non_reduced_dims(rank);\n   absl::c_iota(non_reduced_dims, 0);\n   non_reduced_dims.erase(\n@@ -187,13 +182,14 @@ mlir::Value EmitNonMinorReduction(\n   if (minor_dim_reduced) {\n     output_shape.push_back(minor_dim_size);\n   }\n+\n   auto output_buffer_shape =\n       mlir::MemRefType::get(output_shape, result_type.getElementType());\n   auto buffer = CreateBufferOfShape(builder, loc, output_buffer_shape);\n \n   auto get_source_vector_dim_size = [&](llvm::ArrayRef<int64_t> dims) {\n     return llvm::map_to_vector(\n-        dims, [&](int64_t dim) { return source_vector_type.getDimSize(dim); });\n+        dims, [&](int64_t dim) { return source_tensor_type.getDimSize(dim); });\n   };\n \n   // Outer loop is non-minor non-reduced dimensions.\n@@ -216,7 +212,7 @@ mlir::Value EmitNonMinorReduction(\n         }\n         // Get the first iteration\n         mlir::Value minor_accumilator =\n-            ExtractVector(builder, loc, source_vector, zeroth_step_indices);\n+            ExtractVector(builder, loc, source_tensor, zeroth_step_indices);\n         // Inner loop is the non-minor reduced dimension.\n         mlir::scf::LoopNest loop_nest = mlir::scf::buildLoopNest(\n             builder, loc, lbs, ubs, step, minor_accumilator,\n@@ -235,74 +231,52 @@ mlir::Value EmitNonMinorReduction(\n               }\n \n               mlir::Value vector_slice =\n-                  ExtractVector(builder, loc, source_vector, indices);\n+                  ExtractVector(builder, loc, source_tensor, indices);\n \n               return {VectorizeBody(builder, loc, body, vector_slice,\n                                     minor_accumilator.front())};\n             });\n \n-        InsertVectorIntoBuffer(builder, loc, loop_nest.results.front(), buffer,\n-                               outer_induction_vars);\n-        return;\n+        InsertValue(builder, loc, loop_nest.results.front(), buffer,\n+                    outer_induction_vars);\n       });\n \n-  // If the minor dimension is also reduced then it extracts directly from the\n-  // buffer to avoid the additional vector -> subvector operation.\n-  if (minor_dim_reduced) {\n-    return buffer;\n-  }\n-\n-  return ExtractVectorFromBuffer(builder, loc, buffer);\n+  return buffer;\n }\n \n-mlir::TypedValue<mlir::VectorType> EmitMinorReduction(\n-    mlir::OpBuilder& builder, mlir::Location loc, mlir::VectorType result_type,\n-    mlir::Value input, mlir::Value init_value, mlir::Block& body) {\n+mlir::TypedValue<mlir::MemRefType> EmitMinorReduction(\n+    mlir::OpBuilder& builder, mlir::Location loc,\n+    mlir::RankedTensorType result_type,\n+    mlir::TypedValue<mlir::ShapedType> input, mlir::Value init_value,\n+    mlir::Block& body) {\n   absl::StatusOr<mlir::vector::CombiningKind> kind_or = GetCombiningKind(body);\n   if (!kind_or.ok()) {\n     body.getParentOp()->emitRemark() << kind_or.status().ToString();\n   }\n \n-  // TODO(willfroom): we could reuse the non minor result buffer.\n-  auto minor_result_buffer = CreateBufferOfShape(builder, loc, result_type);\n-  auto maybe_input_buffer =\n-      mlir::dyn_cast<mlir::TypedValue<mlir::MemRefType>>(input);\n-\n-  auto maybe_input_type =\n-      llvm::TypeSwitch<mlir::Type, std::optional<mlir::ShapedType>>(\n-          input.getType())\n-          .Case<mlir::MemRefType>([&](auto op) { return input.getType(); })\n-          .Case<mlir::VectorType>([&](auto op) { return input.getType(); })\n-          .Default([&](auto op) { return std::nullopt; });\n-\n-  if (!maybe_input_type.has_value()) {\n-    return nullptr;\n-  }\n-\n-  int64_t minor_dim_size = maybe_input_type->getShape().back();\n+  auto input_type = input.getType();\n+  int64_t minor_dim_size = input_type.getShape().back();\n \n   auto [lbs, ubs, step] = GetLoopBounds(builder, loc, result_type.getShape());\n \n+  auto buffer = CreateBufferOfShape(builder, loc, result_type);\n+\n   mlir::scf::buildLoopNest(\n       builder, loc, lbs, ubs, step,\n       [&](mlir::OpBuilder& builder, mlir::Location loc,\n           mlir::ValueRange induction_vars) {\n-        mlir::Value vector_slice =\n-            maybe_input_buffer\n-                ? ExtractVectorFromBuffer(builder, loc, maybe_input_buffer,\n-                                          induction_vars)\n-                : ExtractVector(builder, loc, input, induction_vars);\n-\n         if (kind_or.ok()) {\n           // TODO(willfroom): Investigate tree-reduction to split the reduction\n           // op into natural sizes (2, 4, 8, 16, ...) and then remove the\n           // reassociation flag.\n+          mlir::Value vector_slice =\n+              ExtractVector(builder, loc, input, induction_vars);\n           mlir::Value reduced_scalar =\n               builder.create<mlir::vector::ReductionOp>(\n                   loc, *kind_or, vector_slice, init_value,\n                   mlir::arith::FastMathFlags::reassoc);\n-          InsertVectorIntoBuffer(builder, loc, reduced_scalar,\n-                                 minor_result_buffer, induction_vars);\n+\n+          InsertValue(builder, loc, reduced_scalar, buffer, induction_vars);\n           return;\n         }\n \n@@ -312,50 +286,51 @@ mlir::TypedValue<mlir::VectorType> EmitMinorReduction(\n             [&](mlir::OpBuilder& builder, mlir::Location loc,\n                 mlir::ValueRange index, mlir::ValueRange carry_value)\n                 -> mlir::SmallVector<mlir::Value> {\n+              auto full_index = llvm::to_vector(\n+                  llvm::concat<mlir::Value>(induction_vars, index));\n+              mlir::TypedValue<mlir::VectorType> element_vector =\n+                  ExtractVector(builder, loc, input, full_index);\n               mlir::Value element =\n-                  ExtractVector(builder, loc, vector_slice, index);\n+                  mlir::vector::ExtractOp::create(builder, loc, element_vector);\n               return {VectorizeBody(builder, loc, body, element,\n                                     carry_value.front())};\n             });\n \n-        InsertVectorIntoBuffer(builder, loc,\n-                               minor_reduction_loop.results.front(),\n-                               minor_result_buffer, induction_vars);\n-        return;\n+        InsertValue(builder, loc, minor_reduction_loop.results.front(), buffer,\n+                    induction_vars);\n       });\n \n-  return ExtractVectorFromBuffer(builder, loc, minor_result_buffer);\n+  return buffer;\n }\n \n mlir::Value EmitVectorizedReduction(\n-    mlir::OpBuilder& builder, mlir::Location loc, mlir::VectorType result_type,\n-    mlir::TypedValue<mlir::VectorType> source, mlir::Value init_value,\n+    mlir::OpBuilder& builder, mlir::Location loc,\n+    mlir::RankedTensorType result_type,\n+    mlir::TypedValue<mlir::RankedTensorType> source, mlir::Value init_value,\n     llvm::ArrayRef<int64_t> reduction_dims, mlir::Block& body) {\n   int64_t rank = source.getType().getRank();\n   int64_t minor_dim = rank - 1;\n \n   bool minor_dim_reduced = reduction_dims.back() == minor_dim;\n   bool non_minor_dim_reduced = reduction_dims.size() > 1 || !minor_dim_reduced;\n \n-  mlir::Value non_minor_result;\n+  mlir::TypedValue<mlir::ShapedType> result;\n   if (non_minor_dim_reduced) {\n-    non_minor_result =\n-        EmitNonMinorReduction(builder, loc, result_type, source, reduction_dims,\n-                              body, minor_dim_reduced);\n+    result = EmitNonMinorReduction(builder, loc, result_type, source,\n+                                   reduction_dims, body, minor_dim_reduced);\n   }\n-  if (!minor_dim_reduced) {\n-    // We add the init value during the minor reduction loop, if that wasn't\n-    // done then we must apply it here.\n-    mlir::Value init_value_vector =\n-        builder.create<mlir::vector::BroadcastOp>(loc, result_type, init_value);\n-\n-    return VectorizeBody(builder, loc, body, non_minor_result,\n-                         init_value_vector);\n+\n+  if (minor_dim_reduced) {\n+    result = EmitMinorReduction(builder, loc, result_type,\n+                                result ? result : source, init_value, body);\n   }\n \n-  return EmitMinorReduction(builder, loc, result_type,\n-                            non_minor_result ? non_minor_result : source,\n-                            init_value, body);\n+  auto to_tensor = mlir::bufferization::ToTensorOp::create(builder, loc,\n+                                                           result_type, result);\n+  // This is a local allocation so we know it doesn't alias.\n+  to_tensor.setRestrict(true);\n+  to_tensor.setWritable(true);\n+  return to_tensor;\n }\n \n }  // namespace xla::cpu"
        },
        {
            "sha": "89443a52c7b3ab4de99c37956b783aa4696b9f3f",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/transforms/vectorized_reduce_emitter.h",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f16a07ff1e3abbfa6dd0f0f4e24849667e923189/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvectorized_reduce_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f16a07ff1e3abbfa6dd0f0f4e24849667e923189/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvectorized_reduce_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftransforms%2Fvectorized_reduce_emitter.h?ref=f16a07ff1e3abbfa6dd0f0f4e24849667e923189",
            "patch": "@@ -38,8 +38,9 @@ namespace xla::cpu {\n // 3. If the dimensions are a combindation of minor & non-minor dimensions we\n //    simply apply strategy 2 followed by strategy 1.\n mlir::Value EmitVectorizedReduction(\n-    mlir::OpBuilder& builder, mlir::Location loc, mlir::VectorType result_type,\n-    mlir::TypedValue<mlir::VectorType> source, mlir::Value init_value,\n+    mlir::OpBuilder& builder, mlir::Location loc,\n+    mlir::RankedTensorType result_type,\n+    mlir::TypedValue<mlir::RankedTensorType> source, mlir::Value init_value,\n     llvm::ArrayRef<int64_t> reduction_dims, mlir::Block& body);\n \n }  // namespace xla::cpu"
        }
    ],
    "stats": {
        "total": 725,
        "additions": 89,
        "deletions": 636
    }
}