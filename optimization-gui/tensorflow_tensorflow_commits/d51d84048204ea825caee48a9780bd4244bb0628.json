{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Add ExecuteReplicated overload when all replicas get the same arguments.\n\nPiperOrigin-RevId: 839238919",
    "sha": "d51d84048204ea825caee48a9780bd4244bb0628",
    "files": [
        {
            "sha": "055f99e34ba9145d98db9a14b78287dad8dc6a91",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test.cc",
            "status": "modified",
            "additions": 122,
            "deletions": 238,
            "changes": 360,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d51d84048204ea825caee48a9780bd4244bb0628/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d51d84048204ea825caee48a9780bd4244bb0628/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc?ref=d51d84048204ea825caee48a9780bd4244bb0628",
            "patch": "@@ -564,9 +564,8 @@ TEST_P(AsyncCollectiveOps, AsyncAllToAllWithSplitDim) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto module, ParseAndReturnVerifiedModule(kModuleStr, kNumReplicas));\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      ExecutionResult execution_result,\n-      ExecuteReplicated(std::move(module), /*arguments=*/{{}, {}}));\n+  TF_ASSERT_OK_AND_ASSIGN(ExecutionResult execution_result,\n+                          ExecuteReplicated(std::move(module)));\n \n   const HloModule* hlo_module = execution_result.optimized_module;\n   HloInstruction* a2a_start =\n@@ -605,20 +604,16 @@ TEST_F(CollectiveOpsTestE2E, AsyncAllToAllMemCpyWithSplitDim) {\n   config.set_debug_options(debug_options);\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n                           ParseAndReturnVerifiedModule(kModuleStr, config));\n+  TF_ASSERT_OK_AND_ASSIGN(ExecutionResult execution_result,\n+                          ExecuteReplicated(std::move(module)));\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto executable, hlo_runner_->CreateExecutable(std::move(module),\n-                                                     /*run_hlo_passes=*/true));\n-  TF_ASSERT_OK_AND_ASSIGN(const HloModule* const executable_module,\n-                          hlo_runner_->HloModuleFromWrapped(executable.get()));\n-\n+  const HloModule* executable_module = execution_result.optimized_module;\n   // Verify that the all-to-all is not decomposed into a tuple all-to-all.\n   const HloInstruction* all_to_all =\n       FindInstruction(executable_module, HloOpcode::kAllToAll);\n   EXPECT_THAT(all_to_all, op::Shape(\"u32[2, 2]\"));\n \n-  TF_ASSERT_OK_AND_ASSIGN(std::vector<Literal> results,\n-                          ExecuteReplicated(executable.get(), kNumReplicas));\n+  const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n   LiteralTestUtil::ExpectR1Equal<uint32_t>({10, 15, 11, 16}, results[0]);\n   LiteralTestUtil::ExpectR1Equal<uint32_t>({20, 25, 21, 26}, results[1]);\n@@ -1008,10 +1003,9 @@ TEST_P(AsyncCollectiveOps, MatmulReplicated) {\n   for (int i = 0; i < fake_arguments.size(); i++) {\n     fake_ptrs[i] = &fake_arguments[i];\n   }\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), fake_ptrs, kNumReplicas, &assn,\n-                        /*run_hlo_passes=*/true, /*use_threads=*/true));\n+  TF_ASSERT_OK_AND_ASSIGN(ExecutionResult execution_result,\n+                          ExecuteReplicated(std::move(module), fake_ptrs));\n+  const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n \n   TF_ASSERT_OK_AND_ASSIGN(\n@@ -1299,12 +1293,6 @@ class CollectiveOpsTestE2EWindowedNonWindowed : public CollectiveOpsTestE2E {\n                    << \" available)\";\n     }\n \n-    DeviceAssignment assn(/*replica_count=*/kNumReplicas,\n-                          /*computation_count=*/kNumPartitions);\n-    for (int64_t i = 0; i < kNumPartitions; ++i) {\n-      assn(0, i) = i;\n-    }\n-\n     HloModuleConfig ref_config =\n         GetModuleConfigForTest(/*replica_count=*/kNumReplicas);\n     auto ref_opts = GetDebugOptionsForTest();\n@@ -1324,10 +1312,9 @@ class CollectiveOpsTestE2EWindowedNonWindowed : public CollectiveOpsTestE2E {\n     }\n \n     TF_ASSERT_OK_AND_ASSIGN(\n-        std::vector<Literal> ref_results,\n-        ExecuteReplicated(std::move(ref_module), ref_fake_ptrs, kNumPartitions,\n-                          &assn, /*run_hlo_passes=*/true,\n-                          /*use_threads=*/true));\n+        ExecutionResult ref_execution_result,\n+        ExecuteReplicated(std::move(ref_module), ref_fake_ptrs));\n+    const std::vector<Literal>& ref_results = ref_execution_result.results;\n \n     HloModuleConfig config =\n         GetModuleConfigForTest(/*replica_count=*/kNumReplicas);\n@@ -1354,10 +1341,9 @@ class CollectiveOpsTestE2EWindowedNonWindowed : public CollectiveOpsTestE2E {\n       fake_ptrs[i] = &fake_arguments[i];\n     }\n \n-    TF_ASSERT_OK_AND_ASSIGN(\n-        std::vector<Literal> results,\n-        ExecuteReplicated(std::move(module), fake_ptrs, kNumPartitions, &assn,\n-                          /*run_hlo_passes=*/true, /*use_threads=*/true));\n+    TF_ASSERT_OK_AND_ASSIGN(ExecutionResult execution_result,\n+                            ExecuteReplicated(std::move(module), fake_ptrs));\n+    const std::vector<Literal>& results = execution_result.results;\n     ASSERT_EQ(results.size(), kNumPartitions);\n \n     ASSERT_EQ(ref_results.size(), kNumPartitions);\n@@ -1733,16 +1719,9 @@ class CollectiveOpsTestE2EPipelinedNonPipelined : public CollectiveOpsTestE2E {\n       fake_ptrs[i] = &fake_arguments[i];\n     }\n \n-    DeviceAssignment assn(/*replica_count=*/kNumReplicas,\n-                          /*computation_count=*/kNumPartitions);\n-    for (int64_t i = 0; i < kNumPartitions; ++i) {\n-      assn(0, i) = i;\n-    }\n-\n-    TF_ASSERT_OK_AND_ASSIGN(\n-        std::vector<Literal> results,\n-        ExecuteReplicated(std::move(module), fake_ptrs, kNumPartitions, &assn,\n-                          /*run_hlo_passes=*/true, /*use_threads=*/true));\n+    TF_ASSERT_OK_AND_ASSIGN(ExecutionResult execution_result,\n+                            ExecuteReplicated(std::move(module), fake_ptrs));\n+    const std::vector<Literal>& results = execution_result.results;\n     ASSERT_EQ(results.size(), kNumPartitions);\n \n     HloModuleConfig ref_config =\n@@ -1761,10 +1740,9 @@ class CollectiveOpsTestE2EPipelinedNonPipelined : public CollectiveOpsTestE2E {\n     }\n \n     TF_ASSERT_OK_AND_ASSIGN(\n-        std::vector<Literal> ref_results,\n-        ExecuteReplicated(std::move(ref_module), ref_fake_ptrs, kNumPartitions,\n-                          &assn,\n-                          /*run_hlo_passes=*/true, /*use_threads=*/true));\n+        ExecutionResult ref_execution_result,\n+        ExecuteReplicated(std::move(ref_module), ref_fake_ptrs));\n+    const std::vector<Literal>& ref_results = ref_execution_result.results;\n     ASSERT_EQ(ref_results.size(), kNumPartitions);\n     ErrorSpec error_spec{1e-5, 1e-5};\n     // Expect same results with and without pipelining of collectives.\n@@ -1965,35 +1943,19 @@ ENTRY entry {\n       << \"Test requires at least \" << kNumReplicas * kNumPartitions\n       << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n \n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas);\n-  config.set_num_partitions(kNumPartitions);\n   TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr,\n+                                                kNumReplicas, kNumPartitions));\n+  TF_ASSERT_OK_AND_ASSIGN(ExecutionResult execution_result,\n+                          ExecuteReplicated(std::move(module)));\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto executable, hlo_runner_->CreateExecutable(std::move(module),\n-                                                     /*run_hlo_passes=*/true));\n-  TF_ASSERT_OK_AND_ASSIGN(const HloModule* const hlo_module,\n-                          hlo_runner_->HloModuleFromWrapped(executable.get()));\n+  const HloModule* hlo_module = execution_result.optimized_module;\n   HloInstruction* all_to_all =\n       FindInstruction(hlo_module, HloOpcode::kAllToAll);\n   EXPECT_THAT(all_to_all, NotNull());\n   EXPECT_EQ(all_to_all->shape().element_type(), BF16);\n \n-  // Execute the test on 2 partitions.\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n-  DeviceAssignment assignment(/*replica_count=*/kNumReplicas,\n-                              /*computation_count=*/kNumPartitions);\n-  for (int64_t i = 0; i < kNumPartitions; ++i) {\n-    assignment(0, i) = i;\n-  }\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), {}, kNumPartitions, &assignment,\n-                        /*run_hlo_passes=*/true,\n-                        /*use_threads=*/true));\n+  const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumPartitions);\n   const bfloat16 four = static_cast<bfloat16>(4.);\n   const bfloat16 eight = static_cast<bfloat16>(8.);\n@@ -2020,37 +1982,22 @@ ENTRY entry {\n       << \"Test requires at least \" << kNumReplicas * kNumPartitions\n       << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n \n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas);\n-  config.set_num_partitions(kNumPartitions);\n-\n-  // Verify that the element type of the all-to-all has been changed to BF16.\n   TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr,\n+                                                kNumReplicas, kNumPartitions));\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto executable, hlo_runner_->CreateExecutable(std::move(module),\n-                                                     /*run_hlo_passes=*/true));\n-  TF_ASSERT_OK_AND_ASSIGN(const HloModule* const hlo_module,\n-                          hlo_runner_->HloModuleFromWrapped(executable.get()));\n+  TF_ASSERT_OK_AND_ASSIGN(ExecutionResult execution_result,\n+                          ExecuteReplicated(std::move(module)));\n+\n+  // Verify that the element type of the all-to-all has been changed to BF16.\n+  const HloModule* hlo_module = execution_result.optimized_module;\n   HloInstruction* all_to_all =\n       FindInstruction(hlo_module, HloOpcode::kAllToAll);\n   EXPECT_THAT(all_to_all, NotNull());\n   EXPECT_EQ(all_to_all->shape().element_type(), BF16);\n \n   // Execute the test on 2 partitions.\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n-  DeviceAssignment assignment(/*replica_count=*/kNumReplicas,\n-                              /*computation_count=*/kNumPartitions);\n-  for (int64_t i = 0; i < kNumPartitions; ++i) {\n-    assignment(0, i) = i;\n-  }\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), {}, kNumPartitions, &assignment,\n-                        /*run_hlo_passes=*/true,\n-                        /*use_threads=*/true));\n+  const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumPartitions);\n   LiteralTestUtil::ExpectR1Equal<float>({4., 4.}, results[0]);\n   LiteralTestUtil::ExpectR1Equal<float>({8., 8.}, results[1]);\n@@ -2226,16 +2173,9 @@ ENTRY main.49 {\n     fake_ptrs[i] = &fake_arguments[i];\n   }\n \n-  DeviceAssignment assn(/*replica_count=*/kNumReplicas,\n-                        /*computation_count=*/kNumPartitions);\n-  for (int64_t i = 0; i < kNumPartitions; ++i) {\n-    assn(0, i) = i;\n-  }\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), fake_ptrs, kNumPartitions, &assn,\n-                        /*run_hlo_passes=*/true, /*use_threads=*/true));\n+  TF_ASSERT_OK_AND_ASSIGN(ExecutionResult execution_result,\n+                          ExecuteReplicated(std::move(module), fake_ptrs));\n+  const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumPartitions);\n \n   HloModuleConfig ref_config =\n@@ -2252,10 +2192,9 @@ ENTRY main.49 {\n   }\n \n   TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> ref_results,\n-      ExecuteReplicated(std::move(ref_module), ref_fake_ptrs, kNumPartitions,\n-                        &assn,\n-                        /*run_hlo_passes=*/true, /*use_threads=*/true));\n+      ExecutionResult ref_execution_result,\n+      ExecuteReplicated(std::move(ref_module), ref_fake_ptrs));\n+  const std::vector<Literal>& ref_results = ref_execution_result.results;\n   ASSERT_EQ(ref_results.size(), kNumPartitions);\n   ErrorSpec error_spec{1e-5, 1e-5};\n   // Expect same results with and without pipelining of collectives.\n@@ -2325,23 +2264,15 @@ ENTRY main {\n     ref_fake_ptrs[i] = &fake_ref_arguments[i];\n   }\n \n-  DeviceAssignment assn(/*replica_count=*/kNumReplicas,\n-                        /*computation_count=*/kNumPartitions);\n-  for (int64_t i = 0; i < kNumPartitions; ++i) {\n-    assn(0, i) = i;\n-  }\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), fake_ptrs, kNumPartitions, &assn,\n-                        /*run_hlo_passes=*/true, /*use_threads=*/true));\n+  TF_ASSERT_OK_AND_ASSIGN(ExecutionResult execution_result,\n+                          ExecuteReplicated(std::move(module), fake_ptrs));\n+  const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumPartitions);\n \n   TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> ref_results,\n-      ExecuteReplicated(std::move(ref_module), ref_fake_ptrs, kNumPartitions,\n-                        &assn,\n-                        /*run_hlo_passes=*/true, /*use_threads=*/true));\n+      ExecutionResult ref_execution_result,\n+      ExecuteReplicated(std::move(ref_module), ref_fake_ptrs));\n+  const std::vector<Literal>& ref_results = ref_execution_result.results;\n   ASSERT_EQ(ref_results.size(), kNumPartitions);\n   ErrorSpec error_spec{1e-5, 1e-5};\n   // Expect same results with and without pipelining of collectives.\n@@ -2551,11 +2482,8 @@ TEST_P(AllReduceTest, AsyncAllReduce_F32_2GPUs) {\n       << \"Test requires at least \" << kNumReplicas << \" devices (\"\n       << hlo_runner_->device_count() << \" available)\";\n \n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(kModuleStr, config));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleStr, kNumReplicas));\n \n   int64_t num_elements =\n       module->entry_computation()->root_instruction()->shape().dimensions()[0];\n@@ -2574,12 +2502,11 @@ TEST_P(AllReduceTest, AsyncAllReduce_F32_2GPUs) {\n       LiteralUtil::CreateFromArray(expected_output);\n \n   TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n+      ExecutionResult execution_result,\n       ExecuteReplicated(std::move(module),\n-                        {{&input_literal1}, {&input_literal2}},\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n+                        std::vector<std::vector<Literal*>>{{&input_literal1},\n+                                                           {&input_literal2}}));\n+  const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n   EXPECT_TRUE(LiteralTestUtil::Equal(expected_output_literal, results[0]));\n   EXPECT_TRUE(LiteralTestUtil::Equal(expected_output_literal, results[1]));\n@@ -2631,11 +2558,8 @@ TEST_P(AllReduceTest, AsyncAllReduceInsideWhile_F32_2GPUs) {\n       << \"Test requires at least \" << kNumReplicas << \" devices (\"\n       << hlo_runner_->device_count() << \" available)\";\n \n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(kModuleStr, config));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleStr, kNumReplicas));\n \n   int64_t num_elements =\n       module->entry_computation()->root_instruction()->shape().dimensions()[0];\n@@ -2655,12 +2579,11 @@ TEST_P(AllReduceTest, AsyncAllReduceInsideWhile_F32_2GPUs) {\n       LiteralUtil::CreateFromArray(expected_output);\n \n   TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n+      ExecutionResult execution_result,\n       ExecuteReplicated(std::move(module),\n-                        {{&input_literal1}, {&input_literal2}},\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n+                        std::vector<std::vector<Literal*>>{{&input_literal1},\n+                                                           {&input_literal2}}));\n+  const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n   EXPECT_TRUE(LiteralTestUtil::Equal(expected_output_literal, results[0]));\n   EXPECT_TRUE(LiteralTestUtil::Equal(expected_output_literal, results[1]));\n@@ -2687,11 +2610,8 @@ TEST_P(AllReduceTest, AsyncAllReduce_BF16_2GPUs) {\n       << \"Test requires at least \" << kNumReplicas << \" devices (\"\n       << hlo_runner_->device_count() << \" available)\";\n \n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(kModuleStr, config));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleStr, kNumReplicas));\n \n   int64_t num_elements =\n       module->entry_computation()->root_instruction()->shape().dimensions()[0];\n@@ -2710,12 +2630,11 @@ TEST_P(AllReduceTest, AsyncAllReduce_BF16_2GPUs) {\n       LiteralUtil::CreateFromArray(expected_output);\n \n   TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n+      ExecutionResult execution_result,\n       ExecuteReplicated(std::move(module),\n-                        {{&input_literal1}, {&input_literal2}},\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n+                        std::vector<std::vector<Literal*>>{{&input_literal1},\n+                                                           {&input_literal2}}));\n+  const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n   EXPECT_TRUE(LiteralTestUtil::Equal(expected_output_literal, results[0]));\n   EXPECT_TRUE(LiteralTestUtil::Equal(expected_output_literal, results[1]));\n@@ -2742,11 +2661,8 @@ TEST_P(AllReduceTest, AsyncAllReduce_PRED_2GPUs) {\n       << \"Test requires at least \" << kNumReplicas << \" devices (\"\n       << hlo_runner_->device_count() << \" available)\";\n \n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(kModuleStr, config));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleStr, kNumReplicas));\n \n   int64_t num_elements =\n       module->entry_computation()->root_instruction()->shape().dimensions()[0];\n@@ -2765,12 +2681,11 @@ TEST_P(AllReduceTest, AsyncAllReduce_PRED_2GPUs) {\n       LiteralUtil::CreateFromArray(expected_output);\n \n   TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n+      ExecutionResult execution_result,\n       ExecuteReplicated(std::move(module),\n-                        {{&input_literal1}, {&input_literal2}},\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n+                        std::vector<std::vector<Literal*>>{{&input_literal1},\n+                                                           {&input_literal2}}));\n+  const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n   EXPECT_TRUE(LiteralTestUtil::Equal(expected_output_literal, results[0]));\n   EXPECT_TRUE(LiteralTestUtil::Equal(expected_output_literal, results[1]));\n@@ -2799,22 +2714,17 @@ TEST_P(AllReduceTest, AsyncAllReduce_8GPUs_AllReplicasOneGroup) {\n                  << hlo_runner_->device_count() << \" available)\";\n   }\n \n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(kModuleStr, config));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleStr, kNumReplicas));\n   TF_ASSERT_OK_AND_ASSIGN(\n       InputsOutputs test_io,\n       BuildTestInputsOutputs(*module, kNumReplicas, /*num_iterations=*/1));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n+      ExecutionResult execution_result,\n       ExecuteReplicated(std::move(module),\n-                        /*arguments=*/test_io.InputLiteralPtrs(),\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n+                        /*arguments=*/test_io.InputLiteralPtrs()))\n+  const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n   for (int i = 0; i < kNumReplicas; ++i) {\n     // NB: nccl accumulation order can be different from expected calculations\n@@ -2871,23 +2781,18 @@ TEST_P(AllReduceTest, AsyncAllReduce_8GPUs_2ReplicasPerGroup) {\n                  << hlo_runner_->device_count() << \" available)\";\n   }\n \n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(kModuleStr, config));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleStr, kNumReplicas));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       InputsOutputs test_io,\n       BuildTestInputsOutputs(*module, kNumReplicas, kNumIterations));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n+      ExecutionResult execution_result,\n       ExecuteReplicated(std::move(module),\n-                        /*arguments=*/test_io.InputLiteralPtrs(),\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n+                        /*arguments=*/test_io.InputLiteralPtrs()));\n+  const std::vector<Literal>& results = execution_result.results;\n   ASSERT_EQ(results.size(), kNumReplicas);\n   for (int i = 0; i < kNumReplicas; ++i) {\n     ASSERT_TRUE(LiteralTestUtil::Equal(test_io.expected_outputs[i], results[i]))\n@@ -3000,9 +2905,7 @@ class CollectiveMetadataTest : public CollectiveOpsE2ETestBase {\n };\n \n TEST_F(CollectiveMetadataTest, ConstructCollectiveMetadata) {\n-  constexpr int kNumReplicas = 2;\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> unoptimized_module,\n-                          ParseAndReturnVerifiedModule(R\"(\n+  const absl::string_view kModuleStr = R\"(\n   HloModule test, replica_count=2\n \n   ENTRY test_computation {\n@@ -3014,35 +2917,29 @@ TEST_F(CollectiveMetadataTest, ConstructCollectiveMetadata) {\n \n     result_tuple = (f32[4], f32[4]{0:S(1)}, f32[1], u64[9]) custom-call(param_0, copy_1, const_0), custom_call_target=\"CollectiveMetadata\", output_to_operand_aliasing={{0}: (0, {}), {1}: (1, {})}\n     ROOT get_tuple_element = u64[9] get-tuple-element(result_tuple), index=3\n-  })\"));\n+  })\";\n+\n+  constexpr int kNumReplicas = 2;\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas)\n+      << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+      << hlo_runner_->device_count() << \" available)\";\n \n   TF_ASSERT_OK_AND_ASSIGN(\n-      std::unique_ptr<OpaqueExecutable> executable,\n-      hlo_runner_->CreateExecutable(std::move(unoptimized_module),\n-                                    /*run_hlo_passes=*/false));\n-  const std::array<Literal, 2> arguments = {\n-      LiteralUtil::CreateR1<float>({1.0f, 2.0f, 3.0f, 4.0f}),\n-      LiteralUtil::CreateR1<float>({1.0f, 2.0f, 3.0f, 4.0f})};\n-  DeviceAssignment device_assignment = MakeDeviceAssignment(kNumReplicas);\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> result,\n-      ExecuteReplicated(\n-          /*executable_provider*/ [&](int64_t) { return executable.get(); },\n-          /*argument_count_provider*/ [&](int64_t) { return arguments.size(); },\n-          /*argument_provider*/\n-          [&](int64_t replica_id, int64_t arg_index) {\n-            return &arguments[arg_index];\n-          },\n-          kNumReplicas,\n-          /*run_hlo_passes=*/false, &device_assignment));\n+      auto unoptimized_module,\n+      ParseAndReturnVerifiedModule(kModuleStr, kNumReplicas));\n \n+  Literal input_0 = LiteralUtil::CreateR1<float>({1.0f, 2.0f, 3.0f, 4.0f});\n+  Literal input_1 = LiteralUtil::CreateR1<float>({1.0f, 2.0f, 3.0f, 4.0f});\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      ExecutionResult execution_result,\n+      ExecuteReplicated(std::move(unoptimized_module),\n+                        /*arguments=*/std::vector<Literal*>{&input_0, &input_1},\n+                        /*run_hlo_passes=*/false));\n+  const std::vector<Literal>& result = execution_result.results;\n   ASSERT_EQ(result.size(), kNumReplicas);\n-  Literal first_result = std::move(result[0]);\n-  Literal second_result = std::move(result[1]);\n \n-  absl::Span<const uint64_t> first_result_data = first_result.data<uint64_t>();\n-  absl::Span<const uint64_t> second_result_data =\n-      second_result.data<uint64_t>();\n+  absl::Span<const uint64_t> first_result_data = result[0].data<uint64_t>();\n+  absl::Span<const uint64_t> second_result_data = result[1].data<uint64_t>();\n   constexpr int kNumElements = 9;\n   ASSERT_EQ(first_result_data.size(), kNumElements);\n   ASSERT_EQ(second_result_data.size(), kNumElements);\n@@ -3067,13 +2964,7 @@ TEST_F(CollectiveMetadataTest, ConstructCollectiveMetadata) {\n }\n \n TEST_F(CollectiveMetadataTest, ConstructCollectiveMetadataWithReplicaGroup) {\n-  constexpr int kNumReplicas = 4;\n-  if (hlo_runner_->device_count() < kNumReplicas) {\n-    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas << \" devices (\"\n-                 << hlo_runner_->device_count() << \" available)\";\n-  }\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> unoptimized_module,\n-                          ParseAndReturnVerifiedModule(R\"(\n+  const absl::string_view kModuleStr = R\"(\n   HloModule test, replica_count=4\n \n   ENTRY test_computation {\n@@ -3083,42 +2974,35 @@ TEST_F(CollectiveMetadataTest, ConstructCollectiveMetadataWithReplicaGroup) {\n \n     result_tuple = (f32[4], f32[4]{0:S(1)}, u64[7]) custom-call(param_0, copy_1), custom_call_target=\"CollectiveMetadata\", output_to_operand_aliasing={{0}: (0, {}), {1}: (1, {})}, backend_config=\"{\\\"collective_metadata_backend_config\\\":{\\\"collective_devices\\\": { \\\"replica_groups\\\": [{\\\"replica_ids\\\": [0,1]}, {\\\"replica_ids\\\": [2,3]}]}}}\"\n     ROOT get_tuple_element = u64[7] get-tuple-element(result_tuple), index=2\n-  })\"));\n+  })\";\n+\n+  constexpr int kNumReplicas = 4;\n+  if (hlo_runner_->device_count() < kNumReplicas) {\n+    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+                 << hlo_runner_->device_count() << \" available)\";\n+  }\n \n   TF_ASSERT_OK_AND_ASSIGN(\n-      std::unique_ptr<OpaqueExecutable> executable,\n-      hlo_runner_->CreateExecutable(std::move(unoptimized_module),\n-                                    /*run_hlo_passes=*/false));\n-  const std::array<Literal, 2> arguments = {\n-      LiteralUtil::CreateR1<float>({1.0f, 2.0f, 3.0f, 4.0f}),\n-      LiteralUtil::CreateR1<float>({1.0f, 2.0f, 3.0f, 4.0f})};\n-  DeviceAssignment device_assignment = MakeDeviceAssignment(kNumReplicas);\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> result,\n-      ExecuteReplicated(\n-          /*executable_provider*/ [&](int64_t) { return executable.get(); },\n-          /*argument_count_provider*/ [&](int64_t) { return arguments.size(); },\n-          /*argument_provider*/\n-          [&](int64_t replica_id, int64_t arg_index) {\n-            return &arguments[arg_index];\n-          },\n-          kNumReplicas,\n-          /*run_hlo_passes=*/false, &device_assignment));\n+      auto module, ParseAndReturnVerifiedModule(kModuleStr, kNumReplicas));\n \n-  ASSERT_EQ(result.size(), kNumReplicas);\n-  Literal replica_0_result_0 = std::move(result[0]);\n-  Literal replica_0_result_1 = std::move(result[1]);\n-  Literal replica_1_result_0 = std::move(result[2]);\n-  Literal replica_1_result_1 = std::move(result[3]);\n+  Literal input_0 = LiteralUtil::CreateR1<float>({1.0f, 2.0f, 3.0f, 4.0f});\n+  Literal input_1 = LiteralUtil::CreateR1<float>({1.0f, 2.0f, 3.0f, 4.0f});\n \n+  TF_ASSERT_OK_AND_ASSIGN(\n+      ExecutionResult execution_result,\n+      ExecuteReplicated(std::move(module),\n+                        /*arguments=*/std::vector<Literal*>{&input_0, &input_1},\n+                        /*run_hlo_passes=*/false));\n+  const std::vector<Literal>& result = execution_result.results;\n+  ASSERT_EQ(result.size(), kNumReplicas);\n   absl::Span<const uint64_t> replica_0_result_0_data =\n-      replica_0_result_0.data<uint64_t>();\n+      result[0].data<uint64_t>();\n   absl::Span<const uint64_t> replica_0_result_1_data =\n-      replica_0_result_1.data<uint64_t>();\n+      result[1].data<uint64_t>();\n   absl::Span<const uint64_t> replica_1_result_0_data =\n-      replica_1_result_0.data<uint64_t>();\n+      result[2].data<uint64_t>();\n   absl::Span<const uint64_t> replica_1_result_1_data =\n-      replica_1_result_1.data<uint64_t>();\n+      result[3].data<uint64_t>();\n \n   // Check the rank in the first position.\n   constexpr int kNumElements = 7;"
        },
        {
            "sha": "57ecd51f6ab1e8a2acea8c2aa32f96ddc7eab221",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test_base.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 7,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d51d84048204ea825caee48a9780bd4244bb0628/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d51d84048204ea825caee48a9780bd4244bb0628/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.cc?ref=d51d84048204ea825caee48a9780bd4244bb0628",
            "patch": "@@ -179,26 +179,40 @@ CollectiveOpsE2ETestBase::ExecuteReplicated(\n \n absl::StatusOr<CollectiveOpsE2ETestBase::ExecutionResult>\n CollectiveOpsE2ETestBase::ExecuteReplicated(std::unique_ptr<HloModule> module) {\n-  std::vector<std::vector<Literal*>> arguments(module->config().replica_count(),\n-                                               std::vector<Literal*>());\n-  return ExecuteReplicated(std::move(module), arguments,\n+  return ExecuteReplicated(std::move(module),\n+                           /*arguments=*/std::vector<Literal*>(),\n                            /*run_hlo_passes=*/true);\n }\n \n+absl::StatusOr<CollectiveOpsE2ETestBase::ExecutionResult>\n+CollectiveOpsE2ETestBase::ExecuteReplicated(\n+    std::unique_ptr<HloModule> module, const std::vector<Literal*>& arguments,\n+    bool run_hlo_passes) {\n+  int64_t num_devices =\n+      module->config().replica_count() * module->config().num_partitions();\n+\n+  return ExecuteReplicated(\n+      std::move(module),\n+      /*arguments=*/std::vector<std::vector<Literal*>>(num_devices, arguments),\n+      /*run_hlo_passes=*/run_hlo_passes);\n+}\n+\n absl::StatusOr<CollectiveOpsE2ETestBase::ExecutionResult>\n CollectiveOpsE2ETestBase::ExecuteReplicated(\n     std::unique_ptr<HloModule> module,\n-    const std::vector<std::vector<Literal*>> arguments, bool run_hlo_passes) {\n+    const std::vector<std::vector<Literal*>>& arguments, bool run_hlo_passes) {\n   int64_t num_replicas = module->config().replica_count();\n   int64_t num_partitions = module->config().num_partitions();\n \n   CHECK(num_replicas > 0 && \"expect at least one replica\");\n   CHECK(num_partitions > 0 && \"expect at least one partition\");\n-  CHECK(num_replicas == arguments.size() &&\n-        \"expect arguments for each replica and partition\");\n \n   DeviceAssignment device_assignment =\n       MakeDeviceAssignment(num_replicas, num_partitions);\n+  int64_t num_devices = num_replicas * num_partitions;\n+\n+  CHECK(num_devices == arguments.size() &&\n+        \"expect arguments for each replica and partition\");\n \n   ExecutionResult execution_result;\n \n@@ -221,7 +235,8 @@ CollectiveOpsE2ETestBase::ExecuteReplicated(\n           [&](int64_t replica_idx, int64_t argument_idx) -> const Literal* {\n             return arguments[replica_idx][argument_idx];\n           },\n-          num_replicas, /*run_hlo_passes=*/run_hlo_passes,\n+          /*num_replicas=*/num_devices,\n+          /*run_hlo_passes=*/run_hlo_passes,\n           /*device_assignment=*/&device_assignment));\n   return execution_result;\n }"
        },
        {
            "sha": "b2c5f2d1fbf154188cd53ec56ef73b8d3ad359aa",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test_base.h",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d51d84048204ea825caee48a9780bd4244bb0628/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d51d84048204ea825caee48a9780bd4244bb0628/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.h?ref=d51d84048204ea825caee48a9780bd4244bb0628",
            "patch": "@@ -78,9 +78,14 @@ class CollectiveOpsE2ETestBase : public HloHardwareIndependentTestBase {\n   absl::StatusOr<ExecutionResult> ExecuteReplicated(\n       std::unique_ptr<HloModule> module);\n \n+  absl::StatusOr<ExecutionResult> ExecuteReplicated(\n+      std::unique_ptr<HloModule> module, const std::vector<Literal*>& arguments,\n+      bool run_hlo_passes = true);\n+\n   absl::StatusOr<ExecutionResult> ExecuteReplicated(\n       std::unique_ptr<HloModule> module,\n-      std::vector<std::vector<Literal*>> arguments, bool run_hlo_passes = true);\n+      const std::vector<std::vector<Literal*>>& arguments,\n+      bool run_hlo_passes = true);\n \n   const se::GpuComputeCapability& Capability() {\n     return hlo_runner_->backend()"
        }
    ],
    "stats": {
        "total": 396,
        "additions": 150,
        "deletions": 246
    }
}