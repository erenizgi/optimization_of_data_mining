{
    "author": "sohaibiftikhar",
    "message": "[XLA:GPU]: Add triton_xla.block_barrier operation.\n\nThe block_barrier wraps up the other atomics work by using atomic_write\nand atomic_spin_wait for implementing a block level barrier on the GPU.\n\nImplementation follows the same principle as that of\nall_reduce_kernel_lib.cu.h#SyncRemoteBlocks\n\nPiperOrigin-RevId: 805841404",
    "sha": "05354cdddaa31a9d7f5da45bb34f1042efdfb28a",
    "files": [
        {
            "sha": "10d002db62eb988390a4cc347192c410057ab2c4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/triton_xla_ops.td",
            "status": "modified",
            "additions": 68,
            "deletions": 0,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05354cdddaa31a9d7f5da45bb34f1042efdfb28a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05354cdddaa31a9d7f5da45bb34f1042efdfb28a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td?ref=05354cdddaa31a9d7f5da45bb34f1042efdfb28a",
            "patch": "@@ -321,5 +321,73 @@ def TTXLA_AtomicSpinWaitOp : TTXLA_Op<\"atomic_spin_wait\",\n     }];\n }\n \n+def TTXLA_BlockBarrierOp : TTXLA_Op<\"block_barrier\", []> {\n+  let summary = \"A block-level barrier for inter-GPU synchronization.\";\n+\n+  let description = [{\n+    This operation implements a block-level barrier between multiple GPUs.\n+    It takes a pointer of pointers to signal buffers (one from each\n+    participating GPU), the rank of the current device, and a signal value to\n+    write/wait for. The signal values on consequent calls to this operation must\n+    increase monotonically for the same signal buffers pointer. A block on one\n+    GPU synchronizes with blocks on other GPUs with the same block id.\n+\n+    *NOTE* Block orchestration order is not guaranteed across GPUs,\n+    so there is a chance of a deadlock should the number of blocks be greater\n+    than the number of SMs on a GPU. That is to say that if BlockN is launched\n+    on GPU0 and BlockM on GPU1 and they both wait for each other to finish,\n+    and there is no place to schedule more blocks on either GPU, then we\n+    will have a deadlock. In general, the user of this operation should avoid\n+    this by not oversubscribing the number of available SMs on a GPU. There is\n+    also a secondary restriction that the number of threads per block must\n+    exceed the number of participating GPUs. In the nominal case, where we are\n+    within a single NV Link domain, this should not be an issue since the number\n+    of participating GPUs would usually be much smaller than the number of\n+    threads per block.\n+\n+    The synchronization logic is as follows:\n+    1. Each GPU (identified by `device_rank`) writes the `signal` value to an\n+       index `blockId * world_size + device_rank` in *every* buffer provided in\n+       `$signal_buffers`.\n+    2. After writing, GPU `r` polls its own buffer (`$signal_buffers[r]`).\n+    3. It waits until the values at locations corresponding to all other ranks\n+       (i.e., `blockId * world_size + [0..world_size)` are greater than or equal\n+       to the `signal` value.\n+    4. When the wait is over, all GPUs working on the same `blockId` have\n+       successfully synchronized.\n+\n+    This operation ensures that GPUs in the same synchronization group do not\n+    get more than one \"wave\" (or chunk) of data processing ahead of each other.\n+    For clarity, a wave here is a set of work items that are processed together.\n+    A single thread block for instance can process multiple waves. In such a\n+    scenario, any given GPU can be at most 1 wave ahead of the other GPUs for\n+    the same block id.\n+\n+    Internally, this operation uses TTXLA_Atomic(Write|SpinWait)Op on signal\n+    buffers to synchronize all participating GPUs.\n+  }];\n+\n+  let arguments = (ins\n+    // Pointers of signal buffers (one from each participating GPU).\n+    TT_PtrOf<[TT_PtrOf<[I32]>]>:$signal_buffers,\n+    // The rank of the current device.\n+    I32:$device_rank,\n+    // The signal value to write/wait for.\n+    I32:$signal_value,\n+    // The number of devices participating in the barrier.\n+    // The length of the first dimension of the signal buffers should be the\n+    // same as this value.\n+    I32Attr:$world_size\n+  );\n+\n+  // This is a barrier op and produces no results.\n+  let results = (outs);\n+\n+  let assemblyFormat = [{\n+    $signal_buffers `,` $device_rank `,` $signal_value `,` attr-dict `:`\n+    functional-type(operands, results)\n+  }];\n+}\n+\n #endif // XLA_BACKENDS_GPU_CODEGEN_TRITON_IR_TRITON_XLA_OPS_TD_\n "
        },
        {
            "sha": "98320135f5489e5d092503abe223997f73867a91",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05354cdddaa31a9d7f5da45bb34f1042efdfb28a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05354cdddaa31a9d7f5da45bb34f1042efdfb28a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD?ref=05354cdddaa31a9d7f5da45bb34f1042efdfb28a",
            "patch": "@@ -39,6 +39,7 @@ cc_library(\n         \"triton_xla_extract_insert_to_triton_pass.cc\",\n         \"triton_xla_fold_transpose_pass.cc\",\n         \"triton_xla_lower_atomics_pass.cc\",\n+        \"triton_xla_lower_block_barrier_pass.cc\",\n         \"triton_xla_lower_get_tid_pass.cc\",\n         \"triton_xla_squeeze_dims_pass.cc\",\n     ],"
        },
        {
            "sha": "3f6d73c2a9d209be5e4434b801111b7afd33d78e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05354cdddaa31a9d7f5da45bb34f1042efdfb28a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05354cdddaa31a9d7f5da45bb34f1042efdfb28a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h?ref=05354cdddaa31a9d7f5da45bb34f1042efdfb28a",
            "patch": "@@ -35,12 +35,14 @@ std::unique_ptr<mlir::Pass> CreateTritonXLAExtractInsertToTritonPass(\n std::unique_ptr<mlir::Pass> CreateTritonXLASqueezeDimsPass();\n std::unique_ptr<mlir::Pass> CreateTritonXLAFoldTransposePass();\n std::unique_ptr<mlir::Pass> CreateGeneralizeKernelSignaturePass();\n+\n std::unique_ptr<mlir::Pass> CreateInt4ToPackedInt4RewritePass(\n     const stream_executor::DeviceDescription& device_description);\n std::unique_ptr<mlir::Pass> CreateRoundF32ToTF32ForTf32DotRewritePass();\n std::unique_ptr<mlir::Pass> CreateExtractTmaInfoPass();\n std::unique_ptr<mlir::Pass> CreateTritonXLALowerGetTidPass();\n std::unique_ptr<mlir::Pass> CreateTritonXLALowerAtomicsPass();\n+std::unique_ptr<mlir::Pass> CreateTritonXLALowerBlockBarrierPass();\n std::unique_ptr<mlir::Pass> CreateTritonXLAConvertUnsupportedTypesPass();\n \n // Returns true if the `op` contains an operation in it's regions that satisfies"
        },
        {
            "sha": "c6501d70f26c658db505006a16e22a46411314f3",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.td",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05354cdddaa31a9d7f5da45bb34f1042efdfb28a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05354cdddaa31a9d7f5da45bb34f1042efdfb28a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td?ref=05354cdddaa31a9d7f5da45bb34f1042efdfb28a",
            "patch": "@@ -129,6 +129,16 @@ def TritonXLALowerAtomicsPass\n   let constructor = \"CreateTritonXLALowerAtomicsPass()\";\n }\n \n+def TritonXLALowerBlockBarrierPass\n+    : Pass<\"triton-xla-block-barrier\", \"mlir::ModuleOp\"> {\n+  let summary = \"Lower a block barrier to triton_xla/triton operations.\";\n+  let description = [{\n+    This pass lowers a block barrier to Triton IR using atomic write and\n+    spin-wait operations.\n+  }];\n+  let constructor = \"CreateTritonXLALowerBlockBarrierPass()\";\n+}\n+\n def TritonXLAConvertUnsupportedTypesPass\n     : Pass<\"convert-unsupported-types\", \"mlir::ModuleOp\"> {\n   let summary = \"Converts types unsupported by Triton into their supported equivalents.\";"
        },
        {
            "sha": "2fd160d0e427493adb82de93d748e4b58c38811f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_block_barrier.mlir",
            "status": "added",
            "additions": 37,
            "deletions": 0,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05354cdddaa31a9d7f5da45bb34f1042efdfb28a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_block_barrier.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05354cdddaa31a9d7f5da45bb34f1042efdfb28a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_block_barrier.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_block_barrier.mlir?ref=05354cdddaa31a9d7f5da45bb34f1042efdfb28a",
            "patch": "@@ -0,0 +1,37 @@\n+// RUN: xla-opt %s --triton-xla-block-barrier | FileCheck %s\n+\n+// CHECK-LABEL: block_barrier_kernel\n+// CHECK-SAME: %[[PTR:.+]]: !tt.ptr<!tt.ptr<i32>>,\n+// CHECK-SAME: %[[SIGNAL_VALUE:.+]]: i32,\n+// CHECK-SAME: %[[RANK:.+]]: i32\n+tt.func @block_barrier_kernel(\n+  %ptr : !tt.ptr<!tt.ptr<i32>>, %signal_value : i32, %rank: i32) {\n+  // CHECK-NEXT: %[[WORLD_SIZE:.+]] = arith.constant 8 : i32\n+  // CHECK-NEXT: %[[TID:.+]] = triton_xla.get_tid\n+  // CHECK-NEXT: %[[PROGRAM_ID:.+]] = tt.get_program_id x\n+  // CHECK-NEXT: %[[COND:.+]] = arith.cmpi ult, %[[TID]], %[[WORLD_SIZE]]\n+  // CHECK-NEXT: scf.if %[[COND]] {\n+  // CHECK-NEXT:   %[[BITCAST:.+]] = tt.bitcast %[[PTR]]\n+  // CHECK-NEXT:   %[[SPLAT_PTR:.+]] = tt.splat %[[BITCAST]]\n+  // CHECK-NEXT:   %[[RANGE:.+]] = tt.make_range {end = 8 : i32, start = 0 : i32}\n+  // CHECK-NEXT:   %[[ADD_PTR:.+]] = tt.addptr %[[SPLAT_PTR]], %[[RANGE]]\n+  // CHECK-NEXT:   %[[LOAD:.+]] = tt.load %[[ADD_PTR]]\n+  // CHECK-NEXT:   %[[INT_TO_PTR:.+]] = tt.int_to_ptr %[[LOAD]]\n+  // CHECK-NEXT:   %[[MULI:.+]] = arith.muli %[[PROGRAM_ID]], %[[WORLD_SIZE]]\n+  // CHECK-NEXT:   %[[ADDI:.+]] = arith.addi %[[MULI]], %[[RANK]]\n+  // CHECK-NEXT:   %[[SPLAT_ADDI:.+]] = tt.splat %[[ADDI]]\n+  // CHECK-NEXT:   %[[ADD_PTR_2:.+]] = tt.addptr %[[INT_TO_PTR]], %[[SPLAT_ADDI]]\n+  // CHECK-NEXT:   triton_xla.atomic_write sys, release, %[[ADD_PTR_2]], %[[SIGNAL_VALUE]]\n+  // CHECK-NEXT:   %[[ADD_PTR_3:.+]] = tt.addptr %[[BITCAST]], %[[RANK]]\n+  // CHECK-NEXT:   %[[LOAD_2:.+]] = tt.load %[[ADD_PTR_3]]\n+  // CHECK-NEXT:   %[[INT_TO_PTR_2:.+]] = tt.int_to_ptr %[[LOAD_2]]\n+  // CHECK-NEXT:   %[[ADD_PTR_4:.+]] = tt.addptr %[[INT_TO_PTR_2]], %[[MULI]]\n+  // CHECK-NEXT:   %[[SPLAT_ADD_PTR_4:.+]] = tt.splat %[[ADD_PTR_4]]\n+  // CHECK-NEXT:   %[[ADD_PTR_5:.+]] = tt.addptr %[[SPLAT_ADD_PTR_4]], %[[RANGE]]\n+  // CHECK-NEXT:   triton_xla.atomic_spin_wait sys, acquire, %[[ADD_PTR_5]], less_than, %[[SIGNAL_VALUE]]\n+  // CHECK-NEXT: }\n+  // CHECK-NEXT: tt.return\n+  triton_xla.block_barrier %ptr, %rank, %signal_value, { world_size = 8 : i32 } :\n+    (!tt.ptr<!tt.ptr<i32>>, i32, i32) -> ()\n+  tt.return\n+}\n\\ No newline at end of file"
        },
        {
            "sha": "c36efa0ba782d7c25de688de9dc0fe45733814ee",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_lower_block_barrier_pass.cc",
            "status": "added",
            "additions": 208,
            "deletions": 0,
            "changes": 208,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05354cdddaa31a9d7f5da45bb34f1042efdfb28a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_block_barrier_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05354cdddaa31a9d7f5da45bb34f1042efdfb28a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_block_barrier_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_block_barrier_pass.cc?ref=05354cdddaa31a9d7f5da45bb34f1042efdfb28a",
            "patch": "@@ -0,0 +1,208 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstdint>\n+#include <memory>\n+#include <utility>\n+\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/SCF/IR/SCF.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/Location.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/TypeRange.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/Triton/IR/Types.h\"\n+\n+namespace mlir::triton::xla {\n+\n+#define GEN_PASS_DEF_TRITONXLALOWERBLOCKBARRIERPASS\n+#include \"xla/backends/gpu/codegen/triton/transforms/passes.h.inc\"\n+\n+namespace {\n+\n+LogicalResult LowerBlockBarrierOp(BlockBarrierOp block_barrier,\n+                                  PatternRewriter& rewriter) {\n+  mlir::ImplicitLocOpBuilder builder(block_barrier.getLoc(), rewriter);\n+\n+  mlir::TypedValue<triton::PointerType> signal_buffers_arg =\n+      block_barrier.getSignalBuffers();\n+  const mlir::TypedValue<mlir::IntegerType> rank =\n+      block_barrier.getDeviceRank();\n+  const mlir::TypedValue<mlir::IntegerType> signal_value =\n+      block_barrier.getSignalValue();\n+  const int32_t world_size = block_barrier.getWorldSize();\n+  // Triton magic constant.\n+  constexpr int32_t kGlobalAddressSpace = 1;\n+\n+  const mlir::TypedValue<mlir::Type> world_size_op =\n+      builder.create<mlir::arith::ConstantOp>(\n+          builder.getI32IntegerAttr(world_size));\n+  const mlir::TypedValue<mlir::IntegerType> thread_id =\n+      builder.create<triton::xla::GetTidOp>();\n+  const mlir::TypedValue<mlir::IntegerType> block_id =\n+      builder.create<triton::GetProgramIdOp>(0);\n+  auto tid_is_lt_world_size = builder.create<mlir::arith::CmpIOp>(\n+      mlir::arith::CmpIPredicate::ult, thread_id, world_size_op);\n+\n+  // Only the first `world_size` threads will execute this block.\n+  builder.create<mlir::scf::IfOp>(\n+      /*cond=*/tid_is_lt_world_size,\n+      // Inside if block so tid must be less than world_size.\n+      /*thenBuilder=*/\n+      [&](mlir::OpBuilder& op_builder, mlir::Location location) {\n+        mlir::ImplicitLocOpBuilder builder(location, op_builder);\n+        // -----\n+        // Types\n+        // -----\n+        const mlir::IntegerType i32_type = builder.getI32Type();\n+        // tensor<world_size x i32>\n+        const auto i32_tensor_type =\n+            mlir::RankedTensorType::get({world_size}, i32_type);\n+        // !tt.ptr<i32>\n+        const auto ptr_to_i32_type = mlir::triton::PointerType::get(\n+            rewriter.getI32Type(), kGlobalAddressSpace);\n+        // !tt.ptr<i64>\n+        const auto ptr_to_i64_type = mlir::triton::PointerType::get(\n+            rewriter.getI64Type(), kGlobalAddressSpace);\n+        // tensor<world_size x !tt.ptr<i32>>\n+        const auto tensor_of_ptr_to_i32_type =\n+            mlir::RankedTensorType::get({world_size}, ptr_to_i32_type);\n+        // tensor<world_size x !tt.ptr<i64>>\n+        const auto tensor_of_i64_ptr_type =\n+            mlir::RankedTensorType::get({world_size}, ptr_to_i64_type);\n+        // -----\n+        // Ops (-> implies return type of the op declared in the next line)\n+        // -----\n+        // Triton seems to fail to do pointer arithmetic on pointer of\n+        // pointers. So we cast the inner one to i64.\n+        // -> !tt.ptr<i64>\n+        auto signal_buffers_i64 = builder.create<mlir::triton::BitcastOp>(\n+            ptr_to_i64_type, signal_buffers_arg);\n+        // SignalBuffers[WorldSize][BlockSize][WorldSize]\n+        // -> tensor<world_size x !tt.ptr<i64>>\n+        auto signal_buffers_tensor = builder.create<mlir::triton::SplatOp>(\n+            tensor_of_i64_ptr_type, signal_buffers_i64);\n+        // -> tensor<world_size x i32>\n+        auto all_ranks = builder.create<mlir::triton::MakeRangeOp>(\n+            i32_tensor_type, 0, world_size);\n+        // Pointer to SignalBuffers[0..WorldSize]\n+        // -> tensor<world_size x !tt.ptr<i64>>\n+        auto signal_buffer_ptr = builder.create<mlir::triton::AddPtrOp>(\n+            tensor_of_i64_ptr_type, signal_buffers_tensor, all_ranks);\n+        // SignalBuffers[0..WorldSize]\n+        // -> tensor<world_size x i64>\n+        auto signal_buffer_i64 = builder.create<mlir::triton::LoadOp>(\n+            /*ptr=*/signal_buffer_ptr,\n+            /*cache=*/mlir::triton::CacheModifier::NONE,\n+            /*evict=*/mlir::triton::EvictionPolicy::NORMAL,\n+            /*isVolatile=*/false);\n+        // -> tensor<world_size x !tt.ptr<i32>>\n+        auto signal_buffer = builder.create<mlir::triton::IntToPtrOp>(\n+            tensor_of_ptr_to_i32_type, signal_buffer_i64);\n+        auto block_offset = builder.create<mlir::arith::MulIOp>(\n+            i32_type, block_id, world_size_op);\n+        auto block_offset_plus_rank =\n+            builder.create<mlir::arith::AddIOp>(i32_type, block_offset, rank);\n+        // -> tensor<world_size x i32>\n+        auto block_offset_plus_rank_tensor =\n+            builder.create<mlir::triton::SplatOp>(i32_tensor_type,\n+                                                  block_offset_plus_rank);\n+        // SignalBuffers[0..WorldSize][block_id][rank]\n+        // -> tensor<world_size x !tt.ptr<i32>>\n+        auto signal_addresses = builder.create<mlir::triton::AddPtrOp>(\n+            tensor_of_ptr_to_i32_type, signal_buffer,\n+            block_offset_plus_rank_tensor);\n+        // Signal all ranks on the same block id.\n+        builder.create<mlir::triton::xla::AtomicWriteOp>(\n+            /*result_types=*/mlir::TypeRange{},\n+            /*ptr=*/signal_addresses,\n+            /*signal_value=*/signal_value,\n+            /*mask=*/mlir::Value{},\n+            /*scope=*/mlir::triton::MemSyncScope::SYSTEM,\n+            /*sem=*/mlir::triton::MemSemantic::RELEASE);\n+        // Pointer to SignalBuffers[rank]\n+        // -> !tt.ptr<i64>\n+        auto read_address_ptr_to_i64 = builder.create<mlir::triton::AddPtrOp>(\n+            signal_buffers_i64.getType(), signal_buffers_i64, rank);\n+        // SignalBuffers[rank]\n+        // -> i64\n+        auto read_address_i64 = builder.create<mlir::triton::LoadOp>(\n+            /*ptr=*/read_address_ptr_to_i64,\n+            /*cache=*/mlir::triton::CacheModifier::NONE,\n+            /*evict=*/mlir::triton::EvictionPolicy::NORMAL,\n+            /*isVolatile=*/false);\n+        // -> !tt.ptr<i32>\n+        auto read_address = builder.create<mlir::triton::IntToPtrOp>(\n+            ptr_to_i32_type, read_address_i64);\n+        // Pointer to SignalBuffers[rank][block_id]\n+        // -> !tt.ptr<i32>\n+        auto read_address_at_block_offset =\n+            builder.create<mlir::triton::AddPtrOp>(ptr_to_i32_type,\n+                                                   read_address, block_offset);\n+        // -> tensor<world_size x !tt.ptr<i32>>\n+        auto read_address_at_block_offset_tensor =\n+            builder.create<mlir::triton::SplatOp>(tensor_of_ptr_to_i32_type,\n+                                                  read_address_at_block_offset);\n+        // SignalBuffers[rank][block_id][0..WorldSize]\n+        // -> tensor<world_size x !tt.ptr<i32>>\n+        auto wait_addresses = builder.create<mlir::triton::AddPtrOp>(\n+            tensor_of_ptr_to_i32_type, read_address_at_block_offset_tensor,\n+            all_ranks);\n+        // Wait for all ranks on the same block id to signal.\n+        builder.create<mlir::triton::xla::AtomicSpinWaitOp>(\n+            /*result_types=*/mlir::TypeRange{},\n+            /*ptr=*/wait_addresses,\n+            /*expected=*/signal_value,\n+            /*mask=*/mlir::Value{},\n+            /*scope=*/mlir::triton::MemSyncScope::SYSTEM,\n+            /*sem=*/mlir::triton::MemSemantic::ACQUIRE,\n+            /*comparator=*/Comparator::LT);\n+        // Terminate the block.\n+        builder.create<mlir::scf::YieldOp>();\n+      });\n+  rewriter.eraseOp(block_barrier);\n+  return success();\n+}\n+\n+class TritonXLALowerBlockBarrierPass\n+    : public impl::TritonXLALowerBlockBarrierPassBase<\n+          TritonXLALowerBlockBarrierPass> {\n+ public:\n+  using Base::Base;\n+\n+ private:\n+  void runOnOperation() override {\n+    RewritePatternSet patterns(&getContext());\n+    patterns.add(LowerBlockBarrierOp);\n+    if (failed(applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n+      return signalPassFailure();\n+    }\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<Pass> CreateTritonXLALowerBlockBarrierPass() {\n+  return std::make_unique<TritonXLALowerBlockBarrierPass>();\n+}\n+\n+}  // namespace mlir::triton::xla"
        }
    ],
    "stats": {
        "total": 326,
        "additions": 326,
        "deletions": 0
    }
}