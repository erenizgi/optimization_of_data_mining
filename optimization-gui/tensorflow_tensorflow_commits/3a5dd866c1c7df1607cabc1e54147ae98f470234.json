{
    "author": "akuegel",
    "message": "[XLA:GPU] Add prefix sum kernel.\n\nThis will be used for a dedicated ScanOp.\n\nPiperOrigin-RevId: 828903522",
    "sha": "3a5dd866c1c7df1607cabc1e54147ae98f470234",
    "files": [
        {
            "sha": "a9c1872516d65d7ffac362a32a1e6bebfe779bde",
            "filename": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "status": "modified",
            "additions": 58,
            "deletions": 0,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a5dd866c1c7df1607cabc1e54147ae98f470234/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a5dd866c1c7df1607cabc1e54147ae98f470234/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD?ref=3a5dd866c1c7df1607cabc1e54147ae98f470234",
            "patch": "@@ -2376,6 +2376,64 @@ cuda_library(\n     alwayslink = 1,\n ) for typename in get_cub_sort_kernel_types()]\n \n+[cuda_library(\n+    name = \"cub_prefix_sum_kernel_cuda_{}\".format(typename),\n+    srcs = [\"cub_prefix_sum_kernel_cuda.cu.cc\"],\n+    # copybara:uncomment compatible_with = [\"//buildenv/target:non_prod\"],\n+    local_defines = [\"CUB_TYPE_\" + typename.upper()],\n+    tags = [\n+        \"cuda-only\",\n+        \"gpu\",\n+    ],\n+    deps = [\n+        \":cuda_platform\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:kernel\",\n+        \"//xla/stream_executor:kernel_spec\",\n+        \"//xla/stream_executor/gpu:gpu_kernel_registry\",\n+        \"//xla/stream_executor/gpu:prefix_sum_kernel\",\n+        \"@local_config_cuda//cuda:cub_headers\",\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+    ],\n+    alwayslink = 1,\n+) for typename in get_cub_sort_kernel_types()]\n+\n+xla_test(\n+    name = \"cub_prefix_sum_kernel_cuda_test\",\n+    srcs = [\"cub_prefix_sum_kernel_cuda_test.cc\"],\n+    backends = [\"gpu\"],\n+    tags = [\"cuda-only\"],\n+    deps = [\n+        \"//xla:shape_util\",\n+        \"//xla:types\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:kernel\",\n+        \"//xla/stream_executor:launch_dim\",\n+        \"//xla/stream_executor:platform\",\n+        \"//xla/stream_executor:platform_manager\",\n+        \"//xla/stream_executor:stream\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor:stream_executor_memory_allocator\",\n+        \"//xla/stream_executor:typed_kernel_factory\",\n+        \"//xla/stream_executor/gpu:gpu_kernel_registry\",\n+        \"//xla/stream_executor/gpu:prefix_sum_kernel\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/lib/math:math_util\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/cleanup\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/numeric:bits\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+    ] + [\":cub_prefix_sum_kernel_cuda_\" + suffix for suffix in get_cub_sort_kernel_types()],\n+)\n+\n cuda_library(\n     name = \"topk_kernel_cuda\",\n     srcs = ["
        },
        {
            "sha": "6a40bb9b70a7aaf0b7b9c7ff364dd6fb1b3989c5",
            "filename": "third_party/xla/xla/stream_executor/cuda/cub_prefix_sum_kernel_cuda.cu.cc",
            "status": "added",
            "additions": 207,
            "deletions": 0,
            "changes": 207,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a5dd866c1c7df1607cabc1e54147ae98f470234/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcub_prefix_sum_kernel_cuda.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a5dd866c1c7df1607cabc1e54147ae98f470234/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcub_prefix_sum_kernel_cuda.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcub_prefix_sum_kernel_cuda.cu.cc?ref=3a5dd866c1c7df1607cabc1e54147ae98f470234",
            "patch": "@@ -0,0 +1,207 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstddef>\n+\n+#include \"cub/block/block_scan.cuh\"\n+#include \"third_party/gpus/cuda/include/cuda.h\"\n+#include \"third_party/gpus/cuda/include/cuda_fp16.h\"\n+#include \"xla/stream_executor/cuda/cuda_platform.h\"\n+#include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n+#include \"xla/stream_executor/gpu/prefix_sum_kernel.h\"\n+#include \"xla/stream_executor/kernel_spec.h\"\n+\n+namespace se = stream_executor;\n+\n+namespace stream_executor::cuda {\n+namespace {\n+\n+template <unsigned int BLOCK_SIZE, typename ElementT>\n+__device__ void RowPrefixSum(const ElementT* data_in, ElementT* data_out,\n+                             size_t num_items) {\n+  // `BLOCK_SIZE` must be a power of 2 no larger than 1024.\n+  static_assert(BLOCK_SIZE <= 1024 && (BLOCK_SIZE & (BLOCK_SIZE - 1)) == 0);\n+  using BlockScan = cub::BlockScan<ElementT, BLOCK_SIZE>;\n+  __shared__ typename BlockScan::TempStorage temp_storage;\n+  ElementT total = 0;\n+  size_t thread_idx =\n+      ((threadIdx.z * blockDim.y) + threadIdx.y) * blockDim.x + threadIdx.x;\n+  for (size_t offset = thread_idx; offset < num_items; offset += BLOCK_SIZE) {\n+    if (offset < num_items) {\n+      ElementT thread_data = data_in[offset];\n+      ElementT block_aggregate;\n+      BlockScan(temp_storage)\n+          .InclusiveSum(thread_data, thread_data, block_aggregate);\n+      data_out[offset] = thread_data + total;\n+      total += block_aggregate;\n+      __syncthreads();\n+    }\n+  }\n+}\n+\n+template <typename ElementT>\n+__global__ void PrefixSum(const void* data_in, void* data_out,\n+                          size_t num_items) {\n+  const ElementT* data_in_typed = static_cast<const ElementT*>(data_in);\n+  ElementT* data_out_typed = static_cast<ElementT*>(data_out);\n+  int64_t block_idx =\n+      ((static_cast<int64_t>(blockIdx.z) * gridDim.y) + blockIdx.y) *\n+          gridDim.x +\n+      blockIdx.x;\n+  int64_t row_offset = block_idx * num_items;\n+  // https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/:\n+  // CUDA architecture limits the numbers of threads per block (1024 threads\n+  // per block limit).\n+  switch (blockDim.x * blockDim.y * blockDim.z) {\n+    case 1024:\n+      RowPrefixSum<1024>(data_in_typed + row_offset,\n+                         data_out_typed + row_offset, num_items);\n+      break;\n+    case 512:\n+      RowPrefixSum<512>(data_in_typed + row_offset, data_out_typed + row_offset,\n+                        num_items);\n+      break;\n+    case 256:\n+      RowPrefixSum<256>(data_in_typed + row_offset, data_out_typed + row_offset,\n+                        num_items);\n+      break;\n+    case 128:\n+      RowPrefixSum<128>(data_in_typed + row_offset, data_out_typed + row_offset,\n+                        num_items);\n+      break;\n+    case 64:\n+      RowPrefixSum<64>(data_in_typed + row_offset, data_out_typed + row_offset,\n+                       num_items);\n+      break;\n+    case 32:\n+      RowPrefixSum<32>(data_in_typed + row_offset, data_out_typed + row_offset,\n+                       num_items);\n+      break;\n+    case 16:\n+      RowPrefixSum<16>(data_in_typed + row_offset, data_out_typed + row_offset,\n+                       num_items);\n+      break;\n+    case 8:\n+      RowPrefixSum<8>(data_in_typed + row_offset, data_out_typed + row_offset,\n+                      num_items);\n+      break;\n+    case 4:\n+      RowPrefixSum<4>(data_in_typed + row_offset, data_out_typed + row_offset,\n+                      num_items);\n+      break;\n+    case 2:\n+      RowPrefixSum<2>(data_in_typed + row_offset, data_out_typed + row_offset,\n+                      num_items);\n+      break;\n+    case 1:\n+      RowPrefixSum<1>(data_in_typed + row_offset, data_out_typed + row_offset,\n+                      num_items);\n+      break;\n+    default:\n+      // Unsupported block size.\n+      assert(false);\n+      return;\n+  }\n+}\n+\n+#define XLA_CUB_PREFIX_SUM_KERNEL_SPEC(primitive_type, native_type)          \\\n+  se::KernelLoaderSpec GetPrefixSum##primitive_type##KernelSpec(int arity) { \\\n+    return se::KernelLoaderSpec::CreateInProcessSymbolSpec(                  \\\n+        absl::bit_cast<void*>(&PrefixSum<native_type>),                      \\\n+        \"PrefixSum##primitive_type##Kernel\", arity);                         \\\n+  }\n+\n+// Floating point types.\n+#ifdef CUB_TYPE_F16\n+XLA_CUB_PREFIX_SUM_KERNEL_SPEC(F16, __half)\n+#endif\n+#ifdef CUB_TYPE_F32\n+XLA_CUB_PREFIX_SUM_KERNEL_SPEC(F32, float)\n+#endif\n+#ifdef CUB_TYPE_F64\n+XLA_CUB_PREFIX_SUM_KERNEL_SPEC(F64, double)\n+#endif\n+\n+// Signed integer types.\n+#ifdef CUB_TYPE_S8\n+XLA_CUB_PREFIX_SUM_KERNEL_SPEC(S8, int8_t)\n+#endif\n+#ifdef CUB_TYPE_S16\n+XLA_CUB_PREFIX_SUM_KERNEL_SPEC(S16, int16_t)\n+#endif\n+#ifdef CUB_TYPE_S32\n+XLA_CUB_PREFIX_SUM_KERNEL_SPEC(S32, int32_t)\n+#endif\n+#ifdef CUB_TYPE_S64\n+XLA_CUB_PREFIX_SUM_KERNEL_SPEC(S64, int64_t)\n+#endif\n+\n+// Unsigned integer types.\n+#ifdef CUB_TYPE_U8\n+XLA_CUB_PREFIX_SUM_KERNEL_SPEC(U8, uint8_t)\n+#endif\n+#ifdef CUB_TYPE_U16\n+XLA_CUB_PREFIX_SUM_KERNEL_SPEC(U16, uint16_t)\n+#endif\n+#ifdef CUB_TYPE_U32\n+XLA_CUB_PREFIX_SUM_KERNEL_SPEC(U32, uint32_t)\n+#endif\n+#ifdef CUB_TYPE_U64\n+XLA_CUB_PREFIX_SUM_KERNEL_SPEC(U64, uint64_t)\n+#endif\n+\n+}  // namespace\n+\n+#define REGISTER_PREFIX_SUM_KERNEL(primitive_type)                 \\\n+  GPU_KERNEL_REGISTRY_REGISTER_KERNEL_STATICALLY(                  \\\n+      PrefixSum##primitive_type##Kernel,                           \\\n+      se::gpu::PrefixSum##primitive_type##Kernel, kCudaPlatformId, \\\n+      GetPrefixSum##primitive_type##KernelSpec)\n+\n+#ifdef CUB_TYPE_F16\n+REGISTER_PREFIX_SUM_KERNEL(F16)\n+#endif\n+#ifdef CUB_TYPE_F32\n+REGISTER_PREFIX_SUM_KERNEL(F32)\n+#endif\n+#ifdef CUB_TYPE_F64\n+REGISTER_PREFIX_SUM_KERNEL(F64)\n+#endif\n+#ifdef CUB_TYPE_S8\n+REGISTER_PREFIX_SUM_KERNEL(S8)\n+#endif\n+#ifdef CUB_TYPE_S16\n+REGISTER_PREFIX_SUM_KERNEL(S16)\n+#endif\n+#ifdef CUB_TYPE_S32\n+REGISTER_PREFIX_SUM_KERNEL(S32)\n+#endif\n+#ifdef CUB_TYPE_S64\n+REGISTER_PREFIX_SUM_KERNEL(S64)\n+#endif\n+#ifdef CUB_TYPE_U8\n+REGISTER_PREFIX_SUM_KERNEL(U8)\n+#endif\n+#ifdef CUB_TYPE_U16\n+REGISTER_PREFIX_SUM_KERNEL(U16)\n+#endif\n+#ifdef CUB_TYPE_U32\n+REGISTER_PREFIX_SUM_KERNEL(U32)\n+#endif\n+#ifdef CUB_TYPE_U64\n+REGISTER_PREFIX_SUM_KERNEL(U64)\n+#endif\n+\n+}  // namespace stream_executor::cuda"
        },
        {
            "sha": "10cb4954d81a0541e6b0cad4f1d1e3d70257647f",
            "filename": "third_party/xla/xla/stream_executor/cuda/cub_prefix_sum_kernel_cuda_test.cc",
            "status": "added",
            "additions": 229,
            "deletions": 0,
            "changes": 229,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a5dd866c1c7df1607cabc1e54147ae98f470234/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcub_prefix_sum_kernel_cuda_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a5dd866c1c7df1607cabc1e54147ae98f470234/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcub_prefix_sum_kernel_cuda_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcub_prefix_sum_kernel_cuda_test.cc?ref=3a5dd866c1c7df1607cabc1e54147ae98f470234",
            "patch": "@@ -0,0 +1,229 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <algorithm>\n+#include <cstddef>\n+#include <cstdint>\n+#include <memory>\n+#include <optional>\n+#include <string>\n+#include <tuple>\n+#include <vector>\n+\n+#include <gtest/gtest.h>\n+#include \"absl/cleanup/cleanup.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/numeric/bits.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/primitive_util.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/gpu/gpu_kernel_registry.h\"\n+#include \"xla/stream_executor/gpu/prefix_sum_kernel.h\"\n+#include \"xla/stream_executor/launch_dim.h\"\n+#include \"xla/stream_executor/platform.h\"\n+#include \"xla/stream_executor/platform_manager.h\"\n+#include \"xla/stream_executor/stream.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n+#include \"xla/stream_executor/typed_kernel_factory.h\"  // IWYU pragma: keep, required for KernelType::FactoryType::Create\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/types.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace se = stream_executor;\n+\n+namespace stream_executor::cuda {\n+namespace {\n+\n+class CubPrefixSumKernelCudaTest\n+    : public ::testing::Test,\n+      public ::testing::WithParamInterface<\n+          std::tuple<xla::PrimitiveType, int, int, bool>> {\n+ protected:\n+  void SetUp() override {\n+    TF_ASSERT_OK_AND_ASSIGN(platform_,\n+                            se::PlatformManager::PlatformWithName(\"CUDA\"));\n+    TF_ASSERT_OK_AND_ASSIGN(executor_, platform_->ExecutorForDevice(0));\n+    TF_ASSERT_OK_AND_ASSIGN(stream_, executor_->CreateStream(std::nullopt));\n+    allocator_ =\n+        std::make_unique<se::StreamExecutorMemoryAllocator>(stream_->parent());\n+  }\n+\n+  template <typename T>\n+  absl::StatusOr<se::DeviceMemory<T>> CheckNotNull(\n+      se::DeviceMemory<T> device_memory, absl::string_view name) {\n+    if (device_memory.is_null()) {\n+      return absl::InternalError(\n+          absl::StrFormat(\"Device memory for %s is null\", name));\n+    }\n+    return device_memory;\n+  }\n+\n+  template <typename Kernel, typename T>\n+  absl::Status ComputePrefixSumOnDevice(const std::vector<T>& input,\n+                                        std::vector<T>& output, size_t num_rows,\n+                                        size_t num_items, bool in_place) {\n+    // Load kernel\n+    gpu::GpuKernelRegistry registry =\n+        gpu::GpuKernelRegistry::GetGlobalRegistry();\n+    TF_ASSIGN_OR_RETURN(auto kernel, registry.LoadKernel<Kernel>(executor_));\n+\n+    // Setup device buffers\n+    TF_ASSIGN_OR_RETURN(\n+        se::DeviceMemory<T> device_input,\n+        CheckNotNull(executor_->AllocateArray<T>(input.size()), \"input\"));\n+    se::DeviceMemory<T> device_output;\n+    if (in_place) {\n+      device_output = device_input;\n+    } else {\n+      TF_ASSIGN_OR_RETURN(\n+          device_output,\n+          CheckNotNull(executor_->AllocateArray<T>(output.size()), \"output\"));\n+    }\n+    auto cleanup = absl::MakeCleanup([&]() {\n+      if (!in_place) {\n+        executor_->Deallocate(&device_output);\n+      }\n+      executor_->Deallocate(&device_input);\n+    });\n+\n+    TF_RETURN_IF_ERROR(stream_->Memcpy(&device_input, input.data(),\n+                                       input.size() * sizeof(input[0])));\n+    size_t num_threads_per_block =\n+        std::min(size_t{1024}, absl::bit_ceil(num_items));\n+    // Call kernel\n+    TF_RETURN_IF_ERROR(\n+        kernel.Launch(stream_executor::ThreadDim(num_threads_per_block, 1, 1),\n+                      stream_executor::BlockDim(num_rows, 1, 1), stream_.get(),\n+                      device_input, device_output, num_items));\n+    TF_RETURN_IF_ERROR(stream_->BlockHostUntilDone());\n+    TF_RETURN_IF_ERROR(stream_->Memcpy(output.data(), device_output,\n+                                       output.size() * sizeof(output[0])));\n+    return absl::OkStatus();\n+  }\n+\n+  template <typename Kernel, typename T>\n+  absl::Status CheckComputePrefixSumOnDevice(size_t num_rows, size_t num_items,\n+                                             bool in_place) {\n+    std::vector<T> input(num_rows * num_items);\n+    std::vector<T> output(input.size());\n+    std::vector<T> expected;\n+    expected.reserve(input.size());\n+    for (int i = 0; i < num_rows; ++i) {\n+      for (int j = 0; j < num_items; ++j) {\n+        // We use only small values, otherwise we will get precision problems\n+        // with small data types.\n+        input[i * num_items + j] = static_cast<T>((i + j) % 8);\n+        expected.push_back(input[i * num_items + j]);\n+        if (j > 0) {\n+          expected.back() += expected[expected.size() - 2];\n+        }\n+      }\n+    }\n+    TF_RETURN_IF_ERROR(ComputePrefixSumOnDevice<Kernel>(input, output, num_rows,\n+                                                        num_items, in_place));\n+    EXPECT_EQ(output, expected);\n+    return absl::OkStatus();\n+  }\n+\n+  se::Platform* platform_;\n+  se::StreamExecutor* executor_;\n+  std::unique_ptr<se::Stream> stream_;\n+  std::unique_ptr<se::StreamExecutorMemoryAllocator> allocator_;\n+};\n+\n+TEST_P(CubPrefixSumKernelCudaTest, TestPrefixSum) {\n+  absl::Status status;\n+  const auto& [primitive_type, num_rows, num_items, in_place] = GetParam();\n+  switch (primitive_type) {\n+    case xla::F16:\n+      status =\n+          CheckComputePrefixSumOnDevice<gpu::PrefixSumF16Kernel, xla::half>(\n+              num_rows, num_items, in_place);\n+      break;\n+    case xla::F32:\n+      status = CheckComputePrefixSumOnDevice<gpu::PrefixSumF32Kernel, float>(\n+          num_rows, num_items, in_place);\n+      break;\n+    case xla::F64:\n+      status = CheckComputePrefixSumOnDevice<gpu::PrefixSumF64Kernel, double>(\n+          num_rows, num_items, in_place);\n+      break;\n+    case xla::S8:\n+      status = CheckComputePrefixSumOnDevice<gpu::PrefixSumS8Kernel, int8_t>(\n+          num_rows, num_items, in_place);\n+      break;\n+    case xla::S16:\n+      status = CheckComputePrefixSumOnDevice<gpu::PrefixSumS16Kernel, int16_t>(\n+          num_rows, num_items, in_place);\n+      break;\n+    case xla::S32:\n+      status = CheckComputePrefixSumOnDevice<gpu::PrefixSumS32Kernel, int32_t>(\n+          num_rows, num_items, in_place);\n+      break;\n+    case xla::S64:\n+      status = CheckComputePrefixSumOnDevice<gpu::PrefixSumS64Kernel, int64_t>(\n+          num_rows, num_items, in_place);\n+      break;\n+    case xla::U8:\n+      status = CheckComputePrefixSumOnDevice<gpu::PrefixSumU8Kernel, uint8_t>(\n+          num_rows, num_items, in_place);\n+      break;\n+    case xla::U16:\n+      status = CheckComputePrefixSumOnDevice<gpu::PrefixSumU16Kernel, uint16_t>(\n+          num_rows, num_items, in_place);\n+      break;\n+    case xla::U32:\n+      status = CheckComputePrefixSumOnDevice<gpu::PrefixSumU32Kernel, uint32_t>(\n+          num_rows, num_items, in_place);\n+      break;\n+    case xla::U64:\n+      status = CheckComputePrefixSumOnDevice<gpu::PrefixSumU64Kernel, uint64_t>(\n+          num_rows, num_items, in_place);\n+      break;\n+    default:\n+      status = absl::OkStatus();\n+  }\n+  TF_EXPECT_OK(status);\n+}\n+\n+std::string ParametersToString(\n+    const ::testing::TestParamInfo<\n+        ::testing::tuple<xla::PrimitiveType, int, int, bool>>& data) {\n+  const auto& [primitive_type, num_rows, num_items, in_place] = data.param;\n+  return absl::StrFormat(\n+      \"Prefix_Sum_%dx%d_%s%s\", num_rows, num_items,\n+      xla::primitive_util::LowercasePrimitiveTypeName(primitive_type),\n+      in_place ? \"_in_place\" : \"\");\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    CubPrefixSumKernelCudaTestInstance, CubPrefixSumKernelCudaTest,\n+    ::testing::Combine(::testing::ValuesIn({xla::F16, xla::F32, xla::F64,\n+                                            xla::S8, xla::S16, xla::S32,\n+                                            xla::S64, xla::U8, xla::U16,\n+                                            xla::U32, xla::U64}),\n+                       ::testing::ValuesIn({1, 2, 3, 128, 511, 512}),\n+                       ::testing::ValuesIn({1, 2, 3, 128, 511, 512}),\n+                       ::testing::ValuesIn({false, true})),\n+    ParametersToString);\n+\n+}  // namespace\n+}  // namespace stream_executor::cuda"
        },
        {
            "sha": "ef1ea32f347a9a9865220ff884f834956ce0cb0e",
            "filename": "third_party/xla/xla/stream_executor/gpu/BUILD",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a5dd866c1c7df1607cabc1e54147ae98f470234/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a5dd866c1c7df1607cabc1e54147ae98f470234/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD?ref=3a5dd866c1c7df1607cabc1e54147ae98f470234",
            "patch": "@@ -1004,3 +1004,14 @@ cc_library(\n         \"//xla/stream_executor:kernel\",\n     ],\n )\n+\n+cc_library(\n+    name = \"prefix_sum_kernel\",\n+    hdrs = [\"prefix_sum_kernel.h\"],\n+    deps = [\n+        \"//xla:types\",\n+        \"//xla/backends/gpu/runtime:buffer_debug_log_structs\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:kernel\",\n+    ],\n+)"
        },
        {
            "sha": "5a6307be4f711f2f5222e000f168952c0c53600c",
            "filename": "third_party/xla/xla/stream_executor/gpu/prefix_sum_kernel.h",
            "status": "added",
            "additions": 73,
            "deletions": 0,
            "changes": 73,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a5dd866c1c7df1607cabc1e54147ae98f470234/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fprefix_sum_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a5dd866c1c7df1607cabc1e54147ae98f470234/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fprefix_sum_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fprefix_sum_kernel.h?ref=3a5dd866c1c7df1607cabc1e54147ae98f470234",
            "patch": "@@ -0,0 +1,73 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_STREAM_EXECUTOR_GPU_PREFIX_SUM_KERNEL_H_\n+#define XLA_STREAM_EXECUTOR_GPU_PREFIX_SUM_KERNEL_H_\n+\n+#include <cstddef>\n+#include <cstdint>\n+\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/kernel.h\"\n+#include \"xla/types.h\"\n+\n+namespace stream_executor::gpu {\n+struct PrefixSumF16Kernel {\n+  using KernelType = TypedKernel<const DeviceMemory<xla::half>,\n+                                 DeviceMemory<xla::half>, size_t>;\n+};\n+struct PrefixSumF32Kernel {\n+  using KernelType =\n+      TypedKernel<const DeviceMemory<float>, DeviceMemory<float>, size_t>;\n+};\n+struct PrefixSumF64Kernel {\n+  using KernelType =\n+      TypedKernel<const DeviceMemory<double>, DeviceMemory<double>, size_t>;\n+};\n+struct PrefixSumS8Kernel {\n+  using KernelType =\n+      TypedKernel<const DeviceMemory<int8_t>, DeviceMemory<int8_t>, size_t>;\n+};\n+struct PrefixSumS16Kernel {\n+  using KernelType =\n+      TypedKernel<const DeviceMemory<int16_t>, DeviceMemory<int16_t>, size_t>;\n+};\n+struct PrefixSumS32Kernel {\n+  using KernelType =\n+      TypedKernel<const DeviceMemory<int32_t>, DeviceMemory<int32_t>, size_t>;\n+};\n+struct PrefixSumS64Kernel {\n+  using KernelType =\n+      TypedKernel<const DeviceMemory<int64_t>, DeviceMemory<int64_t>, size_t>;\n+};\n+struct PrefixSumU8Kernel {\n+  using KernelType =\n+      TypedKernel<const DeviceMemory<uint8_t>, DeviceMemory<uint8_t>, size_t>;\n+};\n+struct PrefixSumU16Kernel {\n+  using KernelType =\n+      TypedKernel<const DeviceMemory<uint16_t>, DeviceMemory<uint16_t>, size_t>;\n+};\n+struct PrefixSumU32Kernel {\n+  using KernelType =\n+      TypedKernel<const DeviceMemory<uint32_t>, DeviceMemory<uint32_t>, size_t>;\n+};\n+struct PrefixSumU64Kernel {\n+  using KernelType =\n+      TypedKernel<const DeviceMemory<uint64_t>, DeviceMemory<uint64_t>, size_t>;\n+};\n+}  // namespace stream_executor::gpu\n+\n+#endif  // XLA_STREAM_EXECUTOR_GPU_PREFIX_SUM_KERNEL_H_"
        }
    ],
    "stats": {
        "total": 578,
        "additions": 578,
        "deletions": 0
    }
}