{
    "author": "pifon2a",
    "message": "[XLA:GPU] Create llvm_emitter out of ir_emitter(_nested).\n\nPiperOrigin-RevId: 838120757",
    "sha": "19cac92f41ebf3db0b116c2ff8d5f0b05a373bde",
    "files": [
        {
            "sha": "e5c7d49fdbf1222e00772fe42a629101d94fa455",
            "filename": "third_party/xla/xla/backends/gpu/codegen/llvm/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/19cac92f41ebf3db0b116c2ff8d5f0b05a373bde/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/19cac92f41ebf3db0b116c2ff8d5f0b05a373bde/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2FBUILD?ref=19cac92f41ebf3db0b116c2ff8d5f0b05a373bde",
            "patch": "@@ -17,15 +17,9 @@ package(\n )\n \n cc_library(\n-    name = \"ir_emitter\",\n-    srcs = [\n-        \"ir_emitter.cc\",\n-        \"ir_emitter_nested.cc\",\n-    ],\n-    hdrs = [\n-        \"ir_emitter.h\",\n-        \"ir_emitter_nested.h\",\n-    ],\n+    name = \"llvm_emitter\",\n+    srcs = [\"llvm_emitter.cc\"],\n+    hdrs = [\"llvm_emitter.h\"],\n     deps = [\n         \":parallel_loop_emitter\",\n         \":sort_util\","
        },
        {
            "sha": "60a20387767a4fb85ab02885f9c9c9d20f50dc0d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/llvm/ir_emitter.h",
            "status": "removed",
            "additions": 0,
            "deletions": 162,
            "changes": 162,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/beaaef308afbbf26d2108c386dac4177bccbddd0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fir_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/beaaef308afbbf26d2108c386dac4177bccbddd0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fir_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fir_emitter.h?ref=beaaef308afbbf26d2108c386dac4177bccbddd0",
            "patch": "@@ -1,162 +0,0 @@\n-/* Copyright 2017 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_BACKENDS_GPU_CODEGEN_LLVM_IR_EMITTER_H_\n-#define XLA_BACKENDS_GPU_CODEGEN_LLVM_IR_EMITTER_H_\n-\n-#include <vector>\n-\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"llvm/IR/Function.h\"\n-#include \"llvm/IR/IRBuilder.h\"\n-#include \"llvm/IR/Value.h\"\n-#include \"xla/backends/gpu/runtime/thunk.h\"\n-#include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/service/gpu/hlo_to_ir_bindings.h\"\n-#include \"xla/service/gpu/ir_emitter_context.h\"\n-#include \"xla/service/llvm_ir/fused_ir_emitter.h\"\n-#include \"xla/service/llvm_ir/ir_array.h\"\n-#include \"xla/service/llvm_ir/ir_builder_mixin.h\"\n-#include \"xla/service/llvm_ir/loop_emitter.h\"\n-#include \"xla/shape_util.h\"\n-\n-namespace xla {\n-namespace gpu {\n-\n-// Abstract base class for translating HLO graphs to LLVM IR for a GPU.\n-//\n-// There are two concrete subclasses of IrEmitter: IrEmitterNested and\n-// IrEmitterUnnested.  In the unnested variety, each HLO gets its own kernel\n-// function, whereas in the nested version the whole computation is emitted as\n-// one *non-kernel* function.\n-//\n-// In XLA, kernel functions never call other kernel functions.  This means that\n-// if we have a kernel -- e.g. implementing a kReduce HLO -- that wants to use\n-// an HLO computation as a \"subroutine\" -- e.g. the HLO computation that\n-// specifies how to reduce two elements -- then the subroutine computation must\n-// be emitted using IrEmitterNested.\n-//\n-// Fusion nodes are a special case.  A fusion node is emitted using\n-// IrEmitterUnnested, but the code is generated using FusedIrEmitter, which is\n-// not a subclass of gpu::IrEmitter, and in fact is better understood as an IR\n-// generator generator.  See comments on that class.\n-class IrEmitter : public DfsHloVisitorWithDefault,\n-                  public IrBuilderMixin<IrEmitter> {\n- public:\n-  IrEmitter(const IrEmitter&) = delete;\n-  IrEmitter& operator=(const IrEmitter&) = delete;\n-\n-  // Constructs an IrEmitter with the given IrEmitter context.\n-  // ir_emitter_context is owned by the caller and should outlive the IrEmitter\n-  // object.\n-  explicit IrEmitter(IrEmitterContext* ir_emitter_context, bool is_nested);\n-\n-  absl::Status DefaultAction(HloInstruction* hlo) override;\n-  absl::Status HandleConstant(HloInstruction* constant) override;\n-  absl::Status HandleGetTupleElement(\n-      HloInstruction* get_tuple_element) override;\n-  absl::Status HandleConvolution(HloInstruction* convolution) override;\n-  absl::Status HandleFft(HloInstruction* fft) override;\n-  absl::Status HandleAllReduce(HloInstruction* crs) override;\n-  absl::Status HandleInfeed(HloInstruction* infeed) override;\n-  absl::Status HandleOutfeed(HloInstruction* outfeed) override;\n-  absl::Status HandleSend(HloInstruction* send) override;\n-  absl::Status HandleSendDone(HloInstruction* send_done) override;\n-  absl::Status HandleRecv(HloInstruction* recv) override;\n-  absl::Status HandleRecvDone(HloInstruction* recv_done) override;\n-  absl::Status HandleParameter(HloInstruction* parameter) override;\n-  absl::Status HandleTuple(HloInstruction* tuple) override;\n-  absl::Status HandleScatter(HloInstruction* scatter) override;\n-  absl::Status HandleCall(HloInstruction* call) override;\n-  absl::Status HandleCustomCall(HloInstruction* custom_call) override;\n-  absl::Status HandleBatchNormInference(HloInstruction* batch_norm) override;\n-  absl::Status HandleBatchNormTraining(HloInstruction* batch_norm) override;\n-  absl::Status HandleBatchNormGrad(HloInstruction* batch_norm) override;\n-  absl::Status HandleAddDependency(HloInstruction* add_dependency) override;\n-\n-  absl::Status FinishVisit(HloInstruction* root) override {\n-    return absl::OkStatus();\n-  }\n-\n-  llvm::IRBuilderBase* builder() { return &b_; }\n-\n- protected:\n-  // Helper for calling HloToIrBindings::GetIrArray.\n-  //\n-  // Gets the IrArray which contains inst.  This array has metadata that makes\n-  // it valid only within the IR that implements consumer.  If you are\n-  // implementing an HLO and want to get its own output buffer, call\n-  // GetIrArray(hlo, hlo).\n-  llvm_ir::IrArray GetIrArray(const HloInstruction& inst,\n-                              const HloInstruction& consumer,\n-                              const ShapeIndex& shape_index = {}) {\n-    return bindings_.GetIrArray(inst, consumer, shape_index);\n-  }\n-  // A convenient helper for calling HloToIrBindings::GetBasePointer.\n-  llvm::Value* GetBasePointer(const HloInstruction& inst,\n-                              ShapeIndexView shape_index = {}) const {\n-    return bindings_.GetBasePointer(inst, shape_index);\n-  }\n-\n-  // Generates the IrArray for each output of an hlo instruction and returns\n-  // a vector containing such IrArrays.\n-  std::vector<llvm_ir::IrArray> ConstructIrArrayForOutputs(\n-      const HloInstruction& hlo);\n-\n-  // Emit a single-threaded or multi-threaded loop that computes every element\n-  // in the result of the given HLO instruction. This produces a series of\n-  // nested loops (e.g. one for each dimension of the `hlo`'s shape). The body\n-  // of the inner-most loop is provided by the body_emitter function.\n-  virtual absl::Status EmitTargetElementLoop(\n-      const HloInstruction& hlo, const llvm_ir::ElementGenerator& body_emitter);\n-\n-  IrEmitterContext* ir_emitter_context_;\n-  llvm::Module* module_;\n-\n-  // The following fields track the IR emission state. According to LLVM memory\n-  // management rules, their memory is owned by the module.\n-  llvm::IRBuilder<> b_;\n-\n-  // Mapping from HLO to its underlying LLVM value.\n-  HloToIrBindings bindings_;\n-\n-  // Bind all argument IrArrays of `fusion` to `fused_emitter`.\n-  void BindFusionArguments(const HloInstruction* fusion,\n-                           FusedIrEmitter* fused_emitter);\n-};\n-\n-absl::StatusOr<ThunkSequence> EmitBitonicSortLLVMIR(\n-    const HloSortInstruction* sort, llvm::Module* llvm_module,\n-    IrEmitterContext* ir_emitter_context);\n-\n-absl::StatusOr<ThunkSequence> EmitPadToStaticLLVMIR(\n-    const HloCustomCallInstruction* hlo, llvm::Module* llvm_module,\n-    IrEmitterContext* ir_emitter_context);\n-\n-absl::StatusOr<ThunkSequence> EmitSliceToDynamicLLVMIR(\n-    const HloCustomCallInstruction* hlo, llvm::Module* llvm_module,\n-    IrEmitterContext* ir_emitter_context);\n-\n-absl::StatusOr<ThunkSequence> EmitRngGetAndUpdateStateLLVMIR(\n-    const HloRngGetAndUpdateStateInstruction* hlo, llvm::Module* llvm_module,\n-    IrEmitterContext* ir_emitter_context);\n-\n-}  // namespace gpu\n-}  // namespace xla\n-\n-#endif  // XLA_BACKENDS_GPU_CODEGEN_LLVM_IR_EMITTER_H_"
        },
        {
            "sha": "dcf83cee18ed541575297a52296d59aae7571d84",
            "filename": "third_party/xla/xla/backends/gpu/codegen/llvm/ir_emitter_nested.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 403,
            "changes": 403,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/beaaef308afbbf26d2108c386dac4177bccbddd0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fir_emitter_nested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/beaaef308afbbf26d2108c386dac4177bccbddd0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fir_emitter_nested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fir_emitter_nested.cc?ref=beaaef308afbbf26d2108c386dac4177bccbddd0",
            "patch": "@@ -1,403 +0,0 @@\n-/* Copyright 2017 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-#include \"xla/backends/gpu/codegen/llvm/ir_emitter_nested.h\"\n-\n-#include <algorithm>\n-#include <cstddef>\n-#include <cstdint>\n-#include <iterator>\n-#include <string>\n-#include <utility>\n-#include <vector>\n-\n-#include \"absl/algorithm/container.h\"\n-#include \"absl/log/check.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/str_cat.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n-#include \"llvm/ADT/ArrayRef.h\"\n-#include \"llvm/IR/Argument.h\"\n-#include \"llvm/IR/BasicBlock.h\"\n-#include \"llvm/IR/Constants.h\"\n-#include \"llvm/IR/DerivedTypes.h\"\n-#include \"llvm/IR/Function.h\"\n-#include \"llvm/IR/GlobalValue.h\"\n-#include \"llvm/IR/GlobalVariable.h\"\n-#include \"llvm/IR/IRBuilder.h\"\n-#include \"llvm/IR/Instructions.h\"\n-#include \"llvm/IR/LLVMContext.h\"\n-#include \"llvm/Support/Casting.h\"\n-#include \"llvm/TargetParser/Triple.h\"\n-#include \"xla/backends/gpu/codegen/llvm/ir_emitter.h\"\n-#include \"xla/codegen/emitters/computation_fingerprint.h\"\n-#include \"xla/hlo/ir/hlo_computation.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/literal.h\"\n-#include \"xla/service/gpu/gpu_constants.h\"\n-#include \"xla/service/gpu/gpu_executable.h\"\n-#include \"xla/service/gpu/ir_emission_utils.h\"\n-#include \"xla/service/gpu/ir_emitter_context.h\"\n-#include \"xla/service/llvm_ir/buffer_assignment_util.h\"\n-#include \"xla/service/llvm_ir/ir_array.h\"\n-#include \"xla/service/llvm_ir/llvm_util.h\"\n-#include \"xla/service/llvm_ir/loop_emitter.h\"\n-#include \"xla/service/llvm_ir/tuple_ops.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/shape_util.h\"\n-#include \"xla/status_macros.h\"\n-#include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/util.h\"\n-#include \"tsl/platform/errors.h\"\n-#include \"tsl/platform/fingerprint.h\"\n-#include \"tsl/platform/statusor.h\"\n-\n-namespace xla {\n-namespace gpu {\n-namespace {\n-\n-class IrEmitterNested : public IrEmitter {\n- public:\n-  // Constructs an LLVM IR emitter for a nested HLO computation. `function` is\n-  // the containing IR function this emitter produces IR to. See\n-  // IrEmitter::IrEmitter for the meanings of other arguments.\n-  IrEmitterNested(const HloComputation& nested_computation,\n-                  IrEmitterContext* ir_emitter_context);\n-\n-  IrEmitterNested(const IrEmitterNested&) = delete;\n-  IrEmitterNested& operator=(const IrEmitterNested&) = delete;\n-\n-  // Overrides the default empty implementation. Binds the given instruction\n-  // \"parameter\" with the parameter of the IR function.\n-  absl::Status HandleParameter(HloInstruction* parameter) override;\n-\n-  // Generate the code for the computation passed in the constructor, if it\n-  // wasn't already generated previously.\n-  // As well as generting the code for the function, emits code for global\n-  // constants, and also populates related information to 'ir_emitter_context_'\n-  // for large-constant initializations. Large constants don't get initializers\n-  // in the generated code and so must be initialized by XLA. The value of these\n-  // constants will be stored in 'content'. Constants with initializers in the\n-  // generated code will have empty 'content'.\n-  //\n-  // The allocation index for these constants will always be -1 (i.e. doesn't\n-  // correspond to any allocation)\n-  absl::StatusOr<llvm::Function*> CodegenNestedComputation();\n-\n- protected:\n-  absl::Status EmitTargetElementLoop(\n-      const HloInstruction& hlo,\n-      const llvm_ir::ElementGenerator& element_generator) override;\n-\n- private:\n-  // Emits constants to generated LLVM IR, and also populates related\n-  // information to 'ir_emitter_context_' for large-constant initializations.\n-  absl::Status EmitConstants(const HloComputation& computation);\n-\n-  const HloComputation& nested_computation_;\n-};\n-\n-IrEmitterNested::IrEmitterNested(const HloComputation& nested_computation,\n-                                 IrEmitterContext* ir_emitter_context)\n-    : IrEmitter(ir_emitter_context,\n-                /*is_nested=*/true),\n-      nested_computation_(nested_computation) {}\n-\n-// Nested function serves the same purpose on GPU as a thread-local function on\n-// a CPU.\n-absl::StatusOr<llvm::Function*> IrEmitterNested::CodegenNestedComputation() {\n-  // Include a fingerprint of the HLO in the function name to make the name\n-  // unique.\n-  tsl::Fprint128 fingerprint = tsl::Fingerprint128(\n-      emitters::GetComputationFingerprint(&nested_computation_, {}));\n-  std::string function_name = llvm_ir::SanitizeFunctionName(absl::StrCat(\n-      nested_computation_.name(), \"_\", fingerprint.low64, fingerprint.high64));\n-\n-  auto* function = module_->getFunction(function_name);\n-  if (function) return function;\n-\n-  TF_RETURN_IF_ERROR(EmitConstants(nested_computation_));\n-  std::vector<const HloInstruction*> io_hlos;\n-  std::vector<llvm::Type*> argument_types;\n-  std::vector<int64_t> argument_dereferenceable_bytes;\n-  const auto& params = nested_computation_.parameter_instructions();\n-  const auto n = params.size() + 1;\n-  io_hlos.reserve(n - 1);\n-  argument_types.reserve(n);\n-  argument_dereferenceable_bytes.reserve(n);\n-  for (const HloInstruction* param : params) {\n-    io_hlos.push_back(param);\n-    const Shape& param_shape = param->shape();\n-    argument_types.push_back(b_.getPtrTy());\n-    int64_t param_size =\n-        llvm_ir::ByteSizeOf(param_shape, module_->getDataLayout());\n-    argument_dereferenceable_bytes.push_back(param_size);\n-  }\n-\n-  const HloInstruction* root = nested_computation_.root_instruction();\n-  {\n-    const Shape& root_shape = root->shape();\n-    argument_types.push_back(b_.getPtrTy());\n-    int64_t root_size =\n-        llvm_ir::ByteSizeOf(root_shape, module_->getDataLayout());\n-    argument_dereferenceable_bytes.push_back(root_size);\n-  }\n-\n-  llvm::FunctionType* function_type =\n-      llvm::FunctionType::get(b_.getVoidTy(), argument_types, false);\n-  function = llvm::Function::Create(\n-      function_type,                       // The function type.\n-      llvm::GlobalValue::InternalLinkage,  // The linkage type.\n-      function_name,\n-      module_);  // The parent LLVM module.\n-  for (size_t arg_no = 0; arg_no < argument_dereferenceable_bytes.size();\n-       ++arg_no) {\n-    int64_t arg_size = argument_dereferenceable_bytes[arg_no];\n-    if (arg_size > 0) {\n-      function->addDereferenceableParamAttr(arg_no, arg_size);\n-    }\n-  }\n-\n-  // TODO(b/65380986): Investigate if adding fast math flags for generated\n-  // kernels makes sense.\n-\n-  llvm::BasicBlock* entry_bb =\n-      llvm::BasicBlock::Create(function->getContext(), \"entry\", function);\n-  // Emit a \"return void\" at entry_bb's end, and sets the insert point before\n-  // that return instruction.\n-  llvm::ReturnInst* ret_instr =\n-      llvm::ReturnInst::Create(function->getContext(), entry_bb);\n-  b_.SetInsertPoint(ret_instr);\n-\n-  std::vector<const HloInstruction*> non_io_hlos;\n-  non_io_hlos.push_back(root);\n-  for (const auto* hlo : nested_computation_.instructions()) {\n-    if (hlo->opcode() != HloOpcode::kParameter &&\n-        hlo != nested_computation_.root_instruction()) {\n-      non_io_hlos.push_back(hlo);\n-    }\n-  }\n-  bindings_.EmitBasePointersForHlos(io_hlos, non_io_hlos);\n-\n-  TF_RETURN_IF_ERROR(nested_computation_.root_instruction()->Accept(this));\n-  b_.SetInsertPoint(ret_instr);\n-\n-  // Function epilogue: copy the output value back.\n-  {\n-    // TODO(cheshire) Duplication vs. EmitThreadLocalFunctionEpilogue\n-    const HloInstruction* root_instruction =\n-        nested_computation_.root_instruction();\n-    llvm::Value* root_value = bindings_.GetBasePointer(*root_instruction);\n-    const Shape& return_shape = root_instruction->shape();\n-\n-    // Last argument is the out parameter.\n-    llvm::Argument* out_parameter = std::prev(function->arg_end(), 1);\n-\n-    if (ShapeUtil::IsScalar(return_shape)) {\n-      llvm::Value* ret_value =\n-          Load(llvm_ir::ShapeToIrType(return_shape, module_->getContext()),\n-               root_value, \"load_ret_value\");\n-      Store(ret_value, out_parameter);\n-    } else {\n-      CHECK(return_shape.IsTuple());\n-      llvm::Type* tuple_type =\n-          llvm_ir::ShapeToIrType(return_shape, module_->getContext());\n-\n-      for (int i = 0; i < return_shape.tuple_shapes().size(); i++) {\n-        const Shape& element_shape = return_shape.tuple_shapes(i);\n-        llvm::Value* destination = llvm_ir::EmitGetTupleElement(\n-            element_shape,\n-            /*index=*/i,\n-            /*alignment=*/1, out_parameter, tuple_type, &b_);\n-        llvm::Value* source = llvm_ir::EmitGetTupleElement(\n-            element_shape,\n-            /*index=*/i,\n-            /*alignment=*/1, root_value,\n-            llvm_ir::ShapeToIrType(root_instruction->shape(),\n-                                   module_->getContext()),\n-            &b_);\n-        Store(Load(llvm_ir::ShapeToIrType(element_shape, module_->getContext()),\n-                   source),\n-              destination);\n-      }\n-    }\n-  }\n-  b_.SetInsertPoint(ret_instr);\n-  return function;\n-}\n-\n-absl::Status IrEmitterNested::HandleParameter(HloInstruction* parameter) {\n-  return absl::OkStatus();\n-}\n-\n-absl::Status IrEmitterNested::EmitTargetElementLoop(\n-    const HloInstruction& hlo,\n-    const llvm_ir::ElementGenerator& element_generator) {\n-  // For MOF we give the loop emitter an array for every output it should\n-  // generate.\n-  if (hlo.shape().IsTuple()) {\n-    std::vector<llvm_ir::IrArray> target_arrays =\n-        ConstructIrArrayForOutputs(hlo);\n-    TF_RETURN_IF_ERROR(\n-        llvm_ir::LoopEmitter(element_generator, target_arrays, &b_).EmitLoop());\n-    llvm_ir::EmitTuple(GetIrArray(hlo, hlo), target_arrays, &b_);\n-    return absl::OkStatus();\n-  }\n-  return llvm_ir::LoopEmitter(element_generator, GetIrArray(hlo, hlo), &b_)\n-      .EmitLoop();\n-}\n-\n-absl::Status IrEmitterNested::EmitConstants(const HloComputation& computation) {\n-  for (HloInstruction* instr : computation.instructions()) {\n-    if (instr->opcode() != HloOpcode::kConstant) {\n-      continue;\n-    }\n-    const Literal& literal = instr->literal();\n-\n-    // These globals will be looked up by name by GpuExecutable so we need to\n-    // give them an external linkage.  Not all of their uses are visible in\n-    // the LLVM IR (e.g. TupleThunk) so we can't give then a linkage that\n-    // merely preserves their names (like available_externally), we also need\n-    // to ensure that they stick around even if they're \"unused\".\n-    //\n-    // We may have to be more clever here in the future if we notice that we're\n-    // keeping around too many globals because of their linkage.\n-    std::string global_name = llvm_ir::ConstantHloToGlobalName(*instr);\n-\n-    auto base = static_cast<const uint8_t*>(literal.untyped_data());\n-    GpuExecutable::ConstantInfo info = AppendGlobalConstant(\n-        module_, literal.element_count(),\n-        ShapeUtil::ByteSizeOfPrimitiveType(literal.shape().element_type()),\n-        global_name, /*allocation_idx=*/-1,\n-        DenseDataIntermediate::Alias(\n-            absl::MakeSpan(base, base + literal.size_bytes())));\n-    ir_emitter_context_->constants().push_back(std::move(info));\n-  }\n-  return absl::OkStatus();\n-}\n-\n-// Casts the provided llvm::Value* to the default address space. This is useful\n-// in particular for generating IR for AMDGPU target, as its kernel variables\n-// are in address space 5 instead of the default address space.\n-llvm::Value* AddrCastToDefault(llvm::Value* arg, llvm::IRBuilderBase& b) {\n-  llvm::Type* arg_type = arg->getType();\n-  CHECK(arg_type->isPointerTy());\n-  if (arg_type->getPointerAddressSpace() != 0) {\n-    llvm::Type* generic_arg_type = llvm::PointerType::get(\n-        llvm::cast<llvm::PointerType>(arg_type)->getContext(), 0);\n-    llvm::Value* addrspacecast_arg =\n-        b.CreateAddrSpaceCast(arg, generic_arg_type);\n-    return addrspacecast_arg;\n-  }\n-  return arg;\n-}\n-\n-}  // namespace\n-\n-absl::Status CallNestedComputation(llvm::IRBuilderBase* builder,\n-                                   IrEmitterContext& ir_emitter_context,\n-                                   llvm::Module* llvm_module,\n-                                   const HloComputation& computation,\n-                                   absl::Span<llvm::Value* const> operands,\n-                                   llvm::Value* output) {\n-  TF_RET_CHECK(computation.num_parameters() > 0);\n-\n-  TF_ASSIGN_OR_RETURN(llvm::Function * emitted_function,\n-                      IrEmitterNested(computation, &ir_emitter_context)\n-                          .CodegenNestedComputation());\n-\n-  // Operands are in default address space for non-AMDGPU target.\n-  // However for AMDGPU target, addrspacecast alloca variables from\n-  // addrspace 5 to addrspace 0 is needed.\n-  std::vector<llvm::Value*> arguments;\n-  absl::c_transform(\n-      operands, std::back_inserter(arguments),\n-      [builder](llvm::Value* arg) { return AddrCastToDefault(arg, *builder); });\n-\n-  llvm::Value* casted_output = AddrCastToDefault(output, *builder);\n-  arguments.push_back(casted_output);\n-\n-  builder->CreateCall(emitted_function, arguments);\n-\n-  return absl::OkStatus();\n-}\n-\n-GpuExecutable::ConstantInfo AppendGlobalConstant(\n-    llvm::Module* module, int64_t num_elements, int64_t bytes_per_element,\n-    absl::string_view symbol_name, int allocation_idx,\n-    DenseDataIntermediate content) {\n-  // LLVM and PTXAS don't deal well with large constants, so we only emit very\n-  // small constants directly in LLVM IR.  Larger constants are emitted with\n-  // zero initializers in LLVM IR and are later overwritten when the PTX/CUBIN\n-  // is loaded.\n-  bool should_emit_initializer = num_elements <= 1;\n-\n-  llvm::IRBuilder<> b(module->getContext());\n-  // Ptxas has issues if the constant allocation is smaller than 64 bytes.\n-  // TODO(b/253259975): Remove when fixed ptxas version is submitted.\n-  constexpr int64_t kMinConstAllocationInBytes = 64;\n-  bool needs_padding =\n-      num_elements * bytes_per_element < kMinConstAllocationInBytes;\n-\n-  llvm::ArrayType* global_type = llvm::ArrayType::get(\n-      b.getInt8Ty(),\n-      std::max(num_elements * bytes_per_element, kMinConstAllocationInBytes));\n-\n-  GpuExecutable::ConstantInfo info;\n-  llvm::Constant* initializer = [&]() -> llvm::Constant* {\n-    if (!should_emit_initializer) {\n-      info.content = std::move(content);\n-      return llvm::ConstantAggregateZero::get(global_type);\n-    }\n-\n-    std::vector<uint8_t> padded(kMinConstAllocationInBytes, 0);\n-    absl::c_copy(content.span(), padded.begin());\n-    return llvm::ConstantDataArray::get<uint8_t>(\n-        module->getContext(),\n-        needs_padding ? llvm::ArrayRef<uint8_t>(padded)\n-                      : llvm::ArrayRef<uint8_t>(content.span().data(),\n-                                                content.span().size()));\n-  }();\n-\n-  // Explicitly set global addrspace for SPIR backend.\n-  int addrspace = llvm::Triple(module->getTargetTriple()).isSPIR() ? 1 : 0;\n-  // These globals will be looked up by name by GpuExecutable so we need to\n-  // give them an external linkage.  Not all of their uses are visible in\n-  // the LLVM IR so we can't give then a linkage that merely preserves their\n-  // names (like available_externally), we also need to ensure that they stick\n-  // around even if they're \"unused\".\n-  //\n-  // We may have to be more clever here in the future if we notice that we're\n-  // keeping around too many globals because of their linkage.\n-  auto* global_for_const = new llvm::GlobalVariable(\n-      global_type, /*isConstant=*/should_emit_initializer,\n-      llvm::GlobalValue::ExternalLinkage,\n-      /*Initializer=*/initializer, symbol_name,\n-      /*TLMode=*/llvm::GlobalValue::NotThreadLocal,\n-      /*AddressSpace=*/addrspace,\n-      /*isExternallyInitialized=*/false);\n-  global_for_const->setAlignment(llvm::Align(kConstantBufferAlignBytes));\n-  module->insertGlobalVariable(global_for_const);\n-\n-  info.symbol_name.assign(symbol_name);\n-  info.allocation_index = allocation_idx;\n-  return info;\n-}\n-\n-}  // namespace gpu\n-}  // namespace xla"
        },
        {
            "sha": "7d82e8db2a451a7b56fab0d3e3f1e8b1cb7ed60a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/llvm/ir_emitter_nested.h",
            "status": "removed",
            "additions": 0,
            "deletions": 69,
            "changes": 69,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/beaaef308afbbf26d2108c386dac4177bccbddd0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fir_emitter_nested.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/beaaef308afbbf26d2108c386dac4177bccbddd0/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fir_emitter_nested.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fir_emitter_nested.h?ref=beaaef308afbbf26d2108c386dac4177bccbddd0",
            "patch": "@@ -1,69 +0,0 @@\n-/* Copyright 2018 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_BACKENDS_GPU_CODEGEN_LLVM_IR_EMITTER_NESTED_H_\n-#define XLA_BACKENDS_GPU_CODEGEN_LLVM_IR_EMITTER_NESTED_H_\n-\n-#include <cstdint>\n-#include <vector>\n-\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n-#include \"llvm/IR/IRBuilder.h\"\n-#include \"llvm/IR/Value.h\"\n-#include \"xla/hlo/ir/hlo_computation.h\"\n-#include \"xla/service/gpu/gpu_executable.h\"\n-#include \"xla/service/gpu/ir_emission_utils.h\"\n-#include \"xla/service/gpu/ir_emitter_context.h\"\n-\n-namespace xla {\n-namespace gpu {\n-\n-// Emits LLVM IR for a \"nested computation\" into a non-kernel device function.\n-//\n-// This is used to emit code for HloComputations that don't require a separate\n-// kernel call.  For example, IrEmitterNested is used to emit code for a kReduce\n-// HLO's elementwise reduction computation.  Notably, IrEmitterNested is *not*\n-// used to emit code for fusion nodes -- fusion nodes use FusedIrEmitter, which\n-// is a different beast altogether.\n-//\n-// IrEmitterNested generates a non-kernel function with the following\n-// parameters:\n-//\n-//   - N pointers to the buffers of each of the N parameters to the computation,\n-//   - a pointer to the output buffer of the computation, and\n-//   - a pointer to the top-level temp buffer.\n-absl::Status CallNestedComputation(llvm::IRBuilderBase* builder,\n-                                   IrEmitterContext& ir_emitter_context,\n-                                   llvm::Module* llvm_module,\n-                                   const HloComputation& computation,\n-                                   absl::Span<llvm::Value* const> operands,\n-                                   llvm::Value* output);\n-\n-// Emit a constant with a given number of element, given byte size of the\n-// element, given symbol name and content.\n-GpuExecutable::ConstantInfo AppendGlobalConstant(llvm::Module* module,\n-                                                 int64_t num_elements,\n-                                                 int64_t bytes_per_element,\n-                                                 absl::string_view symbol_name,\n-                                                 int allocation_idx,\n-                                                 DenseDataIntermediate content);\n-\n-}  // namespace gpu\n-}  // namespace xla\n-\n-#endif  // XLA_BACKENDS_GPU_CODEGEN_LLVM_IR_EMITTER_NESTED_H_"
        },
        {
            "sha": "2cdd6ea1a8cadad2ca75a42df200ba56500ac849",
            "filename": "third_party/xla/xla/backends/gpu/codegen/llvm/llvm_emitter.cc",
            "status": "renamed",
            "additions": 530,
            "deletions": 103,
            "changes": 633,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/19cac92f41ebf3db0b116c2ff8d5f0b05a373bde/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fllvm_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/19cac92f41ebf3db0b116c2ff8d5f0b05a373bde/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fllvm_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fllvm_emitter.cc?ref=19cac92f41ebf3db0b116c2ff8d5f0b05a373bde",
            "patch": "@@ -13,7 +13,7 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include \"xla/backends/gpu/codegen/llvm/ir_emitter.h\"\n+#include \"xla/backends/gpu/codegen/llvm/llvm_emitter.h\"\n \n #include <algorithm>\n #include <cstdint>\n@@ -24,34 +24,53 @@ limitations under the License.\n #include <vector>\n \n // IWYU pragma: no_include \"llvm/IR/Intrinsics.gen.inc\"\n+#include \"absl/algorithm/container.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/str_join.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"llvm/ADT/ArrayRef.h\"\n #include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/IR/Argument.h\"\n #include \"llvm/IR/BasicBlock.h\"\n+#include \"llvm/IR/Constants.h\"\n #include \"llvm/IR/DerivedTypes.h\"\n+#include \"llvm/IR/Function.h\"\n+#include \"llvm/IR/GlobalValue.h\"\n+#include \"llvm/IR/GlobalVariable.h\"\n #include \"llvm/IR/IRBuilder.h\"\n #include \"llvm/IR/Instructions.h\"\n+#include \"llvm/IR/LLVMContext.h\"\n #include \"llvm/IR/Module.h\"\n+#include \"llvm/Support/Casting.h\"\n+#include \"llvm/TargetParser/Triple.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n-#include \"xla/backends/gpu/codegen/llvm/ir_emitter_nested.h\"\n #include \"xla/backends/gpu/codegen/llvm/parallel_loop_emitter.h\"\n #include \"xla/backends/gpu/codegen/llvm/sort_util.h\"\n #include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk_id.h\"\n+#include \"xla/codegen/emitters/computation_fingerprint.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/literal.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/elemental_ir_emitter.h\"\n #include \"xla/service/gpu/gpu_constants.h\"\n+#include \"xla/service/gpu/gpu_executable.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/ir_emitter_context.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/llvm_ir/buffer_assignment_util.h\"\n #include \"xla/service/llvm_ir/fused_ir_emitter.h\"\n #include \"xla/service/llvm_ir/ir_array.h\"\n #include \"xla/service/llvm_ir/kernel_support_library.h\"\n@@ -66,16 +85,257 @@ limitations under the License.\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n+#include \"tsl/platform/errors.h\"\n+#include \"tsl/platform/fingerprint.h\"\n+#include \"tsl/platform/statusor.h\"\n \n-namespace xla {\n+namespace xla::gpu {\n+namespace {\n+\n+// Emits LLVM IR for a \"nested computation\" into a non-kernel device function.\n+//\n+// This is used to emit code for HloComputations that don't require a separate\n+// kernel call.  For example, IrEmitterNested is used to emit code for a kReduce\n+// HLO's elementwise reduction computation.  Notably, IrEmitterNested is *not*\n+// used to emit code for fusion nodes -- fusion nodes use FusedIrEmitter, which\n+// is a different beast altogether.\n+//\n+// IrEmitterNested generates a non-kernel function with the following\n+// parameters:\n+//\n+//   - N pointers to the buffers of each of the N parameters to the computation,\n+//   - a pointer to the output buffer of the computation, and\n+//   - a pointer to the top-level temp buffer.\n+absl::Status CallNestedComputation(llvm::IRBuilderBase* builder,\n+                                   IrEmitterContext& ir_emitter_context,\n+                                   llvm::Module* llvm_module,\n+                                   const HloComputation& computation,\n+                                   absl::Span<llvm::Value* const> operands,\n+                                   llvm::Value* output);\n+\n+// Abstract base class for translating HLO graphs to LLVM IR for a GPU.\n+//\n+// There are two concrete subclasses of IrEmitter: IrEmitterNested and\n+// IrEmitterUnnested.  In the unnested variety, each HLO gets its own kernel\n+// function, whereas in the nested version the whole computation is emitted as\n+// one *non-kernel* function.\n+//\n+// In XLA, kernel functions never call other kernel functions.  This means that\n+// if we have a kernel -- e.g. implementing a kReduce HLO -- that wants to use\n+// an HLO computation as a \"subroutine\" -- e.g. the HLO computation that\n+// specifies how to reduce two elements -- then the subroutine computation must\n+// be emitted using IrEmitterNested.\n+//\n+// Fusion nodes are a special case.  A fusion node is emitted using\n+// IrEmitterUnnested, but the code is generated using FusedIrEmitter, which is\n+// not a subclass of gpu::IrEmitter, and in fact is better understood as an IR\n+// generator generator.  See comments on that class.\n+class IrEmitter : public DfsHloVisitorWithDefault,\n+                  public IrBuilderMixin<IrEmitter> {\n+ public:\n+  IrEmitter(const IrEmitter&) = delete;\n+  IrEmitter& operator=(const IrEmitter&) = delete;\n+\n+  // Constructs an IrEmitter with the given IrEmitter context.\n+  // ir_emitter_context is owned by the caller and should outlive the IrEmitter\n+  // object.\n+  explicit IrEmitter(IrEmitterContext* ir_emitter_context, bool is_nested)\n+      : ir_emitter_context_(ir_emitter_context),\n+        module_(ir_emitter_context->llvm_module()),\n+        b_(module_->getContext()),\n+        bindings_(&b_, module_, is_nested) {}\n+\n+  absl::Status DefaultAction(HloInstruction* hlo) override;\n+\n+  absl::Status HandleConstant(HloInstruction* constant) override {\n+    return absl::OkStatus();\n+  }\n+  absl::Status HandleGetTupleElement(\n+      HloInstruction* get_tuple_element) override {\n+    auto operand = get_tuple_element->operand(0);\n+    CHECK(bindings_.BoundToIrValue(*operand));\n+    bindings_.BindHloToIrValue(\n+        *get_tuple_element,\n+        llvm_ir::EmitGetTupleElement(\n+            get_tuple_element->shape(), get_tuple_element->tuple_index(),\n+            /*alignment=*/1, GetBasePointer(*operand),\n+            llvm_ir::ShapeToIrType(operand->shape(), module_->getContext()),\n+            &b_));\n+    return absl::OkStatus();\n+  }\n+  absl::Status HandleConvolution(HloInstruction* convolution) override {\n+    if (ShapeUtil::IsZeroElementArray(convolution->shape())) {\n+      // Emit no code for an empty output.\n+      return absl::OkStatus();\n+    }\n+    return Unimplemented(\n+        \"Hit a case for convolution that is not implemented on GPU.\");\n+  }\n \n-namespace gpu {\n+  absl::Status HandleFft(HloInstruction* fft) override {\n+    if (ShapeUtil::IsZeroElementArray(fft->shape())) {\n+      // Emit no code for an empty output.\n+      return absl::OkStatus();\n+    }\n+    return Unimplemented(\"Hit a case for fft that is not implemented on GPU.\");\n+  }\n+\n+  absl::Status HandleAllReduce(HloInstruction* crs) override {\n+    return Unimplemented(\n+        \"AllReduce cannot be nested inside of fusion, map, etc.\");\n+  }\n+  absl::Status HandleInfeed(HloInstruction*) override {\n+    return Unimplemented(\"Infeed is not supported on GPU.\");\n+  }\n \n-IrEmitter::IrEmitter(IrEmitterContext* ir_emitter_context, bool is_nested)\n-    : ir_emitter_context_(ir_emitter_context),\n-      module_(ir_emitter_context->llvm_module()),\n-      b_(module_->getContext()),\n-      bindings_(&b_, module_, is_nested) {}\n+  absl::Status HandleOutfeed(HloInstruction*) override {\n+    return Unimplemented(\"Outfeed is not supported on GPU.\");\n+  }\n+  absl::Status HandleSend(HloInstruction*) override {\n+    return Unimplemented(\"Send is not implemented on GPU\");\n+  }\n+\n+  absl::Status HandleSendDone(HloInstruction*) override {\n+    return Unimplemented(\"Send-Done is not implemented on GPU\");\n+  }\n+\n+  absl::Status HandleRecv(HloInstruction*) override {\n+    return Unimplemented(\"Recv is not implemented on GPU\");\n+  }\n+\n+  absl::Status HandleRecvDone(HloInstruction*) override {\n+    return Unimplemented(\"Recv-done is not implemented on GPU\");\n+  }\n+\n+  absl::Status HandleScatter(HloInstruction*) override {\n+    return Unimplemented(\"Scatter is not implemented on GPUs.\");\n+  }\n+  absl::Status HandleParameter(HloInstruction* parameter) override {\n+    return absl::OkStatus();\n+  }\n+  absl::Status HandleTuple(HloInstruction* tuple) override {\n+    std::vector<llvm::Value*> base_ptrs;\n+    for (const HloInstruction* operand : tuple->operands()) {\n+      base_ptrs.push_back(GetBasePointer(*operand));\n+    }\n+    llvm_ir::EmitTuple(GetIrArray(*tuple, *tuple), base_ptrs, &b_);\n+    return absl::OkStatus();\n+  }\n+  absl::Status HandleCall(HloInstruction* call) override {\n+    std::vector<llvm::Value*> operand_addresses;\n+    for (HloInstruction* operand : call->operands()) {\n+      operand_addresses.push_back(GetBasePointer(*operand));\n+    }\n+    return CallNestedComputation(&b_, *ir_emitter_context_, module_,\n+                                 *call->to_apply(), operand_addresses,\n+                                 GetBasePointer(*call));\n+  }\n+\n+  absl::Status HandleCustomCall(HloInstruction*) override {\n+    return Unimplemented(\"custom-call\");\n+  }\n+\n+  absl::Status HandleBatchNormInference(HloInstruction* batch_norm) override;\n+  absl::Status HandleBatchNormTraining(HloInstruction* batch_norm) override;\n+  absl::Status HandleBatchNormGrad(HloInstruction* batch_norm) override;\n+  absl::Status HandleAddDependency(HloInstruction* add_dependency) override;\n+\n+  absl::Status FinishVisit(HloInstruction* root) override {\n+    return absl::OkStatus();\n+  }\n+\n+  llvm::IRBuilderBase* builder() { return &b_; }\n+\n+ protected:\n+  // Helper for calling HloToIrBindings::GetIrArray.\n+  //\n+  // Gets the IrArray which contains inst.  This array has metadata that makes\n+  // it valid only within the IR that implements consumer.  If you are\n+  // implementing an HLO and want to get its own output buffer, call\n+  // GetIrArray(hlo, hlo).\n+  llvm_ir::IrArray GetIrArray(const HloInstruction& inst,\n+                              const HloInstruction& consumer,\n+                              const ShapeIndex& shape_index = {}) {\n+    return bindings_.GetIrArray(inst, consumer, shape_index);\n+  }\n+  // A convenient helper for calling HloToIrBindings::GetBasePointer.\n+  llvm::Value* GetBasePointer(const HloInstruction& inst,\n+                              ShapeIndexView shape_index = {}) const {\n+    return bindings_.GetBasePointer(inst, shape_index);\n+  }\n+\n+  // Generates the IrArray for each output of an hlo instruction and returns\n+  // a vector containing such IrArrays.\n+  std::vector<llvm_ir::IrArray> ConstructIrArrayForOutputs(\n+      const HloInstruction& hlo);\n+\n+  // Emit a single-threaded or multi-threaded loop that computes every element\n+  // in the result of the given HLO instruction. This produces a series of\n+  // nested loops (e.g. one for each dimension of the `hlo`'s shape). The body\n+  // of the inner-most loop is provided by the body_emitter function.\n+  virtual absl::Status EmitTargetElementLoop(\n+      const HloInstruction& hlo, const llvm_ir::ElementGenerator& body_emitter);\n+\n+  IrEmitterContext* ir_emitter_context_;\n+  llvm::Module* module_;\n+\n+  // The following fields track the IR emission state. According to LLVM memory\n+  // management rules, their memory is owned by the module.\n+  llvm::IRBuilder<> b_;\n+\n+  // Mapping from HLO to its underlying LLVM value.\n+  HloToIrBindings bindings_;\n+\n+  // Bind all argument IrArrays of `fusion` to `fused_emitter`.\n+  void BindFusionArguments(const HloInstruction* fusion,\n+                           FusedIrEmitter* fused_emitter);\n+};\n+\n+class IrEmitterNested : public IrEmitter {\n+ public:\n+  // Constructs an LLVM IR emitter for a nested HLO computation. `function` is\n+  // the containing IR function this emitter produces IR to. See\n+  // IrEmitter::IrEmitter for the meanings of other arguments.\n+  IrEmitterNested(const HloComputation& nested_computation,\n+                  IrEmitterContext* ir_emitter_context)\n+      : IrEmitter(ir_emitter_context,\n+                  /*is_nested=*/true),\n+        nested_computation_(nested_computation) {}\n+\n+  IrEmitterNested(const IrEmitterNested&) = delete;\n+  IrEmitterNested& operator=(const IrEmitterNested&) = delete;\n+\n+  // Overrides the default empty implementation. Binds the given instruction\n+  // \"parameter\" with the parameter of the IR function.\n+  absl::Status HandleParameter(HloInstruction* parameter) override {\n+    return absl::OkStatus();\n+  }\n+\n+  // Generate the code for the computation passed in the constructor, if it\n+  // wasn't already generated previously.\n+  // As well as generting the code for the function, emits code for global\n+  // constants, and also populates related information to 'ir_emitter_context_'\n+  // for large-constant initializations. Large constants don't get initializers\n+  // in the generated code and so must be initialized by XLA. The value of these\n+  // constants will be stored in 'content'. Constants with initializers in the\n+  // generated code will have empty 'content'.\n+  //\n+  // The allocation index for these constants will always be -1 (i.e. doesn't\n+  // correspond to any allocation)\n+  absl::StatusOr<llvm::Function*> CodegenNestedComputation();\n+\n+ protected:\n+  absl::Status EmitTargetElementLoop(\n+      const HloInstruction& hlo,\n+      const llvm_ir::ElementGenerator& element_generator) override;\n+\n+ private:\n+  // Emits constants to generated LLVM IR, and also populates related\n+  // information to 'ir_emitter_context_' for large-constant initializations.\n+  absl::Status EmitConstants(const HloComputation& computation);\n+\n+  const HloComputation& nested_computation_;\n+};\n \n absl::Status IrEmitter::DefaultAction(HloInstruction* hlo) {\n   ElementalIrEmitter::HloToElementGeneratorMap operand_to_generator;\n@@ -90,10 +350,6 @@ absl::Status IrEmitter::DefaultAction(HloInstruction* hlo) {\n                 .MakeElementGenerator(hlo, operand_to_generator));\n }\n \n-absl::Status IrEmitter::HandleConstant(HloInstruction* constant) {\n-  return absl::OkStatus();\n-}\n-\n absl::Status IrEmitter::HandleAddDependency(HloInstruction* add_dependency) {\n   VLOG(2) << \"HandleAddDependency: \" << add_dependency->ToString();\n   const HloInstruction* operand = add_dependency->operand(0);\n@@ -106,100 +362,48 @@ absl::Status IrEmitter::HandleAddDependency(HloInstruction* add_dependency) {\n   return absl::OkStatus();\n }\n \n-absl::Status IrEmitter::HandleGetTupleElement(\n-    HloInstruction* get_tuple_element) {\n-  auto operand = get_tuple_element->operand(0);\n-  CHECK(bindings_.BoundToIrValue(*operand));\n-  bindings_.BindHloToIrValue(\n-      *get_tuple_element,\n-      llvm_ir::EmitGetTupleElement(\n-          get_tuple_element->shape(), get_tuple_element->tuple_index(),\n-          // TODO(b/26344050): tighten the alignment here\n-          // based on the real element type.\n-          /*alignment=*/1, GetBasePointer(*operand),\n-          llvm_ir::ShapeToIrType(operand->shape(), module_->getContext()),\n-          &b_));\n-  return absl::OkStatus();\n-}\n-\n-absl::Status IrEmitter::HandleSend(HloInstruction*) {\n-  return Unimplemented(\"Send is not implemented on GPU\");\n-}\n-\n-absl::Status IrEmitter::HandleSendDone(HloInstruction*) {\n-  return Unimplemented(\"Send-Done is not implemented on GPU\");\n-}\n-\n-absl::Status IrEmitter::HandleRecv(HloInstruction*) {\n-  return Unimplemented(\"Recv is not implemented on GPU\");\n-}\n-\n-absl::Status IrEmitter::HandleRecvDone(HloInstruction*) {\n-  return Unimplemented(\"Recv-done is not implemented on GPU\");\n-}\n-\n-absl::Status IrEmitter::HandleScatter(HloInstruction*) {\n-  return Unimplemented(\"Scatter is not implemented on GPUs.\");\n-}\n-\n-absl::Status IrEmitter::HandleTuple(HloInstruction* tuple) {\n-  std::vector<llvm::Value*> base_ptrs;\n-  for (const HloInstruction* operand : tuple->operands()) {\n-    base_ptrs.push_back(GetBasePointer(*operand));\n-  }\n-  llvm_ir::EmitTuple(GetIrArray(*tuple, *tuple), base_ptrs, &b_);\n-  return absl::OkStatus();\n-}\n-\n-absl::Status IrEmitter::HandleConvolution(HloInstruction* convolution) {\n-  if (ShapeUtil::IsZeroElementArray(convolution->shape())) {\n-    // Emit no code for an empty output.\n-    return absl::OkStatus();\n+// Casts the provided llvm::Value* to the default address space. This is useful\n+// in particular for generating IR for AMDGPU target, as its kernel variables\n+// are in address space 5 instead of the default address space.\n+llvm::Value* AddrCastToDefault(llvm::Value* arg, llvm::IRBuilderBase& b) {\n+  llvm::Type* arg_type = arg->getType();\n+  CHECK(arg_type->isPointerTy());\n+  if (arg_type->getPointerAddressSpace() != 0) {\n+    llvm::Type* generic_arg_type = llvm::PointerType::get(\n+        llvm::cast<llvm::PointerType>(arg_type)->getContext(), 0);\n+    llvm::Value* addrspacecast_arg =\n+        b.CreateAddrSpaceCast(arg, generic_arg_type);\n+    return addrspacecast_arg;\n   }\n-  // TODO(b/31409998): Support convolution with dilation.\n-  return Unimplemented(\n-      \"Hit a case for convolution that is not implemented on GPU.\");\n+  return arg;\n }\n \n-absl::Status IrEmitter::HandleFft(HloInstruction* fft) {\n-  if (ShapeUtil::IsZeroElementArray(fft->shape())) {\n-    // Emit no code for an empty output.\n-    return absl::OkStatus();\n-  }\n-  return Unimplemented(\"Hit a case for fft that is not implemented on GPU.\");\n-}\n+absl::Status CallNestedComputation(llvm::IRBuilderBase* builder,\n+                                   IrEmitterContext& ir_emitter_context,\n+                                   llvm::Module* llvm_module,\n+                                   const HloComputation& computation,\n+                                   absl::Span<llvm::Value* const> operands,\n+                                   llvm::Value* output) {\n+  TF_RET_CHECK(computation.num_parameters() > 0);\n \n-absl::Status IrEmitter::HandleAllReduce(HloInstruction* crs) {\n-  return Unimplemented(\n-      \"AllReduce cannot be nested inside of fusion, map, etc.\");\n-}\n+  TF_ASSIGN_OR_RETURN(llvm::Function * emitted_function,\n+                      IrEmitterNested(computation, &ir_emitter_context)\n+                          .CodegenNestedComputation());\n \n-absl::Status IrEmitter::HandleParameter(HloInstruction* parameter) {\n-  return absl::OkStatus();\n-}\n+  // Operands are in default address space for non-AMDGPU target.\n+  // However for AMDGPU target, addrspacecast alloca variables from\n+  // addrspace 5 to addrspace 0 is needed.\n+  std::vector<llvm::Value*> arguments;\n+  absl::c_transform(\n+      operands, std::back_inserter(arguments),\n+      [builder](llvm::Value* arg) { return AddrCastToDefault(arg, *builder); });\n \n-absl::Status IrEmitter::HandleCall(HloInstruction* call) {\n-  std::vector<llvm::Value*> operand_addresses;\n-  for (HloInstruction* operand : call->operands()) {\n-    operand_addresses.push_back(GetBasePointer(*operand));\n-  }\n-  return CallNestedComputation(&b_, *ir_emitter_context_, module_,\n-                               *call->to_apply(), operand_addresses,\n-                               GetBasePointer(*call));\n-}\n+  llvm::Value* casted_output = AddrCastToDefault(output, *builder);\n+  arguments.push_back(casted_output);\n \n-absl::Status IrEmitter::HandleCustomCall(HloInstruction*) {\n-  return Unimplemented(\"custom-call\");\n-}\n-\n-absl::Status IrEmitter::HandleInfeed(HloInstruction*) {\n-  // TODO(b/30467474): Implement infeed on GPU.\n-  return Unimplemented(\"Infeed is not supported on GPU.\");\n-}\n+  builder->CreateCall(emitted_function, arguments);\n \n-absl::Status IrEmitter::HandleOutfeed(HloInstruction*) {\n-  // TODO(b/34359662): Implement outfeed on GPU.\n-  return Unimplemented(\"Outfeed is not supported on GPU.\");\n+  return absl::OkStatus();\n }\n \n absl::Status IrEmitter::HandleBatchNormInference(HloInstruction*) {\n@@ -255,7 +459,170 @@ void IrEmitter::BindFusionArguments(const HloInstruction* fusion,\n   }\n }\n \n-namespace {\n+// Nested function serves the same purpose on GPU as a thread-local function on\n+// a CPU.\n+absl::StatusOr<llvm::Function*> IrEmitterNested::CodegenNestedComputation() {\n+  // Include a fingerprint of the HLO in the function name to make the name\n+  // unique.\n+  tsl::Fprint128 fingerprint = tsl::Fingerprint128(\n+      emitters::GetComputationFingerprint(&nested_computation_, {}));\n+  std::string function_name = llvm_ir::SanitizeFunctionName(absl::StrCat(\n+      nested_computation_.name(), \"_\", fingerprint.low64, fingerprint.high64));\n+\n+  auto* function = module_->getFunction(function_name);\n+  if (function) return function;\n+\n+  TF_RETURN_IF_ERROR(EmitConstants(nested_computation_));\n+  std::vector<const HloInstruction*> io_hlos;\n+  std::vector<llvm::Type*> argument_types;\n+  std::vector<int64_t> argument_dereferenceable_bytes;\n+  const auto& params = nested_computation_.parameter_instructions();\n+  const auto n = params.size() + 1;\n+  io_hlos.reserve(n - 1);\n+  argument_types.reserve(n);\n+  argument_dereferenceable_bytes.reserve(n);\n+  for (const HloInstruction* param : params) {\n+    io_hlos.push_back(param);\n+    const Shape& param_shape = param->shape();\n+    argument_types.push_back(b_.getPtrTy());\n+    int64_t param_size =\n+        llvm_ir::ByteSizeOf(param_shape, module_->getDataLayout());\n+    argument_dereferenceable_bytes.push_back(param_size);\n+  }\n+\n+  const HloInstruction* root = nested_computation_.root_instruction();\n+  {\n+    const Shape& root_shape = root->shape();\n+    argument_types.push_back(b_.getPtrTy());\n+    int64_t root_size =\n+        llvm_ir::ByteSizeOf(root_shape, module_->getDataLayout());\n+    argument_dereferenceable_bytes.push_back(root_size);\n+  }\n+\n+  llvm::FunctionType* function_type =\n+      llvm::FunctionType::get(b_.getVoidTy(), argument_types, false);\n+  function = llvm::Function::Create(\n+      function_type,                       // The function type.\n+      llvm::GlobalValue::InternalLinkage,  // The linkage type.\n+      function_name,\n+      module_);  // The parent LLVM module.\n+  for (size_t arg_no = 0; arg_no < argument_dereferenceable_bytes.size();\n+       ++arg_no) {\n+    int64_t arg_size = argument_dereferenceable_bytes[arg_no];\n+    if (arg_size > 0) {\n+      function->addDereferenceableParamAttr(arg_no, arg_size);\n+    }\n+  }\n+\n+  llvm::BasicBlock* entry_bb =\n+      llvm::BasicBlock::Create(function->getContext(), \"entry\", function);\n+  // Emit a \"return void\" at entry_bb's end, and sets the insert point before\n+  // that return instruction.\n+  llvm::ReturnInst* ret_instr =\n+      llvm::ReturnInst::Create(function->getContext(), entry_bb);\n+  b_.SetInsertPoint(ret_instr);\n+\n+  std::vector<const HloInstruction*> non_io_hlos;\n+  non_io_hlos.push_back(root);\n+  for (const auto* hlo : nested_computation_.instructions()) {\n+    if (hlo->opcode() != HloOpcode::kParameter &&\n+        hlo != nested_computation_.root_instruction()) {\n+      non_io_hlos.push_back(hlo);\n+    }\n+  }\n+  bindings_.EmitBasePointersForHlos(io_hlos, non_io_hlos);\n+\n+  TF_RETURN_IF_ERROR(nested_computation_.root_instruction()->Accept(this));\n+  b_.SetInsertPoint(ret_instr);\n+\n+  // Function epilogue: copy the output value back.\n+  {\n+    const HloInstruction* root_instruction =\n+        nested_computation_.root_instruction();\n+    llvm::Value* root_value = bindings_.GetBasePointer(*root_instruction);\n+    const Shape& return_shape = root_instruction->shape();\n+\n+    // Last argument is the out parameter.\n+    llvm::Argument* out_parameter = std::prev(function->arg_end(), 1);\n+\n+    if (ShapeUtil::IsScalar(return_shape)) {\n+      llvm::Value* ret_value =\n+          Load(llvm_ir::ShapeToIrType(return_shape, module_->getContext()),\n+               root_value, \"load_ret_value\");\n+      Store(ret_value, out_parameter);\n+    } else {\n+      CHECK(return_shape.IsTuple());\n+      llvm::Type* tuple_type =\n+          llvm_ir::ShapeToIrType(return_shape, module_->getContext());\n+\n+      for (int i = 0; i < return_shape.tuple_shapes().size(); i++) {\n+        const Shape& element_shape = return_shape.tuple_shapes(i);\n+        llvm::Value* destination = llvm_ir::EmitGetTupleElement(\n+            element_shape,\n+            /*index=*/i,\n+            /*alignment=*/1, out_parameter, tuple_type, &b_);\n+        llvm::Value* source = llvm_ir::EmitGetTupleElement(\n+            element_shape,\n+            /*index=*/i,\n+            /*alignment=*/1, root_value,\n+            llvm_ir::ShapeToIrType(root_instruction->shape(),\n+                                   module_->getContext()),\n+            &b_);\n+        Store(Load(llvm_ir::ShapeToIrType(element_shape, module_->getContext()),\n+                   source),\n+              destination);\n+      }\n+    }\n+  }\n+  b_.SetInsertPoint(ret_instr);\n+  return function;\n+}\n+\n+absl::Status IrEmitterNested::EmitTargetElementLoop(\n+    const HloInstruction& hlo,\n+    const llvm_ir::ElementGenerator& element_generator) {\n+  // For MOF we give the loop emitter an array for every output it should\n+  // generate.\n+  if (hlo.shape().IsTuple()) {\n+    std::vector<llvm_ir::IrArray> target_arrays =\n+        ConstructIrArrayForOutputs(hlo);\n+    TF_RETURN_IF_ERROR(\n+        llvm_ir::LoopEmitter(element_generator, target_arrays, &b_).EmitLoop());\n+    llvm_ir::EmitTuple(GetIrArray(hlo, hlo), target_arrays, &b_);\n+    return absl::OkStatus();\n+  }\n+  return llvm_ir::LoopEmitter(element_generator, GetIrArray(hlo, hlo), &b_)\n+      .EmitLoop();\n+}\n+\n+absl::Status IrEmitterNested::EmitConstants(const HloComputation& computation) {\n+  for (HloInstruction* instr : computation.instructions()) {\n+    if (instr->opcode() != HloOpcode::kConstant) {\n+      continue;\n+    }\n+    const Literal& literal = instr->literal();\n+\n+    // These globals will be looked up by name by GpuExecutable so we need to\n+    // give them an external linkage.  Not all of their uses are visible in\n+    // the LLVM IR (e.g. TupleThunk) so we can't give then a linkage that\n+    // merely preserves their names (like available_externally), we also need\n+    // to ensure that they stick around even if they're \"unused\".\n+    //\n+    // We may have to be more clever here in the future if we notice that we're\n+    // keeping around too many globals because of their linkage.\n+    std::string global_name = llvm_ir::ConstantHloToGlobalName(*instr);\n+\n+    auto base = static_cast<const uint8_t*>(literal.untyped_data());\n+    GpuExecutable::ConstantInfo info = AppendGlobalConstant(\n+        module_, literal.element_count(),\n+        ShapeUtil::ByteSizeOfPrimitiveType(literal.shape().element_type()),\n+        global_name, /*allocation_idx=*/-1,\n+        DenseDataIntermediate::Alias(\n+            absl::MakeSpan(base, base + literal.size_bytes())));\n+    ir_emitter_context_->constants().push_back(std::move(info));\n+  }\n+  return absl::OkStatus();\n+}\n \n struct KernelThunkInfo {\n   std::vector<llvm_ir::IrArray> ir_arrays;\n@@ -361,6 +728,68 @@ void CreateStore(llvm::Value* data, llvm::Value* address, int alignment_bytes,\n \n }  // namespace\n \n+GpuExecutable::ConstantInfo AppendGlobalConstant(\n+    llvm::Module* module, int64_t num_elements, int64_t bytes_per_element,\n+    absl::string_view symbol_name, int allocation_idx,\n+    DenseDataIntermediate content) {\n+  // LLVM and PTXAS don't deal well with large constants, so we only emit very\n+  // small constants directly in LLVM IR.  Larger constants are emitted with\n+  // zero initializers in LLVM IR and are later overwritten when the PTX/CUBIN\n+  // is loaded.\n+  bool should_emit_initializer = num_elements <= 1;\n+\n+  llvm::IRBuilder<> b(module->getContext());\n+  // Ptxas has issues if the constant allocation is smaller than 64 bytes.\n+  // TODO(b/253259975): Remove when fixed ptxas version is submitted.\n+  constexpr int64_t kMinConstAllocationInBytes = 64;\n+  bool needs_padding =\n+      num_elements * bytes_per_element < kMinConstAllocationInBytes;\n+\n+  llvm::ArrayType* global_type = llvm::ArrayType::get(\n+      b.getInt8Ty(),\n+      std::max(num_elements * bytes_per_element, kMinConstAllocationInBytes));\n+\n+  GpuExecutable::ConstantInfo info;\n+  llvm::Constant* initializer = [&]() -> llvm::Constant* {\n+    if (!should_emit_initializer) {\n+      info.content = std::move(content);\n+      return llvm::ConstantAggregateZero::get(global_type);\n+    }\n+\n+    std::vector<uint8_t> padded(kMinConstAllocationInBytes, 0);\n+    absl::c_copy(content.span(), padded.begin());\n+    return llvm::ConstantDataArray::get<uint8_t>(\n+        module->getContext(),\n+        needs_padding ? llvm::ArrayRef<uint8_t>(padded)\n+                      : llvm::ArrayRef<uint8_t>(content.span().data(),\n+                                                content.span().size()));\n+  }();\n+\n+  // Explicitly set global addrspace for SPIR backend.\n+  int addrspace = llvm::Triple(module->getTargetTriple()).isSPIR() ? 1 : 0;\n+  // These globals will be looked up by name by GpuExecutable so we need to\n+  // give them an external linkage.  Not all of their uses are visible in\n+  // the LLVM IR so we can't give then a linkage that merely preserves their\n+  // names (like available_externally), we also need to ensure that they stick\n+  // around even if they're \"unused\".\n+  //\n+  // We may have to be more clever here in the future if we notice that we're\n+  // keeping around too many globals because of their linkage.\n+  auto* global_for_const = new llvm::GlobalVariable(\n+      global_type, /*isConstant=*/should_emit_initializer,\n+      llvm::GlobalValue::ExternalLinkage,\n+      /*Initializer=*/initializer, symbol_name,\n+      /*TLMode=*/llvm::GlobalValue::NotThreadLocal,\n+      /*AddressSpace=*/addrspace,\n+      /*isExternallyInitialized=*/false);\n+  global_for_const->setAlignment(llvm::Align(kConstantBufferAlignBytes));\n+  module->insertGlobalVariable(global_for_const);\n+\n+  info.symbol_name.assign(symbol_name);\n+  info.allocation_index = allocation_idx;\n+  return info;\n+}\n+\n absl::StatusOr<ThunkSequence> EmitBitonicSortLLVMIR(\n     const HloSortInstruction* sort, llvm::Module* llvm_module,\n     IrEmitterContext* ir_emitter_context) {\n@@ -719,7 +1148,6 @@ absl::StatusOr<ThunkSequence> EmitSliceToDynamicLLVMIR(\n       ir_emitter_context->emit_kernels());\n \n   IrEmitter ir_emitter(&local_ir_emitter_context, /*nested=*/false);\n-  // TODO(jurahul): Create an op to represent SliceToDynamic.\n   constexpr int kUnrollFactor = 1;\n   const Shape& input_shape = hlo->operand(0)->shape();\n \n@@ -892,5 +1320,4 @@ absl::StatusOr<ThunkSequence> EmitRngGetAndUpdateStateLLVMIR(\n   return thunk_sequence;\n }\n \n-}  // namespace gpu\n-}  // namespace xla\n+}  // namespace xla::gpu",
            "previous_filename": "third_party/xla/xla/backends/gpu/codegen/llvm/ir_emitter.cc"
        },
        {
            "sha": "e7debbcec3ffc40183fbc45ef6a2fde303413585",
            "filename": "third_party/xla/xla/backends/gpu/codegen/llvm/llvm_emitter.h",
            "status": "added",
            "additions": 67,
            "deletions": 0,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/19cac92f41ebf3db0b116c2ff8d5f0b05a373bde/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fllvm_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/19cac92f41ebf3db0b116c2ff8d5f0b05a373bde/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fllvm_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fllvm_emitter.h?ref=19cac92f41ebf3db0b116c2ff8d5f0b05a373bde",
            "patch": "@@ -0,0 +1,67 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_CODEGEN_LLVM_LLVM_EMITTER_H_\n+#define XLA_BACKENDS_GPU_CODEGEN_LLVM_LLVM_EMITTER_H_\n+\n+#include <vector>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"llvm/IR/Function.h\"\n+#include \"llvm/IR/IRBuilder.h\"\n+#include \"llvm/IR/Value.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/gpu/hlo_to_ir_bindings.h\"\n+#include \"xla/service/gpu/ir_emitter_context.h\"\n+#include \"xla/service/llvm_ir/fused_ir_emitter.h\"\n+#include \"xla/service/llvm_ir/ir_array.h\"\n+#include \"xla/service/llvm_ir/ir_builder_mixin.h\"\n+#include \"xla/service/llvm_ir/loop_emitter.h\"\n+#include \"xla/shape_util.h\"\n+\n+namespace xla::gpu {\n+\n+// Emit a constant with a given number of element, given byte size of the\n+// element, given symbol name and content.\n+GpuExecutable::ConstantInfo AppendGlobalConstant(llvm::Module* module,\n+                                                 int64_t num_elements,\n+                                                 int64_t bytes_per_element,\n+                                                 absl::string_view symbol_name,\n+                                                 int allocation_idx,\n+                                                 DenseDataIntermediate content);\n+\n+absl::StatusOr<ThunkSequence> EmitBitonicSortLLVMIR(\n+    const HloSortInstruction* sort, llvm::Module* llvm_module,\n+    IrEmitterContext* ir_emitter_context);\n+\n+absl::StatusOr<ThunkSequence> EmitPadToStaticLLVMIR(\n+    const HloCustomCallInstruction* hlo, llvm::Module* llvm_module,\n+    IrEmitterContext* ir_emitter_context);\n+\n+absl::StatusOr<ThunkSequence> EmitSliceToDynamicLLVMIR(\n+    const HloCustomCallInstruction* hlo, llvm::Module* llvm_module,\n+    IrEmitterContext* ir_emitter_context);\n+\n+absl::StatusOr<ThunkSequence> EmitRngGetAndUpdateStateLLVMIR(\n+    const HloRngGetAndUpdateStateInstruction* hlo, llvm::Module* llvm_module,\n+    IrEmitterContext* ir_emitter_context);\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_BACKENDS_GPU_CODEGEN_LLVM_LLVM_EMITTER_H_"
        },
        {
            "sha": "b9229262a403eb7b02cb0c69b4845a0089826d69",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/19cac92f41ebf3db0b116c2ff8d5f0b05a373bde/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/19cac92f41ebf3db0b116c2ff8d5f0b05a373bde/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=19cac92f41ebf3db0b116c2ff8d5f0b05a373bde",
            "patch": "@@ -475,7 +475,7 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/gpu/codegen:fusion_emitter\",\n         \"//xla/backends/gpu/codegen:fusions\",\n-        \"//xla/backends/gpu/codegen/llvm:ir_emitter\",\n+        \"//xla/backends/gpu/codegen/llvm:llvm_emitter\",\n         \"//xla/backends/gpu/codegen/triton:fusion_emitter\",\n         \"//xla/backends/gpu/codegen/triton:xtile_compiler\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\","
        },
        {
            "sha": "a78f49828a97ef06b4c765fbb71c8b62532d7eb0",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/19cac92f41ebf3db0b116c2ff8d5f0b05a373bde/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/19cac92f41ebf3db0b116c2ff8d5f0b05a373bde/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc?ref=19cac92f41ebf3db0b116c2ff8d5f0b05a373bde",
            "patch": "@@ -62,8 +62,7 @@ limitations under the License.\n #include \"mlir/Target/LLVMIR/Export.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n #include \"xla/backends/gpu/codegen/fusions.h\"\n-#include \"xla/backends/gpu/codegen/llvm/ir_emitter.h\"\n-#include \"xla/backends/gpu/codegen/llvm/ir_emitter_nested.h\"\n+#include \"xla/backends/gpu/codegen/llvm/llvm_emitter.h\"\n #include \"xla/backends/gpu/codegen/triton/xtile_compiler.h\"\n #include \"xla/backends/gpu/runtime/all_gather_thunk.h\"\n #include \"xla/backends/gpu/runtime/all_reduce_thunk.h\""
        }
    ],
    "stats": {
        "total": 1351,
        "additions": 602,
        "deletions": 749
    }
}