{
    "author": "Moerafaat",
    "message": "Extract launch information from the Triton compilation pipeline and use it instead of XLA's calculation. This is necessary in cases where the pipeline overrides the expected launch configuration.\nThis was observed when auto warp specialization was enabled. Triton requires more threads per block than expected, and this information is available in the module attributes.\n\nPiperOrigin-RevId: 819893926",
    "sha": "f147bddb8dd70afe003dd6f41c77830f8369f4b1",
    "files": [
        {
            "sha": "44364258cf9d956e18628876fc0a202ead3c250e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=f147bddb8dd70afe003dd6f41c77830f8369f4b1",
            "patch": "@@ -54,9 +54,10 @@ cc_library(\n         \"//xla/service/gpu:launch_dimensions\",\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/service/gpu:triton_fusion_analysis\",\n-        \"//xla/service/llvm_ir:ir_array\",\n+        \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor:launch_dim\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n@@ -86,6 +87,7 @@ xla_cc_test(\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor:launch_dim\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_googletest//:gtest\",\n@@ -103,8 +105,10 @@ cc_library(\n     ],\n     deps = [\n         \":tma_utils\",\n+        \"//xla:comparison_util\",\n         \"//xla:literal\",\n         \"//xla:shape_util\",\n+        \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n@@ -116,21 +120,24 @@ cc_library(\n         \"//xla/service/gpu:target_util\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n+        \"//xla/stream_executor/rocm:rocm_compute_capability\",\n         \"//xla/tsl/platform:status\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//llvm:TargetParser\",\n+        \"@llvm-project//llvm:ir_headers\",\n         \"@llvm-project//mlir:ArithDialect\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:LLVMDialect\",\n         \"@llvm-project//mlir:MathDialect\",\n         \"@llvm-project//mlir:Support\",\n-        \"@local_tsl//tsl/platform:statusor\",\n         \"@triton//:TritonDialects\",\n     ],\n )\n@@ -141,6 +148,7 @@ xla_cc_test(\n     deps = [\n         \":emitter_helpers\",\n         \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n+        \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/log:check\","
        },
        {
            "sha": "658272f27bbd9f21aa87eb16164c666cc73513e7",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 81,
            "deletions": 14,
            "changes": 95,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=f147bddb8dd70afe003dd6f41c77830f8369f4b1",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n \n #include <cstdint>\n #include <variant>\n+#include <vector>\n \n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n@@ -26,12 +27,16 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/IR/Metadata.h\"\n+#include \"llvm/IR/Module.h\"\n+#include \"llvm/Support/Casting.h\"\n #include \"llvm/Support/MathExtras.h\"\n #include \"llvm/TargetParser/Triple.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Dialect/Math/IR/Math.h\"\n #include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n@@ -43,6 +48,7 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/backends/gpu/codegen/triton/tma_utils.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n+#include \"xla/comparison_util.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/mlir_hlo/mhlo/IR/hlo_ops.h\"\n@@ -53,9 +59,12 @@ limitations under the License.\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n+#include \"xla/stream_executor/launch_dim.h\"\n+#include \"xla/stream_executor/rocm/rocm_compute_capability.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n-#include \"tsl/platform/statusor.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n \n namespace xla::gpu::triton {\n@@ -526,23 +535,81 @@ Value Bitcast(EmitterLocOpBuilder& b, Value value, Type type) {\n   return b.create<mlir::arith::BitcastOp>(value_type, value);\n }\n \n-absl::StatusOr<stream_executor::gpu::TmaMetadata> ExtractTmaMetadata(\n-    mlir::ModuleOp triton_module, absl::string_view kernel_name) {\n-  stream_executor::gpu::TmaMetadata tma_metadata;\n-  SmallVector<mlir::LLVM::LLVMFuncOp> func_ops;\n-  for (auto func : triton_module.getOps<mlir::LLVM::LLVMFuncOp>()) {\n-    // Custom calls will also match to LLVMFuncOp, so we are only interested in\n-    // the entry function.\n-    if (func.getName().str() == kernel_name) {\n-      func_ops.push_back(func);\n+std::vector<llvm::Metadata*> ExtractNvvmAnnotations(\n+    llvm::Module* ll_triton_module) {\n+  std::vector<llvm::Metadata*> captured_nvvm_annotations;\n+  llvm::NamedMDNode* nvvm_annotations =\n+      ll_triton_module->getNamedMetadata(\"nvvm.annotations\");\n+  if (nvvm_annotations) {\n+    for (llvm::MDNode* operand : nvvm_annotations->operands()) {\n+      captured_nvvm_annotations.push_back(operand);\n     }\n+    ll_triton_module->eraseNamedMetadata(nvvm_annotations);\n   }\n-  CHECK_EQ(func_ops.size(), 1)\n-      << \"Expected a single LLVMFuncOp in the module for the entry function.\";\n+  return captured_nvvm_annotations;\n+}\n \n-  for (auto [idx, arg] : llvm::enumerate(func_ops[0].getArguments())) {\n+absl::StatusOr<stream_executor::ThreadDim> ExtractThreadDims(\n+    mlir::ModuleOp triton_module, mlir::LLVM::LLVMFuncOp func_op) {\n+  // Extract the launch information from the Triton module.\n+  auto threads_per_warp_attr =\n+      triton_module->getAttrOfType<mlir::IntegerAttr>(\"ttg.threads-per-warp\");\n+  if (!threads_per_warp_attr) {\n+    return absl::InternalError(\"ttg.threads-per-warp attribute not found.\");\n+  }\n+  auto num_warps_attr =\n+      triton_module->getAttrOfType<mlir::IntegerAttr>(\"ttg.num-warps\");\n+  if (!num_warps_attr) {\n+    return absl::InternalError(\"ttg.num-warps attribute not found.\");\n+  }\n+  auto total_num_warps_attr =\n+      triton_module->getAttrOfType<mlir::IntegerAttr>(\"ttg.total-num-warps\");\n+  if (!total_num_warps_attr) {\n+    return absl::InternalError(\"ttg.total-num-warps attribute not found.\");\n+  }\n+  auto reqntid_attr =\n+      func_op->getAttrOfType<mlir::DenseI32ArrayAttr>(\"nvvm.reqntid\");\n+  if (!reqntid_attr) {\n+    return absl::InternalError(\"nvvm.reqntid attribute not found.\");\n+  }\n+  auto reqntids = reqntid_attr.asArrayRef();\n+  if (reqntids.empty()) {\n+    return absl::InternalError(\"nvvm.reqntid attribute is empty.\");\n+  }\n+  if (reqntids.size() > 3) {\n+    return absl::InternalError(\n+        \"nvvm.reqntid attribute has more than 3 dimensions.\");\n+  }\n+\n+  // Validate the launch information.\n+  if (num_warps_attr.getInt() != total_num_warps_attr.getInt()) {\n+    VLOG(6)\n+        << \"num_warps and total_num_warps are different! This can happen if \"\n+           \"Triton compilation decides to use a different number of warps than \"\n+           \"configured. e.g. auto warp specialization can do that.\";\n+  }\n+  int64_t expected_total_threads = xla::Product<int32_t>(reqntids);\n+  int64_t actual_total_threads =\n+      total_num_warps_attr.getInt() * threads_per_warp_attr.getInt();\n+  if (actual_total_threads != expected_total_threads) {\n+    return absl::InternalError(absl::StrCat(\n+        \"Expected total threads as per reqntid attribute to be \",\n+        expected_total_threads, \" but got \", actual_total_threads,\n+        \" as per ttg.total-num-warps and tt.threads-per-warp attributes.\"));\n+  }\n+\n+  stream_executor::ThreadDim thread_dims(reqntids[0],\n+                                         reqntids.size() > 1 ? reqntids[1] : 1,\n+                                         reqntids.size() > 2 ? reqntids[2] : 1);\n+  return thread_dims;\n+}\n+\n+absl::StatusOr<stream_executor::gpu::TmaMetadata> ExtractTmaMetadata(\n+    mlir::LLVM::LLVMFuncOp func_op) {\n+  stream_executor::gpu::TmaMetadata tma_metadata;\n+  for (auto [idx, arg] : llvm::enumerate(func_op.getArguments())) {\n     if (auto attr =\n-            func_ops[0].getArgAttrOfType<mlir::triton::xla::TmaDescriptorAttr>(\n+            func_op.getArgAttrOfType<mlir::triton::xla::TmaDescriptorAttr>(\n                 idx, \"tt.tma_descriptor\")) {\n       TF_ASSIGN_OR_RETURN(\n           auto tma_desc,"
        },
        {
            "sha": "80fa8c5021fc38643eb1e25e37c982d7b670a13e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.h",
            "status": "modified",
            "additions": 15,
            "deletions": 2,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h?ref=f147bddb8dd70afe003dd6f41c77830f8369f4b1",
            "patch": "@@ -18,17 +18,21 @@ limitations under the License.\n \n #include <cstdint>\n #include <string>\n+#include <vector>\n \n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/IR/Metadata.h\"\n+#include \"llvm/IR/Module.h\"\n #include \"llvm/Support/raw_ostream.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/Types.h\"\n #include \"mlir/IR/Value.h\"\n@@ -42,6 +46,7 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n+#include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/tsl/platform/status.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -215,10 +220,18 @@ absl::StatusOr<mlir::Value> EmitElementwise(\n \n mlir::Value Bitcast(EmitterLocOpBuilder& b, mlir::Value value, mlir::Type type);\n \n+// Extracts NVVM annotations from the Triton module.\n+std::vector<llvm::Metadata*> ExtractNvvmAnnotations(\n+    llvm::Module* ll_triton_module);\n+\n // Extracts TMA metadata information from LLVM generated by the Triton\n-// compilation. This will be empty if TMA is not used.\n+// compilation. The underlying map will be empty if TMA is not used.\n absl::StatusOr<stream_executor::gpu::TmaMetadata> ExtractTmaMetadata(\n-    mlir::ModuleOp triton_module, absl::string_view kernel_name);\n+    mlir::LLVM::LLVMFuncOp func_op);\n+\n+// Extracts thread dimensions from Triton module attributes.\n+absl::StatusOr<stream_executor::ThreadDim> ExtractThreadDims(\n+    mlir::ModuleOp triton_module, mlir::LLVM::LLVMFuncOp func_op);\n \n }  // namespace xla::gpu::triton\n "
        },
        {
            "sha": "88a6548a88dcd4fffa35d22d67ee1375c9b7f6ba",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers_test.cc",
            "status": "modified",
            "additions": 41,
            "deletions": 10,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers_test.cc?ref=f147bddb8dd70afe003dd6f41c77830f8369f4b1",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"mlir/Parser/Parser.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n+#include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n@@ -36,7 +37,26 @@ namespace xgt = ::xla::gpu::triton;\n namespace xla::gpu {\n namespace {\n \n-TEST(EmitterHelpersTest, ExtractTmaMetadataWorksCorrectlyWhenTmaIsUsed) {\n+class EmitterHelpersTest : public ::testing::Test {\n+ protected:\n+  void SetUp() override {\n+    context_.loadDialect<\n+        mlir::triton::TritonDialect, mlir::triton::gpu::TritonGPUDialect,\n+        mlir::triton::xla::XlaTritonDialect, mlir::LLVM::LLVMDialect>();\n+  }\n+\n+  mlir::OwningOpRef<mlir::ModuleOp> ParseModule(\n+      const std::string& mlir_module) {\n+    mlir::OwningOpRef<mlir::ModuleOp> module =\n+        mlir::parseSourceString<mlir::ModuleOp>(mlir_module, &context_);\n+    CHECK(module);\n+    return module;\n+  }\n+\n+  mlir::MLIRContext context_;\n+};\n+\n+TEST_F(EmitterHelpersTest, ExtractTmaMetadataWorksCorrectlyWhenTmaIsUsed) {\n   const std::string kMlirModule = R\"(\n module {\n   llvm.func @fusion_impl(%arg0: !llvm.ptr<1> {tt.divisibility = 16 : i32},\n@@ -48,16 +68,11 @@ module {\n   }\n })\";\n \n-  mlir::MLIRContext context;\n-  context.loadDialect<\n-      mlir::triton::TritonDialect, mlir::triton::gpu::TritonGPUDialect,\n-      mlir::triton::xla::XlaTritonDialect, mlir::LLVM::LLVMDialect>();\n-  mlir::OwningOpRef<mlir::ModuleOp> module =\n-      mlir::parseSourceString<mlir::ModuleOp>(kMlirModule, &context);\n-  CHECK(module);\n-\n+  mlir::OwningOpRef<mlir::ModuleOp> module = ParseModule(kMlirModule);\n+  mlir::LLVM::LLVMFuncOp func_op =\n+      *module->getOps<mlir::LLVM::LLVMFuncOp>().begin();\n   TF_ASSERT_OK_AND_ASSIGN(stream_executor::gpu::TmaMetadata tma_metadata,\n-                          xgt::ExtractTmaMetadata(module.get(), \"fusion_impl\"));\n+                          xgt::ExtractTmaMetadata(func_op));\n \n   EXPECT_EQ(tma_metadata.arg_index_to_tma_info.size(), 2);\n   EXPECT_TRUE(tma_metadata.arg_index_to_tma_info.contains(1));\n@@ -80,5 +95,21 @@ module {\n   EXPECT_EQ(tma_arg_2.swizzle(),\n             stream_executor::gpu::TmaDescriptor::TmaSwizzle::k128B);\n }\n+\n+TEST_F(EmitterHelpersTest, ExtractThreadDimsWorksCorrectlyWithValidInput) {\n+  const std::string kMlirModule = R\"(\n+module attributes {ttg.global_scratch_memory_alignment = 1 : i32, ttg.global_scratch_memory_size = 0 : i32, \"ttg.num-ctas\" = 1 : i32, \"ttg.num-warps\" = 1 : i32, ttg.shared = 10240 : i32, ttg.target = \"cuda:100\", ttg.tensor_memory_size = 0 : i32, \"ttg.threads-per-warp\" = 32 : i32, \"ttg.total-num-warps\" = 1 : i32} {\n+  llvm.func @fusion_impl() attributes {nvvm.kernel = 1 : ui1, nvvm.reqntid = array<i32: 32>, ttg.global_scratch_memory_alignment = 1 : i32, ttg.global_scratch_memory_size = 0 : i32} {\n+   llvm.return\n+  }\n+})\";\n+\n+  mlir::OwningOpRef<mlir::ModuleOp> module = ParseModule(kMlirModule);\n+  mlir::LLVM::LLVMFuncOp func_op =\n+      *module->getOps<mlir::LLVM::LLVMFuncOp>().begin();\n+  TF_ASSERT_OK_AND_ASSIGN(stream_executor::ThreadDim thread_dims,\n+                          xgt::ExtractThreadDims(module.get(), func_op));\n+  EXPECT_EQ(thread_dims, stream_executor::ThreadDim(32, 1, 1));\n+}\n }  // namespace\n }  // namespace xla::gpu"
        },
        {
            "sha": "6cfefcc6a5f685e0988c82a9c989f88321a5f843",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 41,
            "deletions": 14,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=f147bddb8dd70afe003dd6f41c77830f8369f4b1",
            "patch": "@@ -18,7 +18,6 @@ limitations under the License.\n #include <memory>\n #include <optional>\n #include <string>\n-#include <tuple>\n #include <utility>\n #include <vector>\n \n@@ -55,12 +54,13 @@ limitations under the License.\n #include \"xla/service/gpu/kernel_reuse_cache.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/service/gpu/triton_fusion_analysis.h\"\n-#include \"xla/service/llvm_ir/ir_array.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n@@ -104,16 +104,18 @@ TritonFusion::GenerateTritonKernelAndWrapper(\n   if (fusion_kind == kTritonFusionKind ||\n       fusion_kind == kTritonNestedGemmFusionKind ||\n       fusion_kind == kTritonScaledDotFusionKind) {\n-    std::optional<LaunchConfig> launch_config = this->launch_config();\n-    if (!launch_config.has_value()) {\n+    if (!analysis_.fusion_backend_config().has_block_level_fusion_config()) {\n       return absl::InvalidArgumentError(absl::StrCat(\n           \"Block level fusion config is required for Triton fusions: \",\n           fusion.ToString()));\n     }\n-    TF_ASSIGN_OR_RETURN(triton_wrapper_result,\n-                        TritonWrapper(impl_fn_name, &fusion, cc, device_info,\n-                                      launch_config->block_level_parameters,\n-                                      llvm_module, *symbolic_expr_context));\n+    TF_ASSIGN_OR_RETURN(\n+        triton_wrapper_result,\n+        TritonWrapper(\n+            impl_fn_name, &fusion, cc, device_info,\n+            BlockLevelParameters::FromBlockLevelFusionConfig(\n+                analysis_.fusion_backend_config().block_level_fusion_config()),\n+            llvm_module, *symbolic_expr_context));\n   } else {  // Must be a MatMul\n     CHECK_EQ(fusion_kind, kTritonGemmFusionKind);\n     // TODO(bchetioui): port matmul emitter to fully use the new\n@@ -177,7 +179,21 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n     if (fusion_kind == kTritonFusionKind ||\n         fusion_kind == kTritonNestedGemmFusionKind ||\n         fusion_kind == kTritonScaledDotFusionKind) {\n-      std::optional<LaunchConfig> launch_config = this->launch_config();\n+      std::optional<LaunchConfig> launch_config;\n+      // Currently GetLaunchConfig will compute the same value as the extracted\n+      // one. They are different only when warp specialization is enabled.\n+      // Ideally we should always pass the thread_dims value extracted from\n+      // the Triton compilation. However, we are keeping the old code path\n+      // to maintain the current behavior and be safe.\n+      if (fusion.GetModule()\n+              ->config()\n+              .debug_options()\n+              .xla_gpu_experimental_enable_triton_warp_specialization()) {\n+        launch_config =\n+            this->GetLaunchConfig(triton_wrapper_result.thread_dims);\n+      } else {\n+        launch_config = this->GetLaunchConfig();\n+      }\n       // This check should be enforced by `GenerateTritonKernelWrapper`.\n       CHECK(launch_config.has_value());\n       launch_dimensions = std::move(launch_config->launch_dimensions);\n@@ -283,7 +299,8 @@ int64_t GetNumberOfBlocks(absl::Span<const int64_t> dimensions,\n }\n }  // namespace\n \n-std::optional<TritonFusion::LaunchConfig> TritonFusion::launch_config() const {\n+std::optional<TritonFusion::LaunchConfig> TritonFusion::GetLaunchConfig(\n+    std::optional<se::ThreadDim> thread_dims_override) const {\n   if (analysis_.fusion_backend_config().has_block_level_fusion_config()) {\n     BlockLevelParameters block_level_parameters =\n         BlockLevelParameters::FromBlockLevelFusionConfig(\n@@ -301,10 +318,20 @@ std::optional<TritonFusion::LaunchConfig> TritonFusion::launch_config() const {\n     }\n \n     LaunchConfig launch_config;\n-    launch_config.launch_dimensions = LaunchDimensions{\n-        static_cast<uint64_t>(num_blocks),\n-        static_cast<uint64_t>(block_level_parameters.num_warps *\n-                              WarpSize(analysis_.device_info()))};\n+    // TODO(b/451901200): We eventually also want to be able to predict this\n+    // value without compiling so the cost model can rely on it. Currently, we\n+    // need the override for auto warp specialization.\n+    if (thread_dims_override) {\n+      launch_config.launch_dimensions = LaunchDimensions{\n+          se::BlockDim(num_blocks), thread_dims_override.value()};\n+    } else {\n+      int64_t estimated_threads_per_block =\n+          block_level_parameters.num_warps * WarpSize(analysis_.device_info());\n+      launch_config.launch_dimensions =\n+          LaunchDimensions{static_cast<uint64_t>(num_blocks),\n+                           static_cast<uint64_t>(estimated_threads_per_block)};\n+    }\n+\n     launch_config.block_level_parameters = std::move(block_level_parameters);\n     return launch_config;\n   }"
        },
        {
            "sha": "2405cafd70da5b443f07c0ba0f6dab07a3e04c72",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.h",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.h?ref=f147bddb8dd70afe003dd6f41c77830f8369f4b1",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"xla/service/gpu/ir_emitter_context.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/launch_dim.h\"\n \n namespace xla {\n namespace gpu {\n@@ -49,7 +50,15 @@ class TritonFusion : public FusionInterface {\n   // Returns the launch config for Triton fusions that have a block level fusion\n   // config.\n   // Not supported for MatMul fusions yet.\n-  std::optional<LaunchConfig> launch_config() const;\n+  //\n+  // If `thread_dims_override` is provided, it is used instead of the thread\n+  // dimensions calculated in the function.\n+  // Ideally, we should pass the values extracted from the Triton module after\n+  // compilation, but there are use-cases where we want to call the function\n+  // without compiling, e.g. during cost modelling. In that case, the function\n+  // calculates the values.\n+  std::optional<LaunchConfig> GetLaunchConfig(\n+      std::optional<se::ThreadDim> thread_dims_override = std::nullopt) const;\n \n   // Generates a Triton kernel for the given fusion into the provided LLVM\n   // module, and returns the `TritonWrapperResult` corresponding to the"
        },
        {
            "sha": "2a0cf013e9b2f436571c931665eecffd5017d4dd",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 14,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=f147bddb8dd70afe003dd6f41c77830f8369f4b1",
            "patch": "@@ -2151,14 +2151,8 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n     }\n \n     // Integrate LLVM matmul kernel into XLA's LLVM module.\n-    auto* nvvm_annotations =\n-        ll_triton_module->getNamedMetadata(\"nvvm.annotations\");\n-    if (nvvm_annotations) {\n-      for (auto operand : nvvm_annotations->operands()) {\n-        captured_nvvm_annotations.push_back(operand);\n-      }\n-      ll_triton_module->eraseNamedMetadata(nvvm_annotations);\n-    }\n+    captured_nvvm_annotations =\n+        xgt::ExtractNvvmAnnotations(ll_triton_module.get());\n     ll_triton_module->setDataLayout(llvm_module->getDataLayout());\n     ll_triton_module->setTargetTriple(llvm_module->getTargetTriple());\n     // Use override flag because libdevice functions can be present in both.\n@@ -2190,13 +2184,32 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n                  cluster_info.clusterDimZ == 1);\n   }\n \n-  // It's okay for tma_metadata to be empty; it's only populated when used\n-  // explicitly.\n-  TF_ASSIGN_OR_RETURN(stream_executor::gpu::TmaMetadata tma_metadata,\n-                      xgt::ExtractTmaMetadata(triton_module, kernel_name));\n+  SmallVector<mlir::LLVM::LLVMFuncOp> func_ops;\n+  for (auto func : triton_module.getOps<mlir::LLVM::LLVMFuncOp>()) {\n+    // Custom calls will also match to LLVMFuncOp, so we are only interested in\n+    // the entry function.\n+    if (func.getName().str() == kernel_name) {\n+      func_ops.push_back(func);\n+    }\n+  }\n+  CHECK_EQ(func_ops.size(), 1)\n+      << \"Expected a single LLVMFuncOp in the module for the entry function.\";\n+  mlir::LLVM::LLVMFuncOp func_op = func_ops[0];\n \n-  return {\n-      {shared_mem_bytes, cluster_dim, tma_metadata, captured_nvvm_annotations}};\n+  TF_ASSIGN_OR_RETURN(se::ThreadDim thread_dims,\n+                      xgt::ExtractThreadDims(triton_module, func_op));\n+  TF_ASSIGN_OR_RETURN(stream_executor::gpu::TmaMetadata tma_metadata,\n+                      xgt::ExtractTmaMetadata(func_op));\n+\n+  // Propagate the following extracted information from the Triton module:\n+  // - TMA metadata.\n+  // - Total threads per block. Computed from module attributes.\n+  // - Captured NVVM annotations.\n+  TritonWrapperResult result = {\n+      shared_mem_bytes,          cluster_dim, tma_metadata, thread_dims,\n+      captured_nvvm_annotations,\n+  };\n+  return result;\n }\n \n std::string GetLibdevicePath(const HloModuleConfig& hlo_config,"
        },
        {
            "sha": "423b3a58fc2fb3a47bc5bdea181ffb2b6b584a97",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h?ref=f147bddb8dd70afe003dd6f41c77830f8369f4b1",
            "patch": "@@ -59,7 +59,8 @@ namespace gpu {\n struct TritonWrapperResult {\n   int64_t shmem_bytes = 0;\n   std::optional<se::ClusterDim> cluster_dim;\n-  stream_executor::gpu::TmaMetadata tma_metadata;\n+  se::gpu::TmaMetadata tma_metadata;\n+  se::ThreadDim thread_dims;\n \n   // The captured nvvm.annotations from the lowest level LLVM IR coming from\n   // Triton. We need to propagate them because we later create the kernel and"
        },
        {
            "sha": "e76a3e31bf840423359deb5f1ee5d9782d470c3b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_test.cc",
            "status": "modified",
            "additions": 44,
            "deletions": 2,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_test.cc?ref=f147bddb8dd70afe003dd6f41c77830f8369f4b1",
            "patch": "@@ -31,6 +31,7 @@ limitations under the License.\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/launch_dim.h\"\n #include \"tsl/platform/status_matchers.h\"\n #include \"tsl/platform/statusor.h\"\n \n@@ -74,7 +75,7 @@ ENTRY entry_computation {\n   auto triton_fusion = dynamic_cast<TritonFusion*>(emitter.get());\n   ASSERT_NE(triton_fusion, nullptr);\n   std::optional<TritonFusion::LaunchConfig> launch_config =\n-      triton_fusion->launch_config();\n+      triton_fusion->GetLaunchConfig();\n   ASSERT_NE(launch_config, std::nullopt);\n   EXPECT_EQ(launch_config->launch_dimensions.num_blocks(),\n             /*ceil(125 / 4)=*/32);\n@@ -112,7 +113,7 @@ ENTRY entry_computation {\n       PreBufferAssignmentFusionInfo{analysis}, &symbolic_expr_context);\n   auto triton_fusion_emitter = dynamic_cast<TritonFusion*>(emitter.get());\n   ASSERT_NE(triton_fusion_emitter, nullptr);\n-  EXPECT_EQ(triton_fusion_emitter->launch_config(), std::nullopt);\n+  EXPECT_EQ(triton_fusion_emitter->GetLaunchConfig(), std::nullopt);\n \n   // Ensure that the emitter fails gracefully when the launch config is not set.\n   EXPECT_THAT(triton_fusion_emitter->GenerateTritonKernelAndWrapper(\n@@ -121,6 +122,47 @@ ENTRY entry_computation {\n               absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n }\n \n+TEST_F(\n+    TritonFusionTest,\n+    TritonFusionWithBlockLevelFusionConfig_LaunchConfigOverrideWorksCorrectly) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+triton_computation {\n+  param_0 = f32[125,127] parameter(0)\n+  ROOT abs = f32[125,127] abs(param_0)\n+}\n+\n+ENTRY entry_computation {\n+  param_0 = f32[125,127] parameter(0)\n+  ROOT fusion.1 = f32[125,127] fusion(param_0), kind=kCustom,\n+    calls=triton_computation,\n+    backend_config={\"fusion_backend_config\":{\n+      \"kind\":\"__triton\",\n+      \"block_level_fusion_config\":{\"output_tiles\":[{\"sizes\":[\"4\",\"127\"]}],\n+                                   \"num_warps\":\"4\"}}}\n+})\"));\n+\n+  stream_executor::DeviceDescription device_info =\n+      TestGpuDeviceInfo::RTXA6000DeviceInfo();\n+\n+  auto* root = module->entry_computation()->root_instruction();\n+  HloFusionAnalysis analysis = HloFusionAnalysis::Create(*root, device_info);\n+\n+  mlir::MLIRContext mlir_context;\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n+  std::unique_ptr<FusionInterface> emitter = GetFusionEmitter(\n+      PreBufferAssignmentFusionInfo{analysis}, &symbolic_expr_context);\n+  auto triton_fusion = dynamic_cast<TritonFusion*>(emitter.get());\n+\n+  ASSERT_NE(triton_fusion, nullptr);\n+  std::optional<TritonFusion::LaunchConfig> launch_config =\n+      triton_fusion->GetLaunchConfig(se::ThreadDim(32, 2, 1));\n+  ASSERT_NE(launch_config, std::nullopt);\n+  EXPECT_EQ(launch_config->launch_dimensions.num_blocks(),\n+            /*ceil(125 / 4)=*/32);\n+  EXPECT_EQ(launch_config->launch_dimensions.num_threads_per_block(),\n+            /*32 * 2 * 1=*/64);\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "81f2002f573432357b83b0bff0eed789377ea7b2",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_indexing_performance_model.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_indexing_performance_model.cc?ref=f147bddb8dd70afe003dd6f41c77830f8369f4b1",
            "patch": "@@ -601,7 +601,7 @@ GpuPerformanceModelWithIndexingAnalysis::EstimateRunTimeForTriton(\n   const auto& fusion_analysis =\n       (consumer == nullptr) ? fusion_analysis_cache_->Get(*producer)\n                             : fusion_analysis_cache_->Get(*producer, *consumer);\n-  auto launch_config = TritonFusion(fusion_analysis).launch_config();\n+  auto launch_config = TritonFusion(fusion_analysis).GetLaunchConfig();\n \n   if (!launch_config.has_value()) {\n     return absl::InvalidArgumentError("
        },
        {
            "sha": "47620f317e66a2af214d73dd2fc992f0295db962",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_performance_model_base.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_performance_model_base.cc?ref=f147bddb8dd70afe003dd6f41c77830f8369f4b1",
            "patch": "@@ -156,7 +156,7 @@ LaunchDimensions GpuPerformanceModelBase::EstimateFusionLaunchDimensions(\n   // launch dimensions only for SoftMax fusions.\n   if (const auto* triton_emitter =\n           dynamic_cast<const TritonFusion*>(emitter.get())) {\n-    if (auto launch_config = triton_emitter->launch_config()) {\n+    if (auto launch_config = triton_emitter->GetLaunchConfig()) {\n       return launch_config->launch_dimensions;\n     }\n   }"
        },
        {
            "sha": "6c482d113d1164966b850200e709d13791a9fb65",
            "filename": "third_party/xla/xla/util.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Futil.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Futil.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Futil.cc?ref=f147bddb8dd70afe003dd6f41c77830f8369f4b1",
            "patch": "@@ -334,11 +334,6 @@ void LogLines(absl::LogSeverity sev, absl::string_view text, const char* fname,\n   }\n }\n \n-int64_t Product(absl::Span<const int64_t> xs) {\n-  return absl::c_accumulate(xs, static_cast<int64_t>(1),\n-                            std::multiplies<int64_t>());\n-}\n-\n std::vector<int64_t> ElemwiseProduct(absl::Span<const int64_t> a,\n                                      absl::Span<const int64_t> b) {\n   CHECK_EQ(a.size(), b.size());\n@@ -564,4 +559,5 @@ std::unique_ptr<void, FreeDeleter> AlignedAlloc(std::size_t alignment,\n   return std::unique_ptr<void, FreeDeleter>(raw_ptr, FreeDeleter());\n }\n \n+int64_t Product(absl::Span<const int64_t> xs) { return Product<int64_t>(xs); }\n }  // namespace xla"
        },
        {
            "sha": "80091d8328c49d12187eb260e335fb07d1b98eec",
            "filename": "third_party/xla/xla/util.h",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Futil.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f147bddb8dd70afe003dd6f41c77830f8369f4b1/third_party%2Fxla%2Fxla%2Futil.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Futil.h?ref=f147bddb8dd70afe003dd6f41c77830f8369f4b1",
            "patch": "@@ -756,6 +756,11 @@ std::unique_ptr<Derived> unique_ptr_down_cast(std::unique_ptr<Base> ptr) {\n   return absl::WrapUnique(tensorflow::down_cast<Derived*>(ptr.release()));\n }\n \n+template <typename T>\n+T Product(absl::Span<const T> xs) {\n+  return absl::c_accumulate(xs, static_cast<T>(1), std::multiplies<T>());\n+}\n+\n int64_t Product(absl::Span<const int64_t> xs);\n \n // Returns an array of results after performing elementwise product of a and b."
        }
    ],
    "stats": {
        "total": 346,
        "additions": 279,
        "deletions": 67
    }
}