{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 828956296",
    "sha": "ee5093c3be6d1429c12cc40c1be00dbb7dfb5aba",
    "files": [
        {
            "sha": "13d130d289418cc5c7632dd88c85c5d7f171a4de",
            "filename": "tensorflow/core/distributed_runtime/graph_mgr.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 23,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee5093c3be6d1429c12cc40c1be00dbb7dfb5aba/tensorflow%2Fcore%2Fdistributed_runtime%2Fgraph_mgr.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee5093c3be6d1429c12cc40c1be00dbb7dfb5aba/tensorflow%2Fcore%2Fdistributed_runtime%2Fgraph_mgr.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Fgraph_mgr.cc?ref=ee5093c3be6d1429c12cc40c1be00dbb7dfb5aba",
            "patch": "@@ -107,7 +107,7 @@ GraphMgr::Item::~Item() {\n // NOTE: node->device_name() is not set by GraphConstructor.  We\n // expects that NodeDef in GraphDef given to workers fully specifies\n // device names.\n-static string SplitByDevice(const Node* node) {\n+static std::string SplitByDevice(const Node* node) {\n   return node->assigned_device_name();\n }\n \n@@ -144,7 +144,7 @@ absl::Status GraphMgr::DecorateAndPublishGraphForDebug(\n //\n // \"executors\" are filled with one executor per device if success and\n // the caller takes the ownership of returned executors.\n-absl::Status GraphMgr::InitItem(const string& handle, const GraphDef& gdef,\n+absl::Status GraphMgr::InitItem(const std::string& handle, const GraphDef& gdef,\n                                 const GraphOptions& graph_options,\n                                 const DebugOptions& debug_options,\n                                 const ConfigProto& config_proto,\n@@ -187,14 +187,14 @@ absl::Status GraphMgr::InitItem(const string& handle, const GraphDef& gdef,\n   TF_RETURN_IF_ERROR(ConvertGraphDefToGraph(opts, gdef, &graph));\n \n   // Splits \"graph\" into multiple subgraphs by device names.\n-  std::unordered_map<string, GraphDef> partitions;\n+  std::unordered_map<std::string, GraphDef> partitions;\n   PartitionOptions popts;\n   popts.node_to_loc = SplitByDevice;\n-  popts.new_name = [this](const string& prefix) {\n+  popts.new_name = [this](const std::string& prefix) {\n     mutex_lock l(mu_);\n     return absl::StrCat(prefix, \"_G\", next_id_++);\n   };\n-  popts.get_incarnation = [this](const string& name) -> int64 {\n+  popts.get_incarnation = [this](const std::string& name) -> int64_t {\n     Device* device = nullptr;\n     absl::Status s = device_mgr_->LookupDevice(name, &device);\n     if (s.ok()) {\n@@ -211,7 +211,7 @@ absl::Status GraphMgr::InitItem(const string& handle, const GraphDef& gdef,\n     TF_RETURN_IF_ERROR(AddControlEdges(popts, &partitions));\n   }\n \n-  std::unordered_map<string, std::unique_ptr<Graph>> partition_graphs;\n+  std::unordered_map<std::string, std::unique_ptr<Graph>> partition_graphs;\n   for (auto& partition : partitions) {\n     std::unique_ptr<Graph> device_graph(new Graph(OpRegistry::Global()));\n     GraphConstructorOptions device_opts;\n@@ -236,7 +236,7 @@ absl::Status GraphMgr::InitItem(const string& handle, const GraphDef& gdef,\n   const auto& optimizer_opts = graph_options.optimizer_options();\n   GraphOptimizer optimizer(optimizer_opts);\n   for (auto& p : partition_graphs) {\n-    const string& device_name = p.first;\n+    const std::string& device_name = p.first;\n     std::unique_ptr<Graph>& subgraph = p.second;\n     item->units.resize(item->units.size() + 1);\n     ExecutionUnit* unit = &(item->units.back());\n@@ -316,14 +316,14 @@ absl::Status GraphMgr::InitItem(const string& handle, const GraphDef& gdef,\n   return absl::OkStatus();\n }\n \n-absl::Status GraphMgr::Register(const string& handle, const GraphDef& gdef,\n+absl::Status GraphMgr::Register(const std::string& handle, const GraphDef& gdef,\n                                 const GraphOptions& graph_options,\n                                 const DebugOptions& debug_options,\n                                 const ConfigProto& config_proto,\n                                 int64_t collective_graph_key,\n                                 WorkerSession* session,\n                                 DistributedFunctionLibraryRuntime* cluster_flr,\n-                                string* graph_handle) {\n+                                std::string* graph_handle) {\n   Item* item = new Item;\n   absl::Status s =\n       InitItem(handle, gdef, graph_options, debug_options, config_proto,\n@@ -344,7 +344,7 @@ absl::Status GraphMgr::Register(const string& handle, const GraphDef& gdef,\n   return absl::OkStatus();\n }\n \n-absl::Status GraphMgr::Deregister(const string& handle) {\n+absl::Status GraphMgr::Deregister(const std::string& handle) {\n   Item* item = nullptr;\n   // Removes one item from table_.\n   {\n@@ -380,7 +380,7 @@ absl::Status GraphMgr::DeregisterAll() {\n absl::Status GraphMgr::SendInputs(const int64_t step_id,\n                                   const NamedTensors& in) {\n   Rendezvous* rendezvous = worker_env_->rendezvous_mgr->Find(step_id).release();\n-  std::vector<string> keys;\n+  std::vector<std::string> keys;\n   std::vector<Tensor> tensors_to_send;\n   keys.reserve(in.size());\n   tensors_to_send.reserve(in.size());\n@@ -419,7 +419,7 @@ absl::Status GraphMgr::RecvOutputs(const int64_t step_id, NamedTensors* out) {\n void GraphMgr::RecvOutputsAsync(const int64_t step_id, NamedTensors* out,\n                                 StatusCallback done) {\n   Rendezvous* rendezvous = worker_env_->rendezvous_mgr->Find(step_id).release();\n-  std::vector<string> keys;\n+  std::vector<std::string> keys;\n   std::vector<Tensor>* received_keys = new std::vector<Tensor>;\n   keys.reserve(out->size());\n   received_keys->reserve(out->size());\n@@ -443,13 +443,13 @@ void GraphMgr::RecvOutputsAsync(const int64_t step_id, NamedTensors* out,\n }\n \n void GraphMgr::ExecuteAsync(\n-    const string& handle, const int64_t step_id, const ExecutorOpts& opts,\n+    const std::string& handle, const int64_t step_id, const ExecutorOpts& opts,\n     const NamedTensors& in, WorkerSession* session,\n     StepStatsCollector* collector, MutableRunGraphResponseWrapper* response,\n     CancellationManager* cancellation_manager,\n     tsl::CoordinationServiceAgent* coordination_service_agent,\n     StatusCallback done) {\n-  const uint64 start_time_usecs = Env::Default()->NowMicros();\n+  const uint64_t start_time_usecs = Env::Default()->NowMicros();\n   tsl::profiler::TraceMeProducer activity(\n       // To TraceMeConsumers in ExecutorState::Process/Finish or RunGraphDone.\n       [step_id] {\n@@ -498,7 +498,7 @@ void GraphMgr::ExecuteAsync(\n   // Sends values specified by the caller.\n   size_t input_size = 0;\n   if (s.ok()) {\n-    std::vector<string> keys;\n+    std::vector<std::string> keys;\n     std::vector<Tensor> tensors_to_send;\n     keys.reserve(in.size());\n     tensors_to_send.reserve(in.size());\n@@ -543,17 +543,19 @@ void GraphMgr::ExecuteAsync(\n }\n \n void GraphMgr::StartParallelExecutors(\n-    const string& handle, int64_t step_id, Item* item, Rendezvous* rendezvous,\n-    CollectiveExecutor::Handle* ce_handle, StepStatsCollector* collector,\n-    CostGraphDef* cost_graph, CancellationManager* cancellation_manager,\n-    WorkerSession* session, int64_t start_time_usecs,\n+    const std::string& handle, int64_t step_id, Item* item,\n+    Rendezvous* rendezvous, CollectiveExecutor::Handle* ce_handle,\n+    StepStatsCollector* collector, CostGraphDef* cost_graph,\n+    CancellationManager* cancellation_manager, WorkerSession* session,\n+    int64_t start_time_usecs,\n     tsl::CoordinationServiceAgent* coordination_service_agent,\n     StatusCallback done) {\n   const int num_units = item->units.size();\n   CHECK_GE(num_units, 1);\n-  ScopedStepContainer* step_container = new ScopedStepContainer(\n-      step_id,\n-      [this](const string& name) { device_mgr_->ClearContainers({name}); });\n+  ScopedStepContainer* step_container =\n+      new ScopedStepContainer(step_id, [this](const std::string& name) {\n+        device_mgr_->ClearContainers({name});\n+      });\n   // NOTE: Transfer one ref of rendezvous and item.\n   ExecutorBarrier* barrier =\n       new ExecutorBarrier(num_units, rendezvous,\n@@ -602,7 +604,7 @@ void GraphMgr::BuildCostModel(Item* item, StepStatsCollector* collector,\n                               CostGraphDef* cost_graph) {\n   if (collector && !skip_cost_models_) {\n     // Build the cost model\n-    std::unordered_map<string, const Graph*> device_to_graph;\n+    std::unordered_map<std::string, const Graph*> device_to_graph;\n     for (const auto& unit : item->units) {\n       if (unit.build_cost_model > 0) {\n         device_to_graph[unit.device->name()] = unit.graph.get();"
        },
        {
            "sha": "3458771a21e9b117b47dd7c667ec271470a9d4b3",
            "filename": "tensorflow/core/distributed_runtime/graph_mgr.h",
            "status": "modified",
            "additions": 14,
            "deletions": 13,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee5093c3be6d1429c12cc40c1be00dbb7dfb5aba/tensorflow%2Fcore%2Fdistributed_runtime%2Fgraph_mgr.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee5093c3be6d1429c12cc40c1be00dbb7dfb5aba/tensorflow%2Fcore%2Fdistributed_runtime%2Fgraph_mgr.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Fgraph_mgr.h?ref=ee5093c3be6d1429c12cc40c1be00dbb7dfb5aba",
            "patch": "@@ -85,21 +85,21 @@ class GraphMgr {\n \n   // Registers a graph. Fills in \"handle\". The registered graph retains a\n   // reference to cluster_flr to do cross process function calls.\n-  absl::Status Register(const string& handle, const GraphDef& gdef,\n+  absl::Status Register(const std::string& handle, const GraphDef& gdef,\n                         const GraphOptions& graph_options,\n                         const DebugOptions& debug_options,\n                         const ConfigProto& config_proto,\n                         int64_t collective_graph_key, WorkerSession* session,\n                         DistributedFunctionLibraryRuntime* cluster_flr,\n-                        string* graph_handle);\n+                        std::string* graph_handle);\n \n   // Executes one step of a registered graph \"handle\".\n   //\n   // If \"out\" is not nullptr, \"out\" specifies all keys the execution\n   // should receive upon finish.\n-  typedef std::map<string, Tensor> NamedTensors;\n+  typedef std::map<std::string, Tensor> NamedTensors;\n   typedef std::function<void(const absl::Status&)> StatusCallback;\n-  void ExecuteAsync(const string& handle, const int64_t step_id,\n+  void ExecuteAsync(const std::string& handle, const int64_t step_id,\n                     const ExecutorOpts& opts, const NamedTensors& in,\n                     WorkerSession* session, StepStatsCollector* collector,\n                     MutableRunGraphResponseWrapper* response,\n@@ -113,7 +113,7 @@ class GraphMgr {\n                         StatusCallback done);\n \n   // Deregisters a graph.\n-  absl::Status Deregister(const string& handle);\n+  absl::Status Deregister(const std::string& handle);\n \n   // Deregister all graphs.\n   absl::Status DeregisterAll();\n@@ -137,10 +137,10 @@ class GraphMgr {\n     ~Item() override;\n \n     // Session handle.\n-    string session;\n+    std::string session;\n \n     // Graph handle.\n-    string handle;\n+    std::string handle;\n \n     // Session configuration options for the graph.\n     ConfigProto session_config;\n@@ -177,13 +177,14 @@ class GraphMgr {\n   // TODO(zhifengc): If the client does not call Deregister, we'll\n   // lose memory over time. We should implement a timeout-based\n   // mechanism to gc these graphs.\n-  std::unordered_map<string, Item*> table_;\n+  std::unordered_map<std::string, Item*> table_;\n \n   void StartParallelExecutors(\n-      const string& handle, int64_t step_id, Item* item, Rendezvous* rendezvous,\n-      CollectiveExecutor::Handle* ce_handle, StepStatsCollector* collector,\n-      CostGraphDef* cost_graph, CancellationManager* cancellation_manager,\n-      WorkerSession* session, int64_t start_time_usecs,\n+      const std::string& handle, int64_t step_id, Item* item,\n+      Rendezvous* rendezvous, CollectiveExecutor::Handle* ce_handle,\n+      StepStatsCollector* collector, CostGraphDef* cost_graph,\n+      CancellationManager* cancellation_manager, WorkerSession* session,\n+      int64_t start_time_usecs,\n       tsl::CoordinationServiceAgent* coordination_service_agent,\n       StatusCallback done);\n \n@@ -194,7 +195,7 @@ class GraphMgr {\n   void BuildCostModel(Item* item, StepStatsCollector* collector,\n                       CostGraphDef* cost_graph);\n \n-  absl::Status InitItem(const string& handle, const GraphDef& gdef,\n+  absl::Status InitItem(const std::string& handle, const GraphDef& gdef,\n                         const GraphOptions& graph_options,\n                         const DebugOptions& debug_options,\n                         const ConfigProto& config_proto,"
        }
    ],
    "stats": {
        "total": 75,
        "additions": 39,
        "deletions": 36
    }
}