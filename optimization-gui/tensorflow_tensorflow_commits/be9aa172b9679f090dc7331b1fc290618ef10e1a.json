{
    "author": "akuegel",
    "message": "Use GetInPlaceInputOutputPairs from AliasInfo instead of HloDataflowAnalysis.\n\nPiperOrigin-RevId: 837557364",
    "sha": "be9aa172b9679f090dc7331b1fc290618ef10e1a",
    "files": [
        {
            "sha": "2eab9a0f5a2f214ea138ee2a3b2c8b858183a270",
            "filename": "third_party/xla/xla/hlo/analysis/hlo_dataflow_analysis_test.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 16,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fhlo_dataflow_analysis_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fhlo_dataflow_analysis_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fanalysis%2Fhlo_dataflow_analysis_test.cc?ref=be9aa172b9679f090dc7331b1fc290618ef10e1a",
            "patch": "@@ -3079,7 +3079,10 @@ TEST_F(CanShareOperandBufferWithUserTest, CallToComputationWithFusionRoot) {\n       reverse, {}, call, {}, &alias_info_));\n }\n \n-using GetInPlaceInputOutputPairsTest = HloHardwareIndependentTestBase;\n+class GetInPlaceInputOutputPairsTest : public HloHardwareIndependentTestBase {\n+ protected:\n+  AliasInfo alias_info_;\n+};\n \n TEST_F(GetInPlaceInputOutputPairsTest, DUS) {\n   const char* kModule = R\"(\n@@ -3095,7 +3098,7 @@ TEST_F(GetInPlaceInputOutputPairsTest, DUS) {\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kModule));\n   HloInstruction* dus = module->entry_computation()->root_instruction();\n \n-  auto in_place_pairs = HloDataflowAnalysis::GetInPlaceInputOutputPairs(dus);\n+  auto in_place_pairs = alias_info_.GetInPlaceInputOutputPairs(dus);\n   std::vector<std::pair<HloOperandIndex, ShapeIndex>> expected_pairs;\n   expected_pairs.push_back({HloOperandIndex{0, {}}, {}});\n   EXPECT_EQ(in_place_pairs, expected_pairs);\n@@ -3122,7 +3125,7 @@ TEST_F(GetInPlaceInputOutputPairsTest, DUSFusion) {\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kModule));\n   HloInstruction* fusion = module->entry_computation()->root_instruction();\n \n-  auto in_place_pairs = HloDataflowAnalysis::GetInPlaceInputOutputPairs(fusion);\n+  auto in_place_pairs = alias_info_.GetInPlaceInputOutputPairs(fusion);\n   std::vector<std::pair<HloOperandIndex, ShapeIndex>> expected_pairs;\n   expected_pairs.push_back({HloOperandIndex{0, {}}, {}});\n   EXPECT_EQ(in_place_pairs, expected_pairs);\n@@ -3150,7 +3153,7 @@ TEST_F(GetInPlaceInputOutputPairsTest, DUSFusionWithOutputOperandAliasing) {\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kModule));\n   HloInstruction* fusion = module->entry_computation()->root_instruction();\n \n-  auto in_place_pairs = HloDataflowAnalysis::GetInPlaceInputOutputPairs(fusion);\n+  auto in_place_pairs = alias_info_.GetInPlaceInputOutputPairs(fusion);\n   std::vector<std::pair<HloOperandIndex, ShapeIndex>> expected_pairs;\n   expected_pairs.push_back({HloOperandIndex{0, {}}, {1}});  // discovered\n   expected_pairs.push_back({HloOperandIndex{1, {}}, {0}});  // annotated\n@@ -3176,7 +3179,7 @@ TEST_F(GetInPlaceInputOutputPairsTest, NonDUSFusion) {\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kModule));\n   HloInstruction* fusion = module->entry_computation()->root_instruction();\n \n-  auto in_place_pairs = HloDataflowAnalysis::GetInPlaceInputOutputPairs(fusion);\n+  auto in_place_pairs = alias_info_.GetInPlaceInputOutputPairs(fusion);\n   EXPECT_THAT(in_place_pairs, IsEmpty());\n }\n \n@@ -3198,7 +3201,7 @@ TEST_F(GetInPlaceInputOutputPairsTest, NonDUSFusionWithOutputOperandAliasing) {\n   )\";\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kModule));\n   HloInstruction* fusion = module->entry_computation()->root_instruction();\n-  auto in_place_pairs = HloDataflowAnalysis::GetInPlaceInputOutputPairs(fusion);\n+  auto in_place_pairs = alias_info_.GetInPlaceInputOutputPairs(fusion);\n \n   std::vector<std::pair<HloOperandIndex, ShapeIndex>> expected_pairs;\n   expected_pairs.push_back({HloOperandIndex{0, {}}, {}});\n@@ -3233,7 +3236,7 @@ TEST_F(GetInPlaceInputOutputPairsTest, NestedDUSFusion) {\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kModule));\n   HloInstruction* fusion = module->entry_computation()->root_instruction();\n \n-  auto in_place_pairs = HloDataflowAnalysis::GetInPlaceInputOutputPairs(fusion);\n+  auto in_place_pairs = alias_info_.GetInPlaceInputOutputPairs(fusion);\n   std::vector<std::pair<HloOperandIndex, ShapeIndex>> expected_pairs;\n   expected_pairs.push_back({HloOperandIndex{0, {}}, {}});\n   EXPECT_EQ(in_place_pairs, expected_pairs);\n@@ -3277,12 +3280,12 @@ TEST_F(GetInPlaceInputOutputPairsTest, NestedMultiOutputDUSFusion) {\n   HloInstruction* inner_fusion = FindInstruction(module.get(), \"inner_fusion\");\n \n   auto inner_in_place_pairs =\n-      HloDataflowAnalysis::GetInPlaceInputOutputPairs(inner_fusion);\n+      alias_info_.GetInPlaceInputOutputPairs(inner_fusion);\n   std::vector<std::pair<HloOperandIndex, ShapeIndex>> inner_expected_pairs;\n   inner_expected_pairs.push_back({HloOperandIndex{1, {1}}, {1}});\n   EXPECT_EQ(inner_in_place_pairs, inner_expected_pairs);\n \n-  auto in_place_pairs = HloDataflowAnalysis::GetInPlaceInputOutputPairs(fusion);\n+  auto in_place_pairs = alias_info_.GetInPlaceInputOutputPairs(fusion);\n   std::vector<std::pair<HloOperandIndex, ShapeIndex>> expected_pairs;\n   expected_pairs.push_back({HloOperandIndex{1, {0}}, {2}});\n   EXPECT_EQ(in_place_pairs, expected_pairs);\n@@ -3319,7 +3322,7 @@ TEST_F(GetInPlaceInputOutputPairsTest, NestedLoopWithAliasingInDUSFusion) {\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kModule));\n   HloInstruction* fusion = module->entry_computation()->root_instruction();\n \n-  auto in_place_pairs = HloDataflowAnalysis::GetInPlaceInputOutputPairs(fusion);\n+  auto in_place_pairs = alias_info_.GetInPlaceInputOutputPairs(fusion);\n   std::vector<std::pair<HloOperandIndex, ShapeIndex>> expected_pairs;\n   expected_pairs.push_back({HloOperandIndex{0, {0}}, {}});\n   EXPECT_EQ(in_place_pairs, expected_pairs);\n@@ -3372,7 +3375,7 @@ TEST_F(GetInPlaceInputOutputPairsTest, DUSLoopFusionWithCollective) {\n   )\";\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kModule));\n   HloInstruction* fusion = module->entry_computation()->root_instruction();\n-  auto in_place_pairs = HloDataflowAnalysis::GetInPlaceInputOutputPairs(fusion);\n+  auto in_place_pairs = alias_info_.GetInPlaceInputOutputPairs(fusion);\n   std::vector<std::pair<HloOperandIndex, ShapeIndex>> expected_pairs;\n   expected_pairs.push_back({HloOperandIndex{0, {}}, {1}});\n   EXPECT_EQ(in_place_pairs, expected_pairs);\n@@ -3423,7 +3426,7 @@ TEST_F(GetInPlaceInputOutputPairsTest, DUSOutputFusionWithCollective) {\n   )\";\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kModule));\n   HloInstruction* fusion = module->entry_computation()->root_instruction();\n-  auto in_place_pairs = HloDataflowAnalysis::GetInPlaceInputOutputPairs(fusion);\n+  auto in_place_pairs = alias_info_.GetInPlaceInputOutputPairs(fusion);\n   std::vector<std::pair<HloOperandIndex, ShapeIndex>> expected_pairs;\n   expected_pairs.push_back({HloOperandIndex{0, {}}, {1}});\n   EXPECT_EQ(in_place_pairs, expected_pairs);\n@@ -3456,7 +3459,7 @@ TEST_F(GetInPlaceInputOutputPairsTest, DUSLoopFusionWithBitcast) {\n   )\";\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kModule));\n   HloInstruction* fusion = module->entry_computation()->root_instruction();\n-  auto in_place_pairs = HloDataflowAnalysis::GetInPlaceInputOutputPairs(fusion);\n+  auto in_place_pairs = alias_info_.GetInPlaceInputOutputPairs(fusion);\n   std::vector<std::pair<HloOperandIndex, ShapeIndex>> expected_pairs;\n   // p1 should be aliased with fusion1\n   expected_pairs.push_back({HloOperandIndex{1, {}}, {}});\n@@ -3485,7 +3488,7 @@ ENTRY AllToAll {\n       module->entry_computation()->root_instruction();\n \n   auto in_place_pairs =\n-      HloDataflowAnalysis::GetInPlaceInputOutputPairs(ragged_all_to_all);\n+      alias_info_.GetInPlaceInputOutputPairs(ragged_all_to_all);\n   std::vector<std::pair<HloOperandIndex, ShapeIndex>> expected_pairs;\n   expected_pairs.push_back({HloOperandIndex{1, {}}, {}});\n   EXPECT_EQ(in_place_pairs, expected_pairs);\n@@ -3607,8 +3610,7 @@ TEST_F(GetInPlaceInputOutputPairsTest, nvshmem_ar) {\n   const HloInstruction* ar_start =\n       module->entry_computation()->root_instruction()->operand(0);\n \n-  auto in_place_pairs =\n-      HloDataflowAnalysis::GetInPlaceInputOutputPairs(ar_start);\n+  auto in_place_pairs = alias_info_.GetInPlaceInputOutputPairs(ar_start);\n   std::vector<std::pair<HloOperandIndex, ShapeIndex>> expected_pairs;\n   // For nvshmem allreduce, we expect no aliasing for input and output buffers\n   // therefore empty inplace pairs."
        },
        {
            "sha": "2a6a08d6be04a1dbda92df93b5da1ea850934c24",
            "filename": "third_party/xla/xla/hlo/tools/hlo_opt/opt_lib.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fhlo%2Ftools%2Fhlo_opt%2Fopt_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fhlo%2Ftools%2Fhlo_opt%2Fopt_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftools%2Fhlo_opt%2Fopt_lib.cc?ref=be9aa172b9679f090dc7331b1fc290618ef10e1a",
            "patch": "@@ -246,7 +246,7 @@ void OptProvider::RegisterAllHardwareIndependentPasses() {\n   RegisterPass<AsyncCollectiveCreator>(\n       AsyncCollectiveCreator::CollectiveCreatorConfig());\n   RegisterPass<BFloat16ConversionFolding>(\n-      /*bfloat16_support=*/bfloat16_support);\n+      /*bfloat16_support=*/bfloat16_support, alias_info_.get());\n   RegisterPass<BFloat16MixedPrecisionRemoval>();\n   RegisterPass<BFloat16Propagation>(/*bfloat16_support=*/bfloat16_support,\n                                     alias_info_.get());"
        },
        {
            "sha": "075f76ce3cfd6cb768f9d1ea30892a464dbc4fbb",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2FBUILD?ref=be9aa172b9679f090dc7331b1fc290618ef10e1a",
            "patch": "@@ -158,7 +158,7 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/hlo/analysis:hlo_dataflow_analysis\",\n+        \"//xla/hlo/analysis:alias_info\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/service:float_support\",\n@@ -178,6 +178,7 @@ xla_cc_test(\n         \":bfloat16_conversion_folding\",\n         \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/hlo/analysis:alias_info\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/hlo/testlib:test_helpers\","
        },
        {
            "sha": "427618cd82c3cfc899541e1d32b8ac2ff5f7a815",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/bfloat16_conversion_folding.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 20,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fbfloat16_conversion_folding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fbfloat16_conversion_folding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fbfloat16_conversion_folding.cc?ref=be9aa172b9679f090dc7331b1fc290618ef10e1a",
            "patch": "@@ -23,7 +23,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"xla/hlo/analysis/hlo_dataflow_analysis.h\"\n+#include \"xla/hlo/analysis/alias_info.h\"\n #include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -40,9 +40,11 @@ class BFloat16ConversionFoldingVisitor : public DfsHloVisitorWithDefault {\n  public:\n   explicit BFloat16ConversionFoldingVisitor(\n       HloComputation* computation, const FloatSupport* bfloat16_support,\n+      const AliasInfo* alias_info,\n       BFloat16ConversionFolding* bfloat16_conversion_folding)\n       : computation_(computation),\n         bfloat16_support_(bfloat16_support),\n+        alias_info_(alias_info),\n         bfloat16_conversion_folding_(bfloat16_conversion_folding) {}\n \n   absl::Status DefaultAction(HloInstruction* hlo) override;\n@@ -52,9 +54,10 @@ class BFloat16ConversionFoldingVisitor : public DfsHloVisitorWithDefault {\n \n   static bool Run(HloComputation* computation,\n                   const FloatSupport* bfloat16_support,\n+                  const AliasInfo* alias_info,\n                   BFloat16ConversionFolding* bfloat16_conversion_folding) {\n-    BFloat16ConversionFoldingVisitor visitor(computation, bfloat16_support,\n-                                             bfloat16_conversion_folding);\n+    BFloat16ConversionFoldingVisitor visitor(\n+        computation, bfloat16_support, alias_info, bfloat16_conversion_folding);\n     CHECK_OK(computation->Accept(&visitor));\n     return visitor.changed_;\n   }\n@@ -77,6 +80,7 @@ class BFloat16ConversionFoldingVisitor : public DfsHloVisitorWithDefault {\n \n   HloComputation* computation_;\n   const FloatSupport* bfloat16_support_;\n+  const AliasInfo* alias_info_;\n   BFloat16ConversionFolding* bfloat16_conversion_folding_;\n   bool changed_ = false;\n };\n@@ -172,22 +176,22 @@ absl::Status BFloat16ConversionFoldingVisitor::DefaultAction(\n   // Do not fold BF16 conversions for instructions related to tuples, entry and\n   // exit of a computation, fusion, convert, side-effecting instructions,\n   // in-place operations and control flow.\n-  if (hlo->opcode() == HloOpcode::kTuple ||                             //\n-      hlo->opcode() == HloOpcode::kGetTupleElement ||                   //\n-      hlo->opcode() == HloOpcode::kConstant ||                          //\n-      hlo->opcode() == HloOpcode::kParameter ||                         //\n-      hlo->opcode() == HloOpcode::kFusion ||                            //\n-      hlo->opcode() == HloOpcode::kBitcast ||                           //\n-      hlo->opcode() == HloOpcode::kBitcastConvert ||                    //\n-      hlo->opcode() == HloOpcode::kConvert ||                           //\n-      hlo->opcode() == HloOpcode::kCall ||                              //\n-      hlo->opcode() == HloOpcode::kCustomCall ||                        //\n-      hlo->opcode() == HloOpcode::kWhile ||                             //\n-      hlo->opcode() == HloOpcode::kConditional ||                       //\n-      hlo->opcode() == HloOpcode::kAsyncStart ||                        //\n-      hlo->opcode() == HloOpcode::kAsyncDone ||                         //\n-      hlo->opcode() == HloOpcode::kOptimizationBarrier ||               //\n-      !HloDataflowAnalysis::GetInPlaceInputOutputPairs(hlo).empty() ||  //\n+  if (hlo->opcode() == HloOpcode::kTuple ||                     //\n+      hlo->opcode() == HloOpcode::kGetTupleElement ||           //\n+      hlo->opcode() == HloOpcode::kConstant ||                  //\n+      hlo->opcode() == HloOpcode::kParameter ||                 //\n+      hlo->opcode() == HloOpcode::kFusion ||                    //\n+      hlo->opcode() == HloOpcode::kBitcast ||                   //\n+      hlo->opcode() == HloOpcode::kBitcastConvert ||            //\n+      hlo->opcode() == HloOpcode::kConvert ||                   //\n+      hlo->opcode() == HloOpcode::kCall ||                      //\n+      hlo->opcode() == HloOpcode::kCustomCall ||                //\n+      hlo->opcode() == HloOpcode::kWhile ||                     //\n+      hlo->opcode() == HloOpcode::kConditional ||               //\n+      hlo->opcode() == HloOpcode::kAsyncStart ||                //\n+      hlo->opcode() == HloOpcode::kAsyncDone ||                 //\n+      hlo->opcode() == HloOpcode::kOptimizationBarrier ||       //\n+      !alias_info_->GetInPlaceInputOutputPairs(hlo).empty() ||  //\n       hlo->HasSideEffectNoRecurse()) {\n     return absl::OkStatus();\n   }\n@@ -275,7 +279,8 @@ absl::StatusOr<bool> BFloat16ConversionFolding::RunImpl(\n                         module->ToString());\n   bool changed = false;\n   for (auto* comp : module->MakeNonfusionComputations(execution_threads)) {\n-    if (BFloat16ConversionFoldingVisitor::Run(comp, bfloat16_support_, this)) {\n+    if (BFloat16ConversionFoldingVisitor::Run(comp, bfloat16_support_,\n+                                              alias_info_, this)) {\n       changed = true;\n     }\n   }"
        },
        {
            "sha": "5504eea89ca18fd1909aeb39a51785531d00e4fd",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/bfloat16_conversion_folding.h",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fbfloat16_conversion_folding.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fbfloat16_conversion_folding.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fbfloat16_conversion_folding.h?ref=be9aa172b9679f090dc7331b1fc290618ef10e1a",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n #include \"absl/log/check.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/analysis/alias_info.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/pass/hlo_pass_interface.h\"\n #include \"xla/service/float_support.h\"\n@@ -38,8 +39,9 @@ namespace xla {\n // changed made by this pass.\n class BFloat16ConversionFolding : public HloModulePass {\n  public:\n-  explicit BFloat16ConversionFolding(const FloatSupport* bfloat16_support)\n-      : bfloat16_support_(bfloat16_support) {\n+  BFloat16ConversionFolding(const FloatSupport* bfloat16_support,\n+                            const AliasInfo* alias_info)\n+      : bfloat16_support_(bfloat16_support), alias_info_(alias_info) {\n     DCHECK(bfloat16_support->LowPrecisionType() == BF16);\n   }\n \n@@ -55,6 +57,7 @@ class BFloat16ConversionFolding : public HloModulePass {\n \n  private:\n   const FloatSupport* bfloat16_support_;\n+  const AliasInfo* alias_info_;\n };\n \n }  // namespace xla"
        },
        {
            "sha": "c51cc123a7a5c353ff4ea932bceabed7b266f9f8",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/bfloat16_conversion_folding_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fbfloat16_conversion_folding_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fbfloat16_conversion_folding_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fbfloat16_conversion_folding_test.cc?ref=be9aa172b9679f090dc7331b1fc290618ef10e1a",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <optional>\n \n #include \"absl/status/statusor.h\"\n+#include \"xla/hlo/analysis/alias_info.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -80,11 +81,12 @@ class BFloat16ConversionFoldingTest : public HloHardwareIndependentTestBase {\n \n   bool FoldConversions(HloModule* module) {\n     TestBFloat16Support bfloat16_support_;\n-    BFloat16ConversionFolding fold(&bfloat16_support_);\n+    BFloat16ConversionFolding fold(&bfloat16_support_, &alias_info_);\n     absl::StatusOr<bool> result = fold.Run(module);\n     EXPECT_IS_OK(result.status());\n     return result.value();\n   }\n+  AliasInfo alias_info_;\n };\n \n TEST_F(BFloat16ConversionFoldingTest, FoldIfSupported) {"
        },
        {
            "sha": "a088474d26d21ad71b5eef644c05b1bc991a1a91",
            "filename": "third_party/xla/xla/service/memory_space_assignment/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2FBUILD?ref=be9aa172b9679f090dc7331b1fc290618ef10e1a",
            "patch": "@@ -597,6 +597,7 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/hlo/analysis:alias_info\",\n         \"//xla/hlo/analysis:hlo_alias_analysis\",\n         \"//xla/hlo/analysis:hlo_dataflow_analysis\",\n         \"//xla/hlo/analysis:hlo_operand_index\","
        },
        {
            "sha": "6d105815e5887ddba15f3160aa4e06e22b6c1b98",
            "filename": "third_party/xla/xla/service/memory_space_assignment/algorithm.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 12,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Falgorithm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Falgorithm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Falgorithm.cc?ref=be9aa172b9679f090dc7331b1fc290618ef10e1a",
            "patch": "@@ -51,6 +51,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"xla/debug_options_flags.h\"\n+#include \"xla/hlo/analysis/alias_info.h\"\n #include \"xla/hlo/analysis/hlo_alias_analysis.h\"\n #include \"xla/hlo/analysis/hlo_dataflow_analysis.h\"\n #include \"xla/hlo/analysis/hlo_operand_index.h\"\n@@ -194,7 +195,7 @@ bool LooksLikeAnActivation(const HloInstruction* inst, bool permissive_mode) {\n // aliasing with program output.\n std::vector<HloUse> FindCrossProgramPrefetchUses(\n     absl::Span<const HloUse> buffer_uses,\n-    const HloAliasAnalysis& alias_analysis) {\n+    const HloAliasAnalysis& alias_analysis, const AliasInfo* alias_info) {\n   std::vector<HloUse> uses;\n   if (buffer_uses.empty()) {\n     return uses;\n@@ -214,7 +215,7 @@ std::vector<HloUse> FindCrossProgramPrefetchUses(\n           return false;\n         }\n         auto in_place_pairs =\n-            HloDataflowAnalysis::GetInPlaceInputOutputPairs(use.instruction);\n+            alias_info->GetInPlaceInputOutputPairs(use.instruction);\n         return absl::c_all_of(\n             in_place_pairs,\n             [&](const std::pair<HloOperandIndex, ShapeIndex>& in_place_pair) {\n@@ -238,6 +239,7 @@ std::vector<HloUse> FindCrossProgramPrefetchUses(\n \n bool IsCrossProgramPrefetchCandidate(const HloValue& value,\n                                      const HloAliasAnalysis& alias_analysis,\n+                                     const AliasInfo* alias_info,\n                                      const Options& options) {\n   // Filter out values that alias with the entry computation root.\n   const HloBuffer& buffer = alias_analysis.GetBufferContainingValue(value);\n@@ -251,7 +253,7 @@ bool IsCrossProgramPrefetchCandidate(const HloValue& value,\n     }\n   }\n   std::vector<HloUse> uses =\n-      FindCrossProgramPrefetchUses(value.GetUses(), alias_analysis);\n+      FindCrossProgramPrefetchUses(value.GetUses(), alias_analysis, alias_info);\n   return value.defining_instruction()->parent() ==\n              value.defining_instruction()->GetModule()->entry_computation() &&\n          value.defining_instruction()->opcode() == HloOpcode::kParameter &&\n@@ -320,8 +322,8 @@ struct CrossProgramPrefetches {\n };\n \n CrossProgramPrefetches FindCrossProgramPrefetches(\n-    const HloAliasAnalysis& alias_analysis, const HloLiveRange& hlo_live_range,\n-    const Options& options) {\n+    const HloAliasAnalysis& alias_analysis, const AliasInfo* alias_info,\n+    const HloLiveRange& hlo_live_range, const Options& options) {\n   CrossProgramPrefetches cross_program_prefetches;\n   for (const HloBuffer& buffer : alias_analysis.buffers()) {\n     CHECK_GE(buffer.values().size(), 1);\n@@ -331,7 +333,7 @@ CrossProgramPrefetches FindCrossProgramPrefetches(\n     if (IsUserAnnotatedCrossProgramPrefetch(*value, options)) {\n       cross_program_prefetches.prefetches.push_back(buffer_interval);\n     } else if (IsCrossProgramPrefetchCandidate(*value, alias_analysis,\n-                                               options)) {\n+                                               alias_info, options)) {\n       cross_program_prefetches.candidates.push_back(buffer_interval);\n     } else if (MemorySpaceAssignmentUtils::\n                    DoesCrossProgramPrefetchBufferMatchAnyFilter(\n@@ -393,6 +395,7 @@ bool MsaAlgorithm::MatchesPrefetchContext(\n MsaAlgorithm::MsaAlgorithm(HloModule* module, AllocationSequence* allocations,\n                            const Options& options,\n                            const HloAliasAnalysis& alias_analysis,\n+                           const AliasInfo* alias_info,\n                            const HloLiveRange& hlo_live_range)\n     : GlobalDecreasingSizeBestFitHeap(\n           options.alignment_in_bytes,\n@@ -406,6 +409,7 @@ MsaAlgorithm::MsaAlgorithm(HloModule* module, AllocationSequence* allocations,\n       allocations_(allocations),\n       options_(options),\n       alias_analysis_(alias_analysis),\n+      alias_info_(alias_info),\n       hlo_live_range_(hlo_live_range),\n       peak_memory_usage_(hlo_live_range.schedule_end_time() + 1) {\n   // Override buffer interval compare if provided.\n@@ -3067,8 +3071,8 @@ absl::StatusOr<HeapSimulator::Result<HloValue>> MsaAlgorithm::Finish() {\n   }\n   VLOG(1) << \"Memory pressure = \" << memory_pressure_;\n \n-  CrossProgramPrefetches cross_program_prefetches =\n-      FindCrossProgramPrefetches(alias_analysis_, hlo_live_range_, options_);\n+  CrossProgramPrefetches cross_program_prefetches = FindCrossProgramPrefetches(\n+      alias_analysis_, alias_info_, hlo_live_range_, options_);\n   // Return error if cross program prefetch is disabled and user has requested\n   // cross program prefetch.\n   if (!options_.enable_cross_program_prefetch &&\n@@ -3348,10 +3352,9 @@ absl::StatusOr<HeapSimulator::Result<HloValue>> MsaAlgorithm::Finish() {\n   // Run post allocation transformation and fix the allocation sequence if\n   // needed.\n   if (options_.post_allocation_transformation_fn) {\n-    auto has_in_place_user = [](HloInstruction* instr) {\n+    auto has_in_place_user = [this](HloInstruction* instr) {\n       for (HloInstruction* user : instr->users()) {\n-        auto alias_pairs =\n-            HloDataflowAnalysis::GetInPlaceInputOutputPairs(user);\n+        auto alias_pairs = alias_info_->GetInPlaceInputOutputPairs(user);\n         for (const auto& [operand_index, output_index] : alias_pairs) {\n           if (user->operand(operand_index.operand_number) == instr) {\n             return true;\n@@ -4972,7 +4975,8 @@ void MsaAlgorithm::AllocateCrossProgramPrefetchBuffer(\n \n   // Find the earliest use.\n   const auto& instruction_schedule = hlo_live_range_.instruction_schedule();\n-  auto uses = FindCrossProgramPrefetchUses(buffer->GetUses(), alias_analysis_);\n+  auto uses = FindCrossProgramPrefetchUses(buffer->GetUses(), alias_analysis_,\n+                                           alias_info_);\n   CHECK_GE(uses.size(), 1);\n   auto use_schedule_compare = [&](const HloUse& lhs, const HloUse& rhs) {\n     return instruction_schedule.at(lhs.instruction) <"
        },
        {
            "sha": "65b1ecf0da6007573261f53c7efb7579e72b118f",
            "filename": "third_party/xla/xla/service/memory_space_assignment/algorithm.h",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Falgorithm.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Falgorithm.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Falgorithm.h?ref=be9aa172b9679f090dc7331b1fc290618ef10e1a",
            "patch": "@@ -39,6 +39,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/hlo/analysis/alias_info.h\"\n #include \"xla/hlo/analysis/hlo_alias_analysis.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/utils/hlo_live_range.h\"\n@@ -314,7 +315,7 @@ class MsaAlgorithm : public GlobalDecreasingSizeBestFitHeap<HloValue> {\n  public:\n   MsaAlgorithm(HloModule* module, AllocationSequence* allocations,\n                const Options& options, const HloAliasAnalysis& alias_analysis,\n-               const HloLiveRange& hlo_live_range);\n+               const AliasInfo* alias_info, const HloLiveRange& hlo_live_range);\n \n   // Allocates a buffer in preferred memory with whole program lifetime and\n   // enables prefetching prefetch_candidate from default memory across program\n@@ -1253,6 +1254,7 @@ class MsaAlgorithm : public GlobalDecreasingSizeBestFitHeap<HloValue> {\n   absl::flat_hash_set<int64_t> edge_time_indices_;\n   const Options& options_;\n   const HloAliasAnalysis& alias_analysis_;\n+  const AliasInfo* alias_info_;\n   const HloLiveRange& hlo_live_range_;\n   std::unique_ptr<CallGraph> call_graph_;\n   // We use a interval tree to keep track of the number of outstanding"
        },
        {
            "sha": "58241c0e1472259c639e6cd55462da753aceef4c",
            "filename": "third_party/xla/xla/service/memory_space_assignment/memory_space_assignment.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/be9aa172b9679f090dc7331b1fc290618ef10e1a/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fmemory_space_assignment%2Fmemory_space_assignment.cc?ref=be9aa172b9679f090dc7331b1fc290618ef10e1a",
            "patch": "@@ -448,8 +448,9 @@ MemorySpaceAssignment::RunMemorySpaceAssignment(\n absl::Status MemorySpaceAssignment::FindAllocationSequence(\n     const HloLiveRange& hlo_live_range,\n     const HloAliasAnalysis& alias_analysis) {\n-  auto algorithm = std::make_unique<MsaAlgorithm>(\n-      module_, &allocations_, options_, alias_analysis, hlo_live_range);\n+  auto algorithm = std::make_unique<MsaAlgorithm>(module_, &allocations_,\n+                                                  options_, alias_analysis,\n+                                                  alias_info_, hlo_live_range);\n \n   HeapSimulator::Options heap_simulator_options;\n   heap_simulator_options.may_reuse_operand_buffers = false;"
        }
    ],
    "stats": {
        "total": 133,
        "additions": 77,
        "deletions": 56
    }
}