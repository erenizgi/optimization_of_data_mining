{
    "author": "golechwierowicz",
    "message": "[XLA:GPU] Documentation for LHS cost model.\n\nPiperOrigin-RevId: 810889573",
    "sha": "68d3415ffdb1c383636b6fcd775ed9ea86a01115",
    "files": [
        {
            "sha": "db9f2217666ad59c0479a6632f682ad8097464c6",
            "filename": "third_party/xla/docs/_toc.yaml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/68d3415ffdb1c383636b6fcd775ed9ea86a01115/third_party%2Fxla%2Fdocs%2F_toc.yaml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/68d3415ffdb1c383636b6fcd775ed9ea86a01115/third_party%2Fxla%2Fdocs%2F_toc.yaml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2F_toc.yaml?ref=68d3415ffdb1c383636b6fcd775ed9ea86a01115",
            "patch": "@@ -31,6 +31,8 @@ toc:\n     path: /xla/hermetic_cuda\n   - title: Indexing Analysis\n     path: /xla/indexing\n+  - title: LHS Cost Model\n+    path: /xla/lhs_cost_model\n   - title: Multi-host HLO runner\n     path: /xla/tools_multihost_hlo_runner\n   - title: Persisted autotuning"
        },
        {
            "sha": "0b48e10a9eb139ca81f0424287d50c9ea128adf8",
            "filename": "third_party/xla/docs/flags_guidance.md",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/68d3415ffdb1c383636b6fcd775ed9ea86a01115/third_party%2Fxla%2Fdocs%2Fflags_guidance.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/68d3415ffdb1c383636b6fcd775ed9ea86a01115/third_party%2Fxla%2Fdocs%2Fflags_guidance.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Fflags_guidance.md?ref=68d3415ffdb1c383636b6fcd775ed9ea86a01115",
            "patch": "@@ -44,13 +44,14 @@ should only be adjusted if you encounter HBM \"out of memory\" errors during model\n compilation. In all other scenarios, the default values are recommended, as\n altering them could adversely affect performance.\n \n-Flag                                                          | Description                                                                                                                                                                                                                                       | Default Values                                     | Suggested Values                                | Candidate Values\n-:------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :------------------------------------------------- | :---------------------------------------------- | :---------------\n-**Scheduler** <br> `xla_latency_hiding_scheduler_rerun`       | This setting adjusts the behavior of the latency-hiding scheduler. It works by incrementally reducing the memory limit allocated for scheduling with each \"rerun\" of the process.                                                                 | `xla_latency_hiding_scheduler_rerun=1`             | `xla_latency_hiding_scheduler_rerun=5`          | `0~10(it doesn’t make much sense beyond 10 reruns)`\n-**Fusion** <br> `xla_tpu_rwb_fusion`                          | This flag enables reduce+broadcast type of fusions, and may decrease memory usage.                                                                                                                                                                | `xla_tpu_rwb_fusion=true`                          | `xla_tpu_rwb_fusion=false`                      | `xla_tpu_rwb_fusion=true/false`\n-**Scheduler** <br> `xla_memory_scheduler`                     | This flag specifies the algorithm the memory scheduler will use to minimize memory consumption. Using a more advanced algorithm might get a less memory-consuming schedule, at the cost of longer compilation time.                               | `xla_memory_scheduler=kDefault`                    | `xla_memory_scheduler=kBrkga`                   | `xla_memory_scheduler=kDefault/kList/kDfs/kPostOrder/kBrkga`\n-**Scheduler** <br> `xla_tpu_enable_latency_hiding_scheduler`  | This flag enables the latency-hiding scheduler, which allows us to perform asynchronous collective instead of synchronous ones. Disabling it reduces memory usage at the cost of losing the performance gains from these asynchronous operations. | `xla_tpu_enable_latency_hiding_scheduler=true`     | `xla_tpu_enable_latency_hiding_scheduler=false` | `xla_tpu_enable_latency_hiding_scheduler=true/false`\n-**SPMD** <br> `xla_jf_spmd_threshold_for_windowed_einsum_mib` | This flag sets the lower threshold of the minimum size of the dot to trigger collective matmul. Setting it to a higher value would save memory at the cost of losing opportunities to perform collective matmul.                                  | `xla_jf_spmd_threshold_for_windowed_einsum_mib=-1` | `10Mb~1Gb (i.e. 10*1024*1024 ~ 1024*1024*1024)` | `[0, 9223372036854775807]`\n+Flag                                                                 | Description                                                                                                                                                                                                                                       | Default Values                                         | Suggested Values                                        | Candidate Values\n+:------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :----------------------------------------------------- | :------------------------------------------------------ | :---------------\n+**Scheduler** <br> `xla_latency_hiding_scheduler_rerun`              | This setting adjusts the behavior of the latency-hiding scheduler. It works by incrementally reducing the memory limit allocated for scheduling with each \"rerun\" of the process.                                                                 | `xla_latency_hiding_scheduler_rerun=1`                 | `xla_latency_hiding_scheduler_rerun=5`                  | `0~10(it doesn’t make much sense beyond 10 reruns)`\n+**Fusion** <br> `xla_tpu_rwb_fusion`                                 | This flag enables reduce+broadcast type of fusions, and may decrease memory usage.                                                                                                                                                                | `xla_tpu_rwb_fusion=true`                              | `xla_tpu_rwb_fusion=false`                              | `xla_tpu_rwb_fusion=true/false`\n+**Scheduler** <br> `xla_memory_scheduler`                            | This flag specifies the algorithm the memory scheduler will use to minimize memory consumption. Using a more advanced algorithm might get a less memory-consuming schedule, at the cost of longer compilation time.                               | `xla_memory_scheduler=kDefault`                        | `xla_memory_scheduler=kBrkga`                           | `xla_memory_scheduler=kDefault/kList/kDfs/kPostOrder/kBrkga`\n+**Scheduler** <br> `xla_tpu_enable_latency_hiding_scheduler`         | This flag enables the latency-hiding scheduler, which allows us to perform asynchronous collective instead of synchronous ones. Disabling it reduces memory usage at the cost of losing the performance gains from these asynchronous operations. | `xla_tpu_enable_latency_hiding_scheduler=true`         | `xla_tpu_enable_latency_hiding_scheduler=false`         | `xla_tpu_enable_latency_hiding_scheduler=true/false`\n+**SPMD** <br> `xla_jf_spmd_threshold_for_windowed_einsum_mib`        | This flag sets the lower threshold of the minimum size of the dot to trigger collective matmul. Setting it to a higher value would save memory at the cost of losing opportunities to perform collective matmul.                                  | `xla_jf_spmd_threshold_for_windowed_einsum_mib=-1`     | `10Mb~1Gb (i.e. 10*1024*1024 ~ 1024*1024*1024)`         | `[0, 9223372036854775807]`\n+**Scheduler** <br> `xla_gpu_enable_analytical_sol_latency_estimator` | This flag enables the analytical estimator which maximizes compute-communication overlap on GPUs.                                                                                                                                                 | `xla_gpu_enable_analytical_sol_latency_estimator=true` | `xla_gpu_enable_analytical_sol_latency_estimator=false` | `true/false`\n \n ## Other commonly used flags\n \n@@ -72,6 +73,8 @@ Flag                                            | Type                 | Notes\n | Flag | Type | Notes |\n | :---- | :---- | :----- |\n | `xla_gpu_enable_latency_hiding_scheduler` | Boolean (true/false) |This flag enables latency hiding schedulers to overlap asynchronous communication with computation efficiently. The default value is False. |\n+| `xla_gpu_enable_analytical_sol_latency_estimator` | Boolean (true/false) | Enables platform specific scheduling decisions, which in turn improve compute-communication overlap. The default value is true. |\n+| `xla_gpu_analytical_latency_estimator_options` | Structured string | Configures parameters for the `xla_gpu_enable_analytical_sol_latency_estimator`. Adjust by setting `nic_speed_gbps=$NIC_SPEED,nccl_op_launch_us=$LAUNCH_OVERHEAD,chunk_prep_us=$CHUNK_PREP,rtt_us=$RTT,chunk_size_bytes=$CHUNK_SIZE,gpus_per_node=$GPUS_PER_NODE`. The default value depends on a detected platform. |\n | `xla_gpu_enable_triton_gemm` | Boolean (true/false) | Use Triton-based matrix multiplication. |\n | `xla_gpu_enable_command_buffer` | List of CommandBufferCmdType | Which kind of commands should be captured in command buffers. |\n | `xla_gpu_all_reduce_combine_threshold_bytes` | Integer (bytes) | These flags tune when to combine multiple small AllGather / ReduceScatter / AllReduce into one big AllGather / ReduceScatter / AllReduce to reduce time spent on cross-device communication. For example, for the AllGather / ReduceScatter thresholds on a Transformer-based workload, consider tuning them high enough so as to combine at least a Transformer Layer’s weight AllGather / ReduceScatter. By default, the combine_threshold_bytes is set to 256. |"
        },
        {
            "sha": "6fe52c643b6042dd557950e69e7008c37e34da85",
            "filename": "third_party/xla/docs/lhs_cost_model.md",
            "status": "added",
            "additions": 235,
            "deletions": 0,
            "changes": 235,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/68d3415ffdb1c383636b6fcd775ed9ea86a01115/third_party%2Fxla%2Fdocs%2Flhs_cost_model.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/68d3415ffdb1c383636b6fcd775ed9ea86a01115/third_party%2Fxla%2Fdocs%2Flhs_cost_model.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Flhs_cost_model.md?ref=68d3415ffdb1c383636b6fcd775ed9ea86a01115",
            "patch": "@@ -0,0 +1,235 @@\n+# LHS Cost Model\n+\n+---\n+\n+## tldr;\n+\n+This page describes the internals of the cost model used by Latency Hiding\n+Scheduler. If you are interested in tuning the model go straight to the\n+[Tuning section](#tuning).\n+\n+The Latency Hiding Scheduler (LHS) is a compiler pass that schedules a HLO DAG\n+in a way that minimizes wall time.\n+\n+Its decisions are guided by the unified cost model, which uses a mixture of\n+performance tables and analytical models. In particular XLA embeds performance\n+tables for a GEMMs and fast-interconnect collectives, and uses analytical\n+networking and fusion cost model for other cases. The rest of the document\n+describes the inner workings of these on a high level.\n+\n+---\n+\n+## Performance tables – ICI collectives\n+\n+Performance table consist of two main components: a collector and an\n+interpolator.\n+\n+### Collector\n+\n+[The **collector**](https://github.com/openxla/xla/blob/e6a0a911eb79f540d501458f953393ede9e0048c/xla/tools/collective_perf_table_gen_main.cc#L1) is a C++ tool responsible for generating the performance\n+tables for collective operations. It measures the performance of individual HLO\n+ops (e.g., `all-gather`, `all-reduce`) across a statically defined parameter\n+space.\n+\n+#### How It Works\n+\n+The tool performs a sweep over a range of collective ops, transfer sizes, and\n+transfer schemes for a given cluster. It uses the existing multi-host HLO runner\n+infrastructure and `ExecutionProfile` data to run the generated HLO and gather\n+performance metrics.\n+\n+#### Data Collection Parameters\n+\n+Latency tables are collected for a cross-product of the following parameters:\n+\n+* **Collective Type**:\n+    * `all-reduce`\n+    * `all-gather`\n+    * `reduce-scatter`\n+* **Transfer Size**:\n+    * Logarithmic scale from 1024B up to 2GiB (e.g., 1024B, 2048B, 4096B, ...)\n+* **Transfer Scheme**:\n+    * `rail-aligned`\n+    * `non-rail-aligned`\n+\n+This sweep is run for intra-node clusters with **2, 4, and 8 devices**.\n+\n+#### Output\n+\n+The result of a collection run is a latency table in `.pbtxt` format\n+(approximately 116 KB per platform).\n+\n+### Interpolator\n+\n+[The **interpolator**](https://github.com/openxla/xla/blob/e6a0a911eb79f540d501458f953393ede9e0048c/xla/service/gpu/model/collective_interpolator.h#L40) is the compiler component that consumes the generated\n+performance tables to provide runtime estimates during compilation.\n+\n+#### Internal Data Structure\n+\n+On initialization, the Interpolator processes the performance table into a map.\n+This map uses a tuple of `(collective_type, transfer_scheme)` as its **key**.\n+\n+The **value** associated with each key is a 2D Euclidean plane. This plane\n+indexes the **network throughput** (measured by the Collector) based on two\n+axes:\n+1.  Transfer size.\n+2.  Number of devices involved.\n+\n+#### Lookup and Interpolation\n+\n+When the compiler encounters a collective operation, the Interpolator performs\n+the following steps:\n+\n+1.  It identifies the correct 2D throughput plane using the operation's `(collective_type, transfer_scheme)` as the map key.\n+2.  It then uses a **weighted average retrieval** (based on Euclidean distance) within that 2D plane, using the operation's `(transfer_size, num_devices)` as the query point.\n+3.  The result of this lookup is a single, unique **network throughput** value.\n+\n+### Rationale: Throughput and Extrapolation\n+\n+The system is designed to store **network throughput** rather than raw latency.\n+This design choice significantly simplifies extrapolating performance for\n+transfer sizes not explicitly present in the table.\n+\n+If the latency tables capture network bandwidth saturation at a collective size\n+`S`, the throughput `T` at that point is considered the maximum. For any new\n+collective of size `S'` > `S`, the runtime can be estimated as:\n+\n+<!-- mdformat off(disable mdformat for proper MathJax formatting) -->\n+\n+$$\\text{EstimatedTime}(S') = \\frac{S'}{T_{\\text{saturated}}}$$\n+\n+<!-- mdformat on -->\n+\n+This allows the model to estimate performance for collectives of any size, even\n+those larger than the 2GiB maximum measured by the Collector.\n+\n+**Important:** This extrapolation model relies on the assumption that the\n+generated latency tables **capture true network bandwidth saturation**.\n+If the tables do not contain measurements at or beyond the saturation point, the\n+interpolator will:\n+\n+* **Underestimate** the maximum throughput.\n+* Consequently, **overestimate** the runtime for large transfers.\n+\n+In general XLA:GPU teams maintains performance tables, but in cases user decide\n+to provide their own, it is the responsibility of the user generating the tables\n+to ensure they are representative and include measurements in the\n+bandwidth-saturated region for the target hardware.\n+\n+---\n+\n+## Performance tables – GEMMs\n+\n+Similar to the system for collectives, GEMM latency tables are supported by two\n+components: a **collector** and an **interpolator**.\n+\n+### Collector\n+\n+[The **collector**](https://github.com/openxla/xla/blob/e6a0a911eb79f540d501458f953393ede9e0048c/xla/tools/matmul_perf_table_gen_main.cc) is a C++ tool that computes performance tables for General\n+Matrix Multiplications (GEMMs). It measures the performance of matrix\n+multiplications at the HLO `dot` op level.\n+\n+#### How It Works\n+\n+The tool performs a sweep over a static space of GEMM dimensions (batch,\n+two non-contracting, and one contracting dimension) and data types.\n+\n+* **Default Data Types:** `LHS = bf16,f32`, `RHS = bf16,f32`, `OUT = bf16,f32`.\n+* **Infrastructure:** Re-uses the HLO op profiler.\n+\n+#### Collection Parameters\n+\n+Latency tables are collected for a cross-product of the following dimensions:\n+\n+* **batch:** `{1, 2, 4}`\n+* **m (non-contracting):** `{256, 512, ..., 4096}`\n+* **n (non-contracting):** `{256, 512, ..., 4096}`\n+* **k (contracting):** `{256, 512, ..., 4096}`\n+\n+#### Output and Storage\n+\n+A full sweep generates a `.pbtxt` latency table, ready to be consumed by\n+interpolator.\n+\n+### Interpolator\n+\n+[The **interpolator**](https://github.com/openxla/xla/blob/e6a0a911eb79f540d501458f953393ede9e0048c/xla/service/gpu/model/matmul_interpolator.h#L35) is the compiler component that uses the generated tables to estimate GEMM performance.\n+\n+#### Rationale: FLOPS Saturation\n+\n+The collected latency tables allow the interpolator to reconstruct **FLOPS** for\n+each entry:\n+\n+<!-- mdformat off(disable mdformat for proper MathJax formatting) -->\n+\n+$$\\text{FLOPS} = \\frac{2 \\times b \\times m \\times n \\times k}{\\text{runtime}}$$\n+\n+<!-- mdformat on -->\n+\n+A key insight is that FLOPS **saturate** at a certain point; that is, the\n+hardware reaches peak FLOPS beyond a certain matrix shape. This saturation\n+allows the use of the same extrapolation method employed for collectives.\n+\n+#### Lookup and Interpolation\n+\n+The interpolator builds a **4D Euclidean space** from the table data. To provide\n+a performance estimate, it performs a **weighted-average interpolation** within\n+this 4D space. If there's no table for a certain data type, as a heuristic each\n+dimension is normalized to the number of bytes.\n+\n+---\n+\n+## Analytical Cost Model - DCN\n+\n+### S-curve Collective Cost Model\n+\n+The **S-curve** model is a fully analytical networking roofline model.\n+\n+#### Overview\n+\n+The model is designed to estimate the performance of collective operations based\n+on a set of fixed network properties.\n+\n+#### Model Inputs\n+\n+The model requires two categories of inputs:\n+\n+1.  **Fixed Network Properties (User-Defined):**\n+    * Collective launch overhead\n+    * NIC speed\n+    * RTT (round trip time)\n+\n+    By default, XLA auto-detects a platform and uses values for the most common\n+    architectures. These properties are configurable by the user. See\n+    [Tuning section](#tuning) for details.\n+\n+2.  **Per-Collective Inputs:**\n+    * Collective type (e.g., `AllGather`, `ReduceScatter`)\n+    * Transfer size\n+    * Number of nodes involved in the communication\n+\n+#### Integration\n+\n+The S-curve model is integrated into `XLA:GPU` and is being used on Hopper, and\n+Blackwell.\n+\n+---\n+\n+## Analytical Cost Model - Fusions\n+\n+For other kernels we rely on the [GPU performance cost model](https://github.com/openxla/xla/blob/e6a0a911eb79f540d501458f953393ede9e0048c/xla/service/gpu/model/gpu_performance_model.h) to estimate the\n+right runtimes. You can read more about it [here](https://github.com/openxla/xla/discussions/10065).\n+\n+---\n+\n+## Tuning\n+\n+S-curve model can be tuned by issuing right XLA flags. Default configuration\n+should be good enough in majority of cases, but the model control is exposed in\n+other cases.\n+\n+```\n+export NIC_SPEED_GBPS=... # NIC speed per GPU in Gigabytes\n+export GPUS_PER_NODE=... # Num of GPUs per cluster interconnected with fast network (e.g. NVLINK)\n+export XLA_FLAGS=--xla_gpu_analytical_latency_estimator_options=\"nic_speed_gbps=$NIC_SPEED_GBPS,gpus_per_node=$GPUS_PER_NODE\"\n+```"
        }
    ],
    "stats": {
        "total": 254,
        "additions": 247,
        "deletions": 7
    }
}