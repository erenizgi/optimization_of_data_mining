{
    "author": "basioli-k",
    "message": "[XLA:CPU] Use distinct element values in tiled kernel tests\n\nUsing random values is not best practice.\nUsing inputs made out of the same elements isn't a good test for ops like transpose.\n\nPiperOrigin-RevId: 837188191",
    "sha": "f0936415ab0021b6d8bfd49bc7127abbdbf69170",
    "files": [
        {
            "sha": "061b5ad8e0e1ebd93d863b1e1f7acd37690d2eaa",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/tiled_kernel_test.py",
            "status": "modified",
            "additions": 24,
            "deletions": 20,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f0936415ab0021b6d8bfd49bc7127abbdbf69170/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_kernel_test.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f0936415ab0021b6d8bfd49bc7127abbdbf69170/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_kernel_test.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_kernel_test.py?ref=f0936415ab0021b6d8bfd49bc7127abbdbf69170",
            "patch": "@@ -28,9 +28,13 @@\n \n class InputSpec:\n \n-  def __init__(self, shape: tuple[int, ...], input_value):\n+  def __init__(self, shape: tuple[int, ...]):\n+    \"\"\"Initializes the InputSpec.\n+\n+    Args:\n+      shape: The shape of the input array.\n+    \"\"\"\n     self.shape = shape\n-    self.input_value = input_value\n \n \n def get_random_array(shape: tuple[int, ...], dtype: np.dtype) -> np.ndarray:\n@@ -58,12 +62,8 @@ def compare_kernel(\n       cpu_testlib.JitCompiler(base_testlib.HloModuleConfig()),\n   )\n \n-  # Simply use a all-ones arrays as inputs to make it easy to debug the kernel\n-  # unless random inputs are requested.\n   def get_input(spec: InputSpec):\n-    if spec.input_value is None:\n-      return get_random_array(spec.shape, dtype)\n-    return np.full(shape=spec.shape, fill_value=spec.input_value, dtype=dtype)\n+    return np.arange(np.prod(spec.shape), dtype=dtype).reshape(spec.shape)\n \n   inputs = [get_input(spec) for spec in input_specs]\n \n@@ -105,7 +105,7 @@ def test_slice(self):\n         ir,\n         \"tiled_slice\",\n         1,\n-        [InputSpec((5, 5), 1)],\n+        [InputSpec((5, 5))],\n         (5, 5),\n         np.float32,\n         lambda arg: arg.transpose(),\n@@ -129,7 +129,7 @@ def test_strided(self):\n         ir,\n         \"tiled_slice\",\n         1,\n-        [InputSpec((64, 64), 1)],\n+        [InputSpec((64, 64))],\n         (4, 32),\n         np.float32,\n         lambda arg: arg[::21, ::2],\n@@ -156,7 +156,7 @@ def test_transpose(self):\n         ir,\n         \"tiled_transpose\",\n         8,\n-        [InputSpec((4096, 4096), 1)],\n+        [InputSpec((4096, 4096))],\n         (4096, 4096),\n         np.float32,\n         lambda arg: arg.transpose(),\n@@ -185,7 +185,7 @@ def test_add_tranpose(self):\n         ir,\n         \"add_tranpose\",\n         8,\n-        [InputSpec((4096, 4096), 1)],\n+        [InputSpec((4096, 4096))],\n         (4096, 4096),\n         np.float32,\n         lambda arg: arg + arg.transpose(),\n@@ -213,7 +213,7 @@ def test_dot_single_tile(self):\n         ir,\n         \"dot_single_tile\",\n         1,\n-        [InputSpec((8, 16), 1), InputSpec((16, 8), 1)],\n+        [InputSpec((8, 16)), InputSpec((16, 8))],\n         (8, 8),\n         np.float32,\n         lambda lhs, rhs: lhs @ rhs,\n@@ -242,7 +242,7 @@ def test_dot_scalar_output(self):\n         ir,\n         \"test_dot_scalar_output\",\n         1,\n-        [InputSpec((8, 16), 1), InputSpec((16, 8), 1)],\n+        [InputSpec((8, 16)), InputSpec((16, 8))],\n         (),\n         np.float32,\n         lambda lhs, rhs: np.tensordot(lhs, rhs, axes=[[1, 0], [0, 1]]),\n@@ -275,7 +275,11 @@ def test_dot_fusion_single_tile(self):\n         ir,\n         \"dot_fusion_single_tile\",\n         1,\n-        [InputSpec((8, 16), 1), InputSpec((8, 16), 1), InputSpec((16, 1), 1)],\n+        [\n+            InputSpec((8, 16)),\n+            InputSpec((8, 16)),\n+            InputSpec((16, 1)),\n+        ],\n         (8, 1),\n         np.float32,\n         lambda lhs_0, lhs_1, rhs: np.tanh((lhs_0 + lhs_1) @ rhs),\n@@ -312,7 +316,7 @@ def test_reduction_add_inner(self):\n         ir,\n         \"reduction_add_inner\",\n         4,\n-        [InputSpec((1024, 32), 1), InputSpec((1,), 0)],\n+        [InputSpec((1024, 32)), InputSpec((1,))],\n         (1024,),\n         np.int32,\n         lambda input, init: np.sum(input, axis=1) + init,\n@@ -348,7 +352,7 @@ def test_reduction_add_outer(self):\n         ir,\n         \"reduction_add_outer\",\n         4,\n-        [InputSpec((1024, 32), 1), InputSpec((1,), 0)],\n+        [InputSpec((1024, 32)), InputSpec((1,))],\n         (32,),\n         np.float32,\n         lambda input, init: np.sum(input, axis=0),\n@@ -381,7 +385,7 @@ def test_reduction_middle(self):\n         ir,\n         \"reduction_add_middle\",\n         1,\n-        [InputSpec((8, 4, 2), 1), InputSpec((1,), 0)],\n+        [InputSpec((8, 4, 2)), InputSpec((1,))],\n         (8, 2),\n         np.float32,\n         lambda input, init: np.sum(input, axis=1),\n@@ -414,7 +418,7 @@ def test_reduction_outer_inner(self):\n         ir,\n         \"reduction_add_outer_inner\",\n         1,\n-        [InputSpec((8, 4, 2), 1), InputSpec((1,), 0)],\n+        [InputSpec((8, 4, 2)), InputSpec((1,))],\n         (4,),\n         np.float32,\n         lambda input, init: np.sum(input, axis=(0, 2)),\n@@ -439,7 +443,7 @@ def test_broadcast_in_dim_inner(self):\n         ir,\n         \"broadcast_in_dim_inner\",\n         1,\n-        [InputSpec((4,), None)],\n+        [InputSpec((4,))],\n         (32, 4),\n         np.float32,\n         lambda input: np.broadcast_to(input, (32, 4)),\n@@ -464,7 +468,7 @@ def test_broadcast_in_dim_outer(self):\n         ir,\n         \"broadcast_in_dim_outer\",\n         1,\n-        [InputSpec((4,), None)],\n+        [InputSpec((4,))],\n         (4, 32),\n         np.float32,\n         lambda input: np.transpose(np.broadcast_to(input, (32, 4))),"
        }
    ],
    "stats": {
        "total": 44,
        "additions": 24,
        "deletions": 20
    }
}