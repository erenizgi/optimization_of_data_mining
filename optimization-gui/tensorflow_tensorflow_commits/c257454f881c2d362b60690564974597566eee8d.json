{
    "author": "ezhulenev",
    "message": "[xla:gpu] Add GpuCliqueRendezvous for synchronizing local gpu cliques\n\nThis is a building block for multiple places in XLA:GPU runtime where we need to exchange data between all ranks of a local GpuClique.\n\nPiperOrigin-RevId: 840935802",
    "sha": "c257454f881c2d362b60690564974597566eee8d",
    "files": [
        {
            "sha": "5b85208264fad6fa42cd65fd5a02bb6d16b478c7",
            "filename": "third_party/xla/xla/backends/gpu/collectives/BUILD",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c257454f881c2d362b60690564974597566eee8d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c257454f881c2d362b60690564974597566eee8d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2FBUILD?ref=c257454f881c2d362b60690564974597566eee8d",
            "patch": "@@ -139,6 +139,42 @@ xla_cc_test(\n     ],\n )\n \n+cc_library(\n+    name = \"gpu_clique_rendezvous\",\n+    srcs = [\"gpu_clique_rendezvous.cc\"],\n+    hdrs = [\"gpu_clique_rendezvous.h\"],\n+    deps = [\n+        \":gpu_clique_key\",\n+        \"//xla:util\",\n+        \"//xla/core/collectives:rank_id\",\n+        \"//xla/service:rendezvous\",\n+        \"@com_google_absl//absl/container:btree\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"gpu_clique_rendezvous_test\",\n+    srcs = [\"gpu_clique_rendezvous_test.cc\"],\n+    deps = [\n+        \":gpu_clique_key\",\n+        \":gpu_clique_rendezvous\",\n+        \"//xla/core/collectives:rank_id\",\n+        \"//xla/runtime:device_id\",\n+        \"//xla/service:rendezvous\",\n+        \"//xla/tsl/platform:env\",\n+        \"//xla/tsl/platform:test\",\n+        \"//xla/tsl/platform:test_main\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_googletest//:gtest\",\n+    ],\n+)\n+\n cc_library(\n     name = \"gpu_cliques\",\n     srcs = [\"gpu_cliques.cc\"],"
        },
        {
            "sha": "277cea6198ae803f2097e8a6063b3e8eb36c2ab1",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_clique_rendezvous.cc",
            "status": "added",
            "additions": 108,
            "deletions": 0,
            "changes": 108,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c257454f881c2d362b60690564974597566eee8d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_rendezvous.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c257454f881c2d362b60690564974597566eee8d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_rendezvous.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_rendezvous.cc?ref=c257454f881c2d362b60690564974597566eee8d",
            "patch": "@@ -0,0 +1,108 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/collectives/gpu_clique_rendezvous.h\"\n+\n+#include <any>\n+#include <cstdint>\n+#include <memory>\n+#include <string>\n+#include <utility>\n+\n+#include \"absl/container/btree_map.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/strings/str_join.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/service/rendezvous.h\"\n+#include \"xla/util.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+// Wrap GpuCliqueKey into a unique struct to guarantee we do not accidentally\n+// try to run multiple unrelated rendezvous for a same key.\n+struct GpuCliqueRendezvousKey {\n+  GpuCliqueKey clique_key;\n+\n+  bool operator==(const GpuCliqueRendezvousKey& other) const {\n+    return clique_key == other.clique_key;\n+  }\n+\n+  template <typename H>\n+  friend H AbslHashValue(H h, const GpuCliqueRendezvousKey& key) {\n+    return H::combine(std::move(h), key.clique_key);\n+  }\n+};\n+\n+struct RankData {\n+  RankId rank;\n+  std::any data;\n+};\n+\n+struct RankFormatter {\n+  void operator()(std::string* out, const RankData* param) const {\n+    absl::StrAppend(out, param->rank.value());\n+  }\n+};\n+\n+}  // namespace\n+\n+GpuCliqueRendezvous::GpuCliqueRendezvous(\n+    GpuCliqueKey clique_key, absl::btree_map<RankId, std::any> state)\n+    : clique_key_(std::move(clique_key)), state_(std::move(state)) {}\n+\n+absl::StatusOr<std::shared_ptr<GpuCliqueRendezvous>> GpuCliqueRendezvous::Join(\n+    const GpuCliqueKey& clique_key, RankId rank, std::any data) {\n+  if (!clique_key.is_local()) {\n+    return InvalidArgument(\n+        \"GpuClique rendezvous is not supported for non-local cliques\");\n+  }\n+\n+  VLOG(3) << absl::StrFormat(\"rank=[%d] Join gpu clique rendezvous: %s\",\n+                             rank.value(), clique_key.ToString());\n+\n+  std::string rendezvous_name =\n+      absl::StrFormat(\"GpuCliqueRendezvous for %s\", clique_key.ToString());\n+  GpuCliqueRendezvousKey rendezvous_key = {clique_key};\n+  RankData rendezvous_value = {rank, std::move(data)};\n+\n+  // A callback for rendezvous to construct the GpuCliqueRendezvous.\n+  auto callback = [&](absl::Span<const RankData*> values) {\n+    VLOG(3) << absl::StrFormat(\"ranks=[%s] Complete gpu clique rendezvous: %s\",\n+                               absl::StrJoin(values, \",\", RankFormatter{}),\n+                               clique_key.ToString());\n+\n+    absl::btree_map<RankId, std::any> data;\n+    for (const auto* value : values) {\n+      data[value->rank] = std::move(value->data);\n+    }\n+\n+    return GpuCliqueRendezvous(clique_key, std::move(data));\n+  };\n+\n+  // We expect that all local participants will collectively allocate the\n+  // multicast memory.\n+  int64_t num_participants = clique_key.num_local_participants();\n+  return Rendezvous<GpuCliqueRendezvous>(rendezvous_name, rendezvous_key,\n+                                         rendezvous_value, num_participants,\n+                                         callback);\n+}\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "a3220996f214c5bb6039e88a78837d13680a8dc0",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_clique_rendezvous.h",
            "status": "added",
            "additions": 75,
            "deletions": 0,
            "changes": 75,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c257454f881c2d362b60690564974597566eee8d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_rendezvous.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c257454f881c2d362b60690564974597566eee8d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_rendezvous.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_rendezvous.h?ref=c257454f881c2d362b60690564974597566eee8d",
            "patch": "@@ -0,0 +1,75 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_COLLECTIVES_GPU_CLIQUE_RENDEZVOUS_H_\n+#define XLA_BACKENDS_GPU_COLLECTIVES_GPU_CLIQUE_RENDEZVOUS_H_\n+\n+#include <any>\n+#include <functional>\n+#include <memory>\n+\n+#include \"absl/container/btree_map.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/util.h\"\n+\n+namespace xla::gpu {\n+\n+// A result of a rendezvous for a given clique key, where each participant is\n+// expected to join the rendezvous together with a local data that it wants to\n+// share with other participants. Supported only for local cliques.\n+class GpuCliqueRendezvous {\n+ public:\n+  // Joins a rendezvous for the given clique key and return the associated data.\n+  //\n+  // This is a collective operation that must be called concurrently by all\n+  // participating devices in the clique. Implementation relies on the\n+  // rendezvous synchronization to ensure that all ranks arrive to the barrier.\n+  // The result is collectively owned by all participants.\n+  static absl::StatusOr<std::shared_ptr<GpuCliqueRendezvous>> Join(\n+      const GpuCliqueKey& clique_key, RankId rank, std::any data);\n+\n+  // Returns the clique key associated with this data.\n+  const GpuCliqueKey& clique_key() const { return clique_key_; }\n+\n+  // Returns the state associated with the given rank. If state type is not\n+  // the same as `T`, returns an error.\n+  template <typename T>\n+  absl::StatusOr<std::reference_wrapper<const T>> state(RankId rank) const {\n+    auto it = state_.find(rank);\n+    if (it == state_.end()) {\n+      return NotFound(\"Data not found for rank %d\", rank.value());\n+    }\n+\n+    if (std::any_cast<T>(&it->second) == nullptr) {\n+      return InvalidArgument(\"Data type mismatch for rank %d\", rank.value());\n+    }\n+\n+    const T& data = std::any_cast<const T&>(it->second);\n+    return std::reference_wrapper<const T>(data);\n+  }\n+\n+ private:\n+  GpuCliqueRendezvous(GpuCliqueKey clique_key,\n+                      absl::btree_map<RankId, std::any> state);\n+\n+  GpuCliqueKey clique_key_;\n+  absl::btree_map<RankId, std::any> state_;\n+};\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_BACKENDS_GPU_COLLECTIVES_GPU_CLIQUE_RENDEZVOUS_H_"
        },
        {
            "sha": "58d0bd7b2c402aa5b2c7031555d19a2452344072",
            "filename": "third_party/xla/xla/backends/gpu/collectives/gpu_clique_rendezvous_test.cc",
            "status": "added",
            "additions": 61,
            "deletions": 0,
            "changes": 61,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c257454f881c2d362b60690564974597566eee8d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_rendezvous_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c257454f881c2d362b60690564974597566eee8d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_rendezvous_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fgpu_clique_rendezvous_test.cc?ref=c257454f881c2d362b60690564974597566eee8d",
            "patch": "@@ -0,0 +1,61 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/collectives/gpu_clique_rendezvous.h\"\n+\n+#include <any>\n+#include <cstdint>\n+#include <memory>\n+#include <vector>\n+\n+#include <gtest/gtest.h>\n+#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n+#include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/runtime/device_id.h\"\n+#include \"xla/tsl/platform/env.h\"\n+#include \"xla/tsl/platform/test.h\"\n+#include \"xla/tsl/platform/threadpool.h\"\n+\n+namespace xla::gpu {\n+\n+tsl::thread::ThreadPool CreateThreadPool(int32_t size) {\n+  return tsl::thread::ThreadPool(tsl::Env::Default(), \"rendezvous_test\", size);\n+}\n+\n+TEST(GpuCliqueRendezvousTest, TwoParticipants) {\n+  GlobalDeviceId id0 = GlobalDeviceId(0);\n+  GlobalDeviceId id1 = GlobalDeviceId(1);\n+\n+  GpuCliqueKey key({id0, id1}, /*num_local_participants=*/2, false);\n+\n+  auto task = [&](int32_t id) {\n+    return [&, id] {\n+      auto rendezvous = GpuCliqueRendezvous::Join(key, RankId(id),\n+                                                  std::make_any<int32_t>(id));\n+      ASSERT_OK(rendezvous);\n+\n+      GpuCliqueRendezvous& data = **rendezvous;\n+      ASSERT_EQ(data.clique_key(), key);\n+      ASSERT_EQ(*data.state<int32_t>(RankId(0)), 0);\n+      ASSERT_EQ(*data.state<int32_t>(RankId(1)), 1);\n+    };\n+  };\n+\n+  auto thread_pool = CreateThreadPool(2);\n+  thread_pool.Schedule(task(0));\n+  thread_pool.Schedule(task(1));\n+}\n+\n+}  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 280,
        "additions": 280,
        "deletions": 0
    }
}