{
    "author": "pifon2a",
    "message": "[XLA:GPU] Get rid of the nested ThunkEmitters.\n\nPiperOrigin-RevId: 837631551",
    "sha": "fa512f62ea2c85c8df85ce93326a63cc2ff6864c",
    "files": [
        {
            "sha": "3312f86ee96d2858da2c620b97493933b2e58c6b",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fa512f62ea2c85c8df85ce93326a63cc2ff6864c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fa512f62ea2c85c8df85ce93326a63cc2ff6864c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=fa512f62ea2c85c8df85ce93326a63cc2ff6864c",
            "patch": "@@ -535,6 +535,7 @@ cc_library(\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/mlir/utils:error_util\",\n         \"//xla/mlir_hlo:transforms_gpu_passes\",\n+        \"//xla/runtime:device_id\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:call_graph\",\n         \"//xla/service:collective_ops_utils\",\n@@ -1459,8 +1460,8 @@ cc_library(\n         \"//xla:xla_proto_cc\",\n         \"//xla/backends/gpu/runtime:runtime_intrinsics\",\n         \"//xla/backends/gpu/runtime:sequential_thunk\",\n+        \"//xla/backends/gpu/runtime:thunk\",\n         \"//xla/hlo/analysis:hlo_ordering\",\n-        \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:buffer_value\","
        },
        {
            "sha": "7536f2dc164406012ec095c6c0c726e542223c25",
            "filename": "third_party/xla/xla/service/gpu/compile_module_to_llvm_ir.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fa512f62ea2c85c8df85ce93326a63cc2ff6864c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fa512f62ea2c85c8df85ce93326a63cc2ff6864c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc?ref=fa512f62ea2c85c8df85ce93326a63cc2ff6864c",
            "patch": "@@ -47,6 +47,7 @@ limitations under the License.\n #include \"mlir/Support/LLVM.h\"\n #include \"mlir/Support/LogicalResult.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/hlo/analysis/hlo_ordering.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -192,11 +193,11 @@ absl::StatusOr<std::unique_ptr<SequentialThunk>> LowerHlo(\n   }\n   std::unique_ptr<ThunkEmitter> thunk_emitter =\n       ThunkEmitter::Create(&ir_emitter_context);\n-  {\n     XLA_SCOPED_LOGGING_TIMER(absl::StrCat(\n         \"GpuCompiler::RunBackend - IR emission for \", hlo_module->name()));\n \n-    TF_RETURN_IF_ERROR(thunk_emitter->EmitHloEntryComputation(hlo_module));\n+    TF_ASSIGN_OR_RETURN(auto thunks,\n+                        thunk_emitter->EmitHloEntryComputation(hlo_module));\n \n     RemoveUnusedAndUninitializedGlobals(\n         platform_id, options, ir_emitter_context.llvm_module_constants(),\n@@ -206,8 +207,8 @@ absl::StatusOr<std::unique_ptr<SequentialThunk>> LowerHlo(\n     // out we have no way of telling how far through the process we got).\n     uint64_t end_usecs = tsl::Env::Default()->NowMicros();\n     RecordHloToLlvmDuration(end_usecs - start_usecs);\n-  }\n-  return thunk_emitter->ConsumeThunkSequence();\n+    return std::make_unique<SequentialThunk>(Thunk::ThunkInfo{},\n+                                             std::move(thunks));\n }\n \n }  // namespace"
        },
        {
            "sha": "dc638f983331fddc525f5ca6127477f82901aa63",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fa512f62ea2c85c8df85ce93326a63cc2ff6864c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fa512f62ea2c85c8df85ce93326a63cc2ff6864c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=fa512f62ea2c85c8df85ce93326a63cc2ff6864c",
            "patch": "@@ -3031,8 +3031,9 @@ GpuCompiler::LoadExecutableFromAotResult(\n     TF_RETURN_IF_ERROR(LoadCache(ir_emitter_context, cache_file_path));\n   }\n \n-  auto ir_emitter = ThunkEmitter::Create(&ir_emitter_context);\n-  TF_RETURN_IF_ERROR(ir_emitter->EmitHloEntryComputation(hlo_module.get()));\n+  auto thunk_emitter = ThunkEmitter::Create(&ir_emitter_context);\n+  TF_ASSIGN_OR_RETURN(auto thunks,\n+                      thunk_emitter->EmitHloEntryComputation(hlo_module.get()));\n \n   // Get all other fields required by GpuExecutable.\n   std::vector<GpuExecutable::ConstantInfo> constants =\n@@ -3054,7 +3055,9 @@ GpuCompiler::LoadExecutableFromAotResult(\n         /*dnn_compiled_graphs=*/\n         BinaryMap(proto.dnn_compiled_graphs().cbegin(),\n                   proto.dnn_compiled_graphs().cend()),\n-        /*executable=*/ir_emitter->ConsumeThunkSequence(),\n+        /*executable=*/\n+        std::make_unique<SequentialThunk>(Thunk::ThunkInfo{},\n+                                          std::move(thunks)),\n         /*constants=*/std::move(constants),\n         /*output_info=*/std::move(output_info),\n         /*module_name=*/std::move(hlo_module_name),"
        },
        {
            "sha": "7c345c7e2bfa119e5fb789a1c6ab6f7c040712b2",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 269,
            "deletions": 277,
            "changes": 546,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fa512f62ea2c85c8df85ce93326a63cc2ff6864c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fa512f62ea2c85c8df85ce93326a63cc2ff6864c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc?ref=fa512f62ea2c85c8df85ce93326a63cc2ff6864c",
            "patch": "@@ -15,6 +15,17 @@ limitations under the License.\n \n #include \"xla/service/gpu/thunk_emitter.h\"\n \n+#include <cstddef>\n+#include <cstdint>\n+#include <iterator>\n+#include <memory>\n+#include <numeric>\n+#include <optional>\n+#include <string>\n+#include <tuple>\n+#include <utility>\n+#include <vector>\n+\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/container/inlined_vector.h\"\n@@ -117,6 +128,7 @@ limitations under the License.\n #include \"xla/mlir/utils/error_util.h\"\n #include \"xla/mlir_hlo/transforms/gpu_passes.h\"\n #include \"xla/primitive_util.h\"\n+#include \"xla/runtime/device_id.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/call_graph.h\"\n #include \"xla/service/collective_ops_utils.h\"\n@@ -247,35 +259,46 @@ absl::Status ThunkEmitter::EmitConstant(const HloConstantInstruction* instr) {\n   return absl::OkStatus();\n }\n \n-absl::Status ThunkEmitter::EmitConditional(const HloInstruction* instr) {\n+ThunkSequence GetThunkSequence(std::unique_ptr<Thunk> ir_emitter) {\n+  ThunkSequence thunk_sequence;\n+  thunk_sequence.push_back(std::move(ir_emitter));\n+  return thunk_sequence;\n+}\n+\n+void AppendThunkSequence(ThunkSequence& thunks,\n+                         ThunkSequence& additional_thunks) {\n+  thunks.insert(thunks.end(),\n+                std::make_move_iterator(additional_thunks.begin()),\n+                std::make_move_iterator(additional_thunks.end()));\n+}\n+\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitConditional(\n+    const HloInstruction* instr) {\n   std::vector<std::unique_ptr<SequentialThunk>> branch_thunks;\n   branch_thunks.reserve(instr->branch_count());\n-\n   for (auto comp : instr->branch_computations()) {\n-    auto ir_emitter = ThunkEmitter::Create(ir_emitter_context_);\n-    TF_RETURN_IF_ERROR(ir_emitter->EmitHloComputation(comp));\n+    TF_ASSIGN_OR_RETURN(auto thunk_sequence, EmitHloComputation(comp));\n     Thunk::ThunkInfo branch_thunk_info =\n         Thunk::ThunkInfo::WithProfileAnnotation(\n             instr, ir_emitter_context_->GetNextThunkId());\n     branch_thunk_info.profile_annotation +=\n         absl::StrCat(\"_branch_\", comp->name());\n-    branch_thunks.push_back(\n-        ir_emitter->ConsumeThunkSequence(branch_thunk_info));\n+    branch_thunks.push_back(std::make_unique<SequentialThunk>(\n+        branch_thunk_info, std::move(thunk_sequence)));\n   }\n-\n   TF_ASSIGN_OR_RETURN(auto slice,\n                       GetAllocationSliceForHlo(instr->operand(0), {}));\n   bool branch_index_is_bool = instr->operand(0)->shape().element_type() == PRED;\n-  AddThunkToThunkSequence(std::unique_ptr<Thunk>(new ConditionalThunk(\n+\n+  return GetThunkSequence(std::make_unique<ConditionalThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n-      slice, std::move(branch_thunks), branch_index_is_bool)));\n-  return absl::OkStatus();\n+      slice, std::move(branch_thunks), branch_index_is_bool));\n }\n \n // Input = {dynamic array(with dynamic dimension meta data at the\n // end)} Output = {static array, dynamic_dim0, dynamic_dim1}\n-absl::Status ThunkEmitter::EmitPadToStatic(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitPadToStatic(\n     const HloCustomCallInstruction* instr) {\n   std::string ir_name = std::string(instr->name());\n   auto local_llvm_module =\n@@ -284,46 +307,37 @@ absl::Status ThunkEmitter::EmitPadToStatic(\n   TF_ASSIGN_OR_RETURN(auto thunk_sequence,\n                       EmitPadToStaticLLVMIR(instr, local_llvm_module.get(),\n                                             ir_emitter_context_));\n-  for (auto& thunk : thunk_sequence) {\n-    AddThunkToThunkSequence(std::move(thunk));\n-  }\n   CHECK(!llvm::Linker::linkModules(*ir_emitter_context_->llvm_module(),\n                                    std::move(local_llvm_module),\n                                    llvm::Linker::Flags::OverrideFromSrc));\n-  return absl::OkStatus();\n+  return thunk_sequence;\n }\n \n // Input = {dynamic array(with dynamic dimension meta data at the\n // end)} Output = {static array, dynamic_dim0, dynamic_dim1}\n-absl::Status ThunkEmitter::EmitSliceToDynamic(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitSliceToDynamic(\n     const HloCustomCallInstruction* instr) {\n   std::string ir_name = std::string(instr->name());\n   auto local_llvm_module =\n       CreateLocalLLVMModule(ir_name, ir_emitter_context_->llvm_module());\n   TF_ASSIGN_OR_RETURN(auto thunk_sequence,\n                       EmitSliceToDynamicLLVMIR(instr, local_llvm_module.get(),\n                                                ir_emitter_context_));\n-  for (auto& thunk : thunk_sequence) {\n-    AddThunkToThunkSequence(std::move(thunk));\n-  }\n   CHECK(!llvm::Linker::linkModules(*ir_emitter_context_->llvm_module(),\n                                    std::move(local_llvm_module),\n                                    llvm::Linker::Flags::OverrideFromSrc));\n-  return absl::OkStatus();\n+  return thunk_sequence;\n }\n \n-absl::Status ThunkEmitter::EmitCommandBufferThunk(const HloInstruction* instr) {\n-  // Spawn a new ThunkEmitter to emit thunks for the command\n-  // buffer computation. Then convert emitted thunks to a sequence\n-  // of CommandBufferCmd. The resulting thunk added to the thunk\n-  // sequence is a CommandBufferThunk. Thunks emitted from the\n-  // command buffer computation are discarded.\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCommandBufferThunk(\n+    const HloInstruction* instr) {\n+  // Spawn a new ThunkEmitter to emit thunks for the command buffer computation.\n+  // Then convert emitted thunks to a sequence of CommandBufferCmd. The\n+  // resulting thunk added to the thunk sequence is a CommandBufferThunk. Thunks\n+  // emitted from the command buffer computation are discarded.\n   DCHECK_EQ(instr->called_computations().size(), 1);\n   const HloComputation* command_buffer = instr->called_computations().front();\n-  auto ir_emitter = ThunkEmitter::Create(ir_emitter_context_);\n-  TF_RETURN_IF_ERROR(ir_emitter->EmitHloComputation(command_buffer));\n-  std::unique_ptr<SequentialThunk> thunk_sequence =\n-      ir_emitter->ConsumeThunkSequence();\n+  TF_ASSIGN_OR_RETURN(auto thunk_sequence, EmitHloComputation(command_buffer));\n \n   // Maybe serialize all commands in a sequence by forcing barriers\n   // between all recorded commands. This guarantees that we execute\n@@ -354,20 +368,20 @@ absl::Status ThunkEmitter::EmitCommandBufferThunk(const HloInstruction* instr) {\n   TF_ASSIGN_OR_RETURN(\n       CommandBufferCmdExecutor cmd_executor,\n       ConvertToCommands(\n-          thunk_sequence->thunks(),\n+          thunk_sequence,\n           ConvertToCommandsOptions{synchronization_mode, enable_loop_unroll}));\n \n-  AddThunkToThunkSequence(std::make_unique<CommandBufferThunk>(\n+  return GetThunkSequence(std::make_unique<CommandBufferThunk>(\n       std::move(cmd_executor),\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n-      std::move(thunk_sequence),\n+      std::make_unique<SequentialThunk>(Thunk::ThunkInfo{},\n+                                        std::move(thunk_sequence)),\n       ir_emitter_context_->debug_options()\n           .xla_enable_command_buffers_during_profiling()));\n-  return absl::OkStatus();\n }\n \n-absl::Status ThunkEmitter::EmitConvolutionThunk(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitConvolutionThunk(\n     const HloCustomCallInstruction* instr) {\n   std::vector<BufferAllocation::Slice> operand_slices;\n   operand_slices.reserve(instr->operand_count());\n@@ -377,9 +391,9 @@ absl::Status ThunkEmitter::EmitConvolutionThunk(\n     operand_slices.push_back(slice);\n   }\n \n-  // The first and the last element in the result tuple for a\n-  // convolution are always the result and the scratch buffer. It\n-  // may have auxiliary results in addition to the main result.\n+  // The first and the last element in the result tuple for a convolution are\n+  // always the result and the scratch buffer. It may have auxiliary results in\n+  // addition to the main result.\n   std::vector<BufferAllocation::Slice> result_slices;\n   for (int i = 0; i < instr->shape().tuple_shapes().size() - 1; i++) {\n     TF_ASSIGN_OR_RETURN(BufferAllocation::Slice result_slice,\n@@ -404,18 +418,16 @@ absl::Status ThunkEmitter::EmitConvolutionThunk(\n                                   instr->window(),\n                                   instr->convolution_dimension_numbers(),\n                                   instr->feature_group_count()};\n-\n-  TF_ASSIGN_OR_RETURN(std::unique_ptr<ConvolutionThunk> thunk,\n+  TF_ASSIGN_OR_RETURN(auto thunk,\n                       ConvolutionThunk::Create(\n                           Thunk::ThunkInfo::WithProfileAnnotation(\n                               instr, ir_emitter_context_->GetNextThunkId()),\n                           std::move(descriptor), std::move(operand_slices),\n                           std::move(result_slices), scratch_slice));\n-  AddThunkToThunkSequence(std::move(thunk));\n-  return absl::OkStatus();\n+  return GetThunkSequence(std::move(thunk));\n }\n \n-absl::Status ThunkEmitter::EmitGemmThunk(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitGemmThunk(\n     const HloCustomCallInstruction* instr) {\n   TF_ASSIGN_OR_RETURN(BufferAllocation::Slice a,\n                       GetAllocationSliceForHlo(instr->operand(0), {}));\n@@ -445,11 +457,10 @@ absl::Status ThunkEmitter::EmitGemmThunk(\n           instr, ir_emitter_context_->GetNextThunkId()),\n       std::move(config), a, b, c, workspace,\n       RequireDeterminism(ir_emitter_context_->hlo_module().config()));\n-  AddThunkToThunkSequence(std::move(thunk));\n-  return absl::OkStatus();\n+  return GetThunkSequence(std::move(thunk));\n }\n \n-absl::Status ThunkEmitter::EmitCublasLtMatmulThunk(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCublasLtMatmulThunk(\n     const HloCustomCallInstruction* instr) {\n   TF_ASSIGN_OR_RETURN(const auto gpu_config,\n                       instr->backend_config<xla::gpu::GpuBackendConfig>());\n@@ -527,11 +538,10 @@ absl::Status ThunkEmitter::EmitCublasLtMatmulThunk(\n       std::move(thunk_info), std::move(canonical_hlo), std::move(gemm_config),\n       blas_lt_epilogue, algorithm, a, b, c, d, bias, aux, a_scale, b_scale,\n       c_scale, d_scale, d_amax, workspace_buffer);\n-  AddThunkToThunkSequence(std::move(thunk));\n-  return absl::OkStatus();\n+  return GetThunkSequence(std::move(thunk));\n }\n \n-absl::Status ThunkEmitter::EmitCublasLtMatmulThunkF8(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCublasLtMatmulThunkF8(\n     const HloCustomCallInstruction* instr) {\n   TF_RET_CHECK(instr->operand_count() > 3 && instr->operand_count() < 8);\n   TF_ASSIGN_OR_RETURN(const auto gpu_config,\n@@ -623,11 +633,10 @@ absl::Status ThunkEmitter::EmitCublasLtMatmulThunkF8(\n       std::move(thunk_info), std::move(canonical_hlo), std::move(gemm_config),\n       blas_lt_epilogue, algorithm, a, b, c, d, bias, aux, a_scale, b_scale,\n       c_scale, d_scale, d_amax, workspace_buffer);\n-  AddThunkToThunkSequence(std::move(thunk));\n-  return absl::OkStatus();\n+  return GetThunkSequence(std::move(thunk));\n }\n \n-absl::Status ThunkEmitter::EmitConvolutionReorderThunk(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitConvolutionReorderThunk(\n     const HloCustomCallInstruction* instr) {\n   bool has_bias = instr->operand_count() > 1;\n   Shape shape = has_bias ? instr->shape().tuple_shapes(0) : instr->shape();\n@@ -662,11 +671,10 @@ absl::Status ThunkEmitter::EmitConvolutionReorderThunk(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n       std::move(filter_dimensions), filter_input, filter_output, biases);\n-  AddThunkToThunkSequence(std::move(thunk));\n-  return absl::OkStatus();\n+  return GetThunkSequence(std::move(thunk));\n }\n \n-absl::Status ThunkEmitter::EmitNormThunk(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitNormThunk(\n     const HloCustomCallInstruction* instr) {\n   TF_ASSIGN_OR_RETURN(auto const gpu_backend_config,\n                       instr->backend_config<xla::gpu::GpuBackendConfig>());\n@@ -743,11 +751,10 @@ absl::Status ThunkEmitter::EmitNormThunk(\n                         y_or_dx_slice, bias_slice, expectation_slice,\n                         norm_factor_slice, dy_slice, dscale_slice, dbias_slice,\n                         scratch_slice));\n-  AddThunkToThunkSequence(std::move(thunk));\n-  return absl::OkStatus();\n+  return GetThunkSequence(std::move(thunk));\n }\n \n-absl::Status ThunkEmitter::EmitCuDnnThunk(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCuDnnThunk(\n     const HloCustomCallInstruction* instr) {\n   TF_ASSIGN_OR_RETURN(auto kernel_arguments,\n                       emitters::KernelArguments::Create(\n@@ -762,21 +769,19 @@ absl::Status ThunkEmitter::EmitCuDnnThunk(\n                         instr->backend_config<xla::gpu::GpuBackendConfig>());\n     dropout_seed = gpu_config.cudnn_fmha_backend_config().seed();\n   }\n-  AddThunkToThunkSequence(std::make_unique<CuDnnThunk>(\n+  return GetThunkSequence(std::make_unique<CuDnnThunk>(\n       fingerprint,\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n       kernel_arguments.GetArgumentBufferSlices(),\n       kernel_arguments.GetArgumentOutputFlags(), dropout_seed));\n-  return absl::OkStatus();\n }\n \n-absl::Status ThunkEmitter::EmitPtxCustomCall(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitPtxCustomCall(\n     const HloCustomCallInstruction* instr) {\n   TF_ASSIGN_OR_RETURN(auto thunk,\n                       EmitPtxCustomKernelThunk(instr, ir_emitter_context_));\n-  AddThunkToThunkSequence(std::move(thunk));\n-  return absl::OkStatus();\n+  return GetThunkSequence(std::move(thunk));\n }\n \n absl::StatusOr<BufferAllocation::Slice> ThunkEmitter::GetAllocationSliceForHlo(\n@@ -785,7 +790,7 @@ absl::StatusOr<BufferAllocation::Slice> ThunkEmitter::GetAllocationSliceForHlo(\n                                       instr, index);\n }\n \n-absl::Status ThunkEmitter::EmitCubDeviceRadixSort(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCubDeviceRadixSort(\n     const HloCustomCallInstruction* instr) {\n   if (instr->operand_count() != 1 && instr->operand_count() != 2) {\n     return Internal(\"Invalid number of operands for radix sort\");\n@@ -829,11 +834,10 @@ absl::Status ThunkEmitter::EmitCubDeviceRadixSort(\n           Product(operand_shape.dimensions()) /\n               operand_shape.dimensions(operand_shape.dimensions().size() - 1),\n           ir_emitter_context_->platform_name()));\n-  AddThunkToThunkSequence(std::move(thunk));\n-  return absl::OkStatus();\n+  return GetThunkSequence(std::move(thunk));\n }\n \n-absl::Status ThunkEmitter::EmitCustomCallThunk(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCustomCallThunk(\n     const HloCustomCallInstruction* instr) {\n   const std::string& call_target_name = instr->custom_call_target();\n \n@@ -934,37 +938,38 @@ absl::Status ThunkEmitter::EmitCustomCallThunk(\n   absl::StatusOr<std::unique_ptr<CustomCallThunk>> custom_call_thunk =\n       is_ffi_custom_call ? ffi_thunk() : legacy_thunk();\n \n+  ThunkSequence thunks;\n   if (custom_call_thunk.ok()) {\n-    AddThunkToThunkSequence(std::move(custom_call_thunk.value()));\n-    return absl::OkStatus();\n+    thunks.push_back(std::move(custom_call_thunk.value()));\n   }\n-\n   if (ir_emitter_context_->debug_options().xla_gpu_mock_custom_calls()) {\n     // xla_gpu_mock_custom_calls=true means we won't emit thunks for all custom\n     // call targets that couldn't be found.\n-    return absl::OkStatus();\n+    return thunks;\n   }\n-\n-  return custom_call_thunk.status();\n+  if (!custom_call_thunk.ok()) {\n+    return custom_call_thunk.status();\n+  }\n+  return thunks;\n }\n \n-absl::Status ThunkEmitter::EmitFftThunk(const HloFftInstruction* instr) {\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitFftThunk(\n+    const HloFftInstruction* instr) {\n   TF_ASSIGN_OR_RETURN(BufferAllocation::Slice arg_slice,\n                       GetAllocationSliceForHlo(instr->operand(0)));\n   TF_ASSIGN_OR_RETURN(BufferAllocation::Slice dest_slice,\n                       GetAllocationSliceForHlo(instr));\n-  AddThunkToThunkSequence(std::make_unique<FftThunk>(\n+  return GetThunkSequence(std::make_unique<FftThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n       instr->fft_type(), instr->fft_length(),\n       /*input_buffer=*/arg_slice,\n       /*output_buffer=*/dest_slice,\n       /*input_shape=*/instr->operand(0)->shape(),\n       /*output_shape=*/instr->shape()));\n-  return absl::OkStatus();\n }\n \n-absl::Status ThunkEmitter::EmitTriangularSolveCustomCall(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTriangularSolveCustomCall(\n     const HloInstruction* instr) {\n   TF_RET_CHECK(instr->operand_count() == 2);\n   auto operands = instr->operands();\n@@ -1034,19 +1039,17 @@ absl::Status ThunkEmitter::EmitTriangularSolveCustomCall(\n \n   // Elide the sequential thunk if there's no copy.\n   if (thunks.size() == 1) {\n-    AddThunkToThunkSequence(std::move(thunks[0]));\n-  } else {\n-    auto thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n-        instr, ir_emitter_context_->GetNextThunkId());\n-    // Don't repeat the annotation from inside thunks\n-    thunk_info.profile_annotation = {};\n-    AddThunkToThunkSequence(\n-        std::make_unique<SequentialThunk>(thunk_info, std::move(thunks)));\n+    return thunks;\n   }\n-  return absl::OkStatus();\n+  auto thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n+      instr, ir_emitter_context_->GetNextThunkId());\n+  // Don't repeat the annotation from inside thunks\n+  thunk_info.profile_annotation = {};\n+  return GetThunkSequence(\n+      std::make_unique<SequentialThunk>(thunk_info, std::move(thunks)));\n }\n \n-absl::Status ThunkEmitter::EmitTopKCustomCall(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTopKCustomCall(\n     const HloCustomCallInstruction* instr) {\n   auto operands = instr->operands();\n   const auto& shape = instr->shape();\n@@ -1105,9 +1108,8 @@ absl::Status ThunkEmitter::EmitTopKCustomCall(\n     Thunk::ThunkInfo thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n         instr, ir_emitter_context_->GetNextThunkId());\n     if (use_raft_select_k) {\n-      AddThunkToThunkSequence(std::make_unique<SelectKThunk>(\n+      return GetThunkSequence(std::make_unique<SelectKThunk>(\n           std::move(thunk_info), batch_size, n, k, dtype, kernel_arguments));\n-      return absl::OkStatus();\n     }\n   }\n \n@@ -1123,12 +1125,11 @@ absl::Status ThunkEmitter::EmitTopKCustomCall(\n \n   Thunk::ThunkInfo thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n       instr, ir_emitter_context_->GetNextThunkId());\n-  AddThunkToThunkSequence(std::make_unique<CustomKernelThunk>(\n+  return GetThunkSequence(std::make_unique<CustomKernelThunk>(\n       std::move(thunk_info), std::move(kernel), kernel_arguments));\n-  return absl::OkStatus();\n }\n \n-absl::Status ThunkEmitter::EmitTritonCustomCall(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTritonCustomCall(\n     const HloCustomCallInstruction* instr) {\n   auto generate = [this, &instr]() -> absl::StatusOr<KernelReuseCache::Entry> {\n     mlir::MLIRContext& mlir_context = *ir_emitter_context_->mlir_context();\n@@ -1210,27 +1211,26 @@ absl::Status ThunkEmitter::EmitTritonCustomCall(\n                           ir_emitter_context_->buffer_assignment(),\n                           GetDefaultBufferAlignment(), instr));\n \n-  AddThunkToThunkSequence(std::make_unique<KernelThunk>(\n+  return GetThunkSequence(std::make_unique<KernelThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n       entry->kernel_name, kernel_arguments, entry->launch_dimensions,\n       entry->cluster_dim, entry->shmem_bytes, entry->tma_metadata));\n-  return absl::OkStatus();\n }\n \n-absl::Status ThunkEmitter::EmitAsyncComputation(const HloInstruction* instr) {\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitAsyncComputation(\n+    const HloInstruction* instr) {\n   const HloInstruction* wrapped = instr->async_wrapped_instruction();\n   const ExecutionStreamAssignment& stream_assignment =\n       ir_emitter_context_->execution_stream_assignment();\n   TF_ASSIGN_OR_RETURN(auto stream,\n                       stream_assignment.GetSyncExecutionStreamId(wrapped));\n   TF_RET_CHECK(wrapped->called_computations().size() == 1);\n   auto computation = wrapped->called_computations().front();\n-  auto ir_emitter = ThunkEmitter::Create(ir_emitter_context_);\n-  TF_RETURN_IF_ERROR(ir_emitter->EmitHloComputation(computation));\n-  std::unique_ptr<SequentialThunk> thunk_sequence =\n-      ir_emitter->ConsumeThunkSequence();\n-  for (auto& thunk : thunk_sequence->thunks()) {\n+  TF_ASSIGN_OR_RETURN(auto comp_thunks, EmitHloComputation(computation));\n+  auto sequential_thunk = std::make_unique<SequentialThunk>(\n+      Thunk::ThunkInfo{}, std::move(comp_thunks));\n+  for (auto& thunk : sequential_thunk->thunks()) {\n     thunk->set_execution_stream_id(stream);\n   }\n   auto* async_start = Cast<HloAsyncInstruction>(instr);\n@@ -1242,15 +1242,17 @@ absl::Status ThunkEmitter::EmitAsyncComputation(const HloInstruction* instr) {\n   // main stream has finished calculating any values that may be\n   // used as input. We enforce this by inlining a `WaitForStreams`\n   // thunk on the main stream.\n-  AddThunkToThunkSequence(std::make_unique<WaitForStreamsThunk>(\n+  ThunkSequence thunks;\n+  thunks.push_back(std::make_unique<WaitForStreamsThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n       async_streams.destination_stream_id, async_streams.source_stream_id));\n-  AddThunkToThunkSequence(std::move(thunk_sequence));\n-  return absl::OkStatus();\n+  thunks.push_back(std::move(sequential_thunk));\n+  return thunks;\n }\n \n-absl::Status ThunkEmitter::EmitFusion(const HloFusionInstruction* instr) {\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitFusion(\n+    const HloFusionInstruction* instr) {\n   const se::DeviceDescription& device_info =\n       ir_emitter_context_->gpu_device_info();\n   const HloFusionAnalysis fusion_analysis =\n@@ -1278,30 +1280,29 @@ absl::Status ThunkEmitter::EmitFusion(const HloFusionInstruction* instr) {\n     TF_ASSIGN_OR_RETURN(ExecutionStreamId execution_stream_id,\n                         stream_assignment.GetSyncExecutionStreamId(instr));\n     thunk->set_execution_stream_id(execution_stream_id);\n-    AddThunkToThunkSequence(std::move(thunk));\n   }\n   VLOG(3) << \"ThunkEmitter::EmitFusion:complete\";\n-  return absl::OkStatus();\n+  return std::move(result.thunks);\n }\n \n-absl::Status ThunkEmitter::EmitCopy(const HloInstruction* instr) {\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCopy(\n+    const HloInstruction* instr) {\n   TF_RET_CHECK(LayoutUtil::LayoutsInShapesEqual(\n       instr->operand(0)->shape(), instr->shape(),\n       Layout::Equal().MinorToMajorOnly()));\n   TF_ASSIGN_OR_RETURN(BufferAllocation::Slice src_buffer,\n                       GetAllocationSliceForHlo(instr->operand(0)));\n   TF_ASSIGN_OR_RETURN(BufferAllocation::Slice dst_buffer,\n                       GetAllocationSliceForHlo(instr));\n-  AddThunkToThunkSequence(std::make_unique<DeviceToDeviceCopyThunk>(\n+  return GetThunkSequence(std::make_unique<DeviceToDeviceCopyThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n       /*source_buffer=*/src_buffer,\n       /*destination_buffer=*/dst_buffer,\n       /*mem_size=*/src_buffer.size()));\n-  return absl::OkStatus();\n }\n \n-absl::Status ThunkEmitter::EmitAsyncCustomCallStart(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitAsyncCustomCallStart(\n     const HloInstruction* instr) {\n   const HloInstruction* wrapped = instr->async_wrapped_instruction();\n   auto* async_start = Cast<HloAsyncInstruction>(instr);\n@@ -1310,7 +1311,7 @@ absl::Status ThunkEmitter::EmitAsyncCustomCallStart(\n   TF_ASSIGN_OR_RETURN(\n       ExecutionStreamAssignment::AsyncExecutionStreamIds streams,\n       stream_assignment.GetAsyncExecutionStreamIds(async_start));\n-  AddThunkToThunkSequence(std::make_unique<WaitForStreamsThunk>(\n+  ThunkSequence thunks = GetThunkSequence(std::make_unique<WaitForStreamsThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n       streams.destination_stream_id, streams.source_stream_id));\n@@ -1319,25 +1320,29 @@ absl::Status ThunkEmitter::EmitAsyncCustomCallStart(\n \n   auto* custom_call = Cast<HloCustomCallInstruction>(wrapped);\n   if (IsLegacyCublasMatmul(*wrapped)) {\n-    auto status = EmitGemmThunk(custom_call);\n-    if (status.ok()) {\n-      thunk_sequence_.back()->set_execution_stream_id(execution_stream_id);\n-    }\n-    return status;\n+    TF_ASSIGN_OR_RETURN(auto gemm_thunks, EmitGemmThunk(custom_call));\n+    CHECK_EQ(gemm_thunks.size(), 1);\n+    gemm_thunks.back()->set_execution_stream_id(execution_stream_id);\n+    AppendThunkSequence(thunks, gemm_thunks);\n+    return thunks;\n   }\n   if (IsCublasLtMatmul(*wrapped)) {\n-    auto status = EmitCublasLtMatmulThunk(custom_call);\n-    if (status.ok()) {\n-      thunk_sequence_.back()->set_execution_stream_id(execution_stream_id);\n-    }\n-    return status;\n+    TF_ASSIGN_OR_RETURN(auto cublas_lt_matmul_thunks,\n+                        EmitCublasLtMatmulThunk(custom_call));\n+    CHECK_EQ(cublas_lt_matmul_thunks.size(), 1);\n+    cublas_lt_matmul_thunks.back()->set_execution_stream_id(\n+        execution_stream_id);\n+    AppendThunkSequence(thunks, cublas_lt_matmul_thunks);\n+    return thunks;\n   }\n   if (IsCublasLtMatmulF8(*wrapped)) {\n-    auto status = EmitCublasLtMatmulThunkF8(custom_call);\n-    if (status.ok()) {\n-      thunk_sequence_.back()->set_execution_stream_id(execution_stream_id);\n-    }\n-    return status;\n+    TF_ASSIGN_OR_RETURN(auto cublas_lt_matmul_thunks,\n+                        EmitCublasLtMatmulThunkF8(custom_call));\n+    CHECK_EQ(cublas_lt_matmul_thunks.size(), 1);\n+    cublas_lt_matmul_thunks.back()->set_execution_stream_id(\n+        execution_stream_id);\n+    AppendThunkSequence(thunks, cublas_lt_matmul_thunks);\n+    return thunks;\n   }\n   return Internal(\"Unsupported async custom call instruction: %s\",\n                   HloOpcodeString(wrapped->opcode()));\n@@ -1355,7 +1360,8 @@ absl::Status ThunkEmitter::AssertNonDeterminismIsOkay(\n   return absl::OkStatus();\n }\n \n-absl::Status ThunkEmitter::EmitWhile(const HloInstruction* instr) {\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitWhile(\n+    const HloInstruction* instr) {\n   TF_ASSIGN_OR_RETURN(auto config,\n                       instr->backend_config<xla::WhileLoopBackendConfig>());\n \n@@ -1370,12 +1376,10 @@ absl::Status ThunkEmitter::EmitWhile(const HloInstruction* instr) {\n                       Thunk::ThunkInfo::WithProfileAnnotation(\n                           instr, ir_emitter_context_->GetNextThunkId()),\n                       trip_count));\n-\n-  AddThunkToThunkSequence(std::move(thunk));\n-  return absl::OkStatus();\n+  return GetThunkSequence(std::move(thunk));\n }\n \n-absl::Status ThunkEmitter::EmitRngGetAndUpdateState(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitRngGetAndUpdateState(\n     const HloRngGetAndUpdateStateInstruction* instr) {\n   std::string ir_name = std::string(instr->name());\n   auto local_llvm_module =\n@@ -1384,18 +1388,17 @@ absl::Status ThunkEmitter::EmitRngGetAndUpdateState(\n   TF_ASSIGN_OR_RETURN(auto thunk_sequence,\n                       EmitRngGetAndUpdateStateLLVMIR(\n                           instr, local_llvm_module.get(), ir_emitter_context_));\n-  for (auto& thunk : thunk_sequence) {\n-    AddThunkToThunkSequence(std::move(thunk));\n-  }\n   CHECK(!llvm::Linker::linkModules(*ir_emitter_context_->llvm_module(),\n                                    std::move(local_llvm_module),\n                                    llvm::Linker::Flags::OverrideFromSrc));\n-  return absl::OkStatus();\n+  return thunk_sequence;\n }\n \n-absl::Status ThunkEmitter::EmitSort(const HloSortInstruction* sort) {\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitSort(\n+    const HloSortInstruction* sort) {\n   std::string op_name(sort->name());\n   const Shape& keys_shape = sort->operand(0)->shape();\n+  ThunkSequence thunks;\n   for (int64_t i = 0; i < sort->operand_count(); ++i) {\n     ShapeIndex shape_index =\n         sort->operand_count() > 1 ? ShapeIndex({i}) : ShapeIndex({});\n@@ -1423,7 +1426,7 @@ absl::Status ThunkEmitter::EmitSort(const HloSortInstruction* sort) {\n       // TODO(b/26783907): Figure out why we never seem to share\n       // buffers for key/value sort.\n       VLOG(2) << op_name << \" requires initial D2D copy for operand \" << i;\n-      AddThunkToThunkSequence(std::make_unique<DeviceToDeviceCopyThunk>(\n+      thunks.push_back(std::make_unique<DeviceToDeviceCopyThunk>(\n           Thunk::ThunkInfo::WithProfileAnnotation(\n               sort, ir_emitter_context_->GetNextThunkId()),\n           /*source_buffer=*/source_address,\n@@ -1436,29 +1439,25 @@ absl::Status ThunkEmitter::EmitSort(const HloSortInstruction* sort) {\n   auto local_llvm_module =\n       CreateLocalLLVMModule(op_name, ir_emitter_context_->llvm_module());\n \n-  TF_ASSIGN_OR_RETURN(ThunkSequence thunks,\n+  TF_ASSIGN_OR_RETURN(ThunkSequence sort_thunks,\n                       EmitBitonicSortLLVMIR(sort, local_llvm_module.get(),\n                                             ir_emitter_context_));\n-  for (auto& thunk : thunks) {\n-    AddThunkToThunkSequence(std::move(thunk));\n-  }\n+  AppendThunkSequence(thunks, sort_thunks);\n   llvm::Linker::linkModules(*ir_emitter_context_->llvm_module(),\n                             std::move(local_llvm_module),\n                             llvm::Linker::Flags::OverrideFromSrc);\n-  return absl::OkStatus();\n+  return thunks;\n }\n \n template <typename ThunkType>\n-absl::Status ThunkEmitter::EmitReplicaOrPartitionId(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitReplicaOrPartitionId(\n     const HloInstruction* instr) {\n   TF_ASSIGN_OR_RETURN(BufferAllocation::Slice result_slice,\n                       GetAllocationSliceForHlo(instr, {}));\n-  auto thunk = std::make_unique<ThunkType>(\n+  return GetThunkSequence(std::make_unique<ThunkType>(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n-      result_slice);\n-  AddThunkToThunkSequence(std::move(thunk));\n-  return absl::OkStatus();\n+      result_slice));\n }\n \n bool IsNvshmemCollective(const HloInstruction* instr) {\n@@ -1471,7 +1470,8 @@ bool IsNvshmemCollective(const HloInstruction* instr) {\n   return false;\n }\n \n-absl::Status ThunkEmitter::EmitCollectiveMetadata(const HloInstruction* instr) {\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCollectiveMetadata(\n+    const HloInstruction* instr) {\n   std::vector<CollectiveMetadataThunk::Buffer> buffers;\n   buffers.reserve(instr->operands().size());\n   for (const HloInstruction* operand : instr->operands()) {\n@@ -1486,16 +1486,14 @@ absl::Status ThunkEmitter::EmitCollectiveMetadata(const HloInstruction* instr) {\n   TF_ASSIGN_OR_RETURN(BufferAllocation::Slice result,\n                       GetAllocationSliceForHlo(instr, result_shape_index));\n \n-  auto thunk = std::make_unique<CollectiveMetadataThunk>(\n+  return GetThunkSequence(std::make_unique<CollectiveMetadataThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n       CollectiveMetadataThunk::GetCollectiveConfig(*instr), std::move(buffers),\n-      result);\n-  AddThunkToThunkSequence(std::move(thunk));\n-  return absl::OkStatus();\n+      result));\n }\n \n-absl::Status ThunkEmitter::EmitCollectivePermute(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCollectivePermute(\n     const HloCollectivePermuteInstruction* instr) {\n   // First output is aliased.\n   TF_RET_CHECK(\n@@ -1509,6 +1507,7 @@ absl::Status ThunkEmitter::EmitCollectivePermute(\n \n   auto operands = instr->operands();\n   std::vector<CollectiveThunk::Buffer> buffers;\n+  ThunkSequence thunks;\n   for (int oprd_idx = 0; oprd_idx < operands.size(); ++oprd_idx) {\n     const auto operand = operands.at(oprd_idx);\n     const ShapeIndex nested_shape_idx = {1, oprd_idx}, normal_shape_idx = {1};\n@@ -1531,7 +1530,7 @@ absl::Status ThunkEmitter::EmitCollectivePermute(\n                                                   partition_count)) {\n       // For a degenerate collective permute, just generate a copy\n       // thunk.\n-      AddThunkToThunkSequence(std::make_unique<DeviceToDeviceCopyThunk>(\n+      thunks.push_back(std::make_unique<DeviceToDeviceCopyThunk>(\n           Thunk::ThunkInfo::WithProfileAnnotation(\n               instr, ir_emitter_context_->GetNextThunkId()),\n           /*source_buffer=*/source_slice,\n@@ -1562,7 +1561,7 @@ absl::Status ThunkEmitter::EmitCollectivePermute(\n           ir_emitter_context_->debug_options().xla_gpu_use_memcpy_local_p2p(),\n           GetStreamKindForP2P(instr));\n       GetCollectivesAsyncEvents().try_emplace(instr, thunk->async_events());\n-      AddThunkToThunkSequence(std::move(thunk));\n+      thunks.push_back(std::move(thunk));\n     } else {\n       auto thunk = std::make_unique<CollectivePermuteStartThunk>(\n           Thunk::ThunkInfo::WithProfileAnnotation(\n@@ -1571,14 +1570,14 @@ absl::Status ThunkEmitter::EmitCollectivePermute(\n           ir_emitter_context_->debug_options().xla_gpu_use_memcpy_local_p2p(),\n           GetStreamKindForP2P(instr));\n       GetCollectivesAsyncEvents().try_emplace(instr, thunk->async_events());\n-      AddThunkToThunkSequence(std::move(thunk));\n+      thunks.push_back(std::move(thunk));\n     }\n   }\n-  return absl::OkStatus();\n+  return thunks;\n }\n \n template <typename CollectiveThunkType, typename HloInstType>\n-absl::Status ThunkEmitter::EmitCollectiveThunk(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCollectiveThunk(\n     Thunk::Kind kind, const HloInstruction* async_start,\n     const HloInstType* inst, std::optional<bool> use_global_device_ids) {\n   const auto& hlo_config = ir_emitter_context_->hlo_module().config();\n@@ -1712,8 +1711,7 @@ absl::Status ThunkEmitter::EmitCollectiveThunk(\n           ir_emitter_context_->debug_options().xla_gpu_use_memcpy_local_p2p());\n     }\n     GetCollectivesAsyncEvents().insert({async_start, thunk->async_events()});\n-    AddThunkToThunkSequence(std::move(thunk));\n-    return absl::OkStatus();\n+    return GetThunkSequence(std::move(thunk));\n   }\n \n   if (!is_degenerate) {\n@@ -1886,13 +1884,16 @@ std::vector<const HloInstruction*> GetRealDependencyInstructions(\n   }\n }\n \n-absl::Status ThunkEmitter::EmitCollectiveGroupStartThunk(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCollectiveGroupStartThunk(\n     const HloInstruction* instr) {\n-  emit_group_thunks_ = true;\n   std::optional<AsyncStreamKind> stream_kind;\n+  ThunkSequence thunks;\n   for (const HloInstruction* nested_instruction :\n        instr->async_wrapped_computation()->instructions()) {\n-    TF_RETURN_IF_ERROR(EmitHloInstruction(nested_instruction));\n+    TF_ASSIGN_OR_RETURN(\n+        auto comp_thunks,\n+        EmitHloInstruction(nested_instruction, /*emit_group_thunks=*/true));\n+    AppendThunkSequence(thunks, comp_thunks);\n     if ((nested_instruction->opcode() == HloOpcode::kSend ||\n          nested_instruction->opcode() == HloOpcode::kRecv) &&\n         !stream_kind.has_value()) {\n@@ -1903,18 +1904,16 @@ absl::Status ThunkEmitter::EmitCollectiveGroupStartThunk(\n     }\n   }\n   auto thunk = std::make_unique<CollectiveGroupThunk>(\n-      instr, Thunk::Kind::kGroupStart, std::move(scoped_thunk_sequence_),\n+      instr, Thunk::Kind::kGroupStart, std::move(thunks),\n       stream_kind.value_or(AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE),\n       ir_emitter_context_->GetNextThunkId());\n-  emit_group_thunks_ = false;\n \n   GetCollectivesAsyncEvents().insert({instr, thunk->async_events()});\n-  AddThunkToThunkSequence(std::move(thunk));\n-  return absl::OkStatus();\n+  return GetThunkSequence(std::move(thunk));\n }\n \n-absl::Status ThunkEmitter::EmitCollectiveAsyncDone(Thunk::Kind kind,\n-                                                   const HloInstruction* inst) {\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCollectiveAsyncDone(\n+    Thunk::Kind kind, const HloInstruction* inst) {\n   // Partial pipelining is only implemented for send/recv.\n   bool is_send_recv =\n       kind == Thunk::Kind::kRecvDone || kind == Thunk::Kind::kSendDone;\n@@ -1931,24 +1930,23 @@ absl::Status ThunkEmitter::EmitCollectiveAsyncDone(Thunk::Kind kind,\n   // Can be null if no start thunk was created (e.g. if the start op\n   // is degenerate), in which case there's nothing to do here.\n   if (!async_events_it->second) {\n-    return absl::OkStatus();\n+    return ThunkSequence{};\n   }\n \n   AsyncStreamKind stream_kind = AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE;\n   if (is_send_recv) {\n     stream_kind = GetStreamKindForP2P(start);\n   }\n \n-  AddThunkToThunkSequence(std::make_unique<CollectiveDoneThunk>(\n+  return GetThunkSequence(std::make_unique<CollectiveDoneThunk>(\n       kind,\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           inst, ir_emitter_context_->GetNextThunkId()),\n       async_events_it->second, stream_kind));\n-  return absl::OkStatus();\n }\n \n-absl::Status ThunkEmitter::EmitNvshmemAsyncDone(Thunk::Kind kind,\n-                                                const HloInstruction* inst) {\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitNvshmemAsyncDone(\n+    Thunk::Kind kind, const HloInstruction* inst) {\n   bool is_send_recv = kind == Thunk::Kind::kNvshmemRecvDone ||\n                       kind == Thunk::Kind::kNvshmemSendDone;\n   const HloInstruction* start =\n@@ -1964,7 +1962,7 @@ absl::Status ThunkEmitter::EmitNvshmemAsyncDone(Thunk::Kind kind,\n   // Can be null if no start thunk was created (e.g. if the start op is\n   // degenerate), in which case there's nothing to do here.\n   if (!async_events_it->second) {\n-    return absl::OkStatus();\n+    return ThunkSequence{};\n   }\n \n   AsyncStreamKind stream_kind = AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE;\n@@ -1973,22 +1971,20 @@ absl::Status ThunkEmitter::EmitNvshmemAsyncDone(Thunk::Kind kind,\n   }\n \n   if (kind == Thunk::Kind::kNvshmemCollectivePermuteDone) {\n-    AddThunkToThunkSequence(std::make_unique<NvshmemCollectivePermuteDoneThunk>(\n-        Thunk::ThunkInfo::WithProfileAnnotation(\n-            inst, ir_emitter_context_->GetNextThunkId()),\n-        async_events_it->second, stream_kind));\n-  } else {\n-    AddThunkToThunkSequence(std::make_unique<NvshmemCollectiveDoneThunk>(\n-        kind,\n+    return GetThunkSequence(std::make_unique<NvshmemCollectivePermuteDoneThunk>(\n         Thunk::ThunkInfo::WithProfileAnnotation(\n             inst, ir_emitter_context_->GetNextThunkId()),\n         async_events_it->second, stream_kind));\n   }\n-  return absl::OkStatus();\n+  return GetThunkSequence(std::make_unique<NvshmemCollectiveDoneThunk>(\n+      kind,\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          inst, ir_emitter_context_->GetNextThunkId()),\n+      async_events_it->second, stream_kind));\n }\n \n template <typename NvshmemAllReduceThunkType, typename HloAllReduceInstruction>\n-absl::Status ThunkEmitter::EmitNvshmemThunk(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitNvshmemThunk(\n     Thunk::Kind kind, const HloInstruction* async_start,\n     const HloAllReduceInstruction* inst,\n     std::optional<bool> use_global_device_ids) {\n@@ -2053,19 +2049,17 @@ absl::Status ThunkEmitter::EmitNvshmemThunk(\n         thunk_info, inst, /*buffers=*/std::move(buffers),\n         ir_emitter_context_->debug_options().xla_gpu_use_memcpy_local_p2p());\n     GetCollectivesAsyncEvents().insert({async_start, thunk->async_events()});\n-    AddThunkToThunkSequence(std::move(thunk));\n-    return absl::OkStatus();\n+    return GetThunkSequence(std::move(thunk));\n   }\n \n   if (!is_degenerate) {\n     return implementable_status;\n   }\n-\n   return EmitDegeneratedCollectiveThunk(buffers, async_start, inst);\n }\n \n template <typename HloInstType>\n-absl::Status ThunkEmitter::EmitDegeneratedCollectiveThunk(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitDegeneratedCollectiveThunk(\n     std::vector<CollectiveThunk::Buffer>& buffers,\n     const HloInstruction* async_start, const HloInstType* inst) {\n   // Signal that start thunk not created with nullptr.\n@@ -2084,17 +2078,16 @@ absl::Status ThunkEmitter::EmitDegeneratedCollectiveThunk(\n         /*mem_size=*/ShapeUtil::ByteSizeOf(shape)));\n   }\n   if (thunks.size() == 1) {\n-    AddThunkToThunkSequence(std::move(thunks[0]));\n-  } else {\n-    AddThunkToThunkSequence(std::make_unique<SequentialThunk>(\n-        Thunk::ThunkInfo::WithProfileAnnotation(\n-            inst, ir_emitter_context_->GetNextThunkId()),\n-        std::move(thunks)));\n+    return thunks;\n   }\n-  return absl::OkStatus();\n+  return GetThunkSequence(std::make_unique<SequentialThunk>(\n+      Thunk::ThunkInfo::WithProfileAnnotation(\n+          inst, ir_emitter_context_->GetNextThunkId()),\n+      std::move(thunks)));\n }\n \n-absl::Status ThunkEmitter::EmitInfeed(const HloInfeedInstruction* instr) {\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitInfeed(\n+    const HloInfeedInstruction* instr) {\n   // Infeed instruction returns a tuple containing the result data\n   // and a token. We only need the result data to construct the\n   // infeed thunk.\n@@ -2114,15 +2107,14 @@ absl::Status ThunkEmitter::EmitInfeed(const HloInfeedInstruction* instr) {\n                         instr->ToString(), index.ToString());\n       }));\n \n-  auto thunk = std::make_unique<InfeedThunk>(\n+  return GetThunkSequence(std::make_unique<InfeedThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n-      std::move(shaped_slices));\n-  AddThunkToThunkSequence(std::move(thunk));\n-  return absl::OkStatus();\n+      std::move(shaped_slices)));\n }\n \n-absl::Status ThunkEmitter::EmitOutfeed(const HloOutfeedInstruction* instr) {\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitOutfeed(\n+    const HloOutfeedInstruction* instr) {\n   // HLO outfeed instruction has 2 operands, the source and a token,\n   // and a single token output.\n   const HloInstruction* source = instr->operand(0);\n@@ -2142,28 +2134,23 @@ absl::Status ThunkEmitter::EmitOutfeed(const HloOutfeedInstruction* instr) {\n                         source->ToString(), index.ToString());\n       }));\n \n-  auto thunk = std::make_unique<OutfeedThunk>(\n+  return GetThunkSequence(std::make_unique<OutfeedThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n-      std::move(shaped_slices));\n-  AddThunkToThunkSequence(std::move(thunk));\n-  return absl::OkStatus();\n+      std::move(shaped_slices)));\n }\n \n-\n absl::StatusOr<std::unique_ptr<Thunk>> ThunkEmitter::BuildWhileThunk(\n     const HloInstruction* instr, const Thunk::ThunkInfo& thunk_info,\n     std::optional<int64_t> trip_count) {\n   HloComputation* condition = instr->while_condition();\n   HloComputation* body = instr->while_body();\n \n   // Generate thunk sequence for while 'condition'.\n-  auto ir_emitter_condition = ThunkEmitter::Create(ir_emitter_context_);\n-  TF_RETURN_IF_ERROR(ir_emitter_condition->EmitHloComputation(condition));\n+  TF_ASSIGN_OR_RETURN(auto cond_thunks, EmitHloComputation(condition));\n \n   // Generate thunk sequence for while 'body'.\n-  auto ir_emitter_body = ThunkEmitter::Create(ir_emitter_context_);\n-  TF_RETURN_IF_ERROR(ir_emitter_body->EmitHloComputation(body));\n+  TF_ASSIGN_OR_RETURN(auto body_thunks, EmitHloComputation(body));\n \n   // Buffer slice holding while loop predicate.\n   TF_ASSIGN_OR_RETURN(\n@@ -2176,10 +2163,13 @@ absl::StatusOr<std::unique_ptr<Thunk>> ThunkEmitter::BuildWhileThunk(\n       instr, ir_emitter_context_->GetNextThunkId());\n   body_thunk_info.profile_annotation += \"_body\";\n \n-  return std::unique_ptr<Thunk>(new WhileThunk(\n-      thunk_info, instr, pred,\n-      ir_emitter_condition->ConsumeThunkSequence(cond_thunk_info),\n-      ir_emitter_body->ConsumeThunkSequence(body_thunk_info), trip_count));\n+  return std::unique_ptr<Thunk>(\n+      new WhileThunk(thunk_info, instr, pred,\n+                     std::make_unique<SequentialThunk>(cond_thunk_info,\n+                                                       std::move(cond_thunks)),\n+                     std::make_unique<SequentialThunk>(body_thunk_info,\n+                                                       std::move(body_thunks)),\n+                     trip_count));\n }\n \n static absl::flat_hash_map<std::string, std::string> ConvertFrontendAttributes(\n@@ -2205,7 +2195,7 @@ absl::StatusOr<bool> ShapeHasHostMemorySpace(Shape shape, int index,\n          shape.tuple_shapes(index).layout().memory_space() == host_memory_space;\n }\n \n-absl::Status ThunkEmitter::EmitCopyStartThunk(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCopyStartThunk(\n     const HloCopyStartInstruction* copy_start_instr) {\n   // copy-start has a tuple shape: {host, device, context},\n   // or {device, host, context}.\n@@ -2240,8 +2230,9 @@ absl::Status ThunkEmitter::EmitCopyStartThunk(\n   // Insert a waitFor() thunk for asynchronous memcpy only when the\n   // source and destination stream IDs differ. If the IDs are the\n   // same, the memcpy operation is synchronous within that stream.\n+  ThunkSequence thunks;\n   if (streams.destination_stream_id != streams.source_stream_id) {\n-    AddThunkToThunkSequence(std::make_unique<WaitForStreamsThunk>(\n+    thunks.push_back(std::make_unique<WaitForStreamsThunk>(\n         Thunk::ThunkInfo::WithProfileAnnotation(\n             copy_start_instr, ir_emitter_context_->GetNextThunkId()),\n         streams.destination_stream_id, streams.source_stream_id));\n@@ -2256,7 +2247,7 @@ absl::Status ThunkEmitter::EmitCopyStartThunk(\n         /*copy_events=*/copy_events_,\n         /*copy_start_instr=*/copy_start_instr);\n     thunk->set_execution_stream_id(streams.destination_stream_id);\n-    AddThunkToThunkSequence(std::move(thunk));\n+    thunks.push_back(std::move(thunk));\n   } else {\n     auto thunk = std::make_unique<HostToDeviceCopyThunk>(\n         Thunk::ThunkInfo::WithProfileAnnotation(\n@@ -2267,27 +2258,26 @@ absl::Status ThunkEmitter::EmitCopyStartThunk(\n         /*copy_events=*/copy_events_,\n         /*copy_start_instr=*/copy_start_instr);\n     thunk->set_execution_stream_id(streams.destination_stream_id);\n-    AddThunkToThunkSequence(std::move(thunk));\n+    thunks.push_back(std::move(thunk));\n   }\n-\n-  return absl::OkStatus();\n+  return thunks;\n }\n \n-absl::Status ThunkEmitter::EmitCopyDoneThunk(const HloInstruction* instr) {\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCopyDoneThunk(\n+    const HloInstruction* instr) {\n   const HloInstruction* copy_start_instr = instr->operand(0);\n   CHECK(copy_start_instr->opcode() == HloOpcode::kCopyStart);\n \n-  auto thunk = std::make_unique<CopyDoneThunk>(\n+  return GetThunkSequence(std::make_unique<CopyDoneThunk>(\n       Thunk::kCopyDone,\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           copy_start_instr, ir_emitter_context_->GetNextThunkId()),\n       /*copy_events=*/copy_events_,\n-      /*copy_start_instr=*/copy_start_instr);\n-  AddThunkToThunkSequence(std::move(thunk));\n-  return absl::OkStatus();\n+      /*copy_start_instr=*/copy_start_instr));\n }\n \n-absl::Status ThunkEmitter::EmitSendThunk(const HloSendInstruction* instr) {\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitSendThunk(\n+    const HloSendInstruction* instr, bool emit_group_thunks) {\n   const HloInstruction* src = instr->operand(0);\n   TF_ASSIGN_OR_RETURN(BufferAllocation::Slice slice,\n                       GetAllocationSliceForHlo(src, {}));\n@@ -2324,7 +2314,7 @@ absl::Status ThunkEmitter::EmitSendThunk(const HloSendInstruction* instr) {\n \n     // Wire up async events if the send thunk isn't emitted as a\n     // part of a group thunk.\n-    if (!emit_group_thunks_) {\n+    if (!emit_group_thunks) {\n       const HloInstruction* canonical_send_instr =\n           FindCanonicalSendRecvStartOp(instr);\n       if (collectives_async_events.contains(canonical_send_instr)) {\n@@ -2348,26 +2338,23 @@ absl::Status ThunkEmitter::EmitSendThunk(const HloSendInstruction* instr) {\n         }\n       }\n     }\n-    AddThunkToThunkSequence(std::move(thunk));\n-    return absl::OkStatus();\n+    return GetThunkSequence(std::move(thunk));\n   }\n \n   if (!instr->channel_id().has_value()) {\n     return absl::InternalError(\n         \"Unknown channel id in host transfer send instruction\");\n   }\n \n-  AddThunkToThunkSequence(std::make_unique<HostSendThunk>(\n+  return GetThunkSequence(std::make_unique<HostSendThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n       src->shape(), slice, *instr->channel_id(), send_recv_events_,\n       ConvertFrontendAttributes(instr->frontend_attributes()),\n       DeviceConstraint(instr)));\n-\n-  return absl::OkStatus();\n }\n \n-absl::Status ThunkEmitter::EmitSendDoneThunk(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitSendDoneThunk(\n     const HloSendDoneInstruction* instr) {\n   if (!instr->is_host_transfer()) {\n     if (IsNvshmemCollective(instr)) {\n@@ -2382,14 +2369,14 @@ absl::Status ThunkEmitter::EmitSendDoneThunk(\n         \"instruction\");\n   }\n \n-  AddThunkToThunkSequence(std::make_unique<HostSendDoneThunk>(\n+  return GetThunkSequence(std::make_unique<HostSendDoneThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n       *instr->channel_id(), send_recv_events_, DeviceConstraint(instr)));\n-  return absl::OkStatus();\n }\n \n-absl::Status ThunkEmitter::EmitRecvThunk(const HloRecvInstruction* instr) {\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitRecvThunk(\n+    const HloRecvInstruction* instr, bool emit_group_thunks) {\n   TF_RET_CHECK(instr->shape().IsTuple());\n   TF_ASSIGN_OR_RETURN(BufferAllocation::Slice slice,\n                       GetAllocationSliceForHlo(instr, {0}));\n@@ -2427,7 +2414,7 @@ absl::Status ThunkEmitter::EmitRecvThunk(const HloRecvInstruction* instr) {\n         GetCollectivesAsyncEvents();\n \n     // Wire up async events.\n-    if (!emit_group_thunks_) {\n+    if (!emit_group_thunks) {\n       const HloInstruction* canonical_recv_instr =\n           FindCanonicalSendRecvStartOp(instr);\n       if (collectives_async_events.contains(canonical_recv_instr)) {\n@@ -2451,27 +2438,24 @@ absl::Status ThunkEmitter::EmitRecvThunk(const HloRecvInstruction* instr) {\n         }\n       }\n     }\n-    AddThunkToThunkSequence(std::move(thunk));\n-    return absl::OkStatus();\n+    return GetThunkSequence(std::move(thunk));\n   }\n \n   if (!instr->channel_id().has_value()) {\n     return absl::InternalError(\n         \"Unknown channel id in host transfer recv instruction\");\n   }\n \n-  AddThunkToThunkSequence(std::make_unique<HostRecvThunk>(\n+  return GetThunkSequence(std::make_unique<HostRecvThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n       instr->shape().tuple_shapes()[0], slice, *instr->channel_id(),\n       send_recv_events_,\n       ConvertFrontendAttributes(instr->frontend_attributes()),\n       DeviceConstraint(instr)));\n-\n-  return absl::OkStatus();\n }\n \n-absl::Status ThunkEmitter::EmitRecvDoneThunk(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitRecvDoneThunk(\n     const HloRecvDoneInstruction* instr) {\n   if (!instr->is_host_transfer()) {\n     if (IsNvshmemCollective(instr)) {\n@@ -2484,12 +2468,10 @@ absl::Status ThunkEmitter::EmitRecvDoneThunk(\n         \"Unknown channel id in host transfer recv done \"\n         \"instruction\");\n   }\n-  AddThunkToThunkSequence(std::make_unique<HostRecvDoneThunk>(\n+  return GetThunkSequence(std::make_unique<HostRecvDoneThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n       *instr->channel_id(), send_recv_events_, DeviceConstraint(instr)));\n-\n-  return absl::OkStatus();\n }\n \n // If the fusion instruction is a dynamic-slice-fusion instruction,\n@@ -2505,7 +2487,8 @@ std::optional<const HloInstruction*> GetCollectiveHeroForDynamicSliceFusion(\n       [](const HloInstruction* instr) { return IsCollective(instr); });\n }\n \n-absl::Status ThunkEmitter::EmitHloInstruction(const HloInstruction* instr) {\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitHloInstruction(\n+    const HloInstruction* instr, bool emit_group_thunks) {\n   switch (instr->opcode()) {\n     case HloOpcode::kAllGatherDone:\n       return EmitCollectiveAsyncDone(Thunk::kAllGatherDone, instr);\n@@ -2540,6 +2523,7 @@ absl::Status ThunkEmitter::EmitHloInstruction(const HloInstruction* instr) {\n         return EmitCollectiveAsyncDone(Thunk::kGroupDone, instr);\n       }\n       const HloInstruction* wrapped = instr->async_wrapped_instruction();\n+      ThunkSequence thunks;\n       switch (wrapped->opcode()) {\n         case HloOpcode::kReduceScatter:\n           return EmitCollectiveAsyncDone(Thunk::kReduceScatterDone, instr);\n@@ -2557,10 +2541,13 @@ absl::Status ThunkEmitter::EmitHloInstruction(const HloInstruction* instr) {\n               Cast<HloFusionInstruction>(wrapped));\n           if (collective_hero.has_value()) {\n             switch ((*collective_hero)->opcode()) {\n-              case HloOpcode::kReduceScatter:\n-                TF_RETURN_IF_ERROR(\n+              case HloOpcode::kReduceScatter: {\n+                TF_ASSIGN_OR_RETURN(\n+                    auto async_done_thunks,\n                     EmitCollectiveAsyncDone(Thunk::kReduceScatterDone, instr));\n+                AppendThunkSequence(thunks, async_done_thunks);\n                 break;\n+              }\n               default:\n                 return absl::InternalError(absl::StrFormat(\n                     \"Unhandled collective in dynamic slice fusion \"\n@@ -2581,11 +2568,11 @@ absl::Status ThunkEmitter::EmitHloInstruction(const HloInstruction* instr) {\n             auto async_events =\n                 GetInstructionToHostExecuteAsyncEvents().at(custom_call);\n \n-            AddThunkToThunkSequence(std::make_unique<HostExecuteDoneThunk>(\n+            thunks.push_back(std::make_unique<HostExecuteDoneThunk>(\n                 Thunk::ThunkInfo::WithProfileAnnotation(\n                     instr, ir_emitter_context_->GetNextThunkId()),\n                 async_events));\n-            return absl::OkStatus();\n+            return thunks;\n           }\n           // Wait until the concurrent stream has finished.\n           auto* async_done = Cast<HloAsyncInstruction>(instr);\n@@ -2594,11 +2581,11 @@ absl::Status ThunkEmitter::EmitHloInstruction(const HloInstruction* instr) {\n           TF_ASSIGN_OR_RETURN(\n               ExecutionStreamAssignment::AsyncExecutionStreamIds streams,\n               stream_assignment.GetAsyncExecutionStreamIds(async_done));\n-          AddThunkToThunkSequence(std::make_unique<WaitForStreamsThunk>(\n+          thunks.push_back(std::make_unique<WaitForStreamsThunk>(\n               Thunk::ThunkInfo::WithProfileAnnotation(\n                   instr, ir_emitter_context_->GetNextThunkId()),\n               streams.source_stream_id, streams.destination_stream_id));\n-          return absl::OkStatus();\n+          return thunks;\n         }\n         default:\n           return Internal(\"Unsupported async done wrapped instruction: %s\",\n@@ -2652,11 +2639,16 @@ absl::Status ThunkEmitter::EmitHloInstruction(const HloInstruction* instr) {\n           TF_ASSIGN_OR_RETURN(\n               ExecutionStreamAssignment::AsyncExecutionStreamIds streams,\n               stream_assignment.GetAsyncExecutionStreamIds(async_start));\n-          AddThunkToThunkSequence(std::make_unique<WaitForStreamsThunk>(\n-              Thunk::ThunkInfo::WithProfileAnnotation(\n-                  instr, ir_emitter_context_->GetNextThunkId()),\n-              streams.destination_stream_id, streams.source_stream_id));\n-          return EmitFusion(Cast<HloFusionInstruction>(wrapped));\n+          ThunkSequence thunks =\n+              GetThunkSequence(std::make_unique<WaitForStreamsThunk>(\n+                  Thunk::ThunkInfo::WithProfileAnnotation(\n+                      instr, ir_emitter_context_->GetNextThunkId()),\n+                  streams.destination_stream_id, streams.source_stream_id));\n+\n+          TF_ASSIGN_OR_RETURN(ThunkSequence fusion_thunks,\n+                              EmitFusion(Cast<HloFusionInstruction>(wrapped)));\n+          AppendThunkSequence(thunks, fusion_thunks);\n+          return thunks;\n         }\n         case HloOpcode::kCall: {\n           return EmitAsyncComputation(instr);\n@@ -2720,17 +2712,13 @@ absl::Status ThunkEmitter::EmitHloInstruction(const HloInstruction* instr) {\n             auto [it, inserted] =\n                 GetInstructionToHostExecuteAsyncEvents().emplace(custom_call,\n                                                                  async_events);\n-\n             if (!inserted) {\n               return Internal(\n                   \"Async events already exist for host offloading custom call \"\n                   \"%s.\",\n                   custom_call->ToString());\n             }\n-\n-            AddThunkToThunkSequence(std::move(thunk));\n-\n-            return absl::OkStatus();\n+            return GetThunkSequence(std::move(thunk));\n           }\n           return EmitAsyncCustomCallStart(instr);\n         }\n@@ -2754,8 +2742,10 @@ absl::Status ThunkEmitter::EmitHloInstruction(const HloInstruction* instr) {\n           Cast<HloCollectivePermuteInstruction>(instr));\n     case HloOpcode::kConditional:\n       return EmitConditional(instr);\n-    case HloOpcode::kConstant:\n-      return EmitConstant(Cast<HloConstantInstruction>(instr));\n+    case HloOpcode::kConstant: {\n+      TF_RETURN_IF_ERROR(EmitConstant(Cast<HloConstantInstruction>(instr)));\n+      return ThunkSequence{};\n+    }\n     case HloOpcode::kCustomCall: {\n       auto* custom_call = Cast<HloCustomCallInstruction>(instr);\n       if (IsLegacyCublasMatmul(*instr)) {\n@@ -2803,12 +2793,12 @@ absl::Status ThunkEmitter::EmitHloInstruction(const HloInstruction* instr) {\n         return EmitTritonCustomCall(custom_call);\n       }\n       if (instr->custom_call_target() == kNopCustomCallTarget) {\n-        return absl::OkStatus();\n+        return ThunkSequence{};\n       }\n       if (instr->custom_call_target() == kPinCustomCallTarget ||\n           instr->custom_call_target() == kUnpinCustomCallTarget ||\n           instr->custom_call_target() == kCreateBufferCustomCallTarget) {\n-        return absl::OkStatus();\n+        return ThunkSequence{};\n       }\n       if (instr->custom_call_target() == kCollectiveMetadataCustomCallTarget) {\n         return EmitCollectiveMetadata(instr);\n@@ -2829,7 +2819,7 @@ absl::Status ThunkEmitter::EmitHloInstruction(const HloInstruction* instr) {\n       return EmitFftThunk(Cast<HloFftInstruction>(instr));\n \n     case HloOpcode::kRecv:\n-      return EmitRecvThunk(Cast<HloRecvInstruction>(instr));\n+      return EmitRecvThunk(Cast<HloRecvInstruction>(instr), emit_group_thunks);\n     case HloOpcode::kRecvDone:\n       return EmitRecvDoneThunk(Cast<HloRecvDoneInstruction>(instr));\n \n@@ -2840,7 +2830,7 @@ absl::Status ThunkEmitter::EmitHloInstruction(const HloInstruction* instr) {\n           Cast<HloRngGetAndUpdateStateInstruction>(instr));\n \n     case HloOpcode::kSend:\n-      return EmitSendThunk(Cast<HloSendInstruction>(instr));\n+      return EmitSendThunk(Cast<HloSendInstruction>(instr), emit_group_thunks);\n     case HloOpcode::kSendDone:\n       return EmitSendDoneThunk(Cast<HloSendDoneInstruction>(instr));\n \n@@ -2863,7 +2853,7 @@ absl::Status ThunkEmitter::EmitHloInstruction(const HloInstruction* instr) {\n     case HloOpcode::kGetTupleElement:\n     case HloOpcode::kParameter:\n     case HloOpcode::kTuple:\n-      return absl::OkStatus();\n+      return ThunkSequence{};\n     default:\n       return Internal(\"Unsupported instruction opcode: %s\",\n                       HloOpcodeString(instr->opcode()));\n@@ -2872,26 +2862,28 @@ absl::Status ThunkEmitter::EmitHloInstruction(const HloInstruction* instr) {\n   return Internal(\"Unhandled HLO instruction\");\n }\n \n-absl::Status ThunkEmitter::EmitHloEntryComputation(const HloModule* module) {\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitHloEntryComputation(\n+    const HloModule* module) {\n   return EmitHloComputation(module->entry_computation());\n }\n \n-absl::Status ThunkEmitter::EmitHloComputation(\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitHloComputation(\n     const HloComputation* computation) {\n   const HloSchedule& schedule = computation->parent()->schedule();\n   if (!schedule.is_computation_scheduled(computation)) {\n     return Internal(\"Sequence not found for computation: %s\",\n                     computation->name());\n   }\n \n+  ThunkSequence thunk_sequence;\n   const HloInstructionSequence& sequence = schedule.sequence(computation);\n   absl::flat_hash_map<const HloInstruction*, Thunk*> instr_to_thunk;\n   for (const HloInstruction* instr : sequence.instructions()) {\n-    int64_t previous_thunk_size = thunk_sequence_.size();\n-    TF_RETURN_IF_ERROR(EmitHloInstruction(instr));\n-    if (thunk_sequence_.size() > previous_thunk_size) {\n-      instr_to_thunk[instr] = thunk_sequence_.back().get();\n+    TF_ASSIGN_OR_RETURN(auto thunks, EmitHloInstruction(instr));\n+    if (!thunks.empty()) {\n+      instr_to_thunk[instr] = thunks.back().get();\n     }\n+    AppendThunkSequence(thunk_sequence, thunks);\n     for (const HloInstruction* control_predecessor :\n          instr->control_predecessors()) {\n       std::vector<const HloInstruction*> real_successors =\n@@ -2915,7 +2907,7 @@ absl::Status ThunkEmitter::EmitHloComputation(\n       }\n     }\n   }\n-  return absl::OkStatus();\n+  return thunk_sequence;\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "ed56ffcf0f17769bcdf97886cf8a09e7ddf0e916",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.h",
            "status": "modified",
            "additions": 81,
            "deletions": 94,
            "changes": 175,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fa512f62ea2c85c8df85ce93326a63cc2ff6864c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fa512f62ea2c85c8df85ce93326a63cc2ff6864c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.h?ref=fa512f62ea2c85c8df85ce93326a63cc2ff6864c",
            "patch": "@@ -48,26 +48,7 @@ limitations under the License.\n namespace xla {\n namespace gpu {\n \n-// Emits LLVM IR for an \"unnested computation\".\n-//\n-// An unnested computation is an HloComputation which you run by executing one\n-// or more kernels for each HloInstruction it contains.  Examples of unnested\n-// computations:\n-//\n-//  - An HloModule's root computation,\n-//  - The body of an HLO while loop,\n-//  - The true/false computation of an HLO conditional.\n-//\n-// Note the opportunity for confusion -- the while loop's computation is nested\n-// within the root computation, but it's emitted using ThunkEmitter!  Don't\n-// think about it too hard.\n-//\n-// Examples of things that are not unnested computations:\n-//\n-//  - The body of a fusion node.  ThunkEmitter emits the relevant code\n-//    within a kernel function using FusedIrEmitter.  (FusedIrEmitter is not\n-//    really an IrEmitter, but is more an \"IR generator generator\".)\n-//\n+// Emits Thunks for the given HLO module.\n class ThunkEmitter {\n  public:\n   absl::string_view platform_name() const {\n@@ -80,14 +61,8 @@ class ThunkEmitter {\n   static std::unique_ptr<ThunkEmitter> Create(\n       IrEmitterContext* ir_emitter_context);\n \n-  // Transfers the ownership of thunk_sequence_ out.\n-  std::unique_ptr<SequentialThunk> ConsumeThunkSequence(\n-      Thunk::ThunkInfo thunk_info = Thunk::ThunkInfo{}) {\n-    return std::make_unique<SequentialThunk>(thunk_info,\n-                                             std::move(thunk_sequence_));\n-  }\n-\n-  absl::Status EmitHloEntryComputation(const HloModule* module);\n+  absl::StatusOr<ThunkSequence> EmitHloEntryComputation(\n+      const HloModule* module);\n \n  private:\n   explicit ThunkEmitter(IrEmitterContext* ir_emitter_context);\n@@ -99,97 +74,112 @@ class ThunkEmitter {\n   // the generated code and so must be initialized by XLA. The value of these\n   // constants will be stored in 'content'. Constants with initializers in the\n   // generated code will have empty 'content'.\n-  absl::Status EmitHloComputation(const HloComputation* computation);\n+  absl::StatusOr<ThunkSequence> EmitHloComputation(\n+      const HloComputation* computation);\n \n-  absl::Status EmitCommandBufferThunk(const HloInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitCommandBufferThunk(\n+      const HloInstruction* instr);\n \n   // ThunkEmitter handles the following instructions differently from\n   // IrEmitter. It also mixes in some special handling for custom kernels\n   // via the ThunkEmitter.\n   absl::Status EmitConstant(const HloConstantInstruction* instr);\n \n-  absl::Status EmitConditional(const HloInstruction* instr);\n-  absl::Status EmitConvolutionThunk(const HloCustomCallInstruction* instr);\n-  absl::Status EmitGemmThunk(const HloCustomCallInstruction* instr);\n-  absl::Status EmitCublasLtMatmulThunk(const HloCustomCallInstruction* instr);\n-  absl::Status EmitCublasLtMatmulThunkF8(const HloCustomCallInstruction* instr);\n-  absl::Status EmitConvolutionReorderThunk(\n+  absl::StatusOr<ThunkSequence> EmitConditional(const HloInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitConvolutionThunk(\n+      const HloCustomCallInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitGemmThunk(\n+      const HloCustomCallInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitCublasLtMatmulThunk(\n+      const HloCustomCallInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitCublasLtMatmulThunkF8(\n+      const HloCustomCallInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitConvolutionReorderThunk(\n+      const HloCustomCallInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitNormThunk(\n+      const HloCustomCallInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitCuDnnThunk(\n+      const HloCustomCallInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitPtxCustomCall(\n       const HloCustomCallInstruction* instr);\n-  absl::Status EmitNormThunk(const HloCustomCallInstruction* instr);\n-  absl::Status EmitCuDnnThunk(const HloCustomCallInstruction* instr);\n-  absl::Status EmitPtxCustomCall(const HloCustomCallInstruction* instr);\n-  absl::Status EmitCubDeviceRadixSort(const HloCustomCallInstruction* instr);\n-  absl::Status EmitCustomCallThunk(const HloCustomCallInstruction* instr);\n-  absl::Status EmitFftThunk(const HloFftInstruction* instr);\n-  absl::Status EmitAsyncComputation(const HloInstruction* instr);\n-  absl::Status EmitFusion(const HloFusionInstruction* instr);\n-  absl::Status EmitCopy(const HloInstruction* instr);\n-  absl::Status EmitAsyncCustomCallStart(const HloInstruction* instr);\n-  absl::Status EmitWhile(const HloInstruction* instr);\n-  absl::Status EmitInfeed(const HloInfeedInstruction* instr);\n-  absl::Status EmitOutfeed(const HloOutfeedInstruction* instr);\n-  absl::Status EmitRngGetAndUpdateState(\n+  absl::StatusOr<ThunkSequence> EmitCubDeviceRadixSort(\n+      const HloCustomCallInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitCustomCallThunk(\n+      const HloCustomCallInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitFftThunk(const HloFftInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitAsyncComputation(\n+      const HloInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitFusion(const HloFusionInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitCopy(const HloInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitAsyncCustomCallStart(\n+      const HloInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitWhile(const HloInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitInfeed(const HloInfeedInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitOutfeed(const HloOutfeedInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitRngGetAndUpdateState(\n       const HloRngGetAndUpdateStateInstruction* instr);\n \n-  absl::Status EmitSort(const HloSortInstruction* sort);\n-  absl::Status EmitTriangularSolveCustomCall(const HloInstruction* instr);\n-  absl::Status EmitTopKCustomCall(const HloCustomCallInstruction* instr);\n-  absl::Status EmitTritonCustomCall(const HloCustomCallInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitSort(const HloSortInstruction* sort);\n+  absl::StatusOr<ThunkSequence> EmitTriangularSolveCustomCall(\n+      const HloInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitTopKCustomCall(\n+      const HloCustomCallInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitTritonCustomCall(\n+      const HloCustomCallInstruction* instr);\n \n-  absl::Status EmitSendThunk(const HloSendInstruction* instr);\n-  absl::Status EmitSendDoneThunk(const HloSendDoneInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitSendThunk(const HloSendInstruction* instr,\n+                                              bool emit_group_thunks);\n+  absl::StatusOr<ThunkSequence> EmitSendDoneThunk(\n+      const HloSendDoneInstruction* instr);\n \n-  absl::Status EmitRecvThunk(const HloRecvInstruction* instr);\n-  absl::Status EmitRecvDoneThunk(const HloRecvDoneInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitRecvThunk(const HloRecvInstruction* instr,\n+                                              bool emit_group_thunks);\n+  absl::StatusOr<ThunkSequence> EmitRecvDoneThunk(\n+      const HloRecvDoneInstruction* instr);\n \n   template <typename CollectiveThunkType, typename HloInstType>\n-  absl::Status EmitCollectiveThunk(Thunk::Kind kind,\n-                                   const HloInstruction* async_start,\n-                                   const HloInstType* inst,\n-                                   std::optional<bool> use_global_device_ids);\n+  absl::StatusOr<ThunkSequence> EmitCollectiveThunk(\n+      Thunk::Kind kind, const HloInstruction* async_start,\n+      const HloInstType* inst, std::optional<bool> use_global_device_ids);\n \n-  absl::Status EmitCollectiveAsyncDone(Thunk::Kind kind,\n-                                       const HloInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitCollectiveAsyncDone(\n+      Thunk::Kind kind, const HloInstruction* instr);\n \n   template <typename NvshmemAllReduceThunkType,\n             typename HloAllReduceInstruction>\n-  absl::Status EmitNvshmemThunk(Thunk::Kind kind,\n-                                const HloInstruction* async_start,\n-                                const HloAllReduceInstruction* inst,\n-                                std::optional<bool> use_global_device_ids);\n+  absl::StatusOr<ThunkSequence> EmitNvshmemThunk(\n+      Thunk::Kind kind, const HloInstruction* async_start,\n+      const HloAllReduceInstruction* inst,\n+      std::optional<bool> use_global_device_ids);\n \n-  absl::Status EmitNvshmemAsyncDone(Thunk::Kind kind,\n-                                    const HloInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitNvshmemAsyncDone(\n+      Thunk::Kind kind, const HloInstruction* instr);\n \n   template <typename HloInstType>\n-  absl::Status EmitDegeneratedCollectiveThunk(\n+  absl::StatusOr<ThunkSequence> EmitDegeneratedCollectiveThunk(\n       std::vector<CollectiveThunk::Buffer>& buffers,\n       const HloInstruction* async_start, const HloInstType* inst);\n \n   template <typename ThunkType>\n-  absl::Status EmitReplicaOrPartitionId(const HloInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitReplicaOrPartitionId(\n+      const HloInstruction* instr);\n \n-  absl::Status EmitCollectiveMetadata(const HloInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitCollectiveMetadata(\n+      const HloInstruction* instr);\n \n-  absl::Status EmitCollectivePermute(\n+  absl::StatusOr<ThunkSequence> EmitCollectivePermute(\n       const HloCollectivePermuteInstruction* instr);\n \n-  absl::Status EmitCopyStartThunk(const HloCopyStartInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitCopyStartThunk(\n+      const HloCopyStartInstruction* instr);\n \n-  absl::Status EmitCopyDoneThunk(const HloInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitCopyDoneThunk(const HloInstruction* instr);\n \n-  absl::Status EmitHloInstruction(const HloInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitHloInstruction(\n+      const HloInstruction* instr, bool emit_group_thunks = false);\n \n-  absl::Status EmitCollectiveGroupStartThunk(const HloInstruction* instr);\n-\n-  // Add a owning Thunk object to the thunk sequence.\n-  void AddThunkToThunkSequence(std::unique_ptr<Thunk> thunk) {\n-    if (emit_group_thunks_) {\n-      scoped_thunk_sequence_.emplace_back(std::move(thunk));\n-      return;\n-    }\n-    thunk_sequence_.emplace_back(std::move(thunk));\n-  }\n+  absl::StatusOr<ThunkSequence> EmitCollectiveGroupStartThunk(\n+      const HloInstruction* instr);\n \n   // Input = {static array, dynamic_dim0, dynamic_dim1}\n   // Output = {dynamic array(with dynamic dimension meta data at the end)}\n@@ -235,7 +225,8 @@ class ThunkEmitter {\n   //   return;\n   // }\n   //   ```\n-  absl::Status EmitPadToStatic(const HloCustomCallInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitPadToStatic(\n+      const HloCustomCallInstruction* instr);\n \n   // Input = {dynamic array(with dynamic dimension meta data at the end)}\n   // Output = {static array, dynamic_dim0, dynamic_dim1}\n@@ -281,7 +272,8 @@ class ThunkEmitter {\n   //   return;\n   // }\n   //   ```\n-  absl::Status EmitSliceToDynamic(const HloCustomCallInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitSliceToDynamic(\n+      const HloCustomCallInstruction* instr);\n \n   // Returns a WhileThunk that invokes thunk sequences for 'condition' and\n   // 'body' sub-computations of while instruction.\n@@ -304,11 +296,6 @@ class ThunkEmitter {\n   }\n   IrEmitterContext* ir_emitter_context_;\n \n-  // The thunk sequence this IrEmitter generates for the input computation.\n-  ThunkSequence thunk_sequence_;\n-  ThunkSequence scoped_thunk_sequence_;\n-  bool emit_group_thunks_ = false;\n-\n   // Container for async host send/recv events shared by host send/recv thunks.\n   std::shared_ptr<HostSendRecvAsyncEvents> send_recv_events_;\n "
        }
    ],
    "stats": {
        "total": 742,
        "additions": 363,
        "deletions": 379
    }
}