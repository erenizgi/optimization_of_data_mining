{
    "author": "othakkar",
    "message": "PR #31025: [XLA:CPU][oneDNN] Add a flag to enable oneDNN Custom Calls in Thunk Runtime\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31025\n\nOneDNN custom calls are currently being enabled in an experimental mode. This PR adds a runtime flag (default `false`) to enable oneDNN rewrites via a custom call for the thunk runtime.\nCopybara import of the project:\n\n--\n22a89f16a6ddf3c371c0a6afa31c3d6ac1de760f by Om Thakkar <om.thakkar@intel.com>:\n\nadd a flag to enable oneDNN custom calls in thunk runtime\n\nMerging this change closes #31025\n\nPiperOrigin-RevId: 807412132",
    "sha": "8ae357f6355e514952e5699464aa1cc453931ce9",
    "files": [
        {
            "sha": "dabdc2359462eb7eb2768abb0a22f195076be89a",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8ae357f6355e514952e5699464aa1cc453931ce9/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8ae357f6355e514952e5699464aa1cc453931ce9/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=8ae357f6355e514952e5699464aa1cc453931ce9",
            "patch": "@@ -204,6 +204,7 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_gpu_unsupported_annotate_with_emitter_loc(false);\n   opts.set_xla_debug_buffer_assignment_show_max(15);\n   opts.set_xla_cpu_use_onednn(false);\n+  opts.set_xla_cpu_experimental_onednn_custom_call(false);\n #ifdef XLA_CPU_USE_ACL\n   opts.set_xla_cpu_use_acl(true);\n #endif\n@@ -1046,6 +1047,12 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n                 debug_options->xla_cpu_use_onednn(),\n                 \"Call oneDNN thunks for matmul and convolution fusions in the \"\n                 \"CPU backend.\"));\n+  flag_list->push_back(\n+      tsl::Flag(\"xla_cpu_experimental_onednn_custom_call\",\n+                bool_setter_for(\n+                    &DebugOptions::set_xla_cpu_experimental_onednn_custom_call),\n+                debug_options->xla_cpu_experimental_onednn_custom_call(),\n+                \"Call oneDNN custom call thunks in the CPU backend.\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_cpu_experimental_onednn_fusion_type\",\n       SetterForRepeatedEnum<DebugOptions::LibraryFusionType>("
        },
        {
            "sha": "7316434a7e40a866e716c63bc75797c4be533ee7",
            "filename": "third_party/xla/xla/service/cpu/cpu_compiler.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8ae357f6355e514952e5699464aa1cc453931ce9/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8ae357f6355e514952e5699464aa1cc453931ce9/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc?ref=8ae357f6355e514952e5699464aa1cc453931ce9",
            "patch": "@@ -679,12 +679,14 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n \n   // Rewrite to custom calls with target as oneDNN library calls.\n #if defined(INTEL_MKL)\n-  // AOT compiled code runs in single thread.\n+  // This pass is not supported in the thunk runtime yet.\n   bool is_thunk_runtime = true;\n-  // TODO(intel-tf): Use IsOneDnnCompatible function to determine whether to\n-  // enable OneDnnOpsRewriter after enabling the OneDnnOpsRewriter with thunk\n-  // runtime.\n-  if (!is_thunk_runtime) {\n+  bool use_onednn_custom_call =\n+      module->config()\n+          .debug_options()\n+          .xla_cpu_experimental_onednn_custom_call() &&\n+      is_onednn_compatible;\n+  if (use_onednn_custom_call && !is_thunk_runtime) {\n     // Placing OneDnnOpsRewriter here to match the flax patterns\n     // TODO: Decide where would be the appropriate place for this pass to make\n     // it more generic\n@@ -902,7 +904,10 @@ absl::Status CpuCompiler::RunHloPassesAfterLayoutAssn(\n \n #if defined(INTEL_MKL)\n   // AOT compiled code runs in single thread.\n-  if (is_onednn_compatible) {\n+  bool use_onednn_custom_call =\n+      debug_options.xla_cpu_experimental_onednn_custom_call() &&\n+      is_onednn_compatible;\n+  if (use_onednn_custom_call) {\n     // Run SimplifyFPConversions pass to simplify the BF16 pattern and make it\n     // easier to match.\n     // Remove `f32 -> bf16 -> f32` casts inserted by bf16 normalization."
        },
        {
            "sha": "851d91bf9189854e932112bcbb65578702210645",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8ae357f6355e514952e5699464aa1cc453931ce9/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8ae357f6355e514952e5699464aa1cc453931ce9/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=8ae357f6355e514952e5699464aa1cc453931ce9",
            "patch": "@@ -193,6 +193,9 @@ message DebugOptions {\n   // below!\n   optional bool xla_cpu_enable_fast_min_max = 140;\n \n+  // Call oneDNN custom call thunks in the CPU backend\n+  optional bool xla_cpu_experimental_onednn_custom_call = 412;\n+\n   // Stores the fusion types enabled for oneDNN in DotLibraryRewriter pass.\n   repeated LibraryFusionType xla_cpu_experimental_onednn_fusion_type = 399;\n \n@@ -1335,7 +1338,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 412\n+  // Next id: 413\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 29,
        "additions": 22,
        "deletions": 7
    }
}