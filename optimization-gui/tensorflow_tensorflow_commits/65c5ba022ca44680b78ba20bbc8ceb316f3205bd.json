{
    "author": "KanishAnand",
    "message": "Delete unused `ReshapeToTileDimension`, `ContainsTileSharding` functions.\n\nPiperOrigin-RevId: 837839463",
    "sha": "65c5ba022ca44680b78ba20bbc8ceb316f3205bd",
    "files": [
        {
            "sha": "dcb451b08b0c67a5c8f57f552c0d9e8dbdfaa88b",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 55,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/65c5ba022ca44680b78ba20bbc8ceb316f3205bd/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/65c5ba022ca44680b78ba20bbc8ceb316f3205bd/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc?ref=65c5ba022ca44680b78ba20bbc8ceb316f3205bd",
            "patch": "@@ -1150,61 +1150,6 @@ HloSharding ReverseSharding(const HloSharding& sharding,\n                                      sharding.metadata());\n }\n \n-HloSharding ReshapeToTileDimension(const HloSharding& sharding, int64_t dim,\n-                                   absl::Span<const int64_t> dims) {\n-  CHECK(!sharding.IsTuple() && !sharding.IsTileMaximal());\n-  CHECK_NE(absl::c_find(dims, dim), dims.end()) << \"dim is not in dims\";\n-  // We optimize the tile assignment on the single dimension dim in a way to\n-  // minimize communication among devices caused by the reshard:\n-  // +---+---+               +---+---+              +-+-+-+-+\n-  // |   |   |               |   0   |              | | | | |\n-  // | 0 | 1 |               +-------+              | | | | |\n-  // |   |   |  reshape on   |   1   |  reshape on  | | | | |\n-  // +---+---+   dim 0  =>   +-------+   dim 1  =>  |0|2|1|3|\n-  // |   |   |               |   2   |              | | | | |\n-  // | 2 | 3 |               +-------+              | | | | |\n-  // |   |   |               |   3   |              | | | | |\n-  // +---+---+               +---+---+              +-+-+-+-+\n-\n-  auto old_dims = sharding.tile_assignment().dimensions();\n-  DimensionVector new_dims(old_dims.begin(), old_dims.end());\n-  std::vector<int> not_in_dims, dims_except_the_dim;\n-  for (int64_t i = 0; i < sharding.tile_assignment().num_dimensions(); ++i) {\n-    if (i == dim) {\n-      continue;\n-    } else if (absl::c_find(dims, i) != dims.end()) {\n-      dims_except_the_dim.push_back(i);\n-      new_dims[dim] *= old_dims[i];\n-      new_dims[i] = 1;\n-    } else {\n-      not_in_dims.push_back(i);\n-    }\n-  }\n-  // perm = not_in_dims + {dim} + dims_except_the_dim\n-  std::vector<int> perm;\n-  perm.reserve(sharding.tile_assignment().num_dimensions());\n-  perm.insert(perm.end(), not_in_dims.begin(), not_in_dims.end());\n-  perm.push_back(dim);\n-  perm.insert(perm.end(), dims_except_the_dim.begin(),\n-              dims_except_the_dim.end());\n-\n-  auto new_tile_assignment =\n-      sharding.tile_assignment().Transpose(perm).Reshape(new_dims);\n-  return HloSharding::Tile(new_tile_assignment, sharding.metadata());\n-}\n-\n-bool ContainsTileSharding(const HloModule& module) {\n-  for (const HloComputation* computation : module.computations()) {\n-    for (const HloInstruction* instruction : computation->instructions()) {\n-      if (instruction->has_sharding() &&\n-          !instruction->sharding().IsTileMaximal()) {\n-        return true;\n-      }\n-    }\n-  }\n-  return false;\n-}\n-\n HloSharding PropagateShardingAlongDimsAndReplicateOthers(\n     const HloSharding& source_sharding, absl::Span<const int64_t> source_dims,\n     absl::Span<const int64_t> target_dims, int64_t target_shape_rank) {"
        },
        {
            "sha": "62a37d1b605592d3fe2a599d74ceecd3923b8891",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util.h",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/65c5ba022ca44680b78ba20bbc8ceb316f3205bd/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/65c5ba022ca44680b78ba20bbc8ceb316f3205bd/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.h?ref=65c5ba022ca44680b78ba20bbc8ceb316f3205bd",
            "patch": "@@ -181,17 +181,6 @@ HloSharding PropagateShardingThroughReshape(const Shape& source_shape,\n HloSharding ReverseSharding(const HloSharding& sharding,\n                             absl::Span<const int64_t> dimensions);\n \n-// Returns a sharding tiled on unique dimension dim by reshaping the tile\n-// assignment of the sharding argument. Only dimensions in the dims span\n-// argument are considered for reshaping, the others are ignored.\n-// Assumptions: sharding is tile sharded, and dim must be included in dims.\n-HloSharding ReshapeToTileDimension(const HloSharding& sharding, int64_t dim,\n-                                   absl::Span<const int64_t> dims);\n-\n-// Returns true if the provided module includes one or more instructions with\n-// a tile sharding.\n-bool ContainsTileSharding(const HloModule& module);\n-\n // Returns the preferred output sharding for a gather op based on the sharding\n // of the indices.\n HloSharding GatherOutputShardingFromIndex(const HloSharding& index_sharding,"
        },
        {
            "sha": "bfc8ba586e02fb07ad121e803deea58f52b7e971",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 98,
            "changes": 98,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/65c5ba022ca44680b78ba20bbc8ceb316f3205bd/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/65c5ba022ca44680b78ba20bbc8ceb316f3205bd/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util_test.cc?ref=65c5ba022ca44680b78ba20bbc8ceb316f3205bd",
            "patch": "@@ -487,104 +487,6 @@ TEST(HloShardingUtilTest, ReshapeShardingWithPadding2) {\n   EXPECT_EQ(result.value(), output_sharding);\n }\n \n-TEST(HloShardingUtilTest, ReshapeToTileDimension2D) {\n-  // The two sharding in the vector are the same. They will be processed in\n-  // different branches in ReshapeToTileDimension.\n-  std::vector<HloSharding> shardings = {HloSharding::IotaTile({2, 2}),\n-                                        HloSharding::Tile({{0, 1}, {2, 3}})};\n-\n-  for (const HloSharding& sharding : shardings) {\n-    EXPECT_EQ(ReshapeToTileDimension(sharding, /*dim=*/0, /*dims=*/{0, 1})\n-                  .tile_assignment(),\n-              TileAssignment({4, 1}));\n-    EXPECT_EQ(ReshapeToTileDimension(sharding, /*dim=*/1, /*dims=*/{0, 1})\n-                  .tile_assignment(),\n-              TileAssignment({1, 4}, {2, 2}, {1, 0}));\n-  }\n-}\n-\n-TEST(HloShardingUtilTest, ReshapeToTileDimension3D_Case1) {\n-  std::vector<HloSharding> shardings = {\n-      HloSharding::IotaTile({2, 2, 2}),\n-      HloSharding::Tile({{{0, 1}, {2, 3}}, {{4, 5}, {6, 7}}})};\n-\n-  for (const HloSharding& sharding : shardings) {\n-    EXPECT_EQ(ReshapeToTileDimension(sharding, /*dim=*/0, /*dims=*/{0, 1, 2})\n-                  .tile_assignment(),\n-              TileAssignment({8, 1, 1}));\n-    EXPECT_EQ(ReshapeToTileDimension(sharding, /*dim=*/1, /*dims=*/{0, 1, 2})\n-                  .tile_assignment(),\n-              TileAssignment({1, 8, 1}, {2, 2, 2}, {1, 0, 2}));\n-    EXPECT_EQ(ReshapeToTileDimension(sharding, /*dim=*/2, /*dims=*/{0, 1, 2})\n-                  .tile_assignment(),\n-              TileAssignment({1, 1, 8}, {4, 2}, {1, 0}));\n-\n-    EXPECT_EQ(ReshapeToTileDimension(sharding, /*dim=*/2,\n-                                     /*dims=*/{1, 2})\n-                  .tile_assignment(),\n-              TileAssignment({2, 1, 4}, {2, 2, 2}, {0, 2, 1}));\n-    EXPECT_EQ(ReshapeToTileDimension(sharding, /*dim=*/0,\n-                                     /*dims=*/{0, 2})\n-                  .tile_assignment(),\n-              TileAssignment({4, 2, 1}, {2, 2, 2}, {1, 0, 2}));\n-    EXPECT_EQ(ReshapeToTileDimension(sharding, /*dim=*/2,\n-                                     /*dims=*/{0, 2})\n-                  .tile_assignment(),\n-              TileAssignment({1, 2, 4}, {2, 2, 2}, {1, 2, 0}));\n-  }\n-}\n-\n-TEST(HloShardingUtilTest, ReshapeToTileDimension3D_Case2) {\n-  // The input sharding has a complicated device list.\n-  std::vector<HloSharding> shardings = {\n-      HloSharding::IotaTile({2, 2, 2}, {4, 2}, {1, 0}),\n-      HloSharding::Tile({{{0, 2}, {4, 6}}, {{1, 3}, {5, 7}}})};\n-  for (const HloSharding& sharding : shardings) {\n-    EXPECT_EQ(ReshapeToTileDimension(sharding, /*dim=*/0, /*dims=*/{0, 1, 2})\n-                  .tile_assignment(),\n-              TileAssignment({8, 1, 1}, {4, 2}, {1, 0}));\n-    EXPECT_EQ(ReshapeToTileDimension(sharding, /*dim=*/1, /*dims=*/{0, 1, 2})\n-                  .tile_assignment(),\n-              TileAssignment({1, 8, 1}, {2, 2, 2}, {0, 2, 1}));\n-    EXPECT_EQ(ReshapeToTileDimension(sharding, /*dim=*/2, /*dims=*/{0, 1, 2})\n-                  .tile_assignment(),\n-              TileAssignment({1, 1, 8}, {2, 4}, {1, 0}));\n-  }\n-}\n-\n-TEST(HloShardingUtilTest, ReshapeToTileDimension4D) {\n-  HloSharding sharding1 = HloSharding::IotaTile({2, 3, 5, 7});\n-  HloSharding sharding2 =\n-      HloSharding::Tile(sharding1.tile_assignment().array());\n-  std::vector<HloSharding> shardings = {sharding1, sharding2};\n-\n-  for (const HloSharding& sharding : shardings) {\n-    EXPECT_EQ(ReshapeToTileDimension(sharding, /*dim=*/1, /*dims=*/{0, 1})\n-                  .tile_assignment(),\n-              TileAssignment({1, 6, 5, 7}, {2, 3, 5, 7}, {2, 3, 1, 0}));\n-    EXPECT_EQ(ReshapeToTileDimension(sharding, /*dim=*/1, /*dims=*/{1, 2})\n-                  .tile_assignment(),\n-              TileAssignment({2, 15, 1, 7}, {2, 3, 5, 7}, {0, 3, 1, 2}));\n-    EXPECT_EQ(ReshapeToTileDimension(sharding, /*dim=*/1, /*dims=*/{1, 3})\n-                  .tile_assignment(),\n-              TileAssignment({2, 21, 5, 1}, {2, 3, 5, 7}, {0, 2, 1, 3}));\n-\n-    EXPECT_EQ(ReshapeToTileDimension(sharding, /*dim=*/1, /*dims=*/{0, 1, 2})\n-                  .tile_assignment(),\n-              TileAssignment({1, 30, 1, 7}, {2, 3, 5, 7}, {3, 1, 0, 2}));\n-    EXPECT_EQ(ReshapeToTileDimension(sharding, /*dim=*/1, /*dims=*/{0, 1, 3})\n-                  .tile_assignment(),\n-              TileAssignment({1, 42, 5, 1}, {2, 3, 5, 7}, {2, 1, 0, 3}));\n-    EXPECT_EQ(ReshapeToTileDimension(sharding, /*dim=*/1, /*dims=*/{1, 2, 3})\n-                  .tile_assignment(),\n-              TileAssignment({2, 105, 1, 1}, {2, 3, 5, 7}, {0, 1, 2, 3}));\n-\n-    EXPECT_EQ(ReshapeToTileDimension(sharding, /*dim=*/1, /*dims=*/{0, 1, 2, 3})\n-                  .tile_assignment(),\n-              TileAssignment({1, 210, 1, 1}, {2, 3, 5, 7}, {1, 0, 2, 3}));\n-  }\n-}\n-\n TEST(HloShardingUtilTest, PropagateReshapeShardingTranspose1) {\n   Shape input_shape = ShapeUtil::MakeShape(F32, {6, 4});\n   Shape output_shape = ShapeUtil::MakeShape(F32, {2, 2, 3, 2});"
        }
    ],
    "stats": {
        "total": 164,
        "additions": 0,
        "deletions": 164
    }
}