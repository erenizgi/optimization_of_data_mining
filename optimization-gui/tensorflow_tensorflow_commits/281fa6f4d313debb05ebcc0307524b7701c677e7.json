{
    "author": "tensorflower-gardener",
    "message": "Fix dot library predicates\n\nThis changes the predicates for calling a library for a dot to indicate whether we will actually call the library, not just whether the library supports the dot. This fixes bugs where we incorrectly claim a library will handle the dot.\n\nPiperOrigin-RevId: 825231317",
    "sha": "281fa6f4d313debb05ebcc0307524b7701c677e7",
    "files": [
        {
            "sha": "85ab1f890cc362f6298ff209586f4b161ab290c3",
            "filename": "third_party/xla/xla/service/cpu/cpu_compiler.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 8,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/281fa6f4d313debb05ebcc0307524b7701c677e7/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/281fa6f4d313debb05ebcc0307524b7701c677e7/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc?ref=281fa6f4d313debb05ebcc0307524b7701c677e7",
            "patch": "@@ -654,17 +654,30 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n   pipeline.AddPass<BatchedGatherScatterNormalizer>();\n   pipeline.AddPass<ResultCaster>();\n \n-  // If XNNPACK is enabled, we only need to upcast dots that XnnDotThunk does\n-  // not support. `upcaster_filter` returns false if the instruction shouldn't\n-  // be processed.\n   auto library_supports_dot =\n       LibrarySupportsDot(module, target_machine_features);\n \n-  HloPredicate upcaster_filter = [&](const HloInstruction* instr) {\n-    if (instr->opcode() != HloOpcode::kDot) {\n-      return true;\n+  auto call_library_for_dot = [&](const HloInstruction& instr) {\n+    if (instr.opcode() != HloOpcode::kDot) {\n+      return false;\n+    }\n+\n+    auto dot_strategy = GetDotImplementationStrategy(\n+        module->config(), instr, *target_machine_features,\n+        /*allow_runtime_calls=*/true);\n+    if (dot_strategy != DotImplementationStrategy::kEigen) {\n+      // We aren't going to call a library for this dot.\n+      return false;\n     }\n-    return !library_supports_dot(*instr);\n+\n+    return library_supports_dot(instr);\n+  };\n+\n+  // If YNNPACK is enabled, we only need to upcast dots that YnnDotThunk does\n+  // not support. `upcaster_filter` returns false if the instruction shouldn't\n+  // be processed.\n+  HloPredicate upcaster_filter = [&](const HloInstruction* instr) {\n+    return !call_library_for_dot(*instr);\n   };\n \n   // xla::cpu::GetDotImplementationStrategy (used by call_library_for_dot)\n@@ -728,7 +741,7 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n   // Convert BF16 and F8 operations to F32 and F16 respectively so that the CPU\n   // backend can support BF16/F8 operations without directly implementing a\n   // BF16/F8 lowering for most ops.\n-  CpuFloatSupport bf16_support(BF16, library_supports_dot);\n+  CpuFloatSupport bf16_support(BF16, call_library_for_dot);\n #ifdef XLA_ONEDNN\n   OneDnnFloatSupport onednn_bf16_support(BF16);\n   if (use_onednn_custom_call) {"
        }
    ],
    "stats": {
        "total": 29,
        "additions": 21,
        "deletions": 8
    }
}