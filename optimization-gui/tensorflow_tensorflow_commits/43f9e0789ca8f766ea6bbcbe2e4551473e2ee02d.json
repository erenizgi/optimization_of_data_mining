{
    "author": "frgossen",
    "message": "[XLA:GPU] Add verbose kernel scheduling tracing for debugging\n\nPiperOrigin-RevId: 818918076",
    "sha": "43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d",
    "files": [
        {
            "sha": "5d7dff2ad7f93a376a62f31962c4397b128f3f82",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d",
            "patch": "@@ -932,6 +932,8 @@ cc_library(\n         \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n+        \"@local_tsl//tsl/profiler/lib:traceme\",\n+        \"@local_tsl//tsl/profiler/lib:traceme_encode\",\n     ],\n )\n "
        },
        {
            "sha": "2e9b2dafcfbdb1f5171bca398bfa62cd28f6e555",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk.cc",
            "status": "modified",
            "additions": 57,
            "deletions": 23,
            "changes": 80,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc?ref=43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d",
            "patch": "@@ -50,6 +50,12 @@ limitations under the License.\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"tsl/profiler/lib/traceme.h\"\n+#include \"tsl/profiler/lib/traceme_encode.h\"\n+\n+using tsl::profiler::TraceMe;\n+using tsl::profiler::TraceMeEncode;\n+using tsl::profiler::TraceMeLevel;\n \n namespace xla {\n namespace gpu {\n@@ -223,43 +229,71 @@ static void PrintBufferContents(\n }\n \n absl::Status KernelThunk::ExecuteOnStream(const ExecuteParams& params) {\n+  TraceMe trace(\n+      [] { return TraceMeEncode(\"KernelThunk::ExecuteOnStream\", {}); },\n+      /*level=*/TraceMeLevel::kVerbose);\n+\n   // Load the kernel.\n   se::StreamExecutor* executor = params.stream->parent();\n   se::Kernel* kernel = nullptr;\n \n-  TF_ASSIGN_OR_RETURN(\n-      se::Stream * stream,\n-      GetStreamForExecution(Thunk::execution_stream_id(), params));\n+  se::Stream* stream = nullptr;\n+  {\n+    TraceMe trace(\n+        [] {\n+          return TraceMeEncode(\n+              \"KernelThunk::ExecuteOnStream/GetStreamForExecution\", {});\n+        },\n+        /*level=*/TraceMeLevel::kVerbose);\n+    TF_ASSIGN_OR_RETURN(\n+        stream, GetStreamForExecution(Thunk::execution_stream_id(), params));\n+  }\n \n   {\n+    TraceMe trace(\n+        [] { return TraceMeEncode(\"KernelThunk::ExecuteOnStream/mutex\", {}); },\n+        /*level=*/TraceMeLevel::kVerbose);\n     absl::MutexLock lock(mutex_);\n+    TraceMe trace_find(\n+        [] {\n+          return TraceMeEncode(\"KernelThunk::ExecuteOnStream/mutex/find\", {});\n+        },\n+        /*level=*/TraceMeLevel::kVerbose);\n     auto it = kernel_cache_.find(executor);\n     CHECK(it != kernel_cache_.end())\n         << \"Initialize() not called for StreamExecutor \" << executor;\n     kernel = it->second.get();\n   }\n \n-  int device_ordinal = executor->device_ordinal();\n-  VLOG(3) << \"[\" << device_ordinal << \"] Launching \" << kernel->name();\n   absl::InlinedVector<se::KernelArgument, 4> kernel_args;\n-  for (const auto& [idx, arg] : llvm::enumerate(args_)) {\n-    se::DeviceMemoryBase buf = params.buffer_allocations->GetDeviceAddress(arg);\n-    VLOG(3) << \"[\" << device_ordinal << \"] Arg: alloc #\" << arg.index()\n-            << \", offset: \" << arg.offset() << \": \" << buf.opaque() << \" (\"\n-            << buf.size() << \"B)\";\n-\n-    if (auto it = tma_metadata_.arg_index_to_tma_info.find(idx);\n-        it != tma_metadata_.arg_index_to_tma_info.end()) {\n-      // TMA descriptor argument.\n-      const se::gpu::TmaDescriptor& tma_desc = it->second;\n-      TF_ASSIGN_OR_RETURN(se::TensorMap tensor_map,\n-                          executor->CreateTensorMap(tma_desc, buf.opaque()));\n-      VLOG(3) << \"[\" << device_ordinal << \"]  Using TensorMap for arg #\" << idx\n-              << \": \" << tma_desc.ToString();\n-      kernel_args.push_back(std::move(tensor_map));\n-    } else {\n-      // Buffer argument.\n-      kernel_args.push_back(buf);\n+  {\n+    TraceMe trace(\n+        [] {\n+          return TraceMeEncode(\"KernelThunk::ExecuteOnStream/kernel_args\", {});\n+        },\n+        /*level=*/TraceMeLevel::kVerbose);\n+    int device_ordinal = executor->device_ordinal();\n+    VLOG(3) << \"[\" << device_ordinal << \"] Launching \" << kernel->name();\n+    for (const auto& [idx, arg] : llvm::enumerate(args_)) {\n+      se::DeviceMemoryBase buf =\n+          params.buffer_allocations->GetDeviceAddress(arg);\n+      VLOG(3) << \"[\" << device_ordinal << \"] Arg: alloc #\" << arg.index()\n+              << \", offset: \" << arg.offset() << \": \" << buf.opaque() << \" (\"\n+              << buf.size() << \"B)\";\n+\n+      if (auto it = tma_metadata_.arg_index_to_tma_info.find(idx);\n+          it != tma_metadata_.arg_index_to_tma_info.end()) {\n+        // TMA descriptor argument.\n+        const se::gpu::TmaDescriptor& tma_desc = it->second;\n+        TF_ASSIGN_OR_RETURN(se::TensorMap tensor_map,\n+                            executor->CreateTensorMap(tma_desc, buf.opaque()));\n+        VLOG(3) << \"[\" << device_ordinal << \"]  Using TensorMap for arg #\"\n+                << idx << \": \" << tma_desc.ToString();\n+        kernel_args.push_back(std::move(tensor_map));\n+      } else {\n+        // Buffer argument.\n+        kernel_args.push_back(buf);\n+      }\n     }\n   }\n "
        },
        {
            "sha": "715b409bf8b63ab87fba17be661b08a6e025e0a5",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d",
            "patch": "@@ -2625,6 +2625,8 @@ cc_library(\n         \"@local_tsl//tsl/platform:ml_dtypes\",\n         \"@local_tsl//tsl/platform:status\",\n         \"@local_tsl//tsl/platform:statusor\",\n+        \"@local_tsl//tsl/profiler/lib:traceme\",\n+        \"@local_tsl//tsl/profiler/lib:traceme_encode\",\n     ],\n )\n "
        },
        {
            "sha": "53f6f72afc5ca03fb7d5f56b08df21c4209cf766",
            "filename": "third_party/xla/xla/service/gpu/stream_executor_util.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 3,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fstream_executor_util.cc?ref=43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d",
            "patch": "@@ -67,6 +67,12 @@ limitations under the License.\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/ml_dtypes.h\"\n+#include \"tsl/profiler/lib/traceme.h\"\n+#include \"tsl/profiler/lib/traceme_encode.h\"\n+\n+using tsl::profiler::TraceMe;\n+using tsl::profiler::TraceMeEncode;\n+using tsl::profiler::TraceMeLevel;\n \n namespace xla {\n namespace gpu {\n@@ -405,9 +411,19 @@ absl::Status ExecuteKernelOnStream(\n     se::Kernel& kernel, absl::Span<const se::KernelArgument> args,\n     const LaunchDimensions& dims,\n     const std::optional<se::ClusterDim>& cluster_dim, se::Stream* stream) {\n-  TF_ASSIGN_OR_RETURN(\n-      std::unique_ptr<se::KernelArgsPackedArrayBase> kernel_args,\n-      se::PackKernelArgs(args, kernel.metadata()));\n+  TraceMe trace([] { return TraceMeEncode(\"ExecuteKernelOnStream\", {}); },\n+                /*level=*/TraceMeLevel::kVerbose);\n+\n+  std::unique_ptr<se::KernelArgsPackedArrayBase> kernel_args;\n+  {\n+    TraceMe trace(\n+        [] {\n+          return TraceMeEncode(\"ExecuteKernelOnStream/PackKernelArgs\", {});\n+        },\n+        /*level=*/TraceMeLevel::kVerbose);\n+    TF_ASSIGN_OR_RETURN(kernel_args,\n+                        se::PackKernelArgs(args, kernel.metadata()));\n+  }\n \n   return kernel.Launch(dims.thread_counts_per_block(), dims.block_counts(),\n                        cluster_dim, stream, *kernel_args);"
        },
        {
            "sha": "573889d337985b9d3b1c8df55ac6c2e3bbe261bc",
            "filename": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD?ref=43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d",
            "patch": "@@ -699,6 +699,8 @@ cc_library(\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n         \"@local_tsl//tsl/platform:logging\",\n+        \"@local_tsl//tsl/profiler/lib:traceme\",\n+        \"@local_tsl//tsl/profiler/lib:traceme_encode\",\n     ],\n )\n \n@@ -1380,6 +1382,8 @@ cc_library(\n         \"@com_google_absl//absl/synchronization\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n         \"@local_tsl//tsl/profiler/lib:nvtx_utils\",\n+        \"@local_tsl//tsl/profiler/lib:traceme\",\n+        \"@local_tsl//tsl/profiler/lib:traceme_encode\",\n     ],\n )\n "
        },
        {
            "sha": "367c6aff4ff893c64bb90f5a45d52d8823ed8cbc",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_kernel.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_kernel.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_kernel.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_kernel.cc?ref=43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d",
            "patch": "@@ -34,6 +34,12 @@ limitations under the License.\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"tsl/profiler/lib/traceme.h\"\n+#include \"tsl/profiler/lib/traceme_encode.h\"\n+\n+using tsl::profiler::TraceMe;\n+using tsl::profiler::TraceMeEncode;\n+using tsl::profiler::TraceMeLevel;\n \n namespace stream_executor {\n namespace gpu {\n@@ -84,11 +90,17 @@ absl::Status CudaKernel::Launch(const ThreadDim& thread_dims,\n                                 const BlockDim& block_dims,\n                                 const std::optional<ClusterDim>& cluster_dims,\n                                 Stream* stream, const KernelArgs& args) {\n+  TraceMe trace([] { return TraceMeEncode(\"CudaKernel::Launch\", {}); },\n+                /*level=*/TraceMeLevel::kVerbose);\n+\n   CUfunction function = gpu_function();\n \n   // Launch kernels with packed arguments.\n   auto launch = [this, stream, &cluster_dims, &thread_dims, &block_dims,\n                  function](const KernelArgsPackedArrayBase& packed) {\n+    TraceMe trace([] { return TraceMeEncode(\"CudaKernel::Launch/launch\", {}); },\n+                  /*level=*/TraceMeLevel::kVerbose);\n+\n     int32_t expected_number_of_arguments =\n         Arity() + (packed.number_of_shared_bytes() > 0);\n "
        },
        {
            "sha": "3ecb91a79d74a4dfb91a77677d6bcb0af8e0938c",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_stream.cc",
            "status": "modified",
            "additions": 41,
            "deletions": 17,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_stream.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_stream.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_stream.cc?ref=43f9e0789ca8f766ea6bbcbe2e4551473e2ee02d",
            "patch": "@@ -48,6 +48,12 @@ limitations under the License.\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"tsl/profiler/lib/nvtx_utils.h\"\n+#include \"tsl/profiler/lib/traceme.h\"\n+#include \"tsl/profiler/lib/traceme_encode.h\"\n+\n+using tsl::profiler::TraceMe;\n+using tsl::profiler::TraceMeEncode;\n+using tsl::profiler::TraceMeLevel;\n \n namespace stream_executor {\n namespace gpu {\n@@ -363,6 +369,9 @@ absl::Status LaunchCudaKernel(\n     unsigned int grid_dim_z, unsigned int block_dim_x, unsigned int block_dim_y,\n     unsigned int block_dim_z, unsigned int shared_mem_bytes, CUstream stream,\n     void** kernel_params, void** extra) {\n+  TraceMe trace([] { return TraceMeEncode(\"LaunchCudaKernel\", {}); },\n+                /*level=*/TraceMeLevel::kVerbose);\n+\n   std::unique_ptr<ActivateContext> activation = executor->Activate();\n   VLOG(2) << \"launching kernel: \" << kernel_name << \"; gdx: \" << grid_dim_x\n           << \" gdy: \" << grid_dim_y << \" gdz: \" << grid_dim_z\n@@ -383,15 +392,20 @@ absl::Status LaunchCudaKernel(\n         cuFuncSetCacheConfig(function, CU_FUNC_CACHE_PREFER_SHARED)));\n   }\n \n-  return cuda::ToStatus(\n-      cuLaunchKernel(function, grid_dim_x, grid_dim_y, grid_dim_z, block_dim_x,\n-                     block_dim_y, block_dim_z, shared_mem_bytes, stream,\n-                     kernel_params, extra),\n-      absl::StrCat(\"Failed to launch CUDA kernel: \", kernel_name,\n-                   \"; block dims: \", block_dim_x, \"x\", block_dim_y, \"x\",\n-                   block_dim_z, \"; grid dims: \", grid_dim_x, \"x\", grid_dim_y,\n-                   \"x\", grid_dim_z,\n-                   \"; shared memory size: \", shared_mem_bytes));\n+  {\n+    TraceMe trace(\n+        [&] { return TraceMeEncode(\"LaunchCudaKernel/cuLaunchKernel\", {}); },\n+        /*level=*/TraceMeLevel::kVerbose);\n+    return cuda::ToStatus(\n+        cuLaunchKernel(function, grid_dim_x, grid_dim_y, grid_dim_z,\n+                       block_dim_x, block_dim_y, block_dim_z, shared_mem_bytes,\n+                       stream, kernel_params, extra),\n+        absl::StrCat(\"Failed to launch CUDA kernel: \", kernel_name,\n+                     \"; block dims: \", block_dim_x, \"x\", block_dim_y, \"x\",\n+                     block_dim_z, \"; grid dims: \", grid_dim_x, \"x\", grid_dim_y,\n+                     \"x\", grid_dim_z,\n+                     \"; shared memory size: \", shared_mem_bytes));\n+  }\n }\n \n absl::Status LaunchCudaKernel(\n@@ -402,6 +416,8 @@ absl::Status LaunchCudaKernel(\n     unsigned int block_dim_y, unsigned int block_dim_z,\n     unsigned int shared_mem_bytes, CUstream stream, void** kernel_params,\n     void** extra) {\n+  TraceMe trace([] { return TraceMeEncode(\"LaunchCudaKernel\", {}); },\n+                /*level=*/TraceMeLevel::kVerbose);\n   std::unique_ptr<ActivateContext> activation = executor->Activate();\n   VLOG(2) << \"launching kernel: \" << kernel_name << \"; cdx: \" << cluster_dim_x\n           << \" cdy: \" << cluster_dim_y << \" cdz: \" << cluster_dim_z\n@@ -444,14 +460,19 @@ absl::Status LaunchCudaKernel(\n   launch_config.attrs = &cluster_dims;\n   launch_config.numAttrs = 1;\n \n-  return cuda::ToStatus(\n-      cuLaunchKernelEx(&launch_config, function, kernel_params, extra),\n-      absl::StrCat(\"Failed to launch CUDA kernel: \", kernel_name,\n-                   \"; cluster dims: \", cluster_dim_x, \"x\", cluster_dim_y, \"x\",\n-                   cluster_dim_z, \"; block dims: \", block_dim_x, \"x\",\n-                   block_dim_y, \"x\", block_dim_z, \"; grid dims: \", grid_dim_x,\n-                   \"x\", grid_dim_y, \"x\", grid_dim_z,\n-                   \"; shared memory size: \", shared_mem_bytes));\n+  {\n+    TraceMe trace(\n+        [] { return TraceMeEncode(\"LaunchCudaKernel/cuLaunchKernelEx\", {}); },\n+        /*level=*/TraceMeLevel::kVerbose);\n+    return cuda::ToStatus(\n+        cuLaunchKernelEx(&launch_config, function, kernel_params, extra),\n+        absl::StrCat(\"Failed to launch CUDA kernel: \", kernel_name,\n+                     \"; cluster dims: \", cluster_dim_x, \"x\", cluster_dim_y, \"x\",\n+                     cluster_dim_z, \"; block dims: \", block_dim_x, \"x\",\n+                     block_dim_y, \"x\", block_dim_z, \"; grid dims: \", grid_dim_x,\n+                     \"x\", grid_dim_y, \"x\", grid_dim_z,\n+                     \"; shared memory size: \", shared_mem_bytes));\n+  }\n }\n \n }  // namespace\n@@ -460,6 +481,9 @@ absl::Status CudaStream::LaunchKernel(\n     const ThreadDim& thread_dims, const BlockDim& block_dims,\n     const std::optional<ClusterDim>& cluster_dims, void* function,\n     absl::string_view name, void** args, int64_t shmem_bytes) {\n+  TraceMe trace([] { return TraceMeEncode(\"CudaStream::LaunchKernel\", {}); },\n+                /*level=*/TraceMeLevel::kVerbose);\n+\n   if (cluster_dims.has_value()) {\n     return LaunchCudaKernel(executor_, name, static_cast<CUfunction>(function),\n                             cluster_dims->x, cluster_dims->y, cluster_dims->z,"
        }
    ],
    "stats": {
        "total": 180,
        "additions": 137,
        "deletions": 43
    }
}