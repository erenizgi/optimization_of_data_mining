{
    "author": "tensorflower-gardener",
    "message": "Use XLA_VLOG_DEVICE macro for device-specific logging.\n\nPiperOrigin-RevId: 839141968",
    "sha": "abd1d2ac663f4640124f9926356736298cf518d9",
    "files": [
        {
            "sha": "5817709a2269ed4ae480c5ae856ea3ac5571a833",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_communicator.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc?ref=abd1d2ac663f4640124f9926356736298cf518d9",
            "patch": "@@ -335,16 +335,16 @@ absl::Status NcclCommunicator::RegisterBufferOnce(\n     if (!registered_buffers_.range_to_handle.contains(buffer_range.opaque())) {\n       need_reg = true;\n     } else {\n-      VLOG(5) << \"[\" << device_ordinal\n-              << \"] Buffer range: \" << buffer_range.opaque()\n-              << \" with size: \" << buffer_range.size()\n-              << \" is already registered.\";\n+      XLA_VLOG_DEVICE(5, device_ordinal)\n+          << \"Buffer range: \" << buffer_range.opaque()\n+          << \" with size: \" << buffer_range.size() << \" is already registered.\";\n     }\n   }\n   if (need_reg) {\n-    VLOG(5) << \"[\" << device_ordinal << \"] Registering \"\n-            << buffer_range.opaque() << \" with size: \" << buffer_range.size()\n-            << \", is symmetric: \" << (use_symmetric_buffer ? \"true\" : \"false\");\n+    XLA_VLOG_DEVICE(5, device_ordinal)\n+        << \"Registering \" << buffer_range.opaque()\n+        << \" with size: \" << buffer_range.size()\n+        << \", is symmetric: \" << (use_symmetric_buffer ? \"true\" : \"false\");\n     // Symmetric buffer registration is a collective operation,\n     // we need to do that before locking on a global.\n     TF_ASSIGN_OR_RETURN("
        },
        {
            "sha": "9437cac737cc799d82c4bb01f5f1c361e81fbb40",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=abd1d2ac663f4640124f9926356736298cf518d9",
            "patch": "@@ -973,6 +973,7 @@ cc_library(\n         \":thunk_proto_cc\",\n         \"//xla:shape_util\",\n         \"//xla:types\",\n+        \"//xla:util\",\n         \"//xla/codegen/emitters:kernel_arguments\",\n         \"//xla/runtime:buffer_use\",\n         \"//xla/service:buffer_assignment\",\n@@ -1210,6 +1211,7 @@ cc_library(\n         \":thunk\",\n         \"//xla:future\",\n         \"//xla:shape_util\",\n+        \"//xla:util\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n         \"//xla/backends/gpu/collectives:gpu_collectives\",\n         \"//xla/backends/gpu/collectives:gpu_communicator\",\n@@ -1335,6 +1337,7 @@ cc_library(\n         \":thunk\",\n         \"//xla:future\",\n         \"//xla:status_macros\",\n+        \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n         \"//xla/backends/gpu/collectives:gpu_collectives\",\n@@ -1408,6 +1411,7 @@ cc_library(\n         \":thunk\",\n         \"//xla:future\",\n         \"//xla:shape_util\",\n+        \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n         \"//xla/backends/gpu/collectives:gpu_collectives\",\n@@ -1511,6 +1515,7 @@ cc_library(\n         \":p2p_thunk_common\",\n         \":thunk\",\n         \"//xla:executable_run_options\",\n+        \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n         \"//xla/backends/gpu/collectives:gpu_collectives\",\n@@ -1994,6 +1999,7 @@ cc_library(\n         \":annotation\",\n         \":thunk\",\n         \":thunk_proto_cc\",\n+        \"//xla:util\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -2260,6 +2266,7 @@ cc_library(\n         \":sequential_thunk\",\n         \":thunk\",\n         \":thunk_proto_cc\",\n+        \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_assignment\","
        },
        {
            "sha": "7ab65e853aebc13accf90385ba9b1a76634d58b5",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_gather_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_gather_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_gather_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_gather_thunk.cc?ref=abd1d2ac663f4640124f9926356736298cf518d9",
            "patch": "@@ -38,6 +38,7 @@ limitations under the License.\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n #include \"tsl/platform/casts.h\"\n \n namespace xla {\n@@ -108,7 +109,7 @@ absl::Status RunAllGather(std::vector<DeviceBufferPair>& buffers,\n                           se::Stream& stream, Communicator& comm,\n                           bool use_symmetric_buffer) {\n   int device_ordinal = stream.parent()->device_ordinal();\n-  VLOG(3) << \"[\" << device_ordinal << \"] Performing all-gather\";\n+  XLA_VLOG_DEVICE(3, device_ordinal) << \"Performing all-gather\";\n   TF_RETURN_IF_ERROR(MaybeRegisterBuffers(stream.parent(), buffers, &comm,\n                                           use_symmetric_buffer));\n   auto* gpu_comm = tsl::down_cast<GpuCommunicator*>(&comm);\n@@ -124,8 +125,7 @@ absl::Status RunAllGather(std::vector<DeviceBufferPair>& buffers,\n       });\n \n   TF_RETURN_IF_ERROR(future.Await());\n-  VLOG(3) << \"[\" << device_ordinal\n-          << \"] Done performing all-gather for ordinal: \" << device_ordinal;\n+  XLA_VLOG_DEVICE(3, device_ordinal) << \"Done performing all-gather\";\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "3899d783ae91a33cc445c9253dec976e2c24c7a3",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_thunk.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_thunk.cc?ref=abd1d2ac663f4640124f9926356736298cf518d9",
            "patch": "@@ -43,6 +43,7 @@ limitations under the License.\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/casts.h\"\n \n@@ -88,7 +89,7 @@ absl::Status RunAllReduce(ReductionKind reduction_kind,\n                           se::Stream& stream, Communicator& comm,\n                           bool use_symmetric_buffer) {\n   int device_ordinal = stream.parent()->device_ordinal();\n-  VLOG(3) << \"[\" << device_ordinal << \"] Performing all-reduce\";\n+  XLA_VLOG_DEVICE(3, device_ordinal) << \"Performing all-reduce\";\n   TF_RETURN_IF_ERROR(MaybeRegisterBuffers(stream.parent(), buffers, &comm,\n                                           use_symmetric_buffer));\n \n@@ -105,7 +106,7 @@ absl::Status RunAllReduce(ReductionKind reduction_kind,\n         return absl::OkStatus();\n       });\n   TF_RETURN_IF_ERROR(future.Await());\n-  VLOG(3) << \"[\" << device_ordinal << \"] Done performing all-reduce\";\n+  XLA_VLOG_DEVICE(3, device_ordinal) << \"Done performing all-reduce\";\n   return absl::OkStatus();\n }\n \n@@ -223,7 +224,7 @@ absl::Status RunReduceScatter(ReductionKind reduction_kind,\n                               se::Stream& stream, Communicator& comm,\n                               bool use_symmetric_buffer) {\n   int device_ordinal = stream.parent()->device_ordinal();\n-  VLOG(3) << \"[\" << device_ordinal << \"] Performing reduce-scatter\";\n+  XLA_VLOG_DEVICE(3, device_ordinal) << \"Performing reduce-scatter\";\n   TF_RETURN_IF_ERROR(MaybeRegisterBuffers(stream.parent(), buffers, &comm,\n                                           use_symmetric_buffer));\n \n@@ -248,7 +249,7 @@ absl::Status RunReduceScatter(ReductionKind reduction_kind,\n         return absl::OkStatus();\n       });\n   TF_RETURN_IF_ERROR(future.Await());\n-  VLOG(3) << \"[\" << device_ordinal << \"] Done performing reduce-scatter\";\n+  XLA_VLOG_DEVICE(3, device_ordinal) << \"Done performing reduce-scatter\";\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "d9b31481e4650329b382b0b7d9fdfdd4e5e2152c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_to_all_thunk.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.cc?ref=abd1d2ac663f4640124f9926356736298cf518d9",
            "patch": "@@ -118,8 +118,8 @@ absl::Status AllToAllStartThunk::Initialize(const InitializeParams& params) {\n   TF_RETURN_IF_ERROR(CollectiveThunk::Initialize(params));\n   device_count_ = params.local_device_count;\n   CHECK_GT(device_count_, 0);\n-  VLOG(5) << \"[\" << params.executor->device_ordinal()\n-          << \"] Local device count : \" << device_count_;\n+  XLA_VLOG_DEVICE(5, params.executor->device_ordinal())\n+      << \"Local device count : \" << device_count_;\n \n   if (is_local() && p2p_memcpy_enabled_) {\n     AsyncStreamKind stream_kind = GetAsyncStreamKind();\n@@ -276,9 +276,8 @@ absl::Status RunAllToAll(bool has_split_dimension,\n                          se::Stream& stream, Communicator& comm,\n                          bool use_symmetric_buffer) {\n   int device_ordinal = stream.parent()->device_ordinal();\n-  VLOG(3) << \"[\" << device_ordinal\n-          << \"] Performing all-to-all, has_split_dimension: \"\n-          << has_split_dimension;\n+  XLA_VLOG_DEVICE(3, device_ordinal)\n+      << \"Performing all-to-all, has_split_dimension: \" << has_split_dimension;\n   TF_RETURN_IF_ERROR(MaybeRegisterBuffers(stream.parent(), buffers, &comm,\n                                           use_symmetric_buffer));\n \n@@ -373,7 +372,7 @@ absl::Status RunMemCpyAllToAll(bool has_split_dimension,\n                                se::Event* event,\n                                std::vector<se::Event*>& events) {\n   int device_ordinal = stream.parent()->device_ordinal();\n-  VLOG(3) << \"[\" << device_ordinal << \"] Performing mem-copy-all-to-all\";\n+  XLA_VLOG_DEVICE(3, device_ordinal) << \"Performing mem-copy-all-to-all\";\n   TF_RETURN_IF_ERROR(MaybeRegisterBuffers(stream.parent(), buffers, &comm));\n   TF_ASSIGN_OR_RETURN(int32_t num_ranks, comm.NumRanks());\n   TF_RETURN_IF_ERROR(SyncProgress(\"before memcpy all-to-all\", clique_key, rank,"
        },
        {
            "sha": "a739daa880eda1f2fcd15fd9015e99997aae8c3d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_kernel_thunk.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_kernel_thunk.cc?ref=abd1d2ac663f4640124f9926356736298cf518d9",
            "patch": "@@ -309,21 +309,20 @@ absl::Status CollectiveKernelThunk::ExecuteOnStream(\n       AllReduceLaunchDimensions(buffer.element_count, num_devices, strategy);\n   // In case of two-shot we want to increment in multiples of 2.\n   state->invocation_count += 1 + static_cast<uint32_t>(strategy);\n-  VLOG(3) << \"[\" << device_ordinal\n-          << \"] Performing one-shot all-reduce for clique \"\n-          << clique_key.ToString();\n+  XLA_VLOG_DEVICE(3, device_ordinal)\n+      << \"Performing one-shot all-reduce for clique \" << clique_key.ToString();\n \n   se::DeviceMemoryBase input_buffer_ptr =\n       state->remote_buffer_ptrs[buffer_index];\n   se::DeviceMemoryBase signal_buffer_ptr =\n       state->signal_buffer_ptrs[buffer_index];\n-  VLOG(3) << \"[\" << device_ordinal\n-          << \"] input_buffer_ptr: \" << input_buffer_ptr.opaque()\n-          << \" signal_buffer_ptr: \" << signal_buffer_ptr.opaque();\n-  VLOG(3) << \"[\" << device_ordinal\n-          << \"] launch dimensions: \" << launch_dimensions.num_blocks() << \"x\"\n-          << launch_dimensions.num_threads_per_block()\n-          << \"(block x threadsPerBlock)\";\n+  XLA_VLOG_DEVICE(3, device_ordinal)\n+      << \"input_buffer_ptr: \" << input_buffer_ptr.opaque()\n+      << \" signal_buffer_ptr: \" << signal_buffer_ptr.opaque();\n+  XLA_VLOG_DEVICE(3, device_ordinal)\n+      << \"launch dimensions: \" << launch_dimensions.num_blocks() << \"x\"\n+      << launch_dimensions.num_threads_per_block()\n+      << \"(block x threadsPerBlock)\";\n \n   if (state->kernel != nullptr) {\n     TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase remote_buffers,"
        },
        {
            "sha": "22850cf5a57599ba12f5848f1cfe14ff74b058e8",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_permute_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_permute_thunk.cc?ref=abd1d2ac663f4640124f9926356736298cf518d9",
            "patch": "@@ -53,6 +53,7 @@ limitations under the License.\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/casts.h\"\n \n@@ -361,8 +362,8 @@ absl::Status RunCollectivePermute(\n   //\n \n   int device_ordinal = stream.parent()->device_ordinal();\n-  VLOG(3) << \"[\" << device_ordinal\n-          << \"] Performing collective permute, current_id \" << current_id;\n+  XLA_VLOG_DEVICE(3, device_ordinal)\n+      << \"Performing collective permute, current_id \" << current_id;\n \n   std::optional<int64_t> source_id = source_target.source;\n   std::optional<int64_t> target_id = source_target.target;"
        },
        {
            "sha": "1979f2b043afc26471eb67d5fa3ef43463d00811",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 13,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc?ref=abd1d2ac663f4640124f9926356736298cf518d9",
            "patch": "@@ -218,12 +218,11 @@ absl::Status MaybeRegisterBuffer(se::StreamExecutor* executor,\n                                  Communicator* comm,\n                                  bool use_symmetric_buffer) {\n   TF_ASSIGN_OR_RETURN(auto range, executor->GetMemoryRange(buffer));\n-  VLOG(1) << \"[\" << executor->device_ordinal() << \"] \"\n-          << \"Registering range: \" << range.opaque()\n-          << \" with size: \" << range.size()\n-          << \" for buffer: \" << buffer.opaque()\n-          << \" with size: \" << buffer.size()\n-          << \" is symmetric: \" << (use_symmetric_buffer ? \"true\" : \"false\");\n+  XLA_VLOG_DEVICE(1, executor->device_ordinal())\n+      << \"Registering range: \" << range.opaque()\n+      << \" with size: \" << range.size() << \" for buffer: \" << buffer.opaque()\n+      << \" with size: \" << buffer.size()\n+      << \" is symmetric: \" << (use_symmetric_buffer ? \"true\" : \"false\");\n   // If the collective memory buffer is a slice of a larger preallocated buffer,\n   // we need to register the entire preallocated buffer once.\n   return comm->RegisterBufferOnce(range, executor->device_ordinal(),\n@@ -342,13 +341,13 @@ absl::Status CollectiveThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n     auto global_device_id = params.collective_params->global_device_id;\n     RankId rank = clique_key.rank(global_device_id).value_or(RankId(-1));\n-    VLOG(1) << \"[\" << global_device_id.value()\n-            << \"] Do a rendezvous after a first call to \"\n-            << Thunk::KindToString(kind())\n-            << \"; run_id=\" << params.collective_params->run_id.ToInt()\n-            << \"; num_local_participants=\" << num_local_participants\n-            << \"; rank=\" << rank.value()\n-            << \"; clique_key=\" << clique_key.ToString();\n+    XLA_VLOG_DEVICE(1, global_device_id.value())\n+        << \"Do a rendezvous after a first call to \"\n+        << Thunk::KindToString(kind())\n+        << \"; run_id=\" << params.collective_params->run_id.ToInt()\n+        << \"; num_local_participants=\" << num_local_participants\n+        << \"; rank=\" << rank.value()\n+        << \"; clique_key=\" << clique_key.ToString();\n \n     auto rendezvous_key = FirstCallRendezvousKey{std::move(clique_key)};\n     auto rendezvous_name = absl::StrFormat("
        },
        {
            "sha": "ae4375d79af523f1036c86037551b406702c5c42",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.cc",
            "status": "modified",
            "additions": 48,
            "deletions": 42,
            "changes": 90,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc?ref=abd1d2ac663f4640124f9926356736298cf518d9",
            "patch": "@@ -2134,15 +2134,16 @@ absl::StatusOr<const se::CommandBuffer::Command*> AllReduceCmd::Record(\n                              config().operand_element_type));\n \n   int device_ordinal = execute_params.stream->parent()->device_ordinal();\n-  VLOG(5) << \"[\" << device_ordinal << \"] AllReduceCmd: reduction=\"\n-          << ReductionKindString(reduction_kind_);\n+  XLA_VLOG_DEVICE(5, device_ordinal)\n+      << \"AllReduceCmd: reduction=\" << ReductionKindString(reduction_kind_);\n \n   for (size_t i = 0; i < device_buffers.size(); ++i) {\n-    VLOG(5) << \"[\" << device_ordinal << \"]  Src: \" << buffers_[i].source_buffer\n-            << \" (\" << device_buffers[i].source_buffer.opaque() << \")\";\n-    VLOG(5) << \"[\" << device_ordinal\n-            << \"]  Dst: \" << buffers_[i].destination_buffer << \" (\"\n-            << device_buffers[i].destination_buffer.opaque() << \")\";\n+    XLA_VLOG_DEVICE(5, device_ordinal)\n+        << \"  Src: \" << buffers_[i].source_buffer << \" (\"\n+        << device_buffers[i].source_buffer.opaque() << \")\";\n+    XLA_VLOG_DEVICE(5, device_ordinal)\n+        << \"  Dst: \" << buffers_[i].destination_buffer << \" (\"\n+        << device_buffers[i].destination_buffer.opaque() << \")\";\n   }\n \n   if (!execute_params.collective_params || !execute_params.collective_cliques) {\n@@ -2201,15 +2202,16 @@ absl::StatusOr<const se::CommandBuffer::Command*> ReduceScatterCmd::Record(\n                              config().operand_element_type));\n \n   int device_ordinal = execute_params.stream->parent()->device_ordinal();\n-  VLOG(5) << \"[\" << device_ordinal << \"] ReduceScatterCmd: reduction=\"\n-          << ReductionKindString(reduction_kind_);\n+  XLA_VLOG_DEVICE(5, device_ordinal)\n+      << \"ReduceScatterCmd: reduction=\" << ReductionKindString(reduction_kind_);\n \n   for (size_t i = 0; i < device_buffers.size(); ++i) {\n-    VLOG(5) << \"[\" << device_ordinal << \"]  Src: \" << buffers_[i].source_buffer\n-            << \" (\" << device_buffers[i].source_buffer.opaque() << \")\";\n-    VLOG(5) << \"[\" << device_ordinal\n-            << \"]  Dst: \" << buffers_[i].destination_buffer << \" (\"\n-            << device_buffers[i].destination_buffer.opaque() << \")\";\n+    XLA_VLOG_DEVICE(5, device_ordinal)\n+        << \"  Src: \" << buffers_[i].source_buffer << \" (\"\n+        << device_buffers[i].source_buffer.opaque() << \")\";\n+    XLA_VLOG_DEVICE(5, device_ordinal)\n+        << \"  Dst: \" << buffers_[i].destination_buffer << \" (\"\n+        << device_buffers[i].destination_buffer.opaque() << \")\";\n   }\n \n   if (!execute_params.collective_params || !execute_params.collective_cliques) {\n@@ -2268,15 +2270,16 @@ absl::StatusOr<const se::CommandBuffer::Command*> AllToAllCmd::Record(\n                              config().operand_element_type));\n \n   int device_ordinal = execute_params.stream->parent()->device_ordinal();\n-  VLOG(5) << \"[\" << device_ordinal\n-          << \"] AllToAllCmd, has_split_dimension=\" << has_split_dimension_;\n+  XLA_VLOG_DEVICE(5, device_ordinal)\n+      << \"AllToAllCmd, has_split_dimension=\" << has_split_dimension_;\n \n   for (size_t i = 0; i < device_buffers.size(); ++i) {\n-    VLOG(5) << \"[\" << device_ordinal << \"]  Src: \" << buffers_[i].source_buffer\n-            << \" (\" << device_buffers[i].source_buffer.opaque() << \")\";\n-    VLOG(5) << \"[\" << device_ordinal\n-            << \"]  Dst: \" << buffers_[i].destination_buffer << \" (\"\n-            << device_buffers[i].destination_buffer.opaque() << \")\";\n+    XLA_VLOG_DEVICE(5, device_ordinal)\n+        << \"  Src: \" << buffers_[i].source_buffer << \" (\"\n+        << device_buffers[i].source_buffer.opaque() << \")\";\n+    XLA_VLOG_DEVICE(5, device_ordinal)\n+        << \"  Dst: \" << buffers_[i].destination_buffer << \" (\"\n+        << device_buffers[i].destination_buffer.opaque() << \")\";\n   }\n \n   if (!execute_params.collective_params || !execute_params.collective_cliques) {\n@@ -2334,14 +2337,15 @@ absl::StatusOr<const se::CommandBuffer::Command*> AllGatherCmd::Record(\n                              config().operand_element_type));\n \n   int device_ordinal = execute_params.stream->parent()->device_ordinal();\n-  VLOG(5) << \"[\" << device_ordinal << \"] AllGatherCmd:\";\n+  XLA_VLOG_DEVICE(5, device_ordinal) << \"AllGatherCmd:\";\n \n   for (size_t i = 0; i < device_buffers.size(); ++i) {\n-    VLOG(5) << \"[\" << device_ordinal << \"]  Src: \" << buffers_[i].source_buffer\n-            << \" (\" << device_buffers[i].source_buffer.opaque() << \")\";\n-    VLOG(5) << \"[\" << device_ordinal\n-            << \"]  Dst: \" << buffers_[i].destination_buffer << \" (\"\n-            << device_buffers[i].destination_buffer.opaque() << \")\";\n+    XLA_VLOG_DEVICE(5, device_ordinal)\n+        << \"  Src: \" << buffers_[i].source_buffer << \" (\"\n+        << device_buffers[i].source_buffer.opaque() << \")\";\n+    XLA_VLOG_DEVICE(5, device_ordinal)\n+        << \"  Dst: \" << buffers_[i].destination_buffer << \" (\"\n+        << device_buffers[i].destination_buffer.opaque() << \")\";\n   }\n \n   if (!execute_params.collective_params || !execute_params.collective_cliques) {\n@@ -2399,14 +2403,15 @@ CollectiveBroadcastCmd::Record(const Thunk::ExecuteParams& execute_params,\n                              config().operand_element_type));\n \n   int device_ordinal = execute_params.stream->parent()->device_ordinal();\n-  VLOG(5) << \"[\" << device_ordinal << \"] CollectiveBroadcastCmd:\";\n+  XLA_VLOG_DEVICE(5, device_ordinal) << \"CollectiveBroadcastCmd:\";\n \n   for (size_t i = 0; i < device_buffers.size(); ++i) {\n-    VLOG(5) << \"[\" << device_ordinal << \"]  Src: \" << buffers_[i].source_buffer\n-            << \" (\" << device_buffers[i].source_buffer.opaque() << \")\";\n-    VLOG(5) << \"[\" << device_ordinal\n-            << \"]  Dst: \" << buffers_[i].destination_buffer << \" (\"\n-            << device_buffers[i].destination_buffer.opaque() << \")\";\n+    XLA_VLOG_DEVICE(5, device_ordinal)\n+        << \"  Src: \" << buffers_[i].source_buffer << \" (\"\n+        << device_buffers[i].source_buffer.opaque() << \")\";\n+    XLA_VLOG_DEVICE(5, device_ordinal)\n+        << \"  Dst: \" << buffers_[i].destination_buffer << \" (\"\n+        << device_buffers[i].destination_buffer.opaque() << \")\";\n   }\n \n   if (!execute_params.collective_params || !execute_params.collective_cliques) {\n@@ -2464,14 +2469,15 @@ absl::StatusOr<const se::CommandBuffer::Command*> CollectivePermuteCmd::Record(\n                              config().operand_element_type));\n \n   int device_ordinal = execute_params.stream->parent()->device_ordinal();\n-  VLOG(5) << \"[\" << device_ordinal << \"] CollectivePermuteCmd:\";\n+  XLA_VLOG_DEVICE(5, device_ordinal) << \"CollectivePermuteCmd:\";\n \n   for (size_t i = 0; i < device_buffers.size(); ++i) {\n-    VLOG(5) << \"[\" << device_ordinal << \"]  Src: \" << buffers_[i].source_buffer\n-            << \" (\" << device_buffers[i].source_buffer.opaque() << \")\";\n-    VLOG(5) << \"[\" << device_ordinal\n-            << \"]  Dst: \" << buffers_[i].destination_buffer << \" (\"\n-            << device_buffers[i].destination_buffer.opaque() << \")\";\n+    XLA_VLOG_DEVICE(5, device_ordinal)\n+        << \"  Src: \" << buffers_[i].source_buffer << \" (\"\n+        << device_buffers[i].source_buffer.opaque() << \")\";\n+    XLA_VLOG_DEVICE(5, device_ordinal)\n+        << \"  Dst: \" << buffers_[i].destination_buffer << \" (\"\n+        << device_buffers[i].destination_buffer.opaque() << \")\";\n   }\n \n   if (!execute_params.collective_params || !execute_params.collective_cliques) {\n@@ -2594,9 +2600,9 @@ absl::Status DynamicSliceFusionCmd::Initialize(\n     return absl::OkStatus();\n   }\n \n-  VLOG(2) << \"[\" << params.executor->device_ordinal() << \"] Allocate \"\n-          << offsets_allocs_size_\n-          << \" bytes for transferring offsets on executor: \" << params.executor;\n+  XLA_VLOG_DEVICE(2, params.executor->device_ordinal())\n+      << \"Allocate \" << offsets_allocs_size_\n+      << \" bytes for transferring offsets on executor: \" << params.executor;\n   TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<se::MemoryAllocation> allocation,\n       params.executor->HostMemoryAllocate(offsets_allocs_size_));"
        },
        {
            "sha": "146057910cf011f2a25ac0ec67dca4fdcb20342c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc?ref=abd1d2ac663f4640124f9926356736298cf518d9",
            "patch": "@@ -47,6 +47,7 @@ limitations under the License.\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n #include \"tsl/profiler/lib/traceme.h\"\n #include \"tsl/profiler/lib/traceme_encode.h\"\n \n@@ -214,22 +215,22 @@ absl::Status KernelThunk::ExecuteOnStream(const ExecuteParams& params) {\n         },\n         /*level=*/TraceMeLevel::kVerbose);\n     int device_ordinal = executor->device_ordinal();\n-    VLOG(3) << \"[\" << device_ordinal << \"] Launching \" << kernel->name();\n+    XLA_VLOG_DEVICE(3, device_ordinal) << \"Launching \" << kernel->name();\n     for (const auto& [idx, arg] : llvm::enumerate(args_)) {\n       se::DeviceMemoryBase buf =\n           params.buffer_allocations->GetDeviceAddress(arg);\n-      VLOG(3) << \"[\" << device_ordinal << \"] Arg: alloc #\" << arg.index()\n-              << \", offset: \" << arg.offset() << \": \" << buf.opaque() << \" (\"\n-              << buf.size() << \"B)\";\n+      XLA_VLOG_DEVICE(3, device_ordinal)\n+          << \"Arg: alloc #\" << arg.index() << \", offset: \" << arg.offset()\n+          << \": \" << buf.opaque() << \" (\" << buf.size() << \"B)\";\n \n       if (auto it = tma_metadata_.arg_index_to_tma_info.find(idx);\n           it != tma_metadata_.arg_index_to_tma_info.end()) {\n         // TMA descriptor argument.\n         const se::gpu::TmaDescriptor& tma_desc = it->second;\n         TF_ASSIGN_OR_RETURN(se::TensorMap tensor_map,\n                             executor->CreateTensorMap(tma_desc, buf.opaque()));\n-        VLOG(3) << \"[\" << device_ordinal << \"]  Using TensorMap for arg #\"\n-                << idx << \": \" << tma_desc.ToString();\n+        XLA_VLOG_DEVICE(3, device_ordinal) << \"Using TensorMap for arg #\" << idx\n+                                           << \": \" << tma_desc.ToString();\n         kernel_args.push_back(std::move(tensor_map));\n       } else {\n         // Buffer argument."
        },
        {
            "sha": "80e9f5a90f73c46b588eca278b90ae0a4fee9809",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all_thunk.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc?ref=abd1d2ac663f4640124f9926356736298cf518d9",
            "patch": "@@ -56,6 +56,7 @@ limitations under the License.\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/casts.h\"\n \n@@ -146,9 +147,8 @@ absl::Status RunRaggedAllToAll(\n     const se::DeviceMemoryBase& output_offsets_device_buffer,\n     bool use_symmetric_buffer) {\n   int device_ordinal = stream.parent()->device_ordinal();\n-  VLOG(3) << \"[\" << device_ordinal\n-          << \"] Performing ragged-all-to-all from device ordinal: \"\n-          << device_ordinal;\n+  XLA_VLOG_DEVICE(3, device_ordinal)\n+      << \"Performing ragged-all-to-all from device ordinal: \" << device_ordinal;\n   TF_ASSIGN_OR_RETURN(int32_t num_ranks, comm.NumRanks());\n \n   std::vector<DeviceBufferPair> buffers = original_buffers;\n@@ -316,8 +316,8 @@ absl::Status RaggedAllToAllStartThunk::RunOneShotRaggedAllToAll(\n \n   const int64_t num_ranks = clique_key.num_local_participants();\n \n-  VLOG(3) << \"[\" << device_ordinal\n-          << \"] Performing one-shot ragged-all-to-all rank: \" << rank.value();\n+  XLA_VLOG_DEVICE(3, device_ordinal)\n+      << \"Performing one-shot ragged-all-to-all rank: \" << rank.value();\n \n   PrimitiveType element_type = buffers[0].element_type;\n "
        },
        {
            "sha": "8ef9e6fe12db18fa060060ce64a7c66bfd44ee1c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/sequential_thunk.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsequential_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsequential_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fsequential_thunk.cc?ref=abd1d2ac663f4640124f9926356736298cf518d9",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n #include \"tsl/profiler/lib/scoped_annotation.h\"\n #include \"tsl/profiler/lib/traceme.h\"\n \n@@ -99,13 +100,13 @@ absl::Status SequentialThunk::ExecuteOnStream(const ExecuteParams& params) {\n       continue;\n     }\n \n-    VLOG(1) << \"[\" << params.stream->parent()->device_ordinal() << \"] \"\n-            << \"Start SequentialThunk::ExecuteOnStream: \"\n-            << thunk->profile_annotation();\n+    XLA_VLOG_DEVICE(1, params.stream->parent()->device_ordinal())\n+        << \"Start SequentialThunk::ExecuteOnStream: \"\n+        << thunk->profile_annotation();\n     TF_RETURN_IF_ERROR(thunk->ExecuteOnStream(params));\n-    VLOG(1) << \"[\" << params.stream->parent()->device_ordinal() << \"] \"\n-            << \"End SequentialThunk::ExecuteOnStream: \"\n-            << thunk->profile_annotation();\n+    XLA_VLOG_DEVICE(1, params.stream->parent()->device_ordinal())\n+        << \"End SequentialThunk::ExecuteOnStream: \"\n+        << thunk->profile_annotation();\n   }\n   return absl::OkStatus();\n }"
        },
        {
            "sha": "1eec5b36a98c53678e5d76e19dcbb41ab1caecb4",
            "filename": "third_party/xla/xla/backends/gpu/runtime/while_thunk.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 13,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fwhile_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fwhile_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fwhile_thunk.cc?ref=abd1d2ac663f4640124f9926356736298cf518d9",
            "patch": "@@ -41,6 +41,7 @@ limitations under the License.\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/profiler/lib/traceme.h\"\n \n@@ -130,11 +131,12 @@ absl::Status WhileThunk::ExecuteOnStream(const ExecuteParams& params) {\n \n   int device_ordinal = stream.parent()->device_ordinal();\n   if (trip_count_.has_value()) {\n-    VLOG(2) << \"[\" << device_ordinal << \"] Executing WhileThunk for \"\n-            << *trip_count_ << \" iterations\";\n+    XLA_VLOG_DEVICE(2, device_ordinal)\n+        << \"Executing WhileThunk for \" << *trip_count_ << \" iterations\";\n     for (iter = 0; iter < trip_count_; ++iter) {\n-      VLOG(3) << \"[\" << device_ordinal << \"] Executing iteration # \" << iter\n-              << \" (Device: \" << stream.parent()->device_ordinal() << \")\";\n+      XLA_VLOG_DEVICE(3, device_ordinal)\n+          << \"Executing iteration # \" << iter\n+          << \" (Device: \" << stream.parent()->device_ordinal() << \")\";\n       TF_RETURN_IF_ERROR(body_thunk_sequence_->ExecuteOnStream(params));\n     }\n     return absl::OkStatus();\n@@ -154,8 +156,8 @@ absl::Status WhileThunk::ExecuteOnStream(const ExecuteParams& params) {\n   while (true) {\n     TraceMe trace(\n         [&] { return TraceMeEncode(\"While\", {{\"iteration:\", iter}}); });\n-    VLOG(3) << \"[\" << device_ordinal\n-            << \"] Executing WhileThunk condition computation; iter=\" << iter;\n+    XLA_VLOG_DEVICE(3, device_ordinal)\n+        << \"Executing WhileThunk condition computation; iter=\" << iter;\n     TF_RETURN_IF_ERROR(condition_thunk_sequence_->ExecuteOnStream(params));\n \n     // Copy the result of condition computation and break the loop if 'false'.\n@@ -168,17 +170,16 @@ absl::Status WhileThunk::ExecuteOnStream(const ExecuteParams& params) {\n           blocked.message()));\n     }\n \n-    VLOG(3) << \"[\" << device_ordinal\n-            << \"] condition_result = \" << *condition_result;\n+    XLA_VLOG_DEVICE(3, device_ordinal)\n+        << \"condition_result = \" << *condition_result;\n     if (!*condition_result) {\n-      VLOG(3) << \"[\" << device_ordinal\n-              << \"] Break WhileThunk loop; iter=\" << iter;\n+      XLA_VLOG_DEVICE(3, device_ordinal)\n+          << \"Break WhileThunk loop; iter=\" << iter;\n       break;\n     }\n \n-    VLOG(3) << \"[\" << device_ordinal\n-            << \"] Executing WhileThunk body computation; iter=\" << iter\n-            << \" (Device: \" << stream.parent()->device_ordinal() << \")\";\n+    XLA_VLOG_DEVICE(3, device_ordinal)\n+        << \"Executing WhileThunk body computation; iter=\" << iter;\n     TF_RETURN_IF_ERROR(body_thunk_sequence_->ExecuteOnStream(params));\n     ++iter;\n   }"
        },
        {
            "sha": "5cfc473697101c7a8f433384104b9e8fa882c390",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=abd1d2ac663f4640124f9926356736298cf518d9",
            "patch": "@@ -1580,7 +1580,8 @@ StreamExecutorGpuClient::RunAsync(\n   }\n   xla::gpu::BufferAllocations buffer_allocations(buffers, device_ordinal,\n                                                  memory_allocator);\n-  VLOG(3) << \"[\" << device_ordinal << \"] \" << buffer_allocations.ToString();\n+  XLA_VLOG_DEVICE(3, device_ordinal)\n+      << \"Buffer allocations: \" << buffer_allocations.ToString();\n \n   std::set<se::DeviceMemoryBase> buffers_in_result;\n \n@@ -1598,8 +1599,9 @@ StreamExecutorGpuClient::RunAsync(\n         allocations[output_info.allocation_index];\n     se::DeviceMemoryBase result_buffer;\n \n-    VLOG(4) << \"[\" << device_ordinal << \"] Looking at: allocation \"\n-            << output_info.allocation_index << \" @ index: \" << index.ToString();\n+    XLA_VLOG_DEVICE(4, device_ordinal)\n+        << \"Looking at: allocation \" << output_info.allocation_index\n+        << \" @ index: \" << index.ToString();\n \n     if (output_info.alias_config) {\n       PjRtStreamExecutorExecutionInput& input =\n@@ -1626,9 +1628,9 @@ StreamExecutorGpuClient::RunAsync(\n         // The guard is above is not to insert copy-protection when aliasing\n         // pass-through params, as we do not need to write into the output\n         // buffer.\n-        VLOG(3) << \"[\" << device_ordinal\n-                << \"] Using copy-protection: aliasing is specified, but the \"\n-                   \"buffer is not donated; allocating a fresh buffer\";\n+        XLA_VLOG_DEVICE(3, device_ordinal)\n+            << \"Using copy-protection: aliasing is specified, but the \"\n+               \"buffer is not donated; allocating a fresh buffer\";\n         int64_t allocation_size = ShapeUtil::ByteSizeOf(\n             ShapeUtil::GetSubshape(gpu_exec->result_shape(), index));\n         absl::StatusOr<se::OwningDeviceMemory> allocated_buffer ="
        },
        {
            "sha": "37aa16910510786eb45a6e37a63b650e7990971f",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/abd1d2ac663f4640124f9926356736298cf518d9/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=abd1d2ac663f4640124f9926356736298cf518d9",
            "patch": "@@ -492,11 +492,11 @@ absl::Status ExecuteThunksImpl(\n       command_buffer_trace_stream, &collective_params, &collective_cliques,\n       std::move(additional_execution_streams));\n \n-  VLOG(1) << \"[\" << run_options->device_ordinal() << \"] \"\n-          << \"Start GpuExecutable::ExecuteOnStream module: \" << module_name;\n+  XLA_VLOG_DEVICE(1, run_options->device_ordinal())\n+      << \"Start GpuExecutable::ExecuteOnStream module: \" << module_name;\n   TF_RETURN_IF_ERROR(thunk_sequence.ExecuteOnStream(execute_params));\n-  VLOG(1) << \"[\" << run_options->device_ordinal() << \"] \"\n-          << \"End GpuExecutable::ExecuteOnStream module: \" << module_name;\n+  XLA_VLOG_DEVICE(1, run_options->device_ordinal())\n+      << \"End GpuExecutable::ExecuteOnStream module: \" << module_name;\n \n   if (collective_params.need_barrier) {\n     TF_RETURN_IF_ERROR(BarrierAfterExecutable(debug_options, main_stream));"
        }
    ],
    "stats": {
        "total": 271,
        "additions": 144,
        "deletions": 127
    }
}