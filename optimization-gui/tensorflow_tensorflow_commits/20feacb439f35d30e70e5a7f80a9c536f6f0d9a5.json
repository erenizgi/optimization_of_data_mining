{
    "author": "khasanovaa",
    "message": "Remove `legacy_command_buffer_custom_call_targets` from `debug_options` and all code that uses it.\n\nPiperOrigin-RevId: 815729469",
    "sha": "20feacb439f35d30e70e5a7f80a9c536f6f0d9a5",
    "files": [
        {
            "sha": "2b03b665d8156db7a9a1e22c1567662753c30eee",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_conversion_pass.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/20feacb439f35d30e70e5a7f80a9c536f6f0d9a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/20feacb439f35d30e70e5a7f80a9c536f6f0d9a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc?ref=20feacb439f35d30e70e5a7f80a9c536f6f0d9a5",
            "patch": "@@ -68,14 +68,7 @@ CommandBufferConfig GetCommandBufferConfig(\n     commands.insert(static_cast<DebugOptions::CommandBufferCmdType>(cmd_type));\n   }\n \n-  absl::flat_hash_set<std::string> legacy_custom_call_targets;\n-  for (const auto& target :\n-       debug_options.legacy_command_buffer_custom_call_targets()) {\n-    legacy_custom_call_targets.insert(target);\n-  }\n-\n-  CommandBufferConfig config{\n-      std::move(commands), std::move(legacy_custom_call_targets), device_info};\n+  CommandBufferConfig config{std::move(commands), device_info};\n \n   // Erase command buffer cmd types that are not supported by the gpu runtime.\n   static constexpr auto kRequireConditionals = {DebugOptions::CONDITIONAL,\n@@ -203,11 +196,6 @@ bool IsConvertible(const ConditionalThunk& conditional_thunk,\n bool IsConvertible(const CustomCallThunk& custom_call_thunk,\n                    const CommandBufferConfig& config) {\n   const std::string& target_name = custom_call_thunk.target_name();\n-  if (config.enabled_legacy_custom_call_targets.contains(target_name)) {\n-    VLOG(3) << \"Recording legacy custom call target \" << target_name\n-            << \" into command buffer.\";\n-    return true;\n-  }\n \n   // Check if FFI handler is compatible with command buffers.\n   absl::StatusOr<ffi::HandlerRegistration> registration ="
        },
        {
            "sha": "32657b34e28507d026baec5f7af444e84bb8045c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_conversion_pass.h",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/20feacb439f35d30e70e5a7f80a9c536f6f0d9a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/20feacb439f35d30e70e5a7f80a9c536f6f0d9a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.h?ref=20feacb439f35d30e70e5a7f80a9c536f6f0d9a5",
            "patch": "@@ -46,7 +46,6 @@ class CommandBufferConversionPass : public ThunkPassInterface {\n     // DebugOptions control which commands are enabled. Long term we want to\n     // remove that flag and enable all supported commands by default.\n     absl::flat_hash_set<DebugOptions::CommandBufferCmdType> enabled_commands;\n-    absl::flat_hash_set<std::string> enabled_legacy_custom_call_targets;\n     const se::DeviceDescription& device_description;\n   };\n };"
        },
        {
            "sha": "c67f70143d1b9d3318416c5d1d203f60cb9ffe1d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_conversion_pass_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/20feacb439f35d30e70e5a7f80a9c536f6f0d9a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/20feacb439f35d30e70e5a7f80a9c536f6f0d9a5/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass_test.cc?ref=20feacb439f35d30e70e5a7f80a9c536f6f0d9a5",
            "patch": "@@ -778,38 +778,6 @@ TEST(CommandBufferConversionPassTest, ConvertWhileThunkWithAsyncPair) {\n                             Thunk::kAllGatherDone));\n }\n \n-TEST(CommandBufferConversionPassTest,\n-     ConvertsLegacyCustomCallToCommandBufferThunk) {\n-  std::vector<std::unique_ptr<Thunk>> thunks;\n-  thunks.push_back(CreateCustomCallThunk(\"test_legacy_custom_call\"));\n-\n-  auto root_thunk =\n-      std::make_unique<SequentialThunk>(Thunk::ThunkInfo(), std::move(thunks));\n-  DebugOptions debug_options;\n-  debug_options.clear_xla_gpu_enable_command_buffer();\n-  debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::CUSTOM_CALL);\n-  debug_options.add_legacy_command_buffer_custom_call_targets(\n-      \"test_legacy_custom_call\");\n-\n-  se::DeviceDescription device_info = TestGpuDeviceInfo::CudaOrRocmDeviceInfo();\n-  FakeErrorAllocator allocator;\n-\n-  ASSERT_EQ(root_thunk->thunks().size(), 1);\n-\n-  CommandBufferConversionPass pass;\n-\n-  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info, allocator),\n-              IsOkAndHolds(true));\n-  EXPECT_THAT(root_thunk->thunks(), ThunkKindsAre(Thunk::kCommandBuffer));\n-\n-  const auto* command_buffer_thunk =\n-      static_cast<const CommandBufferThunk*>(root_thunk->thunks()[0].get());\n-\n-  const auto& thunks_in_command_buffer =\n-      command_buffer_thunk->thunks()->thunks();\n-  EXPECT_THAT(thunks_in_command_buffer, ThunkKindsAre(Thunk::kCustomCall));\n-}\n-\n TEST(CommandBufferConversionPassTest, ConvertsCuDnnThunkToCommandBufferThunk) {\n   std::vector<std::unique_ptr<Thunk>> thunks;\n "
        },
        {
            "sha": "fb0d55d723122307078f507f06b610d19ef9804c",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/20feacb439f35d30e70e5a7f80a9c536f6f0d9a5/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/20feacb439f35d30e70e5a7f80a9c536f6f0d9a5/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=20feacb439f35d30e70e5a7f80a9c536f6f0d9a5",
            "patch": "@@ -594,16 +594,6 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n         return true;\n       };\n \n-  // Custom \"sub-parser\" lambda for legacy_command_buffer_custom_call_targets.\n-  auto setter_for_legacy_command_buffer_custom_call_targets =\n-      [debug_options](std::string comma_separated_values) {\n-        for (const auto& target : std::vector<std::string>(\n-                 absl::StrSplit(comma_separated_values, ','))) {\n-          debug_options->add_legacy_command_buffer_custom_call_targets(target);\n-        }\n-        return true;\n-      };\n-\n   // Custom \"sub-parser\" lambda for xla_gpu_ptx_file.\n   auto setter_for_xla_gpu_ptx_file = [debug_options](std::string value) {\n     debug_options->add_xla_gpu_ptx_file(value);\n@@ -1617,14 +1607,6 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n       \" + and - as prefix, which indicate adding or removing a command type\"\n       \" to/from the default list.\"));\n \n-  flag_list->push_back(\n-      tsl::Flag(\"legacy_command_buffer_custom_call_targets\",\n-                setter_for_legacy_command_buffer_custom_call_targets, \"\",\n-                \"Comma-separated list of custom call targets with legacy \"\n-                \"registry API (non FFI API), whose targets supports lowering \"\n-                \"to command buffer custom command, i.e., custom call target \"\n-                \"supports cuda-graph capturing for CUDA devices.\"));\n-\n   flag_list->push_back(tsl::Flag(\n       \"xla_gpu_graph_min_graph_size\",\n       int32_setter_for(&DebugOptions::set_xla_gpu_graph_min_graph_size),"
        },
        {
            "sha": "57220bca39a5c452d67f832452581589d653556f",
            "filename": "third_party/xla/xla/service/dump_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/20feacb439f35d30e70e5a7f80a9c536f6f0d9a5/third_party%2Fxla%2Fxla%2Fservice%2Fdump_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/20feacb439f35d30e70e5a7f80a9c536f6f0d9a5/third_party%2Fxla%2Fxla%2Fservice%2Fdump_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fdump_test.cc?ref=20feacb439f35d30e70e5a7f80a9c536f6f0d9a5",
            "patch": "@@ -483,13 +483,12 @@ TEST(DumpTest, GetNonDefaultDebugOptions) {\n \n TEST(DumpTest, DumpRepeatedStringTest) {\n   DebugOptions options = DefaultDebugOptionsIgnoringFlags();\n-  options.add_legacy_command_buffer_custom_call_targets(\"__gpu.gpu.triton\");\n+  options.add_xla_disable_hlo_passes(\"layout-assignment\");\n \n   std::string non_default_options = GetNonDefaultDebugOptions(options);\n   EXPECT_THAT(\n       non_default_options,\n-      testing::HasSubstr(\n-          \"legacy_command_buffer_custom_call_targets: \\\"__gpu.gpu.triton\\\"\"));\n+      testing::HasSubstr(\"xla_disable_hlo_passes: \\\"layout-assignment\\\"\\n\"));\n }\n \n }  // namespace"
        },
        {
            "sha": "62354792be770ae46de585e216aae9fe90636e07",
            "filename": "third_party/xla/xla/service/gpu/transforms/command_buffer_scheduling.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/20feacb439f35d30e70e5a7f80a9c536f6f0d9a5/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/20feacb439f35d30e70e5a7f80a9c536f6f0d9a5/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc?ref=20feacb439f35d30e70e5a7f80a9c536f6f0d9a5",
            "patch": "@@ -257,13 +257,6 @@ static bool IsCommand(const HloCustomCallInstruction* hlo,\n     return false;\n   }\n \n-  if (config.enabled_legacy_custom_call_targets.contains(\n-          hlo->custom_call_target())) {\n-    VLOG(3) << \"Recording legacy custom call target \"\n-            << hlo->custom_call_target() << \" into command buffer.\";\n-    return true;\n-  }\n-\n   // Check if FFI handler is compatible with command buffers.\n   auto registration = ffi::FindHandler(hlo->custom_call_target(), \"gpu\");\n   return registration.ok()\n@@ -934,14 +927,7 @@ absl::StatusOr<bool> CommandBufferScheduling::Run(\n     commands.insert(static_cast<DebugOptions::CommandBufferCmdType>(cmd_type));\n   }\n \n-  absl::flat_hash_set<std::string> legacy_custom_call_targets;\n-  for (const auto& target :\n-       debug_options.legacy_command_buffer_custom_call_targets()) {\n-    legacy_custom_call_targets.insert(target);\n-  }\n-\n   CommandBufferConfig config{std::move(commands),\n-                             std::move(legacy_custom_call_targets),\n                              device_description_};\n \n   // Erase command buffer cmd types that are not supported by the gpu runtime."
        },
        {
            "sha": "9662c0a3b76e9940701db3c378391d8ff0fe2815",
            "filename": "third_party/xla/xla/service/gpu/transforms/command_buffer_scheduling.h",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/20feacb439f35d30e70e5a7f80a9c536f6f0d9a5/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/20feacb439f35d30e70e5a7f80a9c536f6f0d9a5/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.h?ref=20feacb439f35d30e70e5a7f80a9c536f6f0d9a5",
            "patch": "@@ -75,7 +75,6 @@ class CommandBufferScheduling : public HloModulePass {\n     // DebugOptions control which commands are enabled. Long term we want to\n     // remove that flag and enable all supported commands by default.\n     absl::flat_hash_set<DebugOptions::CommandBufferCmdType> enabled_commands;\n-    absl::flat_hash_set<std::string> enabled_legacy_custom_call_targets;\n     const se::DeviceDescription& device_description;\n   };\n "
        },
        {
            "sha": "9f4fa4bfb2b02cbc9bc425c4e0306f00f2252731",
            "filename": "third_party/xla/xla/service/gpu/transforms/command_buffer_scheduling_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/20feacb439f35d30e70e5a7f80a9c536f6f0d9a5/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/20feacb439f35d30e70e5a7f80a9c536f6f0d9a5/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc?ref=20feacb439f35d30e70e5a7f80a9c536f6f0d9a5",
            "patch": "@@ -511,8 +511,8 @@ TEST_F(CommandBufferSchedulingTest, CollectCommandBufferSequence) {\n   }\n   EXPECT_EQ(seq.size(), 10);\n \n-  CommandBufferScheduling::CommandBufferConfig config{\n-      {DebugOptions::FUSION}, {}, device_desc()};\n+  CommandBufferScheduling::CommandBufferConfig config{{DebugOptions::FUSION},\n+                                                      device_desc()};\n \n   std::vector<HloInstructionSequence> command_buffer_sequences =\n       CommandBufferScheduling::CollectCommandBufferSequences(seq, config);"
        },
        {
            "sha": "7ffd17d361561b80cb5632efc0c81708601c0e59",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/20feacb439f35d30e70e5a7f80a9c536f6f0d9a5/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/20feacb439f35d30e70e5a7f80a9c536f6f0d9a5/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=20feacb439f35d30e70e5a7f80a9c536f6f0d9a5",
            "patch": "@@ -1280,12 +1280,7 @@ message DebugOptions {\n   // updating command buffer instance.\n   optional int64 xla_cmd_buffer_trace_cache_size = 311;\n \n-  // Custom call targets with legacy registry API (non FFI API),\n-  // that support recording to command buffer custom command,\n-  // i.e., custom call target supports cuda-graph capturing for CUDA devices.\n-  // This flag is read if CUSTOM_CALL command type is recorded into\n-  // command buffer.\n-  repeated string legacy_command_buffer_custom_call_targets = 314;\n+  reserved 314;  // was legacy_command_buffer_custom_call_targets\n \n   // This flag is used for controlling HLO dumping and NVTX marker. If turned\n   // on, both HLO dumping and NVTX marker will use syntactic sugar wrappers"
        }
    ],
    "stats": {
        "total": 96,
        "additions": 6,
        "deletions": 90
    }
}