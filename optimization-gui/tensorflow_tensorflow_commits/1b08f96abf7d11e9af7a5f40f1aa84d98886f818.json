{
    "author": "ermilovmaxim",
    "message": "Port to new GpuComputeCapability API. Last part\n\nPiperOrigin-RevId: 822676102",
    "sha": "1b08f96abf7d11e9af7a5f40f1aa84d98886f818",
    "files": [
        {
            "sha": "2d06e3c4a70d87eac19a53ff3a646325a82b8381",
            "filename": "third_party/xla/xla/backends/gpu/codegen/dynamic_slice_fusion_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fdynamic_slice_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fdynamic_slice_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fdynamic_slice_fusion_test.cc?ref=1b08f96abf7d11e9af7a5f40f1aa84d98886f818",
            "patch": "@@ -85,8 +85,8 @@ se::StreamExecutor* GpuExecutor() {\n \n bool IsAtLeastCuda12900(const se::StreamExecutor* stream_executor) {\n   const auto& device_description = stream_executor->GetDeviceDescription();\n-  const auto* cuda_cc = std::get_if<se::CudaComputeCapability>(\n-      &device_description.gpu_compute_capability());\n+  const auto* cuda_cc =\n+      device_description.gpu_compute_capability().cuda_compute_capability();\n   if (cuda_cc != nullptr) {\n     if (device_description.driver_version() >=\n             stream_executor::SemanticVersion(12, 9, 0) &&"
        },
        {
            "sha": "bb134b965c0f9d490228fd03c29e4864d934832c",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotune_cache_key.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.cc?ref=1b08f96abf7d11e9af7a5f40f1aa84d98886f818",
            "patch": "@@ -55,12 +55,12 @@ std::string AutotuneCacheKey::HloInstructionToCanonicalString(\n std::string AutotuneCacheKey::DeviceDescriptionToCacheKey(\n     const se::DeviceDescription& device_description) {\n   std::string compute_capability;\n-  if (auto* ccc = std::get_if<se::CudaComputeCapability>(\n-          &device_description.gpu_compute_capability())) {\n+  if (auto* ccc = device_description.gpu_compute_capability()\n+                      .cuda_compute_capability()) {\n     compute_capability = absl::StrCat(\"CUDA: \", ccc->major, \".\", ccc->minor);\n   } else {\n-    auto* rcc = std::get_if<se::RocmComputeCapability>(\n-        &device_description.gpu_compute_capability());\n+    auto* rcc =\n+        device_description.gpu_compute_capability().rocm_compute_capability();\n     CHECK(rcc != nullptr) << \"Unknown compute capability type\";\n     compute_capability = absl::StrCat(\"ROCM: \", rcc->gfx_version());\n   }"
        },
        {
            "sha": "6461e3e5a5da98f597edd1fe39328af1c61a5897",
            "filename": "third_party/xla/xla/service/gpu/autotuning/conv_algorithm_picker_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker_test.cc?ref=1b08f96abf7d11e9af7a5f40f1aa84d98886f818",
            "patch": "@@ -130,9 +130,8 @@ ENTRY main {\n   // Algorithm 14 is disabled for cuDNN 9 on V100\n   TF_ASSERT_OK_AND_ASSIGN(auto dnn_version, GetDnnVersionInfo(stream_exec));\n   if (dnn_version.major_version() >= 9 && dnn_version.major_version() < 10 &&\n-      std::holds_alternative<stream_executor::CudaComputeCapability>(cc) &&\n-      std::get<stream_executor::CudaComputeCapability>(cc).major == 7 &&\n-      std::get<stream_executor::CudaComputeCapability>(cc).minor == 0) {\n+      cc.IsCuda() && cc.cuda_compute_capability()->major == 7 &&\n+      cc.cuda_compute_capability()->minor == 0) {\n     EXPECT_TRUE(conv->backend_config<GpuBackendConfig>()\n                     ->has_cudnn_conv_backend_config() &&\n                 conv->backend_config<GpuBackendConfig>()"
        },
        {
            "sha": "12fe18159260e6ed95f52f6b037ab6d6612964eb",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.h",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h?ref=1b08f96abf7d11e9af7a5f40f1aa84d98886f818",
            "patch": "@@ -201,11 +201,6 @@ class GemmFusionAutotunerImpl {\n     return config_.GetGpuComputeCapability();\n   }\n \n-  bool isRocm() const {\n-    return std::holds_alternative<se::RocmComputeCapability>(\n-        GetComputeCapability());\n-  }\n-\n   bool AddLibConfigs(const HloFusionInstruction& fusion,\n                      const HloDotInstruction* dot,\n                      std::vector<BackendConfig>& configs);"
        },
        {
            "sha": "d48ec378da7a4be856f6619b4334e31f0449a962",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_cuda.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc?ref=1b08f96abf7d11e9af7a5f40f1aa84d98886f818",
            "patch": "@@ -52,7 +52,8 @@ bool GemmFusionAutotunerImpl::AddLibConfigs(\n     const HloFusionInstruction& fusion, const HloDotInstruction* dot,\n     std::vector<BackendConfig>& configs) {\n   // Add cuDNN plans, if available.\n-  auto cc = std::get<se::CudaComputeCapability>(GetComputeCapability());\n+  stream_executor::CudaComputeCapability cc =\n+      *GetComputeCapability().cuda_compute_capability();\n   bool is_cudnn_enabled =\n       !config_.IsDeviceless() &&\n       GetDnnVersionInfoOrDefault(config_.GetExecutor()).major_version() >= 9 &&\n@@ -81,8 +82,8 @@ bool GemmFusionAutotunerImpl::AddLibConfigs(\n \n std::vector<TritonGemmConfig> GemmFusionAutotunerImpl::GetDefaultTritonConfigs()\n     const {\n-  auto compute_capability =\n-      std::get<se::CudaComputeCapability>(GetComputeCapability());\n+  stream_executor::CudaComputeCapability compute_capability =\n+      *GetComputeCapability().cuda_compute_capability();\n   std::vector<TritonGemmConfig> configs;\n \n   if (compute_capability.IsAtLeastBlackwell()) {"
        },
        {
            "sha": "e2dc5ca11d8e59ff806b471aa7b1b3071c267dea",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 43,
            "deletions": 42,
            "changes": 85,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=1b08f96abf7d11e9af7a5f40f1aa84d98886f818",
            "patch": "@@ -197,13 +197,13 @@ class StatelessAutotunerTest : public HloTestBase {\n       SymbolicExprContext* symbolic_expr_context) {\n     const HloFusionInstruction& fusion = *Cast<HloFusionInstruction>(\n         module.entry_computation()->root_instruction());\n-    if (!isRocm()) {\n-      auto cu_compute_capability =\n-          std::get<se::CudaComputeCapability>(compute_capability);\n+    if (GpuComputeComp().IsCuda()) {\n+      auto* cu_compute_capability =\n+          compute_capability.cuda_compute_capability();\n       se::GpuDeviceInfoProto deviceless_proto;\n       auto ccc = deviceless_proto.mutable_cuda_compute_capability();\n-      ccc->set_major(cu_compute_capability.major);\n-      ccc->set_minor(cu_compute_capability.minor);\n+      ccc->set_major(cu_compute_capability->major);\n+      ccc->set_minor(cu_compute_capability->minor);\n     }\n \n     DeviceConfig test_config{backend().default_stream_executor(),\n@@ -237,10 +237,6 @@ class StatelessAutotunerTest : public HloTestBase {\n         .gpu_compute_capability();\n   }\n \n-  bool isRocm() {\n-    return std::holds_alternative<se::RocmComputeCapability>(GpuComputeComp());\n-  }\n-\n   // Returns the config for the current device.\n   absl::StatusOr<std::vector<GemmFusionAutotunerImpl::BackendConfig>>\n   GetPossibleMatmulAutotuneConfigs(const HloModule& module) {\n@@ -321,7 +317,7 @@ TEST_F(StatelessAutotunerTest, CublasFallbackForBf16Bf16F32Algorithm) {\n \n   TF_ASSERT_OK_AND_ASSIGN(auto configs,\n                           GetPossibleMatmulAutotuneConfigs(*module));\n-  if (!isRocm()) {\n+  if (!GpuComputeComp().IsRocm()) {\n     switch (GetCudaComputeCapability().major) {\n       case se::CudaComputeCapability::kAmpere:\n         EXPECT_TRUE(hasCublasConfig(configs))\n@@ -361,8 +357,8 @@ class GemmFusionAutotunerTest : public StatelessAutotunerTest {\n   }\n \n   stream_executor::GpuComputeCapability CudaAmpereOrRocm() {\n-    if (isRocm()) {\n-      return GetRocmComputeCapability();\n+    if (GpuComputeComp().IsRocm()) {\n+      return GpuComputeComp();\n     } else {\n       return stream_executor::GpuComputeCapability{\n           stream_executor::CudaComputeCapability{\n@@ -427,7 +423,8 @@ GetPossibleMatmulAutotuneTritonConfigs(\n   TF_ASSIGN_OR_RETURN(se::DeviceDescription device_description,\n                       se::DeviceDescription::FromProto(\n                           se::GpuDeviceInfoProto::default_instance()));\n-  device_description.set_gpu_compute_capability(compute_capability);\n+  device_description.set_gpu_compute_capability(\n+      se::GpuComputeCapability{compute_capability});\n   // Using H100 numbers as the most relevant example here.\n   // https://docs.nvidia.com/cuda/cuda-c-programming-guide/#features-and-technical-specifications-technical-specifications-per-compute-capability\n   // https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/#nvidia_h100_gpu_architecture_in-depth\n@@ -446,7 +443,7 @@ GetPossibleMatmulAutotuneTritonConfigs(\n }\n \n TEST_F(GemmFusionAutotunerTest, AmpereUsesMoreThanTwoStages) {\n-  if (isRocm()) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not supported on ROCm.\";\n   }\n   std::unique_ptr<VerifiedHloModule> module = ParseAndReturnVerifiedModule(R\"(\n@@ -625,7 +622,7 @@ ENTRY e {\n \n // TODO(b/344770374): Make this test not fragile.\n TEST_F(GemmFusionAutotunerTest, DoNotRunAutotuningKernelSpillingRegisters) {\n-  if (isRocm()) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not supported on ROCm.\";\n   }\n   const std::string kHloText = R\"(\n@@ -808,11 +805,10 @@ ENTRY main {\n \n   TF_EXPECT_OK(HloTestBase::RunHloPass(&pipeline, module.get()));\n   const bool is_at_least_hopper =\n-      std::holds_alternative<se::CudaComputeCapability>(\n-          autotune_config.GetGpuComputeCapability()) &&\n-      std::get<se::CudaComputeCapability>(\n-          autotune_config.GetGpuComputeCapability())\n-          .IsAtLeastHopper();\n+      autotune_config.GetGpuComputeCapability().IsCuda() &&\n+      autotune_config.GetGpuComputeCapability()\n+          .cuda_compute_capability()\n+          ->IsAtLeastHopper();\n   TF_ASSERT_OK_AND_ASSIGN(\n       bool filecheck_matches,\n       RunFileCheck(module->ToString(), is_at_least_hopper\n@@ -822,8 +818,9 @@ ENTRY main {\n }\n \n TEST_F(GemmFusionAutotunerDumpTest, DumpingWorks) {\n-  if (isRocm() || GetDebugOptionsForTest()\n-                      .xla_gpu_experimental_disable_binary_libraries()) {\n+  if (GpuComputeComp().IsRocm() ||\n+      GetDebugOptionsForTest()\n+          .xla_gpu_experimental_disable_binary_libraries()) {\n     GTEST_SKIP() << \"Not supported on ROCm or with binary libraries disabled.\";\n   }\n   HloModuleConfig config;\n@@ -891,8 +888,9 @@ CHECK: cublas\n }\n \n TEST_F(GemmFusionAutotunerTest, AutotuneCuDnnFusion) {\n-  if (isRocm() || GetDebugOptionsForTest()\n-                      .xla_gpu_experimental_disable_binary_libraries()) {\n+  if (GpuComputeComp().IsRocm() ||\n+      GetDebugOptionsForTest()\n+          .xla_gpu_experimental_disable_binary_libraries()) {\n     GTEST_SKIP() << \"Not supported on ROCm or with binary libraries disabled.\";\n   }\n   const std::string kHlo = R\"(\n@@ -1202,7 +1200,7 @@ ENTRY entry {\n }\n \n TEST_F(GemmFusionAutotunerTest, CreatesCustomKernelFusionConfigs) {\n-  if (isRocm()) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not supported on ROCm.\";\n   }\n   const std::string kHlo = R\"(\n@@ -1241,7 +1239,7 @@ TEST_F(GemmFusionAutotunerTest, CreatesCustomKernelFusionConfigs) {\n }\n \n TEST_F(GemmFusionAutotunerTest, GeneratesTwoConfigsForUpcastGemmWithPrologue) {\n-  if (isRocm()) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not supported on ROCm.\";\n   }\n   const std::string kHlo = R\"(\n@@ -1273,8 +1271,9 @@ TEST_F(GemmFusionAutotunerTest, GeneratesTwoConfigsForUpcastGemmWithPrologue) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       const std::vector<GemmFusionAutotunerImpl::BackendConfig> configs,\n       GetPossibleMatmulAutotuneConfigs(\n-          *module, compute_capability, GetToolkitVersion(),\n-          GetDebugOptionsForTest(), &symbolic_expr_context_));\n+          *module, se::GpuComputeCapability{compute_capability},\n+          GetToolkitVersion(), GetDebugOptionsForTest(),\n+          &symbolic_expr_context_));\n   EXPECT_EQ(\n       2, std::count_if(\n              configs.begin(), configs.end(),\n@@ -1287,7 +1286,7 @@ TEST_F(GemmFusionAutotunerTest, GeneratesTwoConfigsForUpcastGemmWithPrologue) {\n TEST_F(GemmFusionAutotunerTest, GeneratesOneConfigForUpcastGemmWithPrologue) {\n   // Same as GeneratesTwoConfigsForUpcastGemmWithPrologue, but with contracting\n   // dimension size = 128 which is not supported by the SplitK kernel.\n-  if (isRocm()) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not supported on ROCm.\";\n   }\n   const std::string kHlo = R\"(\n@@ -1319,8 +1318,9 @@ TEST_F(GemmFusionAutotunerTest, GeneratesOneConfigForUpcastGemmWithPrologue) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       const std::vector<GemmFusionAutotunerImpl::BackendConfig> configs,\n       GetPossibleMatmulAutotuneConfigs(\n-          *module, compute_capability, GetToolkitVersion(),\n-          GetDebugOptionsForTest(), &symbolic_expr_context_));\n+          *module, se::GpuComputeCapability{compute_capability},\n+          GetToolkitVersion(), GetDebugOptionsForTest(),\n+          &symbolic_expr_context_));\n   EXPECT_EQ(\n       1, std::count_if(\n              configs.begin(), configs.end(),\n@@ -1332,7 +1332,7 @@ TEST_F(GemmFusionAutotunerTest, GeneratesOneConfigForUpcastGemmWithPrologue) {\n \n TEST_F(GemmFusionAutotunerTest,\n        GeneratesConfigForUpcastGemmWithPrologueAndEpilogue) {\n-  if (isRocm()) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not supported on ROCm.\";\n   }\n   const std::string kHlo = R\"(\n@@ -1368,8 +1368,9 @@ TEST_F(GemmFusionAutotunerTest,\n   TF_ASSERT_OK_AND_ASSIGN(\n       const std::vector<GemmFusionAutotunerImpl::BackendConfig> configs,\n       GetPossibleMatmulAutotuneConfigs(\n-          *module, compute_capability, GetToolkitVersion(),\n-          GetDebugOptionsForTest(), &symbolic_expr_context_));\n+          *module, se::GpuComputeCapability{compute_capability},\n+          GetToolkitVersion(), GetDebugOptionsForTest(),\n+          &symbolic_expr_context_));\n   EXPECT_EQ(\n       2, std::count_if(\n              configs.begin(), configs.end(),\n@@ -1486,7 +1487,7 @@ class GemmFusionShardedAutotunerTest : public GemmFusionAutotunerTest {\n TEST_F(\n     GemmFusionShardedAutotunerTest,\n     AutotuningSucceedsWhenKeyValueStoreAlreadyContainsAutotuningResultsForTheInputModule) {  // NOLINT(whitespace/line_length)\n-  if (isRocm()) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not supported on ROCm.\";\n   }\n   const std::string kHlo = R\"(\n@@ -1545,7 +1546,7 @@ TEST_F(\n TEST_F(\n     GemmFusionShardedAutotunerTest,\n     AutotuningStoresDifferentResultsForTheSameFusionInDifferentModules) {  // NOLINT(whitespace/line_length)\n-  if (isRocm()) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not supported on ROCm.\";\n   }\n   const std::string kHlo1 = R\"(\n@@ -1626,7 +1627,7 @@ TEST_F(\n }\n \n TEST_F(GemmFusionAutotunerTest, RewritesGemmFusionToCustomKernelFusion) {\n-  if (isRocm()) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not supported on ROCm.\";\n   }\n   const std::string kHlo = R\"(\n@@ -1711,7 +1712,7 @@ ENTRY e {\n }\n \n TEST_F(GemmFusionAutotunerTest, VerifyHopperConfigsAreDifferentFromBlackwell) {\n-  if (isRocm()) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not supported on ROCm.\";\n   }\n \n@@ -1752,7 +1753,7 @@ TEST_F(GemmFusionAutotunerTest, VerifyHopperConfigsAreDifferentFromBlackwell) {\n }\n \n TEST_F(GemmFusionAutotunerTest, ScaledDotConfigsAreGenerated) {\n-  if (isRocm()) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not supported on ROCm.\";\n   }\n \n@@ -1781,7 +1782,7 @@ TEST_F(GemmFusionAutotunerTest, ScaledDotConfigsAreGenerated) {\n }\n \n TEST_F(GemmFusionAutotunerTest, ScaledDotConfigsHaveCuBlasFallback) {\n-  if (isRocm()) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not supported on ROCm.\";\n   }\n \n@@ -1829,7 +1830,7 @@ class GemmFusionAutotunerEnableTma : public GemmFusionAutotunerTest {\n \n TEST_F(GemmFusionAutotunerEnableTma,\n        TmaConfigsAreGeneratedOnlyForHopperAndWorkCorrectly) {\n-  if (isRocm()) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not supported on ROCm.\";\n   }\n \n@@ -1885,7 +1886,7 @@ TEST_F(GemmFusionAutotunerEnableTma,\n \n TEST_F(GemmFusionAutotunerEnableTma,\n        TmaConfigsGeneratedAndRunCorrectlyForDotsOfBroadcasts) {\n-  if (isRocm()) {\n+  if (GpuComputeComp().IsRocm()) {\n     GTEST_SKIP() << \"Not supported on ROCm.\";\n   }\n "
        },
        {
            "sha": "c2e31f4603ccdeb364b97a38c4efd393ccb11b63",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_fused_conv_rewriter_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fused_conv_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fused_conv_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_fused_conv_rewriter_test.cc?ref=1b08f96abf7d11e9af7a5f40f1aa84d98886f818",
            "patch": "@@ -78,11 +78,11 @@ static const std::initializer_list<absl::string_view> kf16f32{\"f16\", \"f32\"};\n class CudnnFusedConvRewriterHloTest : public HloTestBase {\n  public:\n   bool IsCuda() const {\n-    return std::holds_alternative<se::CudaComputeCapability>(\n-        backend()\n-            .default_stream_executor()\n-            ->GetDeviceDescription()\n-            .gpu_compute_capability());\n+    return backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .gpu_compute_capability()\n+        .IsCuda();\n   }\n   se::CudaComputeCapability GetCudaComputeCapability() const {\n     return backend()\n@@ -119,11 +119,11 @@ class CudnnFusedConvRewriterHloTest : public HloTestBase {\n class CudnnFusedConvRewriterTest : public GpuCodegenTest {\n  public:\n   bool IsCuda() const {\n-    return std::holds_alternative<se::CudaComputeCapability>(\n-        backend()\n-            .default_stream_executor()\n-            ->GetDeviceDescription()\n-            .gpu_compute_capability());\n+    return backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .gpu_compute_capability()\n+        .IsCuda();\n   }\n   se::CudaComputeCapability GetCudaComputeCapability() const {\n     return backend()"
        },
        {
            "sha": "39d34d9bec073dc8a6c7894ab6204dc5bd41820e",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 4,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_test.cc?ref=1b08f96abf7d11e9af7a5f40f1aa84d98886f818",
            "patch": "@@ -646,7 +646,9 @@ ENTRY entry {\n       hlo_module->entry_computation()->ComputeProgramShape());\n \n   GpuLayoutAssignment layout_assignment(\n-      &computation_layout, se::RocmComputeCapability::EarliestRDNASupport(),\n+      &computation_layout,\n+      se::GpuComputeCapability{\n+          se::RocmComputeCapability::EarliestRDNASupport()},\n       GetDnnVersion(), GetDeviceDescription());\n \n   EXPECT_THAT(layout_assignment.Run(hlo_module.get()),\n@@ -683,7 +685,9 @@ ENTRY entry {\n       hlo_module->entry_computation()->ComputeProgramShape());\n \n   GpuLayoutAssignment layout_assignment(\n-      &computation_layout, se::RocmComputeCapability::EarliestRDNASupport(),\n+      &computation_layout,\n+      se::GpuComputeCapability{\n+          se::RocmComputeCapability::EarliestRDNASupport()},\n       GetDnnVersion(), GetDeviceDescription());\n \n   EXPECT_THAT(layout_assignment.Run(hlo_module.get()),\n@@ -723,7 +727,9 @@ ENTRY entry {\n       hlo_module->entry_computation()->ComputeProgramShape());\n \n   GpuLayoutAssignment layout_assignment(\n-      &computation_layout, se::RocmComputeCapability::EarliestCDNASupport(),\n+      &computation_layout,\n+      se::GpuComputeCapability{\n+          se::RocmComputeCapability::EarliestCDNASupport()},\n       GetDnnVersion(), GetDeviceDescription());\n \n   EXPECT_THAT(layout_assignment.Run(hlo_module.get()),\n@@ -763,7 +769,9 @@ ENTRY entry {\n       hlo_module->entry_computation()->ComputeProgramShape());\n \n   GpuLayoutAssignment layout_assignment(\n-      &computation_layout, se::RocmComputeCapability::EarliestCDNASupport(),\n+      &computation_layout,\n+      se::GpuComputeCapability{\n+          se::RocmComputeCapability::EarliestCDNASupport()},\n       GetDnnVersion(), GetDeviceDescription());\n \n   EXPECT_THAT(layout_assignment.Run(hlo_module.get()),"
        },
        {
            "sha": "0b1b9b16bd32d874587eeba717badcb090c7e361",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_test.cc?ref=1b08f96abf7d11e9af7a5f40f1aa84d98886f818",
            "patch": "@@ -69,9 +69,8 @@ TEST(CudaExecutorTest, CreateDeviceDescription) {\n   EXPECT_THAT(result->model_str(), Not(IsEmpty()));\n   EXPECT_THAT(result->device_vendor(), \"NVIDIA Corporation\");\n \n-  EXPECT_THAT(result->gpu_compute_capability(),\n-              VariantWith<CudaComputeCapability>(::testing::Field(\n-                  \"major\", &CudaComputeCapability::major, Ge(1))));\n+  EXPECT_THAT(*result->gpu_compute_capability().cuda_compute_capability(),\n+              ::testing::Field(\"major\", &CudaComputeCapability::major, Ge(1)));\n }\n \n TEST(CudaExecutorTest, GetCudaKernel) {"
        },
        {
            "sha": "5369e6c6233ca8785d0c9fca4bcc6285a589714b",
            "filename": "third_party/xla/xla/stream_executor/device_description.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_description.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_description.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_description.cc?ref=1b08f96abf7d11e9af7a5f40f1aa84d98886f818",
            "patch": "@@ -68,12 +68,10 @@ absl::StatusOr<DeviceDescription> DeviceDescription::FromProto(\n \n GpuDeviceInfoProto DeviceDescription::ToGpuProto() const {\n   stream_executor::GpuDeviceInfoProto proto;\n-  if (auto* ptr = std::get_if<stream_executor::CudaComputeCapability>(\n-          &gpu_compute_capability_)) {\n+  if (auto* ptr = gpu_compute_capability_.cuda_compute_capability()) {\n     *proto.mutable_cuda_compute_capability() = ptr->ToProto();\n   }\n-  if (auto* ptr = std::get_if<stream_executor::RocmComputeCapability>(\n-          &gpu_compute_capability_)) {\n+  if (auto* ptr = gpu_compute_capability_.rocm_compute_capability()) {\n     *proto.mutable_rocm_compute_capability() = ptr->ToProto();\n   }\n \n@@ -106,17 +104,15 @@ const GpuComputeCapability &DeviceDescription::gpu_compute_capability() const {\n }\n \n CudaComputeCapability DeviceDescription::cuda_compute_capability() const {\n-  if (auto *ptr =\n-          std::get_if<CudaComputeCapability>(&gpu_compute_capability_)) {\n+  if (auto* ptr = gpu_compute_capability_.cuda_compute_capability()) {\n     return *ptr;\n   }\n   // Fallback for backwards compatibility.\n   return CudaComputeCapability{-1, -1};\n }\n \n RocmComputeCapability DeviceDescription::rocm_compute_capability() const {\n-  if (auto *ptr =\n-          std::get_if<RocmComputeCapability>(&gpu_compute_capability_)) {\n+  if (auto* ptr = gpu_compute_capability_.rocm_compute_capability()) {\n     return *ptr;\n   }\n   return RocmComputeCapability{};"
        },
        {
            "sha": "5ffc38db918f84b5315ffc71291fe177ff7a9228",
            "filename": "third_party/xla/xla/stream_executor/gpu/tma_metadata.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Ftma_metadata.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Ftma_metadata.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Ftma_metadata.cc?ref=1b08f96abf7d11e9af7a5f40f1aa84d98886f818",
            "patch": "@@ -462,8 +462,8 @@ absl::StatusOr<TmaMetadata> TmaMetadata::FromProto(\n \n bool IsTmaAvailableForDevice(\n     const stream_executor::DeviceDescription& device_info) {\n-  if (auto* cuda_cc = std::get_if<stream_executor::CudaComputeCapability>(\n-          &device_info.gpu_compute_capability())) {\n+  if (auto* cuda_cc =\n+          device_info.gpu_compute_capability().cuda_compute_capability()) {\n     return cuda_cc->IsAtLeastHopper();\n   }\n   return false;"
        },
        {
            "sha": "f61314483d09fcbe9796bf08c4fa6d45f2581f1f",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_compute_capability.h",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_compute_capability.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b08f96abf7d11e9af7a5f40f1aa84d98886f818/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_compute_capability.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_compute_capability.h?ref=1b08f96abf7d11e9af7a5f40f1aa84d98886f818",
            "patch": "@@ -51,7 +51,7 @@ class RocmComputeCapability {\n     return RocmComputeCapability{\"gfx1030\"};\n   }\n \n-  std::string gcn_arch_name() const { return gcn_arch_name_; }\n+  const std::string& gcn_arch_name() const { return gcn_arch_name_; }\n \n   std::string ToString() const { return gcn_arch_name(); }\n \n@@ -198,18 +198,17 @@ class RocmComputeCapability {\n   template <typename... ArrayOfStrings>\n   bool IsThisGfxInAnyList(ArrayOfStrings&&... arr) const {\n     static_assert(sizeof...(arr) >= 1);\n-    const auto gfx = gfx_version();\n+    const std::string gfx = gfx_version();\n     return (implIsThisGfxInAnyList(std::begin(arr), std::end(arr), gfx) || ...);\n   }\n \n   /// \\brief Template-less implementation of IsThisGfxInAnyList().\n   /// \\warning Don't use directly!\n   bool implIsThisGfxInAnyList(const absl::string_view* beg,\n                               const absl::string_view* end,\n-                              const std::string& gfx) const {\n-    return std::any_of(beg, end, [&gfx = gfx](const absl::string_view& s) {\n-      return gfx == s;\n-    });\n+                              const absl::string_view gfx) const {\n+    return std::any_of(\n+        beg, end, [&gfx = gfx](const absl::string_view s) { return gfx == s; });\n   }\n \n   std::string gcn_arch_name_{kInvalidGfx};  // default to invalid arch."
        }
    ],
    "stats": {
        "total": 182,
        "additions": 90,
        "deletions": 92
    }
}