{
    "author": "tensorflower-gardener",
    "message": "Handle fission in legacy cache.\n\nPiperOrigin-RevId: 833326696",
    "sha": "37ad2d23c6c36e24300ed970a3075a63008da3df",
    "files": [
        {
            "sha": "8c7ff62f01022a9e079eef5dd6c07728bf6d474f",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/37ad2d23c6c36e24300ed970a3075a63008da3df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/37ad2d23c6c36e24300ed970a3075a63008da3df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=37ad2d23c6c36e24300ed970a3075a63008da3df",
            "patch": "@@ -880,14 +880,14 @@ xla_cc_test(\n         \"//xla/backends/autotuner:autotuner_cache_interface\",\n         \"//xla/backends/autotuner:autotuner_cache_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/parser:hlo_parser\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/protobuf:dnn_proto_cc\",\n         \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@com_google_protobuf//:any_cc_proto\",\n         \"@local_tsl//tsl/platform:path\","
        },
        {
            "sha": "f85951d686c1f14a266c3e96277e7aa128557f5e",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/legacy_cache.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/37ad2d23c6c36e24300ed970a3075a63008da3df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/37ad2d23c6c36e24300ed970a3075a63008da3df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.cc?ref=37ad2d23c6c36e24300ed970a3075a63008da3df",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/service/gpu/autotuning/autotune_cache_key.h\"\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -46,7 +47,7 @@ std::optional<LegacyCache::Config> LegacyCache::Lookup(\n   if (!result->has_value()) {\n     return std::nullopt;\n   }\n-  return GetConfig(result->value());\n+  return GetConfig(result->value(), instr->opcode() == HloOpcode::kFusion);\n }\n \n absl::Status LegacyCache::Insert(const HloInstruction* instr,\n@@ -99,13 +100,16 @@ AutotuneCacheKey LegacyCache::GetAutotuneCacheKey(const HloInstruction& instr) {\n }\n \n std::optional<LegacyCache::Config> LegacyCache::GetConfig(\n-    const AutotuneResult& result) {\n+    const AutotuneResult& result, bool is_fusion_instruction) {\n   Config config;\n   if (result.has_triton()) {\n     config.codegen_backend_name = \"Triton\";\n     config.backend_config.PackFrom(result.triton());\n   } else if (result.has_gemm()) {\n     config.codegen_backend_name = \"Cublas\";\n+    if (is_fusion_instruction) {\n+      config.codegen_backend_name = \"Cublas_fission\";\n+    }\n     config.backend_config.PackFrom(result.gemm());\n   } else if (result.has_algorithm()) {\n     config.codegen_backend_name = \"Cudnn\";\n@@ -124,7 +128,8 @@ std::optional<AutotuneResult> LegacyCache::GetAutotuneResult(\n   AutotuneResult result;\n   if (config.codegen_backend_name == \"Triton\") {\n     config.backend_config.UnpackTo(result.mutable_triton());\n-  } else if (config.codegen_backend_name == \"Cublas\") {\n+  } else if (config.codegen_backend_name == \"Cublas\" ||\n+             config.codegen_backend_name == \"Cublas_fission\") {\n     config.backend_config.UnpackTo(result.mutable_gemm());\n   } else if (config.codegen_backend_name == \"Cudnn\") {\n     config.backend_config.UnpackTo(result.mutable_algorithm());"
        },
        {
            "sha": "48c30b085235a4d111684f69f390f2b1b43daf3f",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/legacy_cache.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/37ad2d23c6c36e24300ed970a3075a63008da3df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/37ad2d23c6c36e24300ed970a3075a63008da3df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.h?ref=37ad2d23c6c36e24300ed970a3075a63008da3df",
            "patch": "@@ -59,7 +59,8 @@ class LegacyCache : public AutotunerCacheInterface {\n \n   // Translates between the AutotunerCacheInterface::Config and the\n   // AutotuneResult.\n-  std::optional<Config> GetConfig(const AutotuneResult& result);\n+  std::optional<Config> GetConfig(const AutotuneResult& result,\n+                                  bool is_fusion_instruction);\n   std::optional<AutotuneResult> GetAutotuneResult(const Config& config);\n \n   const std::string cache_dir_;"
        },
        {
            "sha": "de1f1dee8bc20a50c4eeeb416e2a4f4a2ef6820a",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/legacy_cache_test.cc",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/37ad2d23c6c36e24300ed970a3075a63008da3df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/37ad2d23c6c36e24300ed970a3075a63008da3df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache_test.cc?ref=37ad2d23c6c36e24300ed970a3075a63008da3df",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"xla/backends/autotuner/autotuner_cache.pb.h\"\n #include \"xla/backends/autotuner/autotuner_cache_interface.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -100,6 +101,13 @@ class LegacyCacheTest : public ::testing::Test {\n     return config;\n   }\n \n+  Config CreateDummyCublasFissionConfig() {\n+    Config config;\n+    config.codegen_backend_name = \"Cublas_fission\";\n+    config.backend_config.PackFrom(AutotuneResult::GemmKey());\n+    return config;\n+  }\n+\n   Config CreateDummyCudnnConfig() {\n     Config config;\n     config.codegen_backend_name = \"Cudnn\";\n@@ -166,6 +174,31 @@ TEST_F(LegacyCacheTest, InsertAndLookupCublas) {\n   EXPECT_THAT(cache.Lookup(instr.get()), Optional(ConfigEq(config)));\n }\n \n+TEST_F(LegacyCacheTest, InsertAndLookupCublasFission) {\n+  auto cache = LegacyCache(test_dir_, mode_, device_desc_);\n+  constexpr char kHLO[] = R\"(\n+HloModule test_module\n+\n+fused_computation {\n+  param.0 = f32[] parameter(0)\n+  param.1 = f32[] parameter(1)\n+  ROOT add.0 = f32[] add(param.0, param.1)\n+}\n+\n+ENTRY main {\n+  p0 = f32[] parameter(0)\n+  p1 = f32[] parameter(1)\n+  ROOT fusion.0 = f32[] fusion(p0, p1), kind=kLoop, calls=fused_computation\n+}\n+)\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(kHLO));\n+  auto instr = module->entry_computation()->root_instruction();\n+  Config config = CreateDummyCublasFissionConfig();\n+\n+  TF_ASSERT_OK(cache.Insert(instr, config));\n+  EXPECT_THAT(cache.Lookup(instr), Optional(ConfigEq(config)));\n+}\n+\n TEST_F(LegacyCacheTest, InsertAndLookupCudnn) {\n   auto cache = LegacyCache(test_dir_, mode_, device_desc_);\n   auto instr = CreateDummyInstr(\"hlo3\");"
        }
    ],
    "stats": {
        "total": 49,
        "additions": 44,
        "deletions": 5
    }
}