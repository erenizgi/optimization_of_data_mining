{
    "author": "tensorflower-gardener",
    "message": "[Autotuner] Add Fp8 cuBLASLt fallback for cublas backend.\n\n- This is to match the current behavior in XLA, gemm-rewriter already has lots of checks to rewrite to cublasLt matmul.\n- We are anyway trying to deprecate legacy cuBLAS and enable cuBLASLt.\n\nPiperOrigin-RevId: 839273756",
    "sha": "4db7e0e6515b0645ae70054551923ccdd586bc01",
    "files": [
        {
            "sha": "821224184c4512e0acf483e8039669e5c437efb1",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4db7e0e6515b0645ae70054551923ccdd586bc01/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4db7e0e6515b0645ae70054551923ccdd586bc01/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=4db7e0e6515b0645ae70054551923ccdd586bc01",
            "patch": "@@ -832,7 +832,6 @@ xla_test(\n         \":fission_backend\",\n         \":gpu_codegen_backend\",\n         \"//xla/backends/autotuner:codegen_backend\",\n-        \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass_pipeline\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\","
        },
        {
            "sha": "b8b927405b021488ff8d8fc5b89b347ab7dafb95",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublas.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 9,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4db7e0e6515b0645ae70054551923ccdd586bc01/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4db7e0e6515b0645ae70054551923ccdd586bc01/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.cc?ref=4db7e0e6515b0645ae70054551923ccdd586bc01",
            "patch": "@@ -25,14 +25,9 @@ limitations under the License.\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/hlo/utils/hlo_query.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n-#include \"xla/service/gpu/transforms/dot_algorithm_rewriter.h\"\n-#include \"xla/service/gpu/transforms/gemm_rewriter.h\"\n-#include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/stream_executor/blas.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/device_memory.h\"\n@@ -49,10 +44,19 @@ namespace se = ::stream_executor;\n \n absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n CublasBackend::GetSupportedConfigs(const HloInstruction& instr) {\n-  if (!IsLegacyCublasMatmul(instr)) {\n+  if (!IsSupported(instr)) {\n     return std::vector<std::unique_ptr<BackendConfig>>();\n   }\n \n+  if (ShouldUseCublasLt(instr)) {\n+    std::vector<std::unique_ptr<BackendConfig>> configs;\n+    AutotuneResult::GemmKey gemm_key;\n+    gemm_key.set_algorithm(0);\n+    configs.push_back(std::make_unique<google::protobuf::Any>());\n+    configs.back()->PackFrom(gemm_key);\n+    return configs;\n+  }\n+\n   std::unique_ptr<se::DeviceMemoryAllocator> allocator =\n       std::make_unique<se::StreamExecutorMemoryAllocator>(stream_executor());\n   TF_ASSIGN_OR_RETURN(\n@@ -126,14 +130,16 @@ CublasBackend::GetSupportedConfigs(const HloInstruction& instr) {\n \n absl::StatusOr<std::unique_ptr<BackendConfig>> CublasBackend::GetDefaultConfig(\n     const HloInstruction& instr) {\n-  if (!IsLegacyCublasMatmul(instr)) {\n+  if (!IsSupported(instr)) {\n     return absl::InvalidArgumentError(\n         \"CublasBackend does not support this instruction.\");\n   }\n-\n   AutotuneResult::GemmKey gemm_key;\n   gemm_key.set_algorithm(se::blas::kDefaultAlgorithm);\n   auto any = std::make_unique<google::protobuf::Any>();\n+  if (ShouldUseCublasLt(instr)) {\n+    gemm_key.set_algorithm(0);\n+  }\n   any->PackFrom(gemm_key);\n   return any;\n }\n@@ -154,7 +160,11 @@ absl::Status CublasBackend::ApplyConfig(HloInstruction& instr,\n }\n \n bool CublasBackend::IsSupported(const HloInstruction& instr) {\n-  return IsLegacyCublasMatmul(instr);\n+  return IsLegacyCublasMatmul(instr) || ShouldUseCublasLt(instr);\n+}\n+\n+bool CublasBackend::ShouldUseCublasLt(const HloInstruction& instr) {\n+  return fp8_lt_fallback_ && IsCublasLtMatmulF8(instr);\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "c68ff614727d7fac0a88985e97ca1b187db4a3b6",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublas.h",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4db7e0e6515b0645ae70054551923ccdd586bc01/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4db7e0e6515b0645ae70054551923ccdd586bc01/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.h?ref=4db7e0e6515b0645ae70054551923ccdd586bc01",
            "patch": "@@ -32,7 +32,8 @@ limitations under the License.\n namespace xla {\n namespace gpu {\n \n-// A codegen backend for cuBLAS.\n+// A codegen backend for cuBLAS, with configurable fallback to cuBLAS LT for F8\n+// matmuls.\n // This backend is used to autotune cuBLAS algorithms.\n //\n // Cublas calls are represented as custom-call instructions, with and\n@@ -48,9 +49,11 @@ class CublasBackend : public GpuCodegenBackend {\n  public:\n   explicit CublasBackend(stream_executor::StreamExecutor* stream_executor,\n                          const DebugOptions* debug_options, Compiler* compiler,\n-                         const Compiler::GpuTargetConfig* target_config)\n+                         const Compiler::GpuTargetConfig* target_config,\n+                         bool fp8_lt_fallback = false)\n       : GpuCodegenBackend(\"Cublas\", debug_options, compiler, target_config,\n-                          stream_executor) {}\n+                          stream_executor),\n+        fp8_lt_fallback_(fp8_lt_fallback) {}\n \n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n   GetSupportedConfigs(const HloInstruction& instr) override;\n@@ -62,7 +65,10 @@ class CublasBackend : public GpuCodegenBackend {\n                            const BackendConfig& config) override;\n \n  private:\n+  bool ShouldUseCublasLt(const HloInstruction& instr);\n+\n   bool IsSupported(const HloInstruction& instr) override;\n+  bool fp8_lt_fallback_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "ab544eec85ef0a75fb1ac0662c9748005a5d09e5",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublas_test.cc",
            "status": "modified",
            "additions": 65,
            "deletions": 5,
            "changes": 70,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4db7e0e6515b0645ae70054551923ccdd586bc01/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4db7e0e6515b0645ae70054551923ccdd586bc01/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas_test.cc?ref=4db7e0e6515b0645ae70054551923ccdd586bc01",
            "patch": "@@ -44,6 +44,10 @@ namespace gpu {\n \n using CublasBackendConfig = AutotuneResult::GemmKey;\n \n+using absl_testing::IsOk;\n+using absl_testing::IsOkAndHolds;\n+using ::testing::IsEmpty;\n+using ::testing::Not;\n using ::tsl::proto_testing::EqualsProto;\n \n const char kCublasCustomCallHlo[] = R\"(\n@@ -68,6 +72,48 @@ const char kCublasCustomCallHlo[] = R\"(\n     ROOT %get-tuple-element = f32[100,100]{1,0} get-tuple-element(%custom-call.1), index=0\n   })\";\n \n+const char kCublasLtCustomCallHlo[] = R\"(\n+  HloModule test, entry_computation_layout={(f8e4m3fn[16,32]{1,0}, f8e5m2[32,16]{1,0}, f32[], f32[])->f32[16,16]{1,0}}\n+\n+  ENTRY %test (x: f8e4m3fn[16,32], y: f8e5m2[32,16], x_scale: f32[], y_scale: f32[]) -> f32[16,16] {\n+    %x = f8e4m3fn[16,32]{1,0} parameter(0)\n+    %y = f8e5m2[32,16]{1,0} parameter(1)\n+    %transpose = f8e5m2[16,32]{1,0} transpose(%y), dimensions={1,0}\n+    %x_scale = f32[] parameter(2)\n+    %y_scale = f32[] parameter(3)\n+    %cublas-gemm.1 = (f32[16,16]{1,0}, s8[33554432]{0}) custom-call(%x, %transpose, %x_scale, %y_scale),\n+    custom_call_target=\"__cublas$lt$matmul$f8\",\n+    backend_config={\n+      \"operation_queue_id\":\"0\",\n+      \"wait_on_operation_queues\":[],\n+      \"gemm_backend_config\":{\n+        \"alpha_real\":1,\n+        \"beta\":0,\n+        \"dot_dimension_numbers\":{\n+          \"lhs_contracting_dimensions\":[\"1\"],\n+          \"rhs_contracting_dimensions\":[\"1\"],\n+          \"lhs_batch_dimensions\":[],\n+          \"rhs_batch_dimensions\":[]\n+        },\n+        \"alpha_imag\":0,\n+        \"precision_config\":{\n+          \"operand_precision\":[\"DEFAULT\",\"DEFAULT\"],\n+          \"algorithm\":\"ALG_UNSET\"\n+        },\n+        \"epilogue\":\"DEFAULT\",\n+        \"lhs_stride\":\"512\",\n+        \"rhs_stride\":\"512\",\n+        \"grad_x\":false,\n+        \"grad_y\":false,\n+        \"damax_output\":false\n+      },\n+      \"force_earliest_schedule\":false,\n+      \"reification_cost\":[],\n+      \"device_type\":\"DEVICE_TYPE_INVALID\"\n+    }\n+    ROOT %get-tuple-element = f32[16,16]{1,0} get-tuple-element(%cublas-gemm.1), index=0\n+})\";\n+\n const char kUnsupportedHlo[] = R\"(\n   HloModule module\n \n@@ -122,8 +168,22 @@ TEST_F(CublasBackendTest, GetSupportedConfigsFromCublasCustomCall) {\n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> configs =\n       backend_.GetSupportedConfigs(\n           (*hlo_module->entry_computation()->root_instruction()->operand(0)));\n-  EXPECT_THAT(configs, absl_testing::IsOk());\n-  EXPECT_GT(configs.value().size(), 0);\n+  EXPECT_THAT(configs, IsOkAndHolds(Not(IsEmpty())));\n+}\n+\n+TEST_F(CublasBackendTest, CublasLtCustomCall) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> hlo_module,\n+                          ParseAndReturnVerifiedModule(kCublasLtCustomCallHlo));\n+  const HloInstruction* instr =\n+      hlo_module->entry_computation()->root_instruction()->operand(0);\n+  CublasBackend backend(stream_executor_, &debug_options_, &compiler_,\n+                        &target_config_, /*fp8_lt_fallback=*/true);\n+  absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> configs =\n+      backend.GetSupportedConfigs(*instr);\n+  EXPECT_THAT(configs, IsOkAndHolds(Not(IsEmpty())));\n+\n+  EXPECT_THAT(backend.GetDefaultConfig(*instr), IsOk());\n+  EXPECT_THAT(backend.Compile(*instr, *configs.value()[0]), IsOk());\n }\n \n TEST_F(CublasBackendTest,\n@@ -133,7 +193,7 @@ TEST_F(CublasBackendTest,\n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> configs =\n       backend_.GetSupportedConfigs(\n           (*hlo_module->entry_computation()->root_instruction()));\n-  EXPECT_THAT(configs, absl_testing::IsOkAndHolds(testing::SizeIs(0)));\n+  EXPECT_THAT(configs, IsOkAndHolds(testing::SizeIs(0)));\n }\n \n TEST_F(CublasBackendTest, GetDefaultConfigFromCublasCustomCall) {\n@@ -162,7 +222,7 @@ TEST_F(CublasBackendTest, ApplyConfig) {\n                                     any));\n   EXPECT_THAT(RunFileCheck(hlo_module->ToString(),\n                            \"CHECK: \\\"selected_algorithm\\\":\\\"2\\\"\"),\n-              absl_testing::IsOkAndHolds(true));\n+              IsOkAndHolds(true));\n }\n \n TEST_F(CublasBackendTest, Compile) {\n@@ -174,7 +234,7 @@ TEST_F(CublasBackendTest, Compile) {\n           *(module->entry_computation()->root_instruction()->operand(0))));\n   absl::StatusOr<std::unique_ptr<Executable>> executable = backend_.Compile(\n       *(module->entry_computation()->root_instruction()), *config);\n-  EXPECT_THAT(executable, absl_testing::IsOk());\n+  EXPECT_THAT(executable, IsOk());\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "f6ec44d6aaf7557bafd9db0f53aa4df6d774bd52",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_backend_test.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4db7e0e6515b0645ae70054551923ccdd586bc01/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4db7e0e6515b0645ae70054551923ccdd586bc01/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend_test.cc?ref=4db7e0e6515b0645ae70054551923ccdd586bc01",
            "patch": "@@ -74,6 +74,21 @@ const char kTritonFusionHlo[] = R\"(\n       backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\"}}\n   })\";\n \n+const char kF8TritonFusionHlo[] = R\"(\n+HloModule o\n+\n+gemm_fusion {\n+  p0 = f8e4m3fn[64,6144]{1,0} parameter(0)\n+  p1 = f8e4m3fn[64,6144]{1,0} parameter(1)\n+  ROOT %dot.0 = f32[64,64]{1,0} dot(p0, p1), lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+}\n+\n+ENTRY main {\n+  p0 = f8e4m3fn[64,6144]{1,0} parameter(0)\n+  p1 = f8e4m3fn[64,6144]{1,0} parameter(1)\n+  ROOT %dot.0 = f32[64,64]{1,0} fusion(p0, p1), kind=kCustom, calls=gemm_fusion, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"fusion_backend_config\":{\"kind\":\"__triton_gemm\"},\"force_earliest_schedule\":false}\n+})\";\n+\n const char kUnsupportedFusionHlo[] = R\"(\n   HloModule module\n   computation {\n@@ -144,6 +159,15 @@ class FissionTest : public HloHardwareIndependentTestBase,\n                                            compiler, target_config);\n   }\n \n+  // Static helper to create a CublasBackend.\n+  static std::unique_ptr<GpuCodegenBackend> CreateCublasBackendWiithF8Fallback(\n+      se::StreamExecutor* stream_executor, const DebugOptions* debug_options,\n+      Compiler* compiler, const Compiler::GpuTargetConfig* target_config) {\n+    return std::make_unique<CublasBackend>(stream_executor, debug_options,\n+                                           compiler, target_config,\n+                                           /*enable_f8_fallback=*/true);\n+  }\n+\n   // Static helper to create a CustomKernelBackend.\n   static std::unique_ptr<GpuCodegenBackend> CreateCustomKernelBackend(\n       se::StreamExecutor* stream_executor, const DebugOptions* debug_options,\n@@ -245,6 +269,14 @@ INSTANTIATE_TEST_SUITE_P(\n          {\"custom_call_target=\\\"__cublas$gemm\\\"\",\n           \"\\\"selected_algorithm\\\":\\\"-1\\\"\"},\n          /*expected_backend_name=*/\"Cublas_fission\"},\n+        {\"TritonFusion_CublasLt_F8\",\n+         kF8TritonFusionHlo,\n+         &FissionTest::GetCublasRewriterPipeline,\n+         &FissionTest::CreateCublasBackendWiithF8Fallback,\n+         /*expected_module_substrings=*/\n+         {\"custom_call_target=\\\"__cublas$lt$matmul$f8\\\"\",\n+          \"\\\"selected_algorithm\\\":\\\"0\\\"\"},\n+         /*expected_backend_name=*/\"Cublas_fission\"},\n         {\"TritonFusion_CustomKernel\",\n          kTritonFusionHlo,\n          &FissionTest::GetCustomKernelRewriterPipeline,"
        }
    ],
    "stats": {
        "total": 143,
        "additions": 125,
        "deletions": 18
    }
}