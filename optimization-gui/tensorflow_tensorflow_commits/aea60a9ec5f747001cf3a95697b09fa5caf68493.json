{
    "author": "vwbaker",
    "message": "Modify BlockLevelEmitter to be run before any triton config is found & use the CostModel to generate a default config\n\nThis will allow us to autotune between the \"best\" BlockLevelEmitter config and other backends without destroying compilation time. This will happen before any triton config is ever created so it should check if it's feasible to go through triton at all & then attempt to create a config for it. If an error arises or no config is found, this should be handled gracefully allowing the autotuner to simply not use it as a backend.\n\nPiperOrigin-RevId: 802545126",
    "sha": "aea60a9ec5f747001cf3a95697b09fa5caf68493",
    "files": [
        {
            "sha": "5d5ef0c1ed20c2d494a24f9c595b40676ff3bf48",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea60a9ec5f747001cf3a95697b09fa5caf68493/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea60a9ec5f747001cf3a95697b09fa5caf68493/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=aea60a9ec5f747001cf3a95697b09fa5caf68493",
            "patch": "@@ -45,13 +45,18 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/backends/autotuner:codegen_backend\",\n+        \"//xla/backends/gpu/codegen/triton:support\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service:compiler\",\n+        \"//xla/service:hlo_cost_analysis\",\n+        \"//xla/service:instruction_fusion\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:ir_emission_utils\",\n+        \"//xla/service/gpu/model:fusion_analysis_cache\",\n+        \"//xla/service/gpu/model:gpu_indexing_performance_model\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:stream_executor_h\",\n-        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n@@ -61,6 +66,7 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/types:span\",\n+        \"@llvm-project//mlir:IR\",\n     ],\n )\n \n@@ -86,11 +92,13 @@ xla_test(\n         \"//xla/service:executable\",\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:nvptx_compiler_impl\",\n         \"//xla/stream_executor:device_description_proto_cc\",\n         \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest_main\","
        },
        {
            "sha": "981421f5bb6ce7f9a2ec3c2d470670ccf7b0523a",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter.cc",
            "status": "modified",
            "additions": 53,
            "deletions": 70,
            "changes": 123,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea60a9ec5f747001cf3a95697b09fa5caf68493/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea60a9ec5f747001cf3a95697b09fa5caf68493/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.cc?ref=aea60a9ec5f747001cf3a95697b09fa5caf68493",
            "patch": "@@ -28,14 +28,21 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/types/span.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/backends/gpu/codegen/triton/support.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/model/fusion_analysis_cache.h\"\n+#include \"xla/service/gpu/model/gpu_indexing_performance_model.h\"\n+#include \"xla/service/instruction_fusion.h\"\n #include \"xla/shape.h\"\n-#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/tsl/platform/errors.h\"\n@@ -46,43 +53,6 @@ namespace xla {\n namespace gpu {\n \n namespace {\n-// Computes a tile size for a given dimension `dim` such that:\n-// - It is a power of two,\n-// - It is at least `dim` (i.e., ≥ dim),\n-// - It does not exceed `max_tile_size`.\n-//\n-// - Special cases:\n-//     - If dim is a power of two ≤ max_tile_size, it returns dim.\n-//     - If dim is not a power of two, it returns the next power of two ≥ dim,\n-//       capped at max_tile_size.\n-//     - If dim <= 1, it returns dim.\n-//     - If dim >= max_tile_size, it returns max_tile_size.\n-//\n-// Parameters:\n-// - dim: the size of the dimension to tile (must be ≥ 0).\n-// - max_tile_size: the maximum allowed tile size (must be ≥ 1).\n-//\n-// Returns:\n-// - If dim <= 1: returns dim.\n-// - If dim >= max_tile_size: returns max_tile_size.\n-// - Otherwise: returns the smallest power of two ≥ dim, but ≤ max_tile_size.\n-//\n-// Examples:\n-//   GetTileSize(1, 16)   => 1\n-//   GetTileSize(7, 16)   => 8\n-//   GetTileSize(8, 16)   => 8\n-//   GetTileSize(16, 16)  => 16\n-//   GetTileSize(20, 16)  => 16\n-constexpr int64_t GetTileSize(int64_t dim, int max_tile_size) {\n-  if (dim <= 1) {\n-    return dim;\n-  }\n-  if (dim >= max_tile_size) {\n-    return max_tile_size;\n-  }\n-  return 1LL << static_cast<int64_t>(std::ceil(std::log2(dim)));\n-}\n-\n // Helper: resets all variable dimensions after 'index' to zero\n void ResetTrailingDimensions(const std::vector<int64_t>& input,\n                              std::vector<int64_t>& current, int64_t index) {\n@@ -243,13 +213,14 @@ void ExtendConfigsWithTma(\n absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n BlockLevelEmitterBackend::GetSupportedConfigs(const HloInstruction& instr) {\n   // When use_default_config_ is true, we only return a single config for the\n-  // autotuner to use. It is expected that the default config exists already\n-  // in the HLO fusion and therefore fails if a default config cannot be\n-  // constructed.\n+  // autotuner to use. This is useful to autotune against other backends.\n   if (use_default_config_) {\n-    TF_ASSIGN_OR_RETURN(auto config, GetDefaultConfig(instr));\n+    auto config = GetDefaultConfig(instr);\n+    if (!config.ok()) {\n+      return std::vector<std::unique_ptr<BackendConfig>>();\n+    }\n     std::vector<std::unique_ptr<BackendConfig>> configs;\n-    configs.push_back(std::move(config));\n+    configs.push_back(std::move(config.value()));\n     return configs;\n   }\n \n@@ -328,6 +299,34 @@ BlockLevelEmitterBackend::GetSupportedConfigs(const HloInstruction& instr) {\n   return configs;\n }\n \n+absl::StatusOr<BlockLevelFusionConfig>\n+BlockLevelEmitterBackend::GetCostModelConfig(\n+    const HloInstruction& instr) const {\n+  auto device_info = target_config().device_description;\n+  HloFusionAnalysisCache fusion_analysis_cache(device_info);\n+  mlir::MLIRContext ctx;\n+  GpuPerformanceModelWithIndexingAnalysis indexing_performance_model(\n+      &device_info, &fusion_analysis_cache, shape_size_fn_, &ctx);\n+\n+  auto fusion_adaptor =\n+      HloFusionAdaptor::ForInstruction(Cast<HloFusionInstruction>(&instr));\n+\n+  TF_ASSIGN_OR_RETURN(\n+      TiledRunTimeDataOrError tiled_runtime_data_or_error,\n+      indexing_performance_model.TryFindBestTilingForFusion(*fusion_adaptor));\n+\n+  if (const auto* fusion_decision =\n+          std::get_if<FusionDecision>(&tiled_runtime_data_or_error)) {\n+    return absl::InvalidArgumentError(absl::StrCat(\n+        \"Can't rewrite fusion \", instr.ToString(),\n+        \" because tiling search failed: \", fusion_decision->Explain()));\n+  }\n+  TiledRunTimeData tiled_runtime_data =\n+      std::get<TiledRunTimeData>(std::move(tiled_runtime_data_or_error));\n+\n+  return tiled_runtime_data.block_level_parameters.ToBlockLevelFusionConfig();\n+}\n+\n absl::StatusOr<std::unique_ptr<BackendConfig>>\n BlockLevelEmitterBackend::GetDefaultConfig(const HloInstruction& instr) {\n   if (!IsSupported(instr)) {\n@@ -355,33 +354,16 @@ BlockLevelEmitterBackend::GetDefaultConfig(const HloInstruction& instr) {\n       }\n     }\n   }\n-  // No explicit config found - construct a default one.\n-  BlockLevelFusionConfig config;\n-  // Flatten the output shape(s) of the instruction.\n-  const auto shapes = FlatListOfShapes(instr);\n-  for (const absl::Span<const int64_t> shape : shapes) {\n-    Tile* output_tile = config.add_output_tiles();\n-    for (const int64_t dim : shape) {\n-      // Choose a tile size as the nearest power-of-two <= `dim`, capped at 16.\n-      output_tile->add_sizes(GetTileSize(dim, /*max_tile_size=*/16));\n-    }\n-  }\n-  // Set default kernel execution parameters.\n-  config.set_num_warps(1);           // Number of warps per block.\n-  config.set_num_ctas(1);            // Number of thread blocks (CTAs).\n-  config.set_num_stages(1);          // Number of pipeline stages.\n-  config.set_is_tma_allowed(false);  // Can codegen attempt to use TMA?\n+\n+  // No explicit config found - create one from the cost model if possible.\n+  TF_ASSIGN_OR_RETURN(BlockLevelFusionConfig config, GetCostModelConfig(instr));\n   auto any = std::make_unique<google::protobuf::Any>();\n   any->PackFrom(config);\n   return any;\n }\n \n absl::Status BlockLevelEmitterBackend::ApplyConfig(\n     HloInstruction& instr, const BackendConfig& config) {\n-  if (!IsSupported(instr)) {\n-    return absl::InvalidArgumentError(\n-        \"BlockLevelEmitterBackend does not support this instruction.\");\n-  }\n   // Object nesting structure:\n   // HloInstruction\n   // └── GpuBackendConfig\n@@ -399,25 +381,26 @@ absl::Status BlockLevelEmitterBackend::ApplyConfig(\n                       instr.backend_config<GpuBackendConfig>());\n   FusionBackendConfig& backend_config =\n       *gpu_backend_config.mutable_fusion_backend_config();\n+  backend_config.set_kind(kTritonFusionKind);\n   // Overwrite the block-level fusion config with the new one provided.\n   *backend_config.mutable_block_level_fusion_config() =\n       block_level_fusion_config;\n   // Re-attach the modified GPU config back to the instruction.\n   TF_RETURN_IF_ERROR(instr.set_backend_config(std::move(gpu_backend_config)));\n+  instr.set_fusion_kind(HloInstruction::FusionKind::kCustom);\n   return absl::OkStatus();\n }\n \n bool BlockLevelEmitterBackend::IsSupported(const HloInstruction& instr) {\n   if (instr.opcode() != HloOpcode::kFusion) {\n     return false;\n   }\n-  auto gpu_config = instr.backend_config<GpuBackendConfig>();\n-  if (!gpu_config.ok()) {\n-    return false;\n-  }\n-  const FusionBackendConfig& backend_config =\n-      gpu_config->fusion_backend_config();\n-  return backend_config.kind() == kTritonFusionKind;\n+  const HloComputation* fusion_computation =\n+      Cast<HloFusionInstruction>(&instr)->fused_instructions_computation();\n+  return IsTritonSupportedComputation(\n+             *fusion_computation,\n+             target_config().device_description.gpu_compute_capability())\n+      .CanFuse();\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "c93e71980cf0d5bd880e014929ea72ae110e7798",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter.h",
            "status": "modified",
            "additions": 13,
            "deletions": 2,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea60a9ec5f747001cf3a95697b09fa5caf68493/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea60a9ec5f747001cf3a95697b09fa5caf68493/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.h?ref=aea60a9ec5f747001cf3a95697b09fa5caf68493",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n #define XLA_BACKENDS_GPU_AUTOTUNER_BLOCK_LEVEL_EMITTER_H_\n \n #include <memory>\n+#include <utility>\n #include <vector>\n \n #include \"absl/base/nullability.h\"\n@@ -26,6 +27,8 @@ limitations under the License.\n #include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/compiler.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/xla.pb.h\"\n \n@@ -42,10 +45,13 @@ class BlockLevelEmitterBackend : public GpuCodegenBackend {\n   explicit BlockLevelEmitterBackend(\n       stream_executor::StreamExecutor* absl_nonnull stream_executor,\n       const DebugOptions* absl_nonnull debug_options,\n-      Compiler* absl_nonnull compiler, bool use_default_config = false)\n+      Compiler* absl_nonnull compiler,\n+      HloCostAnalysis::ShapeSizeFunction shape_size_fn,\n+      bool use_default_config = false)\n       : GpuCodegenBackend(\"BlockLevelEmitter\", stream_executor, debug_options,\n                           compiler),\n-        use_default_config_(use_default_config) {}\n+        use_default_config_(use_default_config),\n+        shape_size_fn_(std::move(shape_size_fn)) {}\n \n   // Returns all supported block-level tiling configurations for the given\n   // instruction.\n@@ -64,12 +70,17 @@ class BlockLevelEmitterBackend : public GpuCodegenBackend {\n   bool IsSupported(const HloInstruction& instr);\n \n  private:\n+  absl::StatusOr<BlockLevelFusionConfig> GetCostModelConfig(\n+      const HloInstruction& instr) const;\n   // If true, the backend will return a single default configuration in\n   // GetSupportedConfigs instead of generating all supported configurations.\n   // This is useful to autotune between different backends without increasing\n   // compile time by too much. It will use the default config, likely already\n   // assigned by the cost model.\n   bool use_default_config_;\n+  // A function which returns the size in bytes of the top-level buffer of a\n+  // shape.\n+  HloCostAnalysis::ShapeSizeFunction shape_size_fn_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "6213a5a23f499b86d628d7802ba780f0da5e2d0c",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter_test.cc",
            "status": "modified",
            "additions": 52,
            "deletions": 134,
            "changes": 186,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea60a9ec5f747001cf3a95697b09fa5caf68493/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea60a9ec5f747001cf3a95697b09fa5caf68493/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc?ref=aea60a9ec5f747001cf3a95697b09fa5caf68493",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/substitute.h\"\n #include \"xla/autotuning.pb.h\"\n@@ -29,6 +30,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/service/executable.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/nvptx_compiler.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/device_description.pb.h\"\n@@ -41,7 +43,6 @@ namespace xla {\n namespace gpu {\n \n using ::tsl::proto_testing::EqualsProto;\n-using ::tsl::testing::IsOk;\n \n // Counts the number of configs with is_tma_allowed set to true.\n int CountTmaAllowed(\n@@ -68,7 +69,8 @@ class TritonBlockLevelFusionEmitterBackendTest\n                      .value()\n                      ->ExecutorForDevice(0)\n                      .value(),\n-                 &debug_options_, &compiler_) {\n+                 &debug_options_, &compiler_,\n+                 compiler_.ShapeSizeBytesFunction()) {\n     // TODO(b/315957220): Remove the experimental flags once TMA is enabled by\n     // default.\n     debug_options_.set_xla_gpu_experimental_enable_triton_tma(true);\n@@ -134,12 +136,10 @@ ENTRY %main {\n // BlockLevelFusionConfig when the backend config does not specify\n // a block_level_fusion_config.\n //\n-// The HLO module contains a fusion instruction with a Triton backend config,\n-// but without the detailed block_level_fusion_config settings. This test\n-// verifies that the backend creates a reasonable default config.\n+// We do not test the exact contents of the config here as this is a call to the\n+// cost model, which has its own tests.\n TEST_F(TritonBlockLevelFusionEmitterBackendTest, GetDefaultConfig_Fallback) {\n-  // Parse an HLO module with a fusion instruction having a Triton backend\n-  // config that lacks an explicit block_level_fusion_config.\n+  // Parse an HLO module with a fusion instruction lacking any backend config.\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n                           ParseAndReturnVerifiedModule(R\"(\n HloModule m\n@@ -150,106 +150,8 @@ HloModule m\n \n ENTRY %main {\n   %p0 = f32[16,1,64]{2,1,0} parameter(0), metadata={op_name=\"a\"}\n-  ROOT %wrapped_transpose = f32[64,1,16]{2,1,0} fusion(%p0), kind=kCustom,\n-  calls=%wrapped_transpose_computation,\n-  metadata={op_name=\"a\"},\n-  backend_config={\"fusion_backend_config\":{\"kind\":\"__triton\"}}\n-}\n-)\"));\n-\n-  // Call GetDefaultConfig on the root instruction (the fusion op).\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::unique_ptr<BackendConfig> config,\n-      backend_.GetDefaultConfig(\n-          *(module->entry_computation()->root_instruction())));\n-  // Verify that the returned config is indeed a BlockLevelFusionConfig.\n-  BlockLevelFusionConfig block_level_fusion_config;\n-  ASSERT_TRUE(config->UnpackTo(&block_level_fusion_config));\n-  // Verify that the default config contains default tiles sizes for the\n-  // dimensions of the input.\n-  EXPECT_THAT(block_level_fusion_config, EqualsProto(R\"pb(\n-                output_tiles { sizes: 16 sizes: 1 sizes: 16 }\n-                num_warps: 1\n-                num_ctas: 1\n-                num_stages: 1\n-              )pb\"));\n-}\n-\n-// Tests that GetDefaultConfig correctly handles shapes containing zero-sized\n-// dimensions.\n-//\n-// The HLO module defines a fusion instruction with an input tensor that has a\n-// zero-sized dimension (dimension size 0). The backend config specifies a\n-// Triton fusion kind but does not include a block-level fusion config. This\n-// test verifies that the default config is generated correctly and handles\n-// zero-sized dimensions by preserving them in the output tile sizes.\n-TEST_F(TritonBlockLevelFusionEmitterBackendTest,\n-       GetDefaultConfig_Fallback_ZeroDim) {\n-  // Parse an HLO module with a fusion instruction that has a zero-sized\n-  // dimension in the input shape.\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(R\"(\n-HloModule m\n-%wrapped_transpose_computation {\n-  %param_0 = f32[5,0,10]{2,1,0} parameter(0)\n-  ROOT %transpose.3.1 = f32[10,0,5]{2,1,0} transpose(%param_0), dimensions={2,1,0}\n-}\n-\n-ENTRY %main {\n-  %p0 = f32[5,0,10]{2,1,0} parameter(0), metadata={op_name=\"a\"}\n-  ROOT %wrapped_transpose = f32[10,0,5]{2,1,0} fusion(%p0), kind=kCustom,\n-  calls=%wrapped_transpose_computation,\n-  metadata={op_name=\"a\"},\n-  backend_config={\"fusion_backend_config\":{\"kind\":\"__triton\"}}\n-}\n-)\"));\n-\n-  // Call GetDefaultConfig on the root instruction (the fusion op).\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::unique_ptr<BackendConfig> config,\n-      backend_.GetDefaultConfig(\n-          *(module->entry_computation()->root_instruction())));\n-  // Verify that the returned config is indeed a BlockLevelFusionConfig.\n-  BlockLevelFusionConfig block_level_fusion_config;\n-  ASSERT_TRUE(config->UnpackTo(&block_level_fusion_config));\n-  // Verify the default output tile sizes:\n-  // - The tile size for the dimension with size 10 is 16\n-  // - The zero-sized dimension remains zero\n-  // - The tile size for the dimension with size 5 is 8.\n-  // Also verify default tuning parameters: 1 warp, 1 CTA, 1 stage.\n-  EXPECT_THAT(block_level_fusion_config, EqualsProto(R\"pb(\n-                output_tiles { sizes: 16 sizes: 0 sizes: 8 }\n-                num_warps: 1\n-                num_ctas: 1\n-                num_stages: 1\n-              )pb\"));\n-}\n-\n-// Tests that GetDefaultConfig correctly generates default block-level fusion\n-// configurations for a fusion instruction that returns a tuple of two array\n-// shapes.\n-TEST_F(TritonBlockLevelFusionEmitterBackendTest,\n-       GetDefaultConfig_Fallback_tuple2) {\n-  // Parse and verify an HLO module with a fusion instruction that returns a\n-  // tuple of two array shapes.\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnVerifiedModule(R\"(\n-HloModule m\n-%wrapped_transpose_computation {\n-  %param_0 = f32[16,64]{1,0} parameter(0)\n-  %param_1 = f32[32,12]{1,0} parameter(1)\n-  %transpose.3.1 = f32[64,16]{1,0} transpose(%param_0), dimensions={1,0}\n-  %transpose.4.1 = f32[12,32]{1,0} transpose(%param_1), dimensions={1,0}\n-  ROOT %tu = (f32[64,16]{1,0}, f32[12,32]{1,0}) tuple(%transpose.3.1, %transpose.4.1)\n-}\n-\n-ENTRY %main {\n-  %p0 = f32[16,64]{1,0} parameter(0), metadata={op_name=\"a\"}\n-  %p1 = f32[32,12]{1,0} parameter(1), metadata={op_name=\"b\"}\n-  ROOT %wrapped_transpose = (f32[64,16]{1,0}, f32[12,32]{1,0}) fusion(%p0, %p1), kind=kCustom,\n-  calls=%wrapped_transpose_computation,\n-  metadata={op_name=\"ab\"},\n-  backend_config={\"fusion_backend_config\":{\"kind\":\"__triton\"}}\n+  ROOT %wrapped_transpose = f32[64,1,16]{2,1,0} fusion(%p0), kind=kInput,\n+  calls=%wrapped_transpose_computation\n }\n )\"));\n \n@@ -261,15 +163,11 @@ ENTRY %main {\n   // Verify that the returned config is indeed a BlockLevelFusionConfig.\n   BlockLevelFusionConfig block_level_fusion_config;\n   ASSERT_TRUE(config->UnpackTo(&block_level_fusion_config));\n-  // Check that the config correctly includes tiling info for both tuple\n-  // elements\n-  EXPECT_THAT(block_level_fusion_config, EqualsProto(R\"pb(\n-                output_tiles { sizes: 16 sizes: 16 }\n-                output_tiles { sizes: 16 sizes: 16 }\n-                num_warps: 1\n-                num_ctas: 1\n-                num_stages: 1\n-              )pb\"));\n+  // Verify the config is reasonable.\n+  EXPECT_GE(block_level_fusion_config.output_tiles_size(), 1);\n+  EXPECT_GE(block_level_fusion_config.num_warps(), 1);\n+  EXPECT_GE(block_level_fusion_config.num_ctas(), 1);\n+  EXPECT_GE(block_level_fusion_config.num_stages(), 1);\n }\n \n // Tests that `GetSupportedConfigs` returns a correct list of valid backend\n@@ -291,10 +189,8 @@ HloModule m\n \n ENTRY %main {\n   %p0 = f32[16,1,64]{2,1,0} parameter(0), metadata={op_name=\"a\"}\n-  ROOT %wrapped_transpose = f32[64,1,16]{2,1,0} fusion(%p0), kind=kCustom,\n-  calls=%wrapped_transpose_computation,\n-  metadata={op_name=\"a\"},\n-  backend_config={\"fusion_backend_config\":{\"kind\":\"__triton\"}}\n+  ROOT %wrapped_transpose = f32[64,1,16]{2,1,0} fusion(%p0), kind=kInput,\n+  calls=%wrapped_transpose_computation\n }\n )\"));\n \n@@ -451,10 +347,8 @@ HloModule m\n \n ENTRY %main {\n   %p0 = f32[16,64]{1,0} parameter(0), metadata={op_name=\"a\"}\n-  ROOT %wrapped_transpose = f32[64,16]{1,0} fusion(%p0), kind=kCustom,\n-  calls=%wrapped_transpose_computation,\n-  metadata={op_name=\"a\"},\n-  backend_config={\"fusion_backend_config\":{\"kind\":\"__triton\"}}\n+  ROOT %wrapped_transpose = f32[64,16]{1,0} fusion(%p0), kind=kInput,\n+  calls=%wrapped_transpose_computation\n }\n )\"));\n \n@@ -469,15 +363,6 @@ ENTRY %main {\n   // Verify that the returned config is indeed a BlockLevelFusionConfig.\n   BlockLevelFusionConfig block_level_fusion_config;\n   ASSERT_TRUE(config->UnpackTo(&block_level_fusion_config));\n-  // Verify the contents of the default config:\n-  // - output_tiles: shape is tiled into [16,16] blocks\n-  // - num_warps, num_ctas, num_stages are all 1 (basic launch setup)\n-  EXPECT_THAT(block_level_fusion_config, EqualsProto(R\"pb(\n-                output_tiles { sizes: 16 sizes: 16 }\n-                num_warps: 1\n-                num_ctas: 1\n-                num_stages: 1\n-              )pb\"));\n \n   // Apply the generated config to the fusion instruction.\n   EXPECT_THAT(backend_.ApplyConfig(*instr, *config), absl_testing::IsOk());\n@@ -487,6 +372,9 @@ ENTRY %main {\n   EXPECT_THAT(\n       gpu_backend_config.fusion_backend_config().block_level_fusion_config(),\n       EqualsProto(block_level_fusion_config));\n+  EXPECT_EQ(gpu_backend_config.fusion_backend_config().kind(),\n+            kTritonFusionKind);\n+  EXPECT_EQ(instr->fusion_kind(), HloInstruction::FusionKind::kCustom);\n }\n \n TEST_F(TritonBlockLevelFusionEmitterBackendTest, Compile) {\n@@ -531,10 +419,40 @@ ENTRY %main {\n   EXPECT_THAT(executable, absl_testing::IsOk());\n }\n \n+TEST_F(TritonBlockLevelFusionEmitterBackendTest,\n+       CompileThroughCostModelConfig) {\n+  // Parse an HLO module without any assigned backend config.\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(R\"(\n+HloModule m\n+%wrapped_transpose_computation {\n+  %param_0 = f32[16,64]{1,0} parameter(0)\n+  ROOT %transpose.3.1 = f32[64,16]{1,0} transpose(%param_0), dimensions={1,0}\n+}\n+\n+ENTRY %main {\n+  %p0 = f32[16,64]{1,0} parameter(0)\n+  ROOT %wrapped_transpose = f32[64,16]{1,0} fusion(%p0), kind=kCustom,\n+  calls=%wrapped_transpose_computation\n+}\n+)\"));\n+  // Call GetDefaultConfig on the root instruction (the fusion op).\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<BackendConfig> config,\n+      backend_.GetDefaultConfig(\n+          *(module->entry_computation()->root_instruction())));\n+  // Attempt to compile the root instruction using the retrieved backend config.\n+  absl::StatusOr<std::unique_ptr<Executable>> executable = backend_.Compile(\n+      *(module->entry_computation()->root_instruction()), *config);\n+  // Verify that compilation succeeded and returned a valid executable.\n+  EXPECT_THAT(executable, absl_testing::IsOk());\n+}\n+\n TEST_F(TritonBlockLevelFusionEmitterBackendTest, UseDefaultConfigFlag) {\n   auto backend = BlockLevelEmitterBackend(\n       PlatformUtil::GetDefaultPlatform().value()->ExecutorForDevice(0).value(),\n-      &debug_options_, &compiler_, /*use_default_config=*/true);\n+      &debug_options_, &compiler_, compiler_.ShapeSizeBytesFunction(),\n+      /*use_default_config=*/true);\n   // Parse an HLO module containing a kCustom Triton fusion with a backend\n   // config that includes block-level tiling parameters.\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,"
        }
    ],
    "stats": {
        "total": 334,
        "additions": 127,
        "deletions": 207
    }
}