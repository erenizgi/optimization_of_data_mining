{
    "author": "beckerhe",
    "message": "Replace `CustomCallThunk::Slice` by `ShapedSlice`\n\nThe two types have the same definition and represent the same semantically.\n\nPiperOrigin-RevId: 819044248",
    "sha": "9793c54120740259a952d77f0a49867eaefaecb2",
    "files": [
        {
            "sha": "0fee21aff7068369818699189f07609f66424fa4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/custom.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9793c54120740259a952d77f0a49867eaefaecb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9793c54120740259a952d77f0a49867eaefaecb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc?ref=9793c54120740259a952d77f0a49867eaefaecb2",
            "patch": "@@ -762,7 +762,7 @@ absl::StatusOr<FusionEmissionResult> EmitCustomCall(\n         \"thunks\");\n   }\n \n-  using Slices = std::vector<std::optional<CustomCallThunk::Slice>>;\n+  using Slices = std::vector<std::optional<ShapedSlice>>;\n \n   int64_t num_args = ShapeUtil::GetLeafCount(custom_call.shape());\n   absl::c_for_each(custom_call.operands(), [&](auto* operand) {\n@@ -830,7 +830,7 @@ absl::StatusOr<FusionEmissionResult> EmitCustomCall(\n               arg_idx++, can_compute_indvar_on_host, while_op, indvar_idx,\n               inlined_module));\n \n-          operands.push_back(CustomCallThunk::Slice{slice, subshape});\n+          operands.push_back(ShapedSlice{slice, subshape});\n           arguments.push_back(slice);\n           return absl::OkStatus();\n         }));\n@@ -858,7 +858,7 @@ absl::StatusOr<FusionEmissionResult> EmitCustomCall(\n             arg_idx++, can_compute_indvar_on_host, while_op, indvar_idx,\n             inlined_module));\n \n-        results.push_back(CustomCallThunk::Slice{slice, subshape});\n+        results.push_back(ShapedSlice{slice, subshape});\n         arguments.push_back(slice);\n         return absl::OkStatus();\n       }));\n@@ -986,8 +986,7 @@ absl::StatusOr<FusionEmissionResult> EmitCustomCall(\n                 fake_allocations[fake_arg_idx].get(), 0, operand_byte_size);\n \n             fake_arg_idx++;\n-            fake_operands.push_back(\n-                CustomCallThunk::Slice{fake_slice, subshape});\n+            fake_operands.push_back(ShapedSlice{fake_slice, subshape});\n             return absl::OkStatus();\n           }));\n     }\n@@ -1012,7 +1011,7 @@ absl::StatusOr<FusionEmissionResult> EmitCustomCall(\n               fake_allocations[fake_arg_idx].get(), 0, result_byte_size);\n \n           fake_arg_idx++;\n-          fake_results.push_back(CustomCallThunk::Slice{fake_slice, subshape});\n+          fake_results.push_back(ShapedSlice{fake_slice, subshape});\n           return absl::OkStatus();\n         }));\n "
        },
        {
            "sha": "96a124fdd04dff680265315d537010ab189976fe",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9793c54120740259a952d77f0a49867eaefaecb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9793c54120740259a952d77f0a49867eaefaecb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc?ref=9793c54120740259a952d77f0a49867eaefaecb2",
            "patch": "@@ -1823,10 +1823,9 @@ absl::StatusOr<const se::CommandBuffer::Command*> CustomCallCmd::Record(\n namespace {\n // Records each buffer associated with each slice into the provided vector.\n // Returns an error if any of the slices is missing a buffer allocation.\n-absl::Status GetBuffers(\n-    const Thunk::ExecuteParams& execute_params,\n-    absl::Span<const std::optional<CustomCallCmd::Slice>> slices,\n-    std::vector<void*>& buffers, absl::string_view label) {\n+absl::Status GetBuffers(const Thunk::ExecuteParams& execute_params,\n+                        absl::Span<const std::optional<ShapedSlice>> slices,\n+                        std::vector<void*>& buffers, absl::string_view label) {\n   for (int i = 0; i < slices.size(); ++i) {\n     if (!slices[i].has_value()) {\n       buffers.push_back(nullptr);\n@@ -1908,7 +1907,7 @@ CustomCallCmd::RecordXlaFfiCall(const Thunk::ExecuteParams& execute_params,\n   arguments.reserve(operands_.size());\n \n   for (int i = 0; i < operands_.size(); ++i) {\n-    const std::optional<Slice>& slice = operands_[i];\n+    const std::optional<ShapedSlice>& slice = operands_[i];\n     if (!slice.has_value()) {\n       arguments.push_back(se::DeviceMemoryBase{});\n       continue;\n@@ -1925,7 +1924,7 @@ CustomCallCmd::RecordXlaFfiCall(const Thunk::ExecuteParams& execute_params,\n   results.reserve(results_.size());\n \n   for (int i = 0; i < results_.size(); ++i) {\n-    const std::optional<Slice>& slice = results_[i];\n+    const std::optional<ShapedSlice>& slice = results_[i];\n     if (!slice.has_value()) {\n       results.push_back(se::DeviceMemoryBase{});\n       continue;\n@@ -1976,7 +1975,7 @@ CustomCallCmd::RecordXlaFfiCall(const Thunk::ExecuteParams& execute_params,\n CommandBufferCmd::BufferUseVector CustomCallCmd::buffers() const {\n   CommandBufferCmd::BufferUseVector buffer_usage;\n   for (auto& slices : {operands_, results_}) {\n-    for (const std::optional<Slice>& slice : slices) {\n+    for (const std::optional<ShapedSlice>& slice : slices) {\n       if (slice.has_value()) {\n         buffer_usage.push_back(BufferUse::Write(slice->slice));\n       }"
        },
        {
            "sha": "6e26633255678af7396b7cc3a64664913edef128",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.h",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9793c54120740259a952d77f0a49867eaefaecb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9793c54120740259a952d77f0a49867eaefaecb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h?ref=9793c54120740259a952d77f0a49867eaefaecb2",
            "patch": "@@ -984,15 +984,14 @@ class CuDnnCmd : public TracedCommandBufferCmd {\n \n class CustomCallCmd : public CommandBufferCmd {\n  public:\n-  using Slice = CustomCallThunk::Slice;\n   using CustomCallTarget = CustomCallThunk::CustomCallTarget;\n   using AttributesMap = CustomCallThunk::AttributesMap;\n \n   // This is a legacy custom call API that is discouraged, and will be\n   // deprecated once XLA:FFI mechanism is ready.\n   CustomCallCmd(std::string target_name, CustomCallTarget call_target,\n-                std::vector<std::optional<Slice>> operands,\n-                std::vector<std::optional<Slice>> results,\n+                std::vector<std::optional<ShapedSlice>> operands,\n+                std::vector<std::optional<ShapedSlice>> results,\n                 absl::string_view opaque)\n       : CommandBufferCmd(CommandBufferCmdType::kCustomCallCmd),\n         target_name_(std::move(target_name)),\n@@ -1002,8 +1001,8 @@ class CustomCallCmd : public CommandBufferCmd {\n         results_(std::move(results)) {}\n \n   CustomCallCmd(std::string target_name, XLA_FFI_Handler* handler,\n-                std::vector<std::optional<Slice>> operands,\n-                std::vector<std::optional<Slice>> results,\n+                std::vector<std::optional<ShapedSlice>> operands,\n+                std::vector<std::optional<ShapedSlice>> results,\n                 ffi::CallFrame call_frame,\n                 const HloComputation* called_computation)\n       : CommandBufferCmd(CommandBufferCmdType::kCustomCallCmd),\n@@ -1055,8 +1054,8 @@ class CustomCallCmd : public CommandBufferCmd {\n \n   const HloComputation* called_computation_;\n \n-  std::vector<std::optional<Slice>> operands_;\n-  std::vector<std::optional<Slice>> results_;\n+  std::vector<std::optional<ShapedSlice>> operands_;\n+  std::vector<std::optional<ShapedSlice>> results_;\n };\n \n //===----------------------------------------------------------------------===//"
        },
        {
            "sha": "141985530a05c4dd7296201689cfde9890009491",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 15,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9793c54120740259a952d77f0a49867eaefaecb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9793c54120740259a952d77f0a49867eaefaecb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc?ref=9793c54120740259a952d77f0a49867eaefaecb2",
            "patch": "@@ -67,8 +67,8 @@ using xla::ffi::CallOptions;\n // memory addresses. This is called once when creating the CustomCall thunk,\n // then the thunk will need to update the addresses at runtime.\n static absl::StatusOr<ffi::CallFrame> BuildCallFramePrototype(\n-    absl::Span<const std::optional<CustomCallThunk::Slice>> operands,\n-    absl::Span<const std::optional<CustomCallThunk::Slice>> results,\n+    absl::Span<const std::optional<ShapedSlice>> operands,\n+    absl::Span<const std::optional<ShapedSlice>> results,\n     CustomCallThunk::AttributesMap attributes) {\n   CallFrameBuilder builder(\n       /*num_args=*/operands.size(),\n@@ -172,8 +172,8 @@ ResolveLegacyCustomCall(const CustomCallTargetRegistry& registry,\n \n absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n     ThunkInfo thunk_info, std::string target_name, CustomCallTarget call_target,\n-    std::vector<std::optional<Slice>> operands,\n-    std::vector<std::optional<Slice>> results, std::string opaque) {\n+    std::vector<std::optional<ShapedSlice>> operands,\n+    std::vector<std::optional<ShapedSlice>> results, std::string opaque) {\n   return absl::WrapUnique(new CustomCallThunk(\n       thunk_info, std::move(target_name), std::move(operands),\n       std::move(results), std::move(opaque), std::move(call_target),\n@@ -182,8 +182,8 @@ absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n \n absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n     ThunkInfo thunk_info, std::string target_name,\n-    std::vector<std::optional<Slice>> operands,\n-    std::vector<std::optional<Slice>> results, std::string opaque,\n+    std::vector<std::optional<ShapedSlice>> operands,\n+    std::vector<std::optional<ShapedSlice>> results, std::string opaque,\n     CustomCallApiVersion api_version, absl::string_view platform_name) {\n   if (api_version == CustomCallApiVersion::API_VERSION_TYPED_FFI) {\n     return absl::InvalidArgumentError(\n@@ -203,8 +203,8 @@ absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n \n absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n     ThunkInfo thunk_info, std::string target_name,\n-    std::vector<std::optional<Slice>> operands,\n-    std::vector<std::optional<Slice>> results, AttributesMap attributes,\n+    std::vector<std::optional<ShapedSlice>> operands,\n+    std::vector<std::optional<ShapedSlice>> results, AttributesMap attributes,\n     const HloComputation* called_computation, absl::string_view platform_name) {\n   TF_ASSIGN_OR_RETURN(ffi::HandlerRegistration registration,\n                       ffi::FindHandler(target_name, platform_name));\n@@ -216,8 +216,9 @@ absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n \n absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n     ThunkInfo thunk_info, std::string target_name,\n-    XLA_FFI_Handler_Bundle bundle, std::vector<std::optional<Slice>> operands,\n-    std::vector<std::optional<Slice>> results, AttributesMap attributes,\n+    XLA_FFI_Handler_Bundle bundle,\n+    std::vector<std::optional<ShapedSlice>> operands,\n+    std::vector<std::optional<ShapedSlice>> results, AttributesMap attributes,\n     const HloComputation* called_computation) {\n   auto execution_state = std::make_unique<ffi::ExecutionState>();\n \n@@ -249,8 +250,8 @@ absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n \n CustomCallThunk::CustomCallThunk(\n     ThunkInfo thunk_info, std::string target_name,\n-    std::vector<std::optional<Slice>> operands,\n-    std::vector<std::optional<Slice>> results, std::string opaque,\n+    std::vector<std::optional<ShapedSlice>> operands,\n+    std::vector<std::optional<ShapedSlice>> results, std::string opaque,\n     CustomCallTarget call_target,\n     const std::optional<CustomCallApiVersion>& api_version)\n     : Thunk(Thunk::kCustomCall, thunk_info),\n@@ -263,8 +264,9 @@ CustomCallThunk::CustomCallThunk(\n \n CustomCallThunk::CustomCallThunk(\n     ThunkInfo thunk_info, std::string target_name,\n-    XLA_FFI_Handler_Bundle bundle, std::vector<std::optional<Slice>> operands,\n-    std::vector<std::optional<Slice>> results, CallFrame call_frame,\n+    XLA_FFI_Handler_Bundle bundle,\n+    std::vector<std::optional<ShapedSlice>> operands,\n+    std::vector<std::optional<ShapedSlice>> results, CallFrame call_frame,\n     AttributesMap attributes,\n     std::unique_ptr<ffi::ExecutionState> execution_state,\n     const HloComputation* called_computation)\n@@ -285,7 +287,7 @@ absl::Status CustomCallThunk::ExecuteCustomCall(const ExecuteParams& params) {\n   std::vector<void*> buffers;\n   buffers.reserve(operands_.size() + results_.size());\n   for (auto& slices : {operands_, results_}) {\n-    for (const std::optional<Slice>& slice : slices) {\n+    for (const std::optional<ShapedSlice>& slice : slices) {\n       if (!slice.has_value()) {\n         buffers.push_back(nullptr);\n         continue;"
        },
        {
            "sha": "9a2c0bfc805ff3d7d9671e5bd7a745a04834d13c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk.h",
            "status": "modified",
            "additions": 21,
            "deletions": 26,
            "changes": 47,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9793c54120740259a952d77f0a49867eaefaecb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9793c54120740259a952d77f0a49867eaefaecb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h?ref=9793c54120740259a952d77f0a49867eaefaecb2",
            "patch": "@@ -34,10 +34,8 @@ limitations under the License.\n #include \"xla/ffi/execution_state.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/runtime/object_pool.h\"\n-#include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/custom_call_status.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n-#include \"xla/shape.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n #include \"xla/stream_executor/stream.h\"\n \n@@ -61,38 +59,32 @@ class CustomCallThunk : public Thunk {\n       std::function<void(stream_executor::Stream*, void**, const char*, size_t,\n                          XlaCustomCallStatus*)>;\n \n-  // We keep buffer allocation slice together with its shape to be able to fill\n-  // FFI arguments with required details.\n-  struct Slice {\n-    BufferAllocation::Slice slice;\n-    Shape shape;\n-  };\n-\n   using Attribute = ffi::CallFrameBuilder::Attribute;\n   using AttributesMap = ffi::CallFrameBuilder::AttributesMap;\n \n   // Creates a serializable custom call thunk. The callback is resolved using\n   // the legacy CustomCall registry. For new code please use XLA FFI instead.\n   static absl::StatusOr<std::unique_ptr<CustomCallThunk>> Create(\n       ThunkInfo thunk_info, std::string target_name,\n-      std::vector<std::optional<Slice>> operands,\n-      std::vector<std::optional<Slice>> results, std::string opaque,\n+      std::vector<std::optional<ShapedSlice>> operands,\n+      std::vector<std::optional<ShapedSlice>> results, std::string opaque,\n       CustomCallApiVersion api_version, absl::string_view platform_name);\n \n   // Creates a custom call thunk from the given legacy custom call target.\n   // Note that a thunk created this way can't be serialized to a proto.\n   // This function is only permitted for unit testing code.\n   static absl::StatusOr<std::unique_ptr<CustomCallThunk>> Create(\n       ThunkInfo thunk_info, std::string target_name,\n-      CustomCallTarget call_target, std::vector<std::optional<Slice>> operands,\n-      std::vector<std::optional<Slice>> results, std::string opaque);\n+      CustomCallTarget call_target,\n+      std::vector<std::optional<ShapedSlice>> operands,\n+      std::vector<std::optional<ShapedSlice>> results, std::string opaque);\n \n   // Creates a serializable custom call thunk. The callback is resolved using\n   // XLA FFI.\n   static absl::StatusOr<std::unique_ptr<CustomCallThunk>> Create(\n       ThunkInfo thunk_info, std::string target_name,\n-      std::vector<std::optional<Slice>> operands,\n-      std::vector<std::optional<Slice>> results, AttributesMap attributes,\n+      std::vector<std::optional<ShapedSlice>> operands,\n+      std::vector<std::optional<ShapedSlice>> results, AttributesMap attributes,\n       const HloComputation* called_computation,\n       absl::string_view platform_name);\n \n@@ -101,8 +93,9 @@ class CustomCallThunk : public Thunk {\n   // handler which matches the given bundle.\n   static absl::StatusOr<std::unique_ptr<CustomCallThunk>> Create(\n       ThunkInfo thunk_info, std::string target_name,\n-      XLA_FFI_Handler_Bundle bundle, std::vector<std::optional<Slice>> operands,\n-      std::vector<std::optional<Slice>> results, AttributesMap attributes,\n+      XLA_FFI_Handler_Bundle bundle,\n+      std::vector<std::optional<ShapedSlice>> operands,\n+      std::vector<std::optional<ShapedSlice>> results, AttributesMap attributes,\n       const HloComputation* called_computation);\n \n   absl::Status Prepare(const PrepareParams& params,\n@@ -117,24 +110,26 @@ class CustomCallThunk : public Thunk {\n     return call_frame_ ? std::make_optional(call_frame_->Copy()) : std::nullopt;\n   }\n \n-  const std::vector<std::optional<Slice>>& operands() const {\n+  const std::vector<std::optional<ShapedSlice>>& operands() const {\n     return operands_;\n   }\n-  const std::vector<std::optional<Slice>>& results() const { return results_; }\n+  const std::vector<std::optional<ShapedSlice>>& results() const {\n+    return results_;\n+  }\n \n   absl::string_view opaque() const { return opaque_; }\n \n  private:\n   CustomCallThunk(ThunkInfo thunk_info, std::string target_name,\n-                  std::vector<std::optional<Slice>> operands,\n-                  std::vector<std::optional<Slice>> results, std::string opaque,\n-                  CustomCallTarget call_target,\n+                  std::vector<std::optional<ShapedSlice>> operands,\n+                  std::vector<std::optional<ShapedSlice>> results,\n+                  std::string opaque, CustomCallTarget call_target,\n                   const std::optional<CustomCallApiVersion>& api_version);\n \n   CustomCallThunk(ThunkInfo thunk_info, std::string target_name,\n                   XLA_FFI_Handler_Bundle bundle,\n-                  std::vector<std::optional<Slice>> operands,\n-                  std::vector<std::optional<Slice>> results,\n+                  std::vector<std::optional<ShapedSlice>> operands,\n+                  std::vector<std::optional<ShapedSlice>> results,\n                   ffi::CallFrame call_frame, AttributesMap attributes,\n                   std::unique_ptr<ffi::ExecutionState> execution_state,\n                   const HloComputation* called_computation);\n@@ -153,8 +148,8 @@ class CustomCallThunk : public Thunk {\n   std::optional<CustomCallApiVersion> api_version_;\n   std::string target_name_;\n \n-  std::vector<std::optional<Slice>> operands_;\n-  std::vector<std::optional<Slice>> results_;\n+  std::vector<std::optional<ShapedSlice>> operands_;\n+  std::vector<std::optional<ShapedSlice>> results_;\n \n   // This is a legacy custom call API that is discouraged, and will be\n   // deprecated once XLA:FFI mechanism is ready."
        },
        {
            "sha": "c39007eb477f2dd9361442bba91aad516be3237d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/dynamic_slice_thunk_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 18,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9793c54120740259a952d77f0a49867eaefaecb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9793c54120740259a952d77f0a49867eaefaecb2/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk_test.cc?ref=9793c54120740259a952d77f0a49867eaefaecb2",
            "patch": "@@ -559,12 +559,10 @@ TEST_F(DynamicSliceThunkTest, SlicedMemcpy) {\n       auto registration,\n       xla::ffi::FindHandler(\"__xla_test$$memcpy\", GetPlatformName()));\n \n-  std::vector<std::optional<CustomCallThunk::Slice>> operands{\n-      CustomCallThunk::Slice{slice_src_fake,\n-                             ShapeUtil::MakeShape(PrimitiveType::S32, {8, 8})}};\n-  std::vector<std::optional<CustomCallThunk::Slice>> results{\n-      CustomCallThunk::Slice{slice_dst,\n-                             ShapeUtil::MakeShape(PrimitiveType::S32, {8, 8})}};\n+  std::vector<std::optional<ShapedSlice>> operands{ShapedSlice{\n+      slice_src_fake, ShapeUtil::MakeShape(PrimitiveType::S32, {8, 8})}};\n+  std::vector<std::optional<ShapedSlice>> results{\n+      ShapedSlice{slice_dst, ShapeUtil::MakeShape(PrimitiveType::S32, {8, 8})}};\n \n   // Creating embedded custom call thunk.\n   ThunkSequence seq;\n@@ -722,12 +720,10 @@ TEST_F(DynamicSliceThunkTest, SlicedOutputMemcpy) {\n       auto registration,\n       xla::ffi::FindHandler(\"__xla_test$$memcpy\", GetPlatformName()));\n \n-  std::vector<std::optional<CustomCallThunk::Slice>> operands{\n-      CustomCallThunk::Slice{slice_src_fake,\n-                             ShapeUtil::MakeShape(PrimitiveType::S32, {2, 2})}};\n-  std::vector<std::optional<CustomCallThunk::Slice>> results{\n-      CustomCallThunk::Slice{slice_dst_fake,\n-                             ShapeUtil::MakeShape(PrimitiveType::S32, {2, 2})}};\n+  std::vector<std::optional<ShapedSlice>> operands{ShapedSlice{\n+      slice_src_fake, ShapeUtil::MakeShape(PrimitiveType::S32, {2, 2})}};\n+  std::vector<std::optional<ShapedSlice>> results{ShapedSlice{\n+      slice_dst_fake, ShapeUtil::MakeShape(PrimitiveType::S32, {2, 2})}};\n \n   // Creating embedded custom call thunk.\n   ThunkSequence seq;\n@@ -1444,12 +1440,10 @@ TEST_F(DynamicSliceThunkTest, SlicedMemcpyOOB) {\n       auto registration,\n       xla::ffi::FindHandler(\"__xla_test$$memcpy\", GetPlatformName()));\n \n-  std::vector<std::optional<CustomCallThunk::Slice>> operands{\n-      CustomCallThunk::Slice{slice_src_fake,\n-                             ShapeUtil::MakeShape(PrimitiveType::S32, {2, 2})}};\n-  std::vector<std::optional<CustomCallThunk::Slice>> results{\n-      CustomCallThunk::Slice{slice_dst_fake,\n-                             ShapeUtil::MakeShape(PrimitiveType::S32, {2, 2})}};\n+  std::vector<std::optional<ShapedSlice>> operands{ShapedSlice{\n+      slice_src_fake, ShapeUtil::MakeShape(PrimitiveType::S32, {2, 2})}};\n+  std::vector<std::optional<ShapedSlice>> results{ShapedSlice{\n+      slice_dst_fake, ShapeUtil::MakeShape(PrimitiveType::S32, {2, 2})}};\n \n   // Creating embedded custom call thunk.\n   ThunkSequence seq;"
        },
        {
            "sha": "b3a3a47394e4e05ce9a478c7b64a1a72eabe20fc",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9793c54120740259a952d77f0a49867eaefaecb2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9793c54120740259a952d77f0a49867eaefaecb2/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=9793c54120740259a952d77f0a49867eaefaecb2",
            "patch": "@@ -1168,7 +1168,7 @@ absl::Status IrEmitterUnnested::EmitCustomCallThunk(\n   bool is_ffi_custom_call =\n       instr->api_version() == CustomCallApiVersion::API_VERSION_TYPED_FFI;\n \n-  using Slices = std::vector<std::optional<CustomCallThunk::Slice>>;\n+  using Slices = std::vector<std::optional<ShapedSlice>>;\n \n   Slices operands;\n   for (auto* operand : instr->operands()) {\n@@ -1183,7 +1183,7 @@ absl::Status IrEmitterUnnested::EmitCustomCallThunk(\n           }\n           TF_ASSIGN_OR_RETURN(auto slice,\n                               GetAllocationSliceForHlo(operand, index));\n-          operands.push_back(CustomCallThunk::Slice{slice, subshape});\n+          operands.push_back(ShapedSlice{slice, subshape});\n           return absl::OkStatus();\n         }));\n   }\n@@ -1199,7 +1199,7 @@ absl::Status IrEmitterUnnested::EmitCustomCallThunk(\n           return absl::OkStatus();\n         }\n         TF_ASSIGN_OR_RETURN(auto slice, GetAllocationSliceForHlo(instr, index));\n-        results.push_back(CustomCallThunk::Slice{slice, subshape});\n+        results.push_back(ShapedSlice{slice, subshape});\n         return absl::OkStatus();\n       }));\n "
        }
    ],
    "stats": {
        "total": 152,
        "additions": 70,
        "deletions": 82
    }
}