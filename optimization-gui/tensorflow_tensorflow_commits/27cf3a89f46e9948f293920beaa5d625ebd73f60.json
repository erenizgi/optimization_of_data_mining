{
    "author": "chsigg",
    "message": "[xla:gpu] Drop `gpu_device_description` from `extract/insert` pass.\n\nParsing protos in cl options is possible but wonky and not needed here.\n\nPiperOrigin-RevId: 803413445",
    "sha": "27cf3a89f46e9948f293920beaa5d625ebd73f60",
    "files": [
        {
            "sha": "5b5966de530beb798e3bedeb534576adb70fb0b0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/27cf3a89f46e9948f293920beaa5d625ebd73f60/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/27cf3a89f46e9948f293920beaa5d625ebd73f60/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=27cf3a89f46e9948f293920beaa5d625ebd73f60",
            "patch": "@@ -2030,7 +2030,8 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n   }\n \n   pm.addPass(mlir::triton::xla::CreateTritonXLAExtractInsertToTritonPass(\n-      device_info, block_level_parameters.is_tma_allowed));\n+      block_level_parameters.is_tma_allowed &&\n+      stream_executor::gpu::IsTmaAvailableForDevice(device_info)));\n \n   // Lower affine expressions into arithmetic ops.\n   pm.addPass(mlir::createLowerAffinePass());"
        },
        {
            "sha": "788f5812481a48b7349623522842c9f20a814b2a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/27cf3a89f46e9948f293920beaa5d625ebd73f60/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/27cf3a89f46e9948f293920beaa5d625ebd73f60/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h?ref=27cf3a89f46e9948f293920beaa5d625ebd73f60",
            "patch": "@@ -31,8 +31,7 @@ namespace mlir::triton::xla {\n \n std::unique_ptr<mlir::Pass> CreateTritonXLAExtractInsertToTritonPass();\n std::unique_ptr<mlir::Pass> CreateTritonXLAExtractInsertToTritonPass(\n-    const stream_executor::DeviceDescription& device_description,\n-    bool tma_enabled);\n+    bool allow_tma);\n std::unique_ptr<mlir::Pass> CreateTritonXLASqueezeDimsPass();\n std::unique_ptr<mlir::Pass> CreateTritonXLAFoldTransposePass();\n std::unique_ptr<mlir::Pass> CreateGeneralizeKernelSignaturePass();"
        },
        {
            "sha": "a9206709deaf6bafb6689cb73e94b2f4a82eac43",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.td",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/27cf3a89f46e9948f293920beaa5d625ebd73f60/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/27cf3a89f46e9948f293920beaa5d625ebd73f60/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td?ref=27cf3a89f46e9948f293920beaa5d625ebd73f60",
            "patch": "@@ -25,6 +25,10 @@ def TritonXLAExtractInsertToTritonPass : Pass<\"triton-xla-extract-insert-to-trit\n     Triton ops. It also rewrites `func` args to `tt.ptr` types and removes\n     function return args.\n   }];\n+  let options = [\n+    Option<\"allow_tma_\", \"allow_tma\", \"bool\", \"false\",\n+           \"Whether to permit lowering to TMA.\">,\n+  ];\n   let dependentDialects = [\n     \"triton::TritonDialect\",\n     \"::xla::XlaDialect\""
        },
        {
            "sha": "9d2649bf63751420252e8330fa47f6a1d178fa5b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_extract_insert_to_triton.mlir",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/27cf3a89f46e9948f293920beaa5d625ebd73f60/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_extract_insert_to_triton.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/27cf3a89f46e9948f293920beaa5d625ebd73f60/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_extract_insert_to_triton.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_extract_insert_to_triton.mlir?ref=27cf3a89f46e9948f293920beaa5d625ebd73f60",
            "patch": "@@ -1,9 +1,9 @@\n // RUN: xla-opt %s -split-input-file \\\n-// RUN: -triton-xla-extract-insert-to-triton=\"gpu_device_info='cuda_compute_capability {major: 6}' tma_enabled=0\" \\\n+// RUN: -triton-xla-extract-insert-to-triton \\\n // RUN: | FileCheck %s\n \n // RUN: xla-opt %s -split-input-file \\\n-// RUN: -triton-xla-extract-insert-to-triton=\"gpu_device_info='cuda_compute_capability {major: 9}' tma_enabled=1\" \\\n+// RUN: -triton-xla-extract-insert-to-triton=allow_tma=1 \\\n // RUN: | FileCheck %s --check-prefix=CHECK-TMA\n \n func.func @lower_extract_insert(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>) {"
        },
        {
            "sha": "95b713354f971541c342ec4794518fb9eee80df8",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_extract_insert_to_triton_pass.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 80,
            "changes": 96,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/27cf3a89f46e9948f293920beaa5d625ebd73f60/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/27cf3a89f46e9948f293920beaa5d625ebd73f60/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc?ref=27cf3a89f46e9948f293920beaa5d625ebd73f60",
            "patch": "@@ -34,7 +34,6 @@ limitations under the License.\n #include \"llvm/ADT/ArrayRef.h\"\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n-#include \"llvm/Support/CommandLine.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n@@ -59,7 +58,6 @@ limitations under the License.\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/codegen/emitters/ir/xla_ops.h\"\n #include \"xla/permutation_util.h\"\n-#include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n \n namespace mlir::triton::xla {\n@@ -150,15 +148,12 @@ bool IsOffsetDivisibilityGuaranteed(mlir::Value offset_val,\n //      minor tile dimension (in bytes) must be divisible by 16, it is\n //      sufficient to check that the offset in the minor dimension (in bytes) is\n //      divisible by 16.\n-bool CanUseTma(bool tma_enabled,\n-               const stream_executor::DeviceDescription& device_description,\n-               const ArrayRef<int64_t>& original_shape,\n+bool CanUseTma(bool allow_tma, const ArrayRef<int64_t>& original_shape,\n                const ArrayRef<int64_t>& tile_shape,\n                const ArrayRef<int64_t>& tile_strides, ValueRange offsets,\n                const TypedValue<PointerType>& pointer,\n                const ArrayRef<int64_t>& minor_to_major_layout) {\n-  if (!tma_enabled ||\n-      !stream_executor::gpu::IsTmaAvailableForDevice(device_description)) {\n+  if (!allow_tma) {\n     return false;\n   }\n \n@@ -500,12 +495,8 @@ static std::pair<Value, Value> CreateTensorOfPointersAndMask(\n \n class RewriteExtract : public mlir::OpRewritePattern<ExtractOp> {\n  public:\n-  RewriteExtract(mlir::MLIRContext* context,\n-                 const stream_executor::DeviceDescription* device_description,\n-                 bool tma_enabled)\n-      : OpRewritePattern(context),\n-        device_description_(device_description),\n-        tma_enabled_(tma_enabled) {}\n+  RewriteExtract(mlir::MLIRContext* context, bool allow_tma)\n+      : OpRewritePattern(context), allow_tma_(allow_tma) {}\n   using OpRewritePattern::OpRewritePattern;\n \n  private:\n@@ -532,8 +523,8 @@ class RewriteExtract : public mlir::OpRewritePattern<ExtractOp> {\n     auto sizes = op.getStaticSizes();\n     auto strides = to_vector(op.getStaticStrides());\n \n-    if (CanUseTma(tma_enabled_, *device_description_, src_shape, sizes, strides,\n-                  offsets, op.getSrc(), src_layout)) {\n+    if (CanUseTma(allow_tma_, src_shape, sizes, strides, offsets, op.getSrc(),\n+                  src_layout)) {\n       if (auto result = CanonicalizeTileStrides(strides, sizes, src_shape);\n           !result.ok()) {\n         return rewriter.notifyMatchFailure(op, result.message());\n@@ -594,18 +585,13 @@ class RewriteExtract : public mlir::OpRewritePattern<ExtractOp> {\n     return mlir::success();\n   }\n \n-  const stream_executor::DeviceDescription* device_description_;\n-  const bool tma_enabled_;\n+  const bool allow_tma_;\n };\n \n class RewriteInsert : public mlir::OpRewritePattern<InsertOp> {\n  public:\n-  RewriteInsert(mlir::MLIRContext* context,\n-                const stream_executor::DeviceDescription* device_description,\n-                bool tma_enabled)\n-      : OpRewritePattern(context),\n-        device_description_(device_description),\n-        tma_enabled_(tma_enabled) {}\n+  RewriteInsert(mlir::MLIRContext* context, bool allow_tma)\n+      : OpRewritePattern(context), allow_tma_(allow_tma) {}\n   using OpRewritePattern::OpRewritePattern;\n \n  private:\n@@ -640,8 +626,8 @@ class RewriteInsert : public mlir::OpRewritePattern<InsertOp> {\n     SmallVector<unsigned> reduced_dims = to_vector(*reduction_mask);\n     absl::c_sort(reduced_dims);\n \n-    if (CanUseTma(tma_enabled_, *device_description_, dst_shape, sizes, strides,\n-                  offsets, op.getDst(), dst_layout)) {\n+    if (CanUseTma(allow_tma_, dst_shape, sizes, strides, offsets, op.getDst(),\n+                  dst_layout)) {\n       if (auto result = CanonicalizeTileStrides(strides, sizes, dst_shape);\n           !result.ok()) {\n         return rewriter.notifyMatchFailure(op, result.message());\n@@ -685,8 +671,7 @@ class RewriteInsert : public mlir::OpRewritePattern<InsertOp> {\n     return mlir::success();\n   }\n \n-  const stream_executor::DeviceDescription* device_description_;\n-  const bool tma_enabled_;\n+  const bool allow_tma_;\n };\n \n // Rewriting tensor::InsertOp as tt.store.\n@@ -737,58 +722,18 @@ class RewriteScalarExtract : public mlir::OpRewritePattern<tensor::ExtractOp> {\n   }\n };\n \n-class DeviceDescriptionParser\n-    : public llvm::cl::parser<stream_executor::DeviceDescription> {\n- public:\n-  using parser::parser;\n-\n-  bool parse(llvm::cl::Option& option, StringRef arg_name, StringRef arg_value,\n-             stream_executor::DeviceDescription& value) {\n-    if (arg_value.empty()) {\n-      value = stream_executor::DeviceDescription();\n-      return false;\n-    }\n-    stream_executor::GpuDeviceInfoProto proto;\n-    if (!tsl::protobuf::TextFormat::ParseFromString(arg_value.str(), &proto)) {\n-      return option.error(\"failed to parse GpuDeviceInfoProto from string: \" +\n-                          arg_value);\n-    }\n-    absl::StatusOr<stream_executor::DeviceDescription> device_description =\n-        stream_executor::DeviceDescription::FromProto(proto);\n-    if (!device_description.ok()) {\n-      return option.error(device_description.status().message());\n-    }\n-    value = *device_description;\n-    return false;\n-  }\n-\n-  static void print(raw_ostream& os,\n-                    const stream_executor::DeviceDescription& value) {\n-    os << value.ToString();\n-  }\n-};\n-\n class TritonXLAExtractInsertToTritonPass\n     : public impl::TritonXLAExtractInsertToTritonPassBase<\n           TritonXLAExtractInsertToTritonPass> {\n  public:\n   using Base::Base;\n-  TritonXLAExtractInsertToTritonPass(\n-      const TritonXLAExtractInsertToTritonPass& other)\n-      : Base(other) {}\n-  explicit TritonXLAExtractInsertToTritonPass(\n-      const stream_executor::DeviceDescription& device_description,\n-      bool tma_enabled) {\n-    device_description_ = device_description;\n-    tma_enabled_ = tma_enabled;\n-  }\n \n  private:\n   void runOnOperation() override {\n     mlir::MLIRContext* mlir_context = &getContext();\n     mlir::RewritePatternSet patterns(mlir_context);\n-    patterns.add<RewriteExtract, RewriteInsert>(\n-        mlir_context, &device_description_.getValue(), tma_enabled_.getValue());\n+    patterns.add<RewriteExtract, RewriteInsert>(mlir_context,\n+                                                allow_tma_.getValue());\n     patterns.add<RewriteScalarExtract, RewriteScalarInsert>(mlir_context);\n     if (mlir::failed(\n             mlir::applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n@@ -802,14 +747,6 @@ class TritonXLAExtractInsertToTritonPass\n       return signalPassFailure();\n     }\n   }\n-\n-  Option<stream_executor::DeviceDescription, DeviceDescriptionParser>\n-      device_description_{\n-          *this, \"gpu_device_info\",\n-          ::llvm::cl::desc(\"Serialized stream_executor::GPUDeviceInfo proto\")};\n-  Option<bool> tma_enabled_{*this, \"tma_enabled\",\n-                            ::llvm::cl::desc(\"Flag to enable/disable TMA\"),\n-                            ::llvm::cl::init(false)};\n };\n \n }  // namespace\n@@ -819,10 +756,9 @@ std::unique_ptr<mlir::Pass> CreateTritonXLAExtractInsertToTritonPass() {\n }\n \n std::unique_ptr<mlir::Pass> CreateTritonXLAExtractInsertToTritonPass(\n-    const stream_executor::DeviceDescription& device_description,\n-    bool tma_enabled) {\n+    bool allow_tma) {\n   return std::make_unique<TritonXLAExtractInsertToTritonPass>(\n-      device_description, tma_enabled);\n+      TritonXLAExtractInsertToTritonPassOptions{allow_tma});\n }\n \n }  // namespace mlir::triton::xla"
        },
        {
            "sha": "99cbdb4268dfe425600b7d5edf92d3029a6b274f",
            "filename": "third_party/xla/xla/stream_executor/gpu/tma_metadata.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/27cf3a89f46e9948f293920beaa5d625ebd73f60/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Ftma_metadata.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/27cf3a89f46e9948f293920beaa5d625ebd73f60/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Ftma_metadata.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Ftma_metadata.cc?ref=27cf3a89f46e9948f293920beaa5d625ebd73f60",
            "patch": "@@ -462,9 +462,11 @@ absl::StatusOr<TmaMetadata> TmaMetadata::FromProto(\n \n bool IsTmaAvailableForDevice(\n     const stream_executor::DeviceDescription& device_info) {\n-  bool is_cuda = std::holds_alternative<stream_executor::CudaComputeCapability>(\n-      device_info.gpu_compute_capability());\n-  return is_cuda && device_info.cuda_compute_capability().IsAtLeastHopper();\n+  if (auto* cuda_cc = std::get_if<stream_executor::CudaComputeCapability>(\n+          &device_info.gpu_compute_capability())) {\n+    return cuda_cc->IsAtLeastHopper();\n+  }\n+  return false;\n }\n \n // Limitations of TMA:"
        }
    ],
    "stats": {
        "total": 118,
        "additions": 30,
        "deletions": 88
    }
}