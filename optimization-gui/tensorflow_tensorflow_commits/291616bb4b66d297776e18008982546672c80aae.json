{
    "author": "WillFroom",
    "message": "[XLA:GPU][XTile] Finalize removal of ScalarOrTensor.\n\nThe (almost*) final commit to remove the special casing for 0D tensors so that the triton specific requirement is moved to the lowering stage, e.g. the upcoming XTile::CPU backend doesn't have such a requirement, and consistently using tensors makes it simpler.\n\n*The almost is because we still have to handle the special case for dot but this will soon be migrated to stable hlo.\n\nPiperOrigin-RevId: 828935216",
    "sha": "291616bb4b66d297776e18008982546672c80aae",
    "files": [
        {
            "sha": "2b235826860acbb80661ffb9f8aa490609d19bc5",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/291616bb4b66d297776e18008982546672c80aae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/291616bb4b66d297776e18008982546672c80aae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=291616bb4b66d297776e18008982546672c80aae",
            "patch": "@@ -83,11 +83,6 @@ namespace mh = ::mlir::mhlo;\n namespace mm = ::mlir::math;\n namespace mt = ::mlir::triton;\n \n-ScalarOrTensor::ScalarOrTensor(mlir::Value value) : value_(value) {\n-  CHECK(IsScalar() || UnwrapTensor().getType().getRank() > 0)\n-      << \"0D tensors are not supported by Triton\";\n-}\n-\n SmallVector<int64_t> GetPaddedTileSizes(ArrayRef<int64_t> tile_sizes) {\n   SmallVector<int64_t> result;\n   result.reserve(tile_sizes.size());"
        },
        {
            "sha": "0ef79b2370fb2fc455e5cccf6bda441eec9da78e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.h",
            "status": "modified",
            "additions": 0,
            "deletions": 39,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/291616bb4b66d297776e18008982546672c80aae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/291616bb4b66d297776e18008982546672c80aae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h?ref=291616bb4b66d297776e18008982546672c80aae",
            "patch": "@@ -63,45 +63,6 @@ std::string MlirToString(T&& value) {\n   return result;\n }\n \n-// This is a wrapper around mlir::Value that can hold either a scalar or a\n-// non-0D tensor. An attempt to use this class with 0D tensors will CHECK-fail\n-// because 0D tensors are not supported by Triton.\n-class ScalarOrTensor {\n-  using TensorValue = mlir::TypedValue<mlir::RankedTensorType>;\n-\n- public:\n-  ScalarOrTensor() = default;\n-\n-  // Wraps the given value in a ScalarOrTensor. CHECK-fails if the\n-  // value is a 0D tensor, because Triton does not support 0D tensors.\n-  explicit ScalarOrTensor(mlir::Value value);\n-\n-  bool IsScalar() const { return !IsTensor(); }\n-  bool IsTensor() const { return mlir::isa<TensorValue>(value_); }\n-\n-  mlir::Value UnwrapScalar() const {\n-    CHECK(IsScalar());\n-    return value_;\n-  }\n-\n-  TensorValue UnwrapTensor() const {\n-    CHECK(IsTensor());\n-    return mlir::cast<TensorValue>(value_);\n-  }\n-\n-  // Returns the underlying value regardless of whether it is a scalar or a\n-  // tensor. Only call this method in contexts where the consumer of the result\n-  // both needs to use an `mlir::Value` and functions identically for scalars\n-  // and tensors. In other cases, prefer to use the `UnwrapScalar` or\n-  // `UnwrapTensor` methods.\n-  mlir::Value UnwrapUnsafe() const { return value_; }\n-\n-  mlir::Type getType() const { return value_.getType(); }\n-\n- private:\n-  mlir::Value value_;\n-};\n-\n // Triton requires that all block dimensions are a power of 2.\n // TODO(b/353484968): Delete this function once we have constraints to only\n // propagate tile sizes that are a power of 2."
        },
        {
            "sha": "1922c9831afe1c97513647fd168d729e3abb4ad6",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 35,
            "deletions": 75,
            "changes": 110,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/291616bb4b66d297776e18008982546672c80aae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/291616bb4b66d297776e18008982546672c80aae/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=291616bb4b66d297776e18008982546672c80aae",
            "patch": "@@ -178,26 +178,12 @@ using ::xla::gpu::triton::CreateConst;\n using ::xla::gpu::triton::EmitConstant;\n using ::xla::gpu::triton::EmitElementwise;\n using ::xla::gpu::triton::GetPaddedTileSizes;\n-using ::xla::gpu::triton::ScalarOrTensor;\n using ::xla::gpu::triton::StorageType;\n using ::xla::gpu::triton::TritonType;\n \n namespace {\n \n using TensorValue = mlir::TypedValue<mlir::RankedTensorType>;\n-// Create either a non-0D tensor or a scalar.\n-// If the passed value is a tensor with rank 0, it is wrapped in a ToScalarOp\n-// to extract the scalar and in all other cases, the value is returned as is.\n-ScalarOrTensor MakeScalarOrTensor(EmitterLocOpBuilder& b, mlir::Value value) {\n-  if (auto tensor_value = mlir::dyn_cast<TensorValue>(value);\n-      tensor_value && tensor_value.getType().getRank() == 0) {\n-    // Triton does not support 0-D tensors so we must extract the scalar value.\n-    // TODO(csigg): This should be handled in the extract/insert rewrite.\n-    return ScalarOrTensor(b.createOrFold<xtile::ToScalarOp>(tensor_value));\n-  }\n-\n-  return ScalarOrTensor(value);\n-}\n \n // Create a tensor from the passed value.\n // If the passed value is a scalar, it is wrapped in a ToTensorOp to create a\n@@ -690,7 +676,7 @@ absl::StatusOr<TensorValue> EmitTiledBitcast(\n                                   normalized_reshape);\n }\n \n-absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n+absl::StatusOr<std::vector<TensorValue>> EmitTiledComputation(\n     EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n     const TiledHloComputation& tiled_computation,\n@@ -970,7 +956,7 @@ absl::StatusOr<TensorValue> EmitDot(\n       const TiledHloFusionInstruction* tiled_fusion_operand =\n           static_cast<const TiledHloFusionInstruction*>(operand);\n       TF_ASSIGN_OR_RETURN(\n-          std::vector<ScalarOrTensor> result,\n+          std::vector<TensorValue> result,\n           EmitTiledComputation(\n               b, device_info,\n               ::xla::Cast<HloFusionInstruction>(tiled_fusion_operand->hlo()),\n@@ -981,7 +967,7 @@ absl::StatusOr<TensorValue> EmitDot(\n             \"Expected nested fusion computation to emit a single value, got \",\n             result.size()));\n       }\n-      dot_args.push_back(result.front().UnwrapTensor());\n+      dot_args.push_back(result.front());\n     }\n     Value acc = for_op.getRegionIterArgs().front();\n     int64_t lhs_contracting_dim_idx =\n@@ -1102,7 +1088,7 @@ absl::StatusOr<TensorValue> EmitScaledDot(\n       const TiledHloFusionInstruction* tiled_fusion_operand =\n           static_cast<const TiledHloFusionInstruction*>(operand);\n       TF_ASSIGN_OR_RETURN(\n-          std::vector<ScalarOrTensor> result,\n+          std::vector<TensorValue> result,\n           EmitTiledComputation(\n               b, device_info,\n               ::xla::Cast<HloFusionInstruction>(tiled_fusion_operand->hlo()),\n@@ -1113,7 +1099,7 @@ absl::StatusOr<TensorValue> EmitScaledDot(\n             \"Expected nested fusion computation to emit a single value, got \",\n             result.size()));\n       }\n-      dot_args.push_back(result.front().UnwrapTensor());\n+      dot_args.push_back(result.front());\n     }\n     Value acc = for_op.getRegionIterArgs().front();\n     int64_t lhs_contracting_dim_idx =\n@@ -1280,14 +1266,14 @@ absl::StatusOr<TensorValue> EmitConcatenate(\n         static_cast<const TiledHloFusionInstruction*>(\n             tiled_concatenate.operand(i));\n     TF_ASSIGN_OR_RETURN(\n-        std::vector<ScalarOrTensor> result,\n+        std::vector<TensorValue> result,\n         EmitTiledComputation(\n             b, device_info,\n             ::xla::Cast<HloFusionInstruction>(tiled_fusion_operand->hlo()),\n             *tiled_fusion_operand->called_computation(), block_level_parameters,\n             fn, pid, values));\n     CHECK_EQ(result.size(), 1);\n-    b.create<mlir::scf::YieldOp>(result.front().UnwrapTensor());\n+    b.create<mlir::scf::YieldOp>(result.front());\n   }\n \n   b.setInsertionPointAfter(if_ops.front());\n@@ -1357,7 +1343,7 @@ absl::StatusOr<TensorValue> EmitPad(\n           .getResult());\n }\n \n-absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n+absl::StatusOr<TensorValue> EmitTiledHloInstruction(\n     EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion, const TiledHloInstruction& tiled_hlo,\n     const BlockLevelParameters& block_level_parameters,\n@@ -1405,60 +1391,46 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n           mlir::cast<TensorValue>(Cast(b, parameter, expected_element_type));\n     }\n \n-    return MakeScalarOrTensor(b, parameter);\n+    return parameter;\n   }\n \n   if (hlo->opcode() == HloOpcode::kConcatenate) {\n-    TF_ASSIGN_OR_RETURN(\n-        TensorValue result,\n-        EmitConcatenate(b, device_info, fusion, tiled_hlo,\n-                        block_level_parameters, fn, pid, values));\n-    return MakeScalarOrTensor(b, result);\n+    return EmitConcatenate(b, device_info, fusion, tiled_hlo,\n+                           block_level_parameters, fn, pid, values);\n   }\n \n   if (hlo->opcode() == HloOpcode::kPad) {\n-    TF_ASSIGN_OR_RETURN(TensorValue result,\n-                        EmitPad(b, device_info, tiled_hlo, values, pid));\n-    return MakeScalarOrTensor(b, result);\n+    return EmitPad(b, device_info, tiled_hlo, values, pid);\n   }\n \n   if (hlo->opcode() == HloOpcode::kDot) {\n-    TF_ASSIGN_OR_RETURN(TensorValue result,\n-                        EmitDot(b, device_info, fusion, tiled_hlo,\n-                                block_level_parameters, fn, pid, values));\n-    return MakeScalarOrTensor(b, result);\n+    return EmitDot(b, device_info, fusion, tiled_hlo, block_level_parameters,\n+                   fn, pid, values);\n   }\n \n   if (hlo->opcode() == HloOpcode::kScaledDot) {\n-    TF_ASSIGN_OR_RETURN(TensorValue result,\n-                        EmitScaledDot(b, device_info, fusion, tiled_hlo,\n-                                      block_level_parameters, fn, pid, values));\n-    return MakeScalarOrTensor(b, result);\n+    return EmitScaledDot(b, device_info, fusion, tiled_hlo,\n+                         block_level_parameters, fn, pid, values);\n   }\n \n   if (hlo->opcode() == HloOpcode::kConstant) {\n     if (ShapeUtil::IsEffectiveScalar(hlo->shape())) {\n-      TF_ASSIGN_OR_RETURN(TensorValue constant, EmitConstant(b, *hlo));\n-      return MakeScalarOrTensor(b, constant);\n+      return EmitConstant(b, *hlo);\n     }\n     return absl::UnimplementedError(\n         absl::StrCat(\"Unsupported non-scalar constant \", hlo->ToString()));\n   }\n \n   if (hlo->opcode() == HloOpcode::kIota) {\n-    TF_ASSIGN_OR_RETURN(TensorValue iota_result,\n-                        EmitTiledIota(b, pid, tiled_hlo));\n-    return MakeScalarOrTensor(b, iota_result);\n+    return EmitTiledIota(b, pid, tiled_hlo);\n   }\n \n   if (hlo->opcode() == HloOpcode::kBroadcast) {\n-    return MakeScalarOrTensor(b, EmitTiledBroadcast(b, tiled_hlo, values));\n+    return EmitTiledBroadcast(b, tiled_hlo, values);\n   }\n \n   if (hlo->opcode() == HloOpcode::kReduce) {\n-    TF_ASSIGN_OR_RETURN(TensorValue reduce_result,\n-                        EmitReduce(b, tiled_hlo, values, device_info));\n-    return MakeScalarOrTensor(b, reduce_result);\n+    return EmitReduce(b, tiled_hlo, values, device_info);\n   }\n \n   if (hlo->IsElementwise()) {\n@@ -1470,42 +1442,36 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n     }\n     TF_ASSIGN_OR_RETURN(Value result,\n                         EmitElementwise(b, device_info, *hlo, operands));\n-    return MakeScalarOrTensor(b, result);\n+    return mlir::cast<TensorValue>(result);\n   }\n \n   if (hlo->opcode() == HloOpcode::kReshape) {\n-    TF_ASSIGN_OR_RETURN(TensorValue reshaped_value,\n-                        EmitTiledReshape(b, tiled_hlo.tile_sizes(),\n-                                         values[tiled_hlo.operand(0)]));\n-    return MakeScalarOrTensor(b, reshaped_value);\n+    return EmitTiledReshape(b, tiled_hlo.tile_sizes(),\n+                            values[tiled_hlo.operand(0)]);\n   }\n \n   if (hlo->opcode() == HloOpcode::kBitcast) {\n-    TF_ASSIGN_OR_RETURN(\n-        TensorValue bitcast_value,\n-        EmitTiledBitcast(b, tiled_hlo, values[tiled_hlo.operand(0)]));\n-    return MakeScalarOrTensor(b, bitcast_value);\n+    return EmitTiledBitcast(b, tiled_hlo, values[tiled_hlo.operand(0)]);\n   }\n \n   if (hlo->opcode() == HloOpcode::kTranspose) {\n     auto transpose =\n         ::xla::Cast<const HloTransposeInstruction>(tiled_hlo.hlo());\n-    return MakeScalarOrTensor(\n-        b, EmitTiledTranspose(b, tiled_hlo.tile_sizes(),\n+    return EmitTiledTranspose(b, tiled_hlo.tile_sizes(),\n                               llvm::to_vector(transpose->dimensions()),\n-                              values[tiled_hlo.operand(0)]));\n+                              values[tiled_hlo.operand(0)]);\n   }\n \n   // Slice is currently supported only as an operation on indices\n   // which is pushed to loads and stores. We don't generate any further code.\n   if (hlo->opcode() == HloOpcode::kSlice) {\n-    return MakeScalarOrTensor(b, values[tiled_hlo.operand(0)]);\n+    return values[tiled_hlo.operand(0)];\n   }\n \n   if (hlo->opcode() == HloOpcode::kDynamicSlice) {\n     // Dynamic slice is implemented as a load and does not require any further\n     // processing.\n-    return MakeScalarOrTensor(b, values[tiled_hlo.operand(0)]);\n+    return values[tiled_hlo.operand(0)];\n   }\n \n   return absl::UnimplementedError(\n@@ -1515,7 +1481,7 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n // Emit a sequence of instructions using compatible tiling with producers\n // ordered before consumers in `tiled_computation`. Returns the results for the\n // roots of `tiled_computation`.\n-absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n+absl::StatusOr<std::vector<TensorValue>> EmitTiledComputation(\n     EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const HloFusionInstruction* fusion,\n     const TiledHloComputation& tiled_computation,\n@@ -1546,18 +1512,16 @@ absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n       continue;\n     }\n     TF_ASSIGN_OR_RETURN(\n-        ScalarOrTensor result,\n+        TensorValue result,\n         EmitTiledHloInstruction(b, device_info, fusion, *tiled_hlo,\n                                 block_level_parameters, fn, pid, values));\n-    TF_RET_CHECK(\n-        values.insert({tiled_hlo, MakeTensor(b, result.UnwrapUnsafe())}).second)\n-        << hlo->ToString();\n+    TF_RET_CHECK(values.insert({tiled_hlo, result}).second) << hlo->ToString();\n     VLOG(8) << \"Emitted \" << hlo->ToString(HloPrintOptions::ShortParsable());\n   }\n-  std::vector<ScalarOrTensor> results;\n+  std::vector<TensorValue> results;\n   results.reserve(tiled_computation.GetRoots().size());\n   for (const auto* root : tiled_computation.GetRoots()) {\n-    results.push_back(MakeScalarOrTensor(b, values[root]));\n+    results.push_back(values[root]);\n   }\n   return std::move(results);\n }\n@@ -1795,18 +1759,14 @@ absl::Status EmitGeneric(mlir::OpBuilder builder,\n     Type result_storage_type = StorageType(result_element_type);\n \n     if (result_element_type != result_storage_type) {\n-      result =\n-          ScalarOrTensor(Cast(b, result.UnwrapUnsafe(), result_storage_type));\n+      result = mlir::cast<TensorValue>(Cast(b, result, result_storage_type));\n     }\n \n-    // TODO(csigg): Handle this in extract/insert rewrite.\n-    mlir::Value input_tensor = MakeTensor(b, result.UnwrapUnsafe());\n-\n     TF_ASSIGN_OR_RETURN(\n         auto tile_info,\n         TileInfo::Construct(b, tile_id, /*runtime_values=*/{}, *root));\n \n-    b.create<xtile::InsertTileOp>(input_tensor, arg, tile_info.offsets(),\n+    b.create<xtile::InsertTileOp>(result, arg, tile_info.offsets(),\n                                   tile_info.padded_tile_sizes(),\n                                   tile_info.tile_strides());\n   }"
        }
    ],
    "stats": {
        "total": 154,
        "additions": 35,
        "deletions": 119
    }
}