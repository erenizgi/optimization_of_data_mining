{
    "author": "sergey-kozub",
    "message": "PR #32366: [XLA:GPU] Support global scale in block scaled dot custom call\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32366\n\nüìù Summary of Changes\nAllows fusion global scale multiplication to the scaled dot kernel.\nThe JAX API will be updated to accept the global scale parameter.\n\nüéØ Justification\nWithout this change, the scaled dot followed by a multiply is run as two kernels,\nwhich degrades the performance, as the intermediary results are written and then\nread from the global memory.\n\nüöÄ Kind of Contribution\n‚ö°Ô∏è Performance Improvement\n\nThe custom calls will soon be replaced by scaled dot fusions, but in the meanwhile\nthis change gives the perf improvement to the existing models.\n\nCopybara import of the project:\n\n--\n62b15487a5030902e3b9f6e0d9a5210ebe80d90e by Sergey Kozub <skozub@nvidia.com>:\n\nSupport global scale in block scaled dot custom call\n\nMerging this change closes #32366\n\nPiperOrigin-RevId: 816767126",
    "sha": "ca808ff740a476b3e1c7a602c55b9a02accac373",
    "files": [
        {
            "sha": "dcb7f2cf55b74c7b2711af1ab0eb26dcb277d464",
            "filename": "third_party/xla/xla/service/gpu/transforms/block_scaling_rewriter.cc",
            "status": "modified",
            "additions": 41,
            "deletions": 11,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ca808ff740a476b3e1c7a602c55b9a02accac373/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ca808ff740a476b3e1c7a602c55b9a02accac373/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter.cc?ref=ca808ff740a476b3e1c7a602c55b9a02accac373",
            "patch": "@@ -392,6 +392,7 @@ absl::StatusOr<std::tuple<XlaOp, XlaOp, int64_t>> BuildCudnnScaledDotInput(\n // Build HLO for cuDNN custom call op.\n absl::StatusOr<XlaOp> BuildCudnnScaledDot(XlaOp lhs_input, XlaOp rhs_input,\n                                           XlaOp lhs_scale, XlaOp rhs_scale,\n+                                          XlaOp global_scale,\n                                           const DotDimensionNumbers& dnums,\n                                           PrimitiveType result_type,\n                                           std::optional<int64_t> block_size) {\n@@ -418,9 +419,13 @@ absl::StatusOr<XlaOp> BuildCudnnScaledDot(XlaOp lhs_input, XlaOp rhs_input,\n \n   // Build custom call to cuDNN.\n   std::string custom_call_target{kCudnnBlockScaledDotCallTarget};\n-  XlaOp custom_call = CustomCall(\n-      &builder, custom_call_target,\n-      {lhs_input_op, rhs_input_op, lhs_scale_op, rhs_scale_op}, output_shape);\n+  std::vector<XlaOp> custom_call_operands{lhs_input_op, rhs_input_op,\n+                                          lhs_scale_op, rhs_scale_op};\n+  if (global_scale.valid()) {\n+    custom_call_operands.push_back(global_scale);\n+  }\n+  XlaOp custom_call = CustomCall(&builder, custom_call_target,\n+                                 custom_call_operands, output_shape);\n   XlaOp result = GetTupleElement(custom_call, 0);\n \n   // Slice the result, if necessary.\n@@ -489,9 +494,9 @@ absl::StatusOr<XlaOp> BuildBlockScaledDotInput(\n absl::StatusOr<XlaOp> BuildBlockScaledDot(\n     XlaBuilder& builder, const HloInstruction* lhs_input,\n     const HloInstruction* rhs_input, const HloInstruction* lhs_scale,\n-    const HloInstruction* rhs_scale, const DotDimensionNumbers& dnums,\n-    PrimitiveType result_type, std::optional<int64_t> block_size,\n-    bool allow_cudnn) {\n+    const HloInstruction* rhs_scale, const HloInstruction* global_scale,\n+    const DotDimensionNumbers& dnums, PrimitiveType result_type,\n+    std::optional<int64_t> block_size, bool allow_cudnn) {\n   // Get dot LHS parameter(s).\n   XlaOp lhs_op = Parameter(&builder, 0, lhs_input->shape(), \"lhs\");\n   XlaOp lhs_scale_op = Parameter(&builder, 2, lhs_scale->shape(), \"lhs_scale\");\n@@ -503,13 +508,20 @@ absl::StatusOr<XlaOp> BuildBlockScaledDot(\n     rhs_scale_op = Parameter(&builder, 3, rhs_scale->shape(), \"rhs_scale\");\n   }\n \n+  // Get global scale parameter, if present.\n+  XlaOp global_scale_op;\n+  if (global_scale != nullptr) {\n+    global_scale_op =\n+        Parameter(&builder, 4, global_scale->shape(), \"global_scale\");\n+  }\n+\n   // Use cuDNN kernel, if possible.\n   if (allow_cudnn && rhs_scale_op.valid() &&\n       IsSupportedByCudnn(\n           GetCudnnMxType(lhs_input->shape(), lhs_scale->shape(), block_size),\n           GetCudnnMxType(rhs_input->shape(), rhs_scale->shape(), block_size))) {\n     return BuildCudnnScaledDot(lhs_op, rhs_op, lhs_scale_op, rhs_scale_op,\n-                               dnums, result_type, block_size);\n+                               global_scale_op, dnums, result_type, block_size);\n   }\n \n   // Build general dot op.\n@@ -521,8 +533,15 @@ absl::StatusOr<XlaOp> BuildBlockScaledDot(\n         rhs_op, BuildBlockScaledDotInput(rhs_op, rhs_scale_op, result_type,\n                                          block_size));\n   }\n-  return DotGeneral(lhs_op, rhs_op, dnums, /*precision_config=*/nullptr,\n-                    /*preferred_element_type=*/result_type);\n+  XlaOp result = DotGeneral(lhs_op, rhs_op, dnums, /*precision_config=*/nullptr,\n+                            /*preferred_element_type=*/result_type);\n+\n+  // Apply global scale, if present.\n+  if (global_scale_op.valid()) {\n+    result = Mul(result, global_scale_op,\n+                 /*broadcast_dimensions=*/{});\n+  }\n+  return result;\n }\n \n // Convert scaled dot custom call to HLO computation.\n@@ -531,7 +550,7 @@ absl::StatusOr<HloInstruction*> ExpandBlockScaledDotCustomCall(\n   PrimitiveType result_type = instruction->shape().element_type();\n \n   // Check operand count.\n-  if (instruction->operand_count() != 3 && instruction->operand_count() != 4) {\n+  if (instruction->operand_count() < 3 || instruction->operand_count() > 5) {\n     return InvalidArgument(\n         \"Incorrect number of operands for block scaled dot op\");\n   }\n@@ -554,6 +573,16 @@ absl::StatusOr<HloInstruction*> ExpandBlockScaledDotCustomCall(\n     return InvalidArgument(\"Incorrect output shape for block scaled dot op\");\n   }\n \n+  // Check global scale shape.\n+  if (instruction->operand_count() == 5) {\n+    const Shape& global_scale_shape = instruction->operand(4)->shape();\n+    if (!ShapeUtil::IsScalar(global_scale_shape) ||\n+        global_scale_shape.element_type() != result_type) {\n+      return InvalidArgument(\n+          \"Global scale shape must be a scalar with the result's type\");\n+    }\n+  }\n+\n   // If an explicit block size is passed in the backend config, use it.\n   // This is needed when the scale tensor is padded, the block size cannot be\n   // implied in this case.\n@@ -571,7 +600,8 @@ absl::StatusOr<HloInstruction*> ExpandBlockScaledDotCustomCall(\n   TF_ASSIGN_OR_RETURN(\n       XlaOp block_scaled_dot,\n       BuildBlockScaledDot(builder, operands[0], operands[1], operands[2],\n-                          operands.size() == 4 ? operands[3] : nullptr, dnums,\n+                          operands.size() >= 4 ? operands[3] : nullptr,\n+                          operands.size() == 5 ? operands[4] : nullptr, dnums,\n                           result_type, block_size, allow_cudnn));\n \n   // Reshape to the expected output shape."
        },
        {
            "sha": "69d116f92fc056ba7cc5f53763347dbb1732ccc8",
            "filename": "third_party/xla/xla/service/gpu/transforms/block_scaling_rewriter_cudnn_test.cc",
            "status": "modified",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ca808ff740a476b3e1c7a602c55b9a02accac373/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter_cudnn_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ca808ff740a476b3e1c7a602c55b9a02accac373/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter_cudnn_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter_cudnn_test.cc?ref=ca808ff740a476b3e1c7a602c55b9a02accac373",
            "patch": "@@ -179,5 +179,46 @@ ENTRY main {\n                             \"CHECK: __cudnn$blockScaledDot\");\n }\n \n+TEST_F(BlockScalingRewriterCudnnTest, Nvfp4_GlobalScale) {\n+  constexpr absl::string_view hlo_string = R\"(\n+HloModule test\n+\n+ENTRY main {\n+  %mult_scalar = f16[] constant(6)\n+  %mult = f16[256,256] broadcast(%mult_scalar), dimensions={}\n+  %p0 = f16[256,256] parameter(0)\n+  %p1 = f16[256,256] parameter(1)\n+  %lhs = f4e2m1fn[256,256] convert(f16[256,256] multiply(%p0, %mult))\n+  %rhs = f4e2m1fn[256,256] convert(f16[256,256] multiply(%p1, %mult))\n+  %p2 = f8e4m3fn[256,16] parameter(2)\n+  %p3 = f8e4m3fn[256,16] parameter(3)\n+  %lhs_scale = f8e4m3fn[256,16] abs(%p2)\n+  %rhs_scale = f8e4m3fn[256,16] abs(%p3)\n+  %global_scale = f32[] parameter(4)\n+  ROOT %result = f32[256,256] custom-call(%lhs, %rhs, %lhs_scale, %rhs_scale, %global_scale),\n+      custom_call_target=\"__op$block_scaled_dot\"\n+})\";\n+\n+  EXPECT_TRUE(RunAndCompare(\n+      hlo_string, ErrorSpec(/*aabs=*/1e-4, /*arel=*/1e-5),\n+      /*reference_preprocessor=*/\n+      [](HloModule* reference_module) {\n+        BlockScalingRewriter pass(/*allow_cudnn=*/false);\n+        EXPECT_THAT(RunHloPass(&pass, reference_module),\n+                    absl_testing::IsOkAndHolds(true));\n+      },\n+      /*test_preprocessor=*/\n+      [](HloModule* test_module) {\n+        BlockScalingRewriter pass(/*allow_cudnn=*/true);\n+        EXPECT_THAT(RunHloPass(&pass, test_module),\n+                    absl_testing::IsOkAndHolds(true));\n+      }));\n+\n+  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(false),\n+                            \"CHECK-NOT: __cudnn$blockScaledDot\");\n+  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(true),\n+                            \"CHECK: __cudnn$blockScaledDot\");\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        },
        {
            "sha": "691188d720ce618258ad5ae8235f52d2a1960109",
            "filename": "third_party/xla/xla/service/gpu/transforms/block_scaling_rewriter_test.cc",
            "status": "modified",
            "additions": 39,
            "deletions": 0,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ca808ff740a476b3e1c7a602c55b9a02accac373/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ca808ff740a476b3e1c7a602c55b9a02accac373/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter_test.cc?ref=ca808ff740a476b3e1c7a602c55b9a02accac373",
            "patch": "@@ -116,6 +116,45 @@ ENTRY main {\n })\");\n }\n \n+TEST_F(BlockScalingRewriterTest, ExpandBlockScaledDotGlobalScale) {\n+  constexpr absl::string_view hlo_string = R\"(\n+HloModule test\n+\n+ENTRY main {\n+  %lhs = f8e4m3fn[4,16,256] parameter(0)\n+  %rhs = f8e4m3fn[4,32,256] parameter(1)\n+  %lhs_scale = f8e5m2[4,16,8] parameter(2)\n+  %rhs_scale = f8e5m2[4,32,8] parameter(3)\n+  %global_scale = f32[] parameter(4)\n+  ROOT %result = f32[4,16,32] custom-call(%lhs, %rhs, %lhs_scale, %rhs_scale, %global_scale),\n+      custom_call_target=\"__op$block_scaled_dot\"\n+})\";\n+\n+  BlockScalingRewriter pass(/*allow_cudnn=*/false);\n+  RunAndFilecheckHloRewrite(hlo_string, std::move(pass), R\"(\n+  CHECK: [[lhs_quant:%.+]] = f8e4m3fn[4,16,256]{2,1,0} parameter(0)\n+  CHECK: [[lhs_quant_cvt:%.+]] = f32[4,16,256]{2,1,0} convert([[lhs_quant]])\n+  CHECK: [[lhs_scale:%.+]] = f8e5m2[4,16,8]{2,1,0} parameter(2)\n+  CHECK: [[lhs_scale_cvt:%.+]] = f32[4,16,8]{2,1,0} convert([[lhs_scale]])\n+  CHECK: [[lhs_scale_bc:%.+]] = f32[4,16,8,32]{3,2,1,0} broadcast([[lhs_scale_cvt]])\n+  CHECK: [[lhs_scale_rs:%.+]] = f32[4,16,256]{2,1,0} reshape([[lhs_scale_bc]])\n+  CHECK: [[lhs:%.+]] = f32[4,16,256]{2,1,0} multiply([[lhs_quant_cvt]], [[lhs_scale_rs]])\n+  CHECK: [[rhs_quant:%.+]] = f8e4m3fn[4,32,256]{2,1,0} parameter(1)\n+  CHECK: [[rhs_quant_cvt:%.+]] = f32[4,32,256]{2,1,0} convert([[rhs_quant]])\n+  CHECK: [[rhs_scale:%.+]] = f8e5m2[4,32,8]{2,1,0} parameter(3)\n+  CHECK: [[rhs_scale_cvt:%.+]] = f32[4,32,8]{2,1,0} convert([[rhs_scale]])\n+  CHECK: [[rhs_scale_bc:%.+]] = f32[4,32,8,32]{3,2,1,0} broadcast([[rhs_scale_cvt]])\n+  CHECK: [[rhs_scale_rs:%.+]] = f32[4,32,256]{2,1,0} reshape([[rhs_scale_bc]])\n+  CHECK: [[rhs:%.+]] = f32[4,32,256]{2,1,0} multiply([[rhs_quant_cvt]], [[rhs_scale_rs]])\n+  CHECK: [[dot:%.+]] = f32[4,16,32]{2,1,0} dot([[lhs]], [[rhs]])\n+  CHECK-SAME: lhs_batch_dims={0}, lhs_contracting_dims={2}\n+  CHECK-SAME: rhs_batch_dims={0}, rhs_contracting_dims={2}\n+  CHECK: [[global_scale:%.+]] = f32[] parameter(4)\n+  CHECK: [[global_scale_bc:%.+]] = f32[4,16,32]{2,1,0} broadcast([[global_scale]]), dimensions={}\n+  CHECK: ROOT {{.+}} = f32[4,16,32]{2,1,0} multiply([[dot]], [[global_scale_bc]])\n+})\");\n+}\n+\n TEST_F(BlockScalingRewriterTest, ExpandBlockScaledDotNonDefaultLayout) {\n   constexpr absl::string_view hlo_string = R\"(\n HloModule test"
        },
        {
            "sha": "e0e1138a33c774e1c1e66ed31de1fc04af3b1c4a",
            "filename": "third_party/xla/xla/service/gpu/transforms/cudnn_custom_call_compiler.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ca808ff740a476b3e1c7a602c55b9a02accac373/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_custom_call_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ca808ff740a476b3e1c7a602c55b9a02accac373/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_custom_call_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcudnn_custom_call_compiler.cc?ref=ca808ff740a476b3e1c7a602c55b9a02accac373",
            "patch": "@@ -447,7 +447,8 @@ absl::StatusOr<se::gpu::CudnnGraph> BuildGraphForCustomCallToBackwardFMHAF8(\n \n absl::StatusOr<se::gpu::CudnnGraph> BuildGraphForCustomCallToBlockScaledDot(\n     se::dnn::DnnSupport &dnn_support, HloCustomCallInstruction *custom_call) {\n-  TF_RET_CHECK(custom_call->operand_count() == 4);\n+  const bool has_global_scale = custom_call->operand_count() == 5;\n+  TF_RET_CHECK(custom_call->operand_count() == 4 || has_global_scale);\n   TF_RET_CHECK(custom_call->shape().tuple_shapes().size() == 2);\n \n   TF_ASSIGN_OR_RETURN(TensorDescriptor lhs_data,\n@@ -486,7 +487,7 @@ absl::StatusOr<se::gpu::CudnnGraph> BuildGraphForCustomCallToBlockScaledDot(\n   TF_ASSIGN_OR_RETURN(se::gpu::CudnnGraph graph,\n                       se::gpu::GetCudnnBlockScaledDotOperationGraph(\n                           dnn_support, lhs_data, lhs_scale, rhs_data, rhs_scale,\n-                          result_type, block_size));\n+                          result_type, block_size, has_global_scale));\n   return graph;\n }\n "
        },
        {
            "sha": "439ac237cf370490a122bb1f8c925b33a0fcbc60",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_dnn.cc",
            "status": "modified",
            "additions": 30,
            "deletions": 10,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ca808ff740a476b3e1c7a602c55b9a02accac373/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ca808ff740a476b3e1c7a602c55b9a02accac373/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc?ref=ca808ff740a476b3e1c7a602c55b9a02accac373",
            "patch": "@@ -3040,12 +3040,12 @@ struct OpDescriptor {\n   int64_t uid;                        // The UID of the op.\n   std::vector<int64_t> operand_uids;  // The UIDs of the operands of the op that\n                                       // are part of the graph.\n-  OpMode mode;                    // The mode describing the op.\n-  TensorKind operand_kind;        // The kind of a second operand.\n-  TensorKind result_kind;         // The kind of the output.\n-  dnn::DataType result_type;      // The type of the output.\n-  bool is_virtual;                // A virtual op has a user within the graph.\n-  int sequence_index;             // The index of the op in the sequence.\n+  OpMode mode;                        // The mode describing the op.\n+  TensorKind operand_kind;            // The kind of a second operand.\n+  TensorKind result_kind;             // The kind of the output.\n+  dnn::DataType result_type;          // The type of the output.\n+  bool is_virtual;     // A virtual op has a user within the graph.\n+  int sequence_index;  // The index of the op in the sequence.\n };\n \n // Class describing the graph of ops to be fused into the cuDNN convolution\n@@ -4597,18 +4597,20 @@ absl::StatusOr<CudnnGraph> GetCudnnBlockScaledDotOperationGraph(\n     const dnn::TensorDescriptor& lhs_scale,\n     const dnn::TensorDescriptor& rhs_data,\n     const dnn::TensorDescriptor& rhs_scale, dnn::DataType result_type,\n-    int block_size) {\n+    int block_size, bool has_global_scale) {\n #if CUDNN_VERSION >= 90700\n   using cudnn_frontend::graph::Block_scale_dequantize_attributes;\n   using cudnn_frontend::graph::Matmul_attributes;\n+  using cudnn_frontend::graph::Pointwise_attributes;\n   using cudnn_frontend::graph::Tensor_attributes;\n \n   VLOG(4) << \"\\n lhs_data: \" << lhs_data.ToString()\n           << \"\\n lhs_scale: \" << lhs_scale.ToString()\n           << \"\\n rhs_data: \" << rhs_data.ToString()\n           << \"\\n rhs_scale: \" << rhs_scale.ToString()\n           << \"\\n result_type: \" << dnn::DataType_Name(result_type)\n-          << \"\\n block_size: \" << block_size;\n+          << \"\\n block_size: \" << block_size\n+          << \"\\n global_scale: \" << has_global_scale;\n \n   cudnn_frontend::graph::Graph graph;\n   auto compute_type = cudnn_frontend::DataType_t::FLOAT;\n@@ -4657,9 +4659,27 @@ absl::StatusOr<CudnnGraph> GetCudnnBlockScaledDotOperationGraph(\n \n   auto matmul_attr = Matmul_attributes().set_compute_data_type(compute_type);\n   auto d_tensor = graph.matmul(a_dq, b_dq, matmul_attr);\n-  d_tensor->set_uid(next_uid());\n   d_tensor->set_data_type(ToCudnnFrontendDataType(result_type));\n-  d_tensor->set_is_virtual(false);\n+  if (!has_global_scale) {\n+    d_tensor->set_uid(next_uid());\n+    d_tensor->set_is_virtual(false);\n+  } else {\n+    std::vector<int64_t> scalar(lhs_data.ndims(), 1);\n+    auto scale_attr = Tensor_attributes()\n+                          .set_uid(next_uid())\n+                          .set_dim(scalar)\n+                          .set_stride(scalar)\n+                          .set_data_type(d_tensor->get_data_type());\n+    auto global_scale = graph.tensor(scale_attr.set_name(\"global_scale\"));\n+\n+    auto mul_attr = Pointwise_attributes()\n+                        .set_mode(cudnn_frontend::PointwiseMode_t::MUL)\n+                        .set_compute_data_type(compute_type);\n+    auto result = graph.pointwise(d_tensor, global_scale, mul_attr);\n+    result->set_data_type(d_tensor->get_data_type());\n+    result->set_uid(next_uid());\n+    result->set_is_virtual(false);\n+  }\n \n   CudnnGraph cudnnGraph(std::move(graph));\n   TF_RETURN_IF_ERROR(cudnnGraph.Prepare("
        },
        {
            "sha": "ec248539be953b14b76135143de78b86a070fb00",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_dnn.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ca808ff740a476b3e1c7a602c55b9a02accac373/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ca808ff740a476b3e1c7a602c55b9a02accac373/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.h?ref=ca808ff740a476b3e1c7a602c55b9a02accac373",
            "patch": "@@ -754,7 +754,7 @@ absl::StatusOr<CudnnGraph> GetCudnnBlockScaledDotOperationGraph(\n     const dnn::TensorDescriptor& lhs_scale,\n     const dnn::TensorDescriptor& rhs_data,\n     const dnn::TensorDescriptor& rhs_scale, dnn::DataType result_type,\n-    int block_size);\n+    int block_size, bool has_global_scale);\n \n }  // namespace gpu\n }  // namespace stream_executor"
        }
    ],
    "stats": {
        "total": 179,
        "additions": 155,
        "deletions": 24
    }
}