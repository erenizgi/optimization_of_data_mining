{
    "author": "SiqiaoWu1993",
    "message": "Internal change only\n\nReverts 7107850218d10650c5f81fe7d2727d845c1cda66\n\nPiperOrigin-RevId: 836340200",
    "sha": "d12d02667492e58516b2b122b7d80a4ba1a2d531",
    "files": [
        {
            "sha": "d6a4b0c3fcbf97ee482b4e4f965b3fa30cb12744",
            "filename": "tensorflow/compiler/mlir/tfrt/tests/sink_in_invariant_ops.mlir",
            "status": "modified",
            "additions": 73,
            "deletions": 0,
            "changes": 73,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d12d02667492e58516b2b122b7d80a4ba1a2d531/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftests%2Fsink_in_invariant_ops.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d12d02667492e58516b2b122b7d80a4ba1a2d531/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftests%2Fsink_in_invariant_ops.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftests%2Fsink_in_invariant_ops.mlir?ref=d12d02667492e58516b2b122b7d80a4ba1a2d531",
            "patch": "@@ -195,6 +195,28 @@ func.func @sink_in_stateful_call(%arg0: tensor<i32> {tf_saved_model.index_path =\n   func.return %2 : tensor<i32>\n }\n \n+// Test VarHandleOp getting sinked when it is used by the called function and returned by the called function.\n+\n+// CHECK: func private @func_use_and_return_varhandle([[arg0:.+]]: tensor<!tf_type.resource<tensor<i32>>>)\n+func.func private @func_use_and_return_varhandle(%arg0: tensor<!tf_type.resource<tensor<i32>>>) -> (tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>) {\n+  // CHECK: tf.VarHandleOp\n+  // CHECK-NEXT: tf.ReadVariableOp\n+  %0 = \"tf.ReadVariableOp\"(%arg0) {device = \"cpu\"} : (tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32>\n+\n+  func.return %0, %arg0 : tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>\n+}\n+\n+// CHECK-LABEL: func @sink_in_stateful_call_varhandle_return\n+func.func @sink_in_stateful_call_varhandle_return(%arg0: tensor<i32> {tf_saved_model.index_path = [\"input\"]}) -> (tensor<i32> {tf_saved_model.index_path = [\"r\"]})\n+  attributes {tf_saved_model.exported_names = [\"test_sink_in_stateful_call_varhandle_return\"]} {\n+  // CHECK: tf.VarHandleOp\n+  %0 = \"tf.VarHandleOp\"() {container = \"\", shared_name = \"x\"} : () -> tensor<!tf_type.resource<tensor<i32>>>\n+  // CHECK: \"tf.StatefulPartitionedCall\"(%0)\n+  %1:2 = \"tf.StatefulPartitionedCall\"(%0) {device = \"/CPU:0\", config = \"\", config_proto = \"\", executor_type = \"\", f = @func_use_and_return_varhandle} : (tensor<!tf_type.resource<tensor<i32>>>) -> (tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>)\n+  %2 = \"tf.AddV2\"(%arg0, %1#0) {device = \"/CPU:0\"} : (tensor<i32>, tensor<i32>) -> tensor<i32>\n+  func.return %2 : tensor<i32>\n+}\n+\n // CHECK-LABEL: func @sink_in_if\n func.func @sink_in_if(%arg0: tensor<i32> {tf_saved_model.index_path = [\"input\"]}) -> (tensor<i32> {tf_saved_model.index_path = [\"r\"]})\n   attributes {tf_saved_model.exported_names = [\"test_sink_in_if\"]} {\n@@ -374,3 +396,54 @@ func.func @nested_sink_in_if(%arg: tensor<i32> {tf_saved_model.index_path = [\"in\n }\n \n }\n+\n+// -----\n+\n+module attributes {tf_saved_model.semantics} {\n+\n+// Test sinks crossing nested tf.While and BatchFunction, while the sinkable ops are only copied at the target.\n+\n+// CHECK-LABEL: func private @batched_function\n+func.func private @batched_function(%arg0: tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32>\n+  attributes {tf._input_shapes = [#tf_type.shape<1x3>, #tf_type.shape<*>], tf.signature.is_stateful} {\n+  // CHECK: tf.VarHandleOp\n+  // CHECK-NEXT: tf.ReadVariableOp\n+  %1 = \"tf.ReadVariableOp\"(%arg0) {device = \"/device:CPU:0\"} : (tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32>\n+  %2 = \"tf.Identity\"(%1) {device = \"/device:CPU:0\"} : (tensor<i32>) -> tensor<i32>\n+  func.return %2 : tensor<i32>\n+}\n+\n+// CHECK-LABEL: func private @while_cond_func\n+func.func private @while_cond_func(\n+    %arg0: tensor<i32>,\n+    %arg1: tensor<i32>,\n+    %arg: tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32> {\n+  // CHECK: [[handle:%.*]] = \"tf.VarHandleOp\"()\n+  // CHECK: \"tf.ReadVariableOp\"([[handle]])\n+  %0 = \"tf.ReadVariableOp\"(%arg) {device = \"cpu\"} : (tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32>\n+  func.return %0 : tensor<i32>\n+}\n+\n+// CHECK-LABEL: func private @while_body_func\n+func.func private @while_body_func(\n+    %arg0: tensor<i32>,\n+    %arg1: tensor<i32>,\n+    %arg2: tensor<!tf_type.resource<tensor<i32>>>) -> (tensor<i32>, tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>) {\n+  // CHECK: \"tf.BatchFunction\"(%arg2)\n+  %0 = \"tf.BatchFunction\"(%arg2) {allowed_batch_sizes = [6], batch_timeout_micros = 100000 : i64, batching_queue = \"\", container = \"\", device = \"/device:CPU:0\", enable_large_batch_splitting = false, f = @batched_function, max_batch_size = 6 : i64, max_enqueued_batches = 10 : i64, num_batch_threads = 1 : i64, operandSegmentSizes = array<i32: 1, 0>, shared_name = \"batch/\"} : (tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32>\n+  func.return %0, %arg0, %arg2 : tensor<i32>, tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>\n+}\n+\n+// CHECK-LABEL: func @nested_sink_in_while_and_batch_functions\n+func.func @nested_sink_in_while_and_batch_functions(%arg: tensor<i32> {tf_saved_model.index_path = [\"input\"]}) -> (tensor<i32> {tf_saved_model.index_path = [\"r\"]})\n+  attributes {tf_saved_model.exported_names = [\"test_sink_in_while_and_batch_functions\"]} {\n+  // CHECK: [[handle:%.*]] = \"tf.VarHandleOp\"()\n+  %handle = \"tf.VarHandleOp\"() {container = \"\", shared_name = \"x\"} : () -> tensor<!tf_type.resource<tensor<i32>>>\n+  // CHECK: [[cond:%.*]] = \"tf.Const\"()\n+  %cond = \"tf.Const\"() {device = \"/CPU:0\", value = dense<0> : tensor<i32>} : () -> tensor<i32>\n+  // CHECK: \"tf.While\"([[cond]], [[cond]], [[handle]])\n+  %x:3 = \"tf.While\"(%cond, %cond, %handle) {body = @while_body_func, cond = @while_cond_func, is_stateless = false, parallel_iterations = 10 : i64, shape_invariant} : (tensor<i32>, tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>) -> (tensor<i32>, tensor<i32>, tensor<!tf_type.resource<tensor<i32>>>)\n+  func.return %x#0 : tensor<i32>\n+}\n+\n+}"
        },
        {
            "sha": "990b3da433c327ea00a0f94d7e012e455c79b354",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/passes.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d12d02667492e58516b2b122b7d80a4ba1a2d531/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fpasses.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d12d02667492e58516b2b122b7d80a4ba1a2d531/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fpasses.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fpasses.cc?ref=d12d02667492e58516b2b122b7d80a4ba1a2d531",
            "patch": "@@ -152,7 +152,6 @@ void CreateTFExecutorToTFPreInvariantOptimizationPipelineHelper(\n   pm.addPass(mlir::createInlinerPass());\n   pm.addNestedPass<mlir::func::FuncOp>(\n       mlir::TF::CreateRemoveUnusedWhileResultsPass());\n-  pm.addPass(mlir::TF::CreateTFRegionControlFlowToFunctional());\n \n   // Apply standard optimization after optimizing control flow ops.\n   pm.addPass(mlir::createInlinerPass());\n@@ -163,6 +162,7 @@ void CreateTFExecutorToTFPreInvariantOptimizationPipelineHelper(\n   // by performing shape inference again after reference variable to resource\n   // variable conversion. We should remove this after b/187876545 is fixed.\n   pm.addPass(mlir::TF::CreateTFShapeInferencePass());\n+  pm.addPass(mlir::TF::CreateTFRegionControlFlowToFunctional());\n \n   pm.addNestedPass<mlir::func::FuncOp>(\n       mlir::TFDevice::CreateLaunchToDeviceAttributePass());"
        },
        {
            "sha": "fddb217d4c57ee1d7c8e6bc0ad208ae0f12bf6b7",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/sink_in_invariant_ops.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 4,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d12d02667492e58516b2b122b7d80a4ba1a2d531/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fsink_in_invariant_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d12d02667492e58516b2b122b7d80a4ba1a2d531/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fsink_in_invariant_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fsink_in_invariant_ops.cc?ref=d12d02667492e58516b2b122b7d80a4ba1a2d531",
            "patch": "@@ -49,15 +49,28 @@ bool IsSinkCandidate(mlir::Operation *op) {\n // Check if the op is allowed to be sinked. We are being conservative here to\n // whilelist very limited set of ops here.\n struct AllowSinkHelper {\n-  explicit AllowSinkHelper(mlir::Operation *op, int arg_index) {\n+  explicit AllowSinkHelper(mlir::Operation* sinked_op, mlir::Operation* user,\n+                           int arg_index) {\n     if (llvm::isa<mlir::TF::BatchFunctionOp,\n-                  mlir::TF::StatefulPartitionedCallOp>(op)) {\n+                  mlir::TF::StatefulPartitionedCallOp>(user)) {\n       allow_sink_to = true;\n       callee_arg_index = arg_index;\n       return;\n     }\n \n-    if (llvm::isa<mlir::TF::IfOp>(op) && arg_index > 0) {\n+    // We tend to limit this support on WhileOp to only VarHandleOp to satisfy\n+    // IFRT lowering requirements.\n+    // Sinking other invariants like ConstOp is error-prone because it requires\n+    // non-trivial effort to avoid sinking Consts when they are used by cond\n+    // function and we don't need such support.\n+    if (llvm::isa<mlir::TF::VarHandleOp>(sinked_op) &&\n+        llvm::isa<mlir::TF::WhileOp>(user)) {\n+      allow_sink_to = true;\n+      callee_arg_index = arg_index;\n+      return;\n+    }\n+\n+    if (llvm::isa<mlir::TF::IfOp>(user) && arg_index > 0) {\n       allow_sink_to = true;\n       callee_arg_index = arg_index - 1;\n       return;\n@@ -107,7 +120,8 @@ void FindSinkTarget(\n   for (mlir::OpOperand &use : value.getUses()) {\n     auto *user = use.getOwner();\n \n-    AllowSinkHelper helper(user, use.getOperandNumber());\n+    AllowSinkHelper helper(original.getDefiningOp(), user,\n+                           use.getOperandNumber());\n \n     if (helper.allow_sink_to) {\n       auto values = FindValueInCallees(symbol_table, symbol_users, user,\n@@ -116,6 +130,14 @@ void FindSinkTarget(\n         FindSinkTarget(symbol_table, symbol_users, original, value, targets);\n       }\n     } else if (value != original) {\n+      // If the sinked op is directly used by ReturnOp, we don't sink it.\n+      // One example is for tf.WhileOp, the input and output of the cond\n+      // function and the body function must be the same. If the cond function\n+      // has an input of type tf.VarHandleOp and it just return the VarHandleOp,\n+      // we don't need to sink it.\n+      if (llvm::isa<mlir::func::ReturnOp>(user)) {\n+        continue;\n+      }\n       targets[&use].insert(original);\n     }\n   }"
        },
        {
            "sha": "d6d93d9f2d6f342cbe08c1f5eb0488ac5f9000b4",
            "filename": "tensorflow/compiler/mlir/tfrt/translate/import_model.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d12d02667492e58516b2b122b7d80a4ba1a2d531/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fimport_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d12d02667492e58516b2b122b7d80a4ba1a2d531/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fimport_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftranslate%2Fimport_model.cc?ref=d12d02667492e58516b2b122b7d80a4ba1a2d531",
            "patch": "@@ -204,11 +204,18 @@ absl::Status ConvertTfMlirToRuntimeExecutable(\n         tensorflow::tf2xla::v2::RunFunctionTf2xlaClusteringBridge(\n             module, /*is_supported_by_replicated_brige*/ true,\n             /*is_in_fallback_enabled_mode=*/VLOG_IS_ON(1)));\n+    if (VLOG_IS_ON(1)) {\n+      tensorflow::DumpMlirOpToFile(\"after_tf2xla_clustering_bridge\", module);\n+    }\n \n     TF_RETURN_IF_ERROR(\n         tensorflow::tfrt_compiler::RunLowerClusterToRuntimeOpsPassPipeline(\n             module, tsl::DeviceType(DEVICE_TPU_XLA_JIT)));\n \n+    if (VLOG_IS_ON(1)) {\n+      tensorflow::DumpMlirOpToFile(\"after_lower_cluster_to_runtime_ops\",\n+                                   module);\n+    }\n     TF_RETURN_IF_ERROR(\n         tensorflow::tf2xla::v2::ExportFromTensorflowDialectToExecutor(module));\n   } else if (options.device_target == TfrtDeviceInfraTarget::kTfFallback) {"
        }
    ],
    "stats": {
        "total": 112,
        "additions": 107,
        "deletions": 5
    }
}