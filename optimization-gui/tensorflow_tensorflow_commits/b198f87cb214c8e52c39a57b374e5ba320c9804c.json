{
    "author": "WillFroom",
    "message": "[XLA:CPU/GPU] Emit Arith::NegFOp in tiled emitter.\n\nThis gives better numerical stability on CPU which does support the instruction, we simply rewrite it back to it's original form to ensure the triton lowering works.\n\nPiperOrigin-RevId: 841890088",
    "sha": "b198f87cb214c8e52c39a57b374e5ba320c9804c",
    "files": [
        {
            "sha": "67c1eb9b94d7a7b7b3af4cae1f2b658d0cf767eb",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline.cc?ref=b198f87cb214c8e52c39a57b374e5ba320c9804c",
            "patch": "@@ -40,6 +40,7 @@ void CreateTritonXlaPipeline(\n   pm->addPass(mlir::triton::xla::CreateStableHLOLowerToTritonPass());\n \n   pm->addPass(emitters::CreateSafeIntegerArithmeticPass());\n+  pm->addPass(mlir::triton::xla::CreateUnsupportedElementwiseToTritonPass());\n \n   auto* cuda_cc = gpu_cc.cuda_compute_capability();\n   bool is_at_least_hopper = cuda_cc != nullptr && cuda_cc->IsAtLeastHopper();"
        },
        {
            "sha": "19c2bed34cf55cd92eeb637b00365cbe8a71365c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=b198f87cb214c8e52c39a57b374e5ba320c9804c",
            "patch": "@@ -439,8 +439,10 @@ absl::StatusOr<Value> EmitElementwise(mlir::ImplicitLocOpBuilder& b,\n     case HloOpcode::kNot:\n       return ma::XOrIOp::create(b, inputs[0], OnesLike(b, inputs[0].getType()));\n     case HloOpcode::kNegate:\n-      // NegFOp is not supported by Triton.\n-      return Subtract(b, {ZerosLike(b, inputs[0]), inputs[0]});\n+      if (is_integer) {\n+        return Subtract(b, {ZerosLike(b, inputs[0]), inputs[0]});\n+      }\n+      return ma::NegFOp::create(b, inputs[0]);\n     case HloOpcode::kConvert: {\n       TF_ASSIGN_OR_RETURN(\n           Type dst_ty, PrimitiveTypeToMlirType(b, hlo.shape().element_type()));"
        },
        {
            "sha": "d93e301693005539d8271763d08c0137e309a8e9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=b198f87cb214c8e52c39a57b374e5ba320c9804c",
            "patch": "@@ -3109,10 +3109,10 @@ ENTRY entry_computation {\n       CreateTritonIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n CHECK:     xtile.extract {{.*}} -> tensor<f32>\n CHECK:     tt.extern_elementwise {{.*}} (f32) -> f32\n-CHECK:     arith.subf {{.*}} f32\n+CHECK:     arith.negf {{.*}} f32\n CHECK:     xtile.extract {{.*}} -> tensor<f32>\n CHECK:     tt.extern_elementwise {{.*}} (f32) -> f32\n-CHECK:     arith.subf {{.*}} f32\n+CHECK:     arith.negf {{.*}} f32\n CHECK:     arith.addf {{.*}} f32\n CHECK:     arith.mulf {{.*}} f32\n CHECK:     arith.divf {{.*}} f32\n@@ -3622,7 +3622,7 @@ CHECK:      {{.*}} = scf.for %{{.*}} = %[[C0]] to %[[C4]] step %[[C1]]\n CHECK-SAME: iter_args({{.*}}) -> (tensor<16x64xf32>) {\n CHECK-DAG:  xtile.extract %[[ARG0]]\n CHECK-DAG:  xtile.extract %[[ARG1]]\n-CHECK-DAG:  arith.subf {{.*}} : tensor<16x32xf32>\n+CHECK-DAG:  arith.negf {{.*}} : tensor<16x32xf32>\n CHECK-DAG:  math.absf {{.*}} : tensor<32x64xf32>\n CHECK:      stablehlo.dot_general {{.*}} (tensor<16x32xf32>, tensor<32x64xf32>) -> tensor<16x64xf32>\n CHECK:      arith.addf {{.*}}\n@@ -3643,7 +3643,7 @@ CHECK:      {{.*}} = scf.for %{{.*}} = %[[C0]] to %[[C4]] step %[[C1]]\n CHECK-SAME: iter_args({{.*}}) -> (tensor<16x64xf32>) {\n CHECK-DAG:  xtile.extract %[[ARG0]]\n CHECK-DAG:  xtile.extract %[[ARG1]]\n-CHECK-DAG:  arith.subf {{.*}} : tensor<16x32xf32>\n+CHECK-DAG:  arith.negf {{.*}} : tensor<16x32xf32>\n CHECK-DAG:  math.absf {{.*}} : tensor<32x64xf32>\n CHECK:      tt.dot {{.*}} tensor<16x32xf32> * tensor<32x64xf32> -> tensor<16x64xf32>\n CHECK:      scf.yield {{.*}} : tensor<16x64xf32>"
        },
        {
            "sha": "2f044f7f98afa72afa4851ac1a2375e5212352a8",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc?ref=b198f87cb214c8e52c39a57b374e5ba320c9804c",
            "patch": "@@ -92,7 +92,8 @@ absl::flat_hash_set<HloOpcode> TritonSupportedUnaryElementwiseOps(\n   absl::flat_hash_set<HloOpcode> ret{HloOpcode::kAbs, HloOpcode::kCopy};\n \n   if (element_type != PrimitiveType::F8E5M2 &&\n-      element_type != PrimitiveType::F8E4M3FN) {\n+      element_type != PrimitiveType::F8E4M3FN &&\n+      element_type != PrimitiveType::F8E8M0FNU) {\n     ret.insert(HloOpcode::kNegate);\n   }\n "
        },
        {
            "sha": "7cda77ed4e673a80863598760d95b69b36ad1a48",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc?ref=b198f87cb214c8e52c39a57b374e5ba320c9804c",
            "patch": "@@ -123,9 +123,10 @@ bool DoesOpSupportType(HloOpcode opcode, PrimitiveType type) {\n     case HloOpcode::kDivide:\n     case HloOpcode::kRemainder:\n     case HloOpcode::kSubtract:\n-    case HloOpcode::kNegate:\n     case HloOpcode::kIota:\n       return type != PRED;\n+    case HloOpcode::kNegate:\n+      return type != PRED && type != F8E8M0FNU;\n     case HloOpcode::kRng:\n       return !pu::IsComplexType(type);\n     case HloOpcode::kComplex:"
        },
        {
            "sha": "9a722714a8357fc31d5bd11317c758adec35547c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2FBUILD?ref=b198f87cb214c8e52c39a57b374e5ba320c9804c",
            "patch": "@@ -49,6 +49,7 @@ cc_library(\n         \"triton_xla_math_to_libdevice.cc\",\n         \"triton_xla_squeeze_dims_pass.cc\",\n         \"triton_xla_unswitch_loops_pass.cc\",\n+        \"unsupported_elementwise_to_triton_pass.cc\",\n         \"xtile_lower_to_triton.cc\",\n     ],\n     hdrs = [\"passes.h\"],\n@@ -82,6 +83,7 @@ cc_library(\n         \"@llvm-project//llvm:TargetParser\",\n         \"@llvm-project//mlir:Analysis\",\n         \"@llvm-project//mlir:ArithDialect\",\n+        \"@llvm-project//mlir:ArithUtils\",\n         \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:FunctionInterfaces\",\n         \"@llvm-project//mlir:IR\","
        },
        {
            "sha": "75007131ffdb134bb5df3a098e2b3156865d91f0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.h?ref=b198f87cb214c8e52c39a57b374e5ba320c9804c",
            "patch": "@@ -54,6 +54,7 @@ std::unique_ptr<mlir::Pass> CreateTritonXLAMathToLibdevicePass(\n     absl::string_view libdevice_path, absl::string_view triple);\n std::unique_ptr<mlir::Pass> CreateXTileLowerToTritonPass();\n std::unique_ptr<mlir::Pass> CreateArithFP8ConversionToTritonPass();\n+std::unique_ptr<mlir::Pass> CreateUnsupportedElementwiseToTritonPass();\n \n // Returns true if the `op` contains an operation in it's regions that satisfies\n // the `fn`."
        },
        {
            "sha": "d8779d2ba0f4ce7a7f11088edace8ffb3489b4ca",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/passes.td",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Fpasses.td?ref=b198f87cb214c8e52c39a57b374e5ba320c9804c",
            "patch": "@@ -264,5 +264,13 @@ def ArithFP8ConversionToTritonPass\n     \"::mlir::triton::TritonDialect\",\n   ];\n }\n+def UnsupportedElementwiseToTritonPass\n+    : Pass<\"unsupported-elementwise-to-triton\"> {\n+  let summary =\n+    \"Converts unsupported elementwise operations to their Triton equivalent.\";\n+  let dependentDialects = [\n+    \"::mlir::arith::ArithDialect\",\n+  ];\n+}\n \n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_PASSES_TD_"
        },
        {
            "sha": "8313bbc324ae0684abc7d1ed817f0509eb2bc336",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/unsupported_elementwise_to_triton_pass.mlir",
            "status": "added",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Funsupported_elementwise_to_triton_pass.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Funsupported_elementwise_to_triton_pass.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Funsupported_elementwise_to_triton_pass.mlir?ref=b198f87cb214c8e52c39a57b374e5ba320c9804c",
            "patch": "@@ -0,0 +1,20 @@\n+// RUN: xla-opt %s -split-input-file -unsupported-elementwise-to-triton \\\n+// RUN: | FileCheck %s\n+\n+func.func @converts_tensor_negf_to_subf(%arg0: tensor<10xf32>) -> tensor<10xf32> {\n+  // CHECK: %[[ZERO:.*]] = arith.constant dense<0.000000e+00> : tensor<10xf32>\n+  // CHECK: %[[SUB:.*]] = arith.subf %[[ZERO]], %arg0 : tensor<10xf32>\n+  %0 = arith.negf %arg0 : tensor<10xf32>\n+  // CHECK: return %[[SUB]] : tensor<10xf32>\n+  func.return %0 : tensor<10xf32>\n+}\n+\n+//-----\n+\n+func.func @converts_scalar_negf_to_subf(%arg0: f32) -> f32 {\n+  // CHECK: %[[ZERO:.*]] = arith.constant 0.000000e+00 : f32\n+  // CHECK: %[[SUB:.*]] = arith.subf %[[ZERO]], %arg0 : f32\n+  %0 = arith.negf %arg0 : f32\n+  // CHECK: return %[[SUB]] : f32\n+  func.return %0 : f32\n+}"
        },
        {
            "sha": "fabfc8d9815c3b0a44bfcf81e132a849b8cbe1a3",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/unsupported_elementwise_to_triton_pass.cc",
            "status": "added",
            "additions": 83,
            "deletions": 0,
            "changes": 83,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Funsupported_elementwise_to_triton_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b198f87cb214c8e52c39a57b374e5ba320c9804c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Funsupported_elementwise_to_triton_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Funsupported_elementwise_to_triton_pass.cc?ref=b198f87cb214c8e52c39a57b374e5ba320c9804c",
            "patch": "@@ -0,0 +1,83 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"llvm/ADT/APFloat.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/Arith/Utils/Utils.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/Operation.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/TypeUtilities.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"xla/backends/gpu/codegen/triton/transforms/passes.h\"\n+\n+namespace mlir::triton::xla {\n+\n+#define GEN_PASS_DEF_UNSUPPORTEDELEMENTWISETOTRITONPASS\n+#include \"xla/backends/gpu/codegen/triton/transforms/passes.h.inc\"\n+\n+namespace {\n+\n+class RewriteNegFToSubtract : public OpRewritePattern<mlir::arith::NegFOp> {\n+ public:\n+  using OpRewritePattern::OpRewritePattern;\n+\n+  LogicalResult matchAndRewrite(mlir::arith::NegFOp op,\n+                                PatternRewriter& rewriter) const override {\n+    mlir::Type element_type = getElementTypeOrSelf(op.getType());\n+    auto type = mlir::dyn_cast<mlir::FloatType>(element_type);\n+\n+    if (!type) {\n+      return rewriter.notifyMatchFailure(op, \"expected float type\");\n+    }\n+\n+    const llvm::fltSemantics& semantics = type.getFloatSemantics();\n+    mlir::Value zero_value =\n+        mlir::createScalarOrSplatConstant(rewriter, op->getLoc(), op.getType(),\n+                                          mlir::APFloat::getZero(semantics));\n+\n+    rewriter.replaceOpWithNewOp<mlir::arith::SubFOp>(op, zero_value,\n+                                                     op.getOperand());\n+    return success();\n+  }\n+};\n+\n+struct UnsupportedElementwiseToTritonPass\n+    : public impl::UnsupportedElementwiseToTritonPassBase<\n+          UnsupportedElementwiseToTritonPass> {\n+  void runOnOperation() override {\n+    auto module = getOperation();\n+    mlir::RewritePatternSet patterns(\n+        &getContext(), std::make_unique<RewriteNegFToSubtract>(&getContext()));\n+    if (failed(applyPatternsGreedily(module, std::move(patterns)))) {\n+      signalPassFailure();\n+    }\n+  }\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<mlir::Pass> CreateUnsupportedElementwiseToTritonPass() {\n+  return std::make_unique<UnsupportedElementwiseToTritonPass>();\n+}\n+\n+}  // namespace mlir::triton::xla"
        }
    ],
    "stats": {
        "total": 135,
        "additions": 127,
        "deletions": 8
    }
}