{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 828854028",
    "sha": "ee6b915c070127213cdc7d81c3448c905f1ba3f9",
    "files": [
        {
            "sha": "47e76f81a0328c89c9f9f394efcf6e5388d68c4f",
            "filename": "tensorflow/compiler/tf2xla/rearrange_function_argument.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Frearrange_function_argument.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Frearrange_function_argument.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Frearrange_function_argument.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -304,7 +304,7 @@ absl::Status MaybeRewriteWhileNode(\n                                         resource_input_count, index_mapping));\n \n   // Modify cond and body functions.\n-  for (auto const& attr_name : std::vector<string>{\"cond\", \"body\"}) {\n+  for (auto const& attr_name : std::vector<std::string>{\"cond\", \"body\"}) {\n     NameAttrList attr_value;\n     TF_RETURN_IF_ERROR(GetNodeAttr(n->def(), attr_name, &attr_value));\n     const FunctionBody* fbody;\n@@ -363,7 +363,7 @@ absl::Status MaybeRewriteWhileNode(\n \n     // Save the new FunctionDef.\n     FunctionDef new_fdef;\n-    string new_name =\n+    std::string new_name =\n         fld->UniqueFunctionName(absl::StrCat(attr_value.name(), \"_rearrange_\"));\n     TF_RETURN_IF_ERROR(GraphToFunctionDef(*fbody->graph, new_name, &new_fdef));\n \n@@ -435,7 +435,7 @@ absl::Status MaybeRewriteIfNode(\n \n   std::map<int, int> resource_retval_to_arg, retval_index_mapping;\n   for (auto const& attr_name :\n-       std::vector<string>{\"then_branch\", \"else_branch\"}) {\n+       std::vector<std::string>{\"then_branch\", \"else_branch\"}) {\n     NameAttrList f;\n     TF_RETURN_IF_ERROR(GetNodeAttr(n->def(), attr_name, &f));\n     const FunctionBody* fbody;\n@@ -459,7 +459,7 @@ absl::Status MaybeRewriteIfNode(\n \n     // Save the new FunctionDef.\n     FunctionDef new_fdef;\n-    string new_name =\n+    std::string new_name =\n         fld->UniqueFunctionName(absl::StrCat(f.name(), \"_rearrange_\"));\n     TF_RETURN_IF_ERROR(GraphToFunctionDef(*fbody->graph, new_name, &new_fdef));\n "
        },
        {
            "sha": "39efe2d682eb128072adce3aa3c90d888345d7b9",
            "filename": "tensorflow/compiler/tf2xla/resource_operation_table_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fresource_operation_table_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fresource_operation_table_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fresource_operation_table_test.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -34,15 +34,16 @@ bool HasResourceInputOrOutput(const OpDef& op_def) {\n }\n \n TEST(ResourceOperationTableTest, HaveAllResourceOps) {\n-  absl::flat_hash_map<string, bool> known_resource_ops;\n+  absl::flat_hash_map<std::string, bool> known_resource_ops;\n   for (absl::string_view known_resource_op :\n        resource_op_table_internal::GetKnownResourceOps()) {\n     ASSERT_TRUE(\n-        known_resource_ops.insert({string(known_resource_op), false}).second);\n+        known_resource_ops.insert({std::string(known_resource_op), false})\n+            .second);\n   }\n \n-  std::vector<string> xla_op_names = XlaOpRegistry::GetAllRegisteredOps();\n-  for (const string& xla_op_name : xla_op_names) {\n+  std::vector<std::string> xla_op_names = XlaOpRegistry::GetAllRegisteredOps();\n+  for (const std::string& xla_op_name : xla_op_names) {\n     const OpDef* op_def;\n     TF_ASSERT_OK(OpRegistry::Global()->LookUpOpDef(xla_op_name, &op_def));\n     if (HasResourceInputOrOutput(*op_def)) {\n@@ -52,7 +53,7 @@ TEST(ResourceOperationTableTest, HaveAllResourceOps) {\n     }\n   }\n \n-  std::vector<string> unnecessary_resource_ops;\n+  std::vector<std::string> unnecessary_resource_ops;\n   for (const auto& pair : known_resource_ops) {\n     if (!pair.second) {\n       unnecessary_resource_ops.push_back(pair.first);"
        },
        {
            "sha": "4b285078f94d217ccfd90cd946ab86c587b18a2d",
            "filename": "tensorflow/compiler/tf2xla/sharding_util.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fsharding_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fsharding_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fsharding_util.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -50,7 +50,8 @@ xla::OpMetadata CreateOpMetadata(const std::string& op_type,\n }\n \n void AssignOpMetadataToSharding(xla::OpSharding& sharding,\n-                                const string& op_type, const string& op_name) {\n+                                const std::string& op_type,\n+                                const std::string& op_name) {\n   auto metadata = CreateOpMetadata(op_type, op_name);\n   if (sharding.type() == xla::OpSharding::TUPLE) {\n     for (auto& sharding_element : *sharding.mutable_tuple_shardings()) {\n@@ -69,7 +70,7 @@ absl::Status CoreOutOfRangeError(int core, int num_cores_per_replica) {\n }  // namespace\n \n absl::StatusOr<std::optional<xla::OpSharding>> ParseShardingFromDevice(\n-    const string& device_name, int num_cores_per_replica,\n+    const std::string& device_name, int num_cores_per_replica,\n     std::optional<xla::OpSharding> explicit_sharding,\n     std::optional<xla::OpMetadata> metadata) {\n   if (device_name.empty()) {\n@@ -102,7 +103,7 @@ absl::StatusOr<std::optional<xla::OpSharding>> ParseShardingFromDevice(\n \n absl::StatusOr<std::optional<xla::OpSharding>> ParseShardingFromDevice(\n     const NodeDef& node_def, int num_cores_per_replica, bool add_metadata) {\n-  const string& device_name = node_def.device();\n+  const std::string& device_name = node_def.device();\n   TF_ASSIGN_OR_RETURN(std::optional<xla::OpSharding> sharding,\n                       GetShardingFromNodeDef(node_def, add_metadata));\n   return ParseShardingFromDevice(\n@@ -114,7 +115,7 @@ absl::StatusOr<std::optional<xla::OpSharding>> ParseShardingFromDevice(\n \n absl::StatusOr<std::optional<xla::OpSharding>> ParseShardingFromDevice(\n     const Node& node, int num_cores_per_replica, bool add_metadata) {\n-  string device_name = node.assigned_device_name();\n+  std::string device_name = node.assigned_device_name();\n   if (device_name.empty()) {\n     device_name = node.requested_device();\n   }\n@@ -152,7 +153,7 @@ absl::StatusOr<std::optional<xla::OpSharding>> ParseShardingFromEdgeSource(\n }\n \n void SetShardingDeviceAssignmentFromNode(const Node& src, Node* dst) {\n-  string device_name = src.assigned_device_name();\n+  std::string device_name = src.assigned_device_name();\n   if (device_name.empty()) {\n     device_name = src.requested_device();\n   }\n@@ -169,7 +170,7 @@ absl::StatusOr<std::optional<xla::OpSharding>> GetShardingFromNodeDefInternal(\n   if (!HasNodeAttr(node_def, attribute)) {\n     return std::optional<xla::OpSharding>();\n   }\n-  string value;\n+  std::string value;\n   xla::OpSharding sharding;\n   TF_RETURN_IF_ERROR(GetNodeAttr(node_def, attribute, &value));\n   if (tensorflow::DecodeShardingAttribute(value, sharding).failed()) {"
        },
        {
            "sha": "85259e0c7298832540fab3f708559bfed7afbee9",
            "filename": "tensorflow/compiler/tf2xla/sharding_util.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fsharding_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fsharding_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fsharding_util.h?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -36,7 +36,7 @@ namespace tensorflow {\n // - a non-value if there is no assigned core or\n // - a sharding set as per xla::sharding_builder::AssignDevice.\n absl::StatusOr<std::optional<xla::OpSharding>> ParseShardingFromDevice(\n-    const string& device_name, int num_cores_per_replica,\n+    const std::string& device_name, int num_cores_per_replica,\n     std::optional<xla::OpSharding> explicit_sharding = std::nullopt,\n     std::optional<xla::OpMetadata> metadata = std::nullopt);\n "
        },
        {
            "sha": "c987e8f167422f24c78df913f5bf1cb82110d923",
            "filename": "tensorflow/compiler/tf2xla/sharding_util_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fsharding_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fsharding_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fsharding_util_test.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -33,7 +33,7 @@ TEST(CoreUtilTest, ParseShardingFromDevice) {\n   Graph graph(OpRegistry::Global());\n \n   auto core_from_sharding =\n-      [](std::optional<xla::OpSharding> sharding) -> int64 {\n+      [](std::optional<xla::OpSharding> sharding) -> int64_t {\n     if (sharding.has_value() &&\n         sharding.value().type() == xla::OpSharding::MAXIMAL) {\n       return sharding.value().tile_assignment_devices(0);"
        },
        {
            "sha": "e8b2a56cdf64d27538f29f08f5846182a4490bb9",
            "filename": "tensorflow/compiler/tf2xla/side_effect_util.cc",
            "status": "modified",
            "additions": 36,
            "deletions": 34,
            "changes": 70,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fside_effect_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fside_effect_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fside_effect_util.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -48,8 +48,8 @@ absl::Status SetDeviceOrdinalAttributeForNode(Node* node, int device_ordinal) {\n   } else if (node->IsIfNode()) {\n     AttrValue device_ordinal_value;\n     device_ordinal_value.set_i(device_ordinal);\n-    for (const string& attr_name :\n-         std::vector<string>{\"then_branch\", \"else_branch\"}) {\n+    for (const std::string& attr_name :\n+         std::vector<std::string>{\"then_branch\", \"else_branch\"}) {\n       NameAttrList branch_func;\n       TF_RETURN_IF_ERROR(GetNodeAttr(node->attrs(), attr_name, &branch_func));\n       (*branch_func.mutable_attr())[\"_device_ordinal\"] = device_ordinal_value;\n@@ -59,7 +59,8 @@ absl::Status SetDeviceOrdinalAttributeForNode(Node* node, int device_ordinal) {\n   } else if (node->IsWhileNode()) {\n     AttrValue device_ordinal_value;\n     device_ordinal_value.set_i(device_ordinal);\n-    for (const string& attr_name : std::vector<string>{\"cond\", \"body\"}) {\n+    for (const std::string& attr_name :\n+         std::vector<std::string>{\"cond\", \"body\"}) {\n       NameAttrList branch_func;\n       TF_RETURN_IF_ERROR(GetNodeAttr(node->attrs(), attr_name, &branch_func));\n       (*branch_func.mutable_attr())[\"_device_ordinal\"] = device_ordinal_value;\n@@ -80,39 +81,40 @@ absl::Status SetDeviceOrdinalAttributeForNode(Node* node, int device_ordinal) {\n std::set<std::string> CalculateTokenInputsForOutputToken(const Graph& g) {\n   std::set<std::string> results;\n   Node* first_side_effecting_node_on_path = nullptr;\n-  ReverseDFS(g,\n-             [&](Node* n) {\n-               std::vector<string> token_input_nodes;\n-               if (!GetNodeAttr(n->attrs(), kXlaTokenInputNodesAttrName,\n-                                &token_input_nodes)\n-                        .ok() ||\n-                   token_input_nodes.empty()) {\n-                 return;\n-               }\n-\n-               if (first_side_effecting_node_on_path != nullptr) {\n-                 return;\n-               }\n-\n-               first_side_effecting_node_on_path = n;\n-               string original_node_name;\n-               TF_CHECK_OK(GetNodeAttr(n->def(),\n-                                       kXlaOriginalOutsideCompilationNodeName,\n-                                       &original_node_name));\n-               results.insert(original_node_name);\n-             },\n-             [&](Node* n) {\n-               if (first_side_effecting_node_on_path == n) {\n-                 first_side_effecting_node_on_path = nullptr;\n-               }\n-             },\n-             NodeComparatorName());\n+  ReverseDFS(\n+      g,\n+      [&](Node* n) {\n+        std::vector<std::string> token_input_nodes;\n+        if (!GetNodeAttr(n->attrs(), kXlaTokenInputNodesAttrName,\n+                         &token_input_nodes)\n+                 .ok() ||\n+            token_input_nodes.empty()) {\n+          return;\n+        }\n+\n+        if (first_side_effecting_node_on_path != nullptr) {\n+          return;\n+        }\n+\n+        first_side_effecting_node_on_path = n;\n+        std::string original_node_name;\n+        TF_CHECK_OK(GetNodeAttr(n->def(),\n+                                kXlaOriginalOutsideCompilationNodeName,\n+                                &original_node_name));\n+        results.insert(original_node_name);\n+      },\n+      [&](Node* n) {\n+        if (first_side_effecting_node_on_path == n) {\n+          first_side_effecting_node_on_path = nullptr;\n+        }\n+      },\n+      NodeComparatorName());\n   return results;\n }\n \n bool HasSideEffectingNodes(const Graph& g) {\n   for (Node* n : g.nodes()) {\n-    std::vector<string> token_input_nodes;\n+    std::vector<std::string> token_input_nodes;\n     if (GetNodeAttr(n->attrs(), kXlaTokenInputNodesAttrName, &token_input_nodes)\n             .ok() &&\n         !token_input_nodes.empty()) {\n@@ -123,10 +125,10 @@ bool HasSideEffectingNodes(const Graph& g) {\n }\n \n absl::Status ParseHostComputeCoreList(\n-    absl::Span<const string> list_from_attr,\n-    std::map<string, int>* host_compute_core) {\n+    absl::Span<const std::string> list_from_attr,\n+    std::map<std::string, int>* host_compute_core) {\n   for (const auto& hc_core : list_from_attr) {\n-    std::vector<string> parts = str_util::Split(hc_core, \":\");\n+    std::vector<std::string> parts = str_util::Split(hc_core, \":\");\n     if (parts.size() != 2) {\n       return errors::InvalidArgument(\n           \"Malformed host_compute_core entry \", hc_core,"
        },
        {
            "sha": "9ba994a16a3c8e5f890a31533f37c1025ba2855d",
            "filename": "tensorflow/compiler/tf2xla/side_effect_util.h",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fside_effect_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fside_effect_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fside_effect_util.h?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -61,8 +61,9 @@ bool HasSideEffectingNodes(const Graph& g);\n // Parse the mapping from outside_compilation_subgraph name to core number,\n // which is specified in an attr as a list of strings\n // <subgraph_name>:<core_index>.\n-absl::Status ParseHostComputeCoreList(absl::Span<const string> list_from_attr,\n-                                      std::map<string, int>* host_compute_core);\n+absl::Status ParseHostComputeCoreList(\n+    absl::Span<const std::string> list_from_attr,\n+    std::map<std::string, int>* host_compute_core);\n \n }  // namespace tensorflow\n "
        },
        {
            "sha": "193eb7c08bc08aa1dfd2403dd9c6ed3989ba9a33",
            "filename": "tensorflow/compiler/tf2xla/test_util.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Ftest_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Ftest_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Ftest_util.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -21,12 +21,12 @@ limitations under the License.\n namespace tensorflow {\n \n absl::Status InstantiateFunctionForTest(\n-    const string& name, const FunctionLibraryDefinition& library,\n+    const std::string& name, const FunctionLibraryDefinition& library,\n     InstantiationResultForTest* result) {\n   const FunctionDef* fdef = library.Find(name);\n   TF_RET_CHECK(fdef != nullptr);\n \n-  auto get_func_sig = [&library](const string& op, const OpDef** sig) {\n+  auto get_func_sig = [&library](const std::string& op, const OpDef** sig) {\n     return library.LookUpOpDef(op, sig);\n   };\n   InstantiationResult inst;"
        },
        {
            "sha": "2c9cdc1c352238a967d958f1a2ec69b25f1514ca",
            "filename": "tensorflow/compiler/tf2xla/test_util.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Ftest_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Ftest_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Ftest_util.h?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -41,7 +41,7 @@ struct InstantiationResultForTest {\n // Instantiates a function, producing a GraphDef to compare against the\n // expected graph.\n absl::Status InstantiateFunctionForTest(\n-    const string& name, const FunctionLibraryDefinition& library,\n+    const std::string& name, const FunctionLibraryDefinition& library,\n     InstantiationResultForTest* result);\n \n }  // namespace tensorflow"
        },
        {
            "sha": "eccc2dfaf8d4a45fc32043ea22ea500fe43a135a",
            "filename": "tensorflow/compiler/tf2xla/tf2xla_supported_ops.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Ftf2xla_supported_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Ftf2xla_supported_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Ftf2xla_supported_ops.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -32,7 +32,8 @@ namespace tensorflow {\n namespace tf2xla {\n namespace {\n \n-void PrintSupportedOps(const string& device, const string& regen_run) {\n+void PrintSupportedOps(const std::string& device,\n+                       const std::string& regen_run) {\n   XlaOpRegistry::RegisterCompilationKernels();\n \n   std::vector<const KernelDef*> kdefs =\n@@ -46,10 +47,10 @@ void PrintSupportedOps(const string& device, const string& regen_run) {\n             << \"Operator | Type Constraint\\n\"\n             << \"-------- | ---------------\" << std::endl;\n   for (const KernelDef* kdef : kdefs) {\n-    std::vector<string> constraints;\n+    std::vector<std::string> constraints;\n     constraints.reserve(kdef->constraint().size());\n     for (const KernelDef::AttrConstraint& constraint : kdef->constraint()) {\n-      std::vector<string> types;\n+      std::vector<std::string> types;\n       const auto& allowed_values = constraint.allowed_values().list().type();\n       types.reserve(allowed_values.size());\n       for (int type : allowed_values) {\n@@ -70,18 +71,18 @@ void PrintSupportedOps(const string& device, const string& regen_run) {\n }  // namespace\n \n void SupportedOpsMain(int argc, char** argv, const char* regen_run) {\n-  std::vector<string> device_names = XlaOpRegistry::BackendNames();\n+  std::vector<std::string> device_names = XlaOpRegistry::BackendNames();\n   std::sort(device_names.begin(), device_names.end());\n \n   // Set up and parse flags.\n-  string device;\n+  std::string device;\n   std::vector<Flag> flag_list = {\n       {\"device\", &device,\n        \"Name of the compilation device for which to print supported ops, \"\n        \"one of: \" +\n            absl::StrJoin(device_names, \",\")},\n   };\n-  string usage = Flags::Usage(argv[0], flag_list);\n+  std::string usage = Flags::Usage(argv[0], flag_list);\n   bool parsed_flags_ok = Flags::Parse(&argc, argv, flag_list);\n   QCHECK(parsed_flags_ok) << \"\\n\" << usage;\n   QCHECK(XlaOpRegistry::IsBackendRegistered(device))"
        },
        {
            "sha": "e6efab85c3b5bd9b7f21187c1505256b6ac94d5f",
            "filename": "tensorflow/compiler/tf2xla/tf2xla_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Ftf2xla_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Ftf2xla_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Ftf2xla_test.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -118,8 +118,8 @@ TEST(ConvertGraphDefToXla, Sum) {\n   TF_EXPECT_OK(ConvertGraphDefToXla(graph_def, config, client, &computation));\n \n   // Set up arguments.\n-  auto x_literal = xla::LiteralUtil::CreateR0<int32>(10);\n-  auto y_literal = xla::LiteralUtil::CreateR0<int32>(32);\n+  auto x_literal = xla::LiteralUtil::CreateR0<int32_t>(10);\n+  auto y_literal = xla::LiteralUtil::CreateR0<int32_t>(32);\n   auto x_global_or = client->TransferToServer(x_literal);\n   auto y_global_or = client->TransferToServer(y_literal);\n   TF_EXPECT_OK(x_global_or.status());\n@@ -338,8 +338,8 @@ TEST(ConvertGraphDefToXla, SumWithUnusedArgument) {\n   TF_EXPECT_OK(ConvertGraphDefToXla(graph_def, config, client, &computation));\n \n   // Set up arguments.\n-  auto x_literal = xla::LiteralUtil::CreateR0<int32>(10);\n-  auto y_literal = xla::LiteralUtil::CreateR0<int32>(32);\n+  auto x_literal = xla::LiteralUtil::CreateR0<int32_t>(10);\n+  auto y_literal = xla::LiteralUtil::CreateR0<int32_t>(32);\n   auto x_global_or = client->TransferToServer(x_literal);\n   auto y_global_or = client->TransferToServer(y_literal);\n   auto unused_global_or = client->TransferToServer(y_literal);"
        },
        {
            "sha": "042b572c2343558bf4236a46de2294ff654ca86a",
            "filename": "tensorflow/compiler/tf2xla/tf2xla_util.cc",
            "status": "modified",
            "additions": 43,
            "deletions": 40,
            "changes": 83,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Ftf2xla_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Ftf2xla_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Ftf2xla_util.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -58,8 +58,9 @@ absl::Status ValidateTensorId(const tf2xla::TensorId& id) {\n   return absl::OkStatus();\n }\n \n-absl::Status CheckNameDuplicates(const string& kind, const string& name,\n-                                 std::set<string>* names) {\n+absl::Status CheckNameDuplicates(const std::string& kind,\n+                                 const std::string& name,\n+                                 std::set<std::string>* names) {\n   if (!name.empty()) {\n     if (!names->insert(name).second) {\n       return errors::InvalidArgument(\"duplicate \", kind, \" name: \", name);\n@@ -68,12 +69,12 @@ absl::Status CheckNameDuplicates(const string& kind, const string& name,\n   return absl::OkStatus();\n }\n \n-absl::Status CheckFeedFetchNameConflicts(const string& kind,\n-                                         const std::set<string>& names) {\n+absl::Status CheckFeedFetchNameConflicts(const std::string& kind,\n+                                         const std::set<std::string>& names) {\n   // We don't allow the feeds or fetches to contain both \"foo\" and \"foo_data\",\n   // since that will cause a collision in codegen symbols.\n-  for (const string& name : names) {\n-    const string name_data(name + \"_data\");\n+  for (const std::string& name : names) {\n+    const std::string name_data(name + \"_data\");\n     if (names.find(name_data) != names.end()) {\n       return errors::InvalidArgument(\"conflicting \", kind, \" name: \", name,\n                                      \" and \", name_data);\n@@ -227,7 +228,7 @@ absl::Status ReplaceRetvalInputWithArg(\n // the function to replace _Arg nodes in `const_input_index_to_node` with Const\n // inputs.\n absl::Status PropagateConstIntoFuncAttr(\n-    Node* n, const string& attr_name,\n+    Node* n, const std::string& attr_name,\n     const absl::flat_hash_map<int, const Node*>& const_input_index_to_node,\n     const FunctionLibraryDefinition* lookup_fld, FunctionLibraryDefinition* fld,\n     bool passthrough_arg_to_retval = false) {\n@@ -255,7 +256,7 @@ absl::Status PropagateConstIntoFuncAttr(\n \n   // Save rewritten function.\n   FunctionDef replace_fdef;\n-  string new_func_name =\n+  std::string new_func_name =\n       fld->UniqueFunctionName(absl::StrCat(func_attr.name(), \"_const_\"));\n   const StackTracesMap* stack_traces =\n       lookup_fld->GetStackTraces(func_attr.name());\n@@ -301,21 +302,22 @@ absl::Status PropagateConstIntoIfNode(\n   // Rewrite \"then_branch\" and \"else_branch\" function, replace usage of those\n   // _Arg nodes with corresponding const node.\n   for (const auto& attr_name :\n-       std::vector<string>{\"then_branch\", \"else_branch\"}) {\n+       std::vector<std::string>{\"then_branch\", \"else_branch\"}) {\n     TF_RETURN_IF_ERROR(PropagateConstIntoFuncAttr(\n         if_node, attr_name, const_input_index_to_node, lookup_fld, fld));\n   }\n \n   return absl::OkStatus();\n }\n \n-using GraphCache = absl::flat_hash_map<string, std::unique_ptr<FunctionBody>>;\n+using GraphCache =\n+    absl::flat_hash_map<std::string, std::unique_ptr<FunctionBody>>;\n \n absl::StatusOr<FunctionBody*> FindOrInsert(\n     GraphCache* cache, const NameAttrList& body_attr,\n     const FunctionLibraryDefinition* lookup_fld,\n     const FunctionLibraryDefinition* fallback_fld) {\n-  const string name = body_attr.name();\n+  const std::string name = body_attr.name();\n   std::unique_ptr<FunctionBody>& value = (*cache)[name];\n   if (!value) {\n     const FunctionDef* body_func = lookup_fld->Find(name);\n@@ -413,7 +415,7 @@ absl::Status PropagateConstIntoAndAroundWhileNode(\n   absl::flat_hash_map<int, Node*> const_input_index_to_mutable_node;\n   NameAttrList body_attr;\n   TF_RETURN_IF_ERROR(GetNodeAttr(while_node->def(), \"body\", &body_attr));\n-  const string fn_name = body_attr.name();\n+  const std::string fn_name = body_attr.name();\n   const FunctionDef* body_func = lookup_fld->Find(fn_name);\n   if (!body_func) {\n     return errors::Internal(\"Propagate: Cannot find body function \", fn_name,\n@@ -461,7 +463,7 @@ absl::Status PropagateConstIntoAndAroundWhileNode(\n \n   // Rewrite \"cond\" and \"body\" function, replace usage of those _Arg nodes with\n   // corresponding const node.\n-  for (const auto& attr_name : std::vector<string>{\"cond\", \"body\"}) {\n+  for (const auto& attr_name : std::vector<std::string>{\"cond\", \"body\"}) {\n     TF_RETURN_IF_ERROR(PropagateConstIntoFuncAttr(\n         while_node, attr_name, const_input_index_to_node, lookup_fld, fld,\n         /*passthrough_arg_to_retval=*/attr_name == \"body\"));\n@@ -487,7 +489,7 @@ absl::StatusOr<bool> IsLoopInvariant(\n }\n \n absl::Status ValidateConfig(const tf2xla::Config& config) {\n-  std::set<string> names;\n+  std::set<std::string> names;\n   for (const tf2xla::Feed& feed : config.feed()) {\n     TF_RETURN_IF_ERROR(ValidateTensorId(feed.id()));\n     TF_RETURN_IF_ERROR(TensorShape::IsValidShape(feed.shape()));\n@@ -508,19 +510,20 @@ absl::Status ValidateConfig(const tf2xla::Config& config) {\n \n absl::Status AddPlaceholdersForFeeds(\n     const tf2xla::Config& config, const OpRegistryInterface* op_registry,\n-    std::unordered_map<string, string>* feed_remapping, GraphDef* graph_def) {\n+    std::unordered_map<std::string, std::string>* feed_remapping,\n+    GraphDef* graph_def) {\n   struct PlaceholderInfo {\n     const tf2xla::Feed* feed = nullptr;  // point to Feed in <config>.\n-    string placeholder_name;\n+    std::string placeholder_name;\n     DataType data_type = DT_INVALID;\n   };\n \n   // Put each fed tensor into a map by name:port. A map is used for determinism\n   // when creating placeholders (genrules want deterministic output).\n-  std::map<string, PlaceholderInfo> placeholder_info;\n+  std::map<std::string, PlaceholderInfo> placeholder_info;\n   for (int i = 0; i < config.feed_size(); ++i) {\n     const tf2xla::Feed* feed = &config.feed(i);\n-    const string name_port = TensorIdToString(feed->id());\n+    const std::string name_port = TensorIdToString(feed->id());\n     PlaceholderInfo& info = placeholder_info[name_port];\n     info.feed = feed;\n     info.placeholder_name = absl::StrCat(\"aot_feed_\", feed->id().output_index(),\n@@ -529,7 +532,7 @@ absl::Status AddPlaceholdersForFeeds(\n   }\n \n   // Verify node exists and determine data type.\n-  std::unordered_map<string, const NodeDef*> name_to_node;\n+  std::unordered_map<std::string, const NodeDef*> name_to_node;\n   for (int i = 0; i < graph_def->node_size(); ++i) {\n     name_to_node[graph_def->node(i).name()] = &graph_def->node(i);\n   }\n@@ -609,25 +612,25 @@ absl::Status PruneGraphDefInto(const tf2xla::Config& config, const GraphDef& in,\n   out->clear_node();\n \n   // Tensors needed for feeding.\n-  std::set<std::pair<string, int>> feed_tensors;\n+  std::set<std::pair<std::string, int>> feed_tensors;\n   for (const tf2xla::Feed& feed : config.feed()) {\n     feed_tensors.insert(\n         std::make_pair(feed.id().node_name(), feed.id().output_index()));\n   }\n \n   // Maps node name to reachability.\n-  std::unordered_map<string, std::pair<bool, const NodeDef*>> node_by_name;\n+  std::unordered_map<std::string, std::pair<bool, const NodeDef*>> node_by_name;\n   for (const NodeDef& node : in.node()) {\n     node_by_name[node.name()] = std::pair<bool, const NodeDef*>(false, &node);\n   }\n \n   // Traverse.\n-  std::queue<string> name_queue;\n+  std::queue<std::string> name_queue;\n   for (int i = 0; i < config.fetch_size(); ++i) {\n     name_queue.push(config.fetch(i).id().node_name());\n   }\n   while (!name_queue.empty()) {\n-    const string name = name_queue.front();\n+    const std::string name = name_queue.front();\n     name_queue.pop();\n \n     auto find_it = node_by_name.find(name);\n@@ -642,9 +645,9 @@ absl::Status PruneGraphDefInto(const tf2xla::Config& config, const GraphDef& in,\n     map_entry.first = true;\n \n     // Push input nodes of the currently visited node to name_queue.\n-    for (const string& in_edge : map_entry.second->input()) {\n+    for (const std::string& in_edge : map_entry.second->input()) {\n       auto id = ParseTensorName(in_edge);\n-      const string node_name = string(id.first);\n+      const std::string node_name = std::string(id.first);\n       if (feed_tensors.find(std::make_pair(node_name, id.second)) ==\n           feed_tensors.end()) {\n         name_queue.push(node_name);\n@@ -668,7 +671,7 @@ absl::Status PruneGraphDefInto(const tf2xla::Config& config, const GraphDef& in,\n   return absl::OkStatus();\n }\n \n-string TensorIdToString(const tf2xla::TensorId& id) {\n+std::string TensorIdToString(const tf2xla::TensorId& id) {\n   return absl::StrCat(id.node_name(), \":\", id.output_index());\n }\n \n@@ -682,7 +685,7 @@ absl::Status SetNodeShardingFromNeighbors(Node* n, bool out_edges) {\n         std::optional<xla::OpSharding> sharding,\n         ParseShardingFromDevice(\n             *possible_match,\n-            /*num_cores_per_replica=*/std::numeric_limits<int32>::max(),\n+            /*num_cores_per_replica=*/std::numeric_limits<int32_t>::max(),\n             /*add_metadata=*/false));\n     if (sharding && sharding->type() == xla::OpSharding::MAXIMAL) {\n       const int core_annotation = sharding.value().tile_assignment_devices(0);\n@@ -709,7 +712,7 @@ void AddDtypeToKernelDefConstraint(absl::string_view name, DataType dtype,\n }\n \n namespace {\n-uint32 InitialRandomSeed() {\n+uint32_t InitialRandomSeed() {\n   // Support plumbing the TF seed through to XLA is being worked on.\n   // If a user wants deterministic behavior, their best option\n   // is to start with a known checkpoint. This also handles issues when\n@@ -724,13 +727,13 @@ uint32 InitialRandomSeed() {\n }\n }  // namespace\n \n-uint32 GetXLARandomSeed() {\n+uint32_t GetXLARandomSeed() {\n   // We initialize counter with an odd number and increment it by two\n   // everytime. This ensures that it will never be zero, even\n   // after an overflow. When seeded with zero, some XLA backends\n   // can return all zeros instead of random numbers.\n-  static std::atomic<uint32> counter(InitialRandomSeed());\n-  uint32 seed = counter.fetch_add(2);\n+  static std::atomic<uint32_t> counter(InitialRandomSeed());\n+  uint32_t seed = counter.fetch_add(2);\n   std::srand(seed);\n   return std::rand() | 1;\n }\n@@ -766,7 +769,7 @@ bool HasAssociatedFunction(const NodeDef& node_def,\n std::vector<AssociatedFunctionInfo> GetAssociatedFunctions(\n     const Node& node, const FunctionLibraryDefinition* fld) {\n   std::vector<AssociatedFunctionInfo> results;\n-  const string& op = node.type_string();\n+  const std::string& op = node.type_string();\n   if (fld->Contains(op)) {\n     // This is a function call node.\n     AttrValueMap attrs(node.attrs().begin(), node.attrs().end());\n@@ -795,7 +798,7 @@ std::vector<AssociatedFunctionInfo> GetAssociatedFunctions(\n absl::Status RewriteAssociatedFunction(\n     Graph* graph, Node* node, FunctionLibraryDefinition* fld,\n     const AssociatedFunctionInfo& associated_function,\n-    const string& rewritten_function_name) {\n+    const std::string& rewritten_function_name) {\n   switch (associated_function.type()) {\n     case AssociatedFunctionInfo::kFunctionCallNode: {\n       // Change this node to call the new function.\n@@ -834,7 +837,7 @@ absl::Status RewriteAssociatedFunction(\n       GradientDef gradient_def;\n       gradient_def.set_function_name(func.name());\n       gradient_def.set_gradient_func(rewritten_function_name);\n-      string original_grad_func = fld->FindGradient(func.name());\n+      std::string original_grad_func = fld->FindGradient(func.name());\n       if (original_grad_func.empty()) {\n         TF_RETURN_IF_ERROR(fld->AddGradientDef(gradient_def));\n       } else if (original_grad_func != rewritten_function_name) {\n@@ -863,9 +866,9 @@ absl::Status RewriteAssociatedFunction(\n }\n \n absl::Status CachedFunctionHandles::GetOrInstantiate(\n-    const string& func_name, AttrSlice attrs,\n+    const std::string& func_name, AttrSlice attrs,\n     FunctionLibraryRuntime::Handle* handle) {\n-  string canonicalized_name = Canonicalize(func_name, attrs);\n+  std::string canonicalized_name = Canonicalize(func_name, attrs);\n   auto iter = handles_.find(canonicalized_name);\n   if (iter != handles_.end()) {\n     *handle = iter->second;\n@@ -919,8 +922,8 @@ absl::StatusOr<Node*> ReplaceNode(Graph* g, Node* n, const NodeDef& node_def) {\n }\n \n absl::StatusOr<Node*> BuildIdentityNode(\n-    Graph* graph, const string& node_name, DataType dtype, const Node* input,\n-    std::optional<string> requested_device) {\n+    Graph* graph, const std::string& node_name, DataType dtype,\n+    const Node* input, std::optional<std::string> requested_device) {\n   // Create identity node.\n   NodeDef ndef;\n   ndef.set_name(node_name);\n@@ -975,7 +978,7 @@ absl::Status PruneUnreachableFunctionsFromGraph(\n   g.ToGraphDef(&graph_def);\n   FunctionLibraryDefinition reachable_functions =\n       fld->ReachableDefinitions(graph_def);\n-  for (const string& func_name : fld->ListFunctionNames()) {\n+  for (const std::string& func_name : fld->ListFunctionNames()) {\n     if (!reachable_functions.Find(func_name)) {\n       TF_RETURN_IF_ERROR(fld->RemoveFunction(func_name));\n     }\n@@ -1106,7 +1109,7 @@ absl::Status RewriteTensorListWithConstElement(Graph* g,\n \n     // Add rewritten backward While body function.\n     FunctionDef new_fdef;\n-    string new_name = fld->UniqueFunctionName(\n+    std::string new_name = fld->UniqueFunctionName(\n         absl::StrCat(bwd_body_attr.name(), \"_tl_rewrite_\"));\n     TF_RETURN_IF_ERROR(\n         GraphToFunctionDef(*bwd_fbody->graph, new_name, &new_fdef));"
        },
        {
            "sha": "4da5a474d964dc32aff923ba74a12452312beecb",
            "filename": "tensorflow/compiler/tf2xla/tf2xla_util.h",
            "status": "modified",
            "additions": 22,
            "deletions": 19,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Ftf2xla_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Ftf2xla_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Ftf2xla_util.h?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -41,15 +41,16 @@ absl::Status ValidateConfig(const tf2xla::Config& config);\n // feeds).\n absl::Status AddPlaceholdersForFeeds(\n     const tf2xla::Config& config, const OpRegistryInterface* op_registry,\n-    std::unordered_map<string, string>* feed_remapping, GraphDef* graph_def);\n+    std::unordered_map<std::string, std::string>* feed_remapping,\n+    GraphDef* graph_def);\n \n // Returns in <out> a copy of <in>, pruned to only include fetches from\n // <config>.\n absl::Status PruneGraphDefInto(const tf2xla::Config& config, const GraphDef& in,\n                                GraphDef* out);\n \n // Returns node:port for the given <id>.\n-string TensorIdToString(const tf2xla::TensorId& id);\n+std::string TensorIdToString(const tf2xla::TensorId& id);\n \n // Updates the sharding of <n> based on the sharding of its neighbors.\n // If <out_edges> is true, outgoing edges from <n> are considered; else incoming\n@@ -61,7 +62,7 @@ void AddDtypeToKernelDefConstraint(absl::string_view name, DataType dtype,\n                                    KernelDef* kdef);\n \n // Returns the next random seed to use for seeding xla rng.\n-uint32 GetXLARandomSeed();\n+uint32_t GetXLARandomSeed();\n \n // Indicates how a FunctionDef is associated with a graph node (e.g. the node is\n // a function call, or the node has function attrs).\n@@ -74,22 +75,22 @@ class AssociatedFunctionInfo {\n   };\n \n   // The function is an attr of the node.\n-  static AssociatedFunctionInfo FunctionAttr(const string& func_name,\n+  static AssociatedFunctionInfo FunctionAttr(const std::string& func_name,\n                                              const AttrValueMap& attrs,\n-                                             const string& attr_name) {\n+                                             const std::string& attr_name) {\n     return AssociatedFunctionInfo(kFunctionAttr, func_name, attrs, attr_name);\n   }\n \n   // The node is a function call.\n-  static AssociatedFunctionInfo FunctionCall(const string& func_name,\n+  static AssociatedFunctionInfo FunctionCall(const std::string& func_name,\n                                              const AttrValueMap& attrs) {\n     // attr_name will not be used in this case.\n     return AssociatedFunctionInfo(kFunctionCallNode, func_name, attrs,\n                                   /*attr_name=*/\"\");\n   }\n \n   // The node is a SymbolicGradient op.\n-  static AssociatedFunctionInfo SymbolicGradient(const string& func_name,\n+  static AssociatedFunctionInfo SymbolicGradient(const std::string& func_name,\n                                                  const AttrValueMap& attrs) {\n     // attr_name will not be used in this case.\n     return AssociatedFunctionInfo(kSymbolicGradient, func_name, attrs,\n@@ -98,27 +99,29 @@ class AssociatedFunctionInfo {\n \n   AssociatedFunctionType type() const { return type_; }\n \n-  const string& func_name() const { return func_name_; }\n+  const std::string& func_name() const { return func_name_; }\n \n-  const string& attr_name() const { return attr_name_; }\n+  const std::string& attr_name() const { return attr_name_; }\n \n   const AttrValueMap& attrs() const { return attrs_; }\n \n  private:\n-  AssociatedFunctionInfo(AssociatedFunctionType type, const string& func_name,\n-                         const AttrValueMap& attrs, const string& attr_name)\n+  AssociatedFunctionInfo(AssociatedFunctionType type,\n+                         const std::string& func_name,\n+                         const AttrValueMap& attrs,\n+                         const std::string& attr_name)\n       : type_(type),\n         func_name_(func_name),\n         attrs_(attrs),\n         attr_name_(attr_name) {}\n \n   // Available for all instances.\n   AssociatedFunctionType type_;\n-  string func_name_;\n+  std::string func_name_;\n   AttrValueMap attrs_;\n \n   // Only available if the function is defined in an attr.\n-  string attr_name_;\n+  std::string attr_name_;\n };\n \n // Returns if the NodeDef has associated function.\n@@ -142,7 +145,7 @@ std::vector<AssociatedFunctionInfo> GetAssociatedFunctions(\n absl::Status RewriteAssociatedFunction(\n     Graph* graph, Node* node, FunctionLibraryDefinition* fld,\n     const AssociatedFunctionInfo& associated_function,\n-    const string& rewritten_function_name);\n+    const std::string& rewritten_function_name);\n \n // Class to act as cache for FunctionLibraryRuntime::Handle objects.\n class CachedFunctionHandles {\n@@ -152,7 +155,7 @@ class CachedFunctionHandles {\n   // Populates `handle` for requested function and attributes. If we have\n   // instantiated the function with the same attributes before, `handle` will be\n   // cached handle; otherwise instantiate the function and populate `handle`.\n-  absl::Status GetOrInstantiate(const string& func_name, AttrSlice attrs,\n+  absl::Status GetOrInstantiate(const std::string& func_name, AttrSlice attrs,\n                                 FunctionLibraryRuntime::Handle* handle);\n \n   // Releases all handles in the cache. Returns first non-OK status if any;\n@@ -163,7 +166,7 @@ class CachedFunctionHandles {\n \n  private:\n   FunctionLibraryRuntime* flr_;\n-  std::map<string, FunctionLibraryRuntime::Handle> handles_;\n+  std::map<std::string, FunctionLibraryRuntime::Handle> handles_;\n \n   CachedFunctionHandles(const CachedFunctionHandles&) = delete;\n   void operator=(const CachedFunctionHandles&) = delete;\n@@ -179,9 +182,9 @@ struct OutEdgeInfo {\n absl::StatusOr<Node*> ReplaceNode(Graph* g, Node* n, const NodeDef& node_def);\n \n // Helper function that builds an Identity node.\n-absl::StatusOr<Node*> BuildIdentityNode(Graph* graph, const string& node_name,\n-                                        DataType dtype, const Node* input,\n-                                        std::optional<string> requested_device);\n+absl::StatusOr<Node*> BuildIdentityNode(\n+    Graph* graph, const std::string& node_name, DataType dtype,\n+    const Node* input, std::optional<std::string> requested_device);\n \n // For \"If\"/\"While\" nodes, if some of their inputs are Const nodes, rewrite\n // body functions to use the Const nodes instead of original _Arg nodes."
        },
        {
            "sha": "ef64b82f50e5be7f2e2316724e048034cad77a9d",
            "filename": "tensorflow/compiler/tf2xla/tf2xla_util_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Ftf2xla_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Ftf2xla_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Ftf2xla_util_test.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -157,7 +157,7 @@ TEST(ValidateConfig, ConflictingFetchName) {\n   ExpectErrorContains(ValidateConfig(config), \"conflicting fetch name\");\n }\n \n-static tf2xla::Config FetchesConfig(std::vector<string> fetches) {\n+static tf2xla::Config FetchesConfig(std::vector<std::string> fetches) {\n   tf2xla::Config config;\n   for (const auto& fetch_node_name : fetches) {\n     auto* fetch = config.add_fetch();\n@@ -409,7 +409,7 @@ TEST(PropagateConstIntoFunctionalNodes, CopiedConstNodeHasUniqueName) {\n   TF_ASSERT_OK(GetNodeAttr(while_node->def(), \"body\", &body_fn));\n   const FunctionDef* rewritten_body_fn = fld.Find(body_fn.name());\n   ASSERT_NE(rewritten_body_fn, nullptr);\n-  std::unordered_map<string, NodeDef> nodes;\n+  std::unordered_map<std::string, NodeDef> nodes;\n   for (const NodeDef& node_def : rewritten_body_fn->node_def()) {\n     nodes[node_def.name()] = node_def;\n   }"
        },
        {
            "sha": "add79c369b69ef8dba7a087b786b689f8174a9e1",
            "filename": "tensorflow/compiler/tf2xla/xla_compilation_device.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compilation_device.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compilation_device.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compilation_device.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -39,7 +39,7 @@ class XlaCompilationAllocator : public Allocator {\n   XlaCompilationAllocator() {}\n   ~XlaCompilationAllocator() override {}\n \n-  string Name() override { return \"xla_compilation\"; }\n+  std::string Name() override { return \"xla_compilation\"; }\n \n   void* AllocateRaw(size_t alignment, size_t num_bytes) override {\n     // Regardless of the size requested, always allocates an XlaExpression."
        },
        {
            "sha": "5ee45e499cb49e6872bae994deb2793a1711b3dc",
            "filename": "tensorflow/compiler/tf2xla/xla_compiled_cpu_function.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiled_cpu_function.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiled_cpu_function.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiled_cpu_function.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -34,7 +34,7 @@ namespace tensorflow {\n \n namespace {\n \n-int32 GetResultIndex(const int32* result_index_table, int32 num_results) {\n+int32_t GetResultIndex(const int32_t* result_index_table, int32_t num_results) {\n   auto it =\n       std::min_element(result_index_table, result_index_table + num_results);\n \n@@ -150,7 +150,7 @@ int LookupNameIndex(absl::string_view name, const char** names) {\n \n }  // namespace\n \n-int XlaCompiledCpuFunction::LookupArgIndex(const string& name) const {\n+int XlaCompiledCpuFunction::LookupArgIndex(const std::string& name) const {\n   return LookupNameIndex(name, arg_names_);\n }\n \n@@ -162,7 +162,7 @@ int XlaCompiledCpuFunction::LookupVariableIndex(absl::string_view name) const {\n   return num_args_ - num_variables_ + index;\n }\n \n-int XlaCompiledCpuFunction::LookupResultIndex(const string& name) const {\n+int XlaCompiledCpuFunction::LookupResultIndex(const std::string& name) const {\n   return LookupNameIndex(name, result_names_);\n }\n "
        },
        {
            "sha": "061982db6fd08f0a4855d71d217e992ddac93c5c",
            "filename": "tensorflow/compiler/tf2xla/xla_compiled_cpu_function.h",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiled_cpu_function.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiled_cpu_function.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiled_cpu_function.h?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -129,14 +129,14 @@ class XlaCompiledCpuFunction {\n \n     // Result parameter i is described by\n     // buffer_infos[result_index_table[i]].\n-    const int32* result_index_table_ = nullptr;\n+    const int32_t* result_index_table_ = nullptr;\n \n     // There are num_results result parameters.\n     int64_t num_results_ = 0;\n \n     // Entry parameter i is described by\n     // buffer_infos[arg_index_table[i]].\n-    const int32* arg_index_table_ = nullptr;\n+    const int32_t* arg_index_table_ = nullptr;\n \n     // There are num_args entry parameters.\n     int64_t num_args_ = 0;\n@@ -210,7 +210,7 @@ class XlaCompiledCpuFunction {\n   // TODO(fschneider): For now this always returns an empty string because there\n   // is no support for error reporting in XLA. Remove this once all callers are\n   // updated.\n-  string error_msg() const { return error_msg_; }\n+  std::string error_msg() const { return error_msg_; }\n \n   void set_error_msg(absl::string_view error_msg) { error_msg_ = error_msg; }\n \n@@ -303,7 +303,7 @@ class XlaCompiledCpuFunction {\n   // The index remains constant for every instance of XlaCompiledCpuFunction\n   // generated from the same static data, and might not be cheap to determine.\n   // Recommended usage is to capture this in a variable for re-use.\n-  int LookupArgIndex(const string& name) const;\n+  int LookupArgIndex(const std::string& name) const;\n \n   // Returns the 0-based index for the variable with the given `name`.\n   // Returns -1 if the name wasn't found, or data isn't available.\n@@ -319,7 +319,7 @@ class XlaCompiledCpuFunction {\n   // The index remains constant for every instance of XlaCompiledCpuFunction\n   // generated from the same static data, and might not be cheap to determine.\n   // Recommended usage is to capture this in a variable for re-use.\n-  int LookupResultIndex(const string& name) const;\n+  int LookupResultIndex(const std::string& name) const;\n \n   // Returns the name of the argument at `index`.\n   // Returns nullptr if `HasNameIndices() == false` or `index` is out of range.\n@@ -365,7 +365,7 @@ class XlaCompiledCpuFunction {\n     return buffer_infos_;\n   }\n \n-  int32 num_buffers() const { return num_buffers_; }\n+  int32_t num_buffers() const { return num_buffers_; }\n \n   void** buffer_table() const { return buffer_table_; }\n \n@@ -424,7 +424,7 @@ class XlaCompiledCpuFunction {\n   }\n \n   static void set_static_data_result_index_table(\n-      StaticData* static_data, const int32* result_index_table) {\n+      StaticData* static_data, const int32_t* result_index_table) {\n     static_data->result_index_table_ = result_index_table;\n   }\n \n@@ -434,7 +434,7 @@ class XlaCompiledCpuFunction {\n   }\n \n   static void set_static_data_arg_index_table(StaticData* static_data,\n-                                              const int32* arg_index_table) {\n+                                              const int32_t* arg_index_table) {\n     static_data->arg_index_table_ = arg_index_table;\n   }\n \n@@ -531,21 +531,21 @@ class XlaCompiledCpuFunction {\n \n   // Describes the buffers used by the XLA computation.\n   const xla::cpu::BufferAllocationInfo* const buffer_infos_;\n-  const int32 num_buffers_;\n+  const int32_t num_buffers_;\n \n   // Indices of expanded result tuple.\n-  const int32 num_results_;\n-  const int32* const result_index_table_;\n+  const int32_t num_results_;\n+  const int32_t* const result_index_table_;\n \n   // Argument i needs to be placed in buffer_table_[arg_index_to_temp_index_[i]]\n   // for XLA generated code to be able to find it.\n-  const int32* const arg_index_table_;\n+  const int32_t* const arg_index_table_;\n \n   // The number of incoming arguments.\n-  const int32 num_args_;\n+  const int32_t num_args_;\n \n   // The number of incoming variables.\n-  const int32 num_variables_;\n+  const int32_t num_variables_;\n \n   // Shapes of the input arguments.\n   const ShapeInfo* const arg_shape_infos_;"
        },
        {
            "sha": "7418d9b8454733525d4b892aff2bedb7ab6240f0",
            "filename": "tensorflow/compiler/tf2xla/xla_compiler.cc",
            "status": "modified",
            "additions": 35,
            "deletions": 32,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiler.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -130,7 +130,7 @@ ComputeArgAndRetvalShardings(const Graph& graph) {\n       [](const Node* n) -> absl::StatusOr<std::optional<xla::OpSharding>> {\n     TF_ASSIGN_OR_RETURN(\n         auto sharding,\n-        ParseShardingFromDevice(*n, std::numeric_limits<int32>::max(),\n+        ParseShardingFromDevice(*n, std::numeric_limits<int32_t>::max(),\n                                 /*add_metadata=*/false));\n     return sharding;\n   };\n@@ -173,7 +173,7 @@ absl::Status ExecuteGraph(XlaContext* xla_context, std::unique_ptr<Graph> graph,\n   xla_context->Ref();\n   absl::Status status;\n   auto step_container = std::make_unique<ScopedStepContainer>(\n-      step_id, [&status, device](const string& name) {\n+      step_id, [&status, device](const std::string& name) {\n         status = device->resource_manager()->Cleanup(name);\n       });\n   TF_RETURN_IF_ERROR(step_container->Create(device->resource_manager(),\n@@ -484,8 +484,8 @@ absl::Status BuildComputation(\n \n }  // namespace\n \n-string XlaCompiler::Argument::HumanString() const {\n-  string common;\n+std::string XlaCompiler::Argument::HumanString() const {\n+  std::string common;\n   if (!name.empty()) {\n     common = absl::StrCat(\" name=\", name);\n   }\n@@ -503,7 +503,7 @@ string XlaCompiler::Argument::HumanString() const {\n       return absl::StrCat(\"kind=constant-resource\", common,\n                           \" value=\", constant_value.DebugString());\n     case kResource: {\n-      string output = absl::StrCat(\n+      std::string output = absl::StrCat(\n           \"kind=resource\", common,\n           \" resource_kind=\", XlaResource::KindToString(resource_kind),\n           \" initialized=\", initialized, \" is_fast_mem=\", fast_mem);\n@@ -543,7 +543,7 @@ XlaCompiler::Argument::DimensionSizesAsInlinedVector() const {\n   }\n }\n \n-string XlaCompiler::Argument::ShapeHumanString() const {\n+std::string XlaCompiler::Argument::ShapeHumanString() const {\n   if (absl::holds_alternative<TensorShape>(shape)) {\n     return std::get<TensorShape>(shape).DebugString();\n   } else {\n@@ -592,9 +592,9 @@ XlaCompiler::~XlaCompiler() = default;\n \n int64_t XlaCompiler::NextStepId() { return next_step_id_++; }\n \n-uint64 XlaCompiler::SignatureHash::operator()(\n-    const std::pair<string, std::vector<Argument>>& signature) const {\n-  return std::hash<string>()(signature.first);\n+uint64_t XlaCompiler::SignatureHash::operator()(\n+    const std::pair<std::string, std::vector<Argument>>& signature) const {\n+  return std::hash<std::string>()(signature.first);\n }\n \n static absl::Status GetFunctionBody(const NameAttrList& function,\n@@ -703,9 +703,9 @@ std::unique_ptr<Graph> XlaCompiler::GetGraph(const FunctionBody* fbody) {\n                 flib_runtime_->GetFunctionLibraryDefinition(), &shape_info)\n         .IgnoreError();\n     auto node_name_index = graph->BuildNodeNameIndex();\n-    std::unordered_map<string, std::vector<PartialTensorShape>> shape_map;\n+    std::unordered_map<std::string, std::vector<PartialTensorShape>> shape_map;\n     for (const auto& node_shape_info : shape_info) {\n-      const string& node_name = node_shape_info.first;\n+      const std::string& node_name = node_shape_info.first;\n       const std::vector<InferredShape>& output_shapes = node_shape_info.second;\n       const auto& node_iter = node_name_index.find(node_name);\n       if (node_iter != node_name_index.end()) {\n@@ -726,9 +726,9 @@ std::unique_ptr<Graph> XlaCompiler::GetGraph(const FunctionBody* fbody) {\n               flib_runtime_->GetFunctionLibraryDefinition(), &shape_info)\n       .IgnoreError();\n   auto node_name_index = graph->BuildNodeNameIndex();\n-  std::unordered_map<string, std::vector<PartialTensorShape>> shape_map;\n+  std::unordered_map<std::string, std::vector<PartialTensorShape>> shape_map;\n   for (const auto& node_shape_info : shape_info) {\n-    const string& node_name = node_shape_info.first;\n+    const std::string& node_name = node_shape_info.first;\n     const std::vector<InferredShape>& output_shapes = node_shape_info.second;\n     const auto& node_iter = node_name_index.find(node_name);\n     if (node_iter != node_name_index.end()) {\n@@ -754,7 +754,7 @@ std::vector<std::string> GetValidControlRets(\n   // the map with nodes in FunctionDef control_ret_nodes and later query it\n   // using the nodes in `graph`. The Node pointers would be different but the\n   // Node name is expected to remain the same between the two.\n-  absl::flat_hash_map<string, int> control_ret_nodes_map;\n+  absl::flat_hash_map<std::string, int> control_ret_nodes_map;\n   for (int i = 0; i < orig_control_ret_nodes.size(); ++i) {\n     const Node* n = orig_control_ret_nodes[i];\n     control_ret_nodes_map[n->name()] = i;\n@@ -814,7 +814,7 @@ absl::Status XlaCompiler::CompileFunction(\n     const NameAttrList& fn_name_attrs,\n     absl::Span<const XlaCompiler::Argument> args,\n     XlaCompiler::CompilationResult* result) {\n-  string function_id =\n+  std::string function_id =\n       Canonicalize(fn_name_attrs.name(), AttrSlice(&fn_name_attrs.attr()));\n   VLOG(1) << \"XlaCompiler::CompileFunction \" << function_id;\n \n@@ -1325,7 +1325,7 @@ namespace {\n absl::Status ValidateFunctionDef(const FunctionDef* fdef,\n                                  const FunctionLibraryDefinition& flib_def) {\n   for (const NodeDef& node : fdef->node_def()) {\n-    const string& op = node.op();\n+    const std::string& op = node.op();\n     if (op == FunctionLibraryDefinition::kGradientOp || flib_def.Find(op)) {\n       continue;\n     }\n@@ -1340,7 +1340,8 @@ absl::Status ValidateFunctionDef(const FunctionDef* fdef,\n // Returned pointer points to the internal string either in node's attributes\n // or in its NodeDef. This pointer is valid as long as the node has not been\n // modified.\n-absl::Status GetPotentialFunctionName(const Node& node, const string** name) {\n+absl::Status GetPotentialFunctionName(const Node& node,\n+                                      const std::string** name) {\n   if (node.IsPartitionedCall()) {\n     const AttrValue* attr_value;\n     TF_RETURN_IF_ERROR(\n@@ -1361,7 +1362,8 @@ absl::Status GetPotentialFunctionName(const Node& node, const string** name) {\n // given device_type, invalid data type, missing attributes...)\n absl::Status ValidateGraph(const Graph* graph,\n                            const FunctionLibraryDefinition& flib_def,\n-                           const DeviceType& device_type, const string& name) {\n+                           const DeviceType& device_type,\n+                           const std::string& name) {\n   // Make sure the XLA compilation kernels are registered.  This operation is\n   // idempotent so it is fine if someone called it already.\n   XlaOpRegistry::RegisterCompilationKernels();\n@@ -1398,7 +1400,7 @@ absl::Status ValidateGraph(const Graph* graph,\n     if (node->type_string() == FunctionLibraryDefinition::kGradientOp) {\n       continue;\n     }\n-    const string* function_name;\n+    const std::string* function_name;\n     TF_RETURN_IF_ERROR(GetPotentialFunctionName(*node, &function_name));\n     const FunctionDef* fdef = flib_def.Find(*function_name);\n     absl::Status s;\n@@ -1487,7 +1489,7 @@ void IncreasePrecisionsToAvoidTF32(xla::HloModuleProto& module) {\n }  // namespace\n \n absl::Status XlaCompiler::CompileGraph(\n-    const XlaCompiler::CompileOptions& options, string const& name,\n+    const XlaCompiler::CompileOptions& options, const std::string& name,\n     std::unique_ptr<Graph> graph, absl::Span<const XlaCompiler::Argument> args,\n     CompilationResult* result) {\n   VLOG(1) << \"Executing graph symbolically to populate XlaBuilder.: \" << name;\n@@ -1689,7 +1691,7 @@ xla::ChannelHandle XlaCompiler::NewChannel(\n   return new_handle;\n }\n \n-absl::Status XlaCompiler::GetChannelHandle(const string& key,\n+absl::Status XlaCompiler::GetChannelHandle(const std::string& key,\n                                            xla::ChannelHandle* channel) {\n   auto result = channels_.emplace(key, xla::ChannelHandle());\n   if (result.second) {\n@@ -1701,7 +1703,7 @@ absl::Status XlaCompiler::GetChannelHandle(const string& key,\n }\n \n absl::Status XlaCompiler::GetHostToDeviceChannelHandle(\n-    const string& key, xla::ChannelHandle* channel) {\n+    const std::string& key, xla::ChannelHandle* channel) {\n   auto result = channels_.emplace(key, xla::ChannelHandle());\n   if (result.second) {\n     result.first->second = NewChannel(xla::ChannelHandle::HOST_TO_DEVICE);\n@@ -1712,7 +1714,7 @@ absl::Status XlaCompiler::GetHostToDeviceChannelHandle(\n }\n \n absl::Status XlaCompiler::GetDeviceToHostChannelHandle(\n-    const string& key, xla::ChannelHandle* channel) {\n+    const std::string& key, xla::ChannelHandle* channel) {\n   auto result = channels_.emplace(key, xla::ChannelHandle());\n   if (result.second) {\n     result.first->second = NewChannel(xla::ChannelHandle::DEVICE_TO_HOST);\n@@ -1724,7 +1726,7 @@ absl::Status XlaCompiler::GetDeviceToHostChannelHandle(\n \n namespace {\n \n-void SetTransfer(const string& key, absl::Span<const DataType> types,\n+void SetTransfer(const std::string& key, absl::Span<const DataType> types,\n                  absl::Span<const TensorShape> shapes,\n                  tf2xla::HostTransferMetadata* transfer) {\n   transfer->set_key(key);\n@@ -1739,7 +1741,7 @@ void SetTransfer(const string& key, absl::Span<const DataType> types,\n }  // namespace\n \n absl::Status XlaCompiler::SetDeviceToHostMetadata(\n-    const string& key, absl::Span<const DataType> types,\n+    const std::string& key, absl::Span<const DataType> types,\n     absl::Span<const TensorShape> shapes) {\n   if (host_compute_sends_.find(key) != host_compute_sends_.end()) {\n     tf2xla::HostTransferMetadata& existing_transfer = host_compute_sends_[key];\n@@ -1759,7 +1761,7 @@ absl::Status XlaCompiler::SetDeviceToHostMetadata(\n }\n \n absl::Status XlaCompiler::GetDeviceToHostShapes(\n-    const string& key, std::vector<TensorShape>* shapes) const {\n+    const std::string& key, std::vector<TensorShape>* shapes) const {\n   const auto iter = host_compute_sends_.find(key);\n   if (iter == host_compute_sends_.end()) {\n     return errors::InvalidArgument(\n@@ -1774,7 +1776,7 @@ absl::Status XlaCompiler::GetDeviceToHostShapes(\n }\n \n absl::Status XlaCompiler::SetHostToDeviceMetadata(\n-    const string& key, absl::Span<const DataType> types,\n+    const std::string& key, absl::Span<const DataType> types,\n     absl::Span<const TensorShape> shapes) {\n   if (host_compute_recvs_.find(key) != host_compute_recvs_.end()) {\n     tf2xla::HostTransferMetadata& existing_transfer = host_compute_recvs_[key];\n@@ -1794,7 +1796,7 @@ absl::Status XlaCompiler::SetHostToDeviceMetadata(\n }\n \n absl::Status XlaCompiler::GetHostComputeControlDependency(\n-    const string& host_compute_name, xla::XlaOp* handle) {\n+    const std::string& host_compute_name, xla::XlaOp* handle) {\n   const auto iter = host_compute_control_output_.find(host_compute_name);\n   if (iter == host_compute_control_output_.end()) {\n     return errors::InvalidArgument(\n@@ -1807,7 +1809,7 @@ absl::Status XlaCompiler::GetHostComputeControlDependency(\n }\n \n absl::Status XlaCompiler::SetHostComputeControlDependency(\n-    const string& host_compute_name, const xla::XlaOp handle) {\n+    const std::string& host_compute_name, const xla::XlaOp handle) {\n   if (host_compute_control_output_.find(host_compute_name) !=\n       host_compute_control_output_.end()) {\n     return errors::InvalidArgument(\n@@ -1819,7 +1821,7 @@ absl::Status XlaCompiler::SetHostComputeControlDependency(\n }\n \n void XlaCompiler::PushNodeTokenMapping() {\n-  node_token_mapping_stack_.emplace(std::map<string, xla::XlaOp>{});\n+  node_token_mapping_stack_.emplace(std::map<std::string, xla::XlaOp>{});\n }\n \n absl::Status XlaCompiler::PopNodeTokenMapping() {\n@@ -1832,7 +1834,7 @@ absl::Status XlaCompiler::PopNodeTokenMapping() {\n   return absl::OkStatus();\n }\n \n-absl::Status XlaCompiler::SetNodeToken(const string& node_name,\n+absl::Status XlaCompiler::SetNodeToken(const std::string& node_name,\n                                        const xla::XlaOp op) {\n   if (node_token_mapping_stack_.empty()) {\n     return errors::FailedPrecondition(\n@@ -1847,7 +1849,8 @@ absl::Status XlaCompiler::SetNodeToken(const string& node_name,\n   return absl::OkStatus();\n }\n \n-absl::StatusOr<xla::XlaOp> XlaCompiler::GetNodeToken(const string& node_name) {\n+absl::StatusOr<xla::XlaOp> XlaCompiler::GetNodeToken(\n+    const std::string& node_name) {\n   if (node_token_mapping_stack_.empty()) {\n     return errors::FailedPrecondition(\n         \"Calling GetNodeToken() when node_token_mapping_stack_ is \""
        },
        {
            "sha": "216125f9cb153e557c777fad503ee5274774ce16",
            "filename": "tensorflow/compiler/tf2xla/xla_compiler.h",
            "status": "modified",
            "additions": 25,
            "deletions": 21,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiler.h?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -277,7 +277,8 @@ class XlaCompiler {\n   // Compiles a tensorflow::Graph into an xla::XlaComputation.\n   // Similar to CompileFunction, but takes a Graph as input rather than a\n   // function.\n-  absl::Status CompileGraph(const CompileOptions& options, string const& name,\n+  absl::Status CompileGraph(const CompileOptions& options,\n+                            const std::string& name,\n                             std::unique_ptr<Graph> graph,\n                             absl::Span<const Argument> args,\n                             CompilationResult* result);\n@@ -295,31 +296,32 @@ class XlaCompiler {\n   // Channel handles can be used to communicate between different\n   // computations. Computations that communicate should be compiled with the\n   // same XlaCompiler.\n-  absl::Status GetChannelHandle(const string& key, xla::ChannelHandle* channel);\n+  absl::Status GetChannelHandle(const std::string& key,\n+                                xla::ChannelHandle* channel);\n \n   // Retrieves the host-to-device channel handle associated with `key`.\n   // Allocates a new channel handle if none exists.\n-  absl::Status GetHostToDeviceChannelHandle(const string& key,\n+  absl::Status GetHostToDeviceChannelHandle(const std::string& key,\n                                             xla::ChannelHandle* channel);\n \n   // Retrieves the device-to-host channel handle associated with `key`.\n   // Allocates a new channel handle if none exists.\n-  absl::Status GetDeviceToHostChannelHandle(const string& key,\n+  absl::Status GetDeviceToHostChannelHandle(const std::string& key,\n                                             xla::ChannelHandle* channel);\n \n   // Sets the shapes and types for the device to host transfer associated with\n   // 'key'.\n-  absl::Status SetDeviceToHostMetadata(const string& key,\n+  absl::Status SetDeviceToHostMetadata(const std::string& key,\n                                        absl::Span<const DataType> types,\n                                        absl::Span<const TensorShape> shapes);\n \n   // Gets the shapes the device to host transfer associated with 'key'.\n-  absl::Status GetDeviceToHostShapes(const string& key,\n+  absl::Status GetDeviceToHostShapes(const std::string& key,\n                                      std::vector<TensorShape>* shapes) const;\n \n   // Sets the shapes and types for the host to device transfer associated with\n   // 'key'.\n-  absl::Status SetHostToDeviceMetadata(const string& key,\n+  absl::Status SetHostToDeviceMetadata(const std::string& key,\n                                        absl::Span<const DataType> types,\n                                        absl::Span<const TensorShape> shapes);\n \n@@ -334,19 +336,19 @@ class XlaCompiler {\n   // 'host_compute_name' can be any string the client wishes to use to identify\n   // a given HostCompute Op as long as the names are unique within the\n   // compilation.\n-  absl::Status GetHostComputeControlDependency(const string& host_compute_name,\n-                                               xla::XlaOp* handle);\n-  absl::Status SetHostComputeControlDependency(const string& host_compute_name,\n-                                               xla::XlaOp handle);\n+  absl::Status GetHostComputeControlDependency(\n+      const std::string& host_compute_name, xla::XlaOp* handle);\n+  absl::Status SetHostComputeControlDependency(\n+      const std::string& host_compute_name, xla::XlaOp handle);\n \n   const Options& options() const { return options_; }\n   xla::Client* client() const { return options_.client; }\n   FunctionLibraryRuntime* flib_runtime() const { return flib_runtime_; }\n \n   void PushNodeTokenMapping();\n   absl::Status PopNodeTokenMapping();\n-  absl::Status SetNodeToken(const string& node_name, xla::XlaOp op);\n-  absl::StatusOr<xla::XlaOp> GetNodeToken(const string& node_name);\n+  absl::Status SetNodeToken(const std::string& node_name, xla::XlaOp op);\n+  absl::StatusOr<xla::XlaOp> GetNodeToken(const std::string& node_name);\n \n   // Sets the function body `fbody` to the one registered as `function`.\n   absl::Status FindFunctionBody(const NameAttrList& function,\n@@ -405,20 +407,22 @@ class XlaCompiler {\n   FunctionLibraryRuntime* flib_runtime_;        // owned by pflr_.\n \n   struct SignatureHash {\n-    uint64 operator()(\n-        const std::pair<string, std::vector<Argument>>& signature) const;\n+    uint64_t operator()(\n+        const std::pair<std::string, std::vector<Argument>>& signature) const;\n   };\n \n-  std::unordered_map<std::pair<string, std::vector<Argument>>,\n+  std::unordered_map<std::pair<std::string, std::vector<Argument>>,\n                      CompilationResult, SignatureHash>\n       cache_;\n \n-  std::unordered_map<string, xla::ChannelHandle> channels_;\n+  std::unordered_map<std::string, xla::ChannelHandle> channels_;\n \n-  std::unordered_map<string, tf2xla::HostTransferMetadata> host_compute_sends_;\n-  std::unordered_map<string, tf2xla::HostTransferMetadata> host_compute_recvs_;\n+  std::unordered_map<std::string, tf2xla::HostTransferMetadata>\n+      host_compute_sends_;\n+  std::unordered_map<std::string, tf2xla::HostTransferMetadata>\n+      host_compute_recvs_;\n \n-  std::unordered_map<string, xla::XlaOp> host_compute_control_output_;\n+  std::unordered_map<std::string, xla::XlaOp> host_compute_control_output_;\n \n   // This is used to store <node name, token output> mapping. Side-effecting\n   // ops call SetNodeToken() to record its token output, so later side-effecting\n@@ -427,7 +431,7 @@ class XlaCompiler {\n   // It's a stack because we need a mapping like this for each level of nested\n   // CompileGraph() call. In CompileGraph(), we will push a new mapping to the\n   // stack, and pop the mapping before returning.\n-  std::stack<std::map<string, xla::XlaOp>> node_token_mapping_stack_;\n+  std::stack<std::map<std::string, xla::XlaOp>> node_token_mapping_stack_;\n \n   XlaCompiler(const XlaCompiler&) = delete;\n   void operator=(const XlaCompiler&) = delete;"
        },
        {
            "sha": "2c149eacda678e5c32e82c838a21a8182f058229",
            "filename": "tensorflow/compiler/tf2xla/xla_compiler_test.cc",
            "status": "modified",
            "additions": 64,
            "deletions": 60,
            "changes": 124,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_compiler_test.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -140,7 +140,7 @@ namespace {\n // compiled kernels.\n class DummyResourceForTest : public ResourceBase {\n  public:\n-  string DebugString() const override { return \"dummy\"; }\n+  std::string DebugString() const override { return \"dummy\"; }\n   void Increment() { ++value_; }\n   int Get() { return value_; }\n \n@@ -268,8 +268,8 @@ TEST_F(XlaCompilerTest, Simple) {\n                                      std::move(graph), args, &result));\n \n   // Tests that the generated computation works.\n-  xla::Literal param0_literal = xla::LiteralUtil::CreateR1<int32>({7, 42});\n-  xla::Literal param1_literal = xla::LiteralUtil::CreateR1<int32>({-3, 101});\n+  xla::Literal param0_literal = xla::LiteralUtil::CreateR1<int32_t>({7, 42});\n+  xla::Literal param1_literal = xla::LiteralUtil::CreateR1<int32_t>({-3, 101});\n   std::unique_ptr<xla::GlobalData> param0_data =\n       client_->TransferToServer(param0_literal).value();\n   std::unique_ptr<xla::GlobalData> param1_data =\n@@ -281,7 +281,7 @@ TEST_F(XlaCompilerTest, Simple) {\n           .value();\n   xla::Literal actual_literal = client_->Transfer(*actual).value();\n \n-  xla::Literal expected0 = xla::LiteralUtil::CreateR1<int32>({4, 143});\n+  xla::Literal expected0 = xla::LiteralUtil::CreateR1<int32_t>({4, 143});\n   xla::Literal expected_literal = xla::LiteralUtil::MakeTuple({&expected0});\n   EXPECT_TRUE(xla::LiteralTestUtil::Equal(expected_literal, actual_literal));\n }\n@@ -366,8 +366,8 @@ TEST_F(XlaCompilerTest, OutOfOrderGraph) {\n                                      args, &result));\n \n   // Tests that the generated computation works.\n-  xla::Literal param0_literal = xla::LiteralUtil::CreateR1<int32>({7, 42});\n-  xla::Literal param1_literal = xla::LiteralUtil::CreateR1<int32>({-3, 101});\n+  xla::Literal param0_literal = xla::LiteralUtil::CreateR1<int32_t>({7, 42});\n+  xla::Literal param1_literal = xla::LiteralUtil::CreateR1<int32_t>({-3, 101});\n   std::unique_ptr<xla::GlobalData> param0_data =\n       client_->TransferToServer(param0_literal).value();\n   std::unique_ptr<xla::GlobalData> param1_data =\n@@ -484,7 +484,7 @@ TEST_F(XlaCompilerTest, HonorShapeRepresentationFnForRetVal) {\n   auto read = ops::ReadVariableOp(\n       scope.WithControlDependencies(std::vector<Operation>{write}), var,\n       DT_INT32);\n-  auto read_plus_one = ops::Add(scope, read, ops::Const<int32>(scope, 1));\n+  auto read_plus_one = ops::Add(scope, read, ops::Const<int32_t>(scope, 1));\n   auto d = ops::_Retval(scope.WithOpName(\"D\"), read_plus_one, 0);\n   std::unique_ptr<Graph> graph(new Graph(OpRegistry::Global()));\n   TF_ASSERT_OK(scope.ToGraph(graph.get()));\n@@ -602,7 +602,7 @@ TEST_F(XlaCompilerTest, MixedOrderArguments) {\n     auto read = ops::ReadVariableOp(\n         scope.WithControlDependencies(std::vector<Operation>{write}), var,\n         DT_INT32);\n-    auto read_plus_one = ops::Add(scope, read, ops::Const<int32>(scope, 1));\n+    auto read_plus_one = ops::Add(scope, read, ops::Const<int32_t>(scope, 1));\n     auto d = ops::_Retval(scope.WithOpName(\"D\"), read_plus_one, 0);\n     std::unique_ptr<Graph> graph(new Graph(OpRegistry::Global()));\n     TF_ASSERT_OK(scope.ToGraph(graph.get()));\n@@ -680,7 +680,7 @@ TEST_F(XlaCompilerTest, ConstantOutputs) {\n   // func(a) { b=7; c=-a; return b, c; }\n   Scope scope = Scope::NewRootScope().ExitOnError();\n   auto a = ops::_Arg(scope.WithOpName(\"A\"), DT_INT32, 0);\n-  auto b = ops::Const<int32>(scope.WithOpName(\"B\"), 7);\n+  auto b = ops::Const<int32_t>(scope.WithOpName(\"B\"), 7);\n   auto c = ops::Neg(scope.WithOpName(\"C\"), a);\n   auto d = ops::_Retval(scope.WithOpName(\"D\"), b, 0);\n   auto e = ops::_Retval(scope.WithOpName(\"E\"), c, 1);\n@@ -710,16 +710,16 @@ TEST_F(XlaCompilerTest, ConstantOutputs) {\n     EXPECT_FALSE(result.outputs[1].is_constant);\n \n     // Tests that the generated computation works.\n-    xla::Literal param0_literal = xla::LiteralUtil::CreateR1<int32>({7, 42});\n+    xla::Literal param0_literal = xla::LiteralUtil::CreateR1<int32_t>({7, 42});\n     std::unique_ptr<xla::GlobalData> param0_data =\n         client_->TransferToServer(param0_literal).value();\n \n     std::unique_ptr<xla::GlobalData> actual =\n         client_->Execute(*result.computation, {param0_data.get()}).value();\n     xla::Literal actual_literal = client_->Transfer(*actual).value();\n \n-    xla::Literal expected0 = xla::LiteralUtil::CreateR0<int32>(7);\n-    xla::Literal expected1 = xla::LiteralUtil::CreateR1<int32>({-7, -42});\n+    xla::Literal expected0 = xla::LiteralUtil::CreateR0<int32_t>(7);\n+    xla::Literal expected1 = xla::LiteralUtil::CreateR1<int32_t>({-7, -42});\n     xla::Literal expected =\n         xla::LiteralUtil::MakeTuple({&expected0, &expected1});\n     EXPECT_TRUE(xla::LiteralTestUtil::Equal(expected, actual_literal));\n@@ -885,7 +885,7 @@ TEST_F(XlaCompilerTest, DeterministicCompilation) {\n         // The names of instructions were uniquified by the XlaBuilder and the\n         // unique ids may be different, the rest of the fields should be\n         // identical.\n-        string str1, str2;\n+        std::string str1, str2;\n         LOG(INFO) << \"instr1 = \" << instr1.DebugString();\n         LOG(INFO) << \"instr2 = \" << instr2.DebugString();\n         instr1.AppendPartialToString(&str1);\n@@ -904,7 +904,7 @@ TEST_F(XlaCompilerTest, CanPassTensorArraysToAndFromComputation) {\n   auto flow = ops::Const<float>(scope, {});\n   auto grad1 = ops::TensorArrayGrad(scope, arg, flow, \"grad1\");\n   auto grad2 = ops::TensorArrayGrad(scope, arg, grad1.flow_out, \"grad2\");\n-  auto index = ops::Const<int32>(scope, 1);\n+  auto index = ops::Const<int32_t>(scope, 1);\n   auto write = ops::TensorArrayWrite(scope, grad1.grad_handle, index, index,\n                                      grad2.flow_out);\n   auto read = ops::TensorArrayRead(scope, arg, index, write.flow_out, DT_INT32);\n@@ -933,12 +933,12 @@ TEST_F(XlaCompilerTest, CanPassTensorArraysToAndFromComputation) {\n   const XlaCompiler::ResourceUpdate& update = result.resource_updates[0];\n   EXPECT_EQ(0, update.input_index);\n   EXPECT_EQ(DT_INT32, update.type);\n-  EXPECT_EQ((std::set<string>{\"grad1\", \"grad2\"}),\n+  EXPECT_EQ((std::set<std::string>{\"grad1\", \"grad2\"}),\n             update.tensor_array_gradients_accessed);\n \n   // Tests that the generated computation works.\n-  xla::Literal input_base = xla::LiteralUtil::CreateR1<int32>({7, 42});\n-  xla::Literal input_grad2 = xla::LiteralUtil::CreateR1<int32>({-3, 101});\n+  xla::Literal input_base = xla::LiteralUtil::CreateR1<int32_t>({7, 42});\n+  xla::Literal input_grad2 = xla::LiteralUtil::CreateR1<int32_t>({-3, 101});\n   xla::Literal input = xla::LiteralUtil::MakeTuple({&input_base, &input_grad2});\n   std::unique_ptr<xla::GlobalData> param0_data =\n       client_->TransferToServer(input).value();\n@@ -947,10 +947,10 @@ TEST_F(XlaCompilerTest, CanPassTensorArraysToAndFromComputation) {\n       client_->Execute(*result.computation, {param0_data.get()}).value();\n   xla::Literal actual_literal = client_->Transfer(*actual).value();\n \n-  xla::Literal output_read = xla::LiteralUtil::CreateR0<int32>(42);\n-  xla::Literal output_base = xla::LiteralUtil::CreateR1<int32>({7, 42});\n-  xla::Literal output_grad1 = xla::LiteralUtil::CreateR1<int32>({0, 1});\n-  xla::Literal output_grad2 = xla::LiteralUtil::CreateR1<int32>({-3, 101});\n+  xla::Literal output_read = xla::LiteralUtil::CreateR0<int32_t>(42);\n+  xla::Literal output_base = xla::LiteralUtil::CreateR1<int32_t>({7, 42});\n+  xla::Literal output_grad1 = xla::LiteralUtil::CreateR1<int32_t>({0, 1});\n+  xla::Literal output_grad2 = xla::LiteralUtil::CreateR1<int32_t>({-3, 101});\n   xla::Literal output_resource =\n       xla::LiteralUtil::MakeTuple({&output_base, &output_grad1, &output_grad2});\n   xla::Literal expected_literal =\n@@ -964,7 +964,7 @@ TEST_F(XlaCompilerTest, UnwrittenTensorArrayGradientsAreNotComputationOutputs) {\n   auto arg = ops::_Arg(scope.WithOpName(\"arg\"), DT_RESOURCE, 0);\n   auto flow = ops::Const<float>(scope, {});\n   auto grad1 = ops::TensorArrayGrad(scope, arg, flow, \"grad1\");\n-  auto index = ops::Const<int32>(scope, 1);\n+  auto index = ops::Const<int32_t>(scope, 1);\n   auto read = ops::TensorArrayRead(scope, arg, index, grad1.flow_out, DT_INT32);\n   auto retval = ops::_Retval(scope.WithOpName(\"retval\"), read, 0);\n   std::unique_ptr<Graph> graph(new Graph(OpRegistry::Global()));\n@@ -996,7 +996,7 @@ TEST_F(XlaCompilerTest, NewTensorArrayGradientsAreComputationOutputs) {\n   auto arg = ops::_Arg(scope.WithOpName(\"arg\"), DT_RESOURCE, 0);\n   auto flow = ops::Const<float>(scope, {});\n   auto grad1 = ops::TensorArrayGrad(scope, arg, flow, \"grad2\");\n-  auto index = ops::Const<int32>(scope, 1);\n+  auto index = ops::Const<int32_t>(scope, 1);\n   auto read = ops::TensorArrayRead(scope, arg, index, grad1.flow_out, DT_INT32);\n   auto retval = ops::_Retval(scope.WithOpName(\"retval\"), read, 0);\n   std::unique_ptr<Graph> graph(new Graph(OpRegistry::Global()));\n@@ -1067,8 +1067,8 @@ TEST_F(XlaCompilerTest, FunctionCallWithConstants) {\n   std::unique_ptr<Graph> graph(new Graph(OpRegistry::Global()));\n \n   Scope scope = Scope::NewRootScope().ExitOnError();\n-  auto value = ops::Const<int32>(scope.WithOpName(\"value\"), 1, {});\n-  auto shape = ops::Const<int32>(scope.WithOpName(\"shape\"), {5}, {1});\n+  auto value = ops::Const<int32_t>(scope.WithOpName(\"value\"), 1, {});\n+  auto shape = ops::Const<int32_t>(scope.WithOpName(\"shape\"), {5}, {1});\n   TF_EXPECT_OK(scope.graph()->AddFunctionLibrary(flib));\n \n   NodeDef def;\n@@ -1151,9 +1151,9 @@ TEST_F(XlaCompilerTest, SliceWithDynamicBegins) {\n   std::unique_ptr<Graph> graph(new Graph(OpRegistry::Global()));\n \n   Scope scope = Scope::NewRootScope().ExitOnError();\n-  auto value = ops::Const<int32>(scope.WithOpName(\"shape\"), {5}, {1});\n+  auto value = ops::Const<int32_t>(scope.WithOpName(\"shape\"), {5}, {1});\n   auto begin = ops::_Arg(scope.WithOpName(\"arg\"), DT_INT32, 0);\n-  auto size = ops::Const<int32>(scope.WithOpName(\"value\"), {1}, {1});\n+  auto size = ops::Const<int32_t>(scope.WithOpName(\"value\"), {1}, {1});\n \n   TF_EXPECT_OK(scope.graph()->AddFunctionLibrary(flib));\n \n@@ -1188,8 +1188,8 @@ TEST_F(XlaCompilerTest, SliceWithDynamicBegins) {\n \n void RunAndCheckVariablesComputation(\n     xla::Client* client, const XlaCompiler::CompilationResult& result) {\n-  xla::Literal param0_literal = xla::LiteralUtil::CreateR1<int32>({7, 42});\n-  xla::Literal param1_literal = xla::LiteralUtil::CreateR1<int32>({-3, 101});\n+  xla::Literal param0_literal = xla::LiteralUtil::CreateR1<int32_t>({7, 42});\n+  xla::Literal param1_literal = xla::LiteralUtil::CreateR1<int32_t>({-3, 101});\n   std::unique_ptr<xla::GlobalData> param0_data =\n       client->TransferToServer(param0_literal).value();\n   std::unique_ptr<xla::GlobalData> param1_data =\n@@ -1201,8 +1201,8 @@ void RunAndCheckVariablesComputation(\n           .value();\n   xla::Literal actual_literal = client->Transfer(*actual).value();\n \n-  xla::Literal expected0 = xla::LiteralUtil::CreateR1<int32>({5, 144});\n-  xla::Literal expected1 = xla::LiteralUtil::CreateR1<int32>({4, 143});\n+  xla::Literal expected0 = xla::LiteralUtil::CreateR1<int32_t>({5, 144});\n+  xla::Literal expected1 = xla::LiteralUtil::CreateR1<int32_t>({4, 143});\n   xla::Literal expected_literal =\n       xla::LiteralUtil::MakeTuple({&expected0, &expected1});\n   EXPECT_TRUE(xla::LiteralTestUtil::Equal(expected_literal, actual_literal));\n@@ -1220,7 +1220,7 @@ TEST_F(XlaCompilerTest, Variables) {\n   auto read = ops::ReadVariableOp(\n       scope.WithControlDependencies(std::vector<Operation>{write}), var,\n       DT_INT32);\n-  auto read_plus_one = ops::Add(scope, read, ops::Const<int32>(scope, 1));\n+  auto read_plus_one = ops::Add(scope, read, ops::Const<int32_t>(scope, 1));\n   auto d = ops::_Retval(scope.WithOpName(\"D\"), read_plus_one, 0);\n   std::unique_ptr<Graph> graph(new Graph(OpRegistry::Global()));\n   TF_ASSERT_OK(scope.ToGraph(graph.get()));\n@@ -1356,7 +1356,7 @@ TEST_F(XlaCompilerTest, ReturnResourceHandleOnly) {\n                                      std::move(graph), args, &result));\n \n   // Tests that the generated computation works.\n-  xla::Literal param1_literal = xla::LiteralUtil::CreateR1<int32>({-3, 101});\n+  xla::Literal param1_literal = xla::LiteralUtil::CreateR1<int32_t>({-3, 101});\n   std::unique_ptr<xla::GlobalData> param1_data =\n       client_->TransferToServer(param1_literal).value();\n \n@@ -1379,7 +1379,7 @@ TEST_F(XlaCompilerTest, ReturnResourceHandle) {\n   auto read = ops::ReadVariableOp(\n       scope.WithControlDependencies(std::vector<Operation>{write}), var,\n       DT_INT32);\n-  auto read_plus_one = ops::Add(scope, read, ops::Const<int32>(scope, 1));\n+  auto read_plus_one = ops::Add(scope, read, ops::Const<int32_t>(scope, 1));\n   auto r = ops::_Retval(scope.WithOpName(\"R\"), var, 0);\n   auto d = ops::_Retval(scope.WithOpName(\"D\"), read_plus_one, 1);\n \n@@ -1414,7 +1414,7 @@ absl::StatusOr<std::unique_ptr<Graph>> BuildTestGraph() {\n   auto read = ops::ReadVariableOp(\n       scope.WithControlDependencies(std::vector<Operation>{write}), var,\n       DT_INT32);\n-  auto read_plus_one = ops::Add(scope, read, ops::Const<int32>(scope, 1));\n+  auto read_plus_one = ops::Add(scope, read, ops::Const<int32_t>(scope, 1));\n   auto d = ops::_Retval(scope.WithOpName(\"D\"), read_plus_one, 0);\n   std::unique_ptr<Graph> graph(new Graph(OpRegistry::Global()));\n   TF_RETURN_IF_ERROR(scope.ToGraph(graph.get()));\n@@ -1475,9 +1475,9 @@ TEST_F(XlaCompilerTest, VariableRepresentationShapeFunction) {\n \n   // Tests that the generated computation works.\n   xla::Literal param0_literal =\n-      xla::LiteralUtil::CreateR2<int32>({{4, 55}, {1, -3}});\n+      xla::LiteralUtil::CreateR2<int32_t>({{4, 55}, {1, -3}});\n   xla::Literal param1_literal =\n-      xla::LiteralUtil::CreateR1<int32>({22, 11, 33, 404});\n+      xla::LiteralUtil::CreateR1<int32_t>({22, 11, 33, 404});\n   std::unique_ptr<xla::GlobalData> param0_data =\n       client_->TransferToServer(param0_literal).value();\n   std::unique_ptr<xla::GlobalData> param1_data =\n@@ -1490,8 +1490,9 @@ TEST_F(XlaCompilerTest, VariableRepresentationShapeFunction) {\n   xla::Literal actual_literal = client_->Transfer(*actual).value();\n \n   xla::Literal expected0 =\n-      xla::LiteralUtil::CreateR2<int32>({{27, 67}, {35, 402}});\n-  xla::Literal expected1 = xla::LiteralUtil::CreateR1<int32>({26, 66, 34, 401});\n+      xla::LiteralUtil::CreateR2<int32_t>({{27, 67}, {35, 402}});\n+  xla::Literal expected1 =\n+      xla::LiteralUtil::CreateR1<int32_t>({26, 66, 34, 401});\n   xla::Literal expected_literal =\n       xla::LiteralUtil::MakeTuple({&expected0, &expected1});\n   EXPECT_TRUE(xla::LiteralTestUtil::Equal(expected_literal, actual_literal));\n@@ -1547,9 +1548,9 @@ TEST_F(XlaCompilerTest, ArgRetvalShapeRepresentationFunction) {\n \n   // Tests that the generated computation works.\n   xla::Literal param0_literal =\n-      xla::LiteralUtil::CreateR1<int32>({4, 55, 1, -3});\n+      xla::LiteralUtil::CreateR1<int32_t>({4, 55, 1, -3});\n   xla::Literal param1_literal =\n-      xla::LiteralUtil::CreateR1<int32>({22, 11, 33, 404});\n+      xla::LiteralUtil::CreateR1<int32_t>({22, 11, 33, 404});\n   std::unique_ptr<xla::GlobalData> param0_data =\n       client_->TransferToServer(param0_literal).value();\n   std::unique_ptr<xla::GlobalData> param1_data =\n@@ -1561,8 +1562,10 @@ TEST_F(XlaCompilerTest, ArgRetvalShapeRepresentationFunction) {\n           .value();\n   xla::Literal actual_literal = client_->Transfer(*actual).value();\n \n-  xla::Literal expected0 = xla::LiteralUtil::CreateR1<int32>({27, 67, 35, 402});\n-  xla::Literal expected1 = xla::LiteralUtil::CreateR1<int32>({26, 66, 34, 401});\n+  xla::Literal expected0 =\n+      xla::LiteralUtil::CreateR1<int32_t>({27, 67, 35, 402});\n+  xla::Literal expected1 =\n+      xla::LiteralUtil::CreateR1<int32_t>({26, 66, 34, 401});\n   xla::Literal expected_literal =\n       xla::LiteralUtil::MakeTuple({&expected0, &expected1});\n   EXPECT_TRUE(xla::LiteralTestUtil::Equal(expected_literal, actual_literal));\n@@ -1587,8 +1590,8 @@ TEST_F(XlaCompilerTest, FunctionWithInvalidOp) {\n   std::unique_ptr<Graph> graph(new Graph(OpRegistry::Global()));\n \n   Scope scope = Scope::NewRootScope().ExitOnError();\n-  auto value = ops::Const<int32>(scope.WithOpName(\"value\"), 1, {});\n-  auto shape = ops::Const<int32>(scope.WithOpName(\"shape\"), {5}, {1});\n+  auto value = ops::Const<int32_t>(scope.WithOpName(\"value\"), 1, {});\n+  auto shape = ops::Const<int32_t>(scope.WithOpName(\"shape\"), {5}, {1});\n   TF_ASSERT_OK(scope.graph()->AddFunctionLibrary(flib));\n \n   NodeDef def;\n@@ -1684,7 +1687,8 @@ TEST_F(XlaCompilerTest, TokenInputAndOutput) {\n   side_effecting_op.set_name(\"DummySideEffectingOp\");\n   side_effecting_op.set_op(\"DummySideEffectingOp\");\n   AddNodeAttr(kXlaTokenInputNodesAttrName,\n-              std::vector<string>{kXlaTokenArgNodeName}, &side_effecting_op);\n+              std::vector<std::string>{kXlaTokenArgNodeName},\n+              &side_effecting_op);\n   AddNodeAttr(kXlaOriginalOutsideCompilationNodeName, side_effecting_op.name(),\n               &side_effecting_op);\n   absl::Status status;\n@@ -1768,8 +1772,8 @@ TEST_F(XlaCompilerTest, OpsWithTensorListInput) {\n   }\n \n   Scope scope = Scope::NewRootScope().ExitOnError();\n-  auto element_shape = ops::Const<int32>(scope, {1}, {1});\n-  auto max_elements = ops::Const<int32>(scope, {10}, {});\n+  auto element_shape = ops::Const<int32_t>(scope, {1}, {1});\n+  auto max_elements = ops::Const<int32_t>(scope, {10}, {});\n   auto arg = ops::_Arg(scope.WithOpName(\"arg\"), DT_VARIANT, 0);\n   std::initializer_list<Output> out = {arg, arg};\n   auto add_n = ops::AddN(scope, out);\n@@ -1822,7 +1826,7 @@ TEST_F(XlaCompilerTest, WhileWithResources) {\n     auto arg0 = ops::_Arg(scope.WithOpName(\"arg0\"), DT_INT32, 0);\n     auto arg1 = ops::_Arg(scope.WithOpName(\"arg1\"), DT_RESOURCE, 1);\n     auto arg2 = ops::_Arg(scope.WithOpName(\"arg2\"), DT_RESOURCE, 2);\n-    auto less = ops::Less(scope, arg0, ops::Const<int32>(scope, 10));\n+    auto less = ops::Less(scope, arg0, ops::Const<int32_t>(scope, 10));\n     (void)ops::_Retval(scope.WithOpName(\"ret\"), less, 0);\n     TF_ASSERT_OK(scope.ToGraph(graph.get()));\n     FunctionDef fdef;\n@@ -1899,9 +1903,9 @@ TEST_F(XlaCompilerTest, WhileWithResources) {\n   ASSERT_EQ(output2.input_index, 2);\n \n   // Tests that the generated computation works.\n-  xla::Literal literal0 = xla::LiteralUtil::CreateR0<int32>(0);\n-  xla::Literal literal1 = xla::LiteralUtil::CreateR0<int32>(2);\n-  xla::Literal literal2 = xla::LiteralUtil::CreateR0<int32>(1);\n+  xla::Literal literal0 = xla::LiteralUtil::CreateR0<int32_t>(0);\n+  xla::Literal literal1 = xla::LiteralUtil::CreateR0<int32_t>(2);\n+  xla::Literal literal2 = xla::LiteralUtil::CreateR0<int32_t>(1);\n   std::unique_ptr<xla::GlobalData> data0 =\n       client_->TransferToServer(literal0).value();\n   std::unique_ptr<xla::GlobalData> data1 =\n@@ -1916,9 +1920,9 @@ TEST_F(XlaCompilerTest, WhileWithResources) {\n           .value();\n   xla::Literal actual_literal = client_->Transfer(*actual).value();\n \n-  xla::Literal expected0 = xla::LiteralUtil::CreateR0<int32>(10);\n-  xla::Literal expected1 = xla::LiteralUtil::CreateR0<int32>(2);\n-  xla::Literal expected2 = xla::LiteralUtil::CreateR0<int32>(1);\n+  xla::Literal expected0 = xla::LiteralUtil::CreateR0<int32_t>(10);\n+  xla::Literal expected1 = xla::LiteralUtil::CreateR0<int32_t>(2);\n+  xla::Literal expected2 = xla::LiteralUtil::CreateR0<int32_t>(1);\n   xla::Literal expected_literal =\n       xla::LiteralUtil::MakeTuple({&expected0, &expected1, &expected2});\n   EXPECT_TRUE(xla::LiteralTestUtil::Equal(expected_literal, actual_literal));\n@@ -1978,7 +1982,7 @@ TEST_F(XlaCompilerTest, SetShardingForReturnedTuple) {\n \n TEST_F(XlaCompilerTest, AliasResourceUpdates) {\n   Scope scope = Scope::NewRootScope().ExitOnError();\n-  auto a = ops::Const<int32>(scope.WithOpName(\"A\"), {1, 2});\n+  auto a = ops::Const<int32_t>(scope.WithOpName(\"A\"), {1, 2});\n   auto var = ops::_Arg(scope.WithOpName(\"V\"), DT_RESOURCE, 1);\n   auto write = ops::AssignAddVariableOp(scope, var, a);\n   auto read = ops::ReadVariableOp(\n@@ -2022,7 +2026,7 @@ TEST_F(XlaCompilerTest, AliasResourceUpdates) {\n TEST_F(XlaCompilerTest, SetDeviceToHostMetadataExactDuplicate) {\n   XlaCompiler compiler(DefaultOptions());\n \n-  const string& key = \"comm_key\";\n+  const std::string& key = \"comm_key\";\n   std::vector<DataType> types{DT_INT32};\n   std::vector<TensorShape> shapes{TensorShape({2})};\n \n@@ -2035,7 +2039,7 @@ TEST_F(XlaCompilerTest, SetDeviceToHostMetadataExactDuplicate) {\n TEST_F(XlaCompilerTest, SetDeviceToHostMetadataMismatchedDuplicate) {\n   XlaCompiler compiler(DefaultOptions());\n \n-  const string& key = \"comm_key\";\n+  const std::string& key = \"comm_key\";\n   std::vector<DataType> types{DT_INT32};\n   std::vector<TensorShape> shapes{TensorShape({2})};\n   std::vector<DataType> types2{DT_FLOAT};\n@@ -2051,7 +2055,7 @@ TEST_F(XlaCompilerTest, SetDeviceToHostMetadataMismatchedDuplicate) {\n TEST_F(XlaCompilerTest, SetHostToDeviceMetadataExactDuplicate) {\n   XlaCompiler compiler(DefaultOptions());\n \n-  const string& key = \"comm_key\";\n+  const std::string& key = \"comm_key\";\n   std::vector<DataType> types{DT_INT32};\n   std::vector<TensorShape> shapes{TensorShape({2})};\n \n@@ -2064,7 +2068,7 @@ TEST_F(XlaCompilerTest, SetHostToDeviceMetadataExactDuplicate) {\n TEST_F(XlaCompilerTest, SetHostToDeviceMetadataMismatchedDuplicate) {\n   XlaCompiler compiler(DefaultOptions());\n \n-  const string& key = \"comm_key\";\n+  const std::string& key = \"comm_key\";\n   std::vector<DataType> types{DT_INT32};\n   std::vector<TensorShape> shapes{TensorShape({2})};\n   std::vector<DataType> types2{DT_FLOAT};"
        },
        {
            "sha": "fad607b1ae133395206923cae96185b659da4614",
            "filename": "tensorflow/compiler/tf2xla/xla_context.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_context.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_context.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_context.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -67,7 +67,7 @@ XlaContext::XlaContext(XlaCompiler* compiler, xla::XlaBuilder* builder,\n   }\n }\n \n-string XlaContext::DebugString() const { return \"XLA JIT context\"; }\n+std::string XlaContext::DebugString() const { return \"XLA JIT context\"; }\n \n void XlaContext::SetRetval(int index, const XlaExpression& expression) {\n   const int64_t retvals_size = retvals_.size();\n@@ -84,7 +84,7 @@ XlaResource* XlaContext::AddResource(std::unique_ptr<XlaResource> resource) {\n \n const xla::XlaComputation* XlaContext::GetOrCreateMax(const DataType type) {\n   return LookupOrCreate(type, &max_func_, [type] {\n-    const string type_string = DataTypeString(type);\n+    const std::string type_string = DataTypeString(type);\n     VLOG(1) << \"Building Max() for \" << type_string;\n     xla::XlaBuilder b(\"max<\" + type_string + \">\");\n     xla::PrimitiveType xla_type;\n@@ -100,7 +100,7 @@ const xla::XlaComputation* XlaContext::GetOrCreateMax(const DataType type) {\n \n const xla::XlaComputation* XlaContext::GetOrCreateMin(const DataType type) {\n   return LookupOrCreate(type, &min_func_, [type] {\n-    const string type_string = DataTypeString(type);\n+    const std::string type_string = DataTypeString(type);\n     VLOG(1) << \"Building Min() for \" << type_string;\n     xla::XlaBuilder b(\"min<\" + type_string + \">\");\n     xla::PrimitiveType xla_type;\n@@ -116,7 +116,7 @@ const xla::XlaComputation* XlaContext::GetOrCreateMin(const DataType type) {\n \n const xla::XlaComputation* XlaContext::GetOrCreateAdd(const DataType type) {\n   return LookupOrCreate(type, &add_func_, [type] {\n-    const string type_string = DataTypeString(type);\n+    const std::string type_string = DataTypeString(type);\n     VLOG(1) << \"Building Add() for \" << type_string;\n     xla::XlaBuilder b(\"add<\" + type_string + \">\");\n     xla::PrimitiveType xla_type;\n@@ -133,7 +133,7 @@ const xla::XlaComputation* XlaContext::GetOrCreateAdd(const DataType type) {\n const xla::XlaComputation* XlaContext::GetOrCreateLogAddExp(\n     const DataType type) {\n   return LookupOrCreate(type, &log_add_exp_func_, [type] {\n-    const string type_string = DataTypeString(type);\n+    const std::string type_string = DataTypeString(type);\n     VLOG(1) << \"Building LogAddExp() for \" << type_string;\n     xla::XlaBuilder b(\"log_add_exp<\" + type_string + \">\");\n     xla::PrimitiveType xla_type;\n@@ -154,7 +154,7 @@ const xla::XlaComputation* XlaContext::GetOrCreateLogAddExp(\n \n const xla::XlaComputation* XlaContext::GetOrCreateMul(const DataType type) {\n   return LookupOrCreate(type, &mul_func_, [type] {\n-    const string type_string = DataTypeString(type);\n+    const std::string type_string = DataTypeString(type);\n     VLOG(1) << \"Building Mul() for \" << type_string;\n     xla::XlaBuilder b(\"mul<\" + type_string + \">\");\n     xla::PrimitiveType xla_type;"
        },
        {
            "sha": "1d72f0c756f364f354cf4f29cbead277af13f692",
            "filename": "tensorflow/compiler/tf2xla/xla_context.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_context.h?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -50,7 +50,7 @@ class XlaContext : public ResourceBase {\n              const Graph* graph);\n \n   // Virtual method defined by ResourceBase.\n-  string DebugString() const override;\n+  std::string DebugString() const override;\n \n   XlaCompiler* compiler() const { return compiler_; }\n "
        },
        {
            "sha": "e867dd14209ab805ab3b5fdcdc89dba4583ab3b8",
            "filename": "tensorflow/compiler/tf2xla/xla_expression.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_expression.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_expression.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_expression.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -73,7 +73,7 @@ XlaExpression XlaExpression::Resource(XlaResource* resource) {\n   return e;\n }\n \n-string XlaExpression::HumanString() const {\n+std::string XlaExpression::HumanString() const {\n   switch (kind_) {\n     case Kind::kInvalid:\n       return \"invalid\";"
        },
        {
            "sha": "ed0041fc9942a0f4f439667b543142bc5d017bf9",
            "filename": "tensorflow/compiler/tf2xla/xla_expression.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_expression.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_expression.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_expression.h?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -115,7 +115,7 @@ class XlaExpression {\n   XlaResource* resource() const { return resource_; }\n \n   // Returns a human-readable summary of the expression.\n-  string HumanString() const;\n+  std::string HumanString() const;\n \n   // Returns the value of a kValue or kXlaOp as an xla::XlaOp. Returns\n   // an erroneous XlaOp if the expression is not a constant or an expression."
        },
        {
            "sha": "797002476aeb1c60d42fa4af7e7ac720ead78e66",
            "filename": "tensorflow/compiler/tf2xla/xla_expression_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_expression_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_expression_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_expression_test.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -38,14 +38,15 @@ class XlaExpressionTest : public ::testing::Test {\n   void SetUp() override {\n     client_ = xla::ClientLibrary::LocalClientOrDie();\n     builder_ = std::make_unique<xla::XlaBuilder>(\"acomputation\");\n-    constant_ = test::AsScalar<int32>(42);\n-    op_ = xla::ConstantR0<int32>(builder_.get(), 7);\n+    constant_ = test::AsScalar<int32_t>(42);\n+    op_ = xla::ConstantR0<int32_t>(builder_.get(), 7);\n     non_constant_op_ = xla::Parameter(\n         builder_.get(), 0, xla::ShapeUtil::MakeShape(xla::F32, {}), \"x\");\n     resource_ = std::make_unique<XlaResource>(\n-        XlaResource::kVariable, /*arg_num=*/0, /*name=*/string(\"avariable\"),\n-        DT_INT32, TensorShape({17, 3}), op_, /*tensor_array_size=*/-1,\n-        /*tensor_array_gradients=*/std::set<string>(),\n+        XlaResource::kVariable, /*arg_num=*/0,\n+        /*name=*/std::string(\"avariable\"), DT_INT32, TensorShape({17, 3}), op_,\n+        /*tensor_array_size=*/-1,\n+        /*tensor_array_gradients=*/std::set<std::string>(),\n         /*tensor_array_multiple_writes_aggregate=*/false);\n   }\n \n@@ -87,8 +88,8 @@ TEST_F(XlaExpressionTest, AsXlaOp) {\n                           builder_->BuildConstantSubGraph(const_as_op));\n   TF_ASSERT_OK_AND_ASSIGN(xla::Literal value,\n                           client_->ComputeConstant(computation));\n-  EXPECT_TRUE(xla::LiteralTestUtil::Equal(xla::LiteralUtil::CreateR0<int32>(42),\n-                                          value));\n+  EXPECT_TRUE(xla::LiteralTestUtil::Equal(\n+      xla::LiteralUtil::CreateR0<int32_t>(42), value));\n }\n \n TEST_F(XlaExpressionTest, GetShape) {\n@@ -120,7 +121,7 @@ TEST_F(XlaExpressionTest, ResolveConstant) {\n       std::optional<Tensor> op_constant,\n       XlaExpression::XlaOp(op_, DT_INT32).ResolveConstant(client_));\n   ASSERT_TRUE(op_constant.has_value());\n-  test::ExpectTensorEqual<int32>(test::AsScalar<int32>(7), *op_constant);\n+  test::ExpectTensorEqual<int32_t>(test::AsScalar<int32_t>(7), *op_constant);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::optional<Tensor> op_nonconstant,\n                           XlaExpression::XlaOp(non_constant_op_, DT_FLOAT)\n@@ -131,7 +132,7 @@ TEST_F(XlaExpressionTest, ResolveConstant) {\n       std::optional<Tensor> constant_constant,\n       XlaExpression::Constant(constant_).ResolveConstant(client_));\n   ASSERT_TRUE(constant_constant.has_value());\n-  test::ExpectTensorEqual<int32>(constant_, *constant_constant);\n+  test::ExpectTensorEqual<int32_t>(constant_, *constant_constant);\n }\n \n TEST_F(XlaExpressionTest, ResolveConstantOnResource) {"
        },
        {
            "sha": "0b3425e5b8524a777afcdbba3d794c536b422d04",
            "filename": "tensorflow/compiler/tf2xla/xla_helpers.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_helpers.h?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -136,7 +136,7 @@ struct XlaResourceUpdate {\n   bool modified;\n \n   // If the resource is a TensorArray, the set of gradients read or written.\n-  std::set<string> tensor_array_gradients_accessed;\n+  std::set<std::string> tensor_array_gradients_accessed;\n };\n \n struct XlaCompilationResult {"
        },
        {
            "sha": "b374e8c8e81dd6746d6875d89447a2d82d1a4f00",
            "filename": "tensorflow/compiler/tf2xla/xla_jit_compiled_cpu_function.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_jit_compiled_cpu_function.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_jit_compiled_cpu_function.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_jit_compiled_cpu_function.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -76,12 +76,12 @@ int CountResults(\n // tf2xla::{Feed,Fetch,Variable}. We hold the actual strings in nonempty_names,\n // and hold arrays of pointers in name_ptrs, terminated by a nullptr entry.\n template <typename T>\n-void CollectNames(const T& entries, std::vector<string>* nonempty_names,\n+void CollectNames(const T& entries, std::vector<std::string>* nonempty_names,\n                   std::vector<const char*>* name_ptrs) {\n   // First collect `nonempty_names`, to ensure the underlying strings won't\n   // change out from under us.\n   for (const auto& entry : entries) {\n-    const string& name = entry.name();\n+    const std::string& name = entry.name();\n     if (!name.empty()) {\n       nonempty_names->push_back(name);\n     }\n@@ -90,7 +90,7 @@ void CollectNames(const T& entries, std::vector<string>* nonempty_names,\n   name_ptrs->reserve(entries.size() + 1);  // +1 for nullptr array terminator\n   size_t nonempty_index = 0;\n   for (const auto& entry : entries) {\n-    const string& name = entry.name();\n+    const std::string& name = entry.name();\n     if (!name.empty()) {\n       name_ptrs->push_back(nonempty_names->at(nonempty_index).c_str());\n       ++nonempty_index;\n@@ -158,9 +158,9 @@ XlaJitCompiledCpuFunction::Compile(\n       xla::cpu::CreateBufferAllocationInfos(cpu_executable->module(),\n                                             buffer_assignment);\n \n-  std::vector<int32> arg_index_table =\n+  std::vector<int32_t> arg_index_table =\n       xla::cpu::CreateArgIndexTable(buffer_infos);\n-  std::vector<int32> result_index_table =\n+  std::vector<int32_t> result_index_table =\n       xla::cpu::CreateResultIndexTable(buffer_infos);\n   TF_ASSIGN_OR_RETURN(size_t result_index,\n                       ComputeResultIndex(buffer_assignment));"
        },
        {
            "sha": "6f61f472a2fd5aa392dcfb93a155b63b1b3ab7e7",
            "filename": "tensorflow/compiler/tf2xla/xla_jit_compiled_cpu_function.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_jit_compiled_cpu_function.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_jit_compiled_cpu_function.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_jit_compiled_cpu_function.h?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -86,17 +86,17 @@ class XlaJitCompiledCpuFunction {\n   std::vector<xla::cpu::BufferAllocationInfo> buffer_infos_;\n \n   // The backing array for the arg index table.\n-  std::vector<int32> arg_index_table_;\n+  std::vector<int32_t> arg_index_table_;\n \n   // The backing array for the result index table.\n-  std::vector<int32> result_index_table_;\n+  std::vector<int32_t> result_index_table_;\n \n   // The backing arrays of arg and result names. We hold the actual strings in\n   // nonempty_*_names_, and hold arrays of pointers in *_names_ for the static\n   // data to refer to.\n-  std::vector<string> nonempty_arg_names_;\n-  std::vector<string> nonempty_variable_names_;\n-  std::vector<string> nonempty_result_names_;\n+  std::vector<std::string> nonempty_arg_names_;\n+  std::vector<std::string> nonempty_variable_names_;\n+  std::vector<std::string> nonempty_result_names_;\n   std::vector<const char*> arg_names_;\n   std::vector<const char*> variable_names_;\n   std::vector<const char*> result_names_;"
        },
        {
            "sha": "b49e699d6e267f149b3a740023dd0d294eafd77a",
            "filename": "tensorflow/compiler/tf2xla/xla_jit_compiled_cpu_function_test.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_jit_compiled_cpu_function_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_jit_compiled_cpu_function_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_jit_compiled_cpu_function_test.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -182,18 +182,18 @@ TEST(XlaJitCompiledCpuFunction, Sum) {\n   ASSERT_EQ(function.num_results(), 1);\n \n   // Run the function and check results.\n-  *static_cast<int32*>(function.arg_data(0)) = 10;\n-  *static_cast<int32*>(function.arg_data(1)) = 32;\n+  *static_cast<int32_t*>(function.arg_data(0)) = 10;\n+  *static_cast<int32_t*>(function.arg_data(1)) = 32;\n   EXPECT_TRUE(function.Run());\n   EXPECT_EQ(function.error_msg(), \"\");\n-  EXPECT_EQ(*static_cast<int32*>(function.result_data(0)), 42);\n+  EXPECT_EQ(*static_cast<int32_t*>(function.result_data(0)), 42);\n \n   // Run the function again.\n-  *static_cast<int32*>(function.arg_data(0)) = 100;\n-  *static_cast<int32*>(function.arg_data(1)) = 320;\n+  *static_cast<int32_t*>(function.arg_data(0)) = 100;\n+  *static_cast<int32_t*>(function.arg_data(1)) = 320;\n   EXPECT_TRUE(function.Run());\n   EXPECT_EQ(function.error_msg(), \"\");\n-  EXPECT_EQ(*static_cast<int32*>(function.result_data(0)), 420);\n+  EXPECT_EQ(*static_cast<int32_t*>(function.result_data(0)), 420);\n \n   // Check name to index lookups.\n   EXPECT_TRUE(function.HasNameIndices());\n@@ -268,20 +268,20 @@ TEST(XlaJitCompiledCpuFunction, SumVariable) {\n   ASSERT_EQ(function.num_results(), 2);\n \n   // Run the function and check results.\n-  *static_cast<int32*>(function.arg_data(0)) = 10;\n-  *static_cast<int32*>(function.arg_data(1)) = 32;\n+  *static_cast<int32_t*>(function.arg_data(0)) = 10;\n+  *static_cast<int32_t*>(function.arg_data(1)) = 32;\n   EXPECT_TRUE(function.Run());\n   EXPECT_EQ(function.error_msg(), \"\");\n-  EXPECT_EQ(*static_cast<int32*>(function.result_data(0)), 10);\n-  EXPECT_EQ(*static_cast<int32*>(function.result_data(1)), 42);\n+  EXPECT_EQ(*static_cast<int32_t*>(function.result_data(0)), 10);\n+  EXPECT_EQ(*static_cast<int32_t*>(function.result_data(1)), 42);\n \n   // Run the function again.\n-  *static_cast<int32*>(function.arg_data(0)) = 100;\n-  *static_cast<int32*>(function.arg_data(1)) = 320;\n+  *static_cast<int32_t*>(function.arg_data(0)) = 100;\n+  *static_cast<int32_t*>(function.arg_data(1)) = 320;\n   EXPECT_TRUE(function.Run());\n   EXPECT_EQ(function.error_msg(), \"\");\n-  EXPECT_EQ(*static_cast<int32*>(function.result_data(0)), 100);\n-  EXPECT_EQ(*static_cast<int32*>(function.result_data(1)), 420);\n+  EXPECT_EQ(*static_cast<int32_t*>(function.result_data(0)), 100);\n+  EXPECT_EQ(*static_cast<int32_t*>(function.result_data(1)), 420);\n \n   // Check name to index lookups.\n   EXPECT_TRUE(function.HasNameIndices());\n@@ -325,7 +325,7 @@ TEST(XlaJitCompiledCpuFunction, CanCompileWithAdditionalPlatform) {\n \n     int VisibleDeviceCount() const override { return 0; }\n \n-    const string& Name() const override { return name_; }\n+    const std::string& Name() const override { return name_; }\n \n     absl::StatusOr<std::unique_ptr<se::DeviceDescription>> DescriptionForDevice(\n         int ordinal) const override {\n@@ -338,7 +338,7 @@ TEST(XlaJitCompiledCpuFunction, CanCompileWithAdditionalPlatform) {\n     }\n \n    private:\n-    string name_;\n+    std::string name_;\n   };\n \n   TF_EXPECT_OK("
        },
        {
            "sha": "baefe0138d43ddf35dc9a947e987ccb1b305e6e1",
            "filename": "tensorflow/compiler/tf2xla/xla_op_kernel.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_op_kernel.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_op_kernel.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_op_kernel.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -207,9 +207,9 @@ static absl::Status LiteralToInt64Scalar(const xla::LiteralSlice& literal,\n     return errors::InvalidArgument(\"value is not a scalar\");\n   }\n   if (literal.shape().element_type() == xla::S16) {\n-    *out = literal.Get<int16>({});\n+    *out = literal.Get<int16_t>({});\n   } else if (literal.shape().element_type() == xla::S32) {\n-    *out = literal.Get<int32>({});\n+    *out = literal.Get<int32_t>({});\n   } else if (literal.shape().element_type() == xla::S64) {\n     *out = literal.Get<int64_t>({});\n   } else {\n@@ -370,7 +370,7 @@ static absl::Status LiteralToInt64Vector(const xla::LiteralSlice& literal,\n   int64_t size = xla::ShapeUtil::ElementsIn(literal.shape());\n   if (literal.shape().element_type() == xla::S32) {\n     for (int64_t i = 0; i < size; ++i) {\n-      out->push_back(literal.Get<int32>({i}));\n+      out->push_back(literal.Get<int32_t>({i}));\n     }\n   } else if (literal.shape().element_type() == xla::S64) {\n     for (int64_t i = 0; i < size; ++i) {\n@@ -422,7 +422,7 @@ absl::Status XlaOpKernelContext::ConstantInputAsInt64Literal(\n     case xla::S32: {\n       *out = xla::Literal(\n           xla::ShapeUtil::ChangeElementType(literal.shape(), xla::S64));\n-      auto src_data = literal.data<int32>();\n+      auto src_data = literal.data<int32_t>();\n       for (int64_t i = 0; i < src_data.size(); ++i) {\n         out->data<int64_t>()[i] = src_data[i];\n       }"
        },
        {
            "sha": "c74db8657692295c74cc8de130acf1f96b9446e1",
            "filename": "tensorflow/compiler/tf2xla/xla_op_registry.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 25,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_op_registry.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_op_registry.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_op_registry.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -61,7 +61,7 @@ static absl::Status LaunchOpHasKernelForDevice(const DeviceType& device_type) {\n   NodeDef node_def;\n   node_def.set_name(\"_XlaLaunch-op\");\n   node_def.set_op(\"XlaLaunch\");\n-  string kernel_class_name;\n+  std::string kernel_class_name;\n   TF_RETURN_IF_ERROR(FindKernelDef(device_type, node_def, /*KernelDef*/ nullptr,\n                                    &kernel_class_name));\n   VLOG(1) << \"LaunchOpHasKernelForDevice\"\n@@ -128,7 +128,7 @@ XlaOpRegistry::~XlaOpRegistry() = default;\n }\n \n /* static */ void XlaOpRegistry::RegisterCompilationDevice(\n-    const string& device_name, const DeviceRegistration& registration) {\n+    const std::string& device_name, const DeviceRegistration& registration) {\n   XlaOpRegistry& registry = Instance();\n   mutex_lock lock(registry.mutex_);\n   auto result =\n@@ -138,7 +138,7 @@ XlaOpRegistry::~XlaOpRegistry() = default;\n }\n \n /* static */ void XlaOpRegistry::RegisterBackend(\n-    const string& compilation_device_name,\n+    const std::string& compilation_device_name,\n     absl::Span<const DataType> supported_types, BackendOpFilter op_filter) {\n   XlaOpRegistry& registry = Instance();\n   mutex_lock lock(registry.mutex_);\n@@ -151,14 +151,14 @@ XlaOpRegistry::~XlaOpRegistry() = default;\n }\n \n /* static */ bool XlaOpRegistry::IsCompilationDevice(\n-    const string& device_name) {\n+    const std::string& device_name) {\n   XlaOpRegistry& registry = Instance();\n   mutex_lock lock(registry.mutex_);\n   return registry.backends_.find(device_name) != registry.backends_.end();\n }\n \n /* static */ bool XlaOpRegistry::GetCompilationDevice(\n-    const string& device_name, const DeviceRegistration** registration) {\n+    const std::string& device_name, const DeviceRegistration** registration) {\n   XlaOpRegistry& registry = Instance();\n \n   // Lazily register the CPU and GPU JIT devices the first time\n@@ -235,7 +235,7 @@ void XlaOpRegistry::RegisterCompilationKernels() {\n   // 2. Process op registration without device allowlists:\n   //      this pass registers the kernels for all the other supported backends.\n   for (auto& ops : registry.ops_) {\n-    const string& op_name = ops.first;\n+    const std::string& op_name = ops.first;\n     std::vector<std::unique_ptr<OpRegistration>>& op_registrations = ops.second;\n     // Partition the op registration so that the ones with device allowlists\n     // precede the one without device allowlist.\n@@ -247,7 +247,7 @@ void XlaOpRegistry::RegisterCompilationKernels() {\n     // Collect a set of backend registered by ops with device allowlists.\n     // The op registration without allowlists will register a generic kernel\n     // for all other backends not in this set.\n-    std::unordered_set<string> allowlisted_backend;\n+    std::unordered_set<std::string> allowlisted_backend;\n     for (auto& op_registration : op_registrations) {\n       if (op_registration->has_device_allowlist) {\n         allowlisted_backend.insert(op_registration->device_allowlist.begin(),\n@@ -267,7 +267,7 @@ void XlaOpRegistry::RegisterCompilationKernels() {\n       }\n       TF_CHECK_OK(lookup_status);\n \n-      std::unordered_set<string> type_attrs;\n+      std::unordered_set<std::string> type_attrs;\n       for (const OpDef::AttrDef& attr_def : op_def->attr()) {\n         if (attr_def.type() == \"type\" || attr_def.type() == \"list(type)\") {\n           type_attrs.insert(attr_def.name());\n@@ -309,7 +309,7 @@ void XlaOpRegistry::RegisterCompilationKernels() {\n         // b) the types allowed by the OpDef, and\n         // c) the type constraints.\n         bool unsatisfiable_type_constraint = false;\n-        for (const string& type_attr : type_attrs) {\n+        for (const std::string& type_attr : type_attrs) {\n           KernelDef::AttrConstraint* attr_constraint = kdef->add_constraint();\n           attr_constraint->set_name(type_attr);\n           auto* allowed_values =\n@@ -375,7 +375,7 @@ void XlaOpRegistry::RegisterCompilationKernels() {\n }\n \n std::vector<const KernelDef*> XlaOpRegistry::DeviceKernels(\n-    const string& compilation_device_name,\n+    const std::string& compilation_device_name,\n     bool include_compilation_only_kernels) {\n   // Ensure compilation kernels registered.\n   RegisterCompilationKernels();\n@@ -403,8 +403,8 @@ std::vector<const KernelDef*> XlaOpRegistry::DeviceKernels(\n   return kernels;\n }\n \n-/*static*/ std::vector<string> XlaOpRegistry::GetAllRegisteredOps() {\n-  std::vector<string> ops;\n+/*static*/ std::vector<std::string> XlaOpRegistry::GetAllRegisteredOps() {\n+  std::vector<std::string> ops;\n   XlaOpRegistry& registry = Instance();\n   mutex_lock lock(registry.mutex_);\n   ops.reserve(registry.ops_.size());\n@@ -416,7 +416,7 @@ std::vector<const KernelDef*> XlaOpRegistry::DeviceKernels(\n }\n \n /*static*/ const std::unordered_set<std::string>*\n-XlaOpRegistry::CompileTimeConstantInputArgNames(const string& op) {\n+XlaOpRegistry::CompileTimeConstantInputArgNames(const std::string& op) {\n   XlaOpRegistry& registry = Instance();\n   mutex_lock lock(registry.mutex_);\n   auto it = registry.ops_.find(op);\n@@ -435,10 +435,10 @@ XlaOpRegistry::CompileTimeConstantInputArgNames(const string& op) {\n \n   DCHECK(op_def != nullptr || op_kernel != nullptr);\n \n-  std::unordered_set<string> compile_time_constant_inputs_from_attr;\n-  std::vector<string> compile_time_constant_inputs_vect_from_attr;\n+  std::unordered_set<std::string> compile_time_constant_inputs_from_attr;\n+  std::vector<std::string> compile_time_constant_inputs_vect_from_attr;\n \n-  const std::unordered_set<string>* compile_time_constant_inputs;\n+  const std::unordered_set<std::string>* compile_time_constant_inputs;\n \n   if (TryGetNodeAttr(node_def, kXlaCompileTimeConstantInputsAttr,\n                      &compile_time_constant_inputs_vect_from_attr)) {\n@@ -459,7 +459,7 @@ XlaOpRegistry::CompileTimeConstantInputArgNames(const string& op) {\n           << \" required constants are: \"\n           << absl::StrJoin(*compile_time_constant_inputs, \", \");\n \n-  for (const string& input : *compile_time_constant_inputs) {\n+  for (const std::string& input : *compile_time_constant_inputs) {\n     if (op_def) {\n       NameRangeMap input_name_ranges;\n       TF_RETURN_IF_ERROR(\n@@ -486,7 +486,7 @@ XlaOpRegistry::CompileTimeConstantInputArgNames(const string& op) {\n   return absl::OkStatus();\n }\n \n-/*static*/ bool XlaOpRegistry::IsMetadataOp(const string& op) {\n+/*static*/ bool XlaOpRegistry::IsMetadataOp(const std::string& op) {\n   XlaOpRegistry& registry = Instance();\n   mutex_lock lock(registry.mutex_);\n   auto it = registry.ops_.find(op);\n@@ -500,8 +500,8 @@ XlaOpRegistry::CompileTimeConstantInputArgNames(const string& op) {\n   return it->second.front()->is_metadata_op;\n }\n \n-std::vector<string> XlaOpRegistry::BackendNames() {\n-  std::vector<string> names;\n+std::vector<std::string> XlaOpRegistry::BackendNames() {\n+  std::vector<std::string> names;\n   XlaOpRegistry& registry = Instance();\n   mutex_lock lock(registry.mutex_);\n   names.reserve(registry.backends_.size());\n@@ -511,7 +511,7 @@ std::vector<string> XlaOpRegistry::BackendNames() {\n   return names;\n }\n \n-bool XlaOpRegistry::IsBackendRegistered(const string& name) {\n+bool XlaOpRegistry::IsBackendRegistered(const std::string& name) {\n   XlaOpRegistry& registry = Instance();\n   mutex_lock lock(registry.mutex_);\n   return registry.backends_.find(name) != registry.backends_.end();\n@@ -524,7 +524,7 @@ XlaOpRegistry& XlaOpRegistry::Instance() {\n \n XlaOpRegistrationBuilder::XlaOpRegistrationBuilder(absl::string_view name) {\n   registration_.reset(new XlaOpRegistry::OpRegistration);\n-  registration_->name = string(name);\n+  registration_->name = std::string(name);\n }\n \n XlaOpRegistrationBuilder XlaOpRegistrationBuilder::Name(\n@@ -572,15 +572,15 @@ XlaOpRegistrationBuilder& XlaOpRegistrationBuilder::AllowStringType() {\n XlaOpRegistrationBuilder& XlaOpRegistrationBuilder::TypeConstraint(\n     absl::string_view attr_name, DataType allowed) {\n   std::set<DataType>& types =\n-      registration_->type_constraints[string(attr_name)];\n+      registration_->type_constraints[std::string(attr_name)];\n   types.insert(allowed);\n   return *this;\n }\n \n XlaOpRegistrationBuilder& XlaOpRegistrationBuilder::TypeConstraint(\n     absl::string_view attr_name, absl::Span<const DataType> allowed) {\n   std::set<DataType>& types =\n-      registration_->type_constraints[string(attr_name)];\n+      registration_->type_constraints[std::string(attr_name)];\n   for (DataType t : allowed) {\n     types.insert(t);\n   }\n@@ -628,7 +628,7 @@ XlaBackendRegistrar::XlaBackendRegistrar(\n     absl::string_view name, absl::Span<const DataType> types,\n     XlaOpRegistry::BackendOpFilter op_filter) {\n   XlaOpRegistry& registry = XlaOpRegistry::Instance();\n-  registry.RegisterBackend(string(name), types, op_filter);\n+  registry.RegisterBackend(std::string(name), types, op_filter);\n \n   AddSymbolicExecutionDevice(name);\n }"
        },
        {
            "sha": "9ce6e263f8feb48fbe8efa59c452ff16c0fb377b",
            "filename": "tensorflow/compiler/tf2xla/xla_op_registry.h",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_op_registry.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_op_registry.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_op_registry.h?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -139,7 +139,7 @@ class XlaOpRegistry {\n   // Describes how to compile operators assigned to a device.\n   struct DeviceRegistration {\n     // The name of the an XLA compilation device to use to compile code.\n-    string compilation_device_name;\n+    std::string compilation_device_name;\n \n     // When should we autocluster operators assigned to this device?\n     AutoclusteringPolicy autoclustering_policy;\n@@ -190,33 +190,33 @@ class XlaOpRegistry {\n   // `backend_op_filter` should return true if the op should be registered on\n   // the device; it may optionally modify the KernelDef.\n   typedef bool (*BackendOpFilter)(KernelDef* kdef);\n-  static void RegisterBackend(const string& compilation_device_name,\n+  static void RegisterBackend(const std::string& compilation_device_name,\n                               absl::Span<const DataType> supported_types,\n                               BackendOpFilter op_filter);\n \n   // Returns the names of the registered backends.\n-  static std::vector<string> BackendNames();\n+  static std::vector<std::string> BackendNames();\n \n   // Returns true iff a backend with the given name is registered.\n-  static bool IsBackendRegistered(const string& name);\n+  static bool IsBackendRegistered(const std::string& name);\n \n   // Registers `device_name` for XLA compilation, using information from\n   // `registration`.\n   // Does nothing if a registration for `device_name` already exists.\n-  static void RegisterCompilationDevice(const string& device_name,\n+  static void RegisterCompilationDevice(const std::string& device_name,\n                                         const DeviceRegistration& registration);\n \n   // Returns whether the device name is for the JIT device used exclusively for\n   // TF2XLA conversion.\n-  static bool IsCompilationDevice(const string& device_name);\n+  static bool IsCompilationDevice(const std::string& device_name);\n \n   // Returns the JIT device name associated with 'device_name', setting\n   // 'jit_device_name', 'requires_jit', and 'enabled_jit_by_default', if they\n   // are not null. Returns false and leaves the outputs unchanged if no matching\n   // JIT device is registered.\n   // '*enable_jit_by_default' is set to true if we should try to JIT using this\n   // device when the JIT is enabled via the Session OptimizerOptions.\n-  static bool GetCompilationDevice(const string& device_name,\n+  static bool GetCompilationDevice(const std::string& device_name,\n                                    const DeviceRegistration** registration);\n \n   // Registers all JIT kernels on JIT devices, if not already registered.\n@@ -227,11 +227,11 @@ class XlaOpRegistry {\n   // 'compilation_device_name'.  Does not include kernels registered as\n   // CompilationOnly, iff include_compilation_only_kernels=false.\n   static std::vector<const KernelDef*> DeviceKernels(\n-      const string& compilation_device_name,\n+      const std::string& compilation_device_name,\n       bool include_compilation_only_kernels);\n \n   // Returns all operations for which there are XLA kernels on any device.\n-  static std::vector<string> GetAllRegisteredOps();\n+  static std::vector<std::string> GetAllRegisteredOps();\n \n   // Returns (via `result`) the indices of inputs to `node_def` that must be\n   // compile-time constants. Returns an empty vector if the op is not\n@@ -265,11 +265,11 @@ class XlaOpRegistry {\n   // Return names of arguments for a given op which are supposed to be\n   // constants.\n   static const std::unordered_set<std::string>*\n-  CompileTimeConstantInputArgNames(const string& op);\n+  CompileTimeConstantInputArgNames(const std::string& op);\n \n   // Returns true if `op` is a \"metadata\" op, one that only looks at the shapes\n   // of its operands and not their values.\n-  static bool IsMetadataOp(const string& op);\n+  static bool IsMetadataOp(const std::string& op);\n \n  private:\n   friend class XlaBackendRegistrar;\n@@ -298,15 +298,15 @@ class XlaOpRegistry {\n   };\n \n   // Map from compilation device names to a description of the backend.\n-  std::unordered_map<string, Backend> backends_ TF_GUARDED_BY(mutex_);\n+  std::unordered_map<std::string, Backend> backends_ TF_GUARDED_BY(mutex_);\n \n   // Map from Tensorflow device names to the corresponding JIT device metadata.\n-  std::unordered_map<string, DeviceRegistration> compilation_devices_\n+  std::unordered_map<std::string, DeviceRegistration> compilation_devices_\n       TF_GUARDED_BY(mutex_);\n \n   // A description of a Tensorflow operator that can be compiled to XLA.\n   struct OpRegistration {\n-    string name;\n+    std::string name;\n \n     // Should this operator be registered only on compilation devices, without a\n     // dummy kernel registered on the corresponding XLA device?\n@@ -325,15 +325,15 @@ class XlaOpRegistry {\n     bool allow_string_type = false;\n \n     // Mapping from attribute name to a list of supported types.\n-    std::unordered_map<string, std::set<DataType>> type_constraints;\n+    std::unordered_map<std::string, std::set<DataType>> type_constraints;\n \n     // An optional allowlist of devices. If there is no allowlist, all devices\n     // are permitted.\n     bool has_device_allowlist = false;\n-    std::unordered_set<string> device_allowlist;\n+    std::unordered_set<std::string> device_allowlist;\n \n     // Names of arguments that must be compile-time constants.\n-    std::unordered_set<string> compile_time_constant_inputs;\n+    std::unordered_set<std::string> compile_time_constant_inputs;\n \n     // True if this is a \"metadata\" op, one that only looks at the shapes of its\n     // operands and not their values.\n@@ -360,8 +360,8 @@ class XlaOpRegistry {\n   // Map from operator name to OpRegistrations, populated by REGISTER_XLA_OP.\n   // Registrations present under the same key must satisfy IsCompatible above,\n   // and this is checked during registration.\n-  std::unordered_map<string, std::vector<std::unique_ptr<OpRegistration>>> ops_\n-      TF_GUARDED_BY(mutex_);\n+  std::unordered_map<std::string, std::vector<std::unique_ptr<OpRegistration>>>\n+      ops_ TF_GUARDED_BY(mutex_);\n \n   // Have we already registered the JIT kernels on the JIT devices?\n   bool jit_kernels_registered_ = false;"
        },
        {
            "sha": "962b0e473a826cf175d1ff46081e7181bd8954c7",
            "filename": "tensorflow/compiler/tf2xla/xla_resource.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_resource.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_resource.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_resource.cc?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -51,29 +51,29 @@ namespace tensorflow {\n }\n \n /*static*/ std::unique_ptr<XlaResource> XlaResource::CreateStack(\n-    string name, DataType type, int64_t max_size) {\n+    std::string name, DataType type, int64_t max_size) {\n   return std::make_unique<XlaResource>(\n       XlaResource::kStack, /*arg_num=*/-1, std::move(name), type, TensorShape(),\n       /*initial_value=*/xla::XlaOp(),\n       /*max_array_size=*/max_size,\n-      /*tensor_array_gradients=*/std::set<string>{},\n+      /*tensor_array_gradients=*/std::set<std::string>{},\n       /*tensor_array_multiple_writes_aggregate=*/false);\n }\n \n /*static*/ std::unique_ptr<XlaResource> XlaResource::CreateTensorArray(\n-    string name, DataType type, TensorShape shape, xla::XlaOp initial_value,\n-    int64_t max_array_size) {\n+    std::string name, DataType type, TensorShape shape,\n+    xla::XlaOp initial_value, int64_t max_array_size) {\n   return std::make_unique<XlaResource>(\n       XlaResource::kTensorArray, /*arg_num=*/-1, std::move(name), type, shape,\n       initial_value, max_array_size,\n-      /*tensor_array_gradients=*/std::set<string>{},\n+      /*tensor_array_gradients=*/std::set<std::string>{},\n       /*tensor_array_multiple_writes_aggregate=*/false);\n }\n \n XlaResource::XlaResource(\n-    Kind kind, int arg_num, string name, DataType type, TensorShape shape,\n+    Kind kind, int arg_num, std::string name, DataType type, TensorShape shape,\n     xla::XlaOp initial_value, int64_t max_array_size,\n-    const std::set<string>& tensor_array_gradients,\n+    const std::set<std::string>& tensor_array_gradients,\n     bool tensor_array_multiple_writes_aggregate,\n     const std::optional<ManagedStackTrace>& definition_stack_trace)\n     : kind_(kind),\n@@ -89,7 +89,7 @@ XlaResource::XlaResource(\n       definition_stack_trace_(definition_stack_trace) {\n   CHECK(kind_ != kInvalid);\n \n-  for (const string& gradient : tensor_array_gradients) {\n+  for (const std::string& gradient : tensor_array_gradients) {\n     tensor_array_gradients_[gradient].reset(new XlaResource(\n         /*kind=*/kTensorArray, /*arg_num=*/-1,\n         /*name=*/absl::StrCat(\"TensorArrayGrad: \", name_), type_, shape_,\n@@ -163,7 +163,7 @@ absl::Status XlaResource::SetZeroValue(xla::XlaBuilder* builder) {\n       value_ =\n           xla::Tuple(builder, {xla::Broadcast(XlaHelpers::Zero(builder, type_),\n                                               ta_shape.dim_sizes()),\n-                               xla::ConstantR0<int32>(builder, 0)});\n+                               xla::ConstantR0<int32_t>(builder, 0)});\n       break;\n     }\n \n@@ -175,7 +175,7 @@ absl::Status XlaResource::SetZeroValue(xla::XlaBuilder* builder) {\n }\n \n absl::Status XlaResource::GetOrCreateTensorArrayGradient(\n-    const string& source, xla::XlaBuilder* builder,\n+    const std::string& source, xla::XlaBuilder* builder,\n     XlaResource** gradient_out) {\n   VLOG(2) << \"Gradient lookup for resource: \" << name_\n           << \" gradient: \" << source;\n@@ -214,9 +214,9 @@ absl::Status XlaResource::Pack(xla::XlaOp* pack,\n   return absl::OkStatus();\n }\n \n-absl::Status XlaResource::SetFromPack(const std::set<string>& gradient_sources,\n-                                      const xla::XlaOp pack,\n-                                      xla::XlaBuilder* builder) {\n+absl::Status XlaResource::SetFromPack(\n+    const std::set<std::string>& gradient_sources, const xla::XlaOp pack,\n+    xla::XlaBuilder* builder) {\n   if (gradient_sources.empty()) {\n     if (!initialized()) {\n       initial_value_ = pack;"
        },
        {
            "sha": "07c826d21e8b3d4c60a0b1416bc582cd04ce72e8",
            "filename": "tensorflow/compiler/tf2xla/xla_resource.h",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_resource.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ee6b915c070127213cdc7d81c3448c905f1ba3f9/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_resource.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Ftf2xla%2Fxla_resource.h?ref=ee6b915c070127213cdc7d81c3448c905f1ba3f9",
            "patch": "@@ -43,18 +43,19 @@ class XlaResource {\n   static absl::string_view KindToString(Kind kind);\n \n   // Creates a new Stack resource.\n-  static std::unique_ptr<XlaResource> CreateStack(string name, DataType type,\n+  static std::unique_ptr<XlaResource> CreateStack(std::string name,\n+                                                  DataType type,\n                                                   int64_t max_size);\n \n   // Creates a new TensorArray resource.\n   static std::unique_ptr<XlaResource> CreateTensorArray(\n-      string name, DataType type, TensorShape shape, xla::XlaOp initial_value,\n-      int64_t max_array_size);\n+      std::string name, DataType type, TensorShape shape,\n+      xla::XlaOp initial_value, int64_t max_array_size);\n \n-  XlaResource(Kind kind, int arg_num, string name, DataType type,\n+  XlaResource(Kind kind, int arg_num, std::string name, DataType type,\n               TensorShape shape, xla::XlaOp initial_value,\n               int64_t max_array_size,\n-              const std::set<string>& tensor_array_gradients,\n+              const std::set<std::string>& tensor_array_gradients,\n               bool tensor_array_multiple_writes_aggregate,\n               const std::optional<ManagedStackTrace>& definition_stack_trace =\n                   std::nullopt);\n@@ -72,7 +73,7 @@ class XlaResource {\n   int arg_num() const { return arg_num_; }\n \n   // A descriptive name for the resource, used in error messages.\n-  const string& name() const { return name_; }\n+  const std::string& name() const { return name_; }\n \n   // Current type and value of the resource. Uninitialized resources are\n   // represented by a default (zero) handle and type DT_INVALID.\n@@ -121,7 +122,7 @@ class XlaResource {\n   // exist. The call target must be an initialized TensorArray resource. A\n   // TensorArray can have multiple named gradients; see the operator\n   // documentation for TensorArrayGradV3 for details.\n-  absl::Status GetOrCreateTensorArrayGradient(const string& source,\n+  absl::Status GetOrCreateTensorArrayGradient(const std::string& source,\n                                               xla::XlaBuilder* builder,\n                                               XlaResource** gradient_out);\n \n@@ -138,7 +139,7 @@ class XlaResource {\n   // If `reset_initial_values` is true, sets the initial_values as well as the\n   // values.\n   // Opposite of Pack().\n-  absl::Status SetFromPack(const std::set<string>& gradient_sources,\n+  absl::Status SetFromPack(const std::set<std::string>& gradient_sources,\n                            xla::XlaOp pack, xla::XlaBuilder* builder);\n \n   bool IsOverwritten() { return is_overwritten_; }\n@@ -164,15 +165,15 @@ class XlaResource {\n   // string, irrespective of the number of calls to TensorArrayGrad. The map\n   // is ordered since values are packed into tuples by Pack() sorted by name\n   // order.\n-  const std::map<string, std::unique_ptr<XlaResource>>& tensor_array_gradients()\n-      const {\n+  const std::map<std::string, std::unique_ptr<XlaResource>>&\n+  tensor_array_gradients() const {\n     return tensor_array_gradients_;\n   }\n \n  private:\n   const Kind kind_;\n   const int arg_num_;\n-  const string name_;\n+  const std::string name_;\n \n   DataType type_;\n   TensorShape shape_;\n@@ -186,7 +187,7 @@ class XlaResource {\n   int64_t max_array_size_ = -1;\n   bool tensor_array_multiple_writes_aggregate_ = false;\n \n-  std::map<string, std::unique_ptr<XlaResource>> tensor_array_gradients_;\n+  std::map<std::string, std::unique_ptr<XlaResource>> tensor_array_gradients_;\n   bool is_overwritten_ = false;\n \n   std::optional<ManagedStackTrace> definition_stack_trace_;"
        }
    ],
    "stats": {
        "total": 777,
        "additions": 401,
        "deletions": 376
    }
}