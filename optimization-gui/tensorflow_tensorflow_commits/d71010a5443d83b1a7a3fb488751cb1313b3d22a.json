{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Make `TmaMetadata` a required field in `KernelThunk`.\n\nThe metadata is just a hash map. It's not expensive to construct an empty map and there is not special handling for `nullopt` that is different from having an empty map.\n\nPiperOrigin-RevId: 806236958",
    "sha": "d71010a5443d83b1a7a3fb488751cb1313b3d22a",
    "files": [
        {
            "sha": "8d535de9fd8142788b339899a30872db62f79be1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d71010a5443d83b1a7a3fb488751cb1313b3d22a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d71010a5443d83b1a7a3fb488751cb1313b3d22a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc?ref=d71010a5443d83b1a7a3fb488751cb1313b3d22a",
            "patch": "@@ -320,7 +320,8 @@ absl::StatusOr<FusionEmissionResult> EmitterBase::Emit(\n   FusionEmissionResult result;\n   result.thunks.emplace_back(std::make_unique<KernelThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation(&fusion), entry->kernel_name,\n-      args, launch_dims, entry->cluster_dim, entry->shmem_bytes));\n+      args, launch_dims, entry->cluster_dim, entry->shmem_bytes,\n+      /*tma_metadata=*/se::gpu::TmaMetadata()));\n   return result;\n }\n "
        },
        {
            "sha": "13b3870c509db5fa2355f44a231d38e7e581a7d9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d71010a5443d83b1a7a3fb488751cb1313b3d22a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d71010a5443d83b1a7a3fb488751cb1313b3d22a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h?ref=d71010a5443d83b1a7a3fb488751cb1313b3d22a",
            "patch": "@@ -59,7 +59,7 @@ namespace gpu {\n struct TritonWrapperResult {\n   int64_t shmem_bytes = 0;\n   std::optional<se::ClusterDim> cluster_dim;\n-  std::optional<stream_executor::gpu::TmaMetadata> tma_metadata;\n+  stream_executor::gpu::TmaMetadata tma_metadata;\n \n   // The captured nvvm.annotations from the lowest level LLVM IR coming from\n   // Triton. We need to propagate them because we later create the kernel and"
        },
        {
            "sha": "923f2077c649a99d051197ce6c1ccccad2ae5ba2",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 19,
            "changes": 31,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d71010a5443d83b1a7a3fb488751cb1313b3d22a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d71010a5443d83b1a7a3fb488751cb1313b3d22a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.cc?ref=d71010a5443d83b1a7a3fb488751cb1313b3d22a",
            "patch": "@@ -56,12 +56,12 @@ namespace gpu {\n // KernelThunk\n //===----------------------------------------------------------------------===//\n \n-KernelThunk::KernelThunk(\n-    Thunk::ThunkInfo thunk_info, std::string kernel_name,\n-    const emitters::KernelArguments& kernel_arguments,\n-    LaunchDimensions launch_dimensions,\n-    std::optional<se::ClusterDim> cluster_dim, int64_t shmem_bytes,\n-    std::optional<stream_executor::gpu::TmaMetadata> tma_metadata)\n+KernelThunk::KernelThunk(Thunk::ThunkInfo thunk_info, std::string kernel_name,\n+                         const emitters::KernelArguments& kernel_arguments,\n+                         LaunchDimensions launch_dimensions,\n+                         std::optional<se::ClusterDim> cluster_dim,\n+                         int64_t shmem_bytes,\n+                         stream_executor::gpu::TmaMetadata tma_metadata)\n     : Thunk(Kind::kKernel, std::move(thunk_info)),\n       args_(kernel_arguments.GetArgumentBufferSlices()),\n       written_(kernel_arguments.GetArgumentOutputFlags()),\n@@ -95,9 +95,7 @@ absl::StatusOr<ThunkProto> KernelThunk::ToProto() const {\n     *kernel_proto->mutable_cluster_dim() = cluster_dim_->ToProto();\n   }\n   kernel_proto->set_shmem_bytes(shmem_bytes_);\n-  if (tma_metadata_.has_value()) {\n-    *kernel_proto->mutable_tma_metadata() = tma_metadata_->ToProto();\n-  }\n+  *kernel_proto->mutable_tma_metadata() = tma_metadata_.ToProto();\n   return proto;\n }\n \n@@ -129,12 +127,9 @@ absl::StatusOr<std::unique_ptr<KernelThunk>> KernelThunk::FromProto(\n     arguments.push_back(std::move(argument));\n   }\n \n-  std::optional<stream_executor::gpu::TmaMetadata> tma_metadata;\n-  if (proto.has_tma_metadata()) {\n-    TF_ASSIGN_OR_RETURN(\n-        tma_metadata,\n-        stream_executor::gpu::TmaMetadata::FromProto(proto.tma_metadata()));\n-  }\n+  TF_ASSIGN_OR_RETURN(\n+      stream_executor::gpu::TmaMetadata tma_metadata,\n+      stream_executor::gpu::TmaMetadata::FromProto(proto.tma_metadata()));\n \n   return std::make_unique<KernelThunk>(\n       thunk_info, proto.kernel_name(),\n@@ -223,16 +218,14 @@ absl::Status KernelThunk::ExecuteOnStream(const ExecuteParams& params) {\n   int device_ordinal = executor->device_ordinal();\n   VLOG(3) << \"[\" << device_ordinal << \"] Launching \" << kernel->name();\n   absl::InlinedVector<se::KernelArgument, 4> kernel_args;\n-  stream_executor::gpu::TmaMetadata tma_metadata =\n-      tma_metadata_.value_or(stream_executor::gpu::TmaMetadata{});\n   for (const auto& [idx, arg] : llvm::enumerate(args_)) {\n     se::DeviceMemoryBase buf = params.buffer_allocations->GetDeviceAddress(arg);\n     VLOG(3) << \"[\" << device_ordinal << \"] Arg: alloc #\" << arg.index()\n             << \", offset: \" << arg.offset() << \": \" << buf.opaque() << \" (\"\n             << buf.size() << \"B)\";\n \n-    if (auto it = tma_metadata.arg_index_to_tma_info.find(idx);\n-        it != tma_metadata.arg_index_to_tma_info.end()) {\n+    if (auto it = tma_metadata_.arg_index_to_tma_info.find(idx);\n+        it != tma_metadata_.arg_index_to_tma_info.end()) {\n       // TMA descriptor argument.\n       stream_executor::gpu::TmaDescriptor tma_desc = it->second;\n       TF_ASSIGN_OR_RETURN(se::TensorMap tensor_map,"
        },
        {
            "sha": "60597ca534a393fb5b596f4eb8e38de6e6db2110",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk.h",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d71010a5443d83b1a7a3fb488751cb1313b3d22a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d71010a5443d83b1a7a3fb488751cb1313b3d22a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk.h?ref=d71010a5443d83b1a7a3fb488751cb1313b3d22a",
            "patch": "@@ -76,8 +76,7 @@ class KernelThunk : public Thunk {\n               const emitters::KernelArguments& kernel_arguments,\n               LaunchDimensions launch_dimensions,\n               std::optional<se::ClusterDim> cluster_dim, int64_t shmem_bytes,\n-              std::optional<stream_executor::gpu::TmaMetadata> tma_metadata =\n-                  std::nullopt);\n+              stream_executor::gpu::TmaMetadata tma_metadata);\n   KernelThunk(const KernelThunk&) = delete;\n   KernelThunk& operator=(const KernelThunk&) = delete;\n   ~KernelThunk() override = default;\n@@ -107,7 +106,7 @@ class KernelThunk : public Thunk {\n   // The shared memory required by the kernel.\n   int64_t shmem_bytes() const { return shmem_bytes_; }\n \n-  const std::optional<stream_executor::gpu::TmaMetadata>& tma_metadata() const {\n+  const stream_executor::gpu::TmaMetadata& tma_metadata() const {\n     return tma_metadata_;\n   }\n \n@@ -131,7 +130,7 @@ class KernelThunk : public Thunk {\n \n   // Map of argument index to TmaDescriptor used to create arguments to the\n   // kernel.\n-  const std::optional<stream_executor::gpu::TmaMetadata> tma_metadata_;\n+  stream_executor::gpu::TmaMetadata tma_metadata_;\n \n   // Loaded kernels for each `StreamExecutor`.\n   mutable absl::Mutex mutex_;"
        },
        {
            "sha": "5148997d6dd5dad4bc9433413d43272da5ba5442",
            "filename": "third_party/xla/xla/backends/gpu/runtime/kernel_thunk_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d71010a5443d83b1a7a3fb488751cb1313b3d22a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d71010a5443d83b1a7a3fb488751cb1313b3d22a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fkernel_thunk_test.cc?ref=d71010a5443d83b1a7a3fb488751cb1313b3d22a",
            "patch": "@@ -66,7 +66,7 @@ TEST(KernelThunkTest, CreateWithDefaultValues) {\n                     /*launch_dimensions=*/LaunchDimensions(),\n                     /*cluster_dim=*/se::ClusterDim(),\n                     /*shmem_bytes=*/0,\n-                    /*tma_metadata=*/std::nullopt);\n+                    /*tma_metadata=*/se::gpu::TmaMetadata());\n   EXPECT_EQ(thunk.kind(), Kind::kKernel);\n   EXPECT_TRUE(thunk.kernel_name().empty());\n   EXPECT_TRUE(thunk.arguments().empty());\n@@ -108,7 +108,7 @@ TEST(KernelThunkTest, CreateAndGettersAndToString) {\n                     /*launch_dimensions=*/launch_dimensions,\n                     /*cluster_dim=*/se::ClusterDim(8, 7, 6),\n                     /*shmem_bytes=*/1024,\n-                    /*tma_metadata=*/std::nullopt);\n+                    /*tma_metadata=*/se::gpu::TmaMetadata());\n   EXPECT_EQ(thunk.kind(), Kind::kKernel);\n   EXPECT_EQ(thunk.kernel_name(), \"kernel123\");\n   EXPECT_EQ(thunk.arguments(),"
        },
        {
            "sha": "753914ced39eb7567ddc6e10d546b7b6f4e6376f",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d71010a5443d83b1a7a3fb488751cb1313b3d22a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d71010a5443d83b1a7a3fb488751cb1313b3d22a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=d71010a5443d83b1a7a3fb488751cb1313b3d22a",
            "patch": "@@ -446,6 +446,7 @@ cc_library(\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/stream_executor/gpu:gpu_blas_lt\",\n+        \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/stream_executor/platform:platform_object_registry\",\n         \"//xla/tools:hlo_decomposer_lib\",\n         \"//xla/tsl/platform:errors\",\n@@ -693,13 +694,16 @@ xla_cc_test(\n         \"//xla/codegen/emitters:kernel_arguments\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_assignment\",\n+        \"//xla/service:hlo_module_config\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@local_tsl//tsl/platform:path\","
        },
        {
            "sha": "242b62de2aac9c5c23f3a3e2063fd8f0ee82068a",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d71010a5443d83b1a7a3fb488751cb1313b3d22a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d71010a5443d83b1a7a3fb488751cb1313b3d22a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc?ref=d71010a5443d83b1a7a3fb488751cb1313b3d22a",
            "patch": "@@ -23,6 +23,7 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n@@ -34,10 +35,12 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/hlo_module_config.h\"\n #include \"xla/shape_layout.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/semantic_version.h\"\n #include \"xla/tsl/platform/env.h\"\n #include \"xla/tsl/platform/status_matchers.h\"\n@@ -113,9 +116,13 @@ TEST(GpuExecutableTest, RunThunkPasses) {\n \n     ThunkSequence thunk_sequence;\n     thunk_sequence.push_back(std::make_unique<KernelThunk>(\n-        thunk_info, \"test_kernel\",\n-        emitters::KernelArguments(std::vector<emitters::KernelArgument>()),\n-        LaunchDimensions(), std::nullopt, 0));\n+        thunk_info,\n+        /*kernel_name=*/\"test_kernel\",\n+        /*kernel_arguments=*/emitters::KernelArguments({}),\n+        /*launch_dimensions=*/LaunchDimensions(),\n+        /*cluster_dim=*/std::nullopt,\n+        /*shmem_bytes=*/0,\n+        /*tma_metadata=*/se::gpu::TmaMetadata()));\n     thunk_sequence.push_back(std::make_unique<DeviceToDeviceCopyThunk>(\n         thunk_info, slice, slice, 1024));\n "
        },
        {
            "sha": "d9553e4e9929f842e06bec3a3b4acf4d7655601a",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d71010a5443d83b1a7a3fb488751cb1313b3d22a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d71010a5443d83b1a7a3fb488751cb1313b3d22a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=d71010a5443d83b1a7a3fb488751cb1313b3d22a",
            "patch": "@@ -174,6 +174,7 @@ limitations under the License.\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/gpu/gpu_blas_lt.h\"\n+#include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/gpu_solver_context.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -1578,7 +1579,7 @@ absl::Status IrEmitterUnnested::EmitTritonCustomCall(\n   AddThunkToThunkSequence(std::make_unique<KernelThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation(instr), entry->kernel_name,\n       kernel_arguments, entry->launch_dimensions, entry->cluster_dim,\n-      entry->shmem_bytes));\n+      entry->shmem_bytes, entry->tma_metadata));\n   return absl::OkStatus();\n }\n \n@@ -2606,7 +2607,8 @@ IrEmitterUnnested::BuildKernelThunkForNonFusionOp(\n       Thunk::ThunkInfo::WithProfileAnnotation(instr), kernel->getName().str(),\n       kernel_arguments, launch_dimensions,\n       /*cluster_dim=*/std::nullopt,\n-      /*shmem_bytes=*/0));\n+      /*shmem_bytes=*/0,\n+      /*tma_metadata=*/se::gpu::TmaMetadata()));\n \n   std::vector<llvm_ir::IrArray> ir_arrays;\n   ir_arrays.reserve(kernel_arguments.args().size());"
        },
        {
            "sha": "4cfa8160cd090c287f28d2e35fc2980f093a366b",
            "filename": "third_party/xla/xla/service/gpu/kernel_reuse_cache.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d71010a5443d83b1a7a3fb488751cb1313b3d22a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_reuse_cache.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d71010a5443d83b1a7a3fb488751cb1313b3d22a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_reuse_cache.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fkernel_reuse_cache.h?ref=d71010a5443d83b1a7a3fb488751cb1313b3d22a",
            "patch": "@@ -47,7 +47,7 @@ class KernelReuseCache {\n     std::optional<se::ClusterDim> cluster_dim;\n     int64_t shmem_bytes = 0;\n     std::string binary;\n-    std::optional<stream_executor::gpu::TmaMetadata> tma_metadata;\n+    stream_executor::gpu::TmaMetadata tma_metadata;\n   };\n   struct NamedBinary {\n     std::string name;"
        }
    ],
    "stats": {
        "total": 72,
        "additions": 39,
        "deletions": 33
    }
}