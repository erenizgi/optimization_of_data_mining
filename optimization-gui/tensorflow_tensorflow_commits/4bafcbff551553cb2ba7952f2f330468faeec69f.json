{
    "author": "tensorflower-gardener",
    "message": "Break up SpmdPartitioningVisitor::HandleDynamicUpdateSlice into method-specific implementations\n\nPiperOrigin-RevId: 834117339",
    "sha": "4bafcbff551553cb2ba7952f2f330468faeec69f",
    "files": [
        {
            "sha": "af293c68bc533f8c4febceb56ae047a3aa848054",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 201,
            "deletions": 177,
            "changes": 378,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4bafcbff551553cb2ba7952f2f330468faeec69f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4bafcbff551553cb2ba7952f2f330468faeec69f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=4bafcbff551553cb2ba7952f2f330468faeec69f",
            "patch": "@@ -3755,15 +3755,205 @@ HloInstruction* PadHelper(SpmdPartitioningVisitor& visitor,\n   return result;\n }\n \n-// TODO: b/457492726 - Simplify HandleDynamicUpdateSlice in spmd_partitioner.cc\n+absl::Status SpmdPartitioningVisitor::HandleDUSDefault(\n+    HloInstruction* hlo, const HloInstruction* input_tensor,\n+    const HloInstruction* update_tensor,\n+    std::vector<HloInstruction*>& new_indices,\n+    std::vector<int64_t> slice_dims) {\n+  const HloSharding& input_sharding = input_tensor->sharding();\n+  const HloSharding& output_sharding = hlo->sharding();\n+  const HloSharding& better_sharding =\n+      input_sharding.NumTiles() > output_sharding.NumTiles() ? input_sharding\n+                                                             : output_sharding;\n+\n+  HloSharding replicated_sharding =\n+      hlo_sharding_util::PartiallyReplicateTiledShardingOnDims(better_sharding,\n+                                                               slice_dims);\n+  auto base = GetPartitionedHlo(input_tensor).Reshard(replicated_sharding);\n+  auto operand = GetPartitionedHlo(update_tensor).Reshard(replicated_sharding);\n+  auto dus = b_.AddInstruction(HloInstruction::CreateDynamicUpdateSlice(\n+      base.hlo()->shape(), base.hlo(), operand.hlo(), new_indices));\n+  dus->set_sharding(replicated_sharding);\n+  SetPartitionedHlo(hlo, PartitionedHlo(dus, base.base_shape(), base.state())\n+                             .Reshard(hlo->sharding()));\n+  return absl::OkStatus();\n+};\n+\n+absl::Status SpmdPartitioningVisitor::HandleDUSSinglePartitionUpdate(\n+    HloInstruction* hlo, const HloInstruction* input_tensor,\n+    const HloInstruction* update_tensor,\n+    std::vector<HloInstruction*>& new_indices, std::vector<int64_t> slice_dims,\n+    std::vector<int64_t> partitioned_slice_dims) {\n+  auto add_hlo = [&](std::unique_ptr<HloInstruction> to_add) {\n+    return b_.AddInstruction(std::move(to_add));\n+  };\n+  // Get partitioned input.\n+  const auto& dus_sharding = hlo->sharding();\n+  const auto& partitioned_input =\n+      GetPartitionedHlo(input_tensor).Reshard(dus_sharding).hlo();\n+\n+  HloSharding update_sharding =\n+      hlo_sharding_util::PartiallyReplicateTiledShardingOnDims(dus_sharding,\n+                                                               slice_dims);\n+\n+  HloInstruction* replicate_update =\n+      GetPartitionedHlo(update_tensor).Reshard(update_sharding).hlo();\n+\n+  const Shape& partitioned_shape = partitioned_input->shape();\n+  std::vector<HloInstruction*> partition_ordinals = MakeTiledPartitionOrdinals(\n+      hlo->sharding(), MakePartitioningState().partition_id, &b_);\n+  HloInstruction* all_dims_within_partition = add_hlo(\n+      HloInstruction::CreateConstant(LiteralUtil::CreateR0<bool>(true)));\n+\n+  for (int64_t dim : partitioned_slice_dims) {\n+    // Calculate per partition size.\n+    const int64_t per_partition_size = partitioned_shape.dimensions(dim);\n+\n+    // within_partition = (offset >= partition_id * per_partition_size) &&\n+    //                    (offset < (partition_id + 1) * per_partition_size)\n+    const Shape& compare_shape =\n+        ShapeUtil::ChangeElementType(partition_id_->shape(), PRED);\n+    auto per_partition_size_hlo = add_hlo(HloInstruction::CreateConstant(\n+        LiteralUtil::CreateR0<int>(per_partition_size)));\n+    const Shape& offset_shape = per_partition_size_hlo->shape();\n+    const Shape& index_shape = new_indices[dim]->shape();\n+    if (offset_shape.element_type() != index_shape.element_type()) {\n+      new_indices[dim] = add_hlo(HloInstruction::CreateConvert(\n+          ShapeUtil::ChangeElementType(index_shape,\n+                                       offset_shape.element_type()),\n+          new_indices[dim]));\n+    }\n+    HloInstruction* partition_offset = add_hlo(HloInstruction::CreateBinary(\n+        offset_shape, HloOpcode::kMultiply, partition_ordinals[dim],\n+        per_partition_size_hlo));\n+    // offset >= partition_id * per_partition_size\n+    HloInstruction* offset_ge = add_hlo(HloInstruction::CreateCompare(\n+        compare_shape, new_indices[dim], partition_offset,\n+        ComparisonDirection::kGe));\n+    // offset < (partition_id + 1) * per_partition_size\n+    HloInstruction* offset_lt = add_hlo(HloInstruction::CreateCompare(\n+        compare_shape, new_indices[dim],\n+        add_hlo(HloInstruction::CreateBinary(\n+            offset_shape, HloOpcode::kMultiply,\n+            add_hlo(HloInstruction::CreateBinary(\n+                offset_shape, HloOpcode::kAdd, partition_ordinals[dim],\n+                add_hlo(HloInstruction::CreateConstant(\n+                    LiteralUtil::CreateR0<int>(1))))),\n+            per_partition_size_hlo)),\n+        ComparisonDirection::kLt));\n+    HloInstruction* update_within_partition =\n+        add_hlo(HloInstruction::CreateBinary(compare_shape, HloOpcode::kAnd,\n+                                             offset_ge, offset_lt));\n+\n+    all_dims_within_partition = add_hlo(HloInstruction::CreateBinary(\n+        compare_shape, HloOpcode::kAnd, all_dims_within_partition,\n+        update_within_partition));\n+\n+    // Calculate offset.\n+    // slice dim offset = within_partition ?\n+    //                    offset - partition_id * per_partition_size : 0\n+    new_indices[dim] = add_hlo(HloInstruction::CreateTernary(\n+        new_indices[dim]->shape(), HloOpcode::kSelect, update_within_partition,\n+        add_hlo(HloInstruction::CreateBinary(\n+            new_indices[dim]->shape(), HloOpcode::kSubtract, new_indices[dim],\n+            partition_offset)),\n+        add_hlo(\n+            HloInstruction::CreateConstant(LiteralUtil::CreateR0<int>(0)))));\n+    if (new_indices[dim]->shape().element_type() !=\n+        index_shape.element_type()) {\n+      new_indices[dim] = add_hlo(HloInstruction::CreateConvert(\n+          ShapeUtil::ChangeElementType(new_indices[dim]->shape(),\n+                                       index_shape.element_type()),\n+          new_indices[dim]));\n+    }\n+  }\n+\n+  // Create dynamic update slice.\n+  HloInstruction* dus = add_hlo(HloInstruction::CreateDynamicUpdateSlice(\n+      partitioned_shape, partitioned_input, replicate_update, new_indices));\n+  // Select if update is needed\n+  SetPartitionedHlo(hlo,\n+                    add_hlo(HloInstruction::CreateTernary(\n+                        dus->shape(), HloOpcode::kSelect,\n+                        add_hlo(HloInstruction::CreateBroadcast(\n+                            ShapeUtil::ChangeElementType(dus->shape(), PRED),\n+                            all_dims_within_partition, {})),\n+                        dus, partitioned_input)));\n+  return absl::OkStatus();\n+};\n+\n+absl::Status\n+SpmdPartitioningVisitor::HandleDUSAllPartitionedSliceDimsHaveConstantIndices(\n+    HloInstruction* hlo, const HloInstruction* input_tensor,\n+    const HloInstruction* update_tensor) {\n+  auto add_hlo = [&](std::unique_ptr<HloInstruction> to_add) {\n+    return b_.AddInstruction(std::move(to_add));\n+  };\n+  PaddingConfig padding_config;\n+  for (int64_t input_tensor_dim = 0;\n+       input_tensor_dim < hlo->shape().dimensions().size();\n+       ++input_tensor_dim) {\n+    auto padding_dim = padding_config.add_dimensions();\n+    padding_dim->set_interior_padding(0);\n+\n+    const HloInstruction* dus_index = hlo->operand(input_tensor_dim + 2);\n+    CHECK(dus_index->IsConstant());\n+\n+    int64_t start_index = dus_index->literal().GetIntegralAsS64({}).value();\n+    int64_t end_index =\n+        start_index + update_tensor->shape().dimensions(input_tensor_dim);\n+    int64_t padding_high =\n+        hlo->shape().dimensions(input_tensor_dim) - end_index;\n+    padding_dim->set_edge_padding_low(start_index);\n+    padding_dim->set_edge_padding_high(padding_high);\n+  }\n+\n+  const Shape operand_pred_shape =\n+      ShapeUtil::ChangeElementType(hlo->shape(), PRED);\n+  const Shape update_pred_shape =\n+      ShapeUtil::ChangeElementType(update_tensor->shape(), PRED);\n+  const Shape sharded_update_pred_shape =\n+      MakePartitionedShape(update_pred_shape, hlo->sharding());\n+\n+  auto zeroOperand = CreateZero(sharded_update_pred_shape, &b_);\n+  zeroOperand->set_sharding(hlo->sharding());\n+\n+  HloInstruction* paddingValue = CreateOne(Shape(PRED, {}), &b_);\n+  HloInstruction* maskOp = PadHelper(\n+      *this,\n+      PartitionedHlo(zeroOperand, update_pred_shape, MakePartitioningState()),\n+      paddingValue, padding_config, operand_pred_shape, hlo->sharding());\n+  if (!maskOp) {\n+    maskOp = add_hlo(HloInstruction::CreatePad(operand_pred_shape, zeroOperand,\n+                                               paddingValue, padding_config));\n+    maskOp->set_sharding(hlo->sharding());\n+  }\n+\n+  auto zeroElemOp = add_hlo(HloInstruction::CreateConstant(\n+      LiteralUtil::Zero(hlo->shape().element_type())));\n+  HloInstruction* newOperand =\n+      PadHelper(*this, GetPartitionedHlo(update_tensor), zeroElemOp,\n+                padding_config, hlo->shape(), hlo->sharding());\n+  if (!newOperand) {\n+    newOperand = add_hlo(HloInstruction::CreatePad(\n+        hlo->shape(), GetPartitionedHlo(update_tensor).hlo(), zeroElemOp,\n+        padding_config));\n+    newOperand->set_sharding(hlo->sharding());\n+  }\n+\n+  auto shard_result_shape = MakePartitionedShape(hlo->shape(), hlo->sharding());\n+  auto result = add_hlo(HloInstruction::CreateTernary(\n+      shard_result_shape, HloOpcode::kSelect, maskOp,\n+      GetPartitionedHlo(input_tensor).hlo(), newOperand));\n+  SetPartitionedHlo(hlo, result);\n+  return absl::OkStatus();\n+};\n+\n absl::Status SpmdPartitioningVisitor::HandleDynamicUpdateSlice(\n     HloInstruction* hlo) {\n   if (hlo->sharding().IsTileMaximal()) {\n     return DefaultAction(hlo);\n   }\n-  auto add_hlo = [&](std::unique_ptr<HloInstruction> to_add) {\n-    return b_.AddInstruction(std::move(to_add));\n-  };\n   const HloInstruction* input_tensor = hlo->operand(0);\n   const HloInstruction* update_tensor = hlo->operand(1);\n \n@@ -3783,191 +3973,25 @@ absl::Status SpmdPartitioningVisitor::HandleDynamicUpdateSlice(\n \n   // Method 1. Replicate the slice dimensions for all involved tensors.\n   if (analysis.method == DynamicUpdateSliceMethod::kDefault) {\n-    const HloSharding& input_sharding = input_tensor->sharding();\n-    const HloSharding& output_sharding = hlo->sharding();\n-    const HloSharding& better_sharding =\n-        input_sharding.NumTiles() > output_sharding.NumTiles()\n-            ? input_sharding\n-            : output_sharding;\n-\n-    HloSharding replicated_sharding =\n-        hlo_sharding_util::PartiallyReplicateTiledShardingOnDims(\n-            better_sharding, analysis.slice_dims);\n-    auto base = GetPartitionedHlo(input_tensor).Reshard(replicated_sharding);\n-    auto operand =\n-        GetPartitionedHlo(update_tensor).Reshard(replicated_sharding);\n-    auto dus = b_.AddInstruction(HloInstruction::CreateDynamicUpdateSlice(\n-        base.hlo()->shape(), base.hlo(), operand.hlo(), new_indices));\n-    dus->set_sharding(replicated_sharding);\n-    SetPartitionedHlo(hlo, PartitionedHlo(dus, base.base_shape(), base.state())\n-                               .Reshard(hlo->sharding()));\n-    return absl::OkStatus();\n+    return HandleDUSDefault(hlo, input_tensor, update_tensor, new_indices,\n+                            analysis.slice_dims);\n   }\n \n   // Method 2. Keep the sharding for input and output since the update is fully\n   // contained in a single partition.\n   if (analysis.method == DynamicUpdateSliceMethod::kUpdateOnASinglePartition) {\n-    // Get partitioned input.\n-    const auto& dus_sharding = hlo->sharding();\n-    const auto& partitioned_input =\n-        GetPartitionedHlo(input_tensor).Reshard(dus_sharding).hlo();\n-\n-    HloSharding update_sharding =\n-        hlo_sharding_util::PartiallyReplicateTiledShardingOnDims(\n-            dus_sharding, analysis.slice_dims);\n-\n-    HloInstruction* replicate_update =\n-        GetPartitionedHlo(update_tensor).Reshard(update_sharding).hlo();\n-\n-    const Shape& partitioned_shape = partitioned_input->shape();\n-    std::vector<HloInstruction*> partition_ordinals =\n-        MakeTiledPartitionOrdinals(hlo->sharding(),\n-                                   MakePartitioningState().partition_id, &b_);\n-    HloInstruction* all_dims_within_partition = add_hlo(\n-        HloInstruction::CreateConstant(LiteralUtil::CreateR0<bool>(true)));\n-\n-    for (int64_t dim : analysis.partitioned_slice_dims) {\n-      // Calculate per partition size.\n-      const int64_t per_partition_size = partitioned_shape.dimensions(dim);\n-\n-      // within_partition = (offset >= partition_id * per_partition_size) &&\n-      //                    (offset < (partition_id + 1) * per_partition_size)\n-      const Shape& compare_shape =\n-          ShapeUtil::ChangeElementType(partition_id_->shape(), PRED);\n-      auto per_partition_size_hlo = add_hlo(HloInstruction::CreateConstant(\n-          LiteralUtil::CreateR0<int>(per_partition_size)));\n-      const Shape& offset_shape = per_partition_size_hlo->shape();\n-      const Shape& index_shape = new_indices[dim]->shape();\n-      if (offset_shape.element_type() != index_shape.element_type()) {\n-        new_indices[dim] = add_hlo(HloInstruction::CreateConvert(\n-            ShapeUtil::ChangeElementType(index_shape,\n-                                         offset_shape.element_type()),\n-            new_indices[dim]));\n-      }\n-      HloInstruction* partition_offset = add_hlo(HloInstruction::CreateBinary(\n-          offset_shape, HloOpcode::kMultiply, partition_ordinals[dim],\n-          per_partition_size_hlo));\n-      // offset >= partition_id * per_partition_size\n-      HloInstruction* offset_ge = add_hlo(HloInstruction::CreateCompare(\n-          compare_shape, new_indices[dim], partition_offset,\n-          ComparisonDirection::kGe));\n-      // offset < (partition_id + 1) * per_partition_size\n-      HloInstruction* offset_lt = add_hlo(HloInstruction::CreateCompare(\n-          compare_shape, new_indices[dim],\n-          add_hlo(HloInstruction::CreateBinary(\n-              offset_shape, HloOpcode::kMultiply,\n-              add_hlo(HloInstruction::CreateBinary(\n-                  offset_shape, HloOpcode::kAdd, partition_ordinals[dim],\n-                  add_hlo(HloInstruction::CreateConstant(\n-                      LiteralUtil::CreateR0<int>(1))))),\n-              per_partition_size_hlo)),\n-          ComparisonDirection::kLt));\n-      HloInstruction* update_within_partition =\n-          add_hlo(HloInstruction::CreateBinary(compare_shape, HloOpcode::kAnd,\n-                                               offset_ge, offset_lt));\n-\n-      all_dims_within_partition = add_hlo(HloInstruction::CreateBinary(\n-          compare_shape, HloOpcode::kAnd, all_dims_within_partition,\n-          update_within_partition));\n-\n-      // Calculate offset.\n-      // slice dim offset = within_partition ?\n-      //                    offset - partition_id * per_partition_size : 0\n-      new_indices[dim] = add_hlo(HloInstruction::CreateTernary(\n-          new_indices[dim]->shape(), HloOpcode::kSelect,\n-          update_within_partition,\n-          add_hlo(HloInstruction::CreateBinary(\n-              new_indices[dim]->shape(), HloOpcode::kSubtract, new_indices[dim],\n-              partition_offset)),\n-          add_hlo(\n-              HloInstruction::CreateConstant(LiteralUtil::CreateR0<int>(0)))));\n-      if (new_indices[dim]->shape().element_type() !=\n-          index_shape.element_type()) {\n-        new_indices[dim] = add_hlo(HloInstruction::CreateConvert(\n-            ShapeUtil::ChangeElementType(new_indices[dim]->shape(),\n-                                         index_shape.element_type()),\n-            new_indices[dim]));\n-      }\n-    }\n-\n-    // Create dynamic update slice.\n-    HloInstruction* dus = add_hlo(HloInstruction::CreateDynamicUpdateSlice(\n-        partitioned_shape, partitioned_input, replicate_update, new_indices));\n-    // Select if update is needed\n-    SetPartitionedHlo(hlo,\n-                      add_hlo(HloInstruction::CreateTernary(\n-                          dus->shape(), HloOpcode::kSelect,\n-                          add_hlo(HloInstruction::CreateBroadcast(\n-                              ShapeUtil::ChangeElementType(dus->shape(), PRED),\n-                              all_dims_within_partition, {})),\n-                          dus, partitioned_input)));\n-    return absl::OkStatus();\n+    return HandleDUSSinglePartitionUpdate(hlo, input_tensor, update_tensor,\n+                                          new_indices, analysis.slice_dims,\n+                                          analysis.partitioned_slice_dims);\n   }\n \n   // Method 3: All partitioned slice dimensions have compile-time constant\n   // indices.\n   if (analysis.method == DynamicUpdateSliceMethod::\n                              kAllPartitionedSliceDimsHaveConstantIndices &&\n       module_->config().debug_options().xla_enable_enzyme_comms_opt()) {\n-    PaddingConfig padding_config;\n-    for (int64_t input_tensor_dim = 0;\n-         input_tensor_dim < hlo->shape().dimensions().size();\n-         ++input_tensor_dim) {\n-      auto padding_dim = padding_config.add_dimensions();\n-      padding_dim->set_interior_padding(0);\n-\n-      const HloInstruction* dus_index = hlo->operand(input_tensor_dim + 2);\n-      CHECK(dus_index->IsConstant());\n-\n-      int64_t start_index = dus_index->literal().GetIntegralAsS64({}).value();\n-      int64_t end_index =\n-          start_index + update_tensor->shape().dimensions(input_tensor_dim);\n-      int64_t padding_high =\n-          hlo->shape().dimensions(input_tensor_dim) - end_index;\n-      padding_dim->set_edge_padding_low(start_index);\n-      padding_dim->set_edge_padding_high(padding_high);\n-    }\n-\n-    const Shape operand_pred_shape =\n-        ShapeUtil::ChangeElementType(hlo->shape(), PRED);\n-    const Shape update_pred_shape =\n-        ShapeUtil::ChangeElementType(update_tensor->shape(), PRED);\n-    const Shape sharded_update_pred_shape =\n-        MakePartitionedShape(update_pred_shape, hlo->sharding());\n-\n-    auto zeroOperand = CreateZero(sharded_update_pred_shape, &b_);\n-    zeroOperand->set_sharding(hlo->sharding());\n-\n-    HloInstruction* paddingValue = CreateOne(Shape(PRED, {}), &b_);\n-    HloInstruction* maskOp = PadHelper(\n-        *this,\n-        PartitionedHlo(zeroOperand, update_pred_shape, MakePartitioningState()),\n-        paddingValue, padding_config, operand_pred_shape, hlo->sharding());\n-    if (!maskOp) {\n-      maskOp = add_hlo(HloInstruction::CreatePad(\n-          operand_pred_shape, zeroOperand, paddingValue, padding_config));\n-      maskOp->set_sharding(hlo->sharding());\n-    }\n-\n-    auto zeroElemOp = add_hlo(HloInstruction::CreateConstant(\n-        LiteralUtil::Zero(hlo->shape().element_type())));\n-    HloInstruction* newOperand =\n-        PadHelper(*this, GetPartitionedHlo(update_tensor), zeroElemOp,\n-                  padding_config, hlo->shape(), hlo->sharding());\n-    if (!newOperand) {\n-      newOperand = add_hlo(HloInstruction::CreatePad(\n-          hlo->shape(), GetPartitionedHlo(update_tensor).hlo(), zeroElemOp,\n-          padding_config));\n-      newOperand->set_sharding(hlo->sharding());\n-    }\n-\n-    auto shard_result_shape =\n-        MakePartitionedShape(hlo->shape(), hlo->sharding());\n-    auto result = add_hlo(HloInstruction::CreateTernary(\n-        shard_result_shape, HloOpcode::kSelect, maskOp,\n-        GetPartitionedHlo(input_tensor).hlo(), newOperand));\n-    SetPartitionedHlo(hlo, result);\n-    return absl::OkStatus();\n+    return HandleDUSAllPartitionedSliceDimsHaveConstantIndices(\n+        hlo, input_tensor, update_tensor);\n   }\n   return absl::OkStatus();\n }"
        },
        {
            "sha": "c430d29b65036c4599320b1386a90ef67c9d238f",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.h",
            "status": "modified",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4bafcbff551553cb2ba7952f2f330468faeec69f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4bafcbff551553cb2ba7952f2f330468faeec69f/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h?ref=4bafcbff551553cb2ba7952f2f330468faeec69f",
            "patch": "@@ -913,6 +913,30 @@ class SpmdPartitioningVisitor : public DfsHloVisitorWithDefault {\n   std::vector<PartitionedHlo::PartitioningState> visiting_state_;\n   std::optional<hlo_sharding_util::DeviceGroupTileAssignment> device_groups_;\n   const CallGraph& call_graph_;\n+\n+  // Dispatches DUS handler to one of the three implementations based on\n+  // analysis.\n+\n+  // Method 1. Replicate the slice dimensions for all involved\n+  // tensors.\n+  absl::Status HandleDUSDefault(HloInstruction* hlo,\n+                                const HloInstruction* input_tensor,\n+                                const HloInstruction* update_tensor,\n+                                std::vector<HloInstruction*>& new_indices,\n+                                std::vector<int64_t> slice_dims);\n+  // Method 2. Keep the sharding for input and output since the update is fully\n+  // contained in a single partition.\n+  absl::Status HandleDUSSinglePartitionUpdate(\n+      HloInstruction* hlo, const HloInstruction* input_tensor,\n+      const HloInstruction* update_tensor,\n+      std::vector<HloInstruction*>& new_indices,\n+      std::vector<int64_t> slice_dims,\n+      std::vector<int64_t> partitioned_slice_dims);\n+  // Method 3: All partitioned slice dimensions have compile-time constant\n+  // indices.\n+  absl::Status HandleDUSAllPartitionedSliceDimsHaveConstantIndices(\n+      HloInstruction* hlo, const HloInstruction* input_tensor,\n+      const HloInstruction* update_tensor);\n };\n \n }  // namespace spmd"
        }
    ],
    "stats": {
        "total": 402,
        "additions": 225,
        "deletions": 177
    }
}