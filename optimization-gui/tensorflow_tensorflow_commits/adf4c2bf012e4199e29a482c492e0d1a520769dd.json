{
    "author": "EusebioDM",
    "message": "Split `ConvolutionReorderThunk` into its own file\n\nThere's no shared logic in between the `ConvolutionReorderThunk` and the `ConvolutionThunk` thunk, so I think its cleaner for each to be defined in their own file.\n\nPiperOrigin-RevId: 818613921",
    "sha": "adf4c2bf012e4199e29a482c492e0d1a520769dd",
    "files": [
        {
            "sha": "22b4c4ae6c2ce05fdf419e5984b2deacd3785f61",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/adf4c2bf012e4199e29a482c492e0d1a520769dd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/adf4c2bf012e4199e29a482c492e0d1a520769dd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=adf4c2bf012e4199e29a482c492e0d1a520769dd",
            "patch": "@@ -511,7 +511,6 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service/gpu:gpu_conv_runner\",\n-        \"//xla/service/gpu:gpu_conv_runner_proto_cc\",\n         \"//xla/service/gpu:stream_executor_util\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:device_memory\",\n@@ -526,7 +525,6 @@ cc_library(\n         \"//xla/tsl/protobuf:dnn_proto_cc\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n-        \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/memory\",\n         \"@com_google_absl//absl/status\",\n@@ -551,6 +549,23 @@ xla_cc_test(\n     ],\n )\n \n+cc_library(\n+    name = \"convolution_reorder_thunk\",\n+    srcs = [\"convolution_reorder_thunk.cc\"],\n+    hdrs = [\"convolution_reorder_thunk.h\"],\n+    deps = [\n+        \":thunk\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/stream_executor:device_memory\",\n+        \"//xla/stream_executor:dnn\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n cc_library(\n     name = \"copy_thunk\",\n     srcs = [\"copy_thunk.cc\"],"
        },
        {
            "sha": "fb16abf6a55790809f89d2054afec2381172393d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/convolution_reorder_thunk.cc",
            "status": "added",
            "additions": 86,
            "deletions": 0,
            "changes": 86,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/adf4c2bf012e4199e29a482c492e0d1a520769dd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_reorder_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/adf4c2bf012e4199e29a482c492e0d1a520769dd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_reorder_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_reorder_thunk.cc?ref=adf4c2bf012e4199e29a482c492e0d1a520769dd",
            "patch": "@@ -0,0 +1,86 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/convolution_reorder_thunk.h\"\n+\n+#include <cstdint>\n+#include <optional>\n+#include <utility>\n+\n+#include \"absl/container/inlined_vector.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n+#include \"xla/stream_executor/dnn.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+ConvolutionReorderThunk::ConvolutionReorderThunk(\n+    ThunkInfo thunk_info, absl::Span<int64_t> filter_nchw,\n+    absl::InlinedVector<BufferAllocation::Slice, 2> operand_slices,\n+    absl::InlinedVector<BufferAllocation::Slice, 2> result_slices)\n+    : Thunk(Kind::kConvolutionReorder, thunk_info),\n+      filter_descriptor_(CreateFilterDescriptor(filter_nchw)),\n+      operand_buffers_(operand_slices),\n+      result_buffers_(result_slices) {}\n+\n+absl::Status ConvolutionReorderThunk::ExecuteOnStream(\n+    const ExecuteParams& params) {\n+  bool has_bias = operand_buffers_.size() > 1;\n+  CHECK_EQ(operand_buffers_.size(), result_buffers_.size());\n+\n+  const auto& buffer_allocations = *params.buffer_allocations;\n+\n+  auto filter_input = se::DeviceMemory<int8_t>(\n+      buffer_allocations.GetDeviceAddress(operand_buffers_[0]));\n+  auto filter_output = se::DeviceMemory<int8_t>(\n+      buffer_allocations.GetDeviceAddress(result_buffers_[0]));\n+  auto bias_input =\n+      has_bias ? std::make_optional(se::DeviceMemory<float>(\n+                     buffer_allocations.GetDeviceAddress(operand_buffers_[1])))\n+               : std::nullopt;\n+  auto bias_output =\n+      has_bias ? std::make_optional(se::DeviceMemory<float>(\n+                     buffer_allocations.GetDeviceAddress(result_buffers_[1])))\n+               : std::nullopt;\n+\n+  auto dnn = params.stream->parent()->AsDnn();\n+  if (dnn == nullptr) {\n+    return absl::InternalError(\"No DNN for stream.\");\n+  }\n+  return dnn->CudnnReorderConvolutionFilterAndBias(\n+      params.stream, filter_descriptor_, filter_input, &filter_output,\n+      std::move(bias_input), std::move(bias_output));\n+}\n+\n+se::dnn::FilterDescriptor ConvolutionReorderThunk::CreateFilterDescriptor(\n+    absl::Span<int64_t> filter_nchw) {\n+  CHECK_EQ(filter_nchw.size(), 4);\n+  se::dnn::FilterDescriptor filter_desc(2);\n+  filter_desc.set_layout(se::dnn::FilterLayout::kOutputInputYX32);\n+  filter_desc.set_output_feature_map_count(filter_nchw[0]);\n+  filter_desc.set_input_feature_map_count(filter_nchw[1]);\n+  filter_desc.set_input_filter_height(filter_nchw[2]);\n+  filter_desc.set_input_filter_width(filter_nchw[3]);\n+  return filter_desc;\n+}\n+\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "a35316ccc782997796c7b30ba8c4cc67983110e1",
            "filename": "third_party/xla/xla/backends/gpu/runtime/convolution_reorder_thunk.h",
            "status": "added",
            "additions": 56,
            "deletions": 0,
            "changes": 56,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/adf4c2bf012e4199e29a482c492e0d1a520769dd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_reorder_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/adf4c2bf012e4199e29a482c492e0d1a520769dd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_reorder_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_reorder_thunk.h?ref=adf4c2bf012e4199e29a482c492e0d1a520769dd",
            "patch": "@@ -0,0 +1,56 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_CONVOLUTION_REORDER_THUNK_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_CONVOLUTION_REORDER_THUNK_H_\n+\n+#include <cstdint>\n+\n+#include \"absl/container/inlined_vector.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/stream_executor/dnn.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+// Launches the kernel that reorders input data for int8x32 convolutions.\n+class ConvolutionReorderThunk : public Thunk {\n+ public:\n+  ConvolutionReorderThunk(\n+      ThunkInfo thunk_info, absl::Span<int64_t> filter_nchw,\n+      absl::InlinedVector<BufferAllocation::Slice, 2> operand_slices,\n+      absl::InlinedVector<BufferAllocation::Slice, 2> result_slices);\n+\n+  ConvolutionReorderThunk(const ConvolutionReorderThunk&) = delete;\n+  ConvolutionReorderThunk& operator=(const ConvolutionReorderThunk&) = delete;\n+\n+  absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n+\n+ private:\n+  static se::dnn::FilterDescriptor CreateFilterDescriptor(\n+      absl::Span<int64_t> filter_nchw);\n+\n+  const se::dnn::FilterDescriptor filter_descriptor_;\n+  absl::InlinedVector<BufferAllocation::Slice, 2> operand_buffers_;\n+  absl::InlinedVector<BufferAllocation::Slice, 2> result_buffers_;\n+};\n+\n+}  // namespace gpu\n+}  // namespace xla\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_CONVOLUTION_REORDER_THUNK_H_"
        },
        {
            "sha": "a34d7b91b5cef6969d1c4d7731b67c2c61e4e8c8",
            "filename": "third_party/xla/xla/backends/gpu/runtime/convolution_thunk.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 53,
            "changes": 53,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/adf4c2bf012e4199e29a482c492e0d1a520769dd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/adf4c2bf012e4199e29a482c492e0d1a520769dd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc?ref=adf4c2bf012e4199e29a482c492e0d1a520769dd",
            "patch": "@@ -15,14 +15,11 @@ limitations under the License.\n \n #include \"xla/backends/gpu/runtime/convolution_thunk.h\"\n \n-#include <cstdint>\n #include <memory>\n-#include <optional>\n #include <utility>\n #include <variant>\n #include <vector>\n \n-#include \"absl/container/inlined_vector.h\"\n #include \"absl/log/check.h\"\n #include \"absl/memory/memory.h\"\n #include \"absl/status/status.h\"\n@@ -200,55 +197,5 @@ absl::StatusOr<ThunkProto> ConvolutionThunk::ToProto() const {\n   return proto;\n }\n \n-ConvolutionReorderThunk::ConvolutionReorderThunk(\n-    ThunkInfo thunk_info, absl::Span<int64_t> filter_nchw,\n-    absl::InlinedVector<BufferAllocation::Slice, 2> operand_slices,\n-    absl::InlinedVector<BufferAllocation::Slice, 2> result_slices)\n-    : Thunk(Kind::kConvolutionReorder, thunk_info),\n-      filter_descriptor_(CreateFilterDescriptor(filter_nchw)),\n-      operand_buffers_(operand_slices),\n-      result_buffers_(result_slices) {}\n-\n-absl::Status ConvolutionReorderThunk::ExecuteOnStream(\n-    const ExecuteParams& params) {\n-  bool has_bias = operand_buffers_.size() > 1;\n-  CHECK_EQ(operand_buffers_.size(), result_buffers_.size());\n-\n-  const auto& buffer_allocations = *params.buffer_allocations;\n-\n-  auto filter_input = se::DeviceMemory<int8_t>(\n-      buffer_allocations.GetDeviceAddress(operand_buffers_[0]));\n-  auto filter_output = se::DeviceMemory<int8_t>(\n-      buffer_allocations.GetDeviceAddress(result_buffers_[0]));\n-  auto bias_input =\n-      has_bias ? std::make_optional(se::DeviceMemory<float>(\n-                     buffer_allocations.GetDeviceAddress(operand_buffers_[1])))\n-               : std::nullopt;\n-  auto bias_output =\n-      has_bias ? std::make_optional(se::DeviceMemory<float>(\n-                     buffer_allocations.GetDeviceAddress(result_buffers_[1])))\n-               : std::nullopt;\n-\n-  auto dnn = params.stream->parent()->AsDnn();\n-  if (dnn == nullptr) {\n-    return absl::InternalError(\"No DNN for stream.\");\n-  }\n-  return dnn->CudnnReorderConvolutionFilterAndBias(\n-      params.stream, filter_descriptor_, filter_input, &filter_output,\n-      std::move(bias_input), std::move(bias_output));\n-}\n-\n-se::dnn::FilterDescriptor ConvolutionReorderThunk::CreateFilterDescriptor(\n-    absl::Span<int64_t> filter_nchw) {\n-  CHECK_EQ(filter_nchw.size(), 4);\n-  se::dnn::FilterDescriptor filter_desc(2);\n-  filter_desc.set_layout(se::dnn::FilterLayout::kOutputInputYX32);\n-  filter_desc.set_output_feature_map_count(filter_nchw[0]);\n-  filter_desc.set_input_feature_map_count(filter_nchw[1]);\n-  filter_desc.set_input_filter_height(filter_nchw[2]);\n-  filter_desc.set_input_filter_width(filter_nchw[3]);\n-  return filter_desc;\n-}\n-\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "e456e23b816d8692ba0c7d920817bf830ba8d4db",
            "filename": "third_party/xla/xla/backends/gpu/runtime/convolution_thunk.h",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/adf4c2bf012e4199e29a482c492e0d1a520769dd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/adf4c2bf012e4199e29a482c492e0d1a520769dd/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.h?ref=adf4c2bf012e4199e29a482c492e0d1a520769dd",
            "patch": "@@ -16,21 +16,18 @@ limitations under the License.\n #ifndef XLA_BACKENDS_GPU_RUNTIME_CONVOLUTION_THUNK_H_\n #define XLA_BACKENDS_GPU_RUNTIME_CONVOLUTION_THUNK_H_\n \n-#include <cstdint>\n #include <memory>\n #include <vector>\n \n #include \"absl/base/thread_annotations.h\"\n #include \"absl/container/flat_hash_map.h\"\n-#include \"absl/container/inlined_vector.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/gpu_conv_runner.h\"\n-#include \"xla/stream_executor/dnn.h\"\n #include \"xla/stream_executor/stream.h\"\n \n namespace xla {\n@@ -87,28 +84,6 @@ class ConvolutionThunk : public Thunk {\n       runner_cache_ ABSL_GUARDED_BY(mu_);\n };\n \n-// Launches the kernel that reorders input data for int8x32 convolutions.\n-class ConvolutionReorderThunk : public Thunk {\n- public:\n-  ConvolutionReorderThunk(\n-      ThunkInfo thunk_info, absl::Span<int64_t> filter_nchw,\n-      absl::InlinedVector<BufferAllocation::Slice, 2> operand_slices,\n-      absl::InlinedVector<BufferAllocation::Slice, 2> result_slices);\n-\n-  ConvolutionReorderThunk(const ConvolutionReorderThunk&) = delete;\n-  ConvolutionReorderThunk& operator=(const ConvolutionReorderThunk&) = delete;\n-\n-  absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n-\n- private:\n-  static se::dnn::FilterDescriptor CreateFilterDescriptor(\n-      absl::Span<int64_t> filter_nchw);\n-\n-  const se::dnn::FilterDescriptor filter_descriptor_;\n-  absl::InlinedVector<BufferAllocation::Slice, 2> operand_buffers_;\n-  absl::InlinedVector<BufferAllocation::Slice, 2> result_buffers_;\n-};\n-\n }  // namespace gpu\n }  // namespace xla\n "
        },
        {
            "sha": "8158d28eed77de460cca12125ad25db251dd7700",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/adf4c2bf012e4199e29a482c492e0d1a520769dd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/adf4c2bf012e4199e29a482c492e0d1a520769dd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=adf4c2bf012e4199e29a482c492e0d1a520769dd",
            "patch": "@@ -463,6 +463,7 @@ cc_library(\n         \"//xla/backends/gpu/runtime:command_buffer_cmd_emitter\",\n         \"//xla/backends/gpu/runtime:command_buffer_thunk\",\n         \"//xla/backends/gpu/runtime:conditional_thunk\",\n+        \"//xla/backends/gpu/runtime:convolution_reorder_thunk\",\n         \"//xla/backends/gpu/runtime:convolution_thunk\",\n         \"//xla/backends/gpu/runtime:copy_thunk\",\n         \"//xla/backends/gpu/runtime:cub_sort_thunk\","
        },
        {
            "sha": "10dec475e18167b6082284d7c97b9032d3c9bab7",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/adf4c2bf012e4199e29a482c492e0d1a520769dd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/adf4c2bf012e4199e29a482c492e0d1a520769dd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=adf4c2bf012e4199e29a482c492e0d1a520769dd",
            "patch": "@@ -86,6 +86,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/command_buffer_cmd_emitter.h\"\n #include \"xla/backends/gpu/runtime/command_buffer_thunk.h\"\n #include \"xla/backends/gpu/runtime/conditional_thunk.h\"\n+#include \"xla/backends/gpu/runtime/convolution_reorder_thunk.h\"\n #include \"xla/backends/gpu/runtime/convolution_thunk.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/cub_sort_thunk.h\""
        }
    ],
    "stats": {
        "total": 241,
        "additions": 161,
        "deletions": 80
    }
}