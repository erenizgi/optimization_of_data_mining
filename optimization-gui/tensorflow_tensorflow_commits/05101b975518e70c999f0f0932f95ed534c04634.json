{
    "author": "hyeontaek",
    "message": "[PjRt-IFRT] Temporary workaround for output layout handling\n\nPjRt-IFRT directly or indirectly fetched optimized HLO to get the output\nlayout mode and output layouts. This seems to introduce a regression in\nsome jobs that use PJRT C API and have a too large serialized HLO (> 2 GiB).\n\nAs a workaround, PjRt-IFRT gracefully handles output layout mode and\nlayout discovery errors, and falls back to concrete layouts that are\ndirectly obtained from output `PjRtBuffer`s, should give the same\nbehavior before/after the default layout handling change.\n\nFurther changes will follow to discover default layout modes and layouts\nwithout going through `PjRtLoadedExecutable::GetHloModules()`.\n\nPiperOrigin-RevId: 820785277",
    "sha": "05101b975518e70c999f0f0932f95ed534c04634",
    "files": [
        {
            "sha": "7502c7693cee3794ab12d5b37d9dd1114fde19c7",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_executable.cc",
            "status": "modified",
            "additions": 106,
            "deletions": 22,
            "changes": 128,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05101b975518e70c999f0f0932f95ed534c04634/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05101b975518e70c999f0f0932f95ed534c04634/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.cc?ref=05101b975518e70c999f0f0932f95ed534c04634",
            "patch": "@@ -296,17 +296,34 @@ absl::StatusOr<LoadedExecutableRef> PjRtLoadedExecutable::Create(\n   TF_ASSIGN_OR_RETURN(\n       auto result_memory_kinds,\n       GetFirstModuleOutputMemoryKinds(pjrt_loaded_executable.get()));\n-  TF_ASSIGN_OR_RETURN(auto hlo_modules,\n-                      pjrt_loaded_executable->GetHloModules());\n-  if (hlo_modules.empty()) {\n-    return FailedPrecondition(\"Requires at least one HloModule.\");\n-  }\n-  TF_ASSIGN_OR_RETURN(std::vector<xla::LayoutMode> output_layout_modes,\n-                      GetLayoutModes(*hlo_modules.front(), \"out_layout_modes\",\n-                                     result_element_types.size()));\n-  TF_ASSIGN_OR_RETURN(auto output_layouts,\n-                      GetFirstModuleOutputLayouts(pjrt_loaded_executable.get(),\n-                                                  output_layout_modes));\n+  // Obtaining output layout modes and output layouts directly from\n+  // `PjRtLoadedExecutable` may fail because the currently PjRt implementations\n+  // often fetch and serialize the optimized HLO. For now, we gracefully\n+  // handle it by omitting output layouts at creation time and using output\n+  // `PjRtBuffer`'s concrete layouts.\n+  // TODO(hyeontaek): Add a way to obtain output layout modes and\n+  // `PjRtLoadedExecutable::GetOutputLayouts()` without causing the optimized\n+  // HLO to be serialized and fetched.\n+  std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n+      output_layouts;\n+  absl::StatusOr<std::vector<std::shared_ptr<HloModule>>> hlo_modules =\n+      pjrt_loaded_executable->GetHloModules();\n+  if (hlo_modules.ok()) {\n+    if (hlo_modules->empty()) {\n+      return FailedPrecondition(\"Requires at least one HloModule.\");\n+    }\n+    absl::StatusOr<std::vector<xla::LayoutMode>> output_layout_modes =\n+        GetLayoutModes(*hlo_modules->front(), \"out_layout_modes\",\n+                       result_element_types.size());\n+    if (output_layout_modes.ok()) {\n+      absl::StatusOr<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n+          first_module_output_layouts = GetFirstModuleOutputLayouts(\n+              pjrt_loaded_executable.get(), *output_layout_modes);\n+      if (first_module_output_layouts.ok()) {\n+        output_layouts = *std::move(first_module_output_layouts);\n+      }\n+    }\n+  }\n   return CreateInternal(client, std::move(pjrt_loaded_executable),\n                         result_element_types, result_dimensions,\n                         /*result_hlo_sharding=*/std::nullopt,\n@@ -352,8 +369,8 @@ absl::StatusOr<LoadedExecutableRef> PjRtLoadedExecutable::Create(\n   // will use the MLIR as scratch space, or possibly even deallocate it.\n   TF_ASSIGN_OR_RETURN(const std::vector<xla::Shape> result_shapes,\n                       ResultShapesOfModule(module));\n-  TF_ASSIGN_OR_RETURN(const std::vector<xla::LayoutMode> output_layout_modes,\n-                      GetOutputLayoutModes(module));\n+  absl::StatusOr<std::vector<xla::LayoutMode>> output_layout_modes =\n+      GetOutputLayoutModes(module);\n \n   TF_ASSIGN_OR_RETURN(auto pjrt_loaded_executable,\n                       client->pjrt_client()->CompileAndLoad(\n@@ -372,9 +389,24 @@ absl::StatusOr<LoadedExecutableRef> PjRtLoadedExecutable::Create(\n     TF_ASSIGN_OR_RETURN(\n         auto result_memory_kinds,\n         GetFirstModuleOutputMemoryKinds(pjrt_loaded_executable.get()));\n-    TF_ASSIGN_OR_RETURN(auto output_layouts,\n-                        GetFirstModuleOutputLayouts(\n-                            pjrt_loaded_executable.get(), output_layout_modes));\n+    // Obtaining output layout modes and output layouts directly from\n+    // `PjRtLoadedExecutable` may fail because the currently PjRt\n+    // implementations often fetch and serialize the optimized HLO. For now, we\n+    // gracefully handle it by omitting output layouts at creation time and\n+    // using output `PjRtBuffer`'s concrete layouts.\n+    // TODO(hyeontaek): Add a way to obtain output layout modes and\n+    // `PjRtLoadedExecutable::GetOutputLayouts()` without causing the optimized\n+    // HLO to be serialized and fetched.\n+    std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n+        output_layouts;\n+    if (output_layout_modes.ok()) {\n+      absl::StatusOr<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n+          first_module_output_layouts = GetFirstModuleOutputLayouts(\n+              pjrt_loaded_executable.get(), *output_layout_modes);\n+      if (first_module_output_layouts.ok()) {\n+        output_layouts = *std::move(first_module_output_layouts);\n+      }\n+    }\n     return CreateInternal(client, std::move(pjrt_loaded_executable),\n                           result_element_types, result_dimensions,\n                           /*result_hlo_sharding=*/std::nullopt,\n@@ -405,9 +437,24 @@ absl::StatusOr<LoadedExecutableRef> PjRtLoadedExecutable::Create(\n     TF_ASSIGN_OR_RETURN(\n         auto result_memory_kinds,\n         GetFirstModuleOutputMemoryKinds(pjrt_loaded_executable.get()));\n-    TF_ASSIGN_OR_RETURN(auto output_layouts,\n-                        GetFirstModuleOutputLayouts(\n-                            pjrt_loaded_executable.get(), output_layout_modes));\n+    // Obtaining output layout modes and output layouts directly from\n+    // `PjRtLoadedExecutable` may fail because the currently PjRt\n+    // implementations often fetch and serialize the optimized HLO. For now, we\n+    // gracefully handle it by omitting output layouts at creation time and\n+    // using output `PjRtBuffer`'s concrete layouts.\n+    // TODO(hyeontaek): Add a way to obtain output layout modes and\n+    // `PjRtLoadedExecutable::GetOutputLayouts()` without causing the optimized\n+    // HLO to be serialized and fetched.\n+    std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n+        output_layouts;\n+    if (output_layout_modes.ok()) {\n+      absl::StatusOr<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n+          first_module_output_layouts = GetFirstModuleOutputLayouts(\n+              pjrt_loaded_executable.get(), *output_layout_modes);\n+      if (first_module_output_layouts.ok()) {\n+        output_layouts = *std::move(first_module_output_layouts);\n+      }\n+    }\n     return CreateInternal(\n         client, std::move(pjrt_loaded_executable),\n         shape_partial_info.element_types, shape_partial_info.dimensions,\n@@ -423,7 +470,8 @@ absl::StatusOr<LoadedExecutableRef> PjRtLoadedExecutable::CreateInternal(\n     absl::Span<const xla::DimensionVector> result_dimensions,\n     const std::optional<xla::HloSharding>& result_hlo_sharding,\n     const std::optional<std::vector<absl::string_view>>& result_memory_kinds,\n-    const std::vector<std::shared_ptr<const xla::PjRtLayout>>& output_layouts,\n+    const std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>&\n+        output_layouts,\n     std::vector<tsl::RCReference<LoadedHostCallback>> loaded_host_callbacks,\n     DeviceListRef executable_devices) {\n   // For jit(pmap(...)), the device assignment (passed as `executable_devices`)\n@@ -596,7 +644,8 @@ PjRtLoadedExecutable::PjRtLoadedExecutable(\n         host_send_recv_callbacks,\n     std::vector<DType> output_dtypes, std::vector<Shape> output_shapes,\n     std::vector<ShardingRef> output_shardings,\n-    std::vector<std::shared_ptr<const xla::PjRtLayout>> output_layouts)\n+    std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n+        output_layouts)\n     : client_(client),\n       pjrt_loaded_executable_(std::move(pjrt_loaded_executable)),\n       devices_(std::move(devices)),\n@@ -812,6 +861,41 @@ PjRtLoadedExecutable::Execute(absl::Span<ArrayRef> args,\n   // memory_kind shares the same Sharding object.\n   absl::flat_hash_map<MemoryKind, ShardingRef> single_device_shardings;\n \n+  std::vector<std::shared_ptr<const xla::PjRtLayout>> layouts;\n+  layouts.reserve(num_outputs);\n+  if (output_layouts_.has_value()) {\n+    // TODO(hyeontaek): Once we can get `output_layouts_` reliably, only keep\n+    // this path.\n+    layouts = *output_layouts_;\n+  } else if (!pjrt_outputs.empty()) {\n+    for (int i = 0; i < num_outputs; ++i) {\n+      auto layout = output_dtypes_[i].kind() == xla::ifrt::DType::kToken\n+                        ? std::make_shared<xla::PjRtLayout>(xla::Layout())\n+                        : pjrt_outputs.front()[i]->layout();\n+      layouts.push_back(std::move(layout));\n+    }\n+  } else {\n+    auto maybe_layouts = GetOutputLayouts();\n+    if (absl::IsUnimplemented(maybe_layouts.status())) {\n+      for (int i = 0; i < num_outputs; ++i) {\n+        std::shared_ptr<const xla::PjRtLayout> layout;\n+        if (output_dtypes_[i].kind() == xla::ifrt::DType::kToken) {\n+          layout = std::make_shared<xla::PjRtLayout>(xla::Layout());\n+        } else {\n+          TF_ASSIGN_OR_RETURN(layout,\n+                              client_->GetDefaultPjRtLayout(\n+                                  output_dtypes_[i], output_shapes_[i].dims(),\n+                                  devices_->devices().front(),\n+                                  output_shardings_[i]->memory_kind()));\n+        }\n+        layouts.push_back(std::move(layout));\n+      }\n+    } else {\n+      TF_RETURN_IF_ERROR(maybe_layouts.status());\n+      layouts = *std::move(maybe_layouts);\n+    }\n+  }\n+\n   for (int i = 0; i < num_outputs; ++i) {\n     PjRtArray::PjRtBuffers buffers;\n     buffers.reserve(num_computations);\n@@ -852,7 +936,7 @@ PjRtLoadedExecutable::Execute(absl::Span<ArrayRef> args,\n     }\n     outputs.push_back(*PjRtArray::Create(\n         client_, output_dtypes_[i], output_shapes_[i], *std::move(sharding),\n-        std::move(buffers), output_layouts_[i]));\n+        std::move(buffers), std::move(layouts[i])));\n   }\n \n   ExecuteResult result;"
        },
        {
            "sha": "2e2dea13984d4d2aa83fdcdfa77a9641f773c680",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_executable.h",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05101b975518e70c999f0f0932f95ed534c04634/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05101b975518e70c999f0f0932f95ed534c04634/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_executable.h?ref=05101b975518e70c999f0f0932f95ed534c04634",
            "patch": "@@ -339,7 +339,8 @@ class PjRtLoadedExecutable final\n       absl::Span<const xla::DimensionVector> result_dimensions,\n       const std::optional<xla::HloSharding>& result_hlo_sharding,\n       const std::optional<std::vector<absl::string_view>>& result_memory_kinds,\n-      const std::vector<std::shared_ptr<const xla::PjRtLayout>>& output_layouts,\n+      const std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>&\n+          output_layouts,\n       std::vector<tsl::RCReference<LoadedHostCallback>> loaded_host_callbacks,\n       DeviceListRef executable_devices);\n \n@@ -353,7 +354,8 @@ class PjRtLoadedExecutable final\n           host_send_recv_callbacks,\n       std::vector<DType> output_dtypes, std::vector<Shape> output_shapes,\n       std::vector<ShardingRef> output_shardings,\n-      std::vector<std::shared_ptr<const xla::PjRtLayout>> output_layouts);\n+      std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n+          output_layouts);\n \n   PjRtClient* client_;\n   std::shared_ptr<xla::PjRtLoadedExecutable> pjrt_loaded_executable_;\n@@ -372,7 +374,8 @@ class PjRtLoadedExecutable final\n   std::vector<DType> output_dtypes_;\n   std::vector<Shape> output_shapes_;\n   std::vector<ShardingRef> output_shardings_;\n-  std::vector<std::shared_ptr<const xla::PjRtLayout>> output_layouts_;\n+  std::optional<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n+      output_layouts_;\n   const xla::ifrt::UserContextRef user_context_;\n };\n "
        }
    ],
    "stats": {
        "total": 137,
        "additions": 112,
        "deletions": 25
    }
}