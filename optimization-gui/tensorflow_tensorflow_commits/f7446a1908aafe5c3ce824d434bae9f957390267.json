{
    "author": "pifon2a",
    "message": "[XLA:GPU] Add a tool to optimize llvm::Module and compile to PTX.\n\nPiperOrigin-RevId: 837446324",
    "sha": "f7446a1908aafe5c3ce824d434bae9f957390267",
    "files": [
        {
            "sha": "262a448e9e2f9e43f707e68f5bb409b45ae3c995",
            "filename": "third_party/xla/xla/tools/ptx_opt/BUILD",
            "status": "added",
            "additions": 63,
            "deletions": 0,
            "changes": 63,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7446a1908aafe5c3ce824d434bae9f957390267/third_party%2Fxla%2Fxla%2Ftools%2Fptx_opt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7446a1908aafe5c3ce824d434bae9f957390267/third_party%2Fxla%2Fxla%2Ftools%2Fptx_opt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fptx_opt%2FBUILD?ref=f7446a1908aafe5c3ce824d434bae9f957390267",
            "patch": "@@ -0,0 +1,63 @@\n+load(\"//xla:lit.bzl\", \"lit_test_suite\")\n+load(\"//xla:xla.default.bzl\", \"xla_cc_binary\")\n+load(\"//xla/tsl:tsl.bzl\", \"if_google\")\n+load(\"//xla/tsl/platform/default:build_config_root.bzl\", \"tf_gpu_tests_tags\")\n+load(\"//xla/tsl/platform/default:cuda_build_defs.bzl\", \"if_cuda_is_configured\")\n+\n+package(\n+    # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n+    licenses = [\"notice\"],\n+)\n+\n+xla_cc_binary(\n+    name = \"ptx_opt\",\n+    srcs = [\"ptx_opt.cc\"],\n+    # We want to use this tool for lit tests. Due to hermetic cuda, we need to\n+    # set linkopts in such a way that dynamic libraries are found, which are\n+    # symlinked from the lit_lib directory.\n+    linkopts = [\"-Wl,-rpath,$$ORIGIN/../lit_lib\"],\n+    tags = [\n+        \"cuda-only\",\n+        \"gpu\",\n+    ],\n+    deps = [\n+        \"//xla:debug_options_flags\",\n+        \"//xla/service/gpu/llvm_gpu_backend:load_ir_module\",\n+        \"//xla/service/gpu/llvm_gpu_backend:nvptx_backend\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/tsl/util:command_line_flags\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@llvm-project//llvm:Core\",\n+        \"@llvm-project//llvm:NVPTXCodeGen\",  # buildcleaner: keep\n+        \"@llvm-project//llvm:ObjCARC\",  # buildcleaner: keep\n+        \"@local_tsl//tsl/platform:platform_port\",\n+    ] + if_cuda_is_configured([\n+        \"//xla/stream_executor/cuda:cuda_platform\",\n+    ]),\n+)\n+\n+lit_test_suite(\n+    name = \"ptx_opt_tests\",\n+    # It fails in OSS to load the module correctly.\n+    srcs = if_google(\n+        glob([\"**/*.ll\"]),\n+        [],\n+    ),\n+    args = if_cuda_is_configured([\n+        \"--param=PTX=PTX\",\n+        \"--param=GPU=a100_pcie_80\",\n+    ]),\n+    cfg = \"//xla:lit.cfg.py\",\n+    default_tags = tf_gpu_tests_tags(),\n+    hermetic_cuda_data_dir = \"%S/../../../../../cuda_nvcc\",\n+    tags = [\n+        \"cuda-only\",\n+        \"gpu\",\n+    ],\n+    tools = [\n+        \":ptx_opt\",\n+        \"@llvm-project//llvm:FileCheck\",\n+    ],\n+)"
        },
        {
            "sha": "df00cb8039c253354f4fff7be26c6917e5f641e0",
            "filename": "third_party/xla/xla/tools/ptx_opt/ptx_opt.cc",
            "status": "added",
            "additions": 93,
            "deletions": 0,
            "changes": 93,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7446a1908aafe5c3ce824d434bae9f957390267/third_party%2Fxla%2Fxla%2Ftools%2Fptx_opt%2Fptx_opt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7446a1908aafe5c3ce824d434bae9f957390267/third_party%2Fxla%2Fxla%2Ftools%2Fptx_opt%2Fptx_opt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fptx_opt%2Fptx_opt.cc?ref=f7446a1908aafe5c3ce824d434bae9f957390267",
            "patch": "@@ -0,0 +1,93 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <iostream>\n+#include <memory>\n+#include <string>\n+#include <vector>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"llvm/IR/LLVMContext.h\"\n+#include \"llvm/IR/Module.h\"\n+#include \"xla/debug_options_flags.h\"\n+#include \"xla/service/gpu/llvm_gpu_backend/load_ir_module.h\"\n+#include \"xla/service/gpu/llvm_gpu_backend/nvptx_backend.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/tsl/util/command_line_flags.h\"\n+#include \"tsl/platform/init_main.h\"\n+\n+namespace xla::gpu::nvptx {\n+\n+absl::Status Run(const std::string& arch, const std::string& input_ll_path) {\n+  if (input_ll_path.empty()) {\n+    return absl::InvalidArgumentError(\"Input file path is required.\");\n+  }\n+  if (arch.empty()) {\n+    return absl::InvalidArgumentError(\"--arch is required.\");\n+  }\n+\n+  llvm::LLVMContext ctx;\n+  std::unique_ptr<llvm::Module> module = LoadIRModule(input_ll_path, &ctx);\n+  if (!module) {\n+    return absl::InternalError(\n+        absl::StrCat(\"Failed to load module from \", input_ll_path));\n+  }\n+\n+  // Create a GpuComputeCapability from the arch flag.\n+  auto cuda_compute_capability =\n+      stream_executor::CudaComputeCapability::FromString(arch);\n+  if (!cuda_compute_capability.ok()) {\n+    return absl::InvalidArgumentError(\n+        absl::StrCat(\"Invalid GPU architecture: \", arch));\n+  }\n+\n+  stream_executor::GpuComputeCapability gpu_version(*cuda_compute_capability);\n+\n+  // Get DebugOptions.\n+  DebugOptions debug_options = xla::GetDebugOptionsFromFlags();\n+  auto llvm_opts = GetNVPTXBackendOptions(debug_options);\n+\n+  // Compile to PTX.\n+  auto ptx_or = CompileToPtx(module.get(), gpu_version, debug_options);\n+  std::cout << *ptx_or << std::endl;\n+  return absl::OkStatus();\n+}\n+\n+}  // namespace xla::gpu::nvptx\n+\n+int main(int argc, char* argv[]) {\n+  std::string arch;\n+  std::vector<tsl::Flag> flag_list = {tsl::Flag(\n+      \"arch\", &arch,\n+      \"The GPU architecture to target, e.g., '8.6' or '9.0a' or '10.0f'.\")};\n+  xla::AppendDebugOptionsFlags(&flag_list);\n+\n+  const std::string usage = tsl::Flags::Usage(argv[0], flag_list);\n+  bool parse_result = tsl::Flags::Parse(&argc, argv, flag_list);\n+\n+  tsl::port::InitMain(usage.c_str(), &argc, &argv);\n+  if (!parse_result || argc != 2) {\n+    std::cerr << usage << std::endl;\n+    return 1;\n+  }\n+  absl::Status status = xla::gpu::nvptx::Run(arch, argv[1]);\n+  if (!status.ok()) {\n+    std::cerr << \"Error: \" << status << std::endl;\n+    return 1;\n+  }\n+  return 0;\n+}"
        },
        {
            "sha": "21b49c0f6e8cdebea46fc8f480fd184791db8104",
            "filename": "third_party/xla/xla/tools/ptx_opt/tests/acos.ll",
            "status": "added",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f7446a1908aafe5c3ce824d434bae9f957390267/third_party%2Fxla%2Fxla%2Ftools%2Fptx_opt%2Ftests%2Facos.ll",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f7446a1908aafe5c3ce824d434bae9f957390267/third_party%2Fxla%2Fxla%2Ftools%2Fptx_opt%2Ftests%2Facos.ll",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2Fptx_opt%2Ftests%2Facos.ll?ref=f7446a1908aafe5c3ce824d434bae9f957390267",
            "patch": "@@ -0,0 +1,36 @@\n+; RUN: ptx_opt  %s --arch=9.0 | FileCheck %s\n+\n+target datalayout = \"e-p6:32:32-i64:64-i128:128-i256:256-v16:16-v32:32-n16:32:64\"\n+\n+define ptx_kernel void @loop_acos_fusion(ptr noalias align 16 dereferenceable(1024) %0, ptr noalias align 256 dereferenceable(1024) %1) #0 {\n+  %3 = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !1\n+  %4 = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !2\n+  %5 = mul i32 %3, 128\n+  %6 = add i32 %5, %4\n+  %7 = getelementptr inbounds [256 x float], ptr %0, i32 0, i32 %6\n+  %8 = load float, ptr %7, align 4, !invariant.load !3\n+  %9 = call float @__nv_acosf(float %8)\n+  %10 = getelementptr inbounds [256 x float], ptr %1, i32 0, i32 %6\n+  store float %9, ptr %10, align 4\n+  ret void\n+}\n+\n+; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)\n+declare noundef range(i32 0, 2147483647) i32 @llvm.nvvm.read.ptx.sreg.ctaid.x() #1\n+\n+; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)\n+declare noundef range(i32 0, 1024) i32 @llvm.nvvm.read.ptx.sreg.tid.x() #1\n+\n+declare float @__nv_acosf(float)\n+\n+attributes #0 = { \"nvvm.reqntid\"=\"128,1,1\" }\n+attributes #1 = { nocallback nofree nosync nounwind speculatable willreturn memory(none) }\n+\n+!llvm.module.flags = !{!0}\n+\n+!0 = !{i32 2, !\"Debug Info Version\", i32 3}\n+!1 = !{i32 0, i32 2}\n+!2 = !{i32 0, i32 128}\n+!3 = !{}\n+\n+; CHECK: .target sm_90\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 192,
        "additions": 192,
        "deletions": 0
    }
}