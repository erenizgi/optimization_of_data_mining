{
    "author": "ZixuanJiang",
    "message": "Use brace initialization instead of `std::make_pair`.\n\nPiperOrigin-RevId: 798069513",
    "sha": "53e2370dcdc97582488721ada6c6a61059ab7cd9",
    "files": [
        {
            "sha": "980ed909d5ec8bae854210542163ff5b9b22249e",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/53e2370dcdc97582488721ada6c6a61059ab7cd9/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/53e2370dcdc97582488721ada6c6a61059ab7cd9/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=53e2370dcdc97582488721ada6c6a61059ab7cd9",
            "patch": "@@ -120,7 +120,7 @@ void SpmdLogger::RegisterLogEntry(HloInstruction* hlo,\n     max_value = std::max<int64_t>(max_value, ShapeSizeInBytes(inst->shape()));\n     absl::StrAppend(&report, \"     * \", inst->ToString(), \"\\n\");\n   }\n-  entries_.push_back(std::make_pair(max_value, report));\n+  entries_.push_back({max_value, report});\n }\n \n /* static */ std::string SpmdLogger::ReportBeforePartition(\n@@ -3215,7 +3215,7 @@ absl::Status SpmdPartitioningVisitor::HandleReshape(HloInstruction* hlo) {\n   auto insert_sharding_pair = [&](const HloSharding& in_sharding,\n                                   const HloSharding& out_sharding) {\n     if (in_sharding.NumTiles() == out_sharding.NumTiles()) {\n-      sharding_pairs.push_back(std::make_pair(in_sharding, out_sharding));\n+      sharding_pairs.push_back({in_sharding, out_sharding});\n     }\n   };\n \n@@ -5220,7 +5220,7 @@ SpmdPartitioner::AllGatherShardsInternal(\n     int64_t* next_channel_id, absl::Span<const int64_t> selected_dims,\n     const SPMDCollectiveOpsCreator& collectives_creator, bool per_dim_ag) {\n   if (selected_dims.empty()) {\n-    return std::make_pair(operand, nullptr);\n+    return {operand, nullptr};\n   }\n   CHECK(!sharding.IsTileMaximal());\n   if (per_dim_ag || selected_dims.size() == 1) {\n@@ -5255,7 +5255,7 @@ SpmdPartitioner::AllGatherShardsInternal(\n             /*all_gather_dimension=*/*it);\n       }\n     }\n-    return std::make_pair(result, result);\n+    return {result, result};\n   }\n \n   std::vector<int64_t> shape;\n@@ -5342,7 +5342,7 @@ SpmdPartitioner::AllGatherShardsInternal(\n         i, ag_shape.dimensions(i) * sharding.tile_assignment().dim(i));\n   }\n   result = b->AddInstruction(HloInstruction::CreateReshape(ag_shape, result));\n-  return std::make_pair(result, ag);\n+  return {result, ag};\n }\n \n HloInstruction* SpmdPartitioner::AllReduceAlongShardingDims("
        },
        {
            "sha": "ae67d7dcd78c8fbb021b559603b9de88d0dba04d",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/53e2370dcdc97582488721ada6c6a61059ab7cd9/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/53e2370dcdc97582488721ada6c6a61059ab7cd9/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_test.cc?ref=53e2370dcdc97582488721ada6c6a61059ab7cd9",
            "patch": "@@ -15135,19 +15135,10 @@ TEST_P(SpmdPartitioningTest, GatherCostModelForUnmatchedSharding) {\n   const char* const hlo_string = R\"(\n HloModule pjit\n \n-region_10.581.clone {\n-  Arg_0.53 = bf16[] parameter(0)\n-  Arg_1.53 = bf16[] parameter(1)\n-  ROOT add.1294 = bf16[] add(Arg_0.53, Arg_1.53)\n-}\n-\n ENTRY %main.21 {\n   p0 = bf16[8192,128]{1,0} parameter(0), sharding={devices=[2,4,2]<=[2,4,2]T(2,1,0) last_tile_dim_replicate}\n   p1 = s32[16384,1]{1,0} parameter(1), sharding={devices=[8,1,2]<=[16] last_tile_dim_replicate}\n-  gather.0 = bf16[16384,128]{1,0} gather(p0, p1), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,128}, sharding={devices=[8,2]<=[16]}\n-  constant.2467 = bf16[] constant(0)\n-  reduce.1749 = bf16[16384]{0} reduce(gather.0, constant.2467), dimensions={1}, to_apply=region_10.581.clone, sharding={devices=[8,2]<=[16] last_tile_dim_replicate}\n-  ROOT copy.1 = bf16[16384]{0} copy(reduce.1749), sharding={devices=[8,2]<=[16] last_tile_dim_replicate}\n+  ROOT gather.0 = bf16[16384,128]{1,0} gather(p0, p1), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,128}, sharding={devices=[8,2]<=[16]}\n }\n )\";\n "
        }
    ],
    "stats": {
        "total": 21,
        "additions": 6,
        "deletions": 15
    }
}