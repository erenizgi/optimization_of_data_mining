{
    "author": "vwbaker",
    "message": "Integrate Triton up to\n[7decd43](https://github.com/openai/triton/commits/7decd434a2dbf76e74ea3c2a5ec877326b83f4fa)\n\nhttps://github.com/openxla/triton/tree/triton_integrate_branch-1.12\n\nPiperOrigin-RevId: 810560703",
    "sha": "f2a3cf52d2aed6a15e2671a153ffa9740b378a16",
    "files": [
        {
            "sha": "ae836b4d9efb843a3bfbafb79113efc131fb0429",
            "filename": "third_party/xla/third_party/triton/llvm_integration/cl801607173.patch",
            "status": "modified",
            "additions": 18,
            "deletions": 24,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl801607173.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl801607173.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl801607173.patch?ref=f2a3cf52d2aed6a15e2671a153ffa9740b378a16",
            "patch": "@@ -1,17 +1,10 @@\n \n---- a/third_party/amd/lib/TritonAMDGPUToLLVM/AtomicRMWOpsEmitter.cpp\t2025-07-31 00:13:23.000000000 -0700\n-+++ b/third_party/amd/lib/TritonAMDGPUToLLVM/AtomicRMWOpsEmitter.cpp\t2025-08-31 17:53:42.000000000 -0700\n-@@ -405,12 +405,31 @@\n-   Value mask = targetInfo.ballot(rewriter, loc, i64_ty, done);\n+--- a/third_party/amd/lib/TritonAMDGPUToLLVM/AtomicRMWOpsEmitter.cpp\n++++ b/third_party/amd/lib/TritonAMDGPUToLLVM/AtomicRMWOpsEmitter.cpp\n+@@ -407,13 +407,28 @@ Value AtomicRMWEmitter::atomicIntraWaveReduce(RewriterBase &rewriter,\n    Value start = loopBody->getArgument(0);\n    Value cnt = b.trunc(i32_ty, generatePopcount64(rewriter, mask));\n--  Value mbcntLoRes = rewriter\n--                         .create<ROCDL::MbcntLoOp>(\n--                             loc, i32_ty, b.trunc(i32_ty, mask), b.i32_val(0))\n--                         ->getResult(0);\n--  Value idx = rewriter.create<ROCDL::MbcntHiOp>(\n--      loc, i32_ty, b.trunc(i32_ty, b.lshr(mask, b.i64_val(32))), mbcntLoRes);\n-+\n+   Value maskLo = b.trunc(i32_ty, mask);\n +  NamedAttribute noundef = rewriter.getNamedAttr(\n +      LLVM::LLVMDialect::getNoUndefAttrName(), rewriter.getUnitAttr());\n +  NamedAttribute lowRange = rewriter.getNamedAttr(\n@@ -22,20 +15,21 @@\n +      LLVM::LLVMDialect::getRangeAttrName(),\n +      LLVM::ConstantRangeAttr::get(rewriter.getContext(), APInt::getZero(32),\n +                                   APInt(32, 64)));\n-+\n-+  Value mbcntLoRes =\n+   Value mbcntLoRes =\n+-      ROCDL::MbcntLoOp::create(rewriter, loc, i32_ty, maskLo, b.i32_val(0),\n+-                               /*arg_attrs=*/{}, /*res_attrs=*/{});\n +      ROCDL::MbcntLoOp::create(\n-+          rewriter, loc, i32_ty, b.trunc(i32_ty, mask), b.i32_val(0),\n-+          /*arg_attrs=*/{},\n-+          /*res_attrs=*/\n-+          rewriter.getArrayAttr(\n-+              rewriter.getDictionaryAttr({noundef, lowRange})))\n-+          ->getResult(0);\n-+  Value idx = ROCDL::MbcntHiOp::create(\n-+      rewriter, loc, i32_ty, b.trunc(i32_ty, b.lshr(mask, b.i64_val(32))),\n-+      mbcntLoRes,\n-+      /*arg_attrs=*/{},\n-+      rewriter.getArrayAttr(rewriter.getDictionaryAttr({noundef, highRange})));\n++        rewriter, loc, i32_ty, maskLo, b.i32_val(0),\n++        /*arg_attrs=*/{},\n++        /*res_attrs=*/rewriter.getArrayAttr(\n++          rewriter.getDictionaryAttr({noundef, lowRange})));\n+   Value maskHi = b.trunc(i32_ty, b.lshr(mask, b.i64_val(32)));\n+   Value idx =\n+       ROCDL::MbcntHiOp::create(rewriter, loc, i32_ty, maskHi, mbcntLoRes,\n+-                               /*arg_attrs=*/{}, /*res_attrs=*/{});\n++        /*arg_attrs=*/{},\n++        /*res_attrs=*/rewriter.getArrayAttr(\n++          rewriter.getDictionaryAttr({noundef, highRange})));\n    Value base = b.add(start, cnt);\n    Value leader = b.icmp_eq(idx, b.i32_val(0));\n    cnt = b.sub(cnt, idx);"
        },
        {
            "sha": "5e7c6765cb4c3575e3c22f352f565e48385c9edb",
            "filename": "third_party/xla/third_party/triton/llvm_integration/cl808150672.patch",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl808150672.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl808150672.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fllvm_integration%2Fcl808150672.patch?ref=f2a3cf52d2aed6a15e2671a153ffa9740b378a16",
            "patch": "@@ -1,30 +1,30 @@\n \n---- a/test/Conversion/tritongpu_to_llvm.mlir\t2025-08-22 04:02:56.000000000 -0700\n-+++ b/test/Conversion/tritongpu_to_llvm.mlir\t2025-09-17 07:59:30.000000000 -0700\n+--- a/test/Conversion/tritongpu_to_llvm.mlir\n++++ b/test/Conversion/tritongpu_to_llvm.mlir\n @@ -1,4 +1,7 @@\n -// RUN: triton-opt %s -split-input-file --allocate-shared-memory-nv --convert-triton-gpu-to-llvm 2>/dev/null | FileCheck %s --dump-input-context 20\n +// RUN: triton-opt %s -split-input-file \\\n +// RUN:   --allocate-shared-memory-nv --convert-triton-gpu-to-llvm \\\n +// RUN:   --reconcile-unrealized-casts 2>/dev/null \\\n +// RUN:   | FileCheck %s --dump-input-context 20\n- \n+\n  module attributes {\"ttg.num-ctas\" = 1 : i32, \"ttg.num-warps\" = 4 : i32} {\n-   // CHECK: llvm.func @test_empty_kernel(%arg0: i32, %arg1: !llvm.ptr<1>, %arg2: !llvm.ptr<1>)\n+   // CHECK: llvm.func @test_empty_kernel(%arg0: i32, %arg1: !llvm.ptr<1>, %arg2: !llvm.ptr<1>, %arg3: !llvm.ptr<1>)\n \n---- a/test/Conversion/tritonnvidiagpu_to_llvm.mlir\t2025-07-31 00:13:23.000000000 -0700\n-+++ b/test/Conversion/tritonnvidiagpu_to_llvm.mlir\t2025-09-17 07:59:30.000000000 -0700\n+--- a/test/Conversion/tritonnvidiagpu_to_llvm.mlir\n++++ b/test/Conversion/tritonnvidiagpu_to_llvm.mlir\n @@ -1,4 +1,7 @@\n -// RUN: triton-opt %s -split-input-file --convert-triton-gpu-to-llvm=compute-capability=90 | FileCheck %s\n +// RUN: triton-opt %s -split-input-file \\\n +// RUN: --convert-triton-gpu-to-llvm=compute-capability=90 \\\n +// RUN: --reconcile-unrealized-casts \\\n +// RUN: | FileCheck %s\n- \n+\n  #shared0 = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n  #smem = #ttg.shared_memory\n \n---- a/third_party/amd/lib/TritonAMDGPUTransforms/CanonicalizePointers.cpp\t2025-07-31 00:13:23.000000000 -0700\n-+++ b/third_party/amd/lib/TritonAMDGPUTransforms/CanonicalizePointers.cpp\t2025-09-17 07:59:30.000000000 -0700\n+--- a/third_party/amd/lib/TritonAMDGPUTransforms/CanonicalizePointers.cpp\n++++ b/third_party/amd/lib/TritonAMDGPUTransforms/CanonicalizePointers.cpp\n @@ -1415,7 +1415,7 @@\n      if (auto integerAttr =\n              llvm::dyn_cast_or_null<mlir::IntegerAttr>(maybeAttr)) {"
        },
        {
            "sha": "74e7e69d8d29487dfe53b7607fe61d6e8e14c03d",
            "filename": "third_party/xla/third_party/triton/temporary/convert_layout_op_to_llvm_small_width.patch",
            "status": "added",
            "additions": 71,
            "deletions": 0,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconvert_layout_op_to_llvm_small_width.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconvert_layout_op_to_llvm_small_width.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconvert_layout_op_to_llvm_small_width.patch?ref=f2a3cf52d2aed6a15e2671a153ffa9740b378a16",
            "patch": "@@ -0,0 +1,71 @@\n+This fix should be upstreamed.\n+\n+--- a/lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp\t2025-08-28 04:30:50.000000000 -0700\n++++ b/lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp\t2025-09-10 06:50:50.000000000 -0700\n+@@ -331,11 +331,11 @@\n+     // Pack registers if possible.\n+     int elemsPerVec = 1 << nPack;\n+     int bitsPerVecElem = 32 / elemsPerVec;\n++    bool packTwo16Bit = bitwidth < bitsPerVecElem && elemsPerVec == 2;\n+     if (elemsPerVec > 1) {\n+       SmallVector<Value> packedVals;\n+       packedVals.reserve(regDim / elemsPerVec);\n+-      if (bitwidth < bitsPerVecElem) {\n+-        // Should have bitsPerVecElem == 16 here.\n++      if (packTwo16Bit) {\n+         for (int i = 0; i < regDim; i += elemsPerVec) {\n+           Value x0 = b.zext(i32_ty, b.bitcast(inVals[i], int_ty(bitwidth)));\n+           Value x1 = b.zext(i32_ty, b.bitcast(inVals[i + 1], int_ty(bitwidth)));\n+@@ -343,6 +343,10 @@\n+           packedVals.emplace_back(b.or_(x0, x1));\n+         }\n+       } else {\n++        // For small types, we need to extend the values to i8.\n++        if (bitwidth < 8) {\n++          llvm::for_each(inVals, [&](Value& v) { v = b.zext(i8_ty, v); });\n++        }\n+         for (int i = 0; i < regDim; i += elemsPerVec) {\n+           auto slice = ArrayRef<Value>(inVals).slice(i, elemsPerVec);\n+           Value v = packLLVector(loc, slice, rewriter);\n+@@ -376,9 +380,21 @@\n+     if (elemsPerVec > 1) {\n+       SmallVector<Value> unpackedVals;\n+       unpackedVals.reserve(regDim);\n+-      if (bitwidth >= bitsPerVecElem) {\n++      if (packTwo16Bit) {\n++        for (auto packedVal : outVals) {\n++          Value x0 =\n++              b.trunc(int_ty(bitwidth), b.and_(packedVal, b.i32_val(0xFF)));\n++          Value x1 =\n++              b.trunc(int_ty(bitwidth), b.lshr(packedVal, b.i32_val(16)));\n++          unpackedVals.push_back(b.bitcast(x0, elemTy));\n++          unpackedVals.push_back(b.bitcast(x1, elemTy));\n++        }\n++      } else {\n+         auto packedTy =\n+             bitwidth < bitsPerVecElem ? int_ty(bitsPerVecElem) : elemTy;\n++        if (bitwidth < 8) {\n++          packedTy = i8_ty;\n++        }\n+         auto vecTy = vec_ty(packedTy, elemsPerVec);\n+         auto unpackVal = [&](Value v) {\n+           v = b.bitcast(v, vecTy);\n+@@ -388,14 +404,10 @@\n+           auto unpacked = unpackVal(v);\n+           unpackedVals.append(unpacked.begin(), unpacked.end());\n+         }\n+-      } else {\n+-        for (auto packedVal : outVals) {\n+-          Value x0 =\n+-              b.trunc(int_ty(bitwidth), b.and_(packedVal, b.i32_val(0xFF)));\n+-          Value x1 =\n+-              b.trunc(int_ty(bitwidth), b.lshr(packedVal, b.i32_val(16)));\n+-          unpackedVals.push_back(b.bitcast(x0, elemTy));\n+-          unpackedVals.push_back(b.bitcast(x1, elemTy));\n++        if (bitwidth < 8) {\n++          // Truncate the values to the original bitwidth from i8.\n++          llvm::for_each(unpackedVals,\n++                         [&](Value& v) { v = b.trunc(elemTy, v); });\n+         }\n+       }\n+       outVals = std::move(unpackedVals);"
        },
        {
            "sha": "9748aa50da77cdb1e98a8c1dd6791fe40233e0e1",
            "filename": "third_party/xla/third_party/triton/temporary/convert_layout_op_to_llvm_small_width_2.patch",
            "status": "added",
            "additions": 72,
            "deletions": 0,
            "changes": 72,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconvert_layout_op_to_llvm_small_width_2.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconvert_layout_op_to_llvm_small_width_2.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fconvert_layout_op_to_llvm_small_width_2.patch?ref=f2a3cf52d2aed6a15e2671a153ffa9740b378a16",
            "patch": "@@ -0,0 +1,72 @@\n+# patch to match https://github.com/triton-lang/triton/pull/8155\n+\n+--- a/lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp\t2025-09-11 02:55:30.000000000 -0700\n++++ b/lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp\t2025-09-11 23:33:17.000000000 -0700\n+@@ -331,11 +331,11 @@\n+     // Pack registers if possible.\n+     int elemsPerVec = 1 << nPack;\n+     int bitsPerVecElem = 32 / elemsPerVec;\n+-    bool packTwo16Bit = bitwidth < bitsPerVecElem && elemsPerVec == 2;\n+     if (elemsPerVec > 1) {\n+       SmallVector<Value> packedVals;\n+       packedVals.reserve(regDim / elemsPerVec);\n+-      if (packTwo16Bit) {\n++      if (bitwidth == 8 && bitsPerVecElem == 16) {\n++        // TODO: Can remove `if` part of `if-else` once ptxas bugfix lands.\n+         for (int i = 0; i < regDim; i += elemsPerVec) {\n+           Value x0 = b.zext(i32_ty, b.bitcast(inVals[i], int_ty(bitwidth)));\n+           Value x1 = b.zext(i32_ty, b.bitcast(inVals[i + 1], int_ty(bitwidth)));\n+@@ -343,9 +343,12 @@\n+           packedVals.emplace_back(b.or_(x0, x1));\n+         }\n+       } else {\n+-        // For small types, we need to extend the values to i8.\n+-        if (bitwidth < 8) {\n+-          llvm::for_each(inVals, [&](Value& v) { v = b.zext(i8_ty, v); });\n++        if (bitwidth < bitsPerVecElem) {\n++          for (Value &v : inVals) {\n++            if (elemTy != int_ty(bitwidth))\n++              v = b.bitcast(v, int_ty(bitwidth));\n++            v = b.zext(int_ty(bitsPerVecElem), v);\n++          }\n+         }\n+         for (int i = 0; i < regDim; i += elemsPerVec) {\n+           auto slice = ArrayRef<Value>(inVals).slice(i, elemsPerVec);\n+@@ -380,21 +383,8 @@\n+     if (elemsPerVec > 1) {\n+       SmallVector<Value> unpackedVals;\n+       unpackedVals.reserve(regDim);\n+-      if (packTwo16Bit) {\n+-        for (auto packedVal : outVals) {\n+-          Value x0 =\n+-              b.trunc(int_ty(bitwidth), b.and_(packedVal, b.i32_val(0xFF)));\n+-          Value x1 =\n+-              b.trunc(int_ty(bitwidth), b.lshr(packedVal, b.i32_val(16)));\n+-          unpackedVals.push_back(b.bitcast(x0, elemTy));\n+-          unpackedVals.push_back(b.bitcast(x1, elemTy));\n+-        }\n+-      } else {\n+         auto packedTy =\n+             bitwidth < bitsPerVecElem ? int_ty(bitsPerVecElem) : elemTy;\n+-        if (bitwidth < 8) {\n+-          packedTy = i8_ty;\n+-        }\n+         auto vecTy = vec_ty(packedTy, elemsPerVec);\n+         auto unpackVal = [&](Value v) {\n+           v = b.bitcast(v, vecTy);\n+@@ -404,10 +394,11 @@\n+           auto unpacked = unpackVal(v);\n+           unpackedVals.append(unpacked.begin(), unpacked.end());\n+         }\n+-        if (bitwidth < 8) {\n+-          // Truncate the values to the original bitwidth from i8.\n+-          llvm::for_each(unpackedVals,\n+-                         [&](Value& v) { v = b.trunc(elemTy, v); });\n++      if (bitwidth < bitsPerVecElem) {\n++        for (Value &v : unpackedVals) {\n++          v = b.trunc(int_ty(bitwidth), v);\n++          if (elemTy != int_ty(bitwidth))\n++            v = b.bitcast(v, elemTy);\n+         }\n+       }\n+       outVals = std::move(unpackedVals);"
        },
        {
            "sha": "3668753e439b921e2925717728a16e07737273a7",
            "filename": "third_party/xla/third_party/triton/temporary/disable-filecheck.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fce41472acd9e426b0b6a2355ad0e283b02fd9e8/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fdisable-filecheck.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fce41472acd9e426b0b6a2355ad0e283b02fd9e8/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fdisable-filecheck.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fdisable-filecheck.patch?ref=fce41472acd9e426b0b6a2355ad0e283b02fd9e8",
            "patch": "@@ -1,17 +0,0 @@\n-TODO(b/436582531): The filecheck python package is not available in g3. We\n-should find a replacement for it or have this as a private patch.\n-\n---- a/python/test/unit/language/test_line_info.py\t2025-07-31 05:01:16.000000000 -0700\n-+++ b/python/test/unit/language/test_line_info.py\t2025-08-06 01:12:05.000000000 -0700\n-@@ -255,7 +255,10 @@\n- \n- def test_use_name_loc_as_prefix(fresh_triton_cache):\n-     import inspect\n--    from triton._filecheck import run_filecheck\n-+    # TODO(b/436582531): The filecheck python package is not available in g3.\n-+    # from triton._filecheck import run_filecheck\n-+    def run_filecheck(name, text, template):\n-+        pass\n- \n-     @triton.jit\n-     def kernel_basic(src, N, BLOCK_SIZE: tl.constexpr):"
        },
        {
            "sha": "bf718b017eedb30808b6f2eb948649bdc63e9483",
            "filename": "third_party/xla/third_party/triton/temporary/disable_cublas.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 49,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fce41472acd9e426b0b6a2355ad0e283b02fd9e8/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fdisable_cublas.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fce41472acd9e426b0b6a2355ad0e283b02fd9e8/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fdisable_cublas.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fdisable_cublas.patch?ref=fce41472acd9e426b0b6a2355ad0e283b02fd9e8",
            "patch": "@@ -1,49 +0,0 @@\n-Remove after fixing b/436154455. In my opinion, this should be a private patch,\n-since it appliest to a google-internal issue. We might consider merging with\n-xla/third_party/triton/temporary/tutorial_fixes.patch,\n-because we have a similar issue there. Also related to b/346755023.\n-\n---- a/python/test/unit/language/test_warp_specialization.py\t2025-07-31 05:01:16.000000000 -0700\n-+++ b/python/test/unit/language/test_warp_specialization.py\t2025-08-05 04:15:09.000000000 -0700\n-@@ -7,7 +7,10 @@\n- from triton._internal_testing import is_hip, is_hopper, is_blackwell\n- from triton.tools.tensor_descriptor import TensorDescriptor\n- \n--if not is_hip() and torch.cuda.is_available() and torch.cuda.get_device_capability()[0] in [9, 10]:\n-+# Attempts to dlopen cuBLAS, prevent this path\n-+# TODO: b/436154455 - Re-enable once we can link in cuBLAS properly\n-+# if not is_hip() and torch.cuda.is_available() and torch.cuda.get_device_capability()[0] in [9, 10]:\n-+if False:\n-     from triton._C.libtriton import nvidia\n-     cublas_workspace = torch.empty(32 * 1024 * 1024, device=\"cuda\", dtype=torch.uint8)\n-     cublas = nvidia.cublas.CublasLt(cublas_workspace)\n-@@ -285,9 +288,11 @@\n-     else:\n-         assert \"ttg.warp_specialize\" in ttgir\n- \n--    ref_out = torch.empty((M, N), dtype=dtype, device=device)\n--    cublas.matmul(A, B, ref_out)\n--    torch.testing.assert_close(ref_out.to(torch.float16), C.to(torch.float16), atol=0.03, rtol=0.03)\n-+    # TODO: b/436154455 - Re-enable once we can link in cuBLAS properly\n-+    if cublas is not None:\n-+        ref_out = torch.empty((M, N), dtype=dtype, device=device)\n-+        cublas.matmul(A, B, ref_out)\n-+        torch.testing.assert_close(ref_out.to(torch.float16), C.to(torch.float16), atol=0.03, rtol=0.03)\n- \n- \n- @triton.jit\n-@@ -386,9 +391,11 @@\n-     else:\n-         assert \"ttg.warp_specialize\" in ttgir\n- \n--    ref_out = torch.empty((M, N), dtype=dtype, device=device)\n--    cublas.matmul(A, B, ref_out)\n--    torch.testing.assert_close(ref_out.to(torch.float16), C.to(torch.float16), atol=0.03, rtol=0.03)\n-+    # TODO: b/436154455 - Re-enable once we can link in cuBLAS properly\n-+    if cublas is not None:\n-+        ref_out = torch.empty((M, N), dtype=dtype, device=device)\n-+        cublas.matmul(A, B, ref_out)\n-+        torch.testing.assert_close(ref_out.to(torch.float16), C.to(torch.float16), atol=0.03, rtol=0.03)\n- \n- \n- @triton.jit"
        },
        {
            "sha": "100913dd425573a7912ebc3c5d319e41a6af9dab",
            "filename": "third_party/xla/third_party/triton/temporary/launcher_overflow_fix.patch",
            "status": "added",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Flauncher_overflow_fix.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Flauncher_overflow_fix.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Flauncher_overflow_fix.patch?ref=f2a3cf52d2aed6a15e2671a153ffa9740b378a16",
            "patch": "@@ -0,0 +1,15 @@\n+Merge with launcher.patch\n+\n+--- a/third_party/nvidia/backend/cuda_utils.cc\n++++ b/third_party/nvidia/backend/cuda_utils.cc\n+@@ -605,8 +605,8 @@\n+     return nullptr;\n+   }\n+ \n+-  // +1 for the global scratch pointer.\n+-  std::size_t num_params = signature_metadata.size() + 1;\n++  // +2 for the global scratch pointer and profile scratch pointer.\n++  std::size_t num_params = signature_metadata.size() + 2;\n+   // Use alloca to set up kernel parameters on the stack and avoid dynamic\n+   // memory allocations.\n+   config.params = static_cast<void**>(alloca(num_params * sizeof(void*)));"
        },
        {
            "sha": "a030585d6dc15a204cfcccdb712fd96231587d5a",
            "filename": "third_party/xla/third_party/triton/temporary/no_type_annotation_for_args.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fce41472acd9e426b0b6a2355ad0e283b02fd9e8/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fno_type_annotation_for_args.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fce41472acd9e426b0b6a2355ad0e283b02fd9e8/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fno_type_annotation_for_args.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fno_type_annotation_for_args.patch?ref=fce41472acd9e426b0b6a2355ad0e283b02fd9e8",
            "patch": "@@ -1,15 +0,0 @@\n-Adding the type annotation for the args caused pytype to complain that triton.language.core.tuple is not a container.\n-That's because pytype expands the variadic argument type to tuple[], and this file also defines own `tuple` type, so wrong type is assumed.\n-This patch has to be removed once b/438115892 is fixed.\n-\n---- a/python/triton/language/core.py\t2025-07-31 05:01:16.000000000 -0700\n-+++ b/python/triton/language/core.py\t2025-08-12 00:25:59.000000000 -0700\n-@@ -2784,7 +2784,7 @@\n- @builtin\n- def map_elementwise(\n-     scalar_fn: Callable[..., Tuple[tensor, ...]],\n--    *args: tensor,\n-+    *args,\n-     pack=1,\n-     _semantic=None,\n-     _generator=None,"
        },
        {
            "sha": "260d5b4d5d406b6e3ebf92c92eed3a3708376969",
            "filename": "third_party/xla/third_party/triton/temporary/series.bzl",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fseries.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fseries.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fseries.bzl?ref=f2a3cf52d2aed6a15e2671a153ffa9740b378a16",
            "patch": "@@ -16,5 +16,8 @@ those to this list.\n temporary_patch_list = [\n     \"//third_party/triton:temporary/verify_nvmma_encoding.patch\",\n     \"//third_party/triton:temporary/triton-tensor-layout-init-fiasco.patch\",\n+    \"//third_party/triton:temporary/launcher_overflow_fix.patch\",\n+    \"//third_party/triton:temporary/convert_layout_op_to_llvm_small_width.patch\",\n+    \"//third_party/triton:temporary/convert_layout_op_to_llvm_small_width_2.patch\",\n     # Add new patches just above this line\n ]"
        },
        {
            "sha": "599be88da7b1fd8971bb106faafeb56c4ddbf22a",
            "filename": "third_party/xla/third_party/triton/temporary/triton-tensor-layout-init-fiasco.patch",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ftriton-tensor-layout-init-fiasco.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ftriton-tensor-layout-init-fiasco.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ftriton-tensor-layout-init-fiasco.patch?ref=f2a3cf52d2aed6a15e2671a153ffa9740b378a16",
            "patch": "@@ -3,65 +3,65 @@ Upsteam Pull Request: https://github.com/triton-lang/triton/pull/8117.\n diff --git a/bin/triton-tensor-layout.cpp b/bin/triton-tensor-layout.cpp\n --- a/bin/triton-tensor-layout.cpp\n +++ b/bin/triton-tensor-layout.cpp\n-@@ -41,29 +41,32 @@ using namespace mlir;\n+@@ -39,29 +39,32 @@ using namespace mlir;\n  // CLI options\n  //===--------------------------------------------------------------------===//\n- \n+\n -cl::OptionCategory PrinterCategory(\"Available Print Options\",\n -                                   \"Options for the tensor layout printing.\");\n +static cl::OptionCategory &getPrinterCategory() {\n +  static cl::OptionCategory PrinterCategory(\n +      \"Available Print Options\", \"Options for the tensor layout printing.\");\n +  return PrinterCategory;\n +}\n- \n+\n  static cl::opt<std::string> InputFile(\n      \"i\", cl::desc(\"File that contains the tensor data layout attributes\"),\n -    cl::init(\"\"), cl::value_desc(\"filename\"), cl::cat(PrinterCategory));\n +    cl::init(\"\"), cl::value_desc(\"filename\"), cl::cat(getPrinterCategory()));\n- \n+\n  static cl::opt<std::string>\n      OutputFile(\"o\", cl::desc(\"Output file to write the layout into\"),\n                 cl::init(\"\"), cl::value_desc(\"filename\"),\n -               cl::cat(PrinterCategory));\n +               cl::cat(getPrinterCategory()));\n- \n+\n  static cl::opt<std::string>\n      DataLayoutStr(\"l\", cl::desc(\"Tensor data layout attribute in string\"),\n                    cl::value_desc(\"layout-string\"), cl::init(\"\"),\n -                  cl::cat(PrinterCategory));\n +                  cl::cat(getPrinterCategory()));\n- \n+\n  static cl::list<std::string>\n      AliasName(\"alias-names\",\n                cl::desc(\"A list of alias names (separated by comma) of the \"\n                         \"layout attributes in the input file\"),\n                cl::value_desc(\"name1,name2,name3,...\"), cl::CommaSeparated,\n -              cl::ZeroOrMore, cl::cat(PrinterCategory));\n +              cl::ZeroOrMore, cl::cat(getPrinterCategory()));\n- \n+\n  static cl::opt<bool> UseHWPointOfView(\n      \"use-hw-view\",\n-@@ -71,11 +74,11 @@ static cl::opt<bool> UseHWPointOfView(\n+@@ -69,11 +72,11 @@ static cl::opt<bool> UseHWPointOfView(\n          \"Print the layout in hardware point of view. This means the output is \"\n          \"from the warp's perspective. Otherwise, the output is from the \"\n          \"tensor's perspective (e.g., each element maps to xxx thread).\"),\n -    cl::init(false), cl::cat(PrinterCategory));\n +    cl::init(false), cl::cat(getPrinterCategory()));\n- \n+\n  static cl::opt<std::string> TensorStr(\n      \"t\", cl::desc(\"Tensor shape and element type (e.g., tensor<2x2xf32>)\"),\n -    cl::init(\"\"), cl::value_desc(\"tensor-type\"), cl::cat(PrinterCategory));\n +    cl::init(\"\"), cl::value_desc(\"tensor-type\"), cl::cat(getPrinterCategory()));\n- \n+\n  //===--------------------------------------------------------------------===//\n  // Helper functions\n-@@ -182,7 +185,7 @@ static LogicalResult printLayoutFromStri\n+@@ -180,7 +183,7 @@ static LogicalResult printLayoutFromString(MLIRContext *context,\n  //===--------------------------------------------------------------------===//\n- \n+\n  int main(int argc, char **argv) {\n -  cl::HideUnrelatedOptions(PrinterCategory);\n +  cl::HideUnrelatedOptions(getPrinterCategory());\n    cl::ParseCommandLineOptions(argc, argv, \"tensor layout printer\\n\");\n- \n-   DialectRegistry registry;\n\\ No newline at end of file\n+\n+   DialectRegistry registry;"
        },
        {
            "sha": "aea230995548dde3d0c6c60a7a1f0a740b54ab67",
            "filename": "third_party/xla/third_party/triton/temporary/tutorial_fixes.patch",
            "status": "removed",
            "additions": 0,
            "deletions": 117,
            "changes": 117,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fce41472acd9e426b0b6a2355ad0e283b02fd9e8/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ftutorial_fixes.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fce41472acd9e426b0b6a2355ad0e283b02fd9e8/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ftutorial_fixes.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Ftutorial_fixes.patch?ref=fce41472acd9e426b0b6a2355ad0e283b02fd9e8",
            "patch": "@@ -1,117 +0,0 @@\n-Fixes necessary to get the tutorials to run in our environment.\n-\n-We'll likely have to drag this around, but might be able to reduce some of\n-the diff if we can fix a few bugs referenced within the patch. Could either be\n-an internal or public patch (but probably easier to just have it as public).\n-\n-diff --git a/python/tutorials/05-layer-norm.py b/python/tutorials/05-layer-norm.py\n---- a/python/tutorials/05-layer-norm.py\n-+++ b/python/tutorials/05-layer-norm.py\n-@@ -372,7 +372,7 @@ def bench_layer_norm(M, N, dtype, provid\n- \n- \n- test_layer_norm(1151, 8192, torch.float16)\n--bench_layer_norm.run(save_path='.', print_data=True)\n-+bench_layer_norm.run(print_data=True)\n- \n- # %%\n- # References\n-diff --git a/python/tutorials/06-fused-attention.py b/python/tutorials/06-fused-attention.py\n---- a/python/tutorials/06-fused-attention.py\n-+++ b/python/tutorials/06-fused-attention.py\n-@@ -750,4 +750,4 @@ def bench_flash_attention(BATCH, H, N_CT\n- \n- if __name__ == \"__main__\":\n-     # only works on post-Ampere GPUs right now\n--    bench_flash_attention.run(save_path=\".\", print_data=True)\n-+    bench_flash_attention.run(print_data=True)\n-diff --git a/python/tutorials/09-persistent-matmul.py b/python/tutorials/09-persistent-matmul.py\n---- a/python/tutorials/09-persistent-matmul.py\n-+++ b/python/tutorials/09-persistent-matmul.py\n-@@ -31,7 +31,9 @@ from contextlib import contextmanager\n- \n- from typing import Optional\n- \n--if torch.cuda.is_available():\n-+# Attempts to dlopen cuBLAS, prevent this path\n-+# TODO: b/436154455 - Re-enable once we can link in cuBLAS properly\n-+if False and torch.cuda.is_available():\n-     from triton._C.libtriton import nvidia\n-     cublas_workspace = torch.empty(32 * 1024 * 1024, device=\"cuda\", dtype=torch.uint8)\n-     cublas = nvidia.cublas.CublasLt(cublas_workspace)\n-@@ -619,11 +621,12 @@ def torch_matmul(a, b):\n- \n- @contextmanager\n- def proton_context():\n--    proton.activate(0)\n-+    # proton.activate(0)\n-     try:\n-         yield\n-     finally:\n--        proton.deactivate(0)\n-+        # proton.deactivate(0)\n-+        pass\n- \n- \n- def bench_fn(label, reps, warmup_reps, fn, *args):\n-@@ -734,9 +739,12 @@ if __name__ == \"__main__\":\n-         validate(32, 32, 32, dtype)\n-         validate(8192, 8192, args.K_range[0], dtype)\n- \n--        proton.start(\"matmul\", hook=\"triton\")\n--        proton.deactivate()\n-+        # Proton tries to dlopen libcupti.so,\n-+        # If you want to profile this, run it under NCU\n-+        # TODO: b/436154452 - Re-enabled once this is fixed.\n-+        # proton.start(\"matmul\", hook=\"triton\")\n-+        # proton.deactivate()\n-         for K in range(args.K_range[0], args.K_range[1] + 1, args.K_step):\n-             bench(K, dtype)\n--        proton.finalize()\n--        show_profile(args.prec, \"matmul\")\n-+        # proton.finalize()\n-+        # show_profile(args.prec, \"matmul\")\n-diff --git a/python/tutorials/10-block-scaled-matmul.py b/python/tutorials/10-block-scaled-matmul.py\n---- a/python/tutorials/10-block-scaled-matmul.py\n-+++ b/python/tutorials/10-block-scaled-matmul.py\n-@@ -323,10 +323,10 @@ def bench_block_scaled(K, block_scale_ty\n-         M, N, K, block_scale_type, compute_reference=False)\n-     _ = block_scaled_matmul(a_desc, a_scale, b_desc, b_scale, torch.float16, M, N, K, rep_m, rep_n, rep_k, configs)\n- \n--    proton.activate(0)\n-+    # proton.activate(0)\n-     for _ in range(reps):\n-         _ = block_scaled_matmul(a_desc, a_scale, b_desc, b_scale, torch.float16, M, N, K, rep_m, rep_n, rep_k, configs)\n--    proton.deactivate(0)\n-+    # proton.deactivate(0)\n-     print(\"Done benchmarking\")\n- \n- \n-@@ -361,9 +361,12 @@ if __name__ == \"__main__\":\n-         validate_block_scaled(8192, 8192, 8192, block_scale_type=args.format)\n- \n-         if args.bench:\n--            proton.start(\"block_scaled_matmul\", hook=\"triton\")\n--            proton.deactivate(0)  # Skip argument creation\n-+            # Proton tries to dlopen libcupti.so,\n-+            # If you want to profile this, run it under NCU\n-+            # TODO: b/436154452 - Re-enabled once this is fixed.\n-+            # proton.start(\"block_scaled_matmul\", hook=\"triton\")\n-+            # proton.deactivate(0)  # Skip argument creation\n-             for K in range(args.K_range[0], args.K_range[1] + 1, args.K_step):\n-                 bench_block_scaled(K, reps=10000, block_scale_type=args.format)\n--            proton.finalize()\n--            show_profile(\"block_scaled_matmul\")\n-+            # proton.finalize()\n-+            # show_profile(\"block_scaled_matmul\")\n-diff --git a/python/tutorials/11-programmatic-dependent-launch.py b/python/tutorials/11-programmatic-dependent-launch.py\n---- a/python/tutorials/11-programmatic-dependent-launch.py\n-+++ b/python/tutorials/11-programmatic-dependent-launch.py\n-@@ -111,6 +111,6 @@ if __name__ == \"__main__\":\n- \n-     if supports_pdl():\n-         validate(1024)\n--        benchmark.run(print_data=True, show_plots=True, save_path=\".\")\n-+        benchmark.run(print_data=True, show_plots=True)\n-     else:\n-         print(\"PDL is not supported on this device\")"
        },
        {
            "sha": "e9e66e03defb4a6592fadd1f1ffcf234aa36351c",
            "filename": "third_party/xla/third_party/triton/temporary/verify_nvmma_encoding.patch",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fverify_nvmma_encoding.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fverify_nvmma_encoding.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Ftemporary%2Fverify_nvmma_encoding.patch?ref=f2a3cf52d2aed6a15e2671a153ffa9740b378a16",
            "patch": "@@ -15,13 +15,13 @@\n  #include \"triton/Tools/LayoutUtils.h\"\n -#include \"llvm/Support/Casting.h\"\n -#include \"llvm/Support/LogicalResult.h\"\n- \n+\n  // Provide custom directive handlers for declarative assemblyFormat.\n  // They must be visible before including the generated op classes.\n-@@ -517,10 +518,47 @@\n+@@ -517,10 +518,47 @@ LogicalResult MemDescReshapeOp::verify() {\n    return success();\n  }\n- \n+\n -static LogicalResult inferMemDescReshapeOpEncoding(ArrayRef<int64_t> srcShape,\n +// Verification copied from nvmmaSharedToLinearLayout().\n +LogicalResult verifyNVMMASharedEncoding(std::optional<Location> loc,\n@@ -68,7 +68,7 @@\n    if (auto mmaEncoding = dyn_cast<NVMMASharedEncodingAttr>(srcEnc)) {\n      // TODO: supporting reshape of CTA layouts is non-trivial.\n      if (getNumCTAs(mmaEncoding) > 1)\n-@@ -544,6 +582,11 @@\n+@@ -544,6 +582,11 @@ static LogicalResult inferMemDescReshapeOpEncoding(ArrayRef<int64_t> srcShape,\n          ctx, mmaEncoding.getSwizzlingByteWidth(), mmaEncoding.getTransposed(),\n          mmaEncoding.getElementBitWidth(), mmaEncoding.getFp4Padded(),\n          CTALayout);\n@@ -79,9 +79,9 @@\n +    }\n      // Big guns, check linear layouts are equivalent\n      // We disallow reshaping memdesc_subslice in the verifier\n-     auto srcLL = toLinearLayout(srcShape, srcEnc, srcShape);\n-@@ -565,8 +608,8 @@\n- \n+     // so allocShape == shape\n+@@ -566,8 +609,8 @@ LogicalResult MemDescReshapeOp::inferReturnTypes(\n+\n    Attribute dstEncoding;\n    if (Attribute srcEnc = srcTy.getEncoding()) {\n -    if (failed(inferMemDescReshapeOpEncoding(srcTy.getShape(), srcEnc, dstShape,\n@@ -90,4 +90,4 @@\n +                                             dstShape, dstEncoding)))\n        return failure();\n    }\n- \n+"
        },
        {
            "sha": "4e34ce8ed02a3f66ce9dc68c312c13499e33aef5",
            "filename": "third_party/xla/third_party/triton/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fthird_party%2Ftriton%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fthird_party%2Ftriton%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Ftriton%2Fworkspace.bzl?ref=f2a3cf52d2aed6a15e2671a153ffa9740b378a16",
            "patch": "@@ -7,8 +7,8 @@ load(\"//third_party/triton:temporary/series.bzl\", \"temporary_patch_list\")\n def repo():\n     \"\"\"Imports Triton.\"\"\"\n \n-    TRITON_COMMIT = \"triton_integrate_branch-1.11\"\n-    TRITON_SHA256 = \"1125fd9e344de2cb4041e4a9ec2cf02c307082833e421d87f91ffcf9983f9a90\"\n+    TRITON_COMMIT = \"triton_integrate_branch-1.12\"\n+    TRITON_SHA256 = \"6754c1c474c58916c1ddd88ceb1adb2a553ec3609afbe5fec936902a0297a7ad\"\n     tf_http_archive(\n         name = \"triton\",\n         sha256 = TRITON_SHA256,"
        },
        {
            "sha": "6c9719c9ee2bdd727483a26e543872df6323e90b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline_cuda.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_cuda.cc?ref=f2a3cf52d2aed6a15e2671a153ffa9740b378a16",
            "patch": "@@ -127,6 +127,7 @@ static void MakeTTGIR(mlir::OpPassManager* pm,\n   pm->addPass(ttng::createTritonGPUFenceInsertion({cuda_cc_as_int}));\n   pm->addPass(ttng::createTritonNvidiaGPUMMALoweringPass());\n   pm->addPass(mlir::createSCCPPass());\n+  pm->addPass(mlir::createCSEPass());\n   pm->addPass(mlir::createCanonicalizerPass());\n   // Corresponds to \"mod.get_tensordesc_metadata()\"\n   // in @triton//:third_party/nvidia/backend/compiler.py"
        },
        {
            "sha": "e5e6da54199c3a4a0ed587584463876db74b5f2a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/compilation_pipeline_rocm.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_rocm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_rocm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcompilation_pipeline_rocm.cc?ref=f2a3cf52d2aed6a15e2671a153ffa9740b378a16",
            "patch": "@@ -70,7 +70,8 @@ static void MakeTTGIR(mlir::OpPassManager* pm,\n   pm->addPass(mt::gpu::createTritonGPURemoveLayoutConversions());\n   // TODO ROCm Check if we want to compare MI100 and greater\n   pm->addPass(mlir::createTritonAMDGPUOptimizeEpilogue());\n-  pm->addPass(mt::gpu::createTritonGPUOptimizeDotOperands({true}));\n+  pm->addPass(mt::amdgpu::createTritonAMDGPUOptimizeDotOperands(\n+      {rocm_cc.gfx_version()}));\n   pm->addNestedPass<mlir::triton::FuncOp>(\n       mlir::createTritonAMDGPUHoistLayoutConversions());\n "
        },
        {
            "sha": "b19dd89b306fdde23b941d9e1eec9343f02acb96",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=f2a3cf52d2aed6a15e2671a153ffa9740b378a16",
            "patch": "@@ -236,15 +236,20 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n          llvm::zip(impl_fn->args(), kernel->args())) {\n       impl_fn_arg.replaceAllUsesWith(&kernel_arg);\n     }\n-    // Triton's kernel ABI expects an additional scratchpad global memory.\n+    // Triton's kernel ABI expects additional scratchpad global memory for\n+    // TMA and profiling information.\n     // For now it is only used for on-device creation of TMA descriptors, which\n     // we do not use yet, so we are just replacing this argument with a null\n     // pointer.\n     // TODO: b/381242007 - Allocate a proper buffer if we want to use\n     // device-side TMA APIs.\n-    auto scratchpad_arg = impl_fn->getArg(impl_fn->arg_size() - 1);\n-    scratchpad_arg->replaceAllUsesWith(llvm::ConstantPointerNull::get(\n-        llvm::cast<llvm::PointerType>(scratchpad_arg->getType())));\n+    CHECK_EQ(impl_fn->arg_size(), kernel->arg_size() + 2);\n+    auto tma_scratchpad_arg = impl_fn->getArg(impl_fn->arg_size() - 2);\n+    tma_scratchpad_arg->replaceAllUsesWith(llvm::ConstantPointerNull::get(\n+        llvm::cast<llvm::PointerType>(tma_scratchpad_arg->getType())));\n+    auto profiling_scratchpad_arg = impl_fn->getArg(impl_fn->arg_size() - 1);\n+    profiling_scratchpad_arg->replaceAllUsesWith(llvm::ConstantPointerNull::get(\n+        llvm::cast<llvm::PointerType>(profiling_scratchpad_arg->getType())));\n \n     return {{kernel->getName().str(), launch_dimensions,\n              triton_wrapper_result.cluster_dim,"
        },
        {
            "sha": "3345e6d5595763716eb47270c618de80dbb96b30",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotune_cache_key.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.h?ref=f2a3cf52d2aed6a15e2671a153ffa9740b378a16",
            "patch": "@@ -32,7 +32,7 @@ class AutotuneCacheKey {\n   // Tie a version to the cache key in order to invalidate the cache when\n   // necessary. This should be incremented on triton upgrades or any other\n   // changes that may affect the autotuning results.\n-  static constexpr int kCurrentVersion = 12;\n+  static constexpr int kCurrentVersion = 13;\n \n   AutotuneCacheKey(const se::DeviceDescription& device_description,\n                    const HloInstruction& instruction,"
        },
        {
            "sha": "46fe31883a086953fcc068a89c030deefbfcb50e",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 8,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f2a3cf52d2aed6a15e2671a153ffa9740b378a16/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=f2a3cf52d2aed6a15e2671a153ffa9740b378a16",
            "patch": "@@ -1570,15 +1570,20 @@ absl::Status IrEmitterUnnested::EmitTritonCustomCall(\n            llvm::zip(impl_fn->args(), kernel->args())) {\n         impl_fn_arg.replaceAllUsesWith(&kernel_arg);\n       }\n-      // Triton's kernel ABI expects an additional scratchpad global\n-      // memory. For now it is only used for on-device creation of\n-      // TMA descriptors, which we do not use yet, so we are just\n+      // Triton's kernel ABI expects additional scratchpad global memory for TMA\n+      // and profiling information. For now it is only used for on-device\n+      // creation of TMA descriptors, which we do not use yet, so we are just\n       // replacing this argument with a null pointer.\n-      // TODO: b/381242007 - Allocate a proper buffer if we want to\n-      // use device-side TMA APIs.\n-      auto scratchpad_arg = impl_fn->getArg(impl_fn->arg_size() - 1);\n-      scratchpad_arg->replaceAllUsesWith(llvm::ConstantPointerNull::get(\n-          llvm::cast<llvm::PointerType>(scratchpad_arg->getType())));\n+      // TODO: b/381242007 - Allocate a proper buffer if we want to use\n+      // device-side TMA APIs.\n+      CHECK_EQ(impl_fn->arg_size(), kernel->arg_size() + 2);\n+      auto tma_scratchpad_arg = impl_fn->getArg(impl_fn->arg_size() - 2);\n+      tma_scratchpad_arg->replaceAllUsesWith(llvm::ConstantPointerNull::get(\n+          llvm::cast<llvm::PointerType>(tma_scratchpad_arg->getType())));\n+      auto profiling_scratchpad_arg = impl_fn->getArg(impl_fn->arg_size() - 1);\n+      profiling_scratchpad_arg->replaceAllUsesWith(\n+          llvm::ConstantPointerNull::get(llvm::cast<llvm::PointerType>(\n+              profiling_scratchpad_arg->getType())));\n \n       impl_fn->eraseFromParent();\n "
        }
    ],
    "stats": {
        "total": 507,
        "additions": 238,
        "deletions": 269
    }
}