{
    "author": "pemeliya",
    "message": "PR #32053: [ROCM] Added command buffers support for convolutions\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32053\n\nüìù Summary of Changes\n\nAdded CommandBuffer support for Convolution ops\ngraph capture of convolutions is enabled only for convolution custom call targets explicitly added to '--legacy_command_buffer_custom_call_targets' list: see command_buffer_scheduling_test.cc for an example.\nüéØ Justification\nThis op wase missing for whatever reason: this results in graph fragmentation especially for large models. Hence one gets several (sometimes many) execution graphs instead of just one.\n\nüöÄ Kind of Contribution\n‚ú® New Feature\n\nüß™ Unit Tests:\nAdded new subtest to xla/service/gpu/transforms/command_buffer_scheduling_test.cc\n\nThis is a splitted PR originated from https://github.com/openxla/xla/pull/30855\n\n@xla-rotation could you have a look please ?\nCopybara import of the project:\n\n--\n5f5f5bc8ba8212ceb6afde6f9729ba4a951e4051 by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\nadding coll permute and convolution to command buffers\n\nadded UTs  and convolution command\n\ntest fixes\n\nadded rebase fixes\n\ncapture only those convolution targets which are explictly\n\nRevert \"adding coll permute and convolution to command buffers\"\n\nThis reverts commit 75847e67261b4589162411c9846ed9c0b9fc1ed5.\n\nadded conv to command buffers\n\nfixing build and test\n\n--\ne8afa3296a4a8ad079cde2e84391c7e0006ddf52 by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\nfixing build\n\n--\nb529288221708e59a04bfaacdfa5b7a1c25b091e by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\nrewritten ConvolutionCmd, adapted command_buffer_conv_pass\n\n--\n3ecd3d0516a7766d70d636b2110b4b310a9be7b2 by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\nsome cosmetics\n\nMerging this change closes #32053\n\nPiperOrigin-RevId: 834708288",
    "sha": "4bf20e19f440a47a276cf1988b48683778f673a6",
    "files": [
        {
            "sha": "56ab4106be980145a693356e907e7c714b0f122b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=4bf20e19f440a47a276cf1988b48683778f673a6",
            "patch": "@@ -69,6 +69,7 @@ cc_library(\n         \":collective_broadcast_thunk\",\n         \":collective_permute_thunk\",\n         \":collective_thunk\",\n+        \":convolution_thunk\",\n         \":copy_thunk\",\n         \":custom_call_thunk\",\n         \":dynamic_slice_thunk\",\n@@ -3003,6 +3004,7 @@ xla_test(\n         \":command_buffer_conversion_pass\",\n         \":command_buffer_thunk\",\n         \":conditional_thunk\",\n+        \":convolution_thunk\",\n         \":copy_thunk\",\n         \":cudnn_thunk\",\n         \":custom_call_thunk\","
        },
        {
            "sha": "a33014cd1837b1e94779a30c3a2aabd4e9ace377",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.cc",
            "status": "modified",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc?ref=4bf20e19f440a47a276cf1988b48683778f673a6",
            "patch": "@@ -1750,6 +1750,53 @@ CommandBufferCmd::BufferUseVector CublasLtCmd::buffers() const {\n   return buffer_usage;\n }\n \n+//===----------------------------------------------------------------------===//\n+// ConvolutionCmd\n+//===----------------------------------------------------------------------===//\n+\n+ConvolutionCmd::ConvolutionCmd(const ConvolutionThunk& thunk)\n+    : TracedCommandBufferCmd(CommandBufferCmdType::kConvolutionCmd),\n+      operand_buffers_(thunk.operand_buffers_),\n+      result_buffers_(thunk.result_buffers_),\n+      scratch_buffer_(thunk.scratch_buffer_),\n+      config_(thunk.config_) {}\n+\n+absl::Status ConvolutionCmd::Initialize(const Thunk::InitializeParams& params,\n+                                        StateManager& state) {\n+  // populate cache of ConvRunner\n+  cache_.GetOrCreate(config_, params.stream);\n+  return absl::OkStatus();\n+}\n+\n+absl::StatusOr<const se::CommandBuffer::Command*> ConvolutionCmd::Record(\n+    const Thunk::ExecuteParams& execute_params,\n+    const RecordParams& record_params, RecordAction record_action,\n+    se::CommandBuffer* command_buffer) {\n+  VLOG(5) << \"ConvolutionCmd\";\n+\n+  return RecordTracedCommand(\n+      execute_params, record_params, std::move(record_action), command_buffer,\n+      [&](se::Stream* stream) {\n+        return RunConvolutionOnStream(execute_params, operand_buffers_,\n+                                      result_buffers_, scratch_buffer_, config_,\n+                                      cache_, stream);\n+      });\n+}\n+\n+CommandBufferCmd::BufferUseVector ConvolutionCmd::buffers() const {\n+  BufferUseVector buffer_usage;\n+  buffer_usage.reserve(operand_buffers_.size() + result_buffers_.size() + 1);\n+\n+  for (BufferAllocation::Slice buffer : operand_buffers_) {\n+    buffer_usage.push_back({buffer, MemoryAccess::kRead});\n+  }\n+  for (BufferAllocation::Slice buffer : result_buffers_) {\n+    buffer_usage.push_back({buffer, MemoryAccess::kWrite});\n+  }\n+  buffer_usage.push_back({scratch_buffer_, MemoryAccess::kWrite});\n+  return buffer_usage;\n+}\n+\n //===----------------------------------------------------------------------===//\n // CuDnnCmd\n //===----------------------------------------------------------------------===//"
        },
        {
            "sha": "428d32bc0aeb0c79dfe4df00229e0f6255546172",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.h",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h?ref=4bf20e19f440a47a276cf1988b48683778f673a6",
            "patch": "@@ -39,6 +39,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/runtime/collective_permute_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n+#include \"xla/backends/gpu/runtime/convolution_thunk.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n #include \"xla/backends/gpu/runtime/dynamic_slice_thunk.h\"\n@@ -83,6 +84,7 @@ namespace xla::gpu {\n   V(kLaunchCmd, \"LaunchCmd\")                                     \\\n   V(kCustomKernelLaunchCmd, \"CustomKernelLaunchCmd\")             \\\n   V(kCublasLtCmd, \"CublasLtCmd\")                                 \\\n+  V(kConvolutionCmd, \"ConvolutionCmd\")                           \\\n   V(kCuDnnCmd, \"CuDnnCmd\")                                       \\\n   V(kGemmCmd, \"GemmCmd\")                                         \\\n   V(kMemcpyDeviceToDeviceCmd, \"MemcpyDeviceToDeviceCmd\")         \\\n@@ -961,6 +963,34 @@ class CublasLtCmd : public TracedCommandBufferCmd, public CublasLtMatmulThunk {\n   bool IsNestedCommandBuffer() const final { return true; }\n };\n \n+//===----------------------------------------------------------------------===//\n+// ConvolutionCmd\n+//===----------------------------------------------------------------------===//\n+\n+class ConvolutionCmd : public TracedCommandBufferCmd {\n+ public:\n+  ConvolutionCmd(const ConvolutionThunk& conv_thunk);\n+\n+  absl::Status Initialize(const Thunk::InitializeParams& params,\n+                          StateManager& state) override;\n+\n+  absl::StatusOr<const se::CommandBuffer::Command*> Record(\n+      const Thunk::ExecuteParams& execute_params,\n+      const RecordParams& record_params, RecordAction record_action,\n+      se::CommandBuffer* command_buffer) override;\n+\n+  BufferUseVector buffers() const override;\n+\n+  bool IsNestedCommandBuffer() const final { return true; }\n+\n+ private:\n+  std::vector<BufferAllocation::Slice> operand_buffers_;\n+  std::vector<BufferAllocation::Slice> result_buffers_;\n+  BufferAllocation::Slice scratch_buffer_;\n+  GpuConvConfig config_;\n+  ConvRunnerCache cache_;\n+};\n+\n //===----------------------------------------------------------------------===//\n // CuDnnCmd\n //===----------------------------------------------------------------------===//"
        },
        {
            "sha": "193c705f9b46e5e9a90362a3269ab03fd1770a5c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd_emitter.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd_emitter.cc?ref=4bf20e19f440a47a276cf1988b48683778f673a6",
            "patch": "@@ -244,6 +244,10 @@ static absl::StatusOr<Command> Convert(const CuDnnThunk& thunk) {\n   return std::make_unique<CuDnnCmd>(thunk.arguments(), thunk.graph());\n }\n \n+static absl::StatusOr<Command> Convert(const ConvolutionThunk& thunk) {\n+  return std::make_unique<ConvolutionCmd>(thunk);\n+}\n+\n //===----------------------------------------------------------------------===//\n static absl::StatusOr<Command> CopyMetadata(absl::StatusOr<Command> cmd,\n                                             const Thunk& thunk) {\n@@ -315,6 +319,8 @@ static absl::Status AppendCommands(CommandBufferCmdSequence& cmd_sequence,\n       return append(Convert<WhileThunk>(thunk, options));\n     case Thunk::Kind::kCuDnn:\n       return append(Convert<CuDnnThunk>(thunk));\n+    case Thunk::Kind::kConvolution:\n+      return append(Convert<ConvolutionThunk>(thunk));\n     case Thunk::Kind::kDynamicSlice:\n       return append(Convert<DynamicSliceThunk>(thunk, options));\n "
        },
        {
            "sha": "0c487834a49d76d2bf6e1070590a0cb6c084366d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_conversion_pass.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc?ref=4bf20e19f440a47a276cf1988b48683778f673a6",
            "patch": "@@ -149,6 +149,7 @@ std::optional<DebugOptions::CommandBufferCmdType> GetCommandBufferCmdType(\n     case Thunk::kSend:\n       return DebugOptions::COLLECTIVES;\n     case Thunk::kCuDnn:\n+    case Thunk::kConvolution:\n       return DebugOptions::CUDNN;\n     case Thunk::kCustomCall:\n       return DebugOptions::CUSTOM_CALL;"
        },
        {
            "sha": "38530bf6354e52ee9a255c2c1578ef16e943497e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_conversion_pass_test.cc",
            "status": "modified",
            "additions": 95,
            "deletions": 0,
            "changes": 95,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass_test.cc?ref=4bf20e19f440a47a276cf1988b48683778f673a6",
            "patch": "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/command_buffer_thunk.h\"\n #include \"xla/backends/gpu/runtime/conditional_thunk.h\"\n+#include \"xla/backends/gpu/runtime/convolution_thunk.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/cudnn_thunk.h\"\n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n@@ -152,6 +153,58 @@ std::unique_ptr<GemmThunk> CreateGemmThunk(const BufferAllocation& alloc1) {\n                                      slice1, slice1, slice1, true);\n }\n \n+std::unique_ptr<ConvolutionThunk> CreateConvolutionThunk(\n+    const BufferAllocation& alloc) {\n+  std::vector<BufferAllocation::Slice> operand_slices, result_slices;\n+  for (int i = 0, num = 3; i < num; i++) {\n+    operand_slices.emplace_back(&alloc, i * 16, 16);\n+    result_slices.emplace_back(&alloc, (i + num) * 16, 16);\n+  }\n+\n+  ConvolutionDimensionNumbers dnums;\n+  dnums.set_input_batch_dimension(0);\n+  dnums.set_input_feature_dimension(1);\n+  dnums.add_input_spatial_dimensions(2);\n+  dnums.add_input_spatial_dimensions(3);\n+  dnums.set_kernel_input_feature_dimension(0);\n+  dnums.set_kernel_output_feature_dimension(1);\n+  dnums.add_kernel_spatial_dimensions(2);\n+  dnums.add_kernel_spatial_dimensions(3);\n+  dnums.set_output_batch_dimension(0);\n+  dnums.set_output_feature_dimension(1);\n+  dnums.add_output_spatial_dimensions(2);\n+  dnums.add_output_spatial_dimensions(3);\n+\n+  Window window;\n+  const auto dim0 = window.add_dimensions();\n+  const auto dim1 = window.add_dimensions();\n+  dim0->set_size(4);\n+  dim1->set_size(4);\n+  dim0->set_base_dilation(1);\n+  dim1->set_base_dilation(1);\n+  dim0->set_stride(1);\n+  dim1->set_stride(1);\n+  dim0->set_window_dilation(3);\n+  dim1->set_window_dilation(2);\n+\n+  GpuConvDescriptor desc{\n+      .kind = CudnnConvKind::kForward,\n+      .backend_config = CudnnConvBackendConfig{},\n+      .operand0_shape = ShapeUtil::MakeShape(F32, {60, 38, 17, 13}),\n+      .operand1_shape = ShapeUtil::MakeShapeWithDenseLayout(F32, {38, 10, 4, 4},\n+                                                            {3, 2, 0, 1}),\n+      .result_shape = ShapeUtil::MakeShapeWithType<float>({64, 64, 64, 13}),\n+      .scratch_size = 128 * 1024,\n+      .window = window,\n+      .dnums = dnums,\n+      .feature_group_count = 1};\n+  auto thunk =\n+      ConvolutionThunk::Create(Thunk::ThunkInfo(), desc, operand_slices,\n+                               result_slices, result_slices.back());\n+  TF_CHECK_OK(thunk.status());\n+  return std::move(thunk).value();\n+}\n+\n std::unique_ptr<CollectiveDoneThunk> CreateAllGatherDoneThunk(\n     Thunk* start_thunk) {\n   auto async_events =\n@@ -315,6 +368,48 @@ TEST(CommandBufferConversionPassTest, PartiallyConvertsToCommandBufferThunk) {\n   EXPECT_THAT(thunks_in_command_buffer1, ThunkKindsAre(Thunk::kCopy));\n }\n \n+TEST(CommandBufferConversionPassTest, ConvertConvolutionAndGemmThunks) {\n+  CommandBufferConversionPass pass{\"test\"};\n+\n+  std::vector<std::unique_ptr<Thunk>> thunks;\n+\n+  // Create a {CopyThunk, GemmThunk, ConvolutionThunk}\n+  BufferAllocation alloc0(0, 1024, 0);\n+  BufferAllocation alloc1(1, 2048, 0);\n+  BufferAllocation alloc2(2, 2048, 0);\n+  thunks.push_back(CreateCopyThunk(alloc0));\n+  thunks.push_back(CreateGemmThunk(alloc1));\n+  thunks.push_back(CreateConvolutionThunk(alloc0));\n+\n+  auto root_thunk =\n+      std::make_unique<SequentialThunk>(Thunk::ThunkInfo(), std::move(thunks));\n+  DebugOptions debug_options;\n+\n+  // Enable only FUSION, which means GemmThunk should not be converted.\n+  debug_options.clear_xla_gpu_enable_command_buffer();\n+  debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n+  debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::CUDNN);\n+  debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::CUBLAS);\n+\n+  se::DeviceDescription device_info = TestGpuDeviceInfo::CudaOrRocmDeviceInfo();\n+  FakeErrorAllocator allocator;\n+\n+  ASSERT_EQ(root_thunk->thunks().size(), 3);\n+\n+  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, /*hlo_module=*/nullptr,\n+                       device_info, allocator),\n+              IsOkAndHolds(true));\n+\n+  ASSERT_EQ(root_thunk->thunks().size(), 1);\n+\n+  const auto* command_buffer_thunk =\n+      static_cast<const CommandBufferThunk*>(root_thunk->thunks()[0].get());\n+  const auto& thunks_in_command_buffer =\n+      command_buffer_thunk->thunks()->thunks();\n+  EXPECT_THAT(thunks_in_command_buffer,\n+              ThunkKindsAre(Thunk::kCopy, Thunk::kGemm, Thunk::kConvolution));\n+}\n+\n TEST(CommandBufferConversionPassTest, ConvertsAsyncPairToCommandBuffer) {\n   std::vector<std::unique_ptr<Thunk>> thunks;\n   // Create a start thunk"
        },
        {
            "sha": "2e19a45c2a3ddd954dda9b59ca2ea8acfa6aa392",
            "filename": "third_party/xla/xla/backends/gpu/runtime/convolution_thunk.cc",
            "status": "modified",
            "additions": 38,
            "deletions": 33,
            "changes": 71,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.cc?ref=4bf20e19f440a47a276cf1988b48683778f673a6",
            "patch": "@@ -69,76 +69,81 @@ ConvolutionThunk::ConvolutionThunk(\n       descriptor_(std::move(descriptor)),\n       config_(std::move(config)) {}\n \n-GenericConvRunner& ConvolutionThunk::GetOrCreateRunner(\n-    const stream_executor::Stream* stream, bool* runner_created) {\n+std::pair<RunConvOptions, bool> ConvRunnerCache::GetOrCreate(\n+    const GpuConvConfig& config, const se::Stream* stream) {\n   absl::MutexLock lock(mu_);\n-  auto it = runner_cache_.find(stream);\n-  *runner_created = (it == runner_cache_.end());\n-  if (*runner_created) {\n-    it = runner_cache_\n-             .insert({stream, std::make_unique<GenericConvRunner>(config_)})\n-             .first;\n+  auto [it, inserted] =\n+      cache_.emplace(stream->parent(), std::unique_ptr<GenericConvRunner>{});\n+  if (inserted) {\n+    it->second = std::make_unique<GenericConvRunner>(config);\n   }\n-  return *it->second;\n+  return std::pair{RunConvOptions{nullptr, it->second.get()}, inserted};\n }\n \n-absl::Status ConvolutionThunk::ExecuteOnStream(const ExecuteParams& params) {\n+absl::Status RunConvolutionOnStream(\n+    const Thunk::ExecuteParams& params,\n+    const std::vector<BufferAllocation::Slice>& operand_buffers,\n+    const std::vector<BufferAllocation::Slice>& result_buffers,\n+    const BufferAllocation::Slice& scratch_buffer, const GpuConvConfig& config,\n+    ConvRunnerCache& cache, se::Stream* stream) {\n   const auto& buffer_allocations = *params.buffer_allocations;\n \n   std::vector<se::DeviceMemoryBase> operand_se_buffers, result_se_buffers;\n-  operand_se_buffers.reserve(operand_buffers_.size());\n-  for (BufferAllocation::Slice buffer : operand_buffers_) {\n+  operand_se_buffers.reserve(operand_buffers.size());\n+\n+  for (BufferAllocation::Slice buffer : operand_buffers) {\n     operand_se_buffers.push_back(buffer_allocations.GetDeviceAddress(buffer));\n+    VLOG(5) << \"operand buffer: \" << buffer.ToString()\n+            << \" addr: \" << operand_se_buffers.back().opaque();\n   }\n \n-  result_se_buffers.reserve(result_buffers_.size());\n-  for (BufferAllocation::Slice buffer : result_buffers_) {\n+  result_se_buffers.reserve(result_buffers.size());\n+  for (BufferAllocation::Slice buffer : result_buffers) {\n     result_se_buffers.push_back(buffer_allocations.GetDeviceAddress(buffer));\n+    VLOG(5) << \"result buffer: \" << buffer.ToString()\n+            << \" addr: \" << result_se_buffers.back().opaque();\n   }\n \n   se::DeviceMemoryBase scratch =\n-      buffer_allocations.GetDeviceAddress(scratch_buffer_);\n-\n-  bool runner_created = false;\n-  RunConvOptions opts;\n-  opts.runner_cache = &GetOrCreateRunner(params.stream, &runner_created);\n+      buffer_allocations.GetDeviceAddress(scratch_buffer);\n+  VLOG(5) << \"scratch buffer: \" << scratch_buffer\n+          << \" addr: \" << scratch.opaque();\n \n-  if (runner_created && params.stream->parent()\n+  auto [opts, runner_created] = cache.GetOrCreate(config, stream);\n+  if (runner_created && stream->parent()\n                             ->GetDeviceDescription()\n                             .gpu_compute_capability()\n                             .IsRocm()) {\n     TF_ASSIGN_OR_RETURN(\n         GpuConvParams conv_params,\n-        GetGpuConvParams(config_, operand_se_buffers, result_se_buffers));\n+        GetGpuConvParams(config, operand_se_buffers, result_se_buffers));\n \n     TF_ASSIGN_OR_RETURN(se::dnn::DataType input_type,\n-                        GetDNNDataTypeFromPrimitiveType(config_.input_type));\n+                        GetDNNDataTypeFromPrimitiveType(config.input_type));\n \n     TF_ASSIGN_OR_RETURN(se::dnn::DataType output_type,\n-                        GetDNNDataTypeFromPrimitiveType(config_.output_type));\n+                        GetDNNDataTypeFromPrimitiveType(config.output_type));\n \n-    TF_ASSIGN_OR_RETURN(auto dnn,\n-                        se::dnn::internal::GetDnnFromStream(params.stream));\n+    TF_ASSIGN_OR_RETURN(auto dnn, se::dnn::internal::GetDnnFromStream(stream));\n     se::OwningScratchAllocator<> scratch_allocator(\n         buffer_allocations.device_ordinal(),\n         buffer_allocations.memory_allocator());\n \n     std::vector<se::dnn::ProfileResult> profile_results;\n     dnn->GetMIOpenConvolveAlgorithms(\n-        CudnnConvKindToProto(config_.kind), input_type, output_type,\n-        params.stream, config_.input_descriptor, conv_params.input_buf,\n-        config_.filter_descriptor, conv_params.filter_buf,\n-        config_.output_descriptor, conv_params.output_buf, config_.conv_desc,\n+        CudnnConvKindToProto(config.kind), input_type, output_type, stream,\n+        config.input_descriptor, conv_params.input_buf,\n+        config.filter_descriptor, conv_params.filter_buf,\n+        config.output_descriptor, conv_params.output_buf, config.conv_desc,\n         &scratch_allocator, &profile_results);\n   }\n-\n-  TF_RETURN_IF_ERROR(RunGpuConv(config_, absl::MakeSpan(operand_se_buffers),\n+  TF_RETURN_IF_ERROR(RunGpuConv(config, absl::MakeSpan(operand_se_buffers),\n                                 absl::MakeSpan(result_se_buffers), scratch,\n-                                params.stream, opts));\n+                                stream, opts));\n \n   // Note: Convolution has a tuple buffer as an output, but we don't need to\n   // populate it as no one should be reading from the tuple directly.\n-  if (!params.stream->ok()) {\n+  if (!stream->ok()) {\n     return Internal(\"ConvolutionThunk::ExecuteOnStream failed.\");\n   }\n   return absl::OkStatus();"
        },
        {
            "sha": "1ceb3e99d74478a3c2cbc4d7892f4a5dd99e48f0",
            "filename": "third_party/xla/xla/backends/gpu/runtime/convolution_thunk.h",
            "status": "modified",
            "additions": 36,
            "deletions": 8,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fconvolution_thunk.h?ref=4bf20e19f440a47a276cf1988b48683778f673a6",
            "patch": "@@ -33,12 +33,40 @@ limitations under the License.\n namespace xla {\n namespace gpu {\n \n+struct ConvRunnerCache {\n+  ConvRunnerCache() = default;\n+  ConvRunnerCache(const ConvRunnerCache&) = delete;\n+  ConvRunnerCache& operator=(const ConvRunnerCache&) = delete;\n+\n+  std::pair<RunConvOptions, bool> GetOrCreate(const GpuConvConfig& config,\n+                                              const se::Stream* stream);\n+\n+ private:\n+  absl::Mutex mu_;\n+  absl::flat_hash_map<const se::StreamExecutor*,\n+                      std::unique_ptr<GenericConvRunner>>\n+      cache_ ABSL_GUARDED_BY(mu_);\n+};\n+\n+absl::Status RunConvolutionOnStream(\n+    const Thunk::ExecuteParams& params,\n+    const std::vector<BufferAllocation::Slice>& operand_buffers,\n+    const std::vector<BufferAllocation::Slice>& result_buffers,\n+    const BufferAllocation::Slice& scratch_buffer, const GpuConvConfig& config,\n+    ConvRunnerCache& cache, se::Stream* stream);\n+\n+// Forward declaration needed to initialize ConvolutionCmd with ConvolutionThunk\n+// members.\n+class ConvolutionCmd;\n+\n // This class stores everything that StreamExecutor needs to launch a DNN\n // convolution. It is generated by IrEmitter.\n //\n // This is thread-compatible.\n class ConvolutionThunk : public Thunk {\n  public:\n+  friend class ConvolutionCmd;\n+\n   // Constructs a thunk for launching a DNN convolution.\n   //\n   // operand_slices should be in the same order as cudnn_call->operands().\n@@ -51,7 +79,12 @@ class ConvolutionThunk : public Thunk {\n   ConvolutionThunk(const ConvolutionThunk&) = delete;\n   ConvolutionThunk& operator=(const ConvolutionThunk&) = delete;\n \n-  absl::Status ExecuteOnStream(const ExecuteParams& params) override;\n+  absl::Status ExecuteOnStream(const ExecuteParams& params) override {\n+    VLOG(5) << \"ConvolutionThunk\";\n+    return RunConvolutionOnStream(params, operand_buffers_, result_buffers_,\n+                                  scratch_buffer_, config_, cache_,\n+                                  params.stream);\n+  }\n \n   static absl::StatusOr<std::unique_ptr<ConvolutionThunk>> FromProto(\n       ThunkInfo thunk_info, const ConvolutionThunkProto& proto,\n@@ -66,22 +99,17 @@ class ConvolutionThunk : public Thunk {\n                    std::vector<BufferAllocation::Slice> result_slices,\n                    BufferAllocation::Slice scratch_slice);\n \n+ protected:\n   std::vector<BufferAllocation::Slice> operand_buffers_;\n   std::vector<BufferAllocation::Slice> result_buffers_;\n   BufferAllocation::Slice scratch_buffer_;\n-  GenericConvRunner& GetOrCreateRunner(const stream_executor::Stream* stream,\n-                                       bool* runner_created);\n-\n   // Technically this is only needed during initialization to create the\n   // GpuConvConfig, but the actual GpuConvConfig is hard to serialize. So we\n   // keep the descriptor around for serialization purposes.\n   const GpuConvDescriptor descriptor_;\n   // Convolution config\n   const GpuConvConfig config_;\n-  absl::Mutex mu_;\n-  absl::flat_hash_map<const stream_executor::Stream*,\n-                      std::unique_ptr<GenericConvRunner>>\n-      runner_cache_ ABSL_GUARDED_BY(mu_);\n+  ConvRunnerCache cache_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "098a27b9c71d821ba0a64941ad5e69de7f2fc0aa",
            "filename": "third_party/xla/xla/service/gpu/transforms/command_buffer_scheduling.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 15,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc?ref=4bf20e19f440a47a276cf1988b48683778f673a6",
            "patch": "@@ -234,18 +234,22 @@ static bool IsCommand(const HloCustomCallInstruction* hlo,\n     return true;\n   }\n \n-  if (config.enabled_commands.contains(DebugOptions::CUDNN) &&\n-      IsCustomCallToBlockScaledDot(*hlo)) {\n-    VLOG(3) << \"Recording BlockScaledDot, target \" << hlo->custom_call_target()\n-            << \" into command buffer.\";\n-    return true;\n-  }\n-\n-  if (config.enabled_commands.contains(DebugOptions::CUDNN) &&\n-      IsCustomCallTofMHA(*hlo)) {\n-    VLOG(3) << \"Recording FusedMHA, target \" << hlo->custom_call_target()\n-            << \" into command buffer.\";\n-    return true;\n+  if (config.enabled_commands.contains(DebugOptions::CUDNN)) {\n+    if (IsCustomCallToBlockScaledDot(*hlo)) {\n+      VLOG(3) << \"Recording BlockScaledDot, target \"\n+              << hlo->custom_call_target() << \" into command buffer.\";\n+      return true;\n+    }\n+    if (IsCustomCallTofMHA(*hlo)) {\n+      VLOG(3) << \"Recording FusedMHA, target \" << hlo->custom_call_target()\n+              << \" into command buffer.\";\n+      return true;\n+    }\n+    if (IsCustomCallToDnnConvolution(*hlo)) {\n+      VLOG(3) << \"Recording convolution, target \" << hlo->custom_call_target()\n+              << \" into command buffer.\";\n+      return true;\n+    }\n   }\n \n   if (!config.enabled_commands.contains(DebugOptions::CUSTOM_CALL)) {\n@@ -390,7 +394,9 @@ CommandBufferScheduling::CollectCommandBufferSequences(\n   int64_t num_commands_in_current_seq = 0;\n \n   // Adds `current_seq` to `sequences` if it has enough commands in it.\n-  auto collect_current_seq = [&]() {\n+  auto collect_current_seq = [&](HloInstruction* instr) {\n+    VLOG(1) << \"Stopped at: \" << (instr ? instr->ToString() : \"<end>\")\n+            << \" #commands: \" << num_commands_in_current_seq;\n     if (num_commands_in_current_seq >= std::max(1, min_num_commands)) {\n       RemoveTrailingNoOps(current_seq);\n       sequences.push_back(std::move(current_seq));\n@@ -541,11 +547,11 @@ CommandBufferScheduling::CollectCommandBufferSequences(\n \n     // If we didn't find the next command, collect the current sequence and\n     // start a new one.\n-    collect_current_seq();\n+    collect_current_seq(inst);\n   }\n \n   // Don't forget to collect the final command sequence.\n-  collect_current_seq();\n+  collect_current_seq(nullptr);\n   return sequences;\n }\n "
        },
        {
            "sha": "42c4ebecb4c927a8fd088d5a4334f618beedf927",
            "filename": "third_party/xla/xla/service/gpu/transforms/command_buffer_scheduling_test.cc",
            "status": "modified",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4bf20e19f440a47a276cf1988b48683778f673a6/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc?ref=4bf20e19f440a47a276cf1988b48683778f673a6",
            "patch": "@@ -461,6 +461,50 @@ TEST_F(CommandBufferSchedulingTest, DoNotCaptureUnmatchedAsyncDone) {\n                             });\n }\n \n+TEST_F(CommandBufferSchedulingTest, ConvolutionCustomCallAndAllGather) {\n+  const char* hlo = R\"(\n+    HloModule TestModule, is_scheduled=true\n+\n+    ENTRY main {\n+      a = bf16[4,16,3,68,120]{4,3,2,1,0} parameter(0)\n+      b = bf16[768,16,1,2,2]{4,3,2,1,0} parameter(1)\n+      c = bf16[96]{0} parameter(2)\n+\n+      start = (bf16[96]{0}, bf16[768]{0}) all-gather-start(c),\n+        channel_id=555, replica_groups={{0,1,2,3,4,5,6,7}}, dimensions={0}\n+\n+      done = bf16[768]{0} all-gather-done(start)\n+      ROOT %cudnn-conv-bias-activation = (bf16[4,768,3,34,60]{4,3,2,1,0}, u8[783360]{0}) \n+            custom-call(a, b, done), window={size=1x2x2 stride=1x2x2}, \n+            dim_labels=bf012_oi012->bf012, \n+            custom_call_target=\"__cudnn$convBiasActivationForward\"\n+    })\";\n+\n+  const char* expected = R\"(\n+    CHECK: %command_buffer ([[P0:.+]]: bf16[96], [[P1:.+]]: bf16[4,16,3,68,120], [[P2:.+]]: bf16[768,16,1,2,2]) -> {{.*}} {\n+    CHECK:   %[[P0]] = bf16[96]{0} parameter(0)\n+    CHECK:   %[[P1]] = bf16[4,16,3,68,120]{4,3,2,1,0} parameter(1)\n+    CHECK:   %[[P2]] = bf16[768,16,1,2,2]{4,3,2,1,0} parameter(2)\n+    CHECK:   %[[START:.+]] = {{.*}} all-gather-start(%[[P0]])\n+    CHECK:   %[[DONE:.+]] = bf16[768]{0} all-gather-done(%[[START]])\n+    CHECK:   ROOT %[[CUDNN:.+]] =  {{.*}} custom-call(%[[P1]], %[[P2]], %[[DONE]])\n+    CHECK: }\n+\n+    CHECK: ENTRY %{{.*}} {\n+    CHECK:   %[[A:.+]] = bf16[4,16,3,68,120]{4,3,2,1,0} parameter(0)\n+    CHECK:   %[[B:.+]] = bf16[768,16,1,2,2]{4,3,2,1,0} parameter(1)\n+    CHECK:   %[[C:.+]] = bf16[96]{0} parameter(2)\n+    CHECK:   ROOT %[[CALL:.+]] =  {{.*}} call(%[[C]], %[[A]], %[[B]]),\n+    CHECK:     to_apply=%command_buffer\n+    CHECK: })\";\n+\n+  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n+                            expected, [](HloModule* module) {\n+                              EXPECT_TRUE(module->has_schedule());\n+                              TF_CHECK_OK(module->schedule().Verify());\n+                            });\n+}\n+\n TEST_F(CommandBufferSchedulingTest, CollectCommandBufferSequence) {\n   const char* hlo = R\"(\n       HloModule TestModule, is_scheduled=true"
        }
    ],
    "stats": {
        "total": 376,
        "additions": 320,
        "deletions": 56
    }
}