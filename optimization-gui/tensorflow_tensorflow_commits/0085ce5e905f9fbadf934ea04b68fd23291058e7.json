{
    "author": "beckerhe",
    "message": "Add proto serialization for the CustomCallThunk\n\nPiperOrigin-RevId: 827967159",
    "sha": "0085ce5e905f9fbadf934ea04b68fd23291058e7",
    "files": [
        {
            "sha": "014bdb85e97fedfc38c2a7d6f955b13e133fe6d9",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=0085ce5e905f9fbadf934ea04b68fd23291058e7",
            "patch": "@@ -725,9 +725,11 @@ cc_library(\n         \"@com_google_absl//absl/base:nullability\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/memory\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\",\n@@ -744,19 +746,27 @@ xla_test(\n         \":shaped_slice\",\n         \":thunk\",\n         \"//xla:executable_run_options\",\n+        \"//xla:shape_util\",\n         \"//xla/ffi\",\n+        \"//xla/ffi:attribute_map\",\n         \"//xla/ffi:ffi_api\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/service:buffer_assignment\",\n         \"//xla/service:custom_call_status_public_headers\",\n         \"//xla/service:custom_call_target_registry\",\n         \"//xla/service:executable\",\n+        \"//xla/service:hlo_module_config\",\n         \"//xla/service:platform_util\",\n         \"//xla/service/gpu:buffer_allocations\",\n         \"//xla/service/gpu:resource_requests\",\n+        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/stream_executor:stream_executor_memory_allocator\",\n         \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/util/proto:parse_text_proto\",\n+        \"@com_google_absl//absl/base\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n@@ -2494,6 +2504,7 @@ tf_proto_library(\n         \":shaped_slice_proto\",\n         \"//xla:xla_data_proto\",\n         \"//xla/core/host_offloading:host_offloading_executable_proto\",\n+        \"//xla/ffi:attribute_map_proto\",\n         \"//xla/service:buffer_assignment_proto\",\n         \"//xla/service:hlo_proto\",\n         \"//xla/service/gpu:backend_configs\",\n@@ -2534,6 +2545,7 @@ cc_library(\n         \":convolution_thunk\",\n         \":copy_thunk\",\n         \":cudnn_thunk\",\n+        \":custom_call_thunk\",\n         \":dynamic_slice_thunk\",\n         \":fft_thunk\",\n         \":gemm_thunk\",\n@@ -2549,8 +2561,10 @@ cc_library(\n         \":triangular_solve_thunk\",\n         \":wait_for_streams_thunk\",\n         \":while_thunk\",\n+        \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/base:nullability\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n@@ -2572,7 +2586,12 @@ xla_cc_test(\n         \":thunk_proto_cc\",\n         \":thunk_proto_deserialization\",\n         \":while_thunk\",\n+        \"//xla:shape_util\",\n+        \"//xla/ffi\",\n+        \"//xla/ffi:ffi_api\",\n+        \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_assignment\",\n+        \"//xla/service:hlo_module_config\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/util/proto:parse_text_proto\",\n         \"//xla/tsl/util/proto:proto_matchers\","
        },
        {
            "sha": "4d06e2418559c84966fbc0531011749b600f0370",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk.cc",
            "status": "modified",
            "additions": 81,
            "deletions": 0,
            "changes": 81,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc?ref=0085ce5e905f9fbadf934ea04b68fd23291058e7",
            "patch": "@@ -28,10 +28,12 @@ limitations under the License.\n #include \"absl/algorithm/container.h\"\n #include \"absl/base/nullability.h\"\n #include \"absl/container/inlined_vector.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/memory/memory.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n@@ -540,5 +542,84 @@ absl::Status CustomCallThunk::ExecuteOnStream(const ExecuteParams& params) {\n   return ExecuteCustomCall(params);\n }\n \n+absl::StatusOr<ThunkProto> CustomCallThunk::ToProto() const {\n+  if (!api_version_.has_value()) {\n+    return absl::FailedPreconditionError(\n+        \"CustomCallThunk was created from a non-registered target and cannot \"\n+        \"be serialized to a proto\");\n+  }\n+\n+  ThunkProto proto;\n+  *proto.mutable_thunk_info() = thunk_info().ToProto();\n+  proto.mutable_custom_call_thunk()->set_target_name(target_name_);\n+  proto.mutable_custom_call_thunk()->set_opaque(opaque_);\n+  proto.mutable_custom_call_thunk()->set_api_version(api_version_.value());\n+  if (called_computation_ != nullptr) {\n+    proto.mutable_custom_call_thunk()->set_called_computation(\n+        called_computation_->name());\n+  }\n+\n+  for (const NullableShapedSlice& operand : operands_) {\n+    TF_ASSIGN_OR_RETURN(*proto.mutable_custom_call_thunk()->add_operands(),\n+                        operand.ToProto());\n+  }\n+\n+  for (const NullableShapedSlice& result : results_) {\n+    TF_ASSIGN_OR_RETURN(*proto.mutable_custom_call_thunk()->add_results(),\n+                        result.ToProto());\n+  }\n+\n+  if (attributes_.has_value()) {\n+    *proto.mutable_custom_call_thunk()->mutable_attributes() =\n+        attributes_->ToProto();\n+  }\n+  return proto;\n+}\n+\n+absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::FromProto(\n+    ThunkInfo thunk_info, const CustomCallThunkProto& proto,\n+    absl::Span<const BufferAllocation> buffer_allocations,\n+    const HloModule* absl_nullable hlo_module,\n+    absl::string_view platform_name) {\n+  if (hlo_module == nullptr && proto.has_called_computation()) {\n+    return absl::InvalidArgumentError(\n+        \"HloModule is required to deserialize a CustomCallThunk with a \"\n+        \"called computation\");\n+  }\n+\n+  std::vector<NullableShapedSlice> operands, results;\n+  for (const auto& operand_proto : proto.operands()) {\n+    TF_ASSIGN_OR_RETURN(\n+        NullableShapedSlice operand,\n+        NullableShapedSlice::FromProto(operand_proto, buffer_allocations));\n+    operands.push_back(std::move(operand));\n+  }\n+  for (const auto& result_proto : proto.results()) {\n+    TF_ASSIGN_OR_RETURN(\n+        NullableShapedSlice result,\n+        NullableShapedSlice::FromProto(result_proto, buffer_allocations));\n+    results.push_back(std::move(result));\n+  }\n+  TF_ASSIGN_OR_RETURN(ffi::AttributesMap attributes,\n+                      ffi::AttributesMap::FromProto(proto.attributes()));\n+\n+  HloComputation* called_computation = nullptr;\n+  if (proto.has_called_computation()) {\n+    CHECK(hlo_module != nullptr);  // This check is needed for static analysis.\n+    called_computation =\n+        hlo_module->GetComputationWithName(proto.called_computation());\n+    if (called_computation == nullptr) {\n+      return absl::InvalidArgumentError(absl::StrCat(\n+          \"HloComputation '\", proto.called_computation(),\n+          \"' not found in the HloModule with name '\", hlo_module->name(), \"'\"));\n+    }\n+  }\n+\n+  return CustomCallThunk::Create(std::move(thunk_info), proto.target_name(),\n+                                 std::move(operands), std::move(results),\n+                                 std::move(attributes), called_computation,\n+                                 platform_name);\n+}\n+\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "7324bd1666738fa1f4f1dc09c23536603944b91e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk.h",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h?ref=0085ce5e905f9fbadf934ea04b68fd23291058e7",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n #include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/executable_run_options.h\"\n@@ -40,6 +41,7 @@ limitations under the License.\n #include \"xla/ffi/ffi_api.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/runtime/object_pool.h\"\n+#include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/custom_call_status.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/stream_executor/device_memory_allocator.h\"\n@@ -146,6 +148,14 @@ class CustomCallThunk : public Thunk {\n \n   absl::string_view opaque() const { return opaque_; }\n \n+  absl::StatusOr<ThunkProto> ToProto() const override;\n+\n+  static absl::StatusOr<std::unique_ptr<CustomCallThunk>> FromProto(\n+      ThunkInfo thunk_info, const CustomCallThunkProto& proto,\n+      absl::Span<const BufferAllocation> buffer_allocations,\n+      const HloModule* absl_nullable hlo_module,\n+      absl::string_view platform_name);\n+\n  private:\n   CustomCallThunk(ThunkInfo thunk_info, std::string target_name,\n                   std::vector<NullableShapedSlice> operands,"
        },
        {
            "sha": "29199825bb96aeca2cfa1df4be15df4809038604",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk_test.cc",
            "status": "modified",
            "additions": 124,
            "deletions": 0,
            "changes": 124,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk_test.cc?ref=0085ce5e905f9fbadf934ea04b68fd23291058e7",
            "patch": "@@ -15,34 +15,47 @@ limitations under the License.\n \n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n \n+#include <array>\n #include <cstddef>\n+#include <cstdint>\n #include <memory>\n #include <string>\n #include <utility>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/base/casts.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/executable_run_options.h\"\n+#include \"xla/ffi/attribute_map.h\"\n #include \"xla/ffi/ffi.h\"\n #include \"xla/ffi/ffi_api.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/custom_call_status.h\"\n #include \"xla/service/custom_call_target_registry.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/service/gpu/resource_requests.h\"\n+#include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/service/service_executable_run_options.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream.h\"\n #include \"xla/stream_executor/stream_executor_memory_allocator.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/util/proto/parse_text_proto.h\"\n \n namespace xla::gpu {\n namespace {\n@@ -321,5 +334,116 @@ TEST(CustomCallThunkTest, CustomCallWithOwnedHandlersWithoutExecute) {\n               StatusIs(absl::StatusCode::kInvalidArgument));\n }\n \n+// A simple callback function that expects specific arguments.\n+absl::Status VerifyCallbackArguments(int my_attribute,\n+                                     ffi::AnyBuffer my_operand,\n+                                     ffi::Result<ffi::AnyBuffer> my_result,\n+                                     const HloComputation* called_computation) {\n+  EXPECT_EQ(my_attribute, 42);\n+  EXPECT_EQ(my_operand.element_type(), xla::PrimitiveType::U8);\n+  EXPECT_EQ(my_operand.device_memory().opaque(),\n+            absl::bit_cast<void*>(static_cast<intptr_t>(0xDEADBEEF)));\n+  EXPECT_EQ(my_result->element_type(), xla::PrimitiveType::U16);\n+  EXPECT_EQ(my_result->device_memory().opaque(),\n+            absl::bit_cast<void*>(static_cast<intptr_t>(0xABCDEF)));\n+  EXPECT_EQ(called_computation->name(), \"test_computation\");\n+  return absl::OkStatus();\n+}\n+\n+XLA_FFI_DEFINE_HANDLER(kVerifyCallbackArguments, VerifyCallbackArguments,\n+                       ffi::Ffi::Bind()\n+                           .Attr<int>(\"my_attribute\")\n+                           .Arg<ffi::AnyBuffer>()\n+                           .Ret<ffi::AnyBuffer>()\n+                           .Ctx<ffi::CalledComputation>(),\n+                       {ffi::Traits::kCmdBufferCompatible});\n+\n+constexpr absl::string_view kVerifyCallbackArgumentsCustomCallName =\n+    \"__xla_test$$verify_callback_arguments\";\n+constexpr absl::string_view kTestPlatformName = \"TEST_PLATFORM\";\n+\n+XLA_FFI_REGISTER_HANDLER(ffi::GetXlaFfiApi(),\n+                         kVerifyCallbackArgumentsCustomCallName,\n+                         kTestPlatformName, kVerifyCallbackArguments);\n+\n+TEST(CustomCallThunkTest, ProtoConversion) {\n+  TF_ASSERT_OK_AND_ASSIGN(se::StreamExecutor * executor, GpuExecutor());\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<se::Stream> stream,\n+                          executor->CreateStream());\n+\n+  HloModuleConfig config;\n+  HloModule hlo_module(\"test_module\", config);\n+  HloComputation::Builder builder(\"test_computation\");\n+  // This instruction is pretty arbitrary, we just need a non-empty computation.\n+  builder.AddInstruction(HloInstruction::CreateParameter(\n+      0, ShapeUtil::MakeShape(U32, {42}), \"parameter\"));\n+  hlo_module.AddEntryComputation(builder.Build());\n+\n+  BufferAllocation alloc0{0, 1024, 0};\n+  BufferAllocation alloc1{1, 1024, 0};\n+  ShapedSlice operand_slice{BufferAllocation::Slice{&alloc0, 0, 1024},\n+                            ShapeUtil::MakeShape(U8, {1024})};\n+  ShapedSlice result_slice{BufferAllocation::Slice{&alloc1, 0, 1024},\n+                           ShapeUtil::MakeShape(U16, {512})};\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<CustomCallThunk> original_thunk,\n+      CustomCallThunk::Create(\n+          Thunk::ThunkInfo(),\n+          /*target_name=*/std::string(kVerifyCallbackArgumentsCustomCallName),\n+          /*operands=*/{operand_slice},\n+          /*results=*/{result_slice}, /*attributes=*/{{\"my_attribute\", 42}},\n+          hlo_module.entry_computation(),\n+          /*platform_name=*/kTestPlatformName));\n+  TF_ASSERT_OK_AND_ASSIGN(ThunkProto proto, original_thunk->ToProto());\n+  ASSERT_TRUE(proto.has_custom_call_thunk());\n+  original_thunk.reset();\n+\n+  std::array allocations = {alloc0, alloc1};\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<CustomCallThunk> new_thunk,\n+      CustomCallThunk::FromProto(Thunk::ThunkInfo(), proto.custom_call_thunk(),\n+                                 allocations, &hlo_module, kTestPlatformName));\n+\n+  se::StreamExecutorMemoryAllocator allocator(executor);\n+  BufferAllocations device_allocations(\n+      {stream_executor::DeviceMemoryBase(\n+           absl::bit_cast<void*>(static_cast<intptr_t>(0xDEADBEEF)), 1024),\n+       stream_executor::DeviceMemoryBase(\n+           absl::bit_cast<void*>(static_cast<intptr_t>(0xABCDEF)), 1024)},\n+      0, &allocator);\n+  Thunk::ExecuteParams params = Thunk::ExecuteParams::Create(\n+      ServiceExecutableRunOptions(), device_allocations,\n+      /*stream=*/stream.get(),\n+      /*command_buffer_trace_stream=*/stream.get(),\n+      /*collective_params=*/nullptr,\n+      /*collective_cliques=*/nullptr);\n+  EXPECT_THAT(new_thunk->ExecuteOnStream(params), IsOk());\n+}\n+\n+TEST(CustomCallThunkTest, DeserializationFailsWithMissingHloModule) {\n+  CustomCallThunkProto proto =\n+      tsl::proto_testing::ParseTextProtoOrDie<CustomCallThunkProto>(\n+          R\"pb(\n+            target_name: \"__xla_test$$verify_callback_arguments\"\n+            api_version: API_VERSION_TYPED_FFI\n+            called_computation: \"called_computation\"\n+          )pb\");\n+\n+  HloModuleConfig config;\n+  HloModule hlo_module(\"test_module\", config);\n+  HloComputation::Builder builder(\"not_called_computation\");\n+  // This instruction is pretty arbitrary, we just need a non-empty computation.\n+  builder.AddInstruction(HloInstruction::CreateParameter(\n+      0, ShapeUtil::MakeShape(U32, {42}), \"parameter\"));\n+  hlo_module.AddEntryComputation(builder.Build());\n+\n+  EXPECT_THAT(CustomCallThunk::FromProto(Thunk::ThunkInfo(), proto,\n+                                         /*buffer_allocations=*/{}, &hlo_module,\n+                                         /*platform_name=*/kTestPlatformName),\n+              StatusIs(absl::StatusCode::kInvalidArgument));\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        },
        {
            "sha": "9ee4f097b72ac7c933b854b969f7f712e7f52446",
            "filename": "third_party/xla/xla/backends/gpu/runtime/dynamic_slice_thunk_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk_test.cc?ref=0085ce5e905f9fbadf934ea04b68fd23291058e7",
            "patch": "@@ -115,7 +115,9 @@ void CheckProtoRoundTrip(const DynamicSliceThunk& thunk,\n       [](const ThunkProto& thunk_proto,\n          absl::Span<const BufferAllocation> fake_allocations_span)\n       -> absl::StatusOr<std::unique_ptr<Thunk>> {\n-    return DeserializeThunkProto(thunk_proto, fake_allocations_span);\n+    return DeserializeThunkProto(thunk_proto, fake_allocations_span,\n+                                 /*hlo_module*/ nullptr,\n+                                 /*platform_name=*/\"TEST_PLATFORM\");\n   };\n \n   TF_ASSERT_OK_AND_ASSIGN("
        },
        {
            "sha": "d78f66fd11ca1820e5c66932883f1730ea1e6f7d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.proto",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto?ref=0085ce5e905f9fbadf934ea04b68fd23291058e7",
            "patch": "@@ -21,6 +21,7 @@ import \"xla/backends/gpu/runtime/convolution_filter_thunk.proto\";\n import \"xla/backends/gpu/runtime/dynamic_slice_thunk.proto\";\n import \"xla/backends/gpu/runtime/shaped_slice.proto\";\n import \"xla/core/host_offloading/host_offloading_executable.proto\";\n+import \"xla/ffi/attribute_map.proto\";\n import \"xla/service/buffer_assignment.proto\";\n import \"xla/service/gpu/gpu_conv_runner.proto\";\n import \"xla/service/gpu/gpu_norm_runner.proto\";\n@@ -248,6 +249,18 @@ message FftThunkProto {\n   xla.ShapeProto output_shape = 6;\n }\n \n+message CustomCallThunkProto {\n+  string target_name = 1;\n+  repeated NullableShapedSliceProto operands = 2;\n+  repeated NullableShapedSliceProto results = 3;\n+  string opaque = 4;\n+  CustomCallApiVersion api_version = 5;\n+  xla.ffi.AttributesMapProto attributes = 6;\n+  // The name of the called computation. It needs to match the HloCompuation in\n+  // the HloModule that is used to deserialize the thunk.\n+  optional string called_computation = 7;\n+}\n+\n message ThunkProto {\n   ThunkInfoProto thunk_info = 1;\n \n@@ -280,6 +293,7 @@ message ThunkProto {\n     FftThunkProto fft_thunk = 27;\n     CholeskyThunkProto cholesky_thunk = 28;\n     Memset32BitValueThunkProto memset32bit_value_thunk = 29;\n+    CustomCallThunkProto custom_call_thunk = 30;\n   }\n }\n "
        },
        {
            "sha": "61425b15e162138a4d3758c0e4112d27f4251d45",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_proto_deserialization.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 18,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.cc?ref=0085ce5e905f9fbadf934ea04b68fd23291058e7",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <optional>\n #include <utility>\n \n+#include \"absl/base/nullability.h\"\n #include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n@@ -32,6 +33,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/convolution_thunk.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/cudnn_thunk.h\"\n+#include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n #include \"xla/backends/gpu/runtime/dynamic_slice_thunk.h\"\n #include \"xla/backends/gpu/runtime/fft_thunk.h\"\n #include \"xla/backends/gpu/runtime/gemm_thunk.h\"\n@@ -47,6 +49,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/triangular_solve_thunk.h\"\n #include \"xla/backends/gpu/runtime/wait_for_streams_thunk.h\"\n #include \"xla/backends/gpu/runtime/while_thunk.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n@@ -72,15 +75,19 @@ static std::optional<absl::string_view> GetStoredThunkTypeName(\n \n absl::StatusOr<std::unique_ptr<Thunk>> DeserializeThunkProto(\n     const ThunkProto& thunk_proto,\n-    absl::Span<const BufferAllocation> buffer_allocations) {\n+    absl::Span<const BufferAllocation> buffer_allocations,\n+    const HloModule* absl_nullable hlo_module,\n+    absl::string_view platform_name) {\n   TF_ASSIGN_OR_RETURN(Thunk::ThunkInfo thunk_info,\n                       Thunk::ThunkInfo::FromProto(thunk_proto.thunk_info()));\n+  auto deserializer = [&buffer_allocations, &hlo_module,\n+                       &platform_name](const ThunkProto& thunk_proto) {\n+    return DeserializeThunkProto(thunk_proto, buffer_allocations, hlo_module,\n+                                 platform_name);\n+  };\n \n   switch (thunk_proto.impl_case()) {\n     case ThunkProto::kSequentialThunk: {\n-      auto deserializer = [&buffer_allocations](const ThunkProto& thunk_proto) {\n-        return DeserializeThunkProto(thunk_proto, buffer_allocations);\n-      };\n       return SequentialThunk::FromProto(\n           std::move(thunk_info), thunk_proto.sequential_thunk(), deserializer);\n     }\n@@ -100,18 +107,13 @@ absl::StatusOr<std::unique_ptr<Thunk>> DeserializeThunkProto(\n           std::move(thunk_info), thunk_proto.device_to_device_copy_thunk(),\n           buffer_allocations);\n     case ThunkProto::kWhileThunk:\n-      return WhileThunk::FromProto(\n-          std::move(thunk_info), thunk_proto.while_thunk(), buffer_allocations,\n-          [&buffer_allocations](const ThunkProto& thunk_proto) {\n-            return DeserializeThunkProto(thunk_proto, buffer_allocations);\n-          });\n+      return WhileThunk::FromProto(std::move(thunk_info),\n+                                   thunk_proto.while_thunk(),\n+                                   buffer_allocations, deserializer);\n     case ThunkProto::kConditionalThunk:\n-      return ConditionalThunk::FromProto(\n-          std::move(thunk_info), thunk_proto.conditional_thunk(),\n-          buffer_allocations,\n-          [&buffer_allocations](const ThunkProto& thunk_proto) {\n-            return DeserializeThunkProto(thunk_proto, buffer_allocations);\n-          });\n+      return ConditionalThunk::FromProto(std::move(thunk_info),\n+                                         thunk_proto.conditional_thunk(),\n+                                         buffer_allocations, deserializer);\n     case ThunkProto::kGemmThunk:\n       return GemmThunk::FromProto(std::move(thunk_info),\n                                   thunk_proto.gemm_thunk(), buffer_allocations);\n@@ -170,14 +172,20 @@ absl::StatusOr<std::unique_ptr<Thunk>> DeserializeThunkProto(\n           buffer_allocations);\n     case ThunkProto::kDynamicSliceThunk: {\n       auto deserializer =\n-          [](const ThunkProto& thunk_proto,\n-             absl::Span<const BufferAllocation> custom_allocations) {\n-            return DeserializeThunkProto(thunk_proto, custom_allocations);\n+          [hlo_module, platform_name](\n+              const ThunkProto& thunk_proto,\n+              absl::Span<const BufferAllocation> custom_allocations) {\n+            return DeserializeThunkProto(thunk_proto, custom_allocations,\n+                                         hlo_module, platform_name);\n           };\n       return DynamicSliceThunk::FromProto(std::move(thunk_info),\n                                           thunk_proto.dynamic_slice_thunk(),\n                                           buffer_allocations, deserializer);\n     }\n+    case ThunkProto::kCustomCallThunk:\n+      return CustomCallThunk::FromProto(\n+          std::move(thunk_info), thunk_proto.custom_call_thunk(),\n+          buffer_allocations, hlo_module, platform_name);\n     default:\n       std::optional<absl::string_view> unsupported_thunk_type =\n           GetStoredThunkTypeName(thunk_proto);"
        },
        {
            "sha": "729c09847cd1efb0dd2b37765d3674d0f7d7b62c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_proto_deserialization.h",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization.h?ref=0085ce5e905f9fbadf934ea04b68fd23291058e7",
            "patch": "@@ -18,18 +18,22 @@ limitations under the License.\n \n #include <memory>\n \n+#include \"absl/base/nullability.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/buffer_assignment.h\"\n \n namespace xla::gpu {\n \n // Deserializes the given `thunk_proto` into a Thunk.\n absl::StatusOr<std::unique_ptr<Thunk>> DeserializeThunkProto(\n     const ThunkProto& thunk_proto,\n-    absl::Span<const BufferAllocation> buffer_allocations);\n+    absl::Span<const BufferAllocation> buffer_allocations,\n+    const HloModule* absl_nullable hlo_module, absl::string_view platform_name);\n \n }  // namespace xla::gpu\n "
        },
        {
            "sha": "bbb19ad3af9bfadad64f8b8748e3edbd68dd2979",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_proto_deserialization_test.cc",
            "status": "modified",
            "additions": 139,
            "deletions": 20,
            "changes": 159,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_proto_deserialization_test.cc?ref=0085ce5e905f9fbadf934ea04b68fd23291058e7",
            "patch": "@@ -30,7 +30,14 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/backends/gpu/runtime/while_thunk.h\"\n+#include \"xla/ffi/ffi.h\"\n+#include \"xla/ffi/ffi_api.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/hlo_module_config.h\"\n+#include \"xla/shape_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/util/proto/parse_text_proto.h\"\n #include \"xla/tsl/util/proto/proto_matchers.h\"\n@@ -46,6 +53,8 @@ using ::tsl::proto_testing::EqualsProto;\n using ::tsl::proto_testing::ParseTextProtoOrDie;\n using Kind = Thunk::Kind;\n \n+constexpr absl::string_view kTestPlatformName = \"TEST_PLATFORM\";\n+\n TEST(ThunkProtoDeserializationTest, SequentialThunkChain) {\n   constexpr ExecutionStreamId kExecutionStreamId{123};\n   constexpr absl::string_view kProfileAnnotation = \"profile_annotation\";\n@@ -63,8 +72,10 @@ TEST(ThunkProtoDeserializationTest, SequentialThunkChain) {\n   SequentialThunk outer_thunk(thunk_info, std::move(thunk_sequence));\n \n   TF_ASSERT_OK_AND_ASSIGN(ThunkProto proto, outer_thunk.ToProto());\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Thunk> new_thunk,\n-                          DeserializeThunkProto(proto, {}));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<Thunk> new_thunk,\n+      DeserializeThunkProto(proto, /*buffer_allocations=*/{},\n+                            /*hlo_module=*/nullptr, kTestPlatformName));\n \n   EXPECT_THAT(new_thunk.get(),\n               WhenDynamicCastTo<const SequentialThunk*>(Property(\n@@ -91,8 +102,10 @@ TEST(ThunkProtoDeserializationTest, CopyThunk) {\n       BufferAllocation(/*index=*/0, /*size=*/1024, /*color=*/0),\n       BufferAllocation(/*index=*/1, /*size=*/1024, /*color=*/0)};\n \n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Thunk> thunk,\n-                          DeserializeThunkProto(proto, buffer_allocations));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<Thunk> thunk,\n+      DeserializeThunkProto(proto, buffer_allocations, /*hlo_module=*/nullptr,\n+                            kTestPlatformName));\n   auto* copy_thunk = dynamic_cast<CopyThunk*>(thunk.get());\n   ASSERT_NE(copy_thunk, nullptr);  // Check the cast succeeded\n   TF_ASSERT_OK_AND_ASSIGN(ThunkProto round_trip_proto, copy_thunk->ToProto());\n@@ -123,8 +136,10 @@ TEST(ThunkProtoDeserializationTest, DeviceToHostCopyThunk) {\n       BufferAllocation(/*index=*/0, /*size=*/1024, /*color=*/0),\n       BufferAllocation(/*index=*/1, /*size=*/1024, /*color=*/0)};\n \n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Thunk> thunk,\n-                          DeserializeThunkProto(proto, buffer_allocations));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<Thunk> thunk,\n+      DeserializeThunkProto(proto, buffer_allocations, /*hlo_module=*/nullptr,\n+                            kTestPlatformName));\n   auto* copy_thunk = dynamic_cast<DeviceToHostCopyThunk*>(thunk.get());\n   ASSERT_NE(copy_thunk, nullptr);  // Check the cast succeeded\n   TF_ASSERT_OK_AND_ASSIGN(ThunkProto round_trip_proto, copy_thunk->ToProto());\n@@ -155,8 +170,10 @@ TEST(ThunkProtoDeserializationTest, HostToDeviceCopyThunk) {\n       BufferAllocation(/*index=*/0, /*size=*/1024, /*color=*/0),\n       BufferAllocation(/*index=*/1, /*size=*/1024, /*color=*/0)};\n \n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Thunk> thunk,\n-                          DeserializeThunkProto(proto, buffer_allocations));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<Thunk> thunk,\n+      DeserializeThunkProto(proto, buffer_allocations, /*hlo_module=*/nullptr,\n+                            kTestPlatformName));\n   auto* copy_thunk = dynamic_cast<HostToDeviceCopyThunk*>(thunk.get());\n   ASSERT_NE(copy_thunk, nullptr);  // Check the cast succeeded\n   TF_ASSERT_OK_AND_ASSIGN(ThunkProto round_trip_proto, copy_thunk->ToProto());\n@@ -187,8 +204,10 @@ TEST(ThunkProtoDeserializationTest, DeviceToDeviceCopyThunk) {\n       BufferAllocation(/*index=*/0, /*size=*/1024, /*color=*/0),\n       BufferAllocation(/*index=*/1, /*size=*/1024, /*color=*/0)};\n \n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Thunk> thunk,\n-                          DeserializeThunkProto(proto, buffer_allocations));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<Thunk> thunk,\n+      DeserializeThunkProto(proto, buffer_allocations, /*hlo_module=*/nullptr,\n+                            kTestPlatformName));\n   auto* copy_thunk = dynamic_cast<DeviceToDeviceCopyThunk*>(thunk.get());\n   ASSERT_NE(copy_thunk, nullptr);  // Check the cast succeeded\n   TF_ASSERT_OK_AND_ASSIGN(ThunkProto round_trip_proto, copy_thunk->ToProto());\n@@ -264,8 +283,10 @@ TEST(ThunkProtoDeserializationTest, WhileThunk) {\n       BufferAllocation(/*index=*/4, /*size=*/1024, /*color=*/0),\n       BufferAllocation(/*index=*/5, /*size=*/1024, /*color=*/0)};\n \n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Thunk> athunk,\n-                          DeserializeThunkProto(proto, buffer_allocations));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<Thunk> athunk,\n+      DeserializeThunkProto(proto, buffer_allocations, /*hlo_module=*/nullptr,\n+                            kTestPlatformName));\n   auto* thunk = dynamic_cast<WhileThunk*>(athunk.get());\n   ASSERT_NE(thunk, nullptr);\n   TF_ASSERT_OK_AND_ASSIGN(ThunkProto round_trip_proto, thunk->ToProto());\n@@ -353,8 +374,10 @@ TEST(ThunkProtoDeserializationTest, ConditionalThunk) {\n       BufferAllocation(/*index=*/4, /*size=*/1024, /*color=*/0),\n       BufferAllocation(/*index=*/5, /*size=*/1024, /*color=*/0)};\n \n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Thunk> athunk,\n-                          DeserializeThunkProto(proto, buffer_allocations));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<Thunk> athunk,\n+      DeserializeThunkProto(proto, buffer_allocations, /*hlo_module=*/nullptr,\n+                            kTestPlatformName));\n   auto* thunk = dynamic_cast<ConditionalThunk*>(athunk.get());\n   ASSERT_NE(thunk, nullptr);\n   TF_ASSERT_OK_AND_ASSIGN(ThunkProto round_trip_proto, thunk->ToProto());\n@@ -370,7 +393,8 @@ TEST(ThunkProtoDeserializationTest, WaitForStreamsThunk) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<Thunk> thunk,\n-      DeserializeThunkProto(proto, /*buffer_allocations=*/{}));\n+      DeserializeThunkProto(proto, /*buffer_allocations=*/{},\n+                            /*hlo_module=*/nullptr, kTestPlatformName));\n \n   TF_ASSERT_OK_AND_ASSIGN(ThunkProto round_trip_proto, thunk->ToProto());\n   EXPECT_THAT(round_trip_proto, EqualsProto(proto));\n@@ -391,8 +415,10 @@ TEST(ThunkProtoDeserializationTest, CudnnThunk) {\n       BufferAllocation(/*index=*/1, /*size=*/1024, /*color=*/0),\n   };\n \n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Thunk> thunk,\n-                          DeserializeThunkProto(proto, buffer_allocations));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<Thunk> thunk,\n+      DeserializeThunkProto(proto, buffer_allocations, /*hlo_module=*/nullptr,\n+                            kTestPlatformName));\n \n   TF_ASSERT_OK_AND_ASSIGN(ThunkProto round_trip_proto, thunk->ToProto());\n   EXPECT_THAT(round_trip_proto, EqualsProto(proto));\n@@ -457,8 +483,100 @@ TEST(ThunkProtoDeserializationTest, CublasLtMatmulThunk) {\n       BufferAllocation(/*index=*/5, /*size=*/161600, /*color=*/0),\n   };\n \n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Thunk> thunk,\n-                          DeserializeThunkProto(proto, allocations));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<Thunk> thunk,\n+      DeserializeThunkProto(proto, allocations, /*hlo_module=*/nullptr,\n+                            kTestPlatformName));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(ThunkProto round_trip_proto, thunk->ToProto());\n+  EXPECT_THAT(round_trip_proto, EqualsProto(proto));\n+}\n+\n+XLA_FFI_DEFINE_HANDLER(kSimpleCustomCall, []() { return absl::OkStatus(); },\n+                       ffi::Ffi::Bind(), {ffi::Traits::kCmdBufferCompatible});\n+\n+constexpr absl::string_view kSimpleCustomCallName =\n+    \"__xla_test$$simple_custom_call\";\n+\n+XLA_FFI_REGISTER_HANDLER(ffi::GetXlaFfiApi(), kSimpleCustomCallName,\n+                         \"TEST_PLATFORM\", kSimpleCustomCall);\n+\n+TEST(ThunkProtoDeserializationTest, CustomCallThunk) {\n+  ThunkProto proto = ParseTextProtoOrDie<ThunkProto>(\n+      R\"pb(\n+        thunk_info { execution_stream_id: 7 }\n+        custom_call_thunk {\n+          target_name: \"__xla_test$$simple_custom_call\"\n+          operands {\n+            shaped_slice {\n+              slice { buffer_allocation_index: 0 }\n+              shape {\n+                dimensions: 42\n+                element_type: S32\n+                is_dynamic_dimension: false\n+              }\n+            }\n+          }\n+          operands {\n+            shaped_slice {\n+              slice { buffer_allocation_index: 1 }\n+              shape {\n+                dimensions: 42\n+                element_type: S32\n+                is_dynamic_dimension: false\n+              }\n+            }\n+          }\n+          results {\n+            shaped_slice {\n+              slice { buffer_allocation_index: 2 }\n+              shape {\n+                dimensions: 42\n+                element_type: S32\n+                is_dynamic_dimension: false\n+              }\n+            }\n+          }\n+          results {\n+            shaped_slice {\n+              slice { buffer_allocation_index: 3 }\n+              shape {\n+                dimensions: 42\n+                element_type: S32\n+                is_dynamic_dimension: false\n+              }\n+            }\n+          }\n+          api_version: API_VERSION_TYPED_FFI\n+          attributes {\n+            attrs {\n+              key: \"my_attribute\"\n+              value { scalar { i32: 42 } }\n+            }\n+          }\n+          called_computation: \"called_computation\"\n+        }\n+      )pb\");\n+  std::vector<BufferAllocation> buffer_allocations = {\n+      BufferAllocation(/*index=*/0, /*size=*/1024, /*color=*/0),\n+      BufferAllocation(/*index=*/1, /*size=*/1024, /*color=*/0),\n+      BufferAllocation(/*index=*/2, /*size=*/1024, /*color=*/0),\n+      BufferAllocation(/*index=*/3, /*size=*/1024, /*color=*/0),\n+  };\n+\n+  HloModuleConfig config;\n+  HloModule hlo_module(\"test_module\", config);\n+  HloComputation::Builder builder(\"called_computation\");\n+  // This instruction is pretty arbitrary, we just need a non-empty computation.\n+  builder.AddInstruction(HloInstruction::CreateParameter(\n+      0, ShapeUtil::MakeShape(U32, {42}), \"parameter\"));\n+  hlo_module.AddEntryComputation(builder.Build());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<Thunk> thunk,\n+      DeserializeThunkProto(proto, buffer_allocations, &hlo_module,\n+                            kTestPlatformName));\n+\n   TF_ASSERT_OK_AND_ASSIGN(ThunkProto round_trip_proto, thunk->ToProto());\n   EXPECT_THAT(round_trip_proto, EqualsProto(proto));\n }\n@@ -469,7 +587,8 @@ TEST(ThunkProtoDeserializationTest, EmptyThunkImplReturnsAnError) {\n         thunk_info { execution_stream_id: 7 }\n       )pb\");\n \n-  EXPECT_THAT(DeserializeThunkProto(proto, /*buffer_allocations=*/{}),\n+  EXPECT_THAT(DeserializeThunkProto(proto, /*buffer_allocations=*/{},\n+                                    /*hlo_module=*/nullptr, kTestPlatformName),\n               absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n }\n "
        },
        {
            "sha": "6af5bd4d648ce2b962fea47ec416aac6c6e40ff8",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=0085ce5e905f9fbadf934ea04b68fd23291058e7",
            "patch": "@@ -1221,7 +1221,8 @@ absl::StatusOr<GpuExecutableProto> GpuExecutable::ToProto() const {\n \n absl::StatusOr<std::unique_ptr<GpuExecutable>> GpuExecutable::FromProto(\n     const GpuExecutableProto& proto,\n-    const se::DeviceDescription& device_description) {\n+    const se::DeviceDescription& device_description,\n+    absl::string_view platform_name) {\n   Params params;\n   params.enable_debug_info_manager = false;\n   params.asm_text = proto.asm_text();\n@@ -1263,7 +1264,8 @@ absl::StatusOr<std::unique_ptr<GpuExecutable>> GpuExecutable::FromProto(\n \n   TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<Thunk> thunk,\n-      DeserializeThunkProto(proto.thunk(), params.mlir_allocations.value()));\n+      DeserializeThunkProto(proto.thunk(), params.mlir_allocations.value(),\n+                            params.debug_module.get(), platform_name));\n \n   if (dynamic_cast<const SequentialThunk*>(thunk.get()) == nullptr) {\n     return absl::InvalidArgumentError("
        },
        {
            "sha": "52fa7b468bedfcdd00e0490366fc8676b44a8857",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h?ref=0085ce5e905f9fbadf934ea04b68fd23291058e7",
            "patch": "@@ -218,7 +218,8 @@ class GpuExecutable : public Executable {\n \n   static absl::StatusOr<std::unique_ptr<GpuExecutable>> FromProto(\n       const GpuExecutableProto&,\n-      const se::DeviceDescription& device_description);\n+      const se::DeviceDescription& device_description,\n+      absl::string_view platform);\n \n   absl::StatusOr<GpuExecutableProto> ToProto() const;\n "
        },
        {
            "sha": "9f5c12b29e2fe67ff7b5d967b2bcd0f0c502c046",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0085ce5e905f9fbadf934ea04b68fd23291058e7/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc?ref=0085ce5e905f9fbadf934ea04b68fd23291058e7",
            "patch": "@@ -474,7 +474,7 @@ TEST(GpuExecutableTest, ProtoConversion) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<GpuExecutable> reconstructed_executable,\n-      GpuExecutable::FromProto(proto, device_description));\n+      GpuExecutable::FromProto(proto, device_description, \"TEST_PLATFORM\"));\n   EXPECT_THAT(reconstructed_executable->text(), \"test_asm_text\");\n   EXPECT_THAT(reconstructed_executable->binary(), ElementsAre(1, 2, 3));\n   EXPECT_THAT("
        }
    ],
    "stats": {
        "total": 472,
        "additions": 428,
        "deletions": 44
    }
}