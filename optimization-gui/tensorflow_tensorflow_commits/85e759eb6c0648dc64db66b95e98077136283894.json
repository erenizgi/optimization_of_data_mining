{
    "author": "EusebioDM",
    "message": "Add flag that dumps a serialized `GpuExecutable`\n\nAlso removes an unused import in `gpu_executable.proto`\n\nPiperOrigin-RevId: 828445423",
    "sha": "85e759eb6c0648dc64db66b95e98077136283894",
    "files": [
        {
            "sha": "7e9db93f3a644f0123548adea78745dfc4b4e146",
            "filename": "third_party/xla/xla/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/85e759eb6c0648dc64db66b95e98077136283894/third_party%2Fxla%2Fxla%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/85e759eb6c0648dc64db66b95e98077136283894/third_party%2Fxla%2Fxla%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2FBUILD?ref=85e759eb6c0648dc64db66b95e98077136283894",
            "patch": "@@ -1236,7 +1236,6 @@ cc_library(\n             \"@com_google_absl//absl/algorithm:container\",\n             \"@com_google_absl//absl/base\",\n             \"@com_google_absl//absl/base:no_destructor\",\n-            \"@com_google_absl//absl/base:nullability\",\n             \"@com_google_absl//absl/container:flat_hash_map\",\n             \"@com_google_absl//absl/container:flat_hash_set\",\n             \"@com_google_absl//absl/container:node_hash_map\","
        },
        {
            "sha": "e777a33acf145d3e81e46c07e460f18748952dac",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/85e759eb6c0648dc64db66b95e98077136283894/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/85e759eb6c0648dc64db66b95e98077136283894/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=85e759eb6c0648dc64db66b95e98077136283894",
            "patch": "@@ -1353,6 +1353,14 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n                 debug_options->xla_gpu_experimental_dump_fdo_profiles(),\n                 \"Dumps FDO profiles as text to the directory specified \"\n                 \"by --xla_dump_to.\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_experimental_dump_gpu_executable\",\n+      bool_setter_for(\n+          &DebugOptions::set_xla_gpu_experimental_dump_gpu_executable),\n+      debug_options->xla_gpu_experimental_dump_gpu_executable(),\n+      \"Dump the serialized GPU executables to 'gpu_executable_proto' suffixed \"\n+      \"files, in the directory specified by `xla_dump_to`. No-op if \"\n+      \"`xla_dump_to` isn't set, or during autotuning compilations.\"));\n   flag_list->push_back(\n       tsl::Flag(\"xla_dump_hlo_as_dot\",\n                 bool_setter_for(&DebugOptions::set_xla_dump_hlo_as_dot),"
        },
        {
            "sha": "188d9d6140fd0541e4fd0b704ee08343a74d4f08",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/85e759eb6c0648dc64db66b95e98077136283894/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/85e759eb6c0648dc64db66b95e98077136283894/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=85e759eb6c0648dc64db66b95e98077136283894",
            "patch": "@@ -1622,6 +1622,7 @@ cc_library(\n         \":fusion_pipeline\",\n         \":gpu_constants\",\n         \":gpu_executable\",\n+        \":gpu_executable_proto_cc\",\n         \":gpu_float_support\",\n         \":gpu_hlo_schedule\",\n         \":gpu_latency_hiding_scheduler\",\n@@ -1654,6 +1655,7 @@ cc_library(\n         \"//xla/backends/gpu/runtime:runtime_intrinsics\",\n         \"//xla/backends/gpu/runtime:sequential_thunk\",\n         \"//xla/backends/gpu/runtime:thunk\",\n+        \"//xla/backends/gpu/runtime:thunk_proto_cc\",\n         \"//xla/core/host_offloading:hlo_host_device_type_call_wrapper\",\n         \"//xla/core/host_offloading:host_compute_asyncifier\",\n         \"//xla/hlo/analysis:alias_info\",\n@@ -1731,6 +1733,7 @@ cc_library(\n         \"//xla/hlo/utils:hlo_query\",\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n+        \"//xla/pjrt/proto:compile_options_proto_cc\",\n         \"//xla/service:all_reduce_promotion\",\n         \"//xla/service:all_reduce_reassociate\",\n         \"//xla/service:all_reduce_simplifier\",\n@@ -1876,6 +1879,7 @@ cc_library(\n         \"//xla/tsl/lib/monitoring:counter\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base\",\n@@ -1941,6 +1945,7 @@ xla_test(\n         \"//xla:literal\",\n         \"//xla:literal_util\",\n         \"//xla:shape_util\",\n+        \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/backends/gpu/runtime:sequential_thunk\",\n         \"//xla/backends/gpu/runtime:thunk\",\n@@ -1973,11 +1978,13 @@ xla_test(\n         \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n+        \"//xla/tsl/testing:temporary_directory\",\n         \"@com_google_absl//absl/base:log_severity\",\n         \"@com_google_absl//absl/cleanup\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/log:scoped_mock_log\",\n+        \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\","
        },
        {
            "sha": "70ed92b33c4684f821e0faed6ae31ebaf4335a42",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 44,
            "deletions": 4,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/85e759eb6c0648dc64db66b95e98077136283894/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/85e759eb6c0648dc64db66b95e98077136283894/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=85e759eb6c0648dc64db66b95e98077136283894",
            "patch": "@@ -67,6 +67,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/runtime_intrinsics.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/core/host_offloading/hlo_host_device_type_call_wrapper.h\"\n #include \"xla/core/host_offloading/host_compute_asyncifier.h\"\n #include \"xla/hlo/analysis/alias_info.h\"\n@@ -142,6 +143,7 @@ limitations under the License.\n #include \"xla/hlo/transforms/while_loop_trip_count_annotator.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/maybe_owning.h\"\n+#include \"xla/pjrt/proto/compile_options.pb.h\"\n #include \"xla/service/all_reduce_promotion.h\"\n #include \"xla/service/all_reduce_reassociate.h\"\n #include \"xla/service/all_reduce_simplifier.h\"\n@@ -179,6 +181,7 @@ limitations under the License.\n #include \"xla/service/gpu/fusion_dispatch_pipeline.h\"\n #include \"xla/service/gpu/fusion_pipeline.h\"\n #include \"xla/service/gpu/gpu_executable.h\"\n+#include \"xla/service/gpu/gpu_executable.pb.h\"\n #include \"xla/service/gpu/gpu_float_support.h\"\n #include \"xla/service/gpu/gpu_hlo_schedule.h\"\n #include \"xla/service/gpu/gpu_latency_hiding_scheduler.h\"\n@@ -2094,9 +2097,9 @@ bool ShouldAddCopyForCollectiveMemorySpace(const HloValue* value) {\n            .xla_gpu_experimental_enable_nccl_symmetric_buffers()) {\n     return false;\n   }\n-  // Add copy if a potential collective-memmory-spaced op directly consumes from\n+  // Add copy if a potential collective-memory-spaced op directly consumes from\n   // module input or a constant as they are allocated by bfc ahead of time and\n-  // the alignment might not match collective memory space's requiment.\n+  // the alignment might not match collective memory space's requirement.\n   if (absl::c_linear_search(\n           module->entry_computation()->parameter_instructions(), inst) ||\n       (inst->opcode() == HloOpcode::kConstant)) {\n@@ -2650,6 +2653,40 @@ GpuCompiler::CompileToBackendResult(\n                                    std::move(compile_module_results)};\n }\n \n+static absl::Status DumpGpuExecutableIfEnabled(\n+    const GpuExecutable& gpu_executable,\n+    const Compiler::CompileOptions& compile_options,\n+    const DebugOptions& debug_options) {\n+  // If we were to dump the GPU executable for autotuning, we would end up\n+  // creating lots of tiny executables that aren't event useful for customers.\n+  if (compile_options.is_autotuning_compilation) {\n+    return absl::OkStatus();\n+  }\n+  if (!debug_options.has_xla_dump_to() ||\n+      !debug_options.xla_gpu_experimental_dump_gpu_executable()) {\n+    return absl::OkStatus();\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(GpuExecutableProto gpu_executable_proto,\n+                      gpu_executable.ToProto());\n+  std::string serialized_proto = gpu_executable_proto.SerializeAsString();\n+  if (serialized_proto.empty()) {\n+    return absl::InternalError(\"Failed to serialize GPU executable proto\");\n+  }\n+\n+  ExecutableAndOptionsProto dump_proto;\n+  *dump_proto.mutable_serialized_executable() = std::move(serialized_proto);\n+  constexpr absl::string_view kDumpFilename = \"gpu_executable\";\n+  if (gpu_executable.has_module()) {\n+    DumpPerModuleProtobufToFile(gpu_executable.module(), dump_proto,\n+                                debug_options, kDumpFilename);\n+  } else {\n+    DumpProtobufToFile(dump_proto, debug_options, kDumpFilename);\n+  }\n+\n+  return absl::OkStatus();\n+}\n+\n absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n     std::unique_ptr<HloModule> module, se::StreamExecutor* stream_exec,\n     const CompileOptions& options) {\n@@ -2738,7 +2775,7 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n   std::unique_ptr<GpuAliasInfo> alias_info = GetAliasInfo(gpu_device_info);\n   const GpuAliasInfo* alias_info_ptr = alias_info.get();\n   TF_ASSIGN_OR_RETURN(\n-      auto gpu_executable,\n+      std::unique_ptr<GpuExecutable> gpu_executable,\n       GpuExecutable::Create(GpuExecutable::Params{\n           /*asm_text=*/(options.is_autotuning_compilation &&\n                         !res.backend_result.binary.empty())\n@@ -2760,13 +2797,16 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n           /*buffer_assignment=*/\n           std::move(res.compile_module_results.buffer_assignment),\n           /*alias_info=*/std::move(alias_info),\n-          /*debug_options=*/std::move(debug_opts),\n+          /*debug_options=*/debug_opts,\n           /*device_description=*/gpu_device_info,\n           /*debug_module=*/options.is_autotuning_compilation\n               ? std::unique_ptr<HloModule>()\n               : std::move(module),\n           /*enable_debug_info_manager=*/!options.is_autotuning_compilation}));\n \n+  TF_RETURN_IF_ERROR(\n+      DumpGpuExecutableIfEnabled(*gpu_executable, options, debug_opts));\n+\n   if (embed_ir_in_executable) {\n     std::string ir_module_string_before_opt =\n         llvm_ir::DumpToString(res.compile_module_results.llvm_module.get());"
        },
        {
            "sha": "81bd80f0f523aec8b289fcf310a81dc35793183b",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 65,
            "deletions": 12,
            "changes": 77,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/85e759eb6c0648dc64db66b95e98077136283894/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/85e759eb6c0648dc64db66b95e98077136283894/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=85e759eb6c0648dc64db66b95e98077136283894",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/log/scoped_mock_log.h\"\n+#include \"absl/status/status.h\"\n #include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n@@ -86,6 +87,8 @@ limitations under the License.\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n+#include \"xla/tsl/testing/temporary_directory.h\"\n+#include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/casts.h\"\n #include \"tsl/platform/path.h\"\n@@ -108,6 +111,7 @@ using ::testing::Not;\n using ::testing::StartsWith;\n using ::testing::TempDir;\n using ::tsl::gtl::ValueOrDie;\n+using ::tsl::testing::TemporaryDirectory;\n \n class GpuCompilerTest : public HloTestBase {\n  public:\n@@ -150,6 +154,17 @@ class GpuCompilerTest : public HloTestBase {\n   }\n };\n \n+absl::StatusOr<std::string> ReadNonEmptyFile(absl::string_view file_path) {\n+  std::string str;\n+  tsl::Env* env = tsl::Env::Default();\n+  TF_RETURN_IF_ERROR(tsl::ReadFileToString(env, std::string(file_path), &str));\n+  if (str.empty()) {\n+    return absl::InvalidArgumentError(\n+        absl::StrCat(\"File is empty: \", file_path));\n+  }\n+  return str;\n+}\n+\n TEST_F(GpuCompilerTest, CompiledProgramsCount) {\n   const char* hlo_text = R\"(\n HloModule test\n@@ -388,6 +403,50 @@ ENTRY e {\n   EXPECT_THAT(entry_root, GmockMatch(m::Fusion()));\n }\n \n+TEST_F(GpuCompilerTest, GpuExecutableDump) {\n+  constexpr absl::string_view hlo_text = R\"hlo(\n+    HloModule test\n+\n+    ENTRY main {\n+      p = f32[10]{0} parameter(0)\n+      ROOT neg = f32[10]{0} negate(p)\n+    }\n+)hlo\";\n+  HloModuleConfig config = GetModuleConfigForTest();\n+  DebugOptions& debug_options = config.mutable_debug_options();\n+  debug_options.set_xla_gpu_experimental_dump_gpu_executable(true);\n+  TF_ASSERT_OK_AND_ASSIGN(TemporaryDirectory temp_dir,\n+                          TemporaryDirectory::CreateForCurrentTestcase());\n+  debug_options.set_xla_dump_to(temp_dir.path());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_text, config));\n+  std::string module_name = module->name();\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<Executable> executable,\n+      backend().compiler()->RunBackend(std::move(module),\n+                                       backend().default_stream_executor(),\n+                                       Compiler::CompileOptions()));\n+\n+  std::vector<std::string> dump_files;\n+  TF_ASSERT_OK(tsl::Env::Default()->GetMatchingPaths(\n+      tsl::io::JoinPath(debug_options.xla_dump_to(), \"*gpu_executable.txt\"),\n+      &dump_files));\n+  ASSERT_EQ(dump_files.size(), 1);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::string dump_serialized_contents,\n+                          ReadNonEmptyFile(dump_files[0]));\n+  ExecutableAndOptionsProto dump_content;\n+  ASSERT_TRUE(tsl::protobuf::TextFormat::ParseFromString(\n+      dump_serialized_contents, &dump_content));\n+\n+  GpuExecutableProto gpu_executable_proto;\n+  ASSERT_TRUE(gpu_executable_proto.ParseFromString(\n+      dump_content.serialized_executable()));\n+  EXPECT_THAT(gpu_executable_proto.binary(), Not(IsEmpty()));\n+  EXPECT_EQ(gpu_executable_proto.module_name(), module_name);\n+}\n+\n class PersistedAutotuningTest : public HloTestBase {\n  protected:\n   static constexpr absl::string_view kHloText = R\"(\n@@ -408,14 +467,6 @@ ENTRY e {\n     return filename;\n   }\n \n-  std::string ExpectToReadNonEmptyFile(absl::string_view file_path) {\n-    std::string str;\n-    tsl::Env* env = tsl::Env::Default();\n-    TF_EXPECT_OK(tsl::ReadFileToString(env, std::string(file_path), &str));\n-    EXPECT_THAT(str, Not(IsEmpty()));\n-    return str;\n-  }\n-\n   DebugOptions GetDebugOptionsForTest() const override {\n     DebugOptions options = HloTestBase::GetDebugOptionsForTest();\n     options.set_xla_gpu_dump_autotune_results_to(\n@@ -436,8 +487,9 @@ TEST_F(PersistedAutotuningTest, WriteResultsOnEachCompilation) {\n   // Check that it writes the results on the first compilation.\n   TF_EXPECT_OK(GetOptimizedModule(kHloText).status());\n   {\n-    std::string autotune_results_str =\n-        ExpectToReadNonEmptyFile(xla_gpu_dump_autotune_results_to_);\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        std::string autotune_results_str,\n+        ReadNonEmptyFile(xla_gpu_dump_autotune_results_to_));\n     AutotuneResults results;\n     EXPECT_TRUE(tsl::protobuf::TextFormat::ParseFromString(autotune_results_str,\n                                                            &results));\n@@ -451,8 +503,9 @@ TEST_F(PersistedAutotuningTest, WriteResultsOnEachCompilation) {\n   // Check that it writes the results on the second compilation.\n   TF_EXPECT_OK(GetOptimizedModule(kHloText).status());\n   {\n-    std::string autotune_results_str =\n-        ExpectToReadNonEmptyFile(xla_gpu_dump_autotune_results_to_);\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        std::string autotune_results_str,\n+        ReadNonEmptyFile(xla_gpu_dump_autotune_results_to_));\n     AutotuneResults results;\n     EXPECT_TRUE(tsl::protobuf::TextFormat::ParseFromString(autotune_results_str,\n                                                            &results));"
        },
        {
            "sha": "f9f1ba5f008736fb463153997f31d88384c52e20",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.proto",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/85e759eb6c0648dc64db66b95e98077136283894/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/85e759eb6c0648dc64db66b95e98077136283894/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.proto?ref=85e759eb6c0648dc64db66b95e98077136283894",
            "patch": "@@ -6,7 +6,6 @@ import \"xla/backends/gpu/runtime/thunk.proto\";\n import \"xla/service/gpu/ir_emission_utils.proto\";\n import \"xla/service/hlo.proto\";\n import \"xla/shape_util.proto\";\n-import \"xla/stream_executor/cuda/cuda_compute_capability.proto\";\n import \"xla/stream_executor/device_description.proto\";\n import \"xla/xla.proto\";\n import \"xla/xla_data.proto\";"
        },
        {
            "sha": "598bb2f09c5d0d46b304e3141f17f4dc7eb6fe49",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/85e759eb6c0648dc64db66b95e98077136283894/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/85e759eb6c0648dc64db66b95e98077136283894/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=85e759eb6c0648dc64db66b95e98077136283894",
            "patch": "@@ -634,6 +634,15 @@ message DebugOptions {\n   // Dump FDO profiles in a binary format to a separate file.\n   optional bool xla_gpu_experimental_dump_fdo_profiles = 338;\n \n+  // Dump the serialized GPU executables to 'gpu_executable' suffixed\n+  // files in the directory specified by `xla_dump_to`.\n+  // No-op if `xla_dump_to` isn't set, or during autotuning compilations.\n+  //\n+  // The dumped files are serialized `xla.ExecutableAndOptionsProto` messages,\n+  // which contain the binary serialized `xla.gpu.GpuExecutableProto`, in the\n+  // `serialized_executable` field.\n+  optional bool xla_gpu_experimental_dump_gpu_executable = 427;\n+\n   // Enable windowed einsum(collective matmul) rewrite for all-to-all + gemm\n   // This feature is still experimental and effective only\n   // xla_gpu_multi_streamed_windowed_einsum is set to true.\n@@ -1404,7 +1413,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 427\n+  // Next id: 428\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 153,
        "additions": 134,
        "deletions": 19
    }
}