{
    "author": "ezhulenev",
    "message": "[xla:collectives] Move ReductionKind to a separate target to break dependency on HLO IR\n\nXLA collectives APIs should not depend on compiler IR as it might bring a lot of unnecessary dependencies to the runtime. Extract reduction kind enum + parsing + printing into a separate target under core/collectives\n\nPiperOrigin-RevId: 839600277",
    "sha": "503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
    "files": [
        {
            "sha": "3a7b80258a5cb91369ffabad28f090c406d255a5",
            "filename": "third_party/xla/xla/backends/cpu/runtime/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2FBUILD?ref=503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
            "patch": "@@ -1291,6 +1291,7 @@ cc_library(\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\","
        },
        {
            "sha": "596b9c86865abad9791e2effb1870f44d785f45d",
            "filename": "third_party/xla/xla/backends/cpu/runtime/all_reduce_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fall_reduce_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fall_reduce_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fall_reduce_thunk.cc?ref=503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
            "patch": "@@ -72,9 +72,9 @@ tsl::AsyncValueRef<AllReduceThunk::ExecuteEvent> AllReduceThunk::Execute(\n \n   VLOG(3) << absl::StreamFormat(\n       \"AllReduce: #source_buffers=%d, #destination_buffers=%d, \"\n-      \"reduction_kind=%s, single_replica=%v\",\n-      data.source.size(), data.destination.size(),\n-      ReductionKindToString(reduction_kind_), single_replica_);\n+      \"reduction_kind=%v, single_replica=%v\",\n+      data.source.size(), data.destination.size(), reduction_kind_,\n+      single_replica_);\n \n   for (int i = 0; i < data.source.size(); ++i) {\n     VLOG(3) << absl::StreamFormat("
        },
        {
            "sha": "bca2363e8f6e308315aa4c9f7d365fe48af97175",
            "filename": "third_party/xla/xla/backends/cpu/runtime/reduce_scatter_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Freduce_scatter_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Freduce_scatter_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Freduce_scatter_thunk.cc?ref=503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
            "patch": "@@ -68,9 +68,8 @@ ReduceScatterThunk::Execute(const ExecuteParams& params) {\n \n   VLOG(3) << absl::StreamFormat(\n       \"ReduceScatter: #source_buffers=%d, #destination_buffers=%d, \"\n-      \"reduction_kind=%s\",\n-      data.source.size(), data.destination.size(),\n-      ReductionKindToString(reduction_kind_));\n+      \"reduction_kind=%v\",\n+      data.source.size(), data.destination.size(), reduction_kind_);\n \n   for (int i = 0; i < data.source.size(); ++i) {\n     VLOG(3) << absl::StreamFormat("
        },
        {
            "sha": "c0757f6b959a4e75aac4a6da8e56260f91594d69",
            "filename": "third_party/xla/xla/backends/cpu/runtime/thunk_proto_serdes.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 10,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_proto_serdes.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_proto_serdes.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fthunk_proto_serdes.cc?ref=503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
            "patch": "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n@@ -408,10 +409,7 @@ static absl::Status ToProto(const AllGatherThunk& thunk,\n \n static absl::Status ToProto(const AllReduceThunk& thunk,\n                             AllReduceThunkProto& proto) {\n-  absl::string_view reduction_kind_as_string_view =\n-      ReductionKindToString(thunk.reduction_kind());\n-  std::string reduction_kind_as_string(reduction_kind_as_string_view.begin(),\n-                                       reduction_kind_as_string_view.end());\n+  std::string reduction_kind_as_string = absl::StrCat(thunk.reduction_kind());\n   proto.set_reduction_kind(reduction_kind_as_string);\n   proto.set_single_replica(thunk.single_replica());\n   return absl::OkStatus();\n@@ -425,10 +423,7 @@ static absl::Status ToProto(const AllToAllThunk& thunk,\n \n static absl::Status ToProto(const ReduceScatterThunk& thunk,\n                             ReduceScatterThunkProto& proto) {\n-  absl::string_view reduction_kind_as_string_view =\n-      ReductionKindToString(thunk.reduction_kind());\n-  std::string reduction_kind_as_string(reduction_kind_as_string_view.begin(),\n-                                       reduction_kind_as_string_view.end());\n+  std::string reduction_kind_as_string = absl::StrCat(thunk.reduction_kind());\n   proto.set_reduction_kind(reduction_kind_as_string);\n   return absl::OkStatus();\n }\n@@ -1064,7 +1059,7 @@ static absl::StatusOr<std::unique_ptr<AllReduceThunk>> AllReduceThunkFromProto(\n   const auto& [op_params, op_buffers, op_resources] = collective_thunk_params;\n   TF_ASSIGN_OR_RETURN(\n       ReductionKind reduction_kind,\n-      StringToReductionKind(\n+      ParseReductionKind(\n           proto.collective_thunk().all_reduce_thunk().reduction_kind()));\n \n   return AllReduceThunk::Create(\n@@ -1126,7 +1121,7 @@ ReduceScatterThunkFromProto(\n \n   TF_ASSIGN_OR_RETURN(\n       ReductionKind reduction_kind,\n-      StringToReductionKind(\n+      ParseReductionKind(\n           proto.collective_thunk().reduce_scatter_thunk().reduction_kind()));\n   return ReduceScatterThunk::Create(info, reduction_kind, op_params, op_buffers,\n                                     op_resources);"
        },
        {
            "sha": "6da38eae63927d58ab442f152ec5c26d921f03f4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/copy_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcopy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcopy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcopy_test.cc?ref=503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n "
        },
        {
            "sha": "5922a2f933de4779034b767e9dd9d4f3528c1663",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nccl_communicator.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnccl_communicator.cc?ref=503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
            "patch": "@@ -545,11 +545,11 @@ absl::Status NcclCommunicator::LaunchAllReduce(\n \n   VLOG(3) << absl::StreamFormat(\n       \"[%d] Launch NCCL AllReduce operation; send_buffer=%p; \"\n-      \"recv_buffer=%p; dtype=%s; count=%d; reduction_kind=%s; comm=%p; \"\n+      \"recv_buffer=%p; dtype=%s; count=%d; reduction_kind=%v; comm=%p; \"\n       \"stream=%p\",\n       stream->parent()->device_ordinal(), send_buffer.opaque(),\n       recv_buffer.opaque(), primitive_util::LowercasePrimitiveTypeName(dtype),\n-      count, ReductionKindToString(reduction_kind), comm_, stream);\n+      count, reduction_kind, comm_, stream);\n \n   TF_ASSIGN_OR_RETURN(ncclDataType_t nccl_dtype, ToNcclDataType(dtype, false));\n \n@@ -603,11 +603,11 @@ absl::Status NcclCommunicator::LaunchReduceScatter(\n \n   VLOG(3) << absl::StreamFormat(\n       \"[%d] Launch NCCL ReduceScatter operation; send_buffer=%p; \"\n-      \"recv_buffer=%p; dtype=%s; count=%d; reduction_kind=%s; comm=%p; \"\n+      \"recv_buffer=%p; dtype=%s; count=%d; reduction_kind=%v; comm=%p; \"\n       \"stream=%p\",\n       stream->parent()->device_ordinal(), send_buffer.opaque(),\n       recv_buffer.opaque(), primitive_util::LowercasePrimitiveTypeName(dtype),\n-      count, ReductionKindToString(reduction_kind), comm_, stream);\n+      count, reduction_kind, comm_, stream);\n \n   TF_ASSIGN_OR_RETURN(ncclDataType_t nccl_dtype, ToNcclDataType(dtype, false));\n "
        },
        {
            "sha": "219e50751b80e6d9e0b0e6ed828b64a3bec66c7a",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nvshmem_communicator.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_communicator.cc?ref=503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
            "patch": "@@ -230,13 +230,11 @@ Future<> NvshmemCommunicator::AllReduce(\n   count = ToRealCount(dtype, count);\n   VLOG(3) << absl::StreamFormat(\n       \"Launch NVSHMEM AllReduce operation on device #%d; send_buffer=%p; \"\n-      \"recv_buffer=%p; dtype=%s; count=%d; reduction_kind=%s; comm=node; \"\n-      \"team=%d;\"\n-      \"stream=%p\",\n+      \"recv_buffer=%p; dtype=%s; count=%d; reduction_kind=%v; comm=node; \"\n+      \"team=%d; stream=%p\",\n       nvshmem_team_my_pe(NVSHMEM_TEAM_SHARED), send_buffer.opaque(),\n       recv_buffer.opaque(), primitive_util::LowercasePrimitiveTypeName(dtype),\n-      count, ReductionKindToString(reduction_kind), NVSHMEM_TEAM_SHARED,\n-      stream);\n+      count, reduction_kind, NVSHMEM_TEAM_SHARED, stream);\n \n   switch (dtype) {\n     case PrimitiveType::F64: {"
        },
        {
            "sha": "da5091f2a5997790d1dc0d7ad2bbfb7c15aff9bc",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce.cc?ref=503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
            "patch": "@@ -242,7 +242,7 @@ absl::Status RunAllReduceKernel(\n                      \"of ranks, elements, element type and reduction kind: \",\n                      num_ranks, \", \", num_elements, \", \",\n                      primitive_util::LowercasePrimitiveTypeName(element_type),\n-                     \", \", ReductionKindToString(reduction_kind)));\n+                     \", \", reduction_kind));\n   }\n \n   const auto launch_kernel_impl = [&](auto tag) -> absl::Status {"
        },
        {
            "sha": "30d0d43ddd68b3239252a0f05d370cae41b66e09",
            "filename": "third_party/xla/xla/core/collectives/BUILD",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2FBUILD?ref=503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
            "patch": "@@ -69,10 +69,10 @@ cc_library(\n     hdrs = [\"communicator.h\"],\n     deps = [\n         \":rank_id\",\n+        \":reduction_kind\",\n         \"//xla:future\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/service:collective_ops_utils\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/tsl/concurrency:async_value\",\n@@ -114,3 +114,14 @@ cc_library(\n         \"//xla/tsl/lib/gtl:int_type\",\n     ],\n )\n+\n+cc_library(\n+    name = \"reduction_kind\",\n+    hdrs = [\"reduction_kind.h\"],\n+    deps = [\n+        \"//xla:util\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+    ],\n+)"
        },
        {
            "sha": "b6806f9f189c9d2482d6733c797ad76cd7c60520",
            "filename": "third_party/xla/xla/core/collectives/clique_key.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2Fclique_key.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2Fclique_key.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2Fclique_key.cc?ref=503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
            "patch": "@@ -39,4 +39,5 @@ std::optional<RankId> CliqueKey::rank(GlobalDeviceId id) const {\n   }\n   return std::nullopt;\n }\n+\n }  // namespace xla"
        },
        {
            "sha": "0f60a859db854d460cb5f19f03cb814a4f0206de",
            "filename": "third_party/xla/xla/core/collectives/communicator.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2Fcommunicator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2Fcommunicator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2Fcommunicator.h?ref=503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
            "patch": "@@ -26,8 +26,8 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/types/span.h\"\n #include \"xla/core/collectives/rank_id.h\"\n+#include \"xla/core/collectives/reduction_kind.h\"\n #include \"xla/future.h\"\n-#include \"xla/service/collective_ops_utils.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\""
        },
        {
            "sha": "fa584a975aff5bcda3868207233bc32f70ac19ee",
            "filename": "third_party/xla/xla/core/collectives/reduction_kind.h",
            "status": "added",
            "additions": 63,
            "deletions": 0,
            "changes": 63,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2Freduction_kind.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2Freduction_kind.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcore%2Fcollectives%2Freduction_kind.h?ref=503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
            "patch": "@@ -0,0 +1,63 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_CORE_COLLECTIVES_REDUCTION_KIND_H_\n+#define XLA_CORE_COLLECTIVES_REDUCTION_KIND_H_\n+\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/util.h\"\n+\n+namespace xla {\n+\n+enum class ReductionKind { SUM, PRODUCT, MIN, MAX };\n+\n+template <typename Sink>\n+void AbslStringify(Sink& sink, ReductionKind reduction_kind) {\n+  absl::Format(&sink, \"%s\", [&] {\n+    switch (reduction_kind) {\n+      case ReductionKind::SUM:\n+        return \"sum\";\n+      case ReductionKind::PRODUCT:\n+        return \"prod\";\n+      case ReductionKind::MIN:\n+        return \"min\";\n+      case ReductionKind::MAX:\n+        return \"max\";\n+    }\n+  }());\n+}\n+\n+inline absl::StatusOr<ReductionKind> ParseReductionKind(\n+    absl::string_view reduction_kind) {\n+  if (reduction_kind == \"sum\") {\n+    return ReductionKind::SUM;\n+  }\n+  if (reduction_kind == \"prod\") {\n+    return ReductionKind::PRODUCT;\n+  }\n+  if (reduction_kind == \"min\") {\n+    return ReductionKind::MIN;\n+  }\n+  if (reduction_kind == \"max\") {\n+    return ReductionKind::MAX;\n+  }\n+  return InvalidArgument(\"Invalid reduction kind: %s\", reduction_kind);\n+}\n+\n+}  // namespace xla\n+\n+#endif  // XLA_CORE_COLLECTIVES_REDUCTION_KIND_H_"
        },
        {
            "sha": "2141bf536bb1a671bcecd5cb16ddf714a74968d5",
            "filename": "third_party/xla/xla/pjrt/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD?ref=503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
            "patch": "@@ -218,6 +218,7 @@ xla_test(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n+        \"//xla/backends/gpu:ffi\",\n         \"//xla/ffi\",\n         \"//xla/ffi:ffi_api\",\n         \"//xla/hlo/builder:xla_computation\","
        },
        {
            "sha": "d6283dda89b5feaf7821f2f8f214543278e2fa6a",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc?ref=503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
            "patch": "@@ -53,6 +53,7 @@ limitations under the License.\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n #include \"google/protobuf/text_format.h\"\n+#include \"xla/backends/gpu/ffi.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/ffi/ffi.h\"\n #include \"xla/ffi/ffi_api.h\""
        },
        {
            "sha": "7f75ac4fce2b09c95708f2e68227b0b74ddb7b0b",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
            "patch": "@@ -5226,6 +5226,7 @@ cc_library(\n         \"//xla:status_macros\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/core/collectives:reduction_kind\",\n         \"//xla/hlo/ir:collective_op_group_mode\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/runtime:device_id\","
        },
        {
            "sha": "d7b108c3d2c095d0d216cb228cf6f257f51278ad",
            "filename": "third_party/xla/xla/service/collective_ops_utils.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.cc?ref=503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"absl/strings/str_join.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/core/collectives/reduction_kind.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -52,20 +53,6 @@ limitations under the License.\n namespace xla {\n using CycleType = collective_permute_cycle::CycleType;\n \n-absl::StatusOr<ReductionKind> StringToReductionKind(\n-    absl::string_view reduction_kind) {\n-  if (reduction_kind == \"sum\") {\n-    return ReductionKind::SUM;\n-  } else if (reduction_kind == \"prod\") {\n-    return ReductionKind::PRODUCT;\n-  } else if (reduction_kind == \"min\") {\n-    return ReductionKind::MIN;\n-  } else if (reduction_kind == \"max\") {\n-    return ReductionKind::MAX;\n-  }\n-  return InvalidArgument(\"Invalid reduction kind: %s\", reduction_kind);\n-}\n-\n // Match the instruction to a reduction kind. We can represent and/or of pred as\n // min/max. This works because pred is stored as an 8-bit int of value 0 or 1.\n std::optional<ReductionKind> MatchReductionInstruction("
        },
        {
            "sha": "617a7a8bbb8ebc9fe4fe9ca86e0dd3288ccc3e26",
            "filename": "third_party/xla/xla/service/collective_ops_utils.h",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcollective_ops_utils.h?ref=503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
            "patch": "@@ -18,24 +18,21 @@ limitations under the License.\n \n #include <cstdint>\n #include <optional>\n-#include <set>\n #include <string>\n #include <utility>\n #include <vector>\n \n-#include \"absl/functional/function_ref.h\"\n #include \"absl/log/log.h\"\n-#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/str_join.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/core/collectives/reduction_kind.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/hlo/ir/collective_op_group_mode.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/replica_group.h\"\n #include \"xla/literal.h\"\n@@ -44,26 +41,9 @@ limitations under the License.\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/pattern_matcher.h\"\n #include \"xla/service/source_target_pairs.h\"\n-#include \"xla/stream_executor/device_memory.h\"\n \n namespace xla {\n \n-enum class ReductionKind { SUM, PRODUCT, MIN, MAX };\n-\n-constexpr absl::string_view ReductionKindToString(\n-    ReductionKind reduction_kind) {\n-  switch (reduction_kind) {\n-    case ReductionKind::SUM:\n-      return \"sum\";\n-    case ReductionKind::PRODUCT:\n-      return \"prod\";\n-    case ReductionKind::MIN:\n-      return \"min\";\n-    case ReductionKind::MAX:\n-      return \"max\";\n-  }\n-}\n-\n absl::StatusOr<ReductionKind> StringToReductionKind(\n     absl::string_view reduction_kind);\n "
        },
        {
            "sha": "61d9ef774f6fab4e9fb4041ddb46e77c23312607",
            "filename": "third_party/xla/xla/service/reduce_scatter_reassociate.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fservice%2Freduce_scatter_reassociate.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/503d741bdb52b5bd3c931bbb865703bb4efc0a9e/third_party%2Fxla%2Fxla%2Fservice%2Freduce_scatter_reassociate.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Freduce_scatter_reassociate.cc?ref=503d741bdb52b5bd3c931bbb865703bb4efc0a9e",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n \n #include <optional>\n \n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -40,8 +41,8 @@ namespace {\n //\n // Note: AllReduceKey supports ReduceScatter as well.\n \n-bool AreCompatible(const HloReduceScatterInstruction *rs0,\n-                   const HloReduceScatterInstruction *rs1,\n+bool AreCompatible(const HloReduceScatterInstruction* rs0,\n+                   const HloReduceScatterInstruction* rs1,\n                    ReductionKind op_kind) {\n   std::optional<AllReduceKey> key0 = GetAllReduceKey(rs0);\n   std::optional<AllReduceKey> key1 = GetAllReduceKey(rs1);\n@@ -68,16 +69,16 @@ absl::StatusOr<bool> ReduceScatterReassociate::RunImpl(\n \n   bool changed = false;\n   for (auto computation : module->computations(execution_threads)) {\n-    for (HloInstruction *inst : computation->MakeInstructionPostOrder()) {\n+    for (HloInstruction* inst : computation->MakeInstructionPostOrder()) {\n       std::optional<ReductionKind> kind = MatchReductionInstruction(inst);\n       if (!kind || inst->operand(0)->opcode() != HloOpcode::kReduceScatter ||\n           inst->operand(1)->opcode() != HloOpcode::kReduceScatter ||\n           !inst->shape().IsArray()) {\n         continue;\n       }\n \n-      auto *rs0 = Cast<HloReduceScatterInstruction>(inst->mutable_operand(0));\n-      auto *rs1 = Cast<HloReduceScatterInstruction>(inst->mutable_operand(1));\n+      auto* rs0 = Cast<HloReduceScatterInstruction>(inst->mutable_operand(0));\n+      auto* rs1 = Cast<HloReduceScatterInstruction>(inst->mutable_operand(1));\n       if (!AreCompatible(rs0, rs1, *kind)) {\n         VLOG(2) << \"Reduce-Scatter operations are not compatible, skipping\";\n         continue;\n@@ -97,11 +98,11 @@ absl::StatusOr<bool> ReduceScatterReassociate::RunImpl(\n       }\n \n       // Found pattern op(rs(x), rs(y)). Transform it into rs(op(x,y)).\n-      HloInstruction *new_op =\n+      HloInstruction* new_op =\n           computation->AddInstruction(inst->CloneWithNewOperands(\n               rs0->mutable_operand(0)->shape(),\n               {rs0->mutable_operand(0), rs1->mutable_operand(0)}));\n-      HloInstruction *new_rs = computation->AddInstruction(\n+      HloInstruction* new_rs = computation->AddInstruction(\n           rs0->CloneWithNewOperands(inst->shape(), {new_op}));\n       // In case only one of the two instructions had a scheduling annotation,\n       // delete the potential annotation."
        }
    ],
    "stats": {
        "total": 180,
        "additions": 110,
        "deletions": 70
    }
}