{
    "author": "sergachev",
    "message": "PR #32003: [GPU][NFC] Merge methods querying fusion kind.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32003\n\nCopybara import of the project:\n\n--\n2a3ad034522e871edc9c7f580e86fc3980025542 by Ilia Sergachev <isergachev@nvidia.com>:\n\n[GPU][NFC] Merge methods querying fusion kind.\n\n--\nebeb25599d6017d34ea92ece415a255d109af049 by Ilia Sergachev <isergachev@nvidia.com>:\n\nAddress review requests.\n\nMerging this change closes #32003\n\nPiperOrigin-RevId: 819692807",
    "sha": "2408b9968eaa2ce2c0db63718e14f377639980cf",
    "files": [
        {
            "sha": "9f995e1a0801b1f35ae62825c88ddb2b105ec9c1",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=2408b9968eaa2ce2c0db63718e14f377639980cf",
            "patch": "@@ -437,6 +437,7 @@ cc_library(\n         \"//xla/service:hlo_proto_cc\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:cublas_cudnn\",\n+        \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/gpu/transforms:custom_kernel_fusion_rewriter\","
        },
        {
            "sha": "27f0458795784b2157a40b6682b6f4b13411c2e2",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc?ref=2408b9968eaa2ce2c0db63718e14f377639980cf",
            "patch": "@@ -39,6 +39,7 @@ limitations under the License.\n #include \"xla/service/call_inliner.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/cublas_cudnn.h\"\n+#include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/gpu/transforms/custom_kernel_fusion_rewriter.h\"\n@@ -199,12 +200,7 @@ bool IsCustomKernel(const HloComputation* computation) {\n     return false;\n   }\n \n-  if (!gpu_backend_config->has_fusion_backend_config()) {\n-    return false;\n-  }\n-\n-  return gpu_backend_config->fusion_backend_config().kind() ==\n-         kCustomFusionKind;\n+  return IsGpuFusionKind(*instruction, kCustomFusionKind);\n }\n \n absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>"
        },
        {
            "sha": "06ec0bd2ee0e772a4a6d11b9efb02cfa9496dd9c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=2408b9968eaa2ce2c0db63718e14f377639980cf",
            "patch": "@@ -1030,6 +1030,7 @@ cc_library(\n         \"//xla/service:algorithm_util\",\n         \"//xla/service:instruction_fusion\",\n         \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\","
        },
        {
            "sha": "f828bbcecde2f63027d43f1c12ccd839b28c9d04",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc?ref=2408b9968eaa2ce2c0db63718e14f377639980cf",
            "patch": "@@ -36,6 +36,7 @@ limitations under the License.\n #include \"xla/primitive_util.h\"\n #include \"xla/service/algorithm_util.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n@@ -290,18 +291,11 @@ CodegenDecision CanTritonHandleReduce(\n }\n \n bool IsInTritonNestedGemmFusion(const HloInstruction& hlo) {\n-  const HloComputation* computation = hlo.parent();\n-  if (!computation->IsFusionComputation()) {\n+  if (!hlo.parent()->IsFusionComputation()) {\n     return false;\n   }\n-  absl::StatusOr<GpuBackendConfig> backend_config =\n-      computation->FusionInstruction()->backend_config<GpuBackendConfig>();\n-  if (!backend_config.ok()) {\n-    return false;\n-  }\n-  absl::string_view fusion_kind =\n-      backend_config.value().fusion_backend_config().kind();\n-  return fusion_kind == kTritonNestedGemmFusionKind;\n+  return IsGpuFusionKind(*hlo.parent()->FusionInstruction(),\n+                         kTritonNestedGemmFusionKind);\n }\n \n absl::Status CheckSupportedCheckDotDimensions(const HloDotInstruction& dot) {"
        },
        {
            "sha": "85fc576c2550091bc86ee7829189a0ee8b070f91",
            "filename": "third_party/xla/xla/codegen/tiling/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2FBUILD?ref=2408b9968eaa2ce2c0db63718e14f377639980cf",
            "patch": "@@ -354,6 +354,7 @@ cc_library(\n         \"//xla/service:instruction_fusion\",\n         \"//xla/service:name_uniquer\",\n         \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/tsl/platform:errors\","
        },
        {
            "sha": "ffc4ffe5cf2633881b2503f14d97ea5d8842b572",
            "filename": "third_party/xla/xla/codegen/tiling/symbolic_tile_analysis.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Ftiling%2Fsymbolic_tile_analysis.cc?ref=2408b9968eaa2ce2c0db63718e14f377639980cf",
            "patch": "@@ -68,6 +68,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n #include \"xla/service/instruction_fusion.h\"\n@@ -430,12 +431,8 @@ class OrderedUniquePtrValueHashSet {\n bool IsWithinNestedGemmFusion(const HloInstruction* hlo) {\n   const HloComputation* computation = hlo->parent();\n   if (computation->IsFusionComputation()) {\n-    const gpu::GpuBackendConfig backend_config =\n-        *computation->FusionInstruction()\n-             ->backend_config<gpu::GpuBackendConfig>();\n-    absl::string_view fusion_kind =\n-        backend_config.fusion_backend_config().kind();\n-    return fusion_kind == gpu::kTritonNestedGemmFusionKind;\n+    return gpu::IsGpuFusionKind(*computation->FusionInstruction(),\n+                                gpu::kTritonNestedGemmFusionKind);\n   }\n \n   return false;\n@@ -760,7 +757,6 @@ llvm::SmallVector<const TiledHloInstruction*> MapToTiledInstructions(\n \n }  // anonymous namespace\n \n-\n // Extracts `HloInstruction`s from a span of `HloInstructionAdaptor`s.\n absl::InlinedVector<const HloInstruction*, 2> ToInstructions(\n     absl::Span<const HloInstructionAdaptor> instruction_adaptors) {"
        },
        {
            "sha": "10bb02c41a056039d2c983a486d5694eee3b2043",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=2408b9968eaa2ce2c0db63718e14f377639980cf",
            "patch": "@@ -2439,6 +2439,7 @@ cc_library(\n         \"//xla/service:legalize_scheduling_annotations\",\n         \"//xla/service:p2p_schedule_preparation\",\n         \"//xla/service:profile_guided_latency_estimator\",\n+        \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu/model:analytical_latency_estimator\",\n         \"//xla/service/gpu/model:gpu_hlo_cost_analysis\",\n         \"//xla/service/gpu/model:sol_latency_estimator\","
        },
        {
            "sha": "b05d5a0caf68c941099412eea5e154e080941d26",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=2408b9968eaa2ce2c0db63718e14f377639980cf",
            "patch": "@@ -55,6 +55,7 @@ cc_library(\n         \"//xla/service:executable\",\n         \"//xla/service:shaped_buffer\",\n         \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/service/gpu:stream_executor_util\",\n@@ -154,6 +155,7 @@ cc_library(\n         \"//xla/service:shaped_buffer\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_float_support\",\n+        \"//xla/service/gpu:hlo_fusion_analysis\",\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/service/gpu:split_k_gemm_rewriter\","
        },
        {
            "sha": "cbeb05910eb15d0ac9d570c0afe0c2676378a9b4",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 19,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=2408b9968eaa2ce2c0db63718e14f377639980cf",
            "patch": "@@ -68,6 +68,7 @@ limitations under the License.\n #include \"xla/service/gpu/autotuning/redzone_buffers.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_float_support.h\"\n+#include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/kernels/custom_kernel.h\"\n #include \"xla/service/gpu/kernels/custom_kernel_fusion.h\"\n@@ -563,14 +564,7 @@ bool IsScaledDotFusion(const HloInstruction* fusion_instr) {\n   if (fusion_instr->fusion_kind() != HloInstruction::FusionKind::kCustom) {\n     return false;\n   }\n-  auto config = fusion_instr->backend_config<GpuBackendConfig>();\n-  if (!config.ok()) {\n-    return false;\n-  }\n-  if (config->fusion_backend_config().kind() != kTritonScaledDotFusionKind) {\n-    return false;\n-  }\n-  return true;\n+  return IsGpuFusionKind(*fusion_instr, kTritonScaledDotFusionKind);\n }\n \n absl::Status RewriteGemmFusionToCall(HloInstruction* fusion_instr) {\n@@ -765,15 +759,6 @@ absl::Status GemmFusionAutotunerRewriterVisitor::HandleFusion(\n   return absl::OkStatus();\n }\n \n-bool GemmFusionAutotunerImpl::IsFusionKind(const HloInstruction& hlo,\n-                                           absl::string_view kind) {\n-  auto gpu_config = hlo.backend_config<GpuBackendConfig>();\n-  if (!gpu_config.ok()) {\n-    return false;\n-  }\n-  return gpu_config->fusion_backend_config().kind() == kind;\n-}\n-\n // Methods required for sorting the configs.\n bool GemmFusionAutotunerImpl::CuBlasConfig::operator<(\n     const CuBlasConfig& other) const {\n@@ -909,8 +894,8 @@ GemmFusionAutotunerImpl::GenerateDotConfigs(const HloFusionInstruction& fusion,\n   // Add CustomKernelFusion (Cutlass) configs, if available.\n   // Go through all the instructions in the fusion body try to match them to\n   // a custom kernel fusion pattern.\n-  if ((IsFusionKind(fusion, kCustomFusionKind) ||\n-       IsFusionKind(fusion, kTritonGemmFusionKind)) &&\n+  if ((IsGpuFusionKind(fusion, kCustomFusionKind) ||\n+       IsGpuFusionKind(fusion, kTritonGemmFusionKind)) &&\n       IsAutotuningEnabled() && !config_.IsDeviceless()) {\n     std::vector<BackendConfig> custom_kernel_fusion_configs =\n         GenerateCustomKernelFusionConfigs(fusion,"
        },
        {
            "sha": "50fa6b76ce8d1e9ead50eaa3f4b951843bf0b226",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.h",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.h?ref=2408b9968eaa2ce2c0db63718e14f377639980cf",
            "patch": "@@ -206,8 +206,6 @@ class GemmFusionAutotunerImpl {\n         GetComputeCapability());\n   }\n \n-  bool IsFusionKind(const HloInstruction& hlo, absl::string_view kind);\n-\n   bool AddLibConfigs(const HloFusionInstruction& fusion,\n                      const HloDotInstruction* dot,\n                      std::vector<BackendConfig>& configs);"
        },
        {
            "sha": "2dac39160ef4b927b1c7e348f7b71ad2d59f0fde",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_cuda.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_cuda.cc?ref=2408b9968eaa2ce2c0db63718e14f377639980cf",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"xla/service/gpu/autotuning/gemm_fusion_autotuner.h\"\n #include \"xla/service/gpu/autotuning/triton_configs.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n@@ -59,8 +60,8 @@ bool GemmFusionAutotunerImpl::AddLibConfigs(\n         debug_options_.xla_gpu_cudnn_gemm_fusion_level() > 1) ||\n        (cc.IsAtLeastBlackwell() &&\n         debug_options_.xla_gpu_cudnn_gemm_fusion_level() > 0));\n-  if ((IsFusionKind(fusion, kCuDnnFusionKind) && IsAutotuningEnabled()) ||\n-      (IsFusionKind(fusion, kTritonGemmFusionKind) && is_cudnn_enabled &&\n+  if ((IsGpuFusionKind(fusion, kCuDnnFusionKind) && IsAutotuningEnabled()) ||\n+      (IsGpuFusionKind(fusion, kTritonGemmFusionKind) && is_cudnn_enabled &&\n        algorithm_util::IsSupportedByCudnn(\n            dot->precision_config().algorithm()) &&\n        IsAutotuningEnabled())) {\n@@ -69,7 +70,7 @@ bool GemmFusionAutotunerImpl::AddLibConfigs(\n       configs.push_back(CuDnnConfig{plan_id});\n     }\n   }\n-  if (IsFusionKind(fusion, kCuDnnFusionKind)) {\n+  if (IsGpuFusionKind(fusion, kCuDnnFusionKind)) {\n     if (!IsAutotuningEnabled()) {\n       configs.push_back(CuDnnConfig{-1});\n     }"
        },
        {
            "sha": "6716d81f07c28a79f7be2d197c354b82b01ab909",
            "filename": "third_party/xla/xla/service/gpu/gpu_hlo_schedule.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_hlo_schedule.cc?ref=2408b9968eaa2ce2c0db63718e14f377639980cf",
            "patch": "@@ -54,6 +54,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/flag_utils.h\"\n #include \"xla/service/gpu/gpu_latency_hiding_scheduler.h\"\n+#include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/model/analytical_latency_estimator.h\"\n #include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n@@ -547,12 +548,8 @@ LegalizeSchedulingAnnotations::Config SchedulingAnnotationsConfig() {\n     if (hlo->IsCustomCall(\"__cublas$gemm\")) {\n       return true;\n     }\n-    if (hlo->opcode() == HloOpcode::kFusion && hlo->has_backend_config() &&\n-        hlo->backend_config<GpuBackendConfig>().ok()) {\n-      GpuBackendConfig gpu_config =\n-          hlo->backend_config<GpuBackendConfig>().value();\n-      return gpu_config.has_fusion_backend_config() &&\n-             gpu_config.fusion_backend_config().kind() == kTritonGemmFusionKind;\n+    if (hlo->opcode() == HloOpcode::kFusion) {\n+      return IsGpuFusionKind(*hlo, kTritonGemmFusionKind);\n     }\n     return false;\n   };"
        },
        {
            "sha": "85368317fa137d22f61b707b7208f1e8978c2d9a",
            "filename": "third_party/xla/xla/service/gpu/hlo_fusion_analysis.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.cc?ref=2408b9968eaa2ce2c0db63718e14f377639980cf",
            "patch": "@@ -204,6 +204,14 @@ int SmallestBitWidth(const Container& args) {\n \n }  // namespace\n \n+bool IsGpuFusionKind(const HloInstruction& hlo, absl::string_view kind) {\n+  auto gpu_config = hlo.backend_config<GpuBackendConfig>();\n+  if (!gpu_config.ok()) {\n+    return false;\n+  }\n+  return gpu_config->fusion_backend_config().kind() == kind;\n+}\n+\n HloFusionAnalysis::HloFusionAnalysis(\n     FusionBackendConfig fusion_backend_config, HloFusionSpec fusion_spec,\n     EmitterFusionKind emitter_fusion_kind,"
        },
        {
            "sha": "c5b8980abdd5ab14ed76551b9ff2fbaf03e1071e",
            "filename": "third_party/xla/xla/service/gpu/hlo_fusion_analysis.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.h?ref=2408b9968eaa2ce2c0db63718e14f377639980cf",
            "patch": "@@ -32,6 +32,10 @@ limitations under the License.\n namespace xla {\n namespace gpu {\n \n+// Returns true if the instruction's fusion backend config kind matches the\n+// given one.\n+bool IsGpuFusionKind(const HloInstruction& hlo, absl::string_view kind);\n+\n class HloFusionAnalysis {\n  public:\n   // The type of emitted fusion."
        },
        {
            "sha": "b02f4ffaee992eaa191ff0dc5073625cf12506c5",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc?ref=2408b9968eaa2ce2c0db63718e14f377639980cf",
            "patch": "@@ -557,14 +557,6 @@ bool IsDynamicSliceFusion(const HloInstruction* instr) {\n          name == kDynamicSliceFusionWithDynamicAddressComputationConfigName;\n }\n \n-bool IsDynamicMemcpyFusion(const HloInstruction* instr) {\n-  absl::StatusOr<GpuBackendConfig> backend_config =\n-      instr->backend_config<GpuBackendConfig>();\n-  return backend_config.ok() &&\n-         backend_config->fusion_backend_config().kind() ==\n-             kDynamicMemcpyFusionKind;\n-}\n-\n namespace {\n \n // Whether the instruction is semantically a call."
        },
        {
            "sha": "4b45907d03809ca67539bfecd1348be421a77f49",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils.h",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.h?ref=2408b9968eaa2ce2c0db63718e14f377639980cf",
            "patch": "@@ -140,11 +140,6 @@ std::optional<std::string> GetCustomFusionConfigName(\n // fusion. This is determined by checking the name of custom fusion config.\n bool IsDynamicSliceFusion(const HloInstruction* instr);\n \n-// Returns true if the given instruction is a dynamic memcpy fusion. This\n-// function only checks the fusion kind, which is populated by the\n-// FusionDispatch pipeline.\n-bool IsDynamicMemcpyFusion(const HloInstruction* instr);\n-\n // Returns true if `hlo` will be implemented as a call to a cuSolver routine.\n //\n // This returns true if `hlo` is a CustomCall HLO with a call target equal to"
        },
        {
            "sha": "672b124875394d3d1f0f321c61d91375e0e77375",
            "filename": "third_party/xla/xla/service/gpu/transforms/command_buffer_scheduling.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2408b9968eaa2ce2c0db63718e14f377639980cf/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc?ref=2408b9968eaa2ce2c0db63718e14f377639980cf",
            "patch": "@@ -116,7 +116,8 @@ static bool AsyncStartOrDoneCommandIsSupported(\n \n   if (hlo->async_wrapped_opcode() == HloOpcode::kFusion) {\n     // We don't currently support dynamic memcpy fusions in command buffers.\n-    if (IsDynamicMemcpyFusion(hlo->async_wrapped_instruction())) {\n+    if (IsGpuFusionKind(*hlo->async_wrapped_instruction(),\n+                        kDynamicMemcpyFusionKind)) {\n       return config.enabled_commands.contains(\n           DebugOptions::DYNAMIC_SLICE_COPY_FUSION);\n     }\n@@ -269,7 +270,7 @@ static bool IsCommand(const HloInstruction* hlo,\n     if (backend_config.kind() == kCuDnnFusionKind) {\n       return config.enabled_commands.contains(DebugOptions::CUDNN);\n     }\n-    if (IsDynamicMemcpyFusion(fusion)) {\n+    if (IsGpuFusionKind(*fusion, kDynamicMemcpyFusionKind)) {\n       return config.enabled_commands.contains(\n           DebugOptions::DYNAMIC_SLICE_COPY_FUSION);\n     }\n@@ -811,8 +812,7 @@ absl::StatusOr<bool> CommandBufferScheduling::Run(\n     commands.insert(static_cast<DebugOptions::CommandBufferCmdType>(cmd_type));\n   }\n \n-  CommandBufferConfig config{std::move(commands),\n-                             device_description_};\n+  CommandBufferConfig config{std::move(commands), device_description_};\n \n   // Erase command buffer cmd types that are not supported by the gpu runtime.\n   static constexpr auto kRequireConditionals = {DebugOptions::CONDITIONAL,"
        }
    ],
    "stats": {
        "total": 112,
        "additions": 42,
        "deletions": 70
    }
}