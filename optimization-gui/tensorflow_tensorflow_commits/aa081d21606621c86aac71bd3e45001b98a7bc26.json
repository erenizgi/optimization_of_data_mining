{
    "author": "pifon2a",
    "message": "[XLA:GPU] Do not create an additional LLVM module for Triton.\n\nRight now, we create a local LLVM module in which we link the LLVM module\nresulting from the MLIR->LLVM translation. We don't need it.\n\nAlso I changed the logic for linking. Now we don't initialize llvm::Linker for\nevery linked kernel.\n\nPiperOrigin-RevId: 839924141",
    "sha": "aa081d21606621c86aac71bd3e45001b98a7bc26",
    "files": [
        {
            "sha": "eccb5e80ff5195194283a66146304e2cbfb2da11",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 14,
            "deletions": 18,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=aa081d21606621c86aac71bd3e45001b98a7bc26",
            "patch": "@@ -35,7 +35,6 @@ cc_library(\n         \"//xla/service/gpu:__subpackages__\",\n     ],\n     deps = [\n-        \":fusion_emitter\",\n         \":xtile_compiler\",\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n@@ -67,6 +66,7 @@ cc_library(\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//llvm:TargetParser\",\n         \"@llvm-project//llvm:ir_headers\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Support\",\n@@ -86,6 +86,7 @@ xla_cc_test(\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/service/gpu:hlo_fusion_analysis\",\n+        \"//xla/service/gpu:target_constants\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/tests:xla_internal_test_main\",\n@@ -330,6 +331,7 @@ cc_library(\n         \"//xla/codegen/tiling:tiled_hlo_fusion_instruction\",\n         \"//xla/codegen/tiling:tiled_hlo_instruction\",\n         \"//xla/codegen/tiling:tiled_hlo_schedule\",\n+        \"//xla/codegen/tiling:tiling_specification\",\n         \"//xla/codegen/xtile/ir:xtile\",\n         \"//xla/codegen/xtile/ir/transforms:passes\",\n         \"//xla/hlo/analysis:symbolic_expr\",\n@@ -474,33 +476,21 @@ cc_library(\n     ],\n     visibility = [\"//visibility:private\"],\n     deps = [\n-        \":emitter_helpers\",\n-        \":support\",\n         \"//xla:autotuning_proto_cc\",\n-        \"//xla:util\",\n         \"//xla/codegen/tiling:symbolic_tile_analysis\",\n-        \"//xla/codegen/tiling:tiled_hlo_computation\",\n-        \"//xla/codegen/tiling:tiled_hlo_instruction\",\n-        \"//xla/codegen/xtile/ir:xtile\",\n-        \"//xla/hlo/analysis:symbolic_expr\",\n+        \"//xla/codegen/tiling:tiling_specification\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service:hlo_module_config\",\n-        \"//xla/service/gpu:launch_dimensions\",\n-        \"//xla/service/gpu:matmul_utils\",\n-        \"//xla/service/gpu:triton_fusion_analysis\",\n         \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n-        \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n-        \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//llvm:TargetParser\",\n         \"@llvm-project//llvm:ir_headers\",\n-        \"@llvm-project//mlir:FunctionInterfaces\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Pass\",\n         \"@triton//:TritonDialects\",\n@@ -512,12 +502,9 @@ xla_cc_test(\n     srcs = [\"xtile_compiler_stub_test.cc\"],\n     deps = [\n         \":xtile_compiler_stub_for_testing\",\n-        \"//xla:literal\",\n         \"//xla:literal_util\",\n         \"//xla/codegen/tiling:tiled_hlo_instruction\",\n-        \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service:hlo_module_config\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@llvm-project//mlir:IR\",\n@@ -537,6 +524,7 @@ xla_cc_test(\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/hlo/testlib:verified_hlo_module\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu:target_constants\",\n         \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n@@ -546,6 +534,7 @@ xla_cc_test(\n         \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest\",\n+        \"@llvm-project//llvm:TargetParser\",\n         \"@llvm-project//llvm:ir_headers\",\n         \"@llvm-project//mlir:IR\",\n     ],\n@@ -580,6 +569,7 @@ xla_test(\n         \"//xla/service:pattern_matcher\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu:target_constants\",\n         \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/service/gpu/tests:gpu_codegen_test\",\n         \"//xla/service/gpu/transforms:nest_gemm_fusion\",\n@@ -596,6 +586,7 @@ xla_test(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest\",\n+        \"@llvm-project//llvm:TargetParser\",\n         \"@llvm-project//llvm:ir_headers\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Pass\",\n@@ -746,6 +737,7 @@ xla_test(\n         \"//xla/service:algorithm_util\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/service/gpu:target_constants\",\n         \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/service/gpu/tests:gpu_codegen_test\",\n         \"//xla/stream_executor:device_description\",\n@@ -764,6 +756,7 @@ xla_test(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest\",\n         \"@eigen_archive//:eigen3\",\n+        \"@llvm-project//llvm:TargetParser\",\n         \"@llvm-project//llvm:ir_headers\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Pass\",\n@@ -795,6 +788,7 @@ cc_library(\n         \"//xla/service/gpu:gpu_float_support\",\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:matmul_utils\",\n+        \"//xla/service/gpu:target_constants\",\n         \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/service/gpu/model:triton_emitter_constraints\",\n         \"//xla/stream_executor:device_description\",\n@@ -812,6 +806,7 @@ cc_library(\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_for_library\",\n         \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//llvm:TargetParser\",\n         \"@llvm-project//llvm:ir_headers\",\n         \"@llvm-project//mlir:IR\",\n         \"@local_tsl//tsl/platform:protobuf\",\n@@ -952,6 +947,7 @@ xla_cc_test(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/service/gpu:ir_emission_utils\",\n+        \"//xla/service/gpu:target_constants\",\n         \"//xla/service/gpu/model:block_level_parameters\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\","
        },
        {
            "sha": "5191a7ef22b4383737b535e21921ae45b076264b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc?ref=aa081d21606621c86aac71bd3e45001b98a7bc26",
            "patch": "@@ -76,10 +76,13 @@ struct ModuleWithEmitter : public ModuleWithFusion {\n   std::optional<HloFusionAnalysis> analysis;\n   std::unique_ptr<TritonFusion> emitter;\n   llvm::LLVMContext llvm_context;\n-  llvm::Module llvm_module{\"test_module\", llvm_context};\n+  llvm::Triple target_triple;\n+  std::string data_layout;\n \n   explicit ModuleWithEmitter(std::unique_ptr<HloModule> module_arg)\n-      : ModuleWithFusion{std::move(module_arg)} {}\n+      : ModuleWithFusion{std::move(module_arg)},\n+        target_triple(\"\"),\n+        data_layout(\"\") {}\n };\n \n class CollectiveBlockLevelConfigTest : public HloHardwareIndependentTestBase {\n@@ -276,7 +279,8 @@ TEST_P(CollectiveEmitterParameterizedTest,\n       TritonWrapperResult triton_kernel,\n       triton_fusion->GenerateTritonKernelAndWrapper(\n           *result->FusionInstr(), \"test-all-reduce-start\", device_info_,\n-          &result->llvm_module, &result->mlir_context));\n+          result->target_triple, result->data_layout, &result->llvm_context,\n+          &result->mlir_context));\n }\n \n INSTANTIATE_TEST_SUITE_P("
        },
        {
            "sha": "2caf28722c47e83cb310c33ef19e6d98bbc616be",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=aa081d21606621c86aac71bd3e45001b98a7bc26",
            "patch": "@@ -29,17 +29,17 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/STLExtras.h\"\n-#include \"llvm/IR/Constants.h\"\n #include \"llvm/IR/DerivedTypes.h\"\n #include \"llvm/IR/Function.h\"\n #include \"llvm/IR/IRBuilder.h\"\n #include \"llvm/IR/Metadata.h\"\n #include \"llvm/IR/Module.h\"\n #include \"llvm/Support/Casting.h\"\n+#include \"llvm/TargetParser/Triple.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n-#include \"xla/backends/gpu/codegen/triton/fusion_emitter.h\"\n+#include \"xla/backends/gpu/codegen/triton/xtile_compiler.h\"\n #include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n@@ -92,7 +92,8 @@ static void PopulateNvvmAnnotations(\n absl::StatusOr<TritonWrapperResult>\n TritonFusion::GenerateTritonKernelAndWrapper(\n     const HloFusionInstruction& fusion, absl::string_view impl_fn_name,\n-    const se::DeviceDescription& device_info, llvm::Module* llvm_module,\n+    const se::DeviceDescription& device_info, const llvm::Triple& target_triple,\n+    const std::string& data_layout, llvm::LLVMContext* llvm_context,\n     mlir::MLIRContext* mlir_context) const {\n   const se::GpuComputeCapability& cc = device_info.gpu_compute_capability();\n \n@@ -105,7 +106,7 @@ TritonFusion::GenerateTritonKernelAndWrapper(\n       impl_fn_name, &fusion, cc, device_info,\n       BlockLevelParameters::FromBlockLevelFusionConfig(\n           analysis_.fusion_backend_config().block_level_fusion_config()),\n-      llvm_module, *mlir_context);\n+      target_triple, data_layout, *llvm_context, *mlir_context);\n };\n \n absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n@@ -124,9 +125,7 @@ absl::StatusOr<TritonFusion::EmitResult> TritonFusion::Emit(\n     const HloInstruction* instr_override,\n     absl::Span<const Shape> unmanaged_arguments) const {\n   std::string suggested_kernel_name = std::string(fusion.name());\n-  auto local_module =\n-      ir_emitter_context.CreateLLVMModule(suggested_kernel_name);\n-  llvm::IRBuilder builder(local_module->getContext());\n+  llvm::IRBuilder builder(*ir_emitter_context.llvm_context());\n   VLOG(3) << fusion.ToString();\n   TF_ASSIGN_OR_RETURN(\n       auto kernel_arguments,\n@@ -139,6 +138,7 @@ absl::StatusOr<TritonFusion::EmitResult> TritonFusion::Emit(\n       fusion.fused_instructions_computation();\n   VLOG(3) << \"hlo_computation: \" << hlo_computation->ToString();\n \n+  std::unique_ptr<llvm::Module> local_module;\n   auto generate = [&]() -> absl::StatusOr<KernelReuseCache::Entry> {\n     VLOG(3) << \"Generating: \" << suggested_kernel_name;\n \n@@ -149,7 +149,10 @@ absl::StatusOr<TritonFusion::EmitResult> TritonFusion::Emit(\n         TritonWrapperResult triton_wrapper_result,\n         GenerateTritonKernelAndWrapper(\n             fusion, sanitized_kernel_name, ir_emitter_context.gpu_device_info(),\n-            local_module.get(), ir_emitter_context.mlir_context()));\n+            ir_emitter_context.target_triple(),\n+            ir_emitter_context.data_layout(), ir_emitter_context.llvm_context(),\n+            ir_emitter_context.mlir_context()));\n+    local_module = std::move(triton_wrapper_result.llvm_module);\n \n     auto backend_config =\n         fusion.backend_config<GpuBackendConfig>()->fusion_backend_config();"
        },
        {
            "sha": "42a4861368744b6445e7a6aa7bbfe2081523ef72",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.h",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.h?ref=aa081d21606621c86aac71bd3e45001b98a7bc26",
            "patch": "@@ -17,12 +17,13 @@ limitations under the License.\n \n #include <memory>\n #include <optional>\n-#include <utility>\n \n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/IR/Module.h\"\n+#include \"llvm/TargetParser/Triple.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n #include \"xla/backends/gpu/codegen/triton/xtile_compiler.h\"\n #include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n@@ -31,6 +32,7 @@ limitations under the License.\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emitter_context.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n@@ -98,8 +100,9 @@ class TritonFusion : public FusionInterface {\n   // generated kernel.\n   absl::StatusOr<TritonWrapperResult> GenerateTritonKernelAndWrapper(\n       const HloFusionInstruction& fusion, absl::string_view impl_fn_name,\n-      const se::DeviceDescription& device_info, llvm::Module* llvm_module,\n-      mlir::MLIRContext* mlir_context) const;\n+      const se::DeviceDescription& device_info,\n+      const llvm::Triple& target_triple, const std::string& data_layout,\n+      llvm::LLVMContext* llvm_context, mlir::MLIRContext* mlir_context) const;\n \n  private:\n   const HloFusionAnalysis& analysis_;"
        },
        {
            "sha": "07605fb6d3498704077f20bdfd334e93217cc4fd",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 8,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=aa081d21606621c86aac71bd3e45001b98a7bc26",
            "patch": "@@ -35,6 +35,7 @@ limitations under the License.\n #include \"absl/strings/substitute.h\"\n #include \"Eigen/Core\"\n #include \"llvm/IR/LLVMContext.h\"\n+#include \"llvm/TargetParser/Triple.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"xla/autotuning.pb.h\"\n@@ -55,6 +56,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/service/gpu/target_constants.h\"\n #include \"xla/service/gpu/tests/gpu_codegen_test.h\"\n #include \"xla/shape.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n@@ -1659,15 +1661,16 @@ ENTRY entry {\n   const se::DeviceDescription dev_info =\n       TestGpuDeviceInfo::RTXA6000DeviceInfo();\n   llvm::LLVMContext llvm_ctx;\n-  llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n+  llvm::Triple target_triple(nvptx::TargetTriple());\n+  std::string data_layout(nvptx::DataLayout());\n \n   EXPECT_THAT(\n       TritonWrapper(\"test_fn\", triton_fusion,\n                     se::CudaComputeCapability{se::CudaComputeCapability::kVolta,\n                                               /*minor=*/0},\n-                    dev_info, BlockLevelParameters(), &llvm_module,\n-                    mlir_context),\n+                    dev_info, BlockLevelParameters(), target_triple,\n+                    data_layout, llvm_ctx, mlir_context),\n       absl_testing::StatusIs(\n           absl::StatusCode::kFailedPrecondition,\n           ::testing::HasSubstr(\"Triton support is only enabled for Ampere GPUs \"\n@@ -1717,8 +1720,9 @@ ENTRY entry_computation {\n   const se::DeviceDescription dev_info =\n       TestGpuDeviceInfo::RTXA6000DeviceInfo(compute_capability);\n   llvm::LLVMContext llvm_ctx;\n-  llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n+  llvm::Triple target_triple(nvptx::TargetTriple());\n+  std::string data_layout(nvptx::DataLayout());\n \n   BlockLevelParameters block_level_parameters;\n   block_level_parameters.output_tile_sizes = {{1024, 1}};\n@@ -1729,7 +1733,8 @@ ENTRY entry_computation {\n   // 1048576.\n   EXPECT_THAT(\n       TritonWrapper(\"test_fn\", triton_fusion, compute_capability, dev_info,\n-                    block_level_parameters, &llvm_module, mlir_context),\n+                    block_level_parameters, target_triple, data_layout,\n+                    llvm_ctx, mlir_context),\n       absl_testing::StatusIs(\n           absl::StatusCode::kInvalidArgument,\n           ::testing::HasSubstr(\"Tiling does not satisfy constraints.\")));\n@@ -4553,8 +4558,9 @@ TEST_F(TritonEmitterTest, RocmWarpSizeIsSetCorrectly) {\n       verified_module->entry_computation()->root_instruction());\n \n   llvm::LLVMContext llvm_ctx;\n-  llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n+  llvm::Triple target_triple(nvptx::TargetTriple());\n+  std::string data_layout(nvptx::DataLayout());\n   std::vector<std::string> paths;\n   std::string triton_passes_log;\n \n@@ -4564,7 +4570,8 @@ TEST_F(TritonEmitterTest, RocmWarpSizeIsSetCorrectly) {\n   TF_ASSERT_OK(TritonWrapper(\n       \"test_fn\", triton_fusion,\n       se::GpuComputeCapability{se::RocmComputeCapability(\"gfx942\")}, dev_info,\n-      BlockLevelParameters(), &llvm_module, mlir_context));\n+      BlockLevelParameters(), target_triple, data_layout, llvm_ctx,\n+      mlir_context));\n   TF_EXPECT_OK(tsl::Env::Default()->GetMatchingPaths(\n       tsl::io::JoinPath(output_directory, \"*.triton-passes.log\"), &paths));\n   EXPECT_EQ(paths.size(), 1);\n@@ -4581,7 +4588,8 @@ TEST_F(TritonEmitterTest, RocmWarpSizeIsSetCorrectly) {\n   TF_ASSERT_OK(TritonWrapper(\n       \"test_fn\", triton_fusion,\n       se::GpuComputeCapability{se::RocmComputeCapability(\"gfx1100\")},\n-      dev_info_n, BlockLevelParameters(), &llvm_module, mlir_context));\n+      dev_info_n, BlockLevelParameters(), target_triple, data_layout, llvm_ctx,\n+      mlir_context));\n   TF_EXPECT_OK(tsl::Env::Default()->GetMatchingPaths(\n       tsl::io::JoinPath(output_directory, \"*.triton-passes.log\"), &paths));\n   EXPECT_EQ(paths.size(), 1);"
        },
        {
            "sha": "0e750a6e600c19fdd6d00ef7c67614e8de858852",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_deviceless_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_deviceless_test.cc?ref=aa081d21606621c86aac71bd3e45001b98a7bc26",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include \"absl/status/status_matchers.h\"\n #include \"absl/strings/string_view.h\"\n #include \"llvm/IR/LLVMContext.h\"\n+#include \"llvm/TargetParser/Triple.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/gpu/codegen/triton/xtile_compiler.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n@@ -31,6 +32,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/verified_hlo_module.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/service/gpu/target_constants.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -77,7 +79,8 @@ ENTRY entry {\n   const se::DeviceDescription dev_info =\n       TestGpuDeviceInfo::RTXA6000DeviceInfo();\n   llvm::LLVMContext llvm_ctx;\n-  llvm::Module llvm_module(\"module\", llvm_ctx);\n+  llvm::Triple triple(nvptx::TargetTriple());\n+  std::string data_layout = nvptx::DataLayout();\n   mlir::MLIRContext mlir_context;\n \n   BlockLevelParameters block_level_parameters;\n@@ -87,7 +90,8 @@ ENTRY entry {\n   EXPECT_THAT(TritonWrapper(\n                   \"test_fn\", triton_fusion,\n                   se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n-                  dev_info, block_level_parameters, &llvm_module, mlir_context),\n+                  dev_info, block_level_parameters, triple, data_layout,\n+                  llvm_ctx, mlir_context),\n               absl_testing::StatusIs(\n                   absl::StatusCode::kFailedPrecondition,\n                   ::testing::HasSubstr(\n@@ -156,8 +160,6 @@ ENTRY entry {\n       hlo_module->entry_computation()->root_instruction());\n   const se::DeviceDescription dev_info =\n       TestGpuDeviceInfo::RTXA6000DeviceInfo();\n-  llvm::LLVMContext llvm_ctx;\n-  llvm::Module llvm_module(\"module\", llvm_ctx);\n   mlir::MLIRContext mlir_context;\n \n   EXPECT_OK(\n@@ -230,7 +232,8 @@ ENTRY entry {\n   const se::DeviceDescription dev_info =\n       TestGpuDeviceInfo::RTXB200SXMDeviceInfo();\n   llvm::LLVMContext llvm_ctx;\n-  llvm::Module llvm_module(\"module\", llvm_ctx);\n+  llvm::Triple triple(nvptx::TargetTriple());\n+  std::string data_layout = nvptx::DataLayout();\n   mlir::MLIRContext mlir_context;\n   TF_ASSERT_OK_AND_ASSIGN(\n       TritonWrapperResult result,\n@@ -240,7 +243,7 @@ ENTRY entry {\n                         fusion->backend_config<GpuBackendConfig>()\n                             ->fusion_backend_config()\n                             .block_level_fusion_config()),\n-                    &llvm_module, mlir_context));\n+                    triple, data_layout, llvm_ctx, mlir_context));\n \n   // Warp specialization influences the total number of threads we end up\n   // using. Usually we would expect num_warps * warp_size threads per block, but"
        },
        {
            "sha": "e305a4c5cadbe0c5af3d9160b2d9dfdd2d922ac8",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_test.cc?ref=aa081d21606621c86aac71bd3e45001b98a7bc26",
            "patch": "@@ -31,6 +31,7 @@ limitations under the License.\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n+#include \"xla/service/gpu/target_constants.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -113,9 +114,12 @@ ENTRY entry_computation {\n   EXPECT_EQ(triton_fusion_emitter->GetLaunchConfig(), std::nullopt);\n \n   // Ensure that the emitter fails gracefully when the launch config is not set.\n+  llvm::LLVMContext llvm_ctx;\n+  llvm::Triple triple(nvptx::TargetTriple());\n+  std::string data_layout = nvptx::DataLayout();\n   EXPECT_THAT(triton_fusion_emitter->GenerateTritonKernelAndWrapper(\n                   *::xla::Cast<HloFusionInstruction>(root), \"random_name\",\n-                  device_info, /*llvm_module=*/nullptr, &mlir_context),\n+                  device_info, triple, data_layout, &llvm_ctx, &mlir_context),\n               absl_testing::StatusIs(absl::StatusCode::kInvalidArgument));\n }\n "
        },
        {
            "sha": "22a63ce5eb6263c7157f116f00ae395ee3cd512e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_legacy_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_legacy_test.cc?ref=aa081d21606621c86aac71bd3e45001b98a7bc26",
            "patch": "@@ -142,8 +142,8 @@ ENTRY e {\n                   .block_level_fusion_config());\n       EXPECT_THAT(\n           TritonWrapper(\"test_fn\", &ti.TritonFusion(), GetComputeCapability(),\n-                        dev_info, block_level_parameters, &llvm_module_,\n-                        mlir_context_),\n+                        dev_info, block_level_parameters, target_triple_,\n+                        data_layout_, llvm_ctx_, mlir_context_),\n           absl_testing::StatusIs(\n               absl::StatusCode::kInternal,\n               ::testing::HasSubstr(\"Failed to compile Triton kernel\")));\n@@ -468,9 +468,10 @@ ENTRY e {\n               .backend_config<GpuBackendConfig>()\n               ->fusion_backend_config()\n               .block_level_fusion_config());\n-  TF_EXPECT_OK(TritonWrapper(\n-      \"test_fn\", &ti.TritonFusion(), GetComputeCapability(), dev_info,\n-      block_level_parameters, &llvm_module_, mlir_context_));\n+  TF_EXPECT_OK(TritonWrapper(\"test_fn\", &ti.TritonFusion(),\n+                             GetComputeCapability(), dev_info,\n+                             block_level_parameters, target_triple_,\n+                             data_layout_, llvm_ctx_, mlir_context_));\n }\n \n TEST_F(TritonSupportTestBase,"
        },
        {
            "sha": "b6d6d4bfb4ffc2e313ec29b5e48ae9dbe1b20e27",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_test.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc?ref=aa081d21606621c86aac71bd3e45001b98a7bc26",
            "patch": "@@ -43,6 +43,7 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/service/gpu/target_constants.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -268,10 +269,17 @@ class TritonSupportTest : public TritonSupportTestBase {\n     const se::DeviceDescription dev_info =\n         cc.IsCuda() ? TestGpuDeviceInfo::RTXA6000DeviceInfo(cc)\n                     : TestGpuDeviceInfo::AMDMI210DeviceInfo();\n+    if (cc.IsCuda()) {\n+      data_layout_ = nvptx::DataLayout();\n+      target_triple_ = llvm::Triple(nvptx::TargetTriple());\n+    } else {\n+      data_layout_ = amdgpu::DataLayout();\n+      target_triple_ = llvm::Triple(amdgpu::TargetTriple());\n+    }\n     auto run_triton_codegen = [&]() {\n       return TritonWrapper(\"test_fn\", &ti.TritonFusion(), cc, dev_info,\n-                           block_level_parameters, &llvm_module_,\n-                           mlir_context_);\n+                           block_level_parameters, target_triple_, data_layout_,\n+                           llvm_ctx_, mlir_context_);\n     };\n \n     if (IsTritonSupportedInstruction(ti.Instruction(), cc)) {"
        },
        {
            "sha": "c4e8dcec745e009240514a29a8fa0f8bb80c49fd",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/test_utils.h",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.h?ref=aa081d21606621c86aac71bd3e45001b98a7bc26",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"llvm/IR/LLVMContext.h\"\n #include \"llvm/IR/Module.h\"\n+#include \"llvm/TargetParser/Triple.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n@@ -165,7 +166,8 @@ class TritonSupportTestBase : public HloTestBase {\n       xla::HloOpcode opcode, bool use_nested_gemm_fusions = false);\n \n   llvm::LLVMContext llvm_ctx_;\n-  llvm::Module llvm_module_{\"module\", llvm_ctx_};\n+  llvm::Triple target_triple_;\n+  std::string data_layout_;\n   mlir::MLIRContext mlir_context_;\n   TritonGemmConfig config_{16, 32, 512, 1, 4, 8};\n };"
        },
        {
            "sha": "0900b90f3b6a66c73acabbd0492e0d4d3c6e3489",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/triton_gemm_fusion_test.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 12,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftriton_gemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftriton_gemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftriton_gemm_fusion_test.cc?ref=aa081d21606621c86aac71bd3e45001b98a7bc26",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/strings/substitute.h\"\n #include \"llvm/IR/LLVMContext.h\"\n+#include \"llvm/TargetParser/Triple.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"xla/autotuning.pb.h\"\n@@ -47,6 +48,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/service/gpu/target_constants.h\"\n #include \"xla/service/gpu/tests/gpu_codegen_test.h\"\n #include \"xla/service/gpu/transforms/nest_gemm_fusion.h\"\n #include \"xla/service/pattern_matcher.h\"\n@@ -458,7 +460,8 @@ TEST_F(TritonGemmTest, FailIfTooMuchShmem) {\n   const se::DeviceDescription device_info =\n       TestGpuDeviceInfo::RTXA6000DeviceInfo();\n   llvm::LLVMContext llvm_ctx;\n-  llvm::Module llvm_module(\"module\", llvm_ctx);\n+  llvm::Triple target_triple(nvptx::TargetTriple());\n+  std::string data_layout(nvptx::DataLayout());\n \n   constexpr absl::string_view kHloTextTemplate = R\"(\n triton_gemm_dot {\n@@ -488,7 +491,7 @@ ENTRY entry {\n   EXPECT_THAT(\n       TritonWrapper(\"test_fn\", fusion1, se::GpuComputeCapability{cc},\n                     device_info, module1_and_metadata.block_level_parameters,\n-                    &llvm_module, mlir_context_),\n+                    target_triple, data_layout, llvm_ctx, mlir_context_),\n       absl_testing::StatusIs(\n           tsl::error::RESOURCE_EXHAUSTED,\n           ::testing::HasSubstr(\"Shared memory size limit exceeded\")));\n@@ -504,7 +507,7 @@ ENTRY entry {\n       const auto result,\n       TritonWrapper(\"test_fn\", fusion2, se::GpuComputeCapability{cc},\n                     device_info, module2_and_metadata.block_level_parameters,\n-                    &llvm_module, mlir_context_));\n+                    target_triple, data_layout, llvm_ctx, mlir_context_));\n   // Use optin shared memory which is > shared_memory_per_block.\n   EXPECT_GT(result.shmem_bytes, device_info.shared_memory_per_block());\n }\n@@ -801,7 +804,8 @@ TEST_F(TritonGemmTest, DISABLED_FailForTooComplexTiling) {\n   const se::DeviceDescription device_info =\n       TestGpuDeviceInfo::RTXA6000DeviceInfo();\n   llvm::LLVMContext llvm_ctx;\n-  llvm::Module llvm_module(\"module\", llvm_ctx);\n+  llvm::Triple target_triple(nvptx::TargetTriple());\n+  std::string data_layout(nvptx::DataLayout());\n \n   constexpr absl::string_view kHloTextTemplate = R\"(\n HloModule module\n@@ -834,7 +838,7 @@ ENTRY entry {\n   EXPECT_THAT(\n       TritonWrapper(\"test_fn\", fusion1, se::GpuComputeCapability{cc},\n                     device_info, module1_and_metadata.block_level_parameters,\n-                    &llvm_module, mlir_context_),\n+                    target_triple, data_layout, llvm_ctx, mlir_context_),\n       absl_testing::StatusIs(tsl::error::RESOURCE_EXHAUSTED,\n                              \"Tiling complexity heuristic exceeded\"));\n \n@@ -846,11 +850,11 @@ ENTRY entry {\n   const HloFusionInstruction* fusion2 = Cast<HloFusionInstruction>(\n       module1_and_metadata.computation->FusionInstruction());\n \n-  TF_EXPECT_OK(TritonWrapper(\"test_fn\", fusion2, se::GpuComputeCapability{cc},\n-                             device_info,\n-                             module2_and_metadata.block_level_parameters,\n-                             &llvm_module, mlir_context_)\n-                   .status());\n+  TF_EXPECT_OK(\n+      TritonWrapper(\"test_fn\", fusion2, se::GpuComputeCapability{cc},\n+                    device_info, module2_and_metadata.block_level_parameters,\n+                    target_triple, data_layout, llvm_ctx, mlir_context_)\n+          .status());\n }\n \n // TODO(b/393299275): this test may have some value while Triton tiling\n@@ -1987,14 +1991,15 @@ ENTRY e {\n   const HloFusionInstruction* triton_dot_fusion = Cast<HloFusionInstruction>(\n       optin_shmem_module_and_metadata.computation->FusionInstruction());\n   llvm::LLVMContext llvm_ctx;\n-  llvm::Module llvm_module(\"module\", llvm_ctx);\n+  llvm::Triple target_triple(nvptx::TargetTriple());\n+  std::string data_layout(nvptx::DataLayout());\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       const auto result,\n       TritonWrapper(\"test_fn\", triton_dot_fusion, GpuComputeCapability(),\n                     dev_info,\n                     optin_shmem_module_and_metadata.block_level_parameters,\n-                    &llvm_module, mlir_context_));\n+                    target_triple, data_layout, llvm_ctx, mlir_context_));\n   // The config is chosen so that the used memory size is slightly above the\n   // 48 kB boundary of standard / opt-in shared memory so that any GPU that\n   // has the opt-in one should be able to execute the test."
        },
        {
            "sha": "d4ebf755ddf2defc68024bb9bd73ceaa54226ec1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/xtile_compiler.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 18,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.cc?ref=aa081d21606621c86aac71bd3e45001b98a7bc26",
            "patch": "@@ -288,7 +288,8 @@ absl::StatusOr<TritonWrapperResult> TritonWrapper(\n     const se::GpuComputeCapability& gpu_cc,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    llvm::Module* llvm_module, MLIRContext& mlir_context) {\n+    const llvm::Triple& target_triple, const std::string& data_layout,\n+    llvm::LLVMContext& llvm_context, MLIRContext& mlir_context) {\n   TF_RETURN_IF_ERROR(CheckAtLeastAmpere(gpu_cc));\n \n   TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> triton_module,\n@@ -303,15 +304,17 @@ absl::StatusOr<TritonWrapperResult> TritonWrapper(\n   const HloModule* hlo_module = fusion->GetModule();\n   return CompileTritonToLLVM(fn_name, *hlo_module, device_info,\n                              block_level_parameters, triton_module.get(),\n-                             llvm_module, mlir_context,\n+                             target_triple, data_layout, llvm_context,\n+                             mlir_context,\n                              /*is_xla_fusion=*/true);\n }\n \n absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n     absl::string_view kernel_name, const HloModule& hlo_module,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    mlir::ModuleOp triton_module, llvm::Module* llvm_module,\n+    mlir::ModuleOp triton_module, const llvm::Triple& target_triple,\n+    const std::string& data_layout, llvm::LLVMContext& llvm_context,\n     mlir::MLIRContext& mlir_context, bool is_xla_fusion, bool emit_kernel) {\n   const auto& gpu_cc = device_info.gpu_compute_capability();\n   TF_RETURN_IF_ERROR(CheckAtLeastAmpere(gpu_cc));\n@@ -425,10 +428,10 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n   }\n \n   std::vector<llvm::Metadata*> captured_nvvm_annotations;\n+  std::unique_ptr<llvm::Module> ll_triton_module;\n   if (emit_kernel) {\n-    TF_ASSIGN_OR_RETURN(\n-        std::unique_ptr<llvm::Module> ll_triton_module,\n-        TranslateLLVMToLLVMIR(&llvm_module->getContext(), triton_module));\n+    TF_ASSIGN_OR_RETURN(ll_triton_module,\n+                        TranslateLLVMToLLVMIR(&llvm_context, triton_module));\n \n     XLA_VLOG_LINES(5, llvm_ir::DumpToString(ll_triton_module.get()));\n     if (should_verify) {\n@@ -438,16 +441,12 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n     // Integrate LLVM matmul kernel into XLA's LLVM module.\n     captured_nvvm_annotations =\n         xgt::ExtractNvvmAnnotations(ll_triton_module.get());\n-    ll_triton_module->setDataLayout(llvm_module->getDataLayout());\n-    ll_triton_module->setTargetTriple(llvm_module->getTargetTriple());\n+    ll_triton_module->setDataLayout(data_layout);\n+    ll_triton_module->setTargetTriple(target_triple);\n     // Use override flag because libdevice functions can be present in both.\n-    TF_RET_CHECK(\n-        !llvm::Linker::linkModules(*llvm_module, std::move(ll_triton_module),\n-                                   llvm::Linker::Flags::OverrideFromSrc));\n-\n-    XLA_VLOG_LINES(5, llvm_ir::DumpToString(llvm_module));\n+    XLA_VLOG_LINES(5, llvm_ir::DumpToString(ll_triton_module.get()));\n     if (should_verify) {\n-      VerifyModule(*llvm_module);\n+      VerifyModule(*ll_triton_module);\n     }\n   }\n \n@@ -490,10 +489,12 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n   // - TMA metadata.\n   // - Total threads per block. Computed from module attributes.\n   // - Captured NVVM annotations.\n-  TritonWrapperResult result = {\n-      shared_mem_bytes,          cluster_dim, tma_metadata, thread_dims,\n-      captured_nvvm_annotations,\n-  };\n+  TritonWrapperResult result = {shared_mem_bytes,\n+                                cluster_dim,\n+                                tma_metadata,\n+                                thread_dims,\n+                                captured_nvvm_annotations,\n+                                std::move(ll_triton_module)};\n   return result;\n }\n "
        },
        {
            "sha": "3370b7f81a12c09fe8c46526844f8dd28bc02791",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/xtile_compiler.h",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler.h?ref=aa081d21606621c86aac71bd3e45001b98a7bc26",
            "patch": "@@ -17,25 +17,26 @@ limitations under the License.\n #define XLA_BACKENDS_GPU_CODEGEN_TRITON_XTILE_COMPILER_H_\n \n #include <cstdint>\n+#include <memory>\n #include <optional>\n+#include <string>\n+#include <vector>\n \n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n-#include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/IR/Metadata.h\"\n #include \"llvm/IR/Module.h\"\n #include \"llvm/Support/raw_ostream.h\"\n+#include \"llvm/TargetParser/Triple.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/codegen/tiling/symbolic_tile_analysis.h\"\n-#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n+#include \"xla/codegen/tiling/tiling_specification.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -66,6 +67,7 @@ struct TritonWrapperResult {\n   // Triton. We need to propagate them because we later create the kernel and\n   // splice the impl_fn into it.\n   std::vector<llvm::Metadata*> nvvm_annotations;\n+  std::unique_ptr<llvm::Module> llvm_module;\n };\n \n // Load the MLIR dialects required for Triton IR generation.\n@@ -78,7 +80,8 @@ absl::StatusOr<TritonWrapperResult> TritonWrapper(\n     const se::GpuComputeCapability& cc,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    llvm::Module* llvm_module, mlir::MLIRContext& mlir_context);\n+    const llvm::Triple& target_triple, const std::string& data_layout,\n+    llvm::LLVMContext& llvm_context, mlir::MLIRContext& mlir_context);\n \n // Creates the initial Triton module for the given fusion. Visible for testing,\n // use TritonWrapper instead.\n@@ -96,7 +99,8 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n     absl::string_view kernel_name, const HloModule& hlo_module,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    mlir::ModuleOp triton_module, llvm::Module* llvm_module,\n+    mlir::ModuleOp triton_module, const llvm::Triple& target_triple,\n+    const std::string& data_layout, llvm::LLVMContext& llvm_context,\n     mlir::MLIRContext& mlir_context, bool is_xla_fusion,\n     bool emit_kernel = true);\n "
        },
        {
            "sha": "12efe5347c4aead211a4c3cb7409105e9a0c87d3",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/xtile_compiler_stub.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler_stub.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler_stub.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler_stub.cc?ref=aa081d21606621c86aac71bd3e45001b98a7bc26",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"llvm/IR/Module.h\"\n+#include \"llvm/TargetParser/Triple.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/MLIRContext.h\"\n@@ -27,6 +28,7 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/xtile_compiler.h\"\n #include \"xla/hlo/ir/hlo_clone_context.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n@@ -49,7 +51,8 @@ absl::StatusOr<TritonWrapperResult> TritonWrapper(\n     const se::GpuComputeCapability& cc,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    llvm::Module* llvm_module, mlir::MLIRContext& mlir_context) {\n+    const llvm::Triple& target_triple, const std::string& data_layout,\n+    llvm::LLVMContext& llvm_context, mlir::MLIRContext& mlir_context) {\n   return absl::UnimplementedError(\"not supported for this build configuration\");\n }\n \n@@ -65,7 +68,8 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n     absl::string_view kernel_name, const HloModule& hlo_module,\n     const se::DeviceDescription& device_info,\n     const BlockLevelParameters& block_level_parameters,\n-    mlir::ModuleOp triton_module, llvm::Module* llvm_module,\n+    mlir::ModuleOp triton_module, const llvm::Triple& target_triple,\n+    const std::string& data_layout, llvm::LLVMContext& llvm_context,\n     mlir::MLIRContext& mlir_context, bool is_xla_fusion, bool emit_kernel) {\n   return absl::UnimplementedError(\"not supported for this build configuration\");\n }"
        },
        {
            "sha": "5b47611387f7324a466d8fd8672fd80011b84856",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/xtile_compiler_stub_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler_stub_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler_stub_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fxtile_compiler_stub_test.cc?ref=aa081d21606621c86aac71bd3e45001b98a7bc26",
            "patch": "@@ -28,13 +28,18 @@ namespace {\n \n TEST(TritonStub, CallStubApi) {\n   mlir::MLIRContext mlir_context;\n+  llvm::LLVMContext llvm_context;\n+  llvm::Triple target_triple;\n+  std::string data_layout;\n \n   LoadMlirDialectsForTriton(mlir_context);\n-  EXPECT_FALSE(\n-      TritonWrapper({}, nullptr, {}, {}, {}, nullptr, mlir_context).ok());\n+  EXPECT_FALSE(TritonWrapper({}, nullptr, {}, {}, {}, target_triple,\n+                             data_layout, llvm_context, mlir_context)\n+                   .ok());\n   EXPECT_FALSE(CreateTritonModule({}, nullptr, {}, {}, mlir_context).ok());\n   EXPECT_FALSE(CompileTritonToLLVM(\"\", HloModule(\"test\", HloModuleConfig()), {},\n-                                   {}, {}, nullptr, mlir_context,\n+                                   {}, {}, target_triple, data_layout,\n+                                   llvm_context, mlir_context,\n                                    /*is_xla_fusion=*/true, {})\n                    .ok());\n "
        },
        {
            "sha": "fc0d4bdbbac3747ffecea2c16b2e57edf06b0c21",
            "filename": "third_party/xla/xla/service/gpu/compile_module_to_llvm_ir.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc?ref=aa081d21606621c86aac71bd3e45001b98a7bc26",
            "patch": "@@ -281,10 +281,10 @@ absl::StatusOr<CompileModuleResults> CompileModuleToLlvmIr(\n       split_constants_module\n           ? ir_emitter_context.CreateLLVMModule(hlo_module->name())\n           : thunk_emitter.ConsumeConstantsModule();\n+  llvm::Linker linker(*results.llvm_module);\n   for (auto& kernel_module : thunk_emitter.ConsumeKernelModules()) {\n-    CHECK(!llvm::Linker::linkModules(*results.llvm_module.get(),\n-                                     std::move(kernel_module),\n-                                     llvm::Linker::Flags::OverrideFromSrc));\n+    CHECK(!linker.linkInModule(std::move(kernel_module),\n+                               llvm::Linker::Flags::OverrideFromSrc));\n   }\n   if (split_constants_module) {\n     results.llvm_module_constants = thunk_emitter.ConsumeConstantsModule();"
        },
        {
            "sha": "7ed1d443480120403fb78494b54f994281e0dc7f",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aa081d21606621c86aac71bd3e45001b98a7bc26/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc?ref=aa081d21606621c86aac71bd3e45001b98a7bc26",
            "patch": "@@ -1274,7 +1274,6 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTritonCustomCall(\n     auto kernel_name = ir_emitter_context_->GetSanitizedUniqueName(call.name);\n     VLOG(3) << \"Generating: \" << kernel_name;\n \n-    auto local_module = ir_emitter_context_->CreateLLVMModule(kernel_name);\n     mlir::OwningOpRef<mlir::ModuleOp> triton_module;\n     {\n       mlir::BaseScopedDiagnosticHandler diagnostic_handler(&mlir_context);\n@@ -1313,7 +1312,9 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTritonCustomCall(\n         CompileTritonToLLVM(kernel_name, *hlo_module,\n                             ir_emitter_context_->gpu_device_info(),\n                             block_level_parameters, triton_module.get(),\n-                            local_module.get(), mlir_context,\n+                            ir_emitter_context_->target_triple(),\n+                            ir_emitter_context_->data_layout(),\n+                            *ir_emitter_context_->llvm_context(), mlir_context,\n                             /*is_xla_fusion=*/false, emit_kernels));\n \n     TF_ASSIGN_OR_RETURN(auto kernel_arguments,\n@@ -1329,16 +1330,16 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTritonCustomCall(\n     if (emit_kernels) {\n       TF_ASSIGN_OR_RETURN(\n           llvm::Function * kernel,\n-          RemoveUnusedTritonAbiArguments(local_module.get(),\n+          RemoveUnusedTritonAbiArguments(result.llvm_module.get(),\n                                          *ir_emitter_context_, kernel_name));\n \n       AnnotateAttrsIfUnset(kernel_arguments, *kernel);\n       TF_RETURN_IF_ERROR(AnnotateKernelLaunchDimensions(\n           ir_emitter_context_->gpu_device_info(), launch_dimensions, kernel,\n-          local_module.get()));\n+          result.llvm_module.get()));\n     }\n \n-    kernel_modules_.push_back(std::move(local_module));\n+    kernel_modules_.push_back(std::move(result.llvm_module));\n     return {{kernel_name, launch_dimensions, result.cluster_dim,\n              result.shmem_bytes}};\n   };"
        }
    ],
    "stats": {
        "total": 260,
        "additions": 156,
        "deletions": 104
    }
}