{
    "author": "thcmbs",
    "message": "[XLA:GPU] Create PackedTransposeDescription using TransposeDescription rather than hlo\n\nSo that it benefits from any OTF normalization that we do we creating the TransposeDescription (see child CL)\n\nOtherwise, in the current state we re-use the hlo directly and would miss any normalization applied to transposedescription\n\nI don't think that we should entirely merge them though, given that the Packed description does some transformation only relevant for the packed emitter.\n\nPiperOrigin-RevId: 845260501",
    "sha": "a64fa6b481e0ffd2f81b0fed2392649cee919b28",
    "files": [
        {
            "sha": "531dc6ff0fe71d16da23144097f507c13f11f97d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transpose.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a64fa6b481e0ffd2f81b0fed2392649cee919b28/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a64fa6b481e0ffd2f81b0fed2392649cee919b28/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.cc?ref=a64fa6b481e0ffd2f81b0fed2392649cee919b28",
            "patch": "@@ -543,7 +543,7 @@ std::vector<int64_t> GetBlockCounts(absl::Span<const int64_t> shape,\n }\n \n PackedTranspose::PackedTranspose(const HloFusionAnalysis& analysis,\n-                                 const TransposeSpec& spec,\n+                                 const PackedTransposeDescription& spec,\n                                  absl::Span<const int64_t> output_block_tile,\n                                  int64_t num_shmem_groups,\n                                  MLIRContext* mlir_context)\n@@ -676,8 +676,8 @@ PackedTranspose::WriteResult PackedTranspose::EmitWriteToShMemMlir(\n     auto* root_tuple = fusion.fused_expression_root();\n     for (auto root : side_output_roots_) {\n       auto indexing = ComposeIndexingMaps(\n-          input_indexing,\n-          GetBitcastMap(spec_.input_shape(), root->shape(), mlir_context_));\n+          input_indexing, GetBitcastMap(spec_.original_input_shape(),\n+                                        root->shape(), mlir_context_));\n       indexing.Simplify();\n       side_output_indices.push_back(ApplyIndexing(\n           indexing, thread_and_block_ids, symbol_values, nested_b));\n@@ -864,7 +864,7 @@ IndexingMap PackedTranspose::GetInputIndexing(MLIRContext* mlir_context) const {\n \n   // Actual indexing.\n   auto canonical_input_shape_to_real_shape = GetBitcastMap(\n-      spec_.canonical_input_shape, spec_.input_shape(), mlir_context);\n+      spec_.canonical_input_shape, spec_.original_input_shape(), mlir_context);\n   // When we compose, the constraints w.r.t. to the input dimension sizes will\n   // be added.\n   auto input_indexing = ComposeIndexingMaps(\n@@ -1000,8 +1000,9 @@ IndexingMap PackedTranspose::GetOutputIndexing(\n   canonical_output_indexing.Simplify();\n \n   // Actual indexing.\n-  auto canonical_output_shape_to_real_shape = GetBitcastMap(\n-      spec_.canonical_output_shape, spec_.output_shape(), mlir_context);\n+  auto canonical_output_shape_to_real_shape =\n+      GetBitcastMap(spec_.canonical_output_shape, spec_.original_output_shape(),\n+                    mlir_context);\n   // When we compose, the constraints w.r.t. to the output dimension sizes will\n   // be added.\n   auto output_indexing = ComposeIndexingMaps(\n@@ -1012,8 +1013,7 @@ IndexingMap PackedTranspose::GetOutputIndexing(\n \n std::unique_ptr<EmitterBase> CreateTransposeFusion(\n     const HloFusionAnalysis& analysis, MLIRContext* mlir_context) {\n-  auto spec = GetTransposeSpec(\n-      Cast<HloTransposeInstruction>(analysis.tiled_transpose().instr));\n+  PackedTransposeDescription spec(analysis.tiled_transpose());\n   auto packed_transpose_tile = GetPackedTransposeTileSizes(spec);\n   if (packed_transpose_tile.ok()) {\n     return std::make_unique<PackedTranspose>("
        },
        {
            "sha": "7c92bef8af1d2dd87f6d5d0a78abad586e35629e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transpose.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a64fa6b481e0ffd2f81b0fed2392649cee919b28/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a64fa6b481e0ffd2f81b0fed2392649cee919b28/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftranspose.h?ref=a64fa6b481e0ffd2f81b0fed2392649cee919b28",
            "patch": "@@ -164,9 +164,9 @@ class TransposeFusion : public TransposeFusionBase {\n };\n \n // Packed transpose is a more advanced version of the transpose emitter.\n-// It considers the canonical transpose described by TransposeSpec class,\n-// i.e. [T2, A, T1, B] -> [T1, A, T2, B] and tries to pack as many T1 rows into\n-// shared memory as possible.\n+// It considers the canonical transpose described by PackedTransposeDescription\n+// class, i.e. [T2, A, T1, B] -> [T1, A, T2, B] and tries to pack as many T1\n+// rows into shared memory as possible.\n //\n // Let's describe the algorithm for a concrete example.\n //   bf16 [640,100,6,1] - > bf16 [6,100,640,1]\n@@ -237,7 +237,7 @@ class TransposeFusion : public TransposeFusionBase {\n class PackedTranspose : public TransposeFusionBase {\n  public:\n   explicit PackedTranspose(const HloFusionAnalysis& analysis,\n-                           const TransposeSpec& spec,\n+                           const PackedTransposeDescription& spec,\n                            absl::Span<const int64_t> output_block_tile,\n                            int64_t num_shmem_groups,\n                            mlir::MLIRContext* mlir_context);\n@@ -273,7 +273,7 @@ class PackedTranspose : public TransposeFusionBase {\n   IndexingMap GetShmemReadIndexing(mlir::MLIRContext* ctx) const;\n   IndexingMap GetOutputIndexing(mlir::MLIRContext* ctx) const;\n \n-  TransposeSpec spec_;\n+  PackedTransposeDescription spec_;\n \n   // Tile sizes for the canonical input shape.\n   std::vector<int64_t> output_tile_;"
        },
        {
            "sha": "72c74c7b8ea8d2441a031fc014dd5e52fd01fc66",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 31,
            "changes": 56,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a64fa6b481e0ffd2f81b0fed2392649cee919b28/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a64fa6b481e0ffd2f81b0fed2392649cee919b28/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc?ref=a64fa6b481e0ffd2f81b0fed2392649cee919b28",
            "patch": "@@ -220,8 +220,8 @@ absl::StatusOr<BufferAllocation::Slice> GetAllocationSlice(\n   return buffer_assignment.GetUniqueSlice(instr, index);\n }\n \n-bool IsNormalized(const HloTransposeInstruction& transpose) {\n-  const auto& permutation = transpose.dimensions();\n+bool IsNormalized(const TransposeDescription& desc) {\n+  const auto& permutation = desc.permutation;\n   for (int i = 0; i < permutation.size() - 1; ++i) {\n     if (permutation[i] + 1 == permutation[i + 1]) {\n       return false;\n@@ -230,12 +230,12 @@ bool IsNormalized(const HloTransposeInstruction& transpose) {\n   return true;\n }\n \n-bool CanEmitPackedTranspose(const HloTransposeInstruction& transpose) {\n+bool CanEmitPackedTranspose(const TransposeDescription& desc) {\n   // Support only normalized transposes.\n-  if (!IsNormalized(transpose)) {\n+  if (!IsNormalized(desc)) {\n     return false;\n   }\n-  const auto& spec = GetTransposeSpec(&transpose);\n+  PackedTransposeDescription spec(desc);\n   return GetPackedTransposeTileSizes(spec).ok();\n }\n \n@@ -257,13 +257,15 @@ std::optional<TransposeDescription> GetDescriptionForTiledTransposeEmitter(\n   absl::InlinedVector<int64_t, 3> dimensions(hero.shape().dimensions().begin(),\n                                              hero.shape().dimensions().end());\n   int64_t operand_most_minor_dim = hero.operand(0)->shape().dimensions().back();\n-  if (CanEmitPackedTranspose(*Cast<HloTransposeInstruction>(&hero))) {\n+\n+  TransposeDescription desc{&hero, dimensions, permutation,\n+                            /*shmem_usage=*/0};\n+  if (CanEmitPackedTranspose(desc)) {\n     int64_t vector_size =\n         kBankBitwidth / GetBitwidth(hero.shape().element_type());\n-    int64_t shmem_usage_bytes =\n+    desc.shmem_usage =\n         kNumShmemBanks * (kBankBitwidth / 8) * kNumShmemBanks * vector_size;\n-    return TransposeDescription{&hero, dimensions, permutation,\n-                                shmem_usage_bytes};\n+    return desc;\n   }\n   if (permutation.back() == dimensions.size() - 1) {\n     operand_most_minor_dim =\n@@ -301,17 +303,17 @@ std::optional<TransposeDescription> GetDescriptionForTiledTransposeEmitter(\n   return std::nullopt;\n }\n \n-TransposeSpec GetTransposeSpec(const HloTransposeInstruction* transpose) {\n-  auto inv_permutation = InversePermutation(transpose->dimensions());\n-  auto& output_shape = transpose->shape();\n-  llvm::SmallVector<int64_t, 3> canonical_output_shape =\n-      llvm::to_vector<3>(output_shape.dimensions());\n-  llvm::SmallVector<int64_t, 3> canonical_permutation =\n-      llvm::to_vector<3>(transpose->dimensions());\n+PackedTransposeDescription::PackedTransposeDescription(\n+    const TransposeDescription& description)\n+    : transpose(Cast<HloTransposeInstruction>(description.instr)) {\n+  permutation = llvm::to_vector<3>(description.permutation);\n+  inv_permutation = llvm::to_vector<3>(InversePermutation(permutation));\n+  canonical_output_shape = llvm::to_vector<3>(description.dimensions);\n+  canonical_permutation = llvm::to_vector<3>(description.permutation);\n \n   // If the last dimension is transposed, add a size-1 B dimension.\n   if (canonical_permutation.back() != canonical_output_shape.size() - 1) {\n-    canonical_permutation.push_back(output_shape.dimensions().size());\n+    canonical_permutation.push_back(canonical_output_shape.size());\n     canonical_output_shape.push_back(1);\n   }\n   int64_t dim_t1 = -1;\n@@ -333,21 +335,13 @@ TransposeSpec GetTransposeSpec(const HloTransposeInstruction* transpose) {\n     canonical_permutation.insert(canonical_permutation.begin() + dim_t1,\n                                  dim_t1);\n   }\n-  auto canonical_inv_permutation = InversePermutation(canonical_permutation);\n-  auto canonical_input_shape =\n-      Permute(canonical_output_shape, canonical_inv_permutation);\n-  return TransposeSpec{\n-      transpose,\n-      llvm::to_vector<3>(transpose->dimensions()),\n-      llvm::to_vector<3>(inv_permutation),\n-      canonical_output_shape,\n-      canonical_permutation,\n-      llvm::to_vector<3>(canonical_inv_permutation),\n-      llvm::to_vector<3>(canonical_input_shape),\n-  };\n+  canonical_inv_permutation =\n+      llvm::to_vector<3>(InversePermutation(canonical_permutation));\n+  canonical_input_shape = llvm::to_vector<3>(\n+      Permute(canonical_output_shape, canonical_inv_permutation));\n }\n \n-std::string TransposeSpec::ToString() const {\n+std::string PackedTransposeDescription::ToString() const {\n   return absl::Substitute(R\"(\n transpose: $0\n canonical_input_shape: $1\n@@ -365,7 +359,7 @@ canonical_inv_permutation: $4\n }\n \n absl::StatusOr<absl::InlinedVector<int64_t, 3>> GetPackedTransposeTileSizes(\n-    const TransposeSpec& spec) {\n+    const PackedTransposeDescription& spec) {\n   // Check the side outputs, etc.\n   int64_t bits_per_element = GetBitwidth(spec.elem_type());\n   if (bits_per_element >= kBankBitwidth) {"
        },
        {
            "sha": "510940703bc650417f7df3254eb1710e45ea7641",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils.h",
            "status": "modified",
            "additions": 14,
            "deletions": 7,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a64fa6b481e0ffd2f81b0fed2392649cee919b28/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a64fa6b481e0ffd2f81b0fed2392649cee919b28/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.h?ref=a64fa6b481e0ffd2f81b0fed2392649cee919b28",
            "patch": "@@ -224,13 +224,19 @@ std::optional<TransposeDescription> GetDescriptionForTiledTransposeEmitter(\n // 3. <8x2x32x7x6> -> <6x32x2x7x8> becomes <8x2x32x7x6x1> -> <6x32x2x7x8x1>.\n \n // TODO(b/370690811): Unify this with TransposeDescription.\n-struct TransposeSpec {\n-  PrimitiveType elem_type() const { return input_shape().element_type(); }\n+struct PackedTransposeDescription {\n+  explicit PackedTransposeDescription(const TransposeDescription& description);\n \n-  const Shape& input_shape() const { return transpose->operand(0)->shape(); }\n-  const Shape& output_shape() const { return transpose->shape(); }\n+  PrimitiveType elem_type() const {\n+    return original_input_shape().element_type();\n+  }\n+\n+  const Shape& original_input_shape() const {\n+    return transpose->operand(0)->shape();\n+  }\n+  const Shape& original_output_shape() const { return transpose->shape(); }\n \n-  int64_t rank() const { return input_shape().dimensions().size(); }\n+  int64_t rank() const { return original_input_shape().dimensions().size(); }\n   int64_t canonical_rank() const { return canonical_input_shape.size(); }\n \n   int64_t dim_A() const { return canonical_input_shape[dim_A_id()]; }\n@@ -264,11 +270,12 @@ struct TransposeSpec {\n   llvm::SmallVector<int64_t, 3> canonical_input_shape;\n };\n \n-TransposeSpec GetTransposeSpec(const HloTransposeInstruction* transpose);\n+// Returns true if the given transpose can be emitted using the packed emitter.\n+bool CanEmitPackedTranspose(const TransposeDescription& desc);\n \n // Returns the default tile sizes for the packed transpose emitter.\n absl::StatusOr<absl::InlinedVector<int64_t, 3>> GetPackedTransposeTileSizes(\n-    const TransposeSpec& spec);\n+    const PackedTransposeDescription& spec);\n \n // Verify the given module, and crash if it failed.\n void VerifyModule(const llvm::Module& module);"
        },
        {
            "sha": "f14ff91e330107d4e51da6c2199992b3cbe24a81",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils_test.cc",
            "status": "modified",
            "additions": 83,
            "deletions": 6,
            "changes": 89,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a64fa6b481e0ffd2f81b0fed2392649cee919b28/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a64fa6b481e0ffd2f81b0fed2392649cee919b28/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils_test.cc?ref=a64fa6b481e0ffd2f81b0fed2392649cee919b28",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n #include <array>\n #include <cstdint>\n #include <memory>\n+#include <optional>\n #include <string>\n #include <vector>\n \n@@ -50,10 +51,27 @@ using ::testing::SizeIs;\n \n class IrEmissionUtilsTest : public HloHardwareIndependentTestBase {\n  public:\n-  TransposeSpec GetTransposeSpecFromRoot(absl::string_view hlo_text) {\n+  PackedTransposeDescription GetTransposeSpecFromTransposeDescription(\n+      absl::string_view hlo_text,\n+      std::optional<absl::InlinedVector<int64_t, 3>> permutation = std::nullopt,\n+      std::optional<absl::InlinedVector<int64_t, 3>> dimensions =\n+          std::nullopt) {\n     auto module = ParseAndReturnVerifiedModule(hlo_text).value();\n-    auto* root = module->entry_computation()->root_instruction();\n-    return GetTransposeSpec(Cast<HloTransposeInstruction>(root));\n+    auto* root = Cast<HloTransposeInstruction>(\n+        module->entry_computation()->root_instruction());\n+\n+    if (!permutation.has_value()) {\n+      permutation = absl::InlinedVector<int64_t, 3>(root->dimensions().begin(),\n+                                                    root->dimensions().end());\n+    }\n+    if (!dimensions.has_value()) {\n+      dimensions = absl::InlinedVector<int64_t, 3>(\n+          root->shape().dimensions().begin(), root->shape().dimensions().end());\n+    }\n+\n+    TransposeDescription description{root, *dimensions, *permutation,\n+                                     /*shmem_usage=*/0};\n+    return PackedTransposeDescription(description);\n   }\n };\n \n@@ -1117,7 +1135,7 @@ TEST_F(IrEmissionUtilsTest, MultipleDynamicVariables) {\n }\n \n TEST_F(IrEmissionUtilsTest, Transpose_10) {\n-  auto spec = GetTransposeSpecFromRoot(R\"(ENTRY entry {\n+  auto spec = GetTransposeSpecFromTransposeDescription(R\"(ENTRY entry {\n     p0 = f32[8, 32] parameter(0)\n     ROOT transpose_p0 = f32[32, 8] transpose(p0), dimensions={1, 0}\n   })\");\n@@ -1130,7 +1148,7 @@ TEST_F(IrEmissionUtilsTest, Transpose_10) {\n }\n \n TEST_F(IrEmissionUtilsTest, Transpose_210) {\n-  auto spec = GetTransposeSpecFromRoot(R\"(ENTRY entry {\n+  auto spec = GetTransposeSpecFromTransposeDescription(R\"(ENTRY entry {\n     p0 = f32[8, 2, 32] parameter(0)\n     ROOT transpose_p0 = f32[32, 2, 8] transpose(p0), dimensions={2, 1, 0}\n   })\");\n@@ -1141,7 +1159,7 @@ TEST_F(IrEmissionUtilsTest, Transpose_210) {\n }\n \n TEST_F(IrEmissionUtilsTest, Transpose_102) {\n-  auto spec = GetTransposeSpecFromRoot(R\"(ENTRY entry {\n+  auto spec = GetTransposeSpecFromTransposeDescription(R\"(ENTRY entry {\n     p0 = f32[8, 2, 32, 7, 6] parameter(0)\n     ROOT transpose_p0 = f32[6, 32, 2, 7, 8] transpose(p0),\n       dimensions={4, 2, 1, 3, 0}\n@@ -1152,6 +1170,65 @@ TEST_F(IrEmissionUtilsTest, Transpose_102) {\n   EXPECT_THAT(spec.canonical_inv_permutation, ElementsAre(4, 2, 1, 3, 0, 5));\n }\n \n+TEST_F(IrEmissionUtilsTest,\n+       PackedTransposeDescriptionUsesProvidedDims_Grouping) {\n+  auto spec = GetTransposeSpecFromTransposeDescription(\n+      R\"(ENTRY entry {\n+    p = f32[32,32,64]{2,1,0} parameter(0)\n+    ROOT t = f32[64,32,32]{2,1,0} transpose(p), dimensions={2,0,1}\n+  })\",\n+      /*permutation=*/InlinedVector({1, 0}),\n+      /*dimensions=*/InlinedVector({64, 1024}));\n+\n+  EXPECT_THAT(spec.canonical_output_shape, ElementsAre(64, 1, 1024, 1));\n+}\n+\n+TEST_F(IrEmissionUtilsTest, PackedTransposeDescriptionUsesProvidedDims_10) {\n+  auto spec = GetTransposeSpecFromTransposeDescription(\n+      R\"(ENTRY entry {\n+    p0 = f32[8, 4, 8] parameter(0)\n+    ROOT transpose_p0 = f32[4, 8, 8] transpose(p0), dimensions={1, 2, 0}\n+  })\",\n+      /*permutation=*/InlinedVector({1, 0}),\n+      /*dimensions=*/InlinedVector({32, 8}));\n+  EXPECT_THAT(spec.permutation, ElementsAre(1, 0));\n+  EXPECT_THAT(spec.inv_permutation, ElementsAre(1, 0));\n+  EXPECT_THAT(spec.canonical_input_shape, ElementsAre(8, 1, 32, 1));\n+  EXPECT_THAT(spec.canonical_output_shape, ElementsAre(32, 1, 8, 1));\n+  EXPECT_THAT(spec.canonical_permutation, ElementsAre(2, 1, 0, 3));\n+  EXPECT_THAT(spec.canonical_inv_permutation, ElementsAre(2, 1, 0, 3));\n+}\n+\n+TEST_F(IrEmissionUtilsTest, PackedTransposeDescriptionUsesProvidedDims_210) {\n+  auto spec = GetTransposeSpecFromTransposeDescription(\n+      R\"(ENTRY entry {\n+    p0 = f32[8, 2, 4, 8] parameter(0)\n+    ROOT transpose_p0 = f32[4, 8, 2, 8] transpose(p0),\n+      dimensions={2, 3, 1, 0}\n+  })\",\n+      /*permutation=*/InlinedVector({2, 1, 0}),\n+      /*dimensions=*/InlinedVector({32, 2, 8}));\n+  EXPECT_THAT(spec.canonical_input_shape, ElementsAre(8, 2, 32, 1));\n+  EXPECT_THAT(spec.canonical_output_shape, ElementsAre(32, 2, 8, 1));\n+  EXPECT_THAT(spec.canonical_permutation, ElementsAre(2, 1, 0, 3));\n+  EXPECT_THAT(spec.canonical_inv_permutation, ElementsAre(2, 1, 0, 3));\n+}\n+\n+TEST_F(IrEmissionUtilsTest, PackedTransposeDescriptionUsesProvidedDims_102) {\n+  auto spec = GetTransposeSpecFromTransposeDescription(\n+      R\"(ENTRY entry {\n+    p0 = f32[8, 2, 32, 7, 2, 3] parameter(0)\n+    ROOT transpose_p0 = f32[2, 3, 32, 2, 7, 8] transpose(p0),\n+      dimensions={4, 5, 2, 1, 3, 0}\n+  })\",\n+      /*permutation=*/InlinedVector({4, 2, 1, 3, 0}),\n+      /*dimensions=*/InlinedVector({6, 32, 2, 7, 8}));\n+  EXPECT_THAT(spec.canonical_input_shape, ElementsAre(8, 2, 32, 7, 6, 1));\n+  EXPECT_THAT(spec.canonical_output_shape, ElementsAre(6, 32, 2, 7, 8, 1));\n+  EXPECT_THAT(spec.canonical_permutation, ElementsAre(4, 2, 1, 3, 0, 5));\n+  EXPECT_THAT(spec.canonical_inv_permutation, ElementsAre(4, 2, 1, 3, 0, 5));\n+}\n+\n TEST(DenseDataIntermediateTest, OwnedDataToProto) {\n   const std::vector<uint8_t> data = {1, 2, 3, 4};\n   DenseDataIntermediate constant = DenseDataIntermediate::Own(data);"
        }
    ],
    "stats": {
        "total": 192,
        "additions": 135,
        "deletions": 57
    }
}