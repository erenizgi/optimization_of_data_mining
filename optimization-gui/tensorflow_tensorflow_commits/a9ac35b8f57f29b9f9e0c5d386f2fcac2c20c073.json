{
    "author": "tensorflower-gardener",
    "message": "- Don't move host offloading annotations in licm.\n- Allow loop-related instructions between DS and host offloading annotations.\n\nPiperOrigin-RevId: 818469793",
    "sha": "a9ac35b8f57f29b9f9e0c5d386f2fcac2c20c073",
    "files": [
        {
            "sha": "9beb9f1c6d20ec8b57618de9b2feba2e6d539292",
            "filename": "third_party/xla/xla/hlo/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a9ac35b8f57f29b9f9e0c5d386f2fcac2c20c073/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a9ac35b8f57f29b9f9e0c5d386f2fcac2c20c073/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2FBUILD?ref=a9ac35b8f57f29b9f9e0c5d386f2fcac2c20c073",
            "patch": "@@ -340,6 +340,7 @@ xla_cc_test(\n         \"//xla/service:memory_annotations_hdr\",\n         \"//xla/service:pattern_matcher\",\n         \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\","
        },
        {
            "sha": "260eee1ce22071d27d604079d9274292a776540b",
            "filename": "third_party/xla/xla/hlo/transforms/host_offloader.cc",
            "status": "modified",
            "additions": 27,
            "deletions": 2,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a9ac35b8f57f29b9f9e0c5d386f2fcac2c20c073/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fhost_offloader.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a9ac35b8f57f29b9f9e0c5d386f2fcac2c20c073/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fhost_offloader.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fhost_offloader.cc?ref=a9ac35b8f57f29b9f9e0c5d386f2fcac2c20c073",
            "patch": "@@ -144,8 +144,19 @@ bool HostOffloader::InstructionIsAllowedBetweenDsAndMoveToDevice(\n     return ShapeUtil::ReshapeIsBitcast(instruction->operand(0)->shape(),\n                                        instruction->shape());\n   }\n-  return instruction->opcode() == HloOpcode::kBitcast ||\n-         instruction->opcode() == HloOpcode::kCopy;\n+  if (instruction->opcode() == HloOpcode::kBitcast ||\n+      instruction->opcode() == HloOpcode::kCopy) {\n+    return true;\n+  }\n+  // Allow an annotation to sit inside a loop.\n+  if (instruction->opcode() == HloOpcode::kTuple ||\n+      instruction->opcode() == HloOpcode::kOptimizationBarrier ||\n+      instruction->opcode() == HloOpcode::kGetTupleElement ||\n+      instruction->opcode() == HloOpcode::kParameter ||\n+      instruction->opcode() == HloOpcode::kWhile) {\n+    return true;\n+  }\n+  return false;\n }\n \n absl::StatusOr<bool> HostOffloader::WalkDownHostMemoryOffloadPaths(\n@@ -1268,10 +1279,24 @@ absl::StatusOr<bool> HostOffloader::HandleDynamicUpdateSlices() {\n         operand_memory_space == Layout::kDefaultMemorySpace;\n     if (host_to_device) {\n       // This is only supported via host compute.\n+      if (dus->GetModule()\n+              ->config()\n+              .debug_options()\n+              .xla_disable_automatic_host_compute_offload()) {\n+        return absl::InvalidArgumentError(\n+            \"Automatic host compute offloading is disabled.\");\n+      }\n       host_offload_utils::SetHostComputeFrontendAttribute(*dus);\n       changed = true;\n     } else if (host_to_host) {\n       // Host to host. Execute as host compute. Also set as host memory space.\n+      if (dus->GetModule()\n+              ->config()\n+              .debug_options()\n+              .xla_disable_automatic_host_compute_offload()) {\n+        return absl::InvalidArgumentError(\n+            \"Automatic host compute offloading is disabled.\");\n+      }\n       host_offload_utils::SetHostComputeFrontendAttribute(*dus);\n       SetMemorySpace(dus->mutable_shape(), Layout::kHostMemorySpace);\n       changed = true;"
        },
        {
            "sha": "c49891e9b0302691cfd308539a6c43e26052ada7",
            "filename": "third_party/xla/xla/hlo/transforms/host_offloader_test.cc",
            "status": "modified",
            "additions": 45,
            "deletions": 0,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a9ac35b8f57f29b9f9e0c5d386f2fcac2c20c073/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fhost_offloader_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a9ac35b8f57f29b9f9e0c5d386f2fcac2c20c073/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fhost_offloader_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fhost_offloader_test.cc?ref=a9ac35b8f57f29b9f9e0c5d386f2fcac2c20c073",
            "patch": "@@ -44,6 +44,7 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"tsl/platform/statusor.h\"\n \n@@ -1076,6 +1077,50 @@ ENTRY main {\n   EXPECT_FALSE(HaveRemainingOffloadAnnotations(module.get()));\n }\n \n+TEST_F(HostOffloaderTest, DsWithMoveToDeviceInWhileBody) {\n+  const std::string& hlo_string = R\"(\n+HloModule my_module, entry_computation_layout={(f32[1024,2048]{1,0:T(8,128)S(5)})->f32[8,2048]{1,0:T(8,128)}}\n+while_body {\n+  param = (s32[], f32[8,2048]) parameter(0)\n+  current_iteration_index.0 = s32[] get-tuple-element(param), index=0\n+  gte.1 = f32[8,2048] get-tuple-element(param), index=1\n+  offload_custom_call = f32[8,2048] custom-call(gte.1), custom_call_target=\"MoveToDevice\"\n+  double = f32[8,2048] add(offload_custom_call, offload_custom_call)\n+  constant_1 = s32[] constant(1)\n+  incremented_index.0 = s32[] add(current_iteration_index.0, constant_1)\n+  ROOT tuple = (s32[], f32[8,2048]) tuple(incremented_index.0, double)\n+}\n+while_condition {\n+  param = (s32[], f32[8,2048]) parameter(0)\n+  current_iteration_index.0 = get-tuple-element(param), index=0\n+  constant_2 = s32[] constant(2)\n+  ROOT pred_result = pred[] compare(current_iteration_index.0, constant_2), direction=LT\n+}\n+ENTRY main {\n+  data_param = f32[1024,2048] parameter(0)\n+  constant = s32[] constant(0)\n+  ds = f32[8,2048] slice(data_param), slice={[0:8], [0:2048]}\n+  tuple = (s32[], f32[8,2048]) tuple(constant, ds)\n+  while = (s32[], f32[8,2048]) while(tuple), condition=while_condition, body=while_body\n+  ROOT gte = f32[8,2048] get-tuple-element(while), index=1\n+}\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(hlo_string));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, RunHostOffloader(module.get()));\n+  VLOG(1) << \"module after: \" << module->ToString();\n+\n+  EXPECT_TRUE(changed);\n+  // ds should be rewritten into a dynamic-slice.\n+  HloInstruction* ds = FindInstruction(module.get(), \"dynamic-slice\");\n+  EXPECT_NE(ds, nullptr);\n+  EXPECT_TRUE(ds->shape().layout().memory_space() ==\n+              Layout::kDefaultMemorySpace);\n+  EXPECT_FALSE(ds->has_frontend_attributes());\n+}\n+\n TEST_F(HostOffloaderTest, NoCopyWithOptBarrier) {\n   const std::string& hlo_string = R\"(\n HloModule my_module"
        },
        {
            "sha": "83d8314ded39572f66bc2175374c017f9e1ef2b9",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a9ac35b8f57f29b9f9e0c5d386f2fcac2c20c073/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a9ac35b8f57f29b9f9e0c5d386f2fcac2c20c073/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=a9ac35b8f57f29b9f9e0c5d386f2fcac2c20c073",
            "patch": "@@ -4800,6 +4800,7 @@ cc_library(\n         \"while_loop_invariant_code_motion.h\",\n     ],\n     deps = [\n+        \":memory_annotations_hdr\",\n         \":while_util\",\n         \"//xla:shape_util\",\n         \"//xla:util\","
        },
        {
            "sha": "c8d48876e0771f587737d705089b82114cc4e08d",
            "filename": "third_party/xla/xla/service/while_loop_invariant_code_motion.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a9ac35b8f57f29b9f9e0c5d386f2fcac2c20c073/third_party%2Fxla%2Fxla%2Fservice%2Fwhile_loop_invariant_code_motion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a9ac35b8f57f29b9f9e0c5d386f2fcac2c20c073/third_party%2Fxla%2Fxla%2Fservice%2Fwhile_loop_invariant_code_motion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fwhile_loop_invariant_code_motion.cc?ref=a9ac35b8f57f29b9f9e0c5d386f2fcac2c20c073",
            "patch": "@@ -37,6 +37,7 @@ limitations under the License.\n #include \"xla/hlo/transforms/simplifiers/tuple_simplifier.h\"\n #include \"xla/map_util.h\"\n #include \"xla/service/compile_time_cap.h\"\n+#include \"xla/service/memory_annotations.h\"\n #include \"xla/service/while_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n@@ -145,13 +146,20 @@ WhileLoopInvariantCodeMotion::TryHoistingInvariantInstructionsFromWhileBody(\n     return false;\n   }\n \n-  // LICM in the presence of domain instructions is complex, bail.\n   for (auto* instruction : while_body->MakeInstructionPostOrder()) {\n+    // LICM in the presence of domain instructions is complex, bail.\n     if (instruction->opcode() == HloOpcode::kDomain ||\n         instruction->IsCustomCall(\"SPMDFullToShardShape\") ||\n         instruction->IsCustomCall(\"SPMDShardShapeToFull\")) {\n       return false;\n     }\n+\n+    // Host offloading annotation should stay in its original position.\n+    if (instruction->IsCustomCall(std::vector<absl::string_view>{\n+            memory_annotations::kMoveToDeviceCustomCallTarget,\n+            memory_annotations::kMoveToHostCustomCallTarget})) {\n+      return false;\n+    }\n   }\n \n   // instructions_to_replace[i] is hoisted into a loop invariant instruction"
        }
    ],
    "stats": {
        "total": 86,
        "additions": 83,
        "deletions": 3
    }
}