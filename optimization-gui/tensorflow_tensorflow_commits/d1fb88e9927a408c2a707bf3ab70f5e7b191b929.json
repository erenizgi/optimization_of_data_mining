{
    "author": "hyeontaek",
    "message": "[IFRT] Deprecate `Sharding` methods that do not take `SingleDeviceShardSemantics`\n\nWe formally deprecate old-style `Sharding` methods that do not take a `SingleDeviceShardSemantics` argument. Supplying `SingleDeviceShardSemantics::kAllShards` to the new-style methods will give the behavior of the deprecated methods.\n\nPiperOrigin-RevId: 839975943",
    "sha": "d1fb88e9927a408c2a707bf3ab70f5e7b191b929",
    "files": [
        {
            "sha": "cd93ba17bbcdbf39024436810701bc3db947b7e3",
            "filename": "third_party/xla/xla/python/ifrt/mock.h",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1fb88e9927a408c2a707bf3ab70f5e7b191b929/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fmock.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1fb88e9927a408c2a707bf3ab70f5e7b191b929/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fmock.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fmock.h?ref=d1fb88e9927a408c2a707bf3ab70f5e7b191b929",
            "patch": "@@ -394,24 +394,17 @@ class MockSharding : public llvm::RTTIExtends<MockSharding, Sharding> {\n       : llvm::RTTIExtends<MockSharding, Sharding>(devices, memory_kind,\n                                                   is_fully_replicated) {}\n \n-  MOCK_METHOD((absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>>),\n-              Disassemble, (const Shape& shape), (const, final));\n   MOCK_METHOD((absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>>),\n               Disassemble,\n               (const Shape& shape,\n                SingleDeviceShardSemantics single_device_shard_semantics),\n               (const, final));\n-  MOCK_METHOD(\n-      (absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>>),\n-      Disassemble, (const DynamicShape& dynamic_shape), (const final));\n   MOCK_METHOD(\n       (absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>>),\n       Disassemble,\n       (const DynamicShape& dynamic_shape,\n        SingleDeviceShardSemantics single_device_shard_semantics),\n       (const final));\n-  MOCK_METHOD(absl::StatusOr<std::vector<IndexDomain>>, IndexDomains,\n-              (const Shape& shape), (const, final));\n   MOCK_METHOD(absl::StatusOr<std::vector<IndexDomain>>, IndexDomains,\n               (const Shape& shape,\n                SingleDeviceShardSemantics single_device_shard_semantics),"
        },
        {
            "sha": "b836ea0728cf3eb3e992da2ee7c1b005ffaa810e",
            "filename": "third_party/xla/xla/python/ifrt/sharding.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 88,
            "changes": 88,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1fb88e9927a408c2a707bf3ab70f5e7b191b929/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1fb88e9927a408c2a707bf3ab70f5e7b191b929/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding.cc?ref=d1fb88e9927a408c2a707bf3ab70f5e7b191b929",
            "patch": "@@ -247,12 +247,6 @@ SingleDeviceSharding::WithDeviceAssignment(\n                 memory_kind.value_or(memory_kind_));\n }\n \n-absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>>\n-SingleDeviceSharding::Disassemble(const Shape& shape) const {\n-  DCHECK(this);\n-  return Disassemble(shape, SingleDeviceShardSemantics::kAllShards);\n-}\n-\n absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>>\n SingleDeviceSharding::Disassemble(\n     const Shape& shape,\n@@ -268,11 +262,6 @@ SingleDeviceSharding::Disassemble(\n   return result;\n }\n \n-absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>>\n-SingleDeviceSharding::Disassemble(const DynamicShape& dynamic_shape) const {\n-  DCHECK(this);\n-  return Disassemble(dynamic_shape, SingleDeviceShardSemantics::kAllShards);\n-}\n absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>>\n SingleDeviceSharding::Disassemble(\n     const DynamicShape& dynamic_shape,\n@@ -289,12 +278,6 @@ SingleDeviceSharding::Disassemble(\n   return result;\n }\n \n-absl::StatusOr<std::vector<IndexDomain>> SingleDeviceSharding::IndexDomains(\n-    const Shape& shape) const {\n-  DCHECK(this);\n-  return IndexDomains(shape, SingleDeviceShardSemantics::kAllShards);\n-}\n-\n absl::StatusOr<std::vector<IndexDomain>> SingleDeviceSharding::IndexDomains(\n     const Shape& shape,\n     SingleDeviceShardSemantics single_device_shard_semantics) const {\n@@ -356,12 +339,6 @@ absl::StatusOr<std::unique_ptr<Sharding>> OpaqueSharding::WithDeviceAssignment(\n   return Create(devices.value_or(devices_), memory_kind.value_or(memory_kind_));\n }\n \n-absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>>\n-OpaqueSharding::Disassemble(const Shape& shape) const {\n-  DCHECK(this);\n-  return Disassemble(shape, SingleDeviceShardSemantics::kAllShards);\n-}\n-\n absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>>\n OpaqueSharding::Disassemble(\n     const Shape& shape,\n@@ -371,12 +348,6 @@ OpaqueSharding::Disassemble(\n       \"OpaqueSharding does not have shard shape information\");\n }\n \n-absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>>\n-OpaqueSharding::Disassemble(const DynamicShape& dynamic_shape) const {\n-  DCHECK(this);\n-  return Disassemble(dynamic_shape, SingleDeviceShardSemantics::kAllShards);\n-}\n-\n absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>>\n OpaqueSharding::Disassemble(\n     const DynamicShape& dynamic_shape,\n@@ -386,12 +357,6 @@ OpaqueSharding::Disassemble(\n       \"OpaqueSharding does not have shard shape information\");\n }\n \n-absl::StatusOr<std::vector<IndexDomain>> OpaqueSharding::IndexDomains(\n-    const Shape& shape) const {\n-  DCHECK(this);\n-  return IndexDomains(shape, SingleDeviceShardSemantics::kAllShards);\n-}\n-\n absl::StatusOr<std::vector<IndexDomain>> OpaqueSharding::IndexDomains(\n     const Shape& shape,\n     SingleDeviceShardSemantics single_device_shard_semantics) const {\n@@ -505,12 +470,6 @@ ConcreteSharding::WithDeviceAssignment(\n                 std::get<std::vector<DynamicShape>>(shard_shapes_));\n }\n \n-absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>>\n-ConcreteSharding::Disassemble(const Shape& shape) const {\n-  DCHECK(this);\n-  return Disassemble(shape, SingleDeviceShardSemantics::kAllShards);\n-}\n-\n absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>>\n ConcreteSharding::Disassemble(\n     const Shape& shape,\n@@ -560,12 +519,6 @@ ConcreteSharding::Disassemble(\n   return result;\n }\n \n-absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>>\n-ConcreteSharding::Disassemble(const DynamicShape& dynamic_shape) const {\n-  DCHECK(this);\n-  return Disassemble(dynamic_shape, SingleDeviceShardSemantics::kAllShards);\n-}\n-\n absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>>\n ConcreteSharding::Disassemble(\n     const DynamicShape& dynamic_shape,\n@@ -616,12 +569,6 @@ ConcreteSharding::Disassemble(\n   return result;\n }\n \n-absl::StatusOr<std::vector<IndexDomain>> ConcreteSharding::IndexDomains(\n-    const Shape& shape) const {\n-  DCHECK(this);\n-  return IndexDomains(shape, SingleDeviceShardSemantics::kAllShards);\n-}\n-\n absl::StatusOr<std::vector<IndexDomain>> ConcreteSharding::IndexDomains(\n     const Shape& shape,\n     SingleDeviceShardSemantics single_device_shard_semantics) const {\n@@ -737,12 +684,6 @@ ConcreteEvenSharding::WithDeviceAssignment(\n                 shape_, shard_shape_, is_fully_replicated_);\n }\n \n-absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>>\n-ConcreteEvenSharding::Disassemble(const Shape& shape) const {\n-  DCHECK(this);\n-  return Disassemble(shape, SingleDeviceShardSemantics::kAllShards);\n-}\n-\n absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>>\n ConcreteEvenSharding::Disassemble(\n     const Shape& shape,\n@@ -772,12 +713,6 @@ ConcreteEvenSharding::Disassemble(\n   return result;\n }\n \n-absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>>\n-ConcreteEvenSharding::Disassemble(const DynamicShape& dynamic_shape) const {\n-  DCHECK(this);\n-  return Disassemble(dynamic_shape, SingleDeviceShardSemantics::kAllShards);\n-}\n-\n absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>>\n ConcreteEvenSharding::Disassemble(\n     const DynamicShape& dynamic_shape,\n@@ -789,11 +724,6 @@ ConcreteEvenSharding::Disassemble(\n       dynamic_shape.DebugString());\n }\n \n-absl::StatusOr<std::vector<IndexDomain>> ConcreteEvenSharding::IndexDomains(\n-    const Shape& shape) const {\n-  DCHECK(this);\n-  return IndexDomains(shape, SingleDeviceShardSemantics::kAllShards);\n-}\n absl::StatusOr<std::vector<IndexDomain>> ConcreteEvenSharding::IndexDomains(\n     const Shape& shape,\n     SingleDeviceShardSemantics single_device_shard_semantics) const {\n@@ -841,12 +771,6 @@ ShardingParamSharding::ShardingParamSharding(ShardingParam sharding_param,\n           ComputeIsFullyReplicated(sharding_param)),\n       sharding_param_(sharding_param) {}\n \n-absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>>\n-ShardingParamSharding::Disassemble(const Shape& shape) const {\n-  DCHECK(this);\n-  return Disassemble(shape, SingleDeviceShardSemantics::kAllShards);\n-}\n-\n absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>>\n ShardingParamSharding::Disassemble(\n     const Shape& shape,\n@@ -920,12 +844,6 @@ ShardingParamSharding::WithDeviceAssignment(\n                 memory_kind.value_or(memory_kind_));\n }\n \n-absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>>\n-ShardingParamSharding::Disassemble(const DynamicShape& dynamic_shape) const {\n-  DCHECK(this);\n-  return Disassemble(dynamic_shape, SingleDeviceShardSemantics::kAllShards);\n-}\n-\n absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>>\n ShardingParamSharding::Disassemble(\n     const DynamicShape& dynamic_shape,\n@@ -937,12 +855,6 @@ ShardingParamSharding::Disassemble(\n       dynamic_shape.DebugString());\n }\n \n-absl::StatusOr<std::vector<IndexDomain>> ShardingParamSharding::IndexDomains(\n-    const Shape& shape) const {\n-  DCHECK(this);\n-  return IndexDomains(shape, SingleDeviceShardSemantics::kAllShards);\n-}\n-\n absl::StatusOr<std::vector<IndexDomain>> ShardingParamSharding::IndexDomains(\n     const Shape& shape,\n     SingleDeviceShardSemantics single_device_shard_semantics) const {"
        },
        {
            "sha": "c2c547da6664b1645fcb4ad24a386da5dc22a443",
            "filename": "third_party/xla/xla/python/ifrt/sharding.h",
            "status": "modified",
            "additions": 26,
            "deletions": 42,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1fb88e9927a408c2a707bf3ab70f5e7b191b929/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1fb88e9927a408c2a707bf3ab70f5e7b191b929/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fsharding.h?ref=d1fb88e9927a408c2a707bf3ab70f5e7b191b929",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include <variant>\n #include <vector>\n \n+#include \"absl/base/macros.h\"\n #include \"absl/base/nullability.h\"\n #include \"absl/hash/hash.h\"\n #include \"absl/log/check.h\"\n@@ -125,20 +126,22 @@ class Sharding : public llvm::RTTIExtends<Sharding, Serializable> {\n   // Breaks a shape up into per-device shapes and shardings. See\n   // Array::DisassembleIntoSingleDeviceArrays(). It may return an error if\n   // disassembly is unsupported.\n-  // TODO(hyeontaek): Replace this API with the version that takes\n-  // `SingleDeviceShardSemantics`.\n-  virtual absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>>\n-  Disassemble(const Shape& shape) const = 0;\n+  ABSL_DEPRECATE_AND_INLINE()\n+  absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>> Disassemble(\n+      const Shape& shape) const {\n+    return Disassemble(shape, SingleDeviceShardSemantics::kAllShards);\n+  }\n   virtual absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>>\n   Disassemble(\n       const Shape& shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const = 0;\n \n   // Variant of `Disassemble` that takes a dynamic shape.\n-  // TODO(hyeontaek): Replace this API with the version that takes\n-  // `SingleDeviceShardSemantics`.\n-  virtual absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>>\n-  Disassemble(const DynamicShape& dynamic_shape) const = 0;\n+  ABSL_DEPRECATE_AND_INLINE()\n+  absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>> Disassemble(\n+      const DynamicShape& dynamic_shape) const {\n+    return Disassemble(dynamic_shape, SingleDeviceShardSemantics::kAllShards);\n+  }\n   virtual absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>>\n   Disassemble(\n       const DynamicShape& dynamic_shape,\n@@ -150,10 +153,11 @@ class Sharding : public llvm::RTTIExtends<Sharding, Serializable> {\n   // fully replicated sharding would return a vector of `[IndexDomain(shape)] *\n   // devices().size()` if `single_device_shard_semantics ==\n   // SingleDeviceShardSemantics::kAllShards`.\n-  // TODO(hyeontaek): Replace this API with the version that takes\n-  // `SingleDeviceShardSemantics`.\n-  virtual absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n-      const Shape& shape) const = 0;\n+  ABSL_DEPRECATE_AND_INLINE()\n+  absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n+      const Shape& shape) const {\n+    return IndexDomains(shape, SingleDeviceShardSemantics::kAllShards);\n+  }\n   virtual absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n       const Shape& shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const = 0;\n@@ -249,20 +253,16 @@ class SingleDeviceSharding final\n       std::optional<DeviceListRef> devices,\n       std::optional<MemoryKind> memory_kind) const override;\n \n-  absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>> Disassemble(\n-      const Shape& shape) const override;\n+  using Sharding::Disassemble;\n   absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>> Disassemble(\n       const Shape& shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const override;\n \n-  absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>> Disassemble(\n-      const DynamicShape& dynamic_shape) const override;\n   absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>> Disassemble(\n       const DynamicShape& dynamic_shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const override;\n \n-  absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n-      const Shape& shape) const override;\n+  using Sharding::IndexDomains;\n   absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n       const Shape& shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const override;\n@@ -299,20 +299,16 @@ class OpaqueSharding : public llvm::RTTIExtends<OpaqueSharding, Sharding> {\n       std::optional<DeviceListRef> devices,\n       std::optional<MemoryKind> memory_kind) const override;\n \n-  absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>> Disassemble(\n-      const Shape& shape) const override;\n+  using Sharding::Disassemble;\n   absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>> Disassemble(\n       const Shape& shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const override;\n \n-  absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>> Disassemble(\n-      const DynamicShape& dynamic_shape) const override;\n   absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>> Disassemble(\n       const DynamicShape& dynamic_shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const override;\n \n-  absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n-      const Shape& shape) const override;\n+  using Sharding::IndexDomains;\n   absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n       const Shape& shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const override;\n@@ -396,20 +392,16 @@ class ConcreteSharding : public llvm::RTTIExtends<ConcreteSharding, Sharding> {\n       std::optional<DeviceListRef> devices,\n       std::optional<MemoryKind> memory_kind) const override;\n \n-  absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>> Disassemble(\n-      const Shape& shape) const override;\n+  using Sharding::Disassemble;\n   absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>> Disassemble(\n       const Shape& shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const override;\n \n-  absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>> Disassemble(\n-      const DynamicShape& dynamic_shape) const override;\n   absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>> Disassemble(\n       const DynamicShape& dynamic_shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const override;\n \n-  absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n-      const Shape& shape) const override;\n+  using Sharding::IndexDomains;\n   absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n       const Shape& shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const override;\n@@ -471,20 +463,16 @@ class ConcreteEvenSharding\n       std::optional<DeviceListRef> devices,\n       std::optional<MemoryKind> memory_kind) const override;\n \n-  absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>> Disassemble(\n-      const Shape& shape) const override;\n+  using Sharding::Disassemble;\n   absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>> Disassemble(\n       const Shape& shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const override;\n \n-  absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>> Disassemble(\n-      const DynamicShape& dynamic_shape) const override;\n   absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>> Disassemble(\n       const DynamicShape& dynamic_shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const override;\n \n-  absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n-      const Shape& shape) const override;\n+  using Sharding::IndexDomains;\n   absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n       const Shape& shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const override;\n@@ -523,20 +511,16 @@ class ShardingParamSharding\n       std::optional<DeviceListRef> devices,\n       std::optional<MemoryKind> memory_kind) const override;\n \n-  absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>> Disassemble(\n-      const Shape& shape) const override;\n+  using Sharding::Disassemble;\n   absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>> Disassemble(\n       const Shape& shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const override;\n \n-  absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>> Disassemble(\n-      const DynamicShape& dynamic_shape) const override;\n   absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>> Disassemble(\n       const DynamicShape& dynamic_shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const override;\n \n-  absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n-      const Shape& shape) const override;\n+  using Sharding::IndexDomains;\n   absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n       const Shape& shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const override;"
        },
        {
            "sha": "bed0fa50775d167a81f94011d14508c22654508f",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/xla_sharding.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 19,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1fb88e9927a408c2a707bf3ab70f5e7b191b929/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1fb88e9927a408c2a707bf3ab70f5e7b191b929/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding.cc?ref=d1fb88e9927a408c2a707bf3ab70f5e7b191b929",
            "patch": "@@ -181,11 +181,6 @@ absl::StatusOr<std::unique_ptr<Sharding>> HloSharding::WithDeviceAssignment(\n   return Create(devices.value_or(devices_), memory_kind.value_or(memory_kind_),\n                 xla_hlo_sharding_);\n }\n-absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>>\n-HloSharding::Disassemble(const Shape& shape) const {\n-  DCHECK(this);\n-  return Disassemble(shape, SingleDeviceShardSemantics::kAllShards);\n-}\n \n absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>>\n HloSharding::Disassemble(\n@@ -242,8 +237,9 @@ HloSharding::Disassemble(\n     return result;\n   }\n   // Slow path that uses `IndexDomains()` to handle uneven sharding.\n-  TF_ASSIGN_OR_RETURN(std::vector<IndexDomain> index_domains,\n-                      IndexDomains(shape));\n+  TF_ASSIGN_OR_RETURN(\n+      std::vector<IndexDomain> index_domains,\n+      IndexDomains(shape, SingleDeviceShardSemantics::kAllShards));\n   CHECK_EQ(index_domains.size(), devices_->size());\n   std::vector<std::pair<Shape, ShardingRef>> result;\n   if (single_device_shard_semantics == SingleDeviceShardSemantics::kAllShards) {\n@@ -264,12 +260,6 @@ HloSharding::Disassemble(\n   return result;\n }\n \n-absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>>\n-HloSharding::Disassemble(const DynamicShape& dynamic_shape) const {\n-  DCHECK(this);\n-  return Disassemble(dynamic_shape, SingleDeviceShardSemantics::kAllShards);\n-}\n-\n absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>>\n HloSharding::Disassemble(\n     const DynamicShape& dynamic_shape,\n@@ -281,12 +271,6 @@ HloSharding::Disassemble(\n       dynamic_shape.DebugString());\n }\n \n-absl::StatusOr<std::vector<IndexDomain>> HloSharding::IndexDomains(\n-    const Shape& shape) const {\n-  DCHECK(this);\n-  return IndexDomains(shape, SingleDeviceShardSemantics::kAllShards);\n-}\n-\n absl::StatusOr<std::vector<IndexDomain>> HloSharding::IndexDomains(\n     const Shape& shape,\n     SingleDeviceShardSemantics single_device_shard_semantics) const {"
        },
        {
            "sha": "086df5920e2107f59192e845208e9143556ee07e",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/xla_sharding.h",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d1fb88e9927a408c2a707bf3ab70f5e7b191b929/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d1fb88e9927a408c2a707bf3ab70f5e7b191b929/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding.h?ref=d1fb88e9927a408c2a707bf3ab70f5e7b191b929",
            "patch": "@@ -74,20 +74,16 @@ class HloSharding final\n       std::optional<DeviceListRef> devices,\n       std::optional<MemoryKind> memory_kind) const override;\n \n-  absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>> Disassemble(\n-      const Shape& shape) const override;\n+  using Sharding::Disassemble;\n   absl::StatusOr<std::vector<std::pair<Shape, ShardingRef>>> Disassemble(\n       const Shape& shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const override;\n \n-  absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>> Disassemble(\n-      const DynamicShape& dynamic_shape) const override;\n   absl::StatusOr<std::vector<std::pair<DynamicShape, ShardingRef>>> Disassemble(\n       const DynamicShape& dynamic_shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const override;\n \n-  absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n-      const Shape& shape) const override;\n+  using Sharding::IndexDomains;\n   absl::StatusOr<std::vector<IndexDomain>> IndexDomains(\n       const Shape& shape,\n       SingleDeviceShardSemantics single_device_shard_semantics) const override;"
        }
    ],
    "stats": {
        "total": 193,
        "additions": 31,
        "deletions": 162
    }
}