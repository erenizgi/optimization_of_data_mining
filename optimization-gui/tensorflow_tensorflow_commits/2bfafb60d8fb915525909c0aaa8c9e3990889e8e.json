{
    "author": "khasanovaa",
    "message": "Remove `CommandBufferScheduling` HLO pass.\n\nThe `CommandBufferScheduling` HLO pass is removed, as the command buffer conversion is beeing handled by the `CommandBufferConversionPass` in `GpuExecutable` for more than 3 months now. The flag `xla_gpu_experimental_enable_command_buffer_on_thunks` is also deprecated.\n\nPiperOrigin-RevId: 840245640",
    "sha": "2bfafb60d8fb915525909c0aaa8c9e3990889e8e",
    "files": [
        {
            "sha": "c435337dda368a169706ac7b7523c0134b608e12",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2bfafb60d8fb915525909c0aaa8c9e3990889e8e/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2bfafb60d8fb915525909c0aaa8c9e3990889e8e/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=2bfafb60d8fb915525909c0aaa8c9e3990889e8e",
            "patch": "@@ -460,7 +460,6 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_gpu_experimental_enable_split_k_rewrite(false);\n   opts.set_xla_gpu_experimental_enable_triton_tma(true);\n   opts.set_xla_gpu_experimental_enable_triton_warp_specialization(false);\n-  opts.set_xla_gpu_experimental_enable_command_buffer_on_thunks(true);\n   opts.set_xla_detect_unstable_reductions(DebugOptions::DETECTION_MODE_NONE);\n   opts.set_xla_detect_unstable_reductions_post_optimizations(\n       DebugOptions::DETECTION_MODE_NONE);\n@@ -2635,14 +2634,6 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n               set_xla_gpu_experimental_enable_triton_warp_specialization),\n       debug_options->xla_gpu_experimental_enable_triton_warp_specialization(),\n       \"Enable Triton's auto warp specialization feature where applicable.\"));\n-  flag_list->push_back(tsl::Flag(\n-      \"xla_gpu_experimental_enable_command_buffer_on_thunks\",\n-      bool_setter_for(\n-          &DebugOptions::\n-              set_xla_gpu_experimental_enable_command_buffer_on_thunks),\n-      debug_options->xla_gpu_experimental_enable_command_buffer_on_thunks(),\n-      \"Enables an experimental feature for command buffer conversion on \"\n-      \"thunks.\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_gpu_experimental_use_autotuner_pass\",\n       bool_setter_for("
        },
        {
            "sha": "b3f7f1b4820b5953e4f885454226e93226ff375e",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2bfafb60d8fb915525909c0aaa8c9e3990889e8e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2bfafb60d8fb915525909c0aaa8c9e3990889e8e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=2bfafb60d8fb915525909c0aaa8c9e3990889e8e",
            "patch": "@@ -1763,7 +1763,6 @@ cc_library(\n         \"//xla/service/gpu/transforms:algebraic_simplifier\",\n         \"//xla/service/gpu/transforms:algorithm_checker\",\n         \"//xla/service/gpu/transforms:async_wrapper\",\n-        \"//xla/service/gpu/transforms:command_buffer_scheduling\",\n         \"//xla/service/gpu/transforms:composite_rewriter\",\n         \"//xla/service/gpu/transforms:conv_rewriter\",\n         \"//xla/service/gpu/transforms:cudnn_custom_call_converter\","
        },
        {
            "sha": "b4b717b4f7c40afb83f39bcc680cbb6144e244d2",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2bfafb60d8fb915525909c0aaa8c9e3990889e8e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2bfafb60d8fb915525909c0aaa8c9e3990889e8e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=2bfafb60d8fb915525909c0aaa8c9e3990889e8e",
            "patch": "@@ -219,7 +219,6 @@ limitations under the License.\n #include \"xla/service/gpu/transforms/collectives/convert_async_collectives_to_sync.h\"\n #include \"xla/service/gpu/transforms/collectives/gpu_collective_combiner_utils.h\"\n #include \"xla/service/gpu/transforms/collectives/reduce_scatter_combiner.h\"\n-#include \"xla/service/gpu/transforms/command_buffer_scheduling.h\"\n #include \"xla/service/gpu/transforms/composite_rewriter.h\"\n #include \"xla/service/gpu/transforms/conv_rewriter.h\"\n #include \"xla/service/gpu/transforms/cudnn_custom_call_converter.h\"\n@@ -2900,17 +2899,6 @@ absl::Status GpuCompiler::RunPostSchedulingPipelines(\n         gpu_device_info, ShapeSizeBytesFunction(), &mlir_context_));\n   }\n \n-  // Pipeline with passes which wrap a scheduled module into command buffers.\n-  {\n-    if (!module->config()\n-             .debug_options()\n-             .xla_gpu_experimental_enable_command_buffer_on_thunks()) {\n-      HloPassPipeline& pipeline =\n-          main_pipeline.AddPass<HloPassPipeline>(\"command-buffer-scheduling\");\n-      pipeline.AddPass<CommandBufferScheduling>(gpu_device_info);\n-    }\n-  }\n-\n   // Sanitize constant names. This is in its own pipeline to ensure it always\n   // runs, as the preceding pipeline is conditional.\n   {"
        },
        {
            "sha": "7d5f568bbdbddbd5e980f070102859c74523b3f3",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2bfafb60d8fb915525909c0aaa8c9e3990889e8e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2bfafb60d8fb915525909c0aaa8c9e3990889e8e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=2bfafb60d8fb915525909c0aaa8c9e3990889e8e",
            "patch": "@@ -2139,7 +2139,6 @@ ENTRY main {\n   auto hlo_module = ParseAndReturnVerifiedModule(hlo_text).value();\n \n   DebugOptions debug_options = GetDebugOptionsForTest();\n-  debug_options.set_xla_gpu_experimental_enable_command_buffer_on_thunks(true);\n   debug_options.clear_xla_gpu_enable_command_buffer();\n   debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n   debug_options.set_xla_gpu_graph_min_graph_size(1);"
        },
        {
            "sha": "54ef920750c89d490ab78bdd903fb0f2ef527f1b",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2bfafb60d8fb915525909c0aaa8c9e3990889e8e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2bfafb60d8fb915525909c0aaa8c9e3990889e8e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=2bfafb60d8fb915525909c0aaa8c9e3990889e8e",
            "patch": "@@ -201,10 +201,9 @@ static absl::Status RunThunkPasses(const DebugOptions& debug_options,\n     pipeline.AddPass(std::make_unique<ThunkBufferDebugPass>(\n         ThunkBufferDebugPass::Mode::kFloatChecker));\n   }\n-  if (debug_options.xla_gpu_experimental_enable_command_buffer_on_thunks()) {\n-    pipeline.AddPass(std::make_unique<CommandBufferConversionPass>(\n-        hlo_module ? hlo_module->name() : \"Anonymous\"));\n-  }\n+  pipeline.AddPass(std::make_unique<CommandBufferConversionPass>(\n+      hlo_module ? hlo_module->name() : \"Anonymous\"));\n+\n   TF_ASSIGN_OR_RETURN(bool changed,\n                       pipeline.Run(root_thunk, debug_options, hlo_module,\n                                    device_info, allocator));"
        },
        {
            "sha": "1d5d68823b970f908b6955465b7dfce72e3e1ceb",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2bfafb60d8fb915525909c0aaa8c9e3990889e8e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2bfafb60d8fb915525909c0aaa8c9e3990889e8e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc?ref=2bfafb60d8fb915525909c0aaa8c9e3990889e8e",
            "patch": "@@ -138,7 +138,6 @@ TEST(GpuExecutableTest, RunThunkPasses) {\n       tsl::testing::TemporaryDirectory::CreateForCurrentTestcase());\n   DebugOptions debug_options = GetDebugOptionsFromFlags();\n   debug_options.set_xla_dump_to(dump_dir.path());\n-  debug_options.set_xla_gpu_experimental_enable_command_buffer_on_thunks(true);\n   debug_options.set_xla_gpu_graph_min_graph_size(1);\n   debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n "
        },
        {
            "sha": "a254f99208275b700bc33e3741fb70509770904b",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 66,
            "changes": 66,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2bfafb60d8fb915525909c0aaa8c9e3990889e8e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2bfafb60d8fb915525909c0aaa8c9e3990889e8e/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=2bfafb60d8fb915525909c0aaa8c9e3990889e8e",
            "patch": "@@ -359,72 +359,6 @@ xla_test(\n     ],\n )\n \n-cc_library(\n-    name = \"command_buffer_scheduling\",\n-    srcs = [\"command_buffer_scheduling.cc\"],\n-    hdrs = [\"command_buffer_scheduling.h\"],\n-    deps = [\n-        \"//xla:shape_util\",\n-        \"//xla:util\",\n-        \"//xla:xla_proto_cc\",\n-        \"//xla/ffi:ffi_api\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/pass:hlo_pass\",\n-        \"//xla/hlo/transforms/simplifiers:computation_canonicalizers\",\n-        \"//xla/hlo/utils:hlo_longest_prefix\",\n-        \"//xla/hlo/utils:hlo_traversal\",\n-        \"//xla/service/gpu:backend_configs_cc\",\n-        \"//xla/service/gpu:cublas_cudnn\",\n-        \"//xla/service/gpu:hlo_fusion_analysis\",\n-        \"//xla/service/gpu:ir_emission_utils\",\n-        \"//xla/stream_executor:device_description\",\n-        \"//xla/stream_executor:semantic_version\",\n-        \"//xla/stream_executor/rocm:rocm_compute_capability\",\n-        \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/algorithm:container\",\n-        \"@com_google_absl//absl/container:flat_hash_map\",\n-        \"@com_google_absl//absl/container:flat_hash_set\",\n-        \"@com_google_absl//absl/container:inlined_vector\",\n-        \"@com_google_absl//absl/log\",\n-        \"@com_google_absl//absl/log:check\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings\",\n-        \"@com_google_absl//absl/types:span\",\n-    ],\n-)\n-\n-xla_test(\n-    name = \"command_buffer_scheduling_test\",\n-    srcs = [\"command_buffer_scheduling_test.cc\"],\n-    backends = [\n-        \"cpu\",\n-        \"gpu\",\n-    ],\n-    deps = [\n-        \":command_buffer_scheduling\",\n-        \"//xla:xla_proto_cc\",\n-        \"//xla/backends/gpu/runtime:thunk\",\n-        \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/parser:hlo_parser\",\n-        \"//xla/hlo/testlib:filecheck\",\n-        \"//xla/hlo/testlib:verified_hlo_module\",\n-        \"//xla/service:executable\",\n-        \"//xla/service:hlo_module_config\",\n-        \"//xla/service:hlo_runner_interface\",\n-        \"//xla/service/gpu:gpu_device_info_for_tests\",\n-        \"//xla/service/gpu:gpu_executable\",\n-        \"//xla/stream_executor:device_description\",\n-        \"//xla/tests:hlo_test_base\",\n-        \"//xla/tsl/lib/core:status_test_util\",\n-        \"//xla/tsl/platform:status\",\n-        \"//xla/tsl/platform:statusor\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_googletest//:gtest_main\",\n-    ],\n-)\n-\n cc_library(\n     name = \"conv_padding_legalization\",\n     srcs = [\"conv_padding_legalization.cc\"],"
        },
        {
            "sha": "236ee1dd1a5d17eceee1f163cecbcbcc9063f687",
            "filename": "third_party/xla/xla/service/gpu/transforms/command_buffer_scheduling.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 999,
            "changes": 999,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6ff96032210ee64793439869c7726f7a7ad2360a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6ff96032210ee64793439869c7726f7a7ad2360a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.cc?ref=6ff96032210ee64793439869c7726f7a7ad2360a",
            "patch": "@@ -1,999 +0,0 @@\n-/* Copyright 2023 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/service/gpu/transforms/command_buffer_scheduling.h\"\n-\n-#include <algorithm>\n-#include <cstddef>\n-#include <cstdint>\n-#include <iterator>\n-#include <memory>\n-#include <string>\n-#include <utility>\n-#include <vector>\n-\n-#include \"absl/algorithm/container.h\"\n-#include \"absl/container/flat_hash_map.h\"\n-#include \"absl/container/flat_hash_set.h\"\n-#include \"absl/container/inlined_vector.h\"\n-#include \"absl/log/check.h\"\n-#include \"absl/log/log.h\"\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/str_cat.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"absl/types/span.h\"\n-#include \"xla/ffi/ffi_api.h\"\n-#include \"xla/hlo/ir/hlo_casting_utils.h\"\n-#include \"xla/hlo/ir/hlo_clone_context.h\"\n-#include \"xla/hlo/ir/hlo_computation.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_instructions.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/hlo/ir/hlo_schedule.h\"\n-#include \"xla/hlo/transforms/simplifiers/computation_canonicalizers.h\"\n-#include \"xla/hlo/utils/hlo_longest_prefix.h\"\n-#include \"xla/hlo/utils/hlo_traversal.h\"\n-#include \"xla/service/gpu/backend_configs.pb.h\"\n-#include \"xla/service/gpu/cublas_cudnn.h\"\n-#include \"xla/service/gpu/hlo_fusion_analysis.h\"\n-#include \"xla/service/gpu/ir_emission_utils.h\"\n-#include \"xla/shape.h\"\n-#include \"xla/shape_util.h\"\n-#include \"xla/stream_executor/device_description.h\"\n-#include \"xla/stream_executor/rocm/rocm_compute_capability.h\"\n-#include \"xla/stream_executor/semantic_version.h\"\n-#include \"xla/tsl/platform/errors.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/util.h\"\n-\n-namespace xla::gpu {\n-\n-using CommandBuffer = CommandBufferScheduling::CommandBuffer;\n-using CommandBufferConfig = CommandBufferScheduling::CommandBufferConfig;\n-using ::xla::hlo_longest_prefix::GetLongestOpNamePrefix;\n-\n-// Returns true if HLO computation can be executed as a command buffer.\n-static bool IsCommand(const HloComputation* computation,\n-                      const CommandBufferConfig& config);\n-\n-//===----------------------------------------------------------------------===//\n-// No-op HLO operations.\n-//===----------------------------------------------------------------------===//\n-\n-// Some of the HLO operations do not have corresponding operations at run time\n-// and they can be safely wrapped into command buffers together with load\n-// bearing commands.\n-\n-static bool IsConstant(const HloInstruction* hlo) {\n-  return HloPredicateIsOp<HloOpcode::kConstant>(hlo);\n-}\n-\n-static bool IsParameter(const HloInstruction* hlo) {\n-  return HloPredicateIsOp<HloOpcode::kParameter>(hlo);\n-}\n-\n-// Returns true if instruction is no-op at run time and doesn't have a\n-// corresponding Thunk or Command (metadata only operation).\n-static bool IsNoOp(const HloInstruction* hlo) {\n-  return HloPredicateIsOp<HloOpcode::kBitcast, HloOpcode::kTuple,\n-                          HloOpcode::kGetTupleElement>(hlo);\n-};\n-\n-//===----------------------------------------------------------------------===//\n-// Asynchronous HLO operations mapped to commands.\n-//===----------------------------------------------------------------------===//\n-\n-// Asynchronous HLO operations can be wrapped into command buffers only when\n-// both start and done operations can be put into the same command buffer.\n-// Command buffer semantics implies that when command buffer execution\n-// completes, all recorded commands are also completed, which means that if\n-// done operation is not part of the same command buffer, we would change the\n-// execution semantics and create additional synchronization point.\n-\n-static bool AsyncStartOrDoneCommandIsSupported(\n-    const HloInstruction* hlo, const CommandBufferConfig& config) {\n-  CHECK(hlo->opcode() == HloOpcode::kAsyncStart ||\n-        hlo->opcode() == HloOpcode::kAsyncDone);\n-\n-  if (IsCublasGemm(*hlo->async_wrapped_instruction())) {\n-    return config.enabled_commands.contains(DebugOptions::CUBLAS);\n-  }\n-\n-  if (hlo->async_wrapped_opcode() == HloOpcode::kFusion) {\n-    // We don't currently support dynamic memcpy fusions in command buffers.\n-    if (IsGpuFusionKind(*hlo->async_wrapped_instruction(),\n-                        kDynamicMemcpyFusionKind)) {\n-      return config.enabled_commands.contains(\n-          DebugOptions::DYNAMIC_SLICE_COPY_FUSION);\n-    }\n-\n-    // We currently only support static address computations in command\n-    // buffers.\n-    if (IsDynamicSliceFusion(hlo->async_wrapped_instruction())) {\n-      bool is_static_ds_fusion =\n-          GetCustomFusionConfigName(hlo->async_wrapped_instruction()) ==\n-          kDynamicSliceFusionWithStaticAddressComputationConfigName;\n-      return is_static_ds_fusion && config.enabled_commands.contains(\n-                                        DebugOptions::DYNAMIC_SLICE_FUSION);\n-    }\n-\n-    return config.enabled_commands.contains(DebugOptions::FUSION);\n-  }\n-\n-  if (hlo->async_wrapped_opcode() == HloOpcode::kReduceScatter ||\n-      hlo->async_wrapped_opcode() == HloOpcode::kAllToAll) {\n-    return config.enabled_commands.contains(DebugOptions::COLLECTIVES);\n-  }\n-\n-  return false;\n-}\n-\n-static bool IsAsyncStartCommand(const HloInstruction* hlo,\n-                                const CommandBufferConfig& config) {\n-  if (HloPredicateIsOp<HloOpcode::kAllReduceStart, HloOpcode::kAllGatherStart>(\n-          hlo)) {\n-    return config.enabled_commands.contains(DebugOptions::COLLECTIVES);\n-  }\n-\n-  if (HloPredicateIsOp<HloOpcode::kAsyncStart>(hlo)) {\n-    return AsyncStartOrDoneCommandIsSupported(hlo, config);\n-  }\n-\n-  if (HloPredicateIsOp<HloOpcode::kReduceScatter, HloOpcode::kAllToAll>(hlo)) {\n-    return config.enabled_commands.contains(DebugOptions::COLLECTIVES);\n-  }\n-\n-  return false;\n-}\n-\n-static bool IsAsyncDoneCommand(const HloInstruction* hlo,\n-                               const CommandBufferConfig& config) {\n-  if (HloPredicateIsOp<HloOpcode::kAllReduceDone, HloOpcode::kAllGatherDone>(\n-          hlo)) {\n-    return config.enabled_commands.contains(DebugOptions::COLLECTIVES);\n-  }\n-\n-  if (HloPredicateIsOp<HloOpcode::kAsyncDone>(hlo)) {\n-    return AsyncStartOrDoneCommandIsSupported(hlo, config);\n-  }\n-\n-  return false;\n-}\n-\n-// Finds an async-done HLO operation corresponding on an async-start one.\n-static HloInstruction* FindAsyncDoneCommand(const HloInstruction* start) {\n-  if (HloPredicateIsOp<HloOpcode::kAllReduceStart, HloOpcode::kAllGatherStart>(\n-          start)) {\n-    CHECK(start->users().size() == 1);  // NOLINT, checked by HLO verifier\n-    return start->users().front();\n-  }\n-  if (HloPredicateIsOp<HloOpcode::kAsyncStart>(start)) {\n-    return start->async_chain_done();\n-  }\n-\n-  return nullptr;\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// Synchronous HLO operations mapped to commands.\n-//===----------------------------------------------------------------------===//\n-\n-// Synchronous HLO operations can be wrapped into command buffers when they have\n-// a corresponding commands.\n-\n-// This is a template to define pattern matching functions for HLO instructions\n-// that do not have a corresponding class for them.\n-template <HloOpcode op>\n-static bool IsCommand(const HloInstruction*, const CommandBufferConfig&);\n-\n-// While loops can be executed inside command buffers only if condition and body\n-// regions can be executed as command buffers.\n-template <>\n-bool IsCommand<HloOpcode::kWhile>(const HloInstruction* hlo,\n-                                  const CommandBufferConfig& config) {\n-  return config.enabled_commands.contains(DebugOptions::WHILE) &&\n-         IsCommand(hlo->while_body(), config) &&\n-         IsCommand(hlo->while_condition(), config);\n-}\n-\n-// Conditional can be executed inside command buffers only if all regions of its\n-// branches can be executed as command buffers.\n-template <>\n-bool IsCommand<HloOpcode::kConditional>(const HloInstruction* hlo,\n-                                        const CommandBufferConfig& config) {\n-  return config.enabled_commands.contains(DebugOptions::CONDITIONAL) &&\n-         absl::c_all_of(hlo->branch_computations(),\n-                        [&](const HloComputation* comp) {\n-                          return IsCommand(comp, config);\n-                        });\n-}\n-\n-static bool IsCommand(const HloCustomCallInstruction* hlo,\n-                      const CommandBufferConfig& config) {\n-  // cuBLAS gemms represented in the HLO as custom call instructions.\n-  if (config.enabled_commands.contains(DebugOptions::CUBLAS) &&\n-      IsLegacyCublasMatmul(*hlo)) {\n-    return true;\n-  }\n-\n-  if (config.enabled_commands.contains(DebugOptions::CUBLASLT) &&\n-      (IsCublasLtMatmul(*hlo) || IsCublasLtMatmulF8(*hlo))) {\n-    return true;\n-  }\n-\n-  if (config.enabled_commands.contains(DebugOptions::CUDNN) &&\n-      IsCustomCallToBlockScaledDot(*hlo)) {\n-    VLOG(3) << \"Recording BlockScaledDot, target \" << hlo->custom_call_target()\n-            << \" into command buffer.\";\n-    return true;\n-  }\n-\n-  if (config.enabled_commands.contains(DebugOptions::CUDNN) &&\n-      IsCustomCallTofMHA(*hlo)) {\n-    VLOG(3) << \"Recording FusedMHA, target \" << hlo->custom_call_target()\n-            << \" into command buffer.\";\n-    return true;\n-  }\n-\n-  if (!config.enabled_commands.contains(DebugOptions::CUSTOM_CALL)) {\n-    return false;\n-  }\n-\n-  // Check if FFI handler is compatible with command buffers.\n-  auto registration = ffi::FindHandler(hlo->custom_call_target(), \"gpu\");\n-  return registration.ok()\n-             ? ffi::IsCommandBufferCompatible(registration->metadata)\n-             : false;\n-}\n-\n-static bool IsCommand(const HloInstruction* hlo,\n-                      const CommandBufferConfig& config) {\n-  if (auto* fusion = DynCast<HloFusionInstruction>(hlo)) {\n-    auto gpu_config = fusion->backend_config<GpuBackendConfig>();\n-    const FusionBackendConfig& backend_config =\n-        gpu_config->fusion_backend_config();\n-    if (backend_config.kind() == kCuDnnFusionKind) {\n-      return config.enabled_commands.contains(DebugOptions::CUDNN);\n-    }\n-    if (IsGpuFusionKind(*fusion, kDynamicMemcpyFusionKind)) {\n-      return config.enabled_commands.contains(\n-          DebugOptions::DYNAMIC_SLICE_COPY_FUSION);\n-    }\n-    if (IsDynamicSliceFusion(fusion)) {\n-      auto fusion_analysis =\n-          HloFusionAnalysis::Create(*hlo, config.device_description);\n-      const HloFusionAdaptor& adaptor = fusion_analysis.fusion();\n-      auto hero_adaptor =\n-          HloBfsFindIf(adaptor.GetRoots(), adaptor, [](auto node) {\n-            return node.opcode() == HloOpcode::kCustomCall ||\n-                   node.opcode() == HloOpcode::kReduceScatter;\n-          });\n-      const HloInstruction* hero = &hero_adaptor->instruction();\n-\n-      const absl::string_view& config_name =\n-          backend_config.custom_fusion_config().name();\n-      if (config_name ==\n-          kDynamicSliceFusionWithStaticAddressComputationConfigName) {\n-        return IsCommand(hero, config) || IsAsyncStartCommand(hero, config);\n-      }\n-      // DynamicSliceFusionRewriter currently only rewrites for dynamic slice\n-      // fusion with constant or loop iteration offset values, which are all\n-      // supported by command buffer.\n-      return (config.enabled_commands.contains(\n-                  DebugOptions::DYNAMIC_SLICE_FUSION) &&\n-              (IsCommand(hero, config) || IsAsyncStartCommand(hero, config)));\n-    }\n-\n-    // Cuda has a bug that when the cuda kernel's parameter size is larger than\n-    // 4KB, then cudaGraphAddKernelNode will have segment fault, we disable the\n-    // command buffer lowering for kernels which has over 512 parameters.\n-    // TODO(shawnw): remove this when cuda driver has release a fix.\n-    int64_t total_args = fusion->operands().size();\n-    ShapeUtil::ForEachLeafShape(\n-        fusion->shape(), [&](const Shape& subshape, const ShapeIndex& index) {\n-          if (subshape.IsArray()) {\n-            total_args++;\n-          }\n-        });\n-    if (total_args > 512) {\n-      // shared memory allocation needs a pointer inside kernel's packed\n-      // arguments, and PackKernelArgs will round to 1024 args for the kernel\n-      // has arg count between 512 and 1024.\n-      VLOG(2) << \"disable fusion kernel due to large argument count (>512)\";\n-      return false;\n-    }\n-\n-    return config.enabled_commands.contains(DebugOptions::FUSION);\n-  }\n-\n-  if (DynCast<HloSortInstruction>(hlo)) {\n-    return config.enabled_commands.contains(DebugOptions::FUSION);\n-  }\n-\n-  if (HloPredicateIsOp<HloOpcode::kCopy>(hlo)) {\n-    return config.enabled_commands.contains(DebugOptions::FUSION);\n-  }\n-\n-  if (HloPredicateIsOp<HloOpcode::kPartitionId, HloOpcode::kReplicaId>(hlo)) {\n-    return config.enabled_commands.contains(DebugOptions::FUSION);\n-  }\n-\n-  if (auto* custom_call = DynCast<HloCustomCallInstruction>(hlo)) {\n-    return IsCommand(custom_call, config);\n-  }\n-\n-  if (HloPredicateIsOp<HloOpcode::kWhile>(hlo)) {\n-    return IsCommand<HloOpcode::kWhile>(hlo, config);\n-  }\n-\n-  if (HloPredicateIsOp<HloOpcode::kConditional>(hlo)) {\n-    return IsCommand<HloOpcode::kConditional>(hlo, config);\n-  }\n-\n-  return false;\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// HLO computations mapped to command buffers.\n-//===----------------------------------------------------------------------===//\n-\n-// Returns true if HLO computation can be executed as a command buffer.\n-static bool IsCommand(const HloComputation* computation,\n-                      const CommandBufferConfig& config) {\n-  return absl::c_all_of(\n-      computation->instructions(), [&](const HloInstruction* inst) {\n-        return IsNoOp(inst) || IsConstant(inst) || IsParameter(inst) ||\n-               IsCommand(inst, config) || IsAsyncStartCommand(inst, config) ||\n-               IsAsyncDoneCommand(inst, config);\n-      });\n-}\n-\n-//===----------------------------------------------------------------------===//\n-\n-static void RemoveTrailingNoOps(HloInstructionSequence& seq) {\n-  std::vector<HloInstruction*> instructions = seq.instructions();\n-  for (int i = instructions.size() - 1; i >= 0; i--) {\n-    if (HloInstruction* inst = instructions[i]; IsNoOp(inst)) {\n-      seq.remove_instruction(inst);\n-    } else {\n-      break;\n-    }\n-  }\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// Discovering sequences of compatible Hlo instructions\n-//===----------------------------------------------------------------------===//\n-\n-// The input is a scheduled sequence of instructions. This function collects\n-// subsequences that will be extracted as command buffers.\n-std::vector<HloInstructionSequence>\n-CommandBufferScheduling::CollectCommandBufferSequences(\n-    const HloInstructionSequence schedule, const CommandBufferConfig& config,\n-    int32_t min_num_commands) {\n-  std::vector<HloInstructionSequence> sequences;\n-\n-  HloInstructionSequence current_seq;\n-  int64_t num_commands_in_current_seq = 0;\n-\n-  // Adds `current_seq` to `sequences` if it has enough commands in it.\n-  auto collect_current_seq = [&]() {\n-    if (num_commands_in_current_seq >= std::max(1, min_num_commands)) {\n-      RemoveTrailingNoOps(current_seq);\n-      sequences.push_back(std::move(current_seq));\n-    }\n-    current_seq = HloInstructionSequence();\n-    num_commands_in_current_seq = 0;\n-  };\n-\n-  auto& instructions = schedule.instructions();\n-\n-  // we currently require that when lowering DynamicSliceFusion, the offset\n-  // value should not come from the output of operators that are already\n-  // captured in command buffer.\n-  auto check_dynamic_slice_operand_not_from_seq =\n-      [&](const HloInstructionSequence& seq, const HloInstruction* inst) {\n-        if (!config.enabled_commands.contains(\n-                DebugOptions::DYNAMIC_SLICE_FUSION)) {\n-          return true;\n-        }\n-        const auto* fusion = DynCast<HloFusionInstruction>(inst);\n-        if (!fusion) {\n-          return true;\n-        }\n-\n-        auto gpu_config = fusion->backend_config<GpuBackendConfig>();\n-        const FusionBackendConfig& backend_config =\n-            gpu_config->fusion_backend_config();\n-        const auto& custom_config = backend_config.custom_fusion_config();\n-        if (custom_config.name() !=\n-            kDynamicSliceFusionWithDynamicAddressComputationConfigName) {\n-          return true;\n-        }\n-\n-        auto* fused_computation = fusion->called_computation();\n-        return !absl::c_any_of(\n-            fused_computation->instructions(), [&](const HloInstruction* inst) {\n-              const auto* dynamic_inst =\n-                  DynCast<HloDynamicIndexInstruction>(inst);\n-              if (!dynamic_inst) {\n-                return false;\n-              }\n-              for (auto* operand : dynamic_inst->index_operands()) {\n-                const auto* param = DynCast<HloParameterInstruction>(operand);\n-                const auto* fusion_operand =\n-                    fusion->operand(param->parameter_number());\n-                if (seq.contains(fusion_operand)) {\n-                  return true;\n-                }\n-              }\n-              return false;\n-            });\n-      };\n-\n-  // Collect the sequence of instructions that contains the async start and its\n-  // corresponding done instruction. If there is another start instruction\n-  // between the original start and done, we may potentially extend the sequence\n-  // to include its corresponding done instruction. For example, if we call this\n-  // function on async-start_a in the following sequence:\n-  //\n-  // async_start_a\n-  // async_start_b\n-  // async_done_a\n-  // async_done_b\n-  //\n-  // The returned sequence will contain async_done_b. So that all async pairs\n-  // are captured by the same command buffer.\n-  auto collect_async_region = [&](const HloInstruction* start) {\n-    auto get_index = [&](const HloInstruction* inst) -> size_t {\n-      auto it = absl::c_find(instructions, inst);\n-      return std::distance(instructions.begin(), it);\n-    };\n-\n-    HloInstructionSequence seq;\n-    size_t done_index = get_index(FindAsyncDoneCommand(start));\n-    for (size_t i = get_index(start); i <= done_index; i++) {\n-      HloInstruction* inst = instructions.at(i);\n-      if (IsAsyncStartCommand(inst, config)) {\n-        const HloInstruction* done = FindAsyncDoneCommand(inst);\n-        done_index = std::max(done_index, get_index(done));\n-      }\n-      seq.push_back(inst);\n-    }\n-    return seq;\n-  };\n-\n-  // Check that instructions are safe to be captured by command buffer, and that\n-  // we do not capture unmatched async done instruction.\n-  auto check_async_region = [&](const HloInstructionSequence& seq) {\n-    if (!absl::c_all_of(seq.instructions(), [&](HloInstruction* inst) {\n-          return IsNoOp(inst) ||\n-                 (IsCommand(inst, config) &&\n-                  check_dynamic_slice_operand_not_from_seq(seq, inst) &&\n-                  check_dynamic_slice_operand_not_from_seq(current_seq,\n-                                                           inst)) ||\n-                 IsAsyncStartCommand(inst, config) ||\n-                 IsAsyncDoneCommand(inst, config);\n-        })) {\n-      return false;\n-    }\n-\n-    absl::flat_hash_set<HloInstruction*> done_instructions;\n-    for (const HloInstruction* inst : seq.instructions()) {\n-      if (IsAsyncStartCommand(inst, config)) {\n-        done_instructions.insert(FindAsyncDoneCommand(inst));\n-      }\n-      if (IsAsyncDoneCommand(inst, config)) {\n-        if (!done_instructions.contains(inst)) {\n-          return false;\n-        }\n-      }\n-    }\n-    return true;\n-  };\n-\n-  for (size_t i = 0; i < instructions.size(); i++) {\n-    HloInstruction* inst = instructions.at(i);\n-\n-    // We add no-op instructions to current sequence only if they act as a glue\n-    // between commands. We do not create command sequences consisting only from\n-    // no-op instruction. First and last instruction in the command buffer is\n-    // always a load-bearing command.\n-    if (IsNoOp(inst) && num_commands_in_current_seq) {\n-      current_seq.push_back(inst);\n-      continue;\n-    }\n-\n-    // Synchronous commands always can be added to instruction sequence.\n-    if (IsCommand(inst, config) &&\n-        check_dynamic_slice_operand_not_from_seq(current_seq, inst)) {\n-      num_commands_in_current_seq++;\n-      current_seq.push_back(inst);\n-      continue;\n-    }\n-\n-    // We capture async commands if all instruction between start and done can\n-    // be outlined into a command buffer.\n-    if (IsAsyncStartCommand(inst, config)) {\n-      HloInstructionSequence seq = collect_async_region(inst);\n-      if (check_async_region(seq)) {\n-        num_commands_in_current_seq += seq.instructions().size();\n-        for (HloInstruction* inst : seq.instructions()) {\n-          current_seq.push_back(inst);\n-        }\n-        i += seq.instructions().size() - 1;\n-        continue;\n-      }\n-    }\n-\n-    // If we didn't find the next command, collect the current sequence and\n-    // start a new one.\n-    collect_current_seq();\n-  }\n-\n-  // Don't forget to collect the final command sequence.\n-  collect_current_seq();\n-  return sequences;\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// Prepares command buffer from sequence of instructions\n-//===----------------------------------------------------------------------===//\n-\n-absl::StatusOr<CommandBuffer> CommandBufferScheduling::PrepareCommandBuffer(\n-    const HloInstructionSequence& seq, HloModule* module) {\n-  auto builder = HloComputation::Builder(\"command_buffer\");\n-\n-  absl::Span<HloInstruction* const> instructions =\n-      absl::MakeSpan(seq.instructions());\n-\n-  // A set of instructions that will be moved into command buffer computation.\n-  absl::flat_hash_set<HloInstruction*> in_command_buffer(instructions.begin(),\n-                                                         instructions.end());\n-\n-  // The sequence might use results of instructions that are not captured by the\n-  // sequence. We pass those results as parameters and map the producers of the\n-  // results to their corresponding parameter instructions.\n-  absl::flat_hash_map<HloInstruction*, HloParameterInstruction*> parameters;\n-\n-  // Mapping from command buffer instructions to their clones in the command\n-  // buffer computation body.\n-  absl::flat_hash_map<HloInstruction*, HloInstruction*> inst_mapping;\n-\n-  // Maps HLO instructions in the original computation to instructions in the\n-  // command buffer: (a) a parameter corresponding to captured value (b) cloned\n-  // instruction corresponding to a command.\n-  auto mapped_operands = [&](HloInstruction* instr) {\n-    absl::InlinedVector<HloInstruction*, 4> operands;\n-    for (HloInstruction* operand : instr->operands()) {\n-      if (auto it = inst_mapping.find(operand); it != inst_mapping.end()) {\n-        operands.push_back(it->second);\n-      }\n-    }\n-    return operands;\n-  };\n-\n-  // Create parameters in the command buffer computation for captured values.\n-  for (HloInstruction* inst : instructions) {\n-    for (HloInstruction* operand : inst->operands()) {\n-      // We already mapped instruction to a parameter.\n-      if (parameters.contains(operand)) {\n-        continue;\n-      }\n-\n-      // Operand instruction is a part of the command buffer.\n-      if (in_command_buffer.contains(operand)) {\n-        continue;\n-      }\n-\n-      // Create a new parameter for value defined outside of a command buffer.\n-      int64_t parameter_id = parameters.size();\n-      auto* parameter = Cast<HloParameterInstruction>(\n-          builder.AddInstruction(HloInstruction::CreateParameter(\n-              parameter_id, operand->shape(), \"p\")));\n-\n-      inst_mapping[operand] = parameters[operand] = parameter;\n-    }\n-  }\n-\n-  // Clone commands into the command buffer body with mapped operands.\n-  for (HloInstruction* inst : seq.instructions()) {\n-    HloCloneContext ctx(inst->GetModule());\n-\n-    // Cloned instructions should call the same computations as original\n-    // instructions will be dead code eliminated.\n-    for (HloComputation* called_computation : inst->called_computations()) {\n-      ctx.MapComputation(called_computation, called_computation);\n-    }\n-    inst_mapping[inst] = builder.AddInstruction(\n-        inst->CloneWithNewOperands(inst->shape(), mapped_operands(inst), &ctx));\n-\n-    // Clear the called computations of the old instruction, because it is\n-    // typically not legal for one computation to have more than one caller.\n-    inst->ClearCalledComputations();\n-  }\n-\n-  // Convert parameters to command buffer arguments.\n-  std::vector<HloInstruction*> arguments(parameters.size());\n-  for (auto& [argument, parameter] : parameters) {\n-    arguments[parameter->parameter_number()] = argument;\n-  }\n-\n-  // Collect command buffer `results` (instructions replaced in the original\n-  // computation) and `results` (instructions in the command buffer).\n-  std::vector<HloInstruction*> results;\n-  std::vector<HloInstruction*> returned;\n-\n-  auto has_external_users = [&](HloInstruction* inst) {\n-    return inst->IsRoot() || absl::c_any_of(inst->users(), [&](auto* user) {\n-             return !in_command_buffer.contains(user);\n-           });\n-  };\n-\n-  for (HloInstruction* inst : instructions) {\n-    if (has_external_users(inst)) {\n-      results.push_back(inst);\n-      returned.push_back(inst_mapping[inst]);\n-    }\n-  }\n-\n-  // If we return multiple results wrap them into tuple.\n-  if (returned.size() > 1) {\n-    builder.AddInstruction(HloInstruction::CreateTuple(returned));\n-  }\n-\n-  std::unique_ptr<HloComputation> comp = builder.Build();\n-\n-  return CommandBuffer{std::move(arguments), std::move(results),\n-                       std::move(comp), std::move(inst_mapping)};\n-}\n-\n-//===----------------------------------------------------------------------===//\n-// Rewrites original computation into command buffer call\n-//===----------------------------------------------------------------------===//\n-\n-absl::StatusOr<HloComputation*> CommandBufferScheduling::RewriteCommandBuffer(\n-    HloComputation* parent, const HloInstructionSequence& seq,\n-    CommandBuffer command_buffer) {\n-  if (command_buffer.results.empty()) {\n-    return absl::InternalError(\"command buffer results must not be empty\");\n-  }\n-\n-  // If we have more than one result we return them as tuple, and get individual\n-  // values using `get-tuple-element` instructions. Otherwise we simply return\n-  // a result from a command buffer computation.\n-  Shape cmd_buffer_result_shape;\n-  bool has_single_result = command_buffer.results.size() == 1;\n-\n-  if (has_single_result) {\n-    cmd_buffer_result_shape = command_buffer.results[0]->shape();\n-  } else {\n-    absl::InlinedVector<Shape, 4> shapes;\n-    shapes.reserve(command_buffer.results.size());\n-    for (auto* res : command_buffer.results) {\n-      shapes.push_back(res->shape());\n-    }\n-    cmd_buffer_result_shape = ShapeUtil::MakeTupleShape(shapes);\n-  }\n-\n-  HloComputation* computation =\n-      parent->parent()->AddComputationAndUnifyNamesAndIds(\n-          std::move(command_buffer.computation),\n-          /*is_entry=*/false);\n-\n-  HloInstruction* call = parent->AddInstruction(HloInstruction::CreateCall(\n-      cmd_buffer_result_shape, command_buffer.arguments, computation));\n-\n-  // Replace all users or original results with a command buffer results.\n-  if (has_single_result) {\n-    TF_RETURN_IF_ERROR(command_buffer.results[0]->ReplaceAllUsesWith(call));\n-  } else {\n-    for (int i = 0; i < command_buffer.results.size(); i++) {\n-      TF_RETURN_IF_ERROR(\n-          command_buffer.results[i]->ReplaceAllUsesWith(parent->AddInstruction(\n-              HloInstruction::CreateGetTupleElement(call, i))));\n-    }\n-  }\n-\n-  // As we are running after scheduling we have to keep it valid.\n-  HloSchedule& schedule = parent->parent()->schedule();\n-\n-  // Update schedule to replace the last instruction with a command buffer call.\n-  // Removal of the rest of the instructions in the sequence is handled by\n-  // schedule update below.\n-  HloInstructionSequence& sequence = schedule.GetOrCreateSequence(parent);\n-  sequence.replace_instruction(seq.instructions().back(), call);\n-\n-  // Rebuild original instruction sequence schedule in a newly created\n-  // command buffer computation to guarantee that we'll get exactly the same\n-  // buffer assignment result as if we were running without command buffers.\n-  HloInstructionSequence cmd_buffer_schedule;\n-  for (auto* argument : command_buffer.arguments) {\n-    cmd_buffer_schedule.push_back(command_buffer.inst_mapping[argument]);\n-  }\n-  for (auto* inst : seq.instructions()) {\n-    cmd_buffer_schedule.push_back(command_buffer.inst_mapping[inst]);\n-  }\n-  if (!has_single_result) {\n-    cmd_buffer_schedule.push_back(computation->root_instruction());\n-  }\n-  schedule.set_sequence(computation, cmd_buffer_schedule);\n-\n-  // Forward control dependencies between original instructions to instruction\n-  // in the command buffer computation.\n-  auto& inst_mapping = command_buffer.inst_mapping;\n-  for (HloInstruction* inst : seq.instructions()) {\n-    HloInstruction* cmd_inst = inst_mapping[inst];\n-\n-    // Forward control dependencies to the new instruction inside command\n-    // buffer. If the dependent instruction is not captured by the command\n-    // buffer, forward the dependency to the command buffer call instead.\n-    for (HloInstruction* predecessor : inst->control_predecessors()) {\n-      if (auto it = inst_mapping.find(predecessor); it != inst_mapping.end()) {\n-        // If predecessor mapped to a parameter instruction it means that we\n-        // need to forward control dependency to a call operation, otherwise\n-        // we add control dependency between commands in the command buffer.\n-        HloInstruction* cmd_predecessor = it->second;\n-        if (IsParameter(cmd_predecessor)) {\n-          TF_RETURN_IF_ERROR(predecessor->AddControlDependencyTo(call));\n-        } else {\n-          TF_RETURN_IF_ERROR(cmd_predecessor->AddControlDependencyTo(cmd_inst));\n-        }\n-      } else {\n-        TF_RETURN_IF_ERROR(predecessor->AddControlDependencyTo(call));\n-      }\n-    }\n-\n-    for (HloInstruction* successor : inst->control_successors()) {\n-      if (auto it = inst_mapping.find(successor); it != inst_mapping.end()) {\n-        HloInstruction* cmd_successor = it->second;\n-        TF_RETURN_IF_ERROR(cmd_inst->AddControlDependencyTo(cmd_successor));\n-      } else {\n-        TF_RETURN_IF_ERROR(call->AddControlDependencyTo(successor));\n-      }\n-    }\n-\n-    TF_RETURN_IF_ERROR(inst->DropAllControlDeps());\n-  }\n-\n-  // Traverse in reverse order as original sequence was topologically sorted and\n-  // we can't remove instructions with users.\n-  for (int32_t i = seq.instructions().size() - 1; i >= 0; i--) {\n-    TF_RETURN_IF_ERROR(parent->RemoveInstruction(seq.instructions()[i]));\n-  }\n-\n-  absl::string_view call_prefix =\n-      GetLongestOpNamePrefix(*call, /*ignore_malformed_op_names=*/true);\n-  std::string call_op_name = (call_prefix.empty())\n-                                 ? (std::string)call->name()\n-                                 : absl::StrCat(call_prefix, \"/\", call->name());\n-  call->set_metadata_op_name(call_op_name);\n-\n-  return computation;\n-}\n-\n-//===----------------------------------------------------------------------===//\n-\n-CommandBufferScheduling::CommandBufferScheduling(\n-    const se::DeviceDescription& device_description)\n-    : device_description_(device_description) {}\n-\n-absl::StatusOr<bool> CommandBufferScheduling::RunImpl(\n-    HloModule* module,\n-    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n-  // We run command buffer scheduling after a regular scheduling to guarantee\n-  // that command buffers will not change execution order and buffer assignment\n-  // compared to a regular execution. Some operations (i.e. async collectives)\n-  // can't be captured into command buffers, and forming too large command\n-  // buffers too early can impact async operations scheduling.\n-  if (!module->has_schedule()) {\n-    return Internal(\"module is not scheduled\");\n-  }\n-\n-  const DebugOptions& debug_options = module->config().debug_options();\n-\n-  absl::flat_hash_set<DebugOptions::CommandBufferCmdType> commands;\n-  for (auto cmd_type : debug_options.xla_gpu_enable_command_buffer()) {\n-    commands.insert(static_cast<DebugOptions::CommandBufferCmdType>(cmd_type));\n-  }\n-\n-  CommandBufferConfig config{std::move(commands), device_description_};\n-\n-  // Erase command buffer cmd types that are not supported by the gpu runtime.\n-  static constexpr auto kRequireConditionals = {DebugOptions::CONDITIONAL,\n-                                                DebugOptions::WHILE};\n-  static constexpr auto kRequireTracing = {\n-      DebugOptions::CUBLAS, DebugOptions::CUBLASLT, DebugOptions::CUDNN,\n-      DebugOptions::CUSTOM_CALL, DebugOptions::COLLECTIVES};\n-\n-  auto erase = [&](absl::Span<const DebugOptions::CommandBufferCmdType> cmds) {\n-    for (auto cmd : cmds) {\n-      if (config.enabled_commands.erase(cmd)) {\n-        VLOG(1) << \"Removed command buffer support for \"\n-                << DebugOptions::CommandBufferCmdType_Name(cmd)\n-                << \" as it's not supported with gpu toolkit version \"\n-                << device_description_.runtime_version()\n-                << \" and driver version \"\n-                << device_description_.driver_version()\n-                << \". This might negatively impact peformance. To enable \"\n-                << DebugOptions::CommandBufferCmdType_Name(cmd)\n-                << \" support in command buffers use cuda-compat package: \"\n-#if defined(PLATFORM_GOOGLE)\n-                << \"set CUDA_COMPAT_LOAD=1 env variable.\";\n-#else\n-                << \"https://docs.nvidia.com/deploy/cuda-compatibility/.\";\n-#endif\n-      }\n-    }\n-  };\n-\n-  // Check if CUDA/ROCM driver supports required features.\n-  if (auto* cuda_comp = device_description_.gpu_compute_capability()\n-                            .cuda_compute_capability()) {\n-    if (std::min(device_description_.runtime_version(),\n-                 device_description_.driver_version()) <\n-        se::SemanticVersion{12, 3, 0}) {\n-      erase(kRequireTracing);       // cuStreamBeginCaptureToGraph\n-      erase(kRequireConditionals);  // on-device control flow\n-    }\n-  } else if (const se::RocmComputeCapability* rocm_comp =\n-                 device_description_.gpu_compute_capability()\n-                     .rocm_compute_capability()) {\n-    erase(kRequireConditionals);  // on-device control flow\n-  }\n-\n-  auto order = module->MakeComputationPostOrder();\n-  std::reverse(order.begin(), order.end());\n-  absl::flat_hash_set<HloComputation*> processed_command_buffers;\n-  std::vector<HloComputation*> command_buffer_computations;\n-\n-  auto changed = false;\n-  for (HloComputation* comp : order) {\n-    // Skip special computations that do not have lowering to thunks.\n-    if (comp->IsFusionComputation() || comp->IsAsyncComputation() ||\n-        !comp->caller_instructions(HloOpcode::kCustomCall).empty()) {\n-      continue;\n-    }\n-\n-    // Skip computations that already part of command buffers.\n-    if (processed_command_buffers.contains(comp)) {\n-      continue;\n-    }\n-\n-    TF_ASSIGN_OR_RETURN(bool changed_,\n-                        MoveParametersAndConstantsToFront(*comp));\n-    changed |= changed_;\n-    // The motivation for MoveGTEsRightAfterTupleDefinition is to ensure the\n-    // live range of large elements in the tuple are not extended due to the\n-    // creation of command buffers. For example, consider the following input\n-    // HLO to this pass.\n-    //\n-    //     x = f32[] parameter(0)\n-    //     t = (f32[], f32[10000]) custom-call()\n-    //     ... # Many instructions, none which use t\n-    //     x_squared = f32[] multiply(x, x)\n-    //     t0 = f32[] get-tuple-element(t), index=0\n-    //     y = f32[] add(x_squared, t0)\n-    //\n-    // The 10000-element buffer can immediately be freed after the custom-call,\n-    // as it is unused. However, if `t0` is not moved right after `t`, then the\n-    // scheudling of command buffers might turn the HLO into the following,\n-    // extending the live range of the 10000-element buffer as 't' is passed to\n-    // the command buffer:\n-    //\n-    //     command_buffer {\n-    //       t = (f32[], f32[10000]) paramter(0)\n-    //       x_squared = f32[] multiply(x, x)\n-    //       t0 = f32[] get-tuple-element(t), index=0\n-    //       ROOT y = f32[] add(x_squared, t0)\n-    //     }\n-    //\n-    //     main {\n-    //       x = f32[] parameter(0)\n-    //       t = (f32[], f32[10000]) custom-call()\n-    //       ... # Many instructions, none which use t\n-    //       ROOT y = f32[] call(t), to_apply=command_buffer\n-    //     }\n-    //\n-    // Moving the GTE right after `t` solves this, as command-buffers never\n-    // start with a GTE, so it's impossible for a command buffer to contain the\n-    // GTE but not the custom-call itself.\n-    TF_ASSIGN_OR_RETURN(changed_, MoveGTEsRightAfterTupleDefinition(*comp));\n-    changed |= changed_;\n-\n-    std::vector<HloInstructionSequence> sequences =\n-        CollectCommandBufferSequences(\n-            module->schedule().sequence(comp), config,\n-            debug_options.xla_gpu_graph_min_graph_size());\n-\n-    for (const HloInstructionSequence& seq : sequences) {\n-      TF_ASSIGN_OR_RETURN(CommandBuffer command_buffer,\n-                          PrepareCommandBuffer(seq, comp->parent()));\n-      TF_ASSIGN_OR_RETURN(\n-          HloComputation * command_buffer_computation,\n-          RewriteCommandBuffer(comp, seq, std::move(command_buffer)));\n-      changed = true;\n-\n-      // Track created command buffer computations for diagnostics.\n-      command_buffer_computations.push_back(command_buffer_computation);\n-\n-      // All computations reachable from a command buffer computation are nested\n-      // command buffers (i.e. body computations attached to a while operation).\n-      for (HloComputation* called :\n-           command_buffer_computation->MakeEmbeddedComputationsList()) {\n-        processed_command_buffers.insert(called);\n-      }\n-    }\n-  }\n-  TF_RETURN_IF_ERROR(module->schedule().Update());\n-\n-  if (VLOG_IS_ON(3)) {\n-    // Collect all instructions that are part of any created command buffer\n-    // computations (including their embedded computations).\n-    absl::flat_hash_set<const HloInstruction*> in_command_buffers;\n-    for (HloComputation* cb_comp : command_buffer_computations) {\n-      for (HloInstruction* inst : cb_comp->instructions()) {\n-        if (inst != nullptr && !IsNoOp(inst)) in_command_buffers.insert(inst);\n-      }\n-      for (HloComputation* embedded : cb_comp->MakeEmbeddedComputationsList()) {\n-        for (HloInstruction* inst : embedded->instructions()) {\n-          if (inst != nullptr && !IsNoOp(inst)) in_command_buffers.insert(inst);\n-        }\n-      }\n-    }\n-\n-    // Build a set of command buffer computations to identify their call sites.\n-    absl::flat_hash_set<HloComputation*> cb_comps(\n-        command_buffer_computations.begin(), command_buffer_computations.end());\n-\n-    // Compute coverage and log instructions not captured by any command buffer.\n-    int64_t total_insts = 0;\n-    int64_t not_in_cb_count = 0;\n-    for (HloComputation* comp_all : module->computations()) {\n-      for (HloInstruction* inst : comp_all->instructions()) {\n-        if (inst == nullptr) continue;\n-        // Skip no-op instructions from coverage and diagnostics.\n-        if (IsNoOp(inst)) continue;\n-        // Ignore the kCall that invokes a command buffer computation.\n-        if (inst->opcode() == HloOpcode::kCall) {\n-          bool is_cb_call = false;\n-          for (HloComputation* called : inst->called_computations()) {\n-            if (cb_comps.contains(called)) {\n-              is_cb_call = true;\n-              break;\n-            }\n-          }\n-          if (is_cb_call) continue;\n-        }\n-        total_insts++;\n-        if (!in_command_buffers.contains(inst)) {\n-          not_in_cb_count++;\n-          VLOG(3) << \"Not in command buffer: module=\" << module->name()\n-                  << \" computation=\" << comp_all->name()\n-                  << \" instruction:\" << inst->ToString();\n-        }\n-      }\n-    }\n-\n-    VLOG(3) << \"Command buffer coverage: \" << (total_insts - not_in_cb_count)\n-            << \"/\" << total_insts << \" instructions captured; \"\n-            << not_in_cb_count << \" not captured.\";\n-  }\n-\n-  return changed;\n-}\n-\n-}  // namespace xla::gpu"
        },
        {
            "sha": "ac6c884b4228aa996424f5c65fbf3f050468b0f2",
            "filename": "third_party/xla/xla/service/gpu/transforms/command_buffer_scheduling.h",
            "status": "removed",
            "additions": 0,
            "deletions": 130,
            "changes": 130,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6ff96032210ee64793439869c7726f7a7ad2360a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6ff96032210ee64793439869c7726f7a7ad2360a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling.h?ref=6ff96032210ee64793439869c7726f7a7ad2360a",
            "patch": "@@ -1,130 +0,0 @@\n-/* Copyright 2023 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-#ifndef XLA_SERVICE_GPU_TRANSFORMS_COMMAND_BUFFER_SCHEDULING_H_\n-#define XLA_SERVICE_GPU_TRANSFORMS_COMMAND_BUFFER_SCHEDULING_H_\n-\n-#include <cstdint>\n-#include <memory>\n-#include <string>\n-#include <vector>\n-\n-#include \"absl/container/flat_hash_map.h\"\n-#include \"absl/container/flat_hash_set.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"xla/hlo/ir/hlo_computation.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/ir/hlo_schedule.h\"\n-#include \"xla/hlo/pass/hlo_pass_interface.h\"\n-#include \"xla/stream_executor/device_description.h\"\n-#include \"xla/xla.pb.h\"\n-\n-namespace xla::gpu {\n-\n-// Lift instructions to command buffers.\n-//\n-// Before the pass:\n-//   %fused_computation (param_0: s32[], param_1: s32[]) -> s32[] {\n-//     ...\n-//   }\n-//\n-//   ENTRY %main (a: s32[], b: s32[]) -> s32[] {\n-//     %a = s32[] parameter(0)\n-//     %b = s32[] parameter(1)\n-//     ROOT %fusion = s32[] fusion(s32[] %a, s32[] %b), kind=kLoop,\n-//       calls=%fused_computation\n-//   }\n-//\n-// After the pass:\n-//   %fused_computation (param_0: s32[], param_1: s32[]) -> s32[] {\n-//     ...\n-//   }\n-//\n-//   %command_buffer (param_0: s32[], param_1: s32[]) -> s32[] {\n-//     %param_0 = s32[] parameter(0)\n-//     %param_1 = s32[] parameter(1)\n-//     ROOT %fusion = s32[] fusion(s32[] %param_0, s32[] %param_1), kind=kLoop,\n-//       calls=%fused_computation\n-//   }\n-//\n-//   ENTRY %main (a: s32[], b: s32[]) -> s32[] {\n-//     %a = s32[] parameter(0)\n-//     %b = s32[] parameter(1)\n-//     ROOT %call = s32[] call(s32[] %a, s32[] %b), to_apply=%command_buffer\n-//  }\n-//\n-// We currently do not have a command_buffer HLO operation, so we'll start with\n-// a kCall op code with an attached HLO computation. We'll consider graduating\n-// custom call to a first class operation later.\n-class CommandBufferScheduling : public HloModulePass {\n- public:\n-  struct CommandBufferConfig {\n-    // DebugOptions control which commands are enabled. Long term we want to\n-    // remove that flag and enable all supported commands by default.\n-    absl::flat_hash_set<DebugOptions::CommandBufferCmdType> enabled_commands;\n-    const se::DeviceDescription& device_description;\n-  };\n-\n-  explicit CommandBufferScheduling(\n-      const se::DeviceDescription& device_description);\n-\n-  absl::string_view name() const override {\n-    return \"command-buffer-scheduling\";\n-  }\n-\n-  static std::vector<HloInstructionSequence> CollectCommandBufferSequences(\n-      HloInstructionSequence schedule, const CommandBufferConfig& config,\n-      int32_t min_num_commands = 1);\n-\n-  struct CommandBuffer {\n-    // Command buffer arguments (call instruction arguments).\n-    std::vector<HloInstruction*> arguments;\n-\n-    // Command buffer result (call instruction result tuple).\n-    std::vector<HloInstruction*> results;\n-\n-    // Hlo computation corresponding to a command buffer body.\n-    std::unique_ptr<HloComputation> computation;\n-\n-    // Mapping from original instruction to their clones in the command buffer.\n-    absl::flat_hash_map<HloInstruction*, HloInstruction*> inst_mapping;\n-  };\n-\n-  // Prepares a command buffer from the instruction sequence. Used values\n-  // constructed by instructions outside of the sequence are passed in as\n-  // parameters. Results of instructions in the sequence are returned in a tuple\n-  // (if command buffer has a single result we don't wrap it into tuple).\n-  static absl::StatusOr<CommandBuffer> PrepareCommandBuffer(\n-      const HloInstructionSequence& seq, HloModule* module);\n-\n-  // Rewrites prepared command buffer computation into Hlo operations in the\n-  // parent computation (calls command buffer and replaced all users).\n-  static absl::StatusOr<HloComputation*> RewriteCommandBuffer(\n-      HloComputation* parent, const HloInstructionSequence& seq,\n-      CommandBuffer command_buffer);\n-\n- protected:\n-  absl::StatusOr<bool> RunImpl(\n-      HloModule* module,\n-      const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n-\n- private:\n-  se::DeviceDescription device_description_;\n-};\n-\n-}  // namespace xla::gpu\n-\n-#endif  // XLA_SERVICE_GPU_TRANSFORMS_COMMAND_BUFFER_SCHEDULING_H_"
        },
        {
            "sha": "197d4cff0218844322bf096bac7d59cf33941e29",
            "filename": "third_party/xla/xla/service/gpu/transforms/command_buffer_scheduling_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 1385,
            "changes": 1385,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6ff96032210ee64793439869c7726f7a7ad2360a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6ff96032210ee64793439869c7726f7a7ad2360a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcommand_buffer_scheduling_test.cc?ref=6ff96032210ee64793439869c7726f7a7ad2360a",
            "patch": "@@ -1,1385 +0,0 @@\n-/* Copyright 2023 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-#include \"xla/service/gpu/transforms/command_buffer_scheduling.h\"\n-\n-#include <memory>\n-#include <optional>\n-#include <string>\n-#include <utility>\n-#include <vector>\n-\n-#include <gtest/gtest.h>\n-#include \"absl/status/statusor.h\"\n-#include \"xla/backends/gpu/runtime/thunk.h\"\n-#include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/hlo/ir/hlo_print_options.h\"\n-#include \"xla/hlo/ir/hlo_schedule.h\"\n-#include \"xla/hlo/parser/hlo_parser.h\"\n-#include \"xla/hlo/testlib/filecheck.h\"\n-#include \"xla/hlo/testlib/verified_hlo_module.h\"\n-#include \"xla/service/executable.h\"\n-#include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n-#include \"xla/service/gpu/gpu_executable.h\"\n-#include \"xla/service/hlo_module_config.h\"\n-#include \"xla/service/hlo_runner_interface.h\"\n-#include \"xla/stream_executor/device_description.h\"\n-#include \"xla/tests/hlo_test_base.h\"\n-#include \"xla/tsl/lib/core/status_test_util.h\"\n-#include \"xla/tsl/platform/status.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n-#include \"xla/xla.pb.h\"\n-\n-namespace xla::gpu {\n-namespace {\n-\n-class CommandBufferSchedulingTest : public HloTestBase {\n- public:\n-  se::DeviceDescription device_desc() {\n-    return TestGpuDeviceInfo::CudaOrRocmDeviceInfo();\n-  }\n-\n-  DebugOptions GetDebugOptionsForTest() const override {\n-    auto debug_options = HloTestBase::GetDebugOptionsForTest();\n-    debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n-    debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::CONDITIONAL);\n-    debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::WHILE);\n-    debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::COLLECTIVES);\n-    debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::CUDNN);\n-    debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::CUBLASLT);\n-    debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::CUSTOM_CALL);\n-    debug_options.set_xla_gpu_graph_min_graph_size(2);\n-    return debug_options;\n-  }\n-\n-  const se::GpuComputeCapability& GetGpuComputeCapability() {\n-    return backend()\n-        .default_stream_executor()\n-        ->GetDeviceDescription()\n-        .gpu_compute_capability();\n-  }\n-};\n-\n-using CommandBuffer = CommandBufferScheduling::CommandBuffer;\n-\n-TEST_F(CommandBufferSchedulingTest, SingleCommandBuffer) {\n-  const char* hlo = R\"(\n-      HloModule TestModule, is_scheduled=true\n-\n-      %fused_computation (param_0: s32[], param_1: s32[]) -> s32[] {\n-        %p0 = s32[] parameter(0)\n-        %p1 = s32[] parameter(1)\n-        ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-      }\n-\n-      %fused_computation.1 (param_0: s32[], param_1: s32[]) -> s32[] {\n-        %p0 = s32[] parameter(0)\n-        %p1 = s32[] parameter(1)\n-        ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-      }\n-\n-      ENTRY %main (a: s32[], b: s32[]) -> s32[] {\n-        %a = s32[] parameter(0)\n-        %b = s32[] parameter(1)\n-        %fusion = s32[] fusion(s32[] %a, s32[] %b), kind=kLoop, calls=%fused_computation\n-        %fusion.1 = s32[] fusion(s32[] %a, s32[] %b), kind=kLoop, calls=%fused_computation.1\n-        ROOT %custom-call = s32[] custom-call(s32[] %fusion, s32[] %fusion.1), custom_call_target=\"some target\"\n-      })\";\n-\n-  const char* expected = R\"(\n-// CHECK: %command_buffer ([[P0:.+]]: s32[], [[P1:.+]]: s32[]) -> (s32[], s32[]) {\n-// CHECK:   %[[P0]] = s32[] parameter(0)\n-// CHECK:   %[[P1]] = s32[] parameter(1)\n-// CHECK:   %fusion.2 = s32[] fusion(%[[P0]], %[[P1]]), kind=kLoop, calls=%fused_computation\n-// CHECK:   %fusion.3 = s32[] fusion(%[[P0]], %[[P1]]), kind=kLoop, calls=%fused_computation.1\n-// CHECK:   ROOT %tuple = (s32[], s32[]) tuple(%fusion.2, %fusion.3)\n-// CHECK: }\n-//\n-// CHECK: ENTRY %main (a: s32[], b: s32[]) -> s32[] {\n-// CHECK:   %a = s32[] parameter(0)\n-// CHECK:   %b = s32[] parameter(1)\n-// CHECK:   %call = (s32[], s32[]) call(%a, %b), to_apply=%command_buffer\n-// CHECK:   %get-tuple-element = s32[] get-tuple-element(%call), index=0\n-// CHECK:   %get-tuple-element.1 = s32[] get-tuple-element(%call), index=1\n-// CHECK:   ROOT %custom-call = s32[] custom-call(%get-tuple-element, %get-tuple-element.1), custom_call_target=\"some target\"\n-// CHECK: })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            expected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, MultipleCommandBuffers) {\n-  const char* hlo = R\"(\n-      HloModule TestModule, is_scheduled=true\n-\n-      %fused_computation(param_0: s32[], param_1: s32[]) -> s32[] {\n-        %p0 = s32[] parameter(0)\n-        %p1 = s32[] parameter(1)\n-        ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-      }\n-\n-      %fused_computation.1(param_0: s32[], param_1: s32[]) -> s32[] {\n-        %p0 = s32[] parameter(0)\n-        %p1 = s32[] parameter(1)\n-        ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-      }\n-\n-      %fused_computation.2(param_0: s32[], param_1: s32[]) -> s32[] {\n-        %p0 = s32[] parameter(0)\n-        %p1 = s32[] parameter(1)\n-        ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-      }\n-\n-      %fused_computation.3(param_0: s32[], param_1: s32[]) -> s32[] {\n-        %p0 = s32[] parameter(0)\n-        %p1 = s32[] parameter(1)\n-        ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-      }\n-\n-      ENTRY %main (a: s32[], b: s32[], c: (s32[], s32[])) -> s32[] {\n-        %a = s32[] parameter(0)\n-        %b = s32[] parameter(1)\n-        %c = (s32[], s32[]) parameter(2)\n-        %fusion = s32[] fusion(s32[] %a, s32[] %b), kind=kLoop, calls=%fused_computation\n-        %d = s32[] get-tuple-element((s32[], s32[]) %c), index=0\n-        %fusion.1 = s32[] fusion(s32[] %fusion, s32[] %d), kind=kLoop, calls=%fused_computation.1\n-        %e = s32[] get-tuple-element((s32[], s32[]) %c), index=1\n-        %custom-call = s32[] custom-call(s32[] %fusion.1, s32[] %e), custom_call_target=\"some target\"\n-        %fusion.2 = s32[] fusion(s32[] %custom-call, s32[] %a), kind=kLoop, calls=%fused_computation.2\n-        %fusion.3 = s32[] fusion(s32[] %custom-call, s32[] %fusion.2), kind=kLoop, calls=%fused_computation.3\n-        ROOT %custom-call.1 = s32[] custom-call(s32[] %fusion.3), custom_call_target=\"some target\"\n-      })\";\n-\n-  const char* expected = R\"(\n-// CHECK:  %command_buffer ([[P0:.+]]: s32[], [[P1:.+]]: s32[], [[P2:.+]]: s32[]) -> s32[] {\n-// CHECK:    %[[P0]] = s32[] parameter(0)\n-// CHECK:    %[[P1]] = s32[] parameter(1)\n-// CHECK:    %[[P2]] = s32[] parameter(2)\n-// CHECK:    %[[F0:.+]] = s32[] fusion(%[[P0]], %[[P1]]), kind=kLoop, calls=%fused_computation\n-// CHECK:    ROOT {{.*}} = s32[] fusion(%[[F0]], %[[P2]]), kind=kLoop, calls=%fused_computation.1\n-// CHECK:  }\n-\n-// CHECK:  %command_buffer.1 ([[P0:.+]]: s32[], [[P1:.+]]: s32[]) -> s32[] {\n-// CHECK:    %[[P0]] = s32[] parameter(0)\n-// CHECK:    %[[P1]] = s32[] parameter(1)\n-// CHECK:    %[[F2:.+]] = s32[] fusion(%[[P0]], %[[P1]]), kind=kLoop, calls=%fused_computation.2\n-// CHECK:    ROOT {{.*}} = s32[] fusion(%[[P0]], %[[F2]]), kind=kLoop, calls=%fused_computation.3\n-// CHECK:  }\n-\n-// CHECK:  ENTRY %main (a: s32[], b: s32[], c: (s32[], s32[])) -> s32[] {\n-// CHECK:    %a = s32[] parameter(0)\n-// CHECK:    %b = s32[] parameter(1)\n-// CHECK:    %c = (s32[], s32[]) parameter(2)\n-// CHECK:    %d = s32[] get-tuple-element(%c), index=0\n-// CHECK:    %e = s32[] get-tuple-element(%c), index=1\n-// CHECK:    %[[CMD0:.+]] = s32[] call(%a, %b, %d), to_apply=%command_buffer\n-// CHECK:    %[[CALL:.+]] = s32[] custom-call(%[[CMD0]], %e), custom_call_target=\"some target\"\n-// CHECK:    %[[CMD1:.+]] = s32[] call(%[[CALL]], %a), to_apply=%command_buffer.1\n-// CHECK:    ROOT {{.*}} = s32[] custom-call(%[[CMD1]]), custom_call_target=\"some target\"\n-// CHECK:  })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            expected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, AllReduceStartFollowedByDone) {\n-  const char* hlo = R\"(\n-    HloModule TestModule, is_scheduled=true\n-\n-    %add (p0: s32[4], p1: s32[4]) -> s32[4] {\n-      %p0 = s32[4] parameter(0)\n-      %p1 = s32[4] parameter(1)\n-      ROOT %add = s32[4] add(s32[4] %p0, s32[4] %p1)\n-    }\n-\n-    ENTRY %main (a: s32[4]) -> s32[4] {\n-      %a = s32[4] parameter(0)\n-      %start = s32[4]{0} all-reduce-start(s32[4]{0} %a),\n-        replica_groups={{0,1}}, to_apply=%add,\n-        backend_config={\"collective_backend_config\": {\"is_sync\":true}}\n-      ROOT %done = s32[4]{0} all-reduce-done(s32[4]{0} %start)\n-    })\";\n-\n-  const char* expected = R\"(\n-    CHECK: %command_buffer ([[P0:.+]]: s32[4]) -> s32[4] {\n-    CHECK:   %[[P0]] = s32[4]{0} parameter(0)\n-    CHECK:   %[[START:.+]] = s32[4]{0} all-reduce-start(%[[P0]])\n-    CHECK:   ROOT %[[DONE:.+]] = s32[4]{0} all-reduce-done(%[[START]])\n-    CHECK: }\n-\n-    CHECK: ENTRY %main (a: s32[4]) -> s32[4] {\n-    CHECK:   %[[A:.+]] = s32[4]{0} parameter(0)\n-    CHECK:   ROOT %[[CALL:.+]] = s32[4]{0} call(%[[A]]),\n-    CHECK:     to_apply=%command_buffer\n-    CHECK: })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            expected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, AllGatherStartFollowedByDone) {\n-  const char* hlo = R\"(\n-    HloModule TestModule, is_scheduled=true\n-\n-    ENTRY %main (a: s32[2]) -> s32[4] {\n-      %a = s32[2] parameter(0)\n-\n-      %start = (s32[2]{0}, s32[4]{0}) all-gather-start(%a),\n-        channel_id=555, replica_groups={{0,1}}, dimensions={0},\n-        backend_config={\"collective_backend_config\": {\"is_sync\":true}}\n-\n-      ROOT %done = s32[4]{0} all-gather-done(%start)\n-    })\";\n-\n-  const char* expected = R\"(\n-    CHECK: %command_buffer ([[P0:.+]]: s32[2]) -> s32[4] {\n-    CHECK:   %[[P0]] = s32[2]{0} parameter(0)\n-    CHECK:   %[[START:.+]] = {{.*}} all-gather-start(%[[P0]])\n-    CHECK:   ROOT %[[DONE:.+]] = s32[4]{0} all-gather-done(%[[START]])\n-    CHECK: }\n-\n-    CHECK: ENTRY %main (a: s32[2]) -> s32[4] {\n-    CHECK:   %[[A:.+]] = s32[2]{0} parameter(0)\n-    CHECK:   ROOT %[[CALL:.+]] = s32[4]{0} call(%[[A]]),\n-    CHECK:     to_apply=%command_buffer\n-    CHECK: })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            expected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, ReduceScatterStartFollowedByDone) {\n-  const char* hlo = R\"(\n-    HloModule TestModule, is_scheduled=true\n-\n-    %add (p0: s32[], p1: s32[]) -> s32[] {\n-      %p0 = s32[] parameter(0)\n-      %p1 = s32[] parameter(1)\n-      ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-    }\n-\n-    ENTRY %main (a: s32[4]) -> s32[2] {\n-      %a = s32[4] parameter(0)\n-\n-      %start = ((s32[4]{0}), s32[2]{0}) reduce-scatter-start(%a),\n-        channel_id=555, replica_groups={{0,1}}, dimensions={0}, to_apply=add,\n-        backend_config={\"collective_backend_config\": {\"is_sync\":true}}\n-\n-      ROOT %done = s32[2]{0} reduce-scatter-done(%start)\n-    })\";\n-\n-  const char* expected = R\"(\n-    CHECK: %command_buffer ([[P0:.+]]: s32[4]) -> s32[2] {\n-    CHECK:   %[[P0]] = s32[4]{0} parameter(0)\n-    CHECK:   %[[START:.+]] = {{.*}} reduce-scatter-start(%[[P0]])\n-    CHECK:   ROOT %[[DONE:.+]] = s32[2]{0} reduce-scatter-done(%[[START]])\n-    CHECK: }\n-\n-    CHECK: ENTRY %main (a: s32[4]) -> s32[2] {\n-    CHECK:   %[[A:.+]] = s32[4]{0} parameter(0)\n-    CHECK:   ROOT %[[CALL:.+]] = s32[2]{0} call(%[[A]]),\n-    CHECK:     to_apply=%command_buffer\n-    CHECK: })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            expected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, AllReduceStartFollowedByBitcast) {\n-  const char* hlo = R\"(\n-    HloModule TestModule, is_scheduled=true\n-\n-    %add (p0: s32[4], p1: s32[4]) -> s32[4] {\n-      %p0 = s32[4] parameter(0)\n-      %p1 = s32[4] parameter(1)\n-      ROOT %add = s32[4] add(s32[4] %p0, s32[4] %p1)\n-    }\n-\n-    ENTRY %main (a: s32[4]) -> s32[4] {\n-      %a = s32[4] parameter(0)\n-      %start = s32[4]{0} all-reduce-start(s32[4]{0} %a),\n-        replica_groups={{0,1}}, to_apply=%add,\n-        backend_config={\"collective_backend_config\": {\"is_sync\":true}}\n-      %bitcast = s32[4] bitcast(s32[4]{0} %a)\n-      ROOT %done = s32[4]{0} all-reduce-done(s32[4]{0} %start)\n-    })\";\n-\n-  const char* expected = R\"(\n-    CHECK: %command_buffer ([[P0:.+]]: s32[4]) -> s32[4] {\n-    CHECK:   %[[P0]] = s32[4]{0} parameter(0)\n-    CHECK:   %[[START:.+]] = s32[4]{0} all-reduce-start(%[[P0]])\n-    CHECK:   %[[BITCAST:.+]] = s32[4]{0} bitcast(%[[P0]])\n-    CHECK:   ROOT %[[DONE:.+]] = s32[4]{0} all-reduce-done(%[[START]])\n-    CHECK: }\n-\n-    CHECK: ENTRY %main (a: s32[4]) -> s32[4] {\n-    CHECK:   %[[A:.+]] = s32[4]{0} parameter(0)\n-    CHECK:   ROOT %[[CALL:.+]] = s32[4]{0} call(%[[A]]),\n-    CHECK:     to_apply=%command_buffer\n-    CHECK: })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            expected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, AllReduceStartFollowedAllReduceStart) {\n-  const char* hlo = R\"(\n-    HloModule TestModule, is_scheduled=true\n-\n-    %add (p0: s32[4], p1: s32[4]) -> s32[4] {\n-      %p0 = s32[4] parameter(0)\n-      %p1 = s32[4] parameter(1)\n-      ROOT %add = s32[4] add(s32[4] %p0, s32[4] %p1)\n-    }\n-\n-    ENTRY %main (a: s32[4]) -> s32[4] {\n-      %a = s32[4] parameter(0)\n-      %start1 = s32[4]{0} all-reduce-start(s32[4]{0} %a),\n-        replica_groups={{0,1}}, to_apply=%add,\n-        backend_config={\"collective_backend_config\": {\"is_sync\":true}}\n-      %start2 = s32[4]{0} all-reduce-start(s32[4]{0} %a),\n-        replica_groups={{0,1}}, to_apply=%add,\n-        backend_config={\"collective_backend_config\": {\"is_sync\":true}}\n-      %done1 = s32[4]{0} all-reduce-done(s32[4]{0} %start1)\n-      ROOT %done2 = s32[4]{0} all-reduce-done(s32[4]{0} %start2)\n-    })\";\n-\n-  const char* expected = R\"(\n-    CHECK: %command_buffer ([[P0:.+]]: s32[4]) -> s32[4] {\n-    CHECK:   %[[P0]] = s32[4]{0} parameter(0)\n-    CHECK:   %[[START1:.+]] = s32[4]{0} all-reduce-start(%[[P0]])\n-    CHECK:   %[[START2:.+]] = s32[4]{0} all-reduce-start(%[[P0]])\n-    CHECK:   %[[DONE1:.+]] = s32[4]{0} all-reduce-done(%[[START1]])\n-    CHECK:   ROOT %[[DONE2:.+]] = s32[4]{0} all-reduce-done(%[[START2]])\n-    CHECK: }\n-\n-    CHECK: ENTRY %main (a: s32[4]) -> s32[4] {\n-    CHECK:   %[[A:.+]] = s32[4]{0} parameter(0)\n-    CHECK:   ROOT %[[CALL:.+]] = s32[4]{0} call(%[[A]]),\n-    CHECK:     to_apply=%command_buffer\n-    CHECK: })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            expected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, DoNotCaptureUnmatchedAsyncDone) {\n-  const char* hlo = R\"(\n-    HloModule TestModule, is_scheduled=true\n-\n-    %fused_computation(param_0: s32[], param_1: s32[]) -> s32[] {\n-      %p0 = s32[] parameter(0)\n-      %p1 = s32[] parameter(1)\n-      ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-    }\n-\n-    %fused_computation.1(param_0: s32[], param_1: s32[]) -> s32[] {\n-      %p0 = s32[] parameter(0)\n-      %p1 = s32[] parameter(1)\n-      ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-    }\n-\n-    %add (p0: s32[4], p1: s32[4]) -> s32[4] {\n-      %p0 = s32[4] parameter(0)\n-      %p1 = s32[4] parameter(1)\n-      ROOT %add = s32[4] add(s32[4] %p0, s32[4] %p1)\n-    }\n-\n-    ENTRY %main (a: s32[4], b:s32[]) -> s32[] {\n-      %a = s32[4] parameter(0)\n-      %b = s32[] parameter(1)\n-      %start1 = s32[4]{0} all-reduce-start(s32[4]{0} %a),\n-        replica_groups={{0,1}}, to_apply=%add,\n-        backend_config={\"collective_backend_config\": {\"is_sync\":true}}\n-      %c = s32[] custom-call(), custom_call_target=\"target\"\n-      %start2 = s32[4]{0} all-reduce-start(s32[4]{0} %a),\n-        replica_groups={{0,1}}, to_apply=%add,\n-        backend_config={\"collective_backend_config\": {\"is_sync\":true}}\n-      %done1 = s32[4]{0} all-reduce-done(s32[4]{0} %start1)\n-      %done2 = s32[4]{0} all-reduce-done(s32[4]{0} %start2)\n-      %fusion = s32[] fusion(s32[] %b, s32[] %c), kind=kLoop, calls=%fused_computation\n-      ROOT %fusion.1 = s32[] fusion(s32[] %b, s32[] %c), kind=kLoop, calls=%fused_computation.1\n-    })\";\n-\n-  const char* expected = R\"(\n-    CHECK: %command_buffer ([[P0:.+]]: s32[], [[P1:.+]]: s32[]) -> s32[] {\n-    CHECK:   %[[P0]] = s32[] parameter(0)\n-    CHECK:   %[[P1]] = s32[] parameter(1)\n-    CHECK:   %fusion.2 = s32[] fusion(%[[P0]], %[[P1]]), kind=kLoop, calls=%fused_computation\n-    CHECK:   ROOT %fusion.3 = s32[] fusion(%[[P0]], %[[P1]]), kind=kLoop, calls=%fused_computation.1\n-    CHECK: }\n-\n-    CHECK: ENTRY %main (a: s32[4], b: s32[]) -> s32[] {\n-    CHECK:   %[[A:.+]] = s32[4]{0} parameter(0)\n-    CHECK:   %[[B:.+]] = s32[] parameter(1)\n-    CHECK:   %[[START1:.+]] = s32[4]{0} all-reduce-start(%[[A]])\n-    CHECK:   %[[C:.+]] = s32[] custom-call()\n-    CHECK:   %[[START2:.+]] = s32[4]{0} all-reduce-start(%[[A]])\n-    CHECK:   %[[DONE1:.+]] = s32[4]{0} all-reduce-done(%[[START1]])\n-    CHECK:   %[[DONE2:.+]] = s32[4]{0} all-reduce-done(%[[START2]])\n-    CHECK:   %call = s32[] call(%b, %c), to_apply=%command_buffer\n-    CHECK: })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            expected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, CollectCommandBufferSequence) {\n-  const char* hlo = R\"(\n-      HloModule TestModule, is_scheduled=true\n-\n-      %fused_computation(param_0: s32[], param_1: s32[]) -> s32[] {\n-        %p0 = s32[] parameter(0)\n-        %p1 = s32[] parameter(1)\n-        ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-      }\n-\n-      %fused_computation.1(param_0: s32[], param_1: s32[]) -> s32[] {\n-        %p0 = s32[] parameter(0)\n-        %p1 = s32[] parameter(1)\n-        ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-      }\n-\n-      %fused_computation.2(param_0: s32[], param_1: s32[]) -> s32[] {\n-        %p0 = s32[] parameter(0)\n-        %p1 = s32[] parameter(1)\n-        ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-      }\n-\n-      %fused_computation.3(param_0: s32[], param_1: s32[]) -> s32[] {\n-        %p0 = s32[] parameter(0)\n-        %p1 = s32[] parameter(1)\n-        ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-      }\n-\n-      ENTRY %main (a: s32[], b: s32[], c: (s32[], s32[])) -> s32[] {\n-        %a = s32[] parameter(0)\n-        %b = s32[] parameter(1)\n-        %c = (s32[], s32[]) parameter(2)\n-        %fusion = s32[] fusion(s32[] %a, s32[] %b), kind=kLoop, calls=%fused_computation\n-        %d = s32[] get-tuple-element((s32[], s32[]) %c), index=0\n-        %fusion.1 = s32[] fusion(s32[] %fusion, s32[] %d), kind=kLoop, calls=%fused_computation.1\n-        %e = s32[] get-tuple-element((s32[], s32[]) %c), index=1\n-        %custom-call = s32[] custom-call(s32[] %fusion.1, s32[] %e), custom_call_target=\"some target\"\n-        %fusion.2 = s32[] fusion(s32[] %custom-call, s32[] %a), kind=kLoop, calls=%fused_computation.2\n-        ROOT %fusion.3 = s32[] fusion(s32[] %custom-call, s32[] %fusion.2), kind=kLoop, calls=%fused_computation.3\n-      })\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n-                          ParseAndReturnVerifiedModule(hlo));\n-\n-  HloInstructionSequence seq;\n-  for (HloInstruction* x : module->entry_computation()->instructions()) {\n-    seq.push_back(x);\n-  }\n-  EXPECT_EQ(seq.size(), 10);\n-\n-  CommandBufferScheduling::CommandBufferConfig config{{DebugOptions::FUSION},\n-                                                      device_desc()};\n-\n-  std::vector<HloInstructionSequence> command_buffer_sequences =\n-      CommandBufferScheduling::CollectCommandBufferSequences(seq, config);\n-  EXPECT_EQ(command_buffer_sequences.size(), 2);\n-\n-  std::vector<HloInstruction*> seq_0 =\n-      command_buffer_sequences[0].instructions();\n-  EXPECT_EQ(seq_0.size(), 3);\n-  EXPECT_EQ(seq_0[0]->opcode(), HloOpcode::kFusion);\n-  EXPECT_EQ(seq_0[1]->opcode(), HloOpcode::kGetTupleElement);\n-  EXPECT_EQ(seq_0[2]->opcode(), HloOpcode::kFusion);\n-\n-  std::vector<HloInstruction*> seq_1 =\n-      command_buffer_sequences[1].instructions();\n-  EXPECT_EQ(seq_1.size(), 2);\n-  EXPECT_EQ(seq_1[0]->opcode(), HloOpcode::kFusion);\n-  EXPECT_EQ(seq_1[1]->opcode(), HloOpcode::kFusion);\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, PrepareCommandBuffer) {\n-  const char* hlo = R\"(\n-      HloModule TestModule, is_scheduled=true\n-\n-      %fused_computation(param_0: s32[], param_1: s32[]) -> (s32[], s32[]) {\n-        %p0 = s32[] parameter(0)\n-        %p1 = s32[] parameter(1)\n-        ROOT %tuple.1 = (s32[], s32[]) tuple(s32[] %p0, s32[] %p1)\n-      }\n-\n-      %fused_computation.1(param_0: s32[], param_1: s32[]) -> s32[] {\n-        %p0 = s32[] parameter(0)\n-        %p1 = s32[] parameter(1)\n-        ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-      }\n-\n-      ENTRY %main (a: s32[], b: s32[]) -> s32[] {\n-        %a = s32[] parameter(0)\n-        %b = s32[] custom-call(), custom_call_target=\"target\"\n-        %fusion = (s32[], s32[]) fusion(s32[] %a, s32[] %b), kind=kLoop, calls=%fused_computation\n-        %d = s32[] get-tuple-element((s32[], s32[]) %fusion), index=0\n-        %fusion.1 = s32[] fusion(s32[] %a, s32[] %d), kind=kLoop, calls=%fused_computation.1\n-        ROOT %custom-call = s32[] custom-call(s32[] %fusion.1, s32[] %d), custom_call_target=\"some target\"\n-      })\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnUnverifiedModule(hlo));\n-\n-  EXPECT_EQ(module->entry_computation()->instruction_count(), 6);\n-  std::vector<HloInstruction*> instructions;\n-  HloInstructionSequence seq;\n-  for (HloInstruction* inst : module->entry_computation()->instructions()) {\n-    if (HloPredicateIsOp<HloOpcode::kFusion, HloOpcode::kGetTupleElement>(\n-            inst)) {\n-      seq.push_back(inst);\n-    }\n-    instructions.push_back(inst);\n-  }\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      CommandBuffer command_buffer,\n-      CommandBufferScheduling::PrepareCommandBuffer(seq, module.get()));\n-  HloComputation* computation = module->AddComputation(\n-      std::move(command_buffer.computation), /*is_entry=*/false);\n-\n-  const char* expected = R\"(\n-// CHECK: %command_buffer ([[P0:.+]]: s32[], [[P1:.+]]: s32[]) -> (s32[], s32[]) {\n-// CHECK:  %[[P0]] = s32[] parameter(0)\n-// CHECK:  %[[P1]] = s32[] parameter(1)\n-// CHECK:  %fusion = (s32[], s32[]) fusion(%[[P0]], %[[P1]]), kind=kLoop, calls=%fused_computation\n-// CHECK:  %[[V0:.+]] = s32[] get-tuple-element(%fusion), index=0\n-// CHECK:  %fusion.1 = s32[] fusion(%[[P0]], %[[V0]]), kind=kLoop, calls=%fused_computation.1\n-// CHECK:  ROOT {{.*}} = (s32[], s32[]) tuple(%[[V0]], %fusion.1)\n-// CHECK:})\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      bool filecheck_matches,\n-      RunFileCheck(computation->ToString(\n-                       HloPrintOptions{}.set_print_operand_shape(false)),\n-                   expected));\n-  EXPECT_TRUE(filecheck_matches);\n-\n-  auto& arguments = command_buffer.arguments;\n-  ASSERT_EQ(arguments.size(), 2);\n-  EXPECT_EQ(arguments[0], instructions[0]);\n-  EXPECT_EQ(arguments[1], instructions[1]);\n-\n-  auto& results = command_buffer.results;\n-  ASSERT_EQ(results.size(), 2);\n-  EXPECT_EQ(results[0], instructions[3]);\n-  EXPECT_EQ(results[1], instructions[4]);\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, ForwardControlDependencies) {\n-  const char* hlo = R\"(\n-    HloModule TestModule, is_scheduled=true\n-\n-    %fused_computation (param_0: s32[], param_1: s32[]) -> s32[] {\n-      %p0 = s32[] parameter(0)\n-      %p1 = s32[] parameter(1)\n-      ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-    }\n-\n-    %fused_computation.1 (param_0: s32[], param_1: s32[]) -> s32[] {\n-      %p0 = s32[] parameter(0)\n-      %p1 = s32[] parameter(1)\n-      ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-    }\n-\n-    %fused_computation.2 (param_0: s32[], param_1: s32[]) -> s32[] {\n-      %p0 = s32[] parameter(0)\n-      %p1 = s32[] parameter(1)\n-      ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-    }\n-\n-    ENTRY %main (a: s32[], b: s32[]) -> s32[] {\n-      %a = s32[] parameter(0)\n-      %b = s32[] parameter(1)\n-      %custom-call = s32[] custom-call(), custom_call_target=\"some target\"\n-      %fusion = s32[] fusion(s32[] %a, s32[] %b), kind=kLoop, calls=%fused_computation, control-predecessors={%custom-call}\n-      %fusion.1 = s32[] fusion(s32[] %a, s32[] %b), kind=kLoop, calls=%fused_computation.1, control-predecessors={%fusion}\n-      %custom-call.1 = s32[] custom-call(), custom_call_target=\"some target\"\n-      %fusion.2 = s32[] fusion(s32[] %a, s32[] %b), kind=kLoop, calls=%fused_computation.2, control-predecessors={%fusion.1}\n-      ROOT %custom-call.2 = s32[] custom-call(s32[] %fusion.1, s32[] %fusion.2), custom_call_target=\"some target\"\n-    })\";\n-\n-  const char* expected = R\"(\n-    CHECK: %command_buffer ([[P0:.+]]: s32[], [[P1:.+]]: s32[]) -> s32[] {\n-    CHECK:   %[[P0]] = s32[] parameter(0)\n-    CHECK:   %[[P1]] = s32[] parameter(1)\n-    CHECK:   %[[F0:.+]] = s32[] fusion(%[[P0]], %[[P1]])\n-    CHECK:   ROOT {{.*}} = s32[] fusion(%[[P0]], %[[P1]]), {{.*}} control-predecessors={%[[F0]]}\n-    CHECK: }\n-\n-    CHECK: ENTRY %main (a: s32[], b: s32[]) -> s32[] {\n-    CHECK:   %a = s32[] parameter(0)\n-    CHECK:   %b = s32[] parameter(1)\n-    CHECK:   %custom-call = s32[] custom-call(), custom_call_target=\"some target\"\n-    CHECK:   %call = s32[] call(%a, %b), to_apply=%command_buffer, control-predecessors={%custom-call}\n-    CHECK:   %custom-call.1 = s32[] custom-call(), custom_call_target=\"some target\"\n-    CHECK:   %[[F3:.+]] = s32[] fusion(%a, %b), kind=kLoop, calls=%fused_computation.2, control-predecessors={%call}\n-    CHECK:   ROOT %custom-call.2 = s32[] custom-call(%call, %[[F3]]), custom_call_target=\"some target\"\n-    CHECK: })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            expected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, ForwardControlDependenciesToParams) {\n-  const char* hlo = R\"(\n-    HloModule TestModule, is_scheduled=true\n-\n-    %fused_computation.0 (p0: s32[], p1: s32[]) -> s32[] {\n-      %p0 = s32[] parameter(0)\n-      %p1 = s32[] parameter(1)\n-      ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-    }\n-\n-    %fused_computation.1 (p0: s32[], p1: s32[]) -> s32[] {\n-      %p0 = s32[] parameter(0)\n-      %p1 = s32[] parameter(1)\n-      ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-    }\n-\n-    ENTRY %main (a: s32[], b: s32[]) -> s32[] {\n-      %a = s32[] parameter(0)\n-      %b = s32[] parameter(1)\n-      %custom-call = s32[] custom-call(), custom_call_target=\"some target\"\n-      %fusion = s32[] fusion(s32[] %custom-call, s32[] %a), kind=kLoop, calls=%fused_computation.0, control-predecessors={%custom-call}\n-      ROOT %fusion.1 = s32[] fusion(s32[] %fusion, s32[] %b), kind=kLoop, calls=%fused_computation.1\n-    })\";\n-\n-  const char* expected = R\"(\n-    CHECK: ENTRY %main (a: s32[], b: s32[]) -> s32[] {\n-    CHECK:   %a = s32[] parameter(0)\n-    CHECK:   %b = s32[] parameter(1)\n-    CHECK:   %[[CUSTOM_CALL:.+]] = s32[] custom-call(), custom_call_target=\"some target\"\n-    CHECK:   ROOT {{.*}} call(%[[CUSTOM_CALL]], %a, %b), to_apply=%command_buffer, control-predecessors={%[[CUSTOM_CALL]]}\n-    CHECK: })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            expected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, WhileNotCommand) {\n-  const char* hlo = R\"(\n-    HloModule TestModule, is_scheduled=true\n-\n-    %fused_computation (param_0: f32[1]) -> f32[1] {\n-      %param_0 = f32[1]{0} parameter(0)\n-      ROOT %copy.5 = f32[1]{0} copy(f32[1]{0} %param_0)\n-    }\n-\n-    %fused_computation.1 (param_0.1: f32[1], param_1: f32[1]) -> f32[1] {\n-      %param_0.1 = f32[1]{0} parameter(0)\n-      %param_1 = f32[1]{0} parameter(1)\n-      ROOT %add.2 = f32[1]{0} add(f32[1]{0} %param_0.1, f32[1]{0} %param_1)\n-    }\n-\n-    %fused_computation.2 (param_0.2: f32[1], param_1.1: f32[1]) -> pred[1] {\n-      %param_0.2 = f32[1]{0} parameter(0)\n-      %param_1.1 = f32[1]{0} parameter(1)\n-      ROOT %compare.3 = pred[1]{0} compare(f32[1]{0} %param_0.2, f32[1]{0} %param_1.1), direction=LT\n-    }\n-\n-    %fused_computation.3 (param_0.1: f32[1], param_1: f32[1]) -> f32[1] {\n-      %param_0.1 = f32[1]{0} parameter(0)\n-      %param_1 = f32[1]{0} parameter(1)\n-      ROOT %add.2 = f32[1]{0} add(f32[1]{0} %param_0.1, f32[1]{0} %param_1)\n-    }\n-\n-    %body (Arg_.3: f32[1]) -> f32[1] {\n-      %constant_4 = f32[1]{0} constant({1})\n-      %Arg_.3 = f32[1]{0} parameter(0)\n-      %custom-call = s32[] custom-call(), custom_call_target=\"some target\"\n-      %add = f32[1]{0} fusion(f32[1]{0} %Arg_.3, f32[1]{0} %constant_4), kind=kLoop, calls=%fused_computation.1, control-predecessors={%custom-call}\n-      ROOT %wrapped_add.1 = f32[1]{0} fusion(f32[1]{0} %add, f32[1]{0} %constant_4), kind=kLoop, calls=%fused_computation.3, control-predecessors={%custom-call}\n-    }\n-\n-    %cond (Arg_.11: f32[1]) -> pred[] {\n-      %constant = f32[1]{0} constant({100})\n-      %Arg_.11 = f32[1]{0} parameter(0)\n-      %wrapped_compare.2 = pred[1]{0} fusion(f32[1]{0} %Arg_.11, f32[1]{0} %constant), kind=kLoop, calls=%fused_computation.2\n-      ROOT %bitcast = pred[] bitcast(pred[1]{0} %wrapped_compare.2)\n-    }\n-\n-    ENTRY %main.18 (Arg_0.1: f32[1]) -> f32[] {\n-      %Arg_0.1 = f32[1]{0} parameter(0), sharding={replicated}\n-      %wrapped_copy.4 = f32[1]{0} fusion(f32[1]{0} %Arg_0.1), kind=kLoop, calls=%fused_computation\n-      %while.16 = f32[1]{0} while(f32[1]{0} %wrapped_copy.4), condition=%cond, body=%body\n-      ROOT %bitcast.1 = f32[] bitcast(f32[1]{0} %while.16)\n-    })\";\n-\n-  const char* expected = R\"(\n-    CHECK: %command_buffer ([[P0:.+]]: f32[1], [[P1:.+]]: f32[1]) -> f32[1] {\n-    CHECK:   %[[P0]] = f32[1]{0} parameter(0)\n-    CHECK:   %[[P1]] = f32[1]{0} parameter(1)\n-    CHECK:   %[[ADD:.*]] = f32[1]{0} fusion(%[[P0]], %[[P1]]), kind=kLoop\n-    CHECK:   ROOT {{.*}} = f32[1]{0} fusion(%[[ADD]], %[[P1]]), kind=kLoop\n-    CHECK: }\n-\n-    CHECK: %[[BODY:[a-z_0-9.]+]] ([[P0:.+]]: f32[1]) -> f32[1] {\n-    CHECK:   %[[C1:.*]] = f32[1]{0} constant({1})\n-    CHECK:   %[[P0]] = f32[1]{0} parameter(0)\n-    CHECK:   %[[CC:.*]] = s32[] custom-call(), custom_call_target=\"some target\"\n-    CHECK:   ROOT %call = f32[1]{0} call(%[[P0]], %[[C1]]), to_apply=%command_buffer, control-predecessors={%[[CC]]}\n-    CHECK: }\n-\n-    CHECK: ENTRY %[[MAIN:.+]] ([[ARG0:.+]]: f32[1]) -> f32[] {\n-    CHECK:   %[[ARG0]] = f32[1]{0} parameter(0)\n-    CHECK:   %[[COPY:.*]] = f32[1]{0} fusion(%[[ARG0]]), kind=kLoop\n-    CHECK:   %[[WHILE:.*]] = f32[1]{0} while(%[[COPY]]), condition=%[[COND:[a-z_0-9.]+]], body=%[[BODY]]\n-    CHECK:   ROOT %[[BC:.+]] = f32[] bitcast(%[[WHILE]])\n-    CHECK: })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            expected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, While) {\n-  const auto& gpu_desc = GetGpuComputeCapability();\n-  if (gpu_desc.IsRocm()) {\n-    GTEST_SKIP() << \"Not supported for ROCm!\";\n-  }\n-  const char* hlo = R\"(\n-    HloModule TestModule, is_scheduled=true\n-\n-    %fused_computation (param_0: f32[1]) -> f32[1] {\n-      %param_0 = f32[1]{0} parameter(0)\n-      ROOT %copy.5 = f32[1]{0} copy(f32[1]{0} %param_0)\n-    }\n-\n-    %fused_computation.1 (param_0.1: f32[1], param_1: f32[1]) -> f32[1] {\n-      %param_0.1 = f32[1]{0} parameter(0)\n-      %param_1 = f32[1]{0} parameter(1)\n-      ROOT %add.2 = f32[1]{0} add(f32[1]{0} %param_0.1, f32[1]{0} %param_1)\n-    }\n-\n-    %fused_computation.2 (param_0.2: f32[1], param_1.1: f32[1]) -> pred[1] {\n-      %param_0.2 = f32[1]{0} parameter(0)\n-      %param_1.1 = f32[1]{0} parameter(1)\n-      ROOT %compare.3 = pred[1]{0} compare(f32[1]{0} %param_0.2, f32[1]{0} %param_1.1), direction=LT\n-    }\n-\n-    %body (Arg_.3: f32[1]) -> f32[1] {\n-      %constant_4 = f32[1]{0} constant({1})\n-      %Arg_.3 = f32[1]{0} parameter(0)\n-      ROOT %wrapped_add.1 = f32[1]{0} fusion(f32[1]{0} %Arg_.3, f32[1]{0} %constant_4), kind=kLoop, calls=%fused_computation.1\n-    }\n-\n-    %cond (Arg_.11: f32[1]) -> pred[] {\n-      %constant = f32[1]{0} constant({100})\n-      %Arg_.11 = f32[1]{0} parameter(0)\n-      %wrapped_compare.2 = pred[1]{0} fusion(f32[1]{0} %Arg_.11, f32[1]{0} %constant), kind=kLoop, calls=%fused_computation.2\n-      ROOT %bitcast = pred[] bitcast(pred[1]{0} %wrapped_compare.2)\n-    }\n-\n-    ENTRY %main.18 (Arg_0.1: f32[1]) -> f32[] {\n-      %Arg_0.1 = f32[1]{0} parameter(0), sharding={replicated}\n-      %wrapped_copy.4 = f32[1]{0} fusion(f32[1]{0} %Arg_0.1), kind=kLoop, calls=%fused_computation\n-      %while.16 = f32[1]{0} while(f32[1]{0} %wrapped_copy.4), condition=%cond, body=%body\n-      ROOT %bitcast.1 = f32[] bitcast(f32[1]{0} %while.16)\n-    })\";\n-\n-  const char* expected = R\"(\n-    CHECK: %command_buffer ([[P0:.+]]: f32[1]) -> f32[1] {\n-    CHECK:   %[[P0]] = f32[1]{0} parameter(0)\n-    CHECK:   %[[COPY:.*]] = f32[1]{0} fusion(%[[P0]]), kind=kLoop\n-    CHECK:   ROOT {{.*}} = f32[1]{0} while(%[[COPY]]), condition=%[[COND:[a-z_0-9.]+]], body=%[[BODY:[a-z_0-9.]+]]\n-    CHECK: }\n-\n-    CHECK: ENTRY %[[MAIN:.+]] ([[ARG0:.+]]: f32[1]) -> f32[] {\n-    CHECK:   %[[ARG0]] = f32[1]{0} parameter(0)\n-    CHECK:   %call = f32[1]{0} call(%[[ARG0]]), to_apply=%command_buffer\n-    CHECK:   ROOT %[[BC:.+]] = f32[] bitcast(%call)\n-    CHECK: })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            expected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, Conditional) {\n-  const auto& gpu_desc = GetGpuComputeCapability();\n-  if (gpu_desc.IsRocm()) {\n-    GTEST_SKIP() << \"Not supported for ROCm!\";\n-  }\n-  const char* hlo = R\"(\n-    HloModule TestModule, is_scheduled=true\n-\n-    %fused_computation.1 (param_0.2: s32[5]) -> s32[5] {\n-      %param_0.2 = s32[5]{0} parameter(0)\n-      ROOT %negate.2 = s32[5]{0} negate(s32[5]{0} %param_0.2)\n-    }\n-\n-    %region_0.7 (Arg_.8: s32[5]) -> (s32[5]) {\n-      %Arg_.8 = s32[5]{0} parameter(0)\n-      %wrapped_negate.1 = s32[5]{0} fusion(s32[5]{0} %Arg_.8), kind=kLoop, calls=%fused_computation.1\n-      ROOT %tuple.3 = (s32[5]{0}) tuple(s32[5]{0} %wrapped_negate.1)\n-    }\n-\n-    %fused_computation.2 (param_0.3: s32[5]) -> s32[5] {\n-      %param_0.3 = s32[5]{0} parameter(0)\n-      ROOT %not.2 = s32[5]{0} not(s32[5]{0} %param_0.3)\n-    }\n-\n-    %region_1.10 (Arg_.11: s32[5]) -> (s32[5]) {\n-      %Arg_.11 = s32[5]{0} parameter(0)\n-      %wrapped_not.1 = s32[5]{0} fusion(s32[5]{0} %Arg_.11), kind=kLoop, calls=%fused_computation.2\n-      ROOT %tuple.4 = (s32[5]{0}) tuple(s32[5]{0} %wrapped_not.1)\n-    }\n-\n-    %fused_computation.3 (param_0.4: s32[5]) -> s32[5] {\n-      %param_0.4 = s32[5]{0} parameter(0)\n-      ROOT %multiply.2 = s32[5]{0} multiply(s32[5]{0} %param_0.4, s32[5]{0} %param_0.4)\n-    }\n-\n-    %region_2.13 (Arg_.14: s32[5]) -> (s32[5]) {\n-      %Arg_.14 = s32[5]{0} parameter(0)\n-      %wrapped_multiply.1 = s32[5]{0} fusion(s32[5]{0} %Arg_.14), kind=kLoop, calls=%fused_computation.3\n-      ROOT %tuple.5 = (s32[5]{0}) tuple(s32[5]{0} %wrapped_multiply.1)\n-    }\n-\n-    %fused_computation (param_0.1: s64[]) -> s32[] {\n-      %constant_1 = s32[] constant(0)\n-      %param_0.1 = s64[] parameter(0)\n-      %convert.2 = s32[] convert(s64[] %param_0.1)\n-      %constant_0 = s32[] constant(2)\n-      ROOT %clamp.2 = s32[] clamp(s32[] %constant_1, s32[] %convert.2, s32[] %constant_0)\n-    }\n-\n-    ENTRY %main.17 (Arg_0.1: s64[], Arg_1.2: s32[5]) -> s32[5] {\n-      %Arg_0.1 = s64[] parameter(0), sharding={replicated}\n-      %fusion = s32[] fusion(s64[] %Arg_0.1), kind=kLoop, calls=%fused_computation\n-      %Arg_1.2 = s32[5]{0} parameter(1), sharding={replicated}\n-      %conditional.16.clone = (s32[5]{0}) conditional(s32[] %fusion, s32[5]{0} %Arg_1.2, s32[5]{0} %Arg_1.2, s32[5]{0} %Arg_1.2), branch_computations={%region_0.7, %region_1.10, %region_2.13}\n-      ROOT %get-tuple-element = s32[5]{0} get-tuple-element((s32[5]{0}) %conditional.16.clone), index=0\n-    })\";\n-\n-  const char* expected = R\"(\n-    CHECK: %command_buffer ([[P0:.+]]: s64[], [[P1:.+]]: s32[5]) -> (s32[5]) {\n-    CHECK:   %[[P0]] = s64[] parameter(0)\n-    CHECK:   %[[P1]] = s32[5]{0} parameter(1)\n-    CHECK:   %[[FUSION:.*]] = s32[] fusion(%[[P0]]), kind=kLoop\n-    CHECK:   ROOT {{.*}} = (s32[5]{0}) conditional(%[[FUSION]], %[[P1]], %[[P1]], %[[P1]]), branch_computations={%[[B1:[a-z_0-9.]+]], %[[B2:[a-z_0-9.]+]], %[[B3:[a-z_0-9.]+]]}\n-    CHECK: }\n-\n-    CHECK: ENTRY %[[MAIN:.+]] ([[ARG0:.+]]: s64[], [[ARG1:.+]]: s32[5]) -> s32[5] {\n-    CHECK:   %[[ARG0]] = s64[] parameter(0)\n-    CHECK:   %[[ARG1]] = s32[5]{0} parameter(1)\n-    CHECK:   %call = (s32[5]{0}) call(%[[ARG0]], %[[ARG1]]), to_apply=%command_buffer\n-    CHECK:   ROOT %[[GEP:.+]] = s32[5]{0} get-tuple-element(%call)\n-    CHECK: })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            expected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, CuDnnFusionGraphCaptureWorks) {\n-  const std::string kHloText = R\"(\n-HloModule m, is_scheduled=true\n-\n-fusion0 {\n-  p0 = f32[64,64] parameter(0)\n-  p1 = f32[64,64] parameter(1)\n-  ROOT d = f32[64,64] dot(p0, p1),\n-    lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-}\n-\n-fusion1 {\n-  p0 = f32[64,64] parameter(0)\n-  p1 = f32[64,64] parameter(1)\n-  ROOT d = f32[64,64] dot(p0, p1),\n-    lhs_contracting_dims={0}, rhs_contracting_dims={1}\n-}\n-\n-fusion_a {\n-  p0 = f32[64,64] parameter(0)\n-  p1 = f32[64,64] parameter(1)\n-  ROOT a = f32[64,64] add(p0, p1)\n-}\n-\n-ENTRY e {\n-  p0 = f32[64,64] parameter(0)\n-  p1 = f32[64,64] parameter(1)\n-  d0 = f32[64,64] fusion(p0, p1), kind=kCustom,\n-    calls=fusion0,\n-    backend_config={\"fusion_backend_config\": {\"kind\":\"__cudnn$fusion\"}}\n-  a = f32[64,64] fusion(d0, d0), kind=kLoop, calls=fusion_a\n-  ROOT d1 = f32[64,64] fusion(a, p1), kind=kCustom,\n-    calls=fusion1,\n-    backend_config={\"fusion_backend_config\": {\"kind\":\"__cudnn$fusion\"}}\n-})\";\n-\n-  const std::string kExpected = R\"(\n-; CHECK: ENTRY\n-; CHECK-NEXT: parameter\n-; CHECK-NEXT: parameter\n-; CHECK-NEXT: ROOT\n-; CHECK-SAME: call(\n-; CHECK-SAME: to_apply=%command_buffer\n-})\";\n-\n-  RunAndFilecheckHloRewrite(kHloText, CommandBufferScheduling(device_desc()),\n-                            kExpected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, AsyncCustomCall) {\n-  const char* hlo = R\"(\n-    HloModule m, is_scheduled=true\n-\n-    ENTRY %main (a: s32[], b: s32[]) -> f32[2,2] {\n-      %p = f32[2,2]{1,0} parameter(0)\n-      %start1 = ((f32[2,2], f32[2,2]), (f32[2,2], s8[4]), u32[]) custom-call-start(f32[2,2] %p, f32[2,2] %p), custom_call_target=\"__cublas$gemm\"\n-      %start2 = ((f32[2,2], f32[2,2]), (f32[2,2], s8[4]), u32[]) custom-call-start(f32[2,2] %p, f32[2,2] %p), custom_call_target=\"__cublas$gemm\"\n-      %done1 = (f32[2,2], s8[4]) custom-call-done(((f32[2,2], f32[2,2]), (f32[2,2], s8[4]), u32[]) %start1)\n-      %done2 = (f32[2,2], s8[4]) custom-call-done(((f32[2,2], f32[2,2]), (f32[2,2], s8[4]), u32[]) %start2)\n-      %result1 = f32[2,2] get-tuple-element((f32[2,2], s8[4]) %done1), index=0\n-      %result2 = f32[2,2] get-tuple-element((f32[2,2], s8[4]) %done2), index=0\n-      ROOT %sum = f32[2,2] add(f32[2,2] %result1, f32[2,2] %result2)\n-    })\";\n-\n-  const char* expected = R\"(\n-    CHECK: %command_buffer ([[P:.+]]: f32[2,2]) -> (f32[2,2], (f32[2,2], s8[4])) {\n-    CHECK:   %[[P]] = f32[2,2]{1,0} parameter(0)\n-    CHECK:   %[[S1:.+]] = ((f32[2,2]{1,0}, f32[2,2]{1,0}), (f32[2,2]{1,0}, s8[4]{0}), u32[]) custom-call-start(%[[P]], %[[P]]), custom_call_target=\"__cublas$gemm\"\n-    CHECK:   %[[S2:.+]] = ((f32[2,2]{1,0}, f32[2,2]{1,0}), (f32[2,2]{1,0}, s8[4]{0}), u32[]) custom-call-start(%[[P]], %[[P]]), custom_call_target=\"__cublas$gemm\"\n-    CHECK:   %[[D1:.+]] = (f32[2,2]{1,0}, s8[4]{0}) custom-call-done(%[[S1]])\n-    CHECK:   %[[D2:.+]] = (f32[2,2]{1,0}, s8[4]{0}) custom-call-done(%[[S2]])\n-    CHECK: })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            expected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, AsyncFusion) {\n-  const char* hlo = R\"(\n-    HloModule m, is_scheduled=true\n-\n-    add0 {\n-      %p0 = s32[] parameter(0)\n-      %p1 = s32[] parameter(1)\n-      ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-    }\n-\n-    add1 {\n-      %p0 = s32[] parameter(0)\n-      %p1 = s32[] parameter(1)\n-      ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-    }\n-\n-    ENTRY main {\n-      %a = s32[] parameter(0)\n-      %b = s32[] parameter(1)\n-      %start1 = ((s32[], s32[]), s32[], u32[]) fusion-start(%a, %b),\n-                kind=kLoop, calls=add0\n-      %start2 = ((s32[], s32[]), s32[], u32[]) fusion-start(%a, %b),\n-                kind=kLoop, calls=add1\n-      %done1 = s32[] fusion-done(%start1)\n-      %done2 = s32[] fusion-done(%start2)\n-      ROOT %tuple = (s32[], s32[]) tuple(%done1, %done2)\n-    })\";\n-\n-  const char* expected = R\"(\n-    CHECK: %command_buffer {{.*}} -> (s32[], s32[]) {\n-    CHECK:   %[[S1:.+]] = ((s32[], s32[]), s32[], u32[]) fusion-start\n-    CHECK:   %[[S2:.+]] = ((s32[], s32[]), s32[], u32[]) fusion-start\n-    CHECK:   %[[D1:.+]] = s32[] fusion-done(%[[S1]])\n-    CHECK:   %[[D2:.+]] = s32[] fusion-done(%[[S2]])\n-    CHECK:   ROOT {{.*}} = (s32[], s32[]) tuple(%[[D1]], %[[D2]])\n-    CHECK: })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            expected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, AsyncAlltoAll) {\n-  const char* hlo = R\"(\n-    HloModule m, is_scheduled=true\n-\n-    async_computation.1 {\n-    param.1 = f32[4,8,128]{2,1,0} parameter(0)\n-    ROOT all-to-all.1 = f32[4,8,128]{2,1,0} all-to-all(param.1), channel_id=1, dimensions={1}\n-    }\n-\n-    ENTRY main {\n-    param.0 = f32[4,8,128]{2,1,0} parameter(0)\n-    all-to-all-start = ((f32[4,8,128]{2,1,0}), f32[4,8,128]{2,1,0}) async-start(param.0), calls=async_computation.1\n-    ROOT all-to-all-done = f32[4,8,128]{2,1,0} async-done(all-to-all-start)\n-    })\";\n-\n-  const char* expected = R\"(\n-    CHECK: %command_buffer ([[P:.+]]: f32[4,8,128]) -> f32[4,8,128] {\n-    CHECK:   %[[P]] = f32[4,8,128]{2,1,0} parameter(0)\n-    CHECK:   %[[S1:.+]] = ((f32[4,8,128]{2,1,0}), f32[4,8,128]{2,1,0}) all-to-all-start(%[[P]]), channel_id=1, replica_groups={}, dimensions={1}\n-    CHECK:   ROOT {{.*}} = f32[4,8,128]{2,1,0} all-to-all-done(%[[S1]])\n-    CHECK: })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            expected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, DynamicSliceFusionStaticSlicing) {\n-  if (backend().platform()->Name() == \"Host\" || backend().device_count() < 2) {\n-    GTEST_SKIP() << \"Atleast two GPUs required for this test\";\n-  }\n-  const char* hlo = R\"(\n-  HloModule jit_slice, replica_count=2\n-\n-  add {\n-    a = s32[] parameter(0)\n-    b = s32[] parameter(1)\n-    ROOT add = add(a,b)\n-  }\n-\n-  ENTRY main.9 {\n-    p0 = s32[2,8,32]{2,1,0} parameter(0)\n-    p1 = s32[8,32]{1,0} parameter(1)\n-    a = s32[128,128] parameter(2)\n-    b = s32[128,128] parameter(3)\n-    c0 = s32[] constant(0)\n-    c1 = s32[] constant(1)\n-    slice = s32[1,8,32]{2,1,0} slice(p0), slice={[1:2], [0:8], [0:32]}\n-    input = s32[8,32]{1,0} reshape(slice)\n-    rs = s32[4,32] reduce-scatter(input), channel_id=64, replica_groups={{0,1}}, use_global_device_ids=true, dimensions={0}, to_apply=add\n-    dot = s32[128,128] dot(a,b), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-    ROOT tuple = tuple(rs, dot)\n-  })\";\n-\n-  HloModuleConfig config;\n-  DebugOptions options;\n-  options.set_xla_gpu_enable_dynamic_slice_fusion(true);\n-  options.set_xla_gpu_graph_min_graph_size(0);\n-  config.set_debug_options(options);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> m,\n-                          ParseAndReturnVerifiedModule(hlo, config));\n-  TF_ASSERT_OK_AND_ASSIGN(m, GetOptimizedModule(std::move(m)));\n-\n-  auto get_exec = [&m, this](DebugOptions options)\n-      -> absl::StatusOr<std::unique_ptr<GpuExecutable>> {\n-    std::unique_ptr<HloModule> m_clone = m->Clone();\n-    m_clone->mutable_config().set_debug_options(options);\n-    TF_ASSIGN_OR_RETURN(std::unique_ptr<OpaqueExecutable> wrapped_exec,\n-                        CreateExecutable(std::move(m_clone), false));\n-    TF_ASSIGN_OR_RETURN(std::unique_ptr<Executable> exec,\n-                        test_runner_as_hlo_runner().ExecutableFromWrapped(\n-                            std::move(wrapped_exec)));\n-    return std::unique_ptr<GpuExecutable>(\n-        static_cast<GpuExecutable*>(exec.release()));\n-  };\n-\n-  // DYNAMIC_SLICE_FUSION on, FUSION on\n-  {\n-    options.clear_xla_gpu_enable_command_buffer();\n-    options.add_xla_gpu_enable_command_buffer(\n-        DebugOptions::DYNAMIC_SLICE_FUSION);\n-    options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n-    TF_ASSERT_OK_AND_ASSIGN(auto gpu_exec, get_exec(options));\n-    Thunk* child = gpu_exec->GetThunk().thunks()[0].get();\n-    ASSERT_EQ(child->kind(), Thunk::kCommandBuffer);\n-  }\n-\n-  // DYNAMIC_SLICE_FUSION off, FUSION on\n-  {\n-    options.clear_xla_gpu_enable_command_buffer();\n-    options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n-    TF_ASSERT_OK_AND_ASSIGN(auto gpu_exec, get_exec(options));\n-    Thunk* child = gpu_exec->GetThunk().thunks()[0].get();\n-    ASSERT_NE(child->kind(), Thunk::kCommandBuffer);\n-  }\n-\n-  // Finally compare with/without command buffer.\n-  options.clear_xla_gpu_enable_command_buffer();\n-  m->mutable_config().set_debug_options(options);\n-  std::unique_ptr<HloModule> m_ref = m->Clone();\n-  m->mutable_config().mutable_debug_options().add_xla_gpu_enable_command_buffer(\n-      DebugOptions::DYNAMIC_SLICE_FUSION);\n-  m->mutable_config().mutable_debug_options().add_xla_gpu_enable_command_buffer(\n-      DebugOptions::FUSION);\n-  ASSERT_TRUE(RunAndCompareTwoModulesReplicated(std::move(m_ref), std::move(m),\n-                                                false, true, std::nullopt));\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, AsyncDynamicMemcpyFusion) {\n-  // Regression test to verify that async memcpy fusions are not commands.\n-  const char* hlo = R\"(\n-      HloModule m, is_scheduled=true\n-\n-      %fused_computation {\n-        p0 = s32[64] parameter(0)\n-        c32 = s32[] constant(32)\n-        ROOT %slice = s32[32] dynamic-slice(p0, c32), dynamic_slice_sizes={32}\n-      }\n-      \n-      %async_computation {\n-        p0 = s32[64] parameter(0)\n-        ROOT fusion0 = s32[32] fusion(p0), kind=kLoop,\n-          calls=%fused_computation,\n-          backend_config={\"fusion_backend_config\":{\"kind\":\"__dynamic_memcpy\"}}\n-      }\n-      \n-      main {\n-        p0 = s32[64] parameter(0)\n-        async-start = ((s32[64]), s32[32]) async-start(p0),\n-          calls=async_computation\n-        ROOT async-done = s32[32] async-done(async-start)\n-      })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            std::nullopt /* no change expected*/);\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, ReturnFalseWhenNoChange) {\n-  const char* hlo = R\"(\n-    HloModule module, is_scheduled=true\n-    ENTRY main {\n-      a = s32[8,8] parameter(0)\n-      b = s32[8,8] parameter(1)\n-      ROOT call = s32[8,8] custom-call(a,b), custom_call_target=\"__cublas$gemm\"\n-    }\n-  )\";\n-\n-  HloModuleConfig config;\n-  DebugOptions options = GetDebugOptionsForTest();\n-  options.clear_xla_gpu_enable_command_buffer();\n-  options.add_xla_gpu_enable_command_buffer(DebugOptions::COLLECTIVES);\n-  config.set_debug_options(options);\n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(hlo, config));\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            std::nullopt);\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, ReturnTrueWhenOnlyParamMoved) {\n-  const char* hlo = R\"(\n-    HloModule module, is_scheduled=true\n-    ENTRY main {\n-      a = s32[8,8] parameter(0)\n-      b = s32[8,8] parameter(1)\n-      call = s32[8,8] custom-call(a,b), custom_call_target=\"__cublas$gemm\"\n-      c = s32[8,8] parameter(2)\n-      ROOT call2 = s32[8,8] custom-call(call, c), custom_call_target=\"__cublas$gemm\"\n-    }\n-  )\";\n-\n-  HloModuleConfig config;\n-  DebugOptions options = GetDebugOptionsForTest();\n-  options.clear_xla_gpu_enable_command_buffer();\n-  options.add_xla_gpu_enable_command_buffer(DebugOptions::COLLECTIVES);\n-  config.set_debug_options(options);\n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(hlo, config));\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()), R\"(\n-    // CHECK: %{{.+}} = {{.+}} parameter(0)\n-    // CHECK: %{{.+}} = {{.+}} parameter(1)\n-    // CHECK: %{{.+}} = {{.+}} parameter(2)\n-    // CHECK: %{{.+}} = {{.+}} custom-call\n-    // CHECK: %{{.+}} = {{.+}} custom-call\n-  )\");\n-}\n-\n-TEST_F(CommandBufferSchedulingTest,\n-       DynamicSliceFusionWithDynamicAddressesNotACommand) {\n-  // This is not implemented yet. Once this is implemented in codegen, we can\n-  // remove this test.\n-  if (backend().platform()->Name() == \"Host\") {\n-    GTEST_SKIP() << \"This test requires GPU.\";\n-  }\n-  if (test_runner().device_count() < 2) {\n-    GTEST_SKIP() << \"Skipping test as it requires at least 2 devices.\";\n-  }\n-  const char* hlo = R\"(\n-    HloModule test, replica_count=2\n-    add {\n-      x = s32[] parameter(0)\n-      y = s32[] parameter(1)\n-      ROOT add = s32[] add(x, y)\n-    }\n-    ENTRY main {\n-      destination = s32[2,2,32] parameter(0)\n-      c1 = s32[] constant(1)\n-      c0 = s32[] constant(0)\n-      c4 = s32[] constant(4)\n-      source = s32[8,32] parameter(1)\n-      a = s32[1024,1024] parameter(2)\n-      b = s32[1024,1024] parameter(3)\n-      slice = s32[4,32] slice(source), slice={[4:8], [0:32]}\n-      rs = s32[2,32] reduce-scatter(slice), replica_groups={{0,1}}, dimensions={0}, to_apply=add\n-      reshape = s32[1,2,32] reshape(rs)\n-      dus = s32[2,2,32] dynamic-update-slice(destination, reshape, c1, c0, c0)\n-      dot = s32[1024,1024] dot(a,b), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n-      ROOT tuple = tuple(dus,dot)\n-    }\n-  )\";\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> m,\n-                          ParseAndReturnVerifiedModule(hlo));\n-  auto m_ref = m->Clone();\n-  m->mutable_config().mutable_debug_options().add_xla_gpu_enable_command_buffer(\n-      DebugOptions::DYNAMIC_SLICE_FUSION);\n-  m->mutable_config()\n-      .mutable_debug_options()\n-      .set_xla_gpu_enable_dynamic_slice_fusion(true);\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> m_opt,\n-                          GetOptimizedModule(m->Clone()));\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::unique_ptr<OpaqueExecutable> wrapped_exec,\n-      CreateExecutable(std::move(m_opt), /*run_hlo_passes=*/false));\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Executable> exec,\n-                          test_runner_as_hlo_runner().ExecutableFromWrapped(\n-                              std::move(wrapped_exec)));\n-  HloInstruction* fusion_start =\n-      FindInstruction(&exec->module(), HloOpcode::kAsyncStart);\n-  HloInstruction* fusion_done =\n-      FindInstruction(&exec->module(), HloOpcode::kAsyncDone);\n-  ASSERT_NE(fusion_start, nullptr);\n-  ASSERT_NE(fusion_done, nullptr);\n-  EXPECT_EQ(fusion_start->parent(), exec->module().entry_computation());\n-  EXPECT_EQ(fusion_done->parent(), exec->module().entry_computation());\n-  EXPECT_TRUE(RunAndCompareTwoModulesReplicated(std::move(m_ref), std::move(m),\n-                                                /*run_hlo_passes=*/true,\n-                                                /*use_threads=*/true,\n-                                                /*error=*/std::nullopt));\n-}\n-\n-TEST_F(CommandBufferSchedulingTest, MoveGTEs) {\n-  const char* hlo = R\"(\n-      HloModule m, is_scheduled=true\n-\n-      %fused_computation (param_0: s32[], param_1: s32[]) -> s32[] {\n-        %p0 = s32[] parameter(0)\n-        %p1 = s32[] parameter(1)\n-        ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-      }\n-\n-      %fused_computation.1 (param_0: s32[], param_1: s32[]) -> s32[] {\n-        %p0 = s32[] parameter(0)\n-        %p1 = s32[] parameter(1)\n-        ROOT %add = s32[] add(s32[] %p0, s32[] %p1)\n-      }\n-\n-      main {\n-        x = s32[] parameter(0)\n-        t = (s32[], f32[10000]) custom-call(), custom_call_target=\"some target\"\n-        fusion0 = s32[] fusion(x, x), kind=kLoop, calls=%fused_computation\n-        t0 = s32[] get-tuple-element(t), index=0\n-        ROOT fusion1 = s32[] fusion(fusion0, t0), kind=kLoop, calls=%fused_computation.1\n-      })\";\n-\n-  // The get-tuple-element instruction is moved right after its usage.\n-  const char* expected = R\"(\n-// CHECK:  %command_buffer ([[P0:.+]]: s32[], [[P1:.+]]: s32[]) -> s32[] {\n-// CHECK:    %[[P0]] = s32[] parameter(0)\n-// CHECK:    %[[P1]] = s32[] parameter(1)\n-// CHECK:    %fusion0.1 = s32[] fusion(%[[P0]], %[[P0]]), kind=kLoop, calls=%fused_computation\n-// CHECK:    ROOT %fusion1.1 = s32[] fusion(%fusion0.1, %[[P1]]), kind=kLoop, calls=%fused_computation.1\n-// CHECK:  }\n-\n-// CHECK:  ENTRY %main (x: s32[]) -> s32[] {\n-// CHECK:    %x = s32[] parameter(0)\n-// CHECK:    %t = (s32[], f32[10000]{0}) custom-call(), custom_call_target=\"some target\"\n-// CHECK:    %t0 = s32[] get-tuple-element(%t), index=0\n-// CHECK:    ROOT {{.*}} = s32[] call(%x, %t0), to_apply=%command_buffer\n-// CHECK:  })\";\n-\n-  RunAndFilecheckHloRewrite(hlo, CommandBufferScheduling(device_desc()),\n-                            expected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-class CommandBufferSchedulingTestNoMinSize\n-    : public CommandBufferSchedulingTest {\n- public:\n-  DebugOptions GetDebugOptionsForTest() const override {\n-    auto debug_options = CommandBufferSchedulingTest::GetDebugOptionsForTest();\n-    debug_options.set_xla_gpu_graph_min_graph_size(1);\n-    return debug_options;\n-  }\n-};\n-\n-TEST_F(CommandBufferSchedulingTestNoMinSize, BlockScaledDotGraphCaptureWorks) {\n-  const std::string kHloText = R\"(\n-HloModule m, is_scheduled=true\n-\n-ENTRY e {\n-  %lhs = f8e4m3fn[4,128,128] parameter(0)\n-  %rhs = f8e4m3fn[4,128,128] parameter(1)\n-  %lhs_scale = f8e8m0fnu[4,128,4] parameter(2)\n-  %rhs_scale = f8e8m0fnu[4,128,4] parameter(3)\n-  ROOT %result = f16[4,128,128] custom-call(%lhs, %rhs, %lhs_scale, %rhs_scale),\n-      custom_call_target=\"__cudnn$blockScaledDot\"\n-})\";\n-\n-  const std::string kExpected = R\"(\n-; CHECK: to_apply=%command_buffer\n-})\";\n-\n-  RunAndFilecheckHloRewrite(kHloText, CommandBufferScheduling(device_desc()),\n-                            kExpected, [](HloModule* module) {\n-                              EXPECT_TRUE(module->has_schedule());\n-                              CHECK_OK(module->schedule().Verify());\n-                            });\n-}\n-\n-}  // namespace\n-}  // namespace xla::gpu"
        },
        {
            "sha": "0982c10ad42298ce27c401135c3669d2621d6f95",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2bfafb60d8fb915525909c0aaa8c9e3990889e8e/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2bfafb60d8fb915525909c0aaa8c9e3990889e8e/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=2bfafb60d8fb915525909c0aaa8c9e3990889e8e",
            "patch": "@@ -652,9 +652,6 @@ message DebugOptions {\n   // inputs/outputs.\n   optional bool xla_gpu_experimental_enable_checksum_tracing_on_thunks = 414;\n \n-  // Enables an experimental feature for command buffer conversion on thunks.\n-  optional bool xla_gpu_experimental_enable_command_buffer_on_thunks = 394;\n-\n   // If true, enable autotuning between the native & triton fusion emitters.\n   optional bool xla_gpu_experimental_enable_fusion_autotuner = 409;\n \n@@ -1428,14 +1425,15 @@ message DebugOptions {\n   reserved \"xla_hlo_tfgraph_device_scopes\";\n   reserved \"xla_use_shardy\";\n   reserved \"xla_gpu_unsupported_annotate_with_emitter_loc\";\n+  reserved \"xla_gpu_experimental_enable_command_buffer_on_thunks\";\n \n   reserved 5, 63, 80, 93, 94, 98, 117, 130, 133, 134, 139, 141, 143, 152, 158,\n       160, 161, 162, 167, 168, 169, 171, 172, 173, 176, 177, 178, 179, 180, 183,\n       184, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 204, 206,\n       207, 211, 214, 218, 220, 221, 226, 229, 230, 233, 234, 238, 242, 249, 263,\n       264, 266, 270, 271, 275, 276, 278, 279, 281, 282, 286, 298, 299, 302, 303,\n       309, 313, 314, 319, 320, 325, 326, 332, 346, 352, 358, 361, 367, 369, 371,\n-      385, 398, 402, 423;\n+      385, 394, 398, 402, 423;\n }\n \n // Contains flags which affects the GPU compilation result."
        }
    ],
    "stats": {
        "total": 2617,
        "additions": 5,
        "deletions": 2612
    }
}