{
    "author": "pifon2a",
    "message": "[XLA:GPU] Pass the llvm module explicitly to the BuildKernelPrototype.\n\nPiperOrigin-RevId: 836224001",
    "sha": "1b4c8a955619f4b85f6eed7eb6f24e2e182e3403",
    "files": [
        {
            "sha": "2dcc248a5c0e4c7c34bd2d1de1a92d69e025c33c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b4c8a955619f4b85f6eed7eb6f24e2e182e3403/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b4c8a955619f4b85f6eed7eb6f24e2e182e3403/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD?ref=1b4c8a955619f4b85f6eed7eb6f24e2e182e3403",
            "patch": "@@ -249,7 +249,6 @@ cc_library(\n         \"//xla/codegen/emitters:kernel_api_builder\",\n         \"//xla/codegen/emitters:kernel_arguments\",\n         \"//xla/hlo/analysis:indexing_analysis\",\n-        \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/runtime:work_dimensions\",\n         \"//xla/service/gpu:ir_emitter_context\",\n@@ -270,7 +269,6 @@ cc_library(\n         \"@llvm-project//llvm:ir_headers\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:LLVMDialect\",\n-        \"@llvm-project//mlir:TensorDialect\",\n     ],\n )\n "
        },
        {
            "sha": "25e6e45a13e1706e26dce034d0c3693db9ccc8aa",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b4c8a955619f4b85f6eed7eb6f24e2e182e3403/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b4c8a955619f4b85f6eed7eb6f24e2e182e3403/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc?ref=1b4c8a955619f4b85f6eed7eb6f24e2e182e3403",
            "patch": "@@ -114,23 +114,21 @@ std::string GetSanitizedUniqueName(IrEmitterContext& ir_emitter_context,\n }\n \n absl::StatusOr<llvm::Function*> BuildKernelPrototype(\n-    IrEmitterContext& ir_emitter_context, const std::string& impl_fn_name,\n-    const std::string& suggested_name,\n+    llvm::Module* llvm_module, const se::DeviceDescription& gpu_device_info,\n+    const std::string& impl_fn_name, const std::string& unique_kernel_name,\n     const emitters::KernelArguments& arguments,\n     const LaunchDimensions& launch_dimensions, llvm::IRBuilderBase* builder) {\n   return BuildKernelPrototypeFromUniqueName(\n-      ir_emitter_context, impl_fn_name,\n-      GetSanitizedUniqueName(ir_emitter_context, suggested_name), arguments,\n+      llvm_module, gpu_device_info, impl_fn_name, unique_kernel_name, arguments,\n       launch_dimensions, builder);\n }\n \n absl::StatusOr<llvm::Function*> BuildKernelPrototypeFromUniqueName(\n-    IrEmitterContext& ir_emitter_context, const std::string& impl_fn_name,\n-    const std::string& unique_kernel_name,\n+    llvm::Module* llvm_module, const se::DeviceDescription& gpu_device_info,\n+    const std::string& impl_fn_name, const std::string& unique_kernel_name,\n     const emitters::KernelArguments& arguments,\n     const LaunchDimensions& launch_dimensions, llvm::IRBuilderBase* builder) {\n   // Create the kernel and add it to the module.\n-  auto* llvm_module = ir_emitter_context.llvm_module();\n   llvm::LLVMContext& context = llvm_module->getContext();\n   // Explicitly set global addrspace for SPIR backend.\n   int addrspace = llvm::Triple(llvm_module->getTargetTriple()).isSPIR() ? 1 : 0;\n@@ -144,9 +142,8 @@ absl::StatusOr<llvm::Function*> BuildKernelPrototypeFromUniqueName(\n                              unique_kernel_name, llvm_module);\n \n   AnnotateFunctionAsGpuKernel(llvm_module, kernel, builder);\n-  TF_RETURN_IF_ERROR(\n-      AnnotateKernelLaunchDimensions(ir_emitter_context.gpu_device_info(),\n-                                     launch_dimensions, kernel, llvm_module));\n+  TF_RETURN_IF_ERROR(AnnotateKernelLaunchDimensions(\n+      gpu_device_info, launch_dimensions, kernel, llvm_module));\n \n   // Update the insert point to the entry basic block.\n   llvm::BasicBlock* entry_bb ="
        },
        {
            "sha": "fef11b1f20540d4d30db7b8dac7eb499e1d3f6af",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b4c8a955619f4b85f6eed7eb6f24e2e182e3403/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b4c8a955619f4b85f6eed7eb6f24e2e182e3403/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.h?ref=1b4c8a955619f4b85f6eed7eb6f24e2e182e3403",
            "patch": "@@ -100,14 +100,14 @@ class KernelFusionInterface : public FusionInterface {\n };\n \n absl::StatusOr<llvm::Function*> BuildKernelPrototype(\n-    IrEmitterContext& ir_emitter_context, const std::string& impl_fn_name,\n-    const std::string& suggested_name,\n+    llvm::Module* llvm_module, const se::DeviceDescription& gpu_device_info,\n+    const std::string& impl_fn_name, const std::string& unique_kernel_name,\n     const emitters::KernelArguments& arguments,\n     const LaunchDimensions& launch_dimensions, llvm::IRBuilderBase* builder);\n \n absl::StatusOr<llvm::Function*> BuildKernelPrototypeFromUniqueName(\n-    IrEmitterContext& ir_emitter_context, const std::string& impl_fn_name,\n-    const std::string& unique_kernel_name,\n+    llvm::Module* llvm_module, const se::DeviceDescription& gpu_device_info,\n+    const std::string& impl_fn_name, const std::string& unique_kernel_name,\n     const emitters::KernelArguments& arguments,\n     const LaunchDimensions& launch_dimensions, llvm::IRBuilderBase* builder);\n "
        },
        {
            "sha": "9f8556ce68738ee14323295421b04079a34a0dbe",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b4c8a955619f4b85f6eed7eb6f24e2e182e3403/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b4c8a955619f4b85f6eed7eb6f24e2e182e3403/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=1b4c8a955619f4b85f6eed7eb6f24e2e182e3403",
            "patch": "@@ -159,10 +159,8 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n   auto generate = [&]() -> absl::StatusOr<KernelReuseCache::Entry> {\n     VLOG(3) << \"Generating: \" << suggested_kernel_name;\n \n-    const std::string impl_fn_name =\n-        ir_emitter_context.name_uniquer()->GetUniqueName(\n-            llvm_ir::SanitizeFunctionName(\n-                absl::StrCat(suggested_kernel_name, \"_impl\")));\n+    const std::string impl_fn_name = GetSanitizedUniqueName(\n+        ir_emitter_context, absl::StrCat(suggested_kernel_name, \"_impl\"));\n \n     TF_ASSIGN_OR_RETURN(\n         TritonWrapperResult triton_wrapper_result,\n@@ -214,9 +212,11 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n \n     TF_ASSIGN_OR_RETURN(\n         llvm::Function * kernel,\n-        BuildKernelPrototype(ir_emitter_context, impl_fn_name,\n-                             suggested_kernel_name, kernel_arguments,\n-                             launch_dimensions, &builder));\n+        BuildKernelPrototype(\n+            ir_emitter_context.llvm_module(),\n+            ir_emitter_context.gpu_device_info(), impl_fn_name,\n+            GetSanitizedUniqueName(ir_emitter_context, suggested_kernel_name),\n+            kernel_arguments, launch_dimensions, &builder));\n \n     PopulateNvvmAnnotations(ir_emitter_context.llvm_module(), kernel,\n                             triton_wrapper_result);"
        },
        {
            "sha": "31b5cd515656e2a9f0fc75882ead83bb0516844b",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b4c8a955619f4b85f6eed7eb6f24e2e182e3403/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b4c8a955619f4b85f6eed7eb6f24e2e182e3403/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=1b4c8a955619f4b85f6eed7eb6f24e2e182e3403",
            "patch": "@@ -582,6 +582,7 @@ cc_library(\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Linker\",\n         \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//llvm:TargetParser\",\n         \"@llvm-project//llvm:ir_headers\",\n         \"@llvm-project//mlir:AsmParser\",\n         \"@llvm-project//mlir:BuiltinToLLVMIRTranslation\","
        },
        {
            "sha": "afe44e100d9bc49377cbda77d0abcc14f3953afb",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 17,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b4c8a955619f4b85f6eed7eb6f24e2e182e3403/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b4c8a955619f4b85f6eed7eb6f24e2e182e3403/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=1b4c8a955619f4b85f6eed7eb6f24e2e182e3403",
            "patch": "@@ -56,6 +56,7 @@ limitations under the License.\n #include \"llvm/IR/Module.h\"\n #include \"llvm/Linker/Linker.h\"\n #include \"llvm/Support/Casting.h\"\n+#include \"llvm/TargetParser/Triple.h\"\n #include \"mlir/AsmParser/AsmParser.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Dialect/MemRef/Transforms/Passes.h\"\n@@ -353,13 +354,14 @@ absl::Status IrEmitterUnnested::EmitPadToStatic(\n     const HloCustomCallInstruction* instr) {\n   int unroll_factor = 1;\n   std::string ir_name = std::string(instr->name());\n-\n   const Shape& input_shape = instr->operand(0)->shape();\n \n   LaunchDimensions launch_dimensions = CalculateLaunchDimensions(\n       input_shape, ir_emitter_context_->gpu_device_info(), {unroll_factor});\n-  TF_ASSIGN_OR_RETURN(std::vector<llvm_ir::IrArray> ir_arrays,\n-                      BuildKernelThunkForNonFusionOp(instr, launch_dimensions));\n+  TF_ASSIGN_OR_RETURN(\n+      std::vector<llvm_ir::IrArray> ir_arrays,\n+      BuildKernelThunkForNonFusionOp(ir_emitter_context_->llvm_module(), instr,\n+                                     launch_dimensions));\n \n   const llvm_ir::IrArray& source_array = ir_arrays[0];\n   const llvm_ir::IrArray& output_array = ir_arrays[1];\n@@ -490,8 +492,10 @@ absl::Status IrEmitterUnnested::EmitSliceToDynamic(\n       input_shape, ir_emitter_context_->gpu_device_info(), {unroll_factor});\n   llvm::Type* index_ty =\n       GetIndexTypeForKernel(instr, launch_dimensions.launch_bound(), &b_);\n-  TF_ASSIGN_OR_RETURN(std::vector<llvm_ir::IrArray> ir_arrays,\n-                      BuildKernelThunkForNonFusionOp(instr, launch_dimensions));\n+  TF_ASSIGN_OR_RETURN(\n+      std::vector<llvm_ir::IrArray> ir_arrays,\n+      BuildKernelThunkForNonFusionOp(ir_emitter_context_->llvm_module(), instr,\n+                                     launch_dimensions));\n \n   const Shape& data_shape = ShapeUtil::MakeStaticShape(instr->shape());\n   TF_RET_CHECK(data_shape.IsArray());\n@@ -1423,8 +1427,7 @@ absl::Status IrEmitterUnnested::EmitTritonCustomCall(\n     LoadMlirDialectsForTriton(mlir_context);\n     auto call =\n         TritonCall::Parse(instr->raw_backend_config_string(), &mlir_context);\n-    auto kernel_name =\n-        ir_emitter_context_->name_uniquer()->GetUniqueName(call.name);\n+    auto kernel_name = GetSanitizedUniqueName(*ir_emitter_context_, call.name);\n     VLOG(3) << \"Generating: \" << kernel_name;\n \n     mlir::OwningOpRef<mlir::ModuleOp> triton_module;\n@@ -1485,16 +1488,17 @@ absl::Status IrEmitterUnnested::EmitTritonCustomCall(\n       llvm::Function* impl_fn =\n           ir_emitter_context_->llvm_module()->getFunction(kernel_name);\n       TF_RET_CHECK(impl_fn);\n-      impl_fn->setName(ir_emitter_context_->name_uniquer()->GetUniqueName(\n-          kernel_name + \"_impl\"));\n+      impl_fn->setName(\n+          GetSanitizedUniqueName(*ir_emitter_context_, kernel_name + \"_impl\"));\n \n       llvm::IRBuilder builder(ir_emitter_context_->llvm_module()->getContext());\n \n       TF_ASSIGN_OR_RETURN(llvm::Function * kernel,\n                           BuildKernelPrototypeFromUniqueName(\n-                              *ir_emitter_context_, impl_fn->getName().str(),\n-                              sanitized_kernel_name, kernel_arguments,\n-                              launch_dimensions, &builder));\n+                              ir_emitter_context_->llvm_module(),\n+                              ir_emitter_context_->gpu_device_info(),\n+                              impl_fn->getName().str(), sanitized_kernel_name,\n+                              kernel_arguments, launch_dimensions, &builder));\n \n       // Move function body into kernel prototype.\n       llvm::Function* prototype_func = builder.GetInsertBlock()->getParent();\n@@ -1707,6 +1711,7 @@ absl::Status IrEmitterUnnested::EmitRngGetAndUpdateState(\n   // Emit a kernel to increment the global state for Philox RNG\n   // algorithm.\n   TF_ASSIGN_OR_RETURN(auto ir_arrays, BuildKernelThunkForNonFusionOp(\n+                                          ir_emitter_context_->llvm_module(),\n                                           instr, LaunchDimensions()));\n   llvm::Value* old_state =\n       llvm_ir::RngGetAndUpdateState(instr->delta(), module_, &b_);\n@@ -1866,7 +1871,8 @@ absl::Status IrEmitterUnnested::EmitSort(const HloSortInstruction* sort) {\n                                              : standard_launch_dimensions;\n     TF_ASSIGN_OR_RETURN(\n         std::vector<llvm_ir::IrArray> ir_arrays,\n-        BuildKernelThunkForNonFusionOp(sort, launch_dimensions));\n+        BuildKernelThunkForNonFusionOp(ir_emitter_context_->llvm_module(), sort,\n+                                       launch_dimensions));\n \n     // The first `operand_count()` elements of `ir_arrays` are the input\n     // operands and the rest are the output arrays. Inputs are aliases with\n@@ -2619,7 +2625,8 @@ absl::Status IrEmitterUnnested::EmitOutfeed(\n \n absl::StatusOr<std::vector<llvm_ir::IrArray>>\n IrEmitterUnnested::BuildKernelThunkForNonFusionOp(\n-    const HloInstruction* instr, const LaunchDimensions& launch_dimensions) {\n+    llvm::Module* llvm_module, const HloInstruction* instr,\n+    const LaunchDimensions& launch_dimensions) {\n   std::string suggested_kernel_name(instr->name());\n \n   TF_ASSIGN_OR_RETURN(auto kernel_arguments,\n@@ -2631,9 +2638,11 @@ IrEmitterUnnested::BuildKernelThunkForNonFusionOp(\n \n   TF_ASSIGN_OR_RETURN(\n       llvm::Function * kernel,\n-      BuildKernelPrototype(*ir_emitter_context_, suggested_kernel_name,\n-                           suggested_kernel_name, kernel_arguments,\n-                           launch_dimensions, &b_));\n+      BuildKernelPrototype(\n+          llvm_module, ir_emitter_context_->gpu_device_info(),\n+          suggested_kernel_name,\n+          GetSanitizedUniqueName(*ir_emitter_context_, suggested_kernel_name),\n+          kernel_arguments, launch_dimensions, &b_));\n \n   AddThunkToThunkSequence(std::make_unique<KernelThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation("
        },
        {
            "sha": "f9222b2d4f31c8792eb7c1190114b7dd6beec4c3",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1b4c8a955619f4b85f6eed7eb6f24e2e182e3403/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1b4c8a955619f4b85f6eed7eb6f24e2e182e3403/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.h?ref=1b4c8a955619f4b85f6eed7eb6f24e2e182e3403",
            "patch": "@@ -319,7 +319,8 @@ class IrEmitterUnnested : public IrEmitter {\n   absl::Status EmitSliceToDynamic(const HloCustomCallInstruction* instr);\n \n   absl::StatusOr<std::vector<llvm_ir::IrArray>> BuildKernelThunkForNonFusionOp(\n-      const HloInstruction* instr, const LaunchDimensions& launch_dimensions);\n+      llvm::Module* llvm_module, const HloInstruction* instr,\n+      const LaunchDimensions& launch_dimensions);\n \n   // Returns a WhileThunk that invokes thunk sequences for 'condition' and\n   // 'body' sub-computations of while instruction."
        }
    ],
    "stats": {
        "total": 88,
        "additions": 47,
        "deletions": 41
    }
}