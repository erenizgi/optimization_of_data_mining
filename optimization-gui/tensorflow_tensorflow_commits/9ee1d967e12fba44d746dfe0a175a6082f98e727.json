{
    "author": "ermilovmaxim",
    "message": "initialize nvml in CudaPlatform\n\nPiperOrigin-RevId: 823193773",
    "sha": "9ee1d967e12fba44d746dfe0a175a6082f98e727",
    "files": [
        {
            "sha": "ae38d308320b961452bf66974de161d41d1acc74",
            "filename": "third_party/xla/xla/pjrt/gpu/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9ee1d967e12fba44d746dfe0a175a6082f98e727/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9ee1d967e12fba44d746dfe0a175a6082f98e727/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD?ref=9ee1d967e12fba44d746dfe0a175a6082f98e727",
            "patch": "@@ -175,7 +175,6 @@ cc_library(\n         \"//xla/service/gpu:stream_executor_util\",\n     ]) + if_cuda([\n         # keep sorted\n-        \"//xla/service/gpu/model:gpu_collective_performance_model\",\n         \"//xla/stream_executor/gpu:gpu_cudamallocasync_allocator\",\n         \"//xla/stream_executor/gpu:gpu_stream\",\n         \"@local_config_cuda//cuda:cuda_headers\","
        },
        {
            "sha": "738278d94f6ea457f616f73e04ef06a6e43debcb",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9ee1d967e12fba44d746dfe0a175a6082f98e727/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9ee1d967e12fba44d746dfe0a175a6082f98e727/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=9ee1d967e12fba44d746dfe0a175a6082f98e727",
            "patch": "@@ -136,7 +136,6 @@ limitations under the License.\n #include \"third_party/gpus/cuda/include/cuda.h\"\n #include \"third_party/gpus/cuda/include/cuda_runtime_api.h\"\n #include \"third_party/gpus/cuda/nvml/include/nvml.h\"\n-#include \"xla/service/gpu/model/gpu_collective_performance_model.h\"\n #include \"xla/stream_executor/gpu/gpu_cudamallocasync_allocator.h\"\n #elif TENSORFLOW_USE_ROCM\n #include \"rocm/rocm_config.h\"\n@@ -1177,10 +1176,6 @@ std::vector<std::unique_ptr<PjRtStreamExecutorDevice>> BuildLocalDevices(\n \n absl::StatusOr<std::string> GetDeviceFabricInfo(const int device_ordinal) {\n #if defined(GOOGLE_CUDA) && CUDA_VERSION >= 12040\n-  if (!gpu::GpuPerformanceWithCollectiveModel::InitNvml()) {\n-    return absl::InternalError(\"Failed to initialize NVML library.\");\n-  }\n-\n   char pciBusId[] = \"00000000:00:00.0\";\n   cudaDeviceGetPCIBusId(pciBusId, sizeof(pciBusId), device_ordinal);\n   nvmlDevice_t device;"
        },
        {
            "sha": "389808fa7cd930a053131263e5a4a47d4035d2b6",
            "filename": "third_party/xla/xla/service/gpu/model/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9ee1d967e12fba44d746dfe0a175a6082f98e727/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9ee1d967e12fba44d746dfe0a175a6082f98e727/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD?ref=9ee1d967e12fba44d746dfe0a175a6082f98e727",
            "patch": "@@ -500,25 +500,6 @@ cc_library(\n     ]),\n )\n \n-xla_test(\n-    name = \"gpu_collective_performance_model_test\",\n-    srcs = [\"gpu_collective_performance_model_test.cc\"],\n-    backends = [\"nvgpu_any\"],\n-    tags = [\n-        \"cuda-only\",\n-        \"gpu\",\n-    ],\n-    deps = [\n-        \":gpu_collective_performance_model\",\n-        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n-        \"//xla/service/gpu:backend_configs_cc\",\n-        \"//xla/tests:xla_internal_test_main\",\n-        \"@com_google_googletest//:gtest\",\n-    ] + if_cuda_is_configured([\n-        \"@local_config_cuda//cuda:cuda_headers\",\n-    ]),\n-)\n-\n cc_library(\n     name = \"gpu_indexing_performance_model\",\n     srcs = [\"gpu_indexing_performance_model.cc\"],"
        },
        {
            "sha": "83670af1a175091dd11189a04d79884a2d1f3631",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_collective_performance_model.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9ee1d967e12fba44d746dfe0a175a6082f98e727/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9ee1d967e12fba44d746dfe0a175a6082f98e727/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.cc?ref=9ee1d967e12fba44d746dfe0a175a6082f98e727",
            "patch": "@@ -380,42 +380,14 @@ RocmBandwidthSettings CreateSettings(\n \n }  // namespace\n \n-/*static*/ bool GpuPerformanceWithCollectiveModel::InitNvml() {\n-#if GOOGLE_CUDA && (defined(PLATFORM_POSIX) || defined(PLATFORM_GOOGLE))\n-  nvmlReturn_t init_result = nvmlInit();\n-  if (init_result != NVML_SUCCESS) {\n-    LOG(ERROR) << \"NVML init failed with \" << init_result;\n-  }\n-  return init_result == NVML_SUCCESS;\n-#elif TENSORFLOW_USE_ROCM\n-  return true;\n-#else\n-  return false;\n-#endif  // GOOGLE_CUDA\n-}\n-\n-/*static*/ bool GpuPerformanceWithCollectiveModel::ShutdownNvml() {\n-#if GOOGLE_CUDA\n-  nvmlReturn_t shutdown_result = nvmlShutdown();\n-  return shutdown_result == NVML_SUCCESS;\n-#elif TENSORFLOW_USE_ROCM\n-  return true;\n-#else\n-  return false;\n-#endif  // GOOGLE_CUDA\n-}\n-\n /*static*/ uint32_t\n GpuPerformanceWithCollectiveModel::CheckIfNvlinkSupportsP2P() {\n #if GOOGLE_CUDA\n   // We will use nvml library to detect nvlink capability\n   // to see if it supports p2p communication.\n-  // We first load libnvidia-ml.so and assign symbols to function pointers\n-  // to avoid linking errors.\n   // Then gpu 0 will be used to query for nvlink capability, note that\n   // we only look at link 0 of gpu 0 since all other links are assumed\n   // to have the same capability.\n-  CHECK(InitNvml()) << \"NVML init failed.\";\n   nvmlDevice_t nvml_device;\n   nvmlReturn_t get_device_result = nvmlDeviceGetHandleByIndex(0, &nvml_device);\n   CHECK(get_device_result == NVML_SUCCESS);\n@@ -427,7 +399,6 @@ GpuPerformanceWithCollectiveModel::CheckIfNvlinkSupportsP2P() {\n       &supported_p2p);\n   CHECK(nvlink_cap_result == NVML_SUCCESS ||\n         nvlink_cap_result == NVML_ERROR_NOT_SUPPORTED);\n-  CHECK(ShutdownNvml()) << \"NVML shutdown failed.\";\n   return supported_p2p;\n #else\n   return 0;"
        },
        {
            "sha": "e324e5fcd7121708ce8889a20276e8e75facd81b",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_collective_performance_model.h",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9ee1d967e12fba44d746dfe0a175a6082f98e727/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9ee1d967e12fba44d746dfe0a175a6082f98e727/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.h?ref=9ee1d967e12fba44d746dfe0a175a6082f98e727",
            "patch": "@@ -35,12 +35,6 @@ class GpuPerformanceWithCollectiveModel : public GpuPerformanceModelBase {\n       const HloInstruction& instr, const GpuHloCostAnalysis* cost_analysis,\n       const se::DeviceDescription& gpu_device_info);\n \n-  // Initialize nvml library.\n-  static bool InitNvml();\n-\n-  // Shut down nvml library.\n-  static bool ShutdownNvml();\n-\n   // This checks if the nvlink supports direct P2P communication,\n   // If not, we will use PCIE bandwidth to estimate latency.\n   static uint32_t CheckIfNvlinkSupportsP2P();"
        },
        {
            "sha": "a394d7b602df5c89510bd3a0f9199ed25710bc31",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_collective_performance_model_test.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d55e5c1d9f3f47a5e6459851bb6ba856e642575d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d55e5c1d9f3f47a5e6459851bb6ba856e642575d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model_test.cc?ref=d55e5c1d9f3f47a5e6459851bb6ba856e642575d",
            "patch": "@@ -1,42 +0,0 @@\n-/* Copyright 2024 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/service/gpu/model/gpu_collective_performance_model.h\"\n-\n-#include <gtest/gtest.h>\n-#include \"third_party/gpus/cuda/nvml/include/nvml.h\"\n-#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n-#include \"xla/service/gpu/backend_configs.pb.h\"\n-\n-namespace xla {\n-namespace gpu {\n-namespace {\n-\n-using GpuPerformanceWithCollectiveModelTest = HloHardwareIndependentTestBase;\n-\n-TEST_F(GpuPerformanceWithCollectiveModelTest, TestNvmlLibraryLoading) {\n-  EXPECT_TRUE(GpuPerformanceWithCollectiveModel::InitNvml());\n-  // After successful init, we try to use one of the\n-  // nvml functions to see if the result is good.\n-  nvmlDevice_t nvml_device;\n-  nvmlReturn_t get_device_result = nvmlDeviceGetHandleByIndex(0, &nvml_device);\n-  EXPECT_TRUE(get_device_result == NVML_SUCCESS);\n-\n-  EXPECT_TRUE(GpuPerformanceWithCollectiveModel::InitNvml());\n-}\n-\n-}  // namespace\n-}  // namespace gpu\n-}  // namespace xla"
        },
        {
            "sha": "ab9f4e0da68a329a56e6c0963fc5ada6154b1dd0",
            "filename": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "status": "modified",
            "additions": 30,
            "deletions": 29,
            "changes": 59,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9ee1d967e12fba44d746dfe0a175a6082f98e727/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9ee1d967e12fba44d746dfe0a175a6082f98e727/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD?ref=9ee1d967e12fba44d746dfe0a175a6082f98e727",
            "patch": "@@ -97,35 +97,35 @@ cc_library(\n         \"gpu\",\n     ],\n     visibility = [\"//visibility:public\"],\n-    deps =\n-        [\n-            \":cuda_diagnostics\",\n-            \":cuda_executor\",\n-            \":cuda_platform_id\",\n-            \":cuda_status\",\n-            \"//xla/stream_executor:device_description\",\n-            \"//xla/stream_executor:executor_cache\",\n-            \"//xla/stream_executor:platform\",\n-            \"//xla/stream_executor:platform_manager\",\n-            \"//xla/stream_executor:stream_executor_h\",\n-            \"//xla/stream_executor/platform:initialize\",\n-            \"//xla/tsl/platform:errors\",\n-            \"//xla/tsl/platform:status\",\n-            \"@com_google_absl//absl/base\",\n-            \"@com_google_absl//absl/base:core_headers\",\n-            \"@com_google_absl//absl/log\",\n-            \"@com_google_absl//absl/log:check\",\n-            \"@com_google_absl//absl/memory\",\n-            \"@com_google_absl//absl/status\",\n-            \"@com_google_absl//absl/status:statusor\",\n-            \"@com_google_absl//absl/strings\",\n-            \"@com_google_absl//absl/strings:str_format\",\n-            \"@com_google_absl//absl/synchronization\",\n-            \"@local_config_cuda//cuda:cuda_headers\",\n-            \"@local_tsl//tsl/platform:errors\",\n-            \"@local_tsl//tsl/platform:status\",\n-            \"@local_tsl//tsl/platform:statusor\",\n-        ] + tf_additional_cuda_platform_deps(),\n+    deps = [\n+        \":cuda_diagnostics\",\n+        \":cuda_executor\",\n+        \":cuda_platform_id\",\n+        \":cuda_status\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor:executor_cache\",\n+        \"//xla/stream_executor:platform\",\n+        \"//xla/stream_executor:platform_manager\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor/platform:initialize\",\n+        \"//xla/tsl/cuda:nvml\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:status\",\n+        \"@com_google_absl//absl/base\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/memory\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/synchronization\",\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+        \"@local_tsl//tsl/platform:errors\",\n+        \"@local_tsl//tsl/platform:status\",\n+        \"@local_tsl//tsl/platform:statusor\",\n+    ] + tf_additional_cuda_platform_deps(),\n     alwayslink = True,  # Registers itself with the PlatformManager.\n )\n \n@@ -867,6 +867,7 @@ xla_test(\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_googletest//:gtest_main\",\n+        \"@local_config_cuda//cuda:cuda_headers\",\n         \"@local_tsl//tsl/platform:statusor\",\n         \"@local_tsl//tsl/platform:test\",\n     ],"
        },
        {
            "sha": "19a37d4080e2fc0f1455ea2d63b7a4a7aa57ca91",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_platform.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 4,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9ee1d967e12fba44d746dfe0a175a6082f98e727/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_platform.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9ee1d967e12fba44d746dfe0a175a6082f98e727/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_platform.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_platform.cc?ref=9ee1d967e12fba44d746dfe0a175a6082f98e727",
            "patch": "@@ -23,7 +23,9 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"third_party/gpus/cuda/include/cuda.h\"\n+#include \"third_party/gpus/cuda/nvml/include/nvml.h\"\n #include \"xla/stream_executor/cuda/cuda_diagnostics.h\"\n #include \"xla/stream_executor/cuda/cuda_executor.h\"\n #include \"xla/stream_executor/cuda/cuda_platform_id.h\"\n@@ -44,14 +46,19 @@ namespace {\n static absl::Status InternalInit() {\n   absl::Status status =\n       cuda::ToStatus(cuInit(0 /* = flags */), \"Failed call to cuInit\");\n-  if (status.ok()) {\n+  if (!status.ok()) {\n+    LOG(ERROR) << \"failed call to cuInit: \" << status;\n+    cuda::Diagnostician::LogDiagnosticInformation();\n     return status;\n   }\n \n-  LOG(ERROR) << \"failed call to cuInit: \" << status;\n+  nvmlReturn_t init_result = nvmlInit();\n+  if (init_result != NVML_SUCCESS) {\n+    return absl::InternalError(\n+        absl::StrCat(\"NVML init failed with \", init_result));\n+  }\n \n-  cuda::Diagnostician::LogDiagnosticInformation();\n-  return status;\n+  return absl::OkStatus();\n }\n \n static absl::Status PlatformInitialize() {\n@@ -67,6 +74,13 @@ static absl::Status PlatformInitialize() {\n \n CudaPlatform::CudaPlatform() : name_(\"CUDA\") {}\n \n+CudaPlatform::~CudaPlatform() {\n+  nvmlReturn_t shutdown_result = nvmlShutdown();\n+  if (shutdown_result != NVML_SUCCESS) {\n+    LOG(ERROR) << \"NVML shutdown failed with \" << shutdown_result;\n+  }\n+}\n+\n Platform::Id CudaPlatform::id() const { return cuda::kCudaPlatformId; }\n \n int CudaPlatform::VisibleDeviceCount() const {"
        },
        {
            "sha": "855ed549dd9e9919515eb720060be2a2ba8ea120",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_platform.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9ee1d967e12fba44d746dfe0a175a6082f98e727/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_platform.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9ee1d967e12fba44d746dfe0a175a6082f98e727/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_platform.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_platform.h?ref=9ee1d967e12fba44d746dfe0a175a6082f98e727",
            "patch": "@@ -38,6 +38,7 @@ namespace gpu {\n class CudaPlatform : public Platform {\n  public:\n   CudaPlatform();\n+  ~CudaPlatform() override;\n \n   // Platform interface implementation:\n   // Returns the same value as kCudaPlatform above."
        },
        {
            "sha": "e0083a4aa7c5ddd8faae95a0e3fff726ac9d20b8",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_platform_test.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9ee1d967e12fba44d746dfe0a175a6082f98e727/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_platform_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9ee1d967e12fba44d746dfe0a175a6082f98e727/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_platform_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_platform_test.cc?ref=9ee1d967e12fba44d746dfe0a175a6082f98e727",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/log/check.h\"\n+#include \"third_party/gpus/cuda/nvml/include/nvml.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"tsl/platform/statusor.h\"\n@@ -45,5 +46,17 @@ TEST(CudaPlatformTest, FindExistingWorks) {\n   }\n }\n \n+TEST(CudaPlatformTest, NVML) {\n+  TF_ASSERT_OK_AND_ASSIGN(Platform * platform,\n+                          PlatformManager::PlatformWithName(\"CUDA\"));\n+  CHECK_GT(platform->VisibleDeviceCount(), 0);\n+\n+  // After successful init, we try to use one of the\n+  // nvml functions to see if the result is good.\n+  nvmlDevice_t nvml_device;\n+  nvmlReturn_t get_device_result = nvmlDeviceGetHandleByIndex(0, &nvml_device);\n+  EXPECT_TRUE(get_device_result == NVML_SUCCESS);\n+}\n+\n }  // namespace\n }  // namespace stream_executor::gpu"
        }
    ],
    "stats": {
        "total": 197,
        "additions": 62,
        "deletions": 135
    }
}