{
    "author": "loislo",
    "message": "[XLA:GPU] Enable autotuner for the scaled dot fusions.\n\nWe extend the autotuner to the scaled-dot case.\nThe autotuner has a very restrictive set of config for the scaled dot due to the issues of the scaled-dot -> regular dot AccelerateMatmul rewriter of Triton.\n\nThe numerics is correct for the config that actually works but requires a transpose for the rhs scales. For some reason the scales should have mk and nk shapes.\n\nPiperOrigin-RevId: 810568117",
    "sha": "c3492061fcc3759269e2d766648fd154c5bdd060",
    "files": [
        {
            "sha": "8422f26caedd356b177e8cb68ee00d32655bd793",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c3492061fcc3759269e2d766648fd154c5bdd060/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c3492061fcc3759269e2d766648fd154c5bdd060/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc?ref=c3492061fcc3759269e2d766648fd154c5bdd060",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/backends/gpu/codegen/triton/dot_algorithms.h\"\n \n+#include <cstdint>\n #include <limits>\n #include <optional>\n #include <string>\n@@ -155,6 +156,10 @@ absl::StatusOr<Value> ScaledDot(EmitterLocOpBuilder b,\n   auto lhs_scale = Bitcast(b, operands.lhs_scale, b.getI8Type());\n   auto rhs_scale = Bitcast(b, operands.rhs_scale, b.getI8Type());\n \n+  // TODO(b/436988479): Remove this once we have a fix for the scaled dot\n+  // rewrite on the Triton side. With this transpose we have matching numerics.\n+  rhs_scale = b.create<ttir::TransOp>(rhs_scale, mlir::ArrayRef<int32_t>{1, 0});\n+\n   // make type with the same shape as the scale but with i8 type\n   return b.create<ttir::DotScaledOp>(\n       operands.accumulator.getType(), operands.lhs, operands.rhs,"
        },
        {
            "sha": "520cba9fd01a19ab515cdc481f422e0d9bede66f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_port_test.cc",
            "status": "modified",
            "additions": 45,
            "deletions": 3,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c3492061fcc3759269e2d766648fd154c5bdd060/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c3492061fcc3759269e2d766648fd154c5bdd060/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc?ref=c3492061fcc3759269e2d766648fd154c5bdd060",
            "patch": "@@ -3393,7 +3393,7 @@ ENTRY e {\n   constexpr absl::string_view kExpectedTritonIrTmpl = R\"(\n       CHECK: tt.dot_scaled\n       CHECK: tensor<128x128x$triton_type>, tensor<128x4xi8>\n-      CHECK: tensor<128x256x$triton_type>, tensor<4x256xi8>\n+      CHECK: tensor<128x256x$triton_type>, tensor<256x4xi8>\n       CHECK: -> tensor<128x256xf32>\n   )\";\n   auto expected_triton_ir = absl::StrReplaceAll(\n@@ -3416,16 +3416,58 @@ ENTRY e {\n   }\n }\n \n+TEST_P(TritonScaledDotGemmTest, FP8ScaledDotGetsFusedAndExecutesCorrectly) {\n+  const ScaleDotTestParams& params = GetParam();\n+  if (!GetCudaComputeCapability().IsAtLeastBlackwell()) {\n+    GTEST_SKIP() << \"Skipping test for pre-Blackwell GPUs.\";\n+  }\n+  constexpr absl::string_view kHloTextTemplate = R\"hlo(\n+HloModule FP8ScaledDotGetsFused\n+\n+ENTRY e {\n+  lhs = $lhs_type parameter(0)\n+  lhs_scale = $lhs_scale_type parameter(1)\n+  rhs = $rhs_type parameter(2)\n+  rhs_scale = $rhs_scale_type parameter(3)\n+  ROOT _ = $output_type{1,0} scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+    lhs_contracting_dims={1},\n+    rhs_contracting_dims={0}\n+}\n+)hlo\";\n+\n+  auto hlo_text = params.PrepareHloText(kHloTextTemplate);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_text));\n+\n+  auto debug_options = module->config().debug_options();\n+  debug_options.set_xla_gpu_experimental_scaled_dot_with_triton(true);\n+  debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n+      DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n+  module->mutable_config().set_debug_options(debug_options);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto optimized_module,\n+                          GetOptimizedModule(std::move(module)));\n+  MatchOptimizedHlo(optimized_module->ToString(), R\"(\n+    CHECK: fusion\n+    CHECK: ROOT {{.*}} scaled-dot\n+    CHECK: ENTRY\n+    CHECK: __triton_nested_gemm_fusion\n+  )\");\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      std::move(optimized_module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n INSTANTIATE_TEST_SUITE_P(\n     TritonScaledDotGemmTest, TritonScaledDotGemmTest,\n     ::testing::Values(ScaleDotTestParams{\"f8e4m3fn[128,128]\",\n                                          \"f8e8m0fnu[128,4]\",\n                                          \"f8e4m3fn[128,256]\",\n-                                         \"f8e8m0fnu[4,256]\", \"f32[128,256]\",\n+                                         \"f8e8m0fnu[4,256]\", \"bf16[128,256]\",\n                                          \"f8E4M3FN\"},\n                       ScaleDotTestParams{\"f8e5m2[128,128]\", \"f8e8m0fnu[128,4]\",\n                                          \"f8e5m2[128,256]\", \"f8e8m0fnu[4,256]\",\n-                                         \"f32[128,256]\", \"f8E5M2\"}),\n+                                         \"bf16[128,256]\", \"f8E5M2\"}),\n     ScaleDotTestParams::ToString);\n \n }  // namespace gpu"
        },
        {
            "sha": "7c8efa0b1cb2500933e570bdeb076e77df42f889",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 7,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c3492061fcc3759269e2d766648fd154c5bdd060/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c3492061fcc3759269e2d766648fd154c5bdd060/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=c3492061fcc3759269e2d766648fd154c5bdd060",
            "patch": "@@ -662,7 +662,8 @@ absl::Status GemmFusionAutotunerRewriterVisitor::HandleFusion(\n   // Only autotune Triton, cuDNN, and custom kernel fusions.\n   if (fusion_backend_config.kind() != kTritonGemmFusionKind &&\n       fusion_backend_config.kind() != kCuDnnFusionKind &&\n-      fusion_backend_config.kind() != kCustomFusionKind) {\n+      fusion_backend_config.kind() != kCustomFusionKind &&\n+      fusion_backend_config.kind() != kTritonScaledDotFusionKind) {\n     return absl::OkStatus();\n   }\n \n@@ -900,12 +901,16 @@ absl::StatusOr<std::vector<BackendConfig>>\n GemmFusionAutotunerImpl::GenerateScaledDotConfigs(\n     const HloFusionInstruction& fusion, const HloScaledDotInstruction* dot) {\n   std::vector<BackendConfig> configs;\n-  // Add triton configs.\n-  TF_ASSIGN_OR_RETURN(std::vector<TritonGemmConfig> triton_configs,\n-                      GenerateTritonConfigs(*dot));\n-  configs.reserve(triton_configs.size());\n-  for (TritonGemmConfig& config : triton_configs) {\n-    configs.push_back(std::move(config));\n+  // TODO(b/436988479): fine tune the search space.\n+  for (int block_m = 32; block_m <= 128; block_m *= 2) {\n+    for (int block_n = 32; block_n <= 128; block_n *= 2) {\n+      configs.push_back(TritonGemmConfig(block_m, block_n,\n+                                         /*block_k=*/128, /*split_k=*/1,\n+                                         /*num_stages=*/1,\n+                                         /*num_warps=*/4,\n+                                         /*num_ctas=*/1,\n+                                         /*is_tma_allowed=*/false));\n+    }\n   }\n   return configs;\n }"
        },
        {
            "sha": "2ed8069e731d84188cd1d5211d3e5f7f5088ca4f",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c3492061fcc3759269e2d766648fd154c5bdd060/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c3492061fcc3759269e2d766648fd154c5bdd060/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=c3492061fcc3759269e2d766648fd154c5bdd060",
            "patch": "@@ -733,7 +733,9 @@ absl::Status RunOptimizationPasses(\n     pipeline.AddPass<UnstableReductionDetector>();\n   }\n   pipeline.AddPass<RaggedDotRewriter>();\n-  pipeline.AddPass<ScaledDotRewriter>();\n+  if (!debug_options.xla_gpu_experimental_scaled_dot_with_triton()) {\n+    pipeline.AddPass<ScaledDotRewriter>();\n+  }\n   pipeline.AddPass<BatchedGatherScatterNormalizer>();\n   if (debug_options.xla_gpu_multi_streamed_windowed_einsum()) {\n     pipeline.AddPass<WindowedEinsumHandler>();"
        },
        {
            "sha": "3c90658c8d160d08558e197314e019fc2c960dcc",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion.cc",
            "status": "modified",
            "additions": 72,
            "deletions": 20,
            "changes": 92,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c3492061fcc3759269e2d766648fd154c5bdd060/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c3492061fcc3759269e2d766648fd154c5bdd060/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc?ref=c3492061fcc3759269e2d766648fd154c5bdd060",
            "patch": "@@ -153,14 +153,24 @@ absl::Status FuseInstructionsForConsumer(HloInstruction& root,\n   return absl::OkStatus();\n }\n \n+absl::Status IsDot(const HloInstruction& dot) {\n+  if (dot.opcode() != HloOpcode::kDot &&\n+      dot.opcode() != HloOpcode::kScaledDot) {\n+    return absl::InternalError(\n+        absl::StrCat(\"Expected a dot instruction but got \", dot.ToString()));\n+  }\n+  return absl::OkStatus();\n+}\n+\n // Annotates the given nested fusion with the given tile sizes.\n // Implementation for AnnotateDotLhs/RhsNestedFusion().\n absl::Status AnnotateDotOperandNestedFusionImpl(\n-    HloFusionInstruction& nested_fusion, const HloDotInstruction& dot,\n+    HloFusionInstruction& nested_fusion, const HloInstruction& dot,\n     const TritonGemmConfig& config,\n     absl::Span<const int64_t> contracting_dimensions,  // Must be single element\n     absl::Span<const int64_t> batch_dimensions, int64_t contracting_dim_size,\n     int64_t non_contracting_dim_size) {\n+  TF_RETURN_IF_ERROR(IsDot(dot));\n   if (contracting_dimensions.size() != 1) {\n     return absl::InternalError(\n         absl::StrCat(\"Expected a single lhs contracting dimension but got \",\n@@ -203,8 +213,9 @@ absl::Status AnnotateDotOperandNestedFusionImpl(\n }\n \n absl::Status AnnotateDotLhsNestedFusion(HloFusionInstruction& nested_fusion,\n-                                        const HloDotInstruction& dot,\n+                                        const HloInstruction& dot,\n                                         const TritonGemmConfig& config) {\n+  TF_RETURN_IF_ERROR(IsDot(dot));\n   const DotDimensionNumbers& dimension_numbers = dot.dot_dimension_numbers();\n   return AnnotateDotOperandNestedFusionImpl(\n       nested_fusion, dot, config,\n@@ -213,8 +224,9 @@ absl::Status AnnotateDotLhsNestedFusion(HloFusionInstruction& nested_fusion,\n }\n \n absl::Status AnnotateDotRhsNestedFusion(HloFusionInstruction& nested_fusion,\n-                                        const HloDotInstruction& dot,\n+                                        const HloInstruction& dot,\n                                         const TritonGemmConfig& config) {\n+  TF_RETURN_IF_ERROR(IsDot(dot));\n   const DotDimensionNumbers& dimension_numbers = dot.dot_dimension_numbers();\n   return AnnotateDotOperandNestedFusionImpl(\n       nested_fusion, dot, config,\n@@ -253,10 +265,13 @@ absl::Status FuseAndAnnotateConcatOperands(HloComputation* computation) {\n // Transforms a fusion into an equivalent nested fusion if it has a single dot.\n // Returns ok if the transformation was successful.\n absl::Status MakeNestedFusionFromGemmFusion(HloFusionInstruction* fusion,\n-                                            HloDotInstruction* dot,\n+                                            HloInstruction* dot,\n                                             mlir::MLIRContext* ctx) {\n+  TF_RETURN_IF_ERROR(IsDot(*dot));\n+  const bool is_scaled_dot = dot->opcode() == HloOpcode::kScaledDot;\n+  const int lhs = 0;\n+  const int rhs = is_scaled_dot ? 2 : 1;\n   TF_ASSIGN_OR_RETURN(TritonGemmConfig config, GetTritonGemmConfig(*fusion));\n-\n   HloComputation* computation = fusion->called_computation();\n \n   // First, create nested fusions for the operands of `concatenate` instructions\n@@ -265,18 +280,35 @@ absl::Status MakeNestedFusionFromGemmFusion(HloFusionInstruction* fusion,\n \n   // Left-hand side of the dot.\n   TF_RETURN_IF_ERROR(\n-      FuseInstructionsForConsumer(*dot->mutable_operand(0), *dot));\n+      FuseInstructionsForConsumer(*dot->mutable_operand(lhs), *dot));\n   TF_RETURN_IF_ERROR(AnnotateDotLhsNestedFusion(\n-      *::xla::Cast<HloFusionInstruction>(dot->mutable_operand(0)), *dot,\n+      *::xla::Cast<HloFusionInstruction>(dot->mutable_operand(lhs)), *dot,\n       config));\n \n   // Right-hand side of the dot.\n   TF_RETURN_IF_ERROR(\n-      FuseInstructionsForConsumer(*dot->mutable_operand(1), *dot));\n+      FuseInstructionsForConsumer(*dot->mutable_operand(rhs), *dot));\n   TF_RETURN_IF_ERROR(AnnotateDotRhsNestedFusion(\n-      *::xla::Cast<HloFusionInstruction>(dot->mutable_operand(1)), *dot,\n+      *::xla::Cast<HloFusionInstruction>(dot->mutable_operand(rhs)), *dot,\n       config));\n \n+  if (is_scaled_dot) {\n+    constexpr int kLhsScale = 1;\n+    constexpr int kRhsScale = 3;\n+    constexpr int kContractingScaleFactor = 32;\n+    auto scale_config = config;\n+    scale_config.block_k /= kContractingScaleFactor;\n+    TF_RETURN_IF_ERROR(\n+        FuseInstructionsForConsumer(*dot->mutable_operand(kLhsScale), *dot));\n+    TF_RETURN_IF_ERROR(AnnotateDotLhsNestedFusion(\n+        *::xla::Cast<HloFusionInstruction>(dot->mutable_operand(kLhsScale)),\n+        *dot, scale_config));\n+    TF_RETURN_IF_ERROR(\n+        FuseInstructionsForConsumer(*dot->mutable_operand(kRhsScale), *dot));\n+    TF_RETURN_IF_ERROR(AnnotateDotRhsNestedFusion(\n+        *::xla::Cast<HloFusionInstruction>(dot->mutable_operand(kRhsScale)),\n+        *dot, scale_config));\n+  }\n   // Delete newly unused instructions, if any.\n   TF_ASSIGN_OR_RETURN([[maybe_unused]] bool changed,\n                       HloDCE::RunOnComputation(\n@@ -1157,6 +1189,14 @@ class NestGemmFusionVisitor : public DfsHloRewriteVisitor {\n         return absl::OkStatus();\n       case HloOpcode::kFusion:\n         return AcceptResultingFusion(Cast<HloFusionInstruction>(instruction));\n+      case HloOpcode::kScaledDot:\n+        if (instruction->GetModule()\n+                ->config()\n+                .debug_options()\n+                .xla_gpu_experimental_scaled_dot_with_triton()) {\n+          return absl::OkStatus();\n+        }\n+        return absl::InternalError(\"Scaled dot with Triton is not enabled.\");\n       case HloOpcode::kDot:\n         return AcceptDotInstruction(Cast<HloDotInstruction>(instruction));\n       default:\n@@ -1191,20 +1231,28 @@ class NestGemmFusionVisitor : public DfsHloRewriteVisitor {\n     HloInstruction* instr =\n         hlo_query::GetFirstInstructionWithOpcode(*computation, HloOpcode::kDot);\n     if (instr == nullptr) {\n-      return absl::InternalError(absl::StrCat(\"Computation of fusion \",\n-                                              fusion->ToString(),\n-                                              \" has no dot instruction\"));\n+      instr = hlo_query::GetFirstInstructionWithOpcode(*computation,\n+                                                       HloOpcode::kScaledDot);\n+      if (instr == nullptr) {\n+        return absl::InternalError(absl::StrCat(\"Computation of fusion \",\n+                                                fusion->ToString(),\n+                                                \" has no dot instruction\"));\n+      }\n     }\n+\n     TF_RETURN_IF_ERROR(\n         TryHoistBitcastsInComputationToCallers(instr, call_graph));\n-    HloDotInstruction* dot = Cast<HloDotInstruction>(instr);\n-    TF_RETURN_IF_ERROR(MakeNestedFusionFromGemmFusion(fusion, dot, ctx_));\n+    TF_RETURN_IF_ERROR(MakeNestedFusionFromGemmFusion(fusion, instr, ctx_));\n \n     MarkAsChanged();\n-\n+    bool scaled_dot_enabled =\n+        fusion->GetModule()\n+            ->config()\n+            .debug_options()\n+            .xla_gpu_experimental_scaled_dot_with_triton();\n     if (CodegenDecision can_codegen_computation = IsTritonSupportedComputation(\n             *fusion->called_computation(), compute_capability_);\n-        !can_codegen_computation) {\n+        !scaled_dot_enabled && !can_codegen_computation) {\n       return absl::InternalError(absl::StrCat(\n           \"Computation of fusion \", fusion->ToString(),\n           \" is not supported by Triton: \", can_codegen_computation.Explain()));\n@@ -1226,10 +1274,13 @@ class NestGemmFusionVisitor : public DfsHloRewriteVisitor {\n     HloInstruction* instr =\n         hlo_query::GetFirstInstructionWithOpcode(*computation, HloOpcode::kDot);\n     if (instr == nullptr) {\n-      VLOG(2) << \"Skipping fusion as it has no dot instruction\";\n-      return absl::OkStatus();\n+      instr = hlo_query::GetFirstInstructionWithOpcode(*computation,\n+                                                       HloOpcode::kScaledDot);\n+      if (instr == nullptr) {\n+        VLOG(2) << \"Skipping fusion as it has no dot instruction\";\n+        return absl::OkStatus();\n+      }\n     }\n-\n     {\n       // Symbolic tile analysis and nesting do not support all HLOs yet and\n       // might leave the module in an invalid state. To avoid that we first dry\n@@ -1313,8 +1364,9 @@ absl::StatusOr<bool> NestGemmFusion::Run(\n namespace detail {\n \n absl::StatusOr<BlockLevelParameters> FindBlockLevelParameters(\n-    HloDotInstruction* dot, const TritonGemmConfig& config,\n+    HloInstruction* dot, const TritonGemmConfig& config,\n     mlir::MLIRContext* ctx) {\n+  TF_RETURN_IF_ERROR(IsDot(*dot));\n   HloComputation* computation = dot->parent();\n   VLOG(3) << \"FindOutputTileSizesForEpilogue of computation: \"\n           << computation->ToString();"
        },
        {
            "sha": "b0180fba28b55fc815a0c09d0a62563c0b2aaac5",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c3492061fcc3759269e2d766648fd154c5bdd060/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c3492061fcc3759269e2d766648fd154c5bdd060/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.h?ref=c3492061fcc3759269e2d766648fd154c5bdd060",
            "patch": "@@ -77,7 +77,7 @@ namespace detail {\n // function can be removed once `GpuDotFusionCostModel::EstimateRunTimeForDotOp`\n // is implemented.\n absl::StatusOr<BlockLevelParameters> FindBlockLevelParameters(\n-    HloDotInstruction* dot, const TritonGemmConfig& config,\n+    HloInstruction* dot, const TritonGemmConfig& config,\n     mlir::MLIRContext* ctx);\n \n }  // namespace detail"
        }
    ],
    "stats": {
        "total": 170,
        "additions": 138,
        "deletions": 32
    }
}