{
    "author": "nvgrw",
    "message": "Rename num_replicas to num_devices in HloRunnerInterface to reduce confusion.\n\nnum_replicas in the hlo runner interface is not the number of replicas to\nexecute on, unless the number of partitions is 1. Thus far, tests have only used\na single partition for testing, so they were equivalent. This is now changing.\nThe field should be renamed to avoid confusion.\n\nPiperOrigin-RevId: 842964006",
    "sha": "2a0841fb753474f17b8b47e30d35c58b660ea56a",
    "files": [
        {
            "sha": "9ce2dec42dc21838ff411f389decd5df379ea498",
            "filename": "third_party/xla/xla/service/hlo_runner.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2a0841fb753474f17b8b47e30d35c58b660ea56a/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2a0841fb753474f17b8b47e30d35c58b660ea56a/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner.cc?ref=2a0841fb753474f17b8b47e30d35c58b660ea56a",
            "patch": "@@ -497,7 +497,7 @@ absl::StatusOr<std::vector<Literal>> HloRunner::ExecuteReplicatedImpl(\n   // argument_buffers.\n   const int64_t total_argument_count = [&]() {\n     int64_t total = 0;\n-    for (int64_t i = 0; i < options.num_replicas; ++i) {\n+    for (int64_t i = 0; i < options.num_devices; ++i) {\n       total += argument_count_provider(i);\n     }\n     return total;\n@@ -511,7 +511,7 @@ absl::StatusOr<std::vector<Literal>> HloRunner::ExecuteReplicatedImpl(\n   std::vector<absl::Span<const ShapedBuffer* const>> argument_buffer_slices;\n   int64_t index = 0;\n   RunId run_id;\n-  for (int64_t i = 0; i < options.num_replicas; ++i) {\n+  for (int64_t i = 0; i < options.num_devices; ++i) {\n     int64_t device =\n         (*device_assignment)(i / num_partitions, i % num_partitions);\n     TF_ASSIGN_OR_RETURN(se::StreamExecutor * executor,\n@@ -543,18 +543,18 @@ absl::StatusOr<std::vector<Literal>> HloRunner::ExecuteReplicatedImpl(\n \n   std::unique_ptr<tsl::thread::ThreadPool> pool;\n   TF_RET_CHECK(options.infeed_values.empty() ||\n-               options.infeed_values.size() == options.num_replicas);\n+               options.infeed_values.size() == options.num_devices);\n   int64_t num_threads = options.infeed_values.size();\n   if (ShapeUtil::IsInitialized(options.outfeed_shape)) {\n-    num_threads += options.num_replicas;\n+    num_threads += options.num_devices;\n   }\n   if (num_threads > 0) {\n     pool = std::make_unique<tsl::thread::ThreadPool>(\n         tsl::Env::Default(), \"infeed_outfeed\",\n         /*num_threads=*/num_threads);\n   }\n   if (!options.infeed_values.empty()) {\n-    for (int64_t i = 0; i < options.num_replicas; ++i) {\n+    for (int64_t i = 0; i < options.num_devices; ++i) {\n       int64_t device =\n           (*device_assignment)(i / num_partitions, i % num_partitions);\n       pool->Schedule([this, device, &options, i]() {\n@@ -574,9 +574,9 @@ absl::StatusOr<std::vector<Literal>> HloRunner::ExecuteReplicatedImpl(\n   }\n   if (ShapeUtil::IsInitialized(options.outfeed_shape)) {\n     if (options.outfeed_values) {\n-      options.outfeed_values->resize(options.num_replicas);\n+      options.outfeed_values->resize(options.num_devices);\n     }\n-    for (int64_t i = 0; i < options.num_replicas; ++i) {\n+    for (int64_t i = 0; i < options.num_devices; ++i) {\n       int64_t device =\n           (*device_assignment)(i / num_partitions, i % num_partitions);\n       pool->Schedule([this, device, &options, i]() {\n@@ -606,8 +606,8 @@ absl::StatusOr<std::vector<Literal>> HloRunner::ExecuteReplicatedImpl(\n   VLOG(1) << \"Replicated execution terminated\";\n \n   std::vector<Literal> exec_results;\n-  exec_results.reserve(options.num_replicas);\n-  for (int64_t i = 0; i < options.num_replicas; ++i) {\n+  exec_results.reserve(options.num_devices);\n+  for (int64_t i = 0; i < options.num_devices; ++i) {\n     TF_RETURN_IF_ERROR(streams[i]->BlockHostUntilDone());\n     TF_ASSIGN_OR_RETURN(Literal literal,\n                         backend().transfer_manager()->TransferLiteralFromDevice(\n@@ -636,13 +636,13 @@ absl::StatusOr<std::vector<Literal>> HloRunner::ExecuteReplicated(\n         } else {\n           absl::Mutex mutex;\n           std::vector<absl::StatusOr<ScopedShapedBuffer>> thread_results(\n-              options.num_replicas);\n+              options.num_devices);\n           {\n-            VLOG(1) << \"Creating thread pool for \" << options.num_replicas\n+            VLOG(1) << \"Creating thread pool for \" << options.num_devices\n                     << \" replicas\";\n             tsl::thread::ThreadPool pool(tsl::Env::Default(), \"replicas\",\n-                                         options.num_replicas);\n-            for (int64_t i = 0; i < options.num_replicas; ++i) {\n+                                         options.num_devices);\n+            for (int64_t i = 0; i < options.num_devices; ++i) {\n               pool.Schedule([&, i] {\n                 auto result = executable->ExecuteOnStream(\n                     &service_run_options[i], argument_buffer_slices[i]);\n@@ -678,7 +678,7 @@ absl::StatusOr<std::vector<Literal>> HloRunner::ExecuteReplicated(\n   if (device_assignment == nullptr) {\n     TF_ASSIGN_OR_RETURN(\n         computation_device_assignment,\n-        backend().computation_placer()->AssignDevices(options.num_replicas, 1));\n+        backend().computation_placer()->AssignDevices(options.num_devices, 1));\n     device_assignment = &computation_device_assignment;\n   }\n   CHECK_NE(device_assignment, nullptr);\n@@ -691,13 +691,13 @@ absl::StatusOr<std::vector<Literal>> HloRunner::ExecuteReplicated(\n         std::vector<ScopedShapedBuffer> results;\n         absl::Mutex mutex;\n         std::vector<absl::StatusOr<ScopedShapedBuffer>> thread_results(\n-            options.num_replicas);\n+            options.num_devices);\n         {\n-          VLOG(1) << \"Creating thread pool for \" << options.num_replicas\n+          VLOG(1) << \"Creating thread pool for \" << options.num_devices\n                   << \" replicas\";\n           tsl::thread::ThreadPool pool(tsl::Env::Default(), \"replicas\",\n-                                       options.num_replicas);\n-          for (int64_t i = 0; i < options.num_replicas; ++i) {\n+                                       options.num_devices);\n+          for (int64_t i = 0; i < options.num_devices; ++i) {\n             for (const auto& arg : argument_buffer_slices[i]) {\n               TF_RET_CHECK(arg != nullptr);\n             }\n@@ -732,7 +732,7 @@ absl::StatusOr<std::vector<Literal>> HloRunner::ExecuteReplicated(\n     const ReplicatedExecuteOptions& options) {\n   TF_ASSIGN_OR_RETURN(\n       DeviceAssignment device_assignment,\n-      backend().computation_placer()->AssignDevices(options.num_replicas, 1));\n+      backend().computation_placer()->AssignDevices(options.num_devices, 1));\n   return ExecuteReplicated(std::move(module), options, &device_assignment);\n }\n "
        },
        {
            "sha": "eb498d611853119b2411baa88dc25d1db1d14f2d",
            "filename": "third_party/xla/xla/service/hlo_runner_interface.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2a0841fb753474f17b8b47e30d35c58b660ea56a/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner_interface.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2a0841fb753474f17b8b47e30d35c58b660ea56a/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner_interface.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner_interface.h?ref=2a0841fb753474f17b8b47e30d35c58b660ea56a",
            "patch": "@@ -166,7 +166,7 @@ class HloRunnerInterface {\n   // The options used to configure an ExecuteReplicated() call.\n   struct ReplicatedExecuteOptions {\n     // The number of devices the HLO module should be replicated onto.\n-    int64_t num_replicas = 1;\n+    int64_t num_devices = 1;\n \n     // The arguments to be fed to each replica. Since this is used for a\n     // replicated execution, all the arguments are the same for all replicas."
        },
        {
            "sha": "4d7b4105b32f69105c40b6be1261268f3f5e0ecf",
            "filename": "third_party/xla/xla/service/hlo_runner_pjrt.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2a0841fb753474f17b8b47e30d35c58b660ea56a/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner_pjrt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2a0841fb753474f17b8b47e30d35c58b660ea56a/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner_pjrt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_runner_pjrt.cc?ref=2a0841fb753474f17b8b47e30d35c58b660ea56a",
            "patch": "@@ -560,7 +560,7 @@ absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicated(\n     std::unique_ptr<HloModule> module,\n     const HloRunnerInterface::ReplicatedExecuteOptions& options,\n     DeviceAssignment* device_assignment) {\n-  module->mutable_config().set_replica_count(options.num_replicas);\n+  module->mutable_config().set_replica_count(options.num_devices);\n \n   TF_ASSIGN_OR_RETURN(\n       std::unique_ptr<OpaqueExecutable> executable,\n@@ -618,15 +618,15 @@ absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicated(\n         // The underlying data is modified concurrently. We don't need to\n         // protect access as each replica writes only to its own slot.\n         std::vector<absl::StatusOr<std::vector<std::unique_ptr<PjRtBuffer>>>>\n-            per_replica_results(options.num_replicas);\n+            per_replica_results(options.num_devices);\n         absl::c_fill(per_replica_results,\n                      absl::InternalError(\"No result for replica.\"));\n \n         {\n           // NB: `pool` is joined on destruction.\n           tsl::thread::ThreadPool pool(tsl::Env::Default(), \"replicas\",\n-                                       options.num_replicas);\n-          for (int64_t i = 0; i < options.num_replicas; ++i) {\n+                                       options.num_devices);\n+          for (int64_t i = 0; i < options.num_devices; ++i) {\n             for (const PjRtBuffer* const buffer : argument_buffer_slices[i]) {\n               TF_RET_CHECK(buffer != nullptr);\n             }\n@@ -659,7 +659,7 @@ absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicated(\n         }\n         // Aggregate results.\n         std::vector<std::vector<std::unique_ptr<PjRtBuffer>>> results;\n-        for (int64_t i = 0; i < options.num_replicas; ++i) {\n+        for (int64_t i = 0; i < options.num_devices; ++i) {\n           absl::StatusOr<std::vector<std::unique_ptr<PjRtBuffer>>>&\n               replica_result = per_replica_results[i];\n           if (!replica_result.ok()) {\n@@ -685,12 +685,12 @@ absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicatedImpl(\n     const ReplicatedExecuteOptions& options,\n     DeviceAssignment* device_assignment) {\n   TF_RET_CHECK(options.infeed_values.empty() ||\n-               options.infeed_values.size() == options.num_replicas);\n+               options.infeed_values.size() == options.num_devices);\n \n-  std::vector<PjRtDevice*> replica_devices(options.num_replicas, nullptr);\n+  std::vector<PjRtDevice*> replica_devices(options.num_devices, nullptr);\n   std::vector<std::vector<std::unique_ptr<PjRtBuffer>>> argument_buffer_slices;\n-  argument_buffer_slices.reserve(options.num_replicas);\n-  for (int64_t i = 0; i < options.num_replicas; ++i) {\n+  argument_buffer_slices.reserve(options.num_devices);\n+  for (int64_t i = 0; i < options.num_devices; ++i) {\n     // Amortize device lookup.\n     TF_ASSIGN_OR_RETURN(PjRtDevice* const device_ptr,\n                         pjrt_client_->LookupDevice(\n@@ -732,12 +732,12 @@ absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicatedImpl(\n   if (has_infeed || has_outfeed) {\n     // One infeed per infeed value and one outfeed per replica.\n     const int64_t num_threads =\n-        options.infeed_values.size() + (has_outfeed ? options.num_replicas : 0);\n+        options.infeed_values.size() + (has_outfeed ? options.num_devices : 0);\n     pool = std::make_unique<tsl::thread::ThreadPool>(\n         tsl::Env::Default(), \"infeed_outfeed\", num_threads);\n   }\n   if (has_infeed) {\n-    for (int64_t i = 0; i < options.num_replicas; ++i) {\n+    for (int64_t i = 0; i < options.num_devices; ++i) {\n       pool->Schedule(\n           [device = replica_devices[i],\n            &infeed_literal = *ABSL_DIE_IF_NULL(options.infeed_values[i]),\n@@ -759,9 +759,9 @@ absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicatedImpl(\n   }\n   if (has_outfeed) {\n     if (options.outfeed_values != nullptr) {\n-      options.outfeed_values->resize(options.num_replicas);\n+      options.outfeed_values->resize(options.num_devices);\n     }\n-    for (int64_t i = 0; i < options.num_replicas; ++i) {\n+    for (int64_t i = 0; i < options.num_devices; ++i) {\n       pool->Schedule([i, device = replica_devices[i],\n                       outfeed_values = options.outfeed_values,\n                       outfeed_shape = options.outfeed_shape,\n@@ -796,8 +796,8 @@ absl::StatusOr<std::vector<Literal>> HloRunnerPjRt::ExecuteReplicatedImpl(\n \n   // Get the result from execution.\n   std::vector<Literal> result_literals;\n-  result_literals.reserve(options.num_replicas);\n-  for (int64_t i = 0; i < options.num_replicas; ++i) {\n+  result_literals.reserve(options.num_devices);\n+  for (int64_t i = 0; i < options.num_devices; ++i) {\n     TF_ASSIGN_OR_RETURN(Literal literal,\n                         TransferLiteralsFromDevice(\n                             result_buffers[i], result_buffers[i].size() != 1));"
        },
        {
            "sha": "d8302a06c5ad3e5dfe2a2a528d3b7456ec92aa6d",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test_base.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2a0841fb753474f17b8b47e30d35c58b660ea56a/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2a0841fb753474f17b8b47e30d35c58b660ea56a/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test_base.cc?ref=2a0841fb753474f17b8b47e30d35c58b660ea56a",
            "patch": "@@ -161,7 +161,7 @@ CollectiveOpsE2ETestBase::ExecuteReplicated(\n   // TODO(b/441865120): Use designated initializers this once XLA moves to\n   // C++20.\n   HloRunnerInterface::ReplicatedExecuteOptions options;\n-  options.num_replicas = num_devices;\n+  options.num_devices = num_devices;\n   options.run_hlo_passes = run_hlo_passes;\n   options.use_threads = true;\n "
        },
        {
            "sha": "b27f4e65c9db88dc4584f3bc25e5f71157a41262",
            "filename": "third_party/xla/xla/tests/collective_ops_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2a0841fb753474f17b8b47e30d35c58b660ea56a/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2a0841fb753474f17b8b47e30d35c58b660ea56a/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_test.cc?ref=2a0841fb753474f17b8b47e30d35c58b660ea56a",
            "patch": "@@ -435,7 +435,7 @@ TEST_F(CollectiveOpsTest, AllReduce_ManyConcurrentAllReduces) {\n   auto device_assn = MakeDeviceAssn(devices);\n \n   HloRunnerInterface::ReplicatedExecuteOptions opts;\n-  opts.num_replicas = devices.size();\n+  opts.num_devices = devices.size();\n   opts.use_threads = true;\n   opts.arguments.push_back(&input_literal);\n "
        },
        {
            "sha": "3331f59e00ce21c6e6e90967288f71eb74509299",
            "filename": "third_party/xla/xla/tests/hlo_runner_agnostic_test_base.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2a0841fb753474f17b8b47e30d35c58b660ea56a/third_party%2Fxla%2Fxla%2Ftests%2Fhlo_runner_agnostic_test_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2a0841fb753474f17b8b47e30d35c58b660ea56a/third_party%2Fxla%2Fxla%2Ftests%2Fhlo_runner_agnostic_test_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fhlo_runner_agnostic_test_base.cc?ref=2a0841fb753474f17b8b47e30d35c58b660ea56a",
            "patch": "@@ -152,7 +152,7 @@ HloRunnerAgnosticTestBase::ExecuteReplicated(\n     const int64_t num_replicas, const bool use_threads,\n     const bool run_hlo_passes) {\n   HloRunnerInterface::ReplicatedExecuteOptions options;\n-  options.num_replicas = num_replicas;\n+  options.num_devices = num_replicas;\n   options.arguments = {arguments.begin(), arguments.end()};\n   options.run_hlo_passes = run_hlo_passes;\n   options.use_threads = use_threads;\n@@ -167,7 +167,7 @@ HloRunnerAgnosticTestBase::ExecuteReplicated(\n     const int64_t num_replicas, DeviceAssignment* const device_assignment,\n     const bool run_hlo_passes, const bool use_threads) {\n   HloRunnerInterface::ReplicatedExecuteOptions options;\n-  options.num_replicas = num_replicas;\n+  options.num_devices = num_replicas;\n   options.arguments = {arguments.begin(), arguments.end()};\n   options.run_hlo_passes = run_hlo_passes;\n   options.use_threads = use_threads;\n@@ -184,7 +184,7 @@ HloRunnerAgnosticTestBase::ExecuteReplicated(\n     const int64_t num_replicas, const bool run_hlo_passes,\n     DeviceAssignment* const device_assignment) {\n   HloRunnerInterface::ReplicatedExecuteOptions options;\n-  options.num_replicas = num_replicas;\n+  options.num_devices = num_replicas;\n   options.run_hlo_passes = run_hlo_passes;\n   options.use_threads = true;\n   return test_runner_->ExecuteReplicated(\n@@ -259,11 +259,11 @@ HloRunnerAgnosticTestBase::RunAndCompareTwoModulesReplicated(\n            << \"Number of replicas is not the same: \" << replica_count << \" Vs \"\n            << module_1->config().replica_count();\n   }\n-  if (options.num_replicas != replica_count) {\n+  if (options.num_devices != replica_count) {\n     return ::testing::AssertionFailure()\n            << \"Number of execution replicas is different from number of \"\n               \"replicas in the module: requested number of replicas = \"\n-           << options.num_replicas\n+           << options.num_devices\n            << \", number of replicas in hlo = \" << replica_count;\n   }\n \n@@ -540,7 +540,7 @@ ::testing::AssertionResult HloRunnerAgnosticTestBase::RunReplicated(\n   }\n \n   HloRunnerInterface::ReplicatedExecuteOptions options;\n-  options.num_replicas = num_replicas;\n+  options.num_devices = num_replicas;\n   options.arguments = {fake_argument_ptrs.begin(), fake_argument_ptrs.end()};\n   options.run_hlo_passes = run_hlo_passes;\n   options.use_threads = true;"
        },
        {
            "sha": "e7ff6762b41a72015e3f9b2bee3c20b2b6990845",
            "filename": "third_party/xla/xla/tests/replicated_io_feed_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2a0841fb753474f17b8b47e30d35c58b660ea56a/third_party%2Fxla%2Fxla%2Ftests%2Freplicated_io_feed_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2a0841fb753474f17b8b47e30d35c58b660ea56a/third_party%2Fxla%2Fxla%2Ftests%2Freplicated_io_feed_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Freplicated_io_feed_test.cc?ref=2a0841fb753474f17b8b47e30d35c58b660ea56a",
            "patch": "@@ -63,7 +63,7 @@ TEST_F(ReplicatedIOFeedTest, InfeedAndOutfeed) {\n   std::vector<Literal> outfeed_literals;\n \n   HloRunnerInterface::ReplicatedExecuteOptions opts;\n-  opts.num_replicas = kNumReplicas;\n+  opts.num_devices = kNumReplicas;\n \n   // Initialize infeed literal = replica_id * 10\n   std::vector<Literal> infeed_literals(kNumReplicas);"
        }
    ],
    "stats": {
        "total": 88,
        "additions": 44,
        "deletions": 44
    }
}