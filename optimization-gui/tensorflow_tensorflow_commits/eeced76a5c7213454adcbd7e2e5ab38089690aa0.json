{
    "author": "beckerhe",
    "message": "Refactor: Change GpuExecutable constructor to take individual parameters.\n\nThe `GpuExecutable` constructor now accepts individual fields previously contained within the `GpuExecutable::Params` struct, improving clarity and reducing reliance on the Params struct after its initial use in Create. The `GatherAllocationPtrs` helper function is also updated to take specific allocation-related parameters.\n\nIn a subsequent I'm going to move some of those parameters into a subclass which is why this change is needed.\n\nPiperOrigin-RevId: 839163762",
    "sha": "eeced76a5c7213454adcbd7e2e5ab38089690aa0",
    "files": [
        {
            "sha": "2d5e57791254d648b42a8b9849d8abfcd902eccc",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 44,
            "deletions": 24,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eeced76a5c7213454adcbd7e2e5ab38089690aa0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eeced76a5c7213454adcbd7e2e5ab38089690aa0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=eeced76a5c7213454adcbd7e2e5ab38089690aa0",
            "patch": "@@ -64,6 +64,7 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/dump.h\"\n #include \"xla/service/executable.h\"\n+#include \"xla/service/gpu/alias_info.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/service/gpu/gpu_constants.h\"\n@@ -110,13 +111,14 @@ namespace {\n \n // Chooses the correct allocations to be used within the GpuExecutable code.\n std::vector<const BufferAllocation*> GatherAllocationPtrs(\n-    const GpuExecutable::Params& params,\n+    const std::optional<std::vector<BufferAllocation>>& mlir_allocations,\n+    const BufferAssignment* buffer_assignment,\n     const std::deque<BufferAllocation>& thunk_pass_allocations) {\n   const std::vector<BufferAllocation>* allocation_vec = nullptr;\n-  if (params.mlir_allocations.has_value()) {\n-    allocation_vec = &params.mlir_allocations.value();\n-  } else if (params.buffer_assignment != nullptr) {\n-    allocation_vec = &params.buffer_assignment->Allocations();\n+  if (mlir_allocations.has_value()) {\n+    allocation_vec = &mlir_allocations.value();\n+  } else if (buffer_assignment != nullptr) {\n+    allocation_vec = &buffer_assignment->Allocations();\n   }\n \n   std::vector<const BufferAllocation*> alloc_ptrs;\n@@ -242,33 +244,51 @@ absl::StatusOr<std::unique_ptr<GpuExecutable>> GpuExecutable::Create(\n       params.debug_module.get(), allocator));\n \n   return std::unique_ptr<GpuExecutable>(new GpuExecutable(\n-      std::move(params), std::move(allocator.MutableAllocations())));\n+      std::move(params.debug_module), std::move(params.asm_text),\n+      std::move(params.binary), std::move(params.dnn_compiled_graphs),\n+      std::move(params.device_description), std::move(params.executable),\n+      std::move(params.module_name), std::move(params.program_shape),\n+      std::move(params.mlir_allocations), std::move(params.buffer_assignment),\n+      std::move(allocator.MutableAllocations()), std::move(params.alias_info),\n+      std::move(params.debug_options), std::move(params.constants),\n+      std::move(params.output_info), params.enable_debug_info_manager));\n }\n \n // Implementation note: HLO profiling is always enabled for GPU executables,\n // since we can use timers around thunks.\n GpuExecutable::GpuExecutable(\n-    GpuExecutable::Params params,\n-    std::deque<BufferAllocation> thunk_pass_allocations)\n-    : Executable(std::move(params.debug_module)),\n-      text_(std::move(params.asm_text)),\n-      binary_(std::move(params.binary)),\n-      dnn_compiled_graphs_(std::move(params.dnn_compiled_graphs)),\n-      gpu_version_(params.device_description.gpu_compute_capability()),\n-      thunks_(std::move(params.executable)),\n+    std::unique_ptr<HloModule> debug_module, std::string asm_text,\n+    std::vector<uint8_t> binary, BinaryMap dnn_compiled_graphs,\n+    se::DeviceDescription device_description,\n+    std::unique_ptr<SequentialThunk> executable, std::string module_name,\n+    ProgramShape program_shape,\n+    std::optional<std::vector<BufferAllocation>> mlir_allocations,\n+    std::unique_ptr<const BufferAssignment> buffer_assignment,\n+    std::deque<BufferAllocation> thunk_pass_allocations,\n+    std::unique_ptr<GpuAliasInfo> alias_info, DebugOptions debug_options,\n+    std::vector<ConstantInfo> constants,\n+    absl::flat_hash_map<ShapeIndex, OutputInfo> output_info,\n+    bool enable_debug_info_manager)\n+    : Executable(std::move(debug_module)),\n+      text_(std::move(asm_text)),\n+      binary_(std::move(binary)),\n+      dnn_compiled_graphs_(std::move(dnn_compiled_graphs)),\n+      gpu_version_(device_description.gpu_compute_capability()),\n+      thunks_(std::move(executable)),\n       execution_stream_ids_(GetExecutionStreamIds(*thunks_)),\n-      module_name_(params.module_name),\n-      program_shape_(params.program_shape),\n-      allocation_ptrs_(GatherAllocationPtrs(params, thunk_pass_allocations)),\n-      allocations_(std::move(params.mlir_allocations)),\n-      buffer_assignment_(std::move(params.buffer_assignment)),\n+      module_name_(std::move(module_name)),\n+      program_shape_(std::move(program_shape)),\n+      allocation_ptrs_(GatherAllocationPtrs(\n+          mlir_allocations, buffer_assignment.get(), thunk_pass_allocations)),\n+      allocations_(std::move(mlir_allocations)),\n+      buffer_assignment_(std::move(buffer_assignment)),\n       thunk_pass_allocations_(std::move(thunk_pass_allocations)),\n-      alias_info_(std::move(params.alias_info)),\n+      alias_info_(std::move(alias_info)),\n       debug_buffer_assignment_show_max_(\n-          params.debug_options.xla_debug_buffer_assignment_show_max()),\n-      constants_(std::move(params.constants)),\n-      output_info_(std::move(params.output_info)),\n-      enable_debug_info_manager_(params.enable_debug_info_manager) {\n+          debug_options.xla_debug_buffer_assignment_show_max()),\n+      constants_(std::move(constants)),\n+      output_info_(std::move(output_info)),\n+      enable_debug_info_manager_(enable_debug_info_manager) {\n   if (gpu_version_.IsRocm()) {\n     // ROCm uses hsaco hashes to distinguish between modules.\n     // Bad things happen if multiple modules with identical code are loaded."
        },
        {
            "sha": "aecb01e78c8e94d596fbbd5fab08009b52d2dba3",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.h",
            "status": "modified",
            "additions": 13,
            "deletions": 2,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eeced76a5c7213454adcbd7e2e5ab38089690aa0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eeced76a5c7213454adcbd7e2e5ab38089690aa0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h?ref=eeced76a5c7213454adcbd7e2e5ab38089690aa0",
            "patch": "@@ -230,8 +230,19 @@ class GpuExecutable : public Executable {\n \n  private:\n   // Use GpuExecutable::Create() to create an instance.\n-  explicit GpuExecutable(Params params,\n-                         std::deque<BufferAllocation> thunk_pass_allocations);\n+  explicit GpuExecutable(\n+      std::unique_ptr<HloModule> debug_module, std::string asm_text,\n+      std::vector<uint8_t> binary, BinaryMap dnn_compiled_graphs,\n+      se::DeviceDescription device_description,\n+      std::unique_ptr<SequentialThunk> executable, std::string module_name,\n+      ProgramShape program_shape,\n+      std::optional<std::vector<BufferAllocation>> mlir_allocations,\n+      std::unique_ptr<const BufferAssignment> buffer_assignment,\n+      std::deque<BufferAllocation> thunk_pass_allocations,\n+      std::unique_ptr<GpuAliasInfo> alias_info, DebugOptions debug_options,\n+      std::vector<ConstantInfo> constants,\n+      absl::flat_hash_map<ShapeIndex, OutputInfo> output_info,\n+      bool enable_debug_info_manager);\n \n   // GpuExecutable check with either AMD's ISA version, or Nvidia's major minor\n   // version for compute capability, depending on the hardware."
        }
    ],
    "stats": {
        "total": 83,
        "additions": 57,
        "deletions": 26
    }
}