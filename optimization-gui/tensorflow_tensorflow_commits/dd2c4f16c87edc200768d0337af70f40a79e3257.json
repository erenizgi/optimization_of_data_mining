{
    "author": "loislo",
    "message": "[XLA:GPU] Enable `TritonXLAConvertUnsupportedTypesPass` rewrite for experimental scaled dot behind the xla_gpu_experimental_scaled_dot_with_triton flag.\n\nThe cl fixes the test such the way that it gets lowerable to the mma block_scale op on Blackwell. Hopper version has some issues with the fallback DecomposeScaledBlocked pass. As a result of that the Triton IR is incorrect after AccelerateMatmul pass.\n\nPiperOrigin-RevId: 806190522",
    "sha": "dd2c4f16c87edc200768d0337af70f40a79e3257",
    "files": [
        {
            "sha": "71b8911b053770867be7b9a903222216eb92143c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dd2c4f16c87edc200768d0337af70f40a79e3257/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dd2c4f16c87edc200768d0337af70f40a79e3257/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=dd2c4f16c87edc200768d0337af70f40a79e3257",
            "patch": "@@ -2187,6 +2187,11 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(\n     pm.addPass(\n         mlir::triton::xla::CreateInt4ToPackedInt4RewritePass(device_info));\n   }\n+  if (hlo_module.config()\n+          .debug_options()\n+          .xla_gpu_experimental_scaled_dot_with_triton()) {\n+    pm.addPass(mlir::triton::xla::CreateTritonXLAConvertUnsupportedTypesPass());\n+  }\n \n   pm.addPass(mlir::triton::xla::CreateTritonXLAExtractInsertToTritonPass(\n       block_level_parameters.is_tma_allowed &&"
        },
        {
            "sha": "5ce2218b92cef566ef1112f06933f9da404404f0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_port_test.cc",
            "status": "modified",
            "additions": 30,
            "deletions": 23,
            "changes": 53,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/dd2c4f16c87edc200768d0337af70f40a79e3257/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/dd2c4f16c87edc200768d0337af70f40a79e3257/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc?ref=dd2c4f16c87edc200768d0337af70f40a79e3257",
            "patch": "@@ -3273,7 +3273,8 @@ class TritonScaledDotGemmTest\n     : public TritonGemmTest,\n       public ::testing::WithParamInterface<ScaleDotTestParams> {};\n \n-TEST_P(TritonScaledDotGemmTest, Fp8ScaledDotDoesNotCrash) {\n+TEST_P(TritonScaledDotGemmTest,\n+       FP8ScaledDotCompilesToPtxIntrinsicsWhenAvailable) {\n   const ScaleDotTestParams& params = GetParam();\n   constexpr absl::string_view kHloTextTemplate = R\"hlo(\n HloModule m\n@@ -3299,8 +3300,8 @@ triton_dot {\n       \"fusion_backend_config\":{\n         \"kind\":\"__triton_nested_gemm_fusion\",\n         \"block_level_fusion_config\":{\n-          \"output_tiles\":[{\"sizes\":[\"16\",\"32\"]}],\n-          \"num_warps\":\"1\",\n+          \"output_tiles\":[{\"sizes\":[\"128\",\"128\"]}],\n+          \"num_warps\":\"4\",\n           \"num_stages\":\"1\",\n           \"num_ctas\":\"1\",\n         }\n@@ -3314,8 +3315,8 @@ triton_dot {\n       \"fusion_backend_config\":{\n         \"kind\":\"__triton_nested_gemm_fusion\",\n         \"block_level_fusion_config\":{\n-          \"output_tiles\":[{\"sizes\":[\"16\",\"1\"]}],\n-          \"num_warps\":\"1\",\n+          \"output_tiles\":[{\"sizes\":[\"128\",\"128\"]}],\n+          \"num_warps\":\"4\",\n           \"num_stages\":\"1\",\n           \"num_ctas\":\"1\",\n         }\n@@ -3329,8 +3330,8 @@ triton_dot {\n       \"fusion_backend_config\":{\n         \"kind\":\"__triton_nested_gemm_fusion\",\n         \"block_level_fusion_config\":{\n-          \"output_tiles\":[{\"sizes\":[\"16\",\"32\"]}],\n-          \"num_warps\":\"1\",\n+          \"output_tiles\":[{\"sizes\":[\"128\",\"256\"]}],\n+          \"num_warps\":\"4\",\n           \"num_stages\":\"1\",\n           \"num_ctas\":\"1\",\n         }\n@@ -3344,8 +3345,8 @@ triton_dot {\n       \"fusion_backend_config\":{\n         \"kind\":\"__triton_nested_gemm_fusion\",\n         \"block_level_fusion_config\":{\n-          \"output_tiles\":[{\"sizes\":[\"16\",\"1\"]}],\n-          \"num_warps\":\"1\",\n+          \"output_tiles\":[{\"sizes\":[\"128\", \"256\"]}],\n+          \"num_warps\":\"4\",\n           \"num_stages\":\"1\",\n           \"num_ctas\":\"1\",\n         }\n@@ -3368,8 +3369,8 @@ ENTRY e {\n       \"fusion_backend_config\": {\n         kind: \"__triton_scaled_dot_fusion\",\n         \"block_level_fusion_config\":{\n-          \"output_tiles\":[{\"sizes\":[\"16\", \"16\"]}],\n-          \"num_warps\":\"1\",\n+          \"output_tiles\":[{\"sizes\":[\"128\", \"256\"]}],\n+          \"num_warps\":\"4\",\n           \"num_stages\":\"1\",\n           \"num_ctas\":\"1\"\n         }\n@@ -3380,7 +3381,7 @@ ENTRY e {\n \n   auto hlo_text = params.PrepareHloText(kHloTextTemplate);\n \n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(hlo_text));\n \n   auto debug_options = module->config().debug_options();\n@@ -3389,34 +3390,40 @@ ENTRY e {\n \n   constexpr absl::string_view kExpectedTritonIrTmpl = R\"(\n       CHECK: tt.dot_scaled\n-      CHECK: tensor<16x32x$triton_type>, tensor<16x1xi8>\n-      CHECK: tensor<32x16x$triton_type>, tensor<1x16xi8>\n-      CHECK: -> tensor<16x16xf32>\n+      CHECK: tensor<128x128x$triton_type>, tensor<128x4xi8>\n+      CHECK: tensor<128x256x$triton_type>, tensor<4x256xi8>\n+      CHECK: -> tensor<128x256xf32>\n   )\";\n   auto expected_triton_ir = absl::StrReplaceAll(\n       kExpectedTritonIrTmpl, {{\"$triton_type\", params.expected_triton_type}});\n   EXPECT_THAT(\n       CreateTritonIrAndFileCheck(*module->GetComputationWithName(\"triton_dot\"),\n                                  /*block_level_parameters=*/\n                                  {\n-                                     {{16, 16}},\n-                                     1,\n+                                     {{128, 256}},\n+                                     4,\n                                      1,\n                                      1,\n                                      true,\n                                  },\n                                  expected_triton_ir),\n       absl_testing::IsOk());\n+  if (GetCudaComputeCapability().IsAtLeastBlackwell()) {\n+    CompileAndOptionallyVerifyPtx(\n+        std::move(module), R\"(CHECK: mxf8f6f4.block_scale.scale_vec::1X)\");\n+  }\n }\n \n INSTANTIATE_TEST_SUITE_P(\n     TritonScaledDotGemmTest, TritonScaledDotGemmTest,\n-    ::testing::Values(ScaleDotTestParams{\"f8e4m3fn[64,512]\", \"f8e8m0fnu[64,16]\",\n-                                         \"f8e4m3fn[512,64]\", \"f8e8m0fnu[16,64]\",\n-                                         \"f32[64,64]\", \"f8E4M3FN\"},\n-                      ScaleDotTestParams{\"f8e5m2[64,512]\", \"f8e8m0fnu[64,16]\",\n-                                         \"f8e5m2[512,64]\", \"f8e8m0fnu[16,64]\",\n-                                         \"f32[64,64]\", \"f8E5M2\"}),\n+    ::testing::Values(ScaleDotTestParams{\"f8e4m3fn[128,128]\",\n+                                         \"f8e8m0fnu[128,4]\",\n+                                         \"f8e4m3fn[128,256]\",\n+                                         \"f8e8m0fnu[4,256]\", \"f32[128,256]\",\n+                                         \"f8E4M3FN\"},\n+                      ScaleDotTestParams{\"f8e5m2[128,128]\", \"f8e8m0fnu[128,4]\",\n+                                         \"f8e5m2[128,256]\", \"f8e8m0fnu[4,256]\",\n+                                         \"f32[128,256]\", \"f8E5M2\"}),\n     ScaleDotTestParams::ToString);\n \n }  // namespace gpu"
        }
    ],
    "stats": {
        "total": 58,
        "additions": 35,
        "deletions": 23
    }
}