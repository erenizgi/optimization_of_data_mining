{
    "author": "othakkar",
    "message": "PR #31745: [XLA:CPU][oneDNN] Enable oneDNN Softmax Custom Calls in Thunk Runtime\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31745\n\nThis PR enables support for oneDNN Softmax operations in the XLA:CPU Thunk runtime, building upon the foundational implementation of `OneDnnOpThunk`.\n\nKey changes:\n- Updated thunk emitter to emit `OneDnnOpThunk` for oneDNN softmax during compilation.\n- Enabled oneDNN custom call rewrite for Softmax in `cpu_compiler.cc` via `OneDnnOpsRewriter` (while temporarily disabling the rewrite for oneDNN Layer Norm).\n- Added support for oneDNN Softmax op via `ExecuteOneDnnSoftmax(...)` in `onednn_softmax.cc`.\nCopybara import of the project:\n\n--\na938cb01374c0b9b41864aee872f4c198c2385f2 by Om Thakkar <om.thakkar@intel.com>:\n\nenable oneDNN softmax in thunk runtime\n\n--\n8d13a6f3a01a6f0d269d6a37d4690c177429b249 by Om Thakkar <om.thakkar@intel.com>:\n\nreplace auto with corresponding type\n\n--\nc00c6890ab891926e2a334a4406be7ab388afa7c by Om Thakkar <om.thakkar@intel.com>:\n\nadd NOLINT tags in onednn_ops_rewriter and one minor change in onednn_op_thunk_test\n\n--\n010dfbb9774ec1d59712a114ab81acb239589669 by Om Thakkar <om.thakkar@intel.com>:\n\nremove dead-code for oneDNN softmax related to legacy runtime\n\n--\n0c6c161228656f22b164067768bb0fb4400af146 by Om Thakkar <om.thakkar@intel.com>:\n\nremove workarounds from onednn_ops_rewriter\n\n--\n864b5d73e24ed33bd23bb307714435fdbc1897e1 by Om Thakkar <om.thakkar@intel.com>:\n\nbug fix\n\n--\nb78764d4d34953c7c09c5e20d8d1f078d7a2ce94 by Om Thakkar <om.thakkar@intel.com>:\n\nset --xla_cpu_experimental_onednn_custom_call=true for oneDNN layernorm and softmax unit tests\n\nMerging this change closes #31745\n\nPiperOrigin-RevId: 816638788",
    "sha": "23e545fb391ed9e1d13ea9e67608eb423a088979",
    "files": [
        {
            "sha": "7d19ec45031d1088eac87552bd42df3709481d79",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2FBUILD?ref=23e545fb391ed9e1d13ea9e67608eb423a088979",
            "patch": "@@ -132,6 +132,7 @@ cc_library(\n         \"//xla/service/cpu:onednn_layer_norm\",\n         \"//xla/service/cpu:onednn_matmul\",\n         \"//xla/service/cpu:onednn_memory_util\",\n+        \"//xla/service/cpu:onednn_softmax\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/mkl:onednn\","
        },
        {
            "sha": "6e725703879d232c77694be3cd54e83c89bacaad",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_op_thunk.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.cc?ref=23e545fb391ed9e1d13ea9e67608eb423a088979",
            "patch": "@@ -40,6 +40,7 @@ limitations under the License.\n #include \"xla/service/cpu/onednn_layer_norm.h\"\n #include \"xla/service/cpu/onednn_matmul.h\"\n #include \"xla/service/cpu/onednn_memory_util.h\"\n+#include \"xla/service/cpu/onednn_softmax.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n #include \"xla/tsl/platform/logging.h\"\n@@ -110,6 +111,10 @@ OneDnnOpThunk::OneDnnRuntime::Invoke(\n     const auto& ln_config = std::get<OneDnnNormConfig>(config);\n     ExecuteOneDnnLayerNorm(arguments, results, ln_config, cpu_engine,\n                            onednn_stream, resources);\n+  } else if (target == \"__onednn$softmax\") {\n+    const auto& softmax_config = std::get<OneDnnSoftmaxConfig>(config);\n+    ExecuteOneDnnSoftmax(arguments, results, softmax_config, cpu_engine,\n+                         onednn_stream, resources);\n   } else {\n     return absl::InvalidArgumentError(\n         absl::StrFormat(\"Unsupported oneDNN operation target: `%s`\", target));"
        },
        {
            "sha": "22e389559e3f4058133be5a86cc6815df05bfd39",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_op_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk.h?ref=23e545fb391ed9e1d13ea9e67608eb423a088979",
            "patch": "@@ -48,7 +48,7 @@ class OneDnnOpThunk : public Thunk {\n   // TODO(intel-tf): Add more oneDNN operation configs as needed.\n   using OneDnnOpConfig =\n       std::variant<OneDnnMatMulConfig, OneDnnConvolutionConfig,\n-                   OneDnnNormConfig>;\n+                   OneDnnNormConfig, OneDnnSoftmaxConfig>;\n \n   static absl::StatusOr<std::unique_ptr<OneDnnOpThunk>> Create(\n       const std::string& custom_call_target, Info info, OpBuffers buffers,"
        },
        {
            "sha": "933d657f4bdcf78567fc94009b8f348a0e95cf1f",
            "filename": "third_party/xla/xla/backends/cpu/runtime/onednn/onednn_op_thunk_test.cc",
            "status": "modified",
            "additions": 72,
            "deletions": 0,
            "changes": 72,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fruntime%2Fonednn%2Fonednn_op_thunk_test.cc?ref=23e545fb391ed9e1d13ea9e67608eb423a088979",
            "patch": "@@ -323,5 +323,77 @@ TEST(OneDnnOpThunkTest, SimpleOneDnnLayerNormThunk) {\n   }\n }\n \n+TEST(OneDnnOpThunkTest, SimpleOneDnnSoftmaxThunk) {\n+  // Set up a thread pool for parallel execution\n+  tsl::thread::ThreadPool threads(tsl::Env::Default(), \"test\", 8);\n+  Eigen::ThreadPoolDevice device(threads.AsEigenThreadPool(),\n+                                 threads.NumThreads());\n+\n+  // Input shape (2x3), softmax over axis=1 (last dim)\n+  Shape in_shape = ShapeUtil::MakeShape(F32, {2, 3});\n+  Shape out_shape = in_shape;\n+\n+  // Input:\n+  // [[1,2,3],\n+  //  [4,5,6]]\n+  Literal in_literal = LiteralUtil::CreateR2FromArray2D<float>(\n+      Array2D<float>({{1.f, 2.f, 3.f}, {4.f, 5.f, 6.f}}));\n+  Literal out_literal = LiteralUtil::CreateR2FromArray2D<float>(\n+      Array2D<float>({{0.f, 0.f, 0.f}, {0.f, 0.f, 0.f}}));\n+\n+  // Buffer allocations\n+  auto [in_alloc, out_alloc] = CreateBufferAllocation(in_literal, out_literal);\n+\n+  auto [in_slice, out_slice] = CreateBufferAllocationSlice(in_alloc, out_alloc);\n+\n+  BufferAllocations allocations =\n+      CreateBufferAllocations(in_literal, out_literal);\n+\n+  // Set up op_buffers\n+  OneDnnOpThunk::OpBuffers op_buffers;\n+  op_buffers.arguments_buffers = {in_slice};\n+  op_buffers.arguments_shapes = {in_shape};\n+  op_buffers.results_buffers = {out_slice};\n+  op_buffers.results_shapes = {out_shape};\n+\n+  // Softmax config (axis = 1)\n+  OneDnnSoftmaxConfig softmax_cfg;\n+  softmax_cfg.set_softmax_axis(1);\n+  OneDnnOpThunk::OneDnnOpConfig variant_cfg = softmax_cfg;\n+\n+  // Create thunk for Softmax\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto thunk, OneDnnOpThunk::Create(\"__onednn$softmax\", Thunk::Info(),\n+                                        op_buffers, variant_cfg));\n+\n+  // Execute params\n+  Thunk::ExecuteParams params;\n+  params.buffer_allocations = &allocations;\n+  params.intra_op_threadpool = &device;\n+\n+  tsl::AsyncValueRef<Thunk::ExecuteEvent> exec_event = thunk->Execute(params);\n+  tsl::BlockUntilReady(exec_event);\n+  ASSERT_FALSE(exec_event.IsError())\n+      << \"OneDnnOpThunk softmax execution failed\";\n+\n+  // Compute expected softmax row-wise\n+  auto softmax_row = [](float a, float b, float c) {\n+    float ea = std::exp(a), eb = std::exp(b), ec = std::exp(c);\n+    float s = ea + eb + ec;\n+    return std::array<float, 3>{ea / s, eb / s, ec / s};\n+  };\n+  std::array<float, 3> r0 = softmax_row(1.f, 2.f, 3.f);\n+  std::array<float, 3> r1 = softmax_row(4.f, 5.f, 6.f);\n+\n+  const float kTol = 1e-5f;\n+  // Validate results\n+  for (int i = 0; i < 3; ++i) {\n+    float got0 = out_literal.Get<float>({0, i});\n+    float got1 = out_literal.Get<float>({1, i});\n+    EXPECT_NEAR(got0, r0[i], kTol);\n+    EXPECT_NEAR(got1, r1[i], kTol);\n+  }\n+}\n+\n }  // namespace\n }  // namespace xla::cpu"
        },
        {
            "sha": "b495ab0ef66f905e766e2a6b19b1122cc8a970be",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=23e545fb391ed9e1d13ea9e67608eb423a088979",
            "patch": "@@ -612,7 +612,6 @@ cc_library(\n     ] + if_onednn([\n         \":onednn_convolution\",\n         \":onednn_matmul\",\n-        \":onednn_softmax\",\n     ]),\n )\n \n@@ -1823,7 +1822,6 @@ onednn_cc_library(\n     srcs = [\"onednn_softmax.cc\"],\n     hdrs = [\n         \"onednn_softmax.h\",\n-        \"//xla/tsl/util:onednn_util_hdrs\",\n     ],\n     copts = runtime_copts() + tsl_copts(),\n     visibility = [\"//visibility:public\"],"
        },
        {
            "sha": "f242cecf917068dbf7eb6192a7ad870e4615e6d1",
            "filename": "third_party/xla/xla/service/cpu/cpu_runtime.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.cc?ref=23e545fb391ed9e1d13ea9e67608eb423a088979",
            "patch": "@@ -169,8 +169,6 @@ extern const char* const kPartitionIdSymbolName =\n extern const char* const kReplicaIdSymbolName = \"__xla_cpu_runtime_ReplicaId\";\n extern const char* const kOneDnnMatMulSymbolName =\n     \"__xla_cpu_runtime_OneDnnMatMul\";\n-extern const char* const kOneDnnSoftmaxSymbolName =\n-    \"__xla_cpu_runtime_OneDnnSoftmax\";\n extern const char* const kOneDnnMatMulReorderSymbolName =\n     \"__xla_cpu_runtime_OneDnnMatMulReorder\";\n extern const char* const kHandleFfiCallSymbolName ="
        },
        {
            "sha": "bbdfc6ce6ad9eeec668e2063eec2cb502dee64fe",
            "filename": "third_party/xla/xla/service/cpu/cpu_runtime.h",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_runtime.h?ref=23e545fb391ed9e1d13ea9e67608eb423a088979",
            "patch": "@@ -88,7 +88,6 @@ extern const char* const kAllToAllSymbolName;\n extern const char* const kAllGatherSymbolName;\n extern const char* const kReduceScatterSymbolName;\n extern const char* const kOneDnnMatMulSymbolName;\n-extern const char* const kOneDnnSoftmaxSymbolName;\n extern const char* const kOneDnnMatMulReorderSymbolName;\n extern const char* const kHandleFfiCallSymbolName;\n "
        },
        {
            "sha": "ffa2bbeb912735c43d6bfcdd7a2eeb4759474f02",
            "filename": "third_party/xla/xla/service/cpu/ir_emitter.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.cc?ref=23e545fb391ed9e1d13ea9e67608eb423a088979",
            "patch": "@@ -2562,36 +2562,6 @@ absl::Status IrEmitter::HandleOneDnnMatMulCalls(\n \n   return absl::OkStatus();\n }\n-\n-absl::Status IrEmitter::HandleOneDnnSoftmax(HloInstruction* custom_call) {\n-  // Serialize and emit OneDnnSoftmaxConfig.\n-  auto typed_custom_call = Cast<HloCustomCallInstruction>(custom_call);\n-  auto backend_config = typed_custom_call->backend_config<BackendConfig>();\n-  OneDnnSoftmaxConfig softmax_config;\n-  softmax_config.CopyFrom(backend_config->onednn_softmax_config());\n-  std::string str_config;\n-  softmax_config.SerializeToString(&str_config);\n-  llvm::Value* softmax_config_val =\n-      b()->CreateGlobalStringPtr(llvm_ir::AsStringRef(str_config));\n-\n-  auto input = custom_call->operand(0);\n-  llvm_ir::IrArray input_array(GetIrArrayFor(input));\n-  auto input_stack_alloca = GetAllocaAndEmitMemrefInfo(*b(), input_array);\n-\n-  TF_RETURN_IF_ERROR(EmitTargetAddressForOp(custom_call));\n-  llvm_ir::IrArray result_array = GetIrArrayFor(custom_call);\n-  auto result_stack_alloca = GetAllocaAndEmitMemrefInfo(*b(), result_array);\n-\n-  EmitCallToFunc(runtime::kOneDnnSoftmaxSymbolName,\n-                 {GetExecutableRunOptionsArgument(), input_stack_alloca.value,\n-                  result_stack_alloca.value, softmax_config_val},\n-                 b()->getVoidTy());\n-\n-  input_stack_alloca.EmitLifetimeEnd();\n-  result_stack_alloca.EmitLifetimeEnd();\n-\n-  return absl::OkStatus();\n-}\n #endif  // XLA_ONEDNN\n \n absl::Status IrEmitter::HandleCustomCall(HloInstruction* custom_call) {\n@@ -2609,9 +2579,6 @@ absl::Status IrEmitter::HandleCustomCall(HloInstruction* custom_call) {\n     return HandleOneDnnMatMulCalls(custom_call,\n                                    runtime::kOneDnnMatMulSymbolName);\n   }\n-  if (custom_call->custom_call_target() == \"__onednn$softmax\") {\n-    return HandleOneDnnSoftmax(custom_call);\n-  }\n   if (custom_call->custom_call_target() == \"__onednn$matmul_reorder\") {\n     return HandleOneDnnMatMulCalls(custom_call,\n                                    runtime::kOneDnnMatMulReorderSymbolName);"
        },
        {
            "sha": "fad32a75db510b75bb0f212bbf54a48cf7a944de",
            "filename": "third_party/xla/xla/service/cpu/ir_emitter.h",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fir_emitter.h?ref=23e545fb391ed9e1d13ea9e67608eb423a088979",
            "patch": "@@ -344,7 +344,6 @@ class IrEmitter : public DfsHloVisitorWithDefault,\n       const BufferAllocation::Slice& slice, const Shape& shape);\n   absl::Status HandleOneDnnMatMulCalls(HloInstruction* hlo,\n                                        std::string runtime_symbol_name);\n-  absl::Status HandleOneDnnSoftmax(HloInstruction* hlo);\n #endif  // XLA_ONEDNN\n   // Private helper to initialize an IR function for the computation.\n   void InitializeIrFunction(const std::string& function_name);"
        },
        {
            "sha": "bea079e3d74de69a88b87c2bb1416daa43163f1d",
            "filename": "third_party/xla/xla/service/cpu/onednn_ops_rewriter.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_ops_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_ops_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_ops_rewriter.cc?ref=23e545fb391ed9e1d13ea9e67608eb423a088979",
            "patch": "@@ -596,11 +596,6 @@ class OneDnnOpsRewriterVisitor : public DfsHloRewriteVisitor {\n   }\n \n   absl::Status HandleDivide(HloInstruction* divide_instr) override {\n-    // TODO(intel-tf): remove this restriction after adding oneDNN softmax\n-    // support in thunk runtime.\n-    return absl::OkStatus();\n-\n-    // NOLINTBEGIN(clang-diagnostic-unreachable-code)\n     if (divide_instr->HasControlDependencies()) return absl::OkStatus();\n     if (!IsSupportedType(divide_instr->shape().element_type())) {\n       return absl::OkStatus();\n@@ -623,7 +618,6 @@ class OneDnnOpsRewriterVisitor : public DfsHloRewriteVisitor {\n     TF_RETURN_IF_ERROR(ReplaceInstruction(divide_instr, softmax_call));\n \n     return absl::OkStatus();\n-    // NOLINTEND(clang-diagnostic-unreachable-code)\n   }\n };\n "
        },
        {
            "sha": "e035d16793dafd76881d0ae18f547b8dfc5fefbc",
            "filename": "third_party/xla/xla/service/cpu/onednn_softmax.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 33,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_softmax.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_softmax.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_softmax.cc?ref=23e545fb391ed9e1d13ea9e67608eb423a088979",
            "patch": "@@ -25,59 +25,42 @@ limitations under the License.\n #include \"oneapi/dnnl/dnnl_types.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/service/cpu/backend_config.pb.h\"\n-#include \"xla/service/cpu/onednn_config.pb.h\"\n-#include \"xla/service/cpu/onednn_memory_util.h\"\n #include \"xla/service/cpu/runtime_lightweight_check.h\"\n-#include \"xla/tsl/util/onednn_threadpool.h\"\n // Below must come after `onednn_threadpool.h`\n #include \"unsupported/Eigen/CXX11/Tensor\"  // NOLINT\n \n namespace xla {\n namespace cpu {\n \n-ABSL_ATTRIBUTE_NO_SANITIZE_MEMORY void __xla_cpu_runtime_OneDnnSoftmax(\n-    const void* run_options_ptr, void* input, void* result,\n-    void* softmax_config_ptr) {\n-  const xla::ExecutableRunOptions* run_options =\n-      static_cast<const xla::ExecutableRunOptions*>(run_options_ptr);\n-  XLA_LIGHTWEIGHT_CHECK(run_options != nullptr);\n-  XLA_LIGHTWEIGHT_CHECK(run_options->intra_op_thread_pool() != nullptr);\n-  tsl::OneDnnThreadPool thread_pool(\n-      run_options->intra_op_thread_pool()->getPool(), false);\n-  dnnl::engine cpu_engine(dnnl::engine::kind::cpu, 0);\n-#ifndef ENABLE_ONEDNN_OPENMP\n-  auto onednn_stream = dnnl::stream(\n-      dnnl::threadpool_interop::make_stream(cpu_engine, &thread_pool));\n-#else\n-  auto onednn_stream = dnnl::stream(cpu_engine);\n-#endif  // ENABLE_ONEDNN_OPENMP\n-\n-  std::string config_str(static_cast<const char*>(softmax_config_ptr));\n-  OneDnnSoftmaxConfig softmax_config;\n-  softmax_config.ParseFromString(config_str);\n-\n-  MemrefInfo input_minfo(input);\n-  MemrefInfo result_minfo(result);\n+void ExecuteOneDnnSoftmax(absl::Span<MemrefInfoHandler> arguments,\n+                          absl::Span<MemrefInfoHandler> results,\n+                          OneDnnSoftmaxConfig softmax_config,\n+                          const dnnl::engine& cpu_engine,\n+                          dnnl::stream& onednn_stream,\n+                          OneDnnResources& resources) {\n+  MemrefInfo input_minfo(arguments[0].get());\n+  MemrefInfo result_minfo(results[0].get());\n \n   auto src_md = input_minfo.GetOneDnnMemDesc();\n   auto dst_md = result_minfo.GetOneDnnMemDesc();\n \n-  auto src_mem = dnnl::memory(src_md, cpu_engine, input_minfo.Data());\n-  auto dst_mem = dnnl::memory(dst_md, cpu_engine, result_minfo.Data());\n+  resources.src_mem = dnnl::memory(src_md, cpu_engine, input_minfo.Data());\n+  resources.dst_mem = dnnl::memory(dst_md, cpu_engine, result_minfo.Data());\n \n   int axis = softmax_config.softmax_axis();\n \n   auto softmax_pd = dnnl::softmax_forward::primitive_desc(\n       cpu_engine, dnnl::prop_kind::forward_inference,\n       dnnl::algorithm::softmax_accurate, src_md, dst_md, axis);\n \n-  auto softmax_prim = dnnl::softmax_forward(softmax_pd);\n+  resources.primitive = dnnl::primitive(softmax_pd);\n \n-  std::unordered_map<int, dnnl::memory> softmax_args;\n-  softmax_args.insert({DNNL_ARG_SRC, src_mem});\n-  softmax_args.insert({DNNL_ARG_DST, dst_mem});\n+  std::unordered_map<int, dnnl::memory> softmax_args = {\n+      {DNNL_ARG_SRC, resources.src_mem},\n+      {DNNL_ARG_DST, resources.dst_mem},\n+  };\n \n-  softmax_prim.execute(onednn_stream, softmax_args);\n+  resources.primitive.execute(onednn_stream, softmax_args);\n }\n \n }  // namespace cpu"
        },
        {
            "sha": "031db964761a4a806b96543b8ae3292bb6505f3f",
            "filename": "third_party/xla/xla/service/cpu/onednn_softmax.h",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_softmax.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_softmax.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_softmax.h?ref=23e545fb391ed9e1d13ea9e67608eb423a088979",
            "patch": "@@ -16,14 +16,19 @@ limitations under the License.\n #ifndef XLA_SERVICE_CPU_ONEDNN_SOFTMAX_H_\n #define XLA_SERVICE_CPU_ONEDNN_SOFTMAX_H_\n \n+#include \"dnnl.hpp\"\n+#include \"xla/service/cpu/onednn_config.pb.h\"\n+#include \"xla/service/cpu/onednn_memory_util.h\"\n+\n namespace xla {\n namespace cpu {\n \n-extern \"C\" {\n-extern void __xla_cpu_runtime_OneDnnSoftmax(const void* run_options_ptr,\n-                                            void* input, void* result,\n-                                            void* softmax_config_ptr);\n-}  // extern \"C\"\n+void ExecuteOneDnnSoftmax(absl::Span<MemrefInfoHandler> arguments,\n+                          absl::Span<MemrefInfoHandler> results,\n+                          OneDnnSoftmaxConfig softmax_config,\n+                          const dnnl::engine& cpu_engine,\n+                          dnnl::stream& onednn_stream,\n+                          OneDnnResources& resources);\n \n }  // namespace cpu\n }  // namespace xla"
        },
        {
            "sha": "240f9d4cb109e6c8de01685d000404d774dbf7f5",
            "filename": "third_party/xla/xla/service/cpu/runtime_symbol_generator.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_symbol_generator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_symbol_generator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fruntime_symbol_generator.cc?ref=23e545fb391ed9e1d13ea9e67608eb423a088979",
            "patch": "@@ -57,7 +57,6 @@ limitations under the License.\n #ifdef XLA_ONEDNN\n #include \"xla/service/cpu/onednn_convolution.h\"\n #include \"xla/service/cpu/onednn_matmul.h\"\n-#include \"xla/service/cpu/onednn_softmax.h\"\n #endif  // XLA_ONEDNN\n \n namespace xla::cpu {\n@@ -188,7 +187,6 @@ static bool RegisterKnownJITSymbols() {\n   REGISTER_CPU_RUNTIME_SYMBOL(TopKF32);\n #ifdef XLA_ONEDNN\n   REGISTER_CPU_RUNTIME_SYMBOL(OneDnnMatMul);\n-  REGISTER_CPU_RUNTIME_SYMBOL(OneDnnSoftmax);\n   REGISTER_CPU_RUNTIME_SYMBOL(OneDnnMatMulReorder);\n #endif  // XLA_ONEDNN\n "
        },
        {
            "sha": "2991dd47cbb89934f914c3330d0f6ee42faf8fb1",
            "filename": "third_party/xla/xla/service/cpu/tests/onednn_convolution_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_convolution_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_convolution_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_convolution_test.cc?ref=23e545fb391ed9e1d13ea9e67608eb423a088979",
            "patch": "@@ -35,7 +35,7 @@ class ConvolutionTest : public HloTestBase,\n  protected:\n   DebugOptions GetDebugOptionsForTest() const override {\n     DebugOptions debug_options = HloTestBase::GetDebugOptionsForTest();\n-    debug_options.set_xla_cpu_experimental_onednn_custom_calls(true);\n+    debug_options.set_xla_cpu_experimental_onednn_custom_call(true);\n     return debug_options;\n   }\n "
        },
        {
            "sha": "11ad04d63f37cc96ef09d02cef1fd45b13058257",
            "filename": "third_party/xla/xla/service/cpu/tests/onednn_layer_norm_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_layer_norm_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_layer_norm_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_layer_norm_test.cc?ref=23e545fb391ed9e1d13ea9e67608eb423a088979",
            "patch": "@@ -25,6 +25,12 @@ namespace {\n \n class LayerNormTest : public HloTestBase {\n  protected:\n+  DebugOptions GetDebugOptionsForTest() const override {\n+    DebugOptions debug_options = HloTestBase::GetDebugOptionsForTest();\n+    debug_options.set_xla_cpu_experimental_onednn_custom_call(true);\n+    return debug_options;\n+  }\n+\n   const char* onednn_layer_norm_ =\n       R\"(\n   ; CHECK:     custom_call_target=\"__onednn$layernorm\","
        },
        {
            "sha": "50e0b92eb8b9e32b5feb70e669c76cc2d0435bfd",
            "filename": "third_party/xla/xla/service/cpu/tests/onednn_softmax_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_softmax_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_softmax_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Ftests%2Fonednn_softmax_test.cc?ref=23e545fb391ed9e1d13ea9e67608eb423a088979",
            "patch": "@@ -50,6 +50,7 @@ class OneDnnSoftmaxTest\n  protected:\n   DebugOptions GetDebugOptionsForTest() const override {\n     DebugOptions debug_options = HloTestBase::GetDebugOptionsForTest();\n+    debug_options.set_xla_cpu_experimental_onednn_custom_call(true);\n     return debug_options;\n   }\n "
        },
        {
            "sha": "5eef29119c077f668d8cec60be8b063d66ad74f5",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/23e545fb391ed9e1d13ea9e67608eb423a088979/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc?ref=23e545fb391ed9e1d13ea9e67608eb423a088979",
            "patch": "@@ -1219,6 +1219,8 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitOneDnnOpThunk(\n     config = backend_config->onednn_conv_config();\n   } else if (custom_call_target == \"__onednn$layernorm\") {\n     config = backend_config->onednn_layer_norm_config();\n+  } else if (custom_call_target == \"__onednn$softmax\") {\n+    config = backend_config->onednn_softmax_config();\n   } else {\n     return Unimplemented(\n         \"Custom call target %s is not supported in thunk runtime\",\n@@ -1250,8 +1252,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCustomCallThunk(\n \n   // TODO(penporn): Support these existing targets.\n   auto custom_call_target = custom_call->custom_call_target();\n-  if (custom_call_target == \"PadToStatic\" ||\n-      custom_call_target == \"__onednn$softmax\") {\n+  if (custom_call_target == \"PadToStatic\") {\n     return Unimplemented(\"Custom call target %s is not implemented.\",\n                          custom_call_target);\n   }"
        }
    ],
    "stats": {
        "total": 205,
        "additions": 116,
        "deletions": 89
    }
}