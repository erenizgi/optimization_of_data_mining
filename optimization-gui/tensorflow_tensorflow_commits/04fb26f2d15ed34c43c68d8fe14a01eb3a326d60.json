{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Add multi-host ragged-all-to-all decomposer pattern for combine ra2a.\n\nragged-all-to-all can be used in two pattern:\n\n1. If input size if smaller than the output, we assume that it's a dispatch phase where input is dense and we're distributing data.\n2. If output size if smaller, we assume that input is padded and we're gathering data, distributed with the first pattern.\n\nOriginal decomposer was optimized for dispatch case, because it did cross-host all-gather of dense input It's not optimal for the combine phase, because it results it transferring a lot of padding cross host.\n\nA more optimal set of operations is to do local partial ragged-all-to-all, exchange partial results, and apply them locally.\n\nPiperOrigin-RevId: 826035358",
    "sha": "04fb26f2d15ed34c43c68d8fe14a01eb3a326d60",
    "files": [
        {
            "sha": "4c45c769e6402a38b37135856885ea2c172729ad",
            "filename": "third_party/xla/xla/service/gpu/transforms/ragged_all_to_all_multi_host_decomposer.cc",
            "status": "modified",
            "additions": 236,
            "deletions": 52,
            "changes": 288,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/04fb26f2d15ed34c43c68d8fe14a01eb3a326d60/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/04fb26f2d15ed34c43c68d8fe14a01eb3a326d60/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc?ref=04fb26f2d15ed34c43c68d8fe14a01eb3a326d60",
            "patch": "@@ -52,19 +52,15 @@ using hlo_query::NextChannelId;\n \n // Corrects the offsets in the local metadata to account for the number of input\n // rows in the combined ragged tensor.\n-HloInstruction* CorrectOffsets(HloRaggedAllToAllInstruction* ragged_all_to_all,\n-                               HloInstruction* local_metadata,\n+HloInstruction* CorrectOffsets(int64_t offset, HloInstruction* local_metadata,\n                                HloComputation* computation) {\n   const Shape& shape = local_metadata->shape();\n \n   HloInstruction* iota = computation->AddInstruction(\n       HloInstruction::CreateIota(/*shape=*/shape, /*iota_dimension=*/0));\n \n-  int64_t num_input_rows = ragged_all_to_all->operand(0)->shape().dimensions(0);\n-\n-  HloInstruction* num_input_rows_constant =\n-      computation->AddInstruction(HloInstruction::CreateConstant(\n-          LiteralUtil::CreateR0<int64_t>(num_input_rows)));\n+  HloInstruction* num_input_rows_constant = computation->AddInstruction(\n+      HloInstruction::CreateConstant(LiteralUtil::CreateR0<int64_t>(offset)));\n \n   HloInstruction* num_input_rows_constant_broadcast =\n       computation->AddInstruction(HloInstruction::CreateBroadcast(\n@@ -152,7 +148,8 @@ absl::InlinedVector<HloInstruction*, 4> GetIntraHostMetadata(\n   // Correct input offsets that need to be adjusted for the number of input\n   // rows.\n   metadata_operands[0] =\n-      CorrectOffsets(ragged_all_to_all, metadata_operands[0], computation);\n+      CorrectOffsets(ragged_all_to_all->operand(0)->shape().dimensions(0),\n+                     metadata_operands[0], computation);\n \n   for (int i = 0; i < metadata_operands.size(); ++i) {\n     metadata_operands[i] =\n@@ -168,6 +165,224 @@ absl::InlinedVector<HloInstruction*, 4> GetIntraHostMetadata(\n   return metadata_operands;\n }\n \n+// Decomposes a dispatch `ragged-all-to-all` collective into an inter-host\n+// `all-gather` and an intra-host `ragged-all-to-all`.\n+//\n+// Dispatch phase of MoE layer is characterized by the following properties:\n+//   - The input is dense and all or most of the rows are significant.\n+//   - The output is larger than the input, because we need to have a static\n+//   allocation that will accommodate all the possible rows.\n+// In case of dispatch phase, doing `all-gather` on inputs first is more\n+// efficient, because we're only transferring significant data with up to 2x\n+// overhead.\n+absl::StatusOr<bool> DecomposeDispatchRaggedAllToAll(\n+    HloRaggedAllToAllInstruction* ragged_all_to_all,\n+    HloComputation* computation,\n+    absl::Span<ReplicaGroup const> inter_host_replica_groups,\n+    absl::Span<ReplicaGroup const> intra_host_replica_groups, int64_t num_hosts,\n+    int64_t num_devices_in_replica) {\n+  HloInstruction* input_operand = ragged_all_to_all->mutable_operand(0);\n+\n+  Shape new_input_shape = input_operand->shape();\n+  new_input_shape.set_dimensions(\n+      0, num_hosts * input_operand->shape().dimensions(0));\n+\n+  // The collective can run in two modes: cross-replica and cross-partition. If\n+  // the original `ragged-all-to-all` has a channel id set, then it's a\n+  // cross-partition collective. In that case `all-gather` needs a channel_id\n+  // and `use_global_device_ids=true`.\n+  // Otherwise, when `ragged-all-to-all` has no channel id, it's a cross-replica\n+  // collective. In that case `all-gather` doesn't need a `channel_id` and\n+  // `use_global_device_ids` should be set to false.\n+  HloInstruction* all_gather_input =\n+      computation->AddInstruction(HloInstruction::CreateAllGather(\n+          /*shape=*/new_input_shape,\n+          /*operands=*/{ragged_all_to_all->mutable_operand(0)},\n+          /*all_gather_dimension=*/0,\n+          /*device_list=*/CollectiveDeviceList(inter_host_replica_groups),\n+          /*constrain_layout=*/false,\n+          /*channel_id=*/ragged_all_to_all->channel_id().has_value()\n+              ? std::make_optional(NextChannelId(*computation->parent()))\n+              : std::nullopt,\n+          /*use_global_device_ids=*/\n+          ragged_all_to_all->channel_id().has_value()));\n+\n+  absl::InlinedVector<HloInstruction*, 4> intra_host_metadata =\n+      GetIntraHostMetadata(ragged_all_to_all, computation,\n+                           inter_host_replica_groups, num_hosts,\n+                           num_devices_in_replica);\n+\n+  HloInstruction* new_ragged_all_to_all =\n+      computation->AddInstruction(HloInstruction::CreateRaggedAllToAll(\n+          /*shape=*/ragged_all_to_all->shape(),\n+          /*operands=*/\n+          {all_gather_input, ragged_all_to_all->mutable_operand(1),\n+           intra_host_metadata[0], intra_host_metadata[1],\n+           intra_host_metadata[2], intra_host_metadata[3]},\n+          /*replica_groups=*/intra_host_replica_groups,\n+          /*channel_id=*/ragged_all_to_all->channel_id()));\n+\n+  TF_RETURN_IF_ERROR(computation->ReplaceInstruction(ragged_all_to_all,\n+                                                     new_ragged_all_to_all));\n+\n+  return true;\n+}\n+\n+// Decomposes a combine `ragged-all-to-all` collective.\n+//\n+// Combine phase of MoE layer is characterized by the following properties:\n+//   - The input is larget than the output, because it contains rows distributed\n+//     by the dispatch phase.\n+//   - Most of the input rows are not significant, because it's padded to\n+//     accommodate all possible rows.\n+//   - The distribution of the significant rows depends on the runtime state of\n+//     the MoE layer, so we can't reason about it in an HLO rewrite pass.\n+//\n+// An `all-gather` as a first step would be inefficient in this case, because\n+// we would be transferring a lot of padding. An optimal way is to do\n+// `ragged-all-to-all` within the hosts to partially gather the significant data\n+// into smaller temporary buffer of output size. Exchange the data cross-host\n+// and the do another local `ragged-all-to-all` to the final output. This way we\n+// transfer more significant data with minimal padding with up to 2x overhead.\n+absl::StatusOr<bool> DecomposeCombineRaggedAllToAll(\n+    HloRaggedAllToAllInstruction* ragged_all_to_all,\n+    HloComputation* computation,\n+    absl::Span<ReplicaGroup const> inter_host_replica_groups,\n+    absl::Span<ReplicaGroup const> intra_host_replica_groups, int64_t num_hosts,\n+    int64_t num_devices_in_replica, int64_t num_participating_devices) {\n+  auto* zero = computation->AddInstruction(\n+      HloInstruction::CreateConstant(LiteralUtil::Zero(\n+          ragged_all_to_all->operand(1)->shape().element_type())));\n+\n+  auto* zero_broadcast =\n+      computation->AddInstruction(HloInstruction::CreateBroadcast(\n+          /*shape=*/ragged_all_to_all->operand(1)->shape(), zero,\n+          /*broadcast_dimensions=*/{}));\n+\n+  int64_t num_updates_per_host =\n+      ragged_all_to_all->operand(2)->shape().dimensions(0) / num_hosts;\n+\n+  auto slice_metadata_operand = [&](int64_t host_id,\n+                                    HloInstruction* metadata_operand) {\n+    Shape slice_shape = metadata_operand->shape();\n+    slice_shape.set_dimensions(0, num_updates_per_host);\n+\n+    return computation->AddInstruction(HloInstruction::CreateSlice(\n+        /*shape=*/slice_shape,\n+        /*operand=*/metadata_operand,\n+        /*start_indices=*/{num_updates_per_host * host_id},\n+        /*limit_indices=*/{num_updates_per_host * (host_id + 1)},\n+        /*strides=*/{1}));\n+  };\n+\n+  absl::InlinedVector<HloInstruction*, 4> intra_host_ragged_all_to_alls(\n+      num_hosts);\n+  for (int64_t host_id = 0; host_id < num_hosts; ++host_id) {\n+    absl::InlinedVector<HloInstruction*, 4> ragged_all_to_all_operands{\n+        ragged_all_to_all->mutable_operand(0),\n+        zero_broadcast,\n+        slice_metadata_operand(host_id, ragged_all_to_all->mutable_operand(2)),\n+        slice_metadata_operand(host_id, ragged_all_to_all->mutable_operand(3)),\n+        slice_metadata_operand(host_id, ragged_all_to_all->mutable_operand(4)),\n+        slice_metadata_operand(host_id, ragged_all_to_all->mutable_operand(5)),\n+    };\n+\n+    intra_host_ragged_all_to_alls[host_id] =\n+        computation->AddInstruction(HloInstruction::CreateRaggedAllToAll(\n+            /*shape=*/ragged_all_to_all->shape(),\n+            /*operands=*/ragged_all_to_all_operands,\n+            /*replica_groups=*/intra_host_replica_groups,\n+            /*channel_id=*/ragged_all_to_all->channel_id().has_value()\n+                ? std::make_optional(NextChannelId(*computation->parent()))\n+                : std::nullopt));\n+  }\n+\n+  Shape concatenated_inputs_shape = ragged_all_to_all->shape();\n+  concatenated_inputs_shape.set_dimensions(\n+      0, num_hosts * ragged_all_to_all->shape().dimensions(0));\n+\n+  HloInstruction* concatenated_inputs =\n+      computation->AddInstruction(HloInstruction::CreateConcatenate(\n+          /*shape=*/concatenated_inputs_shape,\n+          /*operands=*/intra_host_ragged_all_to_alls, /*dimension=*/0));\n+\n+  HloInstruction* local_inputs =\n+      computation->AddInstruction(HloInstruction::CreateAllToAll(\n+          concatenated_inputs->shape(), {concatenated_inputs},\n+          /*device_list=*/CollectiveDeviceList(inter_host_replica_groups),\n+          /*constrain_layout=*/false,\n+          /*channel_id=*/ragged_all_to_all->channel_id().has_value()\n+              ? std::make_optional(NextChannelId(*computation->parent()))\n+              : std::nullopt,\n+          /*split_dimension=*/0));\n+\n+  absl::InlinedVector<ReplicaGroup, 16> degenerated_replica_groups(\n+      num_participating_devices);\n+  for (int64_t i = 0; i < num_participating_devices; ++i) {\n+    degenerated_replica_groups[i].add_replica_ids(i);\n+  }\n+\n+  HloInstruction* output_offsets = ragged_all_to_all->mutable_operand(4);\n+  int64_t num_updates_per_replica =\n+      output_offsets->shape().dimensions(0) / num_devices_in_replica;\n+\n+  output_offsets = computation->AddInstruction(HloInstruction::CreateReshape(\n+      /*shape=*/ShapeUtil::MakeShape(\n+          output_offsets->shape().element_type(),\n+          {num_devices_in_replica, num_updates_per_replica}),\n+      /*operand=*/output_offsets));\n+\n+  output_offsets = computation->AddInstruction(HloInstruction::CreateAllToAll(\n+      /*shape=*/output_offsets->shape(),\n+      /*operands=*/{output_offsets},\n+      /*device_list=*/ragged_all_to_all->device_list(),\n+      /*constrain_layout=*/false,\n+      /*channel_id=*/ragged_all_to_all->channel_id().has_value()\n+          ? std::make_optional(NextChannelId(*computation->parent()))\n+          : std::nullopt,\n+      /*split_dimension=*/0));\n+\n+  HloInstruction* corrected_output_offsets = output_offsets;\n+\n+  int64_t num_devices_in_replica_per_host = num_devices_in_replica / num_hosts;\n+\n+  corrected_output_offsets =\n+      computation->AddInstruction(HloInstruction::CreateReshape(\n+          /*shape=*/ShapeUtil::MakeShape(\n+              output_offsets->shape().element_type(),\n+              {num_hosts, num_devices_in_replica_per_host,\n+               num_updates_per_replica}),\n+          /*operand=*/corrected_output_offsets));\n+\n+  corrected_output_offsets =\n+      CorrectOffsets(ragged_all_to_all->operand(1)->shape().dimensions(0),\n+                     corrected_output_offsets, computation);\n+\n+  output_offsets = computation->AddInstruction(HloInstruction::CreateReshape(\n+      /*shape=*/ragged_all_to_all->operand(2)->shape(),\n+      /*operand=*/output_offsets));\n+\n+  corrected_output_offsets =\n+      computation->AddInstruction(HloInstruction::CreateReshape(\n+          /*shape=*/ragged_all_to_all->operand(2)->shape(),\n+          /*operand=*/corrected_output_offsets));\n+\n+  HloInstruction* local_ragged_all_to_all =\n+      computation->AddInstruction(HloInstruction::CreateRaggedAllToAll(\n+          /*shape=*/ragged_all_to_all->shape(),\n+          /*operands=*/\n+          {local_inputs, ragged_all_to_all->mutable_operand(1),\n+           corrected_output_offsets, ragged_all_to_all->mutable_operand(5),\n+           output_offsets, ragged_all_to_all->mutable_operand(5)},\n+          /*device_list=*/CollectiveDeviceList(degenerated_replica_groups),\n+          /*channel_id=*/ragged_all_to_all->channel_id()));\n+\n+  TF_RETURN_IF_ERROR(computation->ReplaceInstruction(ragged_all_to_all,\n+                                                     local_ragged_all_to_all));\n+\n+  return true;\n+}\n+\n absl::StatusOr<bool> DecomposeRaggedAllToAll(\n     HloInstruction* hlo, HloComputation* computation, HloModule* module,\n     int64_t fast_interconnect_slice_size) {\n@@ -250,51 +465,20 @@ absl::StatusOr<bool> DecomposeRaggedAllToAll(\n     }\n   }\n \n-  HloInstruction* input_operand = ragged_all_to_all->mutable_operand(0);\n-\n-  Shape new_input_shape = input_operand->shape();\n-  new_input_shape.set_dimensions(\n-      0, num_hosts * input_operand->shape().dimensions(0));\n-\n-  // The collective can run in two modes: cross-replica and cross-partition. If\n-  // the original `ragged-all-to-all` has a channel id set, then it's a\n-  // cross-partition collective. In that case `all-gather` needs a channel_id\n-  // and `use_global_device_ids=true`.\n-  // Otherwise, when `ragged-all-to-all` has no channel id, it's a cross-replica\n-  // collective. In that case `all-gather` doesn't need a `channel_id` and\n-  // `use_global_device_ids` should be set to false.\n-  HloInstruction* all_gather_input =\n-      computation->AddInstruction(HloInstruction::CreateAllGather(\n-          /*shape=*/new_input_shape,\n-          /*operands=*/{ragged_all_to_all->mutable_operand(0)},\n-          /*all_gather_dimension=*/0,\n-          /*device_list=*/CollectiveDeviceList(inter_host_replica_groups),\n-          /*constrain_layout=*/false,\n-          /*channel_id=*/ragged_all_to_all->channel_id().has_value()\n-              ? std::make_optional(NextChannelId(*computation->parent()))\n-              : std::nullopt,\n-          /*use_global_device_ids=*/\n-          ragged_all_to_all->channel_id().has_value()));\n-\n-  absl::InlinedVector<HloInstruction*, 4> intra_host_metadata =\n-      GetIntraHostMetadata(ragged_all_to_all, computation,\n-                           inter_host_replica_groups, num_hosts,\n-                           num_devices_in_replica);\n-\n-  HloInstruction* new_ragged_all_to_all =\n-      computation->AddInstruction(HloInstruction::CreateRaggedAllToAll(\n-          /*shape=*/ragged_all_to_all->shape(),\n-          /*operands=*/\n-          {all_gather_input, ragged_all_to_all->mutable_operand(1),\n-           intra_host_metadata[0], intra_host_metadata[1],\n-           intra_host_metadata[2], intra_host_metadata[3]},\n-          /*replica_groups=*/intra_host_replica_groups,\n-          /*channel_id=*/ragged_all_to_all->channel_id()));\n-\n-  TF_RETURN_IF_ERROR(\n-      computation->ReplaceInstruction(hlo, new_ragged_all_to_all));\n+  int64_t num_input_rows = ragged_all_to_all->operand(0)->shape().dimensions(0);\n+  int64_t num_output_rows =\n+      ragged_all_to_all->operand(1)->shape().dimensions(0);\n+\n+  if (num_input_rows > num_output_rows) {\n+    return DecomposeCombineRaggedAllToAll(\n+        ragged_all_to_all, computation, inter_host_replica_groups,\n+        intra_host_replica_groups, num_hosts, num_devices_in_replica,\n+        num_participating_devices);\n+  }\n \n-  return true;\n+  return DecomposeDispatchRaggedAllToAll(\n+      ragged_all_to_all, computation, inter_host_replica_groups,\n+      intra_host_replica_groups, num_hosts, num_devices_in_replica);\n }\n \n absl::StatusOr<bool> RaggedAllToAllMultiHostDecomposer::Run("
        },
        {
            "sha": "78de419d19e869e91c1c6e3573f9cab23379536c",
            "filename": "third_party/xla/xla/service/gpu/transforms/ragged_all_to_all_multi_host_decomposer_test.cc",
            "status": "modified",
            "additions": 34,
            "deletions": 0,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/04fb26f2d15ed34c43c68d8fe14a01eb3a326d60/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/04fb26f2d15ed34c43c68d8fe14a01eb3a326d60/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer_test.cc?ref=04fb26f2d15ed34c43c68d8fe14a01eb3a326d60",
            "patch": "@@ -124,6 +124,40 @@ ENTRY main {\n   EXPECT_FALSE(changed);\n }\n \n+TEST_F(RaggedAllToAllDecomposerTest, CombineRaggedAllToAllIsDecomposed) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+HloModule module, replica_count=16\n+\n+ENTRY main {\n+  input = bf16[4096,128] parameter(0)\n+  output = bf16[256,128] parameter(1)\n+  input_offsets = s64[16] parameter(2)\n+  send_sizes = s64[16] parameter(3)\n+  output_offsets = s64[16] parameter(4)\n+  recv_sizes = s64[16] parameter(5)\n+  ROOT ra2a = bf16[256,128] ragged-all-to-all(input, output, input_offsets,\n+    send_sizes, output_offsets, recv_sizes),\n+    replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15}}\n+}\n+)\"));\n+\n+  RaggedAllToAllMultiHostDecomposer decomposer(\n+      /*fast_interconnect_slice_size=*/8);\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, decomposer.Run(module.get(), {}));\n+\n+  EXPECT_TRUE(changed);\n+  TF_EXPECT_OK(VerifyHloModule(module.get(), true, true));\n+  TF_EXPECT_OK(HloDCE().Run(module.get()));\n+  TF_EXPECT_OK(HloCSE(true).Run(module.get()));\n+\n+  EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n+    // CHECK-COUNT-2: ragged-all-to-all{{.*}}, replica_groups={{[{]}}{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}{{[}]}}\n+    // CHECK: all-to-all{{.*}}, replica_groups={{[{]}}{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}{{[}]}}\n+    // CHECK: all-to-all{{.*}}, replica_groups={{[{]}}{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15}{{[}]}}\n+    // CHECK: ragged-all-to-all{{.*}}, replica_groups={{[{]}}{0},{1},{2},{3},{4},{5},{6},{7},{8},{9},{10},{11},{12},{13},{14},{15}{{[}]}}\n+  )\"));\n+}\n+\n TEST_F(RaggedAllToAllDecomposerTest, MultipleReplicaGroupsAreSupported) {\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n HloModule module"
        },
        {
            "sha": "ee9aa3f30003ba7533915ab87c86562cd1bcb5ce",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test.cc",
            "status": "modified",
            "additions": 51,
            "deletions": 23,
            "changes": 74,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/04fb26f2d15ed34c43c68d8fe14a01eb3a326d60/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/04fb26f2d15ed34c43c68d8fe14a01eb3a326d60/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc?ref=04fb26f2d15ed34c43c68d8fe14a01eb3a326d60",
            "patch": "@@ -36,6 +36,7 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/str_replace.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/strings/substitute.h\"\n #include \"absl/types/span.h\"\n #include \"xla/array.h\"\n #include \"xla/error_spec.h\"\n@@ -3564,7 +3565,9 @@ INSTANTIATE_TEST_SUITE_P(\n                           RaggedAllToAllImplTypeName(std::get<1>(info.param)));\n     });\n \n-class RaggedAllToAllMultiHostDecomposerTest : public RaggedAllToAllTestBase {\n+class RaggedAllToAllMultiHostDecomposerTest\n+    : public RaggedAllToAllTestBase,\n+      public ::testing::WithParamInterface<std::tuple<int64_t, int64_t>> {\n  public:\n   RaggedAllToAllMultiHostDecomposerTest()\n       : RaggedAllToAllTestBase(/*enable_async=*/false,\n@@ -3582,21 +3585,25 @@ class RaggedAllToAllMultiHostDecomposerTest : public RaggedAllToAllTestBase {\n   }\n };\n \n-TEST_F(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_2GPUs_SliceSize1) {\n-  absl::string_view kModuleReplicatedStr = R\"(\n+TEST_P(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_2GPUs_SliceSize1) {\n+  auto [num_input_rows, num_output_rows] = GetParam();\n+\n+  std::string kModuleReplicatedStr =\n+      absl::Substitute(R\"(\n   HloModule module, num_partitions=1\n \n   ENTRY entry {\n-    input = f32[512,5,32] parameter(0)\n-    output = f32[512,5,32] parameter(1)\n+    input = f32[$0,5,32] parameter(0)\n+    output = f32[$1,5,32] parameter(1)\n     input_offsets = s32[32] parameter(2)\n     send_sizes = s32[32] parameter(3)\n     output_offsets = s32[32] parameter(4)\n     recv_sizes = s32[32] parameter(5)\n-    ROOT ra2a = f32[512,5,32] ragged-all-to-all(input, output,\n-      input_offsets, send_sizes, output_offsets, recv_sizes), \n+    ROOT ra2a = f32[$1,5,32] ragged-all-to-all(input, output,\n+      input_offsets, send_sizes, output_offsets, recv_sizes),\n       replica_groups={{0,1}}\n-  })\";\n+  })\",\n+                       num_input_rows, num_output_rows);\n \n   const int64_t kNumReplicas = 2;\n   const int64_t kNumPartitions = 1;\n@@ -3633,21 +3640,25 @@ TEST_F(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_2GPUs_SliceSize1) {\n   }\n }\n \n-TEST_F(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_8GPUs_SliceSize4) {\n-  absl::string_view kModuleReplicatedStr = R\"(\n+TEST_P(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_8GPUs_SliceSize4) {\n+  auto [num_input_rows, num_output_rows] = GetParam();\n+\n+  std::string kModuleReplicatedStr =\n+      absl::Substitute(R\"(\n   HloModule module, num_partitions=1\n \n   ENTRY entry {\n-    input = f32[512,5,32] parameter(0)\n-    output = f32[512,5,32] parameter(1)\n+    input = f32[$0,5,32] parameter(0)\n+    output = f32[$1,5,32] parameter(1)\n     input_offsets = s32[32] parameter(2)\n     send_sizes = s32[32] parameter(3)\n     output_offsets = s32[32] parameter(4)\n     recv_sizes = s32[32] parameter(5)\n-    ROOT ra2a = f32[512,5,32] ragged-all-to-all(input, output,\n-      input_offsets, send_sizes, output_offsets, recv_sizes), \n+    ROOT ra2a = f32[$1,5,32] ragged-all-to-all(input, output,\n+      input_offsets, send_sizes, output_offsets, recv_sizes),\n       replica_groups={{0,1,2,3,4,5,6,7}}\n-  })\";\n+  })\",\n+                       num_input_rows, num_output_rows);\n \n   const int64_t kNumReplicas = 8;\n   const int64_t kNumPartitions = 1;\n@@ -3669,7 +3680,7 @@ TEST_F(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_8GPUs_SliceSize4) {\n \n   Array<int64_t> input_sizes(\n       {kNumReplicas, kNumReplicas, kNumUpdatesPerReplica});\n-  input_sizes.FillRandomUniform(0, 10);\n+  input_sizes.FillRandomUniform(0, 16);\n \n   TF_ASSERT_OK(CreateRandomTestData(module.get(), input_sizes));\n \n@@ -3686,22 +3697,26 @@ TEST_F(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_8GPUs_SliceSize4) {\n   }\n }\n \n-TEST_F(RaggedAllToAllMultiHostDecomposerTest,\n+TEST_P(RaggedAllToAllMultiHostDecomposerTest,\n        RaggedAllToAll_8GPUs_SliceSize4_2ReplicaGroups) {\n-  absl::string_view kModuleReplicatedStr = R\"(\n+  auto [num_input_rows, num_output_rows] = GetParam();\n+\n+  std::string kModuleReplicatedStr =\n+      absl::Substitute(R\"(\n   HloModule module, num_partitions=1\n \n   ENTRY entry {\n-    input = f32[512,5,32] parameter(0)\n-    output = f32[512,5,32] parameter(1)\n+    input = f32[$0,5,32] parameter(0)\n+    output = f32[$1,5,32] parameter(1)\n     input_offsets = s32[32] parameter(2)\n     send_sizes = s32[32] parameter(3)\n     output_offsets = s32[32] parameter(4)\n     recv_sizes = s32[32] parameter(5)\n-    ROOT ra2a = f32[512,5,32] ragged-all-to-all(input, output,\n-      input_offsets, send_sizes, output_offsets, recv_sizes), \n+    ROOT ra2a = f32[$1,5,32] ragged-all-to-all(input, output,\n+      input_offsets, send_sizes, output_offsets, recv_sizes),\n       replica_groups={{0,2,4,6},{1,3,5,7}}\n-  })\";\n+  })\",\n+                       num_input_rows, num_output_rows);\n \n   const int64_t kNumReplicas = 8;\n   const int64_t kNumReplicasPerGroup = 4;\n@@ -3741,6 +3756,19 @@ TEST_F(RaggedAllToAllMultiHostDecomposerTest,\n   }\n }\n \n+INSTANTIATE_TEST_SUITE_P(\n+    RaggedAllToAllMultiHostDecomposerTest,\n+    RaggedAllToAllMultiHostDecomposerTest,\n+    ::testing::Values(std::make_tuple(512, 4096), std::make_tuple(4096, 512)),\n+    [](const ::testing::TestParamInfo<std::tuple<int64_t, int64_t>>& info) {\n+      if (std::get<0>(info.param) > std::get<1>(info.param)) {\n+        return absl::StrCat(\"combine_\", std::get<0>(info.param), \"_\",\n+                            std::get<1>(info.param));\n+      }\n+      return absl::StrCat(\"dispatch_\", std::get<0>(info.param), \"_\",\n+                          std::get<1>(info.param));\n+    });\n+\n TEST_F(CollectiveOpsTestE2E, MemcpyP2pWhileLoopCorrectness) {\n   absl::string_view hlo_string = R\"(\n HloModule MemcpyP2pWhileLoopCorrectness, entry_computation_layout={(bf16[128,96]{1,0})->(bf16[32,384]{1,0}, bf16[32,384]{1,0})}, allow_spmd_sharding_propagation_to_output={true,true}, num_partitions=4"
        }
    ],
    "stats": {
        "total": 396,
        "additions": 321,
        "deletions": 75
    }
}