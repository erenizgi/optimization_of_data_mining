{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 846026272",
    "sha": "0e07a3d2e3832807dde0b867ed7c507d572f04c3",
    "files": [
        {
            "sha": "dfc4077e8f10a19b6d2b296ee4e69022adf6692e",
            "filename": "tensorflow/core/tpu/kernels/image_resize_ops.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fimage_resize_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fimage_resize_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fimage_resize_ops.cc?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -57,7 +57,7 @@ class TpuCustomResizeOp : public XlaOpKernel {\n     return output_shape;\n   }\n \n-  string OpaqueField() const {\n+  std::string OpaqueField() const {\n     return absl::StrCat(\"\\\"\", align_corners_, half_pixel_centers_, \"\\\"\");\n   }\n "
        },
        {
            "sha": "2d13813db101cf18e2ceeb543de2e29375ca7629",
            "filename": "tensorflow/core/tpu/kernels/infeed_ops.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Finfeed_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Finfeed_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Finfeed_ops.cc?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -188,7 +188,9 @@ struct LinearizedBuffersWrapper {\n   ~LinearizedBuffersWrapper() = default;\n \n   // These functions are tensorflow::Variant requirements.\n-  string TypeName() const { return \"(anonymous)::LinearizedBuffersWrapper\"; }\n+  std::string TypeName() const {\n+    return \"(anonymous)::LinearizedBuffersWrapper\";\n+  }\n   void Encode(tensorflow::VariantTensorData* data) const {\n     LOG(ERROR) << \"Encode() is not implemented for LinearizedBuffersWrapper \"\n                   \"objects.\";"
        },
        {
            "sha": "2fa5972f29af4678a565cae20c78ab0d55ada190",
            "filename": "tensorflow/core/tpu/kernels/sparse_core_ops_utils.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fsparse_core_ops_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fsparse_core_ops_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fsparse_core_ops_utils.cc?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -44,10 +44,10 @@ limitations under the License.\n \n namespace tensorflow {\n \n-std::vector<int> ConvertBinarySplitsToBucketSplits(int64 split,\n+std::vector<int> ConvertBinarySplitsToBucketSplits(int64_t split,\n                                                    int max_division_level) {\n   std::vector<int> bucket_splits;\n-  uint32 current_index = 0;\n+  uint32_t current_index = 0;\n   while (split > 0) {\n     if (split % 2 == 1) {\n       int split_level = absl::bit_width(current_index + 1) - 1;\n@@ -62,9 +62,9 @@ std::vector<int> ConvertBinarySplitsToBucketSplits(int64 split,\n   return bucket_splits;\n }\n \n-int64 ConvertBucketSplitsToBinarySplits(std::vector<int> bucket_splits,\n-                                        int max_division_level) {\n-  int64 binary_splits = 0;\n+int64_t ConvertBucketSplitsToBinarySplits(std::vector<int> bucket_splits,\n+                                          int max_division_level) {\n+  int64_t binary_splits = 0;\n   for (auto& bucket_split : bucket_splits) {\n     int split_level = max_division_level - 1;\n     while (bucket_split > 0 && bucket_split % 2 == 0) {"
        },
        {
            "sha": "cd958fc5d2218d372ee10362cb9680095ea564ad",
            "filename": "tensorflow/core/tpu/kernels/sparse_core_ops_utils.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fsparse_core_ops_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fsparse_core_ops_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fsparse_core_ops_utils.h?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -33,11 +33,11 @@ namespace tensorflow {\n // Pad value used for SparseCore mini batching logic.\n const int32_t kXlaPadValue = std::numeric_limits<int32_t>::max();\n \n-std::vector<int> ConvertBinarySplitsToBucketSplits(int64 split,\n+std::vector<int> ConvertBinarySplitsToBucketSplits(int64_t split,\n                                                    int max_division_level);\n \n-int64 ConvertBucketSplitsToBinarySplits(std::vector<int> bucket_splits,\n-                                        int max_division_level);\n+int64_t ConvertBucketSplitsToBinarySplits(std::vector<int> bucket_splits,\n+                                          int max_division_level);\n \n absl::Status ValidateInputCombiner(const std::string& combiner);\n "
        },
        {
            "sha": "6a241cdb3a3795af0b8c618468bad90f3336354e",
            "filename": "tensorflow/core/tpu/kernels/sparse_core_ops_utils_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fsparse_core_ops_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fsparse_core_ops_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fsparse_core_ops_utils_test.cc?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -25,35 +25,35 @@ namespace {\n TEST(ConvertSplitsAndBackTest, Split0) {\n   const int max_division_level = 6;\n \n-  int64 original_split = 0;\n+  int64_t original_split = 0;\n   std::vector<int> actual_buckets =\n       ConvertBinarySplitsToBucketSplits(original_split, max_division_level);\n   std::vector<int> expected_buckets = {};\n-  int64 re_split =\n+  int64_t re_split =\n       ConvertBucketSplitsToBinarySplits(expected_buckets, max_division_level);\n   ASSERT_EQ(re_split, original_split);\n }\n \n TEST(ConvertSplitsAndBackTest, Split2) {\n   const int max_division_level = 6;\n \n-  int64 original_split = 2;\n+  int64_t original_split = 2;\n   std::vector<int> actual_buckets =\n       ConvertBinarySplitsToBucketSplits(original_split, max_division_level);\n   std::vector<int> expected_buckets = {16};\n-  int64 re_split =\n+  int64_t re_split =\n       ConvertBucketSplitsToBinarySplits(expected_buckets, max_division_level);\n   ASSERT_EQ(re_split, original_split);\n }\n \n TEST(ConvertSplitsAndBackTest, Split3) {\n   const int max_division_level = 6;\n \n-  int64 original_split = 3;\n+  int64_t original_split = 3;\n   std::vector<int> actual_buckets =\n       ConvertBinarySplitsToBucketSplits(original_split, max_division_level);\n   std::vector<int> expected_buckets = {16, 32};\n-  int64 re_split =\n+  int64_t re_split =\n       ConvertBucketSplitsToBinarySplits(expected_buckets, max_division_level);\n   ASSERT_EQ(re_split, original_split);\n }"
        },
        {
            "sha": "ddd47e0d53c7014090a479821942f6400abc8559",
            "filename": "tensorflow/core/tpu/kernels/sparse_core_preprocess_ops.cc",
            "status": "modified",
            "additions": 98,
            "deletions": 96,
            "changes": 194,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fsparse_core_preprocess_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fsparse_core_preprocess_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fsparse_core_preprocess_ops.cc?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -118,9 +118,9 @@ absl::Status ValidateInputs(const Tensor& indices_or_row_splits,\n }\n \n absl::Status ComputeRowIdsBeforePadding(const Tensor& indices_or_row_splits,\n-                                        const int32 total_id_count,\n-                                        const int32 sample_count,\n-                                        int32* row_ids_before_padding,\n+                                        const int32_t total_id_count,\n+                                        const int32_t sample_count,\n+                                        int32_t* row_ids_before_padding,\n                                         std::vector<int> shape_strides) {\n   // The only difference between dense tensor, sparse tensor and ragged tensor\n   // is the row ids output.\n@@ -129,7 +129,7 @@ absl::Status ComputeRowIdsBeforePadding(const Tensor& indices_or_row_splits,\n     // Row ids are just the index ids.\n     // Note: this path is also taken when the input is a ragged/sparse tensor\n     // with 0 elements. In that case, the row_ids will just be empty as well.\n-    for (int32 i = 0; i < total_id_count; ++i) {\n+    for (int32_t i = 0; i < total_id_count; ++i) {\n       *(row_ids_before_padding + i) = i;\n     }\n   } else if (indices_or_row_splits.dims() == 2 &&\n@@ -140,12 +140,12 @@ absl::Status ComputeRowIdsBeforePadding(const Tensor& indices_or_row_splits,\n     // For 2D sparse tensor, as we always combine on the last dimension.\n     // The row ids are just the sample ids which is the first dim of the\n     // indices.\n-    auto indices_matrix = indices_or_row_splits.matrix<int32>();\n+    auto indices_matrix = indices_or_row_splits.matrix<int32_t>();\n     // TODO(b/432045101): remove this once the bug is fixed.\n     if (indices_matrix.dimension(1) == 2) {\n-      int32 previous_row_id = -1;\n-      for (int32 i = 0; i < total_id_count; ++i) {\n-        int32 current_row_id = indices_matrix(i, 0);\n+      int32_t previous_row_id = -1;\n+      for (int32_t i = 0; i < total_id_count; ++i) {\n+        int32_t current_row_id = indices_matrix(i, 0);\n         if (current_row_id < previous_row_id) {\n           return absl::InvalidArgumentError(\n               \"Invalid indices_or_row_splits input, indices of SparseTensor \"\n@@ -173,7 +173,7 @@ absl::Status ComputeRowIdsBeforePadding(const Tensor& indices_or_row_splits,\n             \"Invalid shape_strides input, expected non-empty shape_strides for \"\n             \"SparseTensor with rank > 2.\");\n       }\n-      int32 previous_row_id = -1;\n+      int32_t previous_row_id = -1;\n       int32_t rank = indices_matrix.dimension(1) - 1;\n       for (int32_t i = 0; i < total_id_count; ++i) {\n         int32_t current_row_id = 0;\n@@ -205,10 +205,10 @@ absl::Status ComputeRowIdsBeforePadding(const Tensor& indices_or_row_splits,\n   } else if (indices_or_row_splits.dims() == 1 &&\n              indices_or_row_splits.NumElements() > 0) {\n     // Ragged tensor to COO format.\n-    const int32* indices_or_row_splits_ptr =\n-        indices_or_row_splits.flat<int32>().data();\n-    int32 current_row_id = -1;\n-    for (int32 i = 0; i < total_id_count; ++i) {\n+    const int32_t* indices_or_row_splits_ptr =\n+        indices_or_row_splits.flat<int32_t>().data();\n+    int32_t current_row_id = -1;\n+    for (int32_t i = 0; i < total_id_count; ++i) {\n       while (i == *(indices_or_row_splits_ptr + 1 + current_row_id)) {\n         current_row_id += 1;\n       }\n@@ -308,7 +308,7 @@ absl::Status SortDedupAndCountStatsOfCooTensor(\n     uint32_t previous_id_array_index = 0;\n     for (int32_t index = 0; index < total_id_count; ++index) {\n       uint64_t item = per_feature_col_ids_index_list[index];\n-      int32 col_id = item >> 32;\n+      int32_t col_id = item >> 32;\n       uint32_t id_array_index = item & 0xffffffff;\n       int32_t row_id = *(row_ids_ptr + id_array_index);\n       // If the row ids and col ids are both same as the previous one,\n@@ -362,9 +362,9 @@ class ConvertToCooTensorOp : public OpKernel {\n     OP_REQUIRES_OK(ctx, ValidateInputs(*indices_or_row_splits, *values,\n                                        *weights, sample_count_));\n \n-    const int32 total_id_count = values->NumElements();\n+    const int32_t total_id_count = values->NumElements();\n \n-    auto row_ids_before_dedup = std::make_unique<int32[]>(total_id_count);\n+    auto row_ids_before_dedup = std::make_unique<int32_t[]>(total_id_count);\n \n     OP_REQUIRES_OK(ctx, ComputeRowIdsBeforePadding(\n                             *indices_or_row_splits, total_id_count,\n@@ -382,14 +382,14 @@ class ConvertToCooTensorOp : public OpKernel {\n     auto combiner_scale_transform_fn =\n         GetCombinerScaleTransformFunction(combiner_);\n \n-    const int32* row_ids_before_dedup_ptr = row_ids_before_dedup.get();\n-    const int32* values_ptr = values->flat<int32>().data();\n+    const int32_t* row_ids_before_dedup_ptr = row_ids_before_dedup.get();\n+    const int32_t* values_ptr = values->flat<int32_t>().data();\n     const float* weights_ptr = weights->flat<float>().data();\n \n     // Dedup the ids within one sample by just checking the adjacent ids. This\n     // will NOT result in a full deduplication.\n-    std::vector<int32> row_ids;\n-    std::vector<int32> col_ids;\n+    std::vector<int32_t> row_ids;\n+    std::vector<int32_t> col_ids;\n     std::vector<float> gains;\n     row_ids.reserve(total_id_count);\n     col_ids.reserve(total_id_count);\n@@ -400,8 +400,8 @@ class ConvertToCooTensorOp : public OpKernel {\n       const float gain = *weights_ptr;\n       const float rescaled_gain = combiner_scale_contribution_fn(gain);\n       for (int token_id = 0; token_id < total_id_count; ++token_id) {\n-        const int32 row_id = *(row_ids_before_dedup_ptr + token_id);\n-        const int32 col_id = *(values_ptr + token_id);\n+        const int32_t row_id = *(row_ids_before_dedup_ptr + token_id);\n+        const int32_t col_id = *(values_ptr + token_id);\n         if (gains_rescale.has_value()) {\n           // Compute the gain rescale before doing the dedup.\n           (*gains_rescale)[row_id] += rescaled_gain;\n@@ -417,8 +417,8 @@ class ConvertToCooTensorOp : public OpKernel {\n       }\n     } else {\n       for (int token_id = 0; token_id < total_id_count; ++token_id) {\n-        const int32 row_id = *(row_ids_before_dedup_ptr + token_id);\n-        const int32 col_id = *(values_ptr + token_id);\n+        const int32_t row_id = *(row_ids_before_dedup_ptr + token_id);\n+        const int32_t col_id = *(values_ptr + token_id);\n         const float gain = *(weights_ptr + token_id);\n         if (gains_rescale.has_value()) {\n           // Compute the gain rescale before doing the dedup.\n@@ -435,7 +435,7 @@ class ConvertToCooTensorOp : public OpKernel {\n       }\n     }\n \n-    const int32 output_id_count = row_ids.size();\n+    const int32_t output_id_count = row_ids.size();\n \n     Tensor* gains_tensor;\n     OP_REQUIRES_OK(ctx,\n@@ -450,8 +450,8 @@ class ConvertToCooTensorOp : public OpKernel {\n         ctx, ctx->allocate_output(\"col_ids\", TensorShape({output_id_count}),\n                                   &col_ids_tensor));\n \n-    int32* row_ids_tensor_ptr = row_ids_tensor->flat<int32>().data();\n-    int32* col_ids_tensor_ptr = col_ids_tensor->flat<int32>().data();\n+    int32_t* row_ids_tensor_ptr = row_ids_tensor->flat<int32_t>().data();\n+    int32_t* col_ids_tensor_ptr = col_ids_tensor->flat<int32_t>().data();\n     float* gains_tensor_ptr = gains_tensor->flat<float>().data();\n \n     if (gains_rescale.has_value()) {\n@@ -535,11 +535,11 @@ void GetMinibatchesInCsrWithPhysicalReplicaOp::Compute(OpKernelContext* ctx) {\n                           feature_width_, &max_ids_per_partition,\n                           &max_unique_ids_per_partition));\n \n-  const int32* row_ids_tensor_ptr = row_ids->flat<int32>().data();\n-  const int32* col_ids_tensor_ptr = col_ids->flat<int32>().data();\n+  const int32_t* row_ids_tensor_ptr = row_ids->flat<int32_t>().data();\n+  const int32_t* col_ids_tensor_ptr = col_ids->flat<int32_t>().data();\n   const float* gains_tensor_ptr = gains->flat<float>().data();\n-  const int64* splits_tensor_ptr = splits->flat<int64>().data();\n-  const int32* id_counts_tensor_ptr = id_counts->flat<int32>().data();\n+  const int64_t* splits_tensor_ptr = splits->flat<int64_t>().data();\n+  const int32_t* id_counts_tensor_ptr = id_counts->flat<int32_t>().data();\n \n   const int32_t total_id_count = row_ids->NumElements();\n \n@@ -556,17 +556,17 @@ void GetMinibatchesInCsrWithPhysicalReplicaOp::Compute(OpKernelContext* ctx) {\n \n   const int max_division_level = GetMinibatchMaxDivisionLevel();\n \n-  const int32 kMaxDivisions = 1 << max_division_level;\n+  const int32_t kMaxDivisions = 1 << max_division_level;\n \n-  int64 binary_splits = 0;\n+  int64_t binary_splits = 0;\n   for (int i = 0; i < splits->NumElements(); ++i) {\n     binary_splits |= *(splits_tensor_ptr + i);\n   }\n \n   std::vector<int> bucket_splits =\n       ConvertBinarySplitsToBucketSplits(binary_splits, max_division_level);\n \n-  const int32 num_minibatch_per_sc = bucket_splits.size() + 1;\n+  const int32_t num_minibatch_per_sc = bucket_splits.size() + 1;\n   sparse_core_ops_stats_handler_->Record(StatsType::NUM_MINIBATCHES_PER_SC,\n                                          num_minibatch_per_sc, device_name_,\n                                          table_name_);\n@@ -588,16 +588,16 @@ void GetMinibatchesInCsrWithPhysicalReplicaOp::Compute(OpKernelContext* ctx) {\n   bucket_splits.insert(bucket_splits.begin(), 0);\n   bucket_splits.push_back(kMaxDivisions);\n \n-  const int32 max_ids_per_chip = max_ids_per_chip_per_sample_ * sample_count_;\n+  const int32_t max_ids_per_chip = max_ids_per_chip_per_sample_ * sample_count_;\n \n   OP_REQUIRES(\n       ctx, max_ids_per_chip % xla_pad_size == 0,\n       absl::InvalidArgumentError(absl::StrCat(\n           \"The max_ids_per_chip is set to be \", max_ids_per_chip,\n           \" which is not divisible by the xla_pad_size \", xla_pad_size, \" .\")));\n \n-  const int32 padded_row_pointers_size_per_sc =\n-      xla::RoundUpTo<int32>(num_physical_replica, xla_pad_size);\n+  const int32_t padded_row_pointers_size_per_sc =\n+      xla::RoundUpTo<int32_t>(num_physical_replica, xla_pad_size);\n \n   Tensor* row_pointers_tensor;\n   OP_REQUIRES_OK(ctx,\n@@ -619,20 +619,21 @@ void GetMinibatchesInCsrWithPhysicalReplicaOp::Compute(OpKernelContext* ctx) {\n   OP_REQUIRES_OK(\n       ctx, ctx->allocate_output(\"sorted_gains\", TensorShape({max_ids_per_chip}),\n                                 &sorted_gains_tensor));\n-  int32* row_pointers_tensor_ptr = row_pointers_tensor->flat<int32>().data();\n-  int32* sorted_sample_ids_tensor_ptr =\n-      sorted_sample_ids_tensor->flat<int32>().data();\n-  int32* sorted_token_ids_tensor_ptr =\n-      sorted_token_ids_tensor->flat<int32>().data();\n+  int32_t* row_pointers_tensor_ptr =\n+      row_pointers_tensor->flat<int32_t>().data();\n+  int32_t* sorted_sample_ids_tensor_ptr =\n+      sorted_sample_ids_tensor->flat<int32_t>().data();\n+  int32_t* sorted_token_ids_tensor_ptr =\n+      sorted_token_ids_tensor->flat<int32_t>().data();\n   float* sorted_gains_tensor_ptr = sorted_gains_tensor->flat<float>().data();\n \n   // This packed id count is used to track how many ids we have packed into\n   // the output tensor and based on this we would know how many ids that we\n   // dropped.\n   int32_t packed_id_count = 0;\n \n-  int32 global_index = 0;\n-  int32 row_pointers_index = 0;\n+  int32_t global_index = 0;\n+  int32_t row_pointers_index = 0;\n   for (int sc_id = 0; sc_id < num_sc_per_chip_; ++sc_id) {\n     for (int i = 1; i < bucket_splits.size(); ++i) {\n       for (int replica_id = 0; replica_id < num_physical_replica;\n@@ -686,8 +687,8 @@ void GetMinibatchesInCsrWithPhysicalReplicaOp::Compute(OpKernelContext* ctx) {\n         }\n \n         *(row_pointers_tensor_ptr + row_pointers_index) = global_index;\n-        int32 num_ids_to_pad_per_replica =\n-            xla::RoundUpTo<int32>(global_index, xla_pad_size) - global_index;\n+        int32_t num_ids_to_pad_per_replica =\n+            xla::RoundUpTo<int32_t>(global_index, xla_pad_size) - global_index;\n         std::fill_n(sorted_token_ids_tensor_ptr + global_index,\n                     num_ids_to_pad_per_replica, kXlaPadValue);\n         std::fill_n(sorted_sample_ids_tensor_ptr + global_index,\n@@ -698,8 +699,8 @@ void GetMinibatchesInCsrWithPhysicalReplicaOp::Compute(OpKernelContext* ctx) {\n         ++row_pointers_index;\n       }\n       // Pad the row_pointers to be memory aligned.\n-      int32 num_row_pointers_to_pad =\n-          xla::RoundUpTo<int32>(row_pointers_index, xla_pad_size) -\n+      int32_t num_row_pointers_to_pad =\n+          xla::RoundUpTo<int32_t>(row_pointers_index, xla_pad_size) -\n           row_pointers_index;\n       std::fill_n(row_pointers_tensor_ptr + row_pointers_index,\n                   num_row_pointers_to_pad, global_index);\n@@ -718,7 +719,7 @@ void GetMinibatchesInCsrWithPhysicalReplicaOp::Compute(OpKernelContext* ctx) {\n                  << \" . This could potentially impact the model quality.\";\n   }\n \n-  int32 row_pointers_unpadded_size =\n+  int32_t row_pointers_unpadded_size =\n       total_num_minibatch * padded_row_pointers_size_per_sc;\n \n   Tensor* num_minibatches_per_physical_sparse_core_tensor;\n@@ -736,11 +737,11 @@ void GetMinibatchesInCsrWithPhysicalReplicaOp::Compute(OpKernelContext* ctx) {\n   OP_REQUIRES_OK(ctx, ctx->allocate_output(\"ids_unpadded_size\", TensorShape({}),\n                                            &ids_unpadded_size_tensor));\n \n-  num_minibatches_per_physical_sparse_core_tensor->flat<int32>()(0) =\n+  num_minibatches_per_physical_sparse_core_tensor->flat<int32_t>()(0) =\n       num_minibatch_per_sc;\n-  row_pointers_unpadded_size_tensor->flat<int32>()(0) =\n+  row_pointers_unpadded_size_tensor->flat<int32_t>()(0) =\n       row_pointers_unpadded_size;\n-  ids_unpadded_size_tensor->flat<int32>()(0) = ids_unpadded_size;\n+  ids_unpadded_size_tensor->flat<int32_t>()(0) = ids_unpadded_size;\n }\n \n #ifdef LIBTPU_ON_GCE\n@@ -778,7 +779,7 @@ void GetMinibatchSplitsWithPhysicalReplicaOp::Compute(OpKernelContext* ctx) {\n   OP_REQUIRES_OK(ctx, ctx->input(\"program_key\", &program_key_t));\n   tstring program_key = program_key_t->vec<tstring>()(0);\n \n-  int32 per_sc_sample_count = sample_count_ / num_sc_per_chip_;\n+  int32_t per_sc_sample_count = sample_count_ / num_sc_per_chip_;\n \n   int64_t max_ids_per_partition = -1;\n   int64_t max_unique_ids_per_partition = -1;\n@@ -802,10 +803,10 @@ void GetMinibatchSplitsWithPhysicalReplicaOp::Compute(OpKernelContext* ctx) {\n   const Tensor* gains;\n   OP_REQUIRES_OK(ctx, ctx->input(\"gains\", &gains));\n \n-  const int32 total_id_count = row_ids->NumElements();\n+  const int32_t total_id_count = row_ids->NumElements();\n \n-  const int32* row_ids_ptr = row_ids->flat<int32>().data();\n-  const int32* col_ids_ptr = col_ids->flat<int32>().data();\n+  const int32_t* row_ids_ptr = row_ids->flat<int32_t>().data();\n+  const int32_t* col_ids_ptr = col_ids->flat<int32_t>().data();\n   const float* gains_ptr = gains->flat<float>().data();\n \n #ifndef NDEBUG\n@@ -829,7 +830,7 @@ void GetMinibatchSplitsWithPhysicalReplicaOp::Compute(OpKernelContext* ctx) {\n \n   const int max_division_level = GetMinibatchMaxDivisionLevel();\n \n-  const int32 kMaxDivisions = 1 << max_division_level;\n+  const int32_t kMaxDivisions = 1 << max_division_level;\n \n   // The id counts tensor is the running sum of the number of ids for all\n   // buckets for all the replicas on each SparseCore.\n@@ -842,7 +843,7 @@ void GetMinibatchSplitsWithPhysicalReplicaOp::Compute(OpKernelContext* ctx) {\n           TensorShape(\n               {kMaxDivisions * num_sc_per_chip_ * num_physical_replica + 1}),\n           &id_counts_tensor));\n-  int32* id_counts_tensor_ptr = id_counts_tensor->flat<int32>().data();\n+  int32_t* id_counts_tensor_ptr = id_counts_tensor->flat<int32_t>().data();\n   *id_counts_tensor_ptr = 0;\n \n   const int32_t division_size =\n@@ -855,8 +856,8 @@ void GetMinibatchSplitsWithPhysicalReplicaOp::Compute(OpKernelContext* ctx) {\n   //                0001011 -> 0001 01 1\n   //      which mean split at level 0 section 0, level 1 section 0 and level\n   //      2 section 0. the split points are [128, 256, 512].\n-  int64 pre_merge_splits = 0;\n-  int64 after_merge_splits = 0;\n+  int64_t pre_merge_splits = 0;\n+  int64_t after_merge_splits = 0;\n   // Vector of uint64_t storing the col ids in the upper 32 bit and the index\n   // to the original id array in the lower 32 bit.\n   std::vector<std::vector<uint64_t>> col_ids_index_list(\n@@ -926,7 +927,7 @@ void GetMinibatchSplitsWithPhysicalReplicaOp::Compute(OpKernelContext* ctx) {\n     int32_t previous_row_id = -1;\n     uint32_t previous_id_array_index = 0;\n     for (uint64_t item : col_ids_index_list[sc_id]) {\n-      int32 col_id = item >> 32;\n+      int32_t col_id = item >> 32;\n       uint32_t id_array_index = item & 0xffffffff;\n       int32_t row_id = *(row_ids_ptr + id_array_index);\n       // If the row ids and col ids are both same as the previous one,\n@@ -1027,9 +1028,9 @@ void GetMinibatchSplitsWithPhysicalReplicaOp::Compute(OpKernelContext* ctx) {\n           if (level > 0 && (pre_merge_splits &\n                             (1LL << (pre_start_bit_pos + (section >> 1)))) == 0)\n             continue;\n-          int32 id_count = id_counter[(section + 1) * section_size] -\n-                           id_counter[section * section_size];\n-          int32 unique_id_count =\n+          int32_t id_count = id_counter[(section + 1) * section_size] -\n+                             id_counter[section * section_size];\n+          int32_t unique_id_count =\n               unique_id_counter[(section + 1) * section_size] -\n               unique_id_counter[section * section_size];\n           // If the number of ids or unique ids exceeds the limit, We need to\n@@ -1155,17 +1156,17 @@ void GetMinibatchSplitsWithPhysicalReplicaOp::Compute(OpKernelContext* ctx) {\n   Tensor* splits_tensor;\n   OP_REQUIRES_OK(\n       ctx, ctx->allocate_output(\"splits\", TensorShape({}), &splits_tensor));\n-  splits_tensor->flat<int64>()(0) = after_merge_splits;\n+  splits_tensor->flat<int64_t>()(0) = after_merge_splits;\n \n   Tensor* max_ids_tensor;\n   OP_REQUIRES_OK(\n       ctx, ctx->allocate_output(\"max_ids\", TensorShape({}), &max_ids_tensor));\n-  max_ids_tensor->flat<int32>()(0) = this_max_ids;\n+  max_ids_tensor->flat<int32_t>()(0) = this_max_ids;\n \n   Tensor* max_uniques_tensor;\n   OP_REQUIRES_OK(ctx, ctx->allocate_output(\"max_uniques\", TensorShape({}),\n                                            &max_uniques_tensor));\n-  max_uniques_tensor->flat<int32>()(0) = this_max_uniques;\n+  max_uniques_tensor->flat<int32_t>()(0) = this_max_uniques;\n }\n \n #ifdef LIBTPU_ON_GCE\n@@ -1197,12 +1198,12 @@ void StoreMinibatchStatisticsInFdoOp::Compute(OpKernelContext* ctx) {\n \n   const Tensor* max_ids_t;\n   OP_REQUIRES_OK(ctx, ctx->input(\"max_ids\", &max_ids_t));\n-  int64_t max_ids = max_ids_t->scalar<int64>()();\n+  int64_t max_ids = max_ids_t->scalar<int64_t>()();\n   const Tensor* max_uniques_t;\n   OP_REQUIRES_OK(ctx, ctx->input(\"max_uniques\", &max_uniques_t));\n-  int64_t max_uniques = max_uniques_t->scalar<int64>()();\n+  int64_t max_uniques = max_uniques_t->scalar<int64_t>()();\n \n-  int32 per_sc_sample_count = sample_count_ / num_sc_per_chip_;\n+  int32_t per_sc_sample_count = sample_count_ / num_sc_per_chip_;\n \n   int64_t max_ids_per_partition = -1;\n   int64_t max_unique_ids_per_partition = -1;\n@@ -1264,10 +1265,10 @@ void ConvertToListOfSparseCoreCooTensorsOp::Compute(OpKernelContext* ctx) {\n   OP_REQUIRES_OK(ctx, ValidateInputs(*indices_or_row_splits, *values, *weights,\n                                      sample_count_));\n \n-  const int32 total_id_count = values->NumElements();\n+  const int32_t total_id_count = values->NumElements();\n \n-  auto row_ids_before_dedup = std::unique_ptr<int32[]>(\n-      new std::remove_extent_t<int32[]>[total_id_count]);\n+  auto row_ids_before_dedup = std::unique_ptr<int32_t[]>(\n+      new std::remove_extent_t<int32_t[]>[total_id_count]);\n \n   OP_REQUIRES_OK(ctx, ComputeRowIdsBeforePadding(*indices_or_row_splits,\n                                                  total_id_count, sample_count_,\n@@ -1285,14 +1286,14 @@ void ConvertToListOfSparseCoreCooTensorsOp::Compute(OpKernelContext* ctx) {\n   auto combiner_scale_transform_fn =\n       GetCombinerScaleTransformFunction(combiner_);\n \n-  const int32* row_ids_before_dedup_ptr = row_ids_before_dedup.get();\n-  const int32* values_ptr = values->flat<int32>().data();\n+  const int32_t* row_ids_before_dedup_ptr = row_ids_before_dedup.get();\n+  const int32_t* values_ptr = values->flat<int32_t>().data();\n   const float* weights_ptr = weights->flat<float>().data();\n \n   // Dedup the ids within one sample by just checking the adjacent ids. This\n   // will NOT result in a full deduplication.\n-  std::vector<int32> row_ids;\n-  std::vector<int32> col_ids;\n+  std::vector<int32_t> row_ids;\n+  std::vector<int32_t> col_ids;\n   std::vector<float> gains;\n   row_ids.reserve(total_id_count);\n   col_ids.reserve(total_id_count);\n@@ -1306,8 +1307,8 @@ void ConvertToListOfSparseCoreCooTensorsOp::Compute(OpKernelContext* ctx) {\n     const float gain = *weights_ptr;\n     const float rescaled_gain = combiner_scale_contribution_fn(gain);\n     for (int token_id = 0; token_id < total_id_count; ++token_id) {\n-      const int32 row_id = *(row_ids_before_dedup_ptr + token_id);\n-      const int32 col_id = *(values_ptr + token_id);\n+      const int32_t row_id = *(row_ids_before_dedup_ptr + token_id);\n+      const int32_t col_id = *(values_ptr + token_id);\n       if (gains_rescale.has_value()) {\n         // Compute the gain rescale before doing the dedup.\n         (*gains_rescale)[row_id] += rescaled_gain;\n@@ -1324,8 +1325,8 @@ void ConvertToListOfSparseCoreCooTensorsOp::Compute(OpKernelContext* ctx) {\n     }\n   } else {\n     for (int token_id = 0; token_id < total_id_count; ++token_id) {\n-      const int32 row_id = *(row_ids_before_dedup_ptr + token_id);\n-      const int32 col_id = *(values_ptr + token_id);\n+      const int32_t row_id = *(row_ids_before_dedup_ptr + token_id);\n+      const int32_t col_id = *(values_ptr + token_id);\n       const float gain = *(weights_ptr + token_id);\n       if (gains_rescale.has_value()) {\n         // Compute the gain rescale before doing the dedup.\n@@ -1371,8 +1372,8 @@ void ConvertToListOfSparseCoreCooTensorsOp::Compute(OpKernelContext* ctx) {\n         ctx, col_ids_output_list.allocate(\n                  i, TensorShape({per_sc_token_count[i]}), &col_ids_tensor));\n \n-    int32* row_ids_tensor_ptr = row_ids_tensor->flat<int32>().data();\n-    int32* col_ids_tensor_ptr = col_ids_tensor->flat<int32>().data();\n+    int32_t* row_ids_tensor_ptr = row_ids_tensor->flat<int32_t>().data();\n+    int32_t* col_ids_tensor_ptr = col_ids_tensor->flat<int32_t>().data();\n     float* gains_tensor_ptr = gains_tensor->flat<float>().data();\n \n     WriteToOutputTensor(\n@@ -1384,10 +1385,10 @@ void ConvertToListOfSparseCoreCooTensorsOp::Compute(OpKernelContext* ctx) {\n }\n \n void ConvertToListOfSparseCoreCooTensorsOp::WriteToOutputTensor(\n-    int32* row_ids, int32* col_ids, float* gains, int32* row_ids_tensor_ptr,\n-    int32* col_ids_tensor_ptr, float* gains_tensor_ptr, int32_t begin_index,\n-    int32_t end_index, int32_t sc_id,\n-    std::optional<std::vector<float>> gains_rescale) {\n+    int32_t* row_ids, int32_t* col_ids, float* gains,\n+    int32_t* row_ids_tensor_ptr, int32_t* col_ids_tensor_ptr,\n+    float* gains_tensor_ptr, int32_t begin_index, int32_t end_index,\n+    int32_t sc_id, std::optional<std::vector<float>> gains_rescale) {\n   tsl::profiler::TraceMe traceme(\n       \"ConvertToListOfSparseCoreCooTensorsOp::WriteToOutputTensor\");\n   if (gains_rescale.has_value()) {\n@@ -1407,12 +1408,13 @@ void ConvertToListOfSparseCoreCooTensorsOp::WriteToOutputTensor(\n     }\n   } else {\n     std::transform(row_ids + begin_index, row_ids + end_index,\n-                   row_ids_tensor_ptr, [this, &sc_id](int32 row_id) -> int32 {\n+                   row_ids_tensor_ptr,\n+                   [this, &sc_id](int32_t row_id) -> int32_t {\n                      return row_id % per_sc_sample_count_ + per_sc_row_offset_ +\n                             per_sc_stacked_table_sample_count_ * sc_id;\n                    });\n     std::transform(col_ids + begin_index, col_ids + end_index,\n-                   col_ids_tensor_ptr, [this](int32 col_id) -> int32 {\n+                   col_ids_tensor_ptr, [this](int32_t col_id) -> int32_t {\n                      return ((col_id + col_shift_) & num_sc_shards_bit_mod_) +\n                             (col_id & num_sc_shards_bit_mod_inv_) + col_offset_;\n                    });\n@@ -1804,7 +1806,7 @@ void ConvertToSparseCoreCsrWrappedCooTensorOp::Compute(OpKernelContext* ctx) {\n         }\n \n         *(row_pointers_tensor_ptr + row_pointers_index) = global_index;\n-        int32 num_ids_to_pad_per_replica =\n+        int32_t num_ids_to_pad_per_replica =\n             xla::RoundUpTo<int32_t>(global_index, xla_pad_size) - global_index;\n \n         std::fill_n(sorted_token_ids_tensor_ptr + global_index,\n@@ -1818,8 +1820,8 @@ void ConvertToSparseCoreCsrWrappedCooTensorOp::Compute(OpKernelContext* ctx) {\n         ++row_pointers_index;\n       }\n       // Pad the row_pointers to be memory aligned.\n-      int32 num_row_pointers_to_pad =\n-          xla::RoundUpTo<int32>(row_pointers_index, xla_pad_size) -\n+      int32_t num_row_pointers_to_pad =\n+          xla::RoundUpTo<int32_t>(row_pointers_index, xla_pad_size) -\n           row_pointers_index;\n       std::fill_n(row_pointers_tensor_ptr + row_pointers_index,\n                   num_row_pointers_to_pad, global_index);\n@@ -1838,7 +1840,7 @@ void ConvertToSparseCoreCsrWrappedCooTensorOp::Compute(OpKernelContext* ctx) {\n                  << \" . This could potentially impact the model quality.\";\n   }\n \n-  int32 row_pointers_unpadded_size =\n+  int32_t row_pointers_unpadded_size =\n       total_num_minibatch * padded_row_pointers_size_per_sc;\n \n   Tensor* num_minibatches_per_sc_tensor;\n@@ -1855,10 +1857,10 @@ void ConvertToSparseCoreCsrWrappedCooTensorOp::Compute(OpKernelContext* ctx) {\n   OP_REQUIRES_OK(ctx, ctx->allocate_output(\"ids_unpadded_size\", TensorShape({}),\n                                            &ids_unpadded_size_tensor));\n \n-  num_minibatches_per_sc_tensor->flat<int32>()(0) = num_minibatch_per_sc;\n-  row_pointers_unpadded_size_tensor->flat<int32>()(0) =\n+  num_minibatches_per_sc_tensor->flat<int32_t>()(0) = num_minibatch_per_sc;\n+  row_pointers_unpadded_size_tensor->flat<int32_t>()(0) =\n       row_pointers_unpadded_size;\n-  ids_unpadded_size_tensor->flat<int32>()(0) = ids_unpadded_size;\n+  ids_unpadded_size_tensor->flat<int32_t>()(0) = ids_unpadded_size;\n }\n \n REGISTER_KERNEL_BUILDER("
        },
        {
            "sha": "706622ae1dfbe4c42ae776c3c5bd1c16e06844c8",
            "filename": "tensorflow/core/tpu/kernels/sparse_core_preprocess_ops.h",
            "status": "modified",
            "additions": 14,
            "deletions": 13,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fsparse_core_preprocess_ops.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fsparse_core_preprocess_ops.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fsparse_core_preprocess_ops.h?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -34,15 +34,15 @@ namespace tensorflow {\n // Struct to describe an embedding lookup input data.\n struct EmbeddingLookupInput {\n   // Which replica it belongs.\n-  int32 replica_id;\n+  int32_t replica_id;\n   // Token id.\n-  int32 token_id;\n+  int32_t token_id;\n   // Sample id.\n-  int32 sample_id;\n+  int32_t sample_id;\n   // Gain.\n   float gain;\n \n-  EmbeddingLookupInput(int32 replica_id, int32 token_id, int32 sample_id,\n+  EmbeddingLookupInput(int32_t replica_id, int32_t token_id, int32_t sample_id,\n                        float gain)\n       : replica_id(replica_id),\n         token_id(token_id),\n@@ -56,9 +56,9 @@ absl::Status ValidateInputs(const Tensor& indices_or_row_splits,\n \n // Compute the row id list before padding.\n absl::Status ComputeRowIdsBeforePadding(const Tensor& indices_or_row_splits,\n-                                        int32 total_id_count,\n-                                        int32 sample_count,\n-                                        int32* row_ids_before_padding,\n+                                        int32_t total_id_count,\n+                                        int32_t sample_count,\n+                                        int32_t* row_ids_before_padding,\n                                         std::vector<int> shape_strides = {});\n \n class GetMinibatchesInCsrWithPhysicalReplicaOp : public OpKernel {\n@@ -101,7 +101,7 @@ class GetMinibatchSplitsWithPhysicalReplicaOp : public OpKernel {\n   void Compute(OpKernelContext* ctx) override;\n \n  protected:\n-  virtual void CalculateHeadroom(int32 this_max_ids, int32 this_max_uniques,\n+  virtual void CalculateHeadroom(int32_t this_max_ids, int32_t this_max_uniques,\n                                  tstring program_key,\n                                  int64_t max_ids_per_partition,\n                                  int64_t max_unique_ids_per_partition,\n@@ -138,7 +138,7 @@ class StoreMinibatchStatisticsInFdoOp : public OpKernel {\n   void Compute(OpKernelContext* ctx) override;\n \n  protected:\n-  virtual void CalculateHeadroom(int32 this_max_ids, int32 this_max_uniques,\n+  virtual void CalculateHeadroom(int32_t this_max_ids, int32_t this_max_uniques,\n                                  tstring program_key,\n                                  int64_t max_ids_per_partition,\n                                  int64_t max_unique_ids_per_partition) {}\n@@ -165,10 +165,11 @@ class ConvertToListOfSparseCoreCooTensorsOp : public OpKernel {\n   void Compute(OpKernelContext* ctx) override;\n \n  private:\n-  void WriteToOutputTensor(int32* row_ids, int32* col_ids, float* gains,\n-                           int32* row_ids_tensor_ptr, int32* col_ids_tensor_ptr,\n-                           float* gains_tensor_ptr, int32_t begin_index,\n-                           int32_t end_index, int32_t sc_id,\n+  void WriteToOutputTensor(int32_t* row_ids, int32_t* col_ids, float* gains,\n+                           int32_t* row_ids_tensor_ptr,\n+                           int32_t* col_ids_tensor_ptr, float* gains_tensor_ptr,\n+                           int32_t begin_index, int32_t end_index,\n+                           int32_t sc_id,\n                            std::optional<std::vector<float>> gains_rescale);\n   int sample_count_;\n   int num_sc_per_chip_;"
        },
        {
            "sha": "f3576628d048bc2f60e5e31cefa8bbb3a8873b23",
            "filename": "tensorflow/core/tpu/kernels/sparse_core_xla_ops.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fsparse_core_xla_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fsparse_core_xla_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fsparse_core_xla_ops.cc?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -143,7 +143,7 @@ class XlaSparseDenseMatmulOp : public XlaOpKernel {\n   void Compile(XlaOpKernelContext* ctx) override {\n     xla::XlaBuilder* builder = ctx->builder();\n \n-    const int32 num_physical_replica =\n+    const int32_t num_physical_replica =\n         stream_executor::tpu::OpsApiFn()->TpuTopology_AvailableCoreCountFn(\n             /*mesh_state=*/nullptr,\n             /*tpu_core_type=*/TpuCoreTypeEnum::kEmbeddingV2);\n@@ -662,7 +662,7 @@ class XlaSparseDenseMatmulGradWithCsrInputBase : public XlaOpKernel {\n                 errors::InvalidArgument(\n                     \"activations input has non static or non-rank 2 shape: \",\n                     activation_shape.ToString()));\n-    int64 num_samples_per_chip = activation_shape.dimensions(0);\n+    int64_t num_samples_per_chip = activation_shape.dimensions(0);\n     OP_REQUIRES(ctx, num_samples_per_chip % num_sparsecores_per_device_ == 0,\n                 errors::InvalidArgument(\n                     \"num_samples_per_chip \", num_samples_per_chip,"
        },
        {
            "sha": "22d18e392201462696147d66a17528c9dd5cb18d",
            "filename": "tensorflow/core/tpu/kernels/topk_ops.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftopk_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftopk_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftopk_ops.cc?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -51,21 +51,21 @@ xla::XlaOp CreateKthOrderStatisticComputation(xla::XlaBuilder* builder,\n   const int64_t width = input_shape.dim_size(1);\n \n   xla::XlaOp input_sm32 = xla::BitcastConvertType(input, xla::S32);\n-  xla::XlaOp zero_r0 = xla::ConstantR0<int32>(builder, 0);\n+  xla::XlaOp zero_r0 = xla::ConstantR0<int32_t>(builder, 0);\n   xla::XlaOp zero_r1 = xla::Broadcast(zero_r0, {height});\n   xla::XlaOp zero_r2 = xla::Broadcast(zero_r0, {height, width});\n \n-  xla::XlaOp max_r0 = xla::ConstantR0<int32>(builder, 0x7FFFFFFF);\n+  xla::XlaOp max_r0 = xla::ConstantR0<int32_t>(builder, 0x7FFFFFFF);\n   xla::XlaOp max_r1 = xla::Broadcast(max_r0, {height});\n \n   // Start at positive zero, so that pivot is always less than top.\n-  xla::XlaOp negative_zero_r0 = xla::ConstantR0<int32>(builder, 0x80000000);\n+  xla::XlaOp negative_zero_r0 = xla::ConstantR0<int32_t>(builder, 0x80000000);\n   xla::XlaOp negative_zero_r1 = xla::Broadcast(negative_zero_r0, {height});\n   xla::XlaOp top_r1 = zero_r1;\n \n-  for (uint32 mask = 1U << 31; mask; mask >>= 1) {\n+  for (uint32_t mask = 1U << 31; mask; mask >>= 1) {\n     xla::XlaOp broadcast_mask_r1 =\n-        xla::Broadcast(xla::ConstantR0<int32>(builder, mask), {height});\n+        xla::Broadcast(xla::ConstantR0<int32_t>(builder, mask), {height});\n \n     // The first iteration of the loop determines if the kth element\n     // is positive or negative. If the kth element is negative, we\n@@ -111,14 +111,14 @@ class KthOrderStatistic : public XlaOpKernel {\n         ctx, input_shape.dims() == 2,\n         InvalidArgument(\"input must be rank-2: \", input_shape.DebugString()));\n \n-    xla::XlaOp k = xla::ConstantR0<int32>(builder, k_);\n+    xla::XlaOp k = xla::ConstantR0<int32_t>(builder, k_);\n     xla::XlaOp kth_order_statistics =\n         CreateKthOrderStatisticComputation(builder, input_shape, input, k);\n     ctx->SetOutput(0, kth_order_statistics);\n   }\n \n  private:\n-  int32 k_;\n+  int32_t k_;\n };\n \n REGISTER_XLA_OP(Name(\"KthOrderStatistic\"), KthOrderStatistic);\n@@ -269,21 +269,21 @@ xla::XlaOp CreateMakeUnique(xla::XlaBuilder* builder, const xla::XlaOp input,\n   // count_mask is used to mask away the low order bits to ensure\n   // that every element is distinct.\n   uint32_t next_power_of_two = absl::bit_ceil<uint64_t>(width);\n-  uint32 count_mask = ~(next_power_of_two - 1);\n+  uint32_t count_mask = ~(next_power_of_two - 1);\n   xla::XlaOp count_mask_r0 = xla::ConstantR0(builder, count_mask);\n   xla::XlaOp count_mask_r2 = xla::Broadcast(count_mask_r0, {height, width});\n \n   // smallest_normal is the bit representation of the smallest\n   // positive normal floating point number. The sign is zero,\n   // exponent is one, and the fraction is zero.\n-  uint32 smallest_normal = 1U << 23;\n+  uint32_t smallest_normal = 1U << 23;\n   xla::XlaOp smallest_normal_r0 = xla::ConstantR0(builder, smallest_normal);\n   xla::XlaOp smallest_normal_r2 =\n       xla::Broadcast(smallest_normal_r0, {height, width});\n \n   // Used to mask away the sign bit when computing the absolute\n   // value.\n-  uint32 low_bit_mask = ~(1U << 31);\n+  uint32_t low_bit_mask = ~(1U << 31);\n   xla::XlaOp low_bit_mask_r0 = xla::ConstantR0(builder, low_bit_mask);\n   xla::XlaOp low_bit_mask_r2 = xla::Broadcast(low_bit_mask_r0, {height, width});\n "
        },
        {
            "sha": "06fde06bdcac84c09fc5168f8b816044f7c14450",
            "filename": "tensorflow/core/tpu/kernels/tpu_compilation_cache_rpc_lookup.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_compilation_cache_rpc_lookup.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_compilation_cache_rpc_lookup.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_compilation_cache_rpc_lookup.h?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -37,23 +37,23 @@ class TpuCompilationCacheRpcLookup : public TpuCompilationCacheLookup {\n  public:\n   using StubType = tpu::grpc::TpuCompilationCacheService::Stub;\n \n-  TpuCompilationCacheRpcLookup(const string& server_address,\n+  TpuCompilationCacheRpcLookup(const std::string& server_address,\n                                int64_t max_cache_size);\n   ~TpuCompilationCacheRpcLookup() override = default;\n \n-  absl::Status Lookup(const string& proto_key,\n+  absl::Status Lookup(const std::string& proto_key,\n                       std::unique_ptr<tpu::CompilationCacheEntryRef>* entry,\n                       tpu::CompilationCacheFetchTarget fetch_target) override;\n \n   absl::Status Lookup(int64_t uid, int proto_index,\n                       std::unique_ptr<tpu::CompilationCacheEntryRef>* entry,\n                       tpu::CompilationCacheFetchTarget fetch_target) override;\n \n-  string DebugString() const override;\n+  std::string DebugString() const override;\n \n  private:\n   // Helper method to make the RPC request to the central cache.\n-  absl::Status RemoteLookupLocked(const string& local_proto_key,\n+  absl::Status RemoteLookupLocked(const std::string& local_proto_key,\n                                   const tpu::GetTpuProgramRequest& request,\n                                   std::shared_ptr<CacheEntry>* cache_entry)\n       ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_);"
        },
        {
            "sha": "4f7af33e8c1c35c7e8d942e473a4b707aebc0d62",
            "filename": "tensorflow/core/tpu/kernels/tpu_compile_op_common.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_compile_op_common.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_compile_op_common.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_compile_op_common.cc?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -167,7 +167,7 @@ void TpuCompileOpKernelCommon::Compute(OpKernelContext* ctx) {\n   });\n \n   absl::Status compile_status = ComputeInternal(ctx);\n-  string status_payload;\n+  std::string status_payload;\n   // Construct payload if compile_status is not ok and there's no payload for\n   // compilation yet.\n   if (!compile_status.ok() &&"
        },
        {
            "sha": "56e2130495750ccfd48eb72fb6daed4a2ef8f346",
            "filename": "tensorflow/core/tpu/kernels/tpu_compile_op_common.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_compile_op_common.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_compile_op_common.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_compile_op_common.h?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -178,7 +178,7 @@ class TpuCompileOpKernelCommon {\n   std::string mlir_module_;\n   // Fingerprint of the MLIR Module created once on construction to avoid paying\n   // the cost on each invocation.\n-  uint64 mlir_module_fingerprint_ = 0;\n+  uint64_t mlir_module_fingerprint_ = 0;\n \n   // Number of different programs to compile. This maps to number of cores in\n   // each replica.\n@@ -198,7 +198,7 @@ class TpuCompileOpKernelCommon {\n \n   absl::Status RegisterXLAFingerprints(\n       const std::vector<TensorShape>& arg_shapes,\n-      TpuProgramGroupInterface* tpu_program_group, uint64 fingerprint);\n+      TpuProgramGroupInterface* tpu_program_group, uint64_t fingerprint);\n };\n \n }  // namespace tpu"
        },
        {
            "sha": "a6bf93239dc3d4b627cd8fbe1a42eb04f122ab0e",
            "filename": "tensorflow/core/tpu/kernels/tpu_embedding_engine_state_interface.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_embedding_engine_state_interface.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_embedding_engine_state_interface.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_embedding_engine_state_interface.h?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -61,7 +61,7 @@ class TpuEmbeddingEngineStateInterface : public ResourceBase {\n     return new TpuEmbeddingEngineStateInterface(state);\n   }\n \n-  string DebugString() const override {\n+  std::string DebugString() const override {\n     return \"TpuEmbeddingEngineStateInterface\";\n   }\n "
        },
        {
            "sha": "46981718facdb4b5e289dd2580ff6d14612e1047",
            "filename": "tensorflow/core/tpu/kernels/tpu_embedding_enqueue_ops.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_embedding_enqueue_ops.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_embedding_enqueue_ops.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_embedding_enqueue_ops.h?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -31,7 +31,8 @@ absl::Status ValidateCombiners(absl::Span<const std::string> combiners);\n // Validates the `mode_override` input of the TPUEnqueue* ops, and, if correct,\n // sets the `mode` to pass on to the TPU Embedding manager.\n absl::Status GetValidatedModeOverride(\n-    const string& mode_override, tpu::TPUEmbeddingConfiguration::Mode* mode);\n+    const std::string& mode_override,\n+    tpu::TPUEmbeddingConfiguration::Mode* mode);\n }  // namespace tensorflow\n \n #endif  // TENSORFLOW_CORE_TPU_KERNELS_TPU_EMBEDDING_ENQUEUE_OPS_H_"
        },
        {
            "sha": "45c5fb52e1d9c9dc73a7054f2ba268a2db732904",
            "filename": "tensorflow/core/tpu/kernels/tpu_functional_ops.h",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_functional_ops.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_functional_ops.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_functional_ops.h?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -84,18 +84,19 @@ GroupedEdges GroupTensorsForOutputPacking(Graph* graph,\n                                           GraphShapeInfo* shape_info);\n \n absl::Status CreateConcatAndSplitNodesForInputTensor(\n-    Graph* graph, const string& cluster_name, EdgeShapes* tpu_input_shapes,\n+    Graph* graph, const std::string& cluster_name, EdgeShapes* tpu_input_shapes,\n     const absl::flat_hash_map<std::string, std::vector<const Edge*>>&\n         grouped_input_edges,\n     int32_t minimum_input_tensors_packing, bool xla_spmd_input_sharded,\n     const XlaShardingInfoMap& xla_sharding_info,\n     const TpuReplicatedInputInfoMap& tpu_replicated_input_info);\n absl::Status CreateConcatAndSplitNodesForOutputTensor(\n-    Graph* graph, const string& cluster_name, EdgeShapes* tpu_output_shapes,\n-    GraphShapeInfo* tpu_inferred_info, GroupedEdges shape_to_output,\n-    int32_t minimum_output_tensors_packing);\n+    Graph* graph, const std::string& cluster_name,\n+    EdgeShapes* tpu_output_shapes, GraphShapeInfo* tpu_inferred_info,\n+    GroupedEdges shape_to_output, int32_t minimum_output_tensors_packing);\n \n-absl::Status InsertReshapeNodePairs(Graph* graph, const string& cluster_name,\n+absl::Status InsertReshapeNodePairs(Graph* graph,\n+                                    const std::string& cluster_name,\n                                     EdgeShapes* tpu_input_shapes,\n                                     int num_cores_per_replica);\n \n@@ -172,7 +173,7 @@ class TPUPartitionedCallOp : public AsyncOpKernel {\n   };\n \n   // This method is thread-safe.\n-  absl::Status GetTpuCoreOrdinal(OpKernelContext* ctx, uint64 input_hash,\n+  absl::Status GetTpuCoreOrdinal(OpKernelContext* ctx, uint64_t input_hash,\n                                  int64_t* ordinal_selector_req_id,\n                                  int32_t* core_ordinal);\n \n@@ -196,11 +197,10 @@ class TPUPartitionedCallOp : public AsyncOpKernel {\n   // device_ordinal: The index of the TPU core that is scheduled to run\n   //   the computation. In the case of XLA SPMD, it is the \"primary\" core, which\n   //   is the smallest index of all the cores.\n-  absl::Status InitializeShardedVarOnTPU(OpKernelContext* ctx,\n-                                         const core::RefCountPtr<Var>& var,\n-                                         std::vector<NodeDef>& ndefs,\n-                                         int split_dim,\n-                                         const std::vector<string>& tpu_devices)\n+  absl::Status InitializeShardedVarOnTPU(\n+      OpKernelContext* ctx, const core::RefCountPtr<Var>& var,\n+      std::vector<NodeDef>& ndefs, int split_dim,\n+      const std::vector<std::string>& tpu_devices)\n       ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n \n   // Check if any of the immediate successors of node has attribute\n@@ -250,7 +250,7 @@ class TPUPartitionedCallOp : public AsyncOpKernel {\n   absl::Status PlacementHelper(\n       const DeviceSet& device_set,\n       const GraphOptimizationPassOptions& optimization_options,\n-      const string& function_name);\n+      const std::string& function_name);\n   // Partitions `graph`, populates `subgraphs` with the partitions, and runs\n   // the post-partitioning graph optimization passes.\n   absl::Status PartitionHelper(\n@@ -263,15 +263,15 @@ class TPUPartitionedCallOp : public AsyncOpKernel {\n   // If `out_flib_def` is not null, it will be set to a copy of `flib_def_` and\n   // used for instantiation.\n   absl::Status InstantiatePartition(\n-      const Graph& graph, const string& function_name,\n-      const string& target_device, FHandle* handle,\n+      const Graph& graph, const std::string& function_name,\n+      const std::string& target_device, FHandle* handle,\n       std::unique_ptr<FunctionLibraryDefinition>* out_flib_def)\n       ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n   // Adds and instantiates functions for each subgraph in `subgraphs` after\n   // rewriting nodes' `device_ordinal` attributes to match `replica_id` when\n   // num_cores_per_replica == 1.\n   absl::Status InstantiateFunctionsFromSubgraphs(\n-      const DeviceSet& device_set, int replica_id, uint64 cache_hash,\n+      const DeviceSet& device_set, int replica_id, uint64_t cache_hash,\n       int num_cores_per_replica,\n       std::unordered_map<std::string, std::unique_ptr<Graph>> subgraphs)\n       ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n@@ -344,7 +344,7 @@ class TPUPartitionedCallOp : public AsyncOpKernel {\n   const std::string local_device_name_;\n   // Maps from cache key to their corresponding functions, which are\n   // represented as (device, handle) pairs.\n-  gtl::FlatMap<uint64, std::vector<DeviceAndFHandle>> partition_cache_\n+  gtl::FlatMap<uint64_t, std::vector<DeviceAndFHandle>> partition_cache_\n       ABSL_GUARDED_BY(mu_);\n \n   // A set contains seen ordinals. Used by variable initialization on TPU.\n@@ -362,7 +362,7 @@ class TPUPartitionedCallOp : public AsyncOpKernel {\n   FunctionLibraryRuntime* library_runtime_;\n \n   // Used to uniquify function names in `flib_def_`.\n-  uint32 suffix_ = 0;\n+  uint32_t suffix_ = 0;\n \n   // Minimum number of run steps (batches) necessary to trigger xla autotuner.\n   int autotuner_thresh_ = 0;\n@@ -371,7 +371,7 @@ class TPUPartitionedCallOp : public AsyncOpKernel {\n   std::shared_ptr<tpu::TPUOrdinalSelector> ordinal_selector_;\n \n   // Maps input hash to TF fingerprint.\n-  absl::flat_hash_map<uint64, uint64> inputs_to_fingerprint_;\n+  absl::flat_hash_map<uint64_t, uint64_t> inputs_to_fingerprint_;\n \n   // List of TPU devices\n   std::vector<Device*> tpu_devices_;"
        },
        {
            "sha": "1d50e75bb804b318c6dcf4c4a69e0dbb9fe60007",
            "filename": "tensorflow/core/tpu/kernels/tpu_mesh_state_interface.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_mesh_state_interface.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_mesh_state_interface.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_mesh_state_interface.h?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -75,7 +75,7 @@ class TpuMeshStateInterface : public tensorflow::ResourceBase {\n                mesh_state_, tpu_core_type);\n   }\n \n-  string DebugString() const override { return \"TpuMeshStateInterface\"; }\n+  std::string DebugString() const override { return \"TpuMeshStateInterface\"; }\n \n  private:\n   XLA_TpuMeshState* mesh_state_;"
        },
        {
            "sha": "6da81d1ffefabe6d3c13c0b55be5488a03cc5761",
            "filename": "tensorflow/core/tpu/kernels/tpu_op_util.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_op_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_op_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_op_util.cc?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -73,8 +73,8 @@ std::string CreateConfigPrefix(const TPUCompileMetadataProto& metadata) {\n }\n }  // namespace\n \n-uint64 CreateFingerprintWithNameAndShapes(\n-    uint64 name, const std::vector<tensorflow::TensorShape>& shapes) {\n+uint64_t CreateFingerprintWithNameAndShapes(\n+    uint64_t name, const std::vector<tensorflow::TensorShape>& shapes) {\n   std::string shape_prefix = CreateShapePrefix(shapes);\n   VLOG(2) << \"CreateFingerprintWithNameAndShapes, name: \" << name\n           << \", shape_prefix: \" << shape_prefix;\n@@ -85,7 +85,7 @@ uint64 CreateFingerprintWithNameAndShapes(\n // Return fingerprint_in_metadata if it's not empty; otherwise read input tensor\n // data to compute the fingerprint.\n std::string GuaranteedConstFingerprint(\n-    const string& fingerprint_in_metadata,\n+    const std::string& fingerprint_in_metadata,\n     const OpInputList& guaranteed_constants) {\n   if (fingerprint_in_metadata.empty()) {\n     uint64_t fingerprint = 0;\n@@ -104,8 +104,8 @@ std::string GuaranteedConstFingerprint(\n // The `guaranteed_constants` must be passed as reference due to the lazy\n // evaluation of `guaranteed_const_fingerprint()` callback.\n TpuCompilationCacheKey CreateCompilationCacheKey(\n-    absl::string_view function_name, uint64 function_library_fingerprint,\n-    uint64 mlir_module_fingerprint, const OpInputList& guaranteed_constants,\n+    absl::string_view function_name, uint64_t function_library_fingerprint,\n+    uint64_t mlir_module_fingerprint, const OpInputList& guaranteed_constants,\n     const std::vector<TensorShape>& dynamic_shapes,\n     const TPUCompileMetadataProto& metadata,\n     const TpuMeshStateInterface& mesh_state, uint64_t session_id,\n@@ -151,7 +151,7 @@ TpuCompilationCacheKey CreateCompilationCacheKey(\n     // reference based on the assumption that these variables lifetime is\n     // managed through the `TPUCompileOpKernelImpl` that outlives the\n     // lifetime of the compilation cache lookups.\n-    string fingerprint;\n+    std::string fingerprint;\n     key.guaranteed_const_fingerprint = [&metadata, &guaranteed_constants,\n                                         fingerprint]() mutable {\n       if (fingerprint.empty()) {"
        },
        {
            "sha": "df68fdaaff39e5b2a7e142469ad999a584ecf6e6",
            "filename": "tensorflow/core/tpu/kernels/tpu_op_util.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_op_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_op_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_op_util.h?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -28,13 +28,13 @@ limitations under the License.\n namespace tensorflow {\n namespace tpu {\n // Creates a fingerprint given the name and the vector of shapes.\n-uint64 CreateFingerprintWithNameAndShapes(\n-    uint64 name, const std::vector<tensorflow::TensorShape>& shapes);\n+uint64_t CreateFingerprintWithNameAndShapes(\n+    uint64_t name, const std::vector<tensorflow::TensorShape>& shapes);\n \n // Creates a unique compilation cache `key`.\n TpuCompilationCacheKey CreateCompilationCacheKey(\n-    absl::string_view function_name, uint64 function_library_fingerprint,\n-    uint64 mlir_module_fingerprint, const OpInputList& guaranteed_constants,\n+    absl::string_view function_name, uint64_t function_library_fingerprint,\n+    uint64_t mlir_module_fingerprint, const OpInputList& guaranteed_constants,\n     const std::vector<TensorShape>& dynamic_shapes,\n     const TPUCompileMetadataProto& metadata,\n     const TpuMeshStateInterface& mesh_state, uint64_t session_id = 0,"
        },
        {
            "sha": "3bf1bfac3fe0bb2eff362072fa5e05b746802c8c",
            "filename": "tensorflow/core/tpu/kernels/tpu_ordinal_selector.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_ordinal_selector.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_ordinal_selector.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_ordinal_selector.h?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -39,7 +39,7 @@ class TPUOrdinalSelector : TPUOrdinalSelectorInterface {\n     stream_executor::tpu::OpsApiFn()->TfTpuOrdinalSelector_DestroyFn(\n         ordinal_selector_);\n   }\n-  int64_t GetOrdinal(std::optional<uint64> key, int64_t* req_id) override {\n+  int64_t GetOrdinal(std::optional<uint64_t> key, int64_t* req_id) override {\n     int64_t ordinal;\n     stream_executor::tpu::OpsApiFn()->TfTpuOrdinalSelector_GetOrdinalFn(\n         ordinal_selector_, key, req_id, &ordinal);"
        },
        {
            "sha": "21ce7b393d6195b41c3244e728bfb15926779de3",
            "filename": "tensorflow/core/tpu/kernels/tpu_ordinal_selector_interface.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_ordinal_selector_interface.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_ordinal_selector_interface.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_ordinal_selector_interface.h?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -26,7 +26,7 @@ namespace tpu {\n class TPUOrdinalSelectorInterface {\n  public:\n   virtual ~TPUOrdinalSelectorInterface() = default;\n-  virtual int64_t GetOrdinal(std::optional<uint64> key, int64_t* req_id) = 0;\n+  virtual int64_t GetOrdinal(std::optional<uint64_t> key, int64_t* req_id) = 0;\n   virtual void DequeueFromCoreSelector(int32_t device_ordinal,\n                                        int64_t req_id) = 0;\n };"
        },
        {
            "sha": "73acdd65ef166c8d133073e6a5db57c429564d91",
            "filename": "tensorflow/core/tpu/kernels/tpu_pod_state.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_pod_state.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_pod_state.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_pod_state.cc?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -123,7 +123,7 @@ TpuPodState::~TpuPodState() {\n   VLOG(1) << \"Shutting down Compilation Cache Service done.\";\n }\n \n-string TpuPodState::DebugString() const {\n+std::string TpuPodState::DebugString() const {\n   return \"Wrapper for distributed TPU state\";\n }\n "
        },
        {
            "sha": "99e2cff3e1f948824784f10d9bd979fb09d489f0",
            "filename": "tensorflow/core/tpu/kernels/tpu_pod_state.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_pod_state.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_pod_state.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_pod_state.h?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -38,7 +38,7 @@ class TpuPodState : public ResourceBase {\n \n   ~TpuPodState() override;\n \n-  string DebugString() const override;\n+  std::string DebugString() const override;\n \n  private:\n   std::unique_ptr<TpuCompilationCacheService> cache_service_;"
        },
        {
            "sha": "25e57e71da8dbf98f52d64150bc909cacf847f29",
            "filename": "tensorflow/core/tpu/kernels/tpu_util.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_util.cc?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -48,7 +48,7 @@ absl::StatusOr<TpuCompilationCacheKey> ParseCompilationCacheKey(\n   TpuCompilationCacheKey parsed_key(splits.at(0));\n   parsed_key.has_guaranteed_const = true;\n   parsed_key.session_handle = splits.at(1);\n-  const string fingerprint = splits.at(2);\n+  const std::string fingerprint = splits.at(2);\n   parsed_key.guaranteed_const_fingerprint = [fingerprint] {\n     return fingerprint;\n   };"
        },
        {
            "sha": "1610d807411cdbd21acb2220c70e6379e226b7e9",
            "filename": "tensorflow/core/tpu/kernels/transfer_ops.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftransfer_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e07a3d2e3832807dde0b867ed7c507d572f04c3/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftransfer_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftransfer_ops.cc?ref=0e07a3d2e3832807dde0b867ed7c507d572f04c3",
            "patch": "@@ -51,7 +51,7 @@ limitations under the License.\n namespace tensorflow {\n \n TpuTransferAsyncOpKernelBase::TpuTransferAsyncOpKernelBase(\n-    OpKernelConstruction* ctx, const string& transfer_type,\n+    OpKernelConstruction* ctx, const std::string& transfer_type,\n     int number_of_threads, std::unique_ptr<TpuTransferOpInterface> transfer_op)\n     : AsyncOpKernel(ctx),\n       transfer_type_(transfer_type),\n@@ -113,7 +113,7 @@ absl::Status TpuTransferAsyncOpKernelBase::RunTransferWithOrdinal(\n }\n \n TpuTransferAsyncOpKernel::TpuTransferAsyncOpKernel(\n-    OpKernelConstruction* ctx, const string& transfer_type,\n+    OpKernelConstruction* ctx, const std::string& transfer_type,\n     int number_of_threads, std::unique_ptr<TpuTransferOpInterface> transfer_op)\n     : TpuTransferAsyncOpKernelBase(ctx, transfer_type, number_of_threads,\n                                    std::move(transfer_op)) {\n@@ -132,15 +132,15 @@ absl::Status TpuTransferAsyncOpKernel::RunTransfer(OpKernelContext* ctx) {\n }\n \n TpuTransferAsyncDynamicOrdinalOpKernel::TpuTransferAsyncDynamicOrdinalOpKernel(\n-    OpKernelConstruction* ctx, const string& transfer_type,\n+    OpKernelConstruction* ctx, const std::string& transfer_type,\n     int number_of_threads, std::unique_ptr<TpuTransferOpInterface> transfer_op)\n     : TpuTransferAsyncOpKernelBase(ctx, transfer_type, number_of_threads,\n                                    std::move(transfer_op)) {}\n \n absl::Status TpuTransferAsyncDynamicOrdinalOpKernel::RunTransfer(\n     OpKernelContext* ctx) {\n   const Tensor& device_ordinal_tensor = ctx->input(0);\n-  const int device_ordinal = device_ordinal_tensor.scalar<int32>()();\n+  const int device_ordinal = device_ordinal_tensor.scalar<int32_t>()();\n   XlaDevice* xla_device =\n       dynamic_cast<XlaDevice*>(ctx->device()->UnderlyingDevice());\n   if (((xla_device == nullptr) || (xla_device->device_type() == DEVICE_CPU)) &&"
        }
    ],
    "stats": {
        "total": 374,
        "additions": 190,
        "deletions": 184
    }
}