{
    "author": "tensorflower-gardener",
    "message": "[XLA:GPU] Temporary revert statically registered collectives allocators since they are breaking OSS JAX tests due dynamic linking.\n\nPiperOrigin-RevId: 845795344",
    "sha": "e8cfd652f5d2dde56fe5e5042086fd91f5723152",
    "files": [
        {
            "sha": "32d6ef67a058ddf6b3dc37c5900144201b18810c",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 1,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e8cfd652f5d2dde56fe5e5042086fd91f5723152/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e8cfd652f5d2dde56fe5e5042086fd91f5723152/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc?ref=e8cfd652f5d2dde56fe5e5042086fd91f5723152",
            "patch": "@@ -1035,7 +1035,28 @@ CudaExecutor::CreateMemoryAllocator(MemorySpace type) {\n   }\n \n   if (type == MemorySpace::kCollective) {\n-    return CreateCollectiveMemoryAllocator(this, collective_allocator_type_);\n+    // TODO(469289220): Use NCCL/NVSHMEM memory allocator here instead.\n+    return std::make_unique<GenericMemoryAllocator>(\n+        [this](uint64_t size)\n+            -> absl::StatusOr<std::unique_ptr<MemoryAllocation>> {\n+          TF_ASSIGN_OR_RETURN(void* ptr, CollectiveMemoryAllocate(this, size));\n+          XLA_VLOG_DEVICE(2, device_ordinal())\n+              << \"allocated \" << ptr << \" for context \" << cuda_context_\n+              << \" of \" << size << \" bytes of collective memory\";\n+          return std::make_unique<GenericMemoryAllocation>(\n+              ptr, size, [this](void* location, uint64_t size) {\n+                auto status = CollectiveMemoryDeallocate(this, location);\n+                if (!status.ok()) {\n+                  XLA_LOG_DEVICE(ERROR, device_ordinal())\n+                      << \"failed to free collective memory at \" << location\n+                      << \"; result: \" << status;\n+                } else {\n+                  XLA_VLOG_DEVICE(2, device_ordinal())\n+                      << \"deallocated collective memory at \" << location\n+                      << \" for context \" << cuda_context_;\n+                }\n+              });\n+        });\n   }\n \n   if (type == MemorySpace::kHost) {"
        }
    ],
    "stats": {
        "total": 23,
        "additions": 22,
        "deletions": 1
    }
}