{
    "author": "KanishAnand",
    "message": "Add tests for recently added `NamedSharding` implementations\n\nPiperOrigin-RevId: 844905452",
    "sha": "05e273cafa8a2fe1fb74b6ec01aaec6aaa1d3f09",
    "files": [
        {
            "sha": "dbfbeaa362fb669ba7945bb3c8502d6c76270270",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05e273cafa8a2fe1fb74b6ec01aaec6aaa1d3f09/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05e273cafa8a2fe1fb74b6ec01aaec6aaa1d3f09/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc?ref=05e273cafa8a2fe1fb74b6ec01aaec6aaa1d3f09",
            "patch": "@@ -1516,9 +1516,10 @@ HloSharding PartiallyReplicateTiledShardingOnDims(\n         sharding.named_sharding().dim_shardings().begin(),\n         sharding.named_sharding().dim_shardings().end());\n     for (int64_t dim : dims_to_replicate) {\n-      if (dim < dim_shardings.size()) {\n-        dim_shardings[dim] = NamedSharding::DimensionSharding();\n-      }\n+      CHECK_LT(dim, dim_shardings.size())\n+          << \"Dimension \" << dim << \" is out of bounds for number dimensions \"\n+          << dim_shardings.size();\n+      dim_shardings[dim] = NamedSharding::DimensionSharding();\n     }\n     return HloSharding(NamedSharding(\n         sharding.named_sharding().mesh(), dim_shardings,\n@@ -1527,12 +1528,10 @@ HloSharding PartiallyReplicateTiledShardingOnDims(\n   }\n \n   int64_t group_count = 1;\n-  DimensionVector valid_dims_to_replicate;\n   for (int64_t dim : dims_to_replicate) {\n-    if (dim >= sharding.TiledDataRank()) {\n-      continue;\n-    }\n-    valid_dims_to_replicate.push_back(dim);\n+    CHECK_LT(dim, sharding.TiledDataRank())\n+        << \"Dimension \" << dim << \" is out of bounds for number dimensions \"\n+        << sharding.TiledDataRank();\n     group_count *= sharding.dimension(dim);\n   }\n   if (group_count == 1) {\n@@ -1544,14 +1543,14 @@ HloSharding PartiallyReplicateTiledShardingOnDims(\n   DimensionVector dim_permutation(sharding.TiledDataRank());\n   absl::c_iota(dim_permutation, 0);\n   absl::c_stable_sort(dim_permutation, [&](const int64_t a, const int64_t b) {\n-    return absl::c_linear_search(valid_dims_to_replicate, a) <\n-           absl::c_linear_search(valid_dims_to_replicate, b);\n+    return absl::c_linear_search(dims_to_replicate, a) <\n+           absl::c_linear_search(dims_to_replicate, b);\n   });\n   auto new_tile =\n       TransposeSharding(sharding, dim_permutation).tile_assignment();\n   DimensionVector new_tile_shape(sharding.dimensions().begin(),\n                                  sharding.dimensions().end());\n-  for (int64_t dim : valid_dims_to_replicate) {\n+  for (int64_t dim : dims_to_replicate) {\n     new_tile_shape[dim] = 1;\n   }\n   if (sharding.ReplicateOnLastTileDim()) {"
        },
        {
            "sha": "d5deceb27c1970c4d27c9ab99e6a0718819ecf69",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util_test.cc",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/05e273cafa8a2fe1fb74b6ec01aaec6aaa1d3f09/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/05e273cafa8a2fe1fb74b6ec01aaec6aaa1d3f09/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util_test.cc?ref=05e273cafa8a2fe1fb74b6ec01aaec6aaa1d3f09",
            "patch": "@@ -647,6 +647,42 @@ TEST(HloShardingUtilTest, PropagateShardingAlongDimsAndReplicateOthers4) {\n   EXPECT_EQ(target_sharding.named_sharding(), expected);\n }\n \n+TEST(HloShardingUtilTest, PartiallyReplicateTiledShardingOnDims) {\n+  Mesh mesh({2, 3, 5, 7, 11}, {\"a\", \"b\", \"c\", \"d\", \"e\"});\n+  NamedSharding source_sharding =\n+      test_utils::FromAxisNames(mesh, {{\"a\"}, {\"b\"}, {\"c\"}, {\"d\"}, {\"e\"}});\n+  std::vector<int64_t> dims_to_replicate = {3, 1};\n+  HloSharding target_sharding = PartiallyReplicateTiledShardingOnDims(\n+      HloSharding(source_sharding), dims_to_replicate);\n+  NamedSharding expected =\n+      test_utils::FromAxisNames(mesh, {{\"a\"}, {}, {\"c\"}, {}, {\"e\"}});\n+  EXPECT_EQ(target_sharding.named_sharding(), expected);\n+}\n+\n+TEST(HloShardingUtilTest, ReplicateAllDataDims) {\n+  Mesh mesh({2, 3, 5, 7, 11}, {\"a\", \"b\", \"c\", \"d\", \"e\"});\n+  NamedSharding source_sharding = test_utils::FromAxisNames(\n+      mesh, {{\"a\"}, {}, {\"c\"}, {}, {\"e\"}}, /*replicated_axes=*/{\"d\"},\n+      /*unreduced_axes=*/{\"b\"});\n+  HloSharding target_sharding =\n+      ReplicateAllDataDims(HloSharding(source_sharding), 3);\n+  NamedSharding expected =\n+      test_utils::FromAxisNames(mesh, {{}, {}, {}}, {\"d\"}, {\"b\"});\n+  EXPECT_EQ(target_sharding.named_sharding(), expected);\n+}\n+\n+TEST(HloShardingUtilTest, RemoveShapeDimensions) {\n+  Mesh mesh({2, 3, 5, 7, 11}, {\"a\", \"b\", \"c\", \"d\", \"e\"});\n+  NamedSharding source_sharding =\n+      test_utils::FromAxisNames(mesh, {{\"a\"}, {}, {\"c\"}, {}, {\"e\"}});\n+  std::vector<int64_t> dims_to_remove = {1, 3};\n+  HloSharding target_sharding =\n+      RemoveShapeDimensions(HloSharding(source_sharding), dims_to_remove);\n+  NamedSharding expected =\n+      test_utils::FromAxisNames(mesh, {{\"a\"}, {\"c\"}, {\"e\"}});\n+  EXPECT_EQ(target_sharding.named_sharding(), expected);\n+}\n+\n TEST(HloShardingUtilTest, MergeManualSubgroupSharding) {\n   TileAssignment tile_assignment({16, 4});\n   std::vector<OpSharding::Type> subgroup_types = {OpSharding::MANUAL,"
        }
    ],
    "stats": {
        "total": 57,
        "additions": 46,
        "deletions": 11
    }
}