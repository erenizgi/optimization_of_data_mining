{
    "author": "thomasjoerg",
    "message": "[XLA:GPU] Add xla_gpu_default_to_alg_dot_bf16_bf16_f32 flag to XLA.\n\nThis flag allows using the dot precision algorithm ALG_DOT_BF16_BF16_F32 for f32 dot ops by default on GPU, which can improve performance at the expense of numerical accuracy.\n\nThis change just adds the flag, the implementation will follow.\n\nPiperOrigin-RevId: 846612133",
    "sha": "1299abfd8f0aa9e64cb64d54d75685ac3de9a5d9",
    "files": [
        {
            "sha": "5ad2bb235fd49cb133f41b9f6b1691f715cce5ea",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1299abfd8f0aa9e64cb64d54d75685ac3de9a5d9/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1299abfd8f0aa9e64cb64d54d75685ac3de9a5d9/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=1299abfd8f0aa9e64cb64d54d75685ac3de9a5d9",
            "patch": "@@ -370,6 +370,7 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_gpu_experimental_enable_fusion_block_level_rewriter(false);\n \n   opts.set_xla_gpu_enable_llvm_module_compilation_parallelism(false);\n+  opts.set_xla_gpu_default_to_alg_dot_bf16_bf16_f32(false);\n   opts.set_xla_gpu_enable_libnvptxcompiler(\n       stream_executor::IsLibNvPtxCompilerSupported());\n   opts.set_xla_gpu_libnvjitlink_mode(DebugOptions::LIB_NV_JIT_LINK_MODE_AUTO);\n@@ -1533,6 +1534,13 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n       \"--xla_gpu_force_compilation_parallelism flag and the thread pool \"\n       \"supplied to GpuCompiler.\"));\n \n+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_default_to_alg_dot_bf16_bf16_f32\",\n+      bool_setter_for(\n+          &DebugOptions::set_xla_gpu_default_to_alg_dot_bf16_bf16_f32),\n+      debug_options->xla_gpu_default_to_alg_dot_bf16_bf16_f32(),\n+      \"Use the dot precision algorithm `ALG_DOT_BF16_BF16_F32 by default for \"\n+      \"f32 dots.\"));\n   flag_list->push_back(\n       tsl::Flag(\"xla_gpu_deterministic_ops\",\n                 bool_setter_for(&DebugOptions::set_xla_gpu_deterministic_ops),"
        },
        {
            "sha": "e0d8356b32869bc5d317d0a5510e29135db77c26",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1299abfd8f0aa9e64cb64d54d75685ac3de9a5d9/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1299abfd8f0aa9e64cb64d54d75685ac3de9a5d9/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=1299abfd8f0aa9e64cb64d54d75685ac3de9a5d9",
            "patch": "@@ -457,6 +457,14 @@ message DebugOptions {\n   // but potentially higher the performance.\n   optional int32 xla_gpu_cudnn_gemm_max_plans = 318;\n \n+  // Allows using the dot precision algorithm `ALG_DOT_BF16_BF16_F32 for f32 dot\n+  // ops by default. This is expected to improve performance at the expense of\n+  // numerical accuracy.\n+  //\n+  // At this point, XLA may still choose a higher precision dot algorithm, but\n+  // we expect this to change at a later point.\n+  optional bool xla_gpu_default_to_alg_dot_bf16_bf16_f32 = 441;\n+\n   // Guarantees run-to-run determinism.\n   // This flag implies --xla_gpu_exclude_nondeterministic_ops and in addition\n   // disables autotuning.\n@@ -1333,7 +1341,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 441\n+  // Next id: 442\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 18,
        "additions": 17,
        "deletions": 1
    }
}