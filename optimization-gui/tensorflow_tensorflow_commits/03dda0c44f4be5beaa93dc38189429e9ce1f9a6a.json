{
    "author": "pifon2a",
    "message": "[XLA:GPU][Emitters] Pass MLIRContext instead of SymbolicExprContext to the GPU emitter.\n\nPiperOrigin-RevId: 830788576",
    "sha": "03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
    "files": [
        {
            "sha": "b5b715231a77517f55041e5c95fa503867c7999f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD?ref=03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
            "patch": "@@ -128,7 +128,6 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n         \"//xla:util\",\n-        \"//xla/backends/gpu/collectives:gpu_clique_key\",\n         \"//xla/backends/gpu/runtime:all_reduce_thunk\",\n         \"//xla/backends/gpu/runtime:collective_thunk\",\n         \"//xla/backends/gpu/runtime:copy_thunk\","
        },
        {
            "sha": "438ae415bbb2b064a1a4b17787af8456592ecb32",
            "filename": "third_party/xla/xla/backends/gpu/codegen/custom.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc?ref=03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
            "patch": "@@ -933,7 +933,7 @@ absl::StatusOr<FusionEmissionResult> EmitCustomCall(\n       mlir::Attribute attr = mlir::parseAttribute(\n           backend_config_str,\n           // TODO: b/451959933 - Use reference or check pointer.\n-          ir_emitter_context.symbolic_expr_context()->GetMLIRContext());\n+          ir_emitter_context.expr_context()->GetMLIRContext());\n       auto dict = mlir::dyn_cast_or_null<mlir::DictionaryAttr>(attr);\n       if (dict == nullptr) {\n         return absl::InternalError("
        },
        {
            "sha": "09d9131fa71e14b3da1c1598acec5458818def25",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/concatenate.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.cc?ref=03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
            "patch": "@@ -78,9 +78,10 @@ ConcatenateFusion::ComputeThreadIdToInputIndexing(\n \n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>>\n ConcatenateFusion::CreateMLIRModule(\n-    SymbolicExprContext& symbolic_expr_context,\n-    const HloFusionInstruction& fusion, const std::string& entry_function_name,\n+    mlir::MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n+    const std::string& entry_function_name,\n     const BufferAssignment* buffer_assignment) const {\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n   emitters::ConcatenateFusionKernelEmitter emitter(\n       symbolic_expr_context, fusion, analysis_.fusion_spec(), buffer_assignment,\n       GetDefaultBufferAlignment(), GetWorkDimensions(), entry_function_name,"
        },
        {
            "sha": "71a6282df7f3c1f9ddfd964bc72aa89c8b302d00",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/concatenate.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fconcatenate.h?ref=03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
            "patch": "@@ -54,7 +54,7 @@ class ConcatenateFusion final : public EmitterBase {\n \n  protected:\n   absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateMLIRModule(\n-      SymbolicExprContext& context, const HloFusionInstruction& fusion,\n+      mlir::MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n       const std::string& entry_function_name,\n       const BufferAssignment* buffer_assignment) const override;\n "
        },
        {
            "sha": "6ef9b955840dea37e4f0ad9d16e4da257ca75dae",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 11,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.cc?ref=03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
            "patch": "@@ -39,6 +39,7 @@ limitations under the License.\n #include \"llvm/IR/IntrinsicsNVPTX.h\"\n #include \"llvm/Linker/Linker.h\"\n #include \"llvm/Support/Casting.h\"\n+#include \"llvm/Support/ThreadPool.h\"\n #include \"llvm/Support/raw_ostream.h\"\n #include \"mlir/Conversion/AffineToStandard/AffineToStandard.h\"\n #include \"mlir/Conversion/ComplexToStandard/ComplexToStandard.h\"\n@@ -125,6 +126,7 @@ namespace gpu {\n namespace {\n \n using llvm::SmallVector;\n+using mlir::MLIRContext;\n using mlir::Value;\n using mlir::ValueRange;\n using mlir::func::FuncOp;\n@@ -267,6 +269,7 @@ absl::StatusOr<FusionEmissionResult> EmitterBase::Emit(\n                                      ir_emitter_context.buffer_assignment(),\n                                      GetDefaultBufferAlignment(), &fusion));\n   auto launch_dims = launch_dimensions();\n+  mlir::MLIRContext& mlir_context = *ir_emitter_context.mlir_context();\n   auto [status_or_entry, cached] =\n       ir_emitter_context.kernel_cache().GetWithStatus(\n           fusion.fused_instructions_computation(), args.args(),\n@@ -276,10 +279,12 @@ absl::StatusOr<FusionEmissionResult> EmitterBase::Emit(\n                 ir_emitter_context.name_uniquer()->GetUniqueName(\n                     llvm_ir::SanitizeFunctionName(std::string(fusion.name())));\n             if (ir_emitter_context.emit_kernels()) {\n+              mlir_context.appendDialectRegistry(GetDialectRegistry());\n+              mlir_context.loadAllAvailableDialects();\n               TF_ASSIGN_OR_RETURN(\n                   auto module,\n                   CreateLLVMModule(\n-                      *ir_emitter_context.symbolic_expr_context(),\n+                      mlir_context,\n                       ir_emitter_context.llvm_module()->getContext(),\n                       ir_emitter_context.gpu_device_info(), fusion, kernel_name,\n                       &ir_emitter_context.buffer_assignment()));\n@@ -326,16 +331,13 @@ absl::StatusOr<FusionEmissionResult> EmitterBase::Emit(\n }\n \n absl::StatusOr<std::unique_ptr<llvm::Module>> EmitterBase::CreateLLVMModule(\n-    SymbolicExprContext& symbolic_expr_context, llvm::LLVMContext& llvm_context,\n+    mlir::MLIRContext& mlir_context, llvm::LLVMContext& llvm_context,\n     const se::DeviceDescription& device, const HloFusionInstruction& fusion,\n     const std::string& entry_function_name,\n     const BufferAssignment* buffer_assignment) const {\n-  mlir::MLIRContext& mlir_context = *symbolic_expr_context.GetMLIRContext();\n-  mlir_context.appendDialectRegistry(GetDialectRegistry());\n-  mlir_context.loadAllAvailableDialects();\n-  TF_ASSIGN_OR_RETURN(auto module,\n-                      CreateMLIRModule(symbolic_expr_context, fusion,\n-                                       entry_function_name, buffer_assignment));\n+  TF_ASSIGN_OR_RETURN(\n+      auto module, CreateMLIRModule(mlir_context, fusion, entry_function_name,\n+                                    buffer_assignment));\n \n   mlir::PassManager pm(&mlir_context);\n   emitters::RegisterOptimizationPasses(pm);\n@@ -353,10 +355,9 @@ absl::StatusOr<std::unique_ptr<llvm::Module>> EmitterBase::CreateLLVMModule(\n }\n \n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitterBase::CreateMLIRModule(\n-    SymbolicExprContext& symbolic_expr_context,\n-    const HloFusionInstruction& fusion, const std::string& entry_function_name,\n+    MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n+    const std::string& entry_function_name,\n     const BufferAssignment* buffer_assignment) const {\n-  mlir::MLIRContext& mlir_context = *symbolic_expr_context.GetMLIRContext();\n   mlir::OpBuilder builder(&mlir_context);\n   auto loc = mlir::NameLoc::get(builder.getStringAttr(fusion.name()));\n   mlir::OwningOpRef<mlir::ModuleOp> module = llvm_ir::CreateMlirModuleOp(loc);\n@@ -367,6 +368,7 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitterBase::CreateMLIRModule(\n                           GetDefaultBufferAlignment(), entry_function_name));\n   SetBackendKind(&mlir_context, entry_func, BackendKind::kGpu);\n \n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n   TF_RETURN_IF_ERROR(\n       EmitMlir(module.get(), entry_func, fusion, symbolic_expr_context));\n   return module;"
        },
        {
            "sha": "005ddca4b286793168d16738e70f6231599602b2",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base.h",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base.h?ref=03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
            "patch": "@@ -59,17 +59,15 @@ class EmitterBase : public KernelFusionInterface {\n   // Visible for testing. `buffer_assignment` is optional for testing (assigns\n   // a different buffer to each tensor).\n   absl::StatusOr<std::unique_ptr<llvm::Module>> CreateLLVMModule(\n-      SymbolicExprContext& symbolic_expr_context,\n-      llvm::LLVMContext& llvm_context, const se::DeviceDescription& device,\n-      const HloFusionInstruction& fusion,\n+      mlir::MLIRContext& mlir_context, llvm::LLVMContext& llvm_context,\n+      const se::DeviceDescription& device, const HloFusionInstruction& fusion,\n       const std::string& entry_function_name,\n       const BufferAssignment* buffer_assignment) const;\n \n   // Visible for testing. `buffer_assignment` is optional for testing (assigns\n   // a different buffer to each tensor).\n   virtual absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateMLIRModule(\n-      SymbolicExprContext& symbolic_expr_context,\n-      const HloFusionInstruction& fusion,\n+      mlir::MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n       const std::string& entry_function_name,\n       const BufferAssignment* buffer_assignment) const;\n "
        },
        {
            "sha": "53390e9cd98774f947f4494f6aa332464f615ff4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/emitter_base_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Femitter_base_test.cc?ref=03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
            "patch": "@@ -92,7 +92,6 @@ class EmitterBaseTest : public HloHardwareIndependentTestBase {\n   }\n \n   mlir::MLIRContext mlir_context_;\n-  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n   stream_executor::DeviceDescription device_info_ =\n       TestGpuDeviceInfo::CudaOrRocmDeviceInfo();\n };\n@@ -113,7 +112,7 @@ TEST_F(EmitterBaseTest, CreateMlirModule) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto mlir_module,\n       emitter.CreateMLIRModule(\n-          symbolic_expr_context_,\n+          mlir_context_,\n           *Cast<HloFusionInstruction>(\n               module->entry_computation()->root_instruction()),\n           \"fusion\",\n@@ -144,7 +143,7 @@ TEST_F(EmitterBaseTest, CreateLLVMModule) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto llvm_module,\n       emitter.CreateLLVMModule(\n-          symbolic_expr_context_, llvm_context, device_info_,\n+          mlir_context_, llvm_context, device_info_,\n           *Cast<HloFusionInstruction>(\n               module->entry_computation()->root_instruction()),\n           \"fusion\","
        },
        {
            "sha": "b0af48bd2c195b11fc4ee922949ea3da6a0c7848",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/in_place_dynamic_update_slice.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.cc?ref=03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
            "patch": "@@ -96,9 +96,10 @@ WorkDimensions InPlaceDynamicUpdateSliceFusion::GetWorkDimensions() const {\n \n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>>\n InPlaceDynamicUpdateSliceFusion::CreateMLIRModule(\n-    SymbolicExprContext& symbolic_expr_context,\n-    const HloFusionInstruction& fusion, const std::string& entry_function_name,\n+    mlir::MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n+    const std::string& entry_function_name,\n     const BufferAssignment* buffer_assignment) const {\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n   emitters::DynamicUpdateSliceKernelEmitter emitter(\n       symbolic_expr_context, fusion, analysis_.fusion_spec(), buffer_assignment,\n       GetDefaultBufferAlignment(), GetWorkDimensions(), entry_function_name,"
        },
        {
            "sha": "72d9a87939f5cbfca799d871f8cb95a85182bcb0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/in_place_dynamic_update_slice.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Fin_place_dynamic_update_slice.h?ref=03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
            "patch": "@@ -69,8 +69,7 @@ class InPlaceDynamicUpdateSliceFusion : public EmitterBase {\n \n  protected:\n   absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateMLIRModule(\n-      SymbolicExprContext& symbolic_expr_context,\n-      const HloFusionInstruction& fusion,\n+      mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n       const std::string& entry_function_name,\n       const BufferAssignment* buffer_assignment) const override;\n "
        },
        {
            "sha": "3427a2b86b27b6d9aa1d149f17f2df882113ab97",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/loop.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.cc?ref=03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
            "patch": "@@ -107,9 +107,10 @@ WorkDimensions LoopFusion::GetWorkDimensions() const {\n }\n \n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> LoopFusion::CreateMLIRModule(\n-    SymbolicExprContext& symbolic_expr_context,\n-    const HloFusionInstruction& fusion, const std::string& entry_function_name,\n+    mlir::MLIRContext& mlir_context, const HloFusionInstruction& fusion,\n+    const std::string& entry_function_name,\n     const BufferAssignment* buffer_assignment) const {\n+  SymbolicExprContext symbolic_expr_context(&mlir_context);\n   emitters::LoopFusionKernelEmitter emitter(\n       symbolic_expr_context, fusion, analysis_.fusion_spec(), buffer_assignment,\n       GetDefaultBufferAlignment(), GetWorkDimensions(), entry_function_name,"
        },
        {
            "sha": "8716e802344aeefbb6293ddaad38caa196cad6f0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/loop.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Floop.h?ref=03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
            "patch": "@@ -52,7 +52,7 @@ class LoopFusion final : public EmitterBase {\n \n  private:\n   absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateMLIRModule(\n-      SymbolicExprContext& context, const HloFusionInstruction& fusion,\n+      mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n       const std::string& entry_function_name,\n       const BufferAssignment* buffer_assignment) const override;\n "
        },
        {
            "sha": "44fcbab44935619ee05cad429ce6e0b500081b72",
            "filename": "third_party/xla/xla/backends/gpu/codegen/tools/fusion_to_mlir.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ffusion_to_mlir.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ffusion_to_mlir.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftools%2Ffusion_to_mlir.cc?ref=03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
            "patch": "@@ -35,7 +35,7 @@ absl::Status Run(const std::string& filename) {\n                       GetEmitter(*module, symbolic_expr_context));\n   TF_ASSIGN_OR_RETURN(auto mlir_module,\n                       emitter_data->emitter->CreateMLIRModule(\n-                          symbolic_expr_context, *emitter_data->fusion, \"main\",\n+                          mlir_context, *emitter_data->fusion, \"main\",\n                           /*buffer_assignment=*/nullptr));\n   llvm::outs() << *mlir_module;\n   return absl::OkStatus();"
        },
        {
            "sha": "f1e502e48573e5bf923aa37acc1cc443b33d845c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
            "patch": "@@ -166,10 +166,10 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n \n     TF_ASSIGN_OR_RETURN(\n         TritonWrapperResult triton_wrapper_result,\n-        GenerateTritonKernelAndWrapper(\n-            fusion, impl_fn_name, ir_emitter_context.gpu_device_info(),\n-            ir_emitter_context.llvm_module(),\n-            ir_emitter_context.symbolic_expr_context()));\n+        GenerateTritonKernelAndWrapper(fusion, impl_fn_name,\n+                                       ir_emitter_context.gpu_device_info(),\n+                                       ir_emitter_context.llvm_module(),\n+                                       ir_emitter_context.expr_context()));\n \n     auto backend_config =\n         fusion.backend_config<GpuBackendConfig>()->fusion_backend_config();"
        },
        {
            "sha": "9b872652720e8190db0507a72b09a350381679cc",
            "filename": "third_party/xla/xla/service/gpu/compile_module_to_llvm_ir.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc?ref=03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
            "patch": "@@ -313,7 +313,7 @@ absl::StatusOr<CompileModuleResults> CompileModuleToLlvmIr(\n   IrEmitterContext ir_emitter_context(\n       hlo_module, results.buffer_assignment.get(),\n       results.execution_stream_assignment.get(), platform->Name(), device_desc,\n-      symbolic_expr_context.get(), results.llvm_module.get(),\n+      mlir_context.get(), results.llvm_module.get(),\n       results.llvm_module_constants.get(),\n       /*emit_kernels=*/true);\n "
        },
        {
            "sha": "a8c725acc3e9c7631e91f8a5145d1bd8ba790515",
            "filename": "third_party/xla/xla/service/gpu/custom_kernel_emitter_cuda.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_kernel_emitter_cuda.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_kernel_emitter_cuda.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcustom_kernel_emitter_cuda.cc?ref=03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
            "patch": "@@ -42,7 +42,7 @@ absl::StatusOr<std::unique_ptr<Thunk>> EmitPtxCustomKernelThunk(\n \n   TF_ASSIGN_OR_RETURN(\n       KernelCall call,\n-      KernelCall::Parse(backend_config_str, context->symbolic_expr_context()));\n+      KernelCall::Parse(backend_config_str, context->expr_context()));\n   if (call.kernel_type != KernelCall::KernelType::kPtxSource) {\n     return absl::InvalidArgumentError(\n         \"PTX custom call backend config is not a PTX source\");"
        },
        {
            "sha": "0cb2abf0a747d728a0205900fed46e121d28850b",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
            "patch": "@@ -2997,10 +2997,8 @@ GpuCompiler::LoadExecutableFromAotResult(\n \n   IrEmitterContext ir_emitter_context(\n       hlo_module.get(), buffer_assignment.get(), &execution_stream_assignment,\n-      platform_name, gpu_device_info, symbolic_expr_context(),\n-      llvm_module.get(),\n-      /*llvm_module_constants=*/nullptr,\n-      /*emit_kernels=*/false);\n+      platform_name, gpu_device_info, mlir_context(), llvm_module.get(),\n+      /*llvm_module_constants=*/nullptr, /*emit_kernels=*/false);\n \n   absl::string_view cache_file_path =\n       hlo_module->config().debug_options().xla_gpu_kernel_cache_file();"
        },
        {
            "sha": "993e3d7a439d7636cb052b9537fded2714fc3bce",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_context.h",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h?ref=03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
            "patch": "@@ -1,4 +1,3 @@\n-#include \"xla/hlo/analysis/symbolic_expr.h\"\n /* Copyright 2017 The OpenXLA Authors.\n \n Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -28,10 +27,12 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"llvm/IR/IRBuilder.h\"\n #include \"llvm/IR/Module.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/Operation.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/host_execute_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk_id.h\"\n+#include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/buffer_assignment.h\"\n@@ -69,15 +70,15 @@ class IrEmitterContext {\n                    const ExecutionStreamAssignment* execution_stream_assignment,\n                    std::string platform_name,\n                    const se::DeviceDescription& gpu_device_info,\n-                   SymbolicExprContext* symbolic_expr_context,\n-                   llvm::Module* llvm_module,\n+                   mlir::MLIRContext* mlir_context, llvm::Module* llvm_module,\n                    llvm::Module* llvm_module_constants, bool emit_kernels)\n       : hlo_module_(hlo_module),\n         buffer_assignment_(buffer_assignment),\n         execution_stream_assignment_(execution_stream_assignment),\n         platform_name_(std::move(platform_name)),\n         gpu_device_info_(gpu_device_info),\n-        symbolic_expr_context_(symbolic_expr_context),\n+        mlir_context_(mlir_context),\n+        expr_context_(mlir_context_),\n         llvm_module_(llvm_module),\n         llvm_module_constants_(llvm_module_constants),\n         emit_kernels_(emit_kernels) {}\n@@ -101,12 +102,12 @@ class IrEmitterContext {\n     return gpu_device_info_.gpu_compute_capability();\n   }\n \n+  mlir::MLIRContext* mlir_context() { return mlir_context_; }\n+\n   // TODO: b/451959933 - Add nullability annotation to be explicit about this\n   // pointer: go/totw/230. Alternatively, return by reference instead of pointer\n   // (and require reference in ctor) to signal that it is always present.\n-  SymbolicExprContext* symbolic_expr_context() {\n-    return symbolic_expr_context_;\n-  }\n+  SymbolicExprContext* expr_context() { return &expr_context_; }\n \n   llvm::Module* llvm_module() { return llvm_module_; }\n   // A separate module can optionally be used to emit constants.\n@@ -158,7 +159,8 @@ class IrEmitterContext {\n   const ExecutionStreamAssignment* execution_stream_assignment_;\n   std::string platform_name_;\n   const se::DeviceDescription& gpu_device_info_;\n-  SymbolicExprContext* symbolic_expr_context_;\n+  mlir::MLIRContext* mlir_context_;\n+  SymbolicExprContext expr_context_;\n   llvm::Module* llvm_module_;\n   llvm::Module* llvm_module_constants_;\n   NameUniquer name_uniquer_;"
        },
        {
            "sha": "7879fa75bfb61d7f28884555e8f72bca116c8b51",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/03dda0c44f4be5beaa93dc38189429e9ce1f9a6a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=03dda0c44f4be5beaa93dc38189429e9ce1f9a6a",
            "patch": "@@ -1217,7 +1217,7 @@ absl::Status IrEmitterUnnested::EmitCustomCallThunk(\n     if (!backend_config_str.empty()) {\n       mlir::Attribute attr = mlir::parseAttribute(\n           backend_config_str,\n-          ir_emitter_context_->symbolic_expr_context()->GetMLIRContext());\n+          ir_emitter_context_->expr_context()->GetMLIRContext());\n       auto dict = mlir::dyn_cast_or_null<mlir::DictionaryAttr>(attr);\n       if (dict == nullptr) {\n         return absl::InternalError(\n@@ -1450,7 +1450,7 @@ absl::Status IrEmitterUnnested::EmitTritonCustomCall(\n     const HloCustomCallInstruction* instr) {\n   auto generate = [this, &instr]() -> absl::StatusOr<KernelReuseCache::Entry> {\n     mlir::MLIRContext& mlir_context =\n-        *ir_emitter_context_->symbolic_expr_context()->GetMLIRContext();\n+        *ir_emitter_context_->expr_context()->GetMLIRContext();\n     LoadMlirDialectsForTriton(mlir_context);\n     auto call =\n         TritonCall::Parse(instr->raw_backend_config_string(), &mlir_context);\n@@ -1627,7 +1627,7 @@ absl::Status IrEmitterUnnested::EmitFusion(const HloFusionInstruction* instr) {\n           /*buffer_assignment=*/\n           &ir_emitter_context_->buffer_assignment(),\n           /*call_graph=*/*call_graph_),\n-      ir_emitter_context_->symbolic_expr_context());\n+      ir_emitter_context_->expr_context());\n   TF_ASSIGN_OR_RETURN(auto result, emitter->Emit(*ir_emitter_context_, *instr));\n \n   const ExecutionStreamAssignment& stream_assignment ="
        }
    ],
    "stats": {
        "total": 106,
        "additions": 53,
        "deletions": 53
    }
}