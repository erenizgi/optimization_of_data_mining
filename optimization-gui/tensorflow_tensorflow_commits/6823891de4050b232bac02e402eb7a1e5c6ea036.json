{
    "author": "ZixuanJiang",
    "message": "All the dus_ar_dims needs padding.\n\nThe dus_ar_dims satisfies that `base_shape().dimensions(i) <= partitions / 2`. Hence, we must introduce padding in this dimension.\n\nPiperOrigin-RevId: 842917144",
    "sha": "6823891de4050b232bac02e402eb7a1e5c6ea036",
    "files": [
        {
            "sha": "1f4c6f84b6b96866b8e3e363a66c2aeec5d4c40c",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 21,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6823891de4050b232bac02e402eb7a1e5c6ea036/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6823891de4050b232bac02e402eb7a1e5c6ea036/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=6823891de4050b232bac02e402eb7a1e5c6ea036",
            "patch": "@@ -1413,29 +1413,16 @@ HloInstruction* PartitionedHlo::ReplicatePartial(\n   if (!dus_ar_dims.empty()) {\n     auto zero = state_.b->AddInstruction(HloInstruction::CreateConstant(\n         LiteralUtil::Zero(shard_shape.element_type())));\n-    std::vector<int64_t> masking_dims;\n-    for (int64_t dim : dus_ar_dims) {\n-      if (shard_shape.dimensions(dim) * sharding().dimension(dim) !=\n-          base_shape().dimensions(dim)) {\n-        // DUS will be out-of-bound and offset will be clamped, so we need to\n-        // mask this dim with 0.\n-        masking_dims.push_back(dim);\n-      }\n-    }\n-    if (!masking_dims.empty()) {\n-      std::vector<int64_t> skipped_dims;\n-      for (int64_t i = 0; i < base_shape().dimensions().size(); ++i) {\n-        if (!absl::c_linear_search(masking_dims, i)) {\n-          skipped_dims.push_back(i);\n-        }\n+    std::vector<int64_t> skipped_dims;\n+    for (int64_t i = 0; i < base_shape().dimensions().size(); ++i) {\n+      if (!absl::c_linear_search(dus_ar_dims, i)) {\n+        skipped_dims.push_back(i);\n       }\n-      result->copy_sharding(hlo_);\n-      result = PartitionedHlo(result, final_result_shape, state_)\n-                   .PadWithValue(zero,\n-                                 /*left_padded_dims=*/{},\n-                                 /*skipped_dims=*/skipped_dims)\n-                   .hlo();\n     }\n+    result->copy_sharding(hlo_);\n+    result = PartitionedHlo(result, final_result_shape, state_)\n+                 .PadWithValue(zero, /*left_padded_dims=*/{}, skipped_dims)\n+                 .hlo();\n     auto zero_bcast = state_.b->AddInstruction(\n         HloInstruction::CreateBroadcast(final_result_shape, zero, {}));\n     auto offsets = MakePartitionOffsets("
        }
    ],
    "stats": {
        "total": 29,
        "additions": 8,
        "deletions": 21
    }
}