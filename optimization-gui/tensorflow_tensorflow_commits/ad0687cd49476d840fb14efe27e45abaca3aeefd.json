{
    "author": "akuegel",
    "message": "[XLA:GPU] Avoid running out of shared memory for prefix sum kernel.\n\nExperimentation shows that with 64 bit types we would run out of shared memory\nwith block size 1024, so only allow up to block size 512.\n\nPiperOrigin-RevId: 830748681",
    "sha": "ad0687cd49476d840fb14efe27e45abaca3aeefd",
    "files": [
        {
            "sha": "7fabc058d8acf03624d57b7baecbfa2a4b3f0838",
            "filename": "third_party/xla/xla/stream_executor/cuda/cub_prefix_sum_kernel_cuda.cu.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ad0687cd49476d840fb14efe27e45abaca3aeefd/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcub_prefix_sum_kernel_cuda.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ad0687cd49476d840fb14efe27e45abaca3aeefd/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcub_prefix_sum_kernel_cuda.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcub_prefix_sum_kernel_cuda.cu.cc?ref=ad0687cd49476d840fb14efe27e45abaca3aeefd",
            "patch": "@@ -31,8 +31,8 @@ namespace {\n template <unsigned int BLOCK_SIZE, typename ElementT>\n __device__ void RowPrefixSum(const ElementT* data_in, ElementT* data_out,\n                              size_t num_items) {\n-  // `BLOCK_SIZE` must be a power of 2 no larger than 1024.\n-  static_assert(BLOCK_SIZE <= 1024 && (BLOCK_SIZE & (BLOCK_SIZE - 1)) == 0);\n+  // `BLOCK_SIZE` must be a power of 2 no larger than 512.\n+  static_assert(BLOCK_SIZE <= 512 && (BLOCK_SIZE & (BLOCK_SIZE - 1)) == 0);\n   using BlockScan = cub::BlockScan<ElementT, BLOCK_SIZE>;\n   __shared__ typename BlockScan::TempStorage temp_storage;\n   ElementT total = 0;\n@@ -63,12 +63,9 @@ __global__ void PrefixSum(const void* data_in, void* data_out,\n   int64_t row_offset = block_idx * num_items;\n   // https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/:\n   // CUDA architecture limits the numbers of threads per block (1024 threads\n-  // per block limit).\n+  // per block limit). We need to limit it to 512 to avoid running out of shared\n+  // memory with 8 byte data types.\n   switch (blockDim.x * blockDim.y * blockDim.z) {\n-    case 1024:\n-      RowPrefixSum<1024>(data_in_typed + row_offset,\n-                         data_out_typed + row_offset, num_items);\n-      break;\n     case 512:\n       RowPrefixSum<512>(data_in_typed + row_offset, data_out_typed + row_offset,\n                         num_items);"
        },
        {
            "sha": "11f74e67eeee687ae1f23feb79dc47647db394a1",
            "filename": "third_party/xla/xla/stream_executor/cuda/cub_prefix_sum_kernel_cuda_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ad0687cd49476d840fb14efe27e45abaca3aeefd/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcub_prefix_sum_kernel_cuda_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ad0687cd49476d840fb14efe27e45abaca3aeefd/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcub_prefix_sum_kernel_cuda_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcub_prefix_sum_kernel_cuda_test.cc?ref=ad0687cd49476d840fb14efe27e45abaca3aeefd",
            "patch": "@@ -106,8 +106,10 @@ class CubPrefixSumKernelCudaTest\n \n     TF_RETURN_IF_ERROR(stream_->Memcpy(&device_input, input.data(),\n                                        input.size() * sizeof(input[0])));\n+    // For large number of items, limit the number of threads per block to 512\n+    // to avoid running out of shared memory.\n     size_t num_threads_per_block =\n-        std::min(size_t{1024}, absl::bit_ceil(num_items));\n+        std::min(size_t{512}, absl::bit_ceil(num_items));\n     // Call kernel\n     TF_RETURN_IF_ERROR(\n         kernel.Launch(stream_executor::ThreadDim(num_threads_per_block, 1, 1),\n@@ -221,7 +223,7 @@ INSTANTIATE_TEST_SUITE_P(\n                                             xla::S64, xla::U8, xla::U16,\n                                             xla::U32, xla::U64}),\n                        ::testing::ValuesIn({1, 2, 3, 128, 511, 512}),\n-                       ::testing::ValuesIn({1, 2, 3, 128, 511, 512}),\n+                       ::testing::ValuesIn({1, 2, 3, 128, 511, 513}),\n                        ::testing::ValuesIn({false, true})),\n     ParametersToString);\n "
        }
    ],
    "stats": {
        "total": 17,
        "additions": 8,
        "deletions": 9
    }
}