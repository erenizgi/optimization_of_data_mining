{
    "author": "metaflow",
    "message": "[XLA:GPU] use new triton support checks in gemm fusion pass\n\nAs the new emitter is now default we have to make sure that gemm fusion\ndoes not fuse ops that are not supported.\n\nMigrated some additional support checks from the legacy checks.\n\nPiperOrigin-RevId: 832229464",
    "sha": "4512a9251b234a4d1d59a3ee1ba5eeb3f17c4b9c",
    "files": [
        {
            "sha": "5d09574131c0e7975fc7fc6ddeef188334b6480c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4512a9251b234a4d1d59a3ee1ba5eeb3f17c4b9c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4512a9251b234a4d1d59a3ee1ba5eeb3f17c4b9c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_test.cc?ref=4512a9251b234a4d1d59a3ee1ba5eeb3f17c4b9c",
            "patch": "@@ -17,7 +17,6 @@ limitations under the License.\n #include <memory>\n #include <string>\n #include <utility>\n-#include <variant>\n #include <vector>\n \n #include <gmock/gmock.h>\n@@ -1566,7 +1565,9 @@ ENTRY e {\n   EXPECT_TRUE(RunAndCompare(hlo_text, ErrorSpec{/*aabs=*/1e-6, /*arel=*/1e-6}));\n }\n \n-TEST_F(TritonGemmTest, DynamicSliceIsSupportedInLhsEndToEnd) {\n+// Dynamic slice is not supported by the generic Triton emitter yet and disabled\n+// in the triton gemm fusion pass.\n+TEST_F(TritonGemmTest, DISABLED_DynamicSliceIsSupportedInLhsEndToEnd) {\n   // The select is used to restrict the start index to values that make sense.\n   // If it was constant, then the dynamic-slice would be optimized to slice. It\n   // is not strictly needed, because we also support clamping the indices."
        },
        {
            "sha": "a8313319af0a284d40c6bc461f0b746a15b1a1e1",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support.cc",
            "status": "modified",
            "additions": 66,
            "deletions": 43,
            "changes": 109,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4512a9251b234a4d1d59a3ee1ba5eeb3f17c4b9c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4512a9251b234a4d1d59a3ee1ba5eeb3f17c4b9c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc?ref=4512a9251b234a4d1d59a3ee1ba5eeb3f17c4b9c",
            "patch": "@@ -16,7 +16,6 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n \n #include <string>\n-#include <variant>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n@@ -41,7 +40,6 @@ limitations under the License.\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla {\n@@ -262,12 +260,13 @@ bool IsTritonSupportedElementwise(HloOpcode opcode, PrimitiveType element_type,\n }\n \n CodegenDecision IsTritonSupportedInstructionImpl(\n-    const HloInstruction& instr, const se::GpuComputeCapability& gpu_version);\n+    const HloInstruction& instr, const se::GpuComputeCapability& gpu_version,\n+    bool is_fused_computation);\n \n // Filters Reduces which can be handled using Triton.\n CodegenDecision CanTritonHandleReduce(\n     const HloReduceInstruction& reduce,\n-    const se::GpuComputeCapability& gpu_version) {\n+    const se::GpuComputeCapability& gpu_version, bool is_fused_computation) {\n   if (reduce.shape().element_type() == PrimitiveType::F8E4M3FN ||\n       reduce.shape().element_type() == PrimitiveType::F8E5M2) {\n     return CodegenDecision::Forbid(\n@@ -276,7 +275,9 @@ CodegenDecision CanTritonHandleReduce(\n \n   bool is_triton_supported_reduction_computation = absl::c_all_of(\n       reduce.to_apply()->instructions(), [&](const HloInstruction* instr) {\n-        return IsTritonSupportedInstructionImpl(*instr, gpu_version).CanFuse();\n+        return IsTritonSupportedInstructionImpl(*instr, gpu_version,\n+                                                is_fused_computation)\n+            .CanFuse();\n       });\n   if (!is_triton_supported_reduction_computation) {\n     return CodegenDecision::Forbid(\n@@ -342,6 +343,14 @@ CodegenDecision AreTypesSupportedByAlgUnsetDot(\n         \"Dot operation only supports F64 result type for F64 input type.\");\n   }\n \n+  if (input_type == F8E5M2 || result_type == F8E5M2) {\n+    if (auto* cuda_cc = gpu_version.cuda_compute_capability();\n+        cuda_cc && !cuda_cc->IsAtLeastAmpere()) {\n+      return CodegenDecision::Forbid(\n+          \"Dot operation for F8E5M2 is not supported before Ampere.\");\n+    }\n+  }\n+\n   if (input_type == F8E4M3FN || result_type == F8E4M3FN) {\n     if (auto* cuda_cc = gpu_version.cuda_compute_capability();\n         cuda_cc && !cuda_cc->IsAtLeastHopper()) {\n@@ -446,25 +455,33 @@ CodegenDecision AreDotAlgorithmInputAndOutputConversionsSupported(\n }\n \n CodegenDecision IsTritonSupportedDot(\n-    const HloDotInstruction& dot, const se::GpuComputeCapability& gpu_version) {\n-  if (!IsInTritonNestedGemmFusion(dot)) {\n-    return CodegenDecision::Forbid(\n-        \"Dot operation is only supported in nested GEMM fusions.\");\n+    const HloDotInstruction& dot, const se::GpuComputeCapability& gpu_version,\n+    bool is_fused_computation) {\n+  // Check that dot matches the pattern expected by the emitter: is in a nested\n+  // GEMM fusion and its operands are fusions.\n+\n+  const HloInstruction* lhs = dot.operand(0);\n+  const HloInstruction* rhs = dot.operand(1);\n+  if (is_fused_computation) {\n+    if (!IsInTritonNestedGemmFusion(dot)) {\n+      return CodegenDecision::Forbid(\n+          \"Dot operation is only supported in nested GEMM fusions.\");\n+    }\n+    if (lhs->opcode() != HloOpcode::kFusion ||\n+        rhs->opcode() != HloOpcode::kFusion) {\n+      return CodegenDecision::Forbid(\n+          \"Only operands that are fusions are supported.\");\n+    }\n   }\n+\n   PrimitiveType result_type = dot.shape().element_type();\n-  const Shape& lhs_shape = dot.operand(0)->shape();\n-  const Shape& rhs_shape = dot.operand(1)->shape();\n+  const Shape& lhs_shape = lhs->shape();\n+  const Shape& rhs_shape = rhs->shape();\n   PrimitiveType lhs_type = lhs_shape.element_type();\n   PrimitiveType rhs_type = rhs_shape.element_type();\n \n-  if (dot.operand(0)->opcode() != HloOpcode::kFusion ||\n-      dot.operand(1)->opcode() != HloOpcode::kFusion) {\n-    return CodegenDecision::Forbid(\n-        \"Only operands that are fusions are supported.\");\n-  }\n-\n-  // TODO(b/393299275): add support tests for mixed types.\n-  if (lhs_type != rhs_type) {\n+  if (lhs_type != rhs_type && !(primitive_util::IsF8Type(lhs_type) &&\n+                                primitive_util::IsF8Type(rhs_type))) {\n     return CodegenDecision::Forbid(\n         \"Dot operation only supports same types for lhs and rhs.\");\n   }\n@@ -551,27 +568,31 @@ CodegenDecision IsTritonSupportedFusion(\n                    \" is not supported: \", decision.Explain()));\n }\n \n-CodegenDecision IsTritonSupportedConcatenate(const HloInstruction& hlo) {\n+CodegenDecision IsTritonSupportedConcatenate(const HloInstruction& hlo,\n+                                             bool is_fused_computation) {\n   CHECK(hlo.opcode() == HloOpcode::kConcatenate);\n-  if (!IsInTritonNestedGemmFusion(hlo)) {\n-    return CodegenDecision::Forbid(\n-        \"Only concatenates in nested GEMM fusions are supported.\");\n-  }\n-  // TODO(b/393299275): remove this operand filter once migration is\n-  // complete and priority fusion can produce nests.\n-  if (absl::c_any_of(hlo.operands(), [](const HloInstruction* operand) {\n-        return operand->opcode() != HloOpcode::kFusion;\n-      })) {\n-    return CodegenDecision::Forbid(\n-        \"Only support concatenates with nested GEMM fusions as a \"\n-        \"parameter.\");\n+  if (is_fused_computation) {\n+    if (!IsInTritonNestedGemmFusion(hlo)) {\n+      return CodegenDecision::Forbid(\n+          \"Only concatenates in nested GEMM fusions are supported.\");\n+    }\n+    // TODO(b/393299275): remove this operand filter once migration is\n+    // complete and priority fusion can produce nests.\n+    if (absl::c_any_of(hlo.operands(), [](const HloInstruction* operand) {\n+          return operand->opcode() != HloOpcode::kFusion;\n+        })) {\n+      return CodegenDecision::Forbid(\n+          \"Only support concatenates with nested GEMM fusions as a \"\n+          \"parameter.\");\n+    }\n   }\n   return CodegenDecision(hlo.shape().element_type() != S4,\n                          \"S4 is not supported.\");\n }\n \n CodegenDecision IsTritonSupportedInstructionImpl(\n-    const HloInstruction& instr, const se::GpuComputeCapability& gpu_version) {\n+    const HloInstruction& instr, const se::GpuComputeCapability& gpu_version,\n+    bool is_fused_computation) {\n   if (internal::IsTritonUnsupportedOpcode(instr.opcode())) {\n     return CodegenDecision::Forbid(\n         absl::StrCat(\"Unsupported opcode \", HloOpcodeString(instr.opcode())));\n@@ -603,7 +624,7 @@ CodegenDecision IsTritonSupportedInstructionImpl(\n   }\n \n   if (instr.opcode() == HloOpcode::kConcatenate) {\n-    return IsTritonSupportedConcatenate(instr);\n+    return IsTritonSupportedConcatenate(instr, is_fused_computation);\n   }\n \n   // Special handling for the kPad instruction. Right now we only support \"high\"\n@@ -647,7 +668,7 @@ CodegenDecision IsTritonSupportedInstructionImpl(\n   switch (instr.opcode()) {\n     case HloOpcode::kReduce: {\n       return CanTritonHandleReduce(*Cast<HloReduceInstruction>(&instr),\n-                                   gpu_version);\n+                                   gpu_version, is_fused_computation);\n     }\n     case HloOpcode::kParameter:\n       return CodegenDecision::Allow();\n@@ -671,8 +692,8 @@ CodegenDecision IsTritonSupportedInstructionImpl(\n       return CodegenDecision(instr.shape().element_type() != S4,\n                              \"S4 is not supported.\");\n     case HloOpcode::kDot:\n-      return IsTritonSupportedDot(*Cast<HloDotInstruction>(&instr),\n-                                  gpu_version);\n+      return IsTritonSupportedDot(*Cast<HloDotInstruction>(&instr), gpu_version,\n+                                  is_fused_computation);\n     case HloOpcode::kFusion:\n       return IsTritonSupportedFusion(*Cast<HloFusionInstruction>(&instr),\n                                      gpu_version);\n@@ -743,9 +764,10 @@ absl::Status EnsureTritonSupportsComputeCapability(\n }\n \n CodegenDecision IsTritonSupportedInstruction(\n-    const HloInstruction& instr, const se::GpuComputeCapability& gpu_version) {\n-  CodegenDecision decision =\n-      IsTritonSupportedInstructionImpl(instr, gpu_version);\n+    const HloInstruction& instr, const se::GpuComputeCapability& gpu_version,\n+    bool is_fused_computation) {\n+  CodegenDecision decision = IsTritonSupportedInstructionImpl(\n+      instr, gpu_version, is_fused_computation);\n   VLOG(2) << absl::StrCat(\"IsTritonSupportedInstruction: \", instr.ToString(),\n                           \" \",\n                           (decision.CanFuse() ? \"yes\" : decision.Explain()));\n@@ -754,7 +776,8 @@ CodegenDecision IsTritonSupportedInstruction(\n \n CodegenDecision IsTritonSupportedComputation(\n     const HloComputation& computation,\n-    const se::GpuComputeCapability& gpu_compute_capability) {\n+    const se::GpuComputeCapability& gpu_compute_capability,\n+    bool is_fused_computation) {\n   VLOG(3) << \"IsTritonSupportedComputation: \" << computation.ToString();\n   for (const auto* instruction : computation.instructions()) {\n     // TODO(b/452478982): This check can be removed if we support Tuple ops\n@@ -765,8 +788,8 @@ CodegenDecision IsTritonSupportedComputation(\n       // supported for fusion roots.\n       continue;\n     }\n-    if (CodegenDecision can_codegen =\n-            IsTritonSupportedInstruction(*instruction, gpu_compute_capability);\n+    if (CodegenDecision can_codegen = IsTritonSupportedInstruction(\n+            *instruction, gpu_compute_capability, is_fused_computation);\n         !can_codegen) {\n       return can_codegen;\n     }"
        },
        {
            "sha": "d524e1a8dc27022871b54c74d1d3df4bcc185e5b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support.h",
            "status": "modified",
            "additions": 12,
            "deletions": 5,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4512a9251b234a4d1d59a3ee1ba5eeb3f17c4b9c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4512a9251b234a4d1d59a3ee1ba5eeb3f17c4b9c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.h?ref=4512a9251b234a4d1d59a3ee1ba5eeb3f17c4b9c",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n \n #include \"absl/status/status.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/service/instruction_fusion.h\"\n #include \"xla/shape.h\"\n@@ -39,26 +40,32 @@ absl::Status EnsureTritonSupportsComputeCapability(\n     const se::GpuComputeCapability& gpu_compute_capability);\n \n // Return `CodegenDecision`'s equivalent of `true` if the parameter instruction\n-// is supported by the Triton emitters for the given compute capability. Note\n-// that this function makes no assumption about what happens if\n-// `FloatNormalization` is run, unlike the legacy Triton utils.\n+// is supported by the Triton emitters for the given compute capability/\n+// `is_fused_computation` indicates that fusions are already formed and\n+// we should run additional checks for them.\n //\n+// TODO(b/393299275): remove this comment.\n // Note: this function is entirely dissociated from the legacy Triton emitters.\n // If you intend to add a feature to the legacy Triton emitters (which you\n // probably shouldn't), use `legacy_triton::IsTritonSupportedInstruction`\n // instead.\n CodegenDecision IsTritonSupportedInstruction(\n-    const HloInstruction& instr, const se::GpuComputeCapability& gpu_version);\n+    const HloInstruction& instr, const se::GpuComputeCapability& gpu_version,\n+    bool is_fused_computation = true);\n \n // Returns `CodegenDecision`'s equivalent of `true` if all the instructions in\n // the parameter computation are supported by the Triton emitters for the given\n // compute capability.\n+// `is_fused_computation` indicates that fusions are already formed and\n+// we should run additional checks for them.\n //\n+// TODO(b/393299275): remove this comment.\n // This function has the same caveats as `IsTritonSupportedInstruction` as\n // defined in the present namespace.\n CodegenDecision IsTritonSupportedComputation(\n     const HloComputation& computation,\n-    const se::GpuComputeCapability& gpu_compute_capability);\n+    const se::GpuComputeCapability& gpu_compute_capability,\n+    bool is_fused_computation = true);\n \n // Returns `true` if the parameter computation is a Triton fused computation,\n // i.e. the calling fusion instruction has `FusionKind::kCustom` and"
        },
        {
            "sha": "d9517092c3d60dcd5e700fe2c97e372c844493f9",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_fusion.cc",
            "status": "modified",
            "additions": 28,
            "deletions": 2,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4512a9251b234a4d1d59a3ee1ba5eeb3f17c4b9c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4512a9251b234a4d1d59a3ee1ba5eeb3f17c4b9c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion.cc?ref=4512a9251b234a4d1d59a3ee1ba5eeb3f17c4b9c",
            "patch": "@@ -57,6 +57,7 @@ limitations under the License.\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n+#include \"tsl/platform/tensor_float_32_utils.h\"\n \n namespace xla {\n namespace gpu {\n@@ -408,6 +409,17 @@ FusionPlanAndRequirements BuildFusionPlanTowardOperands(\n       continue;\n     }\n \n+    // TODO(b/393299275): this check cannot be replaced by a\n+    // `IsTritonSupportedComputation` because we will do some rewrites\n+    // later that might change the decision. For example 'scaled-dot-rewriter'\n+    // replaces unsupported F8E8M0FNU with u8. We should have a more principled\n+    // way check if we will be able to emit the triton code for the fusion.\n+    if (original_hlo.opcode() == HloOpcode::kDynamicSlice) {\n+      // TODO(b/417172838): support dynamic slice op.\n+      fusion_builder.SetShouldFuseNode(node_id, false);\n+      continue;\n+    }\n+\n     auto opt_result = GetOperandDimOrdersAndCombinedReqsIfProfitable(\n         original_hlo, dim_order, properties, gpu_version, combined_reqs);\n     if (!opt_result.has_value()) {\n@@ -704,13 +716,27 @@ absl::StatusOr<Decision> CreateDotFusion(\n     std::vector<HloInstruction*>& fusion_inputs,\n     HloInstruction** fusion_output_ptr) {\n   VLOG(5) << dot.ToString();\n-  if (CodegenDecision is_supported =\n-          legacy_triton::IsTritonSupportedInstruction(dot, gpu_version);\n+  if (CodegenDecision is_supported = IsTritonSupportedInstruction(\n+          dot, gpu_version, /*is_fused_computation=*/false);\n       !is_supported) {\n     VLOG(3) << is_supported.Explain();\n     return Decision::Deny(is_supported.Explain());\n   }\n \n+  // TODO(b/393299275): legacy triton emitter only accepted dots with unset\n+  // algorithm when tf32 was enabled. Keeping this check for now to avoid\n+  // performance regressions. We should investigate how to improve performance,\n+  // or move this check under IsTritonSupportedInstruction.\n+  if (dot.precision_config().algorithm() == PrecisionConfig::ALG_UNSET) {\n+    if (!tsl::tensor_float_32_execution_enabled() ||\n+        absl::c_any_of(dot.precision_config().operand_precision(),\n+                       [](int x) { return x != PrecisionConfig::DEFAULT; })) {\n+      return Decision::Deny(\n+          \"Having non-default operand precisions or TensorFloat-32 disabled \"\n+          \"for Dot op with unset algorithm.\");\n+    }\n+  }\n+\n   std::vector<HlosAndRequirements> hlos_and_reqs;\n   hlos_and_reqs.reserve(dot.operand_count());\n   TF_ASSIGN_OR_RETURN(HlosAndRequirements lhs_hlos_and_reqs,"
        },
        {
            "sha": "a0a0e9f759e5920e413a1e71caf2e8811398b89c",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_fusion_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4512a9251b234a4d1d59a3ee1ba5eeb3f17c4b9c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4512a9251b234a4d1d59a3ee1ba5eeb3f17c4b9c/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_fusion_test.cc?ref=4512a9251b234a4d1d59a3ee1ba5eeb3f17c4b9c",
            "patch": "@@ -264,7 +264,8 @@ ENTRY e {\n   EXPECT_FALSE(GemmFusion(cc).Run(module.get()).value());\n }\n \n-TEST_F(GemmFusionTest, DynamicSliceIsFused) {\n+// TODO(b/417172838): support dynamic slice op.\n+TEST_F(GemmFusionTest, DISABLED_DynamicSliceIsFused) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(R\"(\n ENTRY e {\n@@ -288,7 +289,8 @@ ENTRY e {\n                                     m::Parameter(), m::Constant()))));\n }\n \n-TEST_F(GemmFusionTest, DynamicSlicesAreFusedEvenIfTheyShareIndices) {\n+// TODO(b/417172838): support dynamic slice op.\n+TEST_F(GemmFusionTest, DISABLED_DynamicSlicesAreFusedEvenIfTheyShareIndices) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(R\"(\n ENTRY e {\n@@ -319,7 +321,8 @@ ENTRY e {\n                             m::Parameter(), m::Parameter()))));\n }\n \n-TEST_F(GemmFusionTest, DoNotFuseDynamicSliceOfNonMajorFragments) {\n+// TODO(b/417172838): support dynamic slice op.\n+TEST_F(GemmFusionTest, DISABLED_DoNotFuseDynamicSliceOfNonMajorFragments) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(R\"(\n ENTRY e {\n@@ -338,7 +341,9 @@ ENTRY e {\n   EXPECT_FALSE(GemmFusion(cc).Run(module.get()).value());\n }\n \n-TEST_F(GemmFusionTest, CanFuseDynamicSliceOfContractingDimIfItIsMajor) {\n+// TODO(b/417172838): support dynamic slice op.\n+TEST_F(GemmFusionTest,\n+       DISABLED_CanFuseDynamicSliceOfContractingDimIfItIsMajor) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(R\"(\n ENTRY e {"
        }
    ],
    "stats": {
        "total": 174,
        "additions": 118,
        "deletions": 56
    }
}