{
    "author": "sohaibiftikhar",
    "message": "[XLA:GPU] Handle non-slice arguments for emitted kernels.\n\nSome arguments to kernels may not be managed by the buffer assignments and\nconsequently have no buffer slices attached to them. Examples of these include\nscalars and arguments whose memory is managed by the runtime thunks\n[CollectiveKernelThunk]. This change introduces a new type for arguments to be\npassed in with a shape but without an associated slice.\n\nPiperOrigin-RevId: 837470085",
    "sha": "6861f26f3c2193455bf97e919d9e987e74e6bb32",
    "files": [
        {
            "sha": "657827d4d97b6343560bb06ed220760423a35fc9",
            "filename": "third_party/xla/xla/backends/gpu/codegen/fusion_emitter.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 6,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ffusion_emitter.cc?ref=6861f26f3c2193455bf97e919d9e987e74e6bb32",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n \n #include <string>\n+#include <utility>\n #include <vector>\n \n #include \"absl/log/check.h\"\n@@ -38,8 +39,6 @@ limitations under the License.\n #include \"llvm/Support/Casting.h\"\n #include \"llvm/TargetParser/Triple.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"mlir/IR/AffineExpr.h\"\n-#include \"mlir/IR/AffineMap.h\"\n #include \"xla/codegen/emitters/kernel_api_builder.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n #include \"xla/hlo/analysis/indexing_map.h\"\n@@ -125,10 +124,28 @@ absl::StatusOr<llvm::Function*> BuildKernelPrototypeFromUniqueName(\n   llvm::LLVMContext& context = llvm_module->getContext();\n   // Explicitly set global addrspace for SPIR backend.\n   int addrspace = llvm::Triple(llvm_module->getTargetTriple()).isSPIR() ? 1 : 0;\n+  std::vector<llvm::Type*> kernel_args;\n+  kernel_args.reserve(arguments.args().size());\n+  for (const auto& arg : arguments.args()) {\n+    // Handle pointer arguments.\n+    // Either managed OR unmanaged with non-scalar shape.\n+    if (arg.kind() == emitters::KernelArgument::Kind::kManaged ||\n+        !arg.shape().dimensions().empty()) {\n+      kernel_args.push_back(builder->getPtrTy(addrspace));\n+      continue;\n+    }\n+    // Handle scalars.\n+    llvm::Type* ir_type =\n+        llvm_ir::PrimitiveTypeToIrType(arg.shape().element_type(), context);\n+    if (!ir_type) {\n+      return absl::InvalidArgumentError(\n+          absl::StrCat(\"Unsupported scalar type: \",\n+                       PrimitiveType_Name(arg.shape().element_type())));\n+    }\n+    kernel_args.push_back(ir_type);\n+  }\n   llvm::FunctionType* kernel_type = llvm::FunctionType::get(\n-      /*Result=*/llvm::Type::getVoidTy(context),\n-      std::vector<llvm::Type*>(arguments.args().size(),\n-                               builder->getPtrTy(addrspace)),\n+      /*Result=*/llvm::Type::getVoidTy(context), std::move(kernel_args),\n       /*isVarArg=*/false);\n   llvm::Function* kernel =\n       llvm::Function::Create(kernel_type, llvm::GlobalValue::ExternalLinkage,\n@@ -166,7 +183,8 @@ absl::StatusOr<llvm::Function*> BuildKernelPrototypeFromUniqueName(\n     if (impl_arg && impl_arg->hasAttribute(llvm::Attribute::Alignment)) {\n       kernel->addParamAttr(arg_idx,\n                            impl_arg->getAttribute(llvm::Attribute::Alignment));\n-    } else {\n+    } else if (kernel_argument.alignment() > 0) {\n+      // Scalars don't need an alignment attribute.\n       kernel->addParamAttr(arg_idx,\n                            llvm::Attribute::get(llvm_arg.getContext(),\n                                                 llvm::Attribute::Alignment,"
        },
        {
            "sha": "ca7438dee5ce5eae270757fcf1d0b1a9c743f010",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=6861f26f3c2193455bf97e919d9e987e74e6bb32",
            "patch": "@@ -44,7 +44,6 @@ cc_library(\n         \"//xla/backends/gpu/runtime:kernel_thunk\",\n         \"//xla/backends/gpu/runtime:thunk\",\n         \"//xla/codegen/emitters:kernel_arguments\",\n-        \"//xla/codegen/tiling:tiled_hlo_computation\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service/gpu:backend_configs_cc\","
        },
        {
            "sha": "c044577abc149f25830eec850d92e480ab462ff4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter.cc",
            "status": "modified",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc?ref=6861f26f3c2193455bf97e919d9e987e74e6bb32",
            "patch": "@@ -292,6 +292,37 @@ absl::StatusOr<TensorValue> EmitAllReduce(\n   return accumulator;\n }\n \n+absl::StatusOr<std::vector<Shape>> GetAllReduceUnmanagedKernelArguments(\n+    const HloComputation* computation,\n+    const HloAllReduceInstruction* all_reduce) {\n+  const int32_t num_devices = all_reduce->device_list().num_devices_per_group();\n+  std::vector<Shape> unmanaged_arguments;\n+  unmanaged_arguments.reserve(computation->num_parameters() +\n+                              kNumCollectiveMetadataArgs);\n+\n+  // rank and signal_value\n+  unmanaged_arguments.push_back(ShapeUtil::MakeShape(S32, {}));\n+  unmanaged_arguments.push_back(ShapeUtil::MakeShape(S32, {}));\n+  // The shape for signal and scratch buffers does not really matter in the end\n+  // because this would just be a pointer. For documentation purposes we add\n+  // the correct shape which would be\n+  // - num_devices * num_blocks for the signal buffer.\n+  // - num_devices * shape of the parameter for scratch buffers.\n+  // Since number of blocks is not known in this context we use a constant.\n+  static constexpr int32_t kMaxBlocksPerGrid = 24;\n+  unmanaged_arguments.push_back(\n+      ShapeUtil::MakeShape(S32, {num_devices, kMaxBlocksPerGrid}));\n+  // Scratch buffers\n+  for (const HloInstruction* instr : computation->parameter_instructions()) {\n+    Shape shape =\n+        ShapeUtil::InsertDimensionAtIndex(instr->shape(), 0, num_devices);\n+    unmanaged_arguments.push_back(shape);\n+  }\n+  TF_RET_CHECK(unmanaged_arguments.size() ==\n+               computation->num_parameters() + kNumCollectiveMetadataArgs);\n+  return unmanaged_arguments;\n+}\n+\n }  // namespace\n \n absl::StatusOr<std::optional<BlockLevelFusionConfig>>\n@@ -327,6 +358,19 @@ absl::StatusOr<bool> TrySetGpuBackendConfigForCollective(\n   return true;\n }\n \n+absl::StatusOr<std::vector<Shape>> GetCollectiveUnmanagedKernelArguments(\n+    const HloFusionInstruction* fusion) {\n+  const HloComputation* computation = fusion->fused_instructions_computation();\n+  const HloInstruction* root = computation->root_instruction();\n+  switch (root->opcode()) {\n+    case HloOpcode::kAllReduceStart:\n+      return GetAllReduceUnmanagedKernelArguments(\n+          computation, Cast<HloAllReduceInstruction>(root));\n+    default:\n+      return std::vector<Shape>();\n+  }\n+}\n+\n absl::StatusOr<int32_t> AddCollectiveMetadataArguments(\n     llvm::SmallVector<mlir::Type>& fn_arg_types, EmitterLocOpBuilder& b,\n     const HloComputation* hlo_computation) {"
        },
        {
            "sha": "105e47c53c228803ecd833aa6018dab6a171e802",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter.h",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.h?ref=6861f26f3c2193455bf97e919d9e987e74e6bb32",
            "patch": "@@ -18,6 +18,7 @@ limitations under the License.\n \n #include <cstdint>\n #include <optional>\n+#include <vector>\n \n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/statusor.h\"\n@@ -32,6 +33,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/shape.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/types.h\"  // IWYU pragma: keep\n \n@@ -64,6 +66,11 @@ absl::StatusOr<int32_t> AddCollectiveMetadataArguments(\n     llvm::SmallVector<mlir::Type>& fn_arg_types, EmitterLocOpBuilder& b,\n     const HloComputation* hlo_computation);\n \n+// Version of [AddCollectiveMetadataArguments] that does the same for\n+// [emitters::KernelArgument] structure.\n+absl::StatusOr<std::vector<Shape>> GetCollectiveUnmanagedKernelArguments(\n+    const HloFusionInstruction* fusion);\n+\n // Emits tiled XTile/Triton IR for a collective op.\n // See [EmitTiledHloInstruction] for an overview of how this fits into the\n // emitter."
        },
        {
            "sha": "99f8201d630bbff7d20e6acbb6bd1d2daebf152a",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter_test.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc?ref=6861f26f3c2193455bf97e919d9e987e74e6bb32",
            "patch": "@@ -219,6 +219,25 @@ TEST_F(CollectiveEmitterTest, AllReduceBlockLevelConfigNoReplicaGroups) {\n   EXPECT_EQ(block_level_config, std::nullopt);\n }\n \n+TEST_F(CollectiveEmitterTest, AllReduceGetCollectiveUnmanagedKernelArguments) {\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      const auto module_with_fusion,\n+      BuildModuleWithFusion(GetModuleStr(ShapeUtil::MakeShape(F32, {65536}))));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      const auto unmanaged_arguments,\n+      GetCollectiveUnmanagedKernelArguments(module_with_fusion.FusionInstr()));\n+  ASSERT_EQ(unmanaged_arguments.size(), 4);\n+  EXPECT_EQ(unmanaged_arguments[0].dimensions().size(), 0);\n+  EXPECT_EQ(unmanaged_arguments[1].dimensions().size(), 0);\n+  // num_devices x input_shape\n+  ASSERT_EQ(unmanaged_arguments[2].dimensions().size(), 2);\n+  EXPECT_EQ(unmanaged_arguments[2].dimensions()[0], 2);  // num_devices\n+\n+  ASSERT_EQ(unmanaged_arguments[3].dimensions().size(), 2);\n+  EXPECT_EQ(unmanaged_arguments[3].dimensions()[0], 2);      // num_devices\n+  EXPECT_EQ(unmanaged_arguments[3].dimensions()[1], 65536);  // input_shape[0]\n+}\n+\n TEST_F(CollectiveEmitterTest, AllReduceWithTritonGetLaunchConfig) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<ModuleWithEmitter> result_ptr,"
        },
        {
            "sha": "8f66d9827c70917ba6a6c0324460a793ea1ae214",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=6861f26f3c2193455bf97e919d9e987e74e6bb32",
            "patch": "@@ -110,13 +110,22 @@ TritonFusion::GenerateTritonKernelAndWrapper(\n absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n     IrEmitterContext& ir_emitter_context,\n     const HloFusionInstruction& fusion) const {\n+  return Emit(ir_emitter_context, fusion, nullptr, {});\n+}\n+\n+absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n+    IrEmitterContext& ir_emitter_context, const HloFusionInstruction& fusion,\n+    const HloInstruction* instr_override,\n+    absl::Span<const Shape> unmanaged_arguments) const {\n   llvm::IRBuilder builder(ir_emitter_context.llvm_module()->getContext());\n   VLOG(3) << fusion.ToString();\n   std::string suggested_kernel_name = std::string(fusion.name());\n   TF_ASSIGN_OR_RETURN(\n       auto kernel_arguments,\n-      emitters::KernelArguments::Create(ir_emitter_context.buffer_assignment(),\n-                                        GetDefaultBufferAlignment(), &fusion));\n+      emitters::KernelArguments::Create(\n+          ir_emitter_context.buffer_assignment(), GetDefaultBufferAlignment(),\n+          instr_override != nullptr ? instr_override : &fusion,\n+          unmanaged_arguments));\n \n   const HloComputation* hlo_computation =\n       fusion.fused_instructions_computation();"
        },
        {
            "sha": "b0320eb1a16e30fe220524e2adbde816fe61dbee",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.h",
            "status": "modified",
            "additions": 22,
            "deletions": 1,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.h?ref=6861f26f3c2193455bf97e919d9e987e74e6bb32",
            "patch": "@@ -20,13 +20,15 @@ limitations under the License.\n \n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n #include \"llvm/IR/Module.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n-#include \"xla/codegen/tiling/tiled_hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/gpu/ir_emitter_context.h\"\n #include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/shape.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n \n@@ -47,6 +49,25 @@ class TritonFusion : public FusionInterface {\n       IrEmitterContext& ir_emitter_context,\n       const HloFusionInstruction& fusion) const final;\n \n+  // Overload of [Emit] that allows passing overrides for instructions\n+  // and unmanaged arguments.\n+  // - Instruction overloads are required when we forcibly form fusions for\n+  // instructions by extracting them into a separate HLO module. In this case\n+  // buffer assignments are still associated with the original instruction.\n+  // So the root of the fusion cannot be used to determine the emitted kernel\n+  // arguments.\n+  // - The unmanaged arguments are used for collectives which have extra\n+  // arguments (such as pointers to remote buffers, and metadata arguments).\n+  // Empty unmanaged arguments mean that all arguments have buffer slices\n+  // associated with them.\n+  //\n+  // TODO(b/461717780): Remove the instruction override once we form collective\n+  // based fusions earlier in the compiler pipeline.\n+  absl::StatusOr<FusionEmissionResult> Emit(\n+      IrEmitterContext& ir_emitter_context, const HloFusionInstruction& fusion,\n+      const HloInstruction* instr_override,\n+      absl::Span<const Shape> unmanaged_arguments) const;\n+\n   // Returns the launch config for Triton fusions that have a block level fusion\n   // config.\n   // Not supported for MatMul fusions yet."
        },
        {
            "sha": "3bf04998441f0b2e69da92f447ecb4679612a903",
            "filename": "third_party/xla/xla/codegen/emitters/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2FBUILD?ref=6861f26f3c2193455bf97e919d9e987e74e6bb32",
            "patch": "@@ -237,6 +237,7 @@ xla_cc_test(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n )"
        },
        {
            "sha": "2b61fad0714bacbb30305852347a0da465fba3f2",
            "filename": "third_party/xla/xla/codegen/emitters/kernel_arguments.cc",
            "status": "modified",
            "additions": 36,
            "deletions": 11,
            "changes": 47,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_arguments.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_arguments.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_arguments.cc?ref=6861f26f3c2193455bf97e919d9e987e74e6bb32",
            "patch": "@@ -17,7 +17,6 @@ limitations under the License.\n #include <cstddef>\n #include <cstdint>\n #include <iterator>\n-#include <optional>\n #include <utility>\n #include <vector>\n \n@@ -61,6 +60,12 @@ void FillKernelArgumentAttributes(\n \n   for (int64_t i = 0; i < kernel_arguments.size(); ++i) {\n     KernelArgument& kernel_argument = kernel_arguments[i];\n+    if (kernel_argument.kind() == KernelArgument::Kind::kUnmanaged) {\n+      if (kernel_argument.shape().dimensions().empty()) {\n+        kernel_argument.set_alignment(0);  // scalars have no alignment\n+      }\n+      continue;\n+    }\n \n     auto& first_index = first_indices_for_slices[kernel_argument.slice()];\n     if (first_index.has_value()) {\n@@ -93,7 +98,8 @@ void FillKernelArgumentAttributes(\n \n     kernel_argument.set_aliased(kernel_argument.written() && [&] {\n       for (size_t j = 0; j < kernel_arguments.size(); ++j) {\n-        if (i == j) {\n+        if (i == j ||\n+            kernel_arguments[j].kind() == KernelArgument::Kind::kUnmanaged) {\n           continue;\n         }\n \n@@ -138,13 +144,11 @@ absl::StatusOr<OutputArguments> ExtractOutputArguments(\n       }));\n   return result;\n }\n-\n-}  // namespace\n-\n-absl::StatusOr<KernelArguments> KernelArguments::Create(\n+absl::StatusOr<KernelArguments> CreateKernelArguments(\n     const BufferAssignment& buffer_assignment,\n-    const BufferAlignment& buffer_alignment,\n-    const HloInstruction* hlo_instruction) {\n+    const KernelArguments::BufferAlignment& buffer_alignment,\n+    const HloInstruction* hlo_instruction,\n+    absl::Span<const Shape> unmanaged_arguments) {\n   std::vector<KernelArgument> kernel_arguments;\n   for (const HloInstruction* operand : hlo_instruction->operands()) {\n     TF_ASSIGN_OR_RETURN(BufferAllocation::Slice slice,\n@@ -158,21 +162,42 @@ absl::StatusOr<KernelArguments> KernelArguments::Create(\n \n   absl::c_move(output_result.output_arguments,\n                std::back_inserter(kernel_arguments));\n+  for (const Shape& unmanaged_argument : unmanaged_arguments) {\n+    kernel_arguments.emplace_back(unmanaged_argument);\n+  }\n   FillKernelArgumentAttributes(kernel_arguments, buffer_alignment,\n                                output_result.buffers_written);\n-\n   return KernelArguments(std::move(kernel_arguments));\n }\n \n+}  // namespace\n+\n+absl::StatusOr<KernelArguments> KernelArguments::Create(\n+    const BufferAssignment& buffer_assignment,\n+    const BufferAlignment& buffer_alignment,\n+    const HloInstruction* hlo_instruction) {\n+  return CreateKernelArguments(buffer_assignment, buffer_alignment,\n+                               hlo_instruction, {});\n+}\n+\n+absl::StatusOr<KernelArguments> KernelArguments::Create(\n+    const BufferAssignment& buffer_assignment,\n+    const BufferAlignment& buffer_alignment,\n+    const HloInstruction* hlo_instruction,\n+    absl::Span<const Shape> unmanaged_arguments) {\n+  return CreateKernelArguments(buffer_assignment, buffer_alignment,\n+                               hlo_instruction, unmanaged_arguments);\n+}\n+\n absl::StatusOr<KernelArguments> KernelArguments::Create(\n     const BufferAssignment& buffer_assignment,\n     const BufferAlignment& buffer_alignment,\n     const HloInstruction* hlo_instruction,\n     absl::Span<const int32_t> interleaved_output_indices) {\n   if (interleaved_output_indices.empty()) {\n     // Fall back to regular Create method when no interleaving is requested\n-    return KernelArguments::Create(buffer_assignment, buffer_alignment,\n-                                   hlo_instruction);\n+    return CreateKernelArguments(buffer_assignment, buffer_alignment,\n+                                 hlo_instruction, {});\n   }\n \n   const auto& operands = hlo_instruction->operands();"
        },
        {
            "sha": "c6fae6681c8c61b8e74d6ea19b6a4e98f6f1d4b3",
            "filename": "third_party/xla/xla/codegen/emitters/kernel_arguments.h",
            "status": "modified",
            "additions": 35,
            "deletions": 1,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_arguments.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_arguments.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_arguments.h?ref=6861f26f3c2193455bf97e919d9e987e74e6bb32",
            "patch": "@@ -31,14 +31,29 @@ namespace xla::emitters {\n // Thread-safe.\n class KernelArgument {\n  public:\n+  // Managed arguments are those that are assigned slices during the buffer\n+  // assignment pass.\n+  // Unmanaged arguments can be scalars of tensors. Scalars are simply passed\n+  // by value. For buffers the memory is managed by the runtime thunk.\n+  // The[KernelThunk] currently assumes that all arguments are managed.\n+  // The [CollectiveKernelThunk] distinguishes between the two types of\n+  // arguments because it manages scratch buffers for the collectives itself.\n+  enum class Kind { kManaged, kUnmanaged };\n+\n   KernelArgument(Shape shape, BufferAllocation::Slice slice)\n-      : shape_(shape), slice_(slice) {}\n+      : kind_(Kind::kManaged), shape_(shape), slice_(slice) {}\n+  // Constructor for arguments that don't have an associated slice.\n+  explicit KernelArgument(Shape shape)\n+      : kind_(Kind::kUnmanaged), shape_(shape) {}\n+\n+  Kind kind() const { return kind_; }\n   const Shape& shape() const { return shape_; }\n   const BufferAllocation::Slice& slice() const { return slice_; }\n \n   bool written() const { return written_; }\n   void set_written(bool written) { written_ = written; }\n \n+  // An alignment of 0 means that the alignment attribute shouldn't be set.\n   int64_t alignment() const { return alignment_; }\n   void set_alignment(int64_t alignment) { alignment_ = alignment; }\n \n@@ -49,6 +64,7 @@ class KernelArgument {\n   void set_slice_index(int64_t slice_index) { slice_index_ = slice_index; }\n \n  private:\n+  Kind kind_;\n   Shape shape_;\n   BufferAllocation::Slice slice_;\n   bool aliased_ = true;\n@@ -80,6 +96,15 @@ class KernelArguments {\n     int64_t constant_buffer_align_bytes;\n   };\n \n+  // Creates a KernelArguments object for the given HLO instruction.\n+  // The unmanaged_arguments are added to the end of the list of input/output\n+  // arguments.\n+  static absl::StatusOr<KernelArguments> Create(\n+      const BufferAssignment& buffer_assignment,\n+      const BufferAlignment& buffer_alignment,\n+      const HloInstruction* hlo_instruction,\n+      absl::Span<const Shape> unmanaged_arguments);\n+\n   static absl::StatusOr<KernelArguments> Create(\n       const BufferAssignment& buffer_assignment,\n       const BufferAlignment& buffer_alignment,\n@@ -130,6 +155,15 @@ class KernelArguments {\n     return output_flags;\n   }\n \n+  std::vector<KernelArgument::Kind> GetArgumentKinds() const {\n+    std::vector<KernelArgument::Kind> kinds;\n+    kinds.reserve(args_.size());\n+    for (const KernelArgument& arg : args_) {\n+      kinds.push_back(arg.kind());\n+    }\n+    return kinds;\n+  }\n+\n  private:\n   std::vector<KernelArgument> args_;\n };"
        },
        {
            "sha": "42c10f13a1718f217a45a7ccaeba48cc979d4fde",
            "filename": "third_party/xla/xla/codegen/emitters/kernel_arguments_test.cc",
            "status": "modified",
            "additions": 58,
            "deletions": 2,
            "changes": 60,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_arguments_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_arguments_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Fkernel_arguments_test.cc?ref=6861f26f3c2193455bf97e919d9e987e74e6bb32",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/status_matchers.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n #include \"xla/hlo/analysis/alias_info.h\"\n #include \"xla/hlo/analysis/hlo_ordering.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -137,7 +138,8 @@ ENTRY main {\n   // Test 1: Create regular (non-interleaved) arguments for baseline\n   TF_ASSERT_OK_AND_ASSIGN(\n       KernelArguments regular_args,\n-      KernelArguments::Create(*buffer_assignment, buffer_alignment, root, {}));\n+      KernelArguments::Create(*buffer_assignment, buffer_alignment, root,\n+                              absl::Span<const int32_t>{}));\n \n   // Test 2: Create interleaved arguments\n   // Expected order: input0, output0, input1, output1\n@@ -199,7 +201,8 @@ ENTRY main {\n   // Test 1: Create regular (non-interleaved) arguments for baseline\n   TF_ASSERT_OK_AND_ASSIGN(\n       KernelArguments regular_args,\n-      KernelArguments::Create(*buffer_assignment, buffer_alignment, root, {}));\n+      KernelArguments::Create(*buffer_assignment, buffer_alignment, root,\n+                              absl::Span<const int32_t>{}));\n \n   // Test 2: Create interleaved arguments - output at beginning (position 0)\n   // Expected order: output0, input0 (instead of input0, output0)\n@@ -314,5 +317,58 @@ ENTRY main {\n   ASSERT_EQ(kernel_args.args().size(), 3);  // 2 inputs + 1 output\n }\n \n+TEST_F(KernelArgumentsTest, UnmanagedArguments) {\n+  constexpr absl::string_view kHloString = R\"(\n+    HloModule module\n+\n+    ENTRY entry {\n+      param.0 = f32[1,2,3]{2,1,0} parameter(0)\n+      param.1 = f32[1,2,3]{2,1,0} parameter(1)\n+      ROOT add = f32[1,2,3]{2,1,0} add(param.0, param.1)\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloString));\n+  AliasInfo alias_info;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<BufferAssignment> assignment,\n+      BufferAssigner::Run(\n+          module.get(), std::make_unique<DependencyHloOrdering>(module.get()),\n+          &BufferSizeBytes, &alias_info, [](LogicalBuffer::Color) { return 0; },\n+          /*allocate_buffers_for_constants=*/true));\n+  // Input and output buffers are managed.\n+  EXPECT_THAT(assignment->Allocations(), SizeIs(3));\n+  auto unmanaged_arguments = std::vector{\n+      ShapeUtil::MakeShape(S32, {}), ShapeUtil::MakeShape(U32, {}),\n+      ShapeUtil::MakeShape(F32, {24}), ShapeUtil::MakeShape(F32, {65536})};\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto kernel_arguments,\n+      KernelArguments::Create(*assignment, gpu::GetDefaultBufferAlignment(),\n+                              module->entry_computation()->root_instruction(),\n+                              unmanaged_arguments));\n+  // 3 managed arguments + 4 unmanaged arguments.\n+  ASSERT_THAT(kernel_arguments.args(), SizeIs(7));\n+\n+  constexpr size_t kExpectedBufferSize = 1 * 2 * 3 * sizeof(float);\n+  EXPECT_THAT(\n+      kernel_arguments.GetArgumentBufferSlices(),\n+      ElementsAre(BufferAllocation::Slice(&assignment->Allocations()[1],\n+                                          /*offset=*/0, kExpectedBufferSize),\n+                  BufferAllocation::Slice(&assignment->Allocations()[2],\n+                                          /*offset=*/0, kExpectedBufferSize),\n+                  // The output is last in KernelArguments.\n+                  BufferAllocation::Slice(&assignment->Allocations()[0],\n+                                          /*offset=*/0, kExpectedBufferSize),\n+                  BufferAllocation::Slice(), BufferAllocation::Slice(),\n+                  BufferAllocation::Slice(), BufferAllocation::Slice()));\n+  constexpr auto kManaged = KernelArgument::Kind::kManaged;\n+  constexpr auto kUnmanaged = KernelArgument::Kind::kUnmanaged;\n+  EXPECT_THAT(kernel_arguments.GetArgumentKinds(),\n+              ElementsAre(kManaged, kManaged, kManaged, kUnmanaged, kUnmanaged,\n+                          kUnmanaged, kUnmanaged));\n+}\n+\n }  // namespace\n }  // namespace xla::emitters"
        },
        {
            "sha": "27b126b59b636267c8716464a69dcbfdeb12f16a",
            "filename": "third_party/xla/xla/service/buffer_assignment.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fservice%2Fbuffer_assignment.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6861f26f3c2193455bf97e919d9e987e74e6bb32/third_party%2Fxla%2Fxla%2Fservice%2Fbuffer_assignment.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fbuffer_assignment.h?ref=6861f26f3c2193455bf97e919d9e987e74e6bb32",
            "patch": "@@ -204,6 +204,9 @@ class BufferAllocation {\n     PrimitiveType element_type() const { return element_type_; }\n \n     bool operator==(const Slice& other) const {\n+      if (allocation_ == nullptr) {\n+        return other.allocation_ == nullptr;\n+      }\n       // We don't compare element_type_ because it's not always set, and it's\n       // not relevant for the comparison here.\n       return index() == other.index() && offset_ == other.offset_ &&"
        }
    ],
    "stats": {
        "total": 284,
        "additions": 260,
        "deletions": 24
    }
}