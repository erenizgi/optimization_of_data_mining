{
    "author": "ZixuanJiang",
    "message": "Do not `RewriteLayoutWithShardedShape` if the sharding is unreduced, similar to replicated/maximal/manual sharding.\n\nPiperOrigin-RevId: 829058519",
    "sha": "f3dfbd238dd49a9ff71176fe395e304574081824",
    "files": [
        {
            "sha": "72ffe8250e14ff4980286d36d16de63cdb797f0e",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/layout_util.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f3dfbd238dd49a9ff71176fe395e304574081824/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Flayout_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f3dfbd238dd49a9ff71176fe395e304574081824/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Flayout_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Flayout_util.cc?ref=f3dfbd238dd49a9ff71176fe395e304574081824",
            "patch": "@@ -37,7 +37,8 @@ absl::Status RewriteLayoutWithShardedShape(\n     const LayoutPreferenceFn& layout_preference_fn,\n     const ShapeRepresentationFn& shape_representation_fn,\n     xla::Shape* xla_shape) {\n-  if (sharding && !sharding->IsTileMaximal() && !sharding->IsManual()) {\n+  if (sharding && !sharding->IsTileMaximal() && !sharding->IsManual() &&\n+      !sharding->IsUnreduced()) {\n     // After sharding, per core shape might have different layout. For example,\n     // before sharding, a shape [128, 128] will be assigned default\n     // minor-to-major {1, 0}. But after we shard this shape to [128, 64] * 2,"
        }
    ],
    "stats": {
        "total": 3,
        "additions": 2,
        "deletions": 1
    }
}