{
    "author": "mooskagh",
    "message": "[XLA:GPU] Introduce a flag --xla_gpu_gemm_autotuner_override_file\n\nIn which it's possible to override a set of autotuner configs to use.\n\nPiperOrigin-RevId: 836616632",
    "sha": "66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06",
    "files": [
        {
            "sha": "b3ded36658852cc4ba55a000050577c771d1a7bf",
            "filename": "third_party/xla/xla/autotuning.proto",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fautotuning.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fautotuning.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fautotuning.proto?ref=66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06",
            "patch": "@@ -74,6 +74,7 @@ message AutotuneResult {\n   // If you don't need a proto in your code, please use TritonGemmConfig instead\n   // of using this proto directly.\n   message TritonGemmKey {\n+    // LINT.IfChange\n     int64 block_m = 1;\n     int64 block_n = 2;\n     int64 block_k = 3;\n@@ -83,6 +84,7 @@ message AutotuneResult {\n     int64 num_ctas = 7;\n     bool is_tma_allowed = 8;\n     bool is_warp_specialization_allowed = 9;\n+    // LINT.ThenChange(//tensorflow/compiler/xla/service/gpu/matmul_utils.h)\n   }\n \n   message CustomKernelFusionKey {\n@@ -110,6 +112,10 @@ message AutotuneResult {\n   }\n }\n \n+message TritonGemmConfigsProto {\n+  repeated AutotuneResult.TritonGemmKey config = 1;\n+}\n+\n message AutotuningLog {\n   google.protobuf.Any instr = 1;\n "
        },
        {
            "sha": "d857364d2b0aabfd5f22f5b26ef3d48edb55f9c2",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06",
            "patch": "@@ -2342,6 +2342,13 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n           &DebugOptions::set_xla_gpu_experimental_autotuner_cache_dir),\n       debug_options->xla_gpu_experimental_autotuner_cache_dir(),\n       \"Experimental: Specify the directory to read/write autotuner cache to.\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_gemm_autotuner_override_file\",\n+      string_setter_for(\n+          &DebugOptions::set_xla_gpu_gemm_autotuner_override_file),\n+      debug_options->xla_gpu_gemm_autotuner_override_file(),\n+      \"A textproto file to override autotune results. See also \"\n+      \"`xla_gpu_override_gemm_autotuner` to override with a single config.\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_enable_command_buffers_during_profiling\",\n       bool_setter_for("
        },
        {
            "sha": "3356924cb1adc44ae13e6805e130d20b5aa9dd37",
            "filename": "third_party/xla/xla/service/gpu/autotuning/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2FBUILD?ref=66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06",
            "patch": "@@ -287,6 +287,7 @@ xla_test(\n         \"@com_google_absl//absl/time\",\n         \"@com_google_googletest//:gtest\",\n         \"@llvm-project//mlir:IR\",\n+        \"@local_tsl//tsl/platform:env\",\n         \"@local_tsl//tsl/platform:path\",\n         \"@local_tsl//tsl/platform:platform_port\",\n     ],\n@@ -447,6 +448,7 @@ cc_library(\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\","
        },
        {
            "sha": "16885ec44f9728674e59a89587939227516ac50f",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_util.cc",
            "status": "modified",
            "additions": 30,
            "deletions": 7,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.cc?ref=66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06",
            "patch": "@@ -18,10 +18,13 @@ limitations under the License.\n #include <algorithm>\n #include <array>\n #include <cstdint>\n+#include <iterator>\n #include <optional>\n #include <string>\n #include <utility>\n+#include <vector>\n \n+#include \"absl/algorithm/container.h\"\n #include \"absl/base/const_init.h\"\n #include \"absl/base/thread_annotations.h\"\n #include \"absl/container/flat_hash_map.h\"\n@@ -294,7 +297,7 @@ TryFindInAllCacheTypes(const AutotuneCacheKey& key, absl::string_view cache_dir)\n }\n }  // namespace\n \n-AutotuneConfig AutotuneConfig::FromDebugOptions(\n+absl::StatusOr<AutotuneConfig> AutotuneConfig::FromDebugOptions(\n     const DeviceOrDevicelessConfig& config, const DebugOptions& opts) {\n   int autotune_level = opts.xla_gpu_autotune_level();\n \n@@ -314,12 +317,32 @@ AutotuneConfig AutotuneConfig::FromDebugOptions(\n   std::string autotune_cache_dir = opts.xla_gpu_per_fusion_autotune_cache_dir();\n   DebugOptions_AutotuneCacheMode autotune_cache_mode =\n       opts.xla_gpu_experimental_autotune_cache_mode();\n-  return AutotuneConfig(config, should_init_buffers,\n-                        should_reinit_output_buffer, should_check_correctness,\n-                        should_skip_wrong_results,\n-                        should_crash_on_check_failure, exhaustive_tiling_search,\n-                        should_require_complete_aot_autotune_results,\n-                        autotune_cache_dir, autotune_cache_mode);\n+\n+  std::optional<std::vector<AutotuneResult::TritonGemmKey>>\n+      gemm_config_overrides;\n+  const std::string& override_file =\n+      opts.xla_gpu_gemm_autotuner_override_file();\n+  if (!override_file.empty()) {\n+    std::string file_content;\n+    TF_RETURN_IF_ERROR(tsl::ReadFileToString(tsl::Env::Default(), override_file,\n+                                             &file_content));\n+    TritonGemmConfigsProto configs;\n+    if (!tsl::protobuf::TextFormat::ParseFromString(file_content, &configs)) {\n+      return absl::InvalidArgumentError(\n+          absl::StrCat(\"Could not parse override file: \", override_file));\n+    }\n+    gemm_config_overrides.emplace();\n+    absl::c_copy(configs.config(), std::back_inserter(*gemm_config_overrides));\n+    LOG(INFO) << \"Loaded \" << gemm_config_overrides->size()\n+              << \" gemm config overrides from \" << override_file;\n+  }\n+\n+  return AutotuneConfig(\n+      config, should_init_buffers, should_reinit_output_buffer,\n+      should_check_correctness, should_skip_wrong_results,\n+      should_crash_on_check_failure, exhaustive_tiling_search,\n+      should_require_complete_aot_autotune_results, autotune_cache_dir,\n+      autotune_cache_mode, gemm_config_overrides);\n }\n \n /*static*/ absl::StatusOr<bool> AutotunerUtil::IsInCache("
        },
        {
            "sha": "7cbf9df09aadf0e6ebf402f6eb9e7b1a7f4c5139",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_util.h",
            "status": "modified",
            "additions": 14,
            "deletions": 4,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util.h?ref=66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <optional>\n #include <string>\n #include <variant>\n+#include <vector>\n \n #include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n@@ -123,6 +124,10 @@ class AutotuneConfig {\n   const DebugOptions::AutotuneCacheMode& autotune_cache_mode() const {\n     return autotune_cache_mode_;\n   }\n+  const std::optional<std::vector<AutotuneResult::TritonGemmKey>>&\n+  gemm_config_overrides() const {\n+    return gemm_config_overrides_;\n+  }\n \n   AutotuneConfig(const DeviceOrDevicelessConfig& config,\n                  bool should_init_buffers, bool should_reinit_output_buffer,\n@@ -131,7 +136,9 @@ class AutotuneConfig {\n                  bool exhaustive_tiling_search,\n                  bool should_require_complete_aot_autotune_results,\n                  absl::string_view autotune_cache_dir,\n-                 DebugOptions::AutotuneCacheMode autotune_cache_mode)\n+                 DebugOptions::AutotuneCacheMode autotune_cache_mode,\n+                 std::optional<std::vector<AutotuneResult::TritonGemmKey>>\n+                     gemm_config_overrides)\n       : config_(config),\n         should_init_buffers_(should_init_buffers),\n         should_reinit_output_buffer_(should_reinit_output_buffer),\n@@ -142,11 +149,12 @@ class AutotuneConfig {\n         should_require_complete_aot_autotune_results_(\n             should_require_complete_aot_autotune_results),\n         autotune_cache_dir_(autotune_cache_dir),\n-        autotune_cache_mode_(autotune_cache_mode) {}\n+        autotune_cache_mode_(autotune_cache_mode),\n+        gemm_config_overrides_(gemm_config_overrides) {}\n \n   // Derives the autotune config parameters from the DebugOptions `opts`.\n-  static AutotuneConfig FromDebugOptions(const DeviceOrDevicelessConfig& config,\n-                                         const DebugOptions& opts);\n+  static absl::StatusOr<AutotuneConfig> FromDebugOptions(\n+      const DeviceOrDevicelessConfig& config, const DebugOptions& opts);\n \n   se::StreamExecutor* GetExecutor() const { return config_.GetExecutor(); }\n \n@@ -181,6 +189,8 @@ class AutotuneConfig {\n   bool should_require_complete_aot_autotune_results_;\n   std::string autotune_cache_dir_;\n   DebugOptions::AutotuneCacheMode autotune_cache_mode_;\n+  std::optional<std::vector<AutotuneResult::TritonGemmKey>>\n+      gemm_config_overrides_;\n };\n \n using AutotuneNoCacheFn = std::function<absl::StatusOr<AutotuneResult>()>;"
        },
        {
            "sha": "505d52e5c1673b3f1833ff863ceb91ef6fbb803c",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotuner_util_test.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 12,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotuner_util_test.cc?ref=66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06",
            "patch": "@@ -78,17 +78,17 @@ static constexpr absl::string_view kDeviceDescriptionTextProto = R\"pb(\n static constexpr absl::string_view kDotFusionHloText = R\"hlo(\n     HloModule module\n     fused_computation {\n-          tmp_0 = f16[1,16,17,3]{3,2,1,0} parameter(0) \n+          tmp_0 = f16[1,16,17,3]{3,2,1,0} parameter(0)\n           tmp_1 = f16[16,51]{1,0} bitcast(f16[1,16,17,3]{3,2,1,0} tmp_0)\n           tmp_2 = s8[16,17,3]{2,1,0} parameter(1)\n           tmp_3 = s8[51,16]{0,1} bitcast(s8[16,17,3]{2,1,0} tmp_2)\n           tmp_4 = f16[51,16]{0,1} convert(s8[51,16]{0,1} tmp_3)\n           tmp_5 = f16[16,16]{1,0} dot(f16[16,51]{1,0} tmp_1, f16[51,16]{0,1} tmp_4), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n           ROOT tmp_6 = f16[1,16,16]{2,1,0} bitcast(f16[16,16]{1,0} tmp_5)\n     }\n-    \n+\n     ENTRY main {\n-          p0 = f16[1,16,17,3]{3,2,1,0} parameter(0) \n+          p0 = f16[1,16,17,3]{3,2,1,0} parameter(0)\n           p1 = s8[16,17,3]{2,1,0} parameter(1)\n           ROOT fusion = f16[1,16,16]{2,1,0} fusion(p0, p1), kind=kCustom, calls=fused_computation\n     }\n@@ -225,8 +225,10 @@ TEST_F(AutotunerUtilTest, LoadAutotuneResultsFromFile_TextProto1) {\n   auto options = DebugOptions();\n   options.set_xla_gpu_require_complete_aot_autotune_results(true);\n   stream_executor::StreamExecutor* executor = NewStreamExecutor();\n-  AutotuneConfig config = AutotuneConfig::FromDebugOptions(\n-      DeviceOrDevicelessConfig{DeviceConfig{executor}}, options);\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      AutotuneConfig config,\n+      AutotuneConfig::FromDebugOptions(\n+          DeviceOrDevicelessConfig{DeviceConfig{executor}}, options));\n \n   EXPECT_THAT(AutotunerUtil::IsInCache(key, config),\n               absl_testing::IsOkAndHolds(true))\n@@ -278,8 +280,10 @@ TEST_F(AutotunerUtilTest, FailIfRequireCompleteAotAutotuning) {\n   stream_executor::StreamExecutor* executor = NewStreamExecutor();\n   auto options = DebugOptions();\n   options.set_xla_gpu_require_complete_aot_autotune_results(true);\n-  AutotuneConfig config = AutotuneConfig::FromDebugOptions(\n-      DeviceOrDevicelessConfig{DeviceConfig{executor}}, options);\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      AutotuneConfig config,\n+      AutotuneConfig::FromDebugOptions(\n+          DeviceOrDevicelessConfig{DeviceConfig{executor}}, options));\n   absl::Status s = AutotunerUtil::Autotune(instruction, config, [&] {\n                      return AutotuneResult();\n                    }).status();\n@@ -307,8 +311,10 @@ TEST_F(AutotunerUtilTest, OkIfJitAutotuningDisabledButAlreadyLoadedAOT) {\n \n   {\n     // By default, JIT autotuning is OK.\n-    AutotuneConfig config = AutotuneConfig::FromDebugOptions(\n-        DeviceOrDevicelessConfig{DeviceConfig{executor}}, DebugOptions());\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        AutotuneConfig config,\n+        AutotuneConfig::FromDebugOptions(\n+            DeviceOrDevicelessConfig{DeviceConfig{executor}}, DebugOptions()));\n     TF_EXPECT_OK(AutotunerUtil::Autotune(instruction, config, [&] {\n                    return AutotuneResult();\n                  }).status());\n@@ -320,8 +326,10 @@ TEST_F(AutotunerUtilTest, OkIfJitAutotuningDisabledButAlreadyLoadedAOT) {\n   auto options = DebugOptions();\n   options.set_xla_gpu_require_complete_aot_autotune_results(true);\n \n-  AutotuneConfig config = AutotuneConfig::FromDebugOptions(\n-      DeviceOrDevicelessConfig{DeviceConfig{executor}}, options);\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      AutotuneConfig config,\n+      AutotuneConfig::FromDebugOptions(\n+          DeviceOrDevicelessConfig{DeviceConfig{executor}}, options));\n   // Even though JIT autotuning is disabled, there is no cache miss when running\n   // autotuning for the same entry, so no error should be raised either.\n   TF_EXPECT_OK(AutotunerUtil::Autotune(instruction, config, [&] {\n@@ -395,7 +403,8 @@ class FileBasedCacheTest : public AutotunerUtilTest {\n         /*exhaustive_tiling_search=*/true,\n         /*should_require_complete_aot_autotune_results=*/false,\n         /*autotune_cache_dir=*/cache_dir_,\n-        /*autotune_cache_mode=*/GetCacheMode());\n+        /*autotune_cache_mode=*/GetCacheMode(),\n+        /*gemm_config_overrides=*/std::nullopt);\n   }\n \n   AutotuneCacheKey GetCacheKey() const {"
        },
        {
            "sha": "d8e9450def4b3b324682301ec9bd27e37efdee69",
            "filename": "third_party/xla/xla/service/gpu/autotuning/conv_algorithm_picker_test.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fconv_algorithm_picker_test.cc?ref=66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06",
            "patch": "@@ -88,8 +88,10 @@ ENTRY main {\n   changed = false;\n   DebugOptions opts = DefaultDebugOptionsIgnoringFlags();\n \n-  AutotuneConfig cfg = AutotuneConfig::FromDebugOptions(\n-      DeviceOrDevicelessConfig{DeviceConfig{stream_exec, nullptr}}, opts);\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      AutotuneConfig cfg,\n+      AutotuneConfig::FromDebugOptions(\n+          DeviceOrDevicelessConfig{DeviceConfig{stream_exec, nullptr}}, opts));\n   TF_ASSERT_OK_AND_ASSIGN(changed,\n                           RunHloPass(GpuConvAlgorithmPicker(cfg), m.get()));\n   ASSERT_TRUE(changed);\n@@ -200,8 +202,10 @@ ENTRY main {\n   ASSERT_TRUE(changed);\n \n   DebugOptions opts = DefaultDebugOptionsIgnoringFlags();\n-  AutotuneConfig cfg = AutotuneConfig::FromDebugOptions(\n-      DeviceOrDevicelessConfig{DeviceConfig{stream_exec, nullptr}}, opts);\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      AutotuneConfig cfg,\n+      AutotuneConfig::FromDebugOptions(\n+          DeviceOrDevicelessConfig{DeviceConfig{stream_exec, nullptr}}, opts));\n   TF_ASSERT_OK_AND_ASSIGN(changed,\n                           RunHloPass(GpuConvAlgorithmPicker(cfg), m.get()));\n   ASSERT_TRUE(changed);"
        },
        {
            "sha": "e67c01ca72a518664c8f98cc72bcc1e7fce9102b",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner.cc?ref=66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06",
            "patch": "@@ -1020,7 +1020,17 @@ GemmFusionAutotunerImpl::GenerateTritonConfigs(const HloDotInstruction& dot) {\n       /*autotune_tma=*/autotune_tma,\n       /*autotune_warp_specialization=*/autotune_warp_specialization);\n \n-  if (!debug_options_.xla_gpu_exhaustive_tiling_search()) {\n+  if (auto overrides = config_.gemm_config_overrides(); overrides.has_value()) {\n+    VLOG(1) << \"Restricting configs to the overridden set.\";\n+    std::vector<TritonGemmConfig> allowed_configs;\n+    for (const AutotuneResult::TritonGemmKey& key : *overrides) {\n+      TF_ASSIGN_OR_RETURN(TritonGemmConfig config,\n+                          TritonGemmConfig::FromProto(key));\n+      allowed_configs.push_back(std::move(config));\n+    }\n+    configs =\n+        search_space.OptimizeConfigSet(configs, /*hints=*/allowed_configs);\n+  } else if (!debug_options_.xla_gpu_exhaustive_tiling_search()) {\n     VLOG(1) << \"Restricting configs to the default set.\";\n     configs = search_space.OptimizeConfigSet(\n         configs, /*hints=*/GetDefaultTritonConfigs());"
        },
        {
            "sha": "ed4a2e392df4f8d9b6e8ca46b7bd0eeba5bf8d16",
            "filename": "third_party/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner_test.cc",
            "status": "modified",
            "additions": 97,
            "deletions": 29,
            "changes": 126,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fgemm_fusion_autotuner_test.cc?ref=66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06",
            "patch": "@@ -209,8 +209,10 @@ class StatelessAutotunerTest : public HloTestBase {\n \n     DeviceConfig test_config{backend().default_stream_executor(),\n                              backend().memory_allocator()};\n-    AutotuneConfig autotune_config = AutotuneConfig::FromDebugOptions(\n-        DeviceOrDevicelessConfig{test_config}, debug_options);\n+    TF_ASSIGN_OR_RETURN(\n+        AutotuneConfig autotune_config,\n+        AutotuneConfig::FromDebugOptions(DeviceOrDevicelessConfig{test_config},\n+                                         debug_options));\n     GemmFusionAutotunerImpl autotuner(autotune_config, toolkit_version,\n                                       debug_options, nullptr, mlir_context);\n     return autotuner.GenerateConfigs(fusion);\n@@ -242,8 +244,10 @@ class StatelessAutotunerTest : public HloTestBase {\n   GetPossibleMatmulAutotuneConfigs(const HloModule& module) {\n     DeviceConfig device_config{backend().default_stream_executor(),\n                                backend().memory_allocator()};\n-    AutotuneConfig autotune_config = AutotuneConfig::FromDebugOptions(\n-        DeviceOrDevicelessConfig{device_config}, GetDebugOptionsForTest());\n+    TF_ASSIGN_OR_RETURN(\n+        AutotuneConfig autotune_config,\n+        AutotuneConfig::FromDebugOptions(\n+            DeviceOrDevicelessConfig{device_config}, GetDebugOptionsForTest()));\n     GemmFusionAutotunerImpl autotuner(autotune_config, GetToolkitVersion(),\n                                       GetDebugOptionsForTest(), nullptr,\n                                       &mlir_context_);\n@@ -377,13 +381,14 @@ class GemmFusionAutotunerTest : public StatelessAutotunerTest {\n                                         tsl::port::MaxParallelism());\n     DebugOptions opts;\n     MultiProcessKeyValueStore key_value_store;\n-    pipeline.AddPass<GemmFusionAutotuner>(\n-        AutotuneConfig::FromDebugOptions(\n-            DeviceOrDevicelessConfig{\n-                DeviceConfig{backend().default_stream_executor(),\n-                             backend().memory_allocator()}},\n-            opts),\n-        GetToolkitVersion(), &thread_pool, key_value_store, &mlir_context_);\n+    absl::StatusOr<AutotuneConfig> config = AutotuneConfig::FromDebugOptions(\n+        DeviceOrDevicelessConfig{DeviceConfig{\n+            backend().default_stream_executor(), backend().memory_allocator()}},\n+        opts);\n+    CHECK_OK(config.status());\n+    pipeline.AddPass<GemmFusionAutotuner>(*config, GetToolkitVersion(),\n+                                          &thread_pool, key_value_store,\n+                                          &mlir_context_);\n \n     RunAndFilecheckHloRewrite(\n         hlo, std::move(pipeline), expected, [](const HloModule* m) {\n@@ -432,8 +437,10 @@ GetPossibleMatmulAutotuneTritonConfigs(\n   device_description.set_threads_per_warp(32);\n   device_description.set_shared_memory_per_block_optin(227 * 1024);\n   DevicelessConfig test_config = {device_description};\n-  AutotuneConfig autotune_config = AutotuneConfig::FromDebugOptions(\n-      DeviceOrDevicelessConfig{test_config}, debug_options);\n+  TF_ASSIGN_OR_RETURN(\n+      AutotuneConfig autotune_config,\n+      AutotuneConfig::FromDebugOptions(DeviceOrDevicelessConfig{test_config},\n+                                       debug_options));\n   GemmFusionAutotunerImpl autotuner(autotune_config, toolkit_version,\n                                     debug_options, nullptr, mlir_context);\n   return autotuner.GenerateTritonConfigs(dot);\n@@ -796,10 +803,12 @@ ENTRY main {\n                           ParseAndReturnVerifiedModule(kHloText));\n \n   DebugOptions opts;\n-  AutotuneConfig autotune_config = AutotuneConfig::FromDebugOptions(\n-      DeviceOrDevicelessConfig{DeviceConfig{backend().default_stream_executor(),\n-                                            backend().memory_allocator()}},\n-      opts);\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      AutotuneConfig autotune_config,\n+      AutotuneConfig::FromDebugOptions(DeviceOrDevicelessConfig{DeviceConfig{\n+                                           backend().default_stream_executor(),\n+                                           backend().memory_allocator()}},\n+                                       opts));\n   AutotuneCacheKey cache_key(autotune_config.GetDeviceDescription(),\n                              *module->entry_computation()->root_instruction());\n \n@@ -1042,12 +1051,15 @@ ENTRY e {\n                                       tsl::port::MaxParallelism());\n   DebugOptions opts;\n   MultiProcessKeyValueStore key_value_store;\n-  pipeline.AddPass<GemmFusionAutotuner>(\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      AutotuneConfig config,\n       AutotuneConfig::FromDebugOptions(\n           DeviceOrDevicelessConfig{DevicelessConfig{\n               backend().default_stream_executor()->GetDeviceDescription()}},\n-          opts),\n-      GetToolkitVersion(), &thread_pool, key_value_store, &mlir_context_);\n+          opts));\n+  pipeline.AddPass<GemmFusionAutotuner>(config, GetToolkitVersion(),\n+                                        &thread_pool, key_value_store,\n+                                        &mlir_context_);\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n                           ParseAndReturnVerifiedModule(hlo));\n@@ -1181,8 +1193,10 @@ TEST_F(GemmFusionAutotunerTest, SplitKFLoatNormalization) {\n   ccc->set_minor(compute_capability.minor);\n   DeviceConfig test_config{backend().default_stream_executor(),\n                            backend().memory_allocator()};\n-  AutotuneConfig autotune_config = AutotuneConfig::FromDebugOptions(\n-      DeviceOrDevicelessConfig{test_config}, GetDebugOptionsForTest());\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      AutotuneConfig autotune_config,\n+      AutotuneConfig::FromDebugOptions(DeviceOrDevicelessConfig{test_config},\n+                                       GetDebugOptionsForTest()));\n   GemmFusionAutotunerImpl autotuner(autotune_config, GetToolkitVersion(),\n                                     GetDebugOptionsForTest(), nullptr,\n                                     &mlir_context_);\n@@ -1490,9 +1504,11 @@ class GemmFusionShardedAutotunerTest : public GemmFusionAutotunerTest {\n  protected:\n   AutotuneConfig GetAutotuneConfigForTest() const {\n     return AutotuneConfig::FromDebugOptions(\n-        DeviceOrDevicelessConfig{DeviceConfig{\n-            backend().default_stream_executor(), backend().memory_allocator()}},\n-        GetDebugOptionsForTest());\n+               DeviceOrDevicelessConfig{\n+                   DeviceConfig{backend().default_stream_executor(),\n+                                backend().memory_allocator()}},\n+               GetDebugOptionsForTest())\n+        .value();\n   }\n \n   GemmFusionAutotuner GemmFusionAutotunerForKeyValueStore(\n@@ -1671,10 +1687,12 @@ TEST_F(GemmFusionAutotunerTest, RewritesGemmFusionToCustomKernelFusion) {\n       ParseAndReturnVerifiedModule(kHlo).value();\n \n   DebugOptions opts;\n-  AutotuneConfig autotune_config = AutotuneConfig::FromDebugOptions(\n-      DeviceOrDevicelessConfig{DeviceConfig{backend().default_stream_executor(),\n-                                            backend().memory_allocator()}},\n-      opts);\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      AutotuneConfig autotune_config,\n+      AutotuneConfig::FromDebugOptions(DeviceOrDevicelessConfig{DeviceConfig{\n+                                           backend().default_stream_executor(),\n+                                           backend().memory_allocator()}},\n+                                       opts));\n   AutotuneCacheKey cache_key(autotune_config.GetDeviceDescription(),\n                              *module->entry_computation()->root_instruction());\n   TF_ASSERT_OK_AND_ASSIGN(AutotuneResults autotune_results_override,\n@@ -1921,6 +1939,56 @@ TEST_F(GemmFusionAutotunerEnableTma, TmaRunCorrectlyForDotsOfBroadcasts) {\n   EXPECT_TRUE(RunAndCompare(std::move(module),\n                             ErrorSpec{/*aabs=*/5e-3, /*arel=*/5e-3}));\n }\n+\n+TEST_F(GemmFusionAutotunerTest, ReadsOverrideFile) {\n+  if (GpuComputeComp().IsRocm()) {\n+    GTEST_SKIP() << \"Not supported on ROCm.\";\n+  }\n+  std::string output_directory;\n+  if (!tsl::io::GetTestUndeclaredOutputsDir(&output_directory)) {\n+    output_directory = tsl::testing::TmpDir();\n+  }\n+  const std::string override_file =\n+      tsl::io::JoinPath(output_directory, \"override.textproto\");\n+  // Block M 126 is not really a valid config, but allows us to check that the\n+  // override file was used.\n+  TF_ASSERT_OK(tsl::WriteStringToFile(tsl::Env::Default(), override_file,\n+                                      R\"pb(config {\n+                                             block_m: 126\n+                                             block_n: 32\n+                                             block_k: 16\n+                                             split_k: 1\n+                                             num_stages: 1\n+                                             num_warps: 32\n+                                             num_ctas: 1\n+                                           })pb\"));\n+\n+  DebugOptions debug_options = GetDebugOptionsForTest();\n+  debug_options.set_xla_gpu_gemm_autotuner_override_file(override_file);\n+\n+  std::unique_ptr<VerifiedHloModule> module = ParseAndReturnVerifiedModule(R\"(\n+    ENTRY e {\n+      p0 = f32[64,64] parameter(0)\n+      p1 = f32[64,64] parameter(1)\n+      ROOT r = f32[64,64] dot(p0, p1),\n+        lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+    })\")\n+                                                  .value();\n+\n+  const se::CudaComputeCapability compute_capability{\n+      se::CudaComputeCapability::kAmpere, /*minor=*/0};\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      const std::vector<TritonGemmConfig> configs,\n+      GetPossibleMatmulAutotuneTritonConfigs(\n+          *Cast<HloDotInstruction>(\n+              module->entry_computation()->root_instruction()),\n+          compute_capability, GetToolkitVersion(), debug_options,\n+          &mlir_context_));\n+  EXPECT_TRUE(std::any_of(\n+      configs.begin(), configs.end(),\n+      [](const TritonGemmConfig& config) { return config.block_m == 126; }));\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "5cc2470db9dbcb04bb10744da5ae36282fa30ee4",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06",
            "patch": "@@ -1527,8 +1527,9 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n           gpu_target_config.platform_name == \"ROCM\");\n   DeviceOrDevicelessConfig device_config =\n       GetDeviceConfig(stream_exec, options, gpu_target_config);\n-  AutotuneConfig autotune_config =\n-      AutotuneConfig::FromDebugOptions(device_config, debug_options);\n+  TF_ASSIGN_OR_RETURN(\n+      AutotuneConfig autotune_config,\n+      AutotuneConfig::FromDebugOptions(device_config, debug_options));\n   // Lambdas and related constants:\n   const GpuFloatSupport bf16_support(gpu_version, BF16);\n   const GpuFloatSupport f8e5m2_support(gpu_version, F8E5M2, F16);\n@@ -1888,8 +1889,9 @@ absl::StatusOr<std::unique_ptr<HloModule>> GpuCompiler::RunHloPasses(\n   AutotuneResults autotune_results;\n   DeviceOrDevicelessConfig device_config =\n       GetDeviceConfig(stream_exec, options, gpu_target_config);\n-  AutotuneConfig autotune_config =\n-      AutotuneConfig::FromDebugOptions(device_config, debug_opts);\n+  TF_ASSIGN_OR_RETURN(\n+      AutotuneConfig autotune_config,\n+      AutotuneConfig::FromDebugOptions(device_config, debug_opts));\n   if (!is_deviceless) {\n     TF_RETURN_IF_ERROR(\n         AutotunerUtil::SerializeAutotuneResults(&autotune_results));"
        },
        {
            "sha": "6328207fad737099fec1a2079373014d9560d6ed",
            "filename": "third_party/xla/xla/service/gpu/matmul_utils.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmatmul_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmatmul_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmatmul_utils.h?ref=66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06",
            "patch": "@@ -190,6 +190,7 @@ struct TritonGemmConfig {\n         num_ctas(num_ctas),\n         is_tma_allowed(is_tma_allowed),\n         is_warp_specialization_allowed(is_warp_specialization_allowed) {}\n+  // LINT.IfChange\n   int block_m = 0;\n   int block_n = 0;\n   int block_k = 0;\n@@ -202,6 +203,7 @@ struct TritonGemmConfig {\n   bool is_tma_allowed = false;\n   // Allow/disallow automatic warp specialization.\n   bool is_warp_specialization_allowed = false;\n+  // LINT.ThenChange(//tensorflow/compiler/xla/autotuning.proto)\n \n   // When adding new members, please update all methods, such as ToTuple,\n   // FromProto, ToProto, ToString, etc. Updating ToTuple is not enough."
        },
        {
            "sha": "c8290a01441d0d1b599dbe4ef95dd44280316c7a",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=66bbe24941cb4ac7ff23dbfac968d3d7e9df2a06",
            "patch": "@@ -751,6 +751,10 @@ message DebugOptions {\n \n   optional bool xla_gpu_fused_attention_use_cudnn_rng = 235;\n \n+  // A textproto file to override autotune results. See also\n+  // `xla_gpu_override_gemm_autotuner` to override with a single config.\n+  optional string xla_gpu_gemm_autotuner_override_file = 434;\n+\n   // Threshold to rewrite matmul to cuBLAS or Triton (minimum combined number of\n   // elements of both matrices in non-batch dimensions to be considered for a\n   // rewrite).\n@@ -1316,7 +1320,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 434\n+  // Next id: 435\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 271,
        "additions": 209,
        "deletions": 62
    }
}