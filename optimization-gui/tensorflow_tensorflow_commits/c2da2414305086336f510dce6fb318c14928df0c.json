{
    "author": "abhigunj",
    "message": "Integrate StableHLO at openxla/stablehlo@d496423c\n\nPiperOrigin-RevId: 846467466",
    "sha": "c2da2414305086336f510dce6fb318c14928df0c",
    "files": [
        {
            "sha": "8b137891791fe96927ad78e64b0aad7bded08bdc",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 0,
            "deletions": 1056,
            "changes": 1056,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c2da2414305086336f510dce6fb318c14928df0c/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c2da2414305086336f510dce6fb318c14928df0c/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=c2da2414305086336f510dce6fb318c14928df0c",
            "patch": "@@ -1,1057 +1 @@\n-diff --ruN a/stablehlo/BUILD.bazel b/stablehlo/BUILD.bazel\n---- stablehlo/BUILD.bazel\n-+++ stablehlo/BUILD.bazel\n-@@ -1183,6 +1183,7 @@\n-         \":chlo_ops\",\n-         \":chlo_rewriters_inc_gen\",\n-         \":stablehlo_aggressive_simplification_inc_gen\",\n-+        \":stablehlo_broadcast_lowering\",\n-         \":stablehlo_create_compatibility_expander_inc_gen\",\n-         \":stablehlo_create_complex_math_expander_inc_gen\",\n-         \":stablehlo_legalize_deprecated_ops_inc_gen\",\n-@@ -1922,6 +1923,24 @@\n-     ],\n- )\n- \n-+cc_test(\n-+    name = \"chlo_builder_test\",\n-+    srcs = [\"stablehlo/integrations/cpp/builder/ChloBuilderTest.cpp\"],\n-+    deps = [\n-+        \":attr_type_builder_util\",\n-+        \":chlo_builder\",\n-+        \":func_builder\",\n-+        \":mlir_builder\",\n-+        \":register\",\n-+        \":stablehlo_builder\",\n-+        \":stablehlo_ops\",\n-+        \"@llvm-project//mlir:IR\",\n-+        \"@llvm-project//mlir:Support\",\n-+        \"@llvm-project//third-party/unittest:gmock\",\n-+        \"@llvm-project//third-party/unittest:gtest\",\n-+    ],\n-+)\n-+\n- gentbl_cc_library(\n-     name = \"func_builder_inc\",\n-     tbl_outs = {\n-diff --ruN a/stablehlo/stablehlo/dialect/Base.cpp b/stablehlo/stablehlo/dialect/Base.cpp\n---- stablehlo/stablehlo/dialect/Base.cpp\n-+++ stablehlo/stablehlo/dialect/Base.cpp\n-@@ -29,6 +29,7 @@\n- #include \"llvm/ADT/STLExtras.h\"\n- #include \"llvm/ADT/Sequence.h\"\n- #include \"llvm/ADT/SmallVector.h\"\n-+#include \"llvm/Support/Casting.h\"\n- #include \"llvm/Support/Debug.h\"\n- #include \"llvm/Support/ErrorHandling.h\"\n- #include \"mlir/Dialect/Quant/IR/QuantTypes.h\"\n-@@ -781,6 +782,14 @@\n-           numScales == rankedType.getDimSize(quantDim));\n- }\n- \n-+bool isBoundedDynamic(Type type) {\n-+  RankedTensorType rankedType = dyn_cast<RankedTensorType>(type);\n-+  if (!rankedType) return false;\n-+  auto boundedAttr =\n-+      mlir::dyn_cast_if_present<BoundedAttrInterface>(rankedType.getEncoding());\n-+  return boundedAttr != nullptr;\n-+}\n-+\n- bool hasSingleBoundedDimension(Type type) {\n-   RankedTensorType rankedType = dyn_cast<RankedTensorType>(type);\n-   auto boundedAttr =\n-diff --ruN a/stablehlo/stablehlo/dialect/Base.h b/stablehlo/stablehlo/dialect/Base.h\n---- stablehlo/stablehlo/dialect/Base.h\n-+++ stablehlo/stablehlo/dialect/Base.h\n-@@ -101,6 +101,9 @@\n- // mentioned in the StableHLO specification.\n- bool isValidQuantizedDimension(Type type);\n- \n-+// Returns true if the given type is a bounded dynamic tensor.\n-+bool isBoundedDynamic(Type type);\n-+\n- // Returns true if the given type has a single bounded dimension.\n- bool hasSingleBoundedDimension(Type type);\n- \n-diff --ruN a/stablehlo/stablehlo/dialect/ChloOps.cpp b/stablehlo/stablehlo/dialect/ChloOps.cpp\n---- stablehlo/stablehlo/dialect/ChloOps.cpp\n-+++ stablehlo/stablehlo/dialect/ChloOps.cpp\n-@@ -365,11 +365,14 @@\n-   Type elementType = op.getValue().getType();\n-   Type operandType = op.getOperand().getType();\n-   if (isa<UnrankedTensorType>(operandType)) {\n-+    // TODO(b/326463552): Remove unranked dynamism from CHLO.\n-     inferredReturnShapes.emplace_back(elementType);\n--  } else {\n--    const auto& shape = cast<RankedTensorType>(operandType).getShape();\n--    inferredReturnShapes.emplace_back(shape, elementType);\n--  }\n-+    return success();\n-+  }\n-+  auto rankedType = cast<RankedTensorType>(operandType);\n-+  const auto& shape = rankedType.getShape();\n-+  Attribute encoding = rankedType.getEncoding();\n-+  inferredReturnShapes.emplace_back(shape, elementType, encoding);\n-   return success();\n- }\n- \n-diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/CMakeLists.txt b/stablehlo/stablehlo/integrations/cpp/builder/CMakeLists.txt\n---- stablehlo/stablehlo/integrations/cpp/builder/CMakeLists.txt\n-+++ stablehlo/stablehlo/integrations/cpp/builder/CMakeLists.txt\n-@@ -137,6 +137,7 @@\n-     set_target_properties(check-stablehlo-ci PROPERTIES FOLDER \"Tests\")\n-     add_unittest(check-stablehlo-ci \"unittests\"\n-       MlirBuilderTest.cpp\n-+      ChloBuilderTest.cpp\n-       StablehloBuilderTest.cpp\n-       AttrTypeBuilderUtilTest.cpp\n-     )\n-diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.cpp b/stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.cpp\n---- stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.cpp\n-+++ stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.cpp\n-@@ -31,5 +31,15 @@\n- \n- #include \"stablehlo/integrations/cpp/builder/ChloBuilder.cpp.inc\"\n- \n-+/////////////////\n-+// MANUAL APIs\n-+/////////////////\n-+\n-+MlirOp ConstantLike(MlirOp input, DenseElementsAttr val) {\n-+  MlirBuilder& builder = input.getBuilder();\n-+  auto splat_val = val.getSplatValue<TypedAttr>();\n-+  return builder.create<chlo::ConstantLikeOp>(splat_val, input.getValue());\n-+}\n-+\n- }  // namespace chlo\n- }  // namespace mlir\n-diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.h b/stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.h\n---- stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.h\n-+++ stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.h\n-@@ -19,6 +19,7 @@\n- #include <cstdint>\n- \n- #include \"llvm/ADT/SmallVector.h\"\n-+#include \"mlir/IR/BuiltinAttributes.h\"\n- #include \"stablehlo/dialect/ChloOps.h\"\n- #include \"stablehlo/integrations/cpp/builder/MlirBuilder.h\"\n- \n-@@ -31,6 +32,12 @@\n- \n- #include \"stablehlo/integrations/cpp/builder/ChloBuilder.h.inc\"\n- \n-+/////////////////\n-+// MANUAL APIs\n-+/////////////////\n-+\n-+MlirOp ConstantLike(MlirOp input, DenseElementsAttr val);\n-+\n- }  // namespace chlo\n- }  // namespace mlir\n- \n-diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/ChloBuilderTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/ChloBuilderTest.cpp\n---- stablehlo/stablehlo/integrations/cpp/builder/ChloBuilderTest.cpp\n-+++ stablehlo/stablehlo/integrations/cpp/builder/ChloBuilderTest.cpp\n-@@ -0,0 +1,141 @@\n-+/* Copyright 2025 The OpenXLA Authors.\n-+\n-+Licensed under the Apache License, Version 2.0 (the \"License\");\n-+you may not use this file except in compliance with the License.\n-+You may obtain a copy of the License at\n-+\n-+    http://www.apache.org/licenses/LICENSE-2.0\n-+\n-+Unless required by applicable law or agreed to in writing, software\n-+distributed under the License is distributed on an \"AS IS\" BASIS,\n-+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-+See the License for the specific language governing permissions and\n-+limitations under the License.\n-+==============================================================================*/\n-+\n-+#include <string>\n-+\n-+#include \"mlir/IR/BuiltinAttributes.h\"\n-+#include \"mlir/IR/BuiltinOps.h\"\n-+#include \"mlir/IR/DialectRegistry.h\"\n-+#include \"mlir/IR/MLIRContext.h\"\n-+#include \"mlir/IR/OwningOpRef.h\"\n-+#include \"mlir/IR/Types.h\"\n-+#include \"mlir/IR/Verifier.h\"\n-+#include \"mlir/Support/DebugStringHelper.h\"\n-+#include \"mlir/Support/LLVM.h\"\n-+#include \"stablehlo/dialect/Register.h\"\n-+#include \"stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h\"\n-+#include \"stablehlo/integrations/cpp/builder/ChloBuilder.h\"\n-+#include \"stablehlo/integrations/cpp/builder/FuncBuilder.h\"\n-+#include \"stablehlo/integrations/cpp/builder/MlirBuilder.h\"\n-+#include \"testing/base/public/gunit.h\"\n-+#include \"stablehlo/integrations/cpp/builder/StablehloBuilder.h\"\n-+\n-+namespace mlir {\n-+namespace chlo {\n-+\n-+namespace {\n-+\n-+// Wrap a module builder and register the classes needed\n-+class ChloModuleBuilder {\n-+ public:\n-+  ChloModuleBuilder()\n-+      : context_(), module_builder_(context_, mlir::unknownLoc(context_)) {\n-+    DialectRegistry registry;\n-+    stablehlo::registerAllDialects(registry);\n-+    context_.appendDialectRegistry(registry);\n-+    context_.loadAllAvailableDialects();\n-+  }\n-+\n-+  ModuleBuilder& get() { return module_builder_; }\n-+  ModuleBuilder* operator->() { return &module_builder_; }\n-+\n-+ private:\n-+  MLIRContext context_;\n-+  ModuleBuilder module_builder_;\n-+};\n-+\n-+// TODO: Make a FileCheck matcher\n-+\n-+}  // namespace\n-+\n-+TEST(ChloBuilderTest, SmokeTest) {\n-+  std::string expected = R\"mlir(module {\n-+  func.func @main(%arg0: tensor<2xi64>) -> tensor<2xi64> {\n-+    %0 = chlo.constant dense<1> : tensor<i64>\n-+    %1 = chlo.broadcast_add %arg0, %0 : (tensor<2xi64>, tensor<i64>) -> tensor<2xi64>\n-+    return %1 : tensor<2xi64>\n-+  }\n-+})mlir\";\n-+\n-+  ChloModuleBuilder mb;\n-+  {  // Build Main Func\n-+    Location funcLoc = fileLineColLoc(mb->getContext(), \"main.mlir\", 1, 1);\n-+    func::FunctionBuilder fb(mb.get(), \"main\", funcLoc);\n-+    auto type2xi64 = makeTensorType(mb->getContext(), {2}, ElementType::I64);\n-+    auto typeScalari64 = makeTensorType(mb->getContext(), {}, ElementType::I64);\n-+    auto arg0 = func::Argument(fb, type2xi64);\n-+    auto cst = Constant(fb, mlir::makeConstant(1L, typeScalari64));\n-+    auto add = BroadcastAdd(arg0, cst);\n-+    func::Return(fb, {add});\n-+  }\n-+\n-+  OwningOpRef<ModuleOp> module = mb->build();\n-+  EXPECT_TRUE(succeeded(mlir::verify(*module)));\n-+  EXPECT_EQ(expected, debugString(*module));\n-+}\n-+\n-+TEST(MlirBuilderTest, ConstantLike) {\n-+  std::string expected = R\"mlir(module {\n-+  func.func @main(%arg0: tensor<2xi64>) -> tensor<2xi64> {\n-+    %0 = \"chlo.constant_like\"(%arg0) <{value = 1 : i64}> : (tensor<2xi64>) -> tensor<2xi64>\n-+    return %0 : tensor<2xi64>\n-+  }\n-+})mlir\";\n-+\n-+  ChloModuleBuilder mb;\n-+  {  // Build Main Func\n-+    Location funcLoc = fileLineColLoc(mb->getContext(), \"main.mlir\", 1, 1);\n-+    func::FunctionBuilder fb(mb.get(), \"main\", funcLoc);\n-+    auto type2xi64 = makeTensorType(mb->getContext(), {2}, ElementType::I64);\n-+    auto typeScalari64 = makeTensorType(mb->getContext(), {}, ElementType::I64);\n-+    auto arg0 = func::Argument(fb, type2xi64);\n-+    auto cst = ConstantLike(arg0, mlir::makeConstant(1L, typeScalari64));\n-+    func::Return(fb, {cst});\n-+  }\n-+\n-+  OwningOpRef<ModuleOp> module = mb->build();\n-+  EXPECT_TRUE(succeeded(mlir::verify(*module)));\n-+  EXPECT_EQ(expected, debugString(*module));\n-+}\n-+\n-+TEST(MlirBuilderTest, ConstantLikeBounded) {\n-+  std::string expected = R\"mlir(module {\n-+  func.func @main(%arg0: tensor<2xi64>, %arg1: tensor<i32>) -> tensor<?xi32, #stablehlo.bounds<2>> {\n-+    %0 = stablehlo.set_dimension_size %arg0, %arg1, dim = 0 : (tensor<2xi64>, tensor<i32>) -> tensor<?xi64, #stablehlo.bounds<2>>\n-+    %1 = \"chlo.constant_like\"(%0) <{value = 1 : i32}> : (tensor<?xi64, #stablehlo.bounds<2>>) -> tensor<?xi32, #stablehlo.bounds<2>>\n-+    return %1 : tensor<?xi32, #stablehlo.bounds<2>>\n-+  }\n-+})mlir\";\n-+\n-+  ChloModuleBuilder mb;\n-+  {  // Build Main Func\n-+    Location funcLoc = fileLineColLoc(mb->getContext(), \"main.mlir\", 1, 1);\n-+    func::FunctionBuilder fb(mb.get(), \"main\", funcLoc);\n-+    auto type2xi64 = makeTensorType(mb->getContext(), {2}, ElementType::I64);\n-+    auto typei32 = makeTensorType(mb->getContext(), {}, ElementType::I32);\n-+    auto arg0 = func::Argument(fb, type2xi64);\n-+    auto arg1 = func::Argument(fb, typei32);\n-+    auto sds = stablehlo::SetDimensionSize(arg0, arg1, 0);\n-+    auto cst = ConstantLike(sds, mlir::makeConstant(1L, typei32));\n-+    func::Return(fb, {cst});\n-+  }\n-+\n-+  OwningOpRef<ModuleOp> module = mb->build();\n-+  EXPECT_TRUE(succeeded(mlir::verify(*module)));\n-+  EXPECT_EQ(expected, debugString(*module));\n-+}\n-+\n-+}  // namespace chlo\n-+}  // namespace mlir\n-diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp\n---- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp\n-+++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp\n-@@ -67,6 +67,7 @@\n-   MlirOp operand = input;\n-   auto inputType = mlir::cast<RankedTensorType>(input.getType());\n-   auto resultType = inputType.clone(resultElementType);\n-+  if (inputType == resultType) return input;  // skip no-op convert\n-   if (isa<ComplexType>(inputType.getElementType()) &&\n-       !isa<ComplexType>(resultElementType)) {\n-     operand = stablehlo::Real(operand);\n-diff --ruN a/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir b/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir\n---- stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir\n-+++ stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir\n-@@ -622,6 +622,10 @@\n-   func.return %result : tensor<complex<f32>>\n- }\n- \n-+//////\n-+// Broadcast binary elementwise ops tests are located in\n-+// chlo_legalize_to_stablehlo_broadcast.mlir\n-+\n- // -----\n- \n- // Lower statically shaped `constant_like` to constant.\n-@@ -632,6 +636,24 @@\n-   %result = \"chlo.constant_like\"(%arg) { value = 3.2 : f32 }\n-       : (tensor<1x2xi64>) -> tensor<1x2xf32>\n-   func.return %result : tensor<1x2xf32>\n-+}\n-+\n-+// -----\n-+\n-+// Lower dynamically shaped `constant_like` to broadcasted constant.\n-+// CHECK-LABEL: constant_like_bounded_dynamic_shape\n-+// CHECK-SAME: (%[[ARG0:.*]]: tensor<2xi64>, %[[ARG1:.*]]: tensor<i32>)\n-+func.func @constant_like_bounded_dynamic_shape(%arg0: tensor<2xi64>, %arg1: tensor<i32>) -> tensor<?xi32, #stablehlo.bounds<2>> {\n-+  %0 = stablehlo.set_dimension_size %arg0, %arg1, dim = 0 : (tensor<2xi64>, tensor<i32>) -> tensor<?xi64, #stablehlo.bounds<2>>\n-+  // CHECK-NOT: chlo.constant_like\n-+  // CHECK: %[[ARG0_DYN:.*]] = stablehlo.set_dimension_size %[[ARG0]], %[[ARG1]], dim = 0 : (tensor<2xi64>, tensor<i32>) -> tensor<?xi64, #stablehlo.bounds<2>>\n-+  // CHECK: %[[CST:.*]] = stablehlo.constant dense<1> : tensor<i32>\n-+  // CHECK-NEXT: %[[BCAST:.*]] = stablehlo.broadcast_in_dim %[[CST]], dims = [] : (tensor<i32>) -> tensor<2xi32>\n-+  // CHECK-NEXT: %[[GDS:.*]] = stablehlo.get_dimension_size %[[ARG0_DYN]], dim = 0 : (tensor<?xi64, #stablehlo.bounds<2>>) -> tensor<i32>\n-+  // CHECK-NEXT: %[[SDS:.*]] = stablehlo.set_dimension_size %[[BCAST]], %[[GDS]], dim = 0 : (tensor<2xi32>, tensor<i32>) -> tensor<?xi32, #stablehlo.bounds<2>>\n-+  // CHECK-NEXT: return %[[SDS]] : tensor<?xi32, #stablehlo.bounds<2>>\n-+  %1 = \"chlo.constant_like\"(%0) <{value = 1 : i32}> : (tensor<?xi64, #stablehlo.bounds<2>>) -> tensor<?xi32, #stablehlo.bounds<2>>\n-+  return %1 : tensor<?xi32, #stablehlo.bounds<2>>\n- }\n- \n- // -----\n-diff --ruN a/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir b/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir\n---- stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir\n-+++ stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir\n-@@ -3,8 +3,8 @@\n- // Check the non-broadcast case for each registered op, then just check a\n- // representative op for detailed broadcast semantics.\n- \n--// CHECK-LABEL: @addWithoutBroadcast\n--func.func @addWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n-+// CHECK-LABEL: @add_no_broadcast\n-+func.func @add_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n-   // CHECK: stablehlo.add %arg0, %arg1\n-   %0 = chlo.broadcast_add %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n-   func.return %0 : tensor<4xf32>\n-@@ -12,8 +12,8 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @addStaticBroadcastExpanding\n--func.func @addStaticBroadcastExpanding(%arg0: tensor<4xf32>, %arg1: tensor<f32>) -> tensor<4xf32> {\n-+// CHECK-LABEL: @add_static_broadcast_expanding\n-+func.func @add_static_broadcast_expanding(%arg0: tensor<4xf32>, %arg1: tensor<f32>) -> tensor<4xf32> {\n-   // CHECK:      %[[BROADCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n-   // CHECK-NEXT: stablehlo.add %arg0, %[[BROADCAST]]\n-   // CHECK-NOT: shape\n-@@ -23,8 +23,8 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @addStaticBroadcastSameRank\n--func.func @addStaticBroadcastSameRank(%arg0: tensor<1x4xf32>, %arg1: tensor<4x1xf32>) -> tensor<4x4xf32> {\n-+// CHECK-LABEL: @add_static_broadcast_same_rank\n-+func.func @add_static_broadcast_same_rank(%arg0: tensor<1x4xf32>, %arg1: tensor<4x1xf32>) -> tensor<4x4xf32> {\n-   // CHECK:      %[[ARG0_B:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0, 1] : (tensor<1x4xf32>) -> tensor<4x4xf32>\n-   // CHECK-NEXT: %[[ARG1_B:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<4x1xf32>) -> tensor<4x4xf32>\n-   // CHECK-NEXT: stablehlo.add %[[ARG0_B]], %[[ARG1_B]] : tensor<4x4xf32>\n-@@ -35,11 +35,33 @@\n- \n- // -----\n- \n--\n--// CHECK-LABEL: @dynamicBroadcast\n-+// [<=10] x [<=10] => [<=10]\n-+// CHECK-LABEL: func @add_bounded_dynamic_no_broadcast\n-+func.func @add_bounded_dynamic_no_broadcast(%arg0: tensor<?xf64, #stablehlo.bounds<10>>, %arg1: tensor<?xf64, #stablehlo.bounds<10>>) -> tensor<?xf64, #stablehlo.bounds<10>> {\n-+  // CHECK-NEXT: stablehlo.add %arg0, %arg1\n-+  %0 = chlo.broadcast_add %arg0, %arg1 : (tensor<?xf64, #stablehlo.bounds<10>>, tensor<?xf64, #stablehlo.bounds<10>>) -> tensor<?xf64, #stablehlo.bounds<10>>\n-+  return %0 : tensor<?xf64, #stablehlo.bounds<10>>\n-+}\n-+\n-+// -----\n-+\n-+// [<=10] x [] => [<=10]\n-+// CHECK-LABEL: func @add_bounded_dynamic_expanding\n-+func.func @add_bounded_dynamic_expanding(%arg0: tensor<?xf64, #stablehlo.bounds<10>>, %arg1: tensor<f64>) -> tensor<?xf64, #stablehlo.bounds<10>> {\n-+  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f64>) -> tensor<10xf64>\n-+  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 0\n-+  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST]], %[[DIM_SIZE]], dim = 0\n-+  // CHECK-NEXT: stablehlo.add %arg0, %[[RHS_BCAST_DYN]]\n-+  %0 = chlo.broadcast_add %arg0, %arg1 : (tensor<?xf64, #stablehlo.bounds<10>>, tensor<f64>) -> tensor<?xf64, #stablehlo.bounds<10>>\n-+  return %0 : tensor<?xf64, #stablehlo.bounds<10>>\n-+}\n-+\n-+// -----\n-+\n-+// CHECK-LABEL: @add_dynamic_broadcast\n- // CHECK-SAME: %[[ARG0:.+]]: tensor<?xf32>\n- // CHECK-SAME: %[[ARG1:.+]]: tensor<?x?xf32>\n--func.func @dynamicBroadcast(%arg0: tensor<?xf32>, %arg1: tensor<?x?xf32>) -> tensor<?x?xf32> {\n-+func.func @add_dynamic_broadcast(%arg0: tensor<?xf32>, %arg1: tensor<?x?xf32>) -> tensor<?x?xf32> {\n-   // CHECK-DAG:  %[[ARG0_S:.+]] = shape.shape_of %[[ARG0]]\n-   // CHECK-DAG:  %[[ARG1_S:.+]] = shape.shape_of %[[ARG1]]\n-   // CHECK-NEXT: %[[WITNESS:.+]] = shape.cstr_broadcastable %[[ARG0_S]], %[[ARG1_S]]\n-@@ -57,10 +79,10 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @dynamicBroadcastComplex\n-+// CHECK-LABEL: @dynamic_broadcast_complex\n- // CHECK-SAME: %[[ARG0:.+]]: tensor<?xf32>\n- // CHECK-SAME: %[[ARG1:.+]]: tensor<?x?xf32>\n--func.func @dynamicBroadcastComplex(%arg0: tensor<?xf32>, %arg1: tensor<?x?xf32>) -> tensor<?x?xcomplex<f32>> {\n-+func.func @dynamic_broadcast_complex(%arg0: tensor<?xf32>, %arg1: tensor<?x?xf32>) -> tensor<?x?xcomplex<f32>> {\n-   // CHECK-DAG:  %[[ARG0_S:.+]] = shape.shape_of %[[ARG0]]\n-   // CHECK-DAG:  %[[ARG1_S:.+]] = shape.shape_of %[[ARG1]]\n-   // CHECK-NEXT: %[[WITNESS:.+]] = shape.cstr_broadcastable %[[ARG0_S]], %[[ARG1_S]]\n-@@ -78,10 +100,10 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @dynamicBroadcastCompare\n-+// CHECK-LABEL: @compare_dynamic_broadcast\n- // CHECK-SAME: %[[ARG0:.+]]: tensor<?xf32>\n- // CHECK-SAME: %[[ARG1:.+]]: tensor<?x?xf32>\n--func.func @dynamicBroadcastCompare(%arg0: tensor<?xf32>, %arg1: tensor<?x?xf32>) -> tensor<?x?xi1> {\n-+func.func @compare_dynamic_broadcast(%arg0: tensor<?xf32>, %arg1: tensor<?x?xf32>) -> tensor<?x?xi1> {\n-   // CHECK-DAG: %[[ARG0_S:.+]] = shape.shape_of %[[ARG0]]\n-   // CHECK-DAG: %[[ARG1_S:.+]] = shape.shape_of %[[ARG1]]\n-   // CHECK: %[[WITNESS:.+]] = shape.cstr_broadcastable %[[ARG0_S]], %[[ARG1_S]]\n-@@ -191,8 +213,8 @@\n- // -----\n- \n- // Verifies that broadcast_dimensions validity checks are valid.\n--// CHECK-LABEL: @dynamicNonScalarBroadcastDimensions\n--func.func @dynamicNonScalarBroadcastDimensions(%arg0: tensor<1x4xf32>, %arg1: tensor<4xf32>) -> tensor<1x4xf32> {\n-+// CHECK-LABEL: @dynamic_non_scalar_broadcast_dimensions\n-+func.func @dynamic_non_scalar_broadcast_dimensions(%arg0: tensor<1x4xf32>, %arg1: tensor<4xf32>) -> tensor<1x4xf32> {\n-   // CHECK: stablehlo.add\n-   %0 = chlo.broadcast_add %arg0, %arg1 {broadcast_dimensions =  array<i64: 1> } : (tensor<1x4xf32>, tensor<4xf32>) -> tensor<1x4xf32>\n-   func.return %0 : tensor<1x4xf32>\n-@@ -201,8 +223,8 @@\n- // -----\n- \n- // Verifies that broadcast_dimensions validity checks are valid.\n--// CHECK-LABEL: @dynamicNonScalarByScalarBroadcastDimensions\n--func.func @dynamicNonScalarByScalarBroadcastDimensions(%arg0: tensor<1x4xf32>, %arg1: tensor<f32>) -> tensor<1x4xf32> {\n-+// CHECK-LABEL: @dynamic_non_scalar_by_scalar_broadcast_dimensions\n-+func.func @dynamic_non_scalar_by_scalar_broadcast_dimensions(%arg0: tensor<1x4xf32>, %arg1: tensor<f32>) -> tensor<1x4xf32> {\n-   // CHECK: stablehlo.add\n-   %0 = chlo.broadcast_add %arg0, %arg1 : (tensor<1x4xf32>, tensor<f32>) -> tensor<1x4xf32>\n-   func.return %0 : tensor<1x4xf32>\n-@@ -211,7 +233,7 @@\n- // -----\n- \n- // Verifies that invalid broadcast dimensions are rejected.\n--func.func @dynamicNonScalarBroadcastDimensionsSizeMismatch(%arg0: tensor<1x4xf32>, %arg1: tensor<4xf32>) -> tensor<1x4xf32> {\n-+func.func @dynamic_non_scalar_broadcast_dimensions_size_mismatch(%arg0: tensor<1x4xf32>, %arg1: tensor<4xf32>) -> tensor<1x4xf32> {\n-   // expected-warning @+2 {{unsupported non prefix-padded dynamic rank broadcast_dimensions}}\n-   // expected-error @+1 {{failed to legalize operation}}\n-   %0 = chlo.broadcast_add %arg0, %arg1 {broadcast_dimensions = array<i64: 1, 2>} : (tensor<1x4xf32>, tensor<4xf32>) -> tensor<1x4xf32>\n-@@ -221,7 +243,7 @@\n- // -----\n- \n- // Verifies that invalid broadcast dimensions are rejected.\n--func.func @dynamicNonScalarBroadcastDimensionsMismatch(%arg0: tensor<1x4xf32>, %arg1: tensor<4xf32>) -> tensor<1x4xf32> {\n-+func.func @dynamic_non_scalar_broadcast_dimensions_mismatch(%arg0: tensor<1x4xf32>, %arg1: tensor<4xf32>) -> tensor<1x4xf32> {\n-   // expected-warning @+2 {{unsupported non prefix-padded dynamic rank broadcast_dimensions}}\n-   // expected-error @+1 {{failed to legalize operation}}\n-   %0 = chlo.broadcast_add %arg0, %arg1 {broadcast_dimensions = array<i64: 2>} : (tensor<1x4xf32>, tensor<4xf32>) -> tensor<1x4xf32>\n-@@ -232,8 +254,8 @@\n- // Note that broadcast_add is used as a proxy for all of the template\n- // expansions. Tests below merely verify that the op has an expansion.\n- \n--// CHECK-LABEL: @andWithoutBroadcast\n--func.func @andWithoutBroadcast(%arg0: tensor<4xi1>, %arg1: tensor<4xi1>) -> tensor<4xi1> {\n-+// CHECK-LABEL: @and_no_broadcast\n-+func.func @and_no_broadcast(%arg0: tensor<4xi1>, %arg1: tensor<4xi1>) -> tensor<4xi1> {\n-   // CHECK: stablehlo.and %arg0, %arg1\n-   %0 = chlo.broadcast_and %arg0, %arg1 : (tensor<4xi1>, tensor<4xi1>) -> tensor<4xi1>\n-   func.return %0 : tensor<4xi1>\n-@@ -241,8 +263,8 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @atan2WithoutBroadcast\n--func.func @atan2WithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n-+// CHECK-LABEL: @atan2_no_broadcast\n-+func.func @atan2_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n-   // CHECK: stablehlo.atan2 %arg0, %arg1\n-   %0 = chlo.broadcast_atan2 %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n-   func.return %0 : tensor<4xf32>\n-@@ -250,8 +272,8 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @compareWithoutBroadcast\n--func.func @compareWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xi1> {\n-+// CHECK-LABEL: @compare_no_broadcast\n-+func.func @compare_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xi1> {\n-   // CHECK: stablehlo.compare EQ, %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n-   %0 = chlo.broadcast_compare %arg0, %arg1 {comparison_direction = #chlo<comparison_direction EQ>} : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n-   func.return %0 : tensor<4xi1>\n-@@ -259,8 +281,8 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @complexWithoutBroadcast\n--func.func @complexWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xcomplex<f32>> {\n-+// CHECK-LABEL: @complex_no_broadcast\n-+func.func @complex_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xcomplex<f32>> {\n-   // CHECK: stablehlo.complex %arg0, %arg1 : tensor<4xcomplex<f32>>\n-   %0 = chlo.broadcast_complex %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xcomplex<f32>>\n-   func.return %0 : tensor<4xcomplex<f32>>\n-@@ -268,8 +290,8 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @divideWithoutBroadcast\n--func.func @divideWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n-+// CHECK-LABEL: @divide_no_broadcast\n-+func.func @divide_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n-   // CHECK: stablehlo.divide %arg0, %arg1\n-   %0 = chlo.broadcast_divide %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n-   func.return %0 : tensor<4xf32>\n-@@ -277,8 +299,8 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @maximumWithoutBroadcast\n--func.func @maximumWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n-+// CHECK-LABEL: @maximum_no_broadcast\n-+func.func @maximum_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n-   // CHECK: stablehlo.maximum %arg0, %arg1\n-   %0 = chlo.broadcast_maximum %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n-   func.return %0 : tensor<4xf32>\n-@@ -286,8 +308,8 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @minimumWithoutBroadcast\n--func.func @minimumWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n-+// CHECK-LABEL: @minimum_no_broadcast\n-+func.func @minimum_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n-   // CHECK: stablehlo.minimum %arg0, %arg1\n-   %0 = chlo.broadcast_minimum %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n-   func.return %0 : tensor<4xf32>\n-@@ -295,8 +317,8 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @multiplyWithoutBroadcast\n--func.func @multiplyWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n-+// CHECK-LABEL: @multiply_no_broadcast\n-+func.func @multiply_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n-   // CHECK: stablehlo.multiply %arg0, %arg1\n-   %0 = chlo.broadcast_multiply %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n-   func.return %0 : tensor<4xf32>\n-@@ -304,8 +326,8 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @orWithoutBroadcast\n--func.func @orWithoutBroadcast(%arg0: tensor<4xi1>, %arg1: tensor<4xi1>) -> tensor<4xi1> {\n-+// CHECK-LABEL: @or_no_broadcast\n-+func.func @or_no_broadcast(%arg0: tensor<4xi1>, %arg1: tensor<4xi1>) -> tensor<4xi1> {\n-   // CHECK: stablehlo.or %arg0, %arg1\n-   %0 = chlo.broadcast_or %arg0, %arg1 : (tensor<4xi1>, tensor<4xi1>) -> tensor<4xi1>\n-   func.return %0 : tensor<4xi1>\n-@@ -313,8 +335,8 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @powerWithoutBroadcast\n--func.func @powerWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n-+// CHECK-LABEL: @power_no_broadcast\n-+func.func @power_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n-   // CHECK: stablehlo.power %arg0, %arg1\n-   %0 = chlo.broadcast_power %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n-   func.return %0 : tensor<4xf32>\n-@@ -322,8 +344,8 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @remainderWithoutBroadcast\n--func.func @remainderWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n-+// CHECK-LABEL: @remainder_no_broadcast\n-+func.func @remainder_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n-   // CHECK: stablehlo.remainder %arg0, %arg1\n-   %0 = chlo.broadcast_remainder %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n-   func.return %0 : tensor<4xf32>\n-@@ -331,8 +353,8 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @shift_leftWithoutBroadcast\n--func.func @shift_leftWithoutBroadcast(%arg0: tensor<4xi32>, %arg1: tensor<4xi32>) -> tensor<4xi32> {\n-+// CHECK-LABEL: @shift_left_no_broadcast\n-+func.func @shift_left_no_broadcast(%arg0: tensor<4xi32>, %arg1: tensor<4xi32>) -> tensor<4xi32> {\n-   // CHECK: stablehlo.shift_left %arg0, %arg1\n-   %0 = chlo.broadcast_shift_left %arg0, %arg1 : (tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32>\n-   func.return %0 : tensor<4xi32>\n-@@ -340,8 +362,8 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @shift_right_arithmeticWithoutBroadcast\n--func.func @shift_right_arithmeticWithoutBroadcast(%arg0: tensor<4xi32>, %arg1: tensor<4xi32>) -> tensor<4xi32> {\n-+// CHECK-LABEL: @shift_right_arithmetic_no_broadcast\n-+func.func @shift_right_arithmetic_no_broadcast(%arg0: tensor<4xi32>, %arg1: tensor<4xi32>) -> tensor<4xi32> {\n-   // CHECK: stablehlo.shift_right_arithmetic %arg0, %arg1\n-   %0 = chlo.broadcast_shift_right_arithmetic %arg0, %arg1 : (tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32>\n-   func.return %0 : tensor<4xi32>\n-@@ -349,8 +371,8 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @shift_right_logicalWithoutBroadcast\n--func.func @shift_right_logicalWithoutBroadcast(%arg0: tensor<4xi32>, %arg1: tensor<4xi32>) -> tensor<4xi32> {\n-+// CHECK-LABEL: @shift_right_logical_no_broadcast\n-+func.func @shift_right_logical_no_broadcast(%arg0: tensor<4xi32>, %arg1: tensor<4xi32>) -> tensor<4xi32> {\n-   // CHECK: stablehlo.shift_right_logical %arg0, %arg1\n-   %0 = chlo.broadcast_shift_right_logical %arg0, %arg1 : (tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32>\n-   func.return %0 : tensor<4xi32>\n-@@ -358,8 +380,8 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @subWithoutBroadcast\n--func.func @subWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n-+// CHECK-LABEL: @sub_no_broadcast\n-+func.func @sub_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n-   // CHECK: stablehlo.subtract %arg0, %arg1\n-   %0 = chlo.broadcast_subtract %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n-   func.return %0 : tensor<4xf32>\n-@@ -367,16 +389,16 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @xorWithoutBroadcast\n--func.func @xorWithoutBroadcast(%arg0: tensor<4xi1>, %arg1: tensor<4xi1>) -> tensor<4xi1> {\n-+// CHECK-LABEL: @xor_no_broadcast\n-+func.func @xor_no_broadcast(%arg0: tensor<4xi1>, %arg1: tensor<4xi1>) -> tensor<4xi1> {\n-   // CHECK: stablehlo.xor %arg0, %arg1\n-   %0 = chlo.broadcast_xor %arg0, %arg1 : (tensor<4xi1>, tensor<4xi1>) -> tensor<4xi1>\n-   func.return %0 : tensor<4xi1>\n- }\n- \n- // -----\n--// CHECK-LABEL: @NextAfterWithoutBroadcast\n--func.func @NextAfterWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>)\n-+// CHECK-LABEL: @next_after_no_broadcast\n-+func.func @next_after_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>)\n-     -> tensor<4xf32> {\n-   // CHECK-NOT: chlo.broadcast_next_after\n-   %0 = chlo.broadcast_next_after %arg0, %arg1\n-@@ -386,8 +408,8 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @PolygammaWithoutBroadcast\n--func.func @PolygammaWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>)\n-+// CHECK-LABEL: @Polygamma_no_broadcast\n-+func.func @Polygamma_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>)\n-     -> tensor<4xf32> {\n-   // CHECK-NOT: chlo.broadcast_polygamma\n-   // CHECK-NOT: chlo.polygamma\n-@@ -398,8 +420,8 @@\n- \n- // -----\n- \n--// CHECK-LABEL: @ZetaWithoutBroadcast\n--func.func @ZetaWithoutBroadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>)\n-+// CHECK-LABEL: @Zeta_no_broadcast\n-+func.func @Zeta_no_broadcast(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>)\n-     -> tensor<4xf32> {\n-   // CHECK-NOT: chlo.broadcast_zeta\n-   // CHECK-NOT: chlo.zeta\n-diff --ruN a/stablehlo/stablehlo/tests/ops_stablehlo_roundtrip.mlir b/stablehlo/stablehlo/tests/ops_stablehlo_roundtrip.mlir\n---- stablehlo/stablehlo/tests/ops_stablehlo_roundtrip.mlir\n-+++ stablehlo/stablehlo/tests/ops_stablehlo_roundtrip.mlir\n-@@ -316,7 +316,7 @@\n- // Serialized string:\n- //   \"\\08\\03\\1A\\02\\01\\02\\22\\02\\00\\01\"\n- func.func @test_custom_call2(%arg0: tensor<16x16xf32>) -> tensor<16x16xf32> {\n--  %0 = \"stablehlo.custom_call\"(%arg0) {backend_config = \"\", call_target_name = \"Sharding\", stablehlo.sharding = \"\\08\\03\\1A\\02\\01\\02\\22\\02\\00\\01\"} : (tensor<16x16xf32>) -> tensor<16x16xf32>\n-+  %0 = \"stablehlo.custom_call\"(%arg0) {backend_config = \"\", call_target_name = \"Sharding\", mhlo.sharding = \"\\08\\03\\1A\\02\\01\\02\\22\\02\\00\\01\"} : (tensor<16x16xf32>) -> tensor<16x16xf32>\n-   func.return %0 : tensor<16x16xf32>\n- }\n- \n-diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n---- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n-+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n-@@ -218,6 +218,21 @@\n-   // CHECK-NEXT: return [[TRUE]], [[FALSE]], [[TRUE]], [[TRUE]], [[TRUE]], [[FALSE]], [[TRUE]], [[FALSE]]\n-   return %0, %1, %2, %3, %4, %5, %6, %7 :\n-          tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>\n-+}\n-+\n-+// -----\n-+\n-+// CHECK-LABEL: func.func @compare_fold_with_implicit_comparison_type\n-+func.func @compare_fold_with_implicit_comparison_type() -> (tensor<3xi1>, tensor<3xi1>) {\n-+  %c_0 = stablehlo.constant dense<0> : tensor<3xi64>\n-+  %c = stablehlo.constant dense<[-1, 0, 1]> : tensor<3xi64>\n-+  %c_1 = stablehlo.constant dense<0.0> : tensor<3xf64>\n-+  %c_2 = stablehlo.constant dense<[-1.0, 0.0, 1.0]> : tensor<3xf64>\n-+  %0 = stablehlo.compare GE, %c, %c_0 : (tensor<3xi64>, tensor<3xi64>) -> tensor<3xi1>\n-+  %1 = stablehlo.compare GE, %c_2, %c_1 : (tensor<3xf64>, tensor<3xf64>) -> tensor<3xi1>\n-+  // CHECK-DAG:  [[RES:%.+]] = stablehlo.constant dense<[false, true, true]> : tensor<3xi1>\n-+  // CHECK-NEXT: return [[RES]], [[RES]] : tensor<3xi1>, tensor<3xi1>\n-+  return %0, %1 : tensor<3xi1>, tensor<3xi1>\n- }\n- \n- // -----\n-diff --ruN a/stablehlo/stablehlo/transforms/CMakeLists.txt b/stablehlo/stablehlo/transforms/CMakeLists.txt\n---- stablehlo/stablehlo/transforms/CMakeLists.txt\n-+++ stablehlo/stablehlo/transforms/CMakeLists.txt\n-@@ -113,6 +113,7 @@\n-   MLIRTransformUtils\n-   StablehloBase\n-   StablehloBroadcastUtils\n-+  StablehloBroadcastLowering\n-   StablehloLinalgTransforms\n-   StablehloOps\n-   StablehloOptimizationPasses\n-diff --ruN a/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp b/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp\n---- stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp\n-+++ stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp\n-@@ -35,7 +35,6 @@\n- #include \"mlir/IR/BuiltinAttributes.h\"\n- #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n- #include \"mlir/IR/BuiltinTypes.h\"\n--#include \"mlir/IR/ImplicitLocOpBuilder.h\"\n- #include \"mlir/IR/MLIRContext.h\"\n- #include \"mlir/IR/PatternMatch.h\"\n- #include \"mlir/IR/TypeUtilities.h\"\n-@@ -51,6 +50,7 @@\n- #include \"stablehlo/transforms/ChloDecompositionUtils.h\"\n- #include \"stablehlo/transforms/PassUtils.h\"\n- #include \"stablehlo/transforms/Passes.h\"\n-+#include \"stablehlo/transforms/StablehloBroadcastLowering.h\"\n- \n- // This must precede all other headers, otherwise during Windows cross\n- // compilation, M_PI will not be defined.\n-@@ -201,34 +201,13 @@\n-       val);\n- }\n- \n--// Broadcast using numpy-style broadcasting semantics.\n--// This is only valid if the CHLO op has static shaped operands, and no\n--// explicitly specified broadcast_dimensions.\n--//\n--// Asserts that input is ranked tensor type.\n--Value numpyBroadcastIfNeeded(Value op, RankedTensorType opResultType,\n--                             PatternRewriter& rewriter) {\n--  RankedTensorType inputType = cast<RankedTensorType>(op.getType());\n--  RankedTensorType broadcastedResultType =\n--      opResultType.clone(inputType.getElementType());\n--\n--  // No broadcasting needed if input type matches broadcasted result type.\n--  if (inputType == broadcastedResultType) return op;\n--\n--  // broadcast dims are the last dims for numpy style broadcasting.\n--  int64_t inputRank = inputType.getRank();\n--  int64_t resultRank = opResultType.getRank();\n--  auto broadcastDimensions =\n--      llvm::to_vector(llvm::seq<int64_t>(resultRank - inputRank, resultRank));\n--  return stablehlo::BroadcastInDimOp::create(rewriter, op.getLoc(),\n--                                             broadcastedResultType, op,\n--                                             broadcastDimensions)\n--      .getResult();\n--}\n--\n- //===----------------------------------------------------------------------===//\n- // Broadcasting Patterns.\n- //===----------------------------------------------------------------------===//\n-+\n-+bool isStaticOrBoundedDynamicTensor(RankedTensorType type) {\n-+  return type.hasStaticShape() || hlo::isBoundedDynamic(type);\n-+}\n- \n- // Converts binary ops that statically are determined to not broadcast directly\n- // to the corresponding stablehlo non-broadcasting op.\n-@@ -243,12 +222,14 @@\n-     // Only rewrite for statically determinable non-broadcasting cases.\n-     auto lhsType = dyn_cast<RankedTensorType>(adaptor.getLhs().getType());\n-     auto rhsType = dyn_cast<RankedTensorType>(adaptor.getRhs().getType());\n--    if (!lhsType || !rhsType || lhsType.getShape() != rhsType.getShape() ||\n--        !lhsType.hasStaticShape() || !rhsType.hasStaticShape())\n-+    if (!lhsType || !rhsType || !isStaticOrBoundedDynamicTensor(lhsType) ||\n-+        !isStaticOrBoundedDynamicTensor(rhsType) ||\n-+        lhsType.getShape() != rhsType.getShape() ||\n-+        lhsType.getEncoding() != rhsType.getEncoding())\n-       return rewriter.notifyMatchFailure(\n-           op,\n-           \"expected LHS and RHS to be ranked tensors with matching shapes that \"\n--          \"are all static\");\n-+          \"are all static or bounded dynamic\");\n- \n-     rewriter.replaceOp(\n-         op, ValueRange{Adaptor::createOp(op, op.getType(),\n-@@ -270,41 +251,46 @@\n-     // Only rewrite for statically determinable non-broadcasting cases.\n-     auto lhsType = dyn_cast<RankedTensorType>(adaptor.getLhs().getType());\n-     auto rhsType = dyn_cast<RankedTensorType>(adaptor.getRhs().getType());\n--    if (!lhsType || !rhsType || !lhsType.hasStaticShape() ||\n--        !rhsType.hasStaticShape())\n-+    if (!lhsType || !rhsType || !isStaticOrBoundedDynamicTensor(lhsType) ||\n-+        !isStaticOrBoundedDynamicTensor(rhsType))\n-       return rewriter.notifyMatchFailure(\n-           op,\n--          \"expected LHS and RHS to be ranked tensor types with static \"\n--          \"shape\");\n-+          \"expected LHS and RHS to be ranked tensor types with static or \"\n-+          \"bounded dynamic shape\");\n- \n-     // Rely on CHLO type inference to figure out the proper broadcasted shape.\n-     auto resultType = dyn_cast<RankedTensorType>(op.getResult().getType());\n--    if (!resultType || !resultType.hasStaticShape())\n-+    if (!resultType || !isStaticOrBoundedDynamicTensor(resultType))\n-       return rewriter.notifyMatchFailure(\n--          op, \"expected result to be a ranked tensor type with static shape\");\n-+          op,\n-+          \"expected result to be a ranked tensor type with static or bounded \"\n-+          \"dynamic shape\");\n- \n-     auto lhs = adaptor.getLhs();\n-     auto rhs = adaptor.getRhs();\n-     auto broadcastDimensions = adaptor.getBroadcastDimensions();\n-     if (broadcastDimensions &&\n--        !hlo::isLegalNumpyRankedBroadcast(lhs, rhs, *broadcastDimensions))\n-+        !hlo::isLegalNumpyRankedBroadcast(lhs, rhs, *broadcastDimensions)) {\n-       return rewriter.notifyMatchFailure(\n-           op,\n-           \"expected implicit broadcast_dimensions or numpy-style broadcasting\");\n-+    }\n- \n-     LLVM_DEBUG(llvm::dbgs()\n-                << \"CHLO Decomposing \" << op->getName() << \" with broadcast \"\n-                << lhsType << \" x \" << rhsType << \" -> \" << resultType << \"\\n\");\n- \n--    // If operands are static directly create stablehlo broadcasting ops.\n--    // Use numpy-style broadcasting with using StableHLO broadcast ops,\n--    // when user didn't specify broadcast_dimensions.\n--    auto lhsBroadcast =\n--        numpyBroadcastIfNeeded(adaptor.getLhs(), resultType, rewriter);\n--    auto rhsBroadcast =\n--        numpyBroadcastIfNeeded(adaptor.getRhs(), resultType, rewriter);\n--    auto result = Adaptor::createOp(op, resultType,\n--                                    {lhsBroadcast, rhsBroadcast}, rewriter);\n-+    // If operands are static or bounded dynamic, directly create stablehlo\n-+    // broadcasting ops. Use numpy-style broadcasting with using StableHLO\n-+    // broadcast ops. Can leave off broadcast_dimensions since the above\n-+    // logic verifies that they are the default for numpy-style broadcasting.\n-+    mlir::SmallVector<Value> broadcastOperands = {lhs, rhs};\n-+    auto broadcasted_values =\n-+        stablehlo::numpyBroadcastIfNeeded(rewriter, broadcastOperands);\n-+    if (failed(broadcasted_values)) return failure();\n-+\n-+    auto result =\n-+        Adaptor::createOp(op, resultType, *broadcasted_values, rewriter);\n-     rewriter.replaceOp(op, {result.getResult()});\n-     return success();\n-   }\n-@@ -425,7 +411,21 @@\n-       return success();\n-     }\n- \n--    // Lower to broadcasted constant.\n-+    // Lower to cst -> broadcast -> set_dimension_size if bounded dynamic.\n-+    if (hlo::isBoundedDynamic(resultTy)) {\n-+      Value constant = mlir::stablehlo::ConstantOp::create(\n-+          rewriter, op.getLoc(), op.getValue());\n-+      mlir::FailureOr<stablehlo::Dimensions> operandDims =\n-+          getDimensions(adaptor.getOperand());\n-+      if (failed(operandDims)) return failure();\n-+      mlir::FailureOr<Value> broadcast =\n-+          stablehlo::numpyBroadcastIfNeeded(rewriter, constant, *operandDims);\n-+      if (failed(broadcast)) return failure();\n-+      rewriter.replaceOp(op, *broadcast);\n-+      return success();\n-+    }\n-+\n-+    // Lower unbounded dynamic to broadcasted constant.\n-     Location loc = op.getLoc();\n-     Value constant =\n-         mlir::stablehlo::ConstantOp::create(rewriter, loc, op.getValue());\n-diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n---- stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n-+++ stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n-@@ -59,26 +59,6 @@\n-   };\n- }\n- \n--FailureOr<Dimensions> getDimensions(Value op) {\n--  // Get tensor type\n--  mlir::RankedTensorType tensor_type = dyn_cast<RankedTensorType>(op.getType());\n--  if (!tensor_type)\n--    return emitError(op.getLoc(),\n--                     \"expected ranked tensor type for broadcast inputs\");\n--\n--  auto encoding =\n--      mlir::dyn_cast_if_present<mlir::stablehlo::TypeExtensionsAttr>(\n--          tensor_type.getEncoding());\n--\n--  Dimensions dimensions;\n--  dimensions.reserve(tensor_type.getRank());\n--  for (int64_t idx = 0; idx < tensor_type.getRank(); ++idx) {\n--    auto dimInfo = getDimensionInfo(op, tensor_type, encoding, idx);\n--    dimensions.push_back(dimInfo);\n--  }\n--  return dimensions;\n--}\n--\n- FailureOr<Dimensions> getNumpyBroadcastShapeWithBounds(Value op,\n-                                                        const Dimensions& a,\n-                                                        const Dimensions& b) {\n-@@ -130,6 +110,28 @@\n-   LLVM_DEBUG(llvm::dbgs() << \"[getNumpyBroadcastShapeWithBounds] result: \"\n-                           << toString(result) << \"\\n\");\n-   return result;\n-+}\n-+\n-+}  // namespace\n-+\n-+FailureOr<Dimensions> getDimensions(Value op) {\n-+  // Get tensor type\n-+  mlir::RankedTensorType tensor_type = dyn_cast<RankedTensorType>(op.getType());\n-+  if (!tensor_type)\n-+    return emitError(op.getLoc(),\n-+                     \"expected ranked tensor type for broadcast inputs\");\n-+\n-+  auto encoding =\n-+      mlir::dyn_cast_if_present<mlir::stablehlo::TypeExtensionsAttr>(\n-+          tensor_type.getEncoding());\n-+\n-+  Dimensions dimensions;\n-+  dimensions.reserve(tensor_type.getRank());\n-+  for (int64_t idx = 0; idx < tensor_type.getRank(); ++idx) {\n-+    auto dimInfo = getDimensionInfo(op, tensor_type, encoding, idx);\n-+    dimensions.push_back(dimInfo);\n-+  }\n-+  return dimensions;\n- }\n- \n- mlir::RankedTensorType getRankedTensorType(const Dimensions& dims,\n-@@ -155,7 +157,6 @@\n-   return mlir::RankedTensorType::get(shape, element_type, encoding);\n- }\n- \n--}  // namespace\n- \n- FailureOr<Dimensions> getNumpyBroadcastShape(OpBuilder& builder,\n-                                              ArrayRef<Value> ops) {\n-diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h b/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n---- stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n-+++ stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n-@@ -47,6 +47,14 @@\n- using Dimensions = SmallVector<DimensionInfo>;\n- std::string toString(const Dimensions& dims);\n- \n-+// Returns the dimensions of the given op, or failure if the op's type is not a\n-+// ranked tensor.\n-+FailureOr<Dimensions> getDimensions(Value op);\n-+\n-+// Returns the ranked tensor type with the given dimensions and element type.\n-+mlir::RankedTensorType getRankedTensorType(const Dimensions& dims,\n-+                                           mlir::Type element_type);\n-+\n- // Returns the common shape these ops would broadcast to, or an error if the\n- // ops are not broadcastable.\n- FailureOr<Dimensions> getNumpyBroadcastShape(OpBuilder& builder,\n-diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n---- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n-+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n-@@ -701,22 +701,50 @@\n-     if (failed(validateShapeFoldDtype(rewriter, op, resultType)))\n-       return failure();\n- \n-+    ComparisonType comparisonType = getComparisonType(op);\n-+    if (comparisonType == ComparisonType::NOTYPE)\n-+      return rewriter.notifyMatchFailure(\n-+          op, \"Could not determine comparison type.\");\n-+\n-+    LLVM_DEBUG(llvm::dbgs() << \"comparisonType: \" << comparisonType << \"\\n\");\n-+\n-     auto res = foldBinaryOpIntOrFloat<FoldCompare, IntegerAttr, IntegerAttr>(\n--        rewriter, op,\n--        FoldCompare(op.getComparisonDirection(), op.getCompareType()));\n-+        rewriter, op, FoldCompare(op.getComparisonDirection(), comparisonType));\n-     if (failed(res)) return failure();\n-     rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());\n-     return success();\n-   }\n- \n-+  // Return the comparison type if set, else return the assumed comparison type\n-+  // according to the StableHLO spec.\n-+  ComparisonType getComparisonType(CompareOp op) const {\n-+    auto compareType = op.getCompareType();\n-+    if (compareType.has_value() &&\n-+        compareType.value() != ComparisonType::NOTYPE)\n-+      return *compareType;\n-+\n-+    Type elementType = op.getLhs().getType().getElementType();\n-+    if (elementType.isUnsignedInteger() || elementType.isSignlessInteger(1))\n-+      return ComparisonType::UNSIGNED;\n-+    if (elementType.isSignlessInteger())\n-+      return ComparisonType::SIGNED;\n-+    else if (elementType.isFloat() || mlir::isa<ComplexType>(elementType))\n-+      return ComparisonType::FLOAT;\n-+    else\n-+      return ComparisonType::NOTYPE;\n-+  }\n-+\n-   struct FoldCompare {\n-     FoldCompare(ComparisonDirection direction,\n--                std::optional<ComparisonType> kind)\n-+                ComparisonType kind)\n-         : direction(direction), kind(kind) {}\n-     ComparisonDirection direction;\n--    std::optional<ComparisonType> kind;\n-+    ComparisonType kind;\n- \n-     APInt operator()(APFloat lhs, APFloat rhs) {\n-+      if (kind != ComparisonType::FLOAT && kind != ComparisonType::TOTALORDER)\n-+        llvm::report_fatal_error(\"invalid float comparison\");\n-+\n-       bool result = false;\n-       switch (direction) {\n-         case ComparisonDirection::EQ:\n-@@ -741,6 +769,9 @@\n-       return APInt(/*bitwidth=*/1, result);\n-     }\n-     APInt operator()(APInt lhs, APInt rhs) {\n-+      if (kind != ComparisonType::UNSIGNED && kind != ComparisonType::SIGNED)\n-+        llvm::report_fatal_error(\"invalid integer comparison\");\n-+\n-       bool result = false;\n-       switch (direction) {\n-         case ComparisonDirection::EQ:\n "
        },
        {
            "sha": "ed5215b42b4c3093174496dac09265daa1ab342f",
            "filename": "third_party/xla/third_party/stablehlo/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c2da2414305086336f510dce6fb318c14928df0c/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c2da2414305086336f510dce6fb318c14928df0c/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Fworkspace.bzl?ref=c2da2414305086336f510dce6fb318c14928df0c",
            "patch": "@@ -4,8 +4,8 @@ load(\"//third_party:repo.bzl\", \"tf_http_archive\", \"tf_mirror_urls\")\n \n def repo():\n     # LINT.IfChange\n-    STABLEHLO_COMMIT = \"1ef9e390b5295e676d2b864fe1924bc2f3f4cf0f\"\n-    STABLEHLO_SHA256 = \"818c951ad0ba0ac6c26d3ed01fed8f9a0e5ca93f5aed35005f75f0faf11bdfb0\"\n+    STABLEHLO_COMMIT = \"d496423cdb7f7d5272f14d517681202a0b9cbe41\"\n+    STABLEHLO_SHA256 = \"eac3bd19f6c0b86ed3216b63d871d7c34a1aa679ca4a34975fe70fd043b34b85\"\n     # LINT.ThenChange(Google-internal path)\n \n     tf_http_archive("
        }
    ],
    "stats": {
        "total": 1060,
        "additions": 2,
        "deletions": 1058
    }
}