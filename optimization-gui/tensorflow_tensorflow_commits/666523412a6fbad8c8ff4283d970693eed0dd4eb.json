{
    "author": "bartchr808",
    "message": "#sdy Make sure the named scope of shard maps is preserved.\n\nThere was a bug that, due to the MLIR inliner not fusing location information, but the XLA inliner fusing it, the final optimized HLO didn't have the entire call stack for operations inside the `shard_map`.\n\nFor example after JAX lowering the stack for the shard map was `jit(train)/transformer/attention/` and then inside an operation would have `foo/add`. After inlining the operation would just have `foo/add`. But it should be `jit(train)/transformer/attention/foo/add`.\n\nNOTE: this changes the behavior of how ManualComputations are exported out of ShardyXLA. They will now always be FullToShard/ShardToFull with a call instruction - never inlined. As we need the XLA inliner to merge the location info.\nPiperOrigin-RevId: 803000362",
    "sha": "666523412a6fbad8c8ff4283d970693eed0dd4eb",
    "files": [
        {
            "sha": "297264c02bef0a75218904941c222b6fed4c79ea",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/stablehlo_utils.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fstablehlo_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fstablehlo_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fstablehlo_utils.cc?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -49,7 +49,6 @@ absl::StatusOr<std::unique_ptr<xla::HloModule>> ConvertShardyToHlo(\n   // need to add an option to to convert to custom call @Sharding.\n   xla::sdy::StablehloExportPipelineOptions options;\n   options.keepHloShardingConstraints = true;\n-  options.keepShardMapBodyAsFunc = true;\n   xla::sdy::addStablehloExportPipeline(pm, options);\n \n   mlir::BaseScopedDiagnosticHandler diagnostic_handler("
        },
        {
            "sha": "b847b95271696339ffd81939083aa90e85f614ac",
            "filename": "third_party/xla/xla/mlir_hlo/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fmlir_hlo%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fmlir_hlo%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2FBUILD?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -410,6 +410,7 @@ cc_library(\n     strip_include_prefix = \".\",\n     deps = [\n         \":mlir_hlo\",\n+        \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:FuncTransforms\",\n         \"@llvm-project//mlir:IR\","
        },
        {
            "sha": "0e13dcb3c39a9950c68600207a00c4646d62ee33",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/utils/type_conversion.cc",
            "status": "modified",
            "additions": 59,
            "deletions": 2,
            "changes": 61,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Ftype_conversion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Ftype_conversion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Ftype_conversion.cc?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -15,19 +15,26 @@ limitations under the License.\n \n #include \"mhlo/utils/type_conversion.h\"\n \n-#include <optional>\n+#include <cassert>\n+#include <cstddef>\n \n+#include \"llvm/ADT/STLExtras.h\"\n #include \"mhlo/IR/hlo_ops.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/Dialect/Func/Transforms/FuncConversions.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/Dialect.h\"\n+#include \"mlir/IR/Location.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"mlir/IR/Types.h\"\n #include \"mlir/IR/Value.h\"\n+#include \"mlir/IR/ValueRange.h\"\n #include \"mlir/Support/LLVM.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n #include \"stablehlo/dialect/StablehloOps.h\"\n \n namespace mlir {\n@@ -94,6 +101,55 @@ Value scalarToTensor(OpBuilder& builder, Type type,\n   return result;\n }\n \n+// Flatten the given value ranges into a single vector of values.\n+SmallVector<Value> flattenValues(ArrayRef<ValueRange> values) {\n+  SmallVector<Value> result;\n+  for (const auto& vals : values) llvm::append_range(result, vals);\n+  return result;\n+}\n+\n+// Exact same as `CallOpSignatureConversion`, except this one preserves\n+// discardable attributes.\n+struct CallOpSignatureConversion : public OpConversionPattern<func::CallOp> {\n+  using OpConversionPattern<func::CallOp>::OpConversionPattern;\n+\n+  /// Hook for derived classes to implement combined matching and rewriting.\n+  LogicalResult matchAndRewrite(\n+      func::CallOp callOp, OneToNOpAdaptor adaptor,\n+      ConversionPatternRewriter& rewriter) const override {\n+    // Convert the original function results. Keep track of how many result\n+    // types an original result type is converted into.\n+    SmallVector<size_t> numResultsReplacements;\n+    SmallVector<Type, 1> convertedResults;\n+    size_t numFlattenedResults = 0;\n+    for (auto [idx, type] : llvm::enumerate(callOp.getResultTypes())) {\n+      if (failed(typeConverter->convertTypes(type, convertedResults)))\n+        return failure();\n+      numResultsReplacements.push_back(convertedResults.size() -\n+                                       numFlattenedResults);\n+      numFlattenedResults = convertedResults.size();\n+    }\n+\n+    // Substitute with the new result types from the corresponding FuncType\n+    // conversion.\n+    auto newCallOp = func::CallOp::create(rewriter, callOp.getLoc(),\n+                                          callOp.getCallee(), convertedResults,\n+                                          flattenValues(adaptor.getOperands()));\n+    newCallOp->setAttrs(callOp->getAttrs());\n+    SmallVector<ValueRange> replacements;\n+    size_t offset = 0;\n+    for (int i = 0, e = callOp->getNumResults(); i < e; ++i) {\n+      replacements.push_back(\n+          newCallOp->getResults().slice(offset, numResultsReplacements[i]));\n+      offset += numResultsReplacements[i];\n+    }\n+    assert(offset == convertedResults.size() &&\n+           \"expected that all converted results are used\");\n+    rewriter.replaceOpWithMultiple(callOp, replacements);\n+    return success();\n+  }\n+};\n+\n }  // namespace\n \n RemoveSignTypeConverter::RemoveSignTypeConverter() {\n@@ -232,7 +288,8 @@ void registerFuncOpsForTypeConversion(ConversionTarget& target,\n   });\n   populateFunctionOpInterfaceTypeConversionPattern<func::FuncOp>(patterns,\n                                                                  converter);\n-  populateCallOpTypeConversionPattern(patterns, converter);\n+  patterns.add<mhlo::CallOpSignatureConversion>(converter,\n+                                                patterns.getContext());\n   populateReturnOpTypeConversionPattern(patterns, converter);\n }\n "
        },
        {
            "sha": "413cd48e22b6be3298de914e365e682dea1ad01f",
            "filename": "third_party/xla/xla/mlir_hlo/tests/Dialect/mhlo/stablehlo-legalize-to-hlo.mlir",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fstablehlo-legalize-to-hlo.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fstablehlo-legalize-to-hlo.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fstablehlo-legalize-to-hlo.mlir?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -2468,6 +2468,23 @@ func.func @type_tuple(%arg0: tuple<tensor<f32>>) -> tuple<!stablehlo.token> {\n \n // -----\n \n+// ============ TYPES ============\n+// Tests how StableHLO types are legalized to MHLO types.\n+\n+\n+// Make sure discardable attributes on CallOps with token types are preserved\n+// CHECK-LABEL: preserve_discardable_attrs_on_call\n+func.func @preserve_discardable_attrs_on_call(%arg0: !stablehlo.token {mhlo.sharding = \"{replicated}\"}) -> !stablehlo.token {\n+  // CHECK: \"func.call\"(%arg1) <{callee = @calling_func}> {mhlo.sharding = \"{manual}\"} : (!mhlo.token) -> !mhlo.token\n+  %0 = call @calling_func(%arg0) {mhlo.sharding = \"{manual}\"} : (!stablehlo.token) -> !stablehlo.token\n+  return %0 : !stablehlo.token\n+}\n+func.func @calling_func(%arg0: !stablehlo.token {mhlo.sharding = \"{manual}\"}) -> (!stablehlo.token {mhlo.sharding = \"{manual}\"}) {\n+  return %arg0 : !stablehlo.token\n+}\n+\n+// -----\n+\n // ============ NEGATIVE TESTS ============\n // Some ops, attributes and types used in StableHLO programs are not supported in MHLO.\n // For those cases, we have negative tests below."
        },
        {
            "sha": "b6c5a7613bdd570d14edc8fb726a3e9e1515ea1f",
            "filename": "third_party/xla/xla/service/call_inliner.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fcall_inliner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fcall_inliner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcall_inliner.cc?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -308,7 +308,7 @@ bool CallInliner::IsInlineableCallOp(HloInstruction* instruction) const {\n   if (instruction->GetModule()->config().use_shardy_partitioner() &&\n       (absl::StrContains(instruction->to_apply()->name(), \"shmap_body\") ||\n        absl::StrContains(instruction->to_apply()->name(),\n-                         sdy::kManualComputationBodyFuncName.str()))) {\n+                         sdy::kManualComputationFuncName.str()))) {\n     // TODO(b/436603025). Remove this special handling by marking the\n     // instruction as uninlineable with the frontend attribute.\n     //\n@@ -318,7 +318,7 @@ bool CallInliner::IsInlineableCallOp(HloInstruction* instruction) const {\n     // - shmap_body: We do not want to inline the bodies of JAX shard maps to\n     //   import them into an `sdy.ManualComputationOp`. This is for the MHLO\n     //   round-trip pipeline\n-    // - kManualComputationBodyFuncName: Same as shmap_body except for the SDY\n+    // - kManualComputationFuncName: Same as shmap_body except for the SDY\n     //   round-trip pipeline.\n     return false;\n   }"
        },
        {
            "sha": "73d042f3a7d25cfaf6b7719b95e6f06483ae861f",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -338,6 +338,7 @@ cc_library(\n         \"//xla/service/llvm_ir:llvm_command_line_options\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/service/spmd:stateful_rng_spmd_partitioner\",\n+        \"//xla/service/spmd/shardy:constants\",\n         \"//xla/service/spmd/shardy:shardy_xla_pass\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream_executor_h\","
        },
        {
            "sha": "a38ddd6408e83c5950d6b57de52abda4f8d5ec99",
            "filename": "third_party/xla/xla/service/cpu/cpu_compiler.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fcpu_compiler.cc?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -43,6 +43,7 @@ limitations under the License.\n #include \"absl/memory/memory.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/match.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/str_join.h\"\n@@ -217,6 +218,7 @@ limitations under the License.\n #include \"xla/service/sharding_propagation.h\"\n #include \"xla/service/sharding_remover.h\"\n #include \"xla/service/slow_operation_alarm.h\"\n+#include \"xla/service/spmd/shardy/constants.h\"\n #include \"xla/service/spmd/shardy/shardy_xla_pass.h\"\n #include \"xla/service/spmd/stateful_rng_spmd_partitioner.h\"\n #include \"xla/service/topk_rewriter.h\"\n@@ -560,6 +562,16 @@ absl::Status CpuCompiler::RunHloPassesThroughLayoutAssn(\n     }\n     spmd_pipeline.AddPass<spmd::StatefulRngSpmdPartitioner>(\n         num_partitions, module->config().replica_count());\n+    spmd_pipeline.AddPass<xla::CallInliner>(\n+        /*single_call_site=*/false,\n+        /*update_domain=*/false,\n+        /*composites_to_preserve=*/absl::flat_hash_set<std::string>{},\n+        /*uniquify_channel_ids=*/false,\n+        /*should_inline=*/\n+        [](const xla::CallGraph& call_graph, xla::HloInstruction* instruction) {\n+          return absl::StrContains(instruction->to_apply()->name(),\n+                                   sdy::kInlineableManualComputationFuncName);\n+        });\n     TF_RETURN_IF_ERROR(spmd_pipeline.Run(module).status());\n   } else {\n     HloPassPipeline sharding_removal_pipeline(\"sharding-removal\");"
        },
        {
            "sha": "7f98cbd3e657ae4a4cbbf00ec906606e6b76933a",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -2309,6 +2309,8 @@ cc_library(\n         \"//xla/hlo/transforms/simplifiers:reshape_mover\",\n         \"//xla/hlo/transforms/simplifiers:sort_simplifier\",\n         \"//xla/hlo/transforms/simplifiers:tuple_simplifier\",\n+        \"//xla/service:call_graph\",\n+        \"//xla/service:call_inliner\",\n         \"//xla/service:conditional_simplifier\",\n         \"//xla/service:gather_expander\",\n         \"//xla/service:hlo_module_config\",\n@@ -2319,10 +2321,13 @@ cc_library(\n         \"//xla/service/gpu/transforms:algebraic_simplifier\",\n         \"//xla/service/spmd:collective_permute_motion\",\n         \"//xla/service/spmd:stateful_rng_spmd_partitioner\",\n+        \"//xla/service/spmd/shardy:constants\",\n         \"//xla/service/spmd/shardy:shardy_xla_pass\",\n         \"//xla/stream_executor:device_description\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/functional:function_ref\",\n         \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/strings\",\n     ],\n )\n "
        },
        {
            "sha": "8b1e5ee5874aada474bba137315143dfcc072134",
            "filename": "third_party/xla/xla/service/gpu/gpu_spmd_pipeline.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_spmd_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_spmd_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_spmd_pipeline.cc?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -17,9 +17,12 @@ limitations under the License.\n \n #include <cstdint>\n #include <optional>\n+#include <string>\n \n+#include \"absl/container/flat_hash_set.h\"\n #include \"absl/functional/function_ref.h\"\n #include \"absl/log/check.h\"\n+#include \"absl/strings/match.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_schedule.h\"\n@@ -32,13 +35,16 @@ limitations under the License.\n #include \"xla/hlo/transforms/simplifiers/reshape_mover.h\"\n #include \"xla/hlo/transforms/simplifiers/sort_simplifier.h\"\n #include \"xla/hlo/transforms/simplifiers/tuple_simplifier.h\"\n+#include \"xla/service/call_graph.h\"\n+#include \"xla/service/call_inliner.h\"\n #include \"xla/service/conditional_simplifier.h\"\n #include \"xla/service/gather_expander.h\"\n #include \"xla/service/gpu/transforms/algebraic_simplifier.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/scatter_expander.h\"\n #include \"xla/service/sharding_propagation.h\"\n #include \"xla/service/spmd/collective_permute_motion.h\"\n+#include \"xla/service/spmd/shardy/constants.h\"\n #include \"xla/service/spmd/shardy/shardy_xla_pass.h\"\n #include \"xla/service/spmd/stateful_rng_spmd_partitioner.h\"\n #include \"xla/service/while_loop_constant_sinking.h\"\n@@ -128,6 +134,20 @@ void AddSPMDPasses(\n       /*disable_ag_rewrite_for_multiple_consumers=*/true,\n       /*enable_partial_windowed_einsums=*/true, oper_size_threshold,\n       max_windowed_einsum_iteration);\n+  // NOTE: even though the inliner is called in `RunPreSPMDPartitionerPasses`,\n+  // it doesn't inline functions needed for ShardyXLA. ShardyXLA will also leave\n+  // functions called `kInlineableManualComputationFuncName` not inlined, so\n+  // we need to call the inliner again here.\n+  spmd_pipeline.AddPass<xla::CallInliner>(\n+      /*single_call_site=*/false,\n+      /*update_domain=*/false,\n+      /*composites_to_preserve=*/absl::flat_hash_set<std::string>{},\n+      /*uniquify_channel_ids=*/false,\n+      /*should_inline=*/\n+      [](const xla::CallGraph& call_graph, xla::HloInstruction* instruction) {\n+        return absl::StrContains(instruction->to_apply()->name(),\n+                                 sdy::kInlineableManualComputationFuncName);\n+      });\n   spmd_pipeline.AddPass<CollectivePermuteMotion>();\n }\n "
        },
        {
            "sha": "62a9a9ff76df9105b1217e5af6532be1b0d3bca0",
            "filename": "third_party/xla/xla/service/spmd/shardy/constants.h",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fconstants.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fconstants.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fconstants.h?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -132,11 +132,16 @@ inline constexpr llvm::StringRef kManualAxes = \"xla.sdy.manual_axes\";\n // `true` or not set.\n inline constexpr char kHasUnreducedAxes[] = \"xla.sdy.has_unreduced_axes\";\n \n-// The function name of the of the body of a `ManualComputationOp` during Shardy\n-// round tripping. Used\n-inline constexpr llvm::StringRef kManualComputationBodyFuncName =\n+// The function name of the body of a `ManualComputationOp` during Shardy\n+// round tripping.\n+inline constexpr llvm::StringRef kManualComputationFuncName =\n     \"xla.sdy.manual_computation_body\";\n \n+// The function name of the body of a `ManualComputationOp` that should\n+// be inlined after stablehlo-round-trip-export\n+inline constexpr llvm::StringRef kInlineableManualComputationFuncName =\n+    \"xla.sdy.inlinable_manual_computation_body\";\n+\n // The target name of the custom call that changes operands from global to local\n // shape during Shardy round tripping.\n inline constexpr llvm::StringRef kGlobalToLocalShapeCallTargetName ="
        },
        {
            "sha": "7803fcd862cf9f0a54e5fea6cd21bda58170cba7",
            "filename": "third_party/xla/xla/service/spmd/shardy/sdy_round_trip/clone_manual_computation_calls.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fsdy_round_trip%2Fclone_manual_computation_calls.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fsdy_round_trip%2Fclone_manual_computation_calls.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fsdy_round_trip%2Fclone_manual_computation_calls.cc?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -68,7 +68,7 @@ class SdyRoundTripCloneManualComputationCallsPass\n     // Clone multiple calls to the same function.\n     llvm::DenseSet<StringRef> seenCalleeNames;\n     moduleOp->walk([&](CallOp op) {\n-      if (!op.getCallee().contains(kManualComputationBodyFuncName)) {\n+      if (!op.getCallee().contains(kManualComputationFuncName)) {\n         return;\n       }\n       if (seenCalleeNames.insert(op.getCallee()).second) {"
        },
        {
            "sha": "cd32cf0daad54d4f6495c680a12a3de89e9ba8ef",
            "filename": "third_party/xla/xla/service/spmd/shardy/sdy_round_trip/shard_map_export.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fsdy_round_trip%2Fshard_map_export.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fsdy_round_trip%2Fshard_map_export.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fsdy_round_trip%2Fshard_map_export.cc?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -82,7 +82,7 @@ class SdyRoundTripShardMapExportPass\n       mlir::TypeRange localResultTypes =\n           sdy::getBodyTerminatorOpOperandTypes(manualComputation);\n       auto funcOp = FuncOp::create(\n-          rewriter, loc, kManualComputationBodyFuncName,\n+          rewriter, loc, kManualComputationFuncName,\n           rewriter.getFunctionType(manualCompBodyArgTypes, localResultTypes));\n       mlir::StringAttr funcName = symbolTable.insert(funcOp);\n "
        },
        {
            "sha": "b7fc7b48c2be4e8df7f71afe3c45915c96db5247",
            "filename": "third_party/xla/xla/service/spmd/shardy/sdy_round_trip/shard_map_import.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fsdy_round_trip%2Fshard_map_import.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fsdy_round_trip%2Fshard_map_import.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fsdy_round_trip%2Fshard_map_import.cc?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -77,7 +77,7 @@ class ManualComputationPattern : public OpConversionPattern<CallOp> {\n   mlir::LogicalResult matchAndRewrite(\n       CallOp callOp, OpAdaptor adaptor,\n       mlir::ConversionPatternRewriter& rewriter) const override {\n-    if (!callOp.getCallee().contains(kManualComputationBodyFuncName)) {\n+    if (!callOp.getCallee().contains(kManualComputationFuncName)) {\n       return mlir::failure();\n     }\n \n@@ -219,7 +219,7 @@ class SdyRoundTripShardMapImportPass\n     MLIRContext& context = getContext();\n     mlir::ConversionTarget target(context);\n     target.addDynamicallyLegalOp<CallOp>([](CallOp op) {\n-      return !op.getCallee().contains(kManualComputationBodyFuncName);\n+      return !op.getCallee().contains(kManualComputationFuncName);\n     });\n     target.addLegalOp<sdy::ManualComputationOp, sdy::ReturnOp, CustomCallOp>();\n     mlir::RewritePatternSet patterns(&context);"
        },
        {
            "sha": "844cec98c2be8d60ad2495a5adba0ac3fbfcf9b1",
            "filename": "third_party/xla/xla/service/spmd/shardy/shardy_xla_pass.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fshardy_xla_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fshardy_xla_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fshardy_xla_pass.cc?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -405,7 +405,7 @@ bool eraseInlineableAttrForShardyManualComputations(HloModule* module) {\n         continue;\n       }\n       if (absl::StrContains(instruction->to_apply()->name(),\n-                            sdy::kManualComputationBodyFuncName.str())) {\n+                            sdy::kManualComputationFuncName.str())) {\n         instruction->erase_frontend_attribute(kXlaInlineableAttr);\n         // TODO(b/436603025). CallInliner do not inline the Shardy related\n         // manual computations based on the callee name. We have to rename the"
        },
        {
            "sha": "03bdd41fad50fb4c6405514081c9b6e59ed1c85b",
            "filename": "third_party/xla/xla/service/spmd/shardy/shardy_xla_pass_test.cc",
            "status": "modified",
            "additions": 30,
            "deletions": 4,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fshardy_xla_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fshardy_xla_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fshardy_xla_pass_test.cc?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -770,10 +770,10 @@ TEST_F(ShardyXLATest, ShardMap) {\n                           ParseAndReturnVerifiedModule(hloString));\n   runShardyWithStablehloImport(module.get());\n \n-  // The entry computation and the region_add for the all-reduce. shmap_body is\n-  // inlined.\n-  EXPECT_EQ(module->computation_count(), 2);\n-  EXPECT_EQ(FindInstruction(module.get(), xla::HloOpcode::kCall), nullptr);\n+  // The entry computation, the region_add for the all-reduce, and the\n+  // shmap_body.\n+  EXPECT_EQ(module->computation_count(), 3);\n+  EXPECT_TRUE(FindInstruction(module.get(), xla::HloOpcode::kCall));\n \n   auto* dot = FindInstruction(module.get(), xla::HloOpcode::kDot);\n   EXPECT_NE(dot, nullptr);\n@@ -1129,5 +1129,31 @@ TEST_F(ShardyXLATest, UpdateInlineableAttr) {\n   EXPECT_EQ(root->to_apply()->name(), \"inlineable_callee\");\n }\n \n+TEST_F(ShardyXLATest, ManualComputationCallOpWithToken) {\n+  const char* const hloString = R\"(\n+    HloModule main, entry_computation_layout={(token[])->token[]}, frontend_attributes={xla.sdy.meshes={mesh = #sdy.mesh<[\"x\"=2]>}}\n+\n+    %xla.sdy.manual_computation_body.4 (Arg_0.3: token[]) -> token[] {\n+      ROOT %Arg_0.3 = token[] parameter(0)\n+    }\n+\n+    ENTRY %main.7 (Arg_0.1: token[]) -> token[] {\n+      %Arg_0.1 = token[] parameter(0)\n+      %custom-call.2 = token[] custom-call(%Arg_0.1), custom_call_target=\"xla.sdy.GlobalToLocalShape\", custom_call_has_side_effect=true, frontend_attributes={xla.sdy.in_shardings=\"#sdy.sharding_per_value<[<@mesh, []>]>\",xla.sdy.manual_axes=\"#sdy<manual_axes{\\\"x\\\"}>\"}\n+      %call.5 = token[] call(%custom-call.2), to_apply=%xla.sdy.manual_computation_body.4, frontend_attributes={inlineable=\"false\"}\n+      ROOT %custom-call.6 = token[] custom-call(%call.5), custom_call_target=\"xla.sdy.LocalToGlobalShape\", custom_call_has_side_effect=true, frontend_attributes={xla.sdy.manual_axes=\"#sdy<manual_axes{\\\"x\\\"}>\",xla.sdy.out_shardings=\"#sdy.sharding_per_value<[<@mesh, []>]>\"}\n+    })\";\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(hloString));\n+  runShardyWithSdyImport(module.get());\n+  HloInstruction* callInst =\n+      FindInstruction(module.get(), xla::HloOpcode::kCall);\n+  EXPECT_TRUE(callInst);\n+  // StableHLO->HLO conversion used to discard the sharding attribute, due to\n+  // MLIR TypeConversion on CallOps not preserving them. This test ensures that\n+  // the sharding attribute is preserved.\n+  EXPECT_EQ(callInst->sharding().ToString(), \"{manual}\");\n+}\n+\n }  // namespace sdy\n }  // namespace xla"
        },
        {
            "sha": "fa07172afcf334921e078ceb91b5c386e8b674e0",
            "filename": "third_party/xla/xla/service/spmd/shardy/stablehlo_round_trip/export_shardings.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_shardings.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_shardings.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_shardings.cc?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -113,9 +113,8 @@ bool allShardingsUnreduced(ArrayRef<TensorShardingAttr> shardings) {\n }\n \n // Convert the shardings from kShardingAttr into kXlaShardingAttr.\n-LogicalResult exportFunc(FuncOp funcOp, const SymbolTable& symbolTable,\n-                         OpBuilder& builder,\n-                         bool addMissingShardingToControlFlow) {\n+void exportFunc(FuncOp funcOp, const SymbolTable& symbolTable,\n+                OpBuilder& builder, bool addMissingShardingToControlFlow) {\n   std::function<StringAttr(const HloSharding&)> getStringAttr =\n       [&](const HloSharding& hloSharding) {\n         return builder.getStringAttr(hloSharding.ToString());\n@@ -181,8 +180,6 @@ LogicalResult exportFunc(FuncOp funcOp, const SymbolTable& symbolTable,\n       op->setAttr(kXlaShardingAttr, getStringAttr(HloSharding::Replicate()));\n     }\n   });\n-\n-  return success();\n }\n \n class ExportStablehloShardingsPass\n@@ -203,10 +200,7 @@ class ExportStablehloShardingsPass\n     auto builder = OpBuilder::atBlockBegin(&moduleOp.getBodyRegion().front());\n \n     for (auto funcOp : moduleOp.getOps<FuncOp>()) {\n-      if (mlir::failed(exportFunc(funcOp, symbolTable, builder,\n-                                  addMissingShardingToControlFlow))) {\n-        signalPassFailure();\n-      }\n+      exportFunc(funcOp, symbolTable, builder, addMissingShardingToControlFlow);\n     }\n \n     moduleOp.walk([&](stablehlo::CustomCallOp customCall) {"
        },
        {
            "sha": "1a0086454d45aa33dd3fecfc7f851dbe87028390",
            "filename": "third_party/xla/xla/service/spmd/shardy/stablehlo_round_trip/shard_map_export.cc",
            "status": "modified",
            "additions": 57,
            "deletions": 42,
            "changes": 99,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fshard_map_export.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fshard_map_export.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fshard_map_export.cc?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #include \"xla/service/spmd/shardy/stablehlo_round_trip/shard_map_export.h\"\n \n #include <cassert>\n+#include <cstdint>\n #include <memory>\n #include <tuple>\n #include <utility>\n@@ -49,6 +50,7 @@ limitations under the License.\n #include \"mlir/Pass/PassRegistry.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"mlir/Support/TypeID.h\"\n+#include \"mlir/Support/WalkResult.h\"\n #include \"mlir/Transforms/DialectConversion.h\"\n #include \"mlir/Transforms/InliningUtils.h\"\n #include \"shardy/dialect/sdy/ir/constants.h\"\n@@ -194,7 +196,7 @@ void setManualAxesForOpsInBody(\n         // meshes.\n         bool hasOtherMesh = false;\n         for (TensorShardingAttr opInBodySharding :\n-             mlir::sdy::getShardings(opInBody)) {\n+             sdy::getShardings(opInBody)) {\n           if (opInBodySharding.getMeshName() != meshName) {\n             hasOtherMesh = true;\n             MeshAttr otherMesh = opInBodySharding.getMesh(opInBody);\n@@ -220,6 +222,20 @@ void setNonEmptyManualAxes(Operation* op, ManualAxesAttr manualAxesAttr) {\n   }\n }\n \n+void setBlockArgManualAxes(FuncOp funcOp, mlir::BlockArgument blockArg,\n+                           ManualAxesAttr manualAxesAttr) {\n+  if (!manualAxesAttr.empty()) {\n+    funcOp.setArgAttr(blockArg.getArgNumber(), kManualAxes, manualAxesAttr);\n+  }\n+}\n+\n+void setFuncResultManualAxes(FuncOp funcOp, int64_t resultIndex,\n+                             ManualAxesAttr manualAxesAttr) {\n+  if (!manualAxesAttr.empty()) {\n+    funcOp.setResultAttr(resultIndex, kManualAxes, manualAxesAttr);\n+  }\n+}\n+\n // Converts `op` to the pattern that XLA recognizes.\n //\n // The pattern is:\n@@ -237,8 +253,7 @@ void setNonEmptyManualAxes(Operation* op, ManualAxesAttr manualAxesAttr) {\n void convertManualComputationOp(\n     ManualComputationOp op,\n     const ManualComputationToParentManualAxes& parentManualCompAxes,\n-    mlir::SymbolTable& symbolTable, bool keepShardMapBodyAsFunc,\n-    bool createHloShardingConstraints) {\n+    mlir::SymbolTable& symbolTable, bool createHloShardingConstraints) {\n   MLIRContext* context = op.getContext();\n   mlir::IRRewriter rewriter(op);\n   TensorShardingAttr sharding = getFirstSharding(op);\n@@ -297,33 +312,46 @@ void convertManualComputationOp(\n     fullToShardResults.push_back(fullToShard.getResult(0));\n   }\n \n+  rewriter.setInsertionPointToEnd(\n+      &op->getParentOfType<ModuleOp>().getRegion().front());\n   Operation* terminator = sdy::getBodyTerminator(op);\n-\n-  mlir::ValueRange localResults = terminator->getOperands();\n-  if (keepShardMapBodyAsFunc) {\n-    rewriter.setInsertionPointToEnd(\n-        &op->getParentOfType<ModuleOp>().getRegion().front());\n-    auto funcOp =\n-        FuncOp::create(rewriter, loc, \"shmap_body\",\n-                       rewriter.getFunctionType(op.getBody().getArgumentTypes(),\n-                                                terminator->getOperandTypes()));\n-    mlir::StringAttr funcName = symbolTable.insert(funcOp);\n-\n-    rewriter.setInsertionPointAfter(op);\n-    auto callOp = CallOp::create(rewriter, loc, terminator->getOperandTypes(),\n-                                 funcName, fullToShardResults);\n-    sdy::inlineRegionAndConvertTerminatorOp<mlir::func::ReturnOp>(\n-        op.getBody(), funcOp.getBody());\n-    localResults = callOp->getResults();\n-  } else {\n-    rewriter.setInsertionPointAfter(op);\n+  auto funcOp =\n+      FuncOp::create(rewriter, loc, kInlineableManualComputationFuncName,\n+                     rewriter.getFunctionType(op.getBody().getArgumentTypes(),\n+                                              terminator->getOperandTypes()));\n+  mlir::StringAttr funcName = symbolTable.insert(funcOp);\n+\n+  rewriter.setInsertionPointAfter(op);\n+  auto callOp = CallOp::create(rewriter, loc, terminator->getOperandTypes(),\n+                               funcName, fullToShardResults);\n+  setNonEmptyManualAxes(callOp, regionManualAxesAttr);\n+  sdy::inlineRegionAndConvertTerminatorOp<mlir::func::ReturnOp>(\n+      op.getBody(), funcOp.getBody());\n+  for (auto [blockArg, sharding] : llvm::zip_equal(\n+           funcOp.getArguments(), op.getInShardings().getShardings())) {\n+    if (sharding) {\n+      setSharding(blockArg, eraseManualAxes(sharding, manualAxes.region));\n+      setBlockArgManualAxes(funcOp, blockArg, regionManualAxesAttr);\n+    }\n+  }\n+  for (auto [i, sharding] :\n+       llvm::enumerate(op.getOutShardings().getShardings())) {\n+    if (sharding) {\n+      sdy::setFuncResultSharding(funcOp, i,\n+                                 eraseManualAxes(sharding, manualAxes.region));\n+      setFuncResultManualAxes(funcOp, i, regionManualAxesAttr);\n+    }\n   }\n \n+  SmallVector<TensorShardingAttr> erasedManualAxisOutShardings;\n+  erasedManualAxisOutShardings.reserve(op.getNumResults());\n   // Add custom_call @SPMDShardToFullShape and sharding-constraint for each\n   // operand of terminator.\n   for (auto [localResult, oldResult, outSharding] :\n-       llvm::zip_equal(localResults, op.getResults(),\n+       llvm::zip_equal(callOp->getResults(), op.getResults(),\n                        op.getOutShardings().getShardings())) {\n+    erasedManualAxisOutShardings.push_back(\n+        eraseManualAxes(outSharding, manualAxes.region));\n     if (!mlir::isa<mlir::ShapedType>(oldResult.getType())) {\n       oldResult.replaceAllUsesWith(localResult);\n       continue;\n@@ -337,8 +365,7 @@ void convertManualComputationOp(\n     } else {\n       shardingConstraint = CopyOp::create(rewriter, loc, localResult);\n     }\n-    sdy::setShardings(shardingConstraint,\n-                      eraseManualAxes(outSharding, manualAxes.region));\n+    sdy::setShardings(shardingConstraint, erasedManualAxisOutShardings.back());\n     setNonEmptyManualAxes(shardingConstraint, regionManualAxesAttr);\n \n     auto shardToFull = CustomCallOp::create(rewriter, loc, oldResult.getType(),\n@@ -349,10 +376,8 @@ void convertManualComputationOp(\n \n     oldResult.replaceAllUsesWith(shardToFull.getResult(0));\n   }\n-  if (!keepShardMapBodyAsFunc) {\n-    rewriter.inlineBlockBefore(&op.getBody().front(), op, fullToShardResults);\n-    rewriter.eraseOp(terminator);\n-  }\n+\n+  setShardings(callOp, erasedManualAxisOutShardings);\n   rewriter.eraseOp(op);\n }\n \n@@ -361,16 +386,13 @@ class ShardMapExportPass\n  public:\n   MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(ShardMapExportPass)\n \n-  explicit ShardMapExportPass(bool keepShardMapBodyAsFunc,\n-                              bool createHloShardingConstraints) {\n-    this->keepShardMapBodyAsFunc = keepShardMapBodyAsFunc;\n+  explicit ShardMapExportPass(bool createHloShardingConstraints) {\n     this->createHloShardingConstraints = createHloShardingConstraints;\n   }\n \n   ShardMapExportPass() = default;\n \n   explicit ShardMapExportPass(const ShardMapExportPass& other) {\n-    this->keepShardMapBodyAsFunc = other.keepShardMapBodyAsFunc;\n     this->createHloShardingConstraints = other.createHloShardingConstraints;\n   }\n \n@@ -398,7 +420,6 @@ class ShardMapExportPass\n     // above.\n     module->walk([&](ManualComputationOp op) {\n       convertManualComputationOp(op, parentManualCompAxes, symbolTable,\n-                                 keepShardMapBodyAsFunc,\n                                  createHloShardingConstraints);\n     });\n   }\n@@ -422,19 +443,13 @@ class ShardMapExportPass\n       llvm::cl::desc(\n           \"Whether to create @Sharding custom calls or MHLO copy ops.\"),\n       llvm::cl::init(false)};\n-  Option<bool> keepShardMapBodyAsFunc{\n-      *this, \"keep-shard-map-body-as-func\",\n-      llvm::cl::desc(\n-          \"Whether to keep the body of the shard map as a function or inline.\"),\n-      llvm::cl::init(false)};\n };\n \n }  // namespace\n \n std::unique_ptr<mlir::Pass> createStablehloRoundTripShardMapExportPass(\n-    bool keepShardMapBodyAsFunc, bool createHloShardingConstraints) {\n-  return std::make_unique<ShardMapExportPass>(keepShardMapBodyAsFunc,\n-                                              createHloShardingConstraints);\n+    bool createHloShardingConstraints) {\n+  return std::make_unique<ShardMapExportPass>(createHloShardingConstraints);\n }\n \n void registerStablehloRoundTripShardMapExportPass() {"
        },
        {
            "sha": "0b069b1ea813f3ffc65f4dd51d3e7ec5d14eb83f",
            "filename": "third_party/xla/xla/service/spmd/shardy/stablehlo_round_trip/shard_map_export.h",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fshard_map_export.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fshard_map_export.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fshard_map_export.h?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -29,7 +29,6 @@ namespace sdy {\n // `kManualAxes` attribute, which will be processed in\n // `ExportStablehloShardingsPass`.\n std::unique_ptr<mlir::Pass> createStablehloRoundTripShardMapExportPass(\n-    bool keepShardMapBodyAsFunc = false,\n     bool createHloShardingConstraints = false);\n \n // Registers the xla-sdy-stablehlo-round-trip-shard-map-export pass."
        },
        {
            "sha": "8521fe4f62b731ab2ec9d689d5d88d42bf5c07ea",
            "filename": "third_party/xla/xla/service/spmd/shardy/stablehlo_round_trip/shard_map_import.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fshard_map_import.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fshard_map_import.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fshard_map_import.cc?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -277,7 +277,8 @@ class ShardMapImportPass\n     llvm::SmallDenseMap<StringRef, mlir::Region*> shardMapNameToMovedRegion;\n     bool success = true;\n     module->walk([&](CallOp op) {\n-      if (!op.getCallee().contains(\"shmap_body\")) {\n+      if (!op.getCallee().contains(\"shmap_body\") &&\n+          !op.getCallee().contains(kInlineableManualComputationFuncName)) {\n         return mlir::WalkResult::advance();\n       }\n "
        },
        {
            "sha": "e4d511fbcafbc373fa689403dbb6f436705529d7",
            "filename": "third_party/xla/xla/service/spmd/shardy/stablehlo_round_trip/stablehlo_export.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fstablehlo_export.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fstablehlo_export.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fstablehlo_export.cc?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -37,7 +37,7 @@ void addStablehloExportPipeline(\n   // folding.\n   pm.addPass(createExportOpsPass(options.keepHloShardingConstraints));\n   pm.addPass(createStablehloRoundTripShardMapExportPass(\n-      options.keepShardMapBodyAsFunc, options.keepHloShardingConstraints));\n+      options.keepHloShardingConstraints));\n   pm.addPass(createExportNamedComputationsPass());\n   // If we don't add a sharding to a control flow op without one,\n   // StableHLO -> HLO conversion won't add a sharding for that op even if a"
        },
        {
            "sha": "718dc588a08062295a8385add9367e88ff8f45f2",
            "filename": "third_party/xla/xla/service/spmd/shardy/stablehlo_round_trip/stablehlo_export.h",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fstablehlo_export.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fstablehlo_export.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fstablehlo_export.h?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -33,12 +33,6 @@ struct StablehloExportPipelineOptions\n         \"calls - the HLO sharding constraint op. Else export \"\n         \"them to MHLO copy ops. By default, export to MHLO copy ops.\"),\n     llvm::cl::init(false)};\n-  Option<bool> keepShardMapBodyAsFunc{\n-      *this, \"keep-shard-map-body-as-func\",\n-      llvm::cl::desc(\n-          \"Whether to keep the body of the shard map as a function. Else the \"\n-          \"body will be inlined. By default, inline the body.\"),\n-      llvm::cl::init(false)};\n };\n \n // Register the xla-sdy-stablehlo-export-pipeline."
        },
        {
            "sha": "ee9fdaff36886ed3540355c825d815d439354438",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/stablehlo_export_pipeline.mlir",
            "status": "modified",
            "additions": 91,
            "deletions": 25,
            "changes": 116,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_export_pipeline.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_export_pipeline.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_export_pipeline.mlir?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -179,11 +179,8 @@ func.func @sharding_in_manual_computation_body(%arg0: tensor<8x16xf32> {sdy.shar\n // CHECK-NEXT: %[[FULL_TO_SHARD_0:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY_0]]) {mhlo.sharding = \"{devices=[1,1,4,4]<=[16] last_tile_dims={manual, replicated}}\"} : (tensor<8x16xf32>) -> tensor<4x8xf32>\n // CHECK-NEXT: %[[COPY_1:.*]] = mhlo.copy %arg1 {mhlo.sharding = \"{devices=[2,1,8]<=[2,2,4]T(1,0,2) last_tile_dim_replicate}\"} : tensor<16x32xf32>\n // CHECK-NEXT: %[[FULL_TO_SHARD_1:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY_1]]) {mhlo.sharding = \"{devices=[1,1,4,4]<=[16] last_tile_dims={manual, replicated}}\"} : (tensor<16x32xf32>) -> tensor<8x32xf32>\n-// CHECK-NEXT: %[[RESHARD:.*]] = mhlo.copy %[[FULL_TO_SHARD_0]] {mhlo.sharding = \"{devices=[1,2,4,2]<=[8,2]T(1,0) last_tile_dims={manual, replicated}}\"} : tensor<4x8xf32>\n-// CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[RESHARD]], %[[RESHARD]] {mhlo.sharding = \"{devices=[2,1,4,2]<=[4,2,2]T(1,0,2) last_tile_dims={manual, replicated}}\"} : tensor<4x8xf32>\n-// CHECK-NEXT: %[[DOT:.*]] = stablehlo.dot %[[ADD]], %[[FULL_TO_SHARD_1]] {mhlo.sharding = \"{devices=[2,2,4]<=[4,4]T(1,0) last_tile_dims={manual}}\"} : (tensor<4x8xf32>, tensor<8x32xf32>) -> tensor<4x32xf32>\n-// CHECK-NEXT: %[[SINE:.*]] = stablehlo.sine %[[DOT]] {mhlo.sharding = \"{devices=[1,1,4,4]<=[16] last_tile_dims={manual, replicated}}\"} : tensor<4x32xf32>\n-// CHECK-NEXT: %[[COPY_2:.*]] = mhlo.copy %[[SINE]] {mhlo.sharding = \"{devices=[1,1,4,4]<=[16] last_tile_dims={manual, replicated}}\"} : tensor<4x32xf32>\n+// CHECK-NEXT: %[[CALL:.*]] = call @local_xla.sdy.inlinable_manual_computation_body(%[[FULL_TO_SHARD_0]], %[[FULL_TO_SHARD_1]]) {mhlo.sharding = \"{devices=[1,1,4,4]<=[16] last_tile_dims={manual, replicated}}\"} : (tensor<4x8xf32>, tensor<8x32xf32>) -> tensor<4x32xf32>\n+// CHECK-NEXT: %[[COPY_2:.*]] = mhlo.copy %[[CALL]] {mhlo.sharding = \"{devices=[1,1,4,4]<=[16] last_tile_dims={manual, replicated}}\"} : tensor<4x32xf32>\n // CHECK-NEXT: %[[SHARD_TO_FULL:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_2]]) {mhlo.sharding = \"{devices=[2,1,8]<=[16] last_tile_dim_replicate}\"} : (tensor<4x32xf32>) -> tensor<8x32xf32>\n // CHECK-NEXT: return %[[SHARD_TO_FULL]] : tensor<8x32xf32>\n   %0 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh_3, [{\"b\"}, {\"a\"}]>, <@mesh_3, [{\"b\"}, {}], replicated={\"a\"}>] out_shardings=[<@mesh_3, [{\"a\"}, {}], replicated={\"b\"}>] manual_axes={\"a\", \"b\"} (%arg2: tensor<4x8xf32>, %arg3: tensor<8x32xf32>) {\n@@ -237,8 +234,8 @@ func.func @named_computation_in_manual_computation_partially_manual(\n       -> (tensor<32x2xi32> {sdy.sharding = #sdy.sharding<@mesh_2, [{\"x\", \"y\"}, {}]>}) {\n   // CHECK-NEXT: %[[COPY_0:.*]] = mhlo.copy %arg0 {mhlo.sharding = \"{devices=[32,1]<=[32]}\"} : tensor<32x2xi32>\n   // CHECK-NEXT: %[[FULL_TO_SHARD:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%0) {mhlo.sharding = \"{devices=[4,1,8]<=[8,4]T(1,0) last_tile_dims={manual}}\"} : (tensor<32x2xi32>) -> tensor<4x2xi32>\n-  // CHECK-NEXT: %[[FOO:.*]] = call @foo(%[[FULL_TO_SHARD]]) {mhlo.sharding = \"{devices=[4,1,8]<=[8,4]T(1,0) last_tile_dims={manual}}\"} : (tensor<4x2xi32>) -> tensor<4x2xi32>\n-  // CHECK-NEXT: %[[COPY_1:.*]] = mhlo.copy %[[FOO]] {mhlo.sharding = \"{devices=[4,1,8]<=[8,4]T(1,0) last_tile_dims={manual}}\"} : tensor<4x2xi32>\n+  // CHECK-NEXT: %[[CALL:.*]] = call @local_xla.sdy.inlinable_manual_computation_body_0(%[[FULL_TO_SHARD]]) {mhlo.sharding = \"{devices=[4,1,8]<=[8,4]T(1,0) last_tile_dims={manual}}\"} : (tensor<4x2xi32>) -> tensor<4x2xi32>\n+  // CHECK-NEXT: %[[COPY_1:.*]] = mhlo.copy %[[CALL]] {mhlo.sharding = \"{devices=[4,1,8]<=[8,4]T(1,0) last_tile_dims={manual}}\"} : tensor<4x2xi32>\n   // CHECK-NEXT: %[[SHARD_TO_FULL:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_1]]) {mhlo.sharding = \"{devices=[32,1]<=[32]}\"} : (tensor<4x2xi32>) -> tensor<32x2xi32>\n   // CHECK-NEXT: return %[[SHARD_TO_FULL]] : tensor<32x2xi32>\n   %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh_2, [{\"x\", \"y\"}, {}]>] out_shardings=[<@mesh_2, [{\"x\", \"y\"}, {}]>] manual_axes={\"x\"} (%arg1: tensor<4x2xi32>) {\n@@ -259,8 +256,8 @@ func.func @named_computation_in_manual_computation_fully_manual(\n       -> (tensor<32xi32> {sdy.sharding = #sdy.sharding<@mesh_2, [{\"x\", \"y\"}]>}) {\n   // CHECK-NEXT: %[[COPY_0:.*]] = mhlo.copy %arg0 {mhlo.sharding = \"{devices=[32]<=[32]}\"} : tensor<32xi32>\n   // CHECK-NEXT: %[[FULL_TO_SHARD:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%0) {mhlo.sharding = \"{manual}\"} : (tensor<32xi32>) -> tensor<1xi32>\n-  // CHECK-NEXT: %[[FOO:.*]] = call @foo_0(%[[FULL_TO_SHARD]]) {mhlo.sharding = \"{manual}\"} : (tensor<1xi32>) -> tensor<1xi32>\n-  // CHECK-NEXT: %[[COPY_1:.*]] = mhlo.copy %[[FOO]] {mhlo.sharding = \"{manual}\"} : tensor<1xi32>\n+  // CHECK-NEXT: %[[CALL:.*]] = call @local_xla.sdy.inlinable_manual_computation_body_1(%[[FULL_TO_SHARD]]) {mhlo.sharding = \"{manual}\"} : (tensor<1xi32>) -> tensor<1xi32>\n+  // CHECK-NEXT: %[[COPY_1:.*]] = mhlo.copy %[[CALL]] {mhlo.sharding = \"{manual}\"} : tensor<1xi32>\n   // CHECK-NEXT: %[[SHARD_TO_FULL:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_1]]) {mhlo.sharding = \"{devices=[32]<=[32]}\"} : (tensor<1xi32>) -> tensor<32xi32>\n   // CHECK-NEXT: return %[[SHARD_TO_FULL]] : tensor<32xi32>\n   %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh_2, [{\"x\", \"y\"}]>] out_shardings=[<@mesh_2, [{\"x\", \"y\"}]>] manual_axes={\"x\", \"y\"} (%arg1: tensor<1xi32>) {\n@@ -279,9 +276,8 @@ func.func @free_axis_inside_in_out_shardings_manual_computation(\n     -> (tensor<4x8xf32> {sdy.sharding = #sdy.sharding<@mesh_5, [{\"i\", ?}, {?}]>}) {\n   // CHECK-NEXT: %[[COPY_OPERAND:.*]] = mhlo.copy %arg0 {mhlo.sharding = \"{devices=[2,1,2]<=[4] last_tile_dim_replicate}\"} : tensor<4x8xf32>\n   // CHECK-NEXT: %[[FULL_TO_SHARD:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY_OPERAND]]) {mhlo.sharding = \"{devices=[2,1,2]<=[4] last_tile_dims={manual}}\"} : (tensor<4x8xf32>) -> tensor<4x8xf32>\n-  // CHECK-NEXT: %[[MULT:.*]] = stablehlo.multiply %[[FULL_TO_SHARD]], %[[FULL_TO_SHARD]] {mhlo.sharding = \"{devices=[2,1,2]<=[4] last_tile_dims={manual}}\"} : tensor<4x8xf32>\n-  // CHECK-NEXT: %[[COPY:.*]] = mhlo.copy %[[MULT]] {mhlo.sharding = \"{devices=[2,1,2]<=[4] last_tile_dims={manual}}\"} : tensor<4x8xf32>\n-  // CHECK-NEXT: %[[COPY_RESULT:.*]] = mhlo.copy %[[COPY]] {mhlo.sharding = \"{devices=[2,1,2]<=[4] last_tile_dims={manual}}\"} : tensor<4x8xf32>\n+  // CHECK-NEXT: %[[CALL:.*]] = call @local_xla.sdy.inlinable_manual_computation_body_2(%[[FULL_TO_SHARD]]) {mhlo.sharding = \"{devices=[2,1,2]<=[4] last_tile_dims={manual}}\"} : (tensor<4x8xf32>) -> tensor<4x8xf32>\n+  // CHECK-NEXT: %[[COPY_RESULT:.*]] = mhlo.copy %[[CALL]] {mhlo.sharding = \"{devices=[2,1,2]<=[4] last_tile_dims={manual}}\"} : tensor<4x8xf32>\n   // CHECK-NEXT: %[[SHARD_TO_FULL:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_RESULT]]) {mhlo.sharding = \"{devices=[2,1,2]<=[4] last_tile_dim_replicate}\"} : (tensor<4x8xf32>) -> tensor<4x8xf32>\n   // CHECK-NEXT: return %[[SHARD_TO_FULL]] : tensor<4x8xf32>\n   %0 = sdy.manual_computation(%arg0)\n@@ -419,10 +415,8 @@ func.func @while_with_no_sharding_inside_manual_comp(\n       -> (tensor<32x2xi32> {sdy.sharding = #sdy.sharding<@mesh_2, [{\"x\", \"y\"}, {}]>}) {\n   // CHECK-NEXT: %[[COPY_0:.*]] = mhlo.copy %arg0 {mhlo.sharding = \"{devices=[32,1]<=[32]}\"}\n   // CHECK-NEXT: %[[FULL_TO_SHARD:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY_0]])\n-  // CHECK:      %[[C0:.*]] = stablehlo.constant {mhlo.sharding = \"{manual}\"} dense<0>\n-  // CHECK:      %[[WHILE:.*]]:2 = stablehlo.while(%iterArg = %[[FULL_TO_SHARD]], %iterArg_1 = %[[C0]])\n-  // CHECK-SAME:   attributes {mhlo.sharding = \"{{[{][{]}}manual}, {manual}}\"}\n-  // CHECK:      %[[COPY_1:.*]] = mhlo.copy %[[WHILE]]#0\n+  // CHECK:      %[[CALL:.*]] = call @local_xla.sdy.inlinable_manual_computation_body_3(%[[FULL_TO_SHARD]]) {mhlo.sharding = \"{manual}\"} : (tensor<1x2xi32>) -> tensor<1x2xi32>\n+  // CHECK:      %[[COPY_1:.*]] = mhlo.copy %[[CALL]]\n   // CHECK-NEXT: %[[SHARD_TO_FULL:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_1]])\n   // CHECK-NEXT: return %[[SHARD_TO_FULL]]\n   %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh_2, [{\"x\", \"y\"}, {}]>] out_shardings=[<@mesh_2, [{\"x\", \"y\"}, {}]>] manual_axes={\"x\", \"y\"} (%arg1: tensor<1x2xi32>) {\n@@ -459,14 +453,8 @@ func.func @all_reduce_input_no_unreduced_axes(%arg0: tensor<8x8xf32> {sdy.shardi\n func.func @all_reduce_input_with_unreduced_axes(%arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_5, [{\"j\"}, {}], unreduced={\"i\"}>}) -> tensor<8x8xf32> {\n   // CHECK-NEXT: %[[COPY_0:.*]] = mhlo.copy %arg0 {mhlo.sharding = \"{devices=[2,1,2]<=[2,2]T(1,0) last_tile_dims={unreduced}}\"}\n   // CHECK-NEXT: %[[FULL_TO_SHARD:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY_0]]) {mhlo.sharding = \"{manual}\"}\n-  // CHECK-NEXT: %[[ALL_REDUCE:.*]] = \"stablehlo.all_reduce\"(%1) <{\n-  // CHECK-SAME:   channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>,\n-  // CHECK-SAME:   replica_groups = dense<{{\\[}}[0, 2], [1, 3]]> : tensor<2x2xi64>, use_global_device_ids}> ({\n-  // CHECK-NEXT: ^bb0(%arg1: tensor<f32>, %arg2: tensor<f32>):\n-  // CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg1, %arg2 {mhlo.sharding = \"{manual}\"}\n-  // CHECK-NEXT:   stablehlo.return %[[ADD]]\n-  // CHECK-NEXT: }) {mhlo.sharding = \"{manual}\"}\n-  // CHECK-NEXT: %[[COPY_1:.*]] = mhlo.copy %[[ALL_REDUCE]] {mhlo.sharding = \"{manual}\"}\n+  // CHECK-NEXT: %[[CALL:.*]] = call @local_xla.sdy.inlinable_manual_computation_body_4(%[[FULL_TO_SHARD]]) {mhlo.sharding = \"{manual}\"} : (tensor<4x8xf32>) -> tensor<4x8xf32>\n+  // CHECK-NEXT: %[[COPY_1:.*]] = mhlo.copy %[[CALL]] {mhlo.sharding = \"{manual}\"}\n   // CHECK-NEXT: %[[SHARD_TO_FULL:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%3) {mhlo.sharding = \"{devices=[2,1,2]<=[2,2]T(1,0) last_tile_dim_replicate}\"}\n   // CHECK-NEXT: return %[[SHARD_TO_FULL]]\n   %0 = sdy.all_reduce {\"i\"} %arg0 out_sharding=<@mesh_5, [{\"j\"}, {}]> : tensor<8x8xf32>\n@@ -491,6 +479,12 @@ func.func @unreduced_func_input(%arg0: tensor<4x8xf32> {sdy.sharding = #sdy.shar\n func.func @unreduced_op(%arg0: tensor<4x64x16xf32> {sdy.sharding = #sdy.sharding<@mesh_5, [{}, {\"i\", \"j\"}, {}]>}) -> tensor<4x16xf32> {\n   // CHECK:      %[[REDUCE:.*]] = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [1] {mhlo.sharding = \"{devices=[2,1,2]<=[2,2]T(1,0) last_tile_dims={unreduced}}\"}\n   // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[REDUCE]], %[[REDUCE]] {mhlo.sharding = \"{devices=[2,1,2]<=[2,2]T(1,0) last_tile_dims={unreduced}}\"}\n+  // CHECK-NEXT: %[[COPY:.*]] = mhlo.copy %[[ADD]] {mhlo.sharding = \"{devices=[2,1,2]<=[2,2]T(1,0) last_tile_dims={unreduced}}\"} : tensor<4x16xf32>\n+  // CHECK-NEXT: %[[FULL_TO_SHARD:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY]]) {mhlo.sharding = \"{manual}\"} : (tensor<4x16xf32>) -> tensor<2x16xf32>\n+  // CHECK-NEXT: %[[CALL:.*]] = call @local_xla.sdy.inlinable_manual_computation_body_5(%[[FULL_TO_SHARD]]) {mhlo.sharding = \"{manual}\"} : (tensor<2x16xf32>) -> tensor<2x16xf32>\n+  // CHECK-NEXT: %[[COPY_1:.*]] = mhlo.copy %[[CALL]] {mhlo.sharding = \"{manual}\"} : tensor<2x16xf32>\n+  // CHECK-NEXT: %[[SHARD_TO_FULL:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_1]]) {mhlo.sharding = \"{devices=[2,1,2]<=[2,2]T(1,0) last_tile_dim_replicate}\"} : (tensor<2x16xf32>) -> tensor<4x16xf32>\n+  // CHECK-NEXT: return %[[SHARD_TO_FULL]] : tensor<4x16xf32>\n   %0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n   %1 = stablehlo.reduce(%arg0 init: %0) applies stablehlo.add across dimensions = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_5, [{\"j\"}, {}], unreduced={\"i\"}>]>} : (tensor<4x64x16xf32>, tensor<f32>) -> tensor<4x16xf32>\n   %2 = stablehlo.add %1, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_5, [{\"j\"}, {}], unreduced={\"i\"}>]>} : tensor<4x16xf32>\n@@ -548,10 +542,82 @@ func.func @unreduced_canonicalization(%arg0: tensor<4x64x16xf32> {sdy.sharding =\n   return %1 : tensor<4x16xf32>\n }\n \n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<4x8xf32> {mhlo.sharding = \"{devices=[1,1,4,4]<=[16] last_tile_dims={manual, replicated}}\"},\n+// CHECK-SAME{LITERAL}:     %arg1: tensor<8x32xf32> {mhlo.sharding = \"{devices=[1,1,4,4]<=[16] last_tile_dims={manual, replicated}}\"})\n+// CHECK-SAME{LITERAL}:     -> (tensor<4x32xf32> {mhlo.sharding = \"{devices=[1,1,4,4]<=[16] last_tile_dims={manual, replicated}}\"}) {\n+// CHECK-NEXT:            %[[RESHARD:.*]] = mhlo.copy %arg0 {mhlo.sharding = \"{devices=[1,2,4,2]<=[8,2]T(1,0) last_tile_dims={manual, replicated}}\"} : tensor<4x8xf32>\n+// CHECK-NEXT:            %[[ADD:.*]] = stablehlo.add %[[RESHARD]], %[[RESHARD]] {mhlo.sharding = \"{devices=[2,1,4,2]<=[4,2,2]T(1,0,2) last_tile_dims={manual, replicated}}\"} : tensor<4x8xf32>\n+// CHECK-NEXT:            %[[DOT:.*]] = stablehlo.dot %[[ADD]], %arg1 {mhlo.sharding = \"{devices=[2,2,4]<=[4,4]T(1,0) last_tile_dims={manual}}\"} : (tensor<4x8xf32>, tensor<8x32xf32>) -> tensor<4x32xf32>\n+// CHECK-NEXT:            %[[SINE:.*]] = stablehlo.sine %[[DOT]] {mhlo.sharding = \"{devices=[1,1,4,4]<=[16] last_tile_dims={manual, replicated}}\"} : tensor<4x32xf32>\n+// CHECK-NEXT:            return %[[SINE]] : tensor<4x32xf32>\n+// CHECK-NEXT:          }\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_0\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<4x2xi32> {mhlo.sharding = \"{devices=[4,1,8]<=[8,4]T(1,0) last_tile_dims={manual}}\"})\n+// CHECK-SAME{LITERAL}:     -> (tensor<4x2xi32> {mhlo.sharding = \"{devices=[4,1,8]<=[8,4]T(1,0) last_tile_dims={manual}}\"}) {\n+// CHECK-NEXT:            %[[CALL:.*]] = call @foo(%arg0) {mhlo.sharding = \"{devices=[4,1,8]<=[8,4]T(1,0) last_tile_dims={manual}}\"} : (tensor<4x2xi32>) -> tensor<4x2xi32>\n+// CHECK-NEXT:            return %[[CALL]] : tensor<4x2xi32>\n+// CHECK-NEXT:          }\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_1\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<1xi32> {mhlo.sharding = \"{manual}\"})\n+// CHECK-SAME{LITERAL}:     -> (tensor<1xi32> {mhlo.sharding = \"{manual}\"}) {\n+// CHECK-NEXT:            %[[CALL:.*]] = call @foo_0(%arg0) {mhlo.sharding = \"{manual}\"} : (tensor<1xi32>) -> tensor<1xi32>\n+// CHECK-NEXT:            return %[[CALL]] : tensor<1xi32>\n+// CHECK-NEXT:          }\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_2\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<4x8xf32> {mhlo.sharding = \"{devices=[2,1,2]<=[4] last_tile_dims={manual}}\"})\n+// CHECK-SAME{LITERAL}:     -> (tensor<4x8xf32> {mhlo.sharding = \"{devices=[2,1,2]<=[4] last_tile_dims={manual}}\"}) {\n+// CHECK-NEXT:            %[[MULT:.*]] = stablehlo.multiply %arg0, %arg0 {mhlo.sharding = \"{devices=[2,1,2]<=[4] last_tile_dims={manual}}\"} : tensor<4x8xf32>\n+// CHECK-NEXT:            %[[COPY:.*]] = mhlo.copy %[[MULT]] {mhlo.sharding = \"{devices=[2,1,2]<=[4] last_tile_dims={manual}}\"} : tensor<4x8xf32>\n+// CHECK-NEXT:            return %[[COPY]] : tensor<4x8xf32>\n+// CHECK-NEXT:          }\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_3\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<1x2xi32> {mhlo.sharding = \"{manual}\"})\n+// CHECK-SAME{LITERAL}:      -> (tensor<1x2xi32> {mhlo.sharding = \"{manual}\"}) {\n+// CHECK-NEXT:            %[[C0:.*]] = stablehlo.constant {mhlo.sharding = \"{manual}\"} dense<0> : tensor<i32>\n+// CHECK-NEXT:            %[[C1:.*]] = stablehlo.constant {mhlo.sharding = \"{manual}\"} dense<32> : tensor<i32>\n+// CHECK-NEXT:             %[[WHILE:.*]]:2 = stablehlo.while(%iterArg = %arg0, %iterArg_1 = %[[C0]]) : tensor<1x2xi32>, tensor<i32>\n+// CHECK-SAME{LITERAL}:        attributes {mhlo.sharding = \"{{manual}, {manual}}\"}\n+// CHECK-NEXT:            cond {\n+// CHECK-NEXT:              %[[COMP:.*]] = stablehlo.compare  LT, %iterArg_1, %[[C1]] {mhlo.sharding = \"{manual}\"} : (tensor<i32>, tensor<i32>) -> tensor<i1>\n+// CHECK-NEXT:              stablehlo.return %[[COMP]] : tensor<i1>\n+// CHECK-NEXT:            } do {\n+// CHECK-NEXT:              stablehlo.return %iterArg, %iterArg_1 : tensor<1x2xi32>, tensor<i32>\n+// CHECK-NEXT:            }\n+// CHECK-NEXT:            return %[[WHILE]]#0 : tensor<1x2xi32>\n+// CHECK-NEXT:          }\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_4\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<4x8xf32> {mhlo.sharding = \"{manual}\"})\n+// CHECK-SAME{LITERAL}:     -> (tensor<4x8xf32> {mhlo.sharding = \"{manual}\"}) {\n+// CHECK-NEXT:            %[[ALL_REDUCE:.*]] = \"stablehlo.all_reduce\"(%arg0)\n+// CHECK-SAME{LITERAL}:     <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 2], [1, 3]]> : tensor<2x2xi64>, use_global_device_ids}> ({\n+// CHECK-NEXT:            ^bb0(%arg1: tensor<f32>, %arg2: tensor<f32>):\n+// CHECK-NEXT:              %[[ADD:.*]] = stablehlo.add %arg1, %arg2 {mhlo.sharding = \"{manual}\"} : tensor<f32>\n+// CHECK-NEXT:              stablehlo.return %[[ADD]] : tensor<f32>\n+// CHECK-NEXT:            }) {mhlo.sharding = \"{manual}\"} : (tensor<4x8xf32>) -> tensor<4x8xf32>\n+// CHECK-NEXT:            return %[[ALL_REDUCE]] : tensor<4x8xf32>\n+// CHECK-NEXT:          }\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_5\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<2x16xf32> {mhlo.sharding = \"{manual}\"})\n+// CHECK-SAME{LITERAL}:     -> (tensor<2x16xf32> {mhlo.sharding = \"{manual}\"}) {\n+// CHECK-NEXT:            %[[ALL_REDUCE:.*]] = \"stablehlo.all_reduce\"(%arg0)\n+// CHECK-SAME{LITERAL}:       <{channel_handle = #stablehlo.channel_handle<handle = 2, type = 1>, replica_groups = dense<[[0, 2], [1, 3]]> : tensor<2x2xi64>, use_global_device_ids}> ({\n+// CHECK-NEXT:            ^bb0(%arg1: tensor<f32>, %arg2: tensor<f32>):\n+// CHECK-NEXT:              %[[ADD:.*]] = stablehlo.add %arg1, %arg2 {mhlo.sharding = \"{manual}\"} : tensor<f32>\n+// CHECK-NEXT:              stablehlo.return %[[ADD]] : tensor<f32>\n+// CHECK-NEXT:            }) {mhlo.sharding = \"{manual}\"} : (tensor<2x16xf32>) -> tensor<2x16xf32>\n+// CHECK-NEXT:            return %[[ALL_REDUCE]] : tensor<2x16xf32>\n+// CHECK-NEXT:          }\n \n // CHECK-LABEL: func private @foo\n // CHECK-SAME:    %arg0: tensor<4x2xi32> {mhlo.sharding = \"{devices=[4,1,8]<=[8,4]T(1,0) last_tile_dims={manual}}\"}\n-// CHECK-SAME:    -> (tensor<4x2xi32> {mhlo.sharding = \"{devices=[4,1,8]<=[8,4]T(1,0) last_tile_dims={manual}}\"}) {\n+// CHECK-SAME:    -> (tensor<4x2xi32> {mhlo.sharding = \"{devices=[4,1,8]<=[8,4]T(1,0) last_tile_dims={manual}}\"})\n // CHECK-NEXT:    %[[MULT:.*]] = stablehlo.multiply %arg0, %arg0 {mhlo.sharding = \"{devices=[4,1,8]<=[8,4]T(1,0) last_tile_dims={manual}}\"} : tensor<4x2xi32>\n // CHECK-NEXT:    return %[[MULT]] : tensor<4x2xi32>\n "
        },
        {
            "sha": "efeeb3a1f9c823dea460e1aba3cbf71269e742b3",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/stablehlo_round_trip_shard_map_export.mlir",
            "status": "modified",
            "additions": 205,
            "deletions": 100,
            "changes": 305,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_round_trip_shard_map_export.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_round_trip_shard_map_export.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_round_trip_shard_map_export.mlir?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -7,20 +7,14 @@ sdy.mesh @maximal_mesh_0 = <[], device_ids=[0]>\n \n // CHECK-LABEL: func @single_manual_comp\n func.func @single_manual_comp(%arg0: tensor<8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_0, [{\"a\", ?}, {\"b\", ?}]>}, %arg1: tensor<16x32xf32> {sdy.sharding = #sdy.sharding<@mesh_0, [{\"b\", ?}, {?}]>}) -> (tensor<8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_0, [{\"a\"}, {}]>}) {\n-  // CHECK-NEXT: %0 = mhlo.copy %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"a\"}, {\"b\"}]>]>} : tensor<8x16xf32>\n-  // CHECK-NEXT: %1 = stablehlo.custom_call @SPMDFullToShardShape(%0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<8x16xf32>) -> tensor<2x8xf32>\n-  // CHECK-NEXT: %2 = mhlo.copy %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"b\"}, {}], replicated={\"a\"}>]>} : tensor<16x32xf32>\n-  // CHECK-NEXT: %3 = stablehlo.custom_call @SPMDFullToShardShape(%2) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<16x32xf32>) -> tensor<8x32xf32>\n-  // CHECK-NEXT: %4 = stablehlo.add %1, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<2x8xf32>\n-  // CHECK-NEXT: %5 = stablehlo.dot %4, %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<2x8xf32>, tensor<8x32xf32>) -> tensor<2x32xf32>\n-  // CHECK-NEXT: %6 = \"stablehlo.all_reduce\"(%5) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>, replica_groups = dense<{{\\[\\[}}0, 1], [2, 3], [4, 5], [6, 7]]> : tensor<4x2xi64>, use_global_device_ids}> ({\n-  // CHECK-NEXT: ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):\n-  // CHECK-NEXT:   %9 = stablehlo.add %arg2, %arg3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, []>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<f32>\n-  // CHECK-NEXT:   stablehlo.return %9 {xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<f32>\n-  // CHECK-NEXT: }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<2x32xf32>) -> tensor<2x32xf32>\n-  // CHECK-NEXT: %7 = mhlo.copy %6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<2x32xf32>\n-  // CHECK-NEXT: %8 = stablehlo.custom_call @SPMDShardToFullShape(%7) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"a\"}, {}], replicated={\"b\"}>]>} : (tensor<2x32xf32>) -> tensor<8x32xf32>\n-  // CHECK-NEXT: return %8 : tensor<8x32xf32>\n+  // CHECK-NEXT: %[[COPY:.*]] = mhlo.copy %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"a\"}, {\"b\"}]>]>} : tensor<8x16xf32>\n+  // CHECK-NEXT: %[[FULL_TO_SHARD:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<8x16xf32>) -> tensor<2x8xf32>\n+  // CHECK-NEXT: %[[COPY_1:.*]] = mhlo.copy %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"b\"}, {}], replicated={\"a\"}>]>} : tensor<16x32xf32>\n+  // CHECK-NEXT: %[[FULL_TO_SHARD_1:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%2) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<16x32xf32>) -> tensor<8x32xf32>\n+  // CHECK-NEXT: %[[CALL:.*]] = call @local_xla.sdy.inlinable_manual_computation_body(%[[FULL_TO_SHARD]], %[[FULL_TO_SHARD_1]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<2x8xf32>, tensor<8x32xf32>) -> tensor<2x32xf32>\n+  // CHECK-NEXT: %[[COPY_2:.*]] = mhlo.copy %[[CALL]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<2x32xf32>\n+  // CHECK-NEXT: %[[SHARD_TO_FULL:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_2]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"a\"}, {}], replicated={\"b\"}>]>} : (tensor<2x32xf32>) -> tensor<8x32xf32>\n+  // CHECK-NEXT: return %[[SHARD_TO_FULL]] : tensor<8x32xf32>\n   %0 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh_0, [{\"a\"}, {\"b\"}]>, <@mesh_0, [{\"b\"}, {}], replicated={\"a\"}>] out_shardings=[<@mesh_0, [{\"a\"}, {}], replicated={\"b\"}>] manual_axes={\"a\", \"b\"} (%arg2: tensor<2x8xf32>, %arg3: tensor<8x32xf32>) {\n     %1 = stablehlo.add %arg2, %arg2 : tensor<2x8xf32>\n     %2 = stablehlo.dot %1, %arg3 : (tensor<2x8xf32>, tensor<8x32xf32>) -> tensor<2x32xf32>\n@@ -37,15 +31,17 @@ func.func @single_manual_comp(%arg0: tensor<8x16xf32> {sdy.sharding = #sdy.shard\n // CHECK-LABEL: func @manual_comp_using_another\n func.func @manual_comp_using_another(%arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_0, [{\"a\"}, {}]>})\n     -> (tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_0, [{}, {\"b\"}]>}) {\n-  // CHECK-NEXT: %0 = mhlo.copy %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"a\"}, {}]>]>} : tensor<8x8xf32>\n-  // CHECK-NEXT: %1 = stablehlo.custom_call @SPMDFullToShardShape(%0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<8x8xf32>) -> tensor<2x8xf32>\n-  // CHECK-NEXT: %2 = mhlo.copy %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<2x8xf32>\n-  // CHECK-NEXT: %3 = stablehlo.custom_call @SPMDShardToFullShape(%2) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"a\"}, {}]>]>} : (tensor<2x8xf32>) -> tensor<8x8xf32>\n-  // CHECK-NEXT: %4 = mhlo.copy %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {\"b\"}]>]>} : tensor<8x8xf32>\n-  // CHECK-NEXT: %5 = stablehlo.custom_call @SPMDFullToShardShape(%4) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"b\"}>} : (tensor<8x8xf32>) -> tensor<8x4xf32>\n-  // CHECK-NEXT: %6 = mhlo.copy %5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"b\"}>} : tensor<8x4xf32>\n-  // CHECK-NEXT: %7 = stablehlo.custom_call @SPMDShardToFullShape(%6) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {\"b\"}]>]>} : (tensor<8x4xf32>) -> tensor<8x8xf32>\n-  // CHECK-NEXT: return %7 : tensor<8x8xf32>\n+  // CHECK-NEXT: %[[COPY:.*]] = mhlo.copy %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"a\"}, {}]>]>} : tensor<8x8xf32>\n+  // CHECK-NEXT: %[[FULL_TO_SHARD:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<8x8xf32>) -> tensor<2x8xf32>\n+  // CHECK-NEXT: %[[CALL:.*]] = call @local_xla.sdy.inlinable_manual_computation_body_0(%[[FULL_TO_SHARD]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<2x8xf32>) -> tensor<2x8xf32>\n+  // CHECK-NEXT: %[[COPY_1:.*]] = mhlo.copy %[[CALL]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<2x8xf32>\n+  // CHECK-NEXT: %[[SHARD_TO_FULL:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_1]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"a\"}, {}]>]>} : (tensor<2x8xf32>) -> tensor<8x8xf32>\n+  // CHECK-NEXT: %[[COPY_2:.*]] = mhlo.copy %[[SHARD_TO_FULL]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {\"b\"}]>]>} : tensor<8x8xf32>\n+  // CHECK-NEXT: %[[FULL_TO_SHARD_1:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY_2]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"b\"}>} : (tensor<8x8xf32>) -> tensor<8x4xf32>\n+   // CHECK-NEXT: %[[CALL_1:.*]] = call @local_xla.sdy.inlinable_manual_computation_body_1(%[[FULL_TO_SHARD_1]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"b\"}>} : (tensor<8x4xf32>) -> tensor<8x4xf32>\n+  // CHECK-NEXT: %[[COPY_3:.*]] = mhlo.copy %[[CALL_1]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"b\"}>} : tensor<8x4xf32>\n+  // CHECK-NEXT: %[[SHARD_TO_FULL_1:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_3]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {\"b\"}]>]>} : (tensor<8x4xf32>) -> tensor<8x8xf32>\n+  // CHECK-NEXT: return %[[SHARD_TO_FULL_1]] : tensor<8x8xf32>\n   %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh_0, [{\"a\"}, {}]>] out_shardings=[<@mesh_0, [{\"a\"}, {}]>] manual_axes={\"a\"} (%arg1: tensor<2x8xf32>) {\n     sdy.return %arg1 : tensor<2x8xf32>\n   } : (tensor<8x8xf32>) -> tensor<8x8xf32>\n@@ -58,15 +54,14 @@ func.func @manual_comp_using_another(%arg0: tensor<8x8xf32> {sdy.sharding = #sdy\n \n // CHECK-LABEL: func @sharding_in_manual_computation_body\n func.func @sharding_in_manual_computation_body(%arg0: tensor<8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{\"a\", ?}, {\"b\", ?}]>}, %arg1: tensor<16x32xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{\"b\", ?}, {?}]>}) -> (tensor<8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{\"a\"}, {}]>}) {\n-  // CHECK-NEXT: %0 = mhlo.copy %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"a\"}, {\"b\"}]>]>} : tensor<8x16xf32>\n-  // CHECK-NEXT: %1 = stablehlo.custom_call @SPMDFullToShardShape(%0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<8x16xf32>) -> tensor<4x8xf32>\n-  // CHECK-NEXT: %2 = mhlo.copy %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"b\"}, {}], replicated={\"a\"}>]>} : tensor<16x32xf32>\n-  // CHECK-NEXT: %3 = stablehlo.custom_call @SPMDFullToShardShape(%2) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<16x32xf32>) -> tensor<8x32xf32>\n-  // CHECK-NEXT: %4 = stablehlo.add %1, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"c\"}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<4x8xf32>\n-  // CHECK-NEXT: %5 = stablehlo.dot %4, %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"d\"}, {\"c\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<4x8xf32>, tensor<8x32xf32>) -> tensor<4x32xf32>\n-  // CHECK-NEXT: %6 = mhlo.copy %5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<4x32xf32>\n-  // CHECK-NEXT: %7 = stablehlo.custom_call @SPMDShardToFullShape(%6) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"a\"}, {}], replicated={\"b\"}>]>} : (tensor<4x32xf32>) -> tensor<8x32xf32>\n-  // CHECK-NEXT: return %7 : tensor<8x32xf32>\n+  // CHECK-NEXT: %[[COPY:.*]] = mhlo.copy %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"a\"}, {\"b\"}]>]>} : tensor<8x16xf32>\n+  // CHECK-NEXT: %[[FULL_TO_SHARD:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<8x16xf32>) -> tensor<4x8xf32>\n+  // CHECK-NEXT: %[[COPY_1:.*]] = mhlo.copy %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"b\"}, {}], replicated={\"a\"}>]>} : tensor<16x32xf32>\n+  // CHECK-NEXT: %[[FULL_TO_SHARD_1:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY_1]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<16x32xf32>) -> tensor<8x32xf32>\n+  // CHECK-NEXT: %[[CALL:.*]] = call @local_xla.sdy.inlinable_manual_computation_body_2(%[[FULL_TO_SHARD]], %[[FULL_TO_SHARD_1]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<4x8xf32>, tensor<8x32xf32>) -> tensor<4x32xf32>\n+  // CHECK-NEXT: %[[COPY_2:.*]] = mhlo.copy %[[CALL]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<4x32xf32>\n+  // CHECK-NEXT: %[[SHARD_TO_FULL:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_2]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"a\"}, {}], replicated={\"b\"}>]>} : (tensor<4x32xf32>) -> tensor<8x32xf32>\n+  // CHECK-NEXT: return %[[SHARD_TO_FULL]] : tensor<8x32xf32>\n   %0 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh_1, [{\"a\"}, {\"b\"}]>, <@mesh_1, [{\"b\"}, {}], replicated={\"a\"}>] out_shardings=[<@mesh_1, [{\"a\"}, {}], replicated={\"b\"}>] manual_axes={\"a\", \"b\"} (%arg2: tensor<4x8xf32>, %arg3: tensor<8x32xf32>) {\n     %1 = stablehlo.add %arg2, %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"c\"}, {}]>]>} : tensor<4x8xf32>\n     %2 = stablehlo.dot %1, %arg3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"d\"}, {\"c\"}]>]>} : (tensor<4x8xf32>, tensor<8x32xf32>) -> tensor<4x32xf32>\n@@ -92,20 +87,12 @@ func.func @call_op_with_no_operands_or_results() {\n \n // CHECK-LABEL: func @nested_shmaps\n func.func @nested_shmaps(%arg0: tensor<4x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{\"a\"}, {\"b\"}, {\"c\"}]>}) -> (tensor<4x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{\"a\", ?}, {?}, {?}]>}) {\n-  // CHECK-NEXT: %0 = mhlo.copy %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"a\"}, {}, {}]>]>} : tensor<4x8x16xf32>\n-  // CHECK-NEXT: %1 = stablehlo.custom_call @SPMDFullToShardShape(%0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<4x8x16xf32>) -> tensor<2x8x16xf32>\n-  // CHECK-NEXT: %2 = mhlo.copy %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {\"b\"}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<2x8x16xf32>\n-  // CHECK-NEXT: %3 = stablehlo.custom_call @SPMDFullToShardShape(%2) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<2x8x16xf32>) -> tensor<2x4x16xf32>\n-  // CHECK-NEXT: %4 = mhlo.copy %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {\"c\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<2x4x16xf32>\n-  // CHECK-NEXT: %5 = stablehlo.custom_call @SPMDFullToShardShape(%4) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\", \"c\"}>} : (tensor<2x4x16xf32>) -> tensor<2x4x8xf32>\n-  // CHECK-NEXT: %6 = stablehlo.multiply %5, %5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\", \"c\"}>} : tensor<2x4x8xf32>\n-  // CHECK-NEXT: %7 = mhlo.copy %6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\", \"c\"}>} : tensor<2x4x8xf32>\n-  // CHECK-NEXT: %8 = stablehlo.custom_call @SPMDShardToFullShape(%7) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {\"c\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<2x4x8xf32>) -> tensor<2x4x16xf32>\n-  // CHECK-NEXT: %9 = mhlo.copy %8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<2x4x16xf32>\n-  // CHECK-NEXT: %10 = stablehlo.custom_call @SPMDShardToFullShape(%9) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {\"b\"}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<2x4x16xf32>) -> tensor<2x8x16xf32>\n-  // CHECK-NEXT: %11 = mhlo.copy %10 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<2x8x16xf32>\n-  // CHECK-NEXT: %12 = stablehlo.custom_call @SPMDShardToFullShape(%11) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"a\"}, {}, {}]>]>} : (tensor<2x8x16xf32>) -> tensor<4x8x16xf32>\n-  // CHECK-NEXT: return %12 : tensor<4x8x16xf32>\n+  // CHECK-NEXT: %[[COPY:.*]] = mhlo.copy %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"a\"}, {}, {}]>]>} : tensor<4x8x16xf32>\n+  // CHECK-NEXT: %[[FULL_TO_SHARD:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<4x8x16xf32>) -> tensor<2x8x16xf32>\n+  // CHECK-NEXT: %[[CALL:.*]] = call @local_xla.sdy.inlinable_manual_computation_body_5(%[[FULL_TO_SHARD]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<2x8x16xf32>) -> tensor<2x8x16xf32>\n+  // CHECK-NEXT: %[[COPY_1:.*]] = mhlo.copy %[[CALL]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<2x8x16xf32>\n+  // CHECK-NEXT: %[[SHARD_TO_FULL:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_1]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"a\"}, {}, {}]>]>} : (tensor<2x8x16xf32>) -> tensor<4x8x16xf32>\n+  // CHECK-NEXT: return %[[SHARD_TO_FULL]] : tensor<4x8x16xf32>\n   %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh_1, [{\"a\"}, {}, {}]>] out_shardings=[<@mesh_1, [{\"a\"}, {}, {}]>] manual_axes={\"a\"} (%arg1: tensor<2x8x16xf32>) {\n     %1 = sdy.manual_computation(%arg1) in_shardings=[<@mesh_1, [{}, {\"b\"}, {}]>] out_shardings=[<@mesh_1, [{}, {\"b\"}, {}]>] manual_axes={\"b\"} (%arg2: tensor<2x4x16xf32>) {\n       %2 = sdy.manual_computation(%arg2) in_shardings=[<@mesh_1, [{}, {}, {\"c\"}]>] out_shardings=[<@mesh_1, [{}, {}, {\"c\"}]>] manual_axes={\"c\"} (%arg3: tensor<2x4x8xf32>) {\n@@ -121,19 +108,12 @@ func.func @nested_shmaps(%arg0: tensor<4x8x16xf32> {sdy.sharding = #sdy.sharding\n \n // CHECK-LABEL: func @nested_shmaps_extra_op\n func.func @nested_shmaps_extra_op(%arg0: tensor<4x8xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{\"a\"}, {\"b\"}]>}) -> (tensor<4x8xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{\"a\", ?}, {?}]>}) {\n-  // CHECK-NEXT: %0 = mhlo.copy %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"a\"}, {}]>]>} : tensor<4x8xf32>\n-  // CHECK-NEXT: %1 = stablehlo.custom_call @SPMDFullToShardShape(%0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<4x8xf32>) -> tensor<2x8xf32>\n-  // CHECK-NEXT: %2 = mhlo.copy %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {\"b\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<2x8xf32>\n-  // CHECK-NEXT: %3 = stablehlo.custom_call @SPMDFullToShardShape(%2) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<2x8xf32>) -> tensor<2x4xf32>\n-  // CHECK-NEXT: %4 = stablehlo.multiply %3, %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<2x4xf32>\n-  // CHECK-NEXT: %5 = stablehlo.add %4, %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"c\"}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<2x4xf32>\n-  // CHECK-NEXT: %6 = stablehlo.subtract %5, %5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"c\", \"d\"}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<2x4xf32>\n-  // CHECK-NEXT: %7 = mhlo.copy %6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<2x4xf32>\n-  // CHECK-NEXT: %8 = stablehlo.custom_call @SPMDShardToFullShape(%7) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {\"b\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<2x4xf32>) -> tensor<2x8xf32>\n-  // CHECK-NEXT: %9 = stablehlo.add %8, %8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<2x8xf32>\n-  // CHECK-NEXT: %10 = mhlo.copy %9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<2x8xf32>\n-  // CHECK-NEXT: %11 = stablehlo.custom_call @SPMDShardToFullShape(%10) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"a\"}, {}]>]>} : (tensor<2x8xf32>) -> tensor<4x8xf32>\n-  // CHECK-NEXT: return %11 : tensor<4x8xf32>\n+  // CHECK-NEXT: %[[COPY:.*]] = mhlo.copy %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"a\"}, {}]>]>} : tensor<4x8xf32>\n+  // CHECK-NEXT: %[[FULL_TO_SHARD:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<4x8xf32>) -> tensor<2x8xf32>\n+  // CHECK-NEXT: %[[CALL:.*]] = call @local_xla.sdy.inlinable_manual_computation_body_7(%[[FULL_TO_SHARD]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<2x8xf32>) -> tensor<2x8xf32>\n+  // CHECK-NEXT: %[[COPY_1:.*]] = mhlo.copy %[[CALL]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<2x8xf32>\n+  // CHECK-NEXT: %[[SHARD_TO_FULL:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_1]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"a\"}, {}]>]>} : (tensor<2x8xf32>) -> tensor<4x8xf32>\n+  // CHECK-NEXT: return %[[SHARD_TO_FULL]] : tensor<4x8xf32>\n   %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh_1, [{\"a\"}, {}]>] out_shardings=[<@mesh_1, [{\"a\"}, {}]>] manual_axes={\"a\"} (%arg1: tensor<2x8xf32>) {\n     %1 = sdy.manual_computation(%arg1) in_shardings=[<@mesh_1, [{}, {\"b\"}]>] out_shardings=[<@mesh_1, [{}, {\"b\"}]>] manual_axes={\"b\"} (%arg2: tensor<2x4xf32>) {\n       %2 = stablehlo.multiply %arg2, %arg2 : tensor<2x4xf32>\n@@ -149,24 +129,24 @@ func.func @nested_shmaps_extra_op(%arg0: tensor<4x8xf32> {sdy.sharding = #sdy.sh\n \n // CHECK-LABEL: func @multiple_manual_computation_uses\n func.func @multiple_manual_computation_uses(%arg0: tensor<2x4x8xi32> {sdy.sharding = #sdy.sharding<@mesh_0, [{}, {}, {\"a\"}]>}, %arg1: tensor<32x16x8xi32> {sdy.sharding = #sdy.sharding<@mesh_0, [{}, {}, {\"a\"}]>}) -> (tensor<131x4x8xi32> {sdy.sharding = #sdy.sharding<@mesh_0, [{?}, {?}, {\"a\"}]>}) {\n-  // CHECK-NEXT: %0 = mhlo.copy %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {\"a\"}]>]>} : tensor<2x4x8xi32>\n-  // CHECK-NEXT: %1 = stablehlo.custom_call @SPMDFullToShardShape(%0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<2x4x8xi32>) -> tensor<2x4x2xi32>\n-  // CHECK-NEXT: %2 = stablehlo.custom_call @sdy_testonly(%1) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<2x4x2xi32>) -> tensor<3x4x2xi32>\n-  // CHECK-NEXT: %3 = mhlo.copy %2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<3x4x2xi32>\n-  // CHECK-NEXT: %4 = stablehlo.custom_call @SPMDShardToFullShape(%3) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {\"a\"}]>]>} : (tensor<3x4x2xi32>) -> tensor<3x4x8xi32>\n-  // CHECK-NEXT: %5 = mhlo.copy %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {\"a\"}]>]>} : tensor<32x16x8xi32>\n-  // CHECK-NEXT: %6 = stablehlo.custom_call @SPMDFullToShardShape(%5) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<32x16x8xi32>) -> tensor<32x16x2xi32>\n-  // CHECK-NEXT: %7 = stablehlo.reshape %6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<32x16x2xi32>) -> tensor<128x4x2xi32>\n-  // CHECK-NEXT: %8 = mhlo.copy %7 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<128x4x2xi32>\n-  // CHECK-NEXT: %9 = stablehlo.custom_call @SPMDShardToFullShape(%8) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {\"a\"}]>]>} : (tensor<128x4x2xi32>) -> tensor<128x4x8xi32>\n-  // CHECK-NEXT: %10 = mhlo.copy %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {\"a\"}]>]>} : tensor<3x4x8xi32>\n-  // CHECK-NEXT: %11 = stablehlo.custom_call @SPMDFullToShardShape(%10) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<3x4x8xi32>) -> tensor<3x4x2xi32>\n-  // CHECK-NEXT: %12 = mhlo.copy %9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {\"a\"}]>]>} : tensor<128x4x8xi32>\n-  // CHECK-NEXT: %13 = stablehlo.custom_call @SPMDFullToShardShape(%12) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<128x4x8xi32>) -> tensor<128x4x2xi32>\n-  // CHECK-NEXT: %14 = stablehlo.concatenate %13, %11, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<128x4x2xi32>, tensor<3x4x2xi32>) -> tensor<131x4x2xi32>\n-  // CHECK-NEXT: %15 = mhlo.copy %14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<131x4x2xi32>\n-  // CHECK-NEXT: %16 = stablehlo.custom_call @SPMDShardToFullShape(%15) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {\"a\"}]>]>} : (tensor<131x4x2xi32>) -> tensor<131x4x8xi32>\n-  // CHECK-NEXT: return %16 : tensor<131x4x8xi32>\n+  // CHECK-NEXT: %[[COPY:.*]] = mhlo.copy %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {\"a\"}]>]>} : tensor<2x4x8xi32>\n+  // CHECK-NEXT: %[[FULL_TO_SHARD:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<2x4x8xi32>) -> tensor<2x4x2xi32>\n+  // CHECK-NEXT: %[[CALL:.*]] = call @local_xla.sdy.inlinable_manual_computation_body_8(%[[FULL_TO_SHARD]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<2x4x2xi32>) -> tensor<3x4x2xi32>\n+  // CHECK-NEXT: %[[COPY_1:.*]] = mhlo.copy %[[CALL]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<3x4x2xi32>\n+  // CHECK-NEXT: %[[SHARD_TO_FULL:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_1]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {\"a\"}]>]>} : (tensor<3x4x2xi32>) -> tensor<3x4x8xi32>\n+  // CHECK-NEXT: %[[COPY_2:.*]] = mhlo.copy %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {\"a\"}]>]>} : tensor<32x16x8xi32>\n+  // CHECK-NEXT: %[[FULL_TO_SHARD_2:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY_2]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<32x16x8xi32>) -> tensor<32x16x2xi32>\n+  // CHECK-NEXT: %[[CALL_1:.*]] = call @local_xla.sdy.inlinable_manual_computation_body_9(%[[FULL_TO_SHARD_2]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<32x16x2xi32>) -> tensor<128x4x2xi32\n+  // CHECK-NEXT: %[[COPY_3:.*]] = mhlo.copy %[[CALL_1]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<128x4x2xi32>\n+  // CHECK-NEXT: %[[SHARD_TO_FULL_1:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_3]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {\"a\"}]>]>} : (tensor<128x4x2xi32>) -> tensor<128x4x8xi32>\n+  // CHECK-NEXT: %[[COPY_4:.*]] = mhlo.copy %[[SHARD_TO_FULL]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {\"a\"}]>]>} : tensor<3x4x8xi32>\n+  // CHECK-NEXT: %[[FULL_TO_SHARD_3:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY_4]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<3x4x8xi32>) -> tensor<3x4x2xi32>\n+  // CHECK-NEXT: %[[COPY_5:.*]] = mhlo.copy %[[SHARD_TO_FULL_1]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {\"a\"}]>]>} : tensor<128x4x8xi32>\n+  // CHECK-NEXT: %[[FULL_TO_SHARD_4:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY_5]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<128x4x8xi32>) -> tensor<128x4x2xi32>\n+  // CHECK-NEXT: %[[CALL_2:.*]] = call @local_xla.sdy.inlinable_manual_computation_body_10(%[[FULL_TO_SHARD_3]], %[[FULL_TO_SHARD_4]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<3x4x2xi32>, tensor<128x4x2xi32>) -> tensor<131x4x2xi32>\n+  // CHECK-NEXT: %[[COPY_6:.*]] = mhlo.copy %[[CALL_2]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<131x4x2xi32>\n+  // CHECK-NEXT: %[[SHARD_TO_FULL_2:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_6]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {\"a\"}]>]>} : (tensor<131x4x2xi32>) -> tensor<131x4x8xi32>\n+  // CHECK-NEXT: return %[[SHARD_TO_FULL_2]] : tensor<131x4x8xi32>\n   %1 = sdy.manual_computation(%arg0) in_shardings=[<@mesh_0, [{}, {}, {\"a\"}]>] out_shardings=[<@mesh_0, [{}, {}, {\"a\"}]>] manual_axes={\"a\"} (%arg2: tensor<2x4x2xi32>) {\n     %4 = stablehlo.custom_call @sdy_testonly(%arg2) : (tensor<2x4x2xi32>) -> tensor<3x4x2xi32>\n     sdy.return %4 : tensor<3x4x2xi32>\n@@ -184,25 +164,16 @@ func.func @multiple_manual_computation_uses(%arg0: tensor<2x4x8xi32> {sdy.shardi\n \n // CHECK-LABEL: func @named_computation_in_manual_computation\n func.func @named_computation_in_manual_computation(%arg0: tensor<32xi32>) -> (tensor<32xi32>, tensor<32xi32>, tensor<16xi32>) {\n-// CHECK-NEXT: %0 = mhlo.copy %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"a\", \"b\"}]>]>} : tensor<32xi32>\n-// CHECK-NEXT: %1 = stablehlo.custom_call @SPMDFullToShardShape(%0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"b\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<32xi32>) -> tensor<8xi32>\n-// CHECK-NEXT: %2:2 = sdy.named_computation<\"foo\">(%1) in_shardings=[<@mesh_0, [{}]>] out_shardings=[<@mesh_0, [{\"b\"}]>, <@mesh_0, [{\"b\"}]>] (%arg1: tensor<8xi32>) {\n-// CHECK-NEXT:   %10 = stablehlo.multiply %arg1, %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"b\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<8xi32>\n-// CHECK-NEXT:   %11 = stablehlo.negate %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<8xi32>\n-// CHECK-NEXT:   sdy.return {xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} %10, %11 : tensor<8xi32>, tensor<8xi32>\n-// CHECK-NEXT: } {xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<8xi32>) -> (tensor<8xi32>, tensor<8xi32>)\n-// CHECK-NEXT: %3 = sdy.named_computation<\"no_input_named_computation\">() in_shardings=[] out_shardings=[<@mesh_0, [{}]>] () {\n-// CHECK-NEXT:   %c = stablehlo.constant {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} dense<[0, 1, 2, 3]> : tensor<4xi32>\n-// CHECK-NEXT:   %10 = stablehlo.negate %c {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<4xi32>\n-// CHECK-NEXT:   sdy.return {xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} %10 : tensor<4xi32>\n-// CHECK-NEXT: } {xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : () -> tensor<4xi32>\n-// CHECK-NEXT: %4 = mhlo.copy %2#0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"b\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<8xi32>\n-// CHECK-NEXT: %5 = stablehlo.custom_call @SPMDShardToFullShape(%4) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"a\", \"b\"}]>]>} : (tensor<8xi32>) -> tensor<32xi32>\n-// CHECK-NEXT: %6 = mhlo.copy %2#1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"b\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<8xi32>\n-// CHECK-NEXT: %7 = stablehlo.custom_call @SPMDShardToFullShape(%6) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"a\", \"b\"}]>]>} : (tensor<8xi32>) -> tensor<32xi32>\n-// CHECK-NEXT: %8 = mhlo.copy %3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"b\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<4xi32>\n-// CHECK-NEXT: %9 = stablehlo.custom_call @SPMDShardToFullShape(%8) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"a\", \"b\"}]>]>} : (tensor<4xi32>) -> tensor<16xi32>\n-// CHECK-NEXT: return %5, %7, %9 : tensor<32xi32>, tensor<32xi32>, tensor<16xi32>\n+  // CHECK-NEXT: %[[COPY:.*]] = mhlo.copy %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"a\", \"b\"}]>]>} : tensor<32xi32>\n+  // CHECK-NEXT: %[[FULL_TO_SHARD:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"b\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<32xi32>) -> tensor<8xi32>\n+  // CHECK-NEXT: %[[CALL:.*]]:3 = call @local_xla.sdy.inlinable_manual_computation_body_11(%[[FULL_TO_SHARD]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"b\"}]>, <@mesh_0, [{\"b\"}]>, <@mesh_0, [{\"b\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<8xi32>) -> (tensor<8xi32>, tensor<8xi32>, tensor<4xi32>)\n+  // CHECK-NEXT: %[[COPY_1:.*]] = mhlo.copy %[[CALL]]#0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"b\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<8xi32>\n+  // CHECK-NEXT: %[[SHARD_TO_FULL:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%3) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"a\", \"b\"}]>]>} : (tensor<8xi32>) -> tensor<32xi32>\n+  // CHECK-NEXT: %[[COPY_2:.*]] = mhlo.copy %[[CALL]]#1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"b\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<8xi32>\n+  // CHECK-NEXT: %[[SHARD_TO_FULL_1:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_2]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"a\", \"b\"}]>]>} : (tensor<8xi32>) -> tensor<32xi32>\n+  // CHECK-NEXT: %[[COPY_3:.*]] = mhlo.copy %[[CALL]]#2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"b\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<4xi32>\n+  // CHECK-NEXT: %[[SHARD_TO_FULL_2:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_3]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"a\", \"b\"}]>]>} : (tensor<4xi32>) -> tensor<16xi32>\n+  // CHECK-NEXT: return %[[SHARD_TO_FULL]], %[[SHARD_TO_FULL_1]], %[[SHARD_TO_FULL_2]] : tensor<32xi32>, tensor<32xi32>, tensor<16xi32>\n   %0:3 = sdy.manual_computation(%arg0) in_shardings=[<@mesh_0, [{\"a\", \"b\"}]>] out_shardings=[<@mesh_0, [{\"a\", \"b\"}]>, <@mesh_0, [{\"a\", \"b\"}]>, <@mesh_0, [{\"a\", \"b\"}]>] manual_axes={\"a\"} (%arg1: tensor<8xi32>) {\n     %1:2 = sdy.named_computation<\"foo\">(%arg1) out_shardings=[<@mesh_0, [{\"b\"}]>, <@mesh_0, [{\"b\"}]>] (%arg2: tensor<8xi32>) {\n       %2 = stablehlo.multiply %arg2, %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"b\"}]>]>} : tensor<8xi32>\n@@ -226,11 +197,10 @@ func.func @manual_computation_with_tokens(\n ) -> (!stablehlo.token, tensor<2xi64>) {\n   // CHECK-NEXT: %[[COPY_OPERAND:.*]] = mhlo.copy %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"b\"}]>]>} : tensor<2xi64>\n   // CHECK-NEXT: %[[FULL_TO_SHARD:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY_OPERAND]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<2xi64>) -> tensor<1xi64>\n-  // CHECK-NEXT: %[[TOKEN_CALL:.*]] = stablehlo.custom_call @sdy_testonly(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, []>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (!stablehlo.token) -> !stablehlo.token\n-  // CHECK-NEXT: stablehlo.custom_call @sdy_testonly(%[[TOKEN_CALL]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, []>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (!stablehlo.token) -> ()\n-  // CHECK-NEXT: %[[COPY_RESULT:.*]] = mhlo.copy %[[FULL_TO_SHARD]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<1xi64>\n+  // CHECK-NEXT: %[[CALL:.*]]:2 = call @local_xla.sdy.inlinable_manual_computation_body_12(%arg0, %[[FULL_TO_SHARD]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, []>, <@mesh_0, [{}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (!stablehlo.token, tensor<1xi64>) -> (!stablehlo.token, tensor<1xi64>)\n+  // CHECK-NEXT: %[[COPY_RESULT:.*]] = mhlo.copy %[[CALL]]#1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<1xi64>\n   // CHECK-NEXT: %[[SHARD_TO_FULL:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_RESULT]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"b\"}]>]>} : (tensor<1xi64>) -> tensor<2xi64>\n-  // CHECK-NEXT: return %[[TOKEN_CALL]], %[[SHARD_TO_FULL]] : !stablehlo.token, tensor<2xi64>\n+  // CHECK-NEXT: return %[[CALL]]#0, %[[SHARD_TO_FULL]] : !stablehlo.token, tensor<2xi64>\n   %0:2 = sdy.manual_computation(%arg0, %arg1)\n       in_shardings=[<@mesh_0, []>, <@mesh_0, [{\"b\"}]>]\n       out_shardings=[<@mesh_0, []>, <@mesh_0, [{\"b\"}]>]\n@@ -241,3 +211,138 @@ func.func @manual_computation_with_tokens(\n   } : (!stablehlo.token, tensor<2xi64>) -> (!stablehlo.token, tensor<2xi64>)\n   return %0#0, %0#1 : !stablehlo.token, tensor<2xi64>\n }\n+\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<2x8xf32> {sdy.sharding = #sdy.sharding<@mesh_0, [{}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>},\n+// CHECK-SAME{LITERAL}:     %arg1: tensor<8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_0, [{}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>})\n+// CHECK-SAME{LITERAL}:     -> (tensor<2x32xf32> {sdy.sharding = #sdy.sharding<@mesh_0, [{}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>}) {\n+// CHECK-NEXT:            %[[ADD:.*]] = stablehlo.add %arg0, %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<2x8xf32>\n+// CHECK-NEXT:            %[[DOT:.*]] = stablehlo.dot %[[ADD]], %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<2x8xf32>, tensor<8x32xf32>) -> tensor<2x32xf32>\n+// CHECK-NEXT:            %[[ALL_REDUCE:.*]] = \"stablehlo.all_reduce\"(%[[DOT]]) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>, replica_groups = dense<{{\\[\\[}}0, 1], [2, 3], [4, 5], [6, 7]]> : tensor<4x2xi64>, use_global_device_ids}> ({\n+// CHECK-NEXT:            ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):\n+// CHECK-NEXT:              %[[ADD_1:.*]] = stablehlo.add %arg2, %arg3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, []>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<f32>\n+// CHECK-NEXT:              stablehlo.return %[[ADD_1]] {xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<f32>\n+// CHECK-NEXT:            }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<2x32xf32>) -> tensor<2x32xf32>\n+// CHECK-NEXT:            return %[[ALL_REDUCE]] : tensor<2x32xf32>\n+// CHECK-NEXT:          }\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_0\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<2x8xf32> {sdy.sharding = #sdy.sharding<@mesh_0, [{}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>})\n+// CHECK-SAME{LITERAL}:     -> (tensor<2x8xf32> {sdy.sharding = #sdy.sharding<@mesh_0, [{}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>}) {\n+// CHECK-NEXT:            return %arg0 : tensor<2x8xf32>\n+// CHECK-NEXT:          }\n+\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_1\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<8x4xf32> {sdy.sharding = #sdy.sharding<@mesh_0, [{}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"b\"}>})\n+// CHECK-SAME{LITERAL}:     -> (tensor<8x4xf32> {sdy.sharding = #sdy.sharding<@mesh_0, [{}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"b\"}>}) {\n+// CHECK-NEXT:            return %arg0 : tensor<8x4xf32>\n+// CHECK-NEXT:          }\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_2\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<4x8xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>},\n+// CHECK-SAME{LITERAL}:     %arg1: tensor<8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>})\n+// CHECK-SAME{LITERAL}:     -> (tensor<4x32xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>}) {\n+// CHECK-NEXT:            %[[ADD:.*]] = stablehlo.add %arg0, %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"c\"}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<4x8xf32>\n+// CHECK-NEXT:            %[[DOT:.*]] = stablehlo.dot %[[ADD]], %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"d\"}, {\"c\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<4x8xf32>, tensor<8x32xf32>) -> tensor<4x32xf32>\n+// CHECK-NEXT:            return %[[DOT]] : tensor<4x32xf32>\n+// CHECK-NEXT:          }\n+\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_3\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<2x4x8xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{}, {}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\", \"c\"}>})\n+// CHECK-SAME{LITERAL}:     -> (tensor<2x4x8xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{}, {}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\", \"c\"}>}) {\n+// CHECK-NEXT:            %[[MULT:.*]] = stablehlo.multiply %arg0, %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\", \"c\"}>} : tensor<2x4x8xf32>\n+// CHECK-NEXT:            return %[[MULT]] : tensor<2x4x8xf32>\n+// CHECK-NEXT:          }\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_4\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<2x4x16xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{}, {}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>})\n+// CHECK-SAME{LITERAL}:     -> (tensor<2x4x16xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{}, {}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>}) {\n+// CHECK-NEXT:            %[[COPY:.*]] = mhlo.copy %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {\"c\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<2x4x16xf32>\n+// CHECK-NEXT:            %[[FULL_TO_SHARD:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\", \"c\"}>} : (tensor<2x4x16xf32>) -> tensor<2x4x8xf32>\n+// CHECK-NEXT:            %[[CALL:.*]] = call @local_xla.sdy.inlinable_manual_computation_body_3(%[[FULL_TO_SHARD]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\", \"c\"}>} : (tensor<2x4x8xf32>) -> tensor<2x4x8xf32>\n+// CHECK-NEXT:            %[[COPY_1:.*]] = mhlo.copy %[[CALL]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\", \"c\"}>} : tensor<2x4x8xf32>\n+// CHECK-NEXT:            %[[SHARD_TO_FULL:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_1]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {\"c\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<2x4x8xf32>) -> tensor<2x4x16xf32>\n+// CHECK-NEXT:            return %[[SHARD_TO_FULL]] : tensor<2x4x16xf32>\n+// CHECK-NEXT:          }\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_5\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<2x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{}, {}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>})\n+// CHECK-SAME{LITERAL}:     -> (tensor<2x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{}, {}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>}) {\n+// CHECK-NEXT:            %[[COPY:.*]] = mhlo.copy %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {\"b\"}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<2x8x16xf32>\n+// CHECK-NEXT:            %[[FULL_TO_SHARD:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<2x8x16xf32>) -> tensor<2x4x16xf32>\n+// CHECK-NEXT:            %[[CALL:.*]] = call @local_xla.sdy.inlinable_manual_computation_body_4(%[[FULL_TO_SHARD]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<2x4x16xf32>) -> tensor<2x4x16xf32>\n+// CHECK-NEXT:            %[[COPY_1:.*]] = mhlo.copy %[[CALL]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<2x4x16xf32>\n+// CHECK-NEXT:            %[[SHARD_TO_FULL:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_1]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {\"b\"}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<2x4x16xf32>) -> tensor<2x8x16xf32>\n+// CHECK-NEXT:            return %[[SHARD_TO_FULL]] : tensor<2x8x16xf32>\n+// CHECK-NEXT:          }\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_6\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<2x4xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>})\n+// CHECK-SAME{LITERAL}:     -> (tensor<2x4xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>}) {\n+// CHECK-NEXT:            %[[MULT:.*]] = stablehlo.multiply %arg0, %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<2x4xf32>\n+// CHECK-NEXT:            %[[ADD:.*]] = stablehlo.add %[[MULT]], %[[MULT]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"c\"}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<2x4xf32>\n+// CHECK-NEXT:            %[[SUB:.*]] = stablehlo.subtract %[[ADD]], %[[ADD]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{\"c\", \"d\"}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<2x4xf32>\n+// CHECK-NEXT:            return %[[SUB]] : tensor<2x4xf32>\n+// CHECK-NEXT:          }\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_7\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<2x8xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>})\n+// CHECK-SAME{LITERAL}:     -> (tensor<2x8xf32> {sdy.sharding = #sdy.sharding<@mesh_1, [{}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>}) {\n+// CHECK-NEXT:            %[[COPY:.*]] = mhlo.copy %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {\"b\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<2x8xf32>\n+// CHECK-NEXT:            %[[FULL_TO_SHARD:.*]] = stablehlo.custom_call @SPMDFullToShardShape(%[[COPY]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<2x8xf32>) -> tensor<2x4xf32>\n+// CHECK-NEXT:            %[[CALL:.*]] = call @local_xla.sdy.inlinable_manual_computation_body_6(%[[FULL_TO_SHARD]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (tensor<2x4xf32>) -> tensor<2x4xf32>\n+// CHECK-NEXT:            %[[COPY_1:.*]] = mhlo.copy %[[CALL]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : tensor<2x4xf32>\n+// CHECK-NEXT:            %[[SHARD_TO_FULL:.*]] = stablehlo.custom_call @SPMDShardToFullShape(%[[COPY_1]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {\"b\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<2x4xf32>) -> tensor<2x8xf32>\n+// CHECK-NEXT:            %[[ADD:.*]] = stablehlo.add %[[SHARD_TO_FULL]], %[[SHARD_TO_FULL]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_1, [{}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<2x8xf32>\n+// CHECK-NEXT:            return %[[ADD]] : tensor<2x8xf32>\n+// CHECK-NEXT:          }\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_8\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<2x4x2xi32> {sdy.sharding = #sdy.sharding<@mesh_0, [{}, {}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>})\n+// CHECK-SAME{LITERAL}:     -> (tensor<3x4x2xi32> {sdy.sharding = #sdy.sharding<@mesh_0, [{}, {}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>}) {\n+// CHECK-NEXT:            %[[CUSTOM_CALL:.*]] = stablehlo.custom_call @sdy_testonly(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<2x4x2xi32>) -> tensor<3x4x2xi32>\n+// CHECK-NEXT:            return %[[CUSTOM_CALL]] : tensor<3x4x2xi32>\n+// CHECK-NEXT:          }\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_9\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<32x16x2xi32> {sdy.sharding = #sdy.sharding<@mesh_0, [{}, {}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>})\n+// CHECK-SAME{LITERAL}:     -> (tensor<128x4x2xi32> {sdy.sharding = #sdy.sharding<@mesh_0, [{}, {}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>}) {\n+// CHECK-NEXT:            %[[RESHAPE:.*]] = stablehlo.reshape %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<32x16x2xi32>) -> tensor<128x4x2xi32>\n+// CHECK-NEXT:            return %[[RESHAPE]] : tensor<128x4x2xi32>\n+// CHECK-NEXT:          }\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_10\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<3x4x2xi32> {sdy.sharding = #sdy.sharding<@mesh_0, [{}, {}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>},\n+// CHECK-SAME{LITERAL}:     %arg1: tensor<128x4x2xi32> {sdy.sharding = #sdy.sharding<@mesh_0, [{}, {}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>}) -> (tensor<131x4x2xi32> {sdy.sharding = #sdy.sharding<@mesh_0, [{}, {}, {}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>}) {\n+// CHECK-NEXT:            %[[CONCAT:.*]] = stablehlo.concatenate %arg1, %arg0, dim = 0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}, {}, {}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<128x4x2xi32>, tensor<3x4x2xi32>) -> tensor<131x4x2xi32>\n+// CHECK-NEXT:            return %[[CONCAT]] : tensor<131x4x2xi32>\n+// CHECK-NEXT:          }\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_11\n+// CHECK-SAME{LITERAL}:     %arg0: tensor<8xi32> {sdy.sharding = #sdy.sharding<@mesh_0, [{\"b\"}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>})\n+// CHECK-SAME{LITERAL}:     -> (tensor<8xi32> {sdy.sharding = #sdy.sharding<@mesh_0, [{\"b\"}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>}, tensor<8xi32> {sdy.sharding = #sdy.sharding<@mesh_0, [{\"b\"}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>},\n+// CHECK-SAME{LITERAL}:         tensor<4xi32> {sdy.sharding = #sdy.sharding<@mesh_0, [{\"b\"}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>}) {\n+// CHECK-NEXT:            %[[MC:.*]]:2 = sdy.named_computation<\"foo\">(%arg0) in_shardings=[<@mesh_0, [{}]>] out_shardings=[<@mesh_0, [{\"b\"}]>, <@mesh_0, [{\"b\"}]>] (%arg1: tensor<8xi32>) {\n+// CHECK-NEXT:              %[[MULT:.*]] = stablehlo.multiply %arg1, %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{\"b\"}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<8xi32>\n+// CHECK-NEXT:              %[[NEGATE:.*]] = stablehlo.negate %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<8xi32>\n+// CHECK-NEXT:              sdy.return {xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} %[[MULT]], %[[NEGATE]] : tensor<8xi32>, tensor<8xi32>\n+// CHECK-NEXT:            } {xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : (tensor<8xi32>) -> (tensor<8xi32>, tensor<8xi32>)\n+// CHECK-NEXT:            %[[NC:.*]] = sdy.named_computation<\"no_input_named_computation\">() in_shardings=[] out_shardings=[<@mesh_0, [{}]>] () {\n+// CHECK-NEXT:              %[[CONST:.*]] = stablehlo.constant {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} dense<[0, 1, 2, 3]> : tensor<4xi32>\n+// CHECK-NEXT:              %[[NEGATE_1:.*]] = stablehlo.negate %[[CONST]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, [{}]>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : tensor<4xi32>\n+// CHECK-NEXT:              sdy.return {xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} %[[NEGATE_1]] : tensor<4xi32>\n+// CHECK-NEXT:            } {xla.sdy.manual_axes = #sdy<manual_axes{\"a\"}>} : () -> tensor<4xi32>\n+// CHECK-NEXT:            return %[[MC]]#0, %[[MC]]#1, %[[NC]] : tensor<8xi32>, tensor<8xi32>, tensor<4xi32>\n+// CHECK-NEXT:          }\n+\n+// CHECK-LABEL:         func @local_xla.sdy.inlinable_manual_computation_body_12\n+// CHECK-SAME{LITERAL}:     %arg0: !stablehlo.token {sdy.sharding = #sdy.sharding<@mesh_0, []>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>},\n+// CHECK-SAME{LITERAL}:     %arg1: tensor<1xi64> {sdy.sharding = #sdy.sharding<@mesh_0, [{}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>})\n+// CHECK-SAME{LITERAL}:     -> (!stablehlo.token {sdy.sharding = #sdy.sharding<@mesh_0, []>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>}, tensor<1xi64> {sdy.sharding = #sdy.sharding<@mesh_0, [{}]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>}) {\n+// CHECK-NEXT:            %[[CUSTOM_CALL:.*]] = stablehlo.custom_call @sdy_testonly(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, []>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (!stablehlo.token) -> !stablehlo.token\n+// CHECK-NEXT:            stablehlo.custom_call @sdy_testonly(%[[CUSTOM_CALL]]) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_0, []>]>, xla.sdy.manual_axes = #sdy<manual_axes{\"a\", \"b\"}>} : (!stablehlo.token) -> ()\n+// CHECK-NEXT:            return %[[CUSTOM_CALL]], %arg1 : !stablehlo.token, tensor<1xi64>\n+// CHECK-NEXT:          }"
        },
        {
            "sha": "05aca1f279eec97faf339bb5448f20b2de4d9fdf",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/stablehlo_round_trip_shard_map_export_import_pipeline.mlir",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_round_trip_shard_map_export_import_pipeline.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/666523412a6fbad8c8ff4283d970693eed0dd4eb/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_round_trip_shard_map_export_import_pipeline.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_round_trip_shard_map_export_import_pipeline.mlir?ref=666523412a6fbad8c8ff4283d970693eed0dd4eb",
            "patch": "@@ -1,4 +1,4 @@\n-// RUN: sdy_opt %s -xla-sdy-stablehlo-export-pipeline='keep-hlo-sharding-constraints=true keep-shard-map-body-as-func=true' -xla-sdy-stablehlo-import-pipeline 2>&1 | FileCheck %s\n+// RUN: sdy_opt %s -xla-sdy-stablehlo-export-pipeline='keep-hlo-sharding-constraints=true' -xla-sdy-stablehlo-import-pipeline 2>&1 | FileCheck %s\n \n sdy.mesh @mesh = <[\"a\"=4, \"b\"=2]>\n "
        }
    ],
    "stats": {
        "total": 723,
        "additions": 520,
        "deletions": 203
    }
}