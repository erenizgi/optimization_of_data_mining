{
    "author": "felixwqp",
    "message": "Populate the cost for async collective in both async-start and the computation root op.\n\nPiperOrigin-RevId: 822223031",
    "sha": "2de2bb8581a6ea982ae4cebcf37c238599251e88",
    "files": [
        {
            "sha": "4827b8d308c42cfaf75d98ed9314684cbfe6c222",
            "filename": "third_party/xla/xla/service/gpu/model/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2de2bb8581a6ea982ae4cebcf37c238599251e88/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2de2bb8581a6ea982ae4cebcf37c238599251e88/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD?ref=2de2bb8581a6ea982ae4cebcf37c238599251e88",
            "patch": "@@ -906,7 +906,9 @@ xla_cc_test(\n     deps = [\n         \":sol_gpu_cost_model_stats_collection\",\n         \"//xla:shape_util\",\n+        \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:gpu_device_info_for_tests\",\n         \"//xla/service/gpu/model/experimental:symbolic_expr\","
        },
        {
            "sha": "f0fa4d8cff4297f700cff215b0440e1f012b09f8",
            "filename": "third_party/xla/xla/service/gpu/model/sol_gpu_cost_model_stats_collection.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2de2bb8581a6ea982ae4cebcf37c238599251e88/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2de2bb8581a6ea982ae4cebcf37c238599251e88/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection.cc?ref=2de2bb8581a6ea982ae4cebcf37c238599251e88",
            "patch": "@@ -60,8 +60,18 @@ bool SetReificationCost(HloInstruction* instr, double cost_us) {\n     return false;\n   }\n   auto reification_cost = gpu_config->add_reification_cost();\n+  VLOG(3) << \"Setting exec_time_us=\" << cost_us << \" for \" << instr->name()\n+          << \" in SolGpuCostModelStatsCollection\";\n   reification_cost->set_exec_time_us(cost_us);\n   reification_cost->set_name(\"sol\");\n+  if (instr->opcode() == HloOpcode::kAsyncStart &&\n+      instr->async_wrapped_instruction() != nullptr) {\n+    VLOG(9) << \"AsyncStart: Setting reification cost for async start \"\n+            << instr->ToString() << \" computation:\"\n+            << instr->async_wrapped_computation()->ToString();\n+    return SetReificationCost(\n+        instr->async_wrapped_computation()->root_instruction(), cost_us);\n+  }\n   return instr->set_backend_config(*gpu_config).ok();\n }\n \n@@ -72,9 +82,13 @@ bool RecordReificationCost(HloInstruction& instr,\n     HloGraphNode from(&instr, /*original_position=*/-1);\n     HloGraphNode to(instr.users()[0], /*original_position=*/-1);\n     if (estimator.IsAsyncPair(from, to)) {\n+      VLOG(10) << \"Recording reification cost for async pair from: \"\n+               << instr.ToString() << \" to: \" << instr.users()[0]->ToString();\n       return SetReificationCost(&instr, estimator.GetLatencyBetween(from, to));\n     }\n   }\n+  VLOG(10) << \"Recording reification cost for single node: \"\n+           << instr.ToString();\n   return SetReificationCost(&instr, estimator.NodeCost(&instr));\n }\n "
        },
        {
            "sha": "e5c28083354e50ff9226605caa951badcf712ed3",
            "filename": "third_party/xla/xla/service/gpu/model/sol_gpu_cost_model_stats_collection_test.cc",
            "status": "modified",
            "additions": 43,
            "deletions": 22,
            "changes": 65,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/2de2bb8581a6ea982ae4cebcf37c238599251e88/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/2de2bb8581a6ea982ae4cebcf37c238599251e88/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fsol_gpu_cost_model_stats_collection_test.cc?ref=2de2bb8581a6ea982ae4cebcf37c238599251e88",
            "patch": "@@ -23,10 +23,12 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/strings/string_view.h\"\n #include \"mlir/IR/MLIRContext.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/model/experimental/symbolic_expr.h\"\n+#include \"xla/service/hlo_cost_analysis.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n@@ -43,26 +45,9 @@ using ShapeSizeFn = std::function<int64_t(const Shape&)>;\n \n class SolGpuCostModelStatsCollectionTest\n     : public HloHardwareIndependentTestBase {\n- public:\n-  explicit SolGpuCostModelStatsCollectionTest() {\n-    ShapeSizeFn shape_size_bytes =\n-        [&shape_size_bytes](const Shape& shape) -> int64_t {\n-      int64_t shape_size = 0;\n-      if (shape.IsTuple()) {\n-        for (auto& sub_shape : shape.tuple_shapes()) {\n-          shape_size += shape_size_bytes(sub_shape);\n-        }\n-        return shape_size;\n-      }\n-      return ShapeUtil::ByteSizeOfElements(shape);\n-    };\n-    shape_size_fn_ = shape_size_bytes;\n-  }\n-\n  protected:\n   se::DeviceDescription device_info_ =\n       TestGpuDeviceInfo::RTXA6000DeviceInfo(se::CudaComputeCapability(9, 0));\n-  ShapeSizeFn shape_size_fn_;\n   int pointer_size_ = 8;\n   mlir::MLIRContext mlir_context_;\n   SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n@@ -89,11 +74,11 @@ TEST_F(SolGpuCostModelStatsCollectionTest,\n \n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kHloText));\n \n-  TF_ASSERT_OK_AND_ASSIGN(\n-      bool changed,\n-      SolGpuCostModelStatsCollection(device_info_, shape_size_fn_,\n-                                     pointer_size_, &symbolic_expr_context_)\n-          .Run(module.get()));\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed,\n+                          SolGpuCostModelStatsCollection(\n+                              device_info_, HloCostAnalysis::DefaultShapeSize,\n+                              pointer_size_, &symbolic_expr_context_)\n+                              .Run(module.get()));\n \n   VLOG(1) << module->ToString();\n \n@@ -105,6 +90,42 @@ TEST_F(SolGpuCostModelStatsCollectionTest,\n                   ->reification_cost(),\n               ElementsAre(Property(&ReificationCost::exec_time_us, Gt(0))));\n }\n+TEST_F(SolGpuCostModelStatsCollectionTest,\n+       RecordsRuntimeInfoForAsyncStartReduceScatter) {\n+  constexpr absl::string_view kHloText = R\"(\n+    HloModule async_rs_test\n+    %add.f32 (x: f32[], y: f32[]) -> f32[] {\n+      %x = f32[] parameter(0)\n+      %y = f32[] parameter(1)\n+      ROOT %add = f32[] add(%x, %y)\n+    }\n+    %async_rs {\n+      %p0 = f32[4096,128256] parameter(0)\n+      ROOT %rs = f32[512,128256] reduce-scatter(%p0), channel_id=1,\n+        replica_groups={{0,1,2,3,4,5,6,7}}, dimensions={0}, to_apply=%add.f32\n+    }\n+    ENTRY main {\n+      %param = f32[4096,128256] parameter(0)\n+      %rs_start = ((f32[4096,128256]), f32[512,128256], u32[])\n+        async-start(%param), calls=%async_rs\n+      ROOT %rs_done = f32[512,128256] async-done(%rs_start)\n+  })\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kHloText));\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed,\n+                          SolGpuCostModelStatsCollection(\n+                              device_info_, HloCostAnalysis::DefaultShapeSize,\n+                              pointer_size_, &symbolic_expr_context_)\n+                              .Run(module.get()));\n+  VLOG(1) << module->ToString();\n+  EXPECT_FALSE(changed);\n+  HloInstruction* rs_start = FindInstruction(module.get(), \"rs_start\");\n+  ASSERT_NE(rs_start, nullptr);\n+  HloComputation* async_comp = rs_start->async_wrapped_computation();\n+  ASSERT_NE(async_comp, nullptr);\n+  HloInstruction* rs_instr = async_comp->root_instruction();\n \n+  EXPECT_THAT(rs_instr->backend_config<GpuBackendConfig>()->reification_cost(),\n+              ElementsAre(Property(&ReificationCost::exec_time_us, Gt(0))));\n+}\n }  // namespace\n }  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 81,
        "additions": 59,
        "deletions": 22
    }
}