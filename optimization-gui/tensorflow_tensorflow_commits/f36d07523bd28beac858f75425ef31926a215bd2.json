{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 805229821",
    "sha": "f36d07523bd28beac858f75425ef31926a215bd2",
    "files": [
        {
            "sha": "83b4257608434634ccaf7dffd49377ad59db4be3",
            "filename": "tensorflow/core/kernels/data/cache_dataset_ops.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 16,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f36d07523bd28beac858f75425ef31926a215bd2/tensorflow%2Fcore%2Fkernels%2Fdata%2Fcache_dataset_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f36d07523bd28beac858f75425ef31926a215bd2/tensorflow%2Fcore%2Fkernels%2Fdata%2Fcache_dataset_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fcache_dataset_ops.cc?ref=f36d07523bd28beac858f75425ef31926a215bd2",
            "patch": "@@ -322,9 +322,8 @@ class CacheDatasetOp::FileDatasetBase : public DatasetBase {\n           : DatasetIterator<FileDatasetBase>(params),\n             cur_index_(0),\n             shard_id_(0),\n-            filename_(\n-                strings::StrCat(params.dataset->filename_, \"_\", shard_id_)),\n-            lockfile_(strings::StrCat(filename_, kLockFileSuffix)),\n+            filename_(absl::StrCat(params.dataset->filename_, \"_\", shard_id_)),\n+            lockfile_(absl::StrCat(filename_, kLockFileSuffix)),\n             lockfile_created_(false),\n             iteration_completed_(false) {}\n \n@@ -333,7 +332,7 @@ class CacheDatasetOp::FileDatasetBase : public DatasetBase {\n           LOG(WARNING) << kIncompleteCacheErrorMessage;\n           std::vector<string> cache_files;\n           absl::Status s = dataset()->env_->GetMatchingPaths(\n-              strings::StrCat(filename_, \"*\"), &cache_files);\n+              absl::StrCat(filename_, \"*\"), &cache_files);\n           if (!s.ok()) {\n             LOG(WARNING) << \"Failed to get matching files on \" << filename_\n                          << \"* : \" << s.ToString();\n@@ -434,8 +433,8 @@ class CacheDatasetOp::FileDatasetBase : public DatasetBase {\n \n           // Start caching to a new shard.\n           shard_id_++;\n-          filename_ = strings::StrCat(dataset()->filename_, \"_\", shard_id_);\n-          lockfile_ = strings::StrCat(filename_, kLockFileSuffix);\n+          filename_ = absl::StrCat(dataset()->filename_, \"_\", shard_id_);\n+          lockfile_ = absl::StrCat(filename_, kLockFileSuffix);\n           lockfile_created_ = false;\n         }\n         TF_RETURN_IF_ERROR(SaveInput(ctx, writer, input_impl_));\n@@ -473,8 +472,8 @@ class CacheDatasetOp::FileDatasetBase : public DatasetBase {\n             return errors::Internal(\"Invalid value for shard_id \", temp);\n           }\n         }\n-        filename_ = strings::StrCat(dataset()->filename_, \"_\", shard_id_);\n-        lockfile_ = strings::StrCat(filename_, kLockFileSuffix);\n+        filename_ = absl::StrCat(dataset()->filename_, \"_\", shard_id_);\n+        lockfile_ = absl::StrCat(filename_, kLockFileSuffix);\n         writer_ = std::make_unique<BundleWriter>(dataset()->env_, filename_);\n         return absl::OkStatus();\n       }\n@@ -527,7 +526,7 @@ class CacheDatasetOp::FileDatasetBase : public DatasetBase {\n         TF_RETURN_IF_ERROR(\n             dataset()->env_->NewWritableFile(lockfile_, &lockfile));\n         TF_RETURN_IF_ERROR(lockfile->Append(\n-            strings::StrCat(kCreatedAt, \": \", EnvTime::NowSeconds())));\n+            absl::StrCat(kCreatedAt, \": \", EnvTime::NowSeconds())));\n \n         // At this point we know that\n         // 1. There is no conflicting checkpoint with prefix `filename_`.\n@@ -558,16 +557,15 @@ class CacheDatasetOp::FileDatasetBase : public DatasetBase {\n           std::vector<tstring> prefixes;\n           prefixes.reserve(shard_id_ + 1);\n           for (size_t i = 0; i <= shard_id_; ++i) {\n-            prefixes.emplace_back(\n-                strings::StrCat(dataset()->filename_, \"_\", i));\n+            prefixes.emplace_back(absl::StrCat(dataset()->filename_, \"_\", i));\n           }\n           TF_RETURN_IF_ERROR(\n               MergeBundles(dataset()->env_, prefixes, dataset()->filename_));\n         }\n         // Delete all lockfiles.\n         for (size_t i = 0; i <= shard_id_; ++i) {\n           TF_RETURN_IF_ERROR(dataset()->env_->DeleteFile(\n-              strings::StrCat(dataset()->filename_, \"_\", i, kLockFileSuffix)));\n+              absl::StrCat(dataset()->filename_, \"_\", i, kLockFileSuffix)));\n         }\n         return absl::OkStatus();\n       }\n@@ -690,12 +688,12 @@ class CacheDatasetOp::FileDatasetBase : public DatasetBase {\n         case Mode::read:\n           iterator_ =\n               std::make_unique<FileReaderIterator>(FileReaderIterator::Params{\n-                  dataset(), strings::StrCat(prefix(), kImpl)});\n+                  dataset(), absl::StrCat(prefix(), kImpl)});\n           break;\n         case Mode::write:\n           iterator_ =\n               std::make_unique<FileWriterIterator>(FileWriterIterator::Params{\n-                  dataset(), strings::StrCat(prefix(), kImpl)});\n+                  dataset(), absl::StrCat(prefix(), kImpl)});\n       }\n       TF_RETURN_IF_ERROR(iterator_->InitializeBase(ctx, this));\n       return iterator_->Initialize(ctx);\n@@ -1058,12 +1056,12 @@ class CacheDatasetOp::MemoryDatasetBase : public DatasetBase {\n       if (cache_->IsCompleted()) {\n         iterator_ = std::make_unique<MemoryReaderIterator>(\n             MemoryReaderIterator::Params{dataset(),\n-                                         strings::StrCat(prefix(), kImpl)},\n+                                         absl::StrCat(prefix(), kImpl)},\n             cache_);\n       } else {\n         iterator_ = std::make_unique<MemoryWriterIterator>(\n             MemoryWriterIterator::Params{dataset(),\n-                                         strings::StrCat(prefix(), kImpl)},\n+                                         absl::StrCat(prefix(), kImpl)},\n             cache_);\n       }\n       TF_RETURN_IF_ERROR(iterator_->InitializeBase(ctx, this));"
        },
        {
            "sha": "2ccf09149c4c34ac65cef860955dbaf2f4764259",
            "filename": "tensorflow/core/kernels/data/cache_dataset_ops_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f36d07523bd28beac858f75425ef31926a215bd2/tensorflow%2Fcore%2Fkernels%2Fdata%2Fcache_dataset_ops_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f36d07523bd28beac858f75425ef31926a215bd2/tensorflow%2Fcore%2Fkernels%2Fdata%2Fcache_dataset_ops_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fcache_dataset_ops_test.cc?ref=f36d07523bd28beac858f75425ef31926a215bd2",
            "patch": "@@ -84,7 +84,7 @@ class CacheDatasetOpTest : public DatasetOpsTestBase {\n     if (!cache_filename_.empty()) {\n       std::vector<string> cache_files;\n       absl::Status s = device_->env()->GetMatchingPaths(\n-          strings::StrCat(cache_filename_, \"*\"), &cache_files);\n+          absl::StrCat(cache_filename_, \"*\"), &cache_files);\n       if (!s.ok()) {\n         LOG(WARNING) << \"Failed to get matching files on \" << cache_filename_\n                      << \"* : \" << s;"
        },
        {
            "sha": "6d4bfc88504a7e6ee2d763de33e9238e516ec284",
            "filename": "tensorflow/core/kernels/data/concatenate_dataset_op.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f36d07523bd28beac858f75425ef31926a215bd2/tensorflow%2Fcore%2Fkernels%2Fdata%2Fconcatenate_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f36d07523bd28beac858f75425ef31926a215bd2/tensorflow%2Fcore%2Fkernels%2Fdata%2Fconcatenate_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fconcatenate_dataset_op.cc?ref=f36d07523bd28beac858f75425ef31926a215bd2",
            "patch": "@@ -200,7 +200,7 @@ class ConcatenateDatasetOp::Dataset : public DatasetBase {\n       TF_ASSIGN_OR_RETURN(input_contexts_,\n                           CreateInputIteratorContexts(ctx, dataset()));\n       TF_RETURN_IF_ERROR(dataset()->input_->MakeIterator(\n-          &input_contexts_[0], this, strings::StrCat(prefix(), \"[0]\"),\n+          &input_contexts_[0], this, absl::StrCat(prefix(), \"[0]\"),\n           &input_impls_[0]));\n \n       ctx->MergeCheckpoint(input_contexts_[0].checkpoint());\n@@ -221,7 +221,7 @@ class ConcatenateDatasetOp::Dataset : public DatasetBase {\n           // Creates the second iterator immediately in the case of\n           // global random shuffling.\n           TF_RETURN_IF_ERROR(dataset()->to_concatenate_->MakeIterator(\n-              &input_contexts_[1], this, strings::StrCat(prefix(), \"[1]\"),\n+              &input_contexts_[1], this, absl::StrCat(prefix(), \"[1]\"),\n               &input_impls_[1]));\n           ctx->MergeCheckpoint(input_contexts_[1].checkpoint());\n         }\n@@ -312,7 +312,7 @@ class ConcatenateDatasetOp::Dataset : public DatasetBase {\n           // Creates the second iterator only when the first iterator\n           // is exhausted to save memory usage.\n           TF_RETURN_IF_ERROR(dataset()->to_concatenate_->MakeIterator(\n-              &input_contexts_[1], this, strings::StrCat(prefix(), \"[1]\"),\n+              &input_contexts_[1], this, absl::StrCat(prefix(), \"[1]\"),\n               &input_impls_[1]));\n           ctx->MergeCheckpoint(input_contexts_[1].checkpoint());\n         }\n@@ -401,7 +401,7 @@ class ConcatenateDatasetOp::Dataset : public DatasetBase {\n \n         if (!static_cast<bool>(input_uninitialized[1])) {\n           TF_RETURN_IF_ERROR(dataset()->to_concatenate_->MakeIterator(\n-              &input_contexts_[1], this, strings::StrCat(prefix(), \"[1]\"),\n+              &input_contexts_[1], this, absl::StrCat(prefix(), \"[1]\"),\n               &input_impls_[1]));\n \n           input_contexts_[1].set_restored_element_count(\n@@ -424,7 +424,7 @@ class ConcatenateDatasetOp::Dataset : public DatasetBase {\n       }\n       if (!static_cast<bool>(input_uninitialized[1])) {\n         TF_RETURN_IF_ERROR(dataset()->to_concatenate_->MakeIterator(\n-            &input_contexts_[1], this, strings::StrCat(prefix(), \"[1]\"),\n+            &input_contexts_[1], this, absl::StrCat(prefix(), \"[1]\"),\n             &input_impls_[1]));\n         ctx->MergeCheckpoint(input_contexts_[1].checkpoint());\n "
        }
    ],
    "stats": {
        "total": 42,
        "additions": 20,
        "deletions": 22
    }
}