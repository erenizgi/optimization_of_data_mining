{
    "author": "basioli-k",
    "message": "[XLA:CPU] Enable creating XLA:CPU client from a given topology\n\nThis will allow us cross compilation via pjrt.\n\nPiperOrigin-RevId: 836168772",
    "sha": "484c657f45104a146250384d4998314e77f48d30",
    "files": [
        {
            "sha": "140c9adba8426d66bfdd5c530aebac6ca9e3175a",
            "filename": "third_party/xla/xla/pjrt/cpu/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/484c657f45104a146250384d4998314e77f48d30/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/484c657f45104a146250384d4998314e77f48d30/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2FBUILD?ref=484c657f45104a146250384d4998314e77f48d30",
            "patch": "@@ -328,6 +328,7 @@ cc_library(\n         \"//xla/pjrt:pjrt_compiler\",\n         \"//xla/pjrt:pjrt_executable\",\n         \"//xla/pjrt/plugin/xla_cpu:cpu_client_options\",\n+        \"//xla/pjrt/plugin/xla_cpu:cpu_topology_description\",\n         \"//xla/pjrt/plugin/xla_cpu:xla_cpu_pjrt_client\",\n         \"//xla/stream_executor/platform:initialize\",\n         \"//xla/tsl/platform:statusor\",\n@@ -336,6 +337,7 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//mlir:IR\",\n+        \"@local_tsl//tsl/platform:casts\",\n     ],\n     alwayslink = True,\n )\n@@ -344,11 +346,16 @@ xla_cc_test(\n     name = \"cpu_pjrt_compiler_test\",\n     srcs = [\"cpu_pjrt_compiler_test.cc\"],\n     deps = [\n+        \":cpu_client\",\n         \":cpu_pjrt_compiler\",\n+        \"//xla:debug_options_flags\",\n+        \"//xla/backends/cpu/codegen:cpu_features\",\n         \"//xla/hlo/builder:xla_computation\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n         \"//xla/mlir_hlo\",\n         \"//xla/pjrt:pjrt_executable\",\n+        \"//xla/pjrt/plugin/xla_cpu:cpu_topology\",\n+        \"//xla/pjrt/plugin/xla_cpu:cpu_topology_description\",\n         \"//xla/pjrt/plugin/xla_cpu:xla_cpu_pjrt_client\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test_main\","
        },
        {
            "sha": "c63289caca5e2bc27014a08f13a31164103937c0",
            "filename": "third_party/xla/xla/pjrt/cpu/cpu_client.cc",
            "status": "modified",
            "additions": 48,
            "deletions": 29,
            "changes": 77,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/484c657f45104a146250384d4998314e77f48d30/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/484c657f45104a146250384d4998314e77f48d30/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.cc?ref=484c657f45104a146250384d4998314e77f48d30",
            "patch": "@@ -146,6 +146,16 @@ static int CpuDeviceCount() {\n   return GetDebugOptionsFromFlags().xla_force_host_platform_device_count();\n }\n \n+namespace cpu {\n+\n+PjRtPlatformId PlatformId() { return tsl::Fingerprint64(CpuName()); }\n+\n+absl::string_view PlatformName() { return CpuName(); }\n+\n+absl::string_view PlatformVersion() { return CpuName(); }\n+\n+}  // namespace cpu\n+\n namespace {\n \n // A custom memory allocator function passed via the CPU client options.\n@@ -192,23 +202,50 @@ absl::StatusOr<std::unique_ptr<PjRtClient>> GetPjRtCpuClient(\n   int cpu_device_count = options.cpu_device_count.value_or(CpuDeviceCount());\n   size_t num_threads = std::max(DefaultThreadPoolSize(), cpu_device_count);\n \n+  std::unique_ptr<CpuDeviceMemory::Allocator> allocator =\n+      options.allocator ? std::make_unique<CustomAllocator>(options.allocator)\n+                        : CpuDeviceMemory::MakeDefaultAllocator();\n+\n+  std::unique_ptr<CpuTopologyDescription> topology = nullptr;\n+  if (!options.topology) {\n+    std::vector<CpuTopology::CpuDevice> cpu_topology_devices;\n+    cpu_topology_devices.reserve(cpu_device_count);\n+    for (int i = 0; i < cpu_device_count; ++i) {\n+      cpu_topology_devices.push_back(\n+          CpuTopology::CpuDevice{options.process_id, i});\n+    }\n+\n+    topology = std::make_unique<CpuTopologyDescription>(\n+        cpu::PlatformId(), cpu::PlatformName(), cpu::PlatformVersion(),\n+        cpu_topology_devices,\n+        cpu::DetectMachineAttributes(\n+            cpu::CpuFeatureFromString(\n+                GetDebugOptionsFromFlags().xla_cpu_max_isa()))\n+            .features);\n+  } else {\n+    topology = std::make_unique<CpuTopologyDescription>(*options.topology);\n+    if (topology->cpu_topology().number_of_devices() != cpu_device_count) {\n+      return InvalidArgument(\n+          \"Number of devices in topology (%d) does not match \"\n+          \"cpu_device_count (%d)\",\n+          topology->cpu_topology().number_of_devices(), cpu_device_count);\n+    }\n+  }\n+\n   std::vector<std::unique_ptr<PjRtCpuDevice>> devices;\n-  for (int i = 0; i < cpu_device_count; ++i) {\n+  devices.reserve(topology->cpu_topology().number_of_devices());\n+  for (const auto& topology_device : topology->cpu_topology().devices()) {\n     auto device = std::make_unique<PjRtCpuDevice>(\n-        options.process_id, /*local_device_id=*/i,\n+        topology_device.process_id, topology_device.local_device_id,\n         options.max_inflight_computations_per_device);\n     devices.push_back(std::move(device));\n   }\n \n-  std::unique_ptr<CpuDeviceMemory::Allocator> allocator =\n-      options.allocator ? std::make_unique<CustomAllocator>(options.allocator)\n-                        : CpuDeviceMemory::MakeDefaultAllocator();\n-\n   return std::unique_ptr<PjRtClient>(new PjRtCpuClient(\n       options.process_id, std::move(devices), std::move(allocator),\n       std::move(options.collectives), num_threads, options.asynchronous,\n       std::move(options.customize_hlo_module_config),\n-      options.max_transpose_threads));\n+      options.max_transpose_threads, std::move(topology)));\n }\n \n // An upper bound on the number of threads to use for intra-op parallelism. It\n@@ -226,26 +263,13 @@ static tsl::ThreadOptions GetThreadOptions() {\n   return thread_options;\n }\n \n-// Returns the CPU devices from the given PjRtCpuDevices.\n-// Precondition: `devices` doesn't contain nullptr.\n-static std::vector<CpuTopology::CpuDevice> GetCpuDevices(\n-    absl::Span<const std::unique_ptr<PjRtCpuDevice>> devices) {\n-  std::vector<CpuTopology::CpuDevice> cpu_devices;\n-  cpu_devices.reserve(devices.size());\n-  for (const auto& device : devices) {\n-    cpu_devices.push_back(CpuTopology::CpuDevice{\n-        device->process_index(), device->local_hardware_id().value()});\n-  }\n-  return cpu_devices;\n-}\n-\n PjRtCpuClient::PjRtCpuClient(\n     int process_index, std::vector<std::unique_ptr<PjRtCpuDevice>> devices,\n     std::shared_ptr<CpuDeviceMemory::Allocator> allocator,\n     std::shared_ptr<cpu::CpuCollectives> collectives, size_t num_threads,\n     bool asynchronous,\n     std::function<void(HloModuleConfig&)> customize_hlo_module_config,\n-    int max_transpose_threads)\n+    int max_transpose_threads, std::unique_ptr<CpuTopologyDescription> topology)\n     : process_index_(process_index),\n       owned_devices_(std::move(devices)),\n       computation_placer_(std::make_unique<ComputationPlacer>()),\n@@ -254,12 +278,7 @@ PjRtCpuClient::PjRtCpuClient(\n           tsl::MakeAvailableAsyncValueRef<CpuEvent>()),\n       transpose_cache_(1024),\n       collectives_(std::move(collectives)),\n-      topology_(platform_id(), platform_name(), platform_version(),\n-                GetCpuDevices(owned_devices_),\n-                cpu::DetectMachineAttributes(\n-                    cpu::CpuFeatureFromString(\n-                        GetDebugOptionsFromFlags().xla_cpu_max_isa()))\n-                    .features),\n+      topology_(std::move(topology)),\n       asynchronous_(asynchronous),\n       customize_hlo_module_config_(std::move(customize_hlo_module_config)),\n       eigen_intraop_pool_(new tsl::thread::ThreadPool(\n@@ -606,7 +625,7 @@ static absl::StatusOr<std::unique_ptr<xla::Executable>> CompileAheadOfTime(\n \n absl::StatusOr<std::unique_ptr<PjRtLoadedExecutable>>\n PjRtCpuClient::CompileAndLoad(mlir::ModuleOp module, CompileOptions options) {\n-  TF_RETURN_IF_ERROR(pjrt::MaybeDumpCompileInputs(options, module, topology_));\n+  TF_RETURN_IF_ERROR(pjrt::MaybeDumpCompileInputs(options, module, *topology_));\n \n   XlaComputation xla_computation;\n   ExecutableBuildOptions& exec_build_options = options.executable_build_options;\n@@ -817,7 +836,7 @@ PjRtCpuClient::CompileInternal(\n     // Overwrite the features with the machine attributes from the topology.\n \n     TF_RETURN_IF_ERROR(target_machine_options.SetFeatures(\n-        absl::StrJoin(topology_.cpu_topology().machine_attributes(), \",\")));\n+        absl::StrJoin(topology_->cpu_topology().machine_attributes(), \",\")));\n \n     compile_options.cpu_target_config.emplace(target_machine_options);\n "
        },
        {
            "sha": "e2e29df765002a09f4769ed7163815b0f4a60c14",
            "filename": "third_party/xla/xla/pjrt/cpu/cpu_client.h",
            "status": "modified",
            "additions": 21,
            "deletions": 8,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/484c657f45104a146250384d4998314e77f48d30/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/484c657f45104a146250384d4998314e77f48d30/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_client.h?ref=484c657f45104a146250384d4998314e77f48d30",
            "patch": "@@ -78,6 +78,16 @@ limitations under the License.\n \n namespace xla {\n \n+namespace cpu {\n+\n+PjRtPlatformId PlatformId();\n+\n+absl::string_view PlatformName();\n+\n+absl::string_view PlatformVersion();\n+\n+}  // namespace cpu\n+\n class PjRtCpuClient final : public CommonPjRtClient {\n  public:\n   ~PjRtCpuClient() override;\n@@ -104,13 +114,15 @@ class PjRtCpuClient final : public CommonPjRtClient {\n \n   absl::Span<PjRtMemorySpace* const> memory_spaces() const override;\n \n-  PjRtPlatformId platform_id() const override {\n-    return tsl::Fingerprint64(CpuName());\n-  }\n+  PjRtPlatformId platform_id() const override { return cpu::PlatformId(); }\n \n-  absl::string_view platform_name() const override { return CpuName(); }\n+  absl::string_view platform_name() const override {\n+    return cpu::PlatformName();\n+  }\n \n-  absl::string_view platform_version() const override { return CpuName(); }\n+  absl::string_view platform_version() const override {\n+    return cpu::PlatformVersion();\n+  }\n \n   absl::StatusOr<DeviceAssignment> GetDefaultDeviceAssignment(\n       int num_replicas, int num_partitions) const override;\n@@ -201,7 +213,7 @@ class PjRtCpuClient final : public CommonPjRtClient {\n \n   absl::StatusOr<const xla::PjRtTopologyDescription*> GetTopologyDescription()\n       const override {\n-    return &topology_;\n+    return topology_.get();\n   }\n \n   absl::StatusOr<std::pair<tsl::RCReference<CommonPjRtRawBuffer>,\n@@ -261,7 +273,8 @@ class PjRtCpuClient final : public CommonPjRtClient {\n       std::shared_ptr<cpu::CpuCollectives> collectives, size_t num_threads,\n       bool asynchronous,\n       std::function<void(HloModuleConfig&)> customize_hlo_module_config,\n-      int max_transpose_threads);\n+      int max_transpose_threads,\n+      std::unique_ptr<CpuTopologyDescription> topology);\n \n   absl::StatusOr<std::unique_ptr<PjRtLoadedExecutable>> CompileInternal(\n       const XlaComputation& computation,\n@@ -314,7 +327,7 @@ class PjRtCpuClient final : public CommonPjRtClient {\n \n   std::shared_ptr<cpu::CpuCollectives> collectives_;\n \n-  xla::CpuTopologyDescription topology_;\n+  std::unique_ptr<xla::CpuTopologyDescription> topology_;\n \n   // Used to control whether asynchronous computation dispatch is available for\n   // this client. Only applies to non-parallel computations."
        },
        {
            "sha": "a6336c19a68c58f7b7a8dcf70f0a7018330d9ce8",
            "filename": "third_party/xla/xla/pjrt/cpu/cpu_pjrt_compiler.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/484c657f45104a146250384d4998314e77f48d30/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_pjrt_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/484c657f45104a146250384d4998314e77f48d30/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_pjrt_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_pjrt_compiler.cc?ref=484c657f45104a146250384d4998314e77f48d30",
            "patch": "@@ -34,9 +34,11 @@ limitations under the License.\n #include \"xla/pjrt/pjrt_compiler.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n #include \"xla/pjrt/plugin/xla_cpu/cpu_client_options.h\"\n+#include \"xla/pjrt/plugin/xla_cpu/cpu_topology_description.h\"\n #include \"xla/pjrt/plugin/xla_cpu/xla_cpu_pjrt_client.h\"\n #include \"xla/stream_executor/platform/initialize.h\"\n #include \"xla/tsl/platform/statusor.h\"\n+#include \"tsl/platform/casts.h\"\n \n namespace xla::cpu {\n \n@@ -57,21 +59,20 @@ class DummyCpuCollectives : public xla::cpu::CpuCollectives {\n \n // Creates a PjRt CPU client from the given topology description.\n //\n-// We only take the number of CPU devices from the topology and assume that the\n-// compilation worker and the worker running the compiled CPU computation are on\n-// the same CPU platform. This may not be true in the future in a highly\n-// disaggregated setup; then, we should extend the PjRt CPU client to\n-// support cross compilation for a non-local platform, and use machine\n-// attributes captured in a CPU topology description.\n absl::StatusOr<std::unique_ptr<xla::PjRtClient>>\n CreatePjRtCpuClientFromTopology(\n     const xla::PjRtTopologyDescription& topology_description) {\n-  // TODO(b/373647598): Use PjRtCompile() without creating a PjRt CPU client\n-  // once it is supported by XLA:CPU.\n   xla::CpuClientOptions options;\n   TF_ASSIGN_OR_RETURN(options.cpu_device_count,\n                       topology_description.CoreCountOfDefaultTypePerProcess());\n   CHECK_GE(*options.cpu_device_count, 1);\n+  auto cpu_topology_description =\n+      tsl::down_cast<const xla::CpuTopologyDescription*>(&topology_description);\n+  if (cpu_topology_description == nullptr) {\n+    return absl::InvalidArgumentError(\n+        \"Topology description is not a CpuTopologyDescription\");\n+  }\n+  options.topology = cpu_topology_description;\n   // We need to provide `CpuCollectives` to be able to compile multi-host/-slice\n   // CPU computations. The details of the collectives is not important because\n   // the compilation only checks if any `CpuCollectives` exists."
        },
        {
            "sha": "e8fd02e09bc7c8f6be4ef75d9d726d8cb0350b3d",
            "filename": "third_party/xla/xla/pjrt/cpu/cpu_pjrt_compiler_test.cc",
            "status": "modified",
            "additions": 71,
            "deletions": 13,
            "changes": 84,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/484c657f45104a146250384d4998314e77f48d30/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_pjrt_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/484c657f45104a146250384d4998314e77f48d30/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_pjrt_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcpu%2Fcpu_pjrt_compiler_test.cc?ref=484c657f45104a146250384d4998314e77f48d30",
            "patch": "@@ -15,17 +15,26 @@ limitations under the License.\n \n #include \"xla/pjrt/cpu/cpu_pjrt_compiler.h\"\n \n+#include <memory>\n+#include <string>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"absl/strings/string_view.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/Parser/Parser.h\"\n+#include \"xla/backends/cpu/codegen/cpu_features.h\"\n+#include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/builder/xla_computation.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/mlir_hlo/mhlo/IR/hlo_ops.h\"\n+#include \"xla/pjrt/cpu/cpu_client.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n-#include \"xla/pjrt/plugin/xla_cpu/xla_cpu_pjrt_client.h\"\n+#include \"xla/pjrt/plugin/xla_cpu/cpu_topology.h\"\n+#include \"xla/pjrt/plugin/xla_cpu/cpu_topology_description.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace xla::cpu {\n@@ -47,17 +56,29 @@ constexpr absl::string_view kMlirProgram = R\"mlir(\n \n using CpuPjrtCompilerTest = xla::HloHardwareIndependentTestBase;\n \n+std::unique_ptr<CpuTopologyDescription> GetDefaultCpuTopologyDescription() {\n+  constexpr int kCpuDeviceCount = 1;\n+  constexpr int kProcessId = 0;\n+  std::vector<CpuTopology::CpuDevice> cpu_topology_devices;\n+  cpu_topology_devices.reserve(kCpuDeviceCount);\n+  for (int i = 0; i < kCpuDeviceCount; ++i) {\n+    cpu_topology_devices.push_back(CpuTopology::CpuDevice{kProcessId, i});\n+  }\n+\n+  return std::make_unique<CpuTopologyDescription>(\n+      cpu::PlatformId(), cpu::PlatformName(), cpu::PlatformVersion(),\n+      cpu_topology_devices,\n+      DetectMachineAttributes(\n+          CpuFeatureFromString(GetDebugOptionsFromFlags().xla_cpu_max_isa()))\n+          .features);\n+}\n+\n TEST_F(CpuPjrtCompilerTest, CompileXlaComputationSuccess) {\n   xla::CompileOptions options;\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kProgram));\n   xla::XlaComputation computation(module->ToProto());\n \n-  // TODO(basioli): Temporary hack to get the cpu topology easily, will update\n-  // test once cross-compilation is supported.\n-  TF_ASSERT_OK_AND_ASSIGN(auto cpu_client, xla::GetXlaPjrtCpuClient({}));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto topology_description,\n-                          cpu_client->GetTopologyDescription());\n+  auto topology_description = GetDefaultCpuTopologyDescription();\n \n   xla::cpu::CpuPjRtCompiler compiler;\n   TF_ASSERT_OK_AND_ASSIGN(\n@@ -73,12 +94,7 @@ TEST_F(CpuPjrtCompilerTest, CompileMlirOpSuccess) {\n   auto mlir_module =\n       mlir::parseSourceString<mlir::ModuleOp>(kMlirProgram, &context);\n \n-  // TODO(basioli): Temporary hack to get the cpu topology easily, will update\n-  // test once cross-compilation is supported.\n-  TF_ASSERT_OK_AND_ASSIGN(auto cpu_client, xla::GetXlaPjrtCpuClient({}));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto topology_description,\n-                          cpu_client->GetTopologyDescription());\n+  auto topology_description = GetDefaultCpuTopologyDescription();\n \n   xla::cpu::CpuPjRtCompiler compiler;\n   TF_ASSERT_OK_AND_ASSIGN(\n@@ -87,5 +103,47 @@ TEST_F(CpuPjrtCompilerTest, CompileMlirOpSuccess) {\n                        /*client=*/nullptr));\n }\n \n+TEST_F(CpuPjrtCompilerTest, CompileXlaComputationWithAvx512FeatureOn) {\n+  xla::CompileOptions options;\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(kProgram));\n+  xla::XlaComputation computation(module->ToProto());\n+\n+  constexpr int kCpuDeviceCount = 1;\n+  constexpr int kProcessId = 0;\n+  std::vector<CpuTopology::CpuDevice> cpu_topology_devices;\n+  cpu_topology_devices.reserve(kCpuDeviceCount);\n+  for (int i = 0; i < kCpuDeviceCount; ++i) {\n+    cpu_topology_devices.push_back(CpuTopology::CpuDevice{kProcessId, i});\n+  }\n+\n+  // Set custom topology.\n+  auto topology_description = std::make_unique<CpuTopologyDescription>(\n+      PlatformId(), PlatformName(), PlatformVersion(), cpu_topology_devices,\n+      std::vector<std::string>{\"+avx512\"});\n+\n+  xla::cpu::CpuPjRtCompiler compiler;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto executable,\n+      compiler.Compile(options, computation, *topology_description,\n+                       /*client=*/nullptr));\n+\n+  // We serialize and then load the executable to confirm that the target\n+  // machine options were set correctly.\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto serialized_executable,\n+                          executable->SerializeExecutable());\n+\n+  ExecutableAndOptionsProto proto;\n+  EXPECT_TRUE(proto.ParseFromString(serialized_executable));\n+\n+  xla::cpu::CompilationResultProto compilation_result_proto;\n+\n+  EXPECT_TRUE(\n+      compilation_result_proto.ParseFromString(proto.serialized_executable()));\n+\n+  EXPECT_THAT(compilation_result_proto.target_machine_options().features(),\n+              testing::HasSubstr(\"+avx512\"));\n+}\n+\n }  // namespace\n }  // namespace xla::cpu"
        },
        {
            "sha": "ccdffc69bee103732856c00baaa977308789ffb4",
            "filename": "third_party/xla/xla/pjrt/plugin/xla_cpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/484c657f45104a146250384d4998314e77f48d30/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_cpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/484c657f45104a146250384d4998314e77f48d30/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_cpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_cpu%2FBUILD?ref=484c657f45104a146250384d4998314e77f48d30",
            "patch": "@@ -43,6 +43,7 @@ cc_library(\n     hdrs = [\"cpu_client_options.h\"],\n     deps = [\n         \":cpu_memory\",\n+        \":cpu_topology_description\",\n         \"//xla/backends/cpu/collectives:cpu_collectives\",\n         \"//xla/service:hlo_module_config\",\n         \"@com_google_absl//absl/status:statusor\","
        },
        {
            "sha": "3af357c6f8676361cf49a61648039f30862d7474",
            "filename": "third_party/xla/xla/pjrt/plugin/xla_cpu/cpu_client_options.h",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/484c657f45104a146250384d4998314e77f48d30/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_cpu%2Fcpu_client_options.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/484c657f45104a146250384d4998314e77f48d30/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_cpu%2Fcpu_client_options.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_cpu%2Fcpu_client_options.h?ref=484c657f45104a146250384d4998314e77f48d30",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"xla/backends/cpu/collectives/cpu_collectives.h\"\n #include \"xla/pjrt/plugin/xla_cpu/cpu_memory.h\"\n+#include \"xla/pjrt/plugin/xla_cpu/cpu_topology_description.h\"\n #include \"xla/service/hlo_module_config.h\"\n \n namespace xla {\n@@ -64,6 +65,11 @@ struct CpuClientOptions {\n   // Maximum number of threads to use for any one transpose. We will use the\n   // the lesser of this number and the thread pool size. 1 = no threading.\n   int max_transpose_threads = 8;\n+\n+  // CPU topology description. Optional. If not provided, the topology will be\n+  // detected from the host and will try to use cpu_device_count and process_id\n+  // to build the topology.\n+  const CpuTopologyDescription* topology = nullptr;\n };\n \n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 221,
        "additions": 163,
        "deletions": 58
    }
}