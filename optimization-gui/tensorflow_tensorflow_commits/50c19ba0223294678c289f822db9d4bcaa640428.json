{
    "author": "tensorflower-gardener",
    "message": "Apply llvm-use-new-mlir-op-builder fixes\n\nThis migrates `builder.create<Op>()` => `Op::create()`\n\nPiperOrigin-RevId: 846246070",
    "sha": "50c19ba0223294678c289f822db9d4bcaa640428",
    "files": [
        {
            "sha": "95c9934d4c7bc60f002c4e3da5fcd411fba3bde9",
            "filename": "third_party/xla/xla/mlir_hlo/deallocation/transforms/buffer_reuse.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fdeallocation%2Ftransforms%2Fbuffer_reuse.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fdeallocation%2Ftransforms%2Fbuffer_reuse.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fdeallocation%2Ftransforms%2Fbuffer_reuse.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -394,8 +394,8 @@ bool hoistAllocs(Block& block) {\n void promoteToStack(memref::DeallocOp dealloc) {\n   auto alloc = dealloc.getMemref().getDefiningOp<memref::AllocOp>();\n   OpBuilder b(alloc);\n-  auto alloca = b.create<memref::AllocaOp>(\n-      alloc->getLoc(), mlir::cast<MemRefType>(alloc->getResultTypes()[0]),\n+  auto alloca = memref::AllocaOp::create(\n+      b, alloc->getLoc(), mlir::cast<MemRefType>(alloc->getResultTypes()[0]),\n       alloc.getAlignmentAttr());\n   alloc->replaceAllUsesWith(ValueRange{alloca.getResult()});\n   alloc->erase();"
        },
        {
            "sha": "b470ad53c61a63a98313f13d476f11b0cc7314e4",
            "filename": "third_party/xla/xla/mlir_hlo/deallocation/utils/util.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fdeallocation%2Futils%2Futil.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fdeallocation%2Futils%2Futil.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fdeallocation%2Futils%2Futil.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -85,25 +85,25 @@ RegionBranchOpInterface moveRegionsToNewOpButKeepOldOp(\n   OpBuilder b(op);\n   RegionBranchOpInterface newOp;\n   if (llvm::isa<scf::ForOp>(op)) {\n-    newOp = b.create<scf::ForOp>(op.getLoc(), op->getOperands()[0],\n-                                 op->getOperands()[1], op->getOperands()[2],\n-                                 op->getOperands().drop_front(3));\n+    newOp = scf::ForOp::create(b, op.getLoc(), op->getOperands()[0],\n+                               op->getOperands()[1], op->getOperands()[2],\n+                               op->getOperands().drop_front(3));\n   } else if (llvm::isa<scf::WhileOp>(op)) {\n-    newOp = b.create<scf::WhileOp>(\n-        op.getLoc(),\n+    newOp = scf::WhileOp::create(\n+        b, op.getLoc(),\n         TypeRange{op->getRegion(0).front().getTerminator()->getOperands()}\n             .drop_front(),\n         op->getOperands());\n   } else if (llvm::isa<scf::IfOp>(op)) {\n-    newOp = b.create<scf::IfOp>(\n-        op.getLoc(),\n+    newOp = scf::IfOp::create(\n+        b, op.getLoc(),\n         TypeRange{op->getRegion(0).front().getTerminator()->getOperands()},\n         op->getOperands()[0], op->getNumRegions() > 1);\n   } else if (llvm::isa<scf::ParallelOp>(op)) {\n     auto parallel = llvm::cast<scf::ParallelOp>(op);\n-    newOp = b.create<scf::ParallelOp>(\n-        op.getLoc(), parallel.getLowerBound(), parallel.getUpperBound(),\n-        parallel.getStep(), parallel.getInitVals());\n+    newOp = scf::ParallelOp::create(b, op.getLoc(), parallel.getLowerBound(),\n+                                    parallel.getUpperBound(),\n+                                    parallel.getStep(), parallel.getInitVals());\n   } else {\n     llvm_unreachable(\"unsupported\");\n   }"
        },
        {
            "sha": "48d15aaafbe12c694e9035d51a055c58e905d033",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/IR/hlo_ops.cc",
            "status": "modified",
            "additions": 150,
            "deletions": 140,
            "changes": 290,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_ops.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -239,7 +239,7 @@ static void replaceOpWithRegion(PatternRewriter& rewriter, Operation* op,\n Value maybeCastTo(OpBuilder& b, Location loc, Value value, Type type) {\n   if (type == value.getType()) return value;\n   assert(type.isIndex() || value.getType().isIndex());\n-  return b.create<arith::IndexCastOp>(loc, type, value);\n+  return arith::IndexCastOp::create(b, loc, type, value);\n }\n \n DenseElementsAttr reshape(DenseElementsAttr attr, ShapedType newType) {\n@@ -941,26 +941,26 @@ LogicalResult DotGeneralOp::reifyReturnTypeShapes(\n   SmallVector<Value> dimensions;\n   for (const int64_t lhsDim : dimNumbers.getLhsBatchingDimensions()) {\n     dimensions.push_back(\n-        builder.create<tensor::DimOp>(getLoc(), adaptor.getLhs(), lhsDim));\n+        tensor::DimOp::create(builder, getLoc(), adaptor.getLhs(), lhsDim));\n   }\n \n   for (int64_t i = 0; i < lhsType.getRank(); i++) {\n     if (!llvm::is_contained(dimNumbers.getLhsContractingDimensions(), i) &&\n         !llvm::is_contained(dimNumbers.getLhsBatchingDimensions(), i)) {\n       dimensions.push_back(\n-          builder.create<tensor::DimOp>(getLoc(), adaptor.getLhs(), i));\n+          tensor::DimOp::create(builder, getLoc(), adaptor.getLhs(), i));\n     }\n   }\n   for (int64_t i = 0; i < rhsType.getRank(); i++) {\n     if (!llvm::is_contained(dimNumbers.getRhsContractingDimensions(), i) &&\n         !llvm::is_contained(dimNumbers.getRhsBatchingDimensions(), i)) {\n       dimensions.push_back(\n-          builder.create<tensor::DimOp>(getLoc(), adaptor.getRhs(), i));\n+          tensor::DimOp::create(builder, getLoc(), adaptor.getRhs(), i));\n     }\n   }\n \n   reifiedReturnShapes.push_back(\n-      builder.create<tensor::FromElementsOp>(getLoc(), dimensions));\n+      tensor::FromElementsOp::create(builder, getLoc(), dimensions));\n   return success();\n }\n \n@@ -1491,11 +1491,11 @@ struct GatherSlice : public OpRewritePattern<GatherOp> {\n     }\n     Type elementType = cast<TensorType>(gather.getType()).getElementType();\n     auto sliceType = RankedTensorType::get(sliceShape, elementType);\n-    Value result = rewriter.create<SliceOp>(\n-        gather.getLoc(), sliceType, gather.getOperand(),\n-        rewriter.getI64TensorAttr(sliceStart),\n-        rewriter.getI64TensorAttr(sliceEnd),\n-        rewriter.getI64TensorAttr(sliceStride));\n+    Value result = SliceOp::create(rewriter, gather.getLoc(), sliceType,\n+                                   gather.getOperand(),\n+                                   rewriter.getI64TensorAttr(sliceStart),\n+                                   rewriter.getI64TensorAttr(sliceEnd),\n+                                   rewriter.getI64TensorAttr(sliceStride));\n \n     auto collapsedSliceDims = dnums.getCollapsedSliceDims();\n     if (!collapsedSliceDims.empty()) {\n@@ -1506,7 +1506,8 @@ struct GatherSlice : public OpRewritePattern<GatherOp> {\n         }\n       }\n       auto reshapeType = RankedTensorType::get(reshapeShape, elementType);\n-      result = rewriter.create<ReshapeOp>(gather.getLoc(), reshapeType, result);\n+      result =\n+          ReshapeOp::create(rewriter, gather.getLoc(), reshapeType, result);\n     }\n \n     result.setType(gather.getType());\n@@ -1541,7 +1542,7 @@ void getSliceSizeValues(GatherOp* gather, OpBuilder& builder, Location loc,\n                         ValueRange operands,\n                         SmallVectorImpl<Value>& sliceSizes) {\n   for (int64_t val : gather->getSliceSizes().getValues<int64_t>()) {\n-    sliceSizes.push_back(builder.create<arith::ConstantIndexOp>(loc, val));\n+    sliceSizes.push_back(arith::ConstantIndexOp::create(builder, loc, val));\n   }\n }\n \n@@ -1552,9 +1553,9 @@ void getSliceSizeValues(DynamicGatherOp* /*dGather*/, OpBuilder& builder,\n   Value sliceSizes = adaptor.getSliceSizes();\n   auto sliceSizesTy = cast<ShapedType>(sliceSizes.getType());\n   for (int64_t i = 0; i < sliceSizesTy.getDimSize(0); ++i) {\n-    Value idx = builder.create<arith::ConstantIndexOp>(loc, i);\n+    Value idx = arith::ConstantIndexOp::create(builder, loc, i);\n     sliceSizeValues.push_back(\n-        builder.create<tensor::ExtractOp>(loc, sliceSizes, idx));\n+        tensor::ExtractOp::create(builder, loc, sliceSizes, idx));\n   }\n }\n \n@@ -1582,7 +1583,7 @@ LogicalResult reifyGatherShape(Op* op, OpBuilder& builder, ValueRange operands,\n \n   auto getStartIndicesDim = [&](int64_t index) {\n     return toShapeElType(\n-        builder.create<tensor::DimOp>(loc, startIndices, index));\n+        tensor::DimOp::create(builder, loc, startIndices, index));\n   };\n   SmallVector<Value, 4> shapeValues;\n   auto getSliceDim = [&sliceSizes](int64_t index) -> Value {\n@@ -1596,8 +1597,9 @@ LogicalResult reifyGatherShape(Op* op, OpBuilder& builder, ValueRange operands,\n                            op->getDimensionNumbers().getIndexVectorDim(),\n                            shapeValues);\n \n-  Value outputShape = builder.create<tensor::FromElementsOp>(\n-      loc, RankedTensorType::get({resultRank}, shapeElTy), shapeValues);\n+  Value outputShape = tensor::FromElementsOp::create(\n+      builder, loc, RankedTensorType::get({resultRank}, shapeElTy),\n+      shapeValues);\n   reifiedReturnShapes.push_back(outputShape);\n \n   return success();\n@@ -1742,8 +1744,8 @@ struct IotaBroadcast : public OpRewritePattern<IotaOp> {\n     auto iotaType = RankedTensorType::get({resultTy.getDimSize(iotaDimension)},\n                                           resultTy.getElementType());\n \n-    auto newIota = rewriter.create<IotaOp>(iota.getLoc(), iotaType,\n-                                           rewriter.getI64IntegerAttr(0));\n+    auto newIota = IotaOp::create(rewriter, iota.getLoc(), iotaType,\n+                                  rewriter.getI64IntegerAttr(0));\n \n     auto broadcastAttr = DenseIntElementsAttr::get(\n         RankedTensorType::get({1}, rewriter.getIntegerType(64)),\n@@ -1808,21 +1810,21 @@ struct DynamicIotaBroadcast : public OpRewritePattern<DynamicIotaOp> {\n     auto iotaDimension = iota.getIotaDimension();\n     auto iotaDimensionInt = iotaDimension;\n \n-    auto convertedShape = rewriter.create<arith::IndexCastOp>(\n-        iota.getLoc(),\n+    auto convertedShape = arith::IndexCastOp::create(\n+        rewriter, iota.getLoc(),\n         RankedTensorType::get(\n             cast<ShapedType>(iota.getOutputShape().getType()).getShape(),\n             rewriter.getI64Type()),\n         iota.getOutputShape());\n \n-    auto slicedShape = rewriter.create<SliceOp>(\n-        iota.getLoc(), convertedShape,\n-        rewriter.getI64TensorAttr(iotaDimensionInt),\n-        rewriter.getI64TensorAttr(iotaDimensionInt + 1),\n-        rewriter.getI64TensorAttr(1));\n+    auto slicedShape =\n+        SliceOp::create(rewriter, iota.getLoc(), convertedShape,\n+                        rewriter.getI64TensorAttr(iotaDimensionInt),\n+                        rewriter.getI64TensorAttr(iotaDimensionInt + 1),\n+                        rewriter.getI64TensorAttr(1));\n \n-    auto convertedSlicedShape = rewriter.create<arith::IndexCastOp>(\n-        iota.getLoc(),\n+    auto convertedSlicedShape = arith::IndexCastOp::create(\n+        rewriter, iota.getLoc(),\n         RankedTensorType::get(\n             {1},\n             cast<ShapedType>(iota.getOutputShape().getType()).getElementType()),\n@@ -1831,9 +1833,9 @@ struct DynamicIotaBroadcast : public OpRewritePattern<DynamicIotaOp> {\n     auto iotaType = RankedTensorType::get(\n         {resultTy.getDimSize(iotaDimensionInt)}, resultTy.getElementType());\n \n-    auto newIota = rewriter.create<DynamicIotaOp>(\n-        iota.getLoc(), iotaType, convertedSlicedShape,\n-        rewriter.getI64IntegerAttr(0));\n+    auto newIota = DynamicIotaOp::create(rewriter, iota.getLoc(), iotaType,\n+                                         convertedSlicedShape,\n+                                         rewriter.getI64IntegerAttr(0));\n \n     auto broadcastAttr = DenseIntElementsAttr::get(\n         RankedTensorType::get({1}, rewriter.getIntegerType(64)),\n@@ -1857,7 +1859,7 @@ static Value castToIndexTensor(OpBuilder& builder, Location loc,\n   ShapedType resultTy = shape::getExtentTensorType(\n       builder.getContext(), cast<ShapedType>(shapeOp.getType()).getDimSize(0));\n   if (shapeOp.getType() == resultTy) return shapeOp;  // Nothing to do.\n-  return builder.create<arith::IndexCastOp>(loc, resultTy, shapeOp);\n+  return arith::IndexCastOp::create(builder, loc, resultTy, shapeOp);\n }\n \n LogicalResult DynamicIotaOp::reifyReturnTypeShapes(\n@@ -2046,8 +2048,8 @@ struct ConvolutionIsDot : public OpRewritePattern<mhlo::ConvolutionOp> {\n \n       auto dotNums = DotDimensionNumbersAttr::get(\n           op.getContext(), {}, {}, {lhsContractDim}, {rhsContractDim});\n-      auto dotOp = rewriter.create<mhlo::DotGeneralOp>(\n-          op.getLoc(), op.getType(), lhs, rhs, dotNums,\n+      auto dotOp = mhlo::DotGeneralOp::create(\n+          rewriter, op.getLoc(), op.getType(), lhs, rhs, dotNums,\n           op.getPrecisionConfig().value_or(nullptr), DotAlgorithmAttr{});\n \n       rewriter.replaceOp(op, dotOp.getResult());\n@@ -2072,8 +2074,8 @@ struct ConvolutionIsDot : public OpRewritePattern<mhlo::ConvolutionOp> {\n     lhsTy = RankedTensorType::get(lhsShape, lhsTy.getElementType());\n     rhsTy = RankedTensorType::get(rhsShape, rhsTy.getElementType());\n \n-    lhs = rewriter.create<mhlo::ReshapeOp>(op.getLoc(), lhsTy, lhs);\n-    rhs = rewriter.create<mhlo::ReshapeOp>(op.getLoc(), rhsTy, rhs);\n+    lhs = mhlo::ReshapeOp::create(rewriter, op.getLoc(), lhsTy, lhs);\n+    rhs = mhlo::ReshapeOp::create(rewriter, op.getLoc(), rhsTy, rhs);\n \n     auto dotTy = RankedTensorType::get(\n         {featureGroupCount, lhsBatchSize, rhsBatchSize / featureGroupCount},\n@@ -2082,8 +2084,8 @@ struct ConvolutionIsDot : public OpRewritePattern<mhlo::ConvolutionOp> {\n     auto dotNums = DotDimensionNumbersAttr::get(\n         op.getContext(), {lhsContractDim}, {rhsContractDim},\n         {lhsContractDim + 1}, {rhsContractDim == 0 ? 2 : 0});\n-    auto dotOp = rewriter.create<mhlo::DotGeneralOp>(\n-        op.getLoc(), dotTy, lhs, rhs, dotNums,\n+    auto dotOp = mhlo::DotGeneralOp::create(\n+        rewriter, op.getLoc(), dotTy, lhs, rhs, dotNums,\n         op.getPrecisionConfig().value_or(nullptr), DotAlgorithmAttr{});\n \n     llvm::SmallVector<int64_t> perms;\n@@ -2095,8 +2097,9 @@ struct ConvolutionIsDot : public OpRewritePattern<mhlo::ConvolutionOp> {\n         {dotTy.getDimSize(perms[0]), dotTy.getDimSize(perms[1]),\n          dotTy.getDimSize(perms[2])},\n         dotTy.getElementType());\n-    auto transposeOp = rewriter.create<mhlo::TransposeOp>(\n-        op.getLoc(), transposeTy, dotOp, rewriter.getI64TensorAttr(perms));\n+    auto transposeOp =\n+        mhlo::TransposeOp::create(rewriter, op.getLoc(), transposeTy, dotOp,\n+                                  rewriter.getI64TensorAttr(perms));\n \n     rewriter.replaceOpWithNewOp<mhlo::ReshapeOp>(op, resultTy, transposeOp);\n     return success();\n@@ -2290,8 +2293,8 @@ struct EliminateRedundantConvert : public OpRewritePattern<ConvertOp> {\n       // like fp16 -> fp32 -> fp64, bf16 -> fp32 -> fp16\n       if (cast<FloatType>(secondType).getWidth() >\n           cast<FloatType>(firstType).getWidth()) {\n-        Value result = rewriter.create<ConvertOp>(loc, op.getResult().getType(),\n-                                                  convertOp.getOperand());\n+        Value result = ConvertOp::create(\n+            rewriter, loc, op.getResult().getType(), convertOp.getOperand());\n         rewriter.replaceOp(op, result);\n         return success();\n       }\n@@ -2301,8 +2304,8 @@ struct EliminateRedundantConvert : public OpRewritePattern<ConvertOp> {\n       // like i16 -> i32 -> i64, u16 -> i32 -> u32\n       if (cast<IntegerType>(secondType).getWidth() >\n           cast<IntegerType>(firstType).getWidth()) {\n-        Value result = rewriter.create<ConvertOp>(loc, op.getResult().getType(),\n-                                                  convertOp.getOperand());\n+        Value result = ConvertOp::create(\n+            rewriter, loc, op.getResult().getType(), convertOp.getOperand());\n         rewriter.replaceOp(op, result);\n         return success();\n       }\n@@ -2702,7 +2705,7 @@ LogicalResult BroadcastOp::reifyReturnTypeShapes(\n   // Collect the broadcast sizes.\n   for (const auto& size : getBroadcastSizes()) {\n     shapeValues.push_back(\n-        builder.create<arith::ConstantIndexOp>(loc, size.getZExtValue()));\n+        arith::ConstantIndexOp::create(builder, loc, size.getZExtValue()));\n   }\n \n   // Collect the operand sizes.\n@@ -2711,8 +2714,8 @@ LogicalResult BroadcastOp::reifyReturnTypeShapes(\n         builder.createOrFold<tensor::DimOp>(loc, operand, index));\n   }\n \n-  reifiedReturnShapes.push_back(builder.create<tensor::FromElementsOp>(\n-      loc,\n+  reifiedReturnShapes.push_back(tensor::FromElementsOp::create(\n+      builder, loc,\n       RankedTensorType::get({static_cast<int64_t>(shapeValues.size())},\n                             builder.getIndexType()),\n       shapeValues));\n@@ -2874,7 +2877,8 @@ namespace {\n template <typename OpTy, typename... Args>\n OpTy refineOpWithNewOp(PatternRewriter& rewriter, Operation* op,\n                        Args&&... args) {\n-  auto newOp = rewriter.create<OpTy>(op->getLoc(), std::forward<Args>(args)...);\n+  auto newOp =\n+      OpTy::create(rewriter, op->getLoc(), std::forward<Args>(args)...);\n \n   llvm::SmallVector<Value> replacementResults;\n   assert(op->getNumResults() == newOp->getNumResults() &&\n@@ -2885,8 +2889,8 @@ OpTy refineOpWithNewOp(PatternRewriter& rewriter, Operation* op,\n     if (llvm::any_of(opResult.getUsers(), [&](Operation* user) {\n           return user->getDialect() != op->getDialect();\n         })) {\n-      replacementResult = rewriter.create<tensor::CastOp>(\n-          op->getLoc(), opResult.getType(), newOpResult);\n+      replacementResult = tensor::CastOp::create(\n+          rewriter, op->getLoc(), opResult.getType(), newOpResult);\n     }\n     replacementResults.push_back(replacementResult);\n   }\n@@ -3274,7 +3278,7 @@ LogicalResult ConcatenateOp::reifyReturnTypeShapes(\n     SmallVector<Value, 4> shapeVals;\n     for (const auto& element : llvm::enumerate(operandType.getShape())) {\n       Value valueDim = toShapeScalarType(\n-          builder.create<tensor::DimOp>(loc, operand, element.index()));\n+          tensor::DimOp::create(builder, loc, operand, element.index()));\n       shapeVals.push_back(valueDim);\n     }\n     allShapeValues.emplace_back(std::move(shapeVals));\n@@ -3289,12 +3293,12 @@ LogicalResult ConcatenateOp::reifyReturnTypeShapes(\n           << \"Concatenate expects all operands must be of the same rank\";\n       return failure();\n     }\n-    shapeValues[axis] = builder.create<arith::AddIOp>(loc, shapeValues[axis],\n-                                                      otherShapeValues[axis]);\n+    shapeValues[axis] = arith::AddIOp::create(builder, loc, shapeValues[axis],\n+                                              otherShapeValues[axis]);\n   }\n \n-  Value outputShape = builder.create<tensor::FromElementsOp>(\n-      loc,\n+  Value outputShape = tensor::FromElementsOp::create(\n+      builder, loc,\n       RankedTensorType::get({static_cast<int64_t>(shapeValues.size())},\n                             shapeScalarType),\n       shapeValues);\n@@ -3489,8 +3493,8 @@ struct DynamicSliceToSlice : public OpRewritePattern<DynamicSliceOp> {\n         sliceStartIndices, dynamicSlice.getSliceSizes(), &rewriter);\n     DenseIntElementsAttr sliceStrides =\n         rewriter.getI64TensorAttr(SmallVector<int64_t, 4>(inputRank, 1));\n-    auto result = rewriter.create<SliceOp>(loc, input, sliceStartIndices,\n-                                           sliceLimits, sliceStrides);\n+    auto result = SliceOp::create(rewriter, loc, input, sliceStartIndices,\n+                                  sliceLimits, sliceStrides);\n     rewriter.replaceOp(dynamicSlice, result);\n     return success();\n   }\n@@ -3568,14 +3572,15 @@ struct RealDSliceToDSlice : public OpRewritePattern<RealDynamicSliceOp> {\n     // Adapt accordingly in order to be compatible with DynamicSliceOp.\n     SmallVector<Value> startIndices;\n     for (auto i = 0; i < static_cast<int64_t>(sliceSizes.size()); ++i) {\n-      auto startIndex1D = rewriter.create<SliceOp>(\n-          op.getLoc(), op.getStartIndices(), rewriter.getI64TensorAttr(i),\n-          rewriter.getI64TensorAttr(i + 1), rewriter.getI64TensorAttr(1));\n+      auto startIndex1D = SliceOp::create(\n+          rewriter, op.getLoc(), op.getStartIndices(),\n+          rewriter.getI64TensorAttr(i), rewriter.getI64TensorAttr(i + 1),\n+          rewriter.getI64TensorAttr(1));\n       auto startIndex0DType = RankedTensorType::get(\n           {},\n           cast<ShapedType>(op.getStartIndices().getType()).getElementType());\n-      auto startIndex0D = rewriter.create<ReshapeOp>(\n-          op.getLoc(), startIndex0DType, startIndex1D);\n+      auto startIndex0D = ReshapeOp::create(rewriter, op.getLoc(),\n+                                            startIndex0DType, startIndex1D);\n       startIndices.push_back(startIndex0D);\n     }\n \n@@ -3610,29 +3615,31 @@ LogicalResult RealDynamicSliceOp::reifyReturnTypeShapes(\n   shapeValues.reserve(operandType.getRank());\n   Type shapeScalarType =\n       cast<ShapedType>(startIndices.getType()).getElementType();\n-  Value one = builder.create<arith::ConstantIndexOp>(loc, 1);\n+  Value one = arith::ConstantIndexOp::create(builder, loc, 1);\n   one = maybeCastTo(builder, loc, one, shapeScalarType);\n   for (const auto& element : llvm::enumerate(operandType.getShape())) {\n-    Value offset = builder.create<arith::ConstantIndexOp>(loc, element.index());\n+    Value offset =\n+        arith::ConstantIndexOp::create(builder, loc, element.index());\n     Value valueStart =\n-        builder.create<tensor::ExtractOp>(loc, startIndices, offset);\n+        tensor::ExtractOp::create(builder, loc, startIndices, offset);\n     Value valueLimit =\n-        builder.create<tensor::ExtractOp>(loc, limitIndices, offset);\n-    Value valueStride = builder.create<tensor::ExtractOp>(loc, strides, offset);\n+        tensor::ExtractOp::create(builder, loc, limitIndices, offset);\n+    Value valueStride =\n+        tensor::ExtractOp::create(builder, loc, strides, offset);\n     // size = (limit - start + stride - 1) / stride\n-    shapeValues.push_back(builder.create<arith::DivSIOp>(\n-        loc,\n-        builder.create<arith::SubIOp>(\n-            loc,\n-            builder.create<arith::AddIOp>(\n-                loc, valueStride,\n-                builder.create<arith::SubIOp>(loc, valueLimit, valueStart)),\n+    shapeValues.push_back(arith::DivSIOp::create(\n+        builder, loc,\n+        arith::SubIOp::create(\n+            builder, loc,\n+            arith::AddIOp::create(\n+                builder, loc, valueStride,\n+                arith::SubIOp::create(builder, loc, valueLimit, valueStart)),\n             one),\n         valueStride));\n   }\n \n-  reifiedReturnShapes.push_back(builder.create<tensor::FromElementsOp>(\n-      loc,\n+  reifiedReturnShapes.push_back(tensor::FromElementsOp::create(\n+      builder, loc,\n       RankedTensorType::get({static_cast<int64_t>(shapeValues.size())},\n                             shapeScalarType),\n       shapeValues));\n@@ -4208,8 +4215,8 @@ struct LowerBoolSplatConstantsIntoRegion : public OpRewritePattern<ReduceOp> {\n     // Create new splat constants to replace block arguments.\n     for (BlockArgument barg : bb.getArguments()) {\n       int argIdx = barg.getArgNumber();\n-      mhlo::ConstantOp newCst = rewriter.create<mhlo::ConstantOp>(\n-          bb.front().getLoc(), barg.getType(), bargCstAttrs[argIdx]);\n+      mhlo::ConstantOp newCst = mhlo::ConstantOp::create(\n+          rewriter, bb.front().getLoc(), barg.getType(), bargCstAttrs[argIdx]);\n       barg.replaceAllUsesWith(newCst);\n     }\n     return success();\n@@ -4230,8 +4237,8 @@ static LogicalResult convertEmptyReduces(ReduceOp op,\n     auto empty = rewriter.getI64TensorAttr({});\n     if (t.hasStaticShape()) {\n       for (auto [init, out] : llvm::zip(op.getInitValues(), op.getResults())) {\n-        out.replaceAllUsesWith(rewriter.create<BroadcastInDimOp>(\n-            op.getLoc(), out.getType(), init, empty));\n+        out.replaceAllUsesWith(BroadcastInDimOp::create(\n+            rewriter, op.getLoc(), out.getType(), init, empty));\n       }\n       return success();\n     }\n@@ -4241,8 +4248,8 @@ static LogicalResult convertEmptyReduces(ReduceOp op,\n       return failure();\n     for (auto [init, shape, out] :\n          llvm::zip(op.getInitValues(), shapes, op.getResults())) {\n-      out.replaceAllUsesWith(rewriter.create<DynamicBroadcastInDimOp>(\n-          op.getLoc(), out.getType(), init, shape, empty));\n+      out.replaceAllUsesWith(DynamicBroadcastInDimOp::create(\n+          rewriter, op.getLoc(), out.getType(), init, shape, empty));\n     }\n     return success();\n   }\n@@ -4282,12 +4289,12 @@ LogicalResult ReduceOp::reifyReturnTypeShapes(\n       continue;\n     }\n     Value valueDim = toShapeScalarType(\n-        builder.create<tensor::DimOp>(loc, inputs[0], element.index()));\n+        tensor::DimOp::create(builder, loc, inputs[0], element.index()));\n     shapeValues.push_back(valueDim);\n   }\n \n-  Value outputShape = builder.create<tensor::FromElementsOp>(\n-      loc,\n+  Value outputShape = tensor::FromElementsOp::create(\n+      builder, loc,\n       RankedTensorType::get({static_cast<int64_t>(shapeValues.size())},\n                             shapeScalarType),\n       shapeValues);\n@@ -4614,36 +4621,37 @@ LogicalResult PadOp::reifyReturnTypeShapes(\n   for (const APInt& val : padInteriorAttr.getValues<APInt>())\n     padInterior.push_back(val.getSExtValue());\n \n-  Value one = builder.create<arith::ConstantIndexOp>(loc, 1).getResult();\n-  Value zero = builder.create<arith::ConstantIndexOp>(loc, 0).getResult();\n+  Value one = arith::ConstantIndexOp::create(builder, loc, 1).getResult();\n+  Value zero = arith::ConstantIndexOp::create(builder, loc, 0).getResult();\n \n   llvm::SmallVector<Value> dimensions;\n   dimensions.reserve(operandTy.getRank());\n   for (int i = 0, s = operandTy.getRank(); i < s; ++i) {\n     Value padEdge =\n-        builder.create<arith::ConstantIndexOp>(loc, padHigh[i] + padLow[i]);\n+        arith::ConstantIndexOp::create(builder, loc, padHigh[i] + padLow[i]);\n \n     // First we grab the initial interior size.\n-    Value dim = builder.create<tensor::DimOp>(loc, operand, i).getResult();\n+    Value dim = tensor::DimOp::create(builder, loc, operand, i).getResult();\n \n     // Compute the interior of the tensor and determine padding size.\n     if (padInterior[i] > 0) {\n       Value padInter =\n-          builder.create<arith::ConstantIndexOp>(loc, padInterior[i])\n+          arith::ConstantIndexOp::create(builder, loc, padInterior[i])\n               .getResult();\n-      Value interior = builder.create<arith::SubIOp>(loc, dim, one).getResult();\n-      interior = builder.create<arith::MaxSIOp>(loc, interior, zero);\n-      interior = builder.create<arith::MulIOp>(loc, interior, padInter);\n-      dim = builder.create<arith::AddIOp>(loc, dim, interior).getResult();\n+      Value interior =\n+          arith::SubIOp::create(builder, loc, dim, one).getResult();\n+      interior = arith::MaxSIOp::create(builder, loc, interior, zero);\n+      interior = arith::MulIOp::create(builder, loc, interior, padInter);\n+      dim = arith::AddIOp::create(builder, loc, dim, interior).getResult();\n     }\n \n     // Then we add the padding on the edge of the tensor.\n-    dim = builder.create<arith::AddIOp>(loc, dim, padEdge).getResult();\n+    dim = arith::AddIOp::create(builder, loc, dim, padEdge).getResult();\n     dimensions.push_back(dim);\n   }\n \n   Value dimensionTensor =\n-      builder.create<tensor::FromElementsOp>(loc, dimensions).getResult();\n+      tensor::FromElementsOp::create(builder, loc, dimensions).getResult();\n   reifiedReturnShapes.push_back(dimensionTensor);\n   return success();\n }\n@@ -4767,38 +4775,40 @@ LogicalResult DynamicPadOp::reifyReturnTypeShapes(\n   };\n \n   Value zero =\n-      toShapeScalarType(builder.create<arith::ConstantIndexOp>(loc, 0));\n-  Value one = toShapeScalarType(builder.create<arith::ConstantIndexOp>(loc, 1));\n+      toShapeScalarType(arith::ConstantIndexOp::create(builder, loc, 0));\n+  Value one =\n+      toShapeScalarType(arith::ConstantIndexOp::create(builder, loc, 1));\n \n   for (int idx : llvm::seq<int>(0, operandType.getShape().size())) {\n     Value valueDim =\n-        toShapeScalarType(builder.create<tensor::DimOp>(loc, operand, idx));\n-    Value offset = builder.create<arith::ConstantIndexOp>(loc, idx);\n+        toShapeScalarType(tensor::DimOp::create(builder, loc, operand, idx));\n+    Value offset = arith::ConstantIndexOp::create(builder, loc, idx);\n     Value valueLow =\n-        builder.create<tensor::ExtractOp>(loc, edgePaddingLow, offset);\n+        tensor::ExtractOp::create(builder, loc, edgePaddingLow, offset);\n     Value valueHigh =\n-        builder.create<tensor::ExtractOp>(loc, edgePaddingHigh, offset);\n+        tensor::ExtractOp::create(builder, loc, edgePaddingHigh, offset);\n     Value valueInterior =\n-        builder.create<tensor::ExtractOp>(loc, interiorPadding, offset);\n+        tensor::ExtractOp::create(builder, loc, interiorPadding, offset);\n     // output_size = input_size + padding_low + padding_high + interior *\n     // max(input_size - 1, 0)\n-    Value valueDimLessThanOne = builder.create<arith::CmpIOp>(\n-        loc, arith::CmpIPredicate::slt, valueDim, one);\n-    Value interiorSize = builder.create<arith::MulIOp>(\n-        loc, valueInterior,\n-        builder.create<mlir::arith::SelectOp>(\n-            loc, valueDimLessThanOne, zero,\n-            builder.create<arith::SubIOp>(loc, valueDim, one)));\n-    shapeValues.push_back(builder.create<arith::AddIOp>(\n-        loc,\n-        builder.create<arith::AddIOp>(\n-            loc, builder.create<arith::AddIOp>(loc, interiorSize, valueDim),\n+    Value valueDimLessThanOne = arith::CmpIOp::create(\n+        builder, loc, arith::CmpIPredicate::slt, valueDim, one);\n+    Value interiorSize = arith::MulIOp::create(\n+        builder, loc, valueInterior,\n+        mlir::arith::SelectOp::create(\n+            builder, loc, valueDimLessThanOne, zero,\n+            arith::SubIOp::create(builder, loc, valueDim, one)));\n+    shapeValues.push_back(arith::AddIOp::create(\n+        builder, loc,\n+        arith::AddIOp::create(\n+            builder, loc,\n+            arith::AddIOp::create(builder, loc, interiorSize, valueDim),\n             valueLow),\n         valueHigh));\n   }\n \n-  reifiedReturnShapes.push_back(builder.create<tensor::FromElementsOp>(\n-      loc,\n+  reifiedReturnShapes.push_back(tensor::FromElementsOp::create(\n+      builder, loc,\n       RankedTensorType::get({static_cast<int64_t>(shapeValues.size())},\n                             shapeScalarType),\n       shapeValues));\n@@ -5684,19 +5694,19 @@ struct SimplifyConcatSlice : public OpRewritePattern<SliceOp> {\n     }\n \n     auto concatRange = OperandRange(subsetStart, subsetEnd);\n-    auto newConcat = rewriter.create<ConcatenateOp>(\n-        concat.getLoc(), concatRange, concat.getDimension());\n+    auto newConcat = ConcatenateOp::create(rewriter, concat.getLoc(),\n+                                           concatRange, concat.getDimension());\n \n     llvm::SmallVector<APInt, 6> newStart(start);\n     llvm::SmallVector<APInt, 6> newLimit(limit);\n     newStart[dimension] -= frontOffset;\n     newLimit[dimension] -= frontOffset;\n \n     auto attrType = cast<ShapedType>(slice.getStartIndices().getType());\n-    auto create = rewriter.create<SliceOp>(\n-        slice.getLoc(), newConcat,\n-        DenseIntElementsAttr::get(attrType, newStart),\n-        DenseIntElementsAttr::get(attrType, newLimit), slice.getStrides());\n+    auto create = SliceOp::create(rewriter, slice.getLoc(), newConcat,\n+                                  DenseIntElementsAttr::get(attrType, newStart),\n+                                  DenseIntElementsAttr::get(attrType, newLimit),\n+                                  slice.getStrides());\n     rewriter.replaceOp(slice, create.getResult());\n     return success();\n   }\n@@ -5763,8 +5773,8 @@ static LogicalResult sortDropEmptyUseArgs(SortOp op,\n     }\n   }\n \n-  auto newOp = rewriter.create<SortOp>(op.getLoc(), newOperands,\n-                                       op.getDimension(), op.getIsStable());\n+  auto newOp = SortOp::create(rewriter, op.getLoc(), newOperands,\n+                              op.getDimension(), op.getIsStable());\n   Region& region = newOp.getComparator();\n   rewriter.inlineRegionBefore(op.getComparator(), region, region.end());\n   region.front().eraseArguments(erasedBlockArgs);\n@@ -5795,9 +5805,8 @@ static LogicalResult sortOpInferDefaultDimension(SortOp op,\n   }\n \n   IntegerAttr dim = rewriter.getI64IntegerAttr(ty.getRank() - 1);\n-  auto newOp =\n-      rewriter.create<SortOp>(op.getLoc(), op.getResultTypes(), op.getInputs(),\n-                              dim, op.getIsStableAttr());\n+  auto newOp = SortOp::create(rewriter, op.getLoc(), op.getResultTypes(),\n+                              op.getInputs(), dim, op.getIsStableAttr());\n   Region& region = newOp.getComparator();\n   rewriter.inlineRegionBefore(op.getComparator(), region, region.end());\n   rewriter.replaceOp(op, newOp.getResults());\n@@ -5964,8 +5973,8 @@ LogicalResult TransposeOp::reifyReturnTypeShapes(\n     shapeValues[std::distance(permutation.begin(), it)] = valueDim;\n   }\n \n-  Value outputShape = builder.create<tensor::FromElementsOp>(\n-      loc,\n+  Value outputShape = tensor::FromElementsOp::create(\n+      builder, loc,\n       RankedTensorType::get({static_cast<int64_t>(shapeValues.size())},\n                             shapeScalarType),\n       shapeValues);\n@@ -6432,8 +6441,8 @@ struct ScatterFullReplace : public OpRewritePattern<ScatterOp> {\n \n     auto dimensions =\n         llvm::to_vector(llvm::seq<int64_t>(0, baseType.getRank()));\n-    auto map = rewriter.create<mhlo::MapOp>(\n-        scatter.getLoc(), scatter->getResultTypes(),\n+    auto map = mhlo::MapOp::create(\n+        rewriter, scatter.getLoc(), scatter->getResultTypes(),\n         ValueRange{scatter.getOperands()[0], scatter.getUpdates()[0]},\n         rewriter.getI64TensorAttr(dimensions));\n     rewriter.inlineRegionBefore(scatter.getRegion(), map.getRegion(),\n@@ -6536,9 +6545,9 @@ static LogicalResult whileCanonicalization(WhileOp whileOp,\n   for (int idx : llvm::reverse(invariantArgIdxs))\n     bodyReturnOp->eraseOperand(idx);\n \n-  WhileOp newWhileOp = rewriter.create<WhileOp>(\n-      whileOp.getLoc(), bodyReturnOp->getOperandTypes(), newOperands,\n-      whileOp->getAttrs());\n+  WhileOp newWhileOp = WhileOp::create(rewriter, whileOp.getLoc(),\n+                                       bodyReturnOp->getOperandTypes(),\n+                                       newOperands, whileOp->getAttrs());\n   newWhileOp.getBodyRegion(0).takeBody(whileOp.getBodyRegion(0));\n   newWhileOp.getBodyRegion(1).takeBody(whileOp.getBodyRegion(1));\n   for (auto results : llvm::zip(resultsToReplace, newWhileOp->getResults()))\n@@ -7546,10 +7555,11 @@ static void buildSortComparisonBody(llvm::ArrayRef<Type> elementTypes,\n     typeAttr = symbolizeComparisonType(*compareType).value();\n   else\n     typeAttr = ComparisonType::NOTYPE;\n-  Value compare = builder->create<mhlo::CompareOp>(\n-      loc, block->getArgument(0), block->getArgument(1), direction, typeAttr);\n+  Value compare =\n+      mhlo::CompareOp::create(*builder, loc, block->getArgument(0),\n+                              block->getArgument(1), direction, typeAttr);\n \n-  builder->create<mhlo::ReturnOp>(loc, compare);\n+  mhlo::ReturnOp::create(*builder, loc, compare);\n }\n \n SortOp createSortOp(PatternRewriter* rewriter, const Location& loc,\n@@ -7559,7 +7569,7 @@ SortOp createSortOp(PatternRewriter* rewriter, const Location& loc,\n   assert(!operands.empty() && \"No operands to sort\");\n   // Create the sort op.\n   auto sortOp =\n-      rewriter->create<mhlo::SortOp>(loc, operands, dimension, isStable);\n+      mhlo::SortOp::create(*rewriter, loc, operands, dimension, isStable);\n \n   // Use TOTALORDER comparison type instead of the default comparison if the\n   // element type is of type float.\n@@ -7595,13 +7605,13 @@ Operation* MhloDialect::materializeConstant(OpBuilder& builder, Attribute value,\n           (attrShapedType.getShape() != resultShapedType.getShape()))\n         return nullptr;\n     }\n-    return builder.create<mhlo::ConstantOp>(loc, type, elementsAttr);\n+    return mhlo::ConstantOp::create(builder, loc, type, elementsAttr);\n   }\n   // HLO dialect constants require the type of value and result to match for\n   // non-quantized tensors.\n   if (type != elementsAttr.getType()) return nullptr;\n \n-  return builder.create<mhlo::ConstantOp>(loc, type, elementsAttr);\n+  return mhlo::ConstantOp::create(builder, loc, type, elementsAttr);\n }\n \n static int64_t getNumLeafBuffers(Type type) {"
        },
        {
            "sha": "561d072ca3a6729c823446879a960cfc03727c27",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/expand_hlo_tuples/expand_hlo_tuples.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fexpand_hlo_tuples%2Fexpand_hlo_tuples.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fexpand_hlo_tuples%2Fexpand_hlo_tuples.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fexpand_hlo_tuples%2Fexpand_hlo_tuples.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -93,7 +93,7 @@ class ExpandHloTuplesPass\n         OpBuilder builder(func.getBody());\n         builder.setInsertionPointToStart(&func.getBody().front());\n         auto newTuple =\n-            builder.create<mhlo::TupleOp>(loc, tupleType, flattenedOperands);\n+            mhlo::TupleOp::create(builder, loc, tupleType, flattenedOperands);\n         func.getArgument(originalArgumentIndex).replaceAllUsesWith(newTuple);\n \n         // Now the original argument has been rewired, we should be able to\n@@ -129,8 +129,8 @@ class ExpandHloTuplesPass\n       return success();\n     }\n \n-    builder.create<mlir::func::ReturnOp>(returnOp.getLoc(),\n-                                         expandedReturnOperands);\n+    mlir::func::ReturnOp::create(builder, returnOp.getLoc(),\n+                                 expandedReturnOperands);\n     returnOp.erase();\n     auto newFuncType = FunctionType::get(\n         oldFuncType.getContext(), expandedInputTypes, expandedResultTypes);"
        },
        {
            "sha": "88d1c06712800cbeb1b57153414676453d44fc5f",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/hlo_legalize_to_memref/hlo_legalize_to_memref.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_memref%2Fhlo_legalize_to_memref.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_memref%2Fhlo_legalize_to_memref.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_memref%2Fhlo_legalize_to_memref.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -141,8 +141,8 @@ struct DynamicReshapeOpInterface\n       if (failed(tensorAlloc)) return failure();\n       auto memrefType =\n           MemRefType::get(bufferType.getShape(), bufferType.getElementType());\n-      operand = rewriter.create<bufferization::ToBufferOp>(\n-          op->getLoc(), memrefType, *tensorAlloc);\n+      operand = bufferization::ToBufferOp::create(rewriter, op->getLoc(),\n+                                                  memrefType, *tensorAlloc);\n     }\n     bufferization::replaceOpWithNewBufferizedOp<memref::ReshapeOp>(\n         rewriter, op, resultType, operand, *outputShapeBuffer);\n@@ -165,8 +165,8 @@ FailureOr<Value> insertDynamicMemrefCastOp(\n   auto resultType = mlir::cast<RankedTensorType>(op.getType());\n   auto resultRank = resultType.getRank();\n \n-  Value zero = rewriter.create<arith::ConstantIndexOp>(loc, 0);\n-  Value one = rewriter.create<arith::ConstantIndexOp>(loc, 1);\n+  Value zero = arith::ConstantIndexOp::create(rewriter, loc, 0);\n+  Value one = arith::ConstantIndexOp::create(rewriter, loc, 1);\n \n   // Compute a reversed scan product. Compute the stride for the dimensions so\n   // far, working from minor to major dimensions. Additionally, save the\n@@ -177,15 +177,15 @@ FailureOr<Value> insertDynamicMemrefCastOp(\n   for (int i = operandRank - 1; i >= 0; --i) {\n     Value operandDimSize =\n         ShapedType::isDynamic(operandShape[i])\n-            ? rewriter.create<memref::DimOp>(loc, operand, i).getResult()\n-            : rewriter.create<arith::ConstantIndexOp>(loc, operandShape[i])\n+            ? memref::DimOp::create(rewriter, loc, operand, i).getResult()\n+            : arith::ConstantIndexOp::create(rewriter, loc, operandShape[i])\n                   .getResult();\n     operandSizes[i] = operandDimSize;\n \n     operandStrides[i] = strideSoFar;\n     if (i > 0) {\n       strideSoFar =\n-          rewriter.create<arith::MulIOp>(loc, strideSoFar, operandDimSize);\n+          arith::MulIOp::create(rewriter, loc, strideSoFar, operandDimSize);\n     }\n   }\n \n@@ -198,15 +198,15 @@ FailureOr<Value> insertDynamicMemrefCastOp(\n     outputToInputDim[dim.value().getSExtValue()] = dim.index();\n   }\n   for (int i = 0; i < resultRank; ++i) {\n-    Value iVal = rewriter.create<arith::ConstantIndexOp>(loc, i);\n+    Value iVal = arith::ConstantIndexOp::create(rewriter, loc, i);\n     FailureOr<Value> outputDimsBuffer =\n         getBuffer(rewriter, op.getOutputDimensions(), options, state);\n     if (failed(outputDimsBuffer)) return failure();\n     Value resultDimSize =\n-        rewriter.create<memref::LoadOp>(loc, *outputDimsBuffer, iVal);\n+        memref::LoadOp::create(rewriter, loc, *outputDimsBuffer, iVal);\n     if (!resultDimSize.getType().isIndex()) {\n-      resultDimSize = rewriter.create<arith::IndexCastOp>(\n-          loc, rewriter.getIndexType(), resultDimSize);\n+      resultDimSize = arith::IndexCastOp::create(\n+          rewriter, loc, rewriter.getIndexType(), resultDimSize);\n     }\n     if (resultType.isDynamicDim(i)) {\n       sizes.push_back(resultDimSize);\n@@ -229,10 +229,11 @@ FailureOr<Value> insertDynamicMemrefCastOp(\n     //    => stride flattened buffer stride\n     // 2) Operand dim < result dim => expansion is needed => stride := 0.\n     int dim = it->second;\n-    Value isExpansion = rewriter.create<arith::CmpIOp>(\n-        loc, arith::CmpIPredicate::slt, operandSizes[dim], resultDimSize);\n-    Value select = rewriter.create<mlir::arith::SelectOp>(\n-        loc, isExpansion, zero, operandStrides[dim]);\n+    Value isExpansion =\n+        arith::CmpIOp::create(rewriter, loc, arith::CmpIPredicate::slt,\n+                              operandSizes[dim], resultDimSize);\n+    Value select = mlir::arith::SelectOp::create(rewriter, loc, isExpansion,\n+                                                 zero, operandStrides[dim]);\n     strides.push_back(select);\n   }\n \n@@ -243,8 +244,8 @@ FailureOr<Value> insertDynamicMemrefCastOp(\n       makeStridedLinearLayoutMap(dynamicLayout,\n                                  /*offset=*/0, rewriter.getContext()));\n \n-  auto transformedOperand = rewriter.create<memref::ReinterpretCastOp>(\n-      loc, typeErasedMemrefType, operand,\n+  auto transformedOperand = memref::ReinterpretCastOp::create(\n+      rewriter, loc, typeErasedMemrefType, operand,\n       /*offset=*/rewriter.getI64IntegerAttr(0), sizes, strides);\n   return transformedOperand.getResult();\n }"
        },
        {
            "sha": "0362efa19df89e7757a49b10704d3e139dd7e0b0",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/hlo_legalize_to_stablehlo/hlo_legalize_to_stablehlo.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_stablehlo%2Fhlo_legalize_to_stablehlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_stablehlo%2Fhlo_legalize_to_stablehlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_stablehlo%2Fhlo_legalize_to_stablehlo.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -442,8 +442,8 @@ FailureOr<func::FuncOp> rewriteMhloRegionAsFunc(\n   auto& block = region.getBlocks().front();\n   auto type = rewriter.getFunctionType(\n       block.getArgumentTypes(), block.getTerminator()->getOperandTypes());\n-  auto funcOp = rewriter.create<func::FuncOp>(\n-      region.getLoc(), op->getName().stripDialect(), type);\n+  auto funcOp = func::FuncOp::create(rewriter, region.getLoc(),\n+                                     op->getName().stripDialect(), type);\n   symTable.insert(funcOp);\n \n   // Move region into new function\n@@ -685,9 +685,9 @@ class HloToStablehloOpConverter\n     // for the generic builder.\n     HloToStablehloOp<HloOpTy> stablehloOp;\n     if constexpr (std::is_same<HloOpTy, mhlo::CaseOp>::value) {\n-      stablehloOp = rewriter.create<stablehlo::CaseOp>(\n-          hloOp.getLoc(), stablehloTypes, stablehloOperands, stablehloAttrs,\n-          hloOp.getBranches().size());\n+      stablehloOp = stablehlo::CaseOp::create(\n+          rewriter, hloOp.getLoc(), stablehloTypes, stablehloOperands,\n+          stablehloAttrs, hloOp.getBranches().size());\n     } else {\n       stablehloOp = rewriter.create<HloToStablehloOp<HloOpTy>>(\n           hloOp.getLoc(), stablehloTypes, stablehloOperands, stablehloAttrs);"
        },
        {
            "sha": "9c5a34351d8ef94492fed36e5b20d85b1eda0613",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/legalize_einsum_to_dot_general/legalize_einsum_to_dot_general.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Flegalize_einsum_to_dot_general%2Flegalize_einsum_to_dot_general.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Flegalize_einsum_to_dot_general%2Flegalize_einsum_to_dot_general.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Flegalize_einsum_to_dot_general%2Flegalize_einsum_to_dot_general.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -159,9 +159,9 @@ struct EinsumToDotGeneralPattern : public OpRewritePattern<EinsumOp> {\n     auto dimNumbers = mhlo::DotDimensionNumbersAttr::get(\n         rewriter.getContext(), lhsBatchingDims, rhsBatchingDims,\n         lhsContractingDims, rhsContractingDims);\n-    auto dotGeneralOp = rewriter.create<DotGeneralOp>(\n-        einsum.getLoc(), dotGeneralResultType, einsum.getLhs(), einsum.getRhs(),\n-        dimNumbers,\n+    auto dotGeneralOp = DotGeneralOp::create(\n+        rewriter, einsum.getLoc(), dotGeneralResultType, einsum.getLhs(),\n+        einsum.getRhs(), dimNumbers,\n         /*precision_config=*/ArrayAttr{}, /*dot_algorithm=*/DotAlgorithmAttr{});\n \n     if (isNaturalOrder) {"
        },
        {
            "sha": "071efb373457606dec4ba343def9e55c59b6fb24",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/legalize_trigonometric_to_approximation/legalize_trigonometric_to_approximation.cc",
            "status": "modified",
            "additions": 44,
            "deletions": 41,
            "changes": 85,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Flegalize_trigonometric_to_approximation%2Flegalize_trigonometric_to_approximation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Flegalize_trigonometric_to_approximation%2Flegalize_trigonometric_to_approximation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Flegalize_trigonometric_to_approximation%2Flegalize_trigonometric_to_approximation.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -63,7 +63,7 @@ class ApproximateOnExtendedF32Lowering : public OpRewritePattern<OpTy> {\n       if (argTy.isF64()) return failure();\n \n       if (argTy.isF16())\n-        arg = rewriter.create<arith::ExtFOp>(loc, rewriter.getF32Type(), arg);\n+        arg = arith::ExtFOp::create(rewriter, loc, rewriter.getF32Type(), arg);\n \n       // If we still do not have f32, fail.\n       if (!arg.getType().isF32()) return failure();\n@@ -77,7 +77,7 @@ class ApproximateOnExtendedF32Lowering : public OpRewritePattern<OpTy> {\n     // Truncate back if needed.\n     if (op.getType().isF16()) {\n       result =\n-          rewriter.create<arith::TruncFOp>(loc, rewriter.getF16Type(), result);\n+          arith::TruncFOp::create(rewriter, loc, rewriter.getF16Type(), result);\n     }\n \n     rewriter.replaceOp(op, {result});\n@@ -108,59 +108,62 @@ class ApproximateTanhLowering\n         4.89352518554385e-03f};\n \n     // Materialize polynomial approximation.\n-    Value inputSquared = rewriter.create<arith::MulFOp>(loc, input, input);\n-    Value numerator = rewriter.create<arith::ConstantOp>(\n-        loc, rewriter.getF32FloatAttr(numeratorCoeffs[0]));\n+    Value inputSquared = arith::MulFOp::create(rewriter, loc, input, input);\n+    Value numerator = arith::ConstantOp::create(\n+        rewriter, loc, rewriter.getF32FloatAttr(numeratorCoeffs[0]));\n     for (int64_t i = 1; i < static_cast<int64_t>(numeratorCoeffs.size()); i++) {\n-      numerator = rewriter.create<arith::AddFOp>(\n-          loc, rewriter.create<arith::MulFOp>(loc, inputSquared, numerator),\n-          rewriter.create<arith::ConstantOp>(\n-              loc, rewriter.getF32FloatAttr(numeratorCoeffs[i])));\n+      numerator = arith::AddFOp::create(\n+          rewriter, loc,\n+          arith::MulFOp::create(rewriter, loc, inputSquared, numerator),\n+          arith::ConstantOp::create(\n+              rewriter, loc, rewriter.getF32FloatAttr(numeratorCoeffs[i])));\n     }\n-    numerator = rewriter.create<arith::MulFOp>(loc, input, numerator);\n-    Value denominator = rewriter.create<arith::ConstantOp>(\n-        loc, rewriter.getF32FloatAttr(denominatorCoeffs[0]));\n+    numerator = arith::MulFOp::create(rewriter, loc, input, numerator);\n+    Value denominator = arith::ConstantOp::create(\n+        rewriter, loc, rewriter.getF32FloatAttr(denominatorCoeffs[0]));\n     for (int64_t i = 1; i < static_cast<int64_t>(denominatorCoeffs.size());\n          i++) {\n-      denominator = rewriter.create<arith::AddFOp>(\n-          loc, rewriter.create<arith::MulFOp>(loc, inputSquared, denominator),\n-          rewriter.create<arith::ConstantOp>(\n-              loc, rewriter.getF32FloatAttr(denominatorCoeffs[i])));\n+      denominator = arith::AddFOp::create(\n+          rewriter, loc,\n+          arith::MulFOp::create(rewriter, loc, inputSquared, denominator),\n+          arith::ConstantOp::create(\n+              rewriter, loc, rewriter.getF32FloatAttr(denominatorCoeffs[i])));\n     }\n-    Value approx = rewriter.create<arith::DivFOp>(loc, numerator, denominator);\n+    Value approx = arith::DivFOp::create(rewriter, loc, numerator, denominator);\n \n     // For small values of |x|, we can approximate tanh(x) = x. For extremely\n     // small values of x (|x| < 1e-37), the other approximation would evaluate\n     // tanh(x) = 0.\n     constexpr float kUseIdentityApprox = 0.0004;\n-    Value absInput = rewriter.create<math::AbsFOp>(loc, input);\n-    Value useIdentityApprox = rewriter.create<arith::CmpFOp>(\n-        loc, arith::CmpFPredicate::OLT, absInput,\n-        rewriter.create<arith::ConstantOp>(\n-            loc, rewriter.getF32FloatAttr(kUseIdentityApprox)));\n-    approx =\n-        rewriter.create<arith::SelectOp>(loc, useIdentityApprox, input, approx);\n+    Value absInput = math::AbsFOp::create(rewriter, loc, input);\n+    Value useIdentityApprox = arith::CmpFOp::create(\n+        rewriter, loc, arith::CmpFPredicate::OLT, absInput,\n+        arith::ConstantOp::create(\n+            rewriter, loc, rewriter.getF32FloatAttr(kUseIdentityApprox)));\n+    approx = arith::SelectOp::create(rewriter, loc, useIdentityApprox, input,\n+                                     approx);\n \n     // For very small/large values, use a constant approximation -1/1.\n-    Value tooLargeInput = rewriter.create<arith::CmpFOp>(\n-        loc, arith::CmpFPredicate::UGT, input,\n-        rewriter.create<arith::ConstantOp>(\n-            loc, rewriter.getF32FloatAttr(7.90531110763549805f)));\n-    Value tooSmallInput = rewriter.create<arith::CmpFOp>(\n-        loc, arith::CmpFPredicate::ULT, input,\n-        rewriter.create<arith::ConstantOp>(\n-            loc, rewriter.getF32FloatAttr(-7.90531110763549805f)));\n-    Value inputIsNan = rewriter.create<arith::CmpFOp>(\n-        loc, arith::CmpFPredicate::UNE, input, input);\n-    approx = rewriter.create<arith::SelectOp>(\n-        loc, tooLargeInput,\n-        rewriter.create<arith::ConstantOp>(loc, rewriter.getF32FloatAttr(1.0)),\n+    Value tooLargeInput = arith::CmpFOp::create(\n+        rewriter, loc, arith::CmpFPredicate::UGT, input,\n+        arith::ConstantOp::create(\n+            rewriter, loc, rewriter.getF32FloatAttr(7.90531110763549805f)));\n+    Value tooSmallInput = arith::CmpFOp::create(\n+        rewriter, loc, arith::CmpFPredicate::ULT, input,\n+        arith::ConstantOp::create(\n+            rewriter, loc, rewriter.getF32FloatAttr(-7.90531110763549805f)));\n+    Value inputIsNan = arith::CmpFOp::create(\n+        rewriter, loc, arith::CmpFPredicate::UNE, input, input);\n+    approx = arith::SelectOp::create(\n+        rewriter, loc, tooLargeInput,\n+        arith::ConstantOp::create(rewriter, loc, rewriter.getF32FloatAttr(1.0)),\n         approx);\n-    approx = rewriter.create<arith::SelectOp>(\n-        loc, tooSmallInput,\n-        rewriter.create<arith::ConstantOp>(loc, rewriter.getF32FloatAttr(-1.0)),\n+    approx = arith::SelectOp::create(\n+        rewriter, loc, tooSmallInput,\n+        arith::ConstantOp::create(rewriter, loc,\n+                                  rewriter.getF32FloatAttr(-1.0)),\n         approx);\n-    approx = rewriter.create<arith::SelectOp>(loc, inputIsNan, input, approx);\n+    approx = arith::SelectOp::create(rewriter, loc, inputIsNan, input, approx);\n \n     return approx;\n   }"
        },
        {
            "sha": "46449eb5cbaadbe22aac9cde3b8aef17f649afd5",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/mhlo_flatten_tuple/mhlo_flatten_tuple.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fmhlo_flatten_tuple%2Fmhlo_flatten_tuple.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fmhlo_flatten_tuple%2Fmhlo_flatten_tuple.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fmhlo_flatten_tuple%2Fmhlo_flatten_tuple.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -66,7 +66,7 @@ Value createTupleValue(OpBuilder &builder, Location loc,\n \n   assert(mlir::cast<TupleType>(tupleType).getTypes().size() ==\n          flattenValues.size());\n-  return builder.create<mhlo::TupleOp>(loc, flattenValues);\n+  return mhlo::TupleOp::create(builder, loc, flattenValues);\n }\n \n void flattenTupleValue(OpBuilder &builder, Location loc, Value value,\n@@ -78,8 +78,9 @@ void flattenTupleValue(OpBuilder &builder, Location loc, Value value,\n   }\n   int flattenIdx = 0;\n   for (auto innerType : tupleType.getTypes()) {\n-    auto innerValue = builder.create<mhlo::GetTupleElementOp>(\n-        loc, innerType, value, builder.getI32IntegerAttr(flattenIdx++));\n+    auto innerValue = mhlo::GetTupleElementOp::create(\n+        builder, loc, innerType, value,\n+        builder.getI32IntegerAttr(flattenIdx++));\n     flattenTupleValue(builder, loc, innerValue, flattenedValues);\n   }\n }\n@@ -114,8 +115,9 @@ struct FlattenCustomCallOp : public OpRewritePattern<CustomCallOp> {\n         flattenTupleType(result, flattenedResultTypes);\n     }\n \n-    auto flattenedCall = rewriter.create<mhlo::CustomCallOp>(\n-        op->getLoc(), flattenedResultTypes, flattenedOperands, op->getAttrs());\n+    auto flattenedCall =\n+        mhlo::CustomCallOp::create(rewriter, op->getLoc(), flattenedResultTypes,\n+                                   flattenedOperands, op->getAttrs());\n \n     rewriter.replaceOp(op, flattenResult\n                                ? createTupleValue(rewriter, op->getLoc(),"
        },
        {
            "sha": "35d3379583437dd51f0861d6c0c1cf331f6f7841",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/optimize_mhlo/optimize_mhlo.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 10,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Foptimize_mhlo%2Foptimize_mhlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Foptimize_mhlo%2Foptimize_mhlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Foptimize_mhlo%2Foptimize_mhlo.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -120,10 +120,11 @@ class GatherIsSlice : public OpRewritePattern<GatherOp> {\n         auto start = getI64ElementsAttr({i}, &rewriter);\n         auto limit = getI64ElementsAttr({i + 1}, &rewriter);\n         auto stride = getI64ElementsAttr({1}, &rewriter);\n-        auto indicesSlice = rewriter.create<SliceOp>(\n-            gather.getLoc(), gatherStartIndices, start, limit, stride);\n-        auto reshaped = rewriter.create<ReshapeOp>(\n-            gather.getLoc(),\n+        auto indicesSlice =\n+            SliceOp::create(rewriter, gather.getLoc(), gatherStartIndices,\n+                            start, limit, stride);\n+        auto reshaped = ReshapeOp::create(\n+            rewriter, gather.getLoc(),\n             RankedTensorType::get({},\n                                   mlir::cast<ShapedType>(indicesSlice.getType())\n                                       .getElementType()),\n@@ -139,9 +140,10 @@ class GatherIsSlice : public OpRewritePattern<GatherOp> {\n     // Start indices have implicit zeros when not specified. This is because\n     // Gather occurs similar to slicing where full slices are inferred. Add any\n     // missing zeros as necessary.\n-    auto zero = rewriter.create<ConstantOp>(\n-        gather.getLoc(), rewriter.getZeroAttr(RankedTensorType::get(\n-                             {}, gatherStartIndicesTy.getElementType())));\n+    auto zero =\n+        ConstantOp::create(rewriter, gather.getLoc(),\n+                           rewriter.getZeroAttr(RankedTensorType::get(\n+                               {}, gatherStartIndicesTy.getElementType())));\n     while (static_cast<int64_t>(sliceStartIndices.size()) <\n            sliceSizesTy.getDimSize(0)) {\n       sliceStartIndices.push_back(zero);\n@@ -153,9 +155,9 @@ class GatherIsSlice : public OpRewritePattern<GatherOp> {\n     }\n \n     auto sliceTy = RankedTensorType::get(sliceShape, resultTy.getElementType());\n-    auto slice = rewriter.create<DynamicSliceOp>(\n-        gather.getLoc(), sliceTy, gather.getOperand(), sliceStartIndices,\n-        gather.getSliceSizes());\n+    auto slice = DynamicSliceOp::create(rewriter, gather.getLoc(), sliceTy,\n+                                        gather.getOperand(), sliceStartIndices,\n+                                        gather.getSliceSizes());\n \n     rewriter.replaceOpWithNewOp<ReshapeOp>(gather, gather.getType(), slice);\n "
        },
        {
            "sha": "bf92c0636b759fb58d394f50f88f89db09e47727",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/prepare_for_export/prepare_for_export.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fprepare_for_export%2Fprepare_for_export.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fprepare_for_export%2Fprepare_for_export.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fprepare_for_export%2Fprepare_for_export.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -71,12 +71,12 @@ void prepareConstantOp(Operation* op, SplatElementsAttr attr) {\n     assert(mlir::isa<FloatType>(complexTy.getElementType()) &&\n            \"unexpected int complex in MHLO\");\n     auto complexVal = attr.getSplatValue<std::complex<APFloat>>();\n-    cst = b.create<ConstantOp>(DenseElementsAttr::get(tensorType, complexVal));\n+    cst = ConstantOp::create(b, DenseElementsAttr::get(tensorType, complexVal));\n   } else {\n-    cst = b.create<ConstantOp>(attr.getSplatValue<Attribute>());\n+    cst = ConstantOp::create(b, attr.getSplatValue<Attribute>());\n   }\n   auto broadcast =\n-      b.create<BroadcastInDimOp>(returnType, cst, b.getI64TensorAttr({}));\n+      BroadcastInDimOp::create(b, returnType, cst, b.getI64TensorAttr({}));\n   if (auto sharding = op->getAttrOfType<mlir::StringAttr>(kShardingAttr)) {\n     // The added broadcast inherits the kShardingAttr from op.\n     broadcast->setAttr(kShardingAttr, sharding);\n@@ -103,8 +103,8 @@ void prepareBroadcastInDim(BroadcastInDimOp bcast) {\n     return rawDims[lhs] < rawDims[rhs];\n   });\n   OpBuilder builder(bcast);\n-  bcast.setOperand(builder.create<TransposeOp>(\n-      bcast.getLoc(), bcast.getOperand(),\n+  bcast.setOperand(TransposeOp::create(\n+      builder, bcast.getLoc(), bcast.getOperand(),\n       DenseIntElementsAttr::get(dims.getType(), transposedDim)));\n   // Now reuse the original broadcast_dimensions and sort it.\n   transposedDim.assign(rawDims.begin(), rawDims.end());"
        },
        {
            "sha": "cb7e6793f19f5c2e6da63355e87667fa7a0944a6",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/stablehlo_legalize_to_hlo/stablehlo_legalize_to_hlo.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fstablehlo_legalize_to_hlo%2Fstablehlo_legalize_to_hlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fstablehlo_legalize_to_hlo%2Fstablehlo_legalize_to_hlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fstablehlo_legalize_to_hlo%2Fstablehlo_legalize_to_hlo.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -404,9 +404,9 @@ class StablehloToHloOpConverter : public OpConversionPattern<StablehloOpTy> {\n     // for the generic builder.\n     StablehloToHloOp<StablehloOpTy> hloOp;\n     if constexpr (std::is_same<StablehloOpTy, stablehlo::CaseOp>::value) {\n-      hloOp = rewriter.create<mhlo::CaseOp>(stablehloOp.getLoc(), hloTypes,\n-                                            hloOperands, hloAttrs,\n-                                            stablehloOp.getBranches().size());\n+      hloOp = mhlo::CaseOp::create(rewriter, stablehloOp.getLoc(), hloTypes,\n+                                   hloOperands, hloAttrs,\n+                                   stablehloOp.getBranches().size());\n     } else {\n       hloOp = rewriter.create<StablehloToHloOp<StablehloOpTy>>(\n           stablehloOp.getLoc(), hloTypes, hloOperands, hloAttrs);"
        },
        {
            "sha": "47791f5ec751c45649de91bbdd4d81fef6c06b57",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/unfuse_batch_norm/unfuse_batch_norm.cc",
            "status": "modified",
            "additions": 50,
            "deletions": 48,
            "changes": 98,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Funfuse_batch_norm%2Funfuse_batch_norm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Funfuse_batch_norm%2Funfuse_batch_norm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Funfuse_batch_norm%2Funfuse_batch_norm.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -50,17 +50,17 @@ Value broadcastToFeatureDim(Location loc, RankedTensorType resultType,\n         loc, resultType, value1d, shapeValue, dims);\n   }\n   assert(resultType.hasStaticShape());\n-  return rewriter.create<mhlo::BroadcastInDimOp>(loc, resultType, value1d,\n-                                                 dims);\n+  return mhlo::BroadcastInDimOp::create(rewriter, loc, resultType, value1d,\n+                                        dims);\n }\n \n // Get the shape of operand, assuming it is a dynamic shape with static rank.\n Value getShapeValue(Location loc, Value operand,\n                     PatternRewriter &rewriter) {  // NOLINT\n   RankedTensorType resultType =\n       mlir::dyn_cast<RankedTensorType>(operand.getType());\n-  return rewriter.create<mlir::shape::ShapeOfOp>(\n-      loc,\n+  return mlir::shape::ShapeOfOp::create(\n+      rewriter, loc,\n       RankedTensorType::get({resultType.getRank()}, rewriter.getIndexType()),\n       operand);\n }\n@@ -90,12 +90,12 @@ Value materializeEpsilon(Operation *op, FloatAttr epsilonAttr, FloatType fpType,\n   auto scalarType = RankedTensorType::get({}, fpType);\n   auto epsilonTensorAttr =\n       DenseElementsAttr::get(scalarType, {mlir::cast<Attribute>(epsilonAttr)});\n-  Value epsilon = b.create<mhlo::ConstantOp>(epsilonTensorAttr);\n+  Value epsilon = mhlo::ConstantOp::create(b, epsilonTensorAttr);\n   auto dimsType = RankedTensorType::get({0}, b.getIntegerType(64));\n   auto dims = DenseIntElementsAttr::get(dimsType, SmallVector<int64_t, 1>{});\n   if (broadcastToType.hasStaticShape()) {\n-    return b.create<mhlo::BroadcastInDimOp>(broadcastToType, epsilon,\n-                                            /*broadcast_dims=*/dims);\n+    return mhlo::BroadcastInDimOp::create(b, broadcastToType, epsilon,\n+                                          /*broadcast_dims=*/dims);\n   }\n   Value shapeValue = getShapeValue(op->getLoc(), broadcastTo, rewriter);\n   return b.createOrFold<mhlo::DynamicBroadcastInDimOp>(broadcastToType, epsilon,\n@@ -134,9 +134,9 @@ class UnfuseBatchNormInferencePattern\n     if (!epsilon) {\n       return failure();\n     }\n-    Value stddev = rewriter.create<mhlo::AddOp>(bnOp.getLoc(),\n-                                                bnOp.getVariance(), epsilon);\n-    stddev = rewriter.create<mhlo::SqrtOp>(bnOp.getLoc(), stddev);\n+    Value stddev = mhlo::AddOp::create(rewriter, bnOp.getLoc(),\n+                                       bnOp.getVariance(), epsilon);\n+    stddev = mhlo::SqrtOp::create(rewriter, bnOp.getLoc(), stddev);\n \n     // Broadcast all terms.\n     Value shapeValue;\n@@ -157,12 +157,12 @@ class UnfuseBatchNormInferencePattern\n \n     // Compute:\n     // scale * (input - mean) / stddev + offset\n-    Value result = rewriter.create<mhlo::SubtractOp>(\n-        bnOp.getLoc(), bnOp.getOperand(), broadcastMean);\n+    Value result = mhlo::SubtractOp::create(rewriter, bnOp.getLoc(),\n+                                            bnOp.getOperand(), broadcastMean);\n     result =\n-        rewriter.create<mhlo::MulOp>(bnOp.getLoc(), result, broadcastScale);\n+        mhlo::MulOp::create(rewriter, bnOp.getLoc(), result, broadcastScale);\n     result =\n-        rewriter.create<mhlo::DivOp>(bnOp.getLoc(), result, broadcastStddev);\n+        mhlo::DivOp::create(rewriter, bnOp.getLoc(), result, broadcastStddev);\n     rewriter.replaceOpWithNewOp<mhlo::AddOp>(bnOp, result, broadcastOffset);\n \n     return success();\n@@ -178,8 +178,8 @@ Value createReduce(Location loc, Value operand, Value zero,\n   Type reduceResultType = RankedTensorType::get(\n       {operandType.getDimSize(featureIndex)}, operandType.getElementType());\n   mhlo::ReduceOp reduce =\n-      rewriter.create<mhlo::ReduceOp>(loc, reduceResultType, operand, zero,\n-                                      rewriter.getI64TensorAttr(reduceDims));\n+      mhlo::ReduceOp::create(rewriter, loc, reduceResultType, operand, zero,\n+                             rewriter.getI64TensorAttr(reduceDims));\n \n   // setup \"mhlo.reduce\"'s body\n   Region &region = reduce.getBody();\n@@ -194,8 +194,8 @@ Value createReduce(Location loc, Value operand, Value zero,\n     OpBuilder::InsertionGuard guard(rewriter);\n     rewriter.setInsertionPointToStart(&block);\n     Value addResult =\n-        rewriter.create<mhlo::AddOp>(loc, *firstArgument, *secondArgument);\n-    rewriter.create<mhlo::ReturnOp>(loc, addResult);\n+        mhlo::AddOp::create(rewriter, loc, *firstArgument, *secondArgument);\n+    mhlo::ReturnOp::create(rewriter, loc, addResult);\n   }\n \n   return reduce.getResult(0);\n@@ -214,17 +214,18 @@ Value calculateReduceSize(Operation *op, Value operand,\n     Value operandShape = getShapeValue(op->getLoc(), operand, rewriter);\n     Value scaleShape = getShapeValue(op->getLoc(), scale, rewriter);\n     Value operandTotalSize =\n-        b.create<shape::NumElementsOp>(indexType, operandShape);\n+        shape::NumElementsOp::create(b, indexType, operandShape);\n     Value scaleTotalSize =\n-        b.create<shape::NumElementsOp>(indexType, scaleShape);\n+        shape::NumElementsOp::create(b, indexType, scaleShape);\n     Value reduceSize =\n-        b.create<shape::DivOp>(indexType, operandTotalSize, scaleTotalSize);\n-    reduceSize = b.create<arith::IndexCastOp>(b.getI64Type(), reduceSize);\n-    reduceSize = b.create<tensor::FromElementsOp>(reduceSize);\n-    reduceSize = b.create<mhlo::ConvertOp>(\n-        RankedTensorType::get({1}, operandType.getElementType()), reduceSize);\n-    reduceSize = b.create<mhlo::ReshapeOp>(\n-        RankedTensorType::get({}, operandType.getElementType()), reduceSize);\n+        shape::DivOp::create(b, indexType, operandTotalSize, scaleTotalSize);\n+    reduceSize = arith::IndexCastOp::create(b, b.getI64Type(), reduceSize);\n+    reduceSize = tensor::FromElementsOp::create(b, reduceSize);\n+    reduceSize = mhlo::ConvertOp::create(\n+        b, RankedTensorType::get({1}, operandType.getElementType()),\n+        reduceSize);\n+    reduceSize = mhlo::ReshapeOp::create(\n+        b, RankedTensorType::get({}, operandType.getElementType()), reduceSize);\n     return b.createOrFold<mhlo::DynamicBroadcastInDimOp>(\n         scaleType, reduceSize, scaleShape, b.getI64TensorAttr({}));\n   }\n@@ -244,8 +245,8 @@ Value calculateReduceSize(Operation *op, Value operand,\n   if (losesInfo) {\n     op->emitWarning(\"Conversion of reduce_dims_size loses precision\");\n   }\n-  Value reduceSize = b.create<mhlo::ConstantOp>(\n-      DenseFPElementsAttr::get(scaleType, floatValue));\n+  Value reduceSize = mhlo::ConstantOp::create(\n+      b, DenseFPElementsAttr::get(scaleType, floatValue));\n   return reduceSize;\n }\n \n@@ -278,8 +279,8 @@ class UnfuseBatchNormTrainingPattern\n     }\n \n     // zero constant\n-    Value constZero = rewriter.create<mhlo::ConstantOp>(\n-        bnOp.getLoc(),\n+    Value constZero = mhlo::ConstantOp::create(\n+        rewriter, bnOp.getLoc(),\n         DenseFPElementsAttr::get(RankedTensorType::get({}, fpType),\n                                  APFloat::getZero(fpType.getFloatSemantics())));\n     // epsilon\n@@ -300,27 +301,28 @@ class UnfuseBatchNormTrainingPattern\n     Value sum = createReduce(bnOp.getLoc(), bnOp.getOperand(), constZero,\n                              dimensionsWithoutFeature, featureIndex, rewriter);\n     // X^2\n-    Value operandSquare = rewriter.create<mhlo::MulOp>(\n-        bnOp.getLoc(), bnOp.getOperand(), bnOp.getOperand());\n+    Value operandSquare = mhlo::MulOp::create(\n+        rewriter, bnOp.getLoc(), bnOp.getOperand(), bnOp.getOperand());\n     // Sum[X^2]\n     Value squareSum =\n         createReduce(bnOp.getLoc(), operandSquare, constZero,\n                      dimensionsWithoutFeature, featureIndex, rewriter);\n     // E[X]\n-    Value mean = rewriter.create<mhlo::DivOp>(bnOp.getLoc(), sum, reduceSize);\n+    Value mean = mhlo::DivOp::create(rewriter, bnOp.getLoc(), sum, reduceSize);\n     // E[X^2]\n     Value squareMean =\n-        rewriter.create<mhlo::DivOp>(bnOp.getLoc(), squareSum, reduceSize);\n+        mhlo::DivOp::create(rewriter, bnOp.getLoc(), squareSum, reduceSize);\n     // E^2[X]\n-    Value meanSquare = rewriter.create<mhlo::MulOp>(bnOp.getLoc(), mean, mean);\n+    Value meanSquare = mhlo::MulOp::create(rewriter, bnOp.getLoc(), mean, mean);\n     // Var[X]\n-    Value var = rewriter.create<mhlo::SubtractOp>(bnOp.getLoc(), squareMean,\n-                                                  meanSquare);\n+    Value var = mhlo::SubtractOp::create(rewriter, bnOp.getLoc(), squareMean,\n+                                         meanSquare);\n     // Var[X] + epsilon\n     Value varAddEpsilon =\n-        rewriter.create<mhlo::AddOp>(bnOp.getLoc(), var, epsilon);\n+        mhlo::AddOp::create(rewriter, bnOp.getLoc(), var, epsilon);\n     // Sqrt(Var[X] + epsilon)\n-    Value sqrtVar = rewriter.create<mhlo::SqrtOp>(bnOp.getLoc(), varAddEpsilon);\n+    Value sqrtVar =\n+        mhlo::SqrtOp::create(rewriter, bnOp.getLoc(), varAddEpsilon);\n \n     Value shapeValue;\n     if (!operandType.hasStaticShape()) {\n@@ -329,27 +331,27 @@ class UnfuseBatchNormTrainingPattern\n     // X - E[X]\n     Value meanBroadcast = broadcastToFeatureDim(\n         bnOp.getLoc(), operandType, mean, shapeValue, featureIndex, rewriter);\n-    Value operandMinusMean = rewriter.create<mhlo::SubtractOp>(\n-        bnOp.getLoc(), bnOp.getOperand(), meanBroadcast);\n+    Value operandMinusMean = mhlo::SubtractOp::create(\n+        rewriter, bnOp.getLoc(), bnOp.getOperand(), meanBroadcast);\n     // (X - E[X]) / Sqrt(Var[X] + epsilon)\n     Value sqrtVarBroadcast =\n         broadcastToFeatureDim(bnOp.getLoc(), operandType, sqrtVar, shapeValue,\n                               featureIndex, rewriter);\n-    Value normalized = rewriter.create<mhlo::DivOp>(\n-        bnOp.getLoc(), operandMinusMean, sqrtVarBroadcast);\n+    Value normalized = mhlo::DivOp::create(rewriter, bnOp.getLoc(),\n+                                           operandMinusMean, sqrtVarBroadcast);\n \n     // ((X - E[X]) / Sqrt(Var[X] + epsilon)) * scale\n     Value scaleBroadcast =\n         broadcastToFeatureDim(bnOp.getLoc(), operandType, bnOp.getScale(),\n                               shapeValue, featureIndex, rewriter);\n-    Value scaledNormalized =\n-        rewriter.create<mhlo::MulOp>(bnOp.getLoc(), normalized, scaleBroadcast);\n+    Value scaledNormalized = mhlo::MulOp::create(rewriter, bnOp.getLoc(),\n+                                                 normalized, scaleBroadcast);\n     // ((X - E[X]) / Sqrt(Var[X] + epsilon)) * scale + offset.\n     Value offsetBroadcast =\n         broadcastToFeatureDim(bnOp.getLoc(), operandType, bnOp.getOffset(),\n                               shapeValue, featureIndex, rewriter);\n-    Value shiftedNormalized = rewriter.create<mhlo::AddOp>(\n-        bnOp.getLoc(), scaledNormalized, offsetBroadcast);\n+    Value shiftedNormalized = mhlo::AddOp::create(\n+        rewriter, bnOp.getLoc(), scaledNormalized, offsetBroadcast);\n \n     // results\n     SmallVector<Value> results = {shiftedNormalized, mean, var};"
        },
        {
            "sha": "ed921647946dc3a59caf9c9e63184f2a33166599",
            "filename": "third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/sdy_refine_shapes.cpp",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fsdy_refine_shapes.cpp",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fsdy_refine_shapes.cpp",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fsdy_refine_shapes.cpp?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -176,8 +176,8 @@ LogicalResult refineValues(\n     };\n     if (llvm::none_of(value.getUses(), isFuncReturn)) continue;\n     rewriter.setInsertionPointAfter(manualComputation);\n-    auto castToUnrefinedType = rewriter.create<UnrealizedConversionCastOp>(\n-        manualComputation->getLoc(), unrefinedType, value);\n+    auto castToUnrefinedType = UnrealizedConversionCastOp::create(\n+        rewriter, manualComputation->getLoc(), unrefinedType, value);\n     value.replaceUsesWithIf(castToUnrefinedType.getOutputs()[0], isFuncReturn);\n   }\n "
        },
        {
            "sha": "bd49a0434e7c00463f28760446ee51a98fd6cf65",
            "filename": "third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/stablehlo_add_quant_dequant_conv.cpp",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_add_quant_dequant_conv.cpp",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_add_quant_dequant_conv.cpp",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_add_quant_dequant_conv.cpp?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -83,12 +83,11 @@ struct AddQuantDeQuantAfterConvolutionOp final\n         cast<ShapedType>(clonedConvOp->getResult(0).getType());\n     auto loc = clonedConvOp->getLoc();\n     auto quantizedType = getQuantizedType(loc, rewriter, convResultType);\n-    auto stablehloQuantizeOp = rewriter.create<stablehlo::UniformQuantizeOp>(\n-        op.getLoc(), quantizedType, clonedConvOp->getResult(0));\n-    auto stablehloDeQuantizeOp =\n-        rewriter.create<stablehlo::UniformDequantizeOp>(\n-            op.getLoc(), op.getType(),\n-            /*input=*/stablehloQuantizeOp.getResult());\n+    auto stablehloQuantizeOp = stablehlo::UniformQuantizeOp::create(\n+        rewriter, op.getLoc(), quantizedType, clonedConvOp->getResult(0));\n+    auto stablehloDeQuantizeOp = stablehlo::UniformDequantizeOp::create(\n+        rewriter, op.getLoc(), op.getType(),\n+        /*input=*/stablehloQuantizeOp.getResult());\n     rewriter.replaceAllUsesWith(op, stablehloDeQuantizeOp.getResult());\n     return success();\n   }"
        },
        {
            "sha": "a4dc72397919c15a236ad1074654dc51185578db",
            "filename": "third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/stablehlo_canonicalize_dynamism.cpp",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_canonicalize_dynamism.cpp",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_canonicalize_dynamism.cpp",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_canonicalize_dynamism.cpp?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -63,9 +63,9 @@ struct CanonicalizeDynamicReduceWindowOpPattern\n                                          \"expected static window_dilations\");\n     if (failed(hlo::matchInts(op.getPadding(), padding)))\n       return rewriter.notifyMatchFailure(op, \"expected static padding\");\n-    auto newOp = rewriter.create<stablehlo::ReduceWindowOp>(\n-        op->getLoc(), op->getResultTypes(), op.getInputs(), op.getInitValues(),\n-        rewriter.getDenseI64ArrayAttr(windowDimensions),\n+    auto newOp = stablehlo::ReduceWindowOp::create(\n+        rewriter, op->getLoc(), op->getResultTypes(), op.getInputs(),\n+        op.getInitValues(), rewriter.getDenseI64ArrayAttr(windowDimensions),\n         rewriter.getDenseI64ArrayAttr(windowStrides),\n         rewriter.getDenseI64ArrayAttr(baseDilations),\n         rewriter.getDenseI64ArrayAttr(windowDilations),"
        },
        {
            "sha": "8f5ba8def72a0f6e109bb2b15561964b609fb9fb",
            "filename": "third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/stablehlo_canonicalize_from_hlo_import.cpp",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_canonicalize_from_hlo_import.cpp",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_canonicalize_from_hlo_import.cpp",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_canonicalize_from_hlo_import.cpp?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -95,8 +95,8 @@ LogicalResult expandTupledTensorInReturnOp(func::FuncOp func) {\n       // Construct a new tuple and rewire it.\n       OpBuilder builder(func.getBody());\n       builder.setInsertionPointToStart(&func.getBody().front());\n-      auto newTuple =\n-          builder.create<stablehlo::TupleOp>(loc, tupleType, flattenedOperands);\n+      auto newTuple = stablehlo::TupleOp::create(builder, loc, tupleType,\n+                                                 flattenedOperands);\n       func.getArgument(originalArgumentIndex).replaceAllUsesWith(newTuple);\n \n       // Now the original argument has been rewired, we should be able to\n@@ -130,8 +130,8 @@ LogicalResult expandTupledTensorInReturnOp(func::FuncOp func) {\n \n   if (returnOp.getOperands() == expandedReturnOperands) return success();\n \n-  builder.create<mlir::func::ReturnOp>(returnOp.getLoc(),\n-                                       expandedReturnOperands);\n+  mlir::func::ReturnOp::create(builder, returnOp.getLoc(),\n+                               expandedReturnOperands);\n   returnOp.erase();\n   auto newFuncType = FunctionType::get(oldFuncType.getContext(),\n                                        expandedInputTypes, expandedResultTypes);\n@@ -174,7 +174,7 @@ Value createTupleValue(OpBuilder &builder, Location loc,\n         createTupleValue(builder, loc, flattenValues, childType));\n   }\n \n-  return builder.create<mlir::stablehlo::TupleOp>(loc, flattenedSubValues)\n+  return mlir::stablehlo::TupleOp::create(builder, loc, flattenedSubValues)\n       .getResult();\n }\n \n@@ -187,8 +187,9 @@ void flattenTupleValue(OpBuilder &builder, Location loc, Value value,\n   }\n   int flattenIdx = 0;\n   for (auto innerType : tupleType.getTypes()) {\n-    auto innerValue = builder.create<stablehlo::GetTupleElementOp>(\n-        loc, innerType, value, builder.getI32IntegerAttr(flattenIdx++));\n+    auto innerValue = stablehlo::GetTupleElementOp::create(\n+        builder, loc, innerType, value,\n+        builder.getI32IntegerAttr(flattenIdx++));\n     flattenTupleValue(builder, loc, innerValue, flattenedValues);\n   }\n }\n@@ -220,8 +221,9 @@ struct FlattenCustomCallOp : public OpRewritePattern<stablehlo::CustomCallOp> {\n                                   op->result_type_end());\n     }\n \n-    auto flattenedCall = rewriter.create<stablehlo::CustomCallOp>(\n-        op->getLoc(), flattenedResultTypes, flattenedOperands, op->getAttrs());\n+    auto flattenedCall = stablehlo::CustomCallOp::create(\n+        rewriter, op->getLoc(), flattenedResultTypes, flattenedOperands,\n+        op->getAttrs());\n \n     if (flattenResult) {\n       ValueRange flattenedResultsRef(flattenedCall.getResults());"
        },
        {
            "sha": "3f5b026fb940f593a44b24f12738b6d12b212962",
            "filename": "third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/stablehlo_legalize_quant_composite.cpp",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_legalize_quant_composite.cpp",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_legalize_quant_composite.cpp",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_legalize_quant_composite.cpp?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -309,8 +309,8 @@ class RewriteFakeQuantCompositeOp\n         quantizedDimension, storageTypeMin, storageTypeMax);\n     RankedTensorType quantizedType = RankedTensorType::get(\n         llvm::cast<ShapedType>(op.getType(0)).getShape(), quantizedElementType);\n-    auto stablehloQuantizeOp = rewriter.create<stablehlo::UniformQuantizeOp>(\n-        op.getLoc(), quantizedType, /*input=*/op.getOperand(0));\n+    auto stablehloQuantizeOp = stablehlo::UniformQuantizeOp::create(\n+        rewriter, op.getLoc(), quantizedType, /*input=*/op.getOperand(0));\n     rewriter.replaceOpWithNewOp<stablehlo::UniformDequantizeOp>(\n         op, op.getType(0),\n         /*input=*/stablehloQuantizeOp.getResult());"
        },
        {
            "sha": "2450fc2393598d25afbc46c285d2d4988caa5cfb",
            "filename": "third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/stablehlo_prepare_for_hlo_export.cpp",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_prepare_for_hlo_export.cpp",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_prepare_for_hlo_export.cpp",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fstablehlo_prepare_for_hlo_export.cpp?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -72,13 +72,13 @@ static void prepareConstantOp(Operation *op, SplatElementsAttr attr) {\n     assert(mlir::isa<FloatType>(complexTy.getElementType()) &&\n            \"unexpected int complex in StableHLO\");\n     auto complexVal = attr.getSplatValue<std::complex<APFloat>>();\n-    cst = b.create<stablehlo::ConstantOp>(\n-        DenseElementsAttr::get(tensorType, complexVal));\n+    cst = stablehlo::ConstantOp::create(\n+        b, DenseElementsAttr::get(tensorType, complexVal));\n   } else {\n-    cst = b.create<stablehlo::ConstantOp>(attr.getSplatValue<Attribute>());\n+    cst = stablehlo::ConstantOp::create(b, attr.getSplatValue<Attribute>());\n   }\n-  auto broadcast = b.create<stablehlo::BroadcastInDimOp>(\n-      returnType, cst, b.getDenseI64ArrayAttr({}));\n+  auto broadcast = stablehlo::BroadcastInDimOp::create(\n+      b, returnType, cst, b.getDenseI64ArrayAttr({}));\n   if (auto sharding = op->getAttrOfType<mlir::StringAttr>(kShardingAttr)) {\n     // The added broadcast inherits the kShardingAttr from op.\n     broadcast->setAttr(kShardingAttr, sharding);\n@@ -103,8 +103,8 @@ static void prepareBroadcastInDim(stablehlo::BroadcastInDimOp bcast) {\n   llvm::sort(transposedDim,\n              [&](int64_t lhs, int64_t rhs) { return dims[lhs] < dims[rhs]; });\n   OpBuilder builder(bcast);\n-  bcast.setOperand(builder.create<stablehlo::TransposeOp>(\n-      bcast.getLoc(), bcast.getOperand(),\n+  bcast.setOperand(stablehlo::TransposeOp::create(\n+      builder, bcast.getLoc(), bcast.getOperand(),\n       mlir::DenseI64ArrayAttr::get(builder.getContext(), transposedDim)));\n   // Now reuse the original broadcast_dimensions and sort it.\n   transposedDim.assign(dims.begin(), dims.end());"
        },
        {
            "sha": "24288b5613acd6ca17d4d2cac220880068f7b0fa",
            "filename": "third_party/xla/xla/mlir_hlo/transforms/alloc_to_arg_pass.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Falloc_to_arg_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Falloc_to_arg_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Falloc_to_arg_pass.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -92,8 +92,8 @@ void AllocToArgPass::runOnOperation() {\n         // buffer.\n         rewriter.setInsertionPoint(allocOp);\n         Value arg = funcOp.getArguments().back();\n-        Value collapsedArg = rewriter.create<memref::CollapseShapeOp>(\n-            loc, arg, expandOp.getReassociationIndices());\n+        Value collapsedArg = memref::CollapseShapeOp::create(\n+            rewriter, loc, arg, expandOp.getReassociationIndices());\n \n         // Replace alloc and its expansion.\n         rewriter.replaceOp(allocOp, collapsedArg);"
        },
        {
            "sha": "73956da6a2587465907a4294cac1506ab12ae3d1",
            "filename": "third_party/xla/xla/mlir_hlo/transforms/bufferize.cc",
            "status": "modified",
            "additions": 142,
            "deletions": 143,
            "changes": 285,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Fbufferize.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Fbufferize.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Fbufferize.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -55,21 +55,21 @@ struct BufferizeConstantOp : public OpConversionPattern<arith::ConstantOp> {\n     // TODO(kramerb): Should this use materializeConstant instead?\n     auto makeConstant = [&](Attribute attr, Type type) -> Value {\n       if (complex::ConstantOp::isBuildableWith(attr, type))\n-        return rewriter.create<complex::ConstantOp>(\n-            loc, type, mlir::cast<ArrayAttr>(attr));\n-      return rewriter.create<arith::ConstantOp>(loc, cast<TypedAttr>(attr));\n+        return complex::ConstantOp::create(rewriter, loc, type,\n+                                           mlir::cast<ArrayAttr>(attr));\n+      return arith::ConstantOp::create(rewriter, loc, cast<TypedAttr>(attr));\n     };\n \n     if (resultRank == 0) {\n-      Value buffer = rewriter.create<memref::AllocOp>(loc, memrefType);\n+      Value buffer = memref::AllocOp::create(rewriter, loc, memrefType);\n       Value constant =\n           makeConstant(elementsAttr.getValues<Attribute>()[0], elementType);\n-      rewriter.create<memref::StoreOp>(loc, constant, buffer);\n+      memref::StoreOp::create(rewriter, loc, constant, buffer);\n       rewriter.replaceOp(op, {buffer});\n       return success();\n     }\n \n-    Value buffer = rewriter.create<memref::AllocaOp>(loc, memrefType);\n+    Value buffer = memref::AllocaOp::create(rewriter, loc, memrefType);\n \n     bool allSameElems = elementsAttr.isSplat();\n     Value value;\n@@ -79,8 +79,8 @@ struct BufferizeConstantOp : public OpConversionPattern<arith::ConstantOp> {\n     for (const auto &en :\n          llvm::enumerate(elementsAttr.getValues<Attribute>())) {\n       if (!allSameElems) value = makeConstant(en.value(), elementType);\n-      Value index = rewriter.create<arith::ConstantIndexOp>(loc, en.index());\n-      rewriter.create<memref::StoreOp>(loc, value, buffer, index);\n+      Value index = arith::ConstantIndexOp::create(rewriter, loc, en.index());\n+      memref::StoreOp::create(rewriter, loc, value, buffer, index);\n     }\n     rewriter.replaceOp(op, {buffer});\n     return success();\n@@ -97,7 +97,7 @@ struct BufferizeAndConvertMinimumBroadcastShapesOp\n       ConversionPatternRewriter &rewriter) const override {\n     auto loc = broadcastShapesOp.getLoc();\n     ImplicitLocOpBuilder lb(loc, rewriter);\n-    Value zero = lb.create<arith::ConstantIndexOp>(0);\n+    Value zero = arith::ConstantIndexOp::create(lb, 0);\n     SmallVector<Value> shapes = adaptor.getShapes();\n     size_t k = shapes.size();\n     SmallVector<Value> ranks;\n@@ -106,12 +106,12 @@ struct BufferizeAndConvertMinimumBroadcastShapesOp\n     // Determine the maximum rank of the operands.\n     Value maxRank;\n     for (size_t i = 0; i < k; ++i) {\n-      Value rank = lb.create<memref::DimOp>(loc, shapes[i], zero);\n+      Value rank = memref::DimOp::create(lb, loc, shapes[i], zero);\n       ranks.push_back(rank);\n       if (i) {\n-        Value rankIsGreater = lb.create<arith::CmpIOp>(\n-            arith::CmpIPredicate::ugt, ranks[i], maxRank);\n-        maxRank = lb.create<arith::SelectOp>(rankIsGreater, ranks[i], maxRank);\n+        Value rankIsGreater = arith::CmpIOp::create(\n+            lb, arith::CmpIPredicate::ugt, ranks[i], maxRank);\n+        maxRank = arith::SelectOp::create(lb, rankIsGreater, ranks[i], maxRank);\n       } else {\n         maxRank = ranks[0];\n       }\n@@ -122,17 +122,17 @@ struct BufferizeAndConvertMinimumBroadcastShapesOp\n     resultShapes.reserve(k);\n     auto resultType =\n         MemRefType::get({ShapedType::kDynamic}, lb.getIndexType());\n-    Value one = lb.create<arith::ConstantIndexOp>(1);\n+    Value one = arith::ConstantIndexOp::create(lb, 1);\n     for (size_t i = 0; i < k; ++i) {\n       // We assume the buffer will be small, so we allocate it on the stack.\n       // TODO(b/181654096): Replace AllocaOp with AllocOp.\n-      auto result = lb.create<memref::AllocaOp>(resultType, ranks[i]);\n-      lb.create<scf::ForOp>(zero, ranks[i], one, mlir::ValueRange(),\n-                            [&one, &result](OpBuilder& b, Location l, Value idx,\n-                                            ValueRange /*vr*/) {\n-                              b.create<memref::StoreOp>(l, one, result, idx);\n-                              b.create<scf::YieldOp>(l, mlir::ValueRange());\n-                            });\n+      auto result = memref::AllocaOp::create(lb, resultType, ranks[i]);\n+      scf::ForOp::create(lb, zero, ranks[i], one, mlir::ValueRange(),\n+                         [&one, &result](OpBuilder& b, Location l, Value idx,\n+                                         ValueRange /*vr*/) {\n+                           memref::StoreOp::create(b, l, one, result, idx);\n+                           scf::YieldOp::create(b, l, mlir::ValueRange());\n+                         });\n       resultShapes.push_back(result);\n     }\n \n@@ -143,10 +143,10 @@ struct BufferizeAndConvertMinimumBroadcastShapesOp\n     // backward, because the broadcasting semantics mean that the last\n     // dimensions of each shape (the least significant ones) are matched\n     // together.\n-    Value two = lb.create<arith::ConstantIndexOp>(2);\n-    Value maxRankPlusTwo = lb.create<arith::AddIOp>(loc, maxRank, two);\n+    Value two = arith::ConstantIndexOp::create(lb, 2);\n+    Value maxRankPlusTwo = arith::AddIOp::create(lb, loc, maxRank, two);\n     Value constantFalse =\n-        lb.create<arith::ConstantOp>(lb.getI1Type(), lb.getBoolAttr(false));\n+        arith::ConstantOp::create(lb, lb.getI1Type(), lb.getBoolAttr(false));\n     SmallVector<Value> initValues;\n     initValues.reserve(k + 3);\n     // Initially, all values are marked as not broadcasted.\n@@ -164,9 +164,9 @@ struct BufferizeAndConvertMinimumBroadcastShapesOp\n     // used as an offset from the end of each shape vector. We iterate until\n     // max_rank + 1 to handle the case that we have a running_product > 1 left\n     // when we have processed all dimensions of the largest shape.\n-    auto mainLoop = lb.create<scf::ForOp>(\n-        one, maxRankPlusTwo, one, initValues,\n-        [&](OpBuilder &b, Location l, Value v, ValueRange vr) {\n+    auto mainLoop = scf::ForOp::create(\n+        lb, one, maxRankPlusTwo, one, initValues,\n+        [&](OpBuilder& b, Location l, Value v, ValueRange vr) {\n           // 'same_size' should track what the size of the dimension is to which\n           // the 1-sized dimensions are broadcasted. If all of the dimensions\n           // are 1, it will stay 1.\n@@ -192,148 +192,147 @@ struct BufferizeAndConvertMinimumBroadcastShapesOp\n           for (size_t i = 0; i < k; ++i) {\n             // Determine the size of the current dimension. If the dimension is\n             // out of bounds, we choose the value 'one'.\n-            Value isOutOfBounds = b.create<arith::CmpIOp>(\n-                l, arith::CmpIPredicate::ult, ranks[i], v);\n-            Value dimension = b.create<arith::SubIOp>(l, ranks[i], v);\n+            Value isOutOfBounds = arith::CmpIOp::create(\n+                b, l, arith::CmpIPredicate::ult, ranks[i], v);\n+            Value dimension = arith::SubIOp::create(b, l, ranks[i], v);\n             resultDimensions.push_back(dimension);\n-            Value currentSize =\n-                b.create<scf::IfOp>(\n-                     l, isOutOfBounds,\n-                     [&](OpBuilder &b, Location l) {\n-                       b.create<scf::YieldOp>(l, one);\n-                     },\n-                     [&](OpBuilder &b, Location l) {\n-                       // Using IfOp instead of SelectOp makes sure that we\n-                       // don't try to load if the dimension is out of bounds.\n-                       Value size =\n-                           b.create<memref::LoadOp>(l, shapes[i], dimension);\n-                       b.create<scf::YieldOp>(l, size);\n-                     })\n-                    .getResult(0);\n+            Value currentSize = scf::IfOp::create(\n+                                    b, l, isOutOfBounds,\n+                                    [&](OpBuilder& b, Location l) {\n+                                      scf::YieldOp::create(b, l, one);\n+                                    },\n+                                    [&](OpBuilder& b, Location l) {\n+                                      // Using IfOp instead of SelectOp makes\n+                                      // sure that we don't try to load if the\n+                                      // dimension is out of bounds.\n+                                      Value size = memref::LoadOp::create(\n+                                          b, l, shapes[i], dimension);\n+                                      scf::YieldOp::create(b, l, size);\n+                                    })\n+                                    .getResult(0);\n             // Compute whether the current dimension does require broadcasting.\n-            Value currentSizeIsNotOne = b.create<arith::CmpIOp>(\n-                l, arith::CmpIPredicate::ne, currentSize, one);\n+            Value currentSizeIsNotOne = arith::CmpIOp::create(\n+                b, l, arith::CmpIPredicate::ne, currentSize, one);\n             noBroadcasting.push_back(currentSizeIsNotOne);\n-            Value newSameSize = b.create<arith::SelectOp>(\n-                l, currentSizeIsNotOne, currentSize, sameSize);\n-            Value sameSizeWasNotOne = b.create<arith::CmpIOp>(\n-                l, arith::CmpIPredicate::ne, sameSize, one);\n-            Value isDifferentSize = b.create<arith::CmpIOp>(\n-                l, arith::CmpIPredicate::ne, sameSize, newSameSize);\n+            Value newSameSize = arith::SelectOp::create(\n+                b, l, currentSizeIsNotOne, currentSize, sameSize);\n+            Value sameSizeWasNotOne = arith::CmpIOp::create(\n+                b, l, arith::CmpIPredicate::ne, sameSize, one);\n+            Value isDifferentSize = arith::CmpIOp::create(\n+                b, l, arith::CmpIPredicate::ne, sameSize, newSameSize);\n             // The broadcast is invalid if the size of the current dimension\n             // is not equal to the expected size, unless the expected size was\n             // still the initial value 1.\n             Value isInvalid =\n-                b.create<arith::AndIOp>(l, sameSizeWasNotOne, isDifferentSize);\n-            currentDimensionHasInvalidBroadcast = b.create<arith::OrIOp>(\n-                l, currentDimensionHasInvalidBroadcast, isInvalid);\n+                arith::AndIOp::create(b, l, sameSizeWasNotOne, isDifferentSize);\n+            currentDimensionHasInvalidBroadcast = arith::OrIOp::create(\n+                b, l, currentDimensionHasInvalidBroadcast, isInvalid);\n             sameSize = newSameSize;\n           }\n \n           // Check whether we have at least one shape that has a different\n           // status regarding whether it needs broadcasting at the current\n           // dimension versus whether it needs broadcasting at the previous\n           // dimension.\n-          Value sameSizeIsOne = b.create<arith::CmpIOp>(\n-              l, arith::CmpIPredicate::eq, sameSize, one);\n+          Value sameSizeIsOne = arith::CmpIOp::create(\n+              b, l, arith::CmpIPredicate::eq, sameSize, one);\n           Value differentBroadcastingSet = constantFalse;\n           for (size_t i = 0; i < k; ++i) {\n             // If all dimensions are 1, we preserve the status whether a shape\n             // needs broadcasting or not, because in that case the dimension can\n             // just be ignored.\n-            noBroadcasting[i] = b.create<arith::SelectOp>(\n-                l, sameSizeIsOne, prevNoBroadcasting[i], noBroadcasting[i]);\n+            noBroadcasting[i] = arith::SelectOp::create(\n+                b, l, sameSizeIsOne, prevNoBroadcasting[i], noBroadcasting[i]);\n             // Compare whether the current shape changes its status regarding\n             // whether it needs broadcasting at the current dimension.\n-            Value broadcastingIsDifferent = b.create<arith::CmpIOp>(\n-                l, arith::CmpIPredicate::ne, prevNoBroadcasting[i],\n-                noBroadcasting[i]);\n-            differentBroadcastingSet = b.create<arith::OrIOp>(\n-                l, differentBroadcastingSet, broadcastingIsDifferent);\n+            Value broadcastingIsDifferent =\n+                arith::CmpIOp::create(b, l, arith::CmpIPredicate::ne,\n+                                      prevNoBroadcasting[i], noBroadcasting[i]);\n+            differentBroadcastingSet = arith::OrIOp::create(\n+                b, l, differentBroadcastingSet, broadcastingIsDifferent);\n           }\n           Value runningProduct = vr[k];\n           Value currentDimensionOffset = vr[k + 1];\n \n           // We need to stop combining dimensions if the set of shapes which\n           // need broadcasting at the current dimension changes compared to the\n           // set of shapes needing broadcasting at the previous dimension.\n-          Value isLastIteration =\n-              b.create<arith::CmpIOp>(l, arith::CmpIPredicate::sgt, v, maxRank);\n-          Value stopCombiningDimensions = b.create<arith::OrIOp>(\n-              l, isLastIteration, differentBroadcastingSet);\n-          auto ifStopCombiningDimensions = b.create<scf::IfOp>(\n-              l, stopCombiningDimensions,\n-              [&](OpBuilder &b, Location l) {\n+          Value isLastIteration = arith::CmpIOp::create(\n+              b, l, arith::CmpIPredicate::sgt, v, maxRank);\n+          Value stopCombiningDimensions = arith::OrIOp::create(\n+              b, l, isLastIteration, differentBroadcastingSet);\n+          auto ifStopCombiningDimensions = scf::IfOp::create(\n+              b, l, stopCombiningDimensions,\n+              [&](OpBuilder& b, Location l) {\n                 // If the running product is not 1, add one dimension of size\n                 // 'running_product' to each shape that didn't need\n                 // broadcasting, otherwise add a 1 dimension if it was\n                 // previously indexed in-bounds.\n-                Value runningProductNotOne = b.create<arith::CmpIOp>(\n-                    l, arith::CmpIPredicate::ne, runningProduct, one);\n+                Value runningProductNotOne = arith::CmpIOp::create(\n+                    b, l, arith::CmpIPredicate::ne, runningProduct, one);\n                 Value newDimensionOffset =\n-                    b.create<scf::IfOp>(\n-                         l, runningProductNotOne,\n-                         [&](OpBuilder &b, Location l) {\n-                           Value newDimensionOffset = b.create<arith::AddIOp>(\n-                               l, currentDimensionOffset, one);\n-                           Value minusOne =\n-                               lb.create<arith::ConstantIndexOp>(-1);\n-                           for (size_t i = 0; i < k; ++i) {\n-                             Value wasInBounds = b.create<arith::CmpIOp>(\n-                                 l, arith::CmpIPredicate::sge,\n-                                 resultDimensions[i], minusOne);\n-                             Value shouldStoreDimension =\n-                                 b.create<arith::OrIOp>(l, wasInBounds,\n-                                                        prevNoBroadcasting[i]);\n-                             b.create<scf::IfOp>(\n-                                 l, shouldStoreDimension,\n-                                 [&](OpBuilder &b, Location l) {\n-                                   Value outputDimension =\n-                                       b.create<arith::SubIOp>(\n-                                           l, ranks[i], newDimensionOffset);\n-                                   // If the shape needed broadcasting at the\n-                                   // previous dimension, we set the output size\n-                                   // to 1, otherwise to 'running_product'.\n-                                   Value outputSize = b.create<arith::SelectOp>(\n-                                       l, prevNoBroadcasting[i], runningProduct,\n-                                       one);\n-                                   b.create<memref::StoreOp>(l, outputSize,\n-                                                             resultShapes[i],\n-                                                             outputDimension);\n-                                   b.create<scf::YieldOp>(l,\n-                                                          mlir::ValueRange());\n-                                 });\n-                           }\n-                           b.create<scf::YieldOp>(l, newDimensionOffset);\n-                         },\n-                         [&](OpBuilder &b, Location l) {\n-                           b.create<scf::YieldOp>(l, currentDimensionOffset);\n-                         })\n+                    scf::IfOp::create(\n+                        b, l, runningProductNotOne,\n+                        [&](OpBuilder& b, Location l) {\n+                          Value newDimensionOffset = arith::AddIOp::create(\n+                              b, l, currentDimensionOffset, one);\n+                          Value minusOne =\n+                              arith::ConstantIndexOp::create(lb, -1);\n+                          for (size_t i = 0; i < k; ++i) {\n+                            Value wasInBounds = arith::CmpIOp::create(\n+                                b, l, arith::CmpIPredicate::sge,\n+                                resultDimensions[i], minusOne);\n+                            Value shouldStoreDimension = arith::OrIOp::create(\n+                                b, l, wasInBounds, prevNoBroadcasting[i]);\n+                            scf::IfOp::create(\n+                                b, l, shouldStoreDimension,\n+                                [&](OpBuilder& b, Location l) {\n+                                  Value outputDimension = arith::SubIOp::create(\n+                                      b, l, ranks[i], newDimensionOffset);\n+                                  // If the shape needed broadcasting at the\n+                                  // previous dimension, we set the output size\n+                                  // to 1, otherwise to 'running_product'.\n+                                  Value outputSize = arith::SelectOp::create(\n+                                      b, l, prevNoBroadcasting[i],\n+                                      runningProduct, one);\n+                                  memref::StoreOp::create(b, l, outputSize,\n+                                                          resultShapes[i],\n+                                                          outputDimension);\n+                                  scf::YieldOp::create(b, l,\n+                                                       mlir::ValueRange());\n+                                });\n+                          }\n+                          scf::YieldOp::create(b, l, newDimensionOffset);\n+                        },\n+                        [&](OpBuilder& b, Location l) {\n+                          scf::YieldOp::create(b, l, currentDimensionOffset);\n+                        })\n                         .getResult(0);\n-                b.create<scf::YieldOp>(\n-                    l, ValueRange{sameSize, newDimensionOffset});\n+                scf::YieldOp::create(b, l,\n+                                     ValueRange{sameSize, newDimensionOffset});\n               },\n-              [&](OpBuilder &b, Location l) {\n+              [&](OpBuilder& b, Location l) {\n                 Value newRunningProduct =\n-                    b.create<arith::MulIOp>(l, runningProduct, sameSize);\n-                b.create<scf::YieldOp>(\n-                    l, ValueRange{newRunningProduct, currentDimensionOffset});\n+                    arith::MulIOp::create(b, l, runningProduct, sameSize);\n+                scf::YieldOp::create(\n+                    b, l,\n+                    ValueRange{newRunningProduct, currentDimensionOffset});\n               });\n           // Add the remaining results.\n           noBroadcasting.push_back(ifStopCombiningDimensions.getResult(0));\n           noBroadcasting.push_back(ifStopCombiningDimensions.getResult(1));\n           Value isInvalid = vr.back();\n-          isInvalid = b.create<arith::OrIOp>(\n-              l, isInvalid, currentDimensionHasInvalidBroadcast);\n+          isInvalid = arith::OrIOp::create(b, l, isInvalid,\n+                                           currentDimensionHasInvalidBroadcast);\n           noBroadcasting.push_back(isInvalid);\n-          b.create<scf::YieldOp>(l, noBroadcasting);\n+          scf::YieldOp::create(b, l, noBroadcasting);\n         });\n     Value isInvalid = mainLoop.getResults().back();\n     for (size_t i = 0; i < k; ++i) {\n       resultShapes[i] =\n           removeLeadingOnesFrom1DMemref(lb, resultShapes[i], ranks[i]);\n       resultShapes[i] =\n-          lb.create<arith::SelectOp>(isInvalid, shapes[i], resultShapes[i]);\n+          arith::SelectOp::create(lb, isInvalid, shapes[i], resultShapes[i]);\n     }\n     rewriter.replaceOp(broadcastShapesOp, resultShapes);\n     return success();\n@@ -346,28 +345,28 @@ struct BufferizeAndConvertMinimumBroadcastShapesOp\n     // boolean flag for whether every size so far was 1, one with the number of\n     // leading 1's.\n     Value constantTrue =\n-        lb.create<arith::ConstantOp>(lb.getI1Type(), lb.getBoolAttr(true));\n-    Value zero = lb.create<arith::ConstantIndexOp>(0);\n-    Value one = lb.create<arith::ConstantIndexOp>(1);\n-    auto leadingOnesLoop = lb.create<scf::ForOp>(\n-        zero, rank, one, ValueRange{constantTrue, zero},\n-        [&](OpBuilder &b, Location l, Value idx, ValueRange vr) {\n-          auto size = b.create<memref::LoadOp>(l, extentMemref, idx);\n+        arith::ConstantOp::create(lb, lb.getI1Type(), lb.getBoolAttr(true));\n+    Value zero = arith::ConstantIndexOp::create(lb, 0);\n+    Value one = arith::ConstantIndexOp::create(lb, 1);\n+    auto leadingOnesLoop = scf::ForOp::create(\n+        lb, zero, rank, one, ValueRange{constantTrue, zero},\n+        [&](OpBuilder& b, Location l, Value idx, ValueRange vr) {\n+          auto size = memref::LoadOp::create(b, l, extentMemref, idx);\n           auto isEqualToOne =\n-              b.create<arith::CmpIOp>(l, arith::CmpIPredicate::eq, size, one);\n-          auto allOnes = b.create<arith::AndIOp>(l, vr.front(), isEqualToOne);\n-          auto increasedValue = b.create<arith::AddIOp>(l, vr.back(), one);\n+              arith::CmpIOp::create(b, l, arith::CmpIPredicate::eq, size, one);\n+          auto allOnes = arith::AndIOp::create(b, l, vr.front(), isEqualToOne);\n+          auto increasedValue = arith::AddIOp::create(b, l, vr.back(), one);\n           auto numberOfLeadingOnes =\n-              b.create<arith::SelectOp>(l, allOnes, increasedValue, vr.back());\n-          b.create<scf::YieldOp>(l, ValueRange{allOnes, numberOfLeadingOnes});\n+              arith::SelectOp::create(b, l, allOnes, increasedValue, vr.back());\n+          scf::YieldOp::create(b, l, ValueRange{allOnes, numberOfLeadingOnes});\n         });\n     return leadingOnesLoop.getResults()[1];\n   }\n \n   Value removeLeadingOnesFrom1DMemref(ImplicitLocOpBuilder &lb,\n                                       Value extentMemref, Value rank) const {\n     Value leadingOnes = countLeadingOnes(lb, extentMemref, rank);\n-    Value newRank = lb.create<arith::SubIOp>(rank, leadingOnes);\n+    Value newRank = arith::SubIOp::create(lb, rank, leadingOnes);\n     auto resultType =\n         MemRefType::get({ShapedType::kDynamic}, lb.getIndexType());\n     // We cannot use SubView here to return a MemRef with 'leading_ones' as\n@@ -377,16 +376,16 @@ struct BufferizeAndConvertMinimumBroadcastShapesOp\n     // another buffer of the desired size and copy the elements over. We assume\n     // the buffer will be small, so we allocate it on the stack.\n     // TODO(b/181654096): Replace AllocaOp with AllocOp.\n-    Value result = lb.create<memref::AllocaOp>(resultType, newRank);\n-    Value zero = lb.create<arith::ConstantIndexOp>(0);\n-    Value one = lb.create<arith::ConstantIndexOp>(1);\n-    lb.create<scf::ForOp>(\n-        zero, newRank, one, mlir::ValueRange(),\n+    Value result = memref::AllocaOp::create(lb, resultType, newRank);\n+    Value zero = arith::ConstantIndexOp::create(lb, 0);\n+    Value one = arith::ConstantIndexOp::create(lb, 1);\n+    scf::ForOp::create(\n+        lb, zero, newRank, one, mlir::ValueRange(),\n         [&](OpBuilder& b, Location l, Value idx, ValueRange /*vr*/) {\n-          Value idxWithOffset = b.create<arith::AddIOp>(l, idx, leadingOnes);\n-          auto size = b.create<memref::LoadOp>(l, extentMemref, idxWithOffset);\n-          b.create<memref::StoreOp>(l, size, result, idx);\n-          b.create<scf::YieldOp>(l, mlir::ValueRange());\n+          Value idxWithOffset = arith::AddIOp::create(b, l, idx, leadingOnes);\n+          auto size = memref::LoadOp::create(b, l, extentMemref, idxWithOffset);\n+          memref::StoreOp::create(b, l, size, result, idx);\n+          scf::YieldOp::create(b, l, mlir::ValueRange());\n         });\n     return result;\n   }"
        },
        {
            "sha": "fa572362080eaa3dfaa7d61310e651ceb7a2689b",
            "filename": "third_party/xla/xla/mlir_hlo/transforms/bufferize_pass.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Fbufferize_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Fbufferize_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Fbufferize_pass.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -93,7 +93,7 @@ static Value materializeToTensor(OpBuilder& builder, TensorType type,\n                                  ValueRange inputs, Location loc) {\n   assert(inputs.size() == 1);\n   assert(mlir::isa<BaseMemRefType>(inputs[0].getType()));\n-  return builder.create<bufferization::ToTensorOp>(loc, type, inputs[0]);\n+  return bufferization::ToTensorOp::create(builder, loc, type, inputs[0]);\n }\n \n // TODO(pifon): Remove as soon as https://reviews.llvm.org/D93126 is landed.\n@@ -129,7 +129,7 @@ class CustomBufferizeTypeConverter : public mlir::TypeConverter {\n       }\n       if (isa<TensorType>(inputs[0].getType())) {\n         // Tensor to MemRef cast.\n-        return builder.create<bufferization::ToBufferOp>(loc, type, inputs[0]);\n+        return bufferization::ToBufferOp::create(builder, loc, type, inputs[0]);\n       }\n       llvm_unreachable(\"only tensor/memref input types supported\");\n     });\n@@ -146,7 +146,7 @@ class CustomBufferizeTypeConverter : public mlir::TypeConverter {\n         return inputs[0];\n       }\n       assert(mlir::isa<TensorType>(inputs[0].getType()));\n-      return builder.create<bufferization::ToBufferOp>(loc, type, inputs[0]);\n+      return bufferization::ToBufferOp::create(builder, loc, type, inputs[0]);\n     });\n   }\n };"
        },
        {
            "sha": "402fd7b112c43288a811dda162ccf31210d02831",
            "filename": "third_party/xla/xla/mlir_hlo/transforms/detensorize_scf_ops.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Fdetensorize_scf_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Fdetensorize_scf_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Fdetensorize_scf_ops.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -62,7 +62,7 @@ struct RegionOpPattern : public OpRewritePattern<T> {\n     ImplicitLocOpBuilder b(op.getLoc(), rewriter);\n     b.setInsertionPoint(result);\n     for (auto [index, operand] : unitTensors(result->getOperands())) {\n-      result->setOperand(index, b.create<tensor::ExtractOp>(operand));\n+      result->setOperand(index, tensor::ExtractOp::create(b, operand));\n     }\n \n     // Fix any block arguments in the op. We're detensorizing all arguments that\n@@ -76,8 +76,8 @@ struct RegionOpPattern : public OpRewritePattern<T> {\n           // Change the argument type to a scalar, but repack it into a tensor.\n           arg.setType(\n               mlir::cast<RankedTensorType>(arg.getType()).getElementType());\n-          auto converted = b.create<tensor::FromElementsOp>(\n-              RankedTensorType::get({}, arg.getType()), arg);\n+          auto converted = tensor::FromElementsOp::create(\n+              b, RankedTensorType::get({}, arg.getType()), arg);\n           arg.replaceAllUsesExcept(converted, converted.getOperation());\n         }\n \n@@ -86,7 +86,7 @@ struct RegionOpPattern : public OpRewritePattern<T> {\n              unitTensors(block.getTerminator()->getOperands())) {\n           b.setInsertionPoint(block.getTerminator());\n           block.getTerminator()->setOperand(\n-              index, b.create<tensor::ExtractOp>(operand));\n+              index, tensor::ExtractOp::create(b, operand));\n         }\n       }\n     }\n@@ -99,7 +99,7 @@ struct RegionOpPattern : public OpRewritePattern<T> {\n       opResult.setType(oldType.getElementType());\n \n       // Convert the scalar back to a tensor in the output.\n-      results[index] = b.create<tensor::FromElementsOp>(oldType, opResult);\n+      results[index] = tensor::FromElementsOp::create(b, oldType, opResult);\n     }\n     rewriter.replaceOp(op.getOperation(), results);\n     return success();"
        },
        {
            "sha": "0022fb5dfdc2134985f6b9a5dd47f585afc6dc13",
            "filename": "third_party/xla/xla/mlir_hlo/transforms/lower_index_cast_pass.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Flower_index_cast_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Flower_index_cast_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Flower_index_cast_pass.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -48,10 +48,10 @@ struct IndexCastConverter : public OpRewritePattern<T> {\n         tensor::createDynamicDimValues(rewriter, op.getLoc(), op.getIn());\n     rewriter.replaceOpWithNewOp<tensor::GenerateOp>(\n         op, resultTy, dynamicExtents,\n-        [&](OpBuilder &b, Location loc, ValueRange args) {\n-          Value extent = b.create<tensor::ExtractOp>(loc, op.getIn(), args);\n-          Value cast = b.create<T>(loc, resultTy.getElementType(), extent);\n-          b.create<tensor::YieldOp>(loc, cast);\n+        [&](OpBuilder& b, Location loc, ValueRange args) {\n+          Value extent = tensor::ExtractOp::create(b, loc, op.getIn(), args);\n+          Value cast = T::create(b, loc, resultTy.getElementType(), extent);\n+          tensor::YieldOp::create(b, loc, cast);\n         });\n     return success();\n   }"
        },
        {
            "sha": "8d624f3afd8f31227f9b8e90466796ee132ca041",
            "filename": "third_party/xla/xla/mlir_hlo/transforms/tile_loops_pass.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Ftile_loops_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Ftile_loops_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Ftile_loops_pass.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -116,7 +116,7 @@ void TileLoopsPass::runOnOperation() {\n       int64_t difference = upper[i].value() - lower[i].value();\n       if (difference % (step[i].value() * unrollFactor) != 0) continue;\n       ploop.getUpperBoundMutable().slice(i, 1).assign(\n-          builder.create<arith::ConstantIndexOp>(loc, unrollFactor));\n+          arith::ConstantIndexOp::create(builder, loc, unrollFactor));\n     }\n   }\n "
        },
        {
            "sha": "28bdded6ed837623a640057a35fc48454a6abc46",
            "filename": "third_party/xla/xla/mlir_hlo/transforms/vectorize_copy.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Fvectorize_copy.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Fvectorize_copy.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftransforms%2Fvectorize_copy.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -88,7 +88,7 @@ struct TileCopyPattern : public OpRewritePattern<memref::CopyOp> {\n                          targetType.getNumElements() <= tileSize;\n \n     if (isContiguous || isSmall) {\n-      rewriter.create<memref::CopyOp>(loc, src, target);\n+      memref::CopyOp::create(rewriter, loc, src, target);\n       return;\n     }\n \n@@ -99,14 +99,14 @@ struct TileCopyPattern : public OpRewritePattern<memref::CopyOp> {\n     const int64_t remainderSize = dimSize % sliceSize;\n     const int64_t upperBound = shape[dim] - remainderSize;\n \n-    Value zero = rewriter.create<arith::ConstantIndexOp>(loc, 0);\n+    Value zero = arith::ConstantIndexOp::create(rewriter, loc, 0);\n     Value tileSizeValue =\n-        rewriter.create<arith::ConstantIndexOp>(loc, sliceSize);\n+        arith::ConstantIndexOp::create(rewriter, loc, sliceSize);\n     Value upperBoundValue =\n-        rewriter.create<arith::ConstantIndexOp>(loc, upperBound);\n+        arith::ConstantIndexOp::create(rewriter, loc, upperBound);\n \n-    auto loop = rewriter.create<scf::ForOp>(loc, zero, upperBoundValue,\n-                                            tileSizeValue, target);\n+    auto loop = scf::ForOp::create(rewriter, loc, zero, upperBoundValue,\n+                                   tileSizeValue, target);\n \n     OpBuilder::InsertionGuard g(rewriter);\n     rewriter.setInsertionPointToStart(loop.getBody());\n@@ -123,7 +123,7 @@ struct TileCopyPattern : public OpRewritePattern<memref::CopyOp> {\n     createLoopsNest(rewriter, loc, dim + 1, srcSubview, targetSubview, shape,\n                     offsets, sizes, strides);\n \n-    rewriter.create<scf::YieldOp>(loc, loop.getRegionIterArgs()[0]);\n+    scf::YieldOp::create(rewriter, loc, loop.getRegionIterArgs()[0]);\n \n     // Remainder copy can only be created for the innermost loop, for other\n     // loops remainder size is guaranteed to be 0.\n@@ -138,8 +138,8 @@ struct TileCopyPattern : public OpRewritePattern<memref::CopyOp> {\n       Value targetRemainderSubview =\n           getSubView(rewriter, loc, target, shape, offsets, sizes, strides);\n \n-      rewriter.create<memref::CopyOp>(loc, srcRemainderSubview,\n-                                      targetRemainderSubview);\n+      memref::CopyOp::create(rewriter, loc, srcRemainderSubview,\n+                             targetRemainderSubview);\n     }\n   }\n \n@@ -154,8 +154,8 @@ struct TileCopyPattern : public OpRewritePattern<memref::CopyOp> {\n         cast<MemRefType>(memref::SubViewOp::inferRankReducedResultType(\n             shape, valType, offsets, sizes, strides));\n \n-    return rewriter.create<memref::SubViewOp>(loc, valSubviewType, val, offsets,\n-                                              sizes, strides);\n+    return memref::SubViewOp::create(rewriter, loc, valSubviewType, val,\n+                                     offsets, sizes, strides);\n   }\n \n   int64_t tileSize;"
        },
        {
            "sha": "3faf853f49cf110fd58a5d5fa34efbb1dac658b6",
            "filename": "third_party/xla/xla/service/spmd/shardy/round_trip_common/open_while_free_vars_sharding.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fopen_while_free_vars_sharding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fopen_while_free_vars_sharding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fopen_while_free_vars_sharding.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -65,9 +65,8 @@ class OpenWhileFreeVarsShardingPass\n           // a sharding constraint.\n           continue;\n         }\n-        auto shardingConstraint =\n-            rewriter.create<mlir::sdy::ShardingConstraintOp>(\n-                freeVar.getLoc(), freeVar, fullyOpenSharding);\n+        auto shardingConstraint = mlir::sdy::ShardingConstraintOp::create(\n+            rewriter, freeVar.getLoc(), freeVar, fullyOpenSharding);\n         // Only replace uses in the regions of the while op.\n         rewriter.replaceUsesWithIf(\n             freeVar, shardingConstraint, [op](mlir::OpOperand& use) {"
        },
        {
            "sha": "e2c26b649550bc9aecb951fe1aa36f594d0313b5",
            "filename": "third_party/xla/xla/service/spmd/shardy/stablehlo_round_trip/export_callback_custom_calls.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_callback_custom_calls.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_callback_custom_calls.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_callback_custom_calls.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -59,8 +59,8 @@ void replaceCallbackWithTupleVersion(CustomCallOp customCall) {\n       mlir::TupleType::get(customCall->getContext(),\n                            {customCall->getResultTypes()}),\n       rewriter);\n-  auto getTupleElement = rewriter.create<mlir::stablehlo::GetTupleElementOp>(\n-      customCall.getLoc(), customCall->getResultTypes().front(),\n+  auto getTupleElement = mlir::stablehlo::GetTupleElementOp::create(\n+      rewriter, customCall.getLoc(), customCall->getResultTypes().front(),\n       tupleCustomCall.getResult(0), rewriter.getI32IntegerAttr(0));\n   getTupleElement->setAttr(kXlaShardingAttr,\n                            customCall->getAttr(kXlaShardingAttr));"
        },
        {
            "sha": "91f44339a22565e5b3ddd3130f4fcb7ab7487b4f",
            "filename": "third_party/xla/xla/service/spmd/shardy/utils.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Futils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/50c19ba0223294678c289f822db9d4bcaa640428/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Futils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Futils.cc?ref=50c19ba0223294678c289f822db9d4bcaa640428",
            "patch": "@@ -260,12 +260,12 @@ void adjustOutputSharding(\n CustomCallOp cloneCustomCallWithNewResultTypes(CustomCallOp op,\n                                                mlir::TypeRange resultTypes,\n                                                mlir::IRRewriter& rewriter) {\n-  auto customCallOp = rewriter.create<CustomCallOp>(\n-      op.getLoc(), resultTypes, op.getOperands(), op.getCallTargetNameAttr(),\n-      op.getHasSideEffectAttr(), op.getBackendConfigAttr(),\n-      op.getApiVersionAttr(), op.getCalledComputations(),\n-      op.getOperandLayoutsAttr(), op.getResultLayoutsAttr(),\n-      op.getOutputOperandAliases());\n+  auto customCallOp = CustomCallOp::create(\n+      rewriter, op.getLoc(), resultTypes, op.getOperands(),\n+      op.getCallTargetNameAttr(), op.getHasSideEffectAttr(),\n+      op.getBackendConfigAttr(), op.getApiVersionAttr(),\n+      op.getCalledComputations(), op.getOperandLayoutsAttr(),\n+      op.getResultLayoutsAttr(), op.getOutputOperandAliases());\n   customCallOp->setDiscardableAttrs(mlir::DictionaryAttr::get(\n       op->getContext(), llvm::to_vector(op->getDiscardableAttrs())));\n   return customCallOp;"
        }
    ],
    "stats": {
        "total": 1021,
        "additions": 520,
        "deletions": 501
    }
}