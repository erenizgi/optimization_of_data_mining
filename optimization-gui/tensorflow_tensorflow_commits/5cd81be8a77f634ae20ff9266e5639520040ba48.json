{
    "author": "basioli-k",
    "message": "[XLA][codegen] Migrate triton specific operations from collective emitter to xtile\n\nSplat is already implemented and has lowerings to triton implemented.\n\nPiperOrigin-RevId: 836879016",
    "sha": "5cd81be8a77f634ae20ff9266e5639520040ba48",
    "files": [
        {
            "sha": "e1ee5bc8e9e8e3c31d8c67b5dc4b4b7ae2db7c53",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5cd81be8a77f634ae20ff9266e5639520040ba48/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5cd81be8a77f634ae20ff9266e5639520040ba48/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=5cd81be8a77f634ae20ff9266e5639520040ba48",
            "patch": "@@ -145,6 +145,7 @@ cc_library(\n         \"@llvm-project//mlir:MathDialect\",\n         \"@llvm-project//mlir:Support\",\n         \"@llvm-project//mlir:TensorDialect\",\n+        \"@stablehlo//:stablehlo_ops\",\n         \"@triton//:TritonDialects\",\n     ],\n )\n@@ -972,6 +973,7 @@ cc_library(\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:NVVMDialect\",\n         \"@llvm-project//mlir:Support\",\n+        \"@llvm-project//mlir:TensorDialect\",\n         \"@triton//:TritonDialects\",\n     ],\n )"
        },
        {
            "sha": "9a26638a303365cefd29e7b9be465bc9b3346240",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5cd81be8a77f634ae20ff9266e5639520040ba48/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5cd81be8a77f634ae20ff9266e5639520040ba48/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc?ref=5cd81be8a77f634ae20ff9266e5639520040ba48",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"llvm/Support/MathExtras.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/Types.h\"\n@@ -235,7 +236,7 @@ absl::StatusOr<TensorValue> EmitAllReduce(\n   mlir::Value accumulator_zero =\n       arith::ConstantOp::create(b, elem_type, b.getZeroAttr(elem_type));\n   TensorValue accumulator =\n-      ttir::SplatOp::create(b, input_tile.getType(), accumulator_zero);\n+      triton::Splat(b, accumulator_zero, input_tile.getType().getShape());\n   for (int rank = 0; rank < world_size; ++rank) {\n     Value rank_idx =\n         arith::ConstantOp::create(b, b.getI64Type(), b.getI64IntegerAttr(rank));"
        },
        {
            "sha": "0391c76a2d3274d2713f2bfeb5ee1343966beb2c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5cd81be8a77f634ae20ff9266e5639520040ba48/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5cd81be8a77f634ae20ff9266e5639520040ba48/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=5cd81be8a77f634ae20ff9266e5639520040ba48",
            "patch": "@@ -47,6 +47,7 @@ limitations under the License.\n #include \"mlir/IR/Value.h\"\n #include \"mlir/IR/ValueRange.h\"\n #include \"mlir/Support/LLVM.h\"\n+#include \"stablehlo/dialect/StablehloOps.h\"\n #include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/backends/gpu/codegen/triton/tma_utils.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n@@ -813,4 +814,25 @@ absl::StatusOr<TensorValue> EmitScope(\n   return values[instructions.back()];\n }\n \n+TensorValue BroadcastInDims(EmitterLocOpBuilder b, TensorValue value,\n+                            ArrayRef<int64_t> output_shape,\n+                            ArrayRef<int64_t> dims) {\n+  CHECK(llvm::is_sorted(dims)) << \"broadcast dims must be sorted\";\n+\n+  auto result_type = mlir::RankedTensorType::get(\n+      output_shape, value.getType().getElementType());\n+\n+  return mlir::stablehlo::BroadcastInDimOp::create(b, result_type, value, dims);\n+}\n+\n+TensorValue Splat(EmitterLocOpBuilder b, Value value,\n+                  ArrayRef<int64_t> output_shape) {\n+  auto tensor_value = mlir::dyn_cast<TensorValue>(value);\n+  if (!tensor_value) {\n+    tensor_value = mlir::tensor::FromElementsOp::create(\n+        b, mlir::RankedTensorType::get({}, value.getType()), value);\n+  }\n+  return BroadcastInDims(b, tensor_value, output_shape, /*dims=*/{});\n+}\n+\n }  // namespace xla::gpu::triton"
        },
        {
            "sha": "699482dc7ae6e5b99a76e33e2fd80a129e773935",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.h",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5cd81be8a77f634ae20ff9266e5639520040ba48/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5cd81be8a77f634ae20ff9266e5639520040ba48/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h?ref=5cd81be8a77f634ae20ff9266e5639520040ba48",
            "patch": "@@ -270,6 +270,15 @@ absl::StatusOr<TensorValue> EmitScope(\n     absl::Span<const HloInstruction* const> instructions,\n     absl::flat_hash_map<const HloInstruction*, TensorValue>& values);\n \n+// Same as HLO BroadcastInDims. The sorted indices in `dims` specify the\n+// mapping of the input dimensions to the output dimensions.\n+TensorValue BroadcastInDims(EmitterLocOpBuilder b, TensorValue value,\n+                            ::mlir::ArrayRef<int64_t> output_shape,\n+                            ::mlir::ArrayRef<int64_t> dims);\n+\n+TensorValue Splat(EmitterLocOpBuilder b, ::mlir::Value value,\n+                  ::mlir::ArrayRef<int64_t> output_shape);\n+\n }  // namespace xla::gpu::triton\n \n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_EMITTER_HELPERS_H_"
        },
        {
            "sha": "d073feb29e8991e73c8bd58fd622100180249e83",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 38,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5cd81be8a77f634ae20ff9266e5639520040ba48/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5cd81be8a77f634ae20ff9266e5639520040ba48/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=5cd81be8a77f634ae20ff9266e5639520040ba48",
            "patch": "@@ -190,29 +190,6 @@ Value MakeIndex(EmitterLocOpBuilder& b, int64_t value) {\n   return arith::ConstantIndexOp::create(b, value);\n }\n \n-// Same as HLO BroadcastInDims. The sorted indices in `dims` specify the mapping\n-// of the input dimensions to the output dimensions.\n-TensorValue BroadcastInDims(EmitterLocOpBuilder b, TensorValue value,\n-                            ArrayRef<int64_t> output_shape,\n-                            ArrayRef<int64_t> dims) {\n-  CHECK(llvm::is_sorted(dims)) << \"broadcast dims must be sorted\";\n-\n-  auto result_type = mlir::RankedTensorType::get(\n-      output_shape, value.getType().getElementType());\n-\n-  return stablehlo::BroadcastInDimOp::create(b, result_type, value, dims);\n-}\n-\n-TensorValue Splat(EmitterLocOpBuilder b, Value value,\n-                  ArrayRef<int64_t> output_shape) {\n-  auto tensor_value = mlir::dyn_cast<TensorValue>(value);\n-  if (!tensor_value) {\n-    tensor_value = mlir::tensor::FromElementsOp::create(\n-        b, mlir::RankedTensorType::get({}, value.getType()), value);\n-  }\n-  return BroadcastInDims(b, tensor_value, output_shape, /*dims=*/{});\n-}\n-\n TensorValue Iota(EmitterLocOpBuilder b, int32_t limit) {\n   auto type = mlir::RankedTensorType::get(limit, b.getI32Type());\n   return stablehlo::IotaOp::create(b, type, /*iota_dimension=*/0);\n@@ -323,8 +300,9 @@ TensorValue EmitTiledBroadcast(\n       GetPaddedTileSizes(output_tile_shape);\n \n   TensorValue input = values[tiled_broadcast.operand(0)];\n-  return BroadcastInDims(b, input, padded_output_tile_shape,\n-                         MakeArrayRef(tiled_broadcast.hlo()->dimensions()));\n+  return triton::BroadcastInDims(\n+      b, input, padded_output_tile_shape,\n+      MakeArrayRef(tiled_broadcast.hlo()->dimensions()));\n }\n \n absl::StatusOr<TensorValue> EmitTiledIota(\n@@ -350,13 +328,14 @@ absl::StatusOr<TensorValue> EmitTiledIota(\n   // First, stride as needed between the iota components.\n   Value range = arith::MulIOp::create(\n       b, Iota(b, padded_tile_sizes[iota_dim]),\n-      Splat(b,\n-            CreateConst(b, b.getI32Type(), tiled_iota.tile_strides()[iota_dim]),\n-            padded_tile_sizes[iota_dim]));\n+      triton::Splat(\n+          b,\n+          CreateConst(b, b.getI32Type(), tiled_iota.tile_strides()[iota_dim]),\n+          padded_tile_sizes[iota_dim]));\n \n   // Then, add the base offset to the iota components.\n   range = arith::AddIOp::create(\n-      b, range, Splat(b, iota_dim_offset, padded_tile_sizes[iota_dim]));\n+      b, range, triton::Splat(b, iota_dim_offset, padded_tile_sizes[iota_dim]));\n \n   // Cast the result to the targeted type.\n   TF_ASSIGN_OR_RETURN(Type iota_element_type,\n@@ -366,8 +345,9 @@ absl::StatusOr<TensorValue> EmitTiledIota(\n \n   // And finally, produce a broadcast along the non-iota dimensions in order to\n   // produce the whole iota tile.\n-  return BroadcastInDims(b, mlir::cast<TensorValue>(range), padded_tile_sizes,\n-                         /*dims=*/{iota_dim});\n+  return triton::BroadcastInDims(b, mlir::cast<TensorValue>(range),\n+                                 padded_tile_sizes,\n+                                 /*dims=*/{iota_dim});\n }\n \n SmallVector<Value> GetRuntimeValues(\n@@ -572,7 +552,8 @@ absl::StatusOr<TensorValue> MaskDotOperand(\n       Value tile_offset = arith::MulIOp::create(\n           b, contracting_dimension_tile_index, tile_size_value);\n       TensorValue range = Iota(b, tile_size);\n-      TensorValue broadcasted_tile_offset = Splat(b, tile_offset, {tile_size});\n+      TensorValue broadcasted_tile_offset =\n+          triton::Splat(b, tile_offset, {tile_size});\n       Value indices = arith::AddIOp::create(b, range, broadcasted_tile_offset);\n \n       Value boundary = CreateConst(b, b.getI32Type(),\n@@ -581,8 +562,8 @@ absl::StatusOr<TensorValue> MaskDotOperand(\n       Value mask = arith::CmpIOp::create(b, arith::CmpIPredicate::slt, indices,\n                                          boundary);\n \n-      mask = BroadcastInDims(b, mlir::cast<TensorValue>(mask), tile_shape,\n-                             {contraction_dimension_index});\n+      mask = triton::BroadcastInDims(b, mlir::cast<TensorValue>(mask),\n+                                     tile_shape, {contraction_dimension_index});\n       TF_ASSIGN_OR_RETURN(\n           auto element_type,\n           TritonType(b, dot_operand.hlo()->shape().element_type()));\n@@ -1134,14 +1115,15 @@ absl::StatusOr<TensorValue> EmitPad(\n \n     // LHS for the compare is an iota broadcasted to the output shape.\n     TensorValue range = Iota(b, pad_output_dim_size);\n-    TensorValue bcast = BroadcastInDims(b, range, padded_tile_sizes,\n-                                        {static_cast<int64_t>(dim_index)});\n+    TensorValue bcast = triton::BroadcastInDims(\n+        b, range, padded_tile_sizes, {static_cast<int64_t>(dim_index)});\n \n     // RHS for the compare is splat(pad_input_dim_size - tile_offset).\n     Value tile_offset_i32 = Cast(b, tile_offset, i32_type);\n     Value threshold = arith::SubIOp::create(\n         b, CreateConst(b, i32_type, pad_input_dim_size), tile_offset_i32);\n-    TensorValue threshold_splat = Splat(b, threshold, padded_tile_sizes);\n+    TensorValue threshold_splat =\n+        triton::Splat(b, threshold, padded_tile_sizes);\n     Value cmp = arith::CmpIOp::create(b, arith::CmpIPredicate::slt, bcast,\n                                       threshold_splat);\n     mask = mask ? arith::AndIOp::create(b, mask, cmp) : cmp;\n@@ -1152,7 +1134,7 @@ absl::StatusOr<TensorValue> EmitPad(\n   const TiledHloInstruction* padding_value = tiled_pad.operand(1);\n \n   TensorValue pad_value_splat =\n-      Splat(b, values[padding_value], padded_tile_sizes);\n+      triton::Splat(b, values[padding_value], padded_tile_sizes);\n   return mlir::cast<TensorValue>(\n       arith::SelectOp::create(b, mask, values[tiled_operand], pad_value_splat)\n           .getResult());"
        }
    ],
    "stats": {
        "total": 94,
        "additions": 55,
        "deletions": 39
    }
}