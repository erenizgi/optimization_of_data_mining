{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Move ragged all to all e2e tests into a separate target.\n\nPiperOrigin-RevId: 837202728",
    "sha": "4524539234275366faadd1d7f6ce674e151cdd8e",
    "files": [
        {
            "sha": "b887d1b8ed0be68967700d76210affc15efcbee4",
            "filename": "third_party/xla/xla/tests/BUILD",
            "status": "modified",
            "additions": 66,
            "deletions": 0,
            "changes": 66,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4524539234275366faadd1d7f6ce674e151cdd8e/third_party%2Fxla%2Fxla%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4524539234275366faadd1d7f6ce674e151cdd8e/third_party%2Fxla%2Fxla%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2FBUILD?ref=4524539234275366faadd1d7f6ce674e151cdd8e",
            "patch": "@@ -2973,6 +2973,72 @@ xla_test(\n     ]),\n )\n \n+xla_test(\n+    name = \"ragged_all_to_all_e2e_test\",\n+    srcs = [\"ragged_all_to_all_e2e_test.cc\"],\n+    backend_tags = {\n+        \"gpu\": [\n+            \"multi_gpu\",\n+            \"no_oss\",\n+        ],\n+    },\n+    backends = [\n+        \"gpu\",\n+    ],\n+    deps = [\n+        \":collective_ops_e2e_test_base\",\n+        \":literal_test_util\",\n+        \":test_utils\",\n+        \":xla_internal_test_main\",\n+        \"//xla:array\",\n+        \"//xla:error_spec\",\n+        \"//xla:literal\",\n+        \"//xla:literal_util\",\n+        \"//xla:types\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/parser:hlo_parser\",\n+        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/hlo/testlib:pattern_matcher_gmock\",\n+        \"//xla/hlo/testlib:verified_hlo_module\",\n+        \"//xla/hlo/utils:hlo_matchers\",\n+        \"//xla/service:backend\",\n+        \"//xla/service:computation_placer_hdr\",\n+        \"//xla/service:hlo_module_config\",\n+        \"//xla/service:hlo_runner\",\n+        \"//xla/service:hlo_runner_interface\",\n+        \"//xla/service:pattern_matcher\",\n+        \"//xla/service:platform_util\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:gpu_memory_space_assignment\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor:platform\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/stream_executor/integrations:device_mem_allocator\",\n+        \"//xla/stream_executor/integrations:stream_executor_allocator\",\n+        \"//xla/stream_executor/integrations:tf_allocator_adapter\",\n+        \"//xla/tsl/framework:bfc_allocator\",\n+        \"//xla/tsl/framework:device_id\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"//xla/tsl/platform:test\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/functional:any_invocable\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@local_tsl//tsl/platform:regexp\",\n+    ],\n+)\n+\n xla_test(\n     name = \"collective_pipeliner_execution_test\",\n     srcs = [\"collective_pipeliner_execution_test.cc\"],"
        },
        {
            "sha": "6a2371bdb0321693cf9ed1fa9096923e27fcd80a",
            "filename": "third_party/xla/xla/tests/collective_ops_e2e_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1005,
            "changes": 1005,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4524539234275366faadd1d7f6ce674e151cdd8e/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4524539234275366faadd1d7f6ce674e151cdd8e/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fcollective_ops_e2e_test.cc?ref=4524539234275366faadd1d7f6ce674e151cdd8e",
            "patch": "@@ -24,7 +24,6 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n-#include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/functional/any_invocable.h\"\n #include \"absl/log/check.h\"\n@@ -36,7 +35,6 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/str_replace.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"absl/strings/substitute.h\"\n #include \"absl/types/span.h\"\n #include \"xla/array.h\"\n #include \"xla/error_spec.h\"\n@@ -48,13 +46,11 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n #include \"xla/hlo/parser/hlo_parser.h\"\n-#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/testlib/pattern_matcher_gmock.h\"\n #include \"xla/hlo/testlib/verified_hlo_module.h\"\n #include \"xla/hlo/utils/hlo_matchers.h\"\n #include \"xla/literal.h\"\n #include \"xla/literal_util.h\"\n-#include \"xla/service/backend.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/hlo_module_config.h\"\n@@ -67,7 +63,6 @@ limitations under the License.\n #include \"xla/tests/literal_test_util.h\"\n #include \"xla/tests/test_utils.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n-#include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/tsl/platform/test.h\"\n #include \"xla/types.h\"\n@@ -2600,1006 +2595,6 @@ ENTRY entry {\n   EXPECT_NE(hlo_module, nullptr);\n }\n \n-enum class RaggedAllToAllImplType {\n-  kNccl,\n-  kMemcpy,\n-  kDecomposer,\n-  kOneShot,\n-};\n-\n-class RaggedAllToAllTestBase : public CollectiveOpsWithFlagsBase {\n- public:\n-  RaggedAllToAllTestBase(bool enable_async, RaggedAllToAllImplType impl_type)\n-      : CollectiveOpsWithFlagsBase(\n-            enable_async, impl_type == RaggedAllToAllImplType::kMemcpy),\n-        impl_type_(impl_type) {}\n-\n-  // Creates random test data for a ragged-all-to-all.\n-  //\n-  // Ragged tensors which are ragged (have various size) along the second most\n-  // changing dimension only, i.e. shape such as [8, (4), 3]. In memory those\n-  // tensors are flattened out the outermost dimension.\n-  //\n-  // A ragged tensor is represented by three arrays: data, offsets, and sizes.\n-  //   * The data array holds the elements of the ragged tensor.\n-  //   * The offsets array holds the starting offset of each ragged row.\n-  //   * The sizes array holds the number of elements in each ragged row.\n-  //\n-  // A ragged-all-to-all of N replicas performance a collective transpose of the\n-  // ragged tensors. Each pair of replicas exchanges one ragged row. To generate\n-  // the test data we need to know the sizes of all ragged rows for each\n-  // replica.\n-  //\n-  // `input_sizes` is an array of shape [num_replicas, num_replicas,\n-  // num_updates_per_replica]. For concenivence, `input_sizes` can be a 2D\n-  // array, in that case `num_updates_per_replica` is assumed to be 1.\n-  absl::Status CreateRandomTestData(HloModule* module,\n-                                    Array<int64_t> input_sizes) {\n-    CHECK(inputs_.empty());\n-    if (input_sizes.num_dimensions() == 2) {\n-      input_sizes.Reshape({input_sizes.dim(0), input_sizes.dim(1), 1});\n-    }\n-    auto ragged_all_to_all =\n-        FindInstruction(module, HloOpcode::kRaggedAllToAll);\n-    EXPECT_THAT(ragged_all_to_all, NotNull());\n-\n-    const std::vector<ReplicaGroup>& replica_groups =\n-        ragged_all_to_all->replica_groups();\n-    EXPECT_FALSE(replica_groups.empty());\n-\n-    int64_t num_total_replicas = input_sizes.dim(0);\n-    int64_t num_replicas = replica_groups[0].replica_ids_size();\n-\n-    EXPECT_TRUE(\n-        absl::c_all_of(replica_groups, [&](const ReplicaGroup& replica_group) {\n-          return replica_group.replica_ids_size() == num_replicas;\n-        }));\n-\n-    inputs_.resize(num_total_replicas);\n-    expected_outputs_.resize(num_total_replicas);\n-    input_offsets_.resize(num_total_replicas);\n-    input_sizes_.resize(num_total_replicas);\n-    output_offsets_.resize(num_total_replicas);\n-    output_sizes_.resize(num_total_replicas);\n-\n-    HloInstruction* output_param =\n-        module->entry_computation()->parameter_instruction(1);\n-\n-    // The ragged-all-to-all accepts an output tensor as a parameter to allow\n-    // buffer reuse. We initialize the output tensor with -1 to make sure that\n-    // we don't accidentally overwrite data that is not part of the\n-    // ragged-all-to-all update.\n-    Array<float> output_init_data(output_param->shape().dimensions());\n-    output_init_data.Fill(-1);\n-\n-    // Iterate over all replica groups and create random test data for each\n-    // group.\n-    for (const ReplicaGroup& replica_group : replica_groups) {\n-      Array<int64_t> input_sizes_per_replica_group(\n-          {num_replicas, input_sizes.dim(1), input_sizes.dim(2)});\n-\n-      for (int64_t i = 0; i < num_replicas; ++i) {\n-        int64_t replica_id = replica_group.replica_ids(i);\n-        input_sizes_per_replica_group.UpdateSlice(\n-            input_sizes.Slice(\n-                {replica_id, 0, 0},\n-                {replica_id + 1, input_sizes.dim(1), input_sizes.dim(2)}),\n-            {i, 0, 0});\n-      }\n-\n-      TF_RETURN_IF_ERROR(CreateRandomTestDataForReplicaGroup(\n-          module, input_sizes_per_replica_group, output_init_data,\n-          replica_group));\n-    }\n-\n-    TF_ASSIGN_OR_RETURN(output_init_,\n-                        LiteralUtil::CreateFromArrayWithLayout(\n-                            output_init_data, output_param->shape().layout())\n-                            .Convert(output_param->shape().element_type()));\n-    return absl::OkStatus();\n-  }\n-\n-  // Create random test data for a ragged-all-to-all for a single replica group.\n-  absl::Status CreateRandomTestDataForReplicaGroup(\n-      HloModule* module, Array<int64_t> input_sizes,\n-      const Array<float>& output_init_data, const ReplicaGroup& replica_group) {\n-    HloInstruction* input_param =\n-        module->entry_computation()->parameter_instruction(0);\n-    HloInstruction* output_param =\n-        module->entry_computation()->parameter_instruction(1);\n-    int64_t num_replicas = replica_group.replica_ids_size();\n-\n-    Array<int64_t> output_sizes = input_sizes;\n-    output_sizes.TransposeDimensions({1, 0, 2});\n-\n-    Array<int64_t> input_offsets = CalculateOffsetsFromSizes(input_sizes);\n-    Array<int64_t> output_offsets = CalculateOffsetsFromSizes(output_sizes);\n-    output_offsets.TransposeDimensions({1, 0, 2});\n-\n-    std::vector<Array<float>> input_data(\n-        num_replicas, Array<float>(input_param->shape().dimensions()));\n-    std::vector<Array<float>> output_data(num_replicas, output_init_data);\n-    FillWithRandomData(input_data, output_data, input_offsets, output_offsets,\n-                       input_sizes);\n-\n-    // Create literals from array data.\n-    for (int64_t i = 0; i < num_replicas; ++i) {\n-      int64_t replica_id = replica_group.replica_ids(i);\n-      TF_ASSIGN_OR_RETURN(inputs_[replica_id],\n-                          LiteralUtil::CreateFromArrayWithLayout(\n-                              input_data[i], input_param->shape().layout())\n-                              .Convert(input_param->shape().element_type()));\n-\n-      TF_ASSIGN_OR_RETURN(expected_outputs_[replica_id],\n-                          LiteralUtil::CreateFromArrayWithLayout(\n-                              output_data[i], output_param->shape().layout())\n-                              .Convert(output_param->shape().element_type()));\n-\n-      TF_ASSIGN_OR_RETURN(\n-          input_offsets_[replica_id],\n-          GetParameterLiteral(module, /*parameter_index=*/2, i, input_offsets));\n-\n-      TF_ASSIGN_OR_RETURN(\n-          input_sizes_[replica_id],\n-          GetParameterLiteral(module, /*parameter_index=*/3, i, input_sizes));\n-\n-      TF_ASSIGN_OR_RETURN(output_offsets_[replica_id],\n-                          GetParameterLiteral(module, /*parameter_index=*/4, i,\n-                                              output_offsets));\n-      TF_ASSIGN_OR_RETURN(\n-          output_sizes_[replica_id],\n-          GetParameterLiteral(module, /*parameter_index=*/5, i, output_sizes));\n-    }\n-    return absl::OkStatus();\n-  }\n-\n-  // Returns a vector of pointers to the literals in the format needed for\n-  // ExecuteReplicated.\n-  std::vector<std::vector<Literal*>> GetInputLiteralPtrs() {\n-    std::vector<std::vector<Literal*>> input_literal_ptrs;\n-    for (int i = 0; i < inputs_.size(); ++i) {\n-      input_literal_ptrs.push_back({&inputs_[i], &output_init_,\n-                                    &input_offsets_[i], &input_sizes_[i],\n-                                    &output_offsets_[i], &output_sizes_[i]});\n-    }\n-    return input_literal_ptrs;\n-  }\n-\n- protected:\n-  DebugOptions GetDebugOptionsForTest() const override {\n-    DebugOptions opts = CollectiveOpsWithFlagsBase::GetDebugOptionsForTest();\n-    opts.set_xla_gpu_unsupported_enable_ragged_all_to_all_decomposer(\n-        impl_type_ == RaggedAllToAllImplType::kDecomposer);\n-    opts.set_xla_gpu_unsupported_use_ragged_all_to_all_one_shot_kernel(\n-        impl_type_ == RaggedAllToAllImplType::kOneShot);\n-    return opts;\n-  }\n-\n-  // Computes ragged tensor offsets based on the sizes of the ragged rows.\n-  Array<int64_t> CalculateOffsetsFromSizes(const Array<int64_t>& sizes) {\n-    int64_t num_replicas = sizes.dim(0);\n-    int64_t num_updates_per_replica = sizes.dim(2);\n-    Array<int64_t> offsets(sizes.dimensions());\n-    for (int i = 0; i < num_replicas; ++i) {\n-      int64_t cur_offset = 0;\n-      for (int j = 0; j < num_replicas; ++j) {\n-        for (int k = 0; k < num_updates_per_replica; ++k) {\n-          offsets(i, j, k) = cur_offset;\n-          cur_offset += sizes(i, j, k);\n-        }\n-      }\n-    }\n-    return offsets;\n-  }\n-\n-  // Fill the input and output tensors with random data. An all-to-all is\n-  // effectively a transpose. We generate a chunk of random data for each update\n-  // of each pair of replicas and write the chunk starting from the (i, j, k)\n-  // offset of the input tensor and starting from the (j, i, k) offset of the\n-  // output tensor.\n-  void FillWithRandomData(std::vector<Array<float>>& input_data,\n-                          std::vector<Array<float>>& output_data,\n-                          const Array<int64_t>& input_offsets,\n-                          const Array<int64_t>& output_offsets,\n-                          const Array<int64_t>& input_sizes) {\n-    int64_t num_replicas = input_sizes.dim(0);\n-    int64_t num_updates_per_replica = input_sizes.dim(2);\n-    std::vector<int64_t> start_indices(input_data[0].num_dimensions());\n-    std::vector<int64_t> chunk_sizes{input_data[0].dimensions().begin(),\n-                                     input_data[0].dimensions().end()};\n-\n-    for (int i = 0; i < num_replicas; ++i) {\n-      for (int j = 0; j < num_replicas; ++j) {\n-        for (int k = 0; k < num_updates_per_replica; ++k) {\n-          chunk_sizes[0] = input_sizes(i, j, k);\n-\n-          Array<float> chunk_data(chunk_sizes);\n-          chunk_data.FillRandomUniform(\n-              1, 127,\n-              /*seed=*/(i * num_replicas + j) * num_updates_per_replica + k);\n-\n-          start_indices[0] = input_offsets(i, j, k);\n-          input_data[i].UpdateSlice(chunk_data, start_indices);\n-\n-          start_indices[0] = output_offsets(i, j, k);\n-          output_data[j].UpdateSlice(chunk_data, start_indices);\n-        }\n-      }\n-    }\n-  }\n-\n-  // Returns a literal for the given parameter of the given replica.\n-  absl::StatusOr<Literal> GetParameterLiteral(HloModule* module,\n-                                              int64_t parameter_index,\n-                                              int64_t replica_id,\n-                                              const Array<int64_t>& data) {\n-    HloInstruction* param =\n-        module->entry_computation()->parameter_instruction(parameter_index);\n-\n-    int64_t num_replicas = data.dim(0);\n-    int64_t num_updates_per_replica = data.dim(2);\n-    Array<int64_t> replica_slice =\n-        data.Slice({replica_id, 0, 0},\n-                   {replica_id + 1, num_replicas, num_updates_per_replica});\n-    replica_slice.Reshape({num_replicas * num_updates_per_replica});\n-    return LiteralUtil::CreateFromArray(replica_slice)\n-        .Convert(param->shape().element_type());\n-  }\n-\n-  // Literates for the input and output data, offset, and size parameters of\n-  // the ragged-all-to-all. Each vector contains one literal per replica.\n-  std::vector<Literal> inputs_;\n-  std::vector<Literal> input_offsets_;\n-  std::vector<Literal> input_sizes_;\n-\n-  std::vector<Literal> expected_outputs_;\n-  std::vector<Literal> output_offsets_;\n-  std::vector<Literal> output_sizes_;\n-\n-  Literal output_init_;\n-\n-  RaggedAllToAllImplType impl_type_;\n-};\n-\n-class RaggedAllToAllTest : public RaggedAllToAllTestBase,\n-                           public ::testing::WithParamInterface<\n-                               std::tuple<bool, RaggedAllToAllImplType>> {\n- public:\n-  RaggedAllToAllTest()\n-      : RaggedAllToAllTestBase(std::get<0>(GetParam()),\n-                               std::get<1>(GetParam())) {}\n-};\n-\n-TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs) {\n-  absl::string_view kModuleReplicatedStr = R\"(\n-  HloModule module, num_partitions=1\n-\n-  ENTRY entry {\n-    input = f32[4] parameter(0)\n-    output = f32[4] parameter(1)\n-    input_offsets = s32[2] parameter(2)\n-    send_sizes = s32[2] parameter(3)\n-    output_offsets = s32[2] parameter(4)\n-    recv_sizes = s32[2] parameter(5)\n-    ROOT ra2a = f32[4] ragged-all-to-all(input, output, input_offsets,\n-    send_sizes, output_offsets, recv_sizes), replica_groups={{0,1}}\n-  })\";\n-\n-  const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n-  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas)\n-      << \"Test requires at least \" << kNumReplicas << \" devices (\"\n-      << hlo_runner_->device_count() << \" available)\";\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n-\n-  TF_ASSERT_OK(CreateRandomTestData(module.get(),\n-                                    /*input_sizes=*/{/*replica_0=*/{1, 1},\n-                                                     /*replica_1=*/{3, 1}}));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n-  ASSERT_EQ(results.size(), kNumReplicas);\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[0], results[0]));\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[1], results[1]));\n-}\n-\n-TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_InputBufferLargerThanOutput) {\n-  absl::string_view kModuleReplicatedStr = R\"(\n-  HloModule module, num_partitions=1\n-\n-  ENTRY entry {\n-    input = f32[32] parameter(0)\n-    output = f32[16] parameter(1)\n-    input_offsets = s32[2] parameter(2)\n-    send_sizes = s32[2] parameter(3)\n-    output_offsets = s32[2] parameter(4)\n-    recv_sizes = s32[2] parameter(5)\n-    ROOT ra2a = f32[16] ragged-all-to-all(input, output, input_offsets,\n-    send_sizes, output_offsets, recv_sizes), replica_groups={{0,1}}\n-  })\";\n-\n-  const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n-  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas)\n-      << \"Test requires at least \" << kNumReplicas << \" devices (\"\n-      << hlo_runner_->device_count() << \" available)\";\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n-\n-  TF_ASSERT_OK(CreateRandomTestData(module.get(),\n-                                    /*input_sizes=*/{/*replica_0=*/{8, 5},\n-                                                     /*replica_1=*/{4, 3}}));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n-  ASSERT_EQ(results.size(), kNumReplicas);\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[0], results[0]));\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[1], results[1]));\n-}\n-\n-TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_OutputBufferLargerThanInput) {\n-  absl::string_view kModuleReplicatedStr = R\"(\n-  HloModule module, num_partitions=1\n-\n-  ENTRY entry {\n-    input = f32[16] parameter(0)\n-    output = f32[32] parameter(1)\n-    input_offsets = s32[2] parameter(2)\n-    send_sizes = s32[2] parameter(3)\n-    output_offsets = s32[2] parameter(4)\n-    recv_sizes = s32[2] parameter(5)\n-    ROOT ra2a = f32[32] ragged-all-to-all(input, output, input_offsets,\n-    send_sizes, output_offsets, recv_sizes), replica_groups={{0,1}}\n-  })\";\n-\n-  const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n-  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n-      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n-\n-  TF_ASSERT_OK(CreateRandomTestData(module.get(),\n-                                    /*input_sizes=*/{/*replica_0=*/{4, 12},\n-                                                     /*replica_1=*/{5, 11}}));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n-  ASSERT_EQ(results.size(), kNumReplicas);\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[0], results[0]));\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[1], results[1]));\n-}\n-\n-TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_MultipleUpdates) {\n-  absl::string_view kModuleReplicatedStr = R\"(\n-  HloModule module, num_partitions=1\n-\n-  ENTRY entry {\n-    input = f32[8] parameter(0)\n-    output = f32[8] parameter(1)\n-    input_offsets = s32[4] parameter(2)\n-    send_sizes = s32[4] parameter(3)\n-    output_offsets = s32[4] parameter(4)\n-    recv_sizes = s32[4] parameter(5)\n-    ROOT ra2a = f32[8] ragged-all-to-all(input, output, input_offsets,\n-    send_sizes, output_offsets, recv_sizes), replica_groups={{0,1}}\n-  })\";\n-\n-  const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n-  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n-      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n-\n-  TF_ASSERT_OK(CreateRandomTestData(\n-      module.get(), /*input_sizes=*/{/*replica_0=*/{{1, 2}, {2, 1}},\n-                                     /*replica_1=*/{{3, 1}, {1, 1}}}));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n-  ASSERT_EQ(results.size(), kNumReplicas);\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[0], results[0]));\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[1], results[1]));\n-}\n-\n-TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_MultiDimData) {\n-  absl::string_view kModuleReplicatedStr = R\"(\n-  HloModule module, num_partitions=1\n-\n-  ENTRY entry {\n-    input = bf16[16, 5, 32] parameter(0)\n-    output = bf16[16, 5, 32] parameter(1)\n-    input_offsets = s64[2] parameter(2)\n-    send_sizes = s64[2] parameter(3)\n-    output_offsets = s64[2] parameter(4)\n-    recv_sizes = s64[2] parameter(5)\n-    ROOT ra2a = bf16[16, 5, 32] ragged-all-to-all(input, output,\n-      input_offsets, send_sizes, output_offsets, recv_sizes),\n-      replica_groups={{0,1}}\n-  })\";\n-\n-  const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n-  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n-      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n-\n-  TF_ASSERT_OK(CreateRandomTestData(module.get(),\n-                                    /*input_sizes=*/{/*replica_0=*/{4, 7},\n-                                                     /*replica_1=*/{2, 5}}));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n-  ASSERT_EQ(results.size(), kNumReplicas);\n-\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[0], results[0]));\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[1], results[1]));\n-}\n-\n-TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_Degenerate) {\n-  absl::string_view kModuleReplicatedStr = R\"(\n-  HloModule module\n-\n-  ENTRY entry {\n-    input = f32[4] parameter(0)\n-    output = f32[4] parameter(1)\n-    input_offsets = s32[1] parameter(2)\n-    send_sizes = s32[1] parameter(3)\n-    output_offsets = s32[1] parameter(4)\n-    recv_sizes = s32[1] parameter(5)\n-    ROOT ra2a = f32[4] ragged-all-to-all(input, output, input_offsets,\n-    send_sizes, output_offsets, recv_sizes), replica_groups={{0},{1}}\n-  })\";\n-\n-  const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n-  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n-      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n-\n-  TF_ASSERT_OK(CreateRandomTestData(module.get(),\n-                                    /*input_sizes=*/{/*replica_0=*/{1},\n-                                                     /*replica_1=*/{3}}));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n-  ASSERT_EQ(results.size(), kNumReplicas);\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[0], results[0]));\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[1], results[1]));\n-}\n-\n-TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_NonDefaultLayout) {\n-  absl::string_view kModuleReplicatedStr = R\"(\n-  HloModule module\n-\n-  ENTRY entry {\n-    input = f32[16,4,8]{0,2,1} parameter(0)\n-    output = f32[16,4,8]{0,1,2} parameter(1)\n-    input_offsets = s32[2] parameter(2)\n-    send_sizes = s32[2] parameter(3)\n-    output_offsets = s32[2] parameter(4)\n-    recv_sizes = s32[2] parameter(5)\n-    ROOT ra2a = f32[16,4,8]{0,1,2} ragged-all-to-all(input, output,\n-      input_offsets, send_sizes, output_offsets, recv_sizes),\n-      replica_groups={{0,1}}\n-  })\";\n-\n-  const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n-  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n-      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n-\n-  auto ragged_all_to_all =\n-      FindInstruction(module.get(), HloOpcode::kRaggedAllToAll);\n-  EXPECT_THAT(ragged_all_to_all, NotNull());\n-\n-  TF_ASSERT_OK(CreateRandomTestData(module.get(),\n-                                    /*input_sizes=*/{/*replica_0=*/{4, 7},\n-                                                     /*replica_1=*/{2, 5}}));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n-  ASSERT_EQ(results.size(), kNumReplicas);\n-\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[0], results[0]));\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[1], results[1]));\n-}\n-\n-TEST_P(RaggedAllToAllTest,\n-       RaggedAllToAll_2GPUs_DevicesInReplicaGroupInReverseOrder) {\n-  absl::string_view kModuleReplicatedStr = R\"(\n-  HloModule module, num_partitions=1\n-\n-  ENTRY entry {\n-    input = f32[4] parameter(0)\n-    output = f32[4] parameter(1)\n-    input_offsets = s32[2] parameter(2)\n-    send_sizes = s32[2] parameter(3)\n-    output_offsets = s32[2] parameter(4)\n-    recv_sizes = s32[2] parameter(5)\n-    ROOT ra2a = f32[4] ragged-all-to-all(input, output, input_offsets,\n-    send_sizes, output_offsets, recv_sizes), replica_groups={{1,0}}\n-  })\";\n-\n-  const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n-  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n-      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n-\n-  TF_ASSERT_OK(CreateRandomTestData(module.get(),\n-                                    /*input_sizes=*/{/*replica_0=*/{1, 1},\n-                                                     /*replica_1=*/{3, 1}}));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n-  ASSERT_EQ(results.size(), kNumReplicas);\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[0], results[0]));\n-  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[1], results[1]));\n-}\n-\n-TEST_P(RaggedAllToAllTest, RaggedAllToAll_8GPUs) {\n-  absl::string_view kModuleReplicatedStr = R\"(\n-  HloModule module, num_partitions=1\n-\n-  ENTRY entry {\n-    input = f32[512, 5, 32] parameter(0)\n-    output = f32[512, 5, 32] parameter(1)\n-    input_offsets = s32[32] parameter(2)\n-    send_sizes = s32[32] parameter(3)\n-    output_offsets = s32[32] parameter(4)\n-    recv_sizes = s32[32] parameter(5)\n-    ROOT ra2a = f32[512, 5, 32] ragged-all-to-all(input, output,\n-      input_offsets, send_sizes, output_offsets, recv_sizes),\n-      replica_groups={{0,1,2,3,4,5,6,7}}\n-  })\";\n-\n-  const int64_t kNumReplicas = 8;\n-  const int64_t kNumPartitions = 1;\n-  const int64_t kNumUpdatesPerReplica = 4;\n-  if (hlo_runner_->device_count() < kNumReplicas * kNumPartitions) {\n-    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-                 << \" devices (\" << hlo_runner_->device_count()\n-                 << \" available)\";\n-  }\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n-\n-  Array<int64_t> input_sizes(\n-      {kNumReplicas, kNumReplicas, kNumUpdatesPerReplica});\n-  input_sizes.FillRandomUniform(0, 10);\n-\n-  TF_ASSERT_OK(CreateRandomTestData(module.get(), input_sizes));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n-  ASSERT_EQ(results.size(), kNumReplicas);\n-\n-  for (int i = 0; i < kNumReplicas; ++i) {\n-    EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[i], results[i]));\n-  }\n-}\n-\n-TEST_P(RaggedAllToAllTest, RaggedAllToAll_8GPUs_2ReplicasPerGroups) {\n-  absl::string_view kModuleReplicatedStr = R\"(\n-  HloModule module, num_partitions=1\n-\n-  ENTRY entry {\n-    input = f32[512, 5, 32] parameter(0)\n-    output = f32[512, 5, 32] parameter(1)\n-    input_offsets = s32[32] parameter(2)\n-    send_sizes = s32[32] parameter(3)\n-    output_offsets = s32[32] parameter(4)\n-    recv_sizes = s32[32] parameter(5)\n-    ROOT ra2a = f32[512, 5, 32] ragged-all-to-all(input, output,\n-      input_offsets, send_sizes, output_offsets, recv_sizes),\n-      replica_groups={{0,4},{1,5},{2,6},{3,7}}\n-  })\";\n-\n-  const int64_t kNumReplicas = 8;\n-  const int64_t kNumReplicasPerGroup = 2;\n-  const int64_t kNumPartitions = 1;\n-  const int64_t kNumUpdatesPerReplica = 16;\n-  if (hlo_runner_->device_count() < kNumReplicas * kNumPartitions) {\n-    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-                 << \" devices (\" << hlo_runner_->device_count()\n-                 << \" available)\";\n-  }\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n-\n-  Array<int64_t> input_sizes(\n-      {kNumReplicas, kNumReplicasPerGroup, kNumUpdatesPerReplica});\n-  input_sizes.FillRandomUniform(0, 10);\n-\n-  TF_ASSERT_OK(CreateRandomTestData(module.get(), input_sizes));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n-  ASSERT_EQ(results.size(), kNumReplicas);\n-\n-  for (int i = 0; i < kNumReplicas; ++i) {\n-    EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[i], results[i]));\n-  }\n-}\n-\n-TEST_P(RaggedAllToAllTest, RaggedAllToAll_8GPUs_4ReplicasPerGroups) {\n-  absl::string_view kModuleReplicatedStr = R\"(\n-  HloModule module, num_partitions=1\n-\n-  ENTRY entry {\n-    input = f32[512, 5, 32] parameter(0)\n-    output = f32[512, 5, 32] parameter(1)\n-    input_offsets = s32[32] parameter(2)\n-    send_sizes = s32[32] parameter(3)\n-    output_offsets = s32[32] parameter(4)\n-    recv_sizes = s32[32] parameter(5)\n-    ROOT ra2a = f32[512, 5, 32] ragged-all-to-all(input, output,\n-      input_offsets, send_sizes, output_offsets, recv_sizes),\n-      replica_groups={{0,1,2,3},{4,5,6,7}}\n-  })\";\n-\n-  const int64_t kNumReplicas = 8;\n-  const int64_t kNumReplicasPerGroup = 4;\n-  const int64_t kNumPartitions = 1;\n-  const int64_t kNumUpdatesPerReplica = 8;\n-  if (hlo_runner_->device_count() < kNumReplicas * kNumPartitions) {\n-    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-                 << \" devices (\" << hlo_runner_->device_count()\n-                 << \" available)\";\n-  }\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n-\n-  Array<int64_t> input_sizes(\n-      {kNumReplicas, kNumReplicasPerGroup, kNumUpdatesPerReplica});\n-  input_sizes.FillRandomUniform(0, 10);\n-\n-  TF_ASSERT_OK(CreateRandomTestData(module.get(), input_sizes));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n-  ASSERT_EQ(results.size(), kNumReplicas);\n-\n-  for (int i = 0; i < kNumReplicas; ++i) {\n-    EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[i], results[i]));\n-  }\n-}\n-\n-std::string RaggedAllToAllImplTypeName(\n-    RaggedAllToAllImplType ragged_all_to_all_impl_type) {\n-  switch (ragged_all_to_all_impl_type) {\n-    case RaggedAllToAllImplType::kNccl:\n-      return \"nccl\";\n-    case RaggedAllToAllImplType::kMemcpy:\n-      return \"memcpy\";\n-    case RaggedAllToAllImplType::kDecomposer:\n-      return \"decomposer\";\n-    case RaggedAllToAllImplType::kOneShot:\n-      return \"one_shot\";\n-    default:\n-      LOG(FATAL) << \"Unknown ragged all-to-all implementation type.\";\n-  }\n-}\n-\n-INSTANTIATE_TEST_SUITE_P(\n-    RaggedAllToAllTest, RaggedAllToAllTest,\n-    ::testing::Combine(::testing::Bool(),\n-                       ::testing::Values(RaggedAllToAllImplType::kNccl,\n-                                         RaggedAllToAllImplType::kMemcpy,\n-                                         RaggedAllToAllImplType::kDecomposer,\n-                                         RaggedAllToAllImplType::kOneShot)),\n-    [](const ::testing::TestParamInfo<std::tuple<bool, RaggedAllToAllImplType>>&\n-           info) {\n-      return absl::StrCat(GetAsyncTestName(std::get<0>(info.param)), \"_\",\n-                          RaggedAllToAllImplTypeName(std::get<1>(info.param)));\n-    });\n-\n-class RaggedAllToAllMultiHostDecomposerTest\n-    : public RaggedAllToAllTestBase,\n-      public ::testing::WithParamInterface<std::tuple<int64_t, int64_t>> {\n- public:\n-  RaggedAllToAllMultiHostDecomposerTest()\n-      : RaggedAllToAllTestBase(/*enable_async=*/false,\n-                               /*impl_type=*/RaggedAllToAllImplType::kOneShot) {\n-  }\n-\n- protected:\n-  DebugOptions GetDebugOptionsForTest() const override {\n-    DebugOptions debug_options =\n-        RaggedAllToAllTestBase::GetDebugOptionsForTest();\n-    debug_options\n-        .set_xla_gpu_unsupported_enable_ragged_all_to_all_multi_host_decomposer(\n-            true);\n-    return debug_options;\n-  }\n-};\n-\n-TEST_P(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_2GPUs_SliceSize1) {\n-  auto [num_input_rows, num_output_rows] = GetParam();\n-\n-  std::string kModuleReplicatedStr =\n-      absl::Substitute(R\"(\n-  HloModule module, num_partitions=1\n-\n-  ENTRY entry {\n-    input = f32[$0,5,32] parameter(0)\n-    output = f32[$1,5,32] parameter(1)\n-    input_offsets = s32[32] parameter(2)\n-    send_sizes = s32[32] parameter(3)\n-    output_offsets = s32[32] parameter(4)\n-    recv_sizes = s32[32] parameter(5)\n-    ROOT ra2a = f32[$1,5,32] ragged-all-to-all(input, output,\n-      input_offsets, send_sizes, output_offsets, recv_sizes),\n-      replica_groups={{0,1}}\n-  })\",\n-                       num_input_rows, num_output_rows);\n-\n-  const int64_t kNumReplicas = 2;\n-  const int64_t kNumPartitions = 1;\n-  const int64_t kNumUpdatesPerReplica = 16;\n-  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n-      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n-\n-  config.mutable_debug_options()\n-      .set_xla_gpu_unsupported_override_fast_interconnect_slice_size(1);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n-\n-  Array<int64_t> input_sizes(\n-      {kNumReplicas, kNumReplicas, kNumUpdatesPerReplica});\n-  input_sizes.FillRandomUniform(0, 10);\n-\n-  TF_ASSERT_OK(CreateRandomTestData(module.get(), input_sizes));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n-  ASSERT_EQ(results.size(), kNumReplicas);\n-\n-  for (int i = 0; i < kNumReplicas; ++i) {\n-    EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[i], results[i]));\n-  }\n-}\n-\n-TEST_P(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_8GPUs_SliceSize4) {\n-  auto [num_input_rows, num_output_rows] = GetParam();\n-\n-  std::string kModuleReplicatedStr =\n-      absl::Substitute(R\"(\n-  HloModule module, num_partitions=1\n-\n-  ENTRY entry {\n-    input = f32[$0,5,32] parameter(0)\n-    output = f32[$1,5,32] parameter(1)\n-    input_offsets = s32[32] parameter(2)\n-    send_sizes = s32[32] parameter(3)\n-    output_offsets = s32[32] parameter(4)\n-    recv_sizes = s32[32] parameter(5)\n-    ROOT ra2a = f32[$1,5,32] ragged-all-to-all(input, output,\n-      input_offsets, send_sizes, output_offsets, recv_sizes),\n-      replica_groups={{0,1,2,3,4,5,6,7}}\n-  })\",\n-                       num_input_rows, num_output_rows);\n-\n-  const int64_t kNumReplicas = 8;\n-  const int64_t kNumPartitions = 1;\n-  const int64_t kNumUpdatesPerReplica = 4;\n-  if (hlo_runner_->device_count() < kNumReplicas * kNumPartitions) {\n-    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-                 << \" devices (\" << hlo_runner_->device_count()\n-                 << \" available)\";\n-  }\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n-\n-  config.mutable_debug_options()\n-      .set_xla_gpu_unsupported_override_fast_interconnect_slice_size(4);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n-\n-  Array<int64_t> input_sizes(\n-      {kNumReplicas, kNumReplicas, kNumUpdatesPerReplica});\n-  input_sizes.FillRandomUniform(0, 16);\n-\n-  TF_ASSERT_OK(CreateRandomTestData(module.get(), input_sizes));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n-  ASSERT_EQ(results.size(), kNumReplicas);\n-\n-  for (int i = 0; i < kNumReplicas; ++i) {\n-    EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[i], results[i]));\n-  }\n-}\n-\n-TEST_P(RaggedAllToAllMultiHostDecomposerTest,\n-       RaggedAllToAll_8GPUs_SliceSize4_2ReplicaGroups) {\n-  auto [num_input_rows, num_output_rows] = GetParam();\n-\n-  std::string kModuleReplicatedStr =\n-      absl::Substitute(R\"(\n-  HloModule module, num_partitions=1\n-\n-  ENTRY entry {\n-    input = f32[$0,5,32] parameter(0)\n-    output = f32[$1,5,32] parameter(1)\n-    input_offsets = s32[32] parameter(2)\n-    send_sizes = s32[32] parameter(3)\n-    output_offsets = s32[32] parameter(4)\n-    recv_sizes = s32[32] parameter(5)\n-    ROOT ra2a = f32[$1,5,32] ragged-all-to-all(input, output,\n-      input_offsets, send_sizes, output_offsets, recv_sizes),\n-      replica_groups={{0,2,4,6},{1,3,5,7}}\n-  })\",\n-                       num_input_rows, num_output_rows);\n-\n-  const int64_t kNumReplicas = 8;\n-  const int64_t kNumReplicasPerGroup = 4;\n-  const int64_t kNumPartitions = 1;\n-  const int64_t kNumUpdatesPerReplica = 8;\n-  if (hlo_runner_->device_count() < kNumReplicas * kNumPartitions) {\n-    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas * kNumPartitions\n-                 << \" devices (\" << hlo_runner_->device_count()\n-                 << \" available)\";\n-  }\n-\n-  HloModuleConfig config =\n-      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n-\n-  config.mutable_debug_options()\n-      .set_xla_gpu_unsupported_override_fast_interconnect_slice_size(4);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n-\n-  Array<int64_t> input_sizes(\n-      {kNumReplicas, kNumReplicasPerGroup, kNumUpdatesPerReplica});\n-  input_sizes.FillRandomUniform(0, 10);\n-\n-  TF_ASSERT_OK(CreateRandomTestData(module.get(), input_sizes));\n-\n-  TF_ASSERT_OK_AND_ASSIGN(\n-      std::vector<Literal> results,\n-      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n-                        /*device_assignment=*/nullptr,\n-                        /*num_replicas=*/kNumReplicas,\n-                        /*run_hlo_passes=*/true));\n-  ASSERT_EQ(results.size(), kNumReplicas);\n-\n-  for (int i = 0; i < kNumReplicas; ++i) {\n-    EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[i], results[i]));\n-  }\n-}\n-\n-INSTANTIATE_TEST_SUITE_P(\n-    RaggedAllToAllMultiHostDecomposerTest,\n-    RaggedAllToAllMultiHostDecomposerTest,\n-    ::testing::Values(std::make_tuple(512, 4096), std::make_tuple(4096, 512)),\n-    [](const ::testing::TestParamInfo<std::tuple<int64_t, int64_t>>& info) {\n-      if (std::get<0>(info.param) > std::get<1>(info.param)) {\n-        return absl::StrCat(\"combine_\", std::get<0>(info.param), \"_\",\n-                            std::get<1>(info.param));\n-      }\n-      return absl::StrCat(\"dispatch_\", std::get<0>(info.param), \"_\",\n-                          std::get<1>(info.param));\n-    });\n-\n TEST_F(CollectiveOpsTestE2E, MemcpyP2pWhileLoopCorrectness) {\n   absl::string_view hlo_string = R\"(\n HloModule MemcpyP2pWhileLoopCorrectness, entry_computation_layout={(bf16[128,96]{1,0})->(bf16[32,384]{1,0}, bf16[32,384]{1,0})}, allow_spmd_sharding_propagation_to_output={true,true}, num_partitions=4"
        },
        {
            "sha": "df3d670a6d77b35ed75548a177db81915a9024d5",
            "filename": "third_party/xla/xla/tests/ragged_all_to_all_e2e_test.cc",
            "status": "added",
            "additions": 1056,
            "deletions": 0,
            "changes": 1056,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4524539234275366faadd1d7f6ce674e151cdd8e/third_party%2Fxla%2Fxla%2Ftests%2Fragged_all_to_all_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4524539234275366faadd1d7f6ce674e151cdd8e/third_party%2Fxla%2Fxla%2Ftests%2Fragged_all_to_all_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fragged_all_to_all_e2e_test.cc?ref=4524539234275366faadd1d7f6ce674e151cdd8e",
            "patch": "@@ -0,0 +1,1056 @@\n+/* Copyright 2023 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstdint>\n+#include <memory>\n+#include <string>\n+#include <tuple>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/strings/substitute.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/array.h\"\n+#include \"xla/hlo/ir/hlo_input_output_alias_config.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/literal.h\"\n+#include \"xla/literal_util.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/hlo_module_config.h\"\n+#include \"xla/service/hlo_runner.h\"\n+#include \"xla/tests/collective_ops_e2e_test_base.h\"\n+#include \"xla/tests/literal_test_util.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/tsl/platform/test.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla {\n+namespace {\n+\n+using ::testing::NotNull;\n+\n+enum class RaggedAllToAllImplType {\n+  kNccl,\n+  kMemcpy,\n+  kDecomposer,\n+  kOneShot,\n+};\n+\n+class RaggedAllToAllTestBase : public CollectiveOpsWithFlagsBase {\n+ public:\n+  RaggedAllToAllTestBase(bool enable_async, RaggedAllToAllImplType impl_type)\n+      : CollectiveOpsWithFlagsBase(\n+            enable_async, impl_type == RaggedAllToAllImplType::kMemcpy),\n+        impl_type_(impl_type) {}\n+\n+  // Creates random test data for a ragged-all-to-all.\n+  //\n+  // Ragged tensors which are ragged (have various size) along the second most\n+  // changing dimension only, i.e. shape such as [8, (4), 3]. In memory those\n+  // tensors are flattened out the outermost dimension.\n+  //\n+  // A ragged tensor is represented by three arrays: data, offsets, and sizes.\n+  //   * The data array holds the elements of the ragged tensor.\n+  //   * The offsets array holds the starting offset of each ragged row.\n+  //   * The sizes array holds the number of elements in each ragged row.\n+  //\n+  // A ragged-all-to-all of N replicas performance a collective transpose of the\n+  // ragged tensors. Each pair of replicas exchanges one ragged row. To generate\n+  // the test data we need to know the sizes of all ragged rows for each\n+  // replica.\n+  //\n+  // `input_sizes` is an array of shape [num_replicas, num_replicas,\n+  // num_updates_per_replica]. For concenivence, `input_sizes` can be a 2D\n+  // array, in that case `num_updates_per_replica` is assumed to be 1.\n+  absl::Status CreateRandomTestData(HloModule* module,\n+                                    Array<int64_t> input_sizes) {\n+    CHECK(inputs_.empty());\n+    if (input_sizes.num_dimensions() == 2) {\n+      input_sizes.Reshape({input_sizes.dim(0), input_sizes.dim(1), 1});\n+    }\n+    auto ragged_all_to_all =\n+        FindInstruction(module, HloOpcode::kRaggedAllToAll);\n+    EXPECT_THAT(ragged_all_to_all, NotNull());\n+\n+    const std::vector<ReplicaGroup>& replica_groups =\n+        ragged_all_to_all->replica_groups();\n+    EXPECT_FALSE(replica_groups.empty());\n+\n+    int64_t num_total_replicas = input_sizes.dim(0);\n+    int64_t num_replicas = replica_groups[0].replica_ids_size();\n+\n+    EXPECT_TRUE(\n+        absl::c_all_of(replica_groups, [&](const ReplicaGroup& replica_group) {\n+          return replica_group.replica_ids_size() == num_replicas;\n+        }));\n+\n+    inputs_.resize(num_total_replicas);\n+    expected_outputs_.resize(num_total_replicas);\n+    input_offsets_.resize(num_total_replicas);\n+    input_sizes_.resize(num_total_replicas);\n+    output_offsets_.resize(num_total_replicas);\n+    output_sizes_.resize(num_total_replicas);\n+\n+    HloInstruction* output_param =\n+        module->entry_computation()->parameter_instruction(1);\n+\n+    // The ragged-all-to-all accepts an output tensor as a parameter to allow\n+    // buffer reuse. We initialize the output tensor with -1 to make sure that\n+    // we don't accidentally overwrite data that is not part of the\n+    // ragged-all-to-all update.\n+    Array<float> output_init_data(output_param->shape().dimensions());\n+    output_init_data.Fill(-1);\n+\n+    // Iterate over all replica groups and create random test data for each\n+    // group.\n+    for (const ReplicaGroup& replica_group : replica_groups) {\n+      Array<int64_t> input_sizes_per_replica_group(\n+          {num_replicas, input_sizes.dim(1), input_sizes.dim(2)});\n+\n+      for (int64_t i = 0; i < num_replicas; ++i) {\n+        int64_t replica_id = replica_group.replica_ids(i);\n+        input_sizes_per_replica_group.UpdateSlice(\n+            input_sizes.Slice(\n+                {replica_id, 0, 0},\n+                {replica_id + 1, input_sizes.dim(1), input_sizes.dim(2)}),\n+            {i, 0, 0});\n+      }\n+\n+      TF_RETURN_IF_ERROR(CreateRandomTestDataForReplicaGroup(\n+          module, input_sizes_per_replica_group, output_init_data,\n+          replica_group));\n+    }\n+\n+    TF_ASSIGN_OR_RETURN(output_init_,\n+                        LiteralUtil::CreateFromArrayWithLayout(\n+                            output_init_data, output_param->shape().layout())\n+                            .Convert(output_param->shape().element_type()));\n+    return absl::OkStatus();\n+  }\n+\n+  // Create random test data for a ragged-all-to-all for a single replica group.\n+  absl::Status CreateRandomTestDataForReplicaGroup(\n+      HloModule* module, Array<int64_t> input_sizes,\n+      const Array<float>& output_init_data, const ReplicaGroup& replica_group) {\n+    HloInstruction* input_param =\n+        module->entry_computation()->parameter_instruction(0);\n+    HloInstruction* output_param =\n+        module->entry_computation()->parameter_instruction(1);\n+    int64_t num_replicas = replica_group.replica_ids_size();\n+\n+    Array<int64_t> output_sizes = input_sizes;\n+    output_sizes.TransposeDimensions({1, 0, 2});\n+\n+    Array<int64_t> input_offsets = CalculateOffsetsFromSizes(input_sizes);\n+    Array<int64_t> output_offsets = CalculateOffsetsFromSizes(output_sizes);\n+    output_offsets.TransposeDimensions({1, 0, 2});\n+\n+    std::vector<Array<float>> input_data(\n+        num_replicas, Array<float>(input_param->shape().dimensions()));\n+    std::vector<Array<float>> output_data(num_replicas, output_init_data);\n+    FillWithRandomData(input_data, output_data, input_offsets, output_offsets,\n+                       input_sizes);\n+\n+    // Create literals from array data.\n+    for (int64_t i = 0; i < num_replicas; ++i) {\n+      int64_t replica_id = replica_group.replica_ids(i);\n+      TF_ASSIGN_OR_RETURN(inputs_[replica_id],\n+                          LiteralUtil::CreateFromArrayWithLayout(\n+                              input_data[i], input_param->shape().layout())\n+                              .Convert(input_param->shape().element_type()));\n+\n+      TF_ASSIGN_OR_RETURN(expected_outputs_[replica_id],\n+                          LiteralUtil::CreateFromArrayWithLayout(\n+                              output_data[i], output_param->shape().layout())\n+                              .Convert(output_param->shape().element_type()));\n+\n+      TF_ASSIGN_OR_RETURN(\n+          input_offsets_[replica_id],\n+          GetParameterLiteral(module, /*parameter_index=*/2, i, input_offsets));\n+\n+      TF_ASSIGN_OR_RETURN(\n+          input_sizes_[replica_id],\n+          GetParameterLiteral(module, /*parameter_index=*/3, i, input_sizes));\n+\n+      TF_ASSIGN_OR_RETURN(output_offsets_[replica_id],\n+                          GetParameterLiteral(module, /*parameter_index=*/4, i,\n+                                              output_offsets));\n+      TF_ASSIGN_OR_RETURN(\n+          output_sizes_[replica_id],\n+          GetParameterLiteral(module, /*parameter_index=*/5, i, output_sizes));\n+    }\n+    return absl::OkStatus();\n+  }\n+\n+  // Returns a vector of pointers to the literals in the format needed for\n+  // ExecuteReplicated.\n+  std::vector<std::vector<Literal*>> GetInputLiteralPtrs() {\n+    std::vector<std::vector<Literal*>> input_literal_ptrs;\n+    for (int i = 0; i < inputs_.size(); ++i) {\n+      input_literal_ptrs.push_back({&inputs_[i], &output_init_,\n+                                    &input_offsets_[i], &input_sizes_[i],\n+                                    &output_offsets_[i], &output_sizes_[i]});\n+    }\n+    return input_literal_ptrs;\n+  }\n+\n+ protected:\n+  DebugOptions GetDebugOptionsForTest() const override {\n+    DebugOptions opts = CollectiveOpsWithFlagsBase::GetDebugOptionsForTest();\n+    opts.set_xla_gpu_unsupported_enable_ragged_all_to_all_decomposer(\n+        impl_type_ == RaggedAllToAllImplType::kDecomposer);\n+    opts.set_xla_gpu_unsupported_use_ragged_all_to_all_one_shot_kernel(\n+        impl_type_ == RaggedAllToAllImplType::kOneShot);\n+    return opts;\n+  }\n+\n+  // Computes ragged tensor offsets based on the sizes of the ragged rows.\n+  Array<int64_t> CalculateOffsetsFromSizes(const Array<int64_t>& sizes) {\n+    int64_t num_replicas = sizes.dim(0);\n+    int64_t num_updates_per_replica = sizes.dim(2);\n+    Array<int64_t> offsets(sizes.dimensions());\n+    for (int i = 0; i < num_replicas; ++i) {\n+      int64_t cur_offset = 0;\n+      for (int j = 0; j < num_replicas; ++j) {\n+        for (int k = 0; k < num_updates_per_replica; ++k) {\n+          offsets(i, j, k) = cur_offset;\n+          cur_offset += sizes(i, j, k);\n+        }\n+      }\n+    }\n+    return offsets;\n+  }\n+\n+  // Fill the input and output tensors with random data. An all-to-all is\n+  // effectively a transpose. We generate a chunk of random data for each update\n+  // of each pair of replicas and write the chunk starting from the (i, j, k)\n+  // offset of the input tensor and starting from the (j, i, k) offset of the\n+  // output tensor.\n+  void FillWithRandomData(std::vector<Array<float>>& input_data,\n+                          std::vector<Array<float>>& output_data,\n+                          const Array<int64_t>& input_offsets,\n+                          const Array<int64_t>& output_offsets,\n+                          const Array<int64_t>& input_sizes) {\n+    int64_t num_replicas = input_sizes.dim(0);\n+    int64_t num_updates_per_replica = input_sizes.dim(2);\n+    std::vector<int64_t> start_indices(input_data[0].num_dimensions());\n+    std::vector<int64_t> chunk_sizes{input_data[0].dimensions().begin(),\n+                                     input_data[0].dimensions().end()};\n+\n+    for (int i = 0; i < num_replicas; ++i) {\n+      for (int j = 0; j < num_replicas; ++j) {\n+        for (int k = 0; k < num_updates_per_replica; ++k) {\n+          chunk_sizes[0] = input_sizes(i, j, k);\n+\n+          Array<float> chunk_data(chunk_sizes);\n+          chunk_data.FillRandomUniform(\n+              1, 127,\n+              /*seed=*/(i * num_replicas + j) * num_updates_per_replica + k);\n+\n+          start_indices[0] = input_offsets(i, j, k);\n+          input_data[i].UpdateSlice(chunk_data, start_indices);\n+\n+          start_indices[0] = output_offsets(i, j, k);\n+          output_data[j].UpdateSlice(chunk_data, start_indices);\n+        }\n+      }\n+    }\n+  }\n+\n+  // Returns a literal for the given parameter of the given replica.\n+  absl::StatusOr<Literal> GetParameterLiteral(HloModule* module,\n+                                              int64_t parameter_index,\n+                                              int64_t replica_id,\n+                                              const Array<int64_t>& data) {\n+    HloInstruction* param =\n+        module->entry_computation()->parameter_instruction(parameter_index);\n+\n+    int64_t num_replicas = data.dim(0);\n+    int64_t num_updates_per_replica = data.dim(2);\n+    Array<int64_t> replica_slice =\n+        data.Slice({replica_id, 0, 0},\n+                   {replica_id + 1, num_replicas, num_updates_per_replica});\n+    replica_slice.Reshape({num_replicas * num_updates_per_replica});\n+    return LiteralUtil::CreateFromArray(replica_slice)\n+        .Convert(param->shape().element_type());\n+  }\n+\n+  // Literates for the input and output data, offset, and size parameters of\n+  // the ragged-all-to-all. Each vector contains one literal per replica.\n+  std::vector<Literal> inputs_;\n+  std::vector<Literal> input_offsets_;\n+  std::vector<Literal> input_sizes_;\n+\n+  std::vector<Literal> expected_outputs_;\n+  std::vector<Literal> output_offsets_;\n+  std::vector<Literal> output_sizes_;\n+\n+  Literal output_init_;\n+\n+  RaggedAllToAllImplType impl_type_;\n+};\n+\n+class RaggedAllToAllTest : public RaggedAllToAllTestBase,\n+                           public ::testing::WithParamInterface<\n+                               std::tuple<bool, RaggedAllToAllImplType>> {\n+ public:\n+  RaggedAllToAllTest()\n+      : RaggedAllToAllTestBase(std::get<0>(GetParam()),\n+                               std::get<1>(GetParam())) {}\n+};\n+\n+TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+  HloModule module, num_partitions=1\n+\n+  ENTRY entry {\n+    input = f32[4] parameter(0)\n+    output = f32[4] parameter(1)\n+    input_offsets = s32[2] parameter(2)\n+    send_sizes = s32[2] parameter(3)\n+    output_offsets = s32[2] parameter(4)\n+    recv_sizes = s32[2] parameter(5)\n+    ROOT ra2a = f32[4] ragged-all-to-all(input, output, input_offsets,\n+    send_sizes, output_offsets, recv_sizes), replica_groups={{0,1}}\n+  })\";\n+\n+  const int64_t kNumReplicas = 2;\n+  const int64_t kNumPartitions = 1;\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas)\n+      << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+      << hlo_runner_->device_count() << \" available)\";\n+\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+\n+  TF_ASSERT_OK(CreateRandomTestData(module.get(),\n+                                    /*input_sizes=*/{/*replica_0=*/{1, 1},\n+                                                     /*replica_1=*/{3, 1}}));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n+                        /*device_assignment=*/nullptr,\n+                        /*num_replicas=*/kNumReplicas,\n+                        /*run_hlo_passes=*/true));\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[0], results[0]));\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[1], results[1]));\n+}\n+\n+TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_InputBufferLargerThanOutput) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+  HloModule module, num_partitions=1\n+\n+  ENTRY entry {\n+    input = f32[32] parameter(0)\n+    output = f32[16] parameter(1)\n+    input_offsets = s32[2] parameter(2)\n+    send_sizes = s32[2] parameter(3)\n+    output_offsets = s32[2] parameter(4)\n+    recv_sizes = s32[2] parameter(5)\n+    ROOT ra2a = f32[16] ragged-all-to-all(input, output, input_offsets,\n+    send_sizes, output_offsets, recv_sizes), replica_groups={{0,1}}\n+  })\";\n+\n+  const int64_t kNumReplicas = 2;\n+  const int64_t kNumPartitions = 1;\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas)\n+      << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+      << hlo_runner_->device_count() << \" available)\";\n+\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+\n+  TF_ASSERT_OK(CreateRandomTestData(module.get(),\n+                                    /*input_sizes=*/{/*replica_0=*/{8, 5},\n+                                                     /*replica_1=*/{4, 3}}));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n+                        /*device_assignment=*/nullptr,\n+                        /*num_replicas=*/kNumReplicas,\n+                        /*run_hlo_passes=*/true));\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[0], results[0]));\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[1], results[1]));\n+}\n+\n+TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_OutputBufferLargerThanInput) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+  HloModule module, num_partitions=1\n+\n+  ENTRY entry {\n+    input = f32[16] parameter(0)\n+    output = f32[32] parameter(1)\n+    input_offsets = s32[2] parameter(2)\n+    send_sizes = s32[2] parameter(3)\n+    output_offsets = s32[2] parameter(4)\n+    recv_sizes = s32[2] parameter(5)\n+    ROOT ra2a = f32[32] ragged-all-to-all(input, output, input_offsets,\n+    send_sizes, output_offsets, recv_sizes), replica_groups={{0,1}}\n+  })\";\n+\n+  const int64_t kNumReplicas = 2;\n+  const int64_t kNumPartitions = 1;\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n+      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n+      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n+\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+\n+  TF_ASSERT_OK(CreateRandomTestData(module.get(),\n+                                    /*input_sizes=*/{/*replica_0=*/{4, 12},\n+                                                     /*replica_1=*/{5, 11}}));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n+                        /*device_assignment=*/nullptr,\n+                        /*num_replicas=*/kNumReplicas,\n+                        /*run_hlo_passes=*/true));\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[0], results[0]));\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[1], results[1]));\n+}\n+\n+TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_MultipleUpdates) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+  HloModule module, num_partitions=1\n+\n+  ENTRY entry {\n+    input = f32[8] parameter(0)\n+    output = f32[8] parameter(1)\n+    input_offsets = s32[4] parameter(2)\n+    send_sizes = s32[4] parameter(3)\n+    output_offsets = s32[4] parameter(4)\n+    recv_sizes = s32[4] parameter(5)\n+    ROOT ra2a = f32[8] ragged-all-to-all(input, output, input_offsets,\n+    send_sizes, output_offsets, recv_sizes), replica_groups={{0,1}}\n+  })\";\n+\n+  const int64_t kNumReplicas = 2;\n+  const int64_t kNumPartitions = 1;\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n+      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n+      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n+\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+\n+  TF_ASSERT_OK(CreateRandomTestData(\n+      module.get(), /*input_sizes=*/{/*replica_0=*/{{1, 2}, {2, 1}},\n+                                     /*replica_1=*/{{3, 1}, {1, 1}}}));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n+                        /*device_assignment=*/nullptr,\n+                        /*num_replicas=*/kNumReplicas,\n+                        /*run_hlo_passes=*/true));\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[0], results[0]));\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[1], results[1]));\n+}\n+\n+TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_MultiDimData) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+  HloModule module, num_partitions=1\n+\n+  ENTRY entry {\n+    input = bf16[16, 5, 32] parameter(0)\n+    output = bf16[16, 5, 32] parameter(1)\n+    input_offsets = s64[2] parameter(2)\n+    send_sizes = s64[2] parameter(3)\n+    output_offsets = s64[2] parameter(4)\n+    recv_sizes = s64[2] parameter(5)\n+    ROOT ra2a = bf16[16, 5, 32] ragged-all-to-all(input, output,\n+      input_offsets, send_sizes, output_offsets, recv_sizes),\n+      replica_groups={{0,1}}\n+  })\";\n+\n+  const int64_t kNumReplicas = 2;\n+  const int64_t kNumPartitions = 1;\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n+      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n+      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n+\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+\n+  TF_ASSERT_OK(CreateRandomTestData(module.get(),\n+                                    /*input_sizes=*/{/*replica_0=*/{4, 7},\n+                                                     /*replica_1=*/{2, 5}}));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n+                        /*device_assignment=*/nullptr,\n+                        /*num_replicas=*/kNumReplicas,\n+                        /*run_hlo_passes=*/true));\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[0], results[0]));\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[1], results[1]));\n+}\n+\n+TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_Degenerate) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+  HloModule module\n+\n+  ENTRY entry {\n+    input = f32[4] parameter(0)\n+    output = f32[4] parameter(1)\n+    input_offsets = s32[1] parameter(2)\n+    send_sizes = s32[1] parameter(3)\n+    output_offsets = s32[1] parameter(4)\n+    recv_sizes = s32[1] parameter(5)\n+    ROOT ra2a = f32[4] ragged-all-to-all(input, output, input_offsets,\n+    send_sizes, output_offsets, recv_sizes), replica_groups={{0},{1}}\n+  })\";\n+\n+  const int64_t kNumReplicas = 2;\n+  const int64_t kNumPartitions = 1;\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n+      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n+      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n+\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+\n+  TF_ASSERT_OK(CreateRandomTestData(module.get(),\n+                                    /*input_sizes=*/{/*replica_0=*/{1},\n+                                                     /*replica_1=*/{3}}));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n+                        /*device_assignment=*/nullptr,\n+                        /*num_replicas=*/kNumReplicas,\n+                        /*run_hlo_passes=*/true));\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[0], results[0]));\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[1], results[1]));\n+}\n+\n+TEST_P(RaggedAllToAllTest, RaggedAllToAll_2GPUs_NonDefaultLayout) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+  HloModule module\n+\n+  ENTRY entry {\n+    input = f32[16,4,8]{0,2,1} parameter(0)\n+    output = f32[16,4,8]{0,1,2} parameter(1)\n+    input_offsets = s32[2] parameter(2)\n+    send_sizes = s32[2] parameter(3)\n+    output_offsets = s32[2] parameter(4)\n+    recv_sizes = s32[2] parameter(5)\n+    ROOT ra2a = f32[16,4,8]{0,1,2} ragged-all-to-all(input, output,\n+      input_offsets, send_sizes, output_offsets, recv_sizes),\n+      replica_groups={{0,1}}\n+  })\";\n+\n+  const int64_t kNumReplicas = 2;\n+  const int64_t kNumPartitions = 1;\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n+      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n+      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n+\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+\n+  auto ragged_all_to_all =\n+      FindInstruction(module.get(), HloOpcode::kRaggedAllToAll);\n+  EXPECT_THAT(ragged_all_to_all, NotNull());\n+\n+  TF_ASSERT_OK(CreateRandomTestData(module.get(),\n+                                    /*input_sizes=*/{/*replica_0=*/{4, 7},\n+                                                     /*replica_1=*/{2, 5}}));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n+                        /*device_assignment=*/nullptr,\n+                        /*num_replicas=*/kNumReplicas,\n+                        /*run_hlo_passes=*/true));\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[0], results[0]));\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[1], results[1]));\n+}\n+\n+TEST_P(RaggedAllToAllTest,\n+       RaggedAllToAll_2GPUs_DevicesInReplicaGroupInReverseOrder) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+  HloModule module, num_partitions=1\n+\n+  ENTRY entry {\n+    input = f32[4] parameter(0)\n+    output = f32[4] parameter(1)\n+    input_offsets = s32[2] parameter(2)\n+    send_sizes = s32[2] parameter(3)\n+    output_offsets = s32[2] parameter(4)\n+    recv_sizes = s32[2] parameter(5)\n+    ROOT ra2a = f32[4] ragged-all-to-all(input, output, input_offsets,\n+    send_sizes, output_offsets, recv_sizes), replica_groups={{1,0}}\n+  })\";\n+\n+  const int64_t kNumReplicas = 2;\n+  const int64_t kNumPartitions = 1;\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n+      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n+      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n+\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+\n+  TF_ASSERT_OK(CreateRandomTestData(module.get(),\n+                                    /*input_sizes=*/{/*replica_0=*/{1, 1},\n+                                                     /*replica_1=*/{3, 1}}));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n+                        /*device_assignment=*/nullptr,\n+                        /*num_replicas=*/kNumReplicas,\n+                        /*run_hlo_passes=*/true));\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[0], results[0]));\n+  EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[1], results[1]));\n+}\n+\n+TEST_P(RaggedAllToAllTest, RaggedAllToAll_8GPUs) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+  HloModule module, num_partitions=1\n+\n+  ENTRY entry {\n+    input = f32[512, 5, 32] parameter(0)\n+    output = f32[512, 5, 32] parameter(1)\n+    input_offsets = s32[32] parameter(2)\n+    send_sizes = s32[32] parameter(3)\n+    output_offsets = s32[32] parameter(4)\n+    recv_sizes = s32[32] parameter(5)\n+    ROOT ra2a = f32[512, 5, 32] ragged-all-to-all(input, output,\n+      input_offsets, send_sizes, output_offsets, recv_sizes),\n+      replica_groups={{0,1,2,3,4,5,6,7}}\n+  })\";\n+\n+  const int64_t kNumReplicas = 8;\n+  const int64_t kNumPartitions = 1;\n+  const int64_t kNumUpdatesPerReplica = 4;\n+  if (hlo_runner_->device_count() < kNumReplicas * kNumPartitions) {\n+    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas * kNumPartitions\n+                 << \" devices (\" << hlo_runner_->device_count()\n+                 << \" available)\";\n+  }\n+\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+\n+  Array<int64_t> input_sizes(\n+      {kNumReplicas, kNumReplicas, kNumUpdatesPerReplica});\n+  input_sizes.FillRandomUniform(0, 10);\n+\n+  TF_ASSERT_OK(CreateRandomTestData(module.get(), input_sizes));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n+                        /*device_assignment=*/nullptr,\n+                        /*num_replicas=*/kNumReplicas,\n+                        /*run_hlo_passes=*/true));\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+\n+  for (int i = 0; i < kNumReplicas; ++i) {\n+    EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[i], results[i]));\n+  }\n+}\n+\n+TEST_P(RaggedAllToAllTest, RaggedAllToAll_8GPUs_2ReplicasPerGroups) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+  HloModule module, num_partitions=1\n+\n+  ENTRY entry {\n+    input = f32[512, 5, 32] parameter(0)\n+    output = f32[512, 5, 32] parameter(1)\n+    input_offsets = s32[32] parameter(2)\n+    send_sizes = s32[32] parameter(3)\n+    output_offsets = s32[32] parameter(4)\n+    recv_sizes = s32[32] parameter(5)\n+    ROOT ra2a = f32[512, 5, 32] ragged-all-to-all(input, output,\n+      input_offsets, send_sizes, output_offsets, recv_sizes),\n+      replica_groups={{0,4},{1,5},{2,6},{3,7}}\n+  })\";\n+\n+  const int64_t kNumReplicas = 8;\n+  const int64_t kNumReplicasPerGroup = 2;\n+  const int64_t kNumPartitions = 1;\n+  const int64_t kNumUpdatesPerReplica = 16;\n+  if (hlo_runner_->device_count() < kNumReplicas * kNumPartitions) {\n+    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas * kNumPartitions\n+                 << \" devices (\" << hlo_runner_->device_count()\n+                 << \" available)\";\n+  }\n+\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+\n+  Array<int64_t> input_sizes(\n+      {kNumReplicas, kNumReplicasPerGroup, kNumUpdatesPerReplica});\n+  input_sizes.FillRandomUniform(0, 10);\n+\n+  TF_ASSERT_OK(CreateRandomTestData(module.get(), input_sizes));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n+                        /*device_assignment=*/nullptr,\n+                        /*num_replicas=*/kNumReplicas,\n+                        /*run_hlo_passes=*/true));\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+\n+  for (int i = 0; i < kNumReplicas; ++i) {\n+    EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[i], results[i]));\n+  }\n+}\n+\n+TEST_P(RaggedAllToAllTest, RaggedAllToAll_8GPUs_4ReplicasPerGroups) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+  HloModule module, num_partitions=1\n+\n+  ENTRY entry {\n+    input = f32[512, 5, 32] parameter(0)\n+    output = f32[512, 5, 32] parameter(1)\n+    input_offsets = s32[32] parameter(2)\n+    send_sizes = s32[32] parameter(3)\n+    output_offsets = s32[32] parameter(4)\n+    recv_sizes = s32[32] parameter(5)\n+    ROOT ra2a = f32[512, 5, 32] ragged-all-to-all(input, output,\n+      input_offsets, send_sizes, output_offsets, recv_sizes),\n+      replica_groups={{0,1,2,3},{4,5,6,7}}\n+  })\";\n+\n+  const int64_t kNumReplicas = 8;\n+  const int64_t kNumReplicasPerGroup = 4;\n+  const int64_t kNumPartitions = 1;\n+  const int64_t kNumUpdatesPerReplica = 8;\n+  if (hlo_runner_->device_count() < kNumReplicas * kNumPartitions) {\n+    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas * kNumPartitions\n+                 << \" devices (\" << hlo_runner_->device_count()\n+                 << \" available)\";\n+  }\n+\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+\n+  Array<int64_t> input_sizes(\n+      {kNumReplicas, kNumReplicasPerGroup, kNumUpdatesPerReplica});\n+  input_sizes.FillRandomUniform(0, 10);\n+\n+  TF_ASSERT_OK(CreateRandomTestData(module.get(), input_sizes));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n+                        /*device_assignment=*/nullptr,\n+                        /*num_replicas=*/kNumReplicas,\n+                        /*run_hlo_passes=*/true));\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+\n+  for (int i = 0; i < kNumReplicas; ++i) {\n+    EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[i], results[i]));\n+  }\n+}\n+\n+std::string RaggedAllToAllImplTypeName(\n+    RaggedAllToAllImplType ragged_all_to_all_impl_type) {\n+  switch (ragged_all_to_all_impl_type) {\n+    case RaggedAllToAllImplType::kNccl:\n+      return \"nccl\";\n+    case RaggedAllToAllImplType::kMemcpy:\n+      return \"memcpy\";\n+    case RaggedAllToAllImplType::kDecomposer:\n+      return \"decomposer\";\n+    case RaggedAllToAllImplType::kOneShot:\n+      return \"one_shot\";\n+    default:\n+      LOG(FATAL) << \"Unknown ragged all-to-all implementation type.\";\n+  }\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    RaggedAllToAllTest, RaggedAllToAllTest,\n+    ::testing::Combine(::testing::Bool(),\n+                       ::testing::Values(RaggedAllToAllImplType::kNccl,\n+                                         RaggedAllToAllImplType::kMemcpy,\n+                                         RaggedAllToAllImplType::kDecomposer,\n+                                         RaggedAllToAllImplType::kOneShot)),\n+    [](const ::testing::TestParamInfo<std::tuple<bool, RaggedAllToAllImplType>>&\n+           info) {\n+      return absl::StrCat(std::get<0>(info.param) ? \"async\" : \"sync\", \"_\",\n+                          RaggedAllToAllImplTypeName(std::get<1>(info.param)));\n+    });\n+\n+class RaggedAllToAllMultiHostDecomposerTest\n+    : public RaggedAllToAllTestBase,\n+      public ::testing::WithParamInterface<std::tuple<int64_t, int64_t>> {\n+ public:\n+  RaggedAllToAllMultiHostDecomposerTest()\n+      : RaggedAllToAllTestBase(/*enable_async=*/false,\n+                               /*impl_type=*/RaggedAllToAllImplType::kOneShot) {\n+  }\n+\n+ protected:\n+  DebugOptions GetDebugOptionsForTest() const override {\n+    DebugOptions debug_options =\n+        RaggedAllToAllTestBase::GetDebugOptionsForTest();\n+    debug_options\n+        .set_xla_gpu_unsupported_enable_ragged_all_to_all_multi_host_decomposer(\n+            true);\n+    return debug_options;\n+  }\n+};\n+\n+TEST_P(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_2GPUs_SliceSize1) {\n+  auto [num_input_rows, num_output_rows] = GetParam();\n+\n+  std::string kModuleReplicatedStr =\n+      absl::Substitute(R\"(\n+  HloModule module, num_partitions=1\n+\n+  ENTRY entry {\n+    input = f32[$0,5,32] parameter(0)\n+    output = f32[$1,5,32] parameter(1)\n+    input_offsets = s32[32] parameter(2)\n+    send_sizes = s32[32] parameter(3)\n+    output_offsets = s32[32] parameter(4)\n+    recv_sizes = s32[32] parameter(5)\n+    ROOT ra2a = f32[$1,5,32] ragged-all-to-all(input, output,\n+      input_offsets, send_sizes, output_offsets, recv_sizes),\n+      replica_groups={{0,1}}\n+  })\",\n+                       num_input_rows, num_output_rows);\n+\n+  const int64_t kNumReplicas = 2;\n+  const int64_t kNumPartitions = 1;\n+  const int64_t kNumUpdatesPerReplica = 16;\n+  ASSERT_GE(hlo_runner_->device_count(), kNumReplicas * kNumPartitions)\n+      << \"Test requires at least \" << kNumReplicas * kNumPartitions\n+      << \" devices (\" << hlo_runner_->device_count() << \" available)\";\n+\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+\n+  config.mutable_debug_options()\n+      .set_xla_gpu_unsupported_override_fast_interconnect_slice_size(1);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+\n+  Array<int64_t> input_sizes(\n+      {kNumReplicas, kNumReplicas, kNumUpdatesPerReplica});\n+  input_sizes.FillRandomUniform(0, 10);\n+\n+  TF_ASSERT_OK(CreateRandomTestData(module.get(), input_sizes));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n+                        /*device_assignment=*/nullptr,\n+                        /*num_replicas=*/kNumReplicas,\n+                        /*run_hlo_passes=*/true));\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+\n+  for (int i = 0; i < kNumReplicas; ++i) {\n+    EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[i], results[i]));\n+  }\n+}\n+\n+TEST_P(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_8GPUs_SliceSize4) {\n+  auto [num_input_rows, num_output_rows] = GetParam();\n+\n+  std::string kModuleReplicatedStr =\n+      absl::Substitute(R\"(\n+  HloModule module, num_partitions=1\n+\n+  ENTRY entry {\n+    input = f32[$0,5,32] parameter(0)\n+    output = f32[$1,5,32] parameter(1)\n+    input_offsets = s32[32] parameter(2)\n+    send_sizes = s32[32] parameter(3)\n+    output_offsets = s32[32] parameter(4)\n+    recv_sizes = s32[32] parameter(5)\n+    ROOT ra2a = f32[$1,5,32] ragged-all-to-all(input, output,\n+      input_offsets, send_sizes, output_offsets, recv_sizes),\n+      replica_groups={{0,1,2,3,4,5,6,7}}\n+  })\",\n+                       num_input_rows, num_output_rows);\n+\n+  const int64_t kNumReplicas = 8;\n+  const int64_t kNumPartitions = 1;\n+  const int64_t kNumUpdatesPerReplica = 4;\n+  if (hlo_runner_->device_count() < kNumReplicas * kNumPartitions) {\n+    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas * kNumPartitions\n+                 << \" devices (\" << hlo_runner_->device_count()\n+                 << \" available)\";\n+  }\n+\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+\n+  config.mutable_debug_options()\n+      .set_xla_gpu_unsupported_override_fast_interconnect_slice_size(4);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+\n+  Array<int64_t> input_sizes(\n+      {kNumReplicas, kNumReplicas, kNumUpdatesPerReplica});\n+  input_sizes.FillRandomUniform(0, 16);\n+\n+  TF_ASSERT_OK(CreateRandomTestData(module.get(), input_sizes));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n+                        /*device_assignment=*/nullptr,\n+                        /*num_replicas=*/kNumReplicas,\n+                        /*run_hlo_passes=*/true));\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+\n+  for (int i = 0; i < kNumReplicas; ++i) {\n+    EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[i], results[i]));\n+  }\n+}\n+\n+TEST_P(RaggedAllToAllMultiHostDecomposerTest,\n+       RaggedAllToAll_8GPUs_SliceSize4_2ReplicaGroups) {\n+  auto [num_input_rows, num_output_rows] = GetParam();\n+\n+  std::string kModuleReplicatedStr =\n+      absl::Substitute(R\"(\n+  HloModule module, num_partitions=1\n+\n+  ENTRY entry {\n+    input = f32[$0,5,32] parameter(0)\n+    output = f32[$1,5,32] parameter(1)\n+    input_offsets = s32[32] parameter(2)\n+    send_sizes = s32[32] parameter(3)\n+    output_offsets = s32[32] parameter(4)\n+    recv_sizes = s32[32] parameter(5)\n+    ROOT ra2a = f32[$1,5,32] ragged-all-to-all(input, output,\n+      input_offsets, send_sizes, output_offsets, recv_sizes),\n+      replica_groups={{0,2,4,6},{1,3,5,7}}\n+  })\",\n+                       num_input_rows, num_output_rows);\n+\n+  const int64_t kNumReplicas = 8;\n+  const int64_t kNumReplicasPerGroup = 4;\n+  const int64_t kNumPartitions = 1;\n+  const int64_t kNumUpdatesPerReplica = 8;\n+  if (hlo_runner_->device_count() < kNumReplicas * kNumPartitions) {\n+    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas * kNumPartitions\n+                 << \" devices (\" << hlo_runner_->device_count()\n+                 << \" available)\";\n+  }\n+\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas * kNumPartitions);\n+\n+  config.mutable_debug_options()\n+      .set_xla_gpu_unsupported_override_fast_interconnect_slice_size(4);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, ParseAndReturnVerifiedModule(kModuleReplicatedStr, config));\n+\n+  Array<int64_t> input_sizes(\n+      {kNumReplicas, kNumReplicasPerGroup, kNumUpdatesPerReplica});\n+  input_sizes.FillRandomUniform(0, 10);\n+\n+  TF_ASSERT_OK(CreateRandomTestData(module.get(), input_sizes));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs(),\n+                        /*device_assignment=*/nullptr,\n+                        /*num_replicas=*/kNumReplicas,\n+                        /*run_hlo_passes=*/true));\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+\n+  for (int i = 0; i < kNumReplicas; ++i) {\n+    EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[i], results[i]));\n+  }\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    RaggedAllToAllMultiHostDecomposerTest,\n+    RaggedAllToAllMultiHostDecomposerTest,\n+    ::testing::Values(std::make_tuple(512, 4096), std::make_tuple(4096, 512)),\n+    [](const ::testing::TestParamInfo<std::tuple<int64_t, int64_t>>& info) {\n+      if (std::get<0>(info.param) > std::get<1>(info.param)) {\n+        return absl::StrCat(\"combine_\", std::get<0>(info.param), \"_\",\n+                            std::get<1>(info.param));\n+      }\n+      return absl::StrCat(\"dispatch_\", std::get<0>(info.param), \"_\",\n+                          std::get<1>(info.param));\n+    });\n+\n+}  // namespace\n+}  // namespace xla"
        }
    ],
    "stats": {
        "total": 2127,
        "additions": 1122,
        "deletions": 1005
    }
}