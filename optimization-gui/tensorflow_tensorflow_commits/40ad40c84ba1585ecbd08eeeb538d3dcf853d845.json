{
    "author": "tensorflower-gardener",
    "message": "Instrument a metric to record mixed_priority_batching_policy.\n\nIn order to monitor the usage of different mixed priority batching policies in production, this CL\n- adds a metric to record the value of the mixed_priority_batching_policy.\n    - Metric name: `\"/tensorflow/serving/batching/mixed_priority_batching_policy\"`\n- modifies an unit test to verify the metric is exported successfully.\n\nPiperOrigin-RevId: 840925152",
    "sha": "40ad40c84ba1585ecbd08eeeb538d3dcf853d845",
    "files": [
        {
            "sha": "abc5fa34f3a3abd188e698f0b350d9cc5e5c2575",
            "filename": "tensorflow/core/kernels/batching_util/batch_resource_base.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/40ad40c84ba1585ecbd08eeeb538d3dcf853d845/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/40ad40c84ba1585ecbd08eeeb538d3dcf853d845/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base.cc?ref=40ad40c84ba1585ecbd08eeeb538d3dcf853d845",
            "patch": "@@ -275,6 +275,19 @@ void RecordBatchParamPaddingPolicy(const std::string& batch_padding_policy,\n   cell->GetCell(model_name, op_name)->Set(batch_padding_policy);\n }\n \n+static auto* mixed_priority_batching_policy_value =\n+    monitoring::Gauge<std::string, 2>::New(\n+        \"/tensorflow/serving/batching/mixed_priority_batching_policy\",\n+        \"The value of BatchFunction.mixed_priority_batching_policy attribute.\",\n+        \"model_name\", \"op_name\");\n+\n+void RecordBatchParamMixedPriorityBatchingPolicy(\n+    MixedPriorityBatchingPolicy mixed_priority_batching_policy,\n+    const std::string& model_name, const std::string& op_name) {\n+  mixed_priority_batching_policy_value->GetCell(model_name, op_name)\n+      ->Set(absl::StrCat(mixed_priority_batching_policy));\n+}\n+\n void RecordBatchParamMaxEnqueuedBatches(int64_t max_enqueued_batches,\n                                         const std::string& model_name,\n                                         const std::string& op_name) {\n@@ -454,6 +467,9 @@ absl::Status BatchResourceBase::RegisterInput(\n     RecordBatchParamPaddingPolicy(\n         this->batcher_queue_options_.batch_padding_policy,\n         GetModelName(context), context->op_kernel().name());\n+    RecordBatchParamMixedPriorityBatchingPolicy(\n+        this->batcher_queue_options_.mixed_priority_batching_policy,\n+        GetModelName(context), context->op_kernel().name());\n   } else if (adaptive_batcher_) {\n     RecordBatchParamBatchTimeoutMicros(\n         adaptive_batcher_queue_options_.batch_timeout_micros,"
        },
        {
            "sha": "0e781747bcf170634b684a7d7bbfdc6ac662c4c6",
            "filename": "tensorflow/core/kernels/batching_util/batch_resource_base_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/40ad40c84ba1585ecbd08eeeb538d3dcf853d845/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/40ad40c84ba1585ecbd08eeeb538d3dcf853d845/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fbatching_util%2Fbatch_resource_base_test.cc?ref=40ad40c84ba1585ecbd08eeeb538d3dcf853d845",
            "patch": "@@ -112,6 +112,8 @@ class BatchResourceBaseWithPriorityTest\n         \"/tensorflow/serving/batching/processed_batch_size_v2\");\n     padding_size_v2_reader_ = std::make_unique<CellReader<Histogram>>(\n         \"/tensorflow/serving/batching/padding_size_v2\");\n+    mixed_priority_policy_reader_ = std::make_unique<CellReader<std::string>>(\n+        \"/tensorflow/serving/batching/mixed_priority_batching_policy\");\n     // Create device_.\n     device_ = DeviceFactory::NewDevice(\"CPU\", SessionOptions{},\n                                        \"/job:a/replica:0/task:0\");\n@@ -166,6 +168,7 @@ class BatchResourceBaseWithPriorityTest\n \n   std::unique_ptr<CellReader<int64_t>> processed_batch_size_v2_reader_;\n   std::unique_ptr<CellReader<Histogram>> padding_size_v2_reader_;\n+  std::unique_ptr<CellReader<std::string>> mixed_priority_policy_reader_;\n   std::unique_ptr<Device> device_;\n   std::unique_ptr<OpKernel> batch_kernel_;\n   Tensor input_tensor_;\n@@ -279,6 +282,9 @@ TEST_P(BatchResourceBaseWithPriorityTest, BatchingWithMixedPriorityPolicy) {\n         /*forced_warmup_batch_size=*/0));\n   }\n   blocking_counter.Wait();\n+  EXPECT_EQ(\n+      mixed_priority_policy_reader_->Read(\"my_model_name\", \"my_batch_node\"),\n+      absl::StrCat(GetParam().mixed_priority_batching_policy));\n \n   for (const auto& [batch_size, expected_count] :\n        GetParam().expected_batch_size_count) {"
        }
    ],
    "stats": {
        "total": 22,
        "additions": 22,
        "deletions": 0
    }
}