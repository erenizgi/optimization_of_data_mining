{
    "author": "allanrenucci",
    "message": "Make `StreamExecutorGpuTopologyDescription` optional in `StreamExecutorGpuClient`.\n\nCreating an instance of `StreamExecutorGpuTopologyDescription` with a null gpu topology is a footgun. While previously allowed, calling most methods on the resulting object would cause a segfault dereferencing the null topology. We now enforce `gpu_toplogy` is not null in the `StreamExecutorGpuTopologyDescription` constructor.\n\nIdeally `StreamExecutorGpuClient` should also reject null topologies but this breaks Tensorflow.\n\nPiperOrigin-RevId: 801778713",
    "sha": "b7c74389d13a6a67137396f735edb88d759a7a9a",
    "files": [
        {
            "sha": "278b7da74ac75dba60ded78b7ef66b2e6095edbe",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/ifrt/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b7c74389d13a6a67137396f735edb88d759a7a9a/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b7c74389d13a6a67137396f735edb88d759a7a9a/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2FBUILD?ref=b7c74389d13a6a67137396f735edb88d759a7a9a",
            "patch": "@@ -212,18 +212,17 @@ tf_cc_test(\n         \"@llvm-project//mlir:AllPassesAndDialects\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Parser\",\n-        \"@llvm-project//mlir:RegisterAllDialects\",\n+        \"@llvm-project//mlir:RegisterAllDialects\",  # buildcleaner: keep\n         \"@local_tsl//tsl/platform:protobuf\",\n-        \"@local_xla//xla/pjrt:pjrt_client\",\n         \"@local_xla//xla/pjrt:pjrt_compiler\",\n-        \"@local_xla//xla/pjrt/gpu:se_gpu_pjrt_client\",\n         \"@local_xla//xla/pjrt/plugin/xla_cpu:cpu_topology_description\",\n         \"@local_xla//xla/python/ifrt\",\n         \"@local_xla//xla/python/ifrt:mock\",\n         \"@local_xla//xla/python/ifrt:test_util\",\n         \"@local_xla//xla/python/pjrt_ifrt\",\n         \"@local_xla//xla/python/pjrt_ifrt:tfrt_cpu_client_test_lib\",\n         \"@local_xla//xla/service:computation_placer_hdr\",\n+        \"@local_xla//xla/tsl/platform:statusor\",\n     ],\n )\n "
        },
        {
            "sha": "22acb4d136e6c5be5f118a1cf698de01adb3124c",
            "filename": "tensorflow/compiler/mlir/tfrt/transforms/ifrt/tf2hlo_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b7c74389d13a6a67137396f735edb88d759a7a9a/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Ftf2hlo_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b7c74389d13a6a67137396f735edb88d759a7a9a/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Ftf2hlo_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftfrt%2Ftransforms%2Fifrt%2Ftf2hlo_test.cc?ref=b7c74389d13a6a67137396f735edb88d759a7a9a",
            "patch": "@@ -37,8 +37,6 @@ limitations under the License.\n #include \"tensorflow/compiler/mlir/tensorflow/dialect_registration.h\"\n #include \"tensorflow/compiler/mlir/tfrt/transforms/ifrt/ifrt_types.h\"\n #include \"tensorflow/compiler/tf2xla/xla_helpers.h\"\n-#include \"xla/pjrt/gpu/se_gpu_pjrt_client.h\"\n-#include \"xla/pjrt/pjrt_client.h\"\n #include \"xla/pjrt/pjrt_compiler.h\"\n #include \"xla/pjrt/plugin/xla_cpu/cpu_topology_description.h\"\n #include \"xla/python/ifrt/client.h\"\n@@ -47,6 +45,7 @@ limitations under the License.\n #include \"xla/python/pjrt_ifrt/pjrt_topology.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"tensorflow/core/platform/resource_loader.h\"\n #include \"tensorflow/core/platform/test.h\"\n #include \"tsl/platform/protobuf.h\"\n@@ -57,7 +56,6 @@ namespace {\n using ::testing::Eq;\n using ::testing::HasSubstr;\n using ::testing::Ne;\n-using tsl::testing::StatusIs;\n \n // TODO(b/229726259): Make EqualsProto available in OSS\n class ProtoStringMatcher {\n@@ -533,9 +531,7 @@ TEST_F(Tf2HloTest, GpuCompile) {\n       .entry_function_name = \"main\",\n       .compile_metadata = compile_metadata,\n       .shape_representation_fn = tensorflow::IdentityShapeRepresentationFn(),\n-      .topology = std::make_shared<xla::ifrt::PjRtTopology>(\n-          std::make_shared<xla::StreamExecutorGpuTopologyDescription>(\n-              xla::CudaId(), xla::CudaName(), /*gpu_topology=*/nullptr)),\n+      .topology = nullptr,\n       .platform_name = xla::CudaName(),\n   };\n "
        },
        {
            "sha": "580d9dd19c2559da619ce54daf46dc7e93989aad",
            "filename": "third_party/xla/xla/pjrt/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b7c74389d13a6a67137396f735edb88d759a7a9a/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b7c74389d13a6a67137396f735edb88d759a7a9a/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD?ref=b7c74389d13a6a67137396f735edb88d759a7a9a",
            "patch": "@@ -631,6 +631,7 @@ cc_library(\n         \"//xla/stream_executor:device_description_proto_cc\",\n         \"//xla/tsl/lib/strings:proto_serialization\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\","
        },
        {
            "sha": "4491fcccb00747fc9f9b3673ecf115b86db397e3",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 9,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b7c74389d13a6a67137396f735edb88d759a7a9a/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b7c74389d13a6a67137396f735edb88d759a7a9a/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=b7c74389d13a6a67137396f735edb88d759a7a9a",
            "patch": "@@ -635,12 +635,14 @@ StreamExecutorGpuClient::StreamExecutorGpuClient(\n           should_stage_host_to_device_transfers, std::move(gpu_run_options)),\n       num_nodes_(num_nodes),\n       abort_collectives_on_failure_(abort_collectives_on_failure),\n-      topology_(xla::StreamExecutorGpuTopologyDescription(\n-          tsl::Fingerprint64(platform_name), platform_name,\n-          std::move(gpu_topology), GetAttrsForDevices(addressable_devices()),\n-          GetTargetConfigForDevices(addressable_devices()))),\n       kv_store_(std::move(kv_store)),\n       distributed_client_(std::move(distributed_client)) {\n+  if (gpu_topology != nullptr) {\n+    topology_.emplace(tsl::Fingerprint64(platform_name), platform_name,\n+                      std::move(gpu_topology),\n+                      GetAttrsForDevices(addressable_devices()),\n+                      GetTargetConfigForDevices(addressable_devices()));\n+  }\n   const int basePinnedId = device_count();\n   for (auto* device : addressable_devices()) {\n     // Use the device id to construct a globally unique memory space id. We do\n@@ -941,11 +943,9 @@ absl::Status StreamExecutorGpuClient::UpdateCompileOptionsInternal(\n     bool lookup_addressable_devices) {\n   TF_RETURN_IF_ERROR(PjRtStreamExecutorClient::UpdateCompileOptionsInternal(\n       options, returned_extras, lookup_addressable_devices));\n-  // TODO: Fix null topology usage in TF.\n-  // https://github.com/search?q=repo%3Atensorflow%2Ftensorflow%20%2F*gpu_topology%3D*%2Fnullptr&type=code\n-  if (topology_.gpu_topology_ptr() != nullptr) {\n+  if (topology_) {\n     options->executable_build_options.set_slice_size(\n-        topology_.gpu_topology().slice_size());\n+        topology_->gpu_topology().slice_size());\n   }\n   return absl::OkStatus();\n }\n@@ -1161,9 +1161,20 @@ StreamExecutorGpuClient::MakeCrossHostReceiveBuffers(\n   return buffers;\n }\n \n+absl::StatusOr<const xla::PjRtTopologyDescription*>\n+StreamExecutorGpuClient::GetTopologyDescription() const {\n+  if (!topology_.has_value()) {\n+    return absl::FailedPreconditionError(\"GPU Topology is missing\");\n+  }\n+  return &*topology_;\n+}\n+\n absl::StatusOr<Layout> StreamExecutorGpuClient::GetDefaultLayout(\n     PrimitiveType element_type, absl::Span<const int64_t> dims) {\n-  return topology_.GetDefaultLayout(element_type, dims);\n+  if (!topology_.has_value()) {\n+    return absl::FailedPreconditionError(\"GPU Topology is missing\");\n+  }\n+  return topology_->GetDefaultLayout(element_type, dims);\n }\n \n absl::StatusOr<std::unique_ptr<PjRtLoadedExecutable>>"
        },
        {
            "sha": "7df2a04d825f2b17ef9444acfb14cf1ea675b9af",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.h",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b7c74389d13a6a67137396f735edb88d759a7a9a/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b7c74389d13a6a67137396f735edb88d759a7a9a/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h?ref=b7c74389d13a6a67137396f735edb88d759a7a9a",
            "patch": "@@ -154,9 +154,7 @@ class StreamExecutorGpuClient : public xla::PjRtStreamExecutorClient {\n                               PjRtCrossHostRecvNotifier notifier) override;\n \n   absl::StatusOr<const xla::PjRtTopologyDescription*> GetTopologyDescription()\n-      const override {\n-    return &topology_;\n-  }\n+      const override;\n \n   absl::StatusOr<Layout> GetDefaultLayout(\n       PrimitiveType element_type, absl::Span<const int64_t> dims) override;\n@@ -186,7 +184,7 @@ class StreamExecutorGpuClient : public xla::PjRtStreamExecutorClient {\n \n   std::optional<int> num_nodes_;\n   const bool abort_collectives_on_failure_ = false;\n-  xla::StreamExecutorGpuTopologyDescription topology_;\n+  std::optional<xla::StreamExecutorGpuTopologyDescription> topology_;\n   std::shared_ptr<KeyValueStoreInterface> kv_store_;\n   std::shared_ptr<DistributedRuntimeClient> distributed_client_;\n "
        },
        {
            "sha": "9c2cc7a6461f7a0cb3080a3f90a38eb16ebba865",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_topology_description.h",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b7c74389d13a6a67137396f735edb88d759a7a9a/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_topology_description.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b7c74389d13a6a67137396f735edb88d759a7a9a/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_topology_description.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_topology_description.h?ref=b7c74389d13a6a67137396f735edb88d759a7a9a",
            "patch": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/container/flat_hash_map.h\"\n+#include \"absl/log/check.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n@@ -45,7 +46,9 @@ class StreamExecutorGpuTopologyDescription : public PjRtTopologyDescription {\n         platform_name_(platform_name),\n         gpu_topology_(std::move(gpu_topology)),\n         attributes_(attributes),\n-        target_config_(std::move(target_config)) {}\n+        target_config_(std::move(target_config)) {\n+    CHECK(gpu_topology_ != nullptr);\n+  }\n \n   bool operator==(const StreamExecutorGpuTopologyDescription& other) const {\n     return this->platform_id() == other.platform_id() &&"
        }
    ],
    "stats": {
        "total": 54,
        "additions": 31,
        "deletions": 23
    }
}