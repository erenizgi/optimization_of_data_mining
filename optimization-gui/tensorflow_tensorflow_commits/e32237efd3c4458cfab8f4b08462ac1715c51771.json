{
    "author": "pifon2a",
    "message": "[XLA:GPU] Split SortRewriter into two passes.\n\n1. SortRewriter: Actually rewrites the sort and puts 1 elem scratch size\n2. EstimateCubScratchSize: Talks to the runner to understand how much memory is needed for the scratch space.\n\nPiperOrigin-RevId: 846436222",
    "sha": "e32237efd3c4458cfab8f4b08462ac1715c51771",
    "files": [
        {
            "sha": "5125bbc4b1fb92073cd73519bc95dc58745eb57c",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=e32237efd3c4458cfab8f4b08462ac1715c51771",
            "patch": "@@ -1565,12 +1565,8 @@ cc_library(\n \n cc_library(\n     name = \"gpu_compiler\",\n-    srcs = [\n-        \"gpu_compiler.cc\",\n-    ],\n-    hdrs = [\n-        \"gpu_compiler.h\",\n-    ],\n+    srcs = [\"gpu_compiler.cc\"],\n+    hdrs = [\"gpu_compiler.h\"],\n     tags = [\"gpu\"],\n     deps = [\n         \":alias_info\",\n@@ -1772,6 +1768,7 @@ cc_library(\n         \"//xla/service/gpu/transforms:dot_strength_reduction\",\n         \"//xla/service/gpu/transforms:double_buffer_loop_unrolling\",\n         \"//xla/service/gpu/transforms:dynamic_slice_fusion_rewriter\",\n+        \"//xla/service/gpu/transforms:estimate_cub_scratch_size\",\n         \"//xla/service/gpu/transforms:explicit_collectives_group_async_wrapper\",\n         \"//xla/service/gpu/transforms:explicit_stream_annotation_async_wrapper\",\n         \"//xla/service/gpu/transforms:fusion_block_level_rewriter\",\n@@ -1848,6 +1845,7 @@ cc_library(\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:status\",\n+        \"//xla/tsl/platform:status_macros\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base\","
        },
        {
            "sha": "ecaf28349a6b0788311008e900cb0845064802a5",
            "filename": "third_party/xla/xla/service/gpu/cublas_cudnn.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcublas_cudnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcublas_cudnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcublas_cudnn.cc?ref=e32237efd3c4458cfab8f4b08462ac1715c51771",
            "patch": "@@ -99,6 +99,8 @@ const absl::string_view kCudnnfMHASoftmaxDropoutBackwardCallTarget =\n     \"__cudnn$fmhaSoftmaxDropoutBackward\";\n \n const absl::string_view kCubDeviceRadixSortTarget = \"__cub$DeviceRadixSort\";\n+const absl::string_view kCubDeviceRadixSortUnassignedScratchSizeTarget =\n+    \"__cub$DeviceRadixSortUnassignedScratchSize\";\n \n bool IsCustomCallToDnnConvolution(const HloInstruction& hlo) {\n   if (hlo.opcode() != HloOpcode::kCustomCall) {\n@@ -186,6 +188,12 @@ bool IsCubDeviceRadixSort(const HloInstruction& hlo) {\n          hlo.custom_call_target() == kCubDeviceRadixSortTarget;\n }\n \n+bool IsCubDeviceRadixSortNoScratchSize(const HloInstruction& hlo) {\n+  return hlo.opcode() == HloOpcode::kCustomCall &&\n+         hlo.custom_call_target() ==\n+             kCubDeviceRadixSortUnassignedScratchSizeTarget;\n+}\n+\n absl::StatusOr<CudnnConvKind> GetCudnnConvKind(\n     const HloCustomCallInstruction* instr) {\n   absl::string_view target = instr->custom_call_target();"
        },
        {
            "sha": "034ec33c9dc98304907c6f08a192a716e00013a7",
            "filename": "third_party/xla/xla/service/gpu/cublas_cudnn.h",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcublas_cudnn.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcublas_cudnn.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcublas_cudnn.h?ref=e32237efd3c4458cfab8f4b08462ac1715c51771",
            "patch": "@@ -219,7 +219,13 @@ bool IsCustomCallToBlockScaledDot(const HloInstruction& hlo);\n // Reference: https://nvlabs.github.io/cub/\n extern const absl::string_view kCubDeviceRadixSortTarget;\n \n+// CUB library call that allows to not specify the scratch size.\n+// EstimateCubScratchSizePass will assign the correct scratch size.\n+extern const absl::string_view kCubDeviceRadixSortUnassignedScratchSizeTarget;\n+\n bool IsCubDeviceRadixSort(const HloInstruction& hlo);\n+bool IsCubDeviceRadixSortNoScratchSize(const HloInstruction& hlo);\n+\n }  // namespace gpu\n }  // namespace xla\n "
        },
        {
            "sha": "73e15a18b29f4b044f4e11a0b4fad5bc35dd4b20",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 86,
            "deletions": 91,
            "changes": 177,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=e32237efd3c4458cfab8f4b08462ac1715c51771",
            "patch": "@@ -232,6 +232,7 @@ limitations under the License.\n #include \"xla/service/gpu/transforms/dot_strength_reduction.h\"\n #include \"xla/service/gpu/transforms/double_buffer_loop_unrolling.h\"\n #include \"xla/service/gpu/transforms/dynamic_slice_fusion_rewriter.h\"\n+#include \"xla/service/gpu/transforms/estimate_cub_scratch_size.h\"\n #include \"xla/service/gpu/transforms/explicit_collectives_group_async_wrapper.h\"\n #include \"xla/service/gpu/transforms/explicit_stream_annotation_async_wrapper.h\"\n #include \"xla/service/gpu/transforms/fusion_wrapper.h\"\n@@ -321,6 +322,7 @@ limitations under the License.\n #include \"tsl/platform/protobuf.h\"  // IWYU pragma: keep\n #include \"tsl/profiler/lib/scoped_annotation.h\"\n #include \"tsl/profiler/lib/traceme.h\"\n+#include \"xla/tsl/platform/status_macros.h\"\n \n #ifdef PLATFORM_GOOGLE\n #include \"xla/hlo/experimental/auto_sharding/auto_sharding.h\"\n@@ -532,7 +534,7 @@ absl::Status SetHostDeviceType(HloInstruction* instr) {\n   TF_ASSIGN_OR_RETURN(auto backend_config,\n                       instr->backend_config<GpuBackendConfig>());\n   backend_config.set_device_type(DEVICE_TYPE_HOST);\n-  TF_RETURN_IF_ERROR(instr->set_backend_config(backend_config));\n+  RETURN_IF_ERROR(instr->set_backend_config(backend_config));\n   return absl::OkStatus();\n }\n \n@@ -557,10 +559,9 @@ bool BackendConfigDeviceTypeIsHost(HloInstruction* instr) {\n }  // namespace\n \n absl::Status RunOptimizationPasses(\n-    HloModule* hlo_module, stream_executor::StreamExecutor* stream_exec,\n-    const Compiler::GpuTargetConfig& gpu_target_config,\n+    HloModule* hlo_module, const Compiler::GpuTargetConfig& gpu_target_config,\n     const AlgebraicSimplifierOptions& layout_insensitive_algsimp_opts,\n-    absl::string_view platform_name) {\n+    absl::string_view platform_name, bool enable_sort_rewriter) {\n   const DebugOptions& debug_options = hlo_module->config().debug_options();\n   se::GpuComputeCapability gpu_version =\n       gpu_target_config.device_description.gpu_compute_capability();\n@@ -617,19 +618,10 @@ absl::Status RunOptimizationPasses(\n   // would do.\n   pipeline.AddPass<PermutationSortExpander>();\n \n-  // SortRewriter needs to ask the device how much scratch space is needed,\n-  // which isn't feasible if we don't have a device.\n-  if (hlo_module->config().debug_options().xla_gpu_enable_cub_radix_sort()) {\n-    if (stream_exec != nullptr) {\n-      pipeline.AddPass<SortRewriter>(gpu_target_config.device_description,\n-                                     std::string{platform_name});\n-    } else {\n-      LOG(WARNING) << \"Using fallback sort algorithm rather than SortRewriter, \"\n-                      \"which will be slower at runtime. To avoid this, \"\n-                      \"compile with a GPU present.\";\n-    }\n+  if (enable_sort_rewriter) {\n+    pipeline.AddPass<SortRewriter>(gpu_target_config.device_description,\n+                                   std::string{platform_name});\n   }\n-\n   // Comparison total order expander\n   pipeline.AddPass<ComparisonExpander>(std::array{std::make_pair(BF16, F32)});\n \n@@ -720,17 +712,10 @@ absl::Status RunOptimizationPasses(\n   // DynamicPadder creates a stable KeyValue sort for dynamic reshapes.\n   pipeline.AddPass<DynamicPadder>(dynamic_padder_options);\n   // SortRewriter needs to run before StableSortExpander.\n-  if (debug_options.xla_gpu_enable_cub_radix_sort()) {\n-    if (stream_exec != nullptr) {\n-      pipeline.AddPass<SortRewriter>(gpu_target_config.device_description,\n-                                     gpu_target_config.platform_name);\n-    } else {\n-      LOG(WARNING) << \"Using fallback sort algorithm rather than SortRewriter, \"\n-                      \"which will be slower at runtime. To avoid this, \"\n-                      \"compile with a GPU present.\";\n-    }\n+  if (enable_sort_rewriter) {\n+    pipeline.AddPass<SortRewriter>(gpu_target_config.device_description,\n+                                   gpu_target_config.platform_name);\n   }\n-\n   // Expand the sort op to support stable sorting if required.\n   pipeline.AddPass<StableSortExpander>();\n \n@@ -1146,17 +1131,17 @@ absl::Status RunFusionPasses(HloModule* hlo_module,\n \n   HloPassPipeline pre_fusion(\"pre-fusion\");\n   pre_fusion.AddPass<AddTrackingSuffixToInstructionNames>();\n-  TF_RETURN_IF_ERROR(pre_fusion.Run(hlo_module).status());\n+  RETURN_IF_ERROR(pre_fusion.Run(hlo_module).status());\n \n-  TF_RETURN_IF_ERROR(\n-      FusionPipeline(hlo_module->config().debug_options(), shape_size_fn,\n-                     alias_info, thread_pool, gpu_device_info, mlir_context)\n-          .Run(hlo_module, {HloInstruction::kMainExecutionThread})\n-          .status());\n+  RETURN_IF_ERROR(FusionPipeline(hlo_module->config().debug_options(),\n+                                 shape_size_fn, alias_info, thread_pool,\n+                                 gpu_device_info, mlir_context)\n+                      .Run(hlo_module, {HloInstruction::kMainExecutionThread})\n+                      .status());\n \n   if (VLOG_IS_ON(2)) {\n     HloFusionStatsVisitor stats;\n-    TF_RETURN_IF_ERROR(hlo_module->entry_computation()->Accept(&stats));\n+    RETURN_IF_ERROR(hlo_module->entry_computation()->Accept(&stats));\n     VLOG(2) << stats.ToString();\n   }\n \n@@ -1384,7 +1369,7 @@ absl::Status RunDynamicSliceFusionPasses(HloModule* hlo_module,\n           });\n       return hero_op.has_value();\n     });\n-    TF_RETURN_IF_ERROR(\n+    RETURN_IF_ERROR(\n         pipeline.Run(hlo_module, {HloInstruction::kMainExecutionThread})\n             .status());\n   }\n@@ -1497,14 +1482,14 @@ absl::Status GpuCompiler::OptimizeHloModule(\n         ClearBackendConfigDeviceType;\n     pipeline.AddPass<HloHostDeviceTypeCallWrapper>(\n         hlo_host_device_type_call_wrapper_options);\n-    TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());\n+    RETURN_IF_ERROR(pipeline.Run(hlo_module).status());\n   }\n \n-  TF_RETURN_IF_ERROR(RunPreSPMDPartitionerPasses(hlo_module));\n+  RETURN_IF_ERROR(RunPreSPMDPartitionerPasses(hlo_module));\n   // Set max_windowed_einsum_iteration to slice_size, as there will be\n   // significant overhead when scaled beyond the maximum size of the\n   // fast-interconnect domain.\n-  TF_RETURN_IF_ERROR(\n+  RETURN_IF_ERROR(\n       RunSPMDPasses(hlo_module, gpu_target_config, alias_info,\n                     layout_insensitive_algsimp_opts,\n                     /*max_windowed_einsum_iteration=*/options.slice_size));\n@@ -1514,7 +1499,7 @@ absl::Status GpuCompiler::OptimizeHloModule(\n     pipeline.AddPass<HostComputeAsyncifier>(BackendConfigDeviceTypeIsHost);\n     pipeline.AddPass<HostOffloadingPrepare>(\n         HostOffloadingPrepare::Rewrite::kConvertToCustomCall);\n-    TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());\n+    RETURN_IF_ERROR(pipeline.Run(hlo_module).status());\n   }\n \n   // Dump the HLO module after SPMD partitioning. There should be no more Python\n@@ -1523,12 +1508,23 @@ absl::Status GpuCompiler::OptimizeHloModule(\n   TF_ASSIGN_OR_RETURN(\n       const stream_executor::Platform* platform,\n       stream_executor::PlatformManager::PlatformWithId(PlatformId()));\n-  TF_RETURN_IF_ERROR(\n-      RunOptimizationPasses(hlo_module, stream_exec, gpu_target_config,\n-                            layout_insensitive_algsimp_opts, platform->Name()));\n+\n+  // SortRewriter needs to ask the device how much scratch space is needed,\n+  // which isn't feasible if we don't have a device.\n+  bool enable_sort_rewriter =\n+      hlo_module->config().debug_options().xla_gpu_enable_cub_radix_sort();\n+  if (stream_exec == nullptr && !options.early_exit_with_layouts) {\n+    LOG(WARNING) << \"Using fallback sort algorithm rather than SortRewriter, \"\n+                    \"which will be slower at runtime. To avoid this, \"\n+                    \"compile with a GPU present.\";\n+    enable_sort_rewriter = false;\n+  }\n+  RETURN_IF_ERROR(RunOptimizationPasses(\n+      hlo_module, gpu_target_config, layout_insensitive_algsimp_opts,\n+      platform->Name(), enable_sort_rewriter));\n   se::GpuComputeCapability gpu_version =\n       device_description.gpu_compute_capability();\n-  TF_RETURN_IF_ERROR(RunCollectiveOptimizationPasses(\n+  RETURN_IF_ERROR(RunCollectiveOptimizationPasses(\n       hlo_module, options, layout_insensitive_algsimp_opts, gpu_version,\n       platform->VisibleDeviceCount(), pointer_size_));\n \n@@ -1540,17 +1536,17 @@ absl::Status GpuCompiler::OptimizeHloModule(\n     TF_ASSIGN_OR_RETURN(dnn_version, GetDnnVersionInfo(stream_exec));\n   }\n \n-  TF_RETURN_IF_ERROR(OptimizeHloConvolutionCanonicalization(\n+  RETURN_IF_ERROR(OptimizeHloConvolutionCanonicalization(\n       hlo_module, gpu_version, dnn_version,\n       device_description.runtime_version()));\n \n-  TF_RETURN_IF_ERROR(RunLayoutAssignmentPasses(\n-      hlo_module, gpu_version, dnn_version, device_description));\n+  RETURN_IF_ERROR(RunLayoutAssignmentPasses(hlo_module, gpu_version,\n+                                            dnn_version, device_description));\n   if (options.early_exit_with_layouts) {\n     return absl::OkStatus();\n   }\n \n-  TF_RETURN_IF_ERROR(RunLayoutNormalizationPasses(\n+  RETURN_IF_ERROR(RunLayoutNormalizationPasses(\n       hlo_module,\n       GetAlgebraicSimplifierOptions(\n           AlgebraicSimplifierMode::kLayoutNormalization,\n@@ -1559,33 +1555,33 @@ absl::Status GpuCompiler::OptimizeHloModule(\n       gpu_version));\n \n   // Run target-specific HLO optimization passes after layout assignment.\n-  TF_RETURN_IF_ERROR(OptimizeHloPostLayoutAssignment(\n+  RETURN_IF_ERROR(OptimizeHloPostLayoutAssignment(\n       hlo_module, stream_exec, options, gpu_target_config, alias_info,\n       thread_pool.get_mutable()));\n \n   // This is a \"low effort, high impact\" fusion that should be run first.\n-  TF_RETURN_IF_ERROR(\n+  RETURN_IF_ERROR(\n       RunDynamicSliceFusionPasses(hlo_module, /*platform_id=*/PlatformId()));\n \n-  TF_RETURN_IF_ERROR(\n+  RETURN_IF_ERROR(\n       RunFusionPasses(hlo_module, gpu_target_config, thread_pool.get_mutable(),\n                       ShapeSizeBytesFunction(), alias_info, &mlir_context_));\n-  TF_RETURN_IF_ERROR(RunPostFusionPasses(hlo_module, device_description,\n-                                         alias_info, pointer_size_, options,\n-                                         &mlir_context_));\n-  TF_RETURN_IF_ERROR(RunAsyncCollectivesConversionPasses(hlo_module));\n-  TF_RETURN_IF_ERROR(RunPostFusionSimplificationPasses(\n+  RETURN_IF_ERROR(RunPostFusionPasses(hlo_module, device_description,\n+                                      alias_info, pointer_size_, options,\n+                                      &mlir_context_));\n+  RETURN_IF_ERROR(RunAsyncCollectivesConversionPasses(hlo_module));\n+  RETURN_IF_ERROR(RunPostFusionSimplificationPasses(\n       hlo_module,\n       GetAlgebraicSimplifierOptions(\n           AlgebraicSimplifierMode::kPostFusionSimplification,\n           hlo_module->config().debug_options(),\n           gpu_target_config.platform_name == \"ROCM\"),\n       gpu_version, gpu_target_config));\n \n-  TF_RETURN_IF_ERROR(RunPostFusionVerificationPasses(\n+  RETURN_IF_ERROR(RunPostFusionVerificationPasses(\n       hlo_module, stream_exec, options, gpu_target_config, &mlir_context_));\n \n-  TF_RETURN_IF_ERROR(\n+  RETURN_IF_ERROR(\n       RunCollectiveScheduleLinearizerPasses(hlo_module, stream_exec));\n \n   {\n@@ -1596,17 +1592,17 @@ absl::Status GpuCompiler::OptimizeHloModule(\n         DebugOptions::DETECTION_MODE_NONE) {\n       pipeline.AddPass<UnstableReductionDetector>();\n     }\n-    TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());\n+    RETURN_IF_ERROR(pipeline.Run(hlo_module).status());\n   }\n \n-  TF_RETURN_IF_ERROR(RunAsyncDotPasses(hlo_module));\n+  RETURN_IF_ERROR(RunAsyncDotPasses(hlo_module));\n   {\n     HloPassPipeline pipeline(\"autotune-fusion-emitters\");\n     pipeline.AddPass<FusionWrapper>(gpu_target_config.device_description);\n-    TF_RETURN_IF_ERROR(AddFusionAutotuningPass(\n+    RETURN_IF_ERROR(AddFusionAutotuningPass(\n         &pipeline, hlo_module, options, thread_pool.get_mutable(), stream_exec,\n         &gpu_target_config, ShapeSizeBytesFunction()));\n-    TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());\n+    RETURN_IF_ERROR(pipeline.Run(hlo_module).status());\n   }\n \n   return absl::OkStatus();\n@@ -1811,7 +1807,8 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n     // annotations, this pass will add the annotations.\n     pipeline.AddPass<SubByteNormalization>(\n         SubByteNormalization::SET_ELEMENT_SIZE);\n-    TF_RETURN_IF_ERROR(\n+    pipeline.AddPass<EstimateCubScratchSize>(gpu_target_config.platform_name);\n+    RETURN_IF_ERROR(\n         pipeline.Run(hlo_module, {HloInstruction::kMainExecutionThread})\n             .status());\n   }\n@@ -1832,7 +1829,7 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n   // f32).\n   add_float_normalization(pipeline);\n \n-  TF_RETURN_IF_ERROR(AddGemmFusionAutotuningPasses(\n+  RETURN_IF_ERROR(AddGemmFusionAutotuningPasses(\n       &pipeline, hlo_module, autotune_config, thread_pool,\n       options.key_value_store,\n       gpu_target_config.device_description.runtime_version(), stream_exec));\n@@ -1848,7 +1845,7 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n   AddGemmRewriterPasses(pipeline, debug_options, gpu_version,\n                         gpu_target_config.device_description.runtime_version());\n \n-  TF_RETURN_IF_ERROR(AddConvAndGemmAutotuningPasses(\n+  RETURN_IF_ERROR(AddConvAndGemmAutotuningPasses(\n       &pipeline, gpu_version, options, hlo_module, autotune_config, thread_pool,\n       stream_exec, &gpu_target_config));\n \n@@ -1930,7 +1927,7 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(\n       \"end-of-post-layout_assignment\");\n #endif  // NDEBUG\n \n-  TF_RETURN_IF_ERROR(\n+  RETURN_IF_ERROR(\n       pipeline.Run(hlo_module, {HloInstruction::kMainExecutionThread})\n           .status());\n   return absl::OkStatus();\n@@ -1947,7 +1944,7 @@ GpuCompiler::GetTargetConfig(const Compiler::CompileOptions& options,\n   }\n   if (!debug_opts.xla_gpu_target_config_filename().empty()) {\n     std::string gpu_target_config_string;\n-    TF_RETURN_IF_ERROR(tsl::ReadFileToString(\n+    RETURN_IF_ERROR(tsl::ReadFileToString(\n         tsl::Env::Default(), debug_opts.xla_gpu_target_config_filename(),\n         &gpu_target_config_string));\n     stream_executor::GpuTargetConfigProto gpu_target_config_proto;\n@@ -1990,7 +1987,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> GpuCompiler::RunHloPasses(\n   }\n \n   const DebugOptions debug_opts = module->config().debug_options();\n-  TF_RETURN_IF_ERROR(LoadAutotuneResultsFromFile(debug_opts));\n+  RETURN_IF_ERROR(LoadAutotuneResultsFromFile(debug_opts));\n   bool is_deviceless = options.gpu_target_config.has_value() ||\n                        !debug_opts.xla_gpu_target_config_filename().empty();\n \n@@ -2012,15 +2009,15 @@ absl::StatusOr<std::unique_ptr<HloModule>> GpuCompiler::RunHloPasses(\n   const se::DeviceDescription& device_description =\n       gpu_target_config.device_description;\n   std::unique_ptr<GpuAliasInfo> alias_info = GetAliasInfo(device_description);\n-  TF_RETURN_IF_ERROR(\n+  RETURN_IF_ERROR(\n       OptimizeHloModule(module.get(), is_deviceless ? nullptr : stream_exec,\n                         options, gpu_target_config, alias_info.get()));\n   if (options.early_exit_with_layouts) {\n     return std::move(module);\n   }\n \n-  TF_RETURN_IF_ERROR(RunPreSchedulingCopyInsertion(*module, device_description,\n-                                                   alias_info.get()));\n+  RETURN_IF_ERROR(RunPreSchedulingCopyInsertion(*module, device_description,\n+                                                alias_info.get()));\n \n   uint64_t end_usecs = tsl::Env::Default()->NowMicros();\n \n@@ -2037,9 +2034,8 @@ absl::StatusOr<std::unique_ptr<HloModule>> GpuCompiler::RunHloPasses(\n       AutotuneConfig autotune_config,\n       AutotuneConfig::FromDebugOptions(device_config, debug_opts));\n   if (!is_deviceless) {\n-    TF_RETURN_IF_ERROR(\n-        AutotunerUtil::SerializeAutotuneResults(&autotune_results));\n-    TF_RETURN_IF_ERROR(SerializeAutotuneResultsToFile(debug_opts));\n+    RETURN_IF_ERROR(AutotunerUtil::SerializeAutotuneResults(&autotune_results));\n+    RETURN_IF_ERROR(SerializeAutotuneResultsToFile(debug_opts));\n   }\n   const std::optional<std::string> optimized_fingerprint =\n       MaybeUploadOptimizedGpuSymbols(module.get(), autotune_results);\n@@ -2100,7 +2096,7 @@ absl::Status RunPostSchedulingCopyInsertion(HloModule* module,\n           ? kRegionBasedLiveRangeAnalysisLimit\n           : 0;\n   CopyInsertion copy_insertion(alias_info, kUseRegionBasedLiveRangeAnalysis);\n-  TF_RETURN_IF_ERROR(copy_insertion.RemoveUnnecessaryCopies(module));\n+  RETURN_IF_ERROR(copy_insertion.RemoveUnnecessaryCopies(module));\n \n   // Stash away the schedule during copy insertion, to avoid validation failures\n   // while the module is in flux.\n@@ -2111,10 +2107,10 @@ absl::Status RunPostSchedulingCopyInsertion(HloModule* module,\n   // whether it is legal to remove a copy. However, copies in the graph may be\n   // necessary for other reason such as preventing a constant from being live\n   // out of the graph. So run AddSpecialCaseCopies to re-insert these copies.\n-  TF_RETURN_IF_ERROR(copy_insertion.CopyInsertion::AddSpecialCaseCopies(\n+  RETURN_IF_ERROR(copy_insertion.CopyInsertion::AddSpecialCaseCopies(\n       module, /*execution_threads=*/{}, ShouldAddCopyForCollectiveMemorySpace));\n \n-  TF_RETURN_IF_ERROR(HloDCE().Run(module).status());\n+  RETURN_IF_ERROR(HloDCE().Run(module).status());\n \n   // The passes above can add and remove copies, update the schedule to\n   // account for these transformations. Newly added instructions will be\n@@ -2123,8 +2119,8 @@ absl::Status RunPostSchedulingCopyInsertion(HloModule* module,\n   // Update and restore the schedule. The saved schedule has a reference to the\n   // updated HLO module. The saved schedule needs to be updated before restoring\n   // it to the module to avoid validation failures.\n-  TF_RETURN_IF_ERROR(saved_schedule.Update());\n-  TF_RETURN_IF_ERROR(module->set_schedule(std::move(saved_schedule)));\n+  RETURN_IF_ERROR(saved_schedule.Update());\n+  RETURN_IF_ERROR(module->set_schedule(std::move(saved_schedule)));\n \n   return absl::OkStatus();\n }\n@@ -2469,9 +2465,9 @@ absl::StatusOr<GpuCompiler::BackendCompileResult> GpuCompiler::CompileAndLink(\n               << current_cache.entries_size() << \" cached kernels.\";\n     }\n     if (!binaries_to_cache.empty()) {\n-      TF_RETURN_IF_ERROR(\n-          UpdateDiskKernelCache(resolved_path, /*do_append=*/cache_file_exists,\n-                                current_cache, binaries_to_cache));\n+      RETURN_IF_ERROR(UpdateDiskKernelCache(resolved_path,\n+                                            /*do_append=*/cache_file_exists,\n+                                            current_cache, binaries_to_cache));\n     }\n   }\n \n@@ -2513,15 +2509,15 @@ GpuCompiler::CompileToBackendResult(\n     const se::DeviceDescription& gpu_device_info) {\n   tsl::profiler::TraceMe traceme(\"CompileToBackendResult\");\n   std::unique_ptr<GpuAliasInfo> alias_info = GetAliasInfo(gpu_device_info);\n-  TF_RETURN_IF_ERROR(\n+  RETURN_IF_ERROR(\n       RunPreSchedulingPasses(module, gpu_device_info, alias_info.get()));\n   TF_ASSIGN_OR_RETURN(ScheduleMetadata schedule_metadata,\n                       ScheduleGpuModule(module, pointer_size_, gpu_device_info,\n                                         &mlir_context_, alias_info.get()));\n   HloPassPipeline pipeline(\"scheduled-gpu-module\");\n   AddHloVerifier(&pipeline);\n-  TF_RETURN_IF_ERROR(pipeline.Run(module).status());\n-  TF_RETURN_IF_ERROR(\n+  RETURN_IF_ERROR(pipeline.Run(module).status());\n+  RETURN_IF_ERROR(\n       RunPostSchedulingPipelines(module, schedule_metadata.scheduler_mem_limit,\n                                  gpu_device_info, alias_info.get()));\n \n@@ -2640,8 +2636,8 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n \n   BinaryMap dnn_compiled_graphs;\n   if (stream_exec) {\n-    TF_RETURN_IF_ERROR(RunCudnnCompilerPasses(module.get(), stream_exec,\n-                                              &dnn_compiled_graphs));\n+    RETURN_IF_ERROR(RunCudnnCompilerPasses(module.get(), stream_exec,\n+                                           &dnn_compiled_graphs));\n   }\n \n   const DebugOptions& debug_opts = module->config().debug_options();\n@@ -2671,7 +2667,7 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(\n     cost_analysis_options.set_bytes_per_second(\n         gpu_device_info.memory_bandwidth());\n     GpuHloCostAnalysis cost_analysis(cost_analysis_options, gpu_device_info);\n-    TF_RETURN_IF_ERROR(module->entry_computation()->Accept(&cost_analysis));\n+    RETURN_IF_ERROR(module->entry_computation()->Accept(&cost_analysis));\n     VLOG(1) << absl::StrFormat(\n         \"#module=%s,program_id=%d# estimated memory r+w %s\", module->name(),\n         module->unique_id(),\n@@ -2995,7 +2991,7 @@ absl::Status GpuCompiler::RunPostSchedulingPipelines(\n     const se::DeviceDescription& gpu_device_info,\n     const GpuAliasInfo* alias_info) {\n   tsl::profiler::TraceMe traceme(\"RunPostSchedulingPipelines\");\n-  TF_RETURN_IF_ERROR(RunPostSchedulingCopyInsertion(module, alias_info));\n+  RETURN_IF_ERROR(RunPostSchedulingCopyInsertion(module, alias_info));\n   HloPassPipeline main_pipeline(\"post-scheduling-passes\");\n \n   // Pipeline for async -> sync conversion on for non-overlapped async ops.\n@@ -3080,7 +3076,7 @@ absl::Status GpuCompiler::LoadAutotuneResultsFromFile(\n     absl::call_once(once, [&file_path, &status] {\n       status = AutotunerUtil::LoadAutotuneResultsFromFile(file_path);\n     });\n-    TF_RETURN_IF_ERROR(status);\n+    RETURN_IF_ERROR(status);\n   }\n   return absl::OkStatus();\n }\n@@ -3093,8 +3089,7 @@ absl::Status GpuCompiler::SerializeAutotuneResultsToFile(\n       !file_path.empty()) {\n     // Warning: This writes the autotune results at every compilation,\n     // possibly multiple times per process.\n-    TF_RETURN_IF_ERROR(\n-        AutotunerUtil::SerializeAutotuneResultsToFile(file_path));\n+    RETURN_IF_ERROR(AutotunerUtil::SerializeAutotuneResultsToFile(file_path));\n   }\n   return absl::OkStatus();\n }\n@@ -3167,7 +3162,7 @@ GpuCompiler::LoadExecutableFromAotResult(\n       hlo_module->config()\n           .debug_options()\n           .xla_gpu_enable_llvm_module_compilation_parallelism()) {\n-    TF_RETURN_IF_ERROR(LoadCache(ir_emitter_context, cache_file_path));\n+    RETURN_IF_ERROR(LoadCache(ir_emitter_context, cache_file_path));\n   }\n \n   ThunkEmitter thunk_emitter(&ir_emitter_context);"
        },
        {
            "sha": "9e9b1289e1ca2fdb171491b5fe36b47f5a402cc1",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test.cc?ref=e32237efd3c4458cfab8f4b08462ac1715c51771",
            "patch": "@@ -2108,7 +2108,7 @@ ENTRY %main {\n   EXPECT_CALL(mock_log,\n               Log(absl::LogSeverity::kWarning, EndsWith(\"/gpu_compiler.cc\"),\n                   StartsWith(\"Using fallback sort algorithm\")))\n-      .Times(2);\n+      .Times(1);\n \n   // StartCapturingLogs has to be called even if we expect not to capture any\n   // logs."
        },
        {
            "sha": "8b3006db1b7a11dc10d04aebda8eaa17f25984ba",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 44,
            "deletions": 2,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=e32237efd3c4458cfab8f4b08462ac1715c51771",
            "patch": "@@ -2602,12 +2602,10 @@ cc_library(\n     srcs = [\"sort_rewriter.cc\"],\n     hdrs = [\"sort_rewriter.h\"],\n     deps = [\n-        \"//xla:comparison_util\",\n         \"//xla:literal_util\",\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/backends/gpu/runtime:cub_sort_thunk\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/pass:hlo_pass\",\n         \"//xla/service:pattern_matcher\",\n@@ -2635,6 +2633,7 @@ xla_test(\n         \"test_migrated_to_hlo_runner_pjrt\",\n     ],\n     deps = [\n+        \":estimate_cub_scratch_size\",\n         \":sort_rewriter\",\n         \"//xla:error_spec\",\n         \"//xla:shape_util\",\n@@ -2655,6 +2654,49 @@ xla_test(\n     ],\n )\n \n+cc_library(\n+    name = \"estimate_cub_scratch_size\",\n+    srcs = [\"estimate_cub_scratch_size.cc\"],\n+    hdrs = [\"estimate_cub_scratch_size.h\"],\n+    deps = [\n+        \"//xla:shape_util\",\n+        \"//xla:util\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/backends/gpu/runtime:cub_sort_thunk\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/pass:hlo_pass\",\n+        \"//xla/service/gpu:cublas_cudnn\",\n+        \"//xla/tsl/platform:status_macros\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n+xla_test(\n+    name = \"estimate_cub_scratch_size_test\",\n+    srcs = [\"estimate_cub_scratch_size_test.cc\"],\n+    backends = [\"h100\"],\n+    tags = [\n+        \"cuda-only\",\n+        \"test_migrated_to_hlo_runner_pjrt\",\n+    ],\n+    deps = [\n+        \":estimate_cub_scratch_size\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla/service:platform_util\",\n+        \"//xla/stream_executor:platform\",\n+        \"//xla/tests:hlo_pjrt_interpreter_reference_mixin\",\n+        \"//xla/tests:hlo_pjrt_test_base\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"stream_attribute_annotator\",\n     srcs = [\"stream_attribute_annotator.cc\"],"
        },
        {
            "sha": "63dd80dc641caed77e92ecc78fd381ef45749e6d",
            "filename": "third_party/xla/xla/service/gpu/transforms/estimate_cub_scratch_size.cc",
            "status": "added",
            "additions": 123,
            "deletions": 0,
            "changes": 123,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Festimate_cub_scratch_size.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Festimate_cub_scratch_size.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Festimate_cub_scratch_size.cc?ref=e32237efd3c4458cfab8f4b08462ac1715c51771",
            "patch": "@@ -0,0 +1,123 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/service/gpu/transforms/estimate_cub_scratch_size.h\"\n+\n+#include <cstdint>\n+#include <memory>\n+#include <optional>\n+#include <vector>\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/runtime/cub_sort_thunk.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/service/gpu/cublas_cudnn.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/util.h\"\n+#include \"xla/xla_data.pb.h\"\n+#include \"xla/tsl/platform/status_macros.h\"\n+\n+namespace xla::gpu {\n+\n+// Rewrites a single sort instruction with a custom call.\n+absl::StatusOr<bool> EstimateCubScratchSize::RunOnInstruction(\n+    HloCustomCallInstruction* custom_call) {\n+  CHECK_EQ(custom_call->custom_call_target(),\n+           kCubDeviceRadixSortUnassignedScratchSizeTarget);\n+  const Shape& key_shape = custom_call->operand(0)->shape();\n+  PrimitiveType key_type = key_shape.element_type();\n+  std::optional<PrimitiveType> value_type;\n+  if (custom_call->operand_count() == 2) {\n+    value_type = custom_call->operand(1)->shape().element_type();\n+  }\n+\n+  ASSIGN_OR_RETURN(\n+      std::unique_ptr<CubSortRunnerInterface> runner,\n+      CubSortRunnerInterface::Create(key_type, value_type, platform_name_));\n+\n+  int64_t num_elements = Product(key_shape.dimensions());\n+  // It is assumed that the sorting happens on the innermost dimension.\n+  int64_t batch_size = num_elements / key_shape.dimensions().back();\n+\n+  ASSIGN_OR_RETURN(int64_t scratch_size,\n+                   runner->GetScratchSize(num_elements, batch_size));\n+\n+  // Align and increase scratch size to fit the offsets.\n+  if (batch_size > 1) {\n+    scratch_size += sizeof(int) - scratch_size % sizeof(int);\n+    scratch_size += (batch_size + 1) * sizeof(int);\n+  }\n+\n+  // Update the custom call.\n+  Shape new_shape = custom_call->shape();\n+  new_shape.mutable_tuple_shapes()->back() =\n+      ShapeUtil::MakeShape(U8, {scratch_size});\n+  HloInstruction* new_custom_call =\n+      custom_call->AddInstruction(HloInstruction::CreateCustomCall(\n+          new_shape, absl::MakeSpan(custom_call->operands()),\n+          kCubDeviceRadixSortTarget));\n+  new_custom_call->SetupDerivedInstruction(custom_call);\n+  RETURN_IF_ERROR(custom_call->parent()->ReplaceInstructionWithDifferentShape(\n+      custom_call, new_custom_call));\n+  return true;\n+}\n+\n+// Rewrites the sorts in the given computation into calls to CUB.\n+absl::StatusOr<bool> EstimateCubScratchSize::RunOnComputation(\n+    HloComputation* computation) {\n+  std::vector<HloCustomCallInstruction*> custom_calls;\n+  for (auto* inst : computation->instructions()) {\n+    if (auto custom_call = DynCast<HloCustomCallInstruction>(inst)) {\n+      if (custom_call->custom_call_target() ==\n+          kCubDeviceRadixSortUnassignedScratchSizeTarget) {\n+        custom_calls.push_back(custom_call);\n+      }\n+    }\n+  }\n+  bool changed = false;\n+  for (auto* call : custom_calls) {\n+    ASSIGN_OR_RETURN(bool result, RunOnInstruction(call));\n+    changed |= result;\n+  }\n+  return changed;\n+}\n+\n+// Replace compatible sort operations with custom calls.\n+absl::StatusOr<bool> EstimateCubScratchSize::RunImpl(\n+    HloModule* module,\n+    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n+  XLA_VLOG_LINES(\n+      3, \"EstimateCubScratchSize::RunImpl(), before:\\n\" + module->ToString());\n+  bool changed = false;\n+  for (HloComputation* computation :\n+       module->MakeNonfusionComputations(execution_threads)) {\n+    ASSIGN_OR_RETURN(bool result, RunOnComputation(computation));\n+    changed |= result;\n+  }\n+  XLA_VLOG_LINES(\n+      3, \"EstimateCubScratchSize::RunImpl(), after:\\n\" + module->ToString());\n+  return changed;\n+}\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "a8207dbef547368ba6e53f415391912bc85494eb",
            "filename": "third_party/xla/xla/service/gpu/transforms/estimate_cub_scratch_size.h",
            "status": "added",
            "additions": 57,
            "deletions": 0,
            "changes": 57,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Festimate_cub_scratch_size.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Festimate_cub_scratch_size.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Festimate_cub_scratch_size.h?ref=e32237efd3c4458cfab8f4b08462ac1715c51771",
            "patch": "@@ -0,0 +1,57 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_SERVICE_GPU_TRANSFORMS_ESTIMATE_CUB_SCRATCH_SIZE_H_\n+#define XLA_SERVICE_GPU_TRANSFORMS_ESTIMATE_CUB_SCRATCH_SIZE_H_\n+\n+#include <string>\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/pass/hlo_pass_interface.h\"\n+\n+namespace xla::gpu {\n+\n+// Updates the scratch size of CUB sort custom calls to match the actual\n+// scratch size. Also changes the custom call target from\n+// kCubDeviceRadixSortUnassignedScratchSizeTarget to kCubDeviceRadixSortTarget.\n+class EstimateCubScratchSize : public HloModulePass {\n+ public:\n+  explicit EstimateCubScratchSize(std::string platform_name)\n+      : platform_name_(platform_name) {}\n+\n+  absl::string_view name() const override {\n+    return \"estimate-cub-scratch-size\";\n+  }\n+\n+ protected:\n+  absl::StatusOr<bool> RunOnInstruction(HloCustomCallInstruction* custom_call);\n+  absl::StatusOr<bool> RunOnComputation(HloComputation* computation);\n+\n+  absl::StatusOr<bool> RunImpl(\n+      HloModule* module,\n+      const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n+\n+ private:\n+  std::string platform_name_;\n+};\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_SERVICE_GPU_TRANSFORMS_ESTIMATE_CUB_SCRATCH_SIZE_H_"
        },
        {
            "sha": "499121a92b4c35cd038c972d2a3c2e88fe07e68d",
            "filename": "third_party/xla/xla/service/gpu/transforms/estimate_cub_scratch_size_test.cc",
            "status": "added",
            "additions": 317,
            "deletions": 0,
            "changes": 317,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Festimate_cub_scratch_size_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Festimate_cub_scratch_size_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Festimate_cub_scratch_size_test.cc?ref=e32237efd3c4458cfab8f4b08462ac1715c51771",
            "patch": "@@ -0,0 +1,317 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/service/gpu/transforms/estimate_cub_scratch_size.h\"\n+\n+#include <string>\n+\n+#include <gtest/gtest.h>\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/service/platform_util.h\"\n+#include \"xla/stream_executor/platform.h\"\n+#include \"xla/tests/hlo_pjrt_interpreter_reference_mixin.h\"\n+#include \"xla/tests/hlo_pjrt_test_base.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/xla_data.pb.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+class EstimateCubScratchSizeTest\n+    : public HloPjRtInterpreterReferenceMixin<HloPjRtTestBase> {\n+ public:\n+  void SetUp() override {\n+    HloPjRtInterpreterReferenceMixin<HloPjRtTestBase>::SetUp();\n+    ASSERT_OK_AND_ASSIGN(test_platform_, PlatformUtil::GetPlatform(\"gpu\"));\n+  }\n+\n+  void RunAndCheck(absl::string_view hlo, absl::string_view expected) {\n+    RunAndFilecheckHloRewrite(\n+        hlo, EstimateCubScratchSize(GetTestPlatform()->Name()), expected);\n+  }\n+\n+  const stream_executor::Platform* GetTestPlatform() const {\n+    return test_platform_;\n+  }\n+\n+ private:\n+  stream_executor::Platform* test_platform_ = nullptr;\n+};\n+\n+// Basic sort: ascending.\n+TEST_F(EstimateCubScratchSizeTest, U32_F32) {\n+  const char hlo[] = R\"(\n+    HloModule m\n+    ENTRY main {\n+      %keys = u32[1000] parameter(0)\n+      %values = f32[1000] parameter(1)\n+      %custom-call = (u32[1000]{0}, f32[1000]{0}, u8[1]{0})\n+        custom-call(%keys, %values),\n+        custom_call_target=\"__cub$DeviceRadixSortUnassignedScratchSize\",\n+        backend_config={\"descending\":false}\n+      ROOT %t = u32[1000]{0} get-tuple-element(%custom-call), index=0\n+  })\";\n+  RunAndCheck(hlo, R\"(\n+    // CHECK: (u32[1000]{0}, f32[1000]{0}, u8[1]{0}) custom-call\n+    // CHECK-SAME: custom_call_target=\"__cub$DeviceRadixSort\",\n+    // CHECK-SAME: backend_config={\"descending\":false}\n+  )\");\n+}\n+\n+TEST_F(EstimateCubScratchSizeTest, F32) {\n+  const char hlo[] = R\"(\n+    HloModule m\n+    ENTRY main {\n+      %keys = f32[1000] parameter(0)\n+      %custom-call = (f32[1000]{0}, u8[1]{0})\n+        custom-call(%keys),\n+        custom_call_target=\"__cub$DeviceRadixSortUnassignedScratchSize\",\n+        backend_config={\"descending\":false}\n+      ROOT %t = f32[1000]{0} get-tuple-element(%custom-call), index=0\n+  })\";\n+  RunAndCheck(hlo, R\"(\n+    // CHECK: (f32[1000]{0}, u8[1]{0}) custom-call\n+    // CHECK-SAME: custom_call_target=\"__cub$DeviceRadixSort\",\n+    // CHECK-SAME: backend_config={\"descending\":false}\n+  )\");\n+}\n+\n+TEST_F(EstimateCubScratchSizeTest, S32_S32) {\n+  const char hlo[] = R\"(\n+    HloModule m\n+    ENTRY main {\n+      %keys = s32[1000] parameter(0)\n+      %values = s32[1000] parameter(1)\n+      %custom-call = (s32[1000]{0}, s32[1000]{0}, u8[1]{0})\n+        custom-call(%keys, %values),\n+        custom_call_target=\"__cub$DeviceRadixSortUnassignedScratchSize\",\n+        backend_config={\"descending\":false}\n+      ROOT %t = s32[1000]{0} get-tuple-element(%custom-call), index=0\n+  })\";\n+  RunAndCheck(hlo, R\"(\n+    // CHECK: (s32[1000]{0}, s32[1000]{0}, u8[1]{0}) custom-call\n+    // CHECK-SAME: custom_call_target=\"__cub$DeviceRadixSort\",\n+    // CHECK-SAME: backend_config={\"descending\":false}\n+  )\");\n+}\n+\n+TEST_F(EstimateCubScratchSizeTest, F32_Descending) {\n+  const char hlo[] = R\"(\n+    HloModule m\n+    ENTRY main {\n+      %keys = f32[1000] parameter(0)\n+      %custom-call = (f32[1000]{0}, u8[1]{0})\n+        custom-call(%keys),\n+        custom_call_target=\"__cub$DeviceRadixSortUnassignedScratchSize\",\n+        backend_config={\"descending\":true}\n+      ROOT %t = f32[1000]{0} get-tuple-element(%custom-call), index=0\n+  })\";\n+  RunAndCheck(hlo, R\"(\n+    // CHECK: (f32[1000]{0}, u8[1]{0}) custom-call\n+    // CHECK-SAME: custom_call_target=\"__cub$DeviceRadixSort\",\n+    // CHECK-SAME: backend_config={\"descending\":true}\n+  )\");\n+}\n+\n+TEST_F(EstimateCubScratchSizeTest, F32_Rank3) {\n+  const char hlo[] = R\"(\n+    HloModule m\n+    ENTRY main {\n+      %keys = f32[10,10,10] parameter(0)\n+      %custom-call = (f32[10,10,10]{2,1,0}, u8[1]{0})\n+        custom-call(%keys),\n+        custom_call_target=\"__cub$DeviceRadixSortUnassignedScratchSize\",\n+        backend_config={\"descending\":false}\n+      ROOT %t = f32[10,10,10]{2,1,0} get-tuple-element(%custom-call), index=0\n+  })\";\n+  RunAndCheck(hlo, R\"(\n+    // CHECK: (f32[10,10,10]{2,1,0}, u8[4756]{0}) custom-call\n+    // CHECK-SAME: custom_call_target=\"__cub$DeviceRadixSort\",\n+    // CHECK-SAME: backend_config={\"descending\":false}\n+  )\");\n+}\n+\n+TEST_F(EstimateCubScratchSizeTest, F32_Rank2) {\n+  const char hlo[] = R\"(\n+    HloModule m\n+    ENTRY main {\n+      %keys = f32[10,100] parameter(0)\n+      %custom-call = (f32[10,100]{1,0}, u8[1]{0})\n+        custom-call(%keys),\n+        custom_call_target=\"__cub$DeviceRadixSortUnassignedScratchSize\",\n+        backend_config={\"descending\":false}\n+      ROOT %t = f32[10,100]{1,0} get-tuple-element(%custom-call), index=0\n+  })\";\n+  RunAndCheck(hlo, R\"(\n+    // CHECK: (f32[10,100]{1,0}, u8[4396]{0}) custom-call\n+    // CHECK-SAME: custom_call_target=\"__cub$DeviceRadixSort\",\n+    // CHECK-SAME: backend_config={\"descending\":false}\n+  )\");\n+}\n+\n+TEST_F(EstimateCubScratchSizeTest, U16_F16_Descending) {\n+  const char hlo[] = R\"(\n+    HloModule m\n+    ENTRY main {\n+      %keys = u16[16,128] parameter(0)\n+      %values = f16[16,128] parameter(1)\n+      %custom-call = (u16[16,128]{1,0}, f16[16,128]{1,0}, u8[1]{0})\n+        custom-call(%keys, %values),\n+        custom_call_target=\"__cub$DeviceRadixSortUnassignedScratchSize\",\n+        backend_config={\"descending\":true}\n+      ROOT %t = u16[16,128]{1,0} get-tuple-element(%custom-call), index=0\n+  })\";\n+  RunAndCheck(hlo, R\"(\n+    // CHECK: (u16[16,128]{1,0}, f16[16,128]{1,0}, u8[8516]{0}) custom-call\n+    // CHECK-SAME: custom_call_target=\"__cub$DeviceRadixSort\",\n+    // CHECK-SAME: backend_config={\"descending\":true}\n+  )\");\n+}\n+\n+TEST_F(EstimateCubScratchSizeTest, U32_F32_Rank2) {\n+  const char hlo[] = R\"(\n+    HloModule m\n+    ENTRY main {\n+      %keys = u32[16,128] parameter(0)\n+      %values = f32[16,128] parameter(1)\n+      %custom-call = (u32[16,128]{1,0}, f32[16,128]{1,0}, u8[1]{0})\n+        custom-call(%keys, %values),\n+        custom_call_target=\"__cub$DeviceRadixSortUnassignedScratchSize\",\n+        backend_config={\"descending\":false}\n+      ROOT %t = u32[16,128]{1,0} get-tuple-element(%custom-call), index=0\n+  })\";\n+  RunAndCheck(hlo, R\"(\n+    // CHECK: (u32[16,128]{1,0}, f32[16,128]{1,0}, u8[16708]{0}) custom-call\n+    // CHECK-SAME: custom_call_target=\"__cub$DeviceRadixSort\",\n+    // CHECK-SAME: backend_config={\"descending\":false}\n+  )\");\n+}\n+\n+TEST_F(EstimateCubScratchSizeTest, U64_F64_Descending) {\n+  const char hlo[] = R\"(\n+    HloModule m\n+    ENTRY main {\n+      %keys = u64[16,128] parameter(0)\n+      %values = f64[16,128] parameter(1)\n+      %custom-call = (u64[16,128]{1,0}, f64[16,128]{1,0}, u8[1]{0})\n+        custom-call(%keys, %values),\n+        custom_call_target=\"__cub$DeviceRadixSortUnassignedScratchSize\",\n+        backend_config={\"descending\":true}\n+      ROOT %t = u64[16,128]{1,0} get-tuple-element(%custom-call), index=0\n+  })\";\n+  RunAndCheck(hlo, R\"(\n+    // CHECK: (u64[16,128]{1,0}, f64[16,128]{1,0}, u8[33092]{0}) custom-call\n+    // CHECK-SAME: custom_call_target=\"__cub$DeviceRadixSort\",\n+    // CHECK-SAME: backend_config={\"descending\":true}\n+  )\");\n+}\n+\n+TEST_F(EstimateCubScratchSizeTest, U16_BF16) {\n+  const char hlo[] = R\"(\n+    HloModule m\n+    ENTRY main {\n+      %keys = u16[16,128] parameter(0)\n+      %values = bf16[16,128] parameter(1)\n+      %custom-call = (u16[16,128]{1,0}, bf16[16,128]{1,0}, u8[1]{0})\n+        custom-call(%keys, %values),\n+        custom_call_target=\"__cub$DeviceRadixSortUnassignedScratchSize\",\n+        backend_config={\"descending\":false}\n+      ROOT %t = u16[16,128]{1,0} get-tuple-element(%custom-call), index=0\n+  })\";\n+  RunAndCheck(hlo, R\"(\n+    // CHECK: (u16[16,128]{1,0}, bf16[16,128]{1,0}, u8[8516]{0}) custom-call\n+    // CHECK-SAME: custom_call_target=\"__cub$DeviceRadixSort\",\n+    // CHECK-SAME: backend_config={\"descending\":false}\n+  )\");\n+}\n+\n+TEST_F(EstimateCubScratchSizeTest, U16_BF16_Descending) {\n+  const char hlo[] = R\"(\n+    HloModule m\n+    ENTRY main {\n+      %keys = u16[16,128] parameter(0)\n+      %values = bf16[16,128] parameter(1)\n+      %custom-call = (u16[16,128]{1,0}, bf16[16,128]{1,0}, u8[1]{0})\n+        custom-call(%keys, %values),\n+        custom_call_target=\"__cub$DeviceRadixSortUnassignedScratchSize\",\n+        backend_config={\"descending\":true}\n+      ROOT %t = u16[16,128]{1,0} get-tuple-element(%custom-call), index=0\n+  })\";\n+  RunAndCheck(hlo, R\"(\n+    // CHECK: (u16[16,128]{1,0}, bf16[16,128]{1,0}, u8[8516]{0}) custom-call\n+    // CHECK-SAME: custom_call_target=\"__cub$DeviceRadixSort\",\n+    // CHECK-SAME: backend_config={\"descending\":true}\n+  )\");\n+}\n+\n+TEST_F(EstimateCubScratchSizeTest, U16_F16) {\n+  const char hlo[] = R\"(\n+    HloModule m\n+    ENTRY main {\n+      %keys = u16[16,128] parameter(0)\n+      %values = f16[16,128] parameter(1)\n+      %custom-call = (u16[16,128]{1,0}, f16[16,128]{1,0}, u8[1]{0})\n+        custom-call(%keys, %values),\n+        custom_call_target=\"__cub$DeviceRadixSortUnassignedScratchSize\",\n+        backend_config={\"descending\":false}\n+      ROOT %t = u16[16,128]{1,0} get-tuple-element(%custom-call), index=0\n+  })\";\n+  RunAndCheck(hlo, R\"(\n+    // CHECK: (u16[16,128]{1,0}, f16[16,128]{1,0}, u8[8516]{0}) custom-call\n+    // CHECK-SAME: custom_call_target=\"__cub$DeviceRadixSort\",\n+    // CHECK-SAME: backend_config={\"descending\":false}\n+  )\");\n+}\n+\n+TEST_F(EstimateCubScratchSizeTest, U32_F32_Rank2_Descending) {\n+  const char hlo[] = R\"(\n+    HloModule m\n+    ENTRY main {\n+      %keys = u32[16,128] parameter(0)\n+      %values = f32[16,128] parameter(1)\n+      %custom-call = (u32[16,128]{1,0}, f32[16,128]{1,0}, u8[1]{0})\n+        custom-call(%keys, %values),\n+        custom_call_target=\"__cub$DeviceRadixSortUnassignedScratchSize\",\n+        backend_config={\"descending\":true}\n+      ROOT %t = u32[16,128]{1,0} get-tuple-element(%custom-call), index=0\n+  })\";\n+  RunAndCheck(hlo, R\"(\n+    // CHECK: (u32[16,128]{1,0}, f32[16,128]{1,0}, u8[16708]{0}) custom-call\n+    // CHECK-SAME: custom_call_target=\"__cub$DeviceRadixSort\",\n+    // CHECK-SAME: backend_config={\"descending\":true}\n+  )\");\n+}\n+\n+TEST_F(EstimateCubScratchSizeTest, U64_F64) {\n+  const char hlo[] = R\"(\n+    HloModule m\n+    ENTRY main {\n+      %keys = u64[16,128] parameter(0)\n+      %values = f64[16,128] parameter(1)\n+      %custom-call = (u64[16,128]{1,0}, f64[16,128]{1,0}, u8[1]{0})\n+        custom-call(%keys, %values),\n+        custom_call_target=\"__cub$DeviceRadixSortUnassignedScratchSize\",\n+        backend_config={\"descending\":false}\n+      ROOT %t = u64[16,128]{1,0} get-tuple-element(%custom-call), index=0\n+  })\";\n+  RunAndCheck(hlo, R\"(\n+    // CHECK: (u64[16,128]{1,0}, f64[16,128]{1,0}, u8[33092]{0}) custom-call\n+    // CHECK-SAME: custom_call_target=\"__cub$DeviceRadixSort\",\n+    // CHECK-SAME: backend_config={\"descending\":false}\n+  )\");\n+}\n+\n+}  // namespace\n+}  // namespace xla::gpu"
        },
        {
            "sha": "87a09a8dd4ab89880caf80291a674b37821eb36b",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment.cc?ref=e32237efd3c4458cfab8f4b08462ac1715c51771",
            "patch": "@@ -523,7 +523,7 @@ absl::Status GpuLayoutAssignment::AddBackendConstraints(\n       TF_RETURN_IF_ERROR(SetOperandLayout(op0_shape, instruction, 0));\n       TF_RETURN_IF_ERROR(SetInstructionLayout(output_shape, instruction));\n     } else if ((HloPredicateIsOp<HloOpcode::kSort>(instruction) ||\n-                IsCubDeviceRadixSort(*instruction)) &&\n+                IsCubDeviceRadixSortNoScratchSize(*instruction)) &&\n                instruction->operand(0)->shape().dimensions().size() > 1) {\n       // Make sure that all the operands and the output(s) have the same layout.\n       Shape keys_shape = instruction->operand(0)->shape();"
        },
        {
            "sha": "d13d61754ed6dded759aa6be2c25e7772c355402",
            "filename": "third_party/xla/xla/service/gpu/transforms/layout_assignment_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Flayout_assignment_test.cc?ref=e32237efd3c4458cfab8f4b08462ac1715c51771",
            "patch": "@@ -417,7 +417,7 @@ TEST_F(LayoutAssignmentTest,\n     values = f32[2,3]{1,0} parameter(0)\n     transpose = f32[3,2]{1,0} transpose(values), dimensions={1,0}\n     ROOT sort = (f32[3,2]{1,0}, f32[3,2]{1,0}, u8[128]{0})\n-        custom-call(keys, transpose), custom_call_target=\"__cub$DeviceRadixSort\"\n+        custom-call(keys, transpose), custom_call_target=\"__cub$DeviceRadixSortUnassignedScratchSize\"\n   })\";\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,"
        },
        {
            "sha": "4ed5d493ab093b5ffe0c607d458db17b46b4e2c8",
            "filename": "third_party/xla/xla/service/gpu/transforms/sort_rewriter.cc",
            "status": "modified",
            "additions": 47,
            "deletions": 31,
            "changes": 78,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsort_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsort_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsort_rewriter.cc?ref=e32237efd3c4458cfab8f4b08462ac1715c51771",
            "patch": "@@ -28,8 +28,6 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n-#include \"xla/backends/gpu/runtime/cub_sort_thunk.h\"\n-#include \"xla/comparison_util.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -49,8 +47,7 @@ limitations under the License.\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n \n-namespace xla {\n-namespace gpu {\n+namespace xla::gpu {\n namespace {\n \n namespace m = match;\n@@ -227,12 +224,44 @@ std::optional<SortComputationAnalysis> AnalyzeSortOp(\n       sort_analysis->sort_order, sort_key_type, sort_value_type};\n }\n \n-// Create runner for CUB sort operation.\n-absl::StatusOr<std::unique_ptr<CubSortRunnerInterface>> CreateRunner(\n-    const SortComputationAnalysis& sort_analysis,\n-    absl::string_view platform_name) {\n-  return CubSortRunnerInterface::Create(\n-      sort_analysis.key_type, sort_analysis.value_type, platform_name);\n+// Returns whether the sort operation is supported by CUB.\n+bool AreOperandTypesSupportedByCub(\n+    const SortComputationAnalysis& sort_analysis) {\n+  PrimitiveType key_type = sort_analysis.key_type;\n+  std::optional<PrimitiveType> value_type = sort_analysis.value_type;\n+  if (!value_type.has_value()) {\n+    switch (key_type) {\n+      case BF16:\n+      case F16:\n+      case F32:\n+      case F64:\n+      case S8:\n+      case S16:\n+      case S32:\n+      case S64:\n+      case U8:\n+      case U16:\n+      case U32:\n+      case U64:\n+        return true;\n+      default:\n+        return false;\n+    }\n+  }\n+  auto value_bitwidth = primitive_util::BitWidth(*value_type);\n+  switch (key_type) {\n+    case U8:\n+    case U16:\n+    case U32:\n+    case U64:\n+    case F32:\n+      return value_bitwidth == 16 || value_bitwidth == 32 ||\n+             value_bitwidth == 64;\n+    case S32:\n+      return value_bitwidth == 32;\n+    default:\n+      return false;\n+  }\n }\n \n // Restore the result shape after sorting a pair of tensors.\n@@ -456,7 +485,7 @@ bool IsCubCompatibleSort(const se::DeviceDescription& device_description,\n     VLOG(2) << \"Only simple compare computations are supported\";\n     return false;\n   }\n-  if (!CreateRunner(*sort_analysis, platform_name).ok()) {\n+  if (!AreOperandTypesSupportedByCub(*sort_analysis)) {\n     VLOG(2) << \"Unsupported operand types (no compiled CUB kernels): \"\n             << PrimitiveType_Name(sort_analysis->key_type) << \" \"\n             << (sort_analysis->value_type.has_value()\n@@ -476,22 +505,6 @@ absl::StatusOr<bool> SortRewriter::RunOnInstruction(\n   // Get the sort tensor index and direction.\n   SortComputationAnalysis sort_analysis = AnalyzeSortOp(*sort_op).value();\n \n-  // Get scratch size requirements from CUB.\n-  const Shape& operand_shape = sort_op->operand(0)->shape();\n-  int64_t batch_size = Product(operand_shape.dimensions()) /\n-                       operand_shape.dimensions(sort_op->sort_dimension());\n-\n-  TF_ASSIGN_OR_RETURN(auto runner, CreateRunner(sort_analysis, platform_name_));\n-  TF_ASSIGN_OR_RETURN(\n-      int64_t scratch_size,\n-      runner->GetScratchSize(Product(operand_shape.dimensions()), batch_size));\n-\n-  // Align and increase scratch size to fit the offsets.\n-  if (batch_size > 1) {\n-    scratch_size += sizeof(int) - scratch_size % sizeof(int);\n-    scratch_size += (batch_size + 1) * sizeof(int);\n-  }\n-\n   // Values are only present if sorting a pair of tensors.\n   HloInstruction* keys;\n   HloInstruction* values = nullptr;\n@@ -519,13 +532,17 @@ absl::StatusOr<bool> SortRewriter::RunOnInstruction(\n     shapes.push_back(values->shape());\n     operands.push_back(values);\n   }\n-  shapes.push_back(ShapeUtil::MakeShape(U8, {scratch_size}));\n+  // The last shape corresponds to the scratch buffer. In this pass we put 1 as\n+  // the scratch size, but later the actual size will be set by the\n+  // AssignCubScratchSize pass.\n+  shapes.push_back(ShapeUtil::MakeShape(U8, {/*scratch_size=*/1}));\n   Shape call_shape = ShapeUtil::MakeTupleShape(absl::MakeSpan(shapes));\n \n   // Build the custom call instruction.\n   HloInstruction* custom_call =\n       sort_op->AddInstruction(HloInstruction::CreateCustomCall(\n-          call_shape, absl::MakeSpan(operands), kCubDeviceRadixSortTarget));\n+          call_shape, absl::MakeSpan(operands),\n+          kCubDeviceRadixSortUnassignedScratchSizeTarget));\n \n   xla::SortOptions backend_config;\n   backend_config.set_descending(sort_analysis.descending);\n@@ -586,5 +603,4 @@ absl::StatusOr<bool> SortRewriter::RunImpl(\n   return changed;\n }\n \n-}  // namespace gpu\n-}  // namespace xla\n+}  // namespace xla::gpu"
        },
        {
            "sha": "cc6dc67fa79bfbce373c9edbad9fed26b88cf9db",
            "filename": "third_party/xla/xla/service/gpu/transforms/sort_rewriter_test.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 11,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsort_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e32237efd3c4458cfab8f4b08462ac1715c51771/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsort_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsort_rewriter_test.cc?ref=e32237efd3c4458cfab8f4b08462ac1715c51771",
            "patch": "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"xla/primitive_util.h\"\n #include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/gpu/transforms/estimate_cub_scratch_size.h\"\n #include \"xla/service/pattern_matcher.h\"\n #include \"xla/service/platform_util.h\"\n #include \"xla/stream_executor/platform.h\"\n@@ -58,10 +59,11 @@ class SortRewriterTest\n \n   bool RunModuleAndPass(HloModule* module) {\n     auto cloned = module->Clone();\n-    bool changed = SortRewriter(TestGpuDeviceInfo::CudaOrRocmDeviceInfo(),\n-                                GetTestPlatform()->Name())\n-                       .Run(module)\n-                       .value();\n+    const std::string& platform_name = GetTestPlatform()->Name();\n+    bool changed =\n+        SortRewriter(TestGpuDeviceInfo::CudaOrRocmDeviceInfo(), platform_name)\n+            .Run(module)\n+            .value();\n     if (changed) {\n       // Here we run an end to end test to make sure that SortRewriter does\n       // not introduce an incorrect rewrite. To do this, we need to clone the\n@@ -106,7 +108,9 @@ ENTRY %main {\n   EXPECT_THAT(\n       module->entry_computation()->root_instruction(),\n       GmockMatch(m::GetTupleElement(\n-          m::CustomCall({kCubDeviceRadixSortTarget}, m::Parameter()), 0)));\n+          m::CustomCall({kCubDeviceRadixSortUnassignedScratchSizeTarget},\n+                        m::Parameter()),\n+          0)));\n   ExpectDirection(module->entry_computation()->root_instruction()->operand(0),\n                   /*descending=*/false);\n }\n@@ -132,7 +136,9 @@ ENTRY %main {\n   EXPECT_THAT(\n       module->entry_computation()->root_instruction(),\n       GmockMatch(m::GetTupleElement(\n-          m::CustomCall({kCubDeviceRadixSortTarget}, m::Parameter()), 0)));\n+          m::CustomCall({kCubDeviceRadixSortUnassignedScratchSizeTarget},\n+                        m::Parameter()),\n+          0)));\n   ExpectDirection(module->entry_computation()->root_instruction()->operand(0),\n                   /*descending=*/true);\n }\n@@ -158,7 +164,9 @@ ENTRY %main {\n   EXPECT_THAT(\n       module->entry_computation()->root_instruction(),\n       GmockMatch(m::GetTupleElement(\n-          m::CustomCall({kCubDeviceRadixSortTarget}, m::Parameter()), 0)));\n+          m::CustomCall({kCubDeviceRadixSortUnassignedScratchSizeTarget},\n+                        m::Parameter()),\n+          0)));\n   ExpectDirection(module->entry_computation()->root_instruction()->operand(0),\n                   /*descending=*/false);\n }\n@@ -512,7 +520,9 @@ ENTRY %main {\n   EXPECT_THAT(\n       module->entry_computation()->root_instruction(),\n       GmockMatch(m::GetTupleElement(\n-          m::CustomCall({kCubDeviceRadixSortTarget}, m::Parameter()), 0)));\n+          m::CustomCall({kCubDeviceRadixSortUnassignedScratchSizeTarget},\n+                        m::Parameter()),\n+          0)));\n   ExpectDirection(module->entry_computation()->root_instruction()->operand(0),\n                   /*descending=*/false);\n }\n@@ -538,7 +548,9 @@ ENTRY %main {\n   EXPECT_THAT(\n       module->entry_computation()->root_instruction(),\n       GmockMatch(m::GetTupleElement(\n-          m::CustomCall({kCubDeviceRadixSortTarget}, m::Parameter()), 0)));\n+          m::CustomCall({kCubDeviceRadixSortUnassignedScratchSizeTarget},\n+                        m::Parameter()),\n+          0)));\n   ExpectDirection(module->entry_computation()->root_instruction()->operand(0),\n                   /*descending=*/false);\n }\n@@ -559,7 +571,7 @@ ENTRY %main {\n       dimensions={0}, to_apply=%compare, metadata={op_type=\"sort\" op_name=\"sort\" source_file=\"path/to/test.cc\" source_line=68}\n })\";\n   constexpr char kExpectedPattern[] = R\"(\n-    // CHECK: %[[CC:.*]] = (u16[1000]{0}, u8[1]{0}) custom-call({{.*}}), custom_call_target=\"__cub$DeviceRadixSort\", metadata={op_type=\"sort\" op_name=\"sort\" source_file=\"path/to/test.cc\" source_line=68}, backend_config={\"descending\":true}\n+    // CHECK: %[[CC:.*]] = (u16[1000]{0}, u8[1]{0}) custom-call({{.*}}), custom_call_target=\"__cub$DeviceRadixSortUnassignedScratchSize\", metadata={op_type=\"sort\" op_name=\"sort\" source_file=\"path/to/test.cc\" source_line=68}, backend_config={\"descending\":true}\n   )\";\n   for (const auto& [device_description, platform_name] :\n        {std::tuple{TestGpuDeviceInfo::RTXA6000DeviceInfo(), \"CUDA\"},\n@@ -602,7 +614,8 @@ ENTRY main {\n   EXPECT_THAT(\n       module->entry_computation()->root_instruction(),\n       GmockMatch(m::GetTupleElement(\n-          m::CustomCall({kCubDeviceRadixSortTarget}, m::Op(), m::Parameter()),\n+          m::CustomCall({kCubDeviceRadixSortUnassignedScratchSizeTarget},\n+                        m::Op(), m::Parameter()),\n           1)))\n       << module->ToString();\n }"
        }
    ],
    "stats": {
        "total": 863,
        "additions": 719,
        "deletions": 144
    }
}