{
    "author": "junwhanahn",
    "message": "Use unbounded parallelism for IFRT IR program compilation\n\nThis artificially limits compilation performance, especially when there are lots of IFRT IR programs to compile.\n\nPiperOrigin-RevId: 836420985",
    "sha": "345f4f76db5d73c81e43ddd2f7d3f82207930dd0",
    "files": [
        {
            "sha": "e5aebbd2c0676fd51cbf3d0ace5286a6e67ecfa6",
            "filename": "third_party/xla/xla/python/ifrt/ir/transforms/multi_threaded_atom_program_compiler.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 26,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/345f4f76db5d73c81e43ddd2f7d3f82207930dd0/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fmulti_threaded_atom_program_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/345f4f76db5d73c81e43ddd2f7d3f82207930dd0/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fmulti_threaded_atom_program_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fmulti_threaded_atom_program_compiler.cc?ref=345f4f76db5d73c81e43ddd2f7d3f82207930dd0",
            "patch": "@@ -60,27 +60,6 @@ namespace ifrt {\n \n namespace {\n \n-// Lazily initialized shared thread pool.\n-tsl::thread::ThreadPool* thread_pool() {\n-  static tsl::thread::ThreadPool* thread_pool = []() {\n-    constexpr int kMaxParallelism = 32;\n-    return new tsl::thread::ThreadPool(tsl::Env::Default(),\n-                                       tsl::ThreadOptions(),\n-                                       \"CompileAtomPrograms\", kMaxParallelism);\n-  }();\n-  return thread_pool;\n-}\n-\n-void ScheduleWork(tsl::thread::ThreadPool* pool,\n-                  absl::AnyInvocable<void()> callee) {\n-  // ThreadPool expects std::function that must be copyable, but we can avoid\n-  // this by using AnyInvocable.\n-  pool->Schedule([ptr = new absl::AnyInvocable<void()>(std::move(callee))]() {\n-    (*ptr)();\n-    delete ptr;\n-  });\n-}\n-\n // Construct a bool vector with a True entry for each input sharding that must\n // be inferred.\n llvm::SmallVector<bool> GetInputShardingPropagation(\n@@ -125,7 +104,7 @@ absl::StatusOr<CompileFuture> MultiThreadedAtomProgramCompiler::CompileModule(\n   auto module_type =\n       call_op->getAttrOfType<mlir::StringAttr>(kIfrtModuleTypeAttrName);\n   if (module_type == kIfrtModuleTypeXla) {\n-    return CompileXla(call_op, module_op, thread_pool());\n+    return CompileXla(call_op, module_op);\n   } else if (module_type == kIfrtModuleTypeMpmdReshard) {\n     return CompileMpmdReshard(module_op);\n   } else if (module_type == nullptr) {\n@@ -190,8 +169,7 @@ MultiThreadedAtomProgramCompiler::GetXlaCompileOptions(\n }\n \n absl::StatusOr<CompileFuture> MultiThreadedAtomProgramCompiler::CompileXla(\n-    CallOp call_op, mlir::ModuleOp module_op,\n-    tsl::thread::ThreadPool* thread_pool) {\n+    CallOp call_op, mlir::ModuleOp module_op) {\n   TF_ASSIGN_OR_RETURN(xla::CompileOptions compile_options,\n                       GetXlaCompileOptions(call_op, module_op));\n \n@@ -203,8 +181,7 @@ absl::StatusOr<CompileFuture> MultiThreadedAtomProgramCompiler::CompileXla(\n       /*context=*/nullptr,  // Shares the same long-living context.\n       mlir::OwningOpRef<mlir::ModuleOp>(module_op.clone()));\n   auto [promise, future] = CompileFuture::MakePromise();\n-  ScheduleWork(\n-      thread_pool,\n+  tsl::Env::Default()->SchedClosure(\n       WithCurrentUserContext([this, hlo_program = std::move(hlo_program),\n                               compile_options = std::move(compile_options),\n                               promise = std::move(promise)]() mutable {"
        },
        {
            "sha": "0b87eb110122282ba1458850147c1f6399f41dd3",
            "filename": "third_party/xla/xla/python/ifrt/ir/transforms/multi_threaded_atom_program_compiler.h",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/345f4f76db5d73c81e43ddd2f7d3f82207930dd0/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fmulti_threaded_atom_program_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/345f4f76db5d73c81e43ddd2f7d3f82207930dd0/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fmulti_threaded_atom_program_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fifrt%2Fir%2Ftransforms%2Fmulti_threaded_atom_program_compiler.h?ref=345f4f76db5d73c81e43ddd2f7d3f82207930dd0",
            "patch": "@@ -60,9 +60,8 @@ class MultiThreadedAtomProgramCompiler {\n   //\n   // Note that the method runs `ifrt-compile-xla-preprocessing-pipeline`\n   // before dispatching compilation.\n-  absl::StatusOr<CompileFuture> CompileXla(\n-      CallOp call_op, mlir::ModuleOp module_op,\n-      tsl::thread::ThreadPool* thread_pool);\n+  absl::StatusOr<CompileFuture> CompileXla(CallOp call_op,\n+                                           mlir::ModuleOp module_op);\n \n   // Returns a future of a AtomProgramCompileResult for the MPMD reshard module.\n   absl::StatusOr<CompileFuture> CompileMpmdReshard(mlir::ModuleOp module_op);"
        }
    ],
    "stats": {
        "total": 34,
        "additions": 5,
        "deletions": 29
    }
}