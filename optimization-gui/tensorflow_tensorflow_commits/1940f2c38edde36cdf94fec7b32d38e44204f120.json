{
    "author": "tensorflower-gardener",
    "message": "[Autotuner] Fix Cudnn config proto in legacy cache.\n\nPiperOrigin-RevId: 806277532",
    "sha": "1940f2c38edde36cdf94fec7b32d38e44204f120",
    "files": [
        {
            "sha": "56bcc238c5d8ab02b6fc051aa0505af039858cc9",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1940f2c38edde36cdf94fec7b32d38e44204f120/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1940f2c38edde36cdf94fec7b32d38e44204f120/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=1940f2c38edde36cdf94fec7b32d38e44204f120",
            "patch": "@@ -740,6 +740,7 @@ cc_library(\n         \"//xla/service/gpu/autotuning:autotune_cache_key\",\n         \"//xla/service/gpu/autotuning:autotuner_util\",\n         \"//xla/stream_executor:device_description\",\n+        \"//xla/tsl/protobuf:dnn_proto_cc\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n@@ -760,6 +761,7 @@ xla_cc_test(\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:env\",\n+        \"//xla/tsl/protobuf:dnn_proto_cc\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@com_google_protobuf//:any_cc_proto\","
        },
        {
            "sha": "894256187840862c46c7e9b03360a9c3b5bfa29b",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/legacy_cache.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1940f2c38edde36cdf94fec7b32d38e44204f120/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1940f2c38edde36cdf94fec7b32d38e44204f120/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache.cc?ref=1940f2c38edde36cdf94fec7b32d38e44204f120",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/gpu/autotuning/autotune_cache_key.h\"\n #include \"xla/service/gpu/autotuning/autotuner_util.h\"\n+#include \"xla/tsl/protobuf/dnn.pb.h\"\n \n namespace xla {\n \n@@ -76,9 +77,9 @@ std::optional<LegacyCache::Config> LegacyCache::GetConfig(\n   } else if (result.has_gemm()) {\n     config.codegen_backend_name = \"Cublas\";\n     config.backend_config.PackFrom(result.gemm());\n-  } else if (result.has_conv()) {\n+  } else if (result.has_algorithm()) {\n     config.codegen_backend_name = \"Cudnn\";\n-    config.backend_config.PackFrom(result.conv());\n+    config.backend_config.PackFrom(result.algorithm());\n   } else {\n     return std::nullopt;\n   }\n@@ -93,7 +94,7 @@ std::optional<AutotuneResult> LegacyCache::GetAutotuneResult(\n   } else if (config.codegen_backend_name == \"Cublas\") {\n     config.backend_config.UnpackTo(result.mutable_gemm());\n   } else if (config.codegen_backend_name == \"Cudnn\") {\n-    config.backend_config.UnpackTo(result.mutable_conv());\n+    config.backend_config.UnpackTo(result.mutable_algorithm());\n   } else {\n     return std::nullopt;\n   }"
        },
        {
            "sha": "c6eceafa2cb9ef2377e08957f88e5b7b84770f7f",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/legacy_cache_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1940f2c38edde36cdf94fec7b32d38e44204f120/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1940f2c38edde36cdf94fec7b32d38e44204f120/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Flegacy_cache_test.cc?ref=1940f2c38edde36cdf94fec7b32d38e44204f120",
            "patch": "@@ -33,6 +33,7 @@ limitations under the License.\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n #include \"xla/tsl/platform/env.h\"\n+#include \"xla/tsl/protobuf/dnn.pb.h\"\n \n namespace xla {\n namespace gpu {\n@@ -100,7 +101,7 @@ class LegacyCacheTest : public ::testing::Test {\n   Config CreateDummyCudnnConfig() {\n     Config config;\n     config.codegen_backend_name = \"Cudnn\";\n-    config.backend_config.PackFrom(AutotuneResult::ConvKey());\n+    config.backend_config.PackFrom(stream_executor::dnn::AlgorithmProto());\n     return config;\n   }\n };"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 8,
        "deletions": 4
    }
}