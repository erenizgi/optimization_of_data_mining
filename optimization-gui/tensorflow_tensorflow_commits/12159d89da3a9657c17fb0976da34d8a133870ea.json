{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 827845891",
    "sha": "12159d89da3a9657c17fb0976da34d8a133870ea",
    "files": [
        {
            "sha": "c1d853d4933190aa842a62bb41d94e70c806ccd5",
            "filename": "tensorflow/core/framework/shape_inference_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fshape_inference_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fshape_inference_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fshape_inference_test.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -230,7 +230,7 @@ TEST_F(ShapeInferenceTest, AttachContext) {\n   // Error when a constant tensor value as shape was requested, but no partial\n   // shapes provided.\n   {\n-    Tensor input_t = ::tensorflow::test::AsTensor<int32>({1, 2, 3, 4, 5});\n+    Tensor input_t = ::tensorflow::test::AsTensor<int32_t>({1, 2, 3, 4, 5});\n     InferenceContext c(kVersion, def, MakeOpDef(2, 2), {S({3}), S({4})},\n                        {nullptr, &input_t}, {}, {});\n     TF_ASSERT_OK(c.construction_status());\n@@ -257,7 +257,7 @@ TEST_F(ShapeInferenceTest, AttachContext) {\n   // Error when a constant tensor value as shape was requested, and a partial\n   // shape was provided.\n   {\n-    Tensor input_t = ::tensorflow::test::AsTensor<int32>({1, 2, 3, 4, 5});\n+    Tensor input_t = ::tensorflow::test::AsTensor<int32_t>({1, 2, 3, 4, 5});\n     InferenceContext c(kVersion, def, MakeOpDef(2, 2), {S({3}), S({4})},\n                        {nullptr, &input_t}, {S({10, -1, 5}), Unknown()}, {});\n     TF_ASSERT_OK(c.construction_status());\n@@ -1129,7 +1129,7 @@ TEST_F(ShapeInferenceTest, MakeShapeFromShapeTensor) {\n   Tensor t;\n   EXPECT_EQ(\"?\", create(nullptr));\n \n-  t = ::tensorflow::test::AsTensor<int32>({1, 2, 3});\n+  t = ::tensorflow::test::AsTensor<int32_t>({1, 2, 3});\n   EXPECT_EQ(\"[1,2,3]\", create(&t));\n \n   t = ::tensorflow::test::AsTensor<int64_t>({3, 2, 1});\n@@ -1142,19 +1142,19 @@ TEST_F(ShapeInferenceTest, MakeShapeFromShapeTensor) {\n   EXPECT_EQ(\"[]\", create(&t));\n \n   // Test negative scalar\n-  t = ::tensorflow::test::AsScalar<int32>(-1);\n+  t = ::tensorflow::test::AsScalar<int32_t>(-1);\n   EXPECT_EQ(\"?\", create(&t));\n \n   t = ::tensorflow::test::AsTensor<float>({1, 2, 3});\n   EXPECT_THAT(create(&t),\n               HasSubstr(\"Input tensor must be int32 or int64, but was float\"));\n \n-  t = ::tensorflow::test::AsScalar<int32>(1);\n+  t = ::tensorflow::test::AsScalar<int32_t>(1);\n   auto s_scalar = create(&t);\n   EXPECT_THAT(s_scalar, HasSubstr(\"Input tensor must be rank 1, or if its rank \"\n                                   \"0 it must have value -1\"));\n \n-  t = ::tensorflow::test::AsTensor<int32>({1, 2}, TensorShape{2, 1});\n+  t = ::tensorflow::test::AsTensor<int32_t>({1, 2}, TensorShape{2, 1});\n   auto s_matrix = create(&t);\n   EXPECT_THAT(s_matrix,\n               HasSubstr(\"Input tensor must be rank 1, but was rank 2\"));\n@@ -1165,7 +1165,7 @@ TEST_F(ShapeInferenceTest, MakeShapeFromShapeTensor) {\n               HasSubstr(\"Invalid value in tensor used for shape: -2\"));\n \n   // Test negative values for the dims.\n-  t = ::tensorflow::test::AsTensor<int32>({3, -2, 1});\n+  t = ::tensorflow::test::AsTensor<int32_t>({3, -2, 1});\n   EXPECT_THAT(create(&t),\n               HasSubstr(\"Invalid value in tensor used for shape: -2\"));\n \n@@ -1304,8 +1304,8 @@ TEST_F(ShapeInferenceTest, InputTensors) {\n }\n \n TEST_F(ShapeInferenceTest, MakeDimForScalarInput) {\n-  Tensor t1 = tensorflow::test::AsScalar<int32>(20);\n-  Tensor t2 = tensorflow::test::AsScalar<int32>(-1);\n+  Tensor t1 = tensorflow::test::AsScalar<int32_t>(20);\n+  Tensor t2 = tensorflow::test::AsScalar<int32_t>(-1);\n   NodeDef def;\n   InferenceContext c(kVersion, def, MakeOpDef(2, 2), {S({}), S({})}, {&t1, &t2},\n                      {}, {});\n@@ -1334,8 +1334,8 @@ TEST_F(ShapeInferenceTest, MakeDimForScalarInput) {\n }\n \n TEST_F(ShapeInferenceTest, MakeDimForScalarInputWithNegativeIndexing) {\n-  Tensor t1 = tensorflow::test::AsScalar<int32>(-2);\n-  Tensor t2 = tensorflow::test::AsScalar<int32>(3);\n+  Tensor t1 = tensorflow::test::AsScalar<int32_t>(-2);\n+  Tensor t2 = tensorflow::test::AsScalar<int32_t>(3);\n   NodeDef def;\n   InferenceContext c(kVersion, def, MakeOpDef(2, 2), {S({}), S({})}, {&t1, &t2},\n                      {}, {});\n@@ -1377,7 +1377,7 @@ TEST_F(ShapeInferenceTest, GetAttr) {\n \n   std::vector<ShapeHandle> empty;\n   InferenceContext c(kVersion, def, op_reg_data.op_def, empty, {}, {}, {});\n-  string value;\n+  std::string value;\n   TF_EXPECT_OK(c.GetAttr(\"foo\", &value));\n   EXPECT_EQ(\"bar\", value);\n }"
        },
        {
            "sha": "d1769e0a0282e5c2248a51c8ce1f6a281a93c77c",
            "filename": "tensorflow/core/framework/shape_inference_testutil.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fshape_inference_testutil.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fshape_inference_testutil.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fshape_inference_testutil.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -30,17 +30,17 @@ namespace shape_inference {\n \n using errors::Unknown;\n \n-absl::Status ShapeInferenceTestutil::InferShapes(ShapeInferenceTestOp op,\n-                                                 const string& ins,\n-                                                 const string& expected_outs) {\n+absl::Status ShapeInferenceTestutil::InferShapes(\n+    ShapeInferenceTestOp op, const std::string& ins,\n+    const std::string& expected_outs) {\n   const OpRegistrationData* op_reg_data;\n   TF_RETURN_IF_ERROR(OpRegistry::Global()->LookUp(op.name, &op_reg_data));\n \n-  std::vector<string> ins_v = str_util::Split(ins, ';');\n+  std::vector<std::string> ins_v = str_util::Split(ins, ';');\n \n   InferenceContext::ShapeManager manager;\n   std::vector<ShapeHandle> in_shapes;\n-  for (const string& spec : ins_v) {\n+  for (const std::string& spec : ins_v) {\n     ShapeHandle shape;\n     TF_RETURN_IF_ERROR(MakeShapeFromString(&manager, spec, &shape));\n     in_shapes.push_back(shape);\n@@ -82,7 +82,8 @@ absl::Status ShapeInferenceTestutil::InferShapes(ShapeInferenceTestOp op,\n   }\n \n   // Verify the output shape.\n-  std::vector<string> expected_outs_v = str_util::Split(expected_outs, ';');\n+  std::vector<std::string> expected_outs_v =\n+      str_util::Split(expected_outs, ';');\n   if (num_outputs != expected_outs_v.size()) {\n     return Unknown(\"The expected output string lists the wrong number of \",\n                    \"outputs. It lists \", expected_outs_v.size(),\n@@ -92,8 +93,9 @@ absl::Status ShapeInferenceTestutil::InferShapes(ShapeInferenceTestOp op,\n     absl::string_view expected(expected_outs_v[i]);\n     shape_inference::ShapeHandle out = c.output(i);\n \n-    string err_prefix = absl::StrCat(\"Output \", i);\n-    string err_suffix = absl::StrCat(\". Output shape was \", c.DebugString(out));\n+    std::string err_prefix = absl::StrCat(\"Output \", i);\n+    std::string err_suffix =\n+        absl::StrCat(\". Output shape was \", c.DebugString(out));\n \n     int in_index = -1;\n     for (int i = 0; i < c.num_inputs(); ++i) {\n@@ -230,7 +232,7 @@ absl::Status ShapeInferenceTestutil::InferShapes(ShapeInferenceTestOp op,\n \n // static\n absl::Status ShapeInferenceTestutil::MakeShapeFromString(\n-    InferenceContext::ShapeManager* manager, const string& spec,\n+    InferenceContext::ShapeManager* manager, const std::string& spec,\n     ShapeHandle* output) {\n   if (spec == \"?\") {\n     *output = manager->UnknownShape();"
        },
        {
            "sha": "5c204012546a22160bd10145a8c0a4217d3d081e",
            "filename": "tensorflow/core/framework/shape_inference_testutil.h",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fshape_inference_testutil.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fshape_inference_testutil.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fshape_inference_testutil.h?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -32,9 +32,10 @@ namespace tensorflow {\n class Tensor;\n \n struct ShapeInferenceTestOp {\n-  typedef std::pair<string, DataType> ShapeAndType;\n-  explicit ShapeInferenceTestOp(absl::string_view name) : name(string(name)) {}\n-  string name;\n+  typedef std::pair<std::string, DataType> ShapeAndType;\n+  explicit ShapeInferenceTestOp(absl::string_view name)\n+      : name(std::string(name)) {}\n+  std::string name;\n   NodeDef node_def;\n   std::vector<const Tensor*> input_tensors;\n   std::vector<std::vector<ShapeAndType>*>\n@@ -67,15 +68,16 @@ class ShapeInferenceTestutil {\n   //            the second is which dimension in that input it corresponds to.\n   // <expected_outs> can be \"e\"; this is used to indicate that shape inference\n   // should have failed.\n-  static absl::Status InferShapes(ShapeInferenceTestOp op, const string& ins,\n-                                  const string& expected_outs);\n+  static absl::Status InferShapes(ShapeInferenceTestOp op,\n+                                  const std::string& ins,\n+                                  const std::string& expected_outs);\n \n  private:\n   ShapeInferenceTestutil() = default;\n \n   // Makes a shape out of 'spec'.\n   static absl::Status MakeShapeFromString(\n-      InferenceContext::ShapeManager* manager, const string& spec,\n+      InferenceContext::ShapeManager* manager, const std::string& spec,\n       ShapeHandle* output);\n };\n "
        },
        {
            "sha": "56b32d859a9d8b0c1a9733070f9c7000face3abd",
            "filename": "tensorflow/core/framework/shape_inference_testutil_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fshape_inference_testutil_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fshape_inference_testutil_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fshape_inference_testutil_test.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -49,8 +49,9 @@ REGISTER_OP(\"OpTwoOut\")\n     .Attr(\"T: numbertype\")\n     .SetShapeFn([](InferenceContext* c) { return (*global_fn_ptr)(c); });\n \n-string RunInferShapes(const string& op_name, const string& ins,\n-                      const string& expected_outs, OpShapeInferenceFn fn) {\n+std::string RunInferShapes(const std::string& op_name, const std::string& ins,\n+                           const std::string& expected_outs,\n+                           OpShapeInferenceFn fn) {\n   ShapeInferenceTestOp op(op_name);\n   const int num_inputs = 1 + std::count(ins.begin(), ins.end(), ';');\n   std::vector<NodeDefBuilder::NodeOut> src_list;\n@@ -91,7 +92,7 @@ TEST(ShapeInferenceTestutilTest, Failures) {\n     c->set_output(0, c->Matrix(InferenceContext::kUnknownDim, 2));\n     return absl::OkStatus();\n   };\n-  const string& op = \"OpOneOut\";\n+  const std::string& op = \"OpOneOut\";\n \n   EXPECT_EQ(\"Shape inference should have returned error\",\n             RunInferShapes(op, \"[1];[2];[1]\", \"e\", fn_copy_input_0));\n@@ -143,7 +144,7 @@ TEST(ShapeInferenceTestutilTest, Failures) {\n                                    c->UnknownDim(), c->Dim(c->input(2), 0)}));\n     return absl::OkStatus();\n   };\n-  const string ins = \"[0,1,?];[2];[1]\";\n+  const std::string ins = \"[0,1,?];[2];[1]\";\n   EXPECT_CONTAINS(RunInferShapes(op, ins, \"[?,2,?,d2_0]\", fn),\n                   \"Output dim 0,0 expected to be an unknown\");\n   EXPECT_CONTAINS(RunInferShapes(op, ins, \"[0,2,?,d2_0]\", fn),"
        },
        {
            "sha": "c7327f5488084211b08198cb3e9d1d34378a0cdd",
            "filename": "tensorflow/core/framework/stats_aggregator.h",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fstats_aggregator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fstats_aggregator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fstats_aggregator.h?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -46,13 +46,13 @@ class StatsAggregator {\n \n   // Add the given `values` to the histogram with the given `name`. Each\n   // element of `values` will be treated as a separate sample in the histogram.\n-  virtual void AddToHistogram(const string& name,\n+  virtual void AddToHistogram(const std::string& name,\n                               absl::Span<const double> values,\n                               int64_t global_step) = 0;\n \n   // TODO(shivaniagrawal): consistency in double and float usage.\n   // Add the given `value` as Scalar with the given `name`.\n-  virtual void AddScalar(const string& name, float value,\n+  virtual void AddScalar(const std::string& name, float value,\n                          int64_t global_step) = 0;\n \n   // Stores a protocol buffer representation of the aggregator state in the\n@@ -64,8 +64,8 @@ class StatsAggregator {\n       SummaryWriterInterface* summary_writer) = 0;\n \n   // Increment the `label` cell of metrics mapped with `name` by given `value`.\n-  virtual void IncrementCounter(const string& name, const string& label,\n-                                int64_t val) = 0;\n+  virtual void IncrementCounter(const std::string& name,\n+                                const std::string& label, int64_t val) = 0;\n };\n \n // A `StatsAggregatorResource` wraps a sharable `StatsAggregator` as a resource\n@@ -86,7 +86,7 @@ class StatsAggregatorResource : public ResourceBase {\n     return stats_aggregator_;\n   }\n \n-  string DebugString() const override { return \"StatsAggregatorResource\"; }\n+  std::string DebugString() const override { return \"StatsAggregatorResource\"; }\n \n  private:\n   const std::shared_ptr<StatsAggregator> stats_aggregator_;"
        },
        {
            "sha": "56a54b04503994ee1a0fb721d10454e4e3a4c015",
            "filename": "tensorflow/core/framework/tensor.cc",
            "status": "modified",
            "additions": 52,
            "deletions": 51,
            "changes": 103,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftensor.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -231,7 +231,7 @@ struct Helper {\n template <>\n struct Helper<tstring> {\n   // Proto message uses RepeatedFieldType to hold repeated T.\n-  typedef protobuf::RepeatedPtrField<string> RepeatedFieldType;\n+  typedef protobuf::RepeatedPtrField<std::string> RepeatedFieldType;\n \n   // Encodes \"n\" elements of type string stored in \"in\" into Cord\n   // \"out\", which is usually the TensorProto::tensor_content.\n@@ -268,7 +268,7 @@ struct Helper<tstring> {\n template <>\n struct Helper<ResourceHandle> {\n   // Proto message uses RepeatedFieldType to hold repeated T.\n-  typedef protobuf::RepeatedPtrField<string> RepeatedFieldType;\n+  typedef protobuf::RepeatedPtrField<std::string> RepeatedFieldType;\n \n   // Encodes \"n\" elements of type ResourceHandle stored in \"in\" into destination\n   // \"out\", which is usually the TensorProto::tensor_content.\n@@ -357,18 +357,18 @@ struct ProtoHelper {};\n \n PROTO_TRAITS(float, float, float);\n PROTO_TRAITS(double, double, double);\n-PROTO_TRAITS(int32, int32, int);\n-PROTO_TRAITS(uint8, int32, int);\n-PROTO_TRAITS(uint16, int32, int);\n-PROTO_TRAITS(uint32, uint32, uint32);\n-PROTO_TRAITS(int16, int32, int);\n-PROTO_TRAITS(int8, int32, int);\n+PROTO_TRAITS(int32_t, int32_t, int);\n+PROTO_TRAITS(uint8_t, int32_t, int);\n+PROTO_TRAITS(uint16_t, int32_t, int);\n+PROTO_TRAITS(uint32_t, uint32_t, uint32);\n+PROTO_TRAITS(int16_t, int32_t, int);\n+PROTO_TRAITS(int8_t, int32_t, int);\n PROTO_TRAITS(bool, bool, bool);\n PROTO_TRAITS(tstring, tstring, string);\n-PROTO_TRAITS(qint8, int32, int);\n-PROTO_TRAITS(quint8, int32, int);\n-PROTO_TRAITS(qint16, int32, int);\n-PROTO_TRAITS(quint16, int32, int);\n+PROTO_TRAITS(qint8, int32_t, int);\n+PROTO_TRAITS(quint8, int32_t, int);\n+PROTO_TRAITS(qint16, int32_t, int);\n+PROTO_TRAITS(quint16, int32_t, int);\n #undef PROTO_TRAITS\n \n template <typename T>\n@@ -416,15 +416,15 @@ struct ProtoHelper<int64_t> {\n };\n \n template <>\n-struct ProtoHelper<uint64> {\n+struct ProtoHelper<uint64_t> {\n   static protobuf::RepeatedField<uint64_t>::const_iterator Begin(\n       const TensorProto& proto) {\n     return proto.uint64_val().begin();\n   }\n   static size_t NumElements(const TensorProto& proto) {\n     return proto.uint64_val().size();\n   }\n-  static void Fill(const uint64* data, size_t n, TensorProto* proto) {\n+  static void Fill(const uint64_t* data, size_t n, TensorProto* proto) {\n     protobuf::RepeatedField<protobuf_uint64> copy(data, data + n);\n     proto->mutable_uint64_val()->Swap(&copy);\n   }\n@@ -502,15 +502,15 @@ struct ProtoHelper<complex128> {\n \n template <>\n struct ProtoHelper<qint32> {\n-  typedef Helper<int32>::RepeatedFieldType FieldType;\n+  typedef Helper<int32_t>::RepeatedFieldType FieldType;\n   static const qint32* Begin(const TensorProto& proto) {\n     return reinterpret_cast<const qint32*>(proto.int_val().data());\n   }\n   static size_t NumElements(const TensorProto& proto) {\n     return proto.int_val().size();\n   }\n   static void Fill(const qint32* data, size_t n, TensorProto* proto) {\n-    const int32* p = reinterpret_cast<const int32*>(data);\n+    const int32_t* p = reinterpret_cast<const int32_t*>(data);\n     FieldType copy(p, p + n);\n     proto->mutable_int_val()->Swap(&copy);\n   }\n@@ -522,7 +522,7 @@ struct ProtoHelper<bfloat16> {\n     proto->mutable_half_val()->Reserve(n);\n     for (size_t i = 0; i < n; ++i) {\n       proto->mutable_half_val()->AddAlreadyReserved(\n-          Eigen::numext::bit_cast<uint16>(data[i]));\n+          Eigen::numext::bit_cast<uint16_t>(data[i]));\n     }\n   }\n };\n@@ -533,14 +533,14 @@ struct ProtoHelper<Eigen::half> {\n     proto->mutable_half_val()->Reserve(n);\n     for (size_t i = 0; i < n; ++i) {\n       proto->mutable_half_val()->AddAlreadyReserved(\n-          Eigen::numext::bit_cast<uint16>(data[i]));\n+          Eigen::numext::bit_cast<uint16_t>(data[i]));\n     }\n   }\n };\n \n template <typename Float8>\n struct Float8ProtoHelper {\n-  typedef string RepeatedFieldType;\n+  typedef std::string RepeatedFieldType;\n   static const Float8* Begin(const TensorProto& proto) {\n     return reinterpret_cast<const Float8*>(proto.float8_val().data());\n   }\n@@ -659,7 +659,7 @@ TensorBuffer* Int4OrInt2FromProtoField(Allocator* a, const TensorProto& in,\n     std::copy_n(begin, n, data);\n   } else if (in_n > 0) {\n     std::copy_n(begin, in_n, data);\n-    const uint16 last = *(data + in_n - 1);\n+    const uint16_t last = *(data + in_n - 1);\n     std::fill_n(data + in_n, n - in_n, last);\n   } else {\n     std::fill_n(data, n, 0);\n@@ -776,7 +776,7 @@ TensorBuffer* FromProtoField<Eigen::half>(Allocator* a, const TensorProto& in,\n                                           int64_t n) {\n   CHECK_GT(n, 0);\n   Buffer<Eigen::half>* buf = new Buffer<Eigen::half>(a, n);\n-  uint16* data = buf->template base<uint16>();\n+  uint16_t* data = buf->template base<uint16_t>();\n   if (data == nullptr) {\n     buf->Unref();\n     return nullptr;\n@@ -787,7 +787,7 @@ TensorBuffer* FromProtoField<Eigen::half>(Allocator* a, const TensorProto& in,\n     std::copy_n(begin, n, data);\n   } else if (in_n > 0) {\n     std::copy_n(begin, in_n, data);\n-    const uint16 last = *(data + in_n - 1);\n+    const uint16_t last = *(data + in_n - 1);\n     std::fill_n(data + in_n, n - in_n, last);\n   } else {\n     std::fill_n(data, n, 0);\n@@ -800,7 +800,7 @@ TensorBuffer* FromProtoField<bfloat16>(Allocator* a, const TensorProto& in,\n                                        int64_t n) {\n   CHECK_GT(n, 0);\n   Buffer<bfloat16>* buf = new Buffer<bfloat16>(a, n);\n-  uint16* data = buf->template base<uint16>();\n+  uint16_t* data = buf->template base<uint16_t>();\n   if (data == nullptr) {\n     buf->Unref();\n     return nullptr;\n@@ -811,7 +811,7 @@ TensorBuffer* FromProtoField<bfloat16>(Allocator* a, const TensorProto& in,\n     std::copy_n(begin, n, data);\n   } else if (in_n > 0) {\n     std::copy_n(begin, in_n, data);\n-    const uint16 last = *(data + in_n - 1);\n+    const uint16_t last = *(data + in_n - 1);\n     std::fill_n(data + in_n, n - in_n, last);\n   } else {\n     std::fill_n(data, n, 0);\n@@ -1240,7 +1240,7 @@ template <typename T>\n const T& PrintOneElement(const T& value, bool print_v2) {\n   return value;\n }\n-string PrintOneElement(const tstring& a, bool print_v2) {\n+std::string PrintOneElement(const tstring& a, bool print_v2) {\n   if (print_v2) {\n     return \"\\\"\" + absl::Utf8SafeCEscape(a) + \"\\\"\";\n   } else {\n@@ -1285,9 +1285,9 @@ uint16_t PrintOneElement(uint2 a, bool print_v2) {\n \n // Print from left dim to right dim recursively.\n template <typename T>\n-void PrintOneDim(int dim_index, const absl::InlinedVector<int64, 4UL>& shape,\n+void PrintOneDim(int dim_index, const absl::InlinedVector<int64_t, 4UL>& shape,\n                  int64_t limit, int shape_size, const T* data,\n-                 int64_t* data_index, string* result) {\n+                 int64_t* data_index, std::string* result) {\n   if (*data_index >= limit) return;\n   int64_t element_count = shape[dim_index];\n   // We have reached the right-most dimension of the tensor.\n@@ -1324,7 +1324,7 @@ void PrintOneDim(int dim_index, const absl::InlinedVector<int64, 4UL>& shape,\n }\n \n // Appends the spacing between elements for a given dim onto a result string\n-void PrintDimSpacing(int dim_index, int num_dims, string* result) {\n+void PrintDimSpacing(int dim_index, int num_dims, std::string* result) {\n   if (dim_index == num_dims - 1) {\n     absl::StrAppend(result, \" \");\n     return;\n@@ -1339,9 +1339,10 @@ void PrintDimSpacing(int dim_index, int num_dims, string* result) {\n \n // Print from left dim to right dim recursively.\n template <typename T>\n-void PrintOneDimV2(int dim_index, const absl::InlinedVector<int64, 4UL>& shape,\n+void PrintOneDimV2(int dim_index,\n+                   const absl::InlinedVector<int64_t, 4UL>& shape,\n                    int64_t num_elts_at_ends, int num_dims, const T* data,\n-                   int64_t data_index, string* result) {\n+                   int64_t data_index, std::string* result) {\n   // We have recursed beyond all the dimensions into a single element\n   // of the tensor.\n   if (dim_index == num_dims) {\n@@ -1384,10 +1385,10 @@ void PrintOneDimV2(int dim_index, const absl::InlinedVector<int64, 4UL>& shape,\n }\n \n template <typename T>\n-string SummarizeArrayInternal(int64_t limit, int64_t num_elts,\n-                              const TensorShape& tensor_shape, const T* array,\n-                              const bool print_v2) {\n-  string ret;\n+std::string SummarizeArrayInternal(int64_t limit, int64_t num_elts,\n+                                   const TensorShape& tensor_shape,\n+                                   const T* array, const bool print_v2) {\n+  std::string ret;\n   const absl::InlinedVector<int64_t, 4UL> shape = tensor_shape.dim_sizes();\n   if (shape.empty()) {\n     for (int64_t i = 0; i < limit; ++i) {\n@@ -1413,18 +1414,18 @@ string SummarizeArrayInternal(int64_t limit, int64_t num_elts,\n }\n \n template <typename T>\n-string SummarizeArray(int64_t limit, int64_t num_elts,\n-                      const TensorShape& tensor_shape, const char* data,\n-                      const bool print_v2) {\n+std::string SummarizeArray(int64_t limit, int64_t num_elts,\n+                           const TensorShape& tensor_shape, const char* data,\n+                           const bool print_v2) {\n   const T* array = reinterpret_cast<const T*>(data);\n   return SummarizeArrayInternal<T>(limit, num_elts, tensor_shape, array,\n                                    print_v2);\n }\n \n template <>\n-string SummarizeArray<bool>(int64_t limit, int64_t num_elts,\n-                            const TensorShape& tensor_shape, const char* data,\n-                            const bool print_v2) {\n+std::string SummarizeArray<bool>(int64_t limit, int64_t num_elts,\n+                                 const TensorShape& tensor_shape,\n+                                 const char* data, const bool print_v2) {\n   if (data == nullptr) {\n     return \"\";  // we already print type and shape\n   }\n@@ -1439,7 +1440,7 @@ string SummarizeArray<bool>(int64_t limit, int64_t num_elts,\n }\n }  // namespace\n \n-string Tensor::SummarizeValue(int64_t max_entries, bool print_v2) const {\n+std::string Tensor::SummarizeValue(int64_t max_entries, bool print_v2) const {\n   const int64_t num_elts = NumElements();\n   if (max_entries < 0) {\n     max_entries = num_elts;\n@@ -1474,29 +1475,29 @@ string Tensor::SummarizeValue(int64_t max_entries, bool print_v2) const {\n       return SummarizeArray<double>(limit, num_elts, shape_, data, print_v2);\n       break;\n     case DT_UINT32:\n-      return SummarizeArray<uint32>(limit, num_elts, shape_, data, print_v2);\n+      return SummarizeArray<uint32_t>(limit, num_elts, shape_, data, print_v2);\n       break;\n     case DT_INT32:\n-      return SummarizeArray<int32>(limit, num_elts, shape_, data, print_v2);\n+      return SummarizeArray<int32_t>(limit, num_elts, shape_, data, print_v2);\n       break;\n     case DT_UINT8:\n     case DT_QUINT8:\n-      return SummarizeArray<uint8>(limit, num_elts, shape_, data, print_v2);\n+      return SummarizeArray<uint8_t>(limit, num_elts, shape_, data, print_v2);\n       break;\n     case DT_UINT16:\n     case DT_QUINT16:\n-      return SummarizeArray<uint16>(limit, num_elts, shape_, data, print_v2);\n+      return SummarizeArray<uint16_t>(limit, num_elts, shape_, data, print_v2);\n       break;\n     case DT_INT16:\n     case DT_QINT16:\n-      return SummarizeArray<int16>(limit, num_elts, shape_, data, print_v2);\n+      return SummarizeArray<int16_t>(limit, num_elts, shape_, data, print_v2);\n       break;\n     case DT_INT8:\n     case DT_QINT8:\n-      return SummarizeArray<int8>(limit, num_elts, shape_, data, print_v2);\n+      return SummarizeArray<int8_t>(limit, num_elts, shape_, data, print_v2);\n       break;\n     case DT_UINT64:\n-      return SummarizeArray<uint64>(limit, num_elts, shape_, data, print_v2);\n+      return SummarizeArray<uint64_t>(limit, num_elts, shape_, data, print_v2);\n       break;\n     case DT_INT64:\n       return SummarizeArray<int64_t>(limit, num_elts, shape_, data, print_v2);\n@@ -1519,7 +1520,7 @@ string Tensor::SummarizeValue(int64_t max_entries, bool print_v2) const {\n       return SummarizeArray<uint2>(limit, num_elts, shape_, data, print_v2);\n     default: {\n       // All irregular cases\n-      string ret;\n+      std::string ret;\n       if (print_v2 && (dims() > 0)) {\n         absl::StrAppend(&ret, \"[\");\n       }\n@@ -1571,13 +1572,13 @@ bool Tensor::SharesBufferWith(const Tensor& b) const {\n          buf_->root_buffer() == b.buf_->root_buffer();\n }\n \n-string Tensor::DebugString(int num_values) const {\n+std::string Tensor::DebugString(int num_values) const {\n   return absl::StrCat(\"Tensor<type: \", DataTypeString(dtype()),\n                       \" shape: \", shape().DebugString(),\n                       \" values: \", SummarizeValue(num_values), \">\");\n }\n \n-string Tensor::DeviceSafeDebugString() const {\n+std::string Tensor::DeviceSafeDebugString() const {\n   return absl::StrCat(\"Tensor<type: \", DataTypeString(dtype()),\n                       \" shape: \", shape().DebugString(), \">\");\n }"
        },
        {
            "sha": "5db5b0bcd74e84179e517df66efad48de011d90e",
            "filename": "tensorflow/core/framework/tensor.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftensor.h?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -217,11 +217,11 @@ class Tensor {\n       : Tensor(scalar_value, host_scalar_tag{}) {}\n   explicit Tensor(int32_t scalar_value)\n       : Tensor(scalar_value, host_scalar_tag{}) {}\n-  explicit Tensor(uint32 scalar_value)\n+  explicit Tensor(uint32_t scalar_value)\n       : Tensor(scalar_value, host_scalar_tag{}) {}\n-  explicit Tensor(uint16 scalar_value)\n+  explicit Tensor(uint16_t scalar_value)\n       : Tensor(scalar_value, host_scalar_tag{}) {}\n-  explicit Tensor(uint8 scalar_value)\n+  explicit Tensor(uint8_t scalar_value)\n       : Tensor(scalar_value, host_scalar_tag{}) {}\n   explicit Tensor(int16_t scalar_value)\n       : Tensor(scalar_value, host_scalar_tag{}) {}\n@@ -235,7 +235,7 @@ class Tensor {\n       : Tensor(scalar_value, host_scalar_tag{}) {}\n   explicit Tensor(int64_t scalar_value)\n       : Tensor(scalar_value, host_scalar_tag{}) {}\n-  explicit Tensor(uint64 scalar_value)\n+  explicit Tensor(uint64_t scalar_value)\n       : Tensor(scalar_value, host_scalar_tag{}) {}\n   explicit Tensor(bool scalar_value)\n       : Tensor(scalar_value, host_scalar_tag{}) {}"
        },
        {
            "sha": "578d4c5697fd8de9e9a8e62c0647406899ed3bd0",
            "filename": "tensorflow/core/framework/tensor_fuzz.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_fuzz.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_fuzz.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftensor_fuzz.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -36,7 +36,7 @@ FUZZ_TEST(TensorFuzz, BuildTensorAlwaysSucceedsWithValidTensorShape)\n                                      /*dim_upper_bound=*/10));\n \n void DebugStringCheck(const Tensor& tensor) {\n-  string out = tensor.DeviceSafeDebugString();\n+  std::string out = tensor.DeviceSafeDebugString();\n }\n FUZZ_TEST(TensorFuzz, DebugStringCheck)\n     .WithDomains("
        },
        {
            "sha": "da2e4c88f3af31d9a2a18ff44636885a504cdd36",
            "filename": "tensorflow/core/framework/tensor_shape.cc",
            "status": "modified",
            "additions": 25,
            "deletions": 24,
            "changes": 49,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_shape.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_shape.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftensor_shape.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -35,7 +35,7 @@ static_assert(sizeof(TensorShapeRep) == sizeof(PartialTensorShape),\n \n template <class Shape>\n static void AppendTo(const TensorShapeBase<Shape>& s,\n-                     absl::InlinedVector<int64, 8UL>* vals) {\n+                     absl::InlinedVector<int64_t, 8UL>* vals) {\n   for (auto dim : s) {\n     vals->push_back(dim.size);\n   }\n@@ -213,10 +213,10 @@ absl::Status TensorShapeBase<Shape>::BuildTensorShapeBase(\n // Returns true iff partial is true and val is < 0.\n // REQUIRES: val < kMaxRep16\n // REQUIRES: partial || val >= 0\n-static inline bool Set16(bool partial, uint16* dst, int dim, int64_t val) {\n+static inline bool Set16(bool partial, uint16_t* dst, int dim, int64_t val) {\n   if (partial) {\n     if (val < 0) {\n-      dst[dim] = std::numeric_limits<uint16>::max();\n+      dst[dim] = std::numeric_limits<uint16_t>::max();\n       return true;\n     }\n   }\n@@ -232,7 +232,8 @@ absl::Status TensorShapeBase<Shape>::InitDims(\n   // Allow sizes that are under kint64max^0.25 so that 4-way multiplication\n   // below cannot overflow.\n   static const int64_t kMaxSmall = 0xd744;\n-  static_assert(kMaxSmall * kMaxSmall * kMaxSmall * kMaxSmall <= kint64max,\n+  static_assert(kMaxSmall * kMaxSmall * kMaxSmall * kMaxSmall <=\n+                    std::numeric_limits<int64_t>::max(),\n                 \"bad overflow check\");\n   bool large_size = false;\n   for (auto s : dim_sizes) {\n@@ -253,7 +254,7 @@ absl::Status TensorShapeBase<Shape>::InitDims(\n \n   if (!large_size) {\n     // Every size fits in 16 bits; use fast-paths for dims in {1,2,3,4}.\n-    uint16* dst = as16()->dims_;\n+    uint16_t* dst = as16()->dims_;\n     switch (dim_sizes.size()) {\n       case 1: {\n         set_ndims_byte(1);\n@@ -358,11 +359,11 @@ int64_t TensorShapeBase<Shape>::dim_size(int d) const {\n   CHECK_GE(d, 0);                  // Crash OK\n   if (d > 0) CHECK_LT(d, dims());  // Crash OK\n   if (tag() == REP16) {\n-    uint16 dim = as16()->dims_[d];\n+    uint16_t dim = as16()->dims_[d];\n     if (kIsPartial && dim == kUnknownRep16) return -1;\n     return dim;\n   } else if (tag() == REP32) {\n-    uint32 dim = as32()->dims_[d];\n+    uint32_t dim = as32()->dims_[d];\n     if (kIsPartial && dim == kUnknownRep32) return -1;\n     return dim;\n   } else {\n@@ -462,10 +463,10 @@ void TensorShapeBase<Shape>::UnsafeAddDim(int64_t size,\n   const int nd = ndims_byte();\n   if (tag() == REP16 && nd < 6 && size < kMaxRep16) {\n     as16()->dims_[nd] =\n-        kIsPartial && size < 0 ? kUnknownRep16 : static_cast<uint16>(size);\n+        kIsPartial && size < 0 ? kUnknownRep16 : static_cast<uint16_t>(size);\n   } else if (tag() == REP32 && nd < 3 && size < kMaxRep32) {\n     as32()->dims_[nd] =\n-        kIsPartial && size < 0 ? kUnknownRep32 : static_cast<uint32>(size);\n+        kIsPartial && size < 0 ? kUnknownRep32 : static_cast<uint32_t>(size);\n   } else if (tag() == REP_OUT_OF_LINE) {\n     as64()->dims_->push_back(size);\n   } else {\n@@ -490,7 +491,7 @@ void TensorShapeBase<Shape>::UnsafeAddDim(int64_t size,\n       for (size_t d = 0; d < vals.size(); d++) {\n         as32()->dims_[d] = kIsPartial && vals[d] < 0\n                                ? kUnknownRep32\n-                               : static_cast<uint32>(vals[d]);\n+                               : static_cast<uint32_t>(vals[d]);\n       }\n     } else {\n       set_tag(REP_OUT_OF_LINE);\n@@ -590,10 +591,10 @@ void TensorShapeBase<Shape>::set_dim(int d, int64_t size) {\n   }\n   if (tag() == REP16 && size < kMaxRep16) {\n     as16()->dims_[d] =\n-        kIsPartial && size < 0 ? kUnknownRep16 : static_cast<uint16>(size);\n+        kIsPartial && size < 0 ? kUnknownRep16 : static_cast<uint16_t>(size);\n   } else if (tag() == REP32 && size < kMaxRep32) {\n     as32()->dims_[d] =\n-        kIsPartial && size < 0 ? kUnknownRep32 : static_cast<uint32>(size);\n+        kIsPartial && size < 0 ? kUnknownRep32 : static_cast<uint32_t>(size);\n   } else if (tag() == REP_OUT_OF_LINE) {\n     (*as64()->dims_)[d] = size;\n   } else {\n@@ -624,10 +625,10 @@ absl::Status TensorShapeBase<Shape>::SetDimWithStatus(int d, int64_t size) {\n \n   if (tag() == REP16 && size < kMaxRep16) {\n     as16()->dims_[d] =\n-        kIsPartial && size < 0 ? kUnknownRep16 : static_cast<uint16>(size);\n+        kIsPartial && size < 0 ? kUnknownRep16 : static_cast<uint16_t>(size);\n   } else if (tag() == REP32 && size < kMaxRep32) {\n     as32()->dims_[d] =\n-        kIsPartial && size < 0 ? kUnknownRep32 : static_cast<uint32>(size);\n+        kIsPartial && size < 0 ? kUnknownRep32 : static_cast<uint32_t>(size);\n   } else if (tag() == REP_OUT_OF_LINE) {\n     (*as64()->dims_)[d] = size;\n   } else {\n@@ -752,10 +753,10 @@ TensorShapeIter<Shape> TensorShapeBase<Shape>::end() const {\n   return TensorShapeIter<Shape>(static_cast<const Shape*>(this), max_dim);\n }\n \n-string TensorShapeRep::DebugString() const {\n+std::string TensorShapeRep::DebugString() const {\n   const auto& shape = *static_cast<const PartialTensorShape*>(this);\n   if (shape.unknown_rank()) return \"<unknown>\";\n-  string s = \"[\";\n+  std::string s = \"[\";\n   for (int i = 0; i < shape.dims(); i++) {\n     if (i > 0) absl::StrAppend(&s, \",\");\n     int64_t dim = shape.dim_size(i);\n@@ -769,8 +770,8 @@ string TensorShapeRep::DebugString() const {\n   return s;\n }\n \n-string TensorShapeRep::DebugString(const TensorShapeProto& proto) {\n-  string s;\n+std::string TensorShapeRep::DebugString(const TensorShapeProto& proto) {\n+  std::string s;\n   if (proto.unknown_rank()) {\n     absl::StrAppend(&s, \"<unknown>\");\n     if (proto.dim_size() == 0) return s;\n@@ -858,15 +859,15 @@ absl::Status MakeShapeHelper(const T* dims, int64_t n, Shape* out) {\n   Status TensorShapeUtils::MakeShape(gtl::ArraySlice<T> shape, Shape* out) { \\\n     return MakeShapeHelper(shape.data(), shape.size(), out);                 \\\n   }\n-MAKE_SHAPE(int32, TensorShape)\n+MAKE_SHAPE(int32_t, TensorShape)\n MAKE_SHAPE(int64_t, TensorShape)\n-MAKE_SHAPE(int32, PartialTensorShape)\n+MAKE_SHAPE(int32_t, PartialTensorShape)\n MAKE_SHAPE(int64_t, PartialTensorShape)\n #undef MAKE_SHAPE\n \n-string TensorShapeUtils::ShapeListString(\n+std::string TensorShapeUtils::ShapeListString(\n     const absl::Span<const TensorShape>& shapes) {\n-  string result = \"[\";\n+  std::string result = \"[\";\n   bool first = true;\n   for (const TensorShape& shape : shapes) {\n     absl::StrAppend(&result, first ? \"\" : \", \", shape.DebugString());\n@@ -985,9 +986,9 @@ bool PartialTensorShape::IsCompatibleWith(\n   return true;\n }\n \n-string PartialTensorShapeUtils::PartialShapeListString(\n+std::string PartialTensorShapeUtils::PartialShapeListString(\n     const absl::Span<const PartialTensorShape>& shapes) {\n-  string result = \"[\";\n+  std::string result = \"[\";\n   bool first = true;\n   for (const PartialTensorShape& shape : shapes) {\n     absl::StrAppend(&result, first ? \"\" : \", \", shape.DebugString());"
        },
        {
            "sha": "dfc292a5f22e50a0f785a361ba10a88878571e54",
            "filename": "tensorflow/core/framework/tensor_shape.h",
            "status": "modified",
            "additions": 24,
            "deletions": 21,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_shape.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_shape.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftensor_shape.h?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -91,21 +91,23 @@ class TensorShapeRep {\n   // For PartialTensorShape, a dimension of static_cast<uint??>(-1) is unknown.\n   // This value is not allowed in TensorShape either for format compatibility.\n   struct Rep16 {\n-    uint16 dims_[6];\n+    uint16_t dims_[6];\n   };\n   struct Rep32 {\n-    uint32 dims_[3];\n+    uint32_t dims_[3];\n   };\n   struct Rep64 {\n     absl::InlinedVector<int64_t, 4UL>* dims_;\n   };\n \n   // We use the max value of uint16 or uint32 to represent unknown shapes, so\n   // the maximum representable valid shape in these representations is one less.\n-  static constexpr int64_t kMaxRep16 = std::numeric_limits<uint16>::max() - 1;\n-  static constexpr int64_t kMaxRep32 = std::numeric_limits<uint32>::max() - 1;\n-  static constexpr uint16 kUnknownRep16 = std::numeric_limits<uint16>::max();\n-  static constexpr uint32 kUnknownRep32 = std::numeric_limits<uint32>::max();\n+  static constexpr int64_t kMaxRep16 = std::numeric_limits<uint16_t>::max() - 1;\n+  static constexpr int64_t kMaxRep32 = std::numeric_limits<uint32_t>::max() - 1;\n+  static constexpr uint16_t kUnknownRep16 =\n+      std::numeric_limits<uint16_t>::max();\n+  static constexpr uint32_t kUnknownRep32 =\n+      std::numeric_limits<uint32_t>::max();\n \n   Rep16* as16() { return reinterpret_cast<Rep16*>(buf()); }\n   Rep32* as32() { return reinterpret_cast<Rep32*>(buf()); }\n@@ -126,31 +128,31 @@ class TensorShapeRep {\n   DataType data_type() const { return static_cast<DataType>(buf()[13]); }\n   void set_data_type(DataType dt) {\n     // We only have 8 bits available to store DataType, so make sure it fits\n-    DCHECK_LT(static_cast<uint32>(dt), 256u);\n-    buf()[13] = static_cast<uint8>(dt);\n+    DCHECK_LT(static_cast<uint32_t>(dt), 256u);\n+    buf()[13] = static_cast<uint8_t>(dt);\n   }\n \n   // We store the number of dimensions in byte 14, and the RepTag in byte 15.\n   // Bytes [0..13] vary depending on the representation.\n   // A value of 255 indicates unknown rank in the PartialTensorShape case.\n-  static constexpr uint8 kUnknownRank = 255;\n-  uint8 ndims_byte() const { return buf()[14]; }\n-  void set_ndims_byte(uint8 nd) { buf()[14] = nd; }\n+  static constexpr uint8_t kUnknownRank = 255;\n+  uint8_t ndims_byte() const { return buf()[14]; }\n+  void set_ndims_byte(uint8_t nd) { buf()[14] = nd; }\n \n   RepTag tag() const { return static_cast<RepTag>(buf()[15]); }\n-  void set_tag(RepTag tag) { buf()[15] = static_cast<uint8>(tag); }\n+  void set_tag(RepTag tag) { buf()[15] = static_cast<uint8_t>(tag); }\n \n   void set_num_elements(int64_t n) { num_elements_ = n; }\n \n  private:\n   void DestructorOutOfLine();\n   void SlowCopyFrom(const TensorShapeRep& b);\n \n-  uint8* buf() { return &u_.buf[0]; }\n-  const uint8* buf() const { return &u_.buf[0]; }\n+  uint8_t* buf() { return &u_.buf[0]; }\n+  const uint8_t* buf() const { return &u_.buf[0]; }\n \n   union {\n-    uint8 buf[16];\n+    uint8_t buf[16];\n     // Force data to be aligned enough for a pointer.\n     Rep64* unused_aligner;\n   } u_;\n@@ -290,7 +292,7 @@ class TensorShapeBase : public TensorShapeRep {\n   /// Return the number of dimensions in the tensor.\n   /// Can be -1 meaning unknown rank for PartialTensorShape.\n   int dims() const {\n-    uint8 dims = ndims_byte();\n+    uint8_t dims = ndims_byte();\n     return kIsPartial && dims == kUnknownRank ? -1 : dims;\n   }\n \n@@ -507,18 +509,19 @@ class TensorShapeUtils {\n \n   /// \\brief Returns a `TensorShape` whose dimensions are\n   /// `dims[0]`, `dims[1]`, ..., `dims[n-1]`.\n-  static absl::Status MakeShape(const int32* dims, int64_t n, TensorShape* out);\n+  static absl::Status MakeShape(const int32_t* dims, int64_t n,\n+                                TensorShape* out);\n   static absl::Status MakeShape(const int64_t* dims, int64_t n,\n                                 TensorShape* out);\n-  static absl::Status MakeShape(absl::Span<const int32> shape,\n+  static absl::Status MakeShape(absl::Span<const int32_t> shape,\n                                 TensorShape* out);\n   static absl::Status MakeShape(absl::Span<const int64_t> shape,\n                                 TensorShape* out);\n-  static absl::Status MakeShape(const int32* dims, int64_t n,\n+  static absl::Status MakeShape(const int32_t* dims, int64_t n,\n                                 PartialTensorShape* out);\n   static absl::Status MakeShape(const int64_t* dims, int64_t n,\n                                 PartialTensorShape* out);\n-  static absl::Status MakeShape(absl::Span<const int32> shape,\n+  static absl::Status MakeShape(absl::Span<const int32_t> shape,\n                                 PartialTensorShape* out);\n   static absl::Status MakeShape(absl::Span<const int64_t> shape,\n                                 PartialTensorShape* out);\n@@ -774,7 +777,7 @@ inline TensorShapeBase<Shape>::TensorShapeBase(DataType dt) {\n   // Optimized implementation of InitDims() where the shape is statically known\n   // to be {0}.\n   set_ndims_byte(1);\n-  uint16* dst = as16()->dims_;\n+  uint16_t* dst = as16()->dims_;\n   *dst = 0;\n   set_num_elements(0);\n }"
        },
        {
            "sha": "5156d62484dbe912346d70ec366829db32896910",
            "filename": "tensorflow/core/framework/tensor_shape_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_shape_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_shape_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftensor_shape_test.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -34,7 +34,7 @@ namespace tensorflow {\n class TensorShapeTestHelper {\n  public:\n   static void set_data_type(TensorShape* s, DataType t) { s->set_data_type(t); }\n-  static uint8 data_type(const TensorShape* s) { return s->data_type(); }\n+  static uint8_t data_type(const TensorShape* s) { return s->data_type(); }\n };\n \n namespace {\n@@ -620,11 +620,11 @@ class TensorShapeOld {\n   TensorShapeIterOld end() const;\n \n   /// For error messages.\n-  string DebugString() const;\n+  std::string DebugString() const;\n \n   /// Same as `TensorShape(proto).DebugString()` but doesn't crash for\n   /// invalid protos.\n-  static string DebugString(const TensorShapeProto& proto);\n+  static std::string DebugString(const TensorShapeProto& proto);\n \n  private:\n   // Recalculates the dimensions of this tensor after they are modified.\n@@ -794,13 +794,13 @@ TensorShapeIterOld TensorShapeOld::end() const {\n   return TensorShapeIterOld(this, dims());\n }\n \n-string TensorShapeOld::DebugString() const {\n+std::string TensorShapeOld::DebugString() const {\n   return absl::StrCat(\n       \"[\", absl::StrJoin(absl::Span<const int64_t>(dim_sizes_), \",\"), \"]\");\n }\n \n-string TensorShapeOld::DebugString(const TensorShapeProto& proto) {\n-  string s = \"[\";\n+std::string TensorShapeOld::DebugString(const TensorShapeProto& proto) {\n+  std::string s = \"[\";\n   bool first = true;\n   for (const auto& d : proto.dim()) {\n     absl::StrAppend(&s, first ? \"\" : \",\", d.size());"
        },
        {
            "sha": "30971ed5675b3911d772d3d48122da53f97e27bb",
            "filename": "tensorflow/core/framework/tensor_slice.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_slice.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_slice.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftensor_slice.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -75,18 +75,20 @@ absl::Status TensorSlice::BuildTensorSlice(const TensorSliceProto& proto,\n   return absl::OkStatus();\n }\n \n-absl::Status TensorSlice::Parse(const string& str, TensorSlice* slice) {\n-  std::vector<string> items = str_util::Split(str, ':', str_util::SkipEmpty());\n+absl::Status TensorSlice::Parse(const std::string& str, TensorSlice* slice) {\n+  std::vector<std::string> items =\n+      str_util::Split(str, ':', str_util::SkipEmpty());\n   slice->starts_.reserve(items.size());\n   slice->lengths_.reserve(items.size());\n-  for (const string& x : items) {\n+  for (const std::string& x : items) {\n     int64_t s, l;\n     if (x == \"-\") {\n       // \"everything\"\n       s = 0;\n       l = kFullExtent;\n     } else {\n-      std::vector<string> sl = str_util::Split(x, ',', str_util::SkipEmpty());\n+      std::vector<std::string> sl =\n+          str_util::Split(x, ',', str_util::SkipEmpty());\n       if (sl.size() != 2 || !absl::SimpleAtoi(sl[0], &s) ||\n           !absl::SimpleAtoi(sl[1], &l)) {\n         return errors::InvalidArgument(\n@@ -152,8 +154,8 @@ void TensorSlice::AsProto(TensorSliceProto* proto) const {\n   }\n }\n \n-string TensorSlice::DebugString() const {\n-  string buffer;\n+std::string TensorSlice::DebugString() const {\n+  std::string buffer;\n   bool first = true;\n   for (int d = 0; d < dims(); ++d) {\n     if (!first) {"
        },
        {
            "sha": "b6fc4f503bdb117b3470a742ecdb53e9a9bf5005",
            "filename": "tensorflow/core/framework/tensor_slice.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_slice.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_slice.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftensor_slice.h?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -54,8 +54,8 @@ class TensorSlice {\n   static absl::Status BuildTensorSlice(const TensorSliceProto& proto,\n                                        TensorSlice* output);\n \n-  static absl::Status Parse(const string& str, TensorSlice* output);\n-  static TensorSlice ParseOrDie(const string& str) {\n+  static absl::Status Parse(const std::string& str, TensorSlice* output);\n+  static TensorSlice ParseOrDie(const std::string& str) {\n     TensorSlice ret;\n     absl::Status s = Parse(str, &ret);\n     if (!s.ok()) {\n@@ -117,7 +117,7 @@ class TensorSlice {\n \n   // Conversion of a TensorSlice to other formats\n   void AsProto(TensorSliceProto* proto) const;\n-  string DebugString() const;\n+  std::string DebugString() const;\n \n   // Fill *indices and *sizes from *this (so that we can use the slice()\n   // function in eigen tensor). We need a tensor shape in case some of the"
        },
        {
            "sha": "411a81b81b9d27493cd3432f4933f75faf972bc1",
            "filename": "tensorflow/core/framework/tensor_test.cc",
            "status": "modified",
            "additions": 65,
            "deletions": 62,
            "changes": 127,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftensor_test.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -62,8 +62,8 @@ bool operator==(const Variant& a, const Variant& b) {\n   a.Encode(&a_data);\n   b.Encode(&b_data);\n \n-  string a_metadata;\n-  string b_metadata;\n+  std::string a_metadata;\n+  std::string b_metadata;\n   a_data.get_metadata(&a_metadata);\n   b_data.get_metadata(&b_metadata);\n   if (a_metadata != b_metadata) return false;\n@@ -74,7 +74,7 @@ bool operator==(const Variant& a, const Variant& b) {\n     TensorProto a_proto, b_proto;\n     a_data.tensors(i).AsProtoTensorContent(&a_proto);\n     b_data.tensors(i).AsProtoTensorContent(&b_proto);\n-    string a_str, b_str;\n+    std::string a_str, b_str;\n     a_proto.SerializeToString(&a_str);\n     b_proto.SerializeToString(&b_str);\n     if (a_str != b_str) return false;\n@@ -95,15 +95,15 @@ TEST(TensorTest, Default) {\n TEST(TensorTest, DataType_Traits) {\n   EXPECT_TRUE(std::is_trivial<float>::value);\n   EXPECT_TRUE(std::is_trivial<double>::value);\n-  EXPECT_TRUE(std::is_trivial<int32>::value);\n-  EXPECT_TRUE(std::is_trivial<uint8>::value);\n-  EXPECT_TRUE(std::is_trivial<uint16>::value);\n-  EXPECT_TRUE(std::is_trivial<int16>::value);\n-  EXPECT_TRUE(std::is_trivial<int8>::value);\n+  EXPECT_TRUE(std::is_trivial<int32_t>::value);\n+  EXPECT_TRUE(std::is_trivial<uint8_t>::value);\n+  EXPECT_TRUE(std::is_trivial<uint16_t>::value);\n+  EXPECT_TRUE(std::is_trivial<int16_t>::value);\n+  EXPECT_TRUE(std::is_trivial<int8_t>::value);\n   EXPECT_TRUE(std::is_trivial<int64_t>::value);\n   EXPECT_TRUE(std::is_trivial<bool>::value);\n   EXPECT_FALSE(std::is_trivial<tstring>::value);\n-  EXPECT_FALSE(std::is_trivial<string>::value);\n+  EXPECT_FALSE(std::is_trivial<std::string>::value);\n \n   EXPECT_EQ(sizeof(bool), 1);\n \n@@ -315,21 +315,21 @@ TEST(Tensor_int8, Simple) {\n   EXPECT_TRUE(t.shape().IsSameSize(TensorShape({10, 20})));\n   for (int64_t a = 0; a < t.shape().dim_size(0); a++) {\n     for (int64_t b = 0; b < t.shape().dim_size(1); b++) {\n-      t.matrix<int8>()(a, b) = static_cast<int8>(a * b);\n+      t.matrix<int8_t>()(a, b) = static_cast<int8_t>(a * b);\n     }\n   }\n-  TestCopies<int8>(t);\n+  TestCopies<int8_t>(t);\n }\n \n TEST(Tensor_int16, Simple) {\n   Tensor t(DT_INT16, TensorShape({10, 20}));\n   EXPECT_TRUE(t.shape().IsSameSize(TensorShape({10, 20})));\n   for (int64_t a = 0; a < t.shape().dim_size(0); a++) {\n     for (int64_t b = 0; b < t.shape().dim_size(1); b++) {\n-      t.matrix<int16>()(a, b) = static_cast<int16>(a * b);\n+      t.matrix<int16_t>()(a, b) = static_cast<int16_t>(a * b);\n     }\n   }\n-  TestCopies<int16>(t);\n+  TestCopies<int16_t>(t);\n }\n \n TEST(Tensor_int32, Simple) {\n@@ -499,32 +499,32 @@ TEST(Tensor_UInt8, Simple) {\n   EXPECT_TRUE(t.shape().IsSameSize(TensorShape({2, 2})));\n   for (int64_t a = 0; a < t.shape().dim_size(0); a++) {\n     for (int64_t b = 0; b < t.shape().dim_size(1); b++) {\n-      t.matrix<uint8>()(a, b) = static_cast<uint8>(a * b);\n+      t.matrix<uint8_t>()(a, b) = static_cast<uint8_t>(a * b);\n     }\n   }\n-  TestCopies<uint8>(t);\n+  TestCopies<uint8_t>(t);\n }\n \n TEST(Tensor_UInt16, Simple) {\n   Tensor t(DT_UINT16, TensorShape({2, 2}));\n   EXPECT_TRUE(t.shape().IsSameSize(TensorShape({2, 2})));\n   for (int64_t a = 0; a < t.shape().dim_size(0); a++) {\n     for (int64_t b = 0; b < t.shape().dim_size(1); b++) {\n-      t.matrix<uint16>()(a, b) = static_cast<uint16>(a * b);\n+      t.matrix<uint16_t>()(a, b) = static_cast<uint16_t>(a * b);\n     }\n   }\n-  TestCopies<uint16>(t);\n+  TestCopies<uint16_t>(t);\n }\n \n TEST(Tensor_UInt32, Simple) {\n   Tensor t(DT_UINT32, TensorShape({2, 2}));\n   EXPECT_TRUE(t.shape().IsSameSize(TensorShape({2, 2})));\n   for (int64_t a = 0; a < t.shape().dim_size(0); a++) {\n     for (int64_t b = 0; b < t.shape().dim_size(1); b++) {\n-      t.matrix<uint32>()(a, b) = static_cast<uint32>(a * b);\n+      t.matrix<uint32_t>()(a, b) = static_cast<uint32_t>(a * b);\n     }\n   }\n-  TestCopies<uint32>(t);\n+  TestCopies<uint32_t>(t);\n }\n \n TEST(Tensor_QInt8, Simple) {\n@@ -576,7 +576,7 @@ TEST(Tensor_QInt32, Simple) {\n   EXPECT_TRUE(t.shape().IsSameSize(TensorShape({2, 2})));\n   for (int64_t a = 0; a < t.shape().dim_size(0); a++) {\n     for (int64_t b = 0; b < t.shape().dim_size(1); b++) {\n-      t.matrix<qint32>()(a, b) = qint32(static_cast<int32>(a * b));\n+      t.matrix<qint32>()(a, b) = qint32(static_cast<int32_t>(a * b));\n     }\n   }\n   TestCopies<qint32>(t);\n@@ -970,26 +970,26 @@ TEST(ReinterpretLastDimension, Reinterpret_NCHW_VECT_C_as_NCHW) {\n     Tensor t_nchw_vect_c(DT_QINT8, TensorShape({2, 3, 5, 7, 4}));\n     auto nchw_vect_c = t_nchw_vect_c.tensor<qint8, 5>();\n     Tensor t_expected_nchw(DT_INT32, TensorShape({2, 3, 5, 7}));\n-    auto expected_nchw = t_expected_nchw.tensor<int32, 4>();\n+    auto expected_nchw = t_expected_nchw.tensor<int32_t, 4>();\n     int8_t val = 0;\n     for (int n = 0; n < t_nchw_vect_c.shape().dim_size(0); ++n) {\n       for (int c = 0; c < t_nchw_vect_c.shape().dim_size(1); ++c) {\n         for (int h = 0; h < t_nchw_vect_c.shape().dim_size(2); ++h, ++val) {\n-          int8 packet[4];\n+          int8_t packet[4];\n           for (int w = 0; w < t_nchw_vect_c.shape().dim_size(3); ++w) {\n             packet[0] = nchw_vect_c(n, c, h, w, 0) = ++val;\n             packet[1] = nchw_vect_c(n, c, h, w, 1) = ++val;\n             packet[2] = nchw_vect_c(n, c, h, w, 2) = ++val;\n             packet[3] = nchw_vect_c(n, c, h, w, 3) = ++val;\n-            expected_nchw(n, c, h, w) = *reinterpret_cast<int32*>(&packet[0]);\n+            expected_nchw(n, c, h, w) = *reinterpret_cast<int32_t*>(&packet[0]);\n           }\n         }\n       }\n     }\n-    auto actual_nchw = t_nchw_vect_c.reinterpret_last_dimension<int32, 4>();\n+    auto actual_nchw = t_nchw_vect_c.reinterpret_last_dimension<int32_t, 4>();\n     const auto& const_t_nchw_vect_c = t_nchw_vect_c;\n     auto const_actual_nchw =\n-        const_t_nchw_vect_c.reinterpret_last_dimension<int32, 4>();\n+        const_t_nchw_vect_c.reinterpret_last_dimension<int32_t, 4>();\n     for (int n = 0; n < t_nchw_vect_c.shape().dim_size(0); ++n) {\n       for (int c = 0; c < t_nchw_vect_c.shape().dim_size(1); ++c) {\n         for (int h = 0; h < t_nchw_vect_c.shape().dim_size(2); ++h) {\n@@ -1217,19 +1217,19 @@ TEST(Tensor_Float, SimpleWithAllocator) {\n }\n \n TEST(Tensor_Int32, SimpleWithHelper) {\n-  Tensor t1 = test::AsTensor<int32>({0, 1, 2, 3, 4, 5}, {2, 3});\n+  Tensor t1 = test::AsTensor<int32_t>({0, 1, 2, 3, 4, 5}, {2, 3});\n   Tensor t2(t1.dtype(), t1.shape());\n-  t2.flat<int32>() = t1.flat<int32>() * 2;\n-  Tensor t3 = test::AsTensor<int32>({0, 2, 4, 6, 8, 10}, t1.shape());\n-  ExpectEqual<int32>(t2, t3);\n+  t2.flat<int32_t>() = t1.flat<int32_t>() * 2;\n+  Tensor t3 = test::AsTensor<int32_t>({0, 2, 4, 6, 8, 10}, t1.shape());\n+  ExpectEqual<int32_t>(t2, t3);\n }\n \n TEST(Tensor_UInt16, SimpleWithHelper) {\n-  Tensor t1 = test::AsTensor<uint16>({0, 1, 2, 3, 4, 5}, {2, 3});\n+  Tensor t1 = test::AsTensor<uint16_t>({0, 1, 2, 3, 4, 5}, {2, 3});\n   Tensor t2(t1.dtype(), t1.shape());\n-  t2.flat<uint16>() = t1.flat<uint16>() * uint16(2);\n-  Tensor t3 = test::AsTensor<uint16>({0, 2, 4, 6, 8, 10}, t1.shape());\n-  ExpectEqual<uint16>(t2, t3);\n+  t2.flat<uint16_t>() = t1.flat<uint16_t>() * uint16_t(2);\n+  Tensor t3 = test::AsTensor<uint16_t>({0, 2, 4, 6, 8, 10}, t1.shape());\n+  ExpectEqual<uint16_t>(t2, t3);\n }\n \n TEST(Tensor_QInt8, SimpleWithHelper) {\n@@ -1413,7 +1413,7 @@ TEST(Tensor_Complex, SimpleWithHelper128) {\n class DummyCPUAllocator : public Allocator {\n  public:\n   DummyCPUAllocator() = default;\n-  string Name() override { return \"cpu\"; }\n+  std::string Name() override { return \"cpu\"; }\n   void* AllocateRaw(size_t alignment, size_t num_bytes) override {\n     return nullptr;\n   }\n@@ -1583,7 +1583,7 @@ TEST(Tensor, Slice_Basic) {\n   {\n     // Test unaligned access via a Slice for 8-bit data type.\n     Tensor x(DT_INT8, TensorShape({30}));\n-    x.flat<int8>().setConstant(0);\n+    x.flat<int8_t>().setConstant(0);\n \n     // Take an unaligned slice.\n     Tensor y = x.Slice(1, 13);\n@@ -1705,37 +1705,40 @@ INSTANTIATE_TEST_SUITE_P(\n                                   {1, 2, 3, 4, 0}),\n          MkTensor<Eigen::QUInt16>(DT_QUINT16, TensorShape({0}), {})},\n         {\"DT_UINT8\",\n-         MkTensor<uint8>(DT_UINT8, TensorShape({5}), {1, 2, 3, 4, 0}),\n-         MkTensor<uint8>(DT_UINT8, TensorShape({2, 2}), {1, 2, 3, 4, 0}),\n-         MkTensor<uint8>(DT_UINT8, TensorShape({2, 2, 1, 1}), {1, 2, 3, 4, 0}),\n-         MkTensor<uint8>(DT_UINT8, TensorShape({0}), {})},\n+         MkTensor<uint8_t>(DT_UINT8, TensorShape({5}), {1, 2, 3, 4, 0}),\n+         MkTensor<uint8_t>(DT_UINT8, TensorShape({2, 2}), {1, 2, 3, 4, 0}),\n+         MkTensor<uint8_t>(DT_UINT8, TensorShape({2, 2, 1, 1}),\n+                           {1, 2, 3, 4, 0}),\n+         MkTensor<uint8_t>(DT_UINT8, TensorShape({0}), {})},\n         {\"DT_UINT16\",\n-         MkTensor<uint16>(DT_UINT16, TensorShape({5}), {1, 2, 3, 4, 0}),\n-         MkTensor<uint16>(DT_UINT16, TensorShape({2, 2}), {1, 2, 3, 4, 0}),\n-         MkTensor<uint16>(DT_UINT16, TensorShape({2, 2, 1, 1}),\n-                          {1, 2, 3, 4, 0}),\n-         MkTensor<uint16>(DT_UINT16, TensorShape({0}), {})},\n+         MkTensor<uint16_t>(DT_UINT16, TensorShape({5}), {1, 2, 3, 4, 0}),\n+         MkTensor<uint16_t>(DT_UINT16, TensorShape({2, 2}), {1, 2, 3, 4, 0}),\n+         MkTensor<uint16_t>(DT_UINT16, TensorShape({2, 2, 1, 1}),\n+                            {1, 2, 3, 4, 0}),\n+         MkTensor<uint16_t>(DT_UINT16, TensorShape({0}), {})},\n         {\"DT_UINT32\",\n-         MkTensor<uint32>(DT_UINT32, TensorShape({5}), {1, 2, 3, 4, 0}),\n-         MkTensor<uint32>(DT_UINT32, TensorShape({2, 2}), {1, 2, 3, 4, 0}),\n-         MkTensor<uint32>(DT_UINT32, TensorShape({2, 2, 1, 1}),\n-                          {1, 2, 3, 4, 0}),\n-         MkTensor<uint32>(DT_UINT32, TensorShape({0}), {})},\n+         MkTensor<uint32_t>(DT_UINT32, TensorShape({5}), {1, 2, 3, 4, 0}),\n+         MkTensor<uint32_t>(DT_UINT32, TensorShape({2, 2}), {1, 2, 3, 4, 0}),\n+         MkTensor<uint32_t>(DT_UINT32, TensorShape({2, 2, 1, 1}),\n+                            {1, 2, 3, 4, 0}),\n+         MkTensor<uint32_t>(DT_UINT32, TensorShape({0}), {})},\n         {\"DT_UINT64\",\n-         MkTensor<uint64>(DT_UINT64, TensorShape({5}), {1, 2, 3, 4, 0}),\n-         MkTensor<uint64>(DT_UINT64, TensorShape({2, 2}), {1, 2, 3, 4, 0}),\n-         MkTensor<uint64>(DT_UINT64, TensorShape({2, 2, 1, 1}),\n-                          {1, 2, 3, 4, 0}),\n-         MkTensor<uint64>(DT_UINT64, TensorShape({0}), {})},\n-        {\"DT_INT8\", MkTensor<int8>(DT_INT8, TensorShape({5}), {1, 2, 3, 4, 0}),\n-         MkTensor<int8>(DT_INT8, TensorShape({2, 2}), {1, 2, 3, 4, 0}),\n-         MkTensor<int8>(DT_INT8, TensorShape({2, 2, 1, 1}), {1, 2, 3, 4, 0}),\n-         MkTensor<int8>(DT_INT8, TensorShape({0}), {})},\n+         MkTensor<uint64_t>(DT_UINT64, TensorShape({5}), {1, 2, 3, 4, 0}),\n+         MkTensor<uint64_t>(DT_UINT64, TensorShape({2, 2}), {1, 2, 3, 4, 0}),\n+         MkTensor<uint64_t>(DT_UINT64, TensorShape({2, 2, 1, 1}),\n+                            {1, 2, 3, 4, 0}),\n+         MkTensor<uint64_t>(DT_UINT64, TensorShape({0}), {})},\n+        {\"DT_INT8\",\n+         MkTensor<int8_t>(DT_INT8, TensorShape({5}), {1, 2, 3, 4, 0}),\n+         MkTensor<int8_t>(DT_INT8, TensorShape({2, 2}), {1, 2, 3, 4, 0}),\n+         MkTensor<int8_t>(DT_INT8, TensorShape({2, 2, 1, 1}), {1, 2, 3, 4, 0}),\n+         MkTensor<int8_t>(DT_INT8, TensorShape({0}), {})},\n         {\"DT_INT16\",\n-         MkTensor<int16>(DT_INT16, TensorShape({5}), {1, 2, 3, 4, 0}),\n-         MkTensor<int16>(DT_INT16, TensorShape({2, 2}), {1, 2, 3, 4, 0}),\n-         MkTensor<int16>(DT_INT16, TensorShape({2, 2, 1, 1}), {1, 2, 3, 4, 0}),\n-         MkTensor<int16>(DT_INT16, TensorShape({0}), {})},\n+         MkTensor<int16_t>(DT_INT16, TensorShape({5}), {1, 2, 3, 4, 0}),\n+         MkTensor<int16_t>(DT_INT16, TensorShape({2, 2}), {1, 2, 3, 4, 0}),\n+         MkTensor<int16_t>(DT_INT16, TensorShape({2, 2, 1, 1}),\n+                           {1, 2, 3, 4, 0}),\n+         MkTensor<int16_t>(DT_INT16, TensorShape({0}), {})},\n         {\"DT_INT32\", MkTensor<int>(DT_INT32, TensorShape({5}), {1, 2, 3, 4, 0}),\n          MkTensor<int>(DT_INT32, TensorShape({2, 2}), {1, 2, 3, 4, 0}),\n          MkTensor<int>(DT_INT32, TensorShape({2, 2, 1, 1}), {1, 2, 3, 4, 0}),"
        },
        {
            "sha": "a8b887d5ef5279ab971536a90e5b2809406806c7",
            "filename": "tensorflow/core/framework/tensor_testutil.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_testutil.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_testutil.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftensor_testutil.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -49,7 +49,8 @@ static ::testing::AssertionResult EqualFailure(const T& x, const T& y) {\n }\n \n template <>\n-::testing::AssertionResult EqualFailure<int8>(const int8& x, const int8& y) {\n+::testing::AssertionResult EqualFailure<int8_t>(const int8_t& x,\n+                                                const int8_t& y) {\n   return EqualFailure(static_cast<int>(x), static_cast<int>(y));\n }\n \n@@ -231,17 +232,17 @@ void ExpectEqual(const Tensor& x, const Tensor& y, Tolerance t) {\n     case DT_DOUBLE:\n       return ExpectEqual<double>(x, y, t);\n     case DT_INT32:\n-      return ExpectEqual<int32>(x, y);\n+      return ExpectEqual<int32_t>(x, y);\n     case DT_UINT32:\n-      return ExpectEqual<uint32>(x, y);\n+      return ExpectEqual<uint32_t>(x, y);\n     case DT_UINT16:\n-      return ExpectEqual<uint16>(x, y);\n+      return ExpectEqual<uint16_t>(x, y);\n     case DT_UINT8:\n-      return ExpectEqual<uint8>(x, y);\n+      return ExpectEqual<uint8_t>(x, y);\n     case DT_INT16:\n-      return ExpectEqual<int16>(x, y);\n+      return ExpectEqual<int16_t>(x, y);\n     case DT_INT8:\n-      return ExpectEqual<int8>(x, y);\n+      return ExpectEqual<int8_t>(x, y);\n     case DT_STRING:\n       return ExpectEqual<tstring>(x, y);\n     case DT_COMPLEX64:\n@@ -251,7 +252,7 @@ void ExpectEqual(const Tensor& x, const Tensor& y, Tolerance t) {\n     case DT_INT64:\n       return ExpectEqual<int64_t>(x, y);\n     case DT_UINT64:\n-      return ExpectEqual<uint64>(x, y);\n+      return ExpectEqual<uint64_t>(x, y);\n     case DT_BOOL:\n       return ExpectEqual<bool>(x, y);\n     case DT_QINT8:"
        },
        {
            "sha": "899efab94e85ee86f3dad26472113b19221a4b3d",
            "filename": "tensorflow/core/framework/tensor_util.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftensor_util.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -430,7 +430,7 @@ absl::Status MakeShape(const Tensor& shape, TensorShape* out) {\n         shape.shape().DebugString());\n   }\n   if (shape.dtype() == DataType::DT_INT32) {\n-    auto vec = shape.flat<int32>();\n+    auto vec = shape.flat<int32_t>();\n     return TensorShapeUtils::MakeShape(vec.data(), vec.size(), out);\n   } else if (shape.dtype() == DataType::DT_INT64) {\n     auto vec = shape.flat<int64_t>();"
        },
        {
            "sha": "7b9aaba557774dd4627e87859ca2fa0a0b24d0d5",
            "filename": "tensorflow/core/framework/tensor_util.h",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftensor_util.h?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -94,14 +94,14 @@ class TensorProtoFieldHelper : public std::false_type {};\n // values in TensorProto. See tensorflow/core/framework/tensor.proto.\n DEFINE_PROTO_FIELD_HELPER(float, float);\n DEFINE_PROTO_FIELD_HELPER(double, double);\n-DEFINE_PROTO_FIELD_HELPER(int8, int);\n-DEFINE_PROTO_FIELD_HELPER(uint8, int);\n-DEFINE_PROTO_FIELD_HELPER(int16, int);\n-DEFINE_PROTO_FIELD_HELPER(uint16, int);\n-DEFINE_PROTO_FIELD_HELPER(int32, int);\n-DEFINE_PROTO_FIELD_HELPER(uint32, uint32);\n+DEFINE_PROTO_FIELD_HELPER(int8_t, int);\n+DEFINE_PROTO_FIELD_HELPER(uint8_t, int);\n+DEFINE_PROTO_FIELD_HELPER(int16_t, int);\n+DEFINE_PROTO_FIELD_HELPER(uint16_t, int);\n+DEFINE_PROTO_FIELD_HELPER(int32_t, int);\n+DEFINE_PROTO_FIELD_HELPER(uint32_t, uint32);\n DEFINE_PROTO_FIELD_HELPER(int64_t, int64);\n-DEFINE_PROTO_FIELD_HELPER(uint64, uint64);\n+DEFINE_PROTO_FIELD_HELPER(uint64_t, uint64);\n DEFINE_PROTO_FIELD_HELPER(bool, bool);\n DEFINE_PROTO_FIELD_HELPER(qint8, int);\n DEFINE_PROTO_FIELD_HELPER(quint8, int);\n@@ -142,13 +142,13 @@ struct CopyHelper<Eigen::half> {\n   template <typename SrcIter>\n   static void ToArray(SrcIter begin, SrcIter end, Eigen::half* dst) {\n     std::transform(begin, end, dst, [](int x) -> Eigen::half {\n-      return Eigen::numext::bit_cast<Eigen::half>(static_cast<uint16>(x));\n+      return Eigen::numext::bit_cast<Eigen::half>(static_cast<uint16_t>(x));\n     });\n   }\n   template <typename SrcIter, typename DstIter>\n   static void FromArray(SrcIter begin, SrcIter end, DstIter dst) {\n     std::transform(begin, end, dst, [](Eigen::half h) -> int {\n-      return static_cast<int>(Eigen::numext::bit_cast<uint16>(h));\n+      return static_cast<int>(Eigen::numext::bit_cast<uint16_t>(h));\n     });\n   }\n };\n@@ -158,13 +158,13 @@ struct CopyHelper<bfloat16> {\n   template <typename SrcIter>\n   static void ToArray(SrcIter begin, SrcIter end, bfloat16* dst) {\n     std::transform(begin, end, dst, [](int x) -> bfloat16 {\n-      return Eigen::numext::bit_cast<bfloat16>(static_cast<uint16>(x));\n+      return Eigen::numext::bit_cast<bfloat16>(static_cast<uint16_t>(x));\n     });\n   }\n   template <typename SrcIter, typename DstIter>\n   static void FromArray(SrcIter begin, SrcIter end, DstIter dst) {\n     std::transform(begin, end, dst, [](bfloat16 bf16) -> int {\n-      return static_cast<int>(Eigen::numext::bit_cast<uint16>(bf16));\n+      return static_cast<int>(Eigen::numext::bit_cast<uint16_t>(bf16));\n     });\n   }\n };\n@@ -245,10 +245,10 @@ class TensorProtoHelper : public std::true_type {\n \n // Specialization for string.\n template <>\n-class TensorProtoHelper<string> : public std::true_type {\n+class TensorProtoHelper<std::string> : public std::true_type {\n  public:\n   static DataType GetDataType() { return DataType::DT_STRING; }\n-  static void AddValue(const string& value, TensorProto* proto) {\n+  static void AddValue(const std::string& value, TensorProto* proto) {\n     *proto->mutable_string_val()->Add() = value;\n   }\n   template <typename IterType>"
        },
        {
            "sha": "04414e89560a627a1526da2a74e8dadd9f9ef25d",
            "filename": "tensorflow/core/framework/tensor_util_test.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 26,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftensor_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftensor_util_test.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -122,7 +122,7 @@ TEST(TensorUtil, DeepCopy) {\n \n TEST(TensorUtil, DeepCopySlice) {\n   Tensor x(DT_INT32, TensorShape({10}));\n-  x.flat<int32>().setConstant(1);\n+  x.flat<int32_t>().setConstant(1);\n \n   // Slice 'x' -- y still refers to the same buffer.\n   Tensor y = x.Slice(2, 6);\n@@ -131,7 +131,7 @@ TEST(TensorUtil, DeepCopySlice) {\n   Tensor z = tensor::DeepCopy(y);\n \n   // Set x to be different.\n-  x.flat<int32>().setConstant(2);\n+  x.flat<int32_t>().setConstant(2);\n \n   EXPECT_EQ(TensorShape({10}), x.shape());\n   EXPECT_EQ(TensorShape({4}), y.shape());\n@@ -142,11 +142,11 @@ TEST(TensorUtil, DeepCopySlice) {\n \n   // x and y should now all be '2', but z should be '1'.\n   for (int i = 0; i < 10; ++i) {\n-    EXPECT_EQ(2, x.flat<int32>()(i));\n+    EXPECT_EQ(2, x.flat<int32_t>()(i));\n   }\n   for (int i = 0; i < 4; ++i) {\n-    EXPECT_EQ(2, y.unaligned_flat<int32>()(i));\n-    EXPECT_EQ(1, z.flat<int32>()(i));\n+    EXPECT_EQ(2, y.unaligned_flat<int32_t>()(i));\n+    EXPECT_EQ(1, z.flat<int32_t>()(i));\n   }\n }\n \n@@ -223,7 +223,7 @@ TEST(TensorUtil, Concat) {\n     Tensor tensor(DT_INT32, TensorShape({size, 2}));\n     for (int i = offset; i < offset + size; ++i) {\n       for (int j = 0; j < 2; ++j) {\n-        tensor.matrix<int32>()(i - offset, j) = 2 * i + j;\n+        tensor.matrix<int32_t>()(i - offset, j) = 2 * i + j;\n       }\n     }\n     to_concat.push_back(tensor);\n@@ -236,7 +236,7 @@ TEST(TensorUtil, Concat) {\n   ASSERT_EQ(TensorShape({total_size, 2}), concated.shape());\n   for (int i = 0; i < total_size; ++i) {\n     for (int j = 0; j < 2; ++j) {\n-      EXPECT_EQ(2 * i + j, concated.matrix<int32>()(i, j));\n+      EXPECT_EQ(2 * i + j, concated.matrix<int32_t>()(i, j));\n     }\n   }\n }\n@@ -296,9 +296,9 @@ TEST(TensorUtil, ConcatSplitStrings) {\n \n TEST(TensorProtoUtil, CreateTensorProtoSpan_string) {\n   // Don't use vector to trigger Span version.\n-  string s[2] = {\"a\", \"b\"};\n+  std::string s[2] = {\"a\", \"b\"};\n   std::vector<size_t> shape{1, 2};\n-  auto proto = tensor::CreateTensorProtoSpan<string>(s, shape);\n+  auto proto = tensor::CreateTensorProtoSpan<std::string>(s, shape);\n   TensorProto expected_tensor_proto;\n   expected_tensor_proto.set_dtype(DT_STRING);\n   expected_tensor_proto.mutable_tensor_shape()->add_dim()->set_size(1);\n@@ -310,9 +310,9 @@ TEST(TensorProtoUtil, CreateTensorProtoSpan_string) {\n \n TEST(TensorProtoUtil, CreateTensorProtoSpan_int32) {\n   // Don't use vector to trigger Span version.\n-  int32 s[2] = {123, 456};\n+  int32_t s[2] = {123, 456};\n   std::vector<size_t> shape{1, 2};\n-  auto proto = tensor::CreateTensorProtoSpan<int32>(s, shape);\n+  auto proto = tensor::CreateTensorProtoSpan<int32_t>(s, shape);\n   TensorProto expected_tensor_proto;\n   expected_tensor_proto.set_dtype(DT_INT32);\n   expected_tensor_proto.mutable_tensor_shape()->add_dim()->set_size(1);\n@@ -323,7 +323,7 @@ TEST(TensorProtoUtil, CreateTensorProtoSpan_int32) {\n }\n \n TEST(TensorProtoUtil, CreatesStringTensorProto) {\n-  std::vector<string> values{\"a\", \"b\", \"c\"};\n+  std::vector<std::string> values{\"a\", \"b\", \"c\"};\n   std::vector<size_t> shape{1, 3};\n \n   auto proto = tensor::CreateTensorProto(values, shape);\n@@ -347,7 +347,7 @@ TEST(TensorProtoUtil, CreatesStringTensorProto) {\n }\n \n TEST(TensorProtoUtil, CreatesInt32TensorProto) {\n-  std::vector<int32> values{1, 2};\n+  std::vector<int32_t> values{1, 2};\n   std::vector<size_t> shape{2};\n \n   auto proto = tensor::CreateTensorProto(values, shape);\n@@ -387,7 +387,7 @@ TEST(TensorProtoUtil, CreatesInt64TensorProto) {\n }\n \n TEST(TensorProtoUtil, CreatesUInt32TensorProto) {\n-  std::vector<uint32> values{1, 2};\n+  std::vector<uint32_t> values{1, 2};\n   std::vector<size_t> shape{2};\n \n   auto proto = tensor::CreateTensorProto(values, shape);\n@@ -407,7 +407,7 @@ TEST(TensorProtoUtil, CreatesUInt32TensorProto) {\n }\n \n TEST(TensorProtoUtil, CreatesUInt64TensorProto) {\n-  std::vector<uint64> values{1, 2};\n+  std::vector<uint64_t> values{1, 2};\n   std::vector<size_t> shape{2};\n \n   auto proto = tensor::CreateTensorProto(values, shape);\n@@ -495,7 +495,7 @@ TEST(TensorProtoUtil, CompressTensorProtoInPlaceTooSmall) {\n       tensor::CreateTensorProto(std::vector<int>(kLength), {kLength});\n   EXPECT_FALSE(tensor::CompressTensorProtoInPlace(&tensor_proto));\n   tensor_proto =\n-      tensor::CreateTensorProto(std::vector<uint8>(kLength), {kLength});\n+      tensor::CreateTensorProto(std::vector<uint8_t>(kLength), {kLength});\n   EXPECT_FALSE(tensor::CompressTensorProtoInPlace(&tensor_proto));\n   tensor_proto =\n       tensor::CreateTensorProto(std::vector<bool>(kLength), {kLength});\n@@ -523,10 +523,10 @@ TEST(TensorProtoUtil, CompressTensorProtoInPlaceAllEqual) {\n             0);\n \n   tensor_proto =\n-      tensor::CreateTensorProto(std::vector<uint8>(kLength), {kLength});\n+      tensor::CreateTensorProto(std::vector<uint8_t>(kLength), {kLength});\n   EXPECT_TRUE(tensor::CompressTensorProtoInPlace(&tensor_proto));\n-  EXPECT_EQ(tensor::internal::TensorProtoHelper<uint8>::NumValues(tensor_proto),\n-            0);\n+  EXPECT_EQ(\n+      tensor::internal::TensorProtoHelper<uint8_t>::NumValues(tensor_proto), 0);\n   tensor_proto =\n       tensor::CreateTensorProto(std::vector<bool>(kLength), {kLength});\n   EXPECT_TRUE(tensor::CompressTensorProtoInPlace(&tensor_proto));\n@@ -645,14 +645,14 @@ TEST(TensorProtoUtil, CompressTensorProtoConstantTail) {\n       ConstantTailTest<double>(kLength, tail_length, as_field);\n       ConstantTailTest<complex64>(kLength, tail_length, as_field);\n       ConstantTailTest<complex128>(kLength, tail_length, as_field);\n-      ConstantTailTest<int32>(kLength, tail_length, as_field);\n-      ConstantTailTest<uint32>(kLength, tail_length, as_field);\n+      ConstantTailTest<int32_t>(kLength, tail_length, as_field);\n+      ConstantTailTest<uint32_t>(kLength, tail_length, as_field);\n       ConstantTailTest<int64_t>(kLength, tail_length, as_field);\n-      ConstantTailTest<uint64>(kLength, tail_length, as_field);\n-      ConstantTailTest<int8>(kLength, tail_length, as_field);\n-      ConstantTailTest<uint8>(kLength, tail_length, as_field);\n-      ConstantTailTest<int16>(kLength, tail_length, as_field);\n-      ConstantTailTest<uint16>(kLength, tail_length, as_field);\n+      ConstantTailTest<uint64_t>(kLength, tail_length, as_field);\n+      ConstantTailTest<int8_t>(kLength, tail_length, as_field);\n+      ConstantTailTest<uint8_t>(kLength, tail_length, as_field);\n+      ConstantTailTest<int16_t>(kLength, tail_length, as_field);\n+      ConstantTailTest<uint16_t>(kLength, tail_length, as_field);\n       ConstantTailTest<Eigen::half>(kLength, tail_length, as_field);\n       ConstantTailTest<bfloat16>(kLength, tail_length, as_field);\n     }"
        },
        {
            "sha": "22c8238f3a40dd308619ddcdf3988153de2b6fc7",
            "filename": "tensorflow/core/framework/thread_factory.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fthread_factory.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fthread_factory.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fthread_factory.h?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -35,7 +35,7 @@ class ThreadFactory {\n   //\n   // NOTE: The caller is responsible for ensuring that this `ThreadFactory`\n   // outlives the returned `Thread`.\n-  virtual std::unique_ptr<Thread> StartThread(const string& name,\n+  virtual std::unique_ptr<Thread> StartThread(const std::string& name,\n                                               std::function<void()> fn) = 0;\n };\n "
        },
        {
            "sha": "c1ebe5577b70d1bd707063729ea8fa2908e4b61e",
            "filename": "tensorflow/core/framework/tracking_allocator_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftracking_allocator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftracking_allocator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftracking_allocator_test.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -26,7 +26,7 @@ namespace tensorflow {\n \n class TestableSizeTrackingAllocator : public Allocator {\n  public:\n-  string Name() override { return \"test\"; }\n+  std::string Name() override { return \"test\"; }\n   void* AllocateRaw(size_t /*alignment*/, size_t num_bytes) override {\n     void* ptr = port::Malloc(num_bytes);\n     size_map_[ptr] = num_bytes;\n@@ -52,7 +52,7 @@ class TestableSizeTrackingAllocator : public Allocator {\n \n class NoMemoryAllocator : public Allocator {\n  public:\n-  string Name() override { return \"test\"; }\n+  std::string Name() override { return \"test\"; }\n   void* AllocateRaw(size_t /*alignment*/, size_t num_bytes) override {\n     return nullptr;\n   }"
        },
        {
            "sha": "22c0d608076af590cf15cf1320982bede214e06d",
            "filename": "tensorflow/core/framework/type_index.h",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftype_index.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftype_index.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftype_index.h?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -50,7 +50,7 @@ class TypeIndex {\n \n   const char* name() const { return name_; }\n \n-  uint64 hash_code() const { return hash_; }\n+  uint64_t hash_code() const { return hash_; }\n \n   // Returns a TypeIndex object that corresponds to a typename.\n   template <typename T>\n@@ -76,17 +76,18 @@ class TypeIndex {\n #endif  // TARGET_OS_OSX\n \n     // No type names available.\n-    return TypeIndex(static_cast<uint64>(reinterpret_cast<intptr_t>(hash_bit)),\n-                     \"[RTTI disabled]\");\n+    return TypeIndex(\n+        static_cast<uint64_t>(reinterpret_cast<intptr_t>(hash_bit)),\n+        \"[RTTI disabled]\");\n #endif  // __GXX_RTTI\n   }\n \n  private:\n   // We hide the constructor of the TypeIndex class. Use the templated\n   // Make<T>() function to create a TypeIndex object.\n-  explicit TypeIndex(const uint64 hash, const char* name)\n+  explicit TypeIndex(const uint64_t hash, const char* name)\n       : hash_(hash), name_(name) {}\n-  uint64 hash_;\n+  uint64_t hash_;\n   const char* name_;\n };\n "
        },
        {
            "sha": "a922ec4dca66b7e232a51c91210f01657d098667",
            "filename": "tensorflow/core/framework/types.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftypes.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftypes.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftypes.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -82,7 +82,7 @@ const std::string DeviceName<Eigen::GpuDevice>::value = DEVICE_GPU;\n #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n \n namespace {\n-string DataTypeStringInternal(DataType dtype) {\n+std::string DataTypeStringInternal(DataType dtype) {\n   switch (dtype) {\n     case DT_INVALID:\n       return \"INVALID\";\n@@ -157,7 +157,7 @@ string DataTypeStringInternal(DataType dtype) {\n }\n }  // end namespace\n \n-string DataTypeString(DataType dtype) {\n+std::string DataTypeString(DataType dtype) {\n   if (IsRefType(dtype)) {\n     DataType non_ref = static_cast<DataType>(dtype - kDataTypeRefOffset);\n     return absl::StrCat(DataTypeStringInternal(non_ref), \"_ref\");\n@@ -277,12 +277,12 @@ bool DataTypeFromString(absl::string_view sp, DataType* dt) {\n   return false;\n }\n \n-string DeviceTypeString(const DeviceType& device_type) {\n+std::string DeviceTypeString(const DeviceType& device_type) {\n   return device_type.type();\n }\n \n-string DataTypeSliceString(const DataTypeSlice types) {\n-  string out;\n+std::string DataTypeSliceString(const DataTypeSlice types) {\n+  std::string out;\n   for (auto it = types.begin(); it != types.end(); ++it) {\n     absl::StrAppend(&out, it == types.begin() ? \"\" : \", \", DataTypeString(*it));\n   }\n@@ -335,17 +335,17 @@ int DataTypeSize(DataType dt) {\n \n DEFINE_DATATYPETOENUM_VALUE(float);\n DEFINE_DATATYPETOENUM_VALUE(double);\n-DEFINE_DATATYPETOENUM_VALUE(int32);\n-DEFINE_DATATYPETOENUM_VALUE(uint32);\n-DEFINE_DATATYPETOENUM_VALUE(uint16);\n-DEFINE_DATATYPETOENUM_VALUE(uint8);\n-DEFINE_DATATYPETOENUM_VALUE(int16);\n-DEFINE_DATATYPETOENUM_VALUE(int8);\n+DEFINE_DATATYPETOENUM_VALUE(int32_t);\n+DEFINE_DATATYPETOENUM_VALUE(uint32_t);\n+DEFINE_DATATYPETOENUM_VALUE(uint16_t);\n+DEFINE_DATATYPETOENUM_VALUE(uint8_t);\n+DEFINE_DATATYPETOENUM_VALUE(int16_t);\n+DEFINE_DATATYPETOENUM_VALUE(int8_t);\n DEFINE_DATATYPETOENUM_VALUE(tstring);\n DEFINE_DATATYPETOENUM_VALUE(complex64);\n DEFINE_DATATYPETOENUM_VALUE(complex128);\n DEFINE_DATATYPETOENUM_VALUE(int64_t);\n-DEFINE_DATATYPETOENUM_VALUE(uint64);\n+DEFINE_DATATYPETOENUM_VALUE(uint64_t);\n DEFINE_DATATYPETOENUM_VALUE(bool);\n DEFINE_DATATYPETOENUM_VALUE(qint8);\n DEFINE_DATATYPETOENUM_VALUE(quint8);"
        },
        {
            "sha": "c47c990d35799e20e9d1f7a5d2fa79fddc66c463",
            "filename": "tensorflow/core/framework/types.h",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftypes.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftypes.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftypes.h?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -82,7 +82,7 @@ typedef absl::InlinedVector<DataType, 4UL> DataTypeVector;\n typedef absl::Span<const DataType> DataTypeSlice;\n \n typedef absl::InlinedVector<DeviceType, 4UL> DeviceTypeVector;\n-typedef absl::InlinedVector<std::pair<DeviceType, int32>, 4UL>\n+typedef absl::InlinedVector<std::pair<DeviceType, int32_t>, 4UL>\n     PrioritizedDeviceTypeVector;\n \n // Convert the enums to strings for errors:\n@@ -98,33 +98,33 @@ inline std::string DataTypeVectorString(const DataTypeVector& dtypes) {\n // cannot represent any of the DT_*_REF values.\n class DataTypeSet {\n  private:\n-  const uint64 mask_;\n+  const uint64_t mask_;\n \n-  static constexpr uint64 kNumBits = 64;\n+  static constexpr uint64_t kNumBits = 64;\n \n  public:\n   constexpr DataTypeSet(const DataTypeSet& other) : mask_(other.mask_) {}\n-  explicit constexpr DataTypeSet(uint64 mask) : mask_(mask) {}\n+  explicit constexpr DataTypeSet(uint64_t mask) : mask_(mask) {}\n \n   constexpr bool Contains(DataType dt) const {\n-    return (static_cast<uint64>(dt) < kNumBits) &&\n-           ((mask_ >> static_cast<uint64>(dt)) & 1ull) != 0ull;\n+    return (static_cast<uint64_t>(dt) < kNumBits) &&\n+           ((mask_ >> static_cast<uint64_t>(dt)) & 1ull) != 0ull;\n   }\n \n   class Iterator {\n     const DataTypeSet& set_;\n-    uint64 pos_;\n+    uint64_t pos_;\n \n    public:\n-    Iterator(const DataTypeSet& set, uint64 pos) : set_(set), pos_(pos) {\n+    Iterator(const DataTypeSet& set, uint64_t pos) : set_(set), pos_(pos) {\n       DCHECK_LE(pos, kNumBits);\n     }\n     DataType operator*() const { return static_cast<DataType>(pos_); }\n     Iterator& operator++() {\n       ++pos_;\n       DCHECK_LE(pos_, kNumBits);\n       if (pos_ < kNumBits) {\n-        uint64 remaining_mask = set_.mask_ >> pos_;\n+        uint64_t remaining_mask = set_.mask_ >> pos_;\n         if (remaining_mask != 0ull) {\n           pos_ += absl::countr_zero(remaining_mask);\n         }\n@@ -171,7 +171,7 @@ class DataTypeSet {\n bool DataTypeFromString(absl::string_view sp, DataType* dt);\n \n constexpr inline DataTypeSet ToSet(DataType dt) {\n-  return DataTypeSet(1ull << static_cast<uint64>(dt));\n+  return DataTypeSet(1ull << static_cast<uint64_t>(dt));\n }\n \n // DT_FLOAT + kDataTypeRefOffset == DT_FLOAT_REF, etc.\n@@ -325,12 +325,12 @@ struct EnumToDataType {};  // Specializations below\n \n MATCH_TYPE_AND_ENUM(float, DT_FLOAT);\n MATCH_TYPE_AND_ENUM(double, DT_DOUBLE);\n-MATCH_TYPE_AND_ENUM(int32, DT_INT32);\n-MATCH_TYPE_AND_ENUM(uint32, DT_UINT32);\n-MATCH_TYPE_AND_ENUM(uint16, DT_UINT16);\n-MATCH_TYPE_AND_ENUM(uint8, DT_UINT8);\n-MATCH_TYPE_AND_ENUM(int16, DT_INT16);\n-MATCH_TYPE_AND_ENUM(int8, DT_INT8);\n+MATCH_TYPE_AND_ENUM(int32_t, DT_INT32);\n+MATCH_TYPE_AND_ENUM(uint32_t, DT_UINT32);\n+MATCH_TYPE_AND_ENUM(uint16_t, DT_UINT16);\n+MATCH_TYPE_AND_ENUM(uint8_t, DT_UINT8);\n+MATCH_TYPE_AND_ENUM(int16_t, DT_INT16);\n+MATCH_TYPE_AND_ENUM(int8_t, DT_INT8);\n MATCH_TYPE_AND_ENUM(tstring, DT_STRING);\n MATCH_TYPE_AND_ENUM(complex64, DT_COMPLEX64);\n MATCH_TYPE_AND_ENUM(complex128, DT_COMPLEX128);\n@@ -382,7 +382,7 @@ struct IsValidDataType<unsigned long> {\n };\n template <>\n struct EnumToDataType<DT_UINT64> {\n-  typedef tensorflow::uint64 Type;\n+  typedef uint64_t Type;\n };\n \n template <>\n@@ -417,7 +417,7 @@ struct IsValidDataType {\n \n // Extra validity checking; not part of public API.\n static_assert(IsValidDataType<int64_t>::value, \"Incorrect impl for int64\");\n-static_assert(IsValidDataType<int32>::value, \"Incorrect impl for int32\");\n+static_assert(IsValidDataType<int32_t>::value, \"Incorrect impl for int32\");\n \n // TODO(jeff): Maybe unify this with Tensor::CanUseDMA, or the underlying\n // is_simple<T> in tensor.cc (and possible choose a more general name?)"
        },
        {
            "sha": "c4e8d49d155c179d6c800023f6921cb289b4cc9c",
            "filename": "tensorflow/core/framework/types_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftypes_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Ftypes_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Ftypes_test.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -138,10 +138,10 @@ TEST(TypesTest, QuantizedTypes) {\n   EXPECT_TRUE(GetQuantized<quint8>());\n   EXPECT_TRUE(GetQuantized<qint32>());\n \n-  EXPECT_FALSE(GetQuantized<int8>());\n-  EXPECT_FALSE(GetQuantized<uint8>());\n-  EXPECT_FALSE(GetQuantized<int16>());\n-  EXPECT_FALSE(GetQuantized<int32>());\n+  EXPECT_FALSE(GetQuantized<int8_t>());\n+  EXPECT_FALSE(GetQuantized<uint8_t>());\n+  EXPECT_FALSE(GetQuantized<int16_t>());\n+  EXPECT_FALSE(GetQuantized<int32_t>());\n \n   EXPECT_TRUE(DataTypeIsQuantized(DT_QINT8));\n   EXPECT_TRUE(DataTypeIsQuantized(DT_QUINT8));\n@@ -173,7 +173,7 @@ TEST(TypesTest, ComplexTypes) {\n \n TEST(TypesTest, IntegerTypes) {\n   for (auto dt : AllTypes()) {\n-    const string name = DataTypeString(dt);\n+    const std::string name = DataTypeString(dt);\n     EXPECT_EQ(DataTypeIsInteger(dt),\n               absl::StartsWith(name, \"int\") || absl::StartsWith(name, \"uint\"))\n         << \"DataTypeInteger failed for \" << name;"
        },
        {
            "sha": "8c183c4374e8bfa2caeeeb4f4abf584985434107",
            "filename": "tensorflow/core/framework/variant.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fvariant.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fvariant.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fvariant.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -47,7 +47,7 @@ const void* Variant::get() const {\n }\n \n template <>\n-string TypeNameVariant(const VariantTensorDataProto& value) {\n+std::string TypeNameVariant(const VariantTensorDataProto& value) {\n   return value.type_name();\n }\n \n@@ -64,19 +64,19 @@ bool DecodeVariant(VariantTensorData* data, VariantTensorDataProto* value) {\n }\n \n template <>\n-void EncodeVariant(const VariantTensorDataProto& value, string* buf) {\n+void EncodeVariant(const VariantTensorDataProto& value, std::string* buf) {\n   value.SerializeToString(buf);\n }\n \n template <>\n-bool DecodeVariant(string* buf, VariantTensorDataProto* value) {\n+bool DecodeVariant(std::string* buf, VariantTensorDataProto* value) {\n   return value->ParseFromString(*buf);\n }\n \n void EncodeVariantList(const Variant* variant_array, int64_t n,\n                        std::unique_ptr<port::StringListEncoder> e) {\n   for (int i = 0; i < n; ++i) {\n-    string s;\n+    std::string s;\n     variant_array[i].Encode(&s);\n     e->Append(s);\n   }\n@@ -85,7 +85,7 @@ void EncodeVariantList(const Variant* variant_array, int64_t n,\n \n bool DecodeVariantList(std::unique_ptr<port::StringListDecoder> d,\n                        Variant* variant_array, int64_t n) {\n-  std::vector<uint32> sizes(n);\n+  std::vector<uint32_t> sizes(n);\n   if (!d->ReadSizes(&sizes)) return false;\n \n   for (int i = 0; i < n; ++i) {\n@@ -94,7 +94,7 @@ bool DecodeVariantList(std::unique_ptr<port::StringListDecoder> d,\n     }\n     // TODO(ebrevdo): Replace with StringPiece?  Any way to make this a\n     // zero-copy operation that keeps a reference to the data in d?\n-    string str(d->Data(sizes[i]), sizes[i]);\n+    std::string str(d->Data(sizes[i]), sizes[i]);\n     if (!variant_array[i].Decode(std::move(str))) return false;\n     if (!DecodeUnaryVariant(&variant_array[i])) {\n       LOG(ERROR) << \"Could not decode variant with type_name: \\\"\""
        },
        {
            "sha": "b735be446fb5c529d7d37456e1304829f167b43b",
            "filename": "tensorflow/core/framework/variant_encode_decode.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fvariant_encode_decode.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fvariant_encode_decode.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fvariant_encode_decode.h?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -131,7 +131,7 @@ struct has_type_name : std::false_type {};\n template <typename C>\n struct has_type_name<\n     C, typename std::enable_if<std::is_same<\n-           decltype(std::declval<C>().TypeName()), string>::value>::type>\n+           decltype(std::declval<C>().TypeName()), std::string>::value>::type>\n     : std::true_type {};\n \n template <typename T, bool = has_type_name<typename std::decay<T>::type>::value,\n@@ -179,18 +179,18 @@ struct has_debug_string : std::false_type {};\n \n template <typename C>\n struct has_debug_string<\n-    C, typename std::enable_if<std::is_same<\n-           decltype(std::declval<C>().DebugString()), string>::value>::type>\n+    C,\n+    typename std::enable_if<std::is_same<\n+        decltype(std::declval<C>().DebugString()), std::string>::value>::type>\n     : std::true_type {};\n \n template <typename C, typename = void>\n struct can_strcat : std::false_type {};\n \n template <typename C>\n-struct can_strcat<\n-    C, typename std::enable_if<std::is_same<\n-           decltype(strings::StrCat(std::declval<C>())), string>::value>::type>\n-    : std::true_type {};\n+struct can_strcat<C, typename std::enable_if<std::is_same<\n+                         decltype(strings::StrCat(std::declval<C>())),\n+                         std::string>::value>::type> : std::true_type {};\n \n template <typename T,\n           bool = has_debug_string<typename std::decay<T>::type>::value,"
        },
        {
            "sha": "9c036722b772b461458167b9c8866830e6b95b1b",
            "filename": "tensorflow/core/framework/variant_op_copy_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fvariant_op_copy_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fvariant_op_copy_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fvariant_op_copy_test.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -59,7 +59,7 @@ static int* GetCopyGPUToGPUCounter() {\n \n struct StoredTensorValue {\n   Tensor stored;\n-  string TypeName() const { return \"StoredTensorValue\"; }\n+  std::string TypeName() const { return \"StoredTensorValue\"; }\n   void Encode(VariantTensorData* data) const { data->tensors_ = {stored}; }\n   bool Decode(const VariantTensorData& data) {\n     CHECK_EQ(1, data.tensors_.size());\n@@ -268,7 +268,7 @@ TEST(VariantOpCopyTest, CreateConstOnGPUFailsGracefully) {\n TEST(VariantOpCopyTest, CreateCopyCPUToCPU) {\n   Scope root = Scope::NewRootScope().WithDevice(\"/cpu:0\");\n   Tensor t_42(DT_INT32, TensorShape({}));\n-  t_42.flat<int32>()(0) = 42;\n+  t_42.flat<int32_t>()(0) = 42;\n   Output create_op = CreateTestVariant(root, t_42);\n   Output identity = ops::Identity(root, create_op);\n \n@@ -285,7 +285,7 @@ TEST(VariantOpCopyTest, CreateCopyCPUToCPU) {\n     EXPECT_EQ(\"StoredTensorValue\", r1.TypeName());\n     const StoredTensorValue* v1 = r1.get<StoredTensorValue>();\n     EXPECT_NE(v1, nullptr);\n-    EXPECT_EQ(42, v1->stored.scalar<int32>()());\n+    EXPECT_EQ(42, v1->stored.scalar<int32_t>()());\n   }\n }\n \n@@ -319,7 +319,7 @@ TEST(VariantOpCopyTest, CreateCopyCPUToGPU) {\n   Scope root = Scope::NewRootScope().WithDevice(\"/cpu:0\");\n   Scope with_gpu = root.WithDevice(\"/gpu:0\");\n   Tensor t_42(DT_INT32, TensorShape({}));\n-  t_42.scalar<int32>()() = 42;\n+  t_42.scalar<int32_t>()() = 42;\n   Output create_op = CreateTestVariant(root, t_42);\n   Output identity = ops::Identity(with_gpu, create_op);\n \n@@ -346,7 +346,7 @@ TEST(VariantOpCopyTest, CreateCopyCPUToGPU) {\n     EXPECT_EQ(\"StoredTensorValue\", r1.TypeName());\n     const StoredTensorValue* v1 = r1.get<StoredTensorValue>();\n     EXPECT_NE(v1, nullptr);\n-    EXPECT_EQ(42, v1->stored.scalar<int32>()());\n+    EXPECT_EQ(42, v1->stored.scalar<int32_t>()());\n   }\n }\n "
        },
        {
            "sha": "3019976645f679d8a6932021d2ae28f3958c9020",
            "filename": "tensorflow/core/framework/variant_op_registry.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fvariant_op_registry.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fvariant_op_registry.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fvariant_op_registry.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -46,9 +46,10 @@ const char* VariantBinaryOpToString(VariantBinaryOp op) {\n   }\n }\n \n-std::unordered_set<string>* UnaryVariantOpRegistry::PersistentStringStorage() {\n-  static std::unordered_set<string>* string_storage =\n-      new std::unordered_set<string>();\n+std::unordered_set<std::string>*\n+UnaryVariantOpRegistry::PersistentStringStorage() {\n+  static std::unordered_set<std::string>* string_storage =\n+      new std::unordered_set<std::string>();\n   return string_storage;\n }\n \n@@ -70,7 +71,7 @@ UnaryVariantOpRegistry::VariantDecodeFn* UnaryVariantOpRegistry::GetDecodeFn(\n }\n \n void UnaryVariantOpRegistry::RegisterDecodeFn(\n-    const string& type_name, const VariantDecodeFn& decode_fn) {\n+    const std::string& type_name, const VariantDecodeFn& decode_fn) {\n   CHECK(!type_name.empty()) << \"Need a valid name for UnaryVariantDecode\";\n   VariantDecodeFn* existing = GetDecodeFn(type_name);\n   CHECK_EQ(existing, nullptr)\n@@ -98,7 +99,7 @@ bool DecodeUnaryVariant(Variant* variant) {\n   if (decode_fn == nullptr) {\n     return false;\n   }\n-  const string type_name = variant->TypeName();\n+  const std::string type_name = variant->TypeName();\n   bool decoded = (*decode_fn)(variant);\n   if (!decoded) return false;\n   if (variant->TypeName() != type_name) {"
        },
        {
            "sha": "5c31066f54a4a420b1eba27821538960a8f233ae",
            "filename": "tensorflow/core/framework/variant_op_registry.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fvariant_op_registry.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fvariant_op_registry.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fvariant_op_registry.h?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -188,7 +188,7 @@ class UnaryVariantOpRegistry {\n   // iterators).  In other words, one may safely point a StringPiece to\n   // a value in the set without that StringPiece being invalidated by\n   // future insertions.\n-  static std::unordered_set<string>* PersistentStringStorage();\n+  static std::unordered_set<std::string>* PersistentStringStorage();\n \n  private:\n   struct TypeIndexHash {"
        },
        {
            "sha": "2506bdd433242dc67aa3163f76f8ee8f5f06f315",
            "filename": "tensorflow/core/framework/variant_op_registry_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fvariant_op_registry_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fvariant_op_registry_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fvariant_op_registry_test.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -38,7 +38,7 @@ typedef Eigen::GpuDevice GPUDevice;\n namespace {\n \n struct VariantValue {\n-  string TypeName() const { return \"TEST VariantValue\"; }\n+  std::string TypeName() const { return \"TEST VariantValue\"; }\n   static absl::Status CPUZerosLikeFn(OpKernelContext* ctx,\n                                      const VariantValue& v,\n                                      VariantValue* v_out) {\n@@ -147,7 +147,7 @@ TEST(VariantOpDecodeRegistryTest, TestEmpty) {\n TEST(VariantOpDecodeRegistryTest, TestDuplicate) {\n   UnaryVariantOpRegistry registry;\n   UnaryVariantOpRegistry::VariantDecodeFn f;\n-  string kTypeName = \"fjfjfj\";\n+  std::string kTypeName = \"fjfjfj\";\n   registry.RegisterDecodeFn(kTypeName, f);\n   EXPECT_DEATH(registry.RegisterDecodeFn(kTypeName, f),\n                \"fjfjfj already registered\");"
        },
        {
            "sha": "906cfaa3d8e58a2389862ebd646f054a6e64292e",
            "filename": "tensorflow/core/framework/variant_tensor_data.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fvariant_tensor_data.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fvariant_tensor_data.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fvariant_tensor_data.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -71,27 +71,27 @@ bool VariantTensorData::FromConstProto(const VariantTensorDataProto& proto) {\n   return true;\n }\n \n-string VariantTensorData::SerializeAsString() const {\n+std::string VariantTensorData::SerializeAsString() const {\n   VariantTensorDataProto proto;\n   ToProto(&proto);\n   return proto.SerializeAsString();\n }\n \n-bool VariantTensorData::SerializeToString(string* buf) {\n+bool VariantTensorData::SerializeToString(std::string* buf) {\n   VariantTensorDataProto proto;\n   ToProto(&proto);\n   return proto.SerializeToString(buf);\n }\n \n-bool VariantTensorData::ParseFromString(string s) {\n+bool VariantTensorData::ParseFromString(std::string s) {\n   VariantTensorDataProto proto;\n   const bool status = proto.ParseFromString(s);\n   if (status) FromProto(std::move(proto));\n   return status;\n }\n \n-string VariantTensorData::DebugString() const {\n-  string repeated_field = \"\";\n+std::string VariantTensorData::DebugString() const {\n+  std::string repeated_field = \"\";\n   for (const auto& t : tensors_) {\n     repeated_field =\n         absl::StrCat(repeated_field, \" tensors: \", t.DebugString());\n@@ -100,7 +100,7 @@ string VariantTensorData::DebugString() const {\n                          repeated_field);\n }\n \n-string ProtoDebugString(const VariantTensorData& object) {\n+std::string ProtoDebugString(const VariantTensorData& object) {\n   return object.DebugString();\n }\n "
        },
        {
            "sha": "bf99cd721ad6dec9b60607ed5083c752cd4dcfae",
            "filename": "tensorflow/core/framework/variant_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fvariant_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/12159d89da3a9657c17fb0976da34d8a133870ea/tensorflow%2Fcore%2Fframework%2Fvariant_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fframework%2Fvariant_test.cc?ref=12159d89da3a9657c17fb0976da34d8a133870ea",
            "patch": "@@ -38,7 +38,7 @@ template <typename T, bool BIG>\n struct Wrapper {\n   T value;\n   char big[BIG ? 256 : 1];\n-  string TypeName() const { return \"POD\"; }\n+  std::string TypeName() const { return \"POD\"; }\n };\n \n template <bool BIG>\n@@ -87,7 +87,7 @@ class MaybeAlive {\n \n   static int LiveCounter() { return live_counter_; }\n \n-  string TypeName() const { return \"MaybeAlive\"; }\n+  std::string TypeName() const { return \"MaybeAlive\"; }\n   void Encode(VariantTensorData* data) const {}\n   bool Decode(VariantTensorData data) { return false; }\n \n@@ -127,7 +127,7 @@ class DeleteCounter {\n   char big_[BIG ? 256 : 1];\n   int* counter_;\n \n-  string TypeName() const { return \"DeleteCounter\"; }\n+  std::string TypeName() const { return \"DeleteCounter\"; }\n   void Encode(VariantTensorData* data) const {}\n   bool Decode(VariantTensorData data) { return false; }\n };\n@@ -248,7 +248,7 @@ class MoveAndCopyCounter {\n   int* move_counter_;\n   int* copy_counter_;\n \n-  string TypeName() const { return \"MoveAndCopyCounter\"; }\n+  std::string TypeName() const { return \"MoveAndCopyCounter\"; }\n   void Encode(VariantTensorData* data) const {}\n   bool Decode(VariantTensorData data) { return false; }\n };\n@@ -538,7 +538,7 @@ struct TensorList {\n     return true;\n   }\n \n-  string TypeName() const { return \"TensorList\"; }\n+  std::string TypeName() const { return \"TensorList\"; }\n \n   std::vector<Tensor> vec;\n };\n@@ -616,7 +616,7 @@ void PodUpdateTest() {\n     float y;\n     char big[BIG ? 256 : 1];\n \n-    string TypeName() const { return \"POD\"; }\n+    std::string TypeName() const { return \"POD\"; }\n   };\n \n   Variant x = Pod{10, 20.f};\n@@ -639,7 +639,7 @@ void TestEncodeDecodePod() {\n     float y;\n     char big[BIG ? 256 : 1];\n \n-    string TypeName() const { return \"POD\"; }\n+    std::string TypeName() const { return \"POD\"; }\n   };\n \n   Variant x;"
        }
    ],
    "stats": {
        "total": 706,
        "additions": 362,
        "deletions": 344
    }
}