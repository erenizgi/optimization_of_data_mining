{
    "author": "loislo",
    "message": "[XLA:GPU] Initial version of Generic Emitter for ScaledDot hlo op.\n\nIt supports only scale factor == 32.\n\nPiperOrigin-RevId: 803488454",
    "sha": "f663012495a853099bbb720ff2ed529e8568e299",
    "files": [
        {
            "sha": "3c84afec5bb04e3fc3a9641b0c5b5ae4c657f2c0",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=f663012495a853099bbb720ff2ed529e8568e299",
            "patch": "@@ -371,6 +371,7 @@ cc_library(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/utils:hlo_traversal\",\n         \"//xla/service:algorithm_util\",\n+        \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -571,6 +572,7 @@ xla_test(\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest\","
        },
        {
            "sha": "e210a97d6af9dd49e5932a7b5e7131965537a2b4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.cc?ref=f663012495a853099bbb720ff2ed529e8568e299",
            "patch": "@@ -31,6 +31,7 @@ limitations under the License.\n #include \"mlir/Dialect/Math/IR/Math.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/Support/LLVM.h\"\n@@ -42,6 +43,7 @@ limitations under the License.\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/service/algorithm_util.h\"\n+#include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -128,6 +130,38 @@ std::vector<Value> SplitF32(EmitterLocOpBuilder b, Value input,\n   return split_inputs;\n }\n \n+absl::StatusOr<ttir::ScaleDotElemType> GetScaleDotElemType(Type value) {\n+  auto type = getElementTypeOrSelf(value);\n+  if (type == mlir::Float8E4M3FNType::get(value.getContext())) {\n+    return ttir::ScaleDotElemType::E4M3;\n+  }\n+  if (type == mlir::Float8E5M2Type::get(value.getContext())) {\n+    return ttir::ScaleDotElemType::E5M2;\n+  }\n+  if (type == mlir::Float4E2M1FNType::get(value.getContext())) {\n+    return ttir::ScaleDotElemType::E2M1;\n+  }\n+  return absl::InvalidArgumentError(\n+      absl::StrCat(\"Unsupported type: \", llvm_ir::DumpToString(type)));\n+}\n+\n+absl::StatusOr<Value> ScaledDot(EmitterLocOpBuilder b,\n+                                ScaledDotOperands& operands) {\n+  TF_ASSIGN_OR_RETURN(auto lhs_dot_elem_type,\n+                      GetScaleDotElemType(operands.lhs.getType()));\n+  TF_ASSIGN_OR_RETURN(auto rhs_dot_elem_type,\n+                      GetScaleDotElemType(operands.rhs.getType()));\n+\n+  auto lhs_scale = Bitcast(b, operands.lhs_scale, b.getI8Type());\n+  auto rhs_scale = Bitcast(b, operands.rhs_scale, b.getI8Type());\n+\n+  // make type with the same shape as the scale but with i8 type\n+  return b.create<ttir::DotScaledOp>(\n+      operands.accumulator.getType(), operands.lhs, operands.rhs,\n+      operands.accumulator, lhs_scale, rhs_scale, lhs_dot_elem_type,\n+      rhs_dot_elem_type, true);\n+}\n+\n Value IEEEDot(EmitterLocOpBuilder b, Value lhs, Value rhs, Value acc) {\n   return b.create<ttir::DotOp>(lhs, rhs, acc,\n                                /*inputPrecision=*/ttir::InputPrecision::IEEE,\n@@ -488,6 +522,12 @@ absl::StatusOr<Value> EmitSingleTileDot(EmitterLocOpBuilder b,\n   return result;\n }\n \n+absl::StatusOr<Value> EmitSingleTileScaledDot(\n+    EmitterLocOpBuilder b, const HloScaledDotInstruction& scaled_dot,\n+    ScaledDotOperands dot_operands) {\n+  return ScaledDot(b, dot_operands);\n+}\n+\n }  // namespace triton\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "211fe354c924ac774594d275c48eb419ded1cf06",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/dot_algorithms.h",
            "status": "modified",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fdot_algorithms.h?ref=f663012495a853099bbb720ff2ed529e8568e299",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include \"mlir/IR/Value.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n \n namespace xla {\n namespace gpu {\n@@ -34,6 +35,16 @@ struct DotOperands {\n   ::mlir::Value accumulator;\n };\n \n+// Carries named `Value`s corresponding to `scaled-dot` operands. This includes\n+// an accumulator and their respective scaling factors.\n+struct ScaledDotOperands {\n+  ::mlir::Value lhs;\n+  ::mlir::Value lhs_scale;\n+  ::mlir::Value rhs;\n+  ::mlir::Value rhs_scale;\n+  ::mlir::Value accumulator;\n+};\n+\n // Returns the type to use for accumulation for the given `dot` instruction.\n // This also handles the case where the algorithm is `ALG_UNSET`.\n absl::StatusOr<::mlir::Type> GetDotAccumulatorType(\n@@ -46,6 +57,19 @@ absl::StatusOr<::mlir::Value> EmitSingleTileDot(EmitterLocOpBuilder b,\n                                                 const HloDotInstruction& dot,\n                                                 DotOperands dot_operands);\n \n+// Emits a single-tile scaled-dot, considering the given `scaled-dot`\n+// instruction's operand precisions. Raises an `InvalidArgumentError` if the\n+// operand types are not supported.\n+absl::StatusOr<::mlir::Value> EmitSingleTileScaledDot(\n+    EmitterLocOpBuilder b, const HloScaledDotInstruction& scaled_dot,\n+    ScaledDotOperands dot_operands);\n+\n+namespace internal {\n+absl::StatusOr<mlir::triton::ScaleDotElemType> GetScaleDotElemType(\n+    mlir::Type value);\n+\n+}  // namespace internal\n+\n }  // namespace triton\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "6cf6c1d36649fb3c308a2496c2d94be966fb7e75",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=f663012495a853099bbb720ff2ed529e8568e299",
            "patch": "@@ -106,6 +106,10 @@ absl::StatusOr<Type> TritonType(EmitterLocOpBuilder& b, PrimitiveType t) {\n       return b.getType<mlir::Float8E5M2Type>();\n     case F8E4M3FN:\n       return b.getType<mlir::Float8E4M3FNType>();\n+    case F8E8M0FNU:\n+      return b.getType<mlir::Float8E8M0FNUType>();\n+    case F4E2M1FN:\n+      return b.getType<mlir::Float4E2M1FNType>();\n     default:\n       return absl::UnimplementedError(\n           absl::StrCat(\"This type is not supported yet: \",\n@@ -114,6 +118,7 @@ absl::StatusOr<Type> TritonType(EmitterLocOpBuilder& b, PrimitiveType t) {\n }\n \n absl::StatusOr<PrimitiveType> GetPrimitiveType(Type t) {\n+  // NOLINTBEGIN(google-readability-braces-around-statements)\n   if (t.isF64()) return F64;\n   if (t.isF32()) return F32;\n   if (t.isF16()) return F16;\n@@ -126,6 +131,8 @@ absl::StatusOr<PrimitiveType> GetPrimitiveType(Type t) {\n   if (t.isInteger(1)) return PRED;\n   if (mlir::isa<mlir::Float8E5M2Type>(t)) return F8E5M2;\n   if (mlir::isa<mlir::Float8E4M3FNType>(t)) return F8E4M3FN;\n+  if (mlir::isa<mlir::Float8E8M0FNUType>(t)) return F8E8M0FNU;\n+  // NOLINTEND(google-readability-braces-around-statements)\n   return absl::UnimplementedError(\"Unsupported type in getPrimitiveType.\\n\");\n }\n \n@@ -502,4 +509,10 @@ absl::StatusOr<ScalarOrTensor> EmitConstant(EmitterLocOpBuilder& b,\n   return CreateConst(b, ty, ScalarConstantValue<double>(constant, F64), shape);\n }\n \n+Value Bitcast(EmitterLocOpBuilder& b, Value value, Type type) {\n+  auto value_type = value.getType();\n+  value_type = mlir::dyn_cast<ShapedType>(value_type).clone(type);\n+  return b.create<mlir::arith::BitcastOp>(value_type, value);\n+}\n+\n }  // namespace xla::gpu::triton"
        },
        {
            "sha": "45ca2c2acdbb77a45d5969c654bdf1513b441f19",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h?ref=f663012495a853099bbb720ff2ed529e8568e299",
            "patch": "@@ -211,6 +211,8 @@ absl::StatusOr<mlir::Value> EmitElementwise(\n     const se::DeviceDescription& device_info, const HloInstruction& hlo,\n     mlir::ValueRange inputs);\n \n+mlir::Value Bitcast(EmitterLocOpBuilder& b, mlir::Value value, mlir::Type type);\n+\n }  // namespace xla::gpu::triton\n \n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_EMITTER_HELPERS_H_"
        },
        {
            "sha": "024b8e7fefa05b9f7b18af7be710aee75d9d930e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=f663012495a853099bbb720ff2ed529e8568e299",
            "patch": "@@ -103,7 +103,8 @@ TritonFusion::GenerateTritonKernelAndWrapper(\n   TritonWrapperResult triton_wrapper_result;\n \n   if (fusion_kind == kTritonFusionKind ||\n-      fusion_kind == kTritonNestedGemmFusionKind) {\n+      fusion_kind == kTritonNestedGemmFusionKind ||\n+      fusion_kind == kTritonScaledDotFusionKind) {\n     std::optional<LaunchConfig> launch_config = this->launch_config();\n     if (!launch_config.has_value()) {\n       return absl::InvalidArgumentError(absl::StrCat(\n@@ -175,7 +176,8 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n \n     LaunchDimensions launch_dimensions;\n     if (fusion_kind == kTritonFusionKind ||\n-        fusion_kind == kTritonNestedGemmFusionKind) {\n+        fusion_kind == kTritonNestedGemmFusionKind ||\n+        fusion_kind == kTritonScaledDotFusionKind) {\n       std::optional<LaunchConfig> launch_config = this->launch_config();\n       // This check should be enforced by `GenerateTritonKernelWrapper`.\n       CHECK(launch_config.has_value());"
        },
        {
            "sha": "aaab980ec45285e5f54ca13e5b81cace5efa2cdf",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 172,
            "deletions": 13,
            "changes": 185,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=f663012495a853099bbb720ff2ed529e8568e299",
            "patch": "@@ -172,6 +172,10 @@ using ::xla::gpu::triton::TritonType;\n \n namespace {\n \n+Value MakeIndex(EmitterLocOpBuilder& b, int64_t value) {\n+  return b.create<arith::ConstantIndexOp>(value);\n+}\n+\n // Emit a value as Index clamped to [lower, upper].\n Value EmitClampedIndex(EmitterLocOpBuilder b, Value value, int64_t lower,\n                        int64_t upper) {\n@@ -757,8 +761,7 @@ absl::StatusOr<int64_t> GetDotLoopIterationCount(\n   // - size from the shape of the operand\n   // - tile size from the tiling of the nested fusion root\n   // using the contracting dimension from the dot instruction.\n-  const HloDotInstruction& dot =\n-      *::xla::Cast<HloDotInstruction>(tiled_dot.hlo());\n+  const HloInstruction& dot = *tiled_dot.hlo();\n   const auto& dims = dot.dot_dimension_numbers();\n   if (dims.lhs_contracting_dimensions_size() != 1) {\n     return absl::UnimplementedError(\n@@ -866,11 +869,11 @@ absl::StatusOr<Value> MaskDotOperand(EmitterLocOpBuilder b,\n // Returns `shape` without all its unit dimensions, as well as the index of the\n // remaining dimensions in the original `shape`.\n std::pair<SmallVector<int64_t>, SmallVector<int64_t>> CollapseUnitDims(\n-    llvm::ArrayRef<int64_t> shape) {\n+    llvm::ArrayRef<int64_t> shape, bool scaled_dot = false) {\n   SmallVector<int64_t> shape_without_unit_dims;\n   SmallVector<int64_t> non_unit_dims_indices;\n   for (auto [i, size] : llvm::enumerate(shape)) {\n-    if (size != 1) {\n+    if (size != 1 || scaled_dot) {\n       shape_without_unit_dims.push_back(size);\n       non_unit_dims_indices.push_back(i);\n     }\n@@ -889,11 +892,12 @@ enum class DotOperandSide { kLhs, kRhs };\n absl::StatusOr<Value> CanonicalizeDotOperand(EmitterLocOpBuilder b,\n                                              Value operand,\n                                              int64_t contracting_dim_idx,\n-                                             DotOperandSide side) {\n+                                             DotOperandSide side,\n+                                             bool scaled_dot = false) {\n   llvm::ArrayRef<int64_t> shape =\n       mlir::cast<ShapedType>(operand.getType()).getShape();\n   auto [shape_without_unit_dims, non_unit_dims_indices] =\n-      CollapseUnitDims(shape);\n+      CollapseUnitDims(shape, scaled_dot);\n \n   if (shape_without_unit_dims.size() != 2) {\n     return absl::FailedPreconditionError(\n@@ -992,14 +996,12 @@ absl::StatusOr<ScalarOrTensor> EmitDot(\n       CreateConst(b, accumulator_type, 0.0f, padded_tile_sizes_no_unit_dims)\n           .UnwrapTensor();\n \n-  auto cindex = [&](int64_t value) -> Value {\n-    return b.create<arith::ConstantIndexOp>(value);\n-  };\n   TF_ASSIGN_OR_RETURN(int64_t loop_iteration_count,\n                       GetDotLoopIterationCount(tiled_hlo_dot));\n   auto for_op = b.create<mlir::scf::ForOp>(\n-      /*lowerBound=*/cindex(0), /*upperBound=*/cindex(loop_iteration_count),\n-      /*step=*/cindex(1), SmallVector<Value>{accumulator});\n+      /*lowerBound=*/MakeIndex(b, 0),\n+      /*upperBound=*/MakeIndex(b, loop_iteration_count),\n+      /*step=*/MakeIndex(b, 1), ValueRange{accumulator});\n   {  // Loop body.\n     mlir::OpBuilder::InsertionGuard g(b);\n     b.setInsertionPointToStart(for_op.getBody());\n@@ -1089,6 +1091,150 @@ absl::StatusOr<ScalarOrTensor> EmitDot(\n   return ScalarOrTensor(result);\n }\n \n+absl::StatusOr<ScalarOrTensor> EmitScaledDot(\n+    EmitterLocOpBuilder b, absl::string_view libdevice_path,\n+    const se::DeviceDescription& device_info,\n+    const HloFusionInstruction* fusion,\n+    const TiledHloInstruction& tiled_hlo_dot, mlir::FunctionOpInterface fn,\n+    Value pid,\n+    absl::flat_hash_map<const TiledHloInstruction*, ScalarOrTensor>& values) {\n+  VLOG(2) << \"EmitScaledDot: \" << tiled_hlo_dot.ToString();\n+  const HloScaledDotInstruction& scaled_dot =\n+      *::xla::Cast<HloScaledDotInstruction>(tiled_hlo_dot.hlo());\n+  if (!absl::c_all_of(tiled_hlo_dot.operands(),\n+                      [](const TiledHloInstruction* operand) {\n+                        return operand->hlo()->opcode() == HloOpcode::kFusion;\n+                      })) {\n+    return absl::FailedPreconditionError(\"Expected dot operands to be fusions\");\n+  }\n+\n+  SmallVector<int64_t> padded_tile_sizes =\n+      GetPaddedTileSizes(tiled_hlo_dot.tile_sizes());\n+\n+  // Sanity check: Triton historically did not support non-2D dots (and still\n+  // doesn't support arbitrary nD dots), so we require that the dot is tiled\n+  // with exactly two non-unit tile sizes. This anyway matches the hardware's\n+  // expectations, so seems like a reasonable requirement.\n+  // TODO(b/393299275): this needs to be enforced in tiling.\n+  if (padded_tile_sizes.size() != 2) {\n+    return absl::FailedPreconditionError(\n+        \"Expected dot to be tiled with exactly two non-unit tile sizes\");\n+  }\n+\n+  Type accumulator_type = b.getF32Type();\n+  Value accumulator =\n+      CreateConst(b, accumulator_type, 0.0f, padded_tile_sizes).UnwrapTensor();\n+\n+  TF_ASSIGN_OR_RETURN(int64_t loop_iteration_count,\n+                      GetDotLoopIterationCount(tiled_hlo_dot));\n+  auto for_op = b.create<mlir::scf::ForOp>(\n+      /*lowerBound=*/MakeIndex(b, 0),\n+      /*upperBound=*/MakeIndex(b, loop_iteration_count),\n+      /*step=*/MakeIndex(b, 1), SmallVector<Value>{accumulator});\n+  {  // Loop body.\n+    mlir::OpBuilder::InsertionGuard g(b);\n+    b.setInsertionPointToStart(for_op.getBody());\n+    SmallVector<TensorValue> dot_args;\n+    Value ki = for_op.getInductionVar();\n+    // Nested fusions are tiled with indexing map\n+    // (pid * loop_iteration_count_value + loop index) -> ....\n+    auto pid_dim = b.getAffineDimExpr(0);\n+    auto ki_symbol = b.getAffineSymbolExpr(0);\n+    IndexingMap computation_index_map{\n+        AffineMap::get(1, 1, {pid_dim * loop_iteration_count + ki_symbol}),\n+        {IndexingMap::Variable{\n+            tiled_hlo_dot.tile_offsets_indexing()->GetDimensionBound(0),\n+            \"pid\"}},\n+        {IndexingMap::Variable{{0, loop_iteration_count - 1}, \"k\"}},\n+        /*rt_vars=*/{}};\n+\n+    Value computation_index = b.create<xla::ApplyIndexingOp>(\n+                                   ValueRange{pid, ki}, computation_index_map)\n+                                  .getResult(0);\n+    for (const TiledHloInstruction* operand : tiled_hlo_dot.operands()) {\n+      VLOG(3) << \"Emitting scaled dot operand: \" << operand->ToString();\n+      const TiledHloFusionInstruction* tiled_fusion_operand =\n+          static_cast<const TiledHloFusionInstruction*>(operand);\n+      TF_ASSIGN_OR_RETURN(\n+          std::vector<ScalarOrTensor> result,\n+          EmitTiledComputation(\n+              b, libdevice_path, device_info,\n+              ::xla::Cast<HloFusionInstruction>(tiled_fusion_operand->hlo()),\n+              *tiled_fusion_operand->called_computation(), fn,\n+              computation_index, values));\n+      if (result.size() != 1) {\n+        return absl::InternalError(absl::StrCat(\n+            \"Expected nested fusion computation to emit a single value, got \",\n+            result.size()));\n+      }\n+      dot_args.push_back(result.front().UnwrapTensor());\n+    }\n+    Value acc = for_op.getRegionIterArgs().front();\n+    int64_t lhs_contracting_dim_idx =\n+        scaled_dot.dot_dimension_numbers().lhs_contracting_dimensions(0);\n+\n+    int64_t rhs_contracting_dim_idx =\n+        scaled_dot.dot_dimension_numbers().rhs_contracting_dimensions(0);\n+\n+    // TODO(b/393299275): masking is only necessary during the last iteration of\n+    // the loop. We should evaluate whether adding a conditional mask helps or\n+    // hinders performance for Triton.\n+    Value ki_i32 = Cast(b, ki, b.getI32Type());\n+    TF_ASSIGN_OR_RETURN(\n+        Value lhs, MaskDotOperand(b, *tiled_hlo_dot.operand(0), dot_args[0],\n+                                  ki_i32, lhs_contracting_dim_idx));\n+    TF_ASSIGN_OR_RETURN(\n+        Value lhs_scale,\n+        MaskDotOperand(b, *tiled_hlo_dot.operand(1), dot_args[1], ki_i32,\n+                       lhs_contracting_dim_idx));\n+\n+    TF_ASSIGN_OR_RETURN(\n+        Value rhs, MaskDotOperand(b, *tiled_hlo_dot.operand(2), dot_args[2],\n+                                  ki_i32, rhs_contracting_dim_idx));\n+\n+    TF_ASSIGN_OR_RETURN(\n+        Value rhs_scale,\n+        MaskDotOperand(b, *tiled_hlo_dot.operand(3), dot_args[3], ki_i32,\n+                       rhs_contracting_dim_idx));\n+\n+    // Canonicalize the dot operands to match Triton's/the hardware's\n+    // expectations.\n+    TF_ASSIGN_OR_RETURN(\n+        lhs, CanonicalizeDotOperand(b, lhs, lhs_contracting_dim_idx,\n+                                    DotOperandSide::kLhs, /*scaled_dot=*/true));\n+    TF_ASSIGN_OR_RETURN(\n+        lhs_scale,\n+        CanonicalizeDotOperand(b, lhs_scale, lhs_contracting_dim_idx,\n+                               DotOperandSide::kLhs, /*scaled_dot=*/true));\n+    TF_ASSIGN_OR_RETURN(\n+        rhs, CanonicalizeDotOperand(b, rhs, rhs_contracting_dim_idx,\n+                                    DotOperandSide::kRhs, /*scaled_dot=*/true));\n+    TF_ASSIGN_OR_RETURN(\n+        rhs_scale,\n+        CanonicalizeDotOperand(b, rhs_scale, rhs_contracting_dim_idx,\n+                               DotOperandSide::kRhs, /*scaled_dot=*/true));\n+\n+    TF_ASSIGN_OR_RETURN(\n+        Value acc_next,\n+        triton::EmitSingleTileScaledDot(\n+            b, scaled_dot,\n+            triton::ScaledDotOperands{lhs, lhs_scale, rhs, rhs_scale, acc}));\n+    b.create<mlir::scf::YieldOp>(acc_next);\n+  }\n+\n+  // The output of the loop may not match the expected output type of the dot.\n+  // We make sure to issue a conversion if necessary.\n+  TF_ASSIGN_OR_RETURN(Type dot_output_type,\n+                      TritonType(b, scaled_dot.shape().element_type()));\n+\n+  Value result = for_op.getResult(0);\n+  if (dot_output_type != accumulator_type) {\n+    result = Cast(b, result, dot_output_type);\n+  }\n+\n+  return ScalarOrTensor(result);\n+}\n+\n absl::StatusOr<ScalarOrTensor> EmitConcatenate(\n     EmitterLocOpBuilder b, absl::string_view libdevice_path,\n     const se::DeviceDescription& device_info,\n@@ -1328,6 +1474,11 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n                    values);\n   }\n \n+  if (hlo->opcode() == HloOpcode::kScaledDot) {\n+    return EmitScaledDot(b, libdevice_path, device_info, fusion, tiled_hlo, fn,\n+                         pid, values);\n+  }\n+\n   if (hlo->opcode() == HloOpcode::kConstant) {\n     if (ShapeUtil::IsEffectiveScalar(hlo->shape())) {\n       return EmitConstant(b, *hlo);\n@@ -1412,6 +1563,12 @@ absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n     // Skip generating nested fusions, they are emitted by their consumer.\n     if (hlo->parent()->IsFusionComputation() &&\n         hlo->opcode() == HloOpcode::kFusion) {\n+      if (hlo->GetModule()\n+              ->config()\n+              .debug_options()\n+              .xla_gpu_experimental_scaled_dot_with_triton()) {\n+        continue;\n+      }\n       CodegenDecision decision = IsTritonSupportedInstruction(\n           *hlo, device_info.gpu_compute_capability());\n       if (!decision.CanFuse()) {\n@@ -1515,7 +1672,8 @@ absl::StatusOr<Tiling> TilingFromAnnotatedFusion(\n   for (const auto& [hlo, num_tiling_parameters] :\n        symbolic_tile_analysis.GetTilingSpecification().parameter_mapping()) {\n     // TODO(b/419026602): handle reductions.\n-    if (hlo->opcode() == HloOpcode::kDot) {\n+    if (hlo->opcode() == HloOpcode::kDot ||\n+        hlo->opcode() == HloOpcode::kScaledDot) {\n       const HloInstruction* lhs = hlo->operand(0);\n       // When encountering a `dot`, we always expect its operands to be nests.\n       auto backend_config = lhs->backend_config<GpuBackendConfig>();\n@@ -1858,7 +2016,8 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n     TF_RETURN_IF_ERROR(EmitMatMul(b, libdevice_path, device_info, fusion, fn,\n                                   block_level_parameters));\n   } else if (fusion_kind == kTritonFusionKind ||\n-             fusion_kind == kTritonNestedGemmFusionKind) {\n+             fusion_kind == kTritonNestedGemmFusionKind ||\n+             fusion_kind == kTritonScaledDotFusionKind) {\n     TF_RETURN_IF_ERROR(EmitGeneric(b, libdevice_path, device_info, fusion, fn,\n                                    block_level_parameters));\n   } else {"
        },
        {
            "sha": "c3a13687f5644e16b85e08abcdc7bec1a1f37e37",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_port_test.cc",
            "status": "modified",
            "additions": 189,
            "deletions": 1,
            "changes": 190,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc?ref=f663012495a853099bbb720ff2ed529e8568e299",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include <cstdlib>\n #include <memory>\n+#include <ostream>\n #include <string>\n #include <utility>\n #include <variant>\n@@ -23,8 +24,10 @@ limitations under the License.\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_replace.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/strings/substitute.h\"\n #include \"llvm/IR/LLVMContext.h\"\n@@ -3229,7 +3232,192 @@ ENTRY e {\n ; CHECK-SAME: __triton_nested_gemm_fusion\n   )\");\n }\n-\n }  // namespace\n+\n+struct ScaleDotTestParams {\n+  std::string lhs_type;\n+  std::string lhs_scale_type;\n+  std::string rhs_type;\n+  std::string rhs_scale_type;\n+  std::string output_type;\n+  std::string expected_triton_type;\n+\n+  std::string PrepareHloText(absl::string_view hlo_template) const {\n+    return absl::StrReplaceAll(hlo_template,\n+                               {{\"$lhs_type\", lhs_type},\n+                                {\"$lhs_scale_type\", lhs_scale_type},\n+                                {\"$rhs_type\", rhs_type},\n+                                {\"$rhs_scale_type\", rhs_scale_type},\n+                                {\"$output_type\", output_type}});\n+  }\n+  static std::string ToString(\n+      const ::testing::TestParamInfo<ScaleDotTestParams>& info) {\n+    const ScaleDotTestParams& params = info.param;\n+    auto name = absl::StrCat(params.lhs_type, \"_\", params.lhs_scale_type, \"_\",\n+                             params.rhs_type, \"_\", params.rhs_scale_type, \"_\",\n+                             params.output_type);\n+    absl::StrReplaceAll({{\"[\", \"_\"}, {\"]\", \"_\"}, {\",\", \"x\"}}, &name);\n+    return name;\n+  }\n+};\n+\n+std::ostream& operator<<(std::ostream& stream, const ScaleDotTestParams& tc) {\n+  return stream << \"{\\n\\tlhs_type:\" << tc.lhs_type\n+                << \",\\n\\tlhs_scale_type:\" << tc.lhs_scale_type\n+                << \",\\n\\trhs_type:\" << tc.rhs_type\n+                << \",\\n\\trhs_scale_type:\" << tc.rhs_scale_type\n+                << \",\\n\\toutput_type:\" << tc.output_type << \"\\n}\";\n+}\n+\n+class TritonScaledDotGemmTest\n+    : public TritonGemmTest,\n+      public ::testing::WithParamInterface<ScaleDotTestParams> {};\n+\n+TEST_P(TritonScaledDotGemmTest, Fp8ScaledDotDoesNotCrash) {\n+  const ScaleDotTestParams& params = GetParam();\n+  constexpr absl::string_view kHloTextTemplate = R\"hlo(\n+HloModule m\n+flhs (p0: $lhs_type) -> $lhs_type {\n+  ROOT p0 = $lhs_type{1,0} parameter(0)\n+}\n+flhs_scale (p0: $lhs_scale_type) -> $lhs_scale_type {\n+  ROOT p0 = $lhs_scale_type{1,0} parameter(0)\n+}\n+frhs (p0: $rhs_type) -> $rhs_type {\n+  ROOT p0 = $rhs_type{1,0} parameter(0)\n+}\n+frhs_scale (p0: $rhs_scale_type) -> $rhs_scale_type {\n+  ROOT p0 = $rhs_scale_type{1,0} parameter(0)\n+}\n+\n+triton_dot {\n+  lhs = $lhs_type parameter(0)\n+  lhs1 = $lhs_type{1,0} fusion(lhs),\n+    kind=kCustom,\n+    calls=flhs,\n+    backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"16\",\"32\"]}],\n+          \"num_warps\":\"1\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\",\n+        }\n+      }\n+    }\n+  lhs_scale = $lhs_scale_type parameter(1)\n+  lhs_scale1 = $lhs_scale_type{1,0} fusion(lhs_scale),\n+    kind=kCustom,\n+    calls=flhs_scale,\n+    backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"16\",\"1\"]}],\n+          \"num_warps\":\"1\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\",\n+        }\n+      }\n+    }\n+  rhs = $rhs_type parameter(2)\n+  rhs1 = $rhs_type{1,0} fusion(rhs),\n+    kind=kCustom,\n+    calls=frhs,\n+    backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"16\",\"32\"]}],\n+          \"num_warps\":\"1\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\",\n+        }\n+      }\n+    }\n+  rhs_scale = $rhs_scale_type parameter(3)\n+  rhs_scale1 = $rhs_scale_type{1,0} fusion(rhs_scale),\n+    kind=kCustom,\n+    calls=frhs_scale,\n+    backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"16\",\"1\"]}],\n+          \"num_warps\":\"1\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\",\n+        }\n+      }\n+    }\n+  ROOT _ = $output_type{1,0} scaled-dot(lhs1, lhs_scale1, rhs1, rhs_scale1),\n+    lhs_contracting_dims={1},\n+    rhs_contracting_dims={0}\n+}\n+\n+ENTRY e {\n+  p0 = $lhs_type{1,0} parameter(0)\n+  p1 = $lhs_scale_type{1,0} parameter(1)\n+  p2 = $rhs_type{1,0} parameter(2)\n+  p3 = $rhs_scale_type{1,0} parameter(3)\n+  ROOT _ = $output_type{1,0} fusion(p0, p1, p2, p3),\n+    kind=kCustom,\n+    calls=triton_dot,\n+    backend_config={\n+      \"fusion_backend_config\": {\n+        kind: \"__triton_scaled_dot_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"16\", \"16\"]}],\n+          \"num_warps\":\"1\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\"\n+        }\n+      }\n+    }\n+}\n+)hlo\";\n+\n+  auto hlo_text = params.PrepareHloText(kHloTextTemplate);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_text));\n+\n+  auto debug_options = module->config().debug_options();\n+  debug_options.set_xla_gpu_experimental_scaled_dot_with_triton(true);\n+  module->mutable_config().set_debug_options(debug_options);\n+\n+  constexpr absl::string_view kExpectedTritonIrTmpl = R\"(\n+      CHECK: tt.dot_scaled\n+      CHECK: tensor<16x32x$triton_type>, tensor<16x1xi8>\n+      CHECK: tensor<32x16x$triton_type>, tensor<1x16xi8>\n+      CHECK: -> tensor<16x16xf32>\n+  )\";\n+  auto expected_triton_ir = absl::StrReplaceAll(\n+      kExpectedTritonIrTmpl, {{\"$triton_type\", params.expected_triton_type}});\n+  EXPECT_THAT(\n+      CreateTritonIrAndFileCheck(*module->GetComputationWithName(\"triton_dot\"),\n+                                 /*block_level_parameters=*/\n+                                 {\n+                                     {{16, 16}},\n+                                     1,\n+                                     1,\n+                                     1,\n+                                     true,\n+                                 },\n+                                 expected_triton_ir),\n+      absl_testing::IsOk());\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    TritonScaledDotGemmTest, TritonScaledDotGemmTest,\n+    ::testing::Values(ScaleDotTestParams{\"f8e4m3fn[64,512]\", \"f8e8m0fnu[64,16]\",\n+                                         \"f8e4m3fn[512,64]\", \"f8e8m0fnu[16,64]\",\n+                                         \"f32[64,64]\", \"f8E4M3FN\"},\n+                      ScaleDotTestParams{\"f8e5m2[64,512]\", \"f8e8m0fnu[64,16]\",\n+                                         \"f8e5m2[512,64]\", \"f8e8m0fnu[16,64]\",\n+                                         \"f32[64,64]\", \"f8E5M2\"}),\n+    ScaleDotTestParams::ToString);\n+\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "315fd1b885f7638644907795851212ab56348190",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=f663012495a853099bbb720ff2ed529e8568e299",
            "patch": "@@ -450,6 +450,7 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_gpu_experimental_enable_command_buffer_on_thunks(true);\n   opts.set_xla_detect_unstable_reductions(\n       DebugOptions::UNSTABLE_REDUCTION_DETECTION_MODE_NONE);\n+  opts.set_xla_gpu_experimental_scaled_dot_with_triton(false);\n   return opts;\n }\n "
        },
        {
            "sha": "f1a772db6aef3d8623d65f1ea45a35cbb3a55341",
            "filename": "third_party/xla/xla/service/gpu/hlo_fusion_analysis.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.cc?ref=f663012495a853099bbb720ff2ed529e8568e299",
            "patch": "@@ -113,7 +113,8 @@ HloFusionAnalysis::EmitterFusionKind GetEmitterFusionKind(\n \n   if (fusion_backend_config.kind() == kTritonFusionKind ||\n       fusion_backend_config.kind() == kTritonGemmFusionKind ||\n-      fusion_backend_config.kind() == kTritonNestedGemmFusionKind) {\n+      fusion_backend_config.kind() == kTritonNestedGemmFusionKind ||\n+      fusion_backend_config.kind() == kTritonScaledDotFusionKind) {\n     return HloFusionAnalysis::EmitterFusionKind::kTriton;\n   }\n "
        },
        {
            "sha": "eb27b5210cce3e93268284b5f4ff94bae208ae56",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f663012495a853099bbb720ff2ed529e8568e299/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=f663012495a853099bbb720ff2ed529e8568e299",
            "patch": "@@ -1330,10 +1330,14 @@ message DebugOptions {\n   // layouts.\n   optional bool xla_early_exit_with_layouts = 397;\n \n+  // If true, the triton fusion emitter will ignore IsTritonSupportedInstruction\n+  // We need this to enable triton scaled dot emitter for testing.\n+  optional bool xla_gpu_experimental_scaled_dot_with_triton = 410;\n+\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 410\n+  // Next id: 411\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 472,
        "additions": 454,
        "deletions": 18
    }
}