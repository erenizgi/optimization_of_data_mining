{
    "author": "thcmbs",
    "message": "[XLA:GPU] Move TransposeDescription back to xla/service/gpu.\n\nOnly used in gpu emitters.\n\nPre requisite to consolidate with TransposeSpec\n\nPiperOrigin-RevId: 844800024",
    "sha": "67360fac620c966137bcb964c30c04b5511d82ef",
    "files": [
        {
            "sha": "58ed76d4ee1a1db648d0dfc6f7417b5bd74619b1",
            "filename": "third_party/xla/xla/codegen/ir_emission_utils.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/67360fac620c966137bcb964c30c04b5511d82ef/third_party%2Fxla%2Fxla%2Fcodegen%2Fir_emission_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/67360fac620c966137bcb964c30c04b5511d82ef/third_party%2Fxla%2Fxla%2Fcodegen%2Fir_emission_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fir_emission_utils.cc?ref=67360fac620c966137bcb964c30c04b5511d82ef",
            "patch": "@@ -32,22 +32,13 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n-#include \"xla/primitive_util.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla {\n-\n-int GetBitwidth(PrimitiveType type) {\n-  if (type == PRED) {\n-    return 8;\n-  }\n-  return primitive_util::BitWidth(type);\n-}\n-\n bool IsIntermediate(const HloInstruction* instr, int allowed_operand_count) {\n   // Number of operands should be in range [1, allowed_operand_count].\n   if (instr->operand_count() == 0 ||"
        },
        {
            "sha": "d2c18b139385c82ff18e436356efb790e281e10c",
            "filename": "third_party/xla/xla/codegen/ir_emission_utils.h",
            "status": "modified",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/67360fac620c966137bcb964c30c04b5511d82ef/third_party%2Fxla%2Fxla%2Fcodegen%2Fir_emission_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/67360fac620c966137bcb964c30c04b5511d82ef/third_party%2Fxla%2Fxla%2Fcodegen%2Fir_emission_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fir_emission_utils.h?ref=67360fac620c966137bcb964c30c04b5511d82ef",
            "patch": "@@ -16,64 +16,22 @@ limitations under the License.\n #ifndef XLA_CODEGEN_IR_EMISSION_UTILS_H_\n #define XLA_CODEGEN_IR_EMISSION_UTILS_H_\n \n-#include <cstdint>\n #include <functional>\n #include <optional>\n #include <vector>\n \n-#include \"absl/container/inlined_vector.h\"\n #include \"absl/functional/any_invocable.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/types/span.h\"\n #include \"xla/codegen/hlo_fusion_spec.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/service/buffer_assignment.h\"\n-#include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla {\n \n-// Returns the bitwidth of the given primitive type. Unfortunately,\n-// primitive_util::BitWidth(PRED) return 1 instead of 8.\n-int GetBitwidth(PrimitiveType type);\n-\n-/// Description of how to emit a given transposition.\n-struct TransposeDescription {\n-  // Transpose instruction.\n-  const HloInstruction* instr;\n-\n-  // Normalized transpose dimensions.\n-  absl::InlinedVector<int64_t, 3> dimensions;\n-\n-  // Permutations of normalized transpose dimensions.\n-  absl::InlinedVector<int64_t, 3> permutation;\n-\n-  // Required amount of shared memory in bytes.\n-  int64_t shmem_usage = 0;\n-\n-  TransposeDescription(const HloInstruction* instr,\n-                       absl::InlinedVector<int64_t, 3> dimensions,\n-                       absl::InlinedVector<int64_t, 3> permutation,\n-                       int64_t shmem_usage)\n-      : instr(instr),\n-        dimensions(dimensions),\n-        permutation(permutation),\n-        shmem_usage(shmem_usage) {}\n-\n-  // Transpose instruction input shape.\n-  const Shape& input_shape() const { return instr->operand(0)->shape(); }\n-\n-  // Returns true, if both descriptions have the same dimensions and\n-  // permutation, even if they're produced by different instructions.\n-  bool IsEquivalent(const TransposeDescription& other) const {\n-    return dimensions == other.dimensions && permutation == other.permutation &&\n-           GetBitwidth(instr->shape().element_type()) ==\n-               GetBitwidth(other.instr->shape().element_type());\n-  }\n-};\n-\n // Checks if the instruction is elementwise.\n bool IsIntermediate(const HloInstruction* instr, int allowed_operand_count = 1);\n "
        },
        {
            "sha": "3566a20ac1c4a3cd31dab8d1999566625655232f",
            "filename": "third_party/xla/xla/service/gpu/hlo_fusion_analysis.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/67360fac620c966137bcb964c30c04b5511d82ef/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/67360fac620c966137bcb964c30c04b5511d82ef/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.cc?ref=67360fac620c966137bcb964c30c04b5511d82ef",
            "patch": "@@ -30,7 +30,6 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/STLExtras.h\"\n #include \"xla/codegen/hlo_fusion_spec.h\"\n-#include \"xla/codegen/ir_emission_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\""
        },
        {
            "sha": "a6bcd04e3712134b0e52f719fa643a395884e76c",
            "filename": "third_party/xla/xla/service/gpu/hlo_fusion_analysis.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/67360fac620c966137bcb964c30c04b5511d82ef/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/67360fac620c966137bcb964c30c04b5511d82ef/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fhlo_fusion_analysis.h?ref=67360fac620c966137bcb964c30c04b5511d82ef",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/stream_executor/device_description.h\"\n \n namespace xla {"
        },
        {
            "sha": "b22a642419c531910857f0c8526420dc917959c8",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/67360fac620c966137bcb964c30c04b5511d82ef/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/67360fac620c966137bcb964c30c04b5511d82ef/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc?ref=67360fac620c966137bcb964c30c04b5511d82ef",
            "patch": "@@ -177,6 +177,13 @@ static bool IsContiguousSlice(\n   return true;\n }\n \n+int GetBitwidth(PrimitiveType type) {\n+  if (type == PRED) {\n+    return 8;\n+  }\n+  return primitive_util::BitWidth(type);\n+}\n+\n bool IsContiguousSlice(const HloInstruction& instr) {\n   if (auto slice = DynCast<HloSliceInstruction>(&instr)) {\n     const Shape& full_shape = slice->operand(0)->shape();"
        },
        {
            "sha": "012716126d0943da08c29d7947cd0e1cefd87446",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils.h",
            "status": "modified",
            "additions": 40,
            "deletions": 1,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/67360fac620c966137bcb964c30c04b5511d82ef/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/67360fac620c966137bcb964c30c04b5511d82ef/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.h?ref=67360fac620c966137bcb964c30c04b5511d82ef",
            "patch": "@@ -33,7 +33,6 @@ limitations under the License.\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/IR/IRBuilder.h\"\n #include \"llvm/IR/Value.h\"\n-#include \"xla/codegen/ir_emission_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_print_options.h\"\n@@ -170,6 +169,46 @@ HloInstructionAdaptor FindNonTrivialHero(const HloInstructionAdaptor& instr);\n // Same as above, but fusion is the parent computation of the hlo instruction.\n const HloInstruction& FindNonTrivialHero(const HloInstruction& instr);\n \n+// Returns the bitwidth of the given primitive type. Unfortunately,\n+// primitive_util::BitWidth(PRED) return 1 instead of 8.\n+int GetBitwidth(PrimitiveType type);\n+\n+/// Description of how to emit a given transposition.\n+struct TransposeDescription {\n+  // Transpose instruction.\n+  const HloInstruction* instr;\n+\n+  // Normalized transpose dimensions.\n+  absl::InlinedVector<int64_t, 3> dimensions;\n+\n+  // Permutations of normalized transpose dimensions.\n+  // Normalized means that permutation[i] + 1 != permutation[i + 1].\n+  absl::InlinedVector<int64_t, 3> permutation;\n+\n+  // Required amount of shared memory in bytes.\n+  int64_t shmem_usage = 0;\n+\n+  TransposeDescription(const HloInstruction* instr,\n+                       absl::InlinedVector<int64_t, 3> dimensions,\n+                       absl::InlinedVector<int64_t, 3> permutation,\n+                       int64_t shmem_usage)\n+      : instr(instr),\n+        dimensions(dimensions),\n+        permutation(permutation),\n+        shmem_usage(shmem_usage) {}\n+\n+  // Transpose instruction input shape.\n+  const Shape& input_shape() const { return instr->operand(0)->shape(); }\n+\n+  // Returns true, if both descriptions have the same dimensions and\n+  // permutation, even if they're produced by different instructions.\n+  bool IsEquivalent(const TransposeDescription& other) const {\n+    return dimensions == other.dimensions && permutation == other.permutation &&\n+           GetBitwidth(instr->shape().element_type()) ==\n+               GetBitwidth(other.instr->shape().element_type());\n+  }\n+};\n+\n std::optional<TransposeDescription> GetDescriptionForTiledTransposeEmitter(\n     const HloInstruction& hero);\n "
        }
    ],
    "stats": {
        "total": 101,
        "additions": 48,
        "deletions": 53
    }
}