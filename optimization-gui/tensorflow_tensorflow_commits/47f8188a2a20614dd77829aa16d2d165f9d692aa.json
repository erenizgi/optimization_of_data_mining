{
    "author": "WillFroom",
    "message": "[XLA:GPU][XTile] Update reduce, nested fusion and emit scope to emit 0D tensors.\n\nOne of a chain of commits to remove the special casing for 0D tensors so that the triton specific requirement is moved to the lowering stage, e.g. the upcoming XTile::CPU backend doesn't have such a requirement, and consistently using tensors makes it simpler.\n\nPiperOrigin-RevId: 828913512",
    "sha": "47f8188a2a20614dd77829aa16d2d165f9d692aa",
    "files": [
        {
            "sha": "7d0b1b308d8b806e3ef1d056438e57a3aa1ca27e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 25,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/47f8188a2a20614dd77829aa16d2d165f9d692aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/47f8188a2a20614dd77829aa16d2d165f9d692aa/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=47f8188a2a20614dd77829aa16d2d165f9d692aa",
            "patch": "@@ -364,13 +364,13 @@ ScalarOrTensor EmitParameterExtract(EmitterLocOpBuilder b,\n   return MakeScalarOrTensor(b, extracted_tensor);\n }\n \n-absl::StatusOr<ScalarOrTensor> EmitScope(\n+absl::StatusOr<TensorValue> EmitScope(\n     EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const TritonFusionAnalysis* analysis,\n     absl::Span<const HloInstruction* const> instructions,\n-    absl::flat_hash_map<const HloInstruction*, ScalarOrTensor>& values);\n+    absl::flat_hash_map<const HloInstruction*, TensorValue>& values);\n \n-absl::StatusOr<ScalarOrTensor> EmitReduce(\n+absl::StatusOr<TensorValue> EmitReduce(\n     EmitterLocOpBuilder b, const TiledHloInstruction& tiled_hlo_reduce,\n     absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values,\n     const se::DeviceDescription& device_info) {\n@@ -429,7 +429,7 @@ absl::StatusOr<ScalarOrTensor> EmitReduce(\n     HloComputation* reduction_computation = hlo_reduce.to_apply();\n \n     std::vector<const HloInstruction*> to_emit;\n-    absl::flat_hash_map<const HloInstruction*, ScalarOrTensor> region_values;\n+    absl::flat_hash_map<const HloInstruction*, TensorValue> region_values;\n     for (const HloInstruction* instr :\n          reduction_computation->MakeInstructionPostOrder()) {\n       if (instr->opcode() == HloOpcode::kParameter) {\n@@ -442,28 +442,22 @@ absl::StatusOr<ScalarOrTensor> EmitReduce(\n           return Internal(\"Expected reducer argument to be a tensor.\");\n         }\n \n-        // Emit from tensor op so that the reducer can be lowered to triton, as\n-        // the triton reducer can only work with scalars.\n-        auto extracted_argument = MakeScalarOrTensor(b, argument);\n-        TF_RET_CHECK(region_values.insert({instr, extracted_argument}).second);\n+        TF_RET_CHECK(region_values.insert({instr, argument}).second);\n       } else {\n         to_emit.push_back(instr);\n       }\n     }\n \n     TF_RET_CHECK(!to_emit.empty());\n \n-    TF_ASSIGN_OR_RETURN(ScalarOrTensor result,\n+    TF_ASSIGN_OR_RETURN(TensorValue result,\n                         EmitScope(b, device_info, /*analysis=*/nullptr, to_emit,\n                                   region_values));\n-    // Emit from_elements op so that the reducer can be lowered to triton, as\n-    // the triton reducer can only work with scalars.\n-    auto result_as_scalar = MakeTensor(b, result.UnwrapUnsafe());\n-    b.create<stablehlo::ReturnOp>(SmallVector<Value>({result_as_scalar}));\n+    b.create<stablehlo::ReturnOp>(SmallVector<Value>({result}));\n     b.setInsertionPointAfter(reduction);\n   }\n \n-  return MakeScalarOrTensor(b, reduction.getResult(0));\n+  return mlir::cast<TensorValue>(reduction.getResult(0));\n }\n \n // Emit code corresponding to a fusion instruction somehow nested within the\n@@ -472,16 +466,16 @@ absl::StatusOr<ScalarOrTensor> EmitReduce(\n // fusion, we simply flatten the fusion inside the computation.\n //\n // TODO(b/331413981): get rid of this special handling once this is solved.\n-absl::StatusOr<ScalarOrTensor> EmitNestedFusion(\n+absl::StatusOr<TensorValue> EmitNestedFusion(\n     EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const HloFusionInstruction& fusion_instruction,\n-    absl::flat_hash_map<const HloInstruction*, ScalarOrTensor>& values) {\n+    absl::flat_hash_map<const HloInstruction*, TensorValue>& values) {\n   // TODO(b/331402498): revisit the order of scope once we completely\n   // deprecate Triton fusion analysis.\n   const HloComputation* fusion_computation =\n       fusion_instruction.fused_instructions_computation();\n \n-  absl::flat_hash_map<const HloInstruction*, ScalarOrTensor> region_values;\n+  absl::flat_hash_map<const HloInstruction*, TensorValue> region_values;\n \n   std::vector<const HloInstruction*> to_emit;\n   for (const HloInstruction* instr :\n@@ -1455,7 +1449,9 @@ absl::StatusOr<ScalarOrTensor> EmitTiledHloInstruction(\n   }\n \n   if (hlo->opcode() == HloOpcode::kReduce) {\n-    return EmitReduce(b, tiled_hlo, values, device_info);\n+    TF_ASSIGN_OR_RETURN(TensorValue reduce_result,\n+                        EmitReduce(b, tiled_hlo, values, device_info));\n+    return MakeScalarOrTensor(b, reduce_result);\n   }\n \n   if (hlo->IsElementwise()) {\n@@ -1561,13 +1557,13 @@ absl::StatusOr<std::vector<ScalarOrTensor>> EmitTiledComputation(\n \n // Emit sequence of instructions using compatible tiling ordered producers\n // before consumers.\n-absl::StatusOr<ScalarOrTensor> EmitScope(\n+absl::StatusOr<TensorValue> EmitScope(\n     EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n     const TritonFusionAnalysis* analysis,\n     absl::Span<const HloInstruction* const> instructions,\n-    absl::flat_hash_map<const HloInstruction*, ScalarOrTensor>& values) {\n+    absl::flat_hash_map<const HloInstruction*, TensorValue>& values) {\n   for (const HloInstruction* hlo : instructions) {\n-    ScalarOrTensor result;\n+    TensorValue result;\n     if (hlo->opcode() == HloOpcode::kConcatenate ||\n         hlo->opcode() == HloOpcode::kDynamicSlice) {\n       // Parameter loads and their concatenations are handled outside EmitScope.\n@@ -1581,20 +1577,19 @@ absl::StatusOr<ScalarOrTensor> EmitScope(\n       TF_RET_CHECK(values.contains(hlo)) << hlo->ToString();\n       continue;\n     } else if (hlo->opcode() == HloOpcode::kConstant) {\n-      TF_ASSIGN_OR_RETURN(TensorValue constant, EmitConstant(b, *hlo));\n-      result = MakeScalarOrTensor(b, constant);\n+      TF_ASSIGN_OR_RETURN(result, EmitConstant(b, *hlo));\n     } else if (hlo->opcode() == HloOpcode::kBroadcast) {\n       return absl::InvalidArgumentError(\n           \"Broadcast is not yet supported in EmitScope().\");\n     } else if (HloInstruction::IsOpElementwise(hlo->opcode())) {\n       std::vector<Value> operands;\n       operands.reserve(hlo->operands().size());\n       for (const HloInstruction* operand : hlo->operands()) {\n-        operands.push_back(MakeTensor(b, values[operand].UnwrapUnsafe()));\n+        operands.push_back(values[operand]);\n       }\n       TF_ASSIGN_OR_RETURN(Value elementwise_result,\n                           EmitElementwise(b, device_info, *hlo, operands));\n-      result = MakeScalarOrTensor(b, elementwise_result);\n+      result = mlir::cast<TensorValue>(elementwise_result);\n     } else if (hlo->opcode() == HloOpcode::kTuple) {\n       TF_RET_CHECK(hlo->IsRoot()) << hlo->ToString();\n     } else if (hlo->opcode() == HloOpcode::kBitcast ||"
        }
    ],
    "stats": {
        "total": 45,
        "additions": 20,
        "deletions": 25
    }
}