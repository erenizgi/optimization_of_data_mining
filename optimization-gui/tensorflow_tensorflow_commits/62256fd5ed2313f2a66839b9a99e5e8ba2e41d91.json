{
    "author": "dimvar",
    "message": "PR #32185: Don't hardcode the configs size in GetSupportedConfigsFromCublasCustomCall, e.g., on Spark it is 9.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32185\n\nThe only requirement in the test is that the size is at least 2.\n\nğŸ“ Summary of Changes\nPlease provide a clear and concise summary of the changes you've made.\n\nğŸ¯ Justification\nExplain why this change is important and which workload benefits from this\nchange.\n\nğŸš€ Kind of Contribution\nPlease remove what does not apply: ğŸ› Bug Fix, âš¡ï¸ Performance Improvement,\nâœ¨ New Feature, â™»ï¸ Cleanup, ğŸ“š Documentation, ğŸ§ª Tests\n\nğŸ“Š Benchmark (for Performance Improvements)\nPlease measure and include speedups for one of the public HLOs in\n`compiler/xla/tools/benchmarks/hlo/`.\n\nğŸ§ª Unit Tests:\nWhat unit tests were added? For example, a new pass should be tested on minimal\nHLO. The transformation can be tested with FileCheck tests or assertions on the\ntransformed HLO.\n\nğŸ§ª Execution Tests:\nWhat execution tests were added? For example, a new optimization should be\ntested with an end-to-end execution test triggering the optimization and\nasserting correctness. Please provide test cases running with at most 2 GPUs.\n\nCopybara import of the project:\n\n--\nf0127ad83b5818ecaa22d93b75c3db7b316bee14 by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:\n\nDon't hardcode the configs size in GetSupportedConfigsFromCublasCustomCall, e.g., on Spark it is 9.\n\nMerging this change closes #32185\n\nPiperOrigin-RevId: 814634368",
    "sha": "62256fd5ed2313f2a66839b9a99e5e8ba2e41d91",
    "files": [
        {
            "sha": "cd881c9240ac9971eb9d7ff6c6c174048715a66c",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/62256fd5ed2313f2a66839b9a99e5e8ba2e41d91/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/62256fd5ed2313f2a66839b9a99e5e8ba2e41d91/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc?ref=62256fd5ed2313f2a66839b9a99e5e8ba2e41d91",
            "patch": "@@ -97,7 +97,7 @@ TEST_F(FissionBackendTest, GetSupportedConfigsFromCublasCustomCall) {\n   absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> configs =\n       backend_.GetSupportedConfigs(\n           (*module->entry_computation()->root_instruction()));\n-  EXPECT_THAT(configs, absl_testing::IsOkAndHolds(SizeIs(10)));\n+  EXPECT_THAT(configs, absl_testing::IsOkAndHolds(SizeIs(testing::Ge(2))));\n   // The first config is the cublas config.\n   AutotuneResult::GemmKey cublas_config;\n   EXPECT_TRUE(configs.value().front()->UnpackTo(&cublas_config));"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}