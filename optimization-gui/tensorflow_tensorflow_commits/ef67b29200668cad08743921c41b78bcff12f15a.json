{
    "author": "mooskagh",
    "message": "Reverts 40ad987e165e1cfcbe2e96992403bf0b358a5a7d\n\nPiperOrigin-RevId: 807128090",
    "sha": "ef67b29200668cad08743921c41b78bcff12f15a",
    "files": [
        {
            "sha": "b16d1e508631e2a55946f8552487947311927dfe",
            "filename": "third_party/xla/xla/service/gpu/amdgpu_compiler.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 2,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ef67b29200668cad08743921c41b78bcff12f15a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ef67b29200668cad08743921c41b78bcff12f15a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Famdgpu_compiler.cc?ref=ef67b29200668cad08743921c41b78bcff12f15a",
            "patch": "@@ -146,8 +146,27 @@ absl::Status AMDGPUCompiler::OptimizeHloConvolutionCanonicalization(\n   pipeline.AddPass<HloPassFix<GpuAlgebraicSimplifier>>(algsimp_options,\n                                                        gpu_version);\n \n-  pipeline.AddPass<ConvertMover>();\n-  pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n+  // tf2xla bridge, DepthwiseConvolutionConverter, ConvRewriter, and\n+  // CudnnSimplifyPadding introduce reshapes and transposes.  Run ReshapeMover\n+  // to a fixed point.  Include algsimp because ReshapeMover relies on it.\n+  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(\n+          \"reshape_mover_after_conv_canonicalization\")] {\n+    ReshapeMoverOptions reshape_mover_options;\n+    reshape_mover_options.reshape_of_1d_broadcast_is_cheap = true;\n+    pipeline.AddPass<ReshapeMover>(reshape_mover_options);\n+    pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n+  }();\n+\n+  // The reshapes and transposes can possibly be eliminated using\n+  // AlgebraicSimplifier. ConvertMover and ReshapeMover fight with each other.\n+  // ConvertMover wants to move some converts down the graph, but ReshapeMover\n+  // wants to move them up the graph. We run ConvertMover and algsimp to a fixed\n+  // point.\n+  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(\n+          \"simplify_after_conv_canonicalization\")] {\n+    pipeline.AddPass<ConvertMover>();\n+    pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n+  }();\n \n   // ConvRewriter, ConvPaddingLegalization and\n   // CudnnConvPadForTensorCores may add instructions which can be simplified"
        },
        {
            "sha": "70843c7c71487164bf5e53148a152951bf61afa1",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ef67b29200668cad08743921c41b78bcff12f15a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ef67b29200668cad08743921c41b78bcff12f15a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=ef67b29200668cad08743921c41b78bcff12f15a",
            "patch": "@@ -922,12 +922,23 @@ absl::Status RunOptimizationPasses(\n     pipeline.AddPass<WhileLoopSimplifier>();\n     pipeline.AddPass<SliceSinker>();\n \n+    ReshapeMoverOptions reshape_mover_options;\n+    reshape_mover_options.reshape_of_1d_broadcast_is_cheap = true;\n+    pipeline.AddPass<ReshapeMover>(reshape_mover_options);\n     pipeline.AddPass<HloConstantFolding>();\n     pipeline.AddPass<ConditionalSimplifier>();\n     pipeline.AddPass<RealImagExpander>();\n     pipeline.AddPass<TransposeFolding>(CanFoldTransposeOperandIntoDot);\n     pipeline.AddPass<HloCSE>(/*is_layout_sensitive=*/false);\n     pipeline.AddPass<HloDCE>();\n+  }();\n+\n+  // ConvertMover and ReshapeMover fight with each other: ConvertMover wants\n+  // to move some converts down the graph, but ReshapeMover wants to move them\n+  // up the graph.  As a compromise, let ReshapeMover run to a fixed point,\n+  // and then run ConvertMover + algsimp to a fixed point.\n+  [&, &pipeline =\n+          pipeline.AddPass<HloPassFix<HloPassPipeline>>(\"simplification-2\")] {\n     pipeline.AddPass<ConvertMover>();\n     pipeline.AddPass<GpuAlgebraicSimplifier>(layout_insensitive_algsimp_opts,\n                                              gpu_version);"
        },
        {
            "sha": "8bafa13d1e7e9f02f9cb72f86edb497033e520ee",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test_autotune_db.textproto",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ef67b29200668cad08743921c41b78bcff12f15a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ef67b29200668cad08743921c41b78bcff12f15a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto?ref=ef67b29200668cad08743921c41b78bcff12f15a",
            "patch": "@@ -63,7 +63,7 @@ results {\n }\n results {\n   device: \"CUDA: 9.0, Cores: 132, GPU clock: 1.98 GHz, Memory bandwidth: 3352 GB/s, L2 cache: 50 MB, DNN version: 1.2.3\"\n-  hlo: \"{\\n  tmp_0 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(0)\\n  tmp_1 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_0)\\n  tmp_2 = bf16[] constant({...})\\n  tmp_3 = bf16[4,32,1024,1024]{3,2,1,0} broadcast(bf16[] tmp_2), dimensions={}\\n  tmp_4 = bf16[4,32,1024,1024]{3,2,1,0} multiply(bf16[4,32,1024,1024]{3,2,1,0} tmp_1, bf16[4,32,1024,1024]{3,2,1,0} tmp_3)\\n  tmp_5 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_4)\\n  tmp_6 = bf16[128,1024,1024]{2,1,0} transpose(bf16[128,1024,1024]{2,1,0} tmp_5), dimensions={0,2,1}\\n  tmp_7 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_6)\\n  tmp_8 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_7)\\n  tmp_9 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(1)\\n  tmp_10 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_9)\\n  tmp_11 = bf16[128,1024,1024]{2,1,0} dot(bf16[128,1024,1024]{2,1,0} tmp_8, bf16[128,1024,1024]{2,1,0} tmp_10), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}\\n  ROOT tmp_12 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_11)\\n}\"\n+  hlo: \"{\\n  tmp_0 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(0)\\n  tmp_1 = bf16[] constant({...})\\n  tmp_2 = bf16[1,4,32,1024,1024]{4,3,2,1,0} broadcast(bf16[] tmp_1), dimensions={}\\n  tmp_3 = bf16[1,4,32,1024,1024]{4,3,2,1,0} multiply(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_0, bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_2)\\n  tmp_4 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_3)\\n  tmp_5 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_4)\\n  tmp_6 = bf16[128,1024,1024]{2,1,0} transpose(bf16[128,1024,1024]{2,1,0} tmp_5), dimensions={0,2,1}\\n  tmp_7 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_6)\\n  tmp_8 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[4,32,1024,1024]{3,2,1,0} tmp_7)\\n  tmp_9 = bf16[1,4,32,1024,1024]{4,3,2,1,0} parameter(1)\\n  tmp_10 = bf16[128,1024,1024]{2,1,0} bitcast(bf16[1,4,32,1024,1024]{4,3,2,1,0} tmp_9)\\n  tmp_11 = bf16[128,1024,1024]{2,1,0} dot(bf16[128,1024,1024]{2,1,0} tmp_8, bf16[128,1024,1024]{2,1,0} tmp_10), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}\\n  ROOT tmp_12 = bf16[4,32,1024,1024]{3,2,1,0} bitcast(bf16[128,1024,1024]{2,1,0} tmp_11)\\n}\"\n   result {\n     gemm {\n       algorithm: -1"
        },
        {
            "sha": "8b1e5ee5874aada474bba137315143dfcc072134",
            "filename": "third_party/xla/xla/service/gpu/gpu_spmd_pipeline.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ef67b29200668cad08743921c41b78bcff12f15a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_spmd_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ef67b29200668cad08743921c41b78bcff12f15a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_spmd_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_spmd_pipeline.cc?ref=ef67b29200668cad08743921c41b78bcff12f15a",
            "patch": "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"xla/hlo/transforms/simplifiers/hlo_constant_folding.h\"\n #include \"xla/hlo/transforms/simplifiers/hlo_constant_splitter.h\"\n #include \"xla/hlo/transforms/simplifiers/hlo_dce.h\"\n+#include \"xla/hlo/transforms/simplifiers/reshape_mover.h\"\n #include \"xla/hlo/transforms/simplifiers/sort_simplifier.h\"\n #include \"xla/hlo/transforms/simplifiers/tuple_simplifier.h\"\n #include \"xla/service/call_graph.h\"\n@@ -78,6 +79,9 @@ void AddSPMDPasses(\n   spmd_simplify.AddPass<WhileLoopConstantSinking>();\n   spmd_simplify.AddPass<WhileLoopSimplifier>();\n \n+  ReshapeMoverOptions reshape_mover_options;\n+  reshape_mover_options.reshape_of_1d_broadcast_is_cheap = true;\n+  spmd_simplify.AddPass<ReshapeMover>(reshape_mover_options);\n   // Run AlgebraicSimplifier directly before HloConstantFolding, because we\n   // need to simplify DynamicSlice(Broadcast) away. Constant folding of\n   // DynamicSlice can be quite costly, as the whole operand will be evaluated."
        },
        {
            "sha": "d456c45b5a51f74d1ed1e8b01fdf2fe4c032907b",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 2,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ef67b29200668cad08743921c41b78bcff12f15a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ef67b29200668cad08743921c41b78bcff12f15a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=ef67b29200668cad08743921c41b78bcff12f15a",
            "patch": "@@ -241,8 +241,27 @@ absl::Status NVPTXCompiler::OptimizeHloConvolutionCanonicalization(\n     pipeline.AddPass<CudnnSimplifyPadding>();\n   }\n \n-  pipeline.AddPass<ConvertMover>();\n-  pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n+  // tf2xla bridge, DepthwiseConvolutionConverter, ConvRewriter, and\n+  // CudnnSimplifyPadding introduce reshapes and transposes.  Run ReshapeMover\n+  // to a fixed point.  Include algsimp because ReshapeMover relies on it.\n+  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(\n+          \"reshape_mover_after_conv_canonicalization\")] {\n+    ReshapeMoverOptions reshape_mover_options;\n+    reshape_mover_options.reshape_of_1d_broadcast_is_cheap = true;\n+    pipeline.AddPass<ReshapeMover>(reshape_mover_options);\n+    pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n+  }();\n+\n+  // The reshapes and transposes can possibly be eliminated using\n+  // AlgebraicSimplifier. ConvertMover and ReshapeMover fight with each other.\n+  // ConvertMover wants to move some converts down the graph, but ReshapeMover\n+  // wants to move them up the graph. We run ConvertMover and algsimp to a fixed\n+  // point.\n+  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(\n+          \"simplify_after_conv_canonicalization\")] {\n+    pipeline.AddPass<ConvertMover>();\n+    pipeline.AddPass<GpuAlgebraicSimplifier>(algsimp_options, gpu_version);\n+  }();\n \n   // ConvRewriter, ConvPaddingLegalization and\n   // CudnnConvPadForTensorCores may add instructions which can be simplified"
        },
        {
            "sha": "7b04315cc624a49198e4bba0b6976e78b84e28f9",
            "filename": "third_party/xla/xla/tests/multioutput_fusion_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ef67b29200668cad08743921c41b78bcff12f15a/third_party%2Fxla%2Fxla%2Ftests%2Fmultioutput_fusion_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ef67b29200668cad08743921c41b78bcff12f15a/third_party%2Fxla%2Fxla%2Ftests%2Fmultioutput_fusion_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fmultioutput_fusion_test.cc?ref=ef67b29200668cad08743921c41b78bcff12f15a",
            "patch": "@@ -47,12 +47,9 @@ class MultiOutputFusionTest : public HloTestBase {\n   // Layout assignment assumes that there are no fusions in the input graph.\n   // Since the purpose of this test is to send pre-fused graphs to XLA, we have\n   // to do layout assignment ourselves.\n-  // Dot strength reduction replaces dot with a multiply and it does require\n-  // layout assignment to ensure compatible physical layouts.\n   DebugOptions GetDebugOptionsForTest() const override {\n     auto opts = HloTestBase::GetDebugOptionsForTest();\n     opts.add_xla_disable_hlo_passes(\"layout-assignment\");\n-    opts.add_xla_disable_hlo_passes(\"dot-strength-reduction\");\n     return opts;\n   }\n "
        }
    ],
    "stats": {
        "total": 66,
        "additions": 58,
        "deletions": 8
    }
}