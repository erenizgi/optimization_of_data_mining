{
    "author": "nputikhin",
    "message": "[XLA:GPU] Introduce thunk pass buffer allocator and checksum pass using it\n\nThe new allocation interface gives us the ability to create new buffers from thunk passes in addition to existing allocations.\n\nThe first pass using this feature is the checksum tracing pass introduced here. Right now all it does is allocating a single buffer but just you wait.\n\nPiperOrigin-RevId: 814193916",
    "sha": "b2c96769f094c3d81f5acd93ab9f557644db2ed3",
    "files": [
        {
            "sha": "10670c6922a65616fa2a3672aa8b01c3906f5c83",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 41,
            "deletions": 2,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=b2c96769f094c3d81f5acd93ab9f557644db2ed3",
            "patch": "@@ -2568,8 +2568,10 @@ cc_library(\n     hdrs = [\"thunk_pass_pipeline.h\"],\n     deps = [\n         \":sequential_thunk\",\n+        \"//xla/service:buffer_assignment\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/base:nullability\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n@@ -2583,8 +2585,11 @@ xla_cc_test(\n         \":sequential_thunk\",\n         \":thunk\",\n         \":thunk_pass_pipeline\",\n+        \"//xla/service:buffer_assignment\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/base:nullability\",\n+        \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n@@ -2613,6 +2618,7 @@ cc_library(\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/base:nullability\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/functional:overload\",\n         \"@com_google_absl//absl/log\",\n@@ -2643,6 +2649,7 @@ xla_test(\n         \":replica_id_thunk\",\n         \":sequential_thunk\",\n         \":thunk\",\n+        \":thunk_pass_pipeline\",\n         \":while_thunk\",\n         \"//xla:shape_util\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\",\n@@ -2657,14 +2664,46 @@ xla_test(\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream_executor_h\",\n-        \"//xla/tsl/platform:status\",\n-        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n )\n \n+cc_library(\n+    name = \"thunk_checksum_tracing_pass\",\n+    srcs = [\"thunk_checksum_tracing_pass.cc\"],\n+    hdrs = [\"thunk_checksum_tracing_pass.h\"],\n+    deps = [\n+        \":sequential_thunk\",\n+        \":thunk_pass_pipeline\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"thunk_checksum_tracing_pass_test\",\n+    srcs = [\"thunk_checksum_tracing_pass_test.cc\"],\n+    deps = [\n+        \":sequential_thunk\",\n+        \":thunk\",\n+        \":thunk_checksum_tracing_pass\",\n+        \":thunk_pass_pipeline\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n cc_library(\n     name = \"runtime_intrinsics\",\n     srcs = [\"runtime_intrinsics.cc\"],"
        },
        {
            "sha": "fd0879e241c0f4df191a7fd2960c2ca2344d9ebd",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_conversion_pass.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.cc?ref=b2c96769f094c3d81f5acd93ab9f557644db2ed3",
            "patch": "@@ -40,6 +40,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk_pass_pipeline.h\"\n #include \"xla/backends/gpu/runtime/while_thunk.h\"\n #include \"xla/ffi/ffi_api.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n@@ -397,7 +398,8 @@ absl::Status FlushCommandBuffer(\n \n absl::StatusOr<bool> CommandBufferConversionPass::Run(\n     SequentialThunk* root_thunk, const DebugOptions& debug_options,\n-    const se::DeviceDescription& device_info) {\n+    const se::DeviceDescription& device_info,\n+    ThunkPassBufferAllocator& allocator) {\n   tsl::profiler::TraceMe traceme(\"CommandBufferConversionPass\");\n \n   CommandBufferConfig config =\n@@ -452,9 +454,9 @@ absl::StatusOr<bool> CommandBufferConversionPass::Run(\n       // If a `WhileThunk` itself is not eligible for conversion into a\n       // command buffer, we attempt to convert thunks within its body\n       auto while_thunk = static_cast<WhileThunk*>(thunk.get());\n-      TF_ASSIGN_OR_RETURN(\n-          bool changed_in_body,\n-          Run(while_thunk->body_thunk_sequence(), debug_options, device_info));\n+      TF_ASSIGN_OR_RETURN(bool changed_in_body,\n+                          Run(while_thunk->body_thunk_sequence(), debug_options,\n+                              device_info, allocator));\n       changed |= changed_in_body;\n     } else if (thunk->kind() == Thunk::kConditional) {\n       // If a `ConditionalThunk` itself is not eligible for conversion into a\n@@ -463,7 +465,7 @@ absl::StatusOr<bool> CommandBufferConversionPass::Run(\n       for (auto& branch_thunk : conditional_thunk->branch_thunks()) {\n         TF_ASSIGN_OR_RETURN(\n             bool changed_in_branch,\n-            Run(branch_thunk.get(), debug_options, device_info));\n+            Run(branch_thunk.get(), debug_options, device_info, allocator));\n         changed |= changed_in_branch;\n       }\n     }"
        },
        {
            "sha": "f63f93173c76b08481aba0b9c026a55ac3054224",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_conversion_pass.h",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass.h?ref=b2c96769f094c3d81f5acd93ab9f557644db2ed3",
            "patch": "@@ -16,9 +16,9 @@ limitations under the License.\n #ifndef XLA_BACKENDS_GPU_RUNTIME_COMMAND_BUFFER_CONVERSION_PASS_H_\n #define XLA_BACKENDS_GPU_RUNTIME_COMMAND_BUFFER_CONVERSION_PASS_H_\n \n-#include <memory>\n #include <string>\n \n+#include \"absl/base/nullability.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n@@ -40,7 +40,8 @@ class CommandBufferConversionPass : public ThunkPassInterface {\n \n   absl::StatusOr<bool> Run(SequentialThunk* root_thunk,\n                            const DebugOptions& debug_options,\n-                           const se::DeviceDescription& device_info) override;\n+                           const se::DeviceDescription& device_info,\n+                           ThunkPassBufferAllocator& allocator) override;\n   struct CommandBufferConfig {\n     // DebugOptions control which commands are enabled. Long term we want to\n     // remove that flag and enable all supported commands by default."
        },
        {
            "sha": "6b1b6f2820d5459762c99abd63590de6fbdb5d6b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_conversion_pass_test.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 17,
            "changes": 57,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_conversion_pass_test.cc?ref=b2c96769f094c3d81f5acd93ab9f557644db2ed3",
            "patch": "@@ -24,7 +24,9 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"absl/status/status.h\"\n #include \"absl/status/status_matchers.h\"\n+#include \"absl/status/statusor.h\"\n #include \"absl/strings/ascii.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/runtime/all_gather_thunk.h\"\n@@ -38,6 +40,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/replica_id_thunk.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk_pass_pipeline.h\"\n #include \"xla/backends/gpu/runtime/while_thunk.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -54,8 +57,6 @@ limitations under the License.\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n-#include \"xla/tsl/platform/status.h\"\n-#include \"xla/tsl/platform/statusor.h\"\n \n namespace xla {\n namespace gpu {\n@@ -216,6 +217,13 @@ std::unique_ptr<PartitionIdThunk> CreatePartitionIdThunk(\n   return std::make_unique<PartitionIdThunk>(Thunk::ThunkInfo(), slice0);\n }\n \n+class FakeErrorAllocator : public ThunkPassBufferAllocator {\n+ public:\n+  absl::StatusOr<BufferAllocation*> NewEmptyAllocation(int64_t size) override {\n+    return absl::InternalError(\"FakeErrorAllocator: Allocation failed\");\n+  }\n+};\n+\n TEST(CommandBufferConversionPassTest, ConvertsToCommandBufferThunk) {\n   std::vector<std::unique_ptr<Thunk>> thunks;\n \n@@ -230,6 +238,7 @@ TEST(CommandBufferConversionPassTest, ConvertsToCommandBufferThunk) {\n   debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n \n   se::DeviceDescription device_info;\n+  FakeErrorAllocator allocator;\n \n   ASSERT_EQ(root_thunk->thunks().size(), 1);\n \n@@ -239,7 +248,7 @@ TEST(CommandBufferConversionPassTest, ConvertsToCommandBufferThunk) {\n   // supported in command buffers. The expected transformation is:\n   // SequentialThunk(CopyThunk) ->\n   // SequentialThunk(CommandBufferThunk(CopyThunk))\n-  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info),\n+  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info, allocator),\n               IsOkAndHolds(true));\n \n   EXPECT_THAT(root_thunk->thunks(), ThunkKindsAre(Thunk::kCommandBuffer));\n@@ -274,10 +283,11 @@ TEST(CommandBufferConversionPassTest, PartiallyConvertsToCommandBufferThunk) {\n   debug_options.clear_xla_gpu_enable_command_buffer();\n   debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n   se::DeviceDescription device_info;\n+  FakeErrorAllocator allocator;\n \n   ASSERT_EQ(root_thunk->thunks().size(), 3);\n \n-  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info),\n+  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info, allocator),\n               IsOkAndHolds(true));\n \n   // Expected transformation: (Copy, Gemm, Copy) -> (CommandBuffer(Copy), Gemm,\n@@ -323,8 +333,9 @@ TEST(CommandBufferConversionPassTest, ConvertsAsyncPairToCommandBuffer) {\n   debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::COLLECTIVES);\n \n   se::DeviceDescription device_info = TestGpuDeviceInfo::CudaOrRocmDeviceInfo();\n+  FakeErrorAllocator allocator;\n   CommandBufferConversionPass pass;\n-  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info),\n+  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info, allocator),\n               IsOkAndHolds(true));\n \n   // Expected transformation:\n@@ -366,10 +377,11 @@ TEST(CommandBufferConversionPassTest,\n   debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::COLLECTIVES);\n \n   se::DeviceDescription device_info = TestGpuDeviceInfo::CudaOrRocmDeviceInfo();\n+  FakeErrorAllocator allocator;\n   CommandBufferConversionPass pass;\n   // Expected no transformation, because there is a non-convertible thunk in\n   // between the asyncs.\n-  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info),\n+  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info, allocator),\n               IsOkAndHolds(false));\n   EXPECT_THAT(root_thunk->thunks(),\n               ThunkKindsAre(Thunk::kAllGatherStart, Thunk::kCopy,\n@@ -400,7 +412,8 @@ TEST(CommandBufferConversionPassTest, ConvertCrossedAsyncs) {\n   debug_options.clear_xla_gpu_enable_command_buffer();\n   debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::COLLECTIVES);\n \n-  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info),\n+  FakeErrorAllocator allocator;\n+  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info, allocator),\n               IsOkAndHolds(true));\n \n   // Expected transformation: Convert all 4 thunks into command buffer\n@@ -444,7 +457,8 @@ TEST(CommandBufferConversionPassTest, ConvertNestedAsyncs) {\n   debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::CUBLAS);\n   debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n \n-  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info),\n+  FakeErrorAllocator allocator;\n+  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info, allocator),\n               IsOkAndHolds(true));\n \n   // Expected transformation: Convert all 5 thunks into command buffer\n@@ -492,7 +506,8 @@ TEST(CommandBufferConversionPassTest, DontConvertAsyncsIfUnpairedStart) {\n   debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::COLLECTIVES);\n   debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n \n-  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info),\n+  FakeErrorAllocator allocator;\n+  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info, allocator),\n               IsOkAndHolds(true));\n \n   // Expected transformation: {Copy, AllGatherStart0, AllGatherStart1,\n@@ -547,8 +562,9 @@ TEST(CommandBufferConversionPassTest, ConvertsAsyncPairsMixedWithOtherThunks) {\n   debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n \n   se::DeviceDescription device_info = TestGpuDeviceInfo::CudaOrRocmDeviceInfo();\n+  FakeErrorAllocator allocator;\n   CommandBufferConversionPass pass;\n-  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info),\n+  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info, allocator),\n               IsOkAndHolds(true));\n \n   // Expected transformation:\n@@ -581,14 +597,15 @@ TEST(CommandBufferConversionPassTest, DontConvertIfNotMinGraphSize) {\n   debug_options.set_xla_gpu_graph_min_graph_size(2);\n \n   se::DeviceDescription device_info;\n+  FakeErrorAllocator allocator;\n \n   ASSERT_EQ(root_thunk->thunks().size(), 1);\n \n   CommandBufferConversionPass pass;\n \n   // The size of the sequence is less than the min graph size, so it should not\n   // be converted to a command buffer.\n-  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info),\n+  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info, allocator),\n               IsOkAndHolds(false));\n   EXPECT_THAT(root_thunk->thunks(), ThunkKindsAre(Thunk::kCopy));\n }\n@@ -621,9 +638,10 @@ TEST(CommandBufferConversionPassTest, ConvertWhileThunk) {\n   debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n   debug_options.set_xla_gpu_graph_min_graph_size(1);\n   se::DeviceDescription device_info = TestGpuDeviceInfo::CudaOrRocmDeviceInfo();\n+  FakeErrorAllocator allocator;\n   ASSERT_EQ(root_thunk->thunks().size(), 1);\n \n-  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info),\n+  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info, allocator),\n               IsOkAndHolds(true));\n \n   // Expected transformation: (While({Copy}, {Gemm})) ->\n@@ -682,9 +700,10 @@ TEST(CommandBufferConversionPassTest,\n   debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n   debug_options.set_xla_gpu_graph_min_graph_size(1);\n   se::DeviceDescription device_info = TestGpuDeviceInfo::CudaOrRocmDeviceInfo();\n+  FakeErrorAllocator allocator;\n   ASSERT_EQ(root_thunk->thunks().size(), 1);\n \n-  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info),\n+  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info, allocator),\n               IsOkAndHolds(true));\n \n   // Expected transformation is: kConditional({kCopy}, {kAllGatherStart, kCopy})\n@@ -731,9 +750,10 @@ TEST(CommandBufferConversionPassTest, ConvertWhileThunkWithAsyncPair) {\n   debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n   debug_options.set_xla_gpu_graph_min_graph_size(1);\n   se::DeviceDescription device_info = TestGpuDeviceInfo::CudaOrRocmDeviceInfo();\n+  FakeErrorAllocator allocator;\n   ASSERT_EQ(root_thunk->thunks().size(), 1);\n \n-  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info),\n+  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info, allocator),\n               IsOkAndHolds(true));\n \n   // Expected transformation: (While({Copy}, {AllGatherStart, Copy,\n@@ -772,12 +792,13 @@ TEST(CommandBufferConversionPassTest,\n       \"test_legacy_custom_call\");\n \n   se::DeviceDescription device_info = TestGpuDeviceInfo::CudaOrRocmDeviceInfo();\n+  FakeErrorAllocator allocator;\n \n   ASSERT_EQ(root_thunk->thunks().size(), 1);\n \n   CommandBufferConversionPass pass;\n \n-  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info),\n+  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info, allocator),\n               IsOkAndHolds(true));\n   EXPECT_THAT(root_thunk->thunks(), ThunkKindsAre(Thunk::kCommandBuffer));\n \n@@ -803,14 +824,15 @@ TEST(CommandBufferConversionPassTest, ConvertsCuDnnThunkToCommandBufferThunk) {\n   debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::CUDNN);\n \n   se::DeviceDescription device_info = TestGpuDeviceInfo::CudaOrRocmDeviceInfo();\n+  FakeErrorAllocator allocator;\n \n   ASSERT_EQ(root_thunk->thunks().size(), 1);\n \n   CommandBufferConversionPass pass;\n \n   // The expected transformation is: SequentialThunk(CuDnnThunk) ->\n   // SequentialThunk(CommandBufferThunk(CuDnnThunk))\n-  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info),\n+  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info, allocator),\n               IsOkAndHolds(true));\n   EXPECT_THAT(root_thunk->thunks(), ThunkKindsAre(Thunk::kCommandBuffer));\n \n@@ -853,9 +875,10 @@ TEST(CommandBufferConversionPassTest, ConvertTheBodyOfWhileThunk) {\n   debug_options.add_xla_gpu_enable_command_buffer(DebugOptions::FUSION);\n   debug_options.set_xla_gpu_graph_min_graph_size(1);\n   se::DeviceDescription device_info = TestGpuDeviceInfo::CudaOrRocmDeviceInfo();\n+  FakeErrorAllocator allocator;\n   ASSERT_EQ(root_thunk->thunks().size(), 1);\n \n-  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info),\n+  ASSERT_THAT(pass.Run(root_thunk.get(), debug_options, device_info, allocator),\n               IsOkAndHolds(true));\n \n   // While thunk is not converted itself, because it has a non-convertible thunk"
        },
        {
            "sha": "418626fe1ace10b3eea613c259c813360f584413",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_checksum_tracing_pass.cc",
            "status": "added",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_checksum_tracing_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_checksum_tracing_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_checksum_tracing_pass.cc?ref=b2c96769f094c3d81f5acd93ab9f557644db2ed3",
            "patch": "@@ -0,0 +1,40 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/thunk_checksum_tracing_pass.h\"\n+\n+#include \"absl/status/statusor.h\"\n+#include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk_pass_pipeline.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+absl::StatusOr<bool> ThunkChecksumTracingPass::Run(\n+    SequentialThunk* root_thunk, const DebugOptions& debug_options,\n+    const se::DeviceDescription& device_info,\n+    ThunkPassBufferAllocator& allocator) {\n+  TF_ASSIGN_OR_RETURN(BufferAllocation * log_alloc,\n+                      allocator.NewEmptyAllocation(1234));\n+  (void)log_alloc;\n+\n+  return false;\n+}\n+\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "dad6d190c57f14d2f20b66e8442327967c3848ff",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_checksum_tracing_pass.h",
            "status": "added",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_checksum_tracing_pass.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_checksum_tracing_pass.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_checksum_tracing_pass.h?ref=b2c96769f094c3d81f5acd93ab9f557644db2ed3",
            "patch": "@@ -0,0 +1,44 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_THUNK_CHECKSUM_TRACING_PASS_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_THUNK_CHECKSUM_TRACING_PASS_H_\n+\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk_pass_pipeline.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+// Adds checksum tracing to thunks.\n+class ThunkChecksumTracingPass : public ThunkPassInterface {\n+ public:\n+  ThunkChecksumTracingPass() = default;\n+\n+  absl::string_view name() const override { return \"thunk-checksum-tracing\"; }\n+\n+  absl::StatusOr<bool> Run(SequentialThunk* root_thunk,\n+                           const DebugOptions& debug_options,\n+                           const se::DeviceDescription& device_info,\n+                           ThunkPassBufferAllocator& allocator) override;\n+};\n+\n+}  // namespace gpu\n+}  // namespace xla\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_THUNK_CHECKSUM_TRACING_PASS_H_"
        },
        {
            "sha": "e42165a7ccda3ea805303cc9734c66ae146266cf",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_checksum_tracing_pass_test.cc",
            "status": "added",
            "additions": 69,
            "deletions": 0,
            "changes": 69,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_checksum_tracing_pass_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_checksum_tracing_pass_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_checksum_tracing_pass_test.cc?ref=b2c96769f094c3d81f5acd93ab9f557644db2ed3",
            "patch": "@@ -0,0 +1,69 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/thunk_checksum_tracing_pass.h\"\n+\n+#include <cstdint>\n+#include <memory>\n+#include <vector>\n+\n+#include <gtest/gtest.h>\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk_pass_pipeline.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+class FakeThunkPassBufferAllocator : public ThunkPassBufferAllocator {\n+ public:\n+  absl::StatusOr<BufferAllocation*> NewEmptyAllocation(int64_t size) override {\n+    if (CreatedAlloc()) {\n+      return absl::InvalidArgumentError(\"Expected only one allocation\");\n+    }\n+    alloc_ = std::make_unique<BufferAllocation>(0, size, 0);\n+    return alloc_.get();\n+  }\n+\n+  bool CreatedAlloc() { return alloc_ != nullptr; }\n+\n+ private:\n+  std::unique_ptr<BufferAllocation> alloc_;\n+};\n+\n+TEST(ThunkChecksumTracingPassTest, CreatesLogAlloc) {\n+  ThunkChecksumTracingPass pass;\n+  auto root_thunk = std::make_unique<SequentialThunk>(\n+      Thunk::ThunkInfo(), std::vector<std::unique_ptr<Thunk>>());\n+  DebugOptions debug_options;\n+  se::DeviceDescription device_info;\n+  FakeThunkPassBufferAllocator allocator;\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      bool changed,\n+      pass.Run(root_thunk.get(), debug_options, device_info, allocator));\n+  EXPECT_FALSE(changed);\n+  EXPECT_TRUE(allocator.CreatedAlloc());\n+}\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "1f768f8fd4d362c36521030f1e1e48c943253bea",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_pass_pipeline.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_pass_pipeline.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_pass_pipeline.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_pass_pipeline.cc?ref=b2c96769f094c3d81f5acd93ab9f557644db2ed3",
            "patch": "@@ -28,12 +28,13 @@ namespace gpu {\n \n absl::StatusOr<bool> ThunkPassPipeline::Run(\n     SequentialThunk* root_thunk, const DebugOptions& debug_options,\n-    const se::DeviceDescription& device_info) {\n+    const se::DeviceDescription& device_info,\n+    ThunkPassBufferAllocator& allocator) {\n   bool changed = false;\n   for (const auto& pass : passes_) {\n     VLOG(1) << \"Running ThunkPass: \" << pass->name();\n-    TF_ASSIGN_OR_RETURN(bool pass_changed,\n-                        pass->Run(root_thunk, debug_options, device_info));\n+    TF_ASSIGN_OR_RETURN(bool pass_changed, pass->Run(root_thunk, debug_options,\n+                                                     device_info, allocator));\n     changed |= pass_changed;\n   }\n   return changed;"
        },
        {
            "sha": "c12c138a4cb8f277831e284b2cc5408a6614045d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_pass_pipeline.h",
            "status": "modified",
            "additions": 20,
            "deletions": 4,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_pass_pipeline.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_pass_pipeline.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_pass_pipeline.h?ref=b2c96769f094c3d81f5acd93ab9f557644db2ed3",
            "patch": "@@ -16,26 +16,41 @@ limitations under the License.\n #ifndef XLA_BACKENDS_GPU_RUNTIME_THUNK_PASS_PIPELINE_H_\n #define XLA_BACKENDS_GPU_RUNTIME_THUNK_PASS_PIPELINE_H_\n \n+#include <cstdint>\n #include <memory>\n #include <string>\n #include <utility>\n #include <vector>\n \n+#include \"absl/base/nullability.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n+#include \"xla/service/buffer_assignment.h\"\n #include \"xla/stream_executor/device_description.h\"\n \n namespace xla {\n namespace gpu {\n \n+class ThunkPassBufferAllocator {\n+ public:\n+  virtual ~ThunkPassBufferAllocator() = default;\n+\n+  // Creates a new allocation of the provided size.\n+  // The ownership of the returned pointer stays with the\n+  // ThunkPassBufferAllocator object.\n+  virtual absl::StatusOr<BufferAllocation* absl_nonnull> NewEmptyAllocation(\n+      int64_t size) = 0;\n+};\n+\n class ThunkPassInterface {\n  public:\n   virtual ~ThunkPassInterface() = default;\n \n-  virtual absl::StatusOr<bool> Run(\n-      SequentialThunk* root_thunk, const DebugOptions& debug_options,\n-      const se::DeviceDescription& device_info) = 0;\n+  virtual absl::StatusOr<bool> Run(SequentialThunk* root_thunk,\n+                                   const DebugOptions& debug_options,\n+                                   const se::DeviceDescription& device_info,\n+                                   ThunkPassBufferAllocator& allocator) = 0;\n \n   virtual absl::string_view name() const = 0;\n };\n@@ -54,7 +69,8 @@ class ThunkPassPipeline : public ThunkPassInterface {\n   // Returns true if any pass changed the thunk tree.\n   absl::StatusOr<bool> Run(SequentialThunk* root_thunk,\n                            const DebugOptions& debug_options,\n-                           const se::DeviceDescription& device_info) override;\n+                           const se::DeviceDescription& device_info,\n+                           ThunkPassBufferAllocator& allocator) override;\n \n  private:\n   std::string name_;"
        },
        {
            "sha": "bbe0b058d7511cf0a465a15e30da6648e48c0902",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk_pass_pipeline_test.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 2,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_pass_pipeline_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_pass_pipeline_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk_pass_pipeline_test.cc?ref=b2c96769f094c3d81f5acd93ab9f557644db2ed3",
            "patch": "@@ -15,14 +15,18 @@ limitations under the License.\n \n #include \"xla/backends/gpu/runtime/thunk_pass_pipeline.h\"\n \n+#include <cstdint>\n #include <memory>\n #include <vector>\n \n #include <gtest/gtest.h>\n+#include \"absl/base/nullability.h\"\n+#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/service/buffer_assignment.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n@@ -36,13 +40,21 @@ class TestPass : public ThunkPassInterface {\n   absl::string_view name() const override { return \"test-pass\"; }\n   absl::StatusOr<bool> Run(SequentialThunk* root_thunk,\n                            const DebugOptions& debug_options,\n-                           const se::DeviceDescription& device_info) override {\n+                           const se::DeviceDescription& device_info,\n+                           ThunkPassBufferAllocator& /*allocator*/) override {\n     root_thunk->thunks().push_back(std::make_unique<SequentialThunk>(\n         Thunk::ThunkInfo(), std::vector<std::unique_ptr<Thunk>>()));\n     return true;\n   }\n };\n \n+class FakeThunkPassBufferAllocator : public ThunkPassBufferAllocator {\n+  absl::StatusOr<BufferAllocation* absl_nonnull> NewEmptyAllocation(\n+      int64_t size) override {\n+    return absl::UnimplementedError(\"NewEmptyAllocation is not implemented.\");\n+  }\n+};\n+\n TEST(ThunkPassPipelineTest, PipelineRunsPass) {\n   ThunkPassPipeline pipeline(\"test-pipeline\");\n   pipeline.AddPass(std::make_unique<TestPass>());\n@@ -51,11 +63,13 @@ TEST(ThunkPassPipelineTest, PipelineRunsPass) {\n       Thunk::ThunkInfo(), std::vector<std::unique_ptr<Thunk>>());\n   DebugOptions debug_options;\n   se::DeviceDescription device_info;\n+  FakeThunkPassBufferAllocator allocator;\n \n   EXPECT_EQ(root_thunk->thunks().size(), 0);\n \n   TF_ASSERT_OK_AND_ASSIGN(\n-      bool changed, pipeline.Run(root_thunk.get(), debug_options, device_info));\n+      bool changed,\n+      pipeline.Run(root_thunk.get(), debug_options, device_info, allocator));\n   EXPECT_TRUE(changed);\n   EXPECT_EQ(root_thunk->thunks().size(), 1);\n }"
        },
        {
            "sha": "a560b9923bbfcea3dc41575e0eb21c278625984f",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=b2c96769f094c3d81f5acd93ab9f557644db2ed3",
            "patch": "@@ -1498,7 +1498,6 @@ cc_library(\n     deps = [\n         \":buffer_assignment\",\n         \":computation_layout\",\n-        \":dump\",\n         \":hlo_execution_profile\",\n         \":hlo_module_config\",\n         \":hlo_proto_cc\","
        },
        {
            "sha": "f330be57638cd7aa08d8e29e36772a464d56353f",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=b2c96769f094c3d81f5acd93ab9f557644db2ed3",
            "patch": "@@ -635,6 +635,7 @@ cc_library(\n         \"//xla/backends/gpu/runtime:nvshmem_collective_thunk\",\n         \"//xla/backends/gpu/runtime:sequential_thunk\",\n         \"//xla/backends/gpu/runtime:thunk\",\n+        \"//xla/backends/gpu/runtime:thunk_checksum_tracing_pass\",\n         \"//xla/backends/gpu/runtime:thunk_pass_pipeline\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_assignment\","
        },
        {
            "sha": "df918fcfd6bbd3f9b905f2fe4414a8eda30d5a8a",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 67,
            "deletions": 16,
            "changes": 83,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=b2c96769f094c3d81f5acd93ab9f557644db2ed3",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n \n #include <algorithm>\n #include <cstdint>\n+#include <deque>\n #include <memory>\n #include <optional>\n #include <set>\n@@ -25,6 +26,7 @@ limitations under the License.\n #include <variant>\n #include <vector>\n \n+#include \"absl/base/nullability.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/container/inlined_vector.h\"\n@@ -43,6 +45,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/nvshmem_collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk_checksum_tracing_pass.h\"\n #include \"xla/backends/gpu/runtime/thunk_pass_pipeline.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/hlo/ir/hlo_input_output_alias_config.h\"\n@@ -99,27 +102,55 @@ namespace {\n \n // Chooses the correct allocations to be used within the GpuExecutable code.\n std::vector<const BufferAllocation*> GatherAllocationPtrs(\n-    const GpuExecutable::Params& params) {\n+    const GpuExecutable::Params& params,\n+    const std::deque<BufferAllocation>& thunk_pass_allocations) {\n   const std::vector<BufferAllocation>* allocation_vec = nullptr;\n   if (params.mlir_allocations.has_value()) {\n     allocation_vec = &params.mlir_allocations.value();\n   } else if (params.buffer_assignment != nullptr) {\n     allocation_vec = &params.buffer_assignment->Allocations();\n   }\n \n-  if (allocation_vec == nullptr) {\n-    return {};\n+  std::vector<const BufferAllocation*> alloc_ptrs;\n+  if (allocation_vec != nullptr) {\n+    alloc_ptrs.reserve(allocation_vec->size());\n+    for (const BufferAllocation& alloc : *allocation_vec) {\n+      alloc_ptrs.push_back(&alloc);\n+    }\n   }\n \n-  std::vector<const BufferAllocation*> alloc_ptrs;\n-  alloc_ptrs.reserve(allocation_vec->size());\n-  for (const BufferAllocation& alloc : *allocation_vec) {\n-    alloc_ptrs.push_back(&alloc);\n+  if (!thunk_pass_allocations.empty()) {\n+    alloc_ptrs.reserve(alloc_ptrs.size() + thunk_pass_allocations.size());\n+    for (const BufferAllocation& alloc : thunk_pass_allocations) {\n+      alloc_ptrs.push_back(&alloc);\n+    }\n   }\n \n   return alloc_ptrs;\n }\n \n+class GpuExecutableThunkPassBufferAllocator : public ThunkPassBufferAllocator {\n+ public:\n+  ~GpuExecutableThunkPassBufferAllocator() override = default;\n+\n+  explicit GpuExecutableThunkPassBufferAllocator(\n+      BufferAllocation::Index start_idx)\n+      : next_idx_(start_idx) {}\n+\n+  absl::StatusOr<BufferAllocation* absl_nonnull> NewEmptyAllocation(\n+      int64_t size) override {\n+    allocations_.push_back(BufferAllocation(next_idx_++, size, /*color=*/0));\n+    return &allocations_.back();\n+  }\n+\n+  std::deque<BufferAllocation>& MutableAllocations() { return allocations_; }\n+\n+ private:\n+  BufferAllocation::Index next_idx_ = 0;\n+  // std::deque is used to ensure pointer stability.\n+  std::deque<BufferAllocation> allocations_;\n+};\n+\n }  // namespace\n \n using ::tsl::profiler::ScopedAnnotation;\n@@ -140,14 +171,19 @@ static absl::flat_hash_set<ExecutionStreamId> GetExecutionStreamIds(\n \n static absl::Status RunThunkPasses(const DebugOptions& debug_options,\n                                    const se::DeviceDescription& device_info,\n+                                   bool enable_experimental_checksum_pass,\n                                    SequentialThunk* root_thunk,\n-                                   HloModule* hlo_module) {\n+                                   HloModule* hlo_module,\n+                                   ThunkPassBufferAllocator& allocator) {\n   ThunkPassPipeline pipeline(\"thunk-passes\");\n+  if (enable_experimental_checksum_pass) {\n+    pipeline.AddPass(std::make_unique<ThunkChecksumTracingPass>());\n+  }\n   if (debug_options.xla_gpu_experimental_enable_command_buffer_on_thunks()) {\n     pipeline.AddPass(std::make_unique<CommandBufferConversionPass>());\n   }\n-  TF_ASSIGN_OR_RETURN(bool changed,\n-                      pipeline.Run(root_thunk, debug_options, device_info));\n+  TF_ASSIGN_OR_RETURN(bool changed, pipeline.Run(root_thunk, debug_options,\n+                                                 device_info, allocator));\n   if (changed) {\n     VLOG(3) << \"Thunk passes changed the thunk tree.\";\n     if (hlo_module && DumpingEnabledForHloModule(*hlo_module)) {\n@@ -162,15 +198,29 @@ static absl::Status RunThunkPasses(const DebugOptions& debug_options,\n \n absl::StatusOr<std::unique_ptr<GpuExecutable>> GpuExecutable::Create(\n     Params params) {\n-  TF_RETURN_IF_ERROR(\n-      RunThunkPasses(params.debug_options, params.device_description,\n-                     params.executable.get(), params.debug_module.get()));\n-  return std::unique_ptr<GpuExecutable>(new GpuExecutable(std::move(params)));\n+  int64_t next_idx = 0;\n+  if (params.mlir_allocations.has_value()) {\n+    next_idx = params.mlir_allocations->size();\n+  } else if (params.buffer_assignment != nullptr) {\n+    next_idx = params.buffer_assignment->Allocations().size();\n+  }\n+\n+  GpuExecutableThunkPassBufferAllocator allocator(next_idx);\n+\n+  TF_RETURN_IF_ERROR(RunThunkPasses(\n+      params.debug_options, params.device_description,\n+      params.enable_experimental_checksum_pass, params.executable.get(),\n+      params.debug_module.get(), allocator));\n+\n+  return std::unique_ptr<GpuExecutable>(new GpuExecutable(\n+      std::move(params), std::move(allocator.MutableAllocations())));\n }\n \n // Implementation note: HLO profiling is always enabled for GPU executables,\n // since we can use timers around thunks.\n-GpuExecutable::GpuExecutable(GpuExecutable::Params params)\n+GpuExecutable::GpuExecutable(\n+    GpuExecutable::Params params,\n+    std::deque<BufferAllocation> thunk_pass_allocations)\n     : Executable(std::move(params.debug_module)),\n       text_(std::move(params.asm_text)),\n       binary_(std::move(params.binary)),\n@@ -180,9 +230,10 @@ GpuExecutable::GpuExecutable(GpuExecutable::Params params)\n       execution_stream_ids_(GetExecutionStreamIds(*thunks_)),\n       module_name_(params.module_name),\n       program_shape_(params.program_shape),\n-      allocation_ptrs_(GatherAllocationPtrs(params)),\n+      allocation_ptrs_(GatherAllocationPtrs(params, thunk_pass_allocations)),\n       allocations_(std::move(params.mlir_allocations)),\n       buffer_assignment_(std::move(params.buffer_assignment)),\n+      thunk_pass_allocations_(std::move(thunk_pass_allocations)),\n       alias_info_(std::move(params.alias_info)),\n       debug_buffer_assignment_show_max_(\n           params.debug_options.xla_debug_buffer_assignment_show_max()),"
        },
        {
            "sha": "e282caf693b517cefd76406a2b32aa55ce3dadb5",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.h",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h?ref=b2c96769f094c3d81f5acd93ab9f557644db2ed3",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n #define XLA_SERVICE_GPU_GPU_EXECUTABLE_H_\n \n #include <cstdint>\n+#include <deque>\n #include <memory>\n #include <optional>\n #include <string>\n@@ -112,6 +113,10 @@ class GpuExecutable : public Executable {\n     se::DeviceDescription device_description;\n     std::unique_ptr<HloModule> debug_module = nullptr;\n     bool enable_debug_info_manager = true;\n+    // TODO: b/444183764 - Guard by a flag instead of param.\n+    // For now the pass is so experimental it's not supposed to be possible\n+    // to enable.\n+    bool enable_experimental_checksum_pass = false;\n   };\n \n   static absl::StatusOr<std::unique_ptr<GpuExecutable>> Create(Params params);\n@@ -211,7 +216,8 @@ class GpuExecutable : public Executable {\n \n  private:\n   // Use GpuExecutable::Create() to create an instance.\n-  explicit GpuExecutable(Params params);\n+  explicit GpuExecutable(Params params,\n+                         std::deque<BufferAllocation> thunk_pass_allocations);\n \n   // GpuExecutable check with either AMD's ISA version, or Nvidia's major minor\n   // version for compute capability, depending on the hardware.\n@@ -290,6 +296,11 @@ class GpuExecutable : public Executable {\n   // This object is also used for dumping debug info.\n   std::shared_ptr<const xla::BufferAssignment> buffer_assignment_;\n \n+  // Extra allocations added by thunk passes outside of the normal buffer\n+  // assignment process.\n+  // std::deque is used to ensure pointer stability.\n+  const std::deque<BufferAllocation> thunk_pass_allocations_;\n+\n   // Backend specific aliasing information whether operands can/should share the\n   // buffer with the user.\n   std::unique_ptr<GpuAliasInfo> alias_info_;"
        },
        {
            "sha": "45254a6266bf56eda1239867145501870a34c687",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable_test.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b2c96769f094c3d81f5acd93ab9f557644db2ed3/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc?ref=b2c96769f094c3d81f5acd93ab9f557644db2ed3",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/service/gpu/gpu_executable.h\"\n \n+#include <cstddef>\n #include <memory>\n #include <optional>\n #include <string>\n@@ -296,5 +297,28 @@ TEST(GpuExecutableTest, MlirAllocationsArePreferred) {\n               ElementsAre(expected_ptr0, expected_ptr1));\n }\n \n+TEST(GpuExecutableTest, ThunkChecksumPassAddsAllocation) {\n+  GpuExecutable::Params params_without_pass;\n+  params_without_pass.executable =\n+      std::make_unique<SequentialThunk>(Thunk::ThunkInfo{}, ThunkSequence{});\n+  params_without_pass.enable_experimental_checksum_pass = false;\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<GpuExecutable> executable_without_pass,\n+      GpuExecutable::Create(std::move(params_without_pass)));\n+  size_t allocations_without_pass =\n+      executable_without_pass->GetAllocations().size();\n+\n+  GpuExecutable::Params params_with_pass;\n+  params_with_pass.executable =\n+      std::make_unique<SequentialThunk>(Thunk::ThunkInfo{}, ThunkSequence{});\n+  params_with_pass.enable_experimental_checksum_pass = true;\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<GpuExecutable> executable_with_pass,\n+                          GpuExecutable::Create(std::move(params_with_pass)));\n+  EXPECT_EQ(executable_with_pass->GetAllocations().size(),\n+            allocations_without_pass + 1);\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 441,
        "additions": 388,
        "deletions": 53
    }
}