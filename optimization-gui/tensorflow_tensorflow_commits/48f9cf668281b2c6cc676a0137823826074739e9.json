{
    "author": "yashk2810",
    "message": "Allow reduced on the fwd pass to jnp.sin even though unreduced is not allowed as an input to sin since sin is a non-linear op.\n\n**Then why allow reduced to a non-linear op?**\n\nWhen unreduced is introduced on the backward pass (since it's the cotangent type of reduced), there will be no non-linearity present!\n\nLet's take the sin example. JAX will do the `cos` on the primal side and the tangent computation will only contain `mul(g, cos_residual)` and `mul` is a bilinear op which can ingest unreduced on one of the operands.\n\n**Those were a lot of words, let's look at a concrete example and see what the forward and backward pass will look like**\n\nConsider this example:\n\n```\n@jax.jit\ndef f(x: f32[4, 2]{R: x}, y: f32[2, 8@x]):\n  x_: f32[4, 2]{R: x} = jnp.sin(x)\n  z: f32[4, 8@x] = x_ @ y\n  return z.sum()\n```\n\nThe backward pass would look like this if you do: `jit(grad(f))`\n\n```\ndef f_bwd(res, dz):\n  (cos_x: f32[4, 2]{R:x},) = res\n  dx_: f32[4, 2]{U: x} = dz: f32[4, 8@x] @ y.T: f32[8@x, 2]\n  dx:  f32[4, 2]{U: x} = mul(dx_, cos_x)\n  return dx\n```\n\nAs you can see `mul` gets 2 inputs `dx_` and `cos_x` (which is the residual). Since mul is bilinear, if one of the inputs is unreduced along a mesh axis, then the other input **has to be reduced** along the same mesh axis.\n\nThere's another thing to note: After grad, the type of `dx` will be `f32[4, 2]{U: x}` which makes sense because the type of `x` was `f32[4, 2]{R: x}`!\n\nPiperOrigin-RevId: 829096082",
    "sha": "48f9cf668281b2c6cc676a0137823826074739e9",
    "files": [
        {
            "sha": "2aa5ea09a6814d1105a6b031c77814dda581549a",
            "filename": "third_party/xla/xla/python/version.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/48f9cf668281b2c6cc676a0137823826074739e9/third_party%2Fxla%2Fxla%2Fpython%2Fversion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/48f9cf668281b2c6cc676a0137823826074739e9/third_party%2Fxla%2Fxla%2Fpython%2Fversion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fversion.h?ref=48f9cf668281b2c6cc676a0137823826074739e9",
            "patch": "@@ -19,6 +19,6 @@ limitations under the License.\n // An increasing version number to protect jax code against breaking changes.\n // In JAX, reference this via jax._src.lib.ifrt_version.\n #define JAX_IFRT_VERSION_NUMBER \\\n-  35  // Explicit `has_custom_layout` argument in PjRt-IFRT Array creation.\n+  36  // Explicit `has_custom_layout` argument in PjRt-IFRT Array creation.\n \n #endif  // XLA_PYTHON_VERSION_H_"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}