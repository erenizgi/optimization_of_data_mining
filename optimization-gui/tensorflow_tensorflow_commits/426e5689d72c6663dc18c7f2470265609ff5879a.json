{
    "author": "akuegel",
    "message": "[XLA:GPU] Double the maximum unroll factor on Blackwell.\n\nOn Blackwell we need to read more memory at once to get good performance.\nMore unrolling achieves this. Guard this change with some additional conditions\nand a flag, as more unrolling may cause performance issues that are not offset\nwith more vectorization.\n\nPiperOrigin-RevId: 833246287",
    "sha": "426e5689d72c6663dc18c7f2470265609ff5879a",
    "files": [
        {
            "sha": "3d1546fa00f1c25b9bb0795f7bf4afd8d2030c17",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/426e5689d72c6663dc18c7f2470265609ff5879a/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/426e5689d72c6663dc18c7f2470265609ff5879a/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=426e5689d72c6663dc18c7f2470265609ff5879a",
            "patch": "@@ -457,6 +457,7 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_gpu_unsupported_enable_all_reduce_decomposer(false);\n   opts.set_xla_gpu_experimental_use_autotuner_pass(false);\n   opts.set_xla_gpu_experimental_enable_fusion_autotuner(true);\n+  opts.set_xla_gpu_experimental_allow_unroll_factor_eight(false);\n   opts.set_xla_gpu_experimental_pack_dot_operands_along_k_dimension(true);\n   opts.set_xla_unsupported_crash_on_hlo_pass_fix_max_iterations(false);\n   opts.set_xla_hlo_pass_fix_detect_cycles(false);\n@@ -2642,6 +2643,12 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n       debug_options->xla_gpu_experimental_use_autotuner_pass(),\n       \"If true, use the AutotunerPass to autotune fusions, instead of the \"\n       \"gemm_fusion_autotuner.\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_experimental_allow_unroll_factor_eight\",\n+      bool_setter_for(\n+          &DebugOptions::set_xla_gpu_experimental_allow_unroll_factor_eight),\n+      debug_options->xla_gpu_experimental_allow_unroll_factor_eight(),\n+      \"If true, allows unroll factor 8 on Blackwell architectures.\"));\n   flag_list->push_back(\n       tsl::Flag(\"xla_detect_unstable_reductions\",\n                 setter_for_xla_detect_unstable_reductions,"
        },
        {
            "sha": "a77e86a4fd7be666a3c46756aa779c21656c3b2e",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/426e5689d72c6663dc18c7f2470265609ff5879a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/426e5689d72c6663dc18c7f2470265609ff5879a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=426e5689d72c6663dc18c7f2470265609ff5879a",
            "patch": "@@ -2944,7 +2944,10 @@ xla_cc_test(\n         \"nomsan\",\n     ],\n     deps = [\n+        \":gpu_device_info_for_tests\",\n         \":gpu_fusible\",\n+        \":hlo_fusion_analysis\",\n+        \"//xla:debug_options_flags\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/parser:hlo_parser\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\","
        },
        {
            "sha": "2f3388cd5ca0b1bd59e143d69e26c95ce242b840",
            "filename": "third_party/xla/xla/service/gpu/gpu_fusible.cc",
            "status": "modified",
            "additions": 67,
            "deletions": 3,
            "changes": 70,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/426e5689d72c6663dc18c7f2470265609ff5879a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/426e5689d72c6663dc18c7f2470265609ff5879a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible.cc?ref=426e5689d72c6663dc18c7f2470265609ff5879a",
            "patch": "@@ -73,8 +73,8 @@ const Shape& GetElementShape(const HloFusionAnalysis& analysis) {\n }\n \n // Computes the maximum valid unroll factor for a given instruction.\n-int ComputeMaxUnrollFactor(int64_t num_elements) {\n-  for (int i = MaxUnrollFactor(); i > 1; i /= 2) {\n+int ComputeMaxUnrollFactor(int64_t num_elements, int64_t max_unroll) {\n+  for (int i = max_unroll; i > 1; i /= 2) {\n     if (num_elements % i == 0) {\n       return i;\n     }\n@@ -930,6 +930,21 @@ LaunchDimensionsConfig ComputeLoopFusionConfig(\n   return ComputeLoopFusionConfig(analysis, GetElementShape(analysis));\n }\n \n+namespace {\n+bool ContainsTransposeWithSmallMostMinorDim(const HloFusionAdaptor& fusion,\n+                                            int64_t unroll_factor) {\n+  return HloAnyOf(fusion, [unroll_factor](HloInstructionAdaptor instr) {\n+    if (instr.opcode() != HloOpcode::kTranspose) {\n+      return false;\n+    }\n+    const HloInstruction& transpose = instr.instruction();\n+    // We can assume that TransposeDimensionGrouper pass has run, so no need\n+    // to try to combine adjacent dimensions.\n+    return transpose.shape().dimensions().back() < unroll_factor;\n+  });\n+}\n+}  // namespace\n+\n LaunchDimensionsConfig ComputeLoopFusionConfig(\n     const HloFusionAnalysis& analysis, const Shape& element_shape) {\n   int unroll_factor = 1;\n@@ -944,7 +959,56 @@ LaunchDimensionsConfig ComputeLoopFusionConfig(\n                           analysis.device_info().core_count();\n   if (num_elements >= n_threads_max &&\n       !MayCausePerformanceDropIfUnrolled(analysis.fusion())) {\n-    unroll_factor = ComputeMaxUnrollFactor(num_elements);\n+    int64_t max_unroll = MaxUnrollFactor();\n+    // On Blackwell we would like to increase the maximum unroll factor to 8, as\n+    // we need more vectorization for full performance.\n+    // However we need to check additional conditions:\n+    //   - Unrolling is potentially bad for fusions with reductions, where one\n+    //     thread will handle the full reduction dimension, so more unrolling\n+    //     can hurt parallelism.\n+    //   - Unrolling is potentially bad for fusions with many outputs, as that\n+    //     might increase register pressure. A thread needs to compute all the\n+    //     outputs first before it can write them due to potential in-place\n+    //     buffers. More unrolling will increase the number of values that need\n+    //     to be computed before writing.\n+    //   - Unrolling is potentially bad for transposes if the most minor\n+    //     dimension of transpose is smaller than the unroll factor. This could\n+    //     potentially be checked with indexing analysis as well, but it is\n+    //     tricky to get the conditions right when bad or unknown indexing\n+    //     should block more unrolling or not. For now, let's keep it simple and\n+    //     only check for transpose.\n+\n+    // For now, don't allow any multi-output fusions. However register pressure\n+    // also does not only depend on the number of outputs, so we might hit it\n+    // also for single fusions, or there could be multi-output fusions that\n+    // don't face register pressure. This part of the heuristic may need\n+    // improvements.\n+    constexpr int kMaxNumOutputsForFullUnrolling = 1;\n+    // On PTX level, we can vectorize with v4.b32, but not with v8.b32. So\n+    // higher unroll factor does not make sense with 32 bit or more.\n+    constexpr int kMaxBitsToVectorizeWithVectorSize4 = 32;\n+    if (analysis.device_info().cuda_compute_capability().IsBlackwell() &&\n+        analysis.emitter_fusion_kind() ==\n+            HloFusionAnalysis::EmitterFusionKind::kLoop &&\n+        analysis.input_output_info().smallest_output_dtype_bits <\n+            kMaxBitsToVectorizeWithVectorSize4 &&\n+        analysis.fusion_root_count() <= kMaxNumOutputsForFullUnrolling &&\n+        analysis.fusion_root(0)\n+            .instruction()\n+            .GetModule()\n+            ->config()\n+            .debug_options()\n+            .xla_gpu_experimental_allow_unroll_factor_eight() &&\n+        !HloAnyOf(\n+            analysis.fusion(),\n+            [](HloInstructionAdaptor node) {\n+              return node.opcode() == HloOpcode::kReduce;\n+            }) &&\n+        !ContainsTransposeWithSmallMostMinorDim(analysis.fusion(),\n+                                                max_unroll * 2)) {\n+      max_unroll *= 2;\n+    }\n+    unroll_factor = ComputeMaxUnrollFactor(num_elements, max_unroll);\n   }\n   // CHECK that unroll_factor is a power-of-2, as needed by the logic below.\n   CHECK(absl::has_single_bit(static_cast<uint64_t>(unroll_factor)));"
        },
        {
            "sha": "3b7796d553cd8c8f4bd55c062db50b012c2e93df",
            "filename": "third_party/xla/xla/service/gpu/gpu_fusible_test.cc",
            "status": "modified",
            "additions": 131,
            "deletions": 0,
            "changes": 131,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/426e5689d72c6663dc18c7f2470265609ff5879a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/426e5689d72c6663dc18c7f2470265609ff5879a/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_fusible_test.cc?ref=426e5689d72c6663dc18c7f2470265609ff5879a",
            "patch": "@@ -20,11 +20,14 @@ limitations under the License.\n \n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n+#include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/utils/hlo_traversal.h\"\n+#include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/instruction_fusion.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -51,6 +54,12 @@ class GpuFusibleTest : public HloHardwareIndependentTestBase {\n     TF_ASSERT_OK_AND_ASSIGN(device_description_, MakeDeviceDescription());\n   }\n \n+  DebugOptions GetDebugOptionsForTest() const override {\n+    auto debug_options = GetDebugOptionsFromFlags();\n+    debug_options.set_xla_gpu_experimental_allow_unroll_factor_eight(true);\n+    return debug_options;\n+  }\n+\n   bool IsReduceInputFusion(const HloInstruction& instr) const {\n     return ::xla::gpu::IsReduceInputFusion(instr, device_description_);\n   }\n@@ -1668,6 +1677,128 @@ ENTRY main {\n   EXPECT_TRUE(MayCausePerformanceDropIfUnrolled(*fusion_adaptor));\n }\n \n+TEST_F(GpuFusibleTest, ComputeLoopFusionConfig) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+HloModule m\n+\n+ENTRY main {\n+  p0 = f16[2048,1024]{1,0} parameter(0)\n+  ROOT res = f16[2048,1024]{1,0} negate(p0)\n+}\n+)\"));\n+  const HloInstruction* root = module->entry_computation()->root_instruction();\n+  se::DeviceDescription device_info_h100{\n+      TestGpuDeviceInfo::RTXH100SXMDeviceInfo()};\n+  auto analysis = HloFusionAnalysis::Create(*root, device_info_h100);\n+  auto config = ComputeLoopFusionConfig(analysis, root->shape());\n+  EXPECT_EQ(config.unroll_factor, 4);\n+\n+  se::DeviceDescription device_info_b200{\n+      TestGpuDeviceInfo::RTXB200SXMDeviceInfo()};\n+  analysis = HloFusionAnalysis::Create(*root, device_info_b200);\n+  config = ComputeLoopFusionConfig(analysis, root->shape());\n+  EXPECT_EQ(config.unroll_factor, 8);\n+}\n+\n+TEST_F(GpuFusibleTest, ComputeLoopFusionConfig32Bit) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+HloModule m\n+\n+ENTRY main {\n+  p0 = f32[2048,1024]{1,0} parameter(0)\n+  ROOT res = f32[2048,1024]{1,0} negate(p0)\n+}\n+)\"));\n+  const HloInstruction* root = module->entry_computation()->root_instruction();\n+  se::DeviceDescription device_info_h100{\n+      TestGpuDeviceInfo::RTXH100SXMDeviceInfo()};\n+  auto analysis = HloFusionAnalysis::Create(*root, device_info_h100);\n+  auto config = ComputeLoopFusionConfig(analysis, root->shape());\n+  EXPECT_EQ(config.unroll_factor, 4);\n+\n+  se::DeviceDescription device_info_b200{\n+      TestGpuDeviceInfo::RTXB200SXMDeviceInfo()};\n+  analysis = HloFusionAnalysis::Create(*root, device_info_b200);\n+  config = ComputeLoopFusionConfig(analysis, root->shape());\n+  EXPECT_EQ(config.unroll_factor, 4);\n+}\n+\n+TEST_F(GpuFusibleTest, ComputeLoopFusionConfigForLoopReduce) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+HloModule m\n+\n+max {\n+  p0 = f16[] parameter(0)\n+  p1 = f16[] parameter(1)\n+  ROOT add = f16[] maximum(p0, p1)\n+}\n+\n+ENTRY main {\n+  p0 = f16[270336,8]{1,0} parameter(0)\n+  neg_inf = f16[] constant(-inf)\n+  ROOT res = f16[270336]{0} reduce(p0, neg_inf), dimensions={1}, to_apply=max\n+}\n+)\"));\n+  const HloInstruction* root = module->entry_computation()->root_instruction();\n+  se::DeviceDescription device_info_h100{\n+      TestGpuDeviceInfo::RTXH100SXMDeviceInfo()};\n+  auto analysis = HloFusionAnalysis::Create(*root, device_info_h100);\n+  auto config = ComputeLoopFusionConfig(analysis, root->shape());\n+  EXPECT_EQ(config.unroll_factor, 4);\n+\n+  se::DeviceDescription device_info_b200{\n+      TestGpuDeviceInfo::RTXB200SXMDeviceInfo()};\n+  analysis = HloFusionAnalysis::Create(*root, device_info_b200);\n+  config = ComputeLoopFusionConfig(analysis, root->shape());\n+  EXPECT_EQ(config.unroll_factor, 4);\n+}\n+\n+TEST_F(GpuFusibleTest, ComputeLoopFusionConfigForLoopTransposeSmallMinorDim) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+HloModule m\n+\n+ENTRY main {\n+  p0 = f16[256,2048,4]{2,1,0} parameter(0)\n+  ROOT res = f16[2048,256,4]{2,1,0} transpose(p0), dimensions={1,0,2}\n+}\n+)\"));\n+  const HloInstruction* root = module->entry_computation()->root_instruction();\n+  se::DeviceDescription device_info_h100{\n+      TestGpuDeviceInfo::RTXH100SXMDeviceInfo()};\n+  auto analysis = HloFusionAnalysis::Create(*root, device_info_h100);\n+  auto config = ComputeLoopFusionConfig(analysis, root->shape());\n+  EXPECT_EQ(config.unroll_factor, 4);\n+\n+  se::DeviceDescription device_info_b200{\n+      TestGpuDeviceInfo::RTXB200SXMDeviceInfo()};\n+  analysis = HloFusionAnalysis::Create(*root, device_info_b200);\n+  config = ComputeLoopFusionConfig(analysis, root->shape());\n+  EXPECT_EQ(config.unroll_factor, 4);\n+}\n+\n+TEST_F(GpuFusibleTest, ComputeLoopFusionConfigForLoopTransposeLargerMinorDim) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+HloModule m\n+\n+ENTRY main {\n+  p0 = f16[256,2048,8]{2,1,0} parameter(0)\n+  ROOT res = f16[2048,256,8]{2,1,0} transpose(p0), dimensions={1,0,2}\n+}\n+)\"));\n+  const HloInstruction* root = module->entry_computation()->root_instruction();\n+  se::DeviceDescription device_info_h100{\n+      TestGpuDeviceInfo::RTXH100SXMDeviceInfo()};\n+  auto analysis = HloFusionAnalysis::Create(*root, device_info_h100);\n+  auto config = ComputeLoopFusionConfig(analysis, root->shape());\n+  EXPECT_EQ(config.unroll_factor, 4);\n+\n+  se::DeviceDescription device_info_b200{\n+      TestGpuDeviceInfo::RTXB200SXMDeviceInfo()};\n+  analysis = HloFusionAnalysis::Create(*root, device_info_b200);\n+  config = ComputeLoopFusionConfig(analysis, root->shape());\n+  EXPECT_EQ(config.unroll_factor, 8);\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "357d37a20351f7d1be2686e7e08f49179eda32f4",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/426e5689d72c6663dc18c7f2470265609ff5879a/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/426e5689d72c6663dc18c7f2470265609ff5879a/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=426e5689d72c6663dc18c7f2470265609ff5879a",
            "patch": "@@ -625,6 +625,12 @@ message DebugOptions {\n \n   optional bool xla_gpu_exhaustive_tiling_search = 219;\n \n+  // If true, allows unroll factor 8 on Blackwell architectures. This is also\n+  // guarded with a heuristic, but the heuristic is not perfect, so enabling\n+  // this flag can cause both performance improvements and performance\n+  // regressions.\n+  optional bool xla_gpu_experimental_allow_unroll_factor_eight = 430;\n+\n   // Specifies the behavior of per kernel autotuning cache.\n   optional AutotuneCacheMode xla_gpu_experimental_autotune_cache_mode = 324;\n \n@@ -1420,7 +1426,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 429\n+  // Next id: 431\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 219,
        "additions": 215,
        "deletions": 4
    }
}