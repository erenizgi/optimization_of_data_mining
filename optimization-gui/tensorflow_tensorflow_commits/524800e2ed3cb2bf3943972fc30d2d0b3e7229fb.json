{
    "author": "beckerhe",
    "message": "Introduce NullableShapedSlice\n\n`NullableShapedSlice` is a terrible name and a replacement for `std::optional<ShapedSlice>` which is used in a bunch of places.\n\nThe main difference is that `NullableShapedSlice` has `ToProto` and `FromProto` functions and is therefore serializable. The type is needed because protobuf doesn't allow optional fields in all cases (thanks protobuf!), therefore we can't always use `optional ShapedSliceProto` to model a `std::optional<ShapedSlice>`.\n\nThe change also moves `ShapedSlice` into its own file and adds missing unit tests.\n\nPiperOrigin-RevId: 827908703",
    "sha": "524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
    "files": [
        {
            "sha": "3cc85e015d6490dbed346336b42586afbd93bb62",
            "filename": "third_party/xla/xla/backends/gpu/codegen/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2FBUILD?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -137,6 +137,7 @@ cc_library(\n         \"//xla/backends/gpu/runtime:dynamic_slice_thunk\",\n         \"//xla/backends/gpu/runtime:gemm_thunk\",\n         \"//xla/backends/gpu/runtime:kernel_thunk\",\n+        \"//xla/backends/gpu/runtime:shaped_slice\",\n         \"//xla/backends/gpu/runtime:thunk\",\n         \"//xla/codegen/emitters:kernel_arguments\",\n         \"//xla/ffi:attribute_map\","
        },
        {
            "sha": "969bdd6d800fcf2b96c91312453bceda32e0d974",
            "filename": "third_party/xla/xla/backends/gpu/codegen/custom.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fcustom.cc?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -36,7 +36,6 @@ limitations under the License.\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n-#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/runtime/all_reduce_thunk.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n@@ -45,6 +44,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/dynamic_slice_thunk.h\"\n #include \"xla/backends/gpu/runtime/gemm_thunk.h\"\n #include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/codegen/emitters/kernel_arguments.h\"\n #include \"xla/ffi/attribute_map.h\"\n@@ -762,7 +762,7 @@ absl::StatusOr<FusionEmissionResult> EmitCustomCall(\n         \"thunks\");\n   }\n \n-  using Slices = std::vector<std::optional<ShapedSlice>>;\n+  using Slices = std::vector<NullableShapedSlice>;\n \n   int64_t num_args = ShapeUtil::GetLeafCount(custom_call.shape());\n   absl::c_for_each(custom_call.operands(), [&](auto* operand) {"
        },
        {
            "sha": "036e372d0392f82dcd22a40ea262571cfe156505",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 48,
            "deletions": 0,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -68,6 +68,7 @@ cc_library(\n         \":custom_call_thunk\",\n         \":dynamic_slice_thunk\",\n         \":gpublas_lt_matmul_thunk\",\n+        \":shaped_slice\",\n         \":thunk\",\n         \":while_thunk\",\n         \"//xla:debug_options_flags\",\n@@ -268,6 +269,7 @@ xla_test(\n         \":dynamic_slice_thunk_proto_cc\",\n         \":gemm_thunk\",\n         \":sequential_thunk\",\n+        \":shaped_slice\",\n         \":thunk\",\n         \":thunk_proto_deserialization\",\n         \"//xla:shape_util\",\n@@ -695,6 +697,7 @@ cc_library(\n     hdrs = [\"custom_call_thunk.h\"],\n     deps = [\n         \":custom_call_target\",\n+        \":shaped_slice\",\n         \":thunk\",\n         \"//xla:executable_run_options\",\n         \"//xla:shape_util\",\n@@ -738,6 +741,7 @@ xla_test(\n     backends = [\"gpu\"],\n     deps = [\n         \":custom_call_thunk\",\n+        \":shaped_slice\",\n         \":thunk\",\n         \"//xla:executable_run_options\",\n         \"//xla/ffi\",\n@@ -919,6 +923,7 @@ cc_library(\n     srcs = [\"infeed_thunk.cc\"],\n     hdrs = [\"infeed_thunk.h\"],\n     deps = [\n+        \":shaped_slice\",\n         \":thunk\",\n         \"//xla:shape_tree\",\n         \"//xla:shape_util\",\n@@ -1730,6 +1735,7 @@ cc_library(\n     srcs = [\"outfeed_thunk.cc\"],\n     hdrs = [\"outfeed_thunk.h\"],\n     deps = [\n+        \":shaped_slice\",\n         \":thunk\",\n         \"//xla:shape_tree\",\n         \"//xla:shape_util\",\n@@ -1924,6 +1930,46 @@ xla_cc_test(\n     ],\n )\n \n+tf_proto_library(\n+    name = \"shaped_slice_proto\",\n+    srcs = [\"shaped_slice.proto\"],\n+    protodeps = [\n+        # keep sorted\n+        \"//xla:xla_data_proto\",\n+        \"//xla/service:buffer_assignment_proto\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"shaped_slice\",\n+    srcs = [\"shaped_slice.cc\"],\n+    hdrs = [\"shaped_slice.h\"],\n+    deps = [\n+        \":shaped_slice_proto_cc\",\n+        \"//xla:shape_util\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"shaped_slice_test\",\n+    srcs = [\"shaped_slice_test.cc\"],\n+    deps = [\n+        \":shaped_slice\",\n+        \":shaped_slice_proto_cc\",\n+        \"//xla:shape_util\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/tsl/util/proto:parse_text_proto\",\n+        \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n xla_cc_test(\n     name = \"for_all_thunks_test\",\n     srcs = [\"for_all_thunks_test.cc\"],\n@@ -2445,6 +2491,7 @@ tf_proto_library(\n         # keep sorted\n         \":convolution_filter_thunk_proto\",\n         \":dynamic_slice_thunk_proto\",\n+        \":shaped_slice_proto\",\n         \"//xla:xla_data_proto\",\n         \"//xla/core/host_offloading:host_offloading_executable_proto\",\n         \"//xla/service:buffer_assignment_proto\",\n@@ -2696,6 +2743,7 @@ cc_library(\n     srcs = [\"host_execute_thunk.cc\"],\n     hdrs = [\"host_execute_thunk.h\"],\n     deps = [\n+        \":shaped_slice_proto_cc\",\n         \":thunk\",\n         \":thunk_proto_cc\",\n         \"//xla:executable_run_options\","
        },
        {
            "sha": "834ccbce6e0e1c864ac0a1614450dbce9183b91f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -56,6 +56,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n #include \"xla/backends/gpu/runtime/dynamic_slice_thunk.h\"\n #include \"xla/backends/gpu/runtime/gpublas_lt_matmul_thunk.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/while_thunk.h\"\n #include \"xla/debug_options_flags.h\"\n@@ -1833,7 +1834,7 @@ namespace {\n // Records each buffer associated with each slice into the provided vector.\n // Returns an error if any of the slices is missing a buffer allocation.\n absl::Status GetBuffers(const Thunk::ExecuteParams& execute_params,\n-                        absl::Span<const std::optional<ShapedSlice>> slices,\n+                        absl::Span<const NullableShapedSlice> slices,\n                         std::vector<void*>& buffers, absl::string_view label) {\n   for (int i = 0; i < slices.size(); ++i) {\n     if (!slices[i].has_value()) {\n@@ -1916,7 +1917,7 @@ CustomCallCmd::RecordXlaFfiCall(const Thunk::ExecuteParams& execute_params,\n   arguments.reserve(operands_.size());\n \n   for (int i = 0; i < operands_.size(); ++i) {\n-    const std::optional<ShapedSlice>& slice = operands_[i];\n+    const NullableShapedSlice& slice = operands_[i];\n     if (!slice.has_value()) {\n       arguments.push_back(se::DeviceMemoryBase{});\n       continue;\n@@ -1933,7 +1934,7 @@ CustomCallCmd::RecordXlaFfiCall(const Thunk::ExecuteParams& execute_params,\n   results.reserve(results_.size());\n \n   for (int i = 0; i < results_.size(); ++i) {\n-    const std::optional<ShapedSlice>& slice = results_[i];\n+    const NullableShapedSlice& slice = results_[i];\n     if (!slice.has_value()) {\n       results.push_back(se::DeviceMemoryBase{});\n       continue;"
        },
        {
            "sha": "685323f32f0d4d23e0315fbe050f169f56928022",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.h",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.h?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -42,6 +42,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/custom_call_thunk.h\"\n #include \"xla/backends/gpu/runtime/dynamic_slice_thunk.h\"\n #include \"xla/backends/gpu/runtime/gpublas_lt_matmul_thunk.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/ffi/api/c_api.h\"\n #include \"xla/ffi/attribute_map.h\"\n@@ -995,8 +996,8 @@ class CustomCallCmd : public CommandBufferCmd {\n   // This is a legacy custom call API that is discouraged, and will be\n   // deprecated once XLA:FFI mechanism is ready.\n   CustomCallCmd(std::string target_name, CustomCallTarget call_target,\n-                std::vector<std::optional<ShapedSlice>> operands,\n-                std::vector<std::optional<ShapedSlice>> results,\n+                std::vector<NullableShapedSlice> operands,\n+                std::vector<NullableShapedSlice> results,\n                 absl::string_view opaque)\n       : CommandBufferCmd(CommandBufferCmdType::kCustomCallCmd),\n         target_name_(std::move(target_name)),\n@@ -1006,8 +1007,8 @@ class CustomCallCmd : public CommandBufferCmd {\n         results_(std::move(results)) {}\n \n   CustomCallCmd(std::string target_name, XLA_FFI_Handler* handler,\n-                std::vector<std::optional<ShapedSlice>> operands,\n-                std::vector<std::optional<ShapedSlice>> results,\n+                std::vector<NullableShapedSlice> operands,\n+                std::vector<NullableShapedSlice> results,\n                 ffi::CallFrame call_frame,\n                 const HloComputation* called_computation)\n       : CommandBufferCmd(CommandBufferCmdType::kCustomCallCmd),\n@@ -1059,8 +1060,8 @@ class CustomCallCmd : public CommandBufferCmd {\n \n   const HloComputation* called_computation_;\n \n-  std::vector<std::optional<ShapedSlice>> operands_;\n-  std::vector<std::optional<ShapedSlice>> results_;\n+  std::vector<NullableShapedSlice> operands_;\n+  std::vector<NullableShapedSlice> results_;\n };\n \n //===----------------------------------------------------------------------===//"
        },
        {
            "sha": "c5e76b852764c814703097c96f132dc3cefcc426",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 20,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.cc?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -36,6 +36,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/runtime/custom_call_target.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/ffi/api/c_api.h\"\n@@ -71,8 +72,8 @@ using xla::ffi::CallOptions;\n // memory addresses. This is called once when creating the CustomCall thunk,\n // then the thunk will need to update the addresses at runtime.\n static absl::StatusOr<ffi::CallFrame> BuildCallFramePrototype(\n-    absl::Span<const std::optional<ShapedSlice>> operands,\n-    absl::Span<const std::optional<ShapedSlice>> results,\n+    absl::Span<const NullableShapedSlice> operands,\n+    absl::Span<const NullableShapedSlice> results,\n     ffi::AttributesMap attributes) {\n   CallFrameBuilder builder(\n       /*num_args=*/operands.size(),\n@@ -176,8 +177,8 @@ ResolveLegacyCustomCall(const CustomCallTargetRegistry& registry,\n \n absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n     ThunkInfo thunk_info, std::string target_name, CustomCallTarget call_target,\n-    std::vector<std::optional<ShapedSlice>> operands,\n-    std::vector<std::optional<ShapedSlice>> results, std::string opaque) {\n+    std::vector<NullableShapedSlice> operands,\n+    std::vector<NullableShapedSlice> results, std::string opaque) {\n   return absl::WrapUnique(new CustomCallThunk(\n       thunk_info, std::move(target_name), std::move(operands),\n       std::move(results), std::move(opaque), std::move(call_target),\n@@ -186,8 +187,8 @@ absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n \n absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n     ThunkInfo thunk_info, std::string target_name,\n-    std::vector<std::optional<ShapedSlice>> operands,\n-    std::vector<std::optional<ShapedSlice>> results, std::string opaque,\n+    std::vector<NullableShapedSlice> operands,\n+    std::vector<NullableShapedSlice> results, std::string opaque,\n     CustomCallApiVersion api_version, absl::string_view platform_name) {\n   if (api_version == CustomCallApiVersion::API_VERSION_TYPED_FFI) {\n     return absl::InvalidArgumentError(\n@@ -207,10 +208,9 @@ absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n \n absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n     ThunkInfo thunk_info, std::string target_name,\n-    std::vector<std::optional<ShapedSlice>> operands,\n-    std::vector<std::optional<ShapedSlice>> results,\n-    ffi::AttributesMap attributes, const HloComputation* called_computation,\n-    absl::string_view platform_name) {\n+    std::vector<NullableShapedSlice> operands,\n+    std::vector<NullableShapedSlice> results, ffi::AttributesMap attributes,\n+    const HloComputation* called_computation, absl::string_view platform_name) {\n   TF_ASSIGN_OR_RETURN(ffi::HandlerRegistration registration,\n                       ffi::FindHandler(target_name, platform_name));\n \n@@ -221,10 +221,9 @@ absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n \n absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n     ThunkInfo thunk_info, std::string target_name,\n-    XLA_FFI_Handler_Bundle bundle,\n-    std::vector<std::optional<ShapedSlice>> operands,\n-    std::vector<std::optional<ShapedSlice>> results,\n-    ffi::AttributesMap attributes, const HloComputation* called_computation) {\n+    XLA_FFI_Handler_Bundle bundle, std::vector<NullableShapedSlice> operands,\n+    std::vector<NullableShapedSlice> results, ffi::AttributesMap attributes,\n+    const HloComputation* called_computation) {\n   auto execution_state = std::make_unique<ffi::ExecutionState>();\n \n   // Initialize FFI handler state if it has an instantiate callback.\n@@ -255,8 +254,8 @@ absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n \n absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n     ThunkInfo thunk_info, std::string target_name, OwnedHandlerBundle bundle,\n-    std::vector<std::optional<ShapedSlice>> operands,\n-    std::vector<std::optional<ShapedSlice>> results,\n+    std::vector<NullableShapedSlice> operands,\n+    std::vector<NullableShapedSlice> results,\n     xla::ffi::AttributesMap attributes,\n     const HloComputation* called_computation) {\n   if (!bundle.execute) {\n@@ -293,8 +292,8 @@ absl::StatusOr<std::unique_ptr<CustomCallThunk>> CustomCallThunk::Create(\n \n CustomCallThunk::CustomCallThunk(\n     ThunkInfo thunk_info, std::string target_name,\n-    std::vector<std::optional<ShapedSlice>> operands,\n-    std::vector<std::optional<ShapedSlice>> results, std::string opaque,\n+    std::vector<NullableShapedSlice> operands,\n+    std::vector<NullableShapedSlice> results, std::string opaque,\n     CustomCallTarget call_target,\n     const std::optional<CustomCallApiVersion>& api_version)\n     : Thunk(Thunk::kCustomCall, thunk_info),\n@@ -308,8 +307,8 @@ CustomCallThunk::CustomCallThunk(\n CustomCallThunk::CustomCallThunk(\n     ThunkInfo thunk_info, std::string target_name,\n     std::variant<XLA_FFI_Handler_Bundle, OwnedHandlerBundle> bundle,\n-    std::vector<std::optional<ShapedSlice>> operands,\n-    std::vector<std::optional<ShapedSlice>> results, CallFrame call_frame,\n+    std::vector<NullableShapedSlice> operands,\n+    std::vector<NullableShapedSlice> results, CallFrame call_frame,\n     ffi::AttributesMap attributes,\n     std::unique_ptr<ffi::ExecutionState> execution_state,\n     const HloComputation* called_computation)"
        },
        {
            "sha": "6bf67447988dea1c574ebddf4652c80af0a0369f",
            "filename": "third_party/xla/xla/backends/gpu/runtime/custom_call_thunk.h",
            "status": "modified",
            "additions": 22,
            "deletions": 26,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcustom_call_thunk.h?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/ffi/api/c_api.h\"\n@@ -79,25 +80,24 @@ class CustomCallThunk : public Thunk {\n   // the legacy CustomCall registry. For new code please use XLA FFI instead.\n   static absl::StatusOr<std::unique_ptr<CustomCallThunk>> Create(\n       ThunkInfo thunk_info, std::string target_name,\n-      std::vector<std::optional<ShapedSlice>> operands,\n-      std::vector<std::optional<ShapedSlice>> results, std::string opaque,\n+      std::vector<NullableShapedSlice> operands,\n+      std::vector<NullableShapedSlice> results, std::string opaque,\n       CustomCallApiVersion api_version, absl::string_view platform_name);\n \n   // Creates a custom call thunk from the given legacy custom call target.\n   // Note that a thunk created this way can't be serialized to a proto.\n   // This function is only permitted for unit testing code.\n   static absl::StatusOr<std::unique_ptr<CustomCallThunk>> Create(\n       ThunkInfo thunk_info, std::string target_name,\n-      CustomCallTarget call_target,\n-      std::vector<std::optional<ShapedSlice>> operands,\n-      std::vector<std::optional<ShapedSlice>> results, std::string opaque);\n+      CustomCallTarget call_target, std::vector<NullableShapedSlice> operands,\n+      std::vector<NullableShapedSlice> results, std::string opaque);\n \n   // Creates a serializable custom call thunk. The callback is resolved using\n   // XLA FFI.\n   static absl::StatusOr<std::unique_ptr<CustomCallThunk>> Create(\n       ThunkInfo thunk_info, std::string target_name,\n-      std::vector<std::optional<ShapedSlice>> operands,\n-      std::vector<std::optional<ShapedSlice>> results,\n+      std::vector<NullableShapedSlice> operands,\n+      std::vector<NullableShapedSlice> results,\n       xla::ffi::AttributesMap attributes,\n       const HloComputation* called_computation,\n       absl::string_view platform_name);\n@@ -107,9 +107,8 @@ class CustomCallThunk : public Thunk {\n   // handler which matches the given bundle.\n   static absl::StatusOr<std::unique_ptr<CustomCallThunk>> Create(\n       ThunkInfo thunk_info, std::string target_name,\n-      XLA_FFI_Handler_Bundle bundle,\n-      std::vector<std::optional<ShapedSlice>> operands,\n-      std::vector<std::optional<ShapedSlice>> results,\n+      XLA_FFI_Handler_Bundle bundle, std::vector<NullableShapedSlice> operands,\n+      std::vector<NullableShapedSlice> results,\n       xla::ffi::AttributesMap attributes,\n       const HloComputation* called_computation);\n \n@@ -118,8 +117,8 @@ class CustomCallThunk : public Thunk {\n   // for the lifetime of the thunk.\n   static absl::StatusOr<std::unique_ptr<CustomCallThunk>> Create(\n       ThunkInfo thunk_info, std::string target_name, OwnedHandlerBundle bundle,\n-      std::vector<std::optional<ShapedSlice>> operands,\n-      std::vector<std::optional<ShapedSlice>> results,\n+      std::vector<NullableShapedSlice> operands,\n+      std::vector<NullableShapedSlice> results,\n       xla::ffi::AttributesMap attributes,\n       const HloComputation* called_computation);\n \n@@ -142,28 +141,24 @@ class CustomCallThunk : public Thunk {\n     return call_frame_ ? std::make_optional(call_frame_->Copy()) : std::nullopt;\n   }\n \n-  const std::vector<std::optional<ShapedSlice>>& operands() const {\n-    return operands_;\n-  }\n-  const std::vector<std::optional<ShapedSlice>>& results() const {\n-    return results_;\n-  }\n+  const std::vector<NullableShapedSlice>& operands() const { return operands_; }\n+  const std::vector<NullableShapedSlice>& results() const { return results_; }\n \n   absl::string_view opaque() const { return opaque_; }\n \n  private:\n   CustomCallThunk(ThunkInfo thunk_info, std::string target_name,\n-                  std::vector<std::optional<ShapedSlice>> operands,\n-                  std::vector<std::optional<ShapedSlice>> results,\n-                  std::string opaque, CustomCallTarget call_target,\n+                  std::vector<NullableShapedSlice> operands,\n+                  std::vector<NullableShapedSlice> results, std::string opaque,\n+                  CustomCallTarget call_target,\n                   const std::optional<CustomCallApiVersion>& api_version);\n \n   CustomCallThunk(\n       ThunkInfo thunk_info, std::string target_name,\n       std::variant<XLA_FFI_Handler_Bundle, OwnedHandlerBundle> bundle,\n-      std::vector<std::optional<ShapedSlice>> operands,\n-      std::vector<std::optional<ShapedSlice>> results,\n-      ffi::CallFrame call_frame, xla::ffi::AttributesMap attributes,\n+      std::vector<NullableShapedSlice> operands,\n+      std::vector<NullableShapedSlice> results, ffi::CallFrame call_frame,\n+      xla::ffi::AttributesMap attributes,\n       std::unique_ptr<ffi::ExecutionState> execution_state,\n       const HloComputation* called_computation);\n \n@@ -195,8 +190,9 @@ class CustomCallThunk : public Thunk {\n   std::optional<CustomCallApiVersion> api_version_;\n   std::string target_name_;\n \n-  std::vector<std::optional<ShapedSlice>> operands_;\n-  std::vector<std::optional<ShapedSlice>> results_;\n+  // Nulled shape slices represent null pointer arguments to the thunk.\n+  std::vector<NullableShapedSlice> operands_;\n+  std::vector<NullableShapedSlice> results_;\n \n   // This is a legacy custom call API that is discouraged, and will be\n   // deprecated once XLA:FFI mechanism is ready."
        },
        {
            "sha": "255a32302b0d9410e243662b1ecb367512769ae9",
            "filename": "third_party/xla/xla/backends/gpu/runtime/dynamic_slice_thunk_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fdynamic_slice_thunk_test.cc?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/dynamic_slice_thunk.pb.h\"\n #include \"xla/backends/gpu/runtime/gemm_thunk.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk_proto_deserialization.h\"\n #include \"xla/ffi/attribute_map.h\"\n@@ -573,9 +574,9 @@ TEST_F(DynamicSliceThunkTest, SlicedMemcpy) {\n       auto registration,\n       xla::ffi::FindHandler(\"__xla_test$$memcpy\", GetPlatformName()));\n \n-  std::vector<std::optional<ShapedSlice>> operands{ShapedSlice{\n+  std::vector<NullableShapedSlice> operands{ShapedSlice{\n       slice_src_fake, ShapeUtil::MakeShape(PrimitiveType::S32, {8, 8})}};\n-  std::vector<std::optional<ShapedSlice>> results{\n+  std::vector<NullableShapedSlice> results{\n       ShapedSlice{slice_dst, ShapeUtil::MakeShape(PrimitiveType::S32, {8, 8})}};\n \n   // Creating embedded custom call thunk.\n@@ -733,9 +734,9 @@ TEST_F(DynamicSliceThunkTest, SlicedOutputMemcpy) {\n       auto registration,\n       xla::ffi::FindHandler(\"__xla_test$$memcpy\", GetPlatformName()));\n \n-  std::vector<std::optional<ShapedSlice>> operands{ShapedSlice{\n+  std::vector<NullableShapedSlice> operands{ShapedSlice{\n       slice_src_fake, ShapeUtil::MakeShape(PrimitiveType::S32, {2, 2})}};\n-  std::vector<std::optional<ShapedSlice>> results{ShapedSlice{\n+  std::vector<NullableShapedSlice> results{ShapedSlice{\n       slice_dst_fake, ShapeUtil::MakeShape(PrimitiveType::S32, {2, 2})}};\n \n   // Creating embedded custom call thunk.\n@@ -1441,9 +1442,9 @@ TEST_F(DynamicSliceThunkTest, SlicedMemcpyOOB) {\n       auto registration,\n       xla::ffi::FindHandler(\"__xla_test$$memcpy\", GetPlatformName()));\n \n-  std::vector<std::optional<ShapedSlice>> operands{ShapedSlice{\n+  std::vector<NullableShapedSlice> operands{ShapedSlice{\n       slice_src_fake, ShapeUtil::MakeShape(PrimitiveType::S32, {2, 2})}};\n-  std::vector<std::optional<ShapedSlice>> results{ShapedSlice{\n+  std::vector<NullableShapedSlice> results{ShapedSlice{\n       slice_dst_fake, ShapeUtil::MakeShape(PrimitiveType::S32, {2, 2})}};\n \n   // Creating embedded custom call thunk."
        },
        {
            "sha": "264c0dec22538c4c4ec6b5a8fa29b911be215e46",
            "filename": "third_party/xla/xla/backends/gpu/runtime/host_execute_thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fhost_execute_thunk.cc?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -26,7 +26,6 @@ limitations under the License.\n \n #include \"absl/base/call_once.h\"\n #include \"absl/base/casts.h\"\n-#include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n@@ -38,6 +37,7 @@ limitations under the License.\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/host_offloading/gpu_host_offloading_allocator.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.pb.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/core/host_offloading/host_offloading_allocator.h\"\n #include \"xla/core/host_offloading/host_offloading_buffer.h\""
        },
        {
            "sha": "1cc609a2d2263f450afe3deb8aebf437fbf93f0b",
            "filename": "third_party/xla/xla/backends/gpu/runtime/infeed_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Finfeed_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Finfeed_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Finfeed_thunk.h?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/service/buffer_assignment.h\"\n "
        },
        {
            "sha": "a951df9eea5cf02b5217fe18d59cc402929d4ab6",
            "filename": "third_party/xla/xla/backends/gpu/runtime/outfeed_thunk.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Foutfeed_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Foutfeed_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Foutfeed_thunk.h?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/service/buffer_assignment.h\"\n "
        },
        {
            "sha": "f648f8f2e59b78c407cf208c4d0b74dcdcf86f14",
            "filename": "third_party/xla/xla/backends/gpu/runtime/shaped_slice.cc",
            "status": "added",
            "additions": 68,
            "deletions": 0,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fshaped_slice.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fshaped_slice.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fshaped_slice.cc?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -0,0 +1,68 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n+\n+#include <optional>\n+#include <utility>\n+\n+#include \"absl/status/statusor.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.pb.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla::gpu {\n+\n+absl::StatusOr<ShapedSlice> ShapedSlice::FromProto(\n+    const ShapedSliceProto& proto,\n+    absl::Span<const BufferAllocation> buffer_allocations) {\n+  ShapedSlice shaped_slice;\n+  TF_ASSIGN_OR_RETURN(\n+      shaped_slice.slice,\n+      BufferAllocation::Slice::FromProto(proto.slice(), buffer_allocations));\n+  TF_ASSIGN_OR_RETURN(shaped_slice.shape, Shape::FromProto(proto.shape()));\n+  return shaped_slice;\n+}\n+\n+absl::StatusOr<ShapedSliceProto> ShapedSlice::ToProto() const {\n+  ShapedSliceProto proto;\n+  TF_ASSIGN_OR_RETURN(*proto.mutable_slice(), slice.ToProto());\n+  *proto.mutable_shape() = shape.ToProto();\n+  return proto;\n+}\n+\n+absl::StatusOr<NullableShapedSlice> NullableShapedSlice::FromProto(\n+    const NullableShapedSliceProto& proto,\n+    absl::Span<const BufferAllocation> buffer_allocations) {\n+  if (proto.has_shaped_slice()) {\n+    TF_ASSIGN_OR_RETURN(\n+        ShapedSlice shaped_slice,\n+        ShapedSlice::FromProto(proto.shaped_slice(), buffer_allocations));\n+    return NullableShapedSlice(std::move(shaped_slice));\n+  }\n+  return NullableShapedSlice(std::nullopt);\n+}\n+\n+absl::StatusOr<NullableShapedSliceProto> NullableShapedSlice::ToProto() const {\n+  NullableShapedSliceProto proto;\n+  if (has_value()) {\n+    TF_ASSIGN_OR_RETURN(*proto.mutable_shaped_slice(), value().ToProto());\n+  }\n+  return proto;\n+}\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "e4306458650a698a04dd321066ce630d14e10e1c",
            "filename": "third_party/xla/xla/backends/gpu/runtime/shaped_slice.h",
            "status": "added",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fshaped_slice.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fshaped_slice.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fshaped_slice.h?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -0,0 +1,91 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_RUNTIME_SHAPED_SLICE_H_\n+#define XLA_BACKENDS_GPU_RUNTIME_SHAPED_SLICE_H_\n+\n+#include <optional>\n+\n+#include \"absl/status/statusor.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.pb.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape.h\"\n+\n+namespace xla::gpu {\n+\n+// A struct that defines a shaped slice, i.e., a BufferAllocation::Slice and its\n+// shape.\n+struct ShapedSlice {\n+  BufferAllocation::Slice slice;\n+  Shape shape;\n+\n+  static absl::StatusOr<ShapedSlice> FromProto(\n+      const ShapedSliceProto& proto,\n+      absl::Span<const BufferAllocation> buffer_allocations);\n+  absl::StatusOr<ShapedSliceProto> ToProto() const;\n+\n+  friend bool operator==(const ShapedSlice& lhs, const ShapedSlice& rhs) {\n+    return lhs.slice == rhs.slice && lhs.shape == rhs.shape;\n+  }\n+\n+  friend bool operator!=(const ShapedSlice& lhs, const ShapedSlice& rhs) {\n+    return !(lhs == rhs);\n+  }\n+\n+  template <typename Sink>\n+  friend void AbslStringify(Sink& sink, const ShapedSlice& shaped_slice) {\n+    absl::Format(&sink, \"ShapedSlice{slice: %v, shape: %v}\", shaped_slice.slice,\n+                 shaped_slice.shape.ToString(/*print_layout=*/true));\n+  }\n+};\n+\n+// A nullable shaped slice is either a ShapedSlice or a nullopt. This is used\n+// to represent the operands and results of a thunk, where a nullopt represents\n+// a null pointer argument to the thunk.\n+class NullableShapedSlice : public std::optional<ShapedSlice> {\n+ public:\n+  using std::optional<ShapedSlice>::optional;\n+\n+  static absl::StatusOr<NullableShapedSlice> FromProto(\n+      const NullableShapedSliceProto& proto,\n+      absl::Span<const BufferAllocation> buffer_allocations);\n+  absl::StatusOr<NullableShapedSliceProto> ToProto() const;\n+\n+  friend bool operator==(const NullableShapedSlice& lhs,\n+                         const NullableShapedSlice& rhs) {\n+    return static_cast<const std::optional<ShapedSlice>&>(lhs) ==\n+           static_cast<const std::optional<ShapedSlice>&>(rhs);\n+  }\n+\n+  friend bool operator!=(const NullableShapedSlice& lhs,\n+                         const NullableShapedSlice& rhs) {\n+    return !(lhs == rhs);\n+  }\n+\n+  template <typename Sink>\n+  friend void AbslStringify(Sink& sink,\n+                            const NullableShapedSlice& shaped_slice) {\n+    if (shaped_slice.has_value()) {\n+      absl::Format(&sink, \"%v\", *shaped_slice);\n+    } else {\n+      absl::Format(&sink, \"null\");\n+    }\n+  }\n+};\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_BACKENDS_GPU_RUNTIME_SHAPED_SLICE_H_"
        },
        {
            "sha": "ddb76b96c82e2f28d44092f3ea389183f5009635",
            "filename": "third_party/xla/xla/backends/gpu/runtime/shaped_slice.proto",
            "status": "added",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fshaped_slice.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fshaped_slice.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fshaped_slice.proto?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -0,0 +1,25 @@\n+syntax = \"proto3\";\n+\n+package xla.gpu;\n+\n+import \"xla/service/buffer_assignment.proto\";\n+import \"xla/xla_data.proto\";\n+\n+option java_multiple_files = true;\n+option java_outer_classname = \"ShapedSlice\";\n+\n+// A shaped slice is a BufferAllocation::Slice and its shape.\n+message ShapedSliceProto {\n+  // The buffer allocation slice.\n+  xla.buffer_assignment.BufferAllocationSliceProto slice = 1;\n+\n+  // The shape of the slice.\n+  xla.ShapeProto shape = 2;\n+}\n+\n+// A nullable shaped slice is either a ShapedSlice or a nullopt. This is used\n+// to represent the operands and results of a thunk, where a nullopt represents\n+// a null pointer argument to the thunk.\n+message NullableShapedSliceProto {\n+  optional ShapedSliceProto shaped_slice = 1;\n+}"
        },
        {
            "sha": "e0318e9b2d43043f675ba6c5571a334e3c041645",
            "filename": "third_party/xla/xla/backends/gpu/runtime/shaped_slice_test.cc",
            "status": "added",
            "additions": 224,
            "deletions": 0,
            "changes": 224,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fshaped_slice_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fshaped_slice_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fshaped_slice_test.cc?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -0,0 +1,224 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/status/status_matchers.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.pb.h\"\n+#include \"xla/primitive_util.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape_util.h\"\n+#include \"xla/tsl/util/proto/parse_text_proto.h\"\n+#include \"xla/tsl/util/proto/proto_matchers.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+using absl_testing::IsOkAndHolds;\n+using ::testing::HasSubstr;\n+using tsl::proto_testing::EqualsProto;\n+using tsl::proto_testing::ParseTextProtoOrDie;\n+\n+TEST(ShapedSliceTest, Stringify) {\n+  constexpr int64_t kNumElements = 1024;\n+  const size_t kSizeInBytes = kNumElements * primitive_util::ByteWidth(F32);\n+  BufferAllocation alloc(/*index=*/0,\n+                         /*size=*/kSizeInBytes,\n+                         /*color=*/0);\n+  ShapedSlice shaped_slice;\n+  shaped_slice.slice =\n+      BufferAllocation::Slice(&alloc, /*offset=*/primitive_util::ByteWidth(F32),\n+                              /*size=*/kSizeInBytes);\n+  shaped_slice.shape = ShapeUtil::MakeShape(F32, {kNumElements - 1});\n+  EXPECT_THAT(absl::StrCat(shaped_slice), HasSubstr(\"ShapedSlice\"));\n+  EXPECT_THAT(absl::StrCat(shaped_slice),\n+              HasSubstr(absl::StrCat(shaped_slice.slice)));\n+  EXPECT_THAT(absl::StrCat(shaped_slice),\n+              HasSubstr(shaped_slice.shape.ToString(/*print_layout=*/true)));\n+}\n+\n+TEST(ShapedSliceTest, ToProto) {\n+  constexpr int64_t kNumElements = 1024;\n+  const size_t kSizeInBytes = kNumElements * primitive_util::ByteWidth(F32);\n+  BufferAllocation alloc(/*index=*/0,\n+                         /*size=*/kSizeInBytes,\n+                         /*color=*/0);\n+  ShapedSlice shaped_slice;\n+  shaped_slice.slice = BufferAllocation::Slice(\n+      &alloc, /*offset=*/primitive_util::ByteWidth(F32),\n+      /*size=*/kSizeInBytes - primitive_util::ByteWidth(F32));\n+  shaped_slice.shape = ShapeUtil::MakeShape(F32, {kNumElements - 1});\n+\n+  EXPECT_THAT(\n+      shaped_slice.ToProto(), IsOkAndHolds(EqualsProto(R\"pb(\n+        slice { buffer_allocation_index: 0 offset: 4 size: 4092 }\n+        shape {\n+          element_type: F32\n+          dimensions: 1023\n+          layout { minor_to_major: 0 tail_padding_alignment_in_elements: 1 }\n+          is_dynamic_dimension: false\n+        }\n+      )pb\")));\n+}\n+\n+TEST(ShapedSliceTest, FromProto) {\n+  ShapedSliceProto proto = ParseTextProtoOrDie<ShapedSliceProto>(R\"pb(\n+    slice { buffer_allocation_index: 0 offset: 4 size: 4092 }\n+    shape {\n+      element_type: F32\n+      dimensions: 1023\n+      layout { minor_to_major: 0 tail_padding_alignment_in_elements: 1 }\n+      is_dynamic_dimension: false\n+    }\n+  )pb\");\n+\n+  constexpr int64_t kNumElementsInBuffer = 1024;\n+  const size_t kSizeInBytes =\n+      kNumElementsInBuffer * primitive_util::ByteWidth(F32);\n+\n+  BufferAllocation alloc(/*index=*/0,\n+                         /*size=*/kSizeInBytes,\n+                         /*color=*/0);\n+\n+  ShapedSlice expected_shaped_slice;\n+  expected_shaped_slice.slice = BufferAllocation::Slice(\n+      &alloc, /*offset=*/primitive_util::ByteWidth(F32),\n+      /*size=*/kSizeInBytes - primitive_util::ByteWidth(F32));\n+\n+  // The slice starts with an offset of one element, therefore it can only hold\n+  // kNumElementsInBuffer - 1 elements.\n+  constexpr int64_t kNumElementsInSlice = kNumElementsInBuffer - 1;\n+\n+  expected_shaped_slice.shape =\n+      ShapeUtil::MakeShape(F32, {kNumElementsInSlice});\n+\n+  std::vector<BufferAllocation> buffer_allocations = {alloc};\n+  EXPECT_THAT(ShapedSlice::FromProto(proto, buffer_allocations),\n+              IsOkAndHolds(expected_shaped_slice));\n+}\n+\n+TEST(NullableShapedSliceTest, StringifyNonEmptySlice) {\n+  constexpr int64_t kNumElementsInBuffer = 1024;\n+  const size_t kSizeInBytes =\n+      kNumElementsInBuffer * primitive_util::ByteWidth(F32);\n+  BufferAllocation alloc(/*index=*/0,\n+                         /*size=*/kSizeInBytes,\n+                         /*color=*/0);\n+  ShapedSlice shaped_slice;\n+  shaped_slice.slice =\n+      BufferAllocation::Slice(&alloc, /*offset=*/primitive_util::ByteWidth(F32),\n+                              /*size=*/kSizeInBytes);\n+\n+  // The slice starts with an offset of one element, therefore it can only hold\n+  // kNumElementsInBuffer - 1 elements.\n+  constexpr int64_t kNumElementsInSlice = kNumElementsInBuffer - 1;\n+  shaped_slice.shape = ShapeUtil::MakeShape(F32, {kNumElementsInSlice});\n+  NullableShapedSlice non_empty_slice(shaped_slice);\n+  EXPECT_THAT(absl::StrCat(non_empty_slice),\n+              HasSubstr(absl::StrCat(shaped_slice)));\n+}\n+\n+TEST(NullableShapedSliceTest, StringifyEmptySlice) {\n+  NullableShapedSlice empty_slice;\n+  EXPECT_THAT(absl::StrCat(empty_slice), HasSubstr(\"null\"));\n+}\n+\n+TEST(NullableShapedSliceTest, ToProtoNonEmptySlice) {\n+  constexpr int64_t kNumElementsInBuffer = 1024;\n+  const size_t kSizeInBytes =\n+      kNumElementsInBuffer * primitive_util::ByteWidth(F32);\n+  BufferAllocation alloc(/*index=*/0,\n+                         /*size=*/kSizeInBytes,\n+                         /*color=*/0);\n+  ShapedSlice shaped_slice;\n+  shaped_slice.slice = BufferAllocation::Slice(\n+      &alloc, /*offset=*/primitive_util::ByteWidth(F32),\n+      /*size=*/kSizeInBytes - primitive_util::ByteWidth(F32));\n+\n+  // The slice starts with an offset of one element, therefore it can only hold\n+  // kNumElementsInBuffer - 1 elements.\n+  constexpr int64_t kNumElementsInSlice = kNumElementsInBuffer - 1;\n+  shaped_slice.shape = ShapeUtil::MakeShape(F32, {kNumElementsInSlice});\n+  NullableShapedSlice non_empty_slice(shaped_slice);\n+  EXPECT_THAT(\n+      non_empty_slice.ToProto(), IsOkAndHolds(EqualsProto(R\"pb(\n+        shaped_slice {\n+          slice { buffer_allocation_index: 0 offset: 4 size: 4092 }\n+          shape {\n+            element_type: F32\n+            dimensions: 1023\n+            layout { minor_to_major: 0 tail_padding_alignment_in_elements: 1 }\n+            is_dynamic_dimension: false\n+          }\n+        }\n+      )pb\")));\n+}\n+\n+TEST(NullableShapedSliceTest, ToProtoEmptySlice) {\n+  NullableShapedSlice empty_slice;\n+  EXPECT_THAT(empty_slice.ToProto(), IsOkAndHolds(EqualsProto(R\"pb()pb\")));\n+}\n+\n+TEST(NullableShapedSliceTest, FromProtoNonEmptySlice) {\n+  NullableShapedSliceProto proto =\n+      ParseTextProtoOrDie<NullableShapedSliceProto>(R\"pb(\n+        shaped_slice {\n+          slice { buffer_allocation_index: 0 offset: 4 size: 4092 }\n+          shape {\n+            element_type: F32\n+            dimensions: 1023\n+            layout { minor_to_major: 0 tail_padding_alignment_in_elements: 1 }\n+            is_dynamic_dimension: false\n+          }\n+        }\n+      )pb\");\n+\n+  constexpr int64_t kNumElementsInBuffer = 1024;\n+  const size_t kSizeInBytes =\n+      kNumElementsInBuffer * primitive_util::ByteWidth(F32);\n+  BufferAllocation alloc(/*index=*/0,\n+                         /*size=*/kSizeInBytes,\n+                         /*color=*/0);\n+  std::vector<BufferAllocation> buffer_allocations = {alloc};\n+  ShapedSlice expected_shaped_slice;\n+  expected_shaped_slice.slice = BufferAllocation::Slice(\n+      &buffer_allocations[0], /*offset=*/primitive_util::ByteWidth(F32),\n+      /*size=*/kSizeInBytes - primitive_util::ByteWidth(F32));\n+\n+  // The slice starts with an offset of one element, therefore it can only hold\n+  // kNumElementsInBuffer - 1 elements.\n+  constexpr size_t kNumElementsInSlice = kNumElementsInBuffer - 1;\n+  expected_shaped_slice.shape =\n+      ShapeUtil::MakeShape(F32, {kNumElementsInSlice});\n+\n+  EXPECT_THAT(NullableShapedSlice::FromProto(\n+                  proto, /*buffer_allocations=*/buffer_allocations),\n+              IsOkAndHolds(NullableShapedSlice(expected_shaped_slice)));\n+}\n+\n+TEST(NullableShapedSliceTest, FromProtoEmptySlice) {\n+  NullableShapedSliceProto proto;\n+  EXPECT_THAT(NullableShapedSlice::FromProto(proto, /*buffer_allocations=*/{}),\n+              IsOkAndHolds(NullableShapedSlice()));\n+}\n+\n+}  // namespace\n+}  // namespace xla::gpu"
        },
        {
            "sha": "c2f8d18760f5ba9bd0fab6ab2d79f587345e7243",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.cc?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -17,14 +17,13 @@ limitations under the License.\n \n #include <algorithm>\n #include <cstdint>\n-#include <memory>\n+#include <optional>\n #include <ostream>\n #include <string>\n #include <utility>\n \n #include \"absl/base/nullability.h\"\n #include \"absl/container/flat_hash_map.h\"\n-#include \"absl/container/inlined_vector.h\"\n #include \"absl/functional/function_ref.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n@@ -42,7 +41,6 @@ limitations under the License.\n #include \"xla/executable_run_options.h\"\n #include \"xla/ffi/execution_context.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n-#include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/global_device_id.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/buffer_allocations.h\"\n@@ -460,23 +458,6 @@ ThunkInfoProto Thunk::ThunkInfo::ToProto() const {\n   return proto;\n }\n \n-absl::StatusOr<ShapedSlice> ShapedSlice::FromProto(\n-    const ShapedSliceProto& proto,\n-    absl::Span<const BufferAllocation> buffer_allocations) {\n-  ShapedSlice shaped_slice;\n-  TF_ASSIGN_OR_RETURN(\n-      shaped_slice.slice,\n-      BufferAllocation::Slice::FromProto(proto.slice(), buffer_allocations));\n-  TF_ASSIGN_OR_RETURN(shaped_slice.shape, Shape::FromProto(proto.shape()));\n-  return shaped_slice;\n-}\n-\n-absl::StatusOr<ShapedSliceProto> ShapedSlice::ToProto() const {\n-  ShapedSliceProto proto;\n-  TF_ASSIGN_OR_RETURN(*proto.mutable_slice(), slice.ToProto());\n-  *proto.mutable_shape() = shape.ToProto();\n-  return proto;\n-}\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "f0ab61703363fbe2b24eaf00cdf01ca06e283948",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.h",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -608,18 +608,6 @@ using ThunkSequence = std::vector<std::unique_ptr<Thunk>>;\n \n std::ostream& operator<<(std::ostream& os, Thunk::Kind kind);\n \n-// A struct that defines a shaped slice, i.e., a BufferAllocation::Slice and its\n-// shape.\n-struct ShapedSlice {\n-  BufferAllocation::Slice slice;\n-  Shape shape;\n-\n-  static absl::StatusOr<ShapedSlice> FromProto(\n-      const ShapedSliceProto& proto,\n-      absl::Span<const BufferAllocation> buffer_allocations);\n-  absl::StatusOr<ShapedSliceProto> ToProto() const;\n-};\n-\n // Returns if the thunk implements a reduction collective (all-reduce or\n // reduce-scatter).\n bool IsReductionCollective(Thunk::Kind kind);"
        },
        {
            "sha": "689f3292a80afb34a9a4ee85aad3db5489faf7ea",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.proto",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -19,6 +19,7 @@ package xla.gpu;\n \n import \"xla/backends/gpu/runtime/convolution_filter_thunk.proto\";\n import \"xla/backends/gpu/runtime/dynamic_slice_thunk.proto\";\n+import \"xla/backends/gpu/runtime/shaped_slice.proto\";\n import \"xla/core/host_offloading/host_offloading_executable.proto\";\n import \"xla/service/buffer_assignment.proto\";\n import \"xla/service/gpu/gpu_conv_runner.proto\";\n@@ -143,11 +144,6 @@ message CudnnThunkProto {\n   optional int64 sdpa_dropout_seed = 3;\n }\n \n-message ShapedSliceProto {\n-  xla.buffer_assignment.BufferAllocationSliceProto slice = 1;\n-  xla.ShapeProto shape = 2;\n-}\n-\n message HostExecuteStartThunkProto {\n   HostOffloadingExecutableProto executable_proto = 1;\n   repeated ShapedSliceProto args = 2;"
        },
        {
            "sha": "99f2d1cc726f2e6fda8b2592e9599c8d1d85ec07",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -498,6 +498,7 @@ cc_library(\n         \"//xla/backends/gpu/runtime:select_k_thunk\",\n         \"//xla/backends/gpu/runtime:send_thunk\",\n         \"//xla/backends/gpu/runtime:sequential_thunk\",\n+        \"//xla/backends/gpu/runtime:shaped_slice\",\n         \"//xla/backends/gpu/runtime:thunk\",\n         \"//xla/backends/gpu/runtime:topk\",\n         \"//xla/backends/gpu/runtime:triangular_solve_thunk\","
        },
        {
            "sha": "344777f9d78f0588fb38a588444ce19878582580",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/524800e2ed3cb2bf3943972fc30d2d0b3e7229fb/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_unnested.cc?ref=524800e2ed3cb2bf3943972fc30d2d0b3e7229fb",
            "patch": "@@ -26,7 +26,6 @@ limitations under the License.\n #include <string>\n #include <tuple>\n #include <utility>\n-#include <variant>\n #include <vector>\n \n #include \"absl/container/flat_hash_map.h\"\n@@ -73,7 +72,6 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n #include \"xla/backends/gpu/codegen/fusions.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter.h\"\n-#include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/runtime/all_gather_thunk.h\"\n #include \"xla/backends/gpu/runtime/all_reduce_thunk.h\"\n #include \"xla/backends/gpu/runtime/all_to_all_thunk.h\"\n@@ -113,6 +111,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/select_k_thunk.h\"\n #include \"xla/backends/gpu/runtime/send_thunk.h\"\n #include \"xla/backends/gpu/runtime/sequential_thunk.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/topk.h\"\n #include \"xla/backends/gpu/runtime/triangular_solve_thunk.h\"\n@@ -169,7 +168,6 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n-#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/gpu/gpu_blas_lt.h\"\n #include \"xla/stream_executor/gpu/tma_metadata.h\"\n@@ -1162,7 +1160,7 @@ absl::Status IrEmitterUnnested::EmitCustomCallThunk(\n   bool is_ffi_custom_call =\n       instr->api_version() == CustomCallApiVersion::API_VERSION_TYPED_FFI;\n \n-  using Slices = std::vector<std::optional<ShapedSlice>>;\n+  using Slices = std::vector<NullableShapedSlice>;\n \n   Slices operands;\n   for (auto* operand : instr->operands()) {"
        }
    ],
    "stats": {
        "total": 631,
        "additions": 526,
        "deletions": 105
    }
}