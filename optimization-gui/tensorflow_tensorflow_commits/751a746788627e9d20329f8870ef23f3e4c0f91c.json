{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 848870601",
    "sha": "751a746788627e9d20329f8870ef23f3e4c0f91c",
    "files": [
        {
            "sha": "7def227a70b126c9ef1310211fdec52aa11a48ed",
            "filename": "tensorflow/core/tpu/kernels/host_compute_ops.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/751a746788627e9d20329f8870ef23f3e4c0f91c/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fhost_compute_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/751a746788627e9d20329f8870ef23f3e4c0f91c/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fhost_compute_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Fhost_compute_ops.cc?ref=751a746788627e9d20329f8870ef23f3e4c0f91c",
            "patch": "@@ -78,7 +78,7 @@ class RecvAtHostOp : public AsyncOpKernel {\n   }\n \n   void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override {\n-    string remote_device;\n+    std::string remote_device;\n     if (!device_ordinal_is_attr) {\n       const Tensor& device_ordinal_tensor = ctx->input(1);\n       OP_REQUIRES_ASYNC(\n@@ -115,7 +115,7 @@ class RecvAtHostOp : public AsyncOpKernel {\n         errors::InvalidArgument(\"Input shape \", input.shape().DebugString(),\n                                 \" is not a vector of length 3.\"),\n         done);\n-    string rendezvous_key_base;\n+    std::string rendezvous_key_base;\n     if (TensorShapeUtils::IsVector(input.shape())) {\n       rendezvous_key_base = input.vec<tstring>()(1);\n     } else {\n@@ -136,7 +136,7 @@ class RecvAtHostOp : public AsyncOpKernel {\n     // Make all the parsed keys before starting any rendezvous->Recv calls to\n     // avoid having to deal with an error case after some Recv have been\n     // started.\n-    std::vector<string> rendezvous_key(ctx->num_outputs());\n+    std::vector<std::string> rendezvous_key(ctx->num_outputs());\n     std::vector<Rendezvous::ParsedKey> parsed_key(ctx->num_outputs());\n     for (int i = 0; i < ctx->num_outputs(); ++i) {\n       rendezvous_key[i] = Rendezvous::CreateKey(\n@@ -158,7 +158,7 @@ class RecvAtHostOp : public AsyncOpKernel {\n       args.device_context = ctx->op_device_context();\n       args.alloc_attrs = ctx->output_alloc_attr(i);\n \n-      const string& key = rendezvous_key[i];\n+      const std::string& key = rendezvous_key[i];\n       VLOG(2) << \"Recv \" << key;\n       ctx->rendezvous()->RecvAsync(\n           parsed_key[i], args,\n@@ -182,10 +182,10 @@ class RecvAtHostOp : public AsyncOpKernel {\n   }\n \n  private:\n-  string key_;\n-  string remote_device_;\n-  string cpu_device_;\n-  string device_type_;\n+  std::string key_;\n+  std::string remote_device_;\n+  std::string cpu_device_;\n+  std::string device_type_;\n \n   // RecvAtHostOp is neither copyable nor movable.\n   RecvAtHostOp(const RecvAtHostOp&) = delete;\n@@ -281,7 +281,7 @@ class SendFromHostOp : public OpKernel {\n                 errors::InvalidArgument(\"Key input shape \",\n                                         key_input.shape().DebugString(),\n                                         \" is not a vector of length 3.\"));\n-    string rendezvous_key_base;\n+    std::string rendezvous_key_base;\n     if (TensorShapeUtils::IsVector(key_input.shape())) {\n       rendezvous_key_base = key_input.vec<tstring>()(1);\n     } else {\n@@ -298,7 +298,7 @@ class SendFromHostOp : public OpKernel {\n       args.alloc_attrs = ctx->input_alloc_attr(i);\n \n       // TODO(misard) Fix this once we have replication.\n-      const string& rendezvous_key = Rendezvous::CreateKey(\n+      const std::string& rendezvous_key = Rendezvous::CreateKey(\n           cpu_device_, /*src_incarnation=*/1,\n           device_ordinal_is_attr ? remote_device_ : remote_device,\n           absl::StrCat(rendezvous_key_base, key_, \"_htod_\", i),\n@@ -313,10 +313,10 @@ class SendFromHostOp : public OpKernel {\n   }\n \n  private:\n-  string key_;\n-  string cpu_device_;\n-  string remote_device_;\n-  string device_type_;\n+  std::string key_;\n+  std::string cpu_device_;\n+  std::string remote_device_;\n+  std::string device_type_;\n \n   // SendFromHostOp is neither copyable nor movable.\n   SendFromHostOp(const SendFromHostOp&) = delete;"
        },
        {
            "sha": "9f447d35ee69aab2a1f4de653b5efbfea799ac59",
            "filename": "tensorflow/core/tpu/kernels/tpu_compilation_cache_external.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/751a746788627e9d20329f8870ef23f3e4c0f91c/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_compilation_cache_external.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/751a746788627e9d20329f8870ef23f3e4c0f91c/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_compilation_cache_external.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_compilation_cache_external.cc?ref=751a746788627e9d20329f8870ef23f3e4c0f91c",
            "patch": "@@ -35,7 +35,7 @@ namespace tpu {\n namespace {\n \n int64_t get_uid() {\n-  uint64 unsigned_rand = random::New64() & INT64_MAX;\n+  uint64_t unsigned_rand = random::New64() & INT64_MAX;\n   return static_cast<int64_t>(unsigned_rand);\n }\n "
        },
        {
            "sha": "ae0a3c7b0bab6eddbf82955336b8f0b2cd03365d",
            "filename": "tensorflow/core/tpu/kernels/tpu_embedding_load_retrieve_ops.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/751a746788627e9d20329f8870ef23f3e4c0f91c/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_embedding_load_retrieve_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/751a746788627e9d20329f8870ef23f3e4c0f91c/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_embedding_load_retrieve_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_embedding_load_retrieve_ops.cc?ref=751a746788627e9d20329f8870ef23f3e4c0f91c",
            "patch": "@@ -46,7 +46,7 @@ namespace tensorflow {\n // Computes (and VLOGs) the expected shapes of the embedding table shards.\n absl::Status ComputeExpectedTableShardShapes(\n     const TPUEmbeddingConfiguration& config, int shard_id, int num_shards,\n-    const string& op_name, std::vector<TensorShape>* table_shapes) {\n+    const std::string& op_name, std::vector<TensorShape>* table_shapes) {\n   std::vector<TensorShapeProto> shape_protos;\n   const int num_tables = config.table_descriptor_size();\n   TF_RETURN_IF_ERROR(TpuEmbeddingShapeUtil::ComputeTableShapes(\n@@ -70,7 +70,7 @@ absl::Status ComputeExpectedTableShardShapes(\n }\n \n // Logs min/max/avg for the specified state_variable array.\n-void LogRangeStatistics(int32 table_id, int32 state_variable_index,\n+void LogRangeStatistics(int32_t table_id, int32_t state_variable_index,\n                         absl::Span<const float> state_variable) {\n   if (VLOG_IS_ON(5)) {\n     float min = std::numeric_limits<float>::infinity();\n@@ -91,7 +91,7 @@ void LogRangeStatistics(int32 table_id, int32 state_variable_index,\n LoadAllTPUEmbeddingParametersOp::LoadAllTPUEmbeddingParametersOp(\n     OpKernelConstruction* ctx)\n     : OpKernel(ctx) {\n-  string config_string;\n+  std::string config_string;\n   OP_REQUIRES_OK(ctx, ctx->GetAttr(\"config\", &config_string));\n \n   OP_REQUIRES(\n@@ -165,7 +165,7 @@ void LoadAllTPUEmbeddingParametersOp::GetStateVariables(\n               \" but config specifies table shape \",\n               table_shapes_[table_id].DebugString()));\n     }\n-    const int64 num_elements = state_variable[0][table_id].NumElements();\n+    const int64_t num_elements = state_variable[0][table_id].NumElements();\n     VLOG(1) << \"Table \" << table_id << \" (name \" << table_descriptor.name()\n             << \") has shape: \" << table_shapes_[table_id].DebugString()\n             << \", number of elements: \" << num_elements;\n@@ -232,7 +232,7 @@ void LoadAllTPUEmbeddingParametersOp::Compute(OpKernelContext* ctx) {\n RetrieveAllTPUEmbeddingParametersOp::RetrieveAllTPUEmbeddingParametersOp(\n     OpKernelConstruction* ctx)\n     : OpKernel(ctx) {\n-  string config_string;\n+  std::string config_string;\n   OP_REQUIRES_OK(ctx, ctx->GetAttr(\"config\", &config_string));\n \n   OP_REQUIRES(\n@@ -297,7 +297,7 @@ void RetrieveAllTPUEmbeddingParametersOp::GetStateVariables(\n                                 \"optimization algorithm specified for table \",\n                                 table_id));\n     num_state_variables[table_id] = state_variable_specs.size();\n-    const int64 num_elements = table_shapes_[table_id].num_elements();\n+    const int64_t num_elements = table_shapes_[table_id].num_elements();\n     for (int i = 0; i < state_variable_specs.size(); ++i) {\n       Tensor* state_variable_tensor;\n       OP_REQUIRES_OK(\n@@ -313,7 +313,7 @@ void RetrieveAllTPUEmbeddingParametersOp::GetStateVariables(\n          i <= tpu::kMaxAuxiliaryParameterCount; ++i) {\n       Tensor* auxiliary_tensor;\n       TensorShape shape;\n-      std::array<int32, 2> dims = {{0, 0}};\n+      std::array<int32_t, 2> dims = {{0, 0}};\n       OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(dims, &shape));\n       OP_REQUIRES_OK(\n           ctx, state_variable[i].allocate(table_id, shape, &auxiliary_tensor));"
        }
    ],
    "stats": {
        "total": 44,
        "additions": 22,
        "deletions": 22
    }
}