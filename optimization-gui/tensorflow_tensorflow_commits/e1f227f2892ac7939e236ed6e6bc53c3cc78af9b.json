{
    "author": "sergey-kozub",
    "message": "PR #32525: [XLA:GPU] Fix block scaled dot global scaling for older cuDNN versions\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32525\n\nüìù Summary of Changes\nPass cuDNN version to the `BlockScalingRewriter` pass, and make lowering decisions based on that.\n\nüéØ Justification\nThe global scaling factor doesn't work before cuDNN v9.13 (the graph is compiled, but the scaling factor is not applied).\nUse the slower lowering (apply global scaling factor outside the fusion) in this case.\n\nüöÄ Kind of Contribution\nüêõ Bug Fix\n\nCopybara import of the project:\n\n--\na47ef5175d076270e371c9e5cf355fc1ad96efc8 by Sergey Kozub <skozub@nvidia.com>:\n\n[XLA:GPU] Fix block scaled dot global scaling for older cuDNN versions\n\nMerging this change closes #32525\n\nPiperOrigin-RevId: 817592016",
    "sha": "e1f227f2892ac7939e236ed6e6bc53c3cc78af9b",
    "files": [
        {
            "sha": "64e365479a4b60619acad2c8c331135a0eb38bb1",
            "filename": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e1f227f2892ac7939e236ed6e6bc53c3cc78af9b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e1f227f2892ac7939e236ed6e6bc53c3cc78af9b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fnvptx_compiler.cc?ref=e1f227f2892ac7939e236ed6e6bc53c3cc78af9b",
            "patch": "@@ -286,8 +286,9 @@ absl::Status NVPTXCompiler::OptimizeHloPostLayoutAssignment(\n   }\n \n   pre_pipeline.AddPass<BlockScalingRewriter>(\n-      /*allow_cudnn=*/cuda_compute_capability.IsAtLeastBlackwell() &&\n-      gpu_target_config.dnn_version_info >= se::dnn::VersionInfo(9, 7));\n+      cuda_compute_capability.IsAtLeastBlackwell()\n+          ? gpu_target_config.dnn_version_info\n+          : se::dnn::VersionInfo{});\n   pre_pipeline.AddPass<DotDimensionMerger>();\n \n   if (!hlo_module->config()"
        },
        {
            "sha": "dab796cee0b7ddb8be2dc18dc0c1fd089b632825",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e1f227f2892ac7939e236ed6e6bc53c3cc78af9b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e1f227f2892ac7939e236ed6e6bc53c3cc78af9b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=e1f227f2892ac7939e236ed6e6bc53c3cc78af9b",
            "patch": "@@ -311,6 +311,7 @@ cc_library(\n         \"//xla/service:shape_inference\",\n         \"//xla/service/gpu:backend_configs_cc\",\n         \"//xla/service/gpu:cublas_cudnn\",\n+        \"//xla/stream_executor:dnn\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -350,6 +351,7 @@ xla_test(\n         \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n+        \"@local_config_cuda//cuda:cudnn_header\",\n     ],\n )\n "
        },
        {
            "sha": "9d62ffbb66bdba05491911c13d677a5c33a8e9ba",
            "filename": "third_party/xla/xla/service/gpu/transforms/block_scaling_rewriter.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 8,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e1f227f2892ac7939e236ed6e6bc53c3cc78af9b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e1f227f2892ac7939e236ed6e6bc53c3cc78af9b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter.cc?ref=e1f227f2892ac7939e236ed6e6bc53c3cc78af9b",
            "patch": "@@ -395,7 +395,11 @@ absl::StatusOr<XlaOp> BuildCudnnScaledDot(XlaOp lhs_input, XlaOp rhs_input,\n                                           XlaOp global_scale,\n                                           const DotDimensionNumbers& dnums,\n                                           PrimitiveType result_type,\n-                                          std::optional<int64_t> block_size) {\n+                                          std::optional<int64_t> block_size,\n+                                          se::dnn::VersionInfo cudnn_version) {\n+  bool cudnn_supports_global_scale =\n+      cudnn_version >= kCudnnSupportsBlockScaledDotWithGlobalScale;\n+\n   // Get inputs from parameters.\n   TF_ASSIGN_OR_RETURN(\n       auto lhs_ops_and_size,\n@@ -421,13 +425,19 @@ absl::StatusOr<XlaOp> BuildCudnnScaledDot(XlaOp lhs_input, XlaOp rhs_input,\n   std::string custom_call_target{kCudnnBlockScaledDotCallTarget};\n   std::vector<XlaOp> custom_call_operands{lhs_input_op, rhs_input_op,\n                                           lhs_scale_op, rhs_scale_op};\n-  if (global_scale.valid()) {\n+  if (global_scale.valid() && cudnn_supports_global_scale) {\n     custom_call_operands.push_back(global_scale);\n   }\n   XlaOp custom_call = CustomCall(&builder, custom_call_target,\n                                  custom_call_operands, output_shape);\n   XlaOp result = GetTupleElement(custom_call, 0);\n \n+  // Apply global scale outside the graph for older cuDNN versions.\n+  if (global_scale.valid() && !cudnn_supports_global_scale) {\n+    result = Mul(result, global_scale,\n+                 /*broadcast_dimensions=*/{});\n+  }\n+\n   // Slice the result, if necessary.\n   if (lhs_size != lhs_shape.dimensions(1) ||\n       rhs_size != rhs_shape.dimensions(1)) {\n@@ -496,7 +506,7 @@ absl::StatusOr<XlaOp> BuildBlockScaledDot(\n     const HloInstruction* rhs_input, const HloInstruction* lhs_scale,\n     const HloInstruction* rhs_scale, const HloInstruction* global_scale,\n     const DotDimensionNumbers& dnums, PrimitiveType result_type,\n-    std::optional<int64_t> block_size, bool allow_cudnn) {\n+    std::optional<int64_t> block_size, se::dnn::VersionInfo cudnn_version) {\n   // Get dot LHS parameter(s).\n   XlaOp lhs_op = Parameter(&builder, 0, lhs_input->shape(), \"lhs\");\n   XlaOp lhs_scale_op = Parameter(&builder, 2, lhs_scale->shape(), \"lhs_scale\");\n@@ -516,12 +526,13 @@ absl::StatusOr<XlaOp> BuildBlockScaledDot(\n   }\n \n   // Use cuDNN kernel, if possible.\n-  if (allow_cudnn && rhs_scale_op.valid() &&\n+  if (cudnn_version >= kCudnnSupportsBlockScaledDot && rhs_scale_op.valid() &&\n       IsSupportedByCudnn(\n           GetCudnnMxType(lhs_input->shape(), lhs_scale->shape(), block_size),\n           GetCudnnMxType(rhs_input->shape(), rhs_scale->shape(), block_size))) {\n     return BuildCudnnScaledDot(lhs_op, rhs_op, lhs_scale_op, rhs_scale_op,\n-                               global_scale_op, dnums, result_type, block_size);\n+                               global_scale_op, dnums, result_type, block_size,\n+                               std::move(cudnn_version));\n   }\n \n   // Build general dot op.\n@@ -546,7 +557,7 @@ absl::StatusOr<XlaOp> BuildBlockScaledDot(\n \n // Convert scaled dot custom call to HLO computation.\n absl::StatusOr<HloInstruction*> ExpandBlockScaledDotCustomCall(\n-    HloInstruction* instruction, bool allow_cudnn) {\n+    HloInstruction* instruction, se::dnn::VersionInfo cudnn_version) {\n   PrimitiveType result_type = instruction->shape().element_type();\n \n   // Check operand count.\n@@ -602,7 +613,7 @@ absl::StatusOr<HloInstruction*> ExpandBlockScaledDotCustomCall(\n       BuildBlockScaledDot(builder, operands[0], operands[1], operands[2],\n                           operands.size() >= 4 ? operands[3] : nullptr,\n                           operands.size() == 5 ? operands[4] : nullptr, dnums,\n-                          result_type, block_size, allow_cudnn));\n+                          result_type, block_size, std::move(cudnn_version)));\n \n   // Reshape to the expected output shape.\n   // This should only happen when a unit-sized dimension is added by the pass.\n@@ -634,7 +645,7 @@ absl::StatusOr<HloInstruction*> BlockScalingRewriter::ExpandInstruction(\n     return ExpandDequantizeCustomCall(instruction);\n   }\n   if (instruction->custom_call_target() == kBlockScaledDotCustomCallTarget) {\n-    return ExpandBlockScaledDotCustomCall(instruction, allow_cudnn_);\n+    return ExpandBlockScaledDotCustomCall(instruction, cudnn_version_);\n   }\n   LOG(FATAL) << \"Unexpected custom call target: \"\n              << instruction->custom_call_target();"
        },
        {
            "sha": "2aea00533f691100914adaa3c3c375daf6f797ad",
            "filename": "third_party/xla/xla/service/gpu/transforms/block_scaling_rewriter.h",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e1f227f2892ac7939e236ed6e6bc53c3cc78af9b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e1f227f2892ac7939e236ed6e6bc53c3cc78af9b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter.h?ref=e1f227f2892ac7939e236ed6e6bc53c3cc78af9b",
            "patch": "@@ -20,9 +20,13 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/transforms/expanders/op_expander_pass.h\"\n+#include \"xla/stream_executor/dnn.h\"\n \n namespace xla::gpu {\n \n+const se::dnn::VersionInfo kCudnnSupportsBlockScaledDot(9, 7);\n+const se::dnn::VersionInfo kCudnnSupportsBlockScaledDotWithGlobalScale(9, 13);\n+\n // This pass converts the block quantize/dequantize operations (represented as\n // custom calls) to XLA graphs or library calls, if available (e.g. cuDNN).\n //\n@@ -68,8 +72,8 @@ namespace xla::gpu {\n //    config if the block scaled dimension is padded.\n class BlockScalingRewriter : public OpExpanderPass {\n  public:\n-  explicit BlockScalingRewriter(bool allow_cudnn)\n-      : allow_cudnn_(allow_cudnn) {};\n+  explicit BlockScalingRewriter(se::dnn::VersionInfo cudnn_version)\n+      : cudnn_version_(cudnn_version) {};\n \n   absl::string_view name() const override { return \"block-scaling-rewriter\"; }\n \n@@ -91,7 +95,7 @@ class BlockScalingRewriter : public OpExpanderPass {\n   static constexpr int kBlockSizeNVFP4 = 16;\n \n  private:\n-  bool allow_cudnn_;\n+  se::dnn::VersionInfo cudnn_version_;\n };\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "2a587f213f34870530bdf0f88cdf5d37b2f63901",
            "filename": "third_party/xla/xla/service/gpu/transforms/block_scaling_rewriter_cudnn_test.cc",
            "status": "modified",
            "additions": 26,
            "deletions": 20,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e1f227f2892ac7939e236ed6e6bc53c3cc78af9b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter_cudnn_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e1f227f2892ac7939e236ed6e6bc53c3cc78af9b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter_cudnn_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter_cudnn_test.cc?ref=e1f227f2892ac7939e236ed6e6bc53c3cc78af9b",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/status/status_matchers.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"third_party/gpus/cudnn/cudnn_version.h\"\n #include \"xla/error_spec.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/service/gpu/transforms/block_scaling_rewriter.h\"\n@@ -29,6 +30,11 @@ namespace {\n using BlockScalingRewriterCudnnTest =\n     HloPjRtInterpreterReferenceMixin<HloPjRtTestBase>;\n \n+const se::dnn::VersionInfo kCudnnDisabled;\n+const se::dnn::VersionInfo kCudnnVersion(CUDNN_VERSION / 10000,\n+                                         CUDNN_VERSION % 10000 / 100,\n+                                         CUDNN_VERSION % 100);\n+\n TEST_F(BlockScalingRewriterCudnnTest, Mxfp8) {\n   constexpr absl::string_view hlo_string = R\"(\n HloModule test\n@@ -46,20 +52,20 @@ ENTRY main {\n       hlo_string, ErrorSpec(/*aabs=*/1e-4, /*arel=*/1e-5),\n       /*reference_preprocessor=*/\n       [](HloModule* reference_module) {\n-        BlockScalingRewriter pass(/*allow_cudnn=*/false);\n+        BlockScalingRewriter pass(kCudnnDisabled);\n         EXPECT_THAT(RunHloPass(&pass, reference_module),\n                     absl_testing::IsOkAndHolds(true));\n       },\n       /*test_preprocessor=*/\n       [](HloModule* test_module) {\n-        BlockScalingRewriter pass(/*allow_cudnn=*/true);\n+        BlockScalingRewriter pass(kCudnnVersion);\n         EXPECT_THAT(RunHloPass(&pass, test_module),\n                     absl_testing::IsOkAndHolds(true));\n       }));\n \n-  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(false),\n+  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(kCudnnDisabled),\n                             \"CHECK-NOT: __cudnn$blockScaledDot\");\n-  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(true),\n+  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(kCudnnVersion),\n                             \"CHECK: __cudnn$blockScaledDot\");\n }\n \n@@ -80,20 +86,20 @@ ENTRY main {\n       hlo_string, ErrorSpec(/*aabs=*/1e-4, /*arel=*/1e-5),\n       /*reference_preprocessor=*/\n       [](HloModule* reference_module) {\n-        BlockScalingRewriter pass(/*allow_cudnn=*/false);\n+        BlockScalingRewriter pass(kCudnnDisabled);\n         EXPECT_THAT(RunHloPass(&pass, reference_module),\n                     absl_testing::IsOkAndHolds(true));\n       },\n       /*test_preprocessor=*/\n       [](HloModule* test_module) {\n-        BlockScalingRewriter pass(/*allow_cudnn=*/true);\n+        BlockScalingRewriter pass(kCudnnVersion);\n         EXPECT_THAT(RunHloPass(&pass, test_module),\n                     absl_testing::IsOkAndHolds(true));\n       }));\n \n-  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(false),\n+  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(kCudnnDisabled),\n                             \"CHECK-NOT: __cudnn$blockScaledDot\");\n-  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(true),\n+  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(kCudnnVersion),\n                             \"CHECK: __cudnn$blockScaledDot\");\n }\n \n@@ -118,20 +124,20 @@ ENTRY main {\n       hlo_string, ErrorSpec(/*aabs=*/1e-4, /*arel=*/1e-5),\n       /*reference_preprocessor=*/\n       [](HloModule* reference_module) {\n-        BlockScalingRewriter pass(/*allow_cudnn=*/false);\n+        BlockScalingRewriter pass(kCudnnDisabled);\n         EXPECT_THAT(RunHloPass(&pass, reference_module),\n                     absl_testing::IsOkAndHolds(true));\n       },\n       /*test_preprocessor=*/\n       [](HloModule* test_module) {\n-        BlockScalingRewriter pass(/*allow_cudnn=*/true);\n+        BlockScalingRewriter pass(kCudnnVersion);\n         EXPECT_THAT(RunHloPass(&pass, test_module),\n                     absl_testing::IsOkAndHolds(true));\n       }));\n \n-  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(false),\n+  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(kCudnnDisabled),\n                             \"CHECK-NOT: __cudnn$blockScaledDot\");\n-  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(true),\n+  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(kCudnnVersion),\n                             \"CHECK: __cudnn$blockScaledDot\");\n }\n \n@@ -162,20 +168,20 @@ ENTRY main {\n       hlo_string, ErrorSpec(/*aabs=*/1e-4, /*arel=*/1e-5),\n       /*reference_preprocessor=*/\n       [](HloModule* reference_module) {\n-        BlockScalingRewriter pass(/*allow_cudnn=*/false);\n+        BlockScalingRewriter pass(kCudnnDisabled);\n         EXPECT_THAT(RunHloPass(&pass, reference_module),\n                     absl_testing::IsOkAndHolds(true));\n       },\n       /*test_preprocessor=*/\n       [](HloModule* test_module) {\n-        BlockScalingRewriter pass(/*allow_cudnn=*/true);\n+        BlockScalingRewriter pass(kCudnnVersion);\n         EXPECT_THAT(RunHloPass(&pass, test_module),\n                     absl_testing::IsOkAndHolds(true));\n       }));\n \n-  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(false),\n+  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(kCudnnDisabled),\n                             \"CHECK-NOT: __cudnn$blockScaledDot\");\n-  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(true),\n+  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(kCudnnVersion),\n                             \"CHECK: __cudnn$blockScaledDot\");\n }\n \n@@ -203,20 +209,20 @@ ENTRY main {\n       hlo_string, ErrorSpec(/*aabs=*/1e-4, /*arel=*/1e-5),\n       /*reference_preprocessor=*/\n       [](HloModule* reference_module) {\n-        BlockScalingRewriter pass(/*allow_cudnn=*/false);\n+        BlockScalingRewriter pass(kCudnnDisabled);\n         EXPECT_THAT(RunHloPass(&pass, reference_module),\n                     absl_testing::IsOkAndHolds(true));\n       },\n       /*test_preprocessor=*/\n       [](HloModule* test_module) {\n-        BlockScalingRewriter pass(/*allow_cudnn=*/true);\n+        BlockScalingRewriter pass(kCudnnVersion);\n         EXPECT_THAT(RunHloPass(&pass, test_module),\n                     absl_testing::IsOkAndHolds(true));\n       }));\n \n-  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(false),\n+  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(kCudnnDisabled),\n                             \"CHECK-NOT: __cudnn$blockScaledDot\");\n-  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(true),\n+  RunAndFilecheckHloRewrite(hlo_string, BlockScalingRewriter(kCudnnVersion),\n                             \"CHECK: __cudnn$blockScaledDot\");\n }\n "
        },
        {
            "sha": "768a340f21649a647b64e284be52a235df77cc86",
            "filename": "third_party/xla/xla/service/gpu/transforms/block_scaling_rewriter_test.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e1f227f2892ac7939e236ed6e6bc53c3cc78af9b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e1f227f2892ac7939e236ed6e6bc53c3cc78af9b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fblock_scaling_rewriter_test.cc?ref=e1f227f2892ac7939e236ed6e6bc53c3cc78af9b",
            "patch": "@@ -39,7 +39,7 @@ ENTRY main {\n       custom_call_target=\"__op$quantize\"\n })\";\n \n-  BlockScalingRewriter pass(/*allow_cudnn=*/false);\n+  BlockScalingRewriter pass(se::dnn::VersionInfo{});\n   RunAndFilecheckHloRewrite(hlo_string, std::move(pass), R\"(\n   CHECK: [[input:%.+]] = f32[10,256]{1,0} parameter(0)\n   CHECK: [[blocks:%.+]] = f32[10,8,32]{2,1,0} reshape([[input]])\n@@ -69,7 +69,7 @@ ENTRY main {\n       custom_call_target=\"__op$dequantize\"\n })\";\n \n-  BlockScalingRewriter pass(/*allow_cudnn=*/false);\n+  BlockScalingRewriter pass(se::dnn::VersionInfo{});\n   RunAndFilecheckHloRewrite(hlo_string, std::move(pass), R\"(\n   CHECK: [[input:%.+]] = f8e4m3fn[10,256]{1,0} parameter(0)\n   CHECK: [[input_cvt:%.+]] = f32[10,256]{1,0} convert([[input]])\n@@ -94,7 +94,7 @@ ENTRY main {\n       custom_call_target=\"__op$block_scaled_dot\"\n })\";\n \n-  BlockScalingRewriter pass(/*allow_cudnn=*/false);\n+  BlockScalingRewriter pass(se::dnn::VersionInfo{});\n   RunAndFilecheckHloRewrite(hlo_string, std::move(pass), R\"(\n   CHECK: [[lhs_quant:%.+]] = f8e4m3fn[4,16,256]{2,1,0} parameter(0)\n   CHECK: [[lhs_quant_cvt:%.+]] = f32[4,16,256]{2,1,0} convert([[lhs_quant]])\n@@ -130,7 +130,7 @@ ENTRY main {\n       custom_call_target=\"__op$block_scaled_dot\"\n })\";\n \n-  BlockScalingRewriter pass(/*allow_cudnn=*/false);\n+  BlockScalingRewriter pass(se::dnn::VersionInfo{});\n   RunAndFilecheckHloRewrite(hlo_string, std::move(pass), R\"(\n   CHECK: [[lhs_quant:%.+]] = f8e4m3fn[4,16,256]{2,1,0} parameter(0)\n   CHECK: [[lhs_quant_cvt:%.+]] = f32[4,16,256]{2,1,0} convert([[lhs_quant]])\n@@ -168,7 +168,7 @@ ENTRY main {\n       custom_call_target=\"__op$block_scaled_dot\"\n })\";\n \n-  BlockScalingRewriter pass(/*allow_cudnn=*/false);\n+  BlockScalingRewriter pass(se::dnn::VersionInfo{});\n   RunAndFilecheckHloRewrite(hlo_string, std::move(pass), R\"(\n   CHECK: [[lhs_quant:%.+]] = f8e4m3fn[4,16,256]{2,0,1} parameter(0)\n   CHECK: [[lhs_quant_cvt:%.+]] = f32[4,16,256]{2,0,1} convert([[lhs_quant]])\n@@ -202,7 +202,7 @@ ENTRY main {\n       custom_call_target=\"__op$block_scaled_dot\"\n })\";\n \n-  BlockScalingRewriter pass(/*allow_cudnn=*/false);\n+  BlockScalingRewriter pass(se::dnn::VersionInfo{});\n   RunAndFilecheckHloRewrite(hlo_string, std::move(pass), R\"(\n   CHECK: [[lhs_quant:%.+]] = f8e4m3fn[16,256]{1,0} parameter(0)\n   CHECK: [[lhs_quant_cvt:%.+]] = f16[16,256]{1,0} convert([[lhs_quant]])\n@@ -231,7 +231,7 @@ ENTRY main {\n       backend_config={\"block_scaled_dot_backend_config\":{block_size:32}}\n })\";\n \n-  BlockScalingRewriter pass(/*allow_cudnn=*/false);\n+  BlockScalingRewriter pass(se::dnn::VersionInfo{});\n   RunAndFilecheckHloRewrite(hlo_string, std::move(pass), R\"(\n   CHECK: [[lhs_quant:%.+]] = f8e4m3fn[4,16,224]{2,1,0} parameter(0)\n   CHECK: [[lhs_quant_cvt:%.+]] = f32[4,16,224]{2,1,0} convert([[lhs_quant]])\n@@ -270,7 +270,7 @@ ENTRY main {\n   TF_ASSERT_OK_AND_ASSIGN(auto test_module,\n                           ParseAndReturnUnverifiedModule(hlo_test));\n \n-  BlockScalingRewriter pass(/*allow_cudnn=*/false);\n+  BlockScalingRewriter pass(se::dnn::VersionInfo{});\n   TF_ASSERT_OK_AND_ASSIGN(\n       auto changed, pass.Run(test_module.get(), /*execution_threads=*/{}));\n   EXPECT_TRUE(changed);\n@@ -302,7 +302,7 @@ ENTRY main {\n       custom_call_target=\"__op$block_scaled_dot\"\n })\";\n \n-  BlockScalingRewriter pass(/*allow_cudnn=*/true);\n+  BlockScalingRewriter pass(kCudnnSupportsBlockScaledDot);\n   RunAndFilecheckHloRewrite(hlo_string, std::move(pass), R\"(\n   CHECK: [[lhs:%.+]] = f8e4m3fn[4,128,128]{2,1,0} parameter(0)\n   CHECK: [[rhs:%.+]] = f8e4m3fn[4,128,128]{2,1,0} parameter(1)\n@@ -333,7 +333,7 @@ ENTRY main {\n       custom_call_target=\"__op$block_scaled_dot\"\n })\";\n \n-  BlockScalingRewriter pass(/*allow_cudnn=*/true);\n+  BlockScalingRewriter pass(kCudnnSupportsBlockScaledDot);\n   RunAndFilecheckHloRewrite(hlo_string, std::move(pass), R\"(\n   CHECK: [[lhs:%.+]] = f8e4m3fn[128,96]{1,0} parameter(0)\n   CHECK: [[lhs_rs:%.+]] = f8e4m3fn[1,128,96]{2,1,0} reshape([[lhs]])\n@@ -373,7 +373,7 @@ ENTRY main {\n       backend_config={\"block_scaled_dot_backend_config\":{block_size:32}}\n })\";\n \n-  BlockScalingRewriter pass(/*allow_cudnn=*/true);\n+  BlockScalingRewriter pass(kCudnnSupportsBlockScaledDot);\n   RunAndFilecheckHloRewrite(hlo_string, std::move(pass), R\"(\n   CHECK: [[lhs:%.+]] = f8e4m3fn[4,120,96]{2,1,0} parameter(0)\n   CHECK: [[lhs_pad:%.+]] = f8e4m3fn[4,128,96]{2,1,0} pad([[lhs]], {{.+}}), padding=0_0x0_8x0_0"
        }
    ],
    "stats": {
        "total": 112,
        "additions": 68,
        "deletions": 44
    }
}