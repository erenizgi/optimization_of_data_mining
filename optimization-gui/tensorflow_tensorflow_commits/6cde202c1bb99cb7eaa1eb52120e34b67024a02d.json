{
    "author": "pifon2a",
    "message": "[XLA:GPU] Create EmitAsyncDone and EmitAsyncStart methods for ThunkEmitter.\n\nAnd a lot of minor refactoring.\n\nPiperOrigin-RevId: 838151169",
    "sha": "6cde202c1bb99cb7eaa1eb52120e34b67024a02d",
    "files": [
        {
            "sha": "3f68b0fb679421ea8ce7307b93f970281c2f2484",
            "filename": "third_party/xla/xla/backends/gpu/codegen/llvm/llvm_emitter.h",
            "status": "modified",
            "additions": 88,
            "deletions": 0,
            "changes": 88,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6cde202c1bb99cb7eaa1eb52120e34b67024a02d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fllvm_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6cde202c1bb99cb7eaa1eb52120e34b67024a02d/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fllvm_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Fllvm%2Fllvm_emitter.h?ref=6cde202c1bb99cb7eaa1eb52120e34b67024a02d",
            "patch": "@@ -50,10 +50,98 @@ absl::StatusOr<ThunkSequence> EmitBitonicSortLLVMIR(\n     const HloSortInstruction* sort, llvm::Module* llvm_module,\n     IrEmitterContext* ir_emitter_context);\n \n+// Input = {static array, dynamic_dim0, dynamic_dim1}\n+// Output = {dynamic array(with dynamic dimension meta data at the end)}\n+// For a tensor with static dimension [2][<=5] and dynamic dimension [2][3]\n+// (`_` stands for padding)\n+// Input = {{1,2,3,_,_,4,5,6_,_}, 2, 3}\n+// Output = {{1,2,3,4,5,6,_,_,_,_,2,3}}\n+\n+// pseudo code for padToStatic on a 2d array\n+//   ```\n+// void padToStatic(int** input, int** output, int threads_per_block,\n+//                  int meta_data_offset, int max_num_element,\n+//                  int static_dim0_size, int static_dim1_size) {\n+//   int* source_array = input[0];\n+//   int* dest_array = output[0];\n+\n+//   // extract the dynamic dimension from the source array's metadata\n+//   int* dyn_dim0_size = source_array + meta_data_offset;\n+//   int* dyn_dim1_size = source_array + meta_data_offset + sizeof(int);\n+\n+//   // only one thread need to store the dynamic index\n+//   int thread_id = GetThreadId();\n+//   int block_id = GetBlockId();\n+//   if (thread_id == 0 && block_id == 0) {\n+//     *output[1] = *dyn_dim0_size;\n+//     *output[2] = *dyn_dim1_size;\n+//   }\n+\n+//   int dyn_element_total = 1;\n+//   dyn_element_total *= *dyn_dim0_size;\n+//   dyn_element_total *= *dyn_dim1_size;\n+//   linear_index = block_id * threads_per_block + thread_id;\n+//   if (linear_index < max_num_element) {\n+//     Index static_index =\n+//         delinerized(linerized_index, static_dim0_size, static_dim1_size);\n+//     if (linerized_index < dyn_element_total) {\n+//       Index dyn_index =\n+//           delinerized(linerized_index, *dyn_dim0_size, *dyn_dim1_size);\n+//       dest_array[dyn_index.dim0][dyn_index.dim1] =\n+//           source_array[static_index.dim0][static_index.dim1];\n+//     }\n+//   }\n+//   return;\n+// }\n+//   ```\n absl::StatusOr<ThunkSequence> EmitPadToStaticLLVMIR(\n     const HloCustomCallInstruction* hlo, llvm::Module* llvm_module,\n     IrEmitterContext* ir_emitter_context);\n \n+// Input = {dynamic array(with dynamic dimension meta data at the end)}\n+// Output = {static array, dynamic_dim0, dynamic_dim1}\n+// For a tensor with static dimension [2][<=5] and dynamic dimension [2][3]\n+// (`_` stands for padding)\n+// Input = {{1,2,3,4,5,6,_,_,_,_,2,3}}\n+// Output = {{1,2,3,_,_,4,5,6_,_}, 2, 3}\n+\n+// pseudo code for sliceToDynamic on a 2d array\n+//   ```\n+// void sliceToDynamic(int** input, int** output, int threads_per_block,\n+//                  int meta_data_offset, int max_num_element,\n+//                  int static_dim0_size, int static_dim1_size) {\n+//   int* source_array = input[0];\n+//   int* dest_array = output[0];\n+\n+//   // calculate the location where metadata needs to be inserted\n+//   int* dyn_dim0_size = dest_array + meta_data_offset;\n+//   int* dyn_dim1_size = dest_array + meta_data_offset + sizeof(int);\n+\n+//   // only one thread need to store the dynamic index\n+//   int thread_id = GetThreadId();\n+//   int block_id = GetBlockId();\n+//   if (thread_id == 0 && block_id == 0) {\n+//     *dyn_dim0_size = *output[1];\n+//     *dyn_dim1_size = *output[2];\n+//   }\n+\n+//   int dyn_element_total = 1;\n+//   dyn_element_total *= *dyn_dim0_size;\n+//   dyn_element_total *= *dyn_dim1_size;\n+//   linear_index = block_id * threads_per_block + thread_id;\n+//   if (linear_index < max_num_element) {\n+//     Index static_index =\n+//         delinerized(linerized_index, static_dim0_size, static_dim1_size);\n+//     if (linerized_index < dyn_element_total) {\n+//       Index dyn_index =\n+//           delinerized(linerized_index, *dyn_dim0_size, *dyn_dim1_size);\n+//       dest_array[static_index.dim0][static_index.dim1] =\n+//           source_array[dyn_index.dim0][dyn_index.dim1];\n+//     }\n+//   }\n+//   return;\n+// }\n+//   ```\n absl::StatusOr<ThunkSequence> EmitSliceToDynamicLLVMIR(\n     const HloCustomCallInstruction* hlo, llvm::Module* llvm_module,\n     IrEmitterContext* ir_emitter_context);"
        },
        {
            "sha": "89a20a6622de13a55f63ee690aa3b5436d56db0e",
            "filename": "third_party/xla/xla/service/gpu/compile_module_to_llvm_ir.cc",
            "status": "modified",
            "additions": 17,
            "deletions": 18,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6cde202c1bb99cb7eaa1eb52120e34b67024a02d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6cde202c1bb99cb7eaa1eb52120e34b67024a02d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fcompile_module_to_llvm_ir.cc?ref=6cde202c1bb99cb7eaa1eb52120e34b67024a02d",
            "patch": "@@ -191,24 +191,23 @@ absl::StatusOr<std::unique_ptr<SequentialThunk>> LowerHlo(\n     TF_RETURN_IF_ERROR(\n         LoadCache(ir_emitter_context, options.xla_gpu_kernel_cache_file()));\n   }\n-  std::unique_ptr<ThunkEmitter> thunk_emitter =\n-      ThunkEmitter::Create(&ir_emitter_context);\n-    XLA_SCOPED_LOGGING_TIMER(absl::StrCat(\n-        \"GpuCompiler::RunBackend - IR emission for \", hlo_module->name()));\n-\n-    TF_ASSIGN_OR_RETURN(auto thunks,\n-                        thunk_emitter->EmitHloEntryComputation(hlo_module));\n-\n-    RemoveUnusedAndUninitializedGlobals(\n-        platform_id, options, ir_emitter_context.llvm_module_constants(),\n-        ir_emitter_context.constants());\n-\n-    // This won't record values for calls that error out (because if they error\n-    // out we have no way of telling how far through the process we got).\n-    uint64_t end_usecs = tsl::Env::Default()->NowMicros();\n-    RecordHloToLlvmDuration(end_usecs - start_usecs);\n-    return std::make_unique<SequentialThunk>(Thunk::ThunkInfo{},\n-                                             std::move(thunks));\n+  auto thunk_emitter = std::make_unique<ThunkEmitter>(&ir_emitter_context);\n+  XLA_SCOPED_LOGGING_TIMER(absl::StrCat(\n+      \"GpuCompiler::RunBackend - IR emission for \", hlo_module->name()));\n+\n+  TF_ASSIGN_OR_RETURN(auto thunks,\n+                      thunk_emitter->EmitHloEntryComputation(hlo_module));\n+\n+  RemoveUnusedAndUninitializedGlobals(\n+      platform_id, options, ir_emitter_context.llvm_module_constants(),\n+      ir_emitter_context.constants());\n+\n+  // This won't record values for calls that error out (because if they error\n+  // out we have no way of telling how far through the process we got).\n+  uint64_t end_usecs = tsl::Env::Default()->NowMicros();\n+  RecordHloToLlvmDuration(end_usecs - start_usecs);\n+  return std::make_unique<SequentialThunk>(Thunk::ThunkInfo{},\n+                                           std::move(thunks));\n }\n \n }  // namespace"
        },
        {
            "sha": "be5761ba7b6cccc30988e03013a2ef267ea6fec8",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6cde202c1bb99cb7eaa1eb52120e34b67024a02d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6cde202c1bb99cb7eaa1eb52120e34b67024a02d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=6cde202c1bb99cb7eaa1eb52120e34b67024a02d",
            "patch": "@@ -3031,7 +3031,7 @@ GpuCompiler::LoadExecutableFromAotResult(\n     TF_RETURN_IF_ERROR(LoadCache(ir_emitter_context, cache_file_path));\n   }\n \n-  auto thunk_emitter = ThunkEmitter::Create(&ir_emitter_context);\n+  auto thunk_emitter = std::make_unique<ThunkEmitter>(&ir_emitter_context);\n   TF_ASSIGN_OR_RETURN(auto thunks,\n                       thunk_emitter->EmitHloEntryComputation(hlo_module.get()));\n "
        },
        {
            "sha": "ebf45eeb6458fd88bd40f6bdf3932da3d35968cc",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_context.h",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6cde202c1bb99cb7eaa1eb52120e34b67024a02d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6cde202c1bb99cb7eaa1eb52120e34b67024a02d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h?ref=6cde202c1bb99cb7eaa1eb52120e34b67024a02d",
            "patch": "@@ -156,6 +156,15 @@ class IrEmitterContext {\n         llvm_ir::SanitizeFunctionName(suggested_name));\n   }\n \n+  std::unique_ptr<llvm::Module> CreateLLVMModule(\n+      const std::string& module_name) {\n+    auto llvm_module =\n+        std::make_unique<llvm::Module>(module_name, llvm_module_->getContext());\n+    llvm_module->setTargetTriple(llvm::Triple(llvm_module_->getTargetTriple()));\n+    llvm_module->setDataLayout(llvm_module_->getDataLayout());\n+    return llvm_module;\n+  }\n+\n  private:\n   const HloModule* hlo_module_;\n   const BufferAssignment* buffer_assignment_;"
        },
        {
            "sha": "c3b6f0da6870f2de8dce0b3b15abbeb480bd05fd",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 431,
            "deletions": 455,
            "changes": 886,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6cde202c1bb99cb7eaa1eb52120e34b67024a02d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6cde202c1bb99cb7eaa1eb52120e34b67024a02d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc?ref=6cde202c1bb99cb7eaa1eb52120e34b67024a02d",
            "patch": "@@ -168,8 +168,7 @@ limitations under the License.\n #include \"tsl/platform/human_readable_json.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n \n-namespace xla {\n-namespace gpu {\n+namespace xla::gpu {\n namespace {\n \n // TODO: move into a host_execute specific file.\n@@ -213,6 +212,128 @@ EmitCollectiveKernelThunk(IrEmitterContext* ir_emitter_context,\n           .xla_gpu_unsupported_use_all_reduce_one_shot_kernel());\n }\n \n+// If the fusion instruction is a dynamic-slice-fusion instruction,\n+// with a collective hero operation, then this function returns the\n+// collective operation. Returns std::nullopt otherwise.\n+std::optional<const HloInstruction*> GetCollectiveHeroForDynamicSliceFusion(\n+    const HloFusionInstruction* instruction) {\n+  if (!IsDynamicSliceFusion(instruction)) {\n+    return std::nullopt;\n+  }\n+  return HloBfsFindIf(\n+      {instruction->fused_instructions_computation()->root_instruction()},\n+      [](const HloInstruction* instr) { return IsCollective(instr); });\n+}\n+\n+// Find the canonical send/recv start op for one of send, recv,\n+// send-done, or recv-done. For trivial cases send/recv and\n+// send-done/recv-done come in pairs and the canonical start op is\n+// the send/recv op of the pair. If send/recv is partially\n+// pipelined, we will use the send/recv leading into the while loop\n+// as the canonical start op, which will serve as a key for the\n+// async events.\n+//\n+// Example:\n+// ```\n+// send_ctx = send(src, ...)  <-- canonical start op\n+// send_ctx_final = while(send_ctx) {\n+//   send_ctx_in = parameter(0)\n+//   send-done(send_ctx_in)\n+//   ...\n+//   ROOT send_ctx_out = send(next_src, ...)\n+// }\n+// send-done(send_ctx_final)\n+// ```\n+static const HloInstruction* FindCanonicalSendRecvStartOp(\n+    const HloInstruction* inst) {\n+  CHECK(inst->opcode() == HloOpcode::kSend ||\n+        inst->opcode() == HloOpcode::kRecv ||\n+        inst->opcode() == HloOpcode::kSendDone ||\n+        inst->opcode() == HloOpcode::kRecvDone);\n+  // If the instruction is wrapped in an async computation, return\n+  // the instruction itself.\n+  if (inst->parent()->IsAsyncComputation()) {\n+    return inst;\n+  }\n+\n+  // Find container while loop and index for the send/recv case or\n+  // return canonical start op directly.\n+  const HloInstruction* while_op = nullptr;\n+  int64_t i = -1;\n+  if (inst->opcode() == HloOpcode::kSend ||\n+      inst->opcode() == HloOpcode::kRecv) {\n+    CHECK_EQ(inst->users().size(), 1);\n+    const HloInstruction* unique_user = inst->users().front();\n+\n+    // Return send/recv inst directly if this is a simple send/recv\n+    // pair.\n+    if (unique_user->opcode() == HloOpcode::kSendDone ||\n+        unique_user->opcode() == HloOpcode::kRecvDone) {\n+      return inst;\n+    }\n+\n+    // Find while loop and index, otherwise.\n+    CHECK(unique_user->opcode() == HloOpcode::kTuple ||\n+          unique_user->opcode() == HloOpcode::kWhile);\n+    if (unique_user->IsRoot()) {\n+      // send/recv op in the loop body.\n+      auto maybe_while_op =\n+          unique_user->parent()->GetUniqueCaller(HloOpcode::kWhile);\n+      CHECK(maybe_while_op);\n+      while_op = *maybe_while_op;\n+      i = unique_user->operand_index(inst);\n+    } else {\n+      // send/recv leading into the loop.\n+      CHECK_EQ(unique_user->users().size(), 1);\n+      CHECK(unique_user->users().front()->opcode() == HloOpcode::kWhile);\n+      while_op = unique_user->users().front();\n+      i = unique_user->operand_index(inst);\n+    }\n+  }\n+\n+  // Find container while loop and index for the send-done/recv-done\n+  // case or return canonical start op directly.\n+  if (inst->opcode() == HloOpcode::kSendDone ||\n+      inst->opcode() == HloOpcode::kRecvDone) {\n+    const HloInstruction* operand = inst->operand(0);\n+\n+    // Return send/recv inst directly if this is a simple send/recv\n+    // pair.\n+    if (operand->opcode() == HloOpcode::kSend ||\n+        operand->opcode() == HloOpcode::kRecv) {\n+      return operand;\n+    }\n+\n+    // Find while loop and index, otherwise.\n+    CHECK(operand->opcode() == HloOpcode::kGetTupleElement);\n+    const auto* gte = Cast<HloGetTupleElementInstruction>(operand);\n+    const HloInstruction* iter_tuple = operand->operand(0);\n+    if (iter_tuple->opcode() == HloOpcode::kParameter) {\n+      // send-done/recv-done in the loop body.\n+      CHECK(Cast<HloParameterInstruction>(iter_tuple)->parameter_number() == 0);\n+      auto maybe_while =\n+          iter_tuple->parent()->GetUniqueCaller(HloOpcode::kWhile);\n+      CHECK(maybe_while);\n+      while_op = *maybe_while;\n+      i = gte->tuple_index();\n+    } else {\n+      // send-done/recv-done proceeding the loop.\n+      CHECK(iter_tuple->opcode() == HloOpcode::kWhile);\n+      while_op = iter_tuple;\n+      i = gte->tuple_index();\n+    }\n+  }\n+\n+  // Extract canonical start op from while loop's init.\n+  CHECK(while_op != nullptr);\n+  CHECK(0 <= i && i < while_op->shape().tuple_shapes().size());\n+  const HloInstruction* init = while_op->operand(0);\n+  const HloInstruction* canonical_start_op = init->operand(i);\n+  CHECK(canonical_start_op->opcode() == HloOpcode::kSend ||\n+        canonical_start_op->opcode() == HloOpcode::kRecv);\n+  return canonical_start_op;\n+}\n+\n }  // namespace\n \n ThunkEmitter::ThunkEmitter(IrEmitterContext* ir_emitter_context)\n@@ -222,12 +343,8 @@ ThunkEmitter::ThunkEmitter(IrEmitterContext* ir_emitter_context)\n       nvshmem_buffer_addresses_(std::make_shared<NvshmemBufferAddresses>()),\n       call_graph_(CallGraph::Build(&ir_emitter_context->hlo_module())) {}\n \n-std::unique_ptr<ThunkEmitter> ThunkEmitter::Create(\n-    IrEmitterContext* ir_emitter_context) {\n-  return std::unique_ptr<ThunkEmitter>(new ThunkEmitter(ir_emitter_context));\n-}\n-\n-absl::Status ThunkEmitter::EmitConstant(const HloConstantInstruction* instr) {\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitConstant(\n+    const HloConstantInstruction* instr) {\n   TF_ASSIGN_OR_RETURN(DenseDataIntermediate content,\n                       LiteralToXlaFormat(instr->literal()));\n \n@@ -245,7 +362,7 @@ absl::Status ThunkEmitter::EmitConstant(const HloConstantInstruction* instr) {\n       ir_emitter_context_->llvm_module_constants(), num_elements, element_bytes,\n       global_name, slice.index(), std::move(content));\n   ir_emitter_context_->constants().push_back(std::move(info));\n-  return absl::OkStatus();\n+  return ThunkSequence{};\n }\n \n ThunkSequence GetThunkSequence(std::unique_ptr<Thunk> ir_emitter) {\n@@ -285,12 +402,12 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitConditional(\n       slice, std::move(branch_thunks), branch_index_is_bool));\n }\n \n-// Input = {dynamic array(with dynamic dimension meta data at the\n-// end)} Output = {static array, dynamic_dim0, dynamic_dim1}\n+// Input = {dynamic array(with dynamic dimension meta data at the end)}\n+// Output = {static array, dynamic_dim0, dynamic_dim1}\n absl::StatusOr<ThunkSequence> ThunkEmitter::EmitPadToStatic(\n     const HloCustomCallInstruction* instr) {\n   std::string ir_name = std::string(instr->name());\n-  auto local_llvm_module = ir_emitter_context_->CreateLocalLLVMModule(ir_name);\n+  auto local_llvm_module = ir_emitter_context_->CreateLLVMModule(ir_name);\n \n   TF_ASSIGN_OR_RETURN(auto thunk_sequence,\n                       EmitPadToStaticLLVMIR(instr, local_llvm_module.get(),\n@@ -301,12 +418,12 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitPadToStatic(\n   return thunk_sequence;\n }\n \n-// Input = {dynamic array(with dynamic dimension meta data at the\n-// end)} Output = {static array, dynamic_dim0, dynamic_dim1}\n+// Input = {dynamic array(with dynamic dimension meta data at the end)}\n+// Output = {static array, dynamic_dim0, dynamic_dim1}\n absl::StatusOr<ThunkSequence> ThunkEmitter::EmitSliceToDynamic(\n     const HloCustomCallInstruction* instr) {\n   std::string ir_name = std::string(instr->name());\n-  auto local_llvm_module = ir_emitter_context_->CreateLocalLLVMModule(ir_name);\n+  auto local_llvm_module = ir_emitter_context_->CreateLLVMModule(ir_name);\n \n   TF_ASSIGN_OR_RETURN(auto thunk_sequence,\n                       EmitSliceToDynamicLLVMIR(instr, local_llvm_module.get(),\n@@ -1126,8 +1243,8 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTritonCustomCall(\n         TritonCall::Parse(instr->raw_backend_config_string(), &mlir_context);\n     auto kernel_name = ir_emitter_context_->GetSanitizedUniqueName(call.name);\n     VLOG(3) << \"Generating: \" << kernel_name;\n-    auto local_module = ir_emitter_context_->CreateLocalLLVMModule(kernel_name);\n \n+    auto local_module = ir_emitter_context_->CreateLLVMModule(kernel_name);\n     mlir::OwningOpRef<mlir::ModuleOp> triton_module;\n     {\n       mlir::BaseScopedDiagnosticHandler diagnostic_handler(&mlir_context);\n@@ -1185,6 +1302,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTritonCustomCall(\n                              kernel_name, launch_dimensions, kernel_arguments)\n                              .status());\n     }\n+\n     TF_RET_CHECK(!llvm::Linker::linkModules(\n         *ir_emitter_context_->llvm_module(), std::move(local_module),\n         llvm::Linker::Flags::OverrideFromSrc));\n@@ -1361,19 +1479,41 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitWhile(\n     trip_count = config.known_trip_count().n();\n   }\n \n+  HloComputation* condition = instr->while_condition();\n+  HloComputation* body = instr->while_body();\n+\n+  // Generate thunk sequence for while 'condition'.\n+  TF_ASSIGN_OR_RETURN(auto cond_thunks, EmitHloComputation(condition));\n+\n+  // Generate thunk sequence for while 'body'.\n+  TF_ASSIGN_OR_RETURN(auto body_thunks, EmitHloComputation(body));\n+\n+  // Buffer slice holding while loop predicate.\n   TF_ASSIGN_OR_RETURN(\n-      auto thunk,\n-      BuildWhileThunk(instr,\n-                      Thunk::ThunkInfo::WithProfileAnnotation(\n-                          instr, ir_emitter_context_->GetNextThunkId()),\n-                      trip_count));\n-  return GetThunkSequence(std::move(thunk));\n+      auto pred, GetAllocationSliceForHlo(condition->root_instruction(), {}));\n+\n+  Thunk::ThunkInfo while_thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n+      instr, ir_emitter_context_->GetNextThunkId());\n+  Thunk::ThunkInfo cond_thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n+      instr, ir_emitter_context_->GetNextThunkId());\n+  cond_thunk_info.profile_annotation += \"_condition\";\n+  Thunk::ThunkInfo body_thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n+      instr, ir_emitter_context_->GetNextThunkId());\n+  body_thunk_info.profile_annotation += \"_body\";\n+\n+  return GetThunkSequence(\n+      std::make_unique<WhileThunk>(while_thunk_info, instr, pred,\n+                                   std::make_unique<SequentialThunk>(\n+                                       cond_thunk_info, std::move(cond_thunks)),\n+                                   std::make_unique<SequentialThunk>(\n+                                       body_thunk_info, std::move(body_thunks)),\n+                                   trip_count));\n }\n \n absl::StatusOr<ThunkSequence> ThunkEmitter::EmitRngGetAndUpdateState(\n     const HloRngGetAndUpdateStateInstruction* instr) {\n   std::string ir_name = std::string(instr->name());\n-  auto local_llvm_module = ir_emitter_context_->CreateLocalLLVMModule(ir_name);\n+  auto local_llvm_module = ir_emitter_context_->CreateLLVMModule(ir_name);\n \n   TF_ASSIGN_OR_RETURN(auto thunk_sequence,\n                       EmitRngGetAndUpdateStateLLVMIR(\n@@ -1426,7 +1566,7 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitSort(\n     }\n   }\n \n-  auto local_llvm_module = ir_emitter_context_->CreateLocalLLVMModule(op_name);\n+  auto local_llvm_module = ir_emitter_context_->CreateLLVMModule(op_name);\n \n   TF_ASSIGN_OR_RETURN(ThunkSequence sort_thunks,\n                       EmitBitonicSortLLVMIR(sort, local_llvm_module.get(),\n@@ -1705,115 +1845,6 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCollectiveThunk(\n   return GetThunkSequence(std::move(thunk));\n }\n \n-// Find the canonical send/recv start op for one of send, recv,\n-// send-done, or recv-done. For trivial cases send/recv and\n-// send-done/recv-done come in pairs and the canonical start op is\n-// the send/recv op of the pair. If send/recv is partially\n-// pipelined, we will use the send/recv leading into the while loop\n-// as the canonical start op, which will serve as a key for the\n-// async events.\n-//\n-// Example:\n-// ```\n-// send_ctx = send(src, ...)  <-- canonical start op\n-// send_ctx_final = while(send_ctx) {\n-//   send_ctx_in = parameter(0)\n-//   send-done(send_ctx_in)\n-//   ...\n-//   ROOT send_ctx_out = send(next_src, ...)\n-// }\n-// send-done(send_ctx_final)\n-// ```\n-static const HloInstruction* FindCanonicalSendRecvStartOp(\n-    const HloInstruction* inst) {\n-  CHECK(inst->opcode() == HloOpcode::kSend ||\n-        inst->opcode() == HloOpcode::kRecv ||\n-        inst->opcode() == HloOpcode::kSendDone ||\n-        inst->opcode() == HloOpcode::kRecvDone);\n-  // If the instruction is wrapped in an async computation, return\n-  // the instruction itself.\n-  if (inst->parent()->IsAsyncComputation()) {\n-    return inst;\n-  }\n-\n-  // Find container while loop and index for the send/recv case or\n-  // return canonical start op directly.\n-  const HloInstruction* while_op = nullptr;\n-  int64_t i = -1;\n-  if (inst->opcode() == HloOpcode::kSend ||\n-      inst->opcode() == HloOpcode::kRecv) {\n-    CHECK_EQ(inst->users().size(), 1);\n-    const HloInstruction* unique_user = inst->users().front();\n-\n-    // Return send/recv inst directly if this is a simple send/recv\n-    // pair.\n-    if (unique_user->opcode() == HloOpcode::kSendDone ||\n-        unique_user->opcode() == HloOpcode::kRecvDone) {\n-      return inst;\n-    }\n-\n-    // Find while loop and index, otherwise.\n-    CHECK(unique_user->opcode() == HloOpcode::kTuple ||\n-          unique_user->opcode() == HloOpcode::kWhile);\n-    if (unique_user->IsRoot()) {\n-      // send/recv op in the loop body.\n-      auto maybe_while_op =\n-          unique_user->parent()->GetUniqueCaller(HloOpcode::kWhile);\n-      CHECK(maybe_while_op);\n-      while_op = *maybe_while_op;\n-      i = unique_user->operand_index(inst);\n-    } else {\n-      // send/recv leading into the loop.\n-      CHECK_EQ(unique_user->users().size(), 1);\n-      CHECK(unique_user->users().front()->opcode() == HloOpcode::kWhile);\n-      while_op = unique_user->users().front();\n-      i = unique_user->operand_index(inst);\n-    }\n-  }\n-\n-  // Find container while loop and index for the send-done/recv-done\n-  // case or return canonical start op directly.\n-  if (inst->opcode() == HloOpcode::kSendDone ||\n-      inst->opcode() == HloOpcode::kRecvDone) {\n-    const HloInstruction* operand = inst->operand(0);\n-\n-    // Return send/recv inst directly if this is a simple send/recv\n-    // pair.\n-    if (operand->opcode() == HloOpcode::kSend ||\n-        operand->opcode() == HloOpcode::kRecv) {\n-      return operand;\n-    }\n-\n-    // Find while loop and index, otherwise.\n-    CHECK(operand->opcode() == HloOpcode::kGetTupleElement);\n-    const auto* gte = Cast<HloGetTupleElementInstruction>(operand);\n-    const HloInstruction* iter_tuple = operand->operand(0);\n-    if (iter_tuple->opcode() == HloOpcode::kParameter) {\n-      // send-done/recv-done in the loop body.\n-      CHECK(Cast<HloParameterInstruction>(iter_tuple)->parameter_number() == 0);\n-      auto maybe_while =\n-          iter_tuple->parent()->GetUniqueCaller(HloOpcode::kWhile);\n-      CHECK(maybe_while);\n-      while_op = *maybe_while;\n-      i = gte->tuple_index();\n-    } else {\n-      // send-done/recv-done proceeding the loop.\n-      CHECK(iter_tuple->opcode() == HloOpcode::kWhile);\n-      while_op = iter_tuple;\n-      i = gte->tuple_index();\n-    }\n-  }\n-\n-  // Extract canonical start op from while loop's init.\n-  CHECK(while_op != nullptr);\n-  CHECK(0 <= i && i < while_op->shape().tuple_shapes().size());\n-  const HloInstruction* init = while_op->operand(0);\n-  const HloInstruction* canonical_start_op = init->operand(i);\n-  CHECK(canonical_start_op->opcode() == HloOpcode::kSend ||\n-        canonical_start_op->opcode() == HloOpcode::kRecv);\n-  return canonical_start_op;\n-}\n-\n std::vector<const HloInstruction*> GetRealDependencyInstructions(\n     const HloInstruction* instr) {\n   std::vector<const HloInstruction*> real_deps;\n@@ -2124,38 +2155,6 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitOutfeed(\n       std::move(shaped_slices)));\n }\n \n-absl::StatusOr<std::unique_ptr<Thunk>> ThunkEmitter::BuildWhileThunk(\n-    const HloInstruction* instr, const Thunk::ThunkInfo& thunk_info,\n-    std::optional<int64_t> trip_count) {\n-  HloComputation* condition = instr->while_condition();\n-  HloComputation* body = instr->while_body();\n-\n-  // Generate thunk sequence for while 'condition'.\n-  TF_ASSIGN_OR_RETURN(auto cond_thunks, EmitHloComputation(condition));\n-\n-  // Generate thunk sequence for while 'body'.\n-  TF_ASSIGN_OR_RETURN(auto body_thunks, EmitHloComputation(body));\n-\n-  // Buffer slice holding while loop predicate.\n-  TF_ASSIGN_OR_RETURN(\n-      auto pred, GetAllocationSliceForHlo(condition->root_instruction(), {}));\n-\n-  Thunk::ThunkInfo cond_thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n-      instr, ir_emitter_context_->GetNextThunkId());\n-  cond_thunk_info.profile_annotation += \"_condition\";\n-  Thunk::ThunkInfo body_thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n-      instr, ir_emitter_context_->GetNextThunkId());\n-  body_thunk_info.profile_annotation += \"_body\";\n-\n-  return std::unique_ptr<Thunk>(\n-      new WhileThunk(thunk_info, instr, pred,\n-                     std::make_unique<SequentialThunk>(cond_thunk_info,\n-                                                       std::move(cond_thunks)),\n-                     std::make_unique<SequentialThunk>(body_thunk_info,\n-                                                       std::move(body_thunks)),\n-                     trip_count));\n-}\n-\n static absl::flat_hash_map<std::string, std::string> ConvertFrontendAttributes(\n     const FrontendAttributes& attrs) {\n   absl::flat_hash_map<std::string, std::string> result;\n@@ -2458,40 +2457,231 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitRecvDoneThunk(\n       *instr->channel_id(), send_recv_events_, DeviceConstraint(instr)));\n }\n \n-// If the fusion instruction is a dynamic-slice-fusion instruction,\n-// with a collective hero operation, then this function returns the\n-// collective operation. Returns std::nullopt otherwise.\n-std::optional<const HloInstruction*> GetCollectiveHeroForDynamicSliceFusion(\n-    const HloFusionInstruction* instruction) {\n-  if (!IsDynamicSliceFusion(instruction)) {\n-    return std::nullopt;\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitAsyncDone(\n+    const HloInstruction* instr) {\n+  if (!instr->async_wrapped_computation()->CanExpandIntoSingleInstruction()) {\n+    return EmitCollectiveAsyncDone(Thunk::kGroupDone, instr);\n+  }\n+  const HloInstruction* wrapped = instr->async_wrapped_instruction();\n+  ThunkSequence thunks;\n+  switch (wrapped->opcode()) {\n+    case HloOpcode::kReduceScatter:\n+      return EmitCollectiveAsyncDone(Thunk::kReduceScatterDone, instr);\n+    case HloOpcode::kAllToAll:\n+      return EmitCollectiveAsyncDone(Thunk::kAllToAllDone, instr);\n+    case HloOpcode::kRaggedAllToAll:\n+      return EmitCollectiveAsyncDone(Thunk::kRaggedAllToAllDone, instr);\n+    case HloOpcode::kCollectiveBroadcast:\n+      return EmitCollectiveAsyncDone(Thunk::kCollectiveBroadcastDone, instr);\n+    case HloOpcode::kCollectivePermute:\n+      return EmitCollectiveAsyncDone(Thunk::kCollectivePermuteDone, instr);\n+    case HloOpcode::kFusion: {\n+      auto collective_hero = GetCollectiveHeroForDynamicSliceFusion(\n+          Cast<HloFusionInstruction>(wrapped));\n+      if (collective_hero.has_value()) {\n+        switch ((*collective_hero)->opcode()) {\n+          case HloOpcode::kReduceScatter: {\n+            TF_ASSIGN_OR_RETURN(\n+                auto async_done_thunks,\n+                EmitCollectiveAsyncDone(Thunk::kReduceScatterDone, instr));\n+            AppendThunkSequence(thunks, async_done_thunks);\n+            break;\n+          }\n+          default:\n+            return absl::InternalError(\n+                absl::StrFormat(\"Unhandled collective in dynamic slice fusion \"\n+                                \"instruction: %s\",\n+                                (*collective_hero)\n+                                    ->fused_instructions_computation()\n+                                    ->ToString()));\n+        }\n+      }\n+      // We still want to emit the stream done thunk.\n+      [[clang::fallthrough]];\n+    }\n+    case HloOpcode::kCall:\n+    case HloOpcode::kCustomCall: {\n+      if (IsHostExecuteCustomCall(*wrapped)) {\n+        auto custom_call = Cast<HloCustomCallInstruction>(wrapped);\n+\n+        auto async_events =\n+            GetInstructionToHostExecuteAsyncEvents().at(custom_call);\n+\n+        thunks.push_back(std::make_unique<HostExecuteDoneThunk>(\n+            Thunk::ThunkInfo::WithProfileAnnotation(\n+                instr, ir_emitter_context_->GetNextThunkId()),\n+            async_events));\n+        return thunks;\n+      }\n+      // Wait until the concurrent stream has finished.\n+      auto* async_done = Cast<HloAsyncInstruction>(instr);\n+      const ExecutionStreamAssignment& stream_assignment =\n+          ir_emitter_context_->execution_stream_assignment();\n+      TF_ASSIGN_OR_RETURN(\n+          ExecutionStreamAssignment::AsyncExecutionStreamIds streams,\n+          stream_assignment.GetAsyncExecutionStreamIds(async_done));\n+      thunks.push_back(std::make_unique<WaitForStreamsThunk>(\n+          Thunk::ThunkInfo::WithProfileAnnotation(\n+              instr, ir_emitter_context_->GetNextThunkId()),\n+          streams.source_stream_id, streams.destination_stream_id));\n+      return thunks;\n+    }\n+    default:\n+      return Internal(\"Unsupported async done wrapped instruction: %s\",\n+                      HloOpcodeString(wrapped->opcode()));\n+  }\n+}\n+\n+absl::StatusOr<ThunkSequence> ThunkEmitter::EmitAsyncStart(\n+    const HloInstruction* instr) {\n+  // Multi-op async start will emit a NCCL group thunk.\n+  if (!instr->async_wrapped_computation()->CanExpandIntoSingleInstruction()) {\n+    return EmitCollectiveGroupStartThunk(instr);\n+  }\n+  const HloInstruction* wrapped = instr->async_wrapped_instruction();\n+  switch (wrapped->opcode()) {\n+    case HloOpcode::kReduceScatter: {\n+      auto* reduce_scatter = Cast<HloReduceScatterInstruction>(wrapped);\n+      return EmitCollectiveThunk<ReduceScatterStartThunk,\n+                                 HloReduceScatterInstruction>(\n+          Thunk::kReduceScatter, instr, reduce_scatter,\n+          reduce_scatter->use_global_device_ids());\n+    }\n+    case HloOpcode::kAllToAll: {\n+      auto* all_to_all = Cast<HloAllToAllInstruction>(wrapped);\n+      return EmitCollectiveThunk<AllToAllStartThunk, HloAllToAllInstruction>(\n+          Thunk::kAllToAll, instr, all_to_all, std::nullopt);\n+    }\n+    case HloOpcode::kRaggedAllToAll: {\n+      auto* ragged_all_to_all = Cast<HloRaggedAllToAllInstruction>(wrapped);\n+      return EmitCollectiveThunk<RaggedAllToAllStartThunk,\n+                                 HloRaggedAllToAllInstruction>(\n+          Thunk::kRaggedAllToAll, instr, ragged_all_to_all, std::nullopt);\n+    }\n+    case HloOpcode::kCollectiveBroadcast: {\n+      auto* collective_broadcast =\n+          Cast<HloCollectiveBroadcastInstruction>(wrapped);\n+      return EmitCollectiveThunk<CollectiveBroadcastStartThunk,\n+                                 HloCollectiveBroadcastInstruction>(\n+          Thunk::kCollectiveBroadcast, instr, collective_broadcast,\n+          std::nullopt);\n+    }\n+    case HloOpcode::kFusion: {\n+      // We'll launch the fusion computation on a concurrent\n+      // stream. The concurrent stream needs to first wait until\n+      // the main stream has finished calculating any values\n+      // that may be used as inputs to the fusion computation.\n+      // We enforce this by inlining a `WaitForStreams` thunk.\n+      auto* async_start = Cast<HloAsyncInstruction>(instr);\n+      const ExecutionStreamAssignment& stream_assignment =\n+          ir_emitter_context_->execution_stream_assignment();\n+      TF_ASSIGN_OR_RETURN(\n+          ExecutionStreamAssignment::AsyncExecutionStreamIds streams,\n+          stream_assignment.GetAsyncExecutionStreamIds(async_start));\n+      ThunkSequence thunks =\n+          GetThunkSequence(std::make_unique<WaitForStreamsThunk>(\n+              Thunk::ThunkInfo::WithProfileAnnotation(\n+                  instr, ir_emitter_context_->GetNextThunkId()),\n+              streams.destination_stream_id, streams.source_stream_id));\n+\n+      TF_ASSIGN_OR_RETURN(ThunkSequence fusion_thunks,\n+                          EmitFusion(Cast<HloFusionInstruction>(wrapped)));\n+      AppendThunkSequence(thunks, fusion_thunks);\n+      return thunks;\n+    }\n+    case HloOpcode::kCall: {\n+      return EmitAsyncComputation(instr);\n+    }\n+    case HloOpcode::kCustomCall: {\n+      if (IsHostExecuteCustomCall(*wrapped)) {\n+        auto custom_call = Cast<HloCustomCallInstruction>(wrapped);\n+\n+        std::unique_ptr<HloModule> hlo_module =\n+            ExtractComputationIntoNewModule(*custom_call->called_computation());\n+\n+        // All offloaded computations are marked as host computations from\n+        // the perspective of the GPU backend. Since these will execute on\n+        // the main thread from the CPU backend perspective, we need to mark\n+        // them as such.\n+        for (auto* computation : hlo_module->computations()) {\n+          computation->SetExecutionThread(HloInstruction::kMainExecutionThread);\n+        }\n+\n+        absl::InlinedVector<HostExecuteStartThunk::SliceAndShape, 4>\n+            operand_slices;\n+        for (HloInstruction* operand : wrapped->operands()) {\n+          for (auto& indexed : ShapeUtil::GetLeafShapes(operand->shape())) {\n+            TF_ASSIGN_OR_RETURN(\n+                auto slice,\n+                ir_emitter_context_->buffer_assignment().GetUniqueSlice(\n+                    operand, indexed.index));\n+            operand_slices.push_back({slice, indexed.shape});\n+          }\n+        }\n+\n+        // Collect buffer slices for all results.\n+        absl::InlinedVector<HostExecuteStartThunk::SliceAndShape, 4>\n+            result_slices;\n+        for (auto& indexed : ShapeUtil::GetLeafShapes(wrapped->shape())) {\n+          TF_ASSIGN_OR_RETURN(\n+              auto slice,\n+              ir_emitter_context_->buffer_assignment().GetUniqueSlice(\n+                  wrapped, indexed.index));\n+          result_slices.push_back({slice, indexed.shape});\n+        }\n+\n+        HostOffloadingExecutableProto host_offloading_executable_proto;\n+        *host_offloading_executable_proto.mutable_hlo_module() =\n+            hlo_module->ToProto();\n+        host_offloading_executable_proto.set_executable_type(\n+            HostOffloadingExecutableProto::EXECUTABLE_TYPE_NANORT);\n+\n+        TF_ASSIGN_OR_RETURN(\n+            auto thunk,\n+            HostExecuteStartThunk::Create(\n+                Thunk::ThunkInfo::WithProfileAnnotation(\n+                    instr, ir_emitter_context_->GetNextThunkId()),\n+                std::move(host_offloading_executable_proto),\n+                std::move(operand_slices), std::move(result_slices)));\n+\n+        auto async_events = thunk->async_events();\n+\n+        auto [it, inserted] = GetInstructionToHostExecuteAsyncEvents().emplace(\n+            custom_call, async_events);\n+        if (!inserted) {\n+          return Internal(\n+              \"Async events already exist for host offloading custom call \"\n+              \"%s.\",\n+              custom_call->ToString());\n+        }\n+        return GetThunkSequence(std::move(thunk));\n+      }\n+      return EmitAsyncCustomCallStart(instr);\n+    }\n+    default:\n+      return Internal(\"Unsupported async start wrapped instruction: %s\",\n+                      HloOpcodeString(wrapped->opcode()));\n   }\n-  return HloBfsFindIf(\n-      {instruction->fused_instructions_computation()->root_instruction()},\n-      [](const HloInstruction* instr) { return IsCollective(instr); });\n }\n \n absl::StatusOr<ThunkSequence> ThunkEmitter::EmitHloInstruction(\n-    const HloInstruction* instr, bool emit_group_thunks) {\n-  switch (instr->opcode()) {\n+    const HloInstruction* hlo, bool emit_group_thunks) {\n+  switch (hlo->opcode()) {\n     case HloOpcode::kAllGatherDone:\n-      return EmitCollectiveAsyncDone(Thunk::kAllGatherDone, instr);\n+      return EmitCollectiveAsyncDone(Thunk::kAllGatherDone, hlo);\n     case HloOpcode::kAllGatherStart: {\n-      auto* all_gather = Cast<HloAllGatherInstruction>(instr);\n+      auto* all_gather = Cast<HloAllGatherInstruction>(hlo);\n       return EmitCollectiveThunk<AllGatherStartThunk, HloAllGatherInstruction>(\n           Thunk::kAllGatherStart, all_gather, all_gather,\n           all_gather->use_global_device_ids());\n     }\n-\n-    case HloOpcode::kAllReduceDone: {\n-      if (IsNvshmemCollective(instr)) {\n-        return EmitNvshmemAsyncDone(Thunk::kNvshmemAllReduceDone, instr);\n-      }\n-      return EmitCollectiveAsyncDone(Thunk::kAllReduceDone, instr);\n-    }\n+    case HloOpcode::kAllReduceDone:\n+      return IsNvshmemCollective(hlo)\n+                 ? EmitNvshmemAsyncDone(Thunk::kNvshmemAllReduceDone, hlo)\n+                 : EmitCollectiveAsyncDone(Thunk::kAllReduceDone, hlo);\n     case HloOpcode::kAllReduceStart: {\n-      auto* all_reduce = Cast<HloAllReduceInstruction>(instr);\n-      if (IsNvshmemCollective(instr)) {\n+      auto* all_reduce = Cast<HloAllReduceInstruction>(hlo);\n+      if (IsNvshmemCollective(hlo)) {\n         return EmitNvshmemThunk<NvshmemAllReduceStartThunk,\n                                 HloAllReduceInstruction>(\n             Thunk::kNvshmemAllReduceStart, all_reduce, all_reduce,\n@@ -2501,331 +2691,119 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitHloInstruction(\n           Thunk::kAllReduceStart, all_reduce, all_reduce,\n           all_reduce->use_global_device_ids());\n     }\n-    case HloOpcode::kAsyncDone: {\n-      if (!instr->async_wrapped_computation()\n-               ->CanExpandIntoSingleInstruction()) {\n-        return EmitCollectiveAsyncDone(Thunk::kGroupDone, instr);\n-      }\n-      const HloInstruction* wrapped = instr->async_wrapped_instruction();\n-      ThunkSequence thunks;\n-      switch (wrapped->opcode()) {\n-        case HloOpcode::kReduceScatter:\n-          return EmitCollectiveAsyncDone(Thunk::kReduceScatterDone, instr);\n-        case HloOpcode::kAllToAll:\n-          return EmitCollectiveAsyncDone(Thunk::kAllToAllDone, instr);\n-        case HloOpcode::kRaggedAllToAll:\n-          return EmitCollectiveAsyncDone(Thunk::kRaggedAllToAllDone, instr);\n-        case HloOpcode::kCollectiveBroadcast:\n-          return EmitCollectiveAsyncDone(Thunk::kCollectiveBroadcastDone,\n-                                         instr);\n-        case HloOpcode::kCollectivePermute:\n-          return EmitCollectiveAsyncDone(Thunk::kCollectivePermuteDone, instr);\n-        case HloOpcode::kFusion: {\n-          auto collective_hero = GetCollectiveHeroForDynamicSliceFusion(\n-              Cast<HloFusionInstruction>(wrapped));\n-          if (collective_hero.has_value()) {\n-            switch ((*collective_hero)->opcode()) {\n-              case HloOpcode::kReduceScatter: {\n-                TF_ASSIGN_OR_RETURN(\n-                    auto async_done_thunks,\n-                    EmitCollectiveAsyncDone(Thunk::kReduceScatterDone, instr));\n-                AppendThunkSequence(thunks, async_done_thunks);\n-                break;\n-              }\n-              default:\n-                return absl::InternalError(absl::StrFormat(\n-                    \"Unhandled collective in dynamic slice fusion \"\n-                    \"instruction: %s\",\n-                    (*collective_hero)\n-                        ->fused_instructions_computation()\n-                        ->ToString()));\n-            }\n-          }\n-          // We still want to emit the stream done thunk.\n-          [[clang::fallthrough]];\n-        }\n-        case HloOpcode::kCall:\n-        case HloOpcode::kCustomCall: {\n-          if (IsHostExecuteCustomCall(*wrapped)) {\n-            auto custom_call = Cast<HloCustomCallInstruction>(wrapped);\n-\n-            auto async_events =\n-                GetInstructionToHostExecuteAsyncEvents().at(custom_call);\n-\n-            thunks.push_back(std::make_unique<HostExecuteDoneThunk>(\n-                Thunk::ThunkInfo::WithProfileAnnotation(\n-                    instr, ir_emitter_context_->GetNextThunkId()),\n-                async_events));\n-            return thunks;\n-          }\n-          // Wait until the concurrent stream has finished.\n-          auto* async_done = Cast<HloAsyncInstruction>(instr);\n-          const ExecutionStreamAssignment& stream_assignment =\n-              ir_emitter_context_->execution_stream_assignment();\n-          TF_ASSIGN_OR_RETURN(\n-              ExecutionStreamAssignment::AsyncExecutionStreamIds streams,\n-              stream_assignment.GetAsyncExecutionStreamIds(async_done));\n-          thunks.push_back(std::make_unique<WaitForStreamsThunk>(\n-              Thunk::ThunkInfo::WithProfileAnnotation(\n-                  instr, ir_emitter_context_->GetNextThunkId()),\n-              streams.source_stream_id, streams.destination_stream_id));\n-          return thunks;\n-        }\n-        default:\n-          return Internal(\"Unsupported async done wrapped instruction: %s\",\n-                          HloOpcodeString(wrapped->opcode()));\n-      }\n-    }\n-    case HloOpcode::kAsyncStart: {\n-      // Multi-op async start will emit a NCCL group thunk.\n-      if (!instr->async_wrapped_computation()\n-               ->CanExpandIntoSingleInstruction()) {\n-        return EmitCollectiveGroupStartThunk(instr);\n-      }\n-      const HloInstruction* wrapped = instr->async_wrapped_instruction();\n-      switch (wrapped->opcode()) {\n-        case HloOpcode::kReduceScatter: {\n-          auto* reduce_scatter = Cast<HloReduceScatterInstruction>(wrapped);\n-          return EmitCollectiveThunk<ReduceScatterStartThunk,\n-                                     HloReduceScatterInstruction>(\n-              Thunk::kReduceScatter, instr, reduce_scatter,\n-              reduce_scatter->use_global_device_ids());\n-        }\n-        case HloOpcode::kAllToAll: {\n-          auto* all_to_all = Cast<HloAllToAllInstruction>(wrapped);\n-          return EmitCollectiveThunk<AllToAllStartThunk,\n-                                     HloAllToAllInstruction>(\n-              Thunk::kAllToAll, instr, all_to_all, std::nullopt);\n-        }\n-        case HloOpcode::kRaggedAllToAll: {\n-          auto* ragged_all_to_all = Cast<HloRaggedAllToAllInstruction>(wrapped);\n-          return EmitCollectiveThunk<RaggedAllToAllStartThunk,\n-                                     HloRaggedAllToAllInstruction>(\n-              Thunk::kRaggedAllToAll, instr, ragged_all_to_all, std::nullopt);\n-        }\n-        case HloOpcode::kCollectiveBroadcast: {\n-          auto* collective_broadcast =\n-              Cast<HloCollectiveBroadcastInstruction>(wrapped);\n-          return EmitCollectiveThunk<CollectiveBroadcastStartThunk,\n-                                     HloCollectiveBroadcastInstruction>(\n-              Thunk::kCollectiveBroadcast, instr, collective_broadcast,\n-              std::nullopt);\n-        }\n-        case HloOpcode::kFusion: {\n-          // We'll launch the fusion computation on a concurrent\n-          // stream. The concurrent stream needs to first wait until\n-          // the main stream has finished calculating any values\n-          // that may be used as inputs to the fusion computation.\n-          // We enforce this by inlining a `WaitForStreams` thunk.\n-          auto* async_start = Cast<HloAsyncInstruction>(instr);\n-          const ExecutionStreamAssignment& stream_assignment =\n-              ir_emitter_context_->execution_stream_assignment();\n-          TF_ASSIGN_OR_RETURN(\n-              ExecutionStreamAssignment::AsyncExecutionStreamIds streams,\n-              stream_assignment.GetAsyncExecutionStreamIds(async_start));\n-          ThunkSequence thunks =\n-              GetThunkSequence(std::make_unique<WaitForStreamsThunk>(\n-                  Thunk::ThunkInfo::WithProfileAnnotation(\n-                      instr, ir_emitter_context_->GetNextThunkId()),\n-                  streams.destination_stream_id, streams.source_stream_id));\n-\n-          TF_ASSIGN_OR_RETURN(ThunkSequence fusion_thunks,\n-                              EmitFusion(Cast<HloFusionInstruction>(wrapped)));\n-          AppendThunkSequence(thunks, fusion_thunks);\n-          return thunks;\n-        }\n-        case HloOpcode::kCall: {\n-          return EmitAsyncComputation(instr);\n-        }\n-        case HloOpcode::kCustomCall: {\n-          if (IsHostExecuteCustomCall(*wrapped)) {\n-            auto custom_call = Cast<HloCustomCallInstruction>(wrapped);\n-\n-            std::unique_ptr<HloModule> hlo_module =\n-                ExtractComputationIntoNewModule(\n-                    *custom_call->called_computation());\n-\n-            // All offloaded computations are marked as host computations from\n-            // the perspective of the GPU backend. Since these will execute on\n-            // the main thread from the CPU backend perspective, we need to mark\n-            // them as such.\n-            for (auto* computation : hlo_module->computations()) {\n-              computation->SetExecutionThread(\n-                  HloInstruction::kMainExecutionThread);\n-            }\n-\n-            absl::InlinedVector<HostExecuteStartThunk::SliceAndShape, 4>\n-                operand_slices;\n-            for (HloInstruction* operand : wrapped->operands()) {\n-              for (auto& indexed : ShapeUtil::GetLeafShapes(operand->shape())) {\n-                TF_ASSIGN_OR_RETURN(\n-                    auto slice,\n-                    ir_emitter_context_->buffer_assignment().GetUniqueSlice(\n-                        operand, indexed.index));\n-                operand_slices.push_back({slice, indexed.shape});\n-              }\n-            }\n-\n-            // Collect buffer slices for all results.\n-            absl::InlinedVector<HostExecuteStartThunk::SliceAndShape, 4>\n-                result_slices;\n-            for (auto& indexed : ShapeUtil::GetLeafShapes(wrapped->shape())) {\n-              TF_ASSIGN_OR_RETURN(\n-                  auto slice,\n-                  ir_emitter_context_->buffer_assignment().GetUniqueSlice(\n-                      wrapped, indexed.index));\n-              result_slices.push_back({slice, indexed.shape});\n-            }\n-\n-            HostOffloadingExecutableProto host_offloading_executable_proto;\n-            *host_offloading_executable_proto.mutable_hlo_module() =\n-                hlo_module->ToProto();\n-            host_offloading_executable_proto.set_executable_type(\n-                HostOffloadingExecutableProto::EXECUTABLE_TYPE_NANORT);\n-\n-            TF_ASSIGN_OR_RETURN(\n-                auto thunk,\n-                HostExecuteStartThunk::Create(\n-                    Thunk::ThunkInfo::WithProfileAnnotation(\n-                        instr, ir_emitter_context_->GetNextThunkId()),\n-                    std::move(host_offloading_executable_proto),\n-                    std::move(operand_slices), std::move(result_slices)));\n-\n-            auto async_events = thunk->async_events();\n-\n-            auto [it, inserted] =\n-                GetInstructionToHostExecuteAsyncEvents().emplace(custom_call,\n-                                                                 async_events);\n-            if (!inserted) {\n-              return Internal(\n-                  \"Async events already exist for host offloading custom call \"\n-                  \"%s.\",\n-                  custom_call->ToString());\n-            }\n-            return GetThunkSequence(std::move(thunk));\n-          }\n-          return EmitAsyncCustomCallStart(instr);\n-        }\n-        default:\n-          return Internal(\"Unsupported async start wrapped instruction: %s\",\n-                          HloOpcodeString(wrapped->opcode()));\n-      }\n-    }\n-\n+    case HloOpcode::kAsyncDone:\n+      return EmitAsyncDone(hlo);\n+    case HloOpcode::kAsyncStart:\n+      return EmitAsyncStart(hlo);\n     case HloOpcode::kCall:\n-      return EmitCommandBufferThunk(instr);\n+      return EmitCommandBufferThunk(hlo);\n     case HloOpcode::kCollectivePermuteDone:\n-      if (IsNvshmemCollective(instr)) {\n-        return EmitNvshmemAsyncDone(Thunk::kNvshmemCollectivePermuteDone,\n-                                    instr);\n-      } else {\n-        return EmitCollectiveAsyncDone(Thunk::kCollectivePermuteDone, instr);\n-      }\n+      return IsNvshmemCollective(hlo)\n+                 ? EmitNvshmemAsyncDone(Thunk::kNvshmemCollectivePermuteDone,\n+                                        hlo)\n+                 : EmitCollectiveAsyncDone(Thunk::kCollectivePermuteDone, hlo);\n     case HloOpcode::kCollectivePermuteStart:\n-      return EmitCollectivePermute(\n-          Cast<HloCollectivePermuteInstruction>(instr));\n+      return EmitCollectivePermute(Cast<HloCollectivePermuteInstruction>(hlo));\n     case HloOpcode::kConditional:\n-      return EmitConditional(instr);\n-    case HloOpcode::kConstant: {\n-      TF_RETURN_IF_ERROR(EmitConstant(Cast<HloConstantInstruction>(instr)));\n-      return ThunkSequence{};\n-    }\n+      return EmitConditional(hlo);\n+    case HloOpcode::kConstant:\n+      return EmitConstant(Cast<HloConstantInstruction>(hlo));\n     case HloOpcode::kCustomCall: {\n-      auto* custom_call = Cast<HloCustomCallInstruction>(instr);\n-      if (IsLegacyCublasMatmul(*instr)) {\n+      auto* custom_call = Cast<HloCustomCallInstruction>(hlo);\n+      if (IsLegacyCublasMatmul(*hlo)) {\n         return EmitGemmThunk(custom_call);\n       }\n-      if (IsCublasLtMatmul(*instr)) {\n+      if (IsCublasLtMatmul(*hlo)) {\n         return EmitCublasLtMatmulThunk(custom_call);\n       }\n-      if (IsCublasLtMatmulF8(*instr)) {\n+      if (IsCublasLtMatmulF8(*hlo)) {\n         return EmitCublasLtMatmulThunkF8(custom_call);\n       }\n-      if (IsCudnnConvolutionReorder(*instr)) {\n+      if (IsCudnnConvolutionReorder(*hlo)) {\n         return EmitConvolutionReorderThunk(custom_call);\n       }\n-      if (IsCustomCallToDnnNorm(*instr)) {\n+      if (IsCustomCallToDnnNorm(*hlo)) {\n         return EmitNormThunk(custom_call);\n       }\n-      if (IsCustomCallTofMHA(*instr) || IsCustomCallTofMHAF8(*instr) ||\n-          IsCustomCallToBlockScaledDot(*instr)) {\n+      if (IsCustomCallTofMHA(*hlo) || IsCustomCallTofMHAF8(*hlo) ||\n+          IsCustomCallToBlockScaledDot(*hlo)) {\n         return EmitCuDnnThunk(custom_call);\n       }\n-      if (IsCustomCallToPtxKernel(*instr)) {\n+      if (IsCustomCallToPtxKernel(*hlo)) {\n         return EmitPtxCustomCall(custom_call);\n       }\n-      if (IsCustomCallToTopK(*instr)) {\n+      if (IsCustomCallToTopK(*hlo)) {\n         return EmitTopKCustomCall(custom_call);\n       }\n-      if (IsCustomCallToDnnConvolution(*instr)) {\n+      if (IsCustomCallToDnnConvolution(*hlo)) {\n         return EmitConvolutionThunk(custom_call);\n       }\n-      if (IsTriangularSolve(*instr)) {\n-        return EmitTriangularSolveCustomCall(instr);\n+      if (IsTriangularSolve(*hlo)) {\n+        return EmitTriangularSolveCustomCall(hlo);\n       }\n-      if (IsCubDeviceRadixSort(*instr)) {\n+      if (IsCubDeviceRadixSort(*hlo)) {\n         return EmitCubDeviceRadixSort(custom_call);\n       }\n       if (custom_call->custom_call_target() == \"PadToStatic\") {\n         return EmitPadToStatic(custom_call);\n       }\n-      if (instr->custom_call_target() == \"SliceToDynamic\") {\n+      if (hlo->custom_call_target() == \"SliceToDynamic\") {\n         return EmitSliceToDynamic(custom_call);\n       }\n-      if (instr->custom_call_target() == \"__gpu$xla.gpu.triton\") {\n+      if (hlo->custom_call_target() == \"__gpu$xla.gpu.triton\") {\n         // TODO(slebedev): Remove this after June 15th 2025.\n         return EmitTritonCustomCall(custom_call);\n       }\n-      if (instr->custom_call_target() == kNopCustomCallTarget) {\n+      if (hlo->custom_call_target() == kNopCustomCallTarget) {\n         return ThunkSequence{};\n       }\n-      if (instr->custom_call_target() == kPinCustomCallTarget ||\n-          instr->custom_call_target() == kUnpinCustomCallTarget ||\n-          instr->custom_call_target() == kCreateBufferCustomCallTarget) {\n+      if (hlo->custom_call_target() == kPinCustomCallTarget ||\n+          hlo->custom_call_target() == kUnpinCustomCallTarget ||\n+          hlo->custom_call_target() == kCreateBufferCustomCallTarget) {\n         return ThunkSequence{};\n       }\n-      if (instr->custom_call_target() == kCollectiveMetadataCustomCallTarget) {\n-        return EmitCollectiveMetadata(instr);\n+      if (hlo->custom_call_target() == kCollectiveMetadataCustomCallTarget) {\n+        return EmitCollectiveMetadata(hlo);\n       }\n       return EmitCustomCallThunk(custom_call);\n     }\n     case HloOpcode::kFusion:\n-      return EmitFusion(Cast<HloFusionInstruction>(instr));\n+      return EmitFusion(Cast<HloFusionInstruction>(hlo));\n     case HloOpcode::kCopy:\n-      return EmitCopy(instr);\n+      return EmitCopy(hlo);\n     case HloOpcode::kInfeed:\n-      return EmitInfeed(Cast<HloInfeedInstruction>(instr));\n+      return EmitInfeed(Cast<HloInfeedInstruction>(hlo));\n     case HloOpcode::kOutfeed:\n-      return EmitOutfeed(Cast<HloOutfeedInstruction>(instr));\n+      return EmitOutfeed(Cast<HloOutfeedInstruction>(hlo));\n     case HloOpcode::kPartitionId:\n-      return EmitReplicaOrPartitionId<PartitionIdThunk>(instr);\n+      return EmitReplicaOrPartitionId<PartitionIdThunk>(hlo);\n     case HloOpcode::kFft:\n-      return EmitFftThunk(Cast<HloFftInstruction>(instr));\n+      return EmitFftThunk(Cast<HloFftInstruction>(hlo));\n \n     case HloOpcode::kRecv:\n-      return EmitRecvThunk(Cast<HloRecvInstruction>(instr), emit_group_thunks);\n+      return EmitRecvThunk(Cast<HloRecvInstruction>(hlo), emit_group_thunks);\n     case HloOpcode::kRecvDone:\n-      return EmitRecvDoneThunk(Cast<HloRecvDoneInstruction>(instr));\n+      return EmitRecvDoneThunk(Cast<HloRecvDoneInstruction>(hlo));\n \n     case HloOpcode::kReplicaId:\n-      return EmitReplicaOrPartitionId<ReplicaIdThunk>(instr);\n+      return EmitReplicaOrPartitionId<ReplicaIdThunk>(hlo);\n     case HloOpcode::kRngGetAndUpdateState:\n       return EmitRngGetAndUpdateState(\n-          Cast<HloRngGetAndUpdateStateInstruction>(instr));\n+          Cast<HloRngGetAndUpdateStateInstruction>(hlo));\n \n     case HloOpcode::kSend:\n-      return EmitSendThunk(Cast<HloSendInstruction>(instr), emit_group_thunks);\n+      return EmitSendThunk(Cast<HloSendInstruction>(hlo), emit_group_thunks);\n     case HloOpcode::kSendDone:\n-      return EmitSendDoneThunk(Cast<HloSendDoneInstruction>(instr));\n+      return EmitSendDoneThunk(Cast<HloSendDoneInstruction>(hlo));\n \n     case HloOpcode::kSort:\n-      return EmitSort(Cast<HloSortInstruction>(instr));\n+      return EmitSort(Cast<HloSortInstruction>(hlo));\n     case HloOpcode::kWhile:\n-      return EmitWhile(instr);\n+      return EmitWhile(hlo);\n     case HloOpcode::kCopyStart:\n-      return EmitCopyStartThunk(Cast<HloCopyStartInstruction>(instr));\n+      return EmitCopyStartThunk(Cast<HloCopyStartInstruction>(hlo));\n     case HloOpcode::kCopyDone:\n-      return EmitCopyDoneThunk(instr);\n+      return EmitCopyDoneThunk(hlo);\n \n     // HLO module is already scheduled, so instructions for ordering\n     // are noops.\n@@ -2840,9 +2818,8 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitHloInstruction(\n       return ThunkSequence{};\n     default:\n       return Internal(\"Unsupported instruction opcode: %s\",\n-                      HloOpcodeString(instr->opcode()));\n+                      HloOpcodeString(hlo->opcode()));\n   }\n-\n   return Internal(\"Unhandled HLO instruction\");\n }\n \n@@ -2894,5 +2871,4 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitHloComputation(\n   return thunk_sequence;\n }\n \n-}  // namespace gpu\n-}  // namespace xla\n+}  // namespace xla::gpu"
        },
        {
            "sha": "e5fc5e47a3ac5c42d40b5c0b162d6483c41354d5",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.h",
            "status": "modified",
            "additions": 106,
            "deletions": 187,
            "changes": 293,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6cde202c1bb99cb7eaa1eb52120e34b67024a02d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6cde202c1bb99cb7eaa1eb52120e34b67024a02d/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.h?ref=6cde202c1bb99cb7eaa1eb52120e34b67024a02d",
            "patch": "@@ -16,20 +16,15 @@ limitations under the License.\n #ifndef XLA_SERVICE_GPU_THUNK_EMITTER_H_\n #define XLA_SERVICE_GPU_THUNK_EMITTER_H_\n \n-#include <array>\n #include <cstdint>\n-#include <functional>\n #include <memory>\n #include <optional>\n #include <string>\n-#include <utility>\n #include <vector>\n \n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n-#include \"llvm/IR/Type.h\"\n-#include \"llvm/IR/Value.h\"\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/backends/gpu/runtime/copy_thunk.h\"\n@@ -45,8 +40,7 @@ limitations under the License.\n #include \"xla/service/gpu/ir_emitter_context.h\"\n #include \"xla/shape_util.h\"\n \n-namespace xla {\n-namespace gpu {\n+namespace xla::gpu {\n \n // Emits Thunks for the given HLO module.\n class ThunkEmitter {\n@@ -55,18 +49,14 @@ class ThunkEmitter {\n     return ir_emitter_context_->platform_name();\n   }\n \n+  explicit ThunkEmitter(IrEmitterContext* ir_emitter_context);\n   ThunkEmitter(const ThunkEmitter&) = delete;\n   ThunkEmitter& operator=(const ThunkEmitter&) = delete;\n \n-  static std::unique_ptr<ThunkEmitter> Create(\n-      IrEmitterContext* ir_emitter_context);\n-\n   absl::StatusOr<ThunkSequence> EmitHloEntryComputation(\n       const HloModule* module);\n \n  private:\n-  explicit ThunkEmitter(IrEmitterContext* ir_emitter_context);\n-\n   // Emits code for the given HLO computation.\n   //\n   // Also populates related information to 'ir_emitter_context_' for\n@@ -77,73 +67,83 @@ class ThunkEmitter {\n   absl::StatusOr<ThunkSequence> EmitHloComputation(\n       const HloComputation* computation);\n \n-  absl::StatusOr<ThunkSequence> EmitCommandBufferThunk(\n-      const HloInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitHloInstruction(\n+      const HloInstruction* hlo, bool emit_group_thunks = false);\n \n-  // ThunkEmitter handles the following instructions differently from\n-  // IrEmitter. It also mixes in some special handling for custom kernels\n-  // via the ThunkEmitter.\n-  absl::Status EmitConstant(const HloConstantInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitAsyncStart(const HloInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitAsyncComputation(const HloInstruction* hlo);\n \n-  absl::StatusOr<ThunkSequence> EmitConditional(const HloInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitConvolutionThunk(\n-      const HloCustomCallInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitGemmThunk(\n-      const HloCustomCallInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitCublasLtMatmulThunk(\n-      const HloCustomCallInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitCublasLtMatmulThunkF8(\n-      const HloCustomCallInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitConvolutionReorderThunk(\n-      const HloCustomCallInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitNormThunk(\n-      const HloCustomCallInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitCuDnnThunk(\n-      const HloCustomCallInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitPtxCustomCall(\n-      const HloCustomCallInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitCubDeviceRadixSort(\n-      const HloCustomCallInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitCustomCallThunk(\n-      const HloCustomCallInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitFftThunk(const HloFftInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitAsyncComputation(\n-      const HloInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitFusion(const HloFusionInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitCopy(const HloInstruction* instr);\n   absl::StatusOr<ThunkSequence> EmitAsyncCustomCallStart(\n-      const HloInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitWhile(const HloInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitInfeed(const HloInfeedInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitOutfeed(const HloOutfeedInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitRngGetAndUpdateState(\n-      const HloRngGetAndUpdateStateInstruction* instr);\n+      const HloInstruction* hlo);\n \n-  absl::StatusOr<ThunkSequence> EmitSort(const HloSortInstruction* sort);\n-  absl::StatusOr<ThunkSequence> EmitTriangularSolveCustomCall(\n-      const HloInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitTopKCustomCall(\n-      const HloCustomCallInstruction* instr);\n-  absl::StatusOr<ThunkSequence> EmitTritonCustomCall(\n-      const HloCustomCallInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitAsyncDone(const HloInstruction* hlo);\n \n-  absl::StatusOr<ThunkSequence> EmitSendThunk(const HloSendInstruction* instr,\n-                                              bool emit_group_thunks);\n-  absl::StatusOr<ThunkSequence> EmitSendDoneThunk(\n-      const HloSendDoneInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitCommandBufferThunk(\n+      const HloInstruction* hlo);\n \n-  absl::StatusOr<ThunkSequence> EmitRecvThunk(const HloRecvInstruction* instr,\n-                                              bool emit_group_thunks);\n-  absl::StatusOr<ThunkSequence> EmitRecvDoneThunk(\n-      const HloRecvDoneInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitCollectiveAsyncDone(\n+      Thunk::Kind kind, const HloInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitCollectiveGroupStartThunk(\n+      const HloInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitCollectiveMetadata(\n+      const HloInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitCollectivePermute(\n+      const HloCollectivePermuteInstruction* hlo);\n \n   template <typename CollectiveThunkType, typename HloInstType>\n   absl::StatusOr<ThunkSequence> EmitCollectiveThunk(\n       Thunk::Kind kind, const HloInstruction* async_start,\n       const HloInstType* inst, std::optional<bool> use_global_device_ids);\n \n-  absl::StatusOr<ThunkSequence> EmitCollectiveAsyncDone(\n-      Thunk::Kind kind, const HloInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitConditional(const HloInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitConstant(const HloConstantInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitConvolutionReorderThunk(\n+      const HloCustomCallInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitConvolutionThunk(\n+      const HloCustomCallInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitCopy(const HloInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitCopyStartThunk(\n+      const HloCopyStartInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitCopyDoneThunk(const HloInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitCuDnnThunk(\n+      const HloCustomCallInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitCubDeviceRadixSort(\n+      const HloCustomCallInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitCublasLtMatmulThunk(\n+      const HloCustomCallInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitCublasLtMatmulThunkF8(\n+      const HloCustomCallInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitCustomCallThunk(\n+      const HloCustomCallInstruction* hlo);\n+\n+  template <typename HloInstType>\n+  absl::StatusOr<ThunkSequence> EmitDegeneratedCollectiveThunk(\n+      std::vector<CollectiveThunk::Buffer>& buffers,\n+      const HloInstruction* async_start, const HloInstType* inst);\n+\n+  absl::StatusOr<ThunkSequence> EmitFusion(const HloFusionInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitFftThunk(const HloFftInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitGemmThunk(\n+      const HloCustomCallInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitInfeed(const HloInfeedInstruction* hlo);\n \n   template <typename NvshmemAllReduceThunkType,\n             typename HloAllReduceInstruction>\n@@ -152,139 +152,59 @@ class ThunkEmitter {\n       const HloAllReduceInstruction* inst,\n       std::optional<bool> use_global_device_ids);\n \n-  absl::StatusOr<ThunkSequence> EmitNvshmemAsyncDone(\n-      Thunk::Kind kind, const HloInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitNvshmemAsyncDone(Thunk::Kind kind,\n+                                                     const HloInstruction* hlo);\n \n-  template <typename HloInstType>\n-  absl::StatusOr<ThunkSequence> EmitDegeneratedCollectiveThunk(\n-      std::vector<CollectiveThunk::Buffer>& buffers,\n-      const HloInstruction* async_start, const HloInstType* inst);\n+  absl::StatusOr<ThunkSequence> EmitNormThunk(\n+      const HloCustomCallInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitOutfeed(const HloOutfeedInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitPadToStatic(\n+      const HloCustomCallInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitPtxCustomCall(\n+      const HloCustomCallInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitRecvDoneThunk(\n+      const HloRecvDoneInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitRecvThunk(const HloRecvInstruction* hlo,\n+                                              bool emit_group_thunks);\n \n   template <typename ThunkType>\n   absl::StatusOr<ThunkSequence> EmitReplicaOrPartitionId(\n-      const HloInstruction* instr);\n+      const HloInstruction* hlo);\n \n-  absl::StatusOr<ThunkSequence> EmitCollectiveMetadata(\n-      const HloInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitRngGetAndUpdateState(\n+      const HloRngGetAndUpdateStateInstruction* hlo);\n \n-  absl::StatusOr<ThunkSequence> EmitCollectivePermute(\n-      const HloCollectivePermuteInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitSliceToDynamic(\n+      const HloCustomCallInstruction* hlo);\n \n-  absl::StatusOr<ThunkSequence> EmitCopyStartThunk(\n-      const HloCopyStartInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitSendDoneThunk(\n+      const HloSendDoneInstruction* hlo);\n \n-  absl::StatusOr<ThunkSequence> EmitCopyDoneThunk(const HloInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitSendThunk(const HloSendInstruction* hlo,\n+                                              bool emit_group_thunks);\n \n-  absl::StatusOr<ThunkSequence> EmitHloInstruction(\n-      const HloInstruction* instr, bool emit_group_thunks = false);\n+  absl::StatusOr<ThunkSequence> EmitSort(const HloSortInstruction* sort);\n \n-  absl::StatusOr<ThunkSequence> EmitCollectiveGroupStartThunk(\n-      const HloInstruction* instr);\n-\n-  // Input = {static array, dynamic_dim0, dynamic_dim1}\n-  // Output = {dynamic array(with dynamic dimension meta data at the end)}\n-  // For a tensor with static dimension [2][<=5] and dynamic dimension [2][3]\n-  // (`_` stands for padding)\n-  // Input = {{1,2,3,_,_,4,5,6_,_}, 2, 3}\n-  // Output = {{1,2,3,4,5,6,_,_,_,_,2,3}}\n-\n-  // pseudo code for padToStatic on a 2d array\n-  //   ```\n-  // void padToStatic(int** input, int** output, int threads_per_block,\n-  //                  int meta_data_offset, int max_num_element,\n-  //                  int static_dim0_size, int static_dim1_size) {\n-  //   int* source_array = input[0];\n-  //   int* dest_array = output[0];\n-\n-  //   // extract the dynamic dimension from the source array's metadata\n-  //   int* dyn_dim0_size = source_array + meta_data_offset;\n-  //   int* dyn_dim1_size = source_array + meta_data_offset + sizeof(int);\n-\n-  //   // only one thread need to store the dynamic index\n-  //   int thread_id = GetThreadId();\n-  //   int block_id = GetBlockId();\n-  //   if (thread_id == 0 && block_id == 0) {\n-  //     *output[1] = *dyn_dim0_size;\n-  //     *output[2] = *dyn_dim1_size;\n-  //   }\n-\n-  //   int dyn_element_total = 1;\n-  //   dyn_element_total *= *dyn_dim0_size;\n-  //   dyn_element_total *= *dyn_dim1_size;\n-  //   linear_index = block_id * threads_per_block + thread_id;\n-  //   if (linear_index < max_num_element) {\n-  //     Index static_index =\n-  //         delinerized(linerized_index, static_dim0_size, static_dim1_size);\n-  //     if (linerized_index < dyn_element_total) {\n-  //       Index dyn_index =\n-  //           delinerized(linerized_index, *dyn_dim0_size, *dyn_dim1_size);\n-  //       dest_array[dyn_index.dim0][dyn_index.dim1] =\n-  //           source_array[static_index.dim0][static_index.dim1];\n-  //     }\n-  //   }\n-  //   return;\n-  // }\n-  //   ```\n-  absl::StatusOr<ThunkSequence> EmitPadToStatic(\n-      const HloCustomCallInstruction* instr);\n-\n-  // Input = {dynamic array(with dynamic dimension meta data at the end)}\n-  // Output = {static array, dynamic_dim0, dynamic_dim1}\n-  // For a tensor with static dimension [2][<=5] and dynamic dimension [2][3]\n-  // (`_` stands for padding)\n-  // Input = {{1,2,3,4,5,6,_,_,_,_,2,3}}\n-  // Output = {{1,2,3,_,_,4,5,6_,_}, 2, 3}\n-\n-  // pseudo code for sliceToDynamic on a 2d array\n-  //   ```\n-  // void sliceToDynamic(int** input, int** output, int threads_per_block,\n-  //                  int meta_data_offset, int max_num_element,\n-  //                  int static_dim0_size, int static_dim1_size) {\n-  //   int* source_array = input[0];\n-  //   int* dest_array = output[0];\n-\n-  //   // calculate the location where metadata needs to be inserted\n-  //   int* dyn_dim0_size = dest_array + meta_data_offset;\n-  //   int* dyn_dim1_size = dest_array + meta_data_offset + sizeof(int);\n-\n-  //   // only one thread need to store the dynamic index\n-  //   int thread_id = GetThreadId();\n-  //   int block_id = GetBlockId();\n-  //   if (thread_id == 0 && block_id == 0) {\n-  //     *dyn_dim0_size = *output[1];\n-  //     *dyn_dim1_size = *output[2];\n-  //   }\n-\n-  //   int dyn_element_total = 1;\n-  //   dyn_element_total *= *dyn_dim0_size;\n-  //   dyn_element_total *= *dyn_dim1_size;\n-  //   linear_index = block_id * threads_per_block + thread_id;\n-  //   if (linear_index < max_num_element) {\n-  //     Index static_index =\n-  //         delinerized(linerized_index, static_dim0_size, static_dim1_size);\n-  //     if (linerized_index < dyn_element_total) {\n-  //       Index dyn_index =\n-  //           delinerized(linerized_index, *dyn_dim0_size, *dyn_dim1_size);\n-  //       dest_array[static_index.dim0][static_index.dim1] =\n-  //           source_array[dyn_index.dim0][dyn_index.dim1];\n-  //     }\n-  //   }\n-  //   return;\n-  // }\n-  //   ```\n-  absl::StatusOr<ThunkSequence> EmitSliceToDynamic(\n-      const HloCustomCallInstruction* instr);\n+  absl::StatusOr<ThunkSequence> EmitTopKCustomCall(\n+      const HloCustomCallInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitTriangularSolveCustomCall(\n+      const HloInstruction* hlo);\n+\n+  absl::StatusOr<ThunkSequence> EmitTritonCustomCall(\n+      const HloCustomCallInstruction* hlo);\n \n-  // Returns a WhileThunk that invokes thunk sequences for 'condition' and\n-  // 'body' sub-computations of while instruction.\n-  absl::StatusOr<std::unique_ptr<Thunk>> BuildWhileThunk(\n-      const HloInstruction* instr, const Thunk::ThunkInfo& thunk_info,\n-      std::optional<int64_t> trip_count);\n+  absl::StatusOr<ThunkSequence> EmitWhile(const HloInstruction* hlo);\n \n   absl::Status AssertNonDeterminismIsOkay(const std::string& op_name);\n \n   absl::StatusOr<BufferAllocation::Slice> GetAllocationSliceForHlo(\n-      const HloInstruction* instr, const ShapeIndex& index = {}) const;\n+      const HloInstruction* hlo, const ShapeIndex& index = {}) const;\n \n   CollectivesAsyncEvents& GetCollectivesAsyncEvents() {\n     return ir_emitter_context_->collectives_async_events();\n@@ -309,7 +229,6 @@ class ThunkEmitter {\n   std::unique_ptr<CallGraph> call_graph_;\n };\n \n-}  // namespace gpu\n-}  // namespace xla\n+}  // namespace xla::gpu\n \n #endif  // XLA_SERVICE_GPU_THUNK_EMITTER_H_"
        }
    ],
    "stats": {
        "total": 1313,
        "additions": 652,
        "deletions": 661
    }
}