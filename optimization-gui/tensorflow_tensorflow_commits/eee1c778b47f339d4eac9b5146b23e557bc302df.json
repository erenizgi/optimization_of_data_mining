{
    "author": "pifon2a",
    "message": "[XLA:GPU] Add a pass to convert TritonGemmConfig to BlockLevelConfig w/o nesting.\n\nPiperOrigin-RevId: 849730168",
    "sha": "eee1c778b47f339d4eac9b5146b23e557bc302df",
    "files": [
        {
            "sha": "06498377cbdec6f7c296cc842494d55ea2b101a2",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 3,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport.cc?ref=eee1c778b47f339d4eac9b5146b23e557bc302df",
            "patch": "@@ -472,6 +472,14 @@ CodegenDecision AreDotAlgorithmInputAndOutputConversionsSupported(\n   return CodegenDecision::Allow();\n }\n \n+bool IsAnnotatedWithTileSizes(const HloInstruction& instr) {\n+  if (!instr.has_backend_config()) {\n+    return false;\n+  }\n+  auto tile_sizes = instr.backend_config<Tile>();\n+  return tile_sizes.ok() && tile_sizes->sizes_size() > 0;\n+}\n+\n CodegenDecision IsTritonSupportedDot(\n     const HloDotInstruction& dot, const se::GpuComputeCapability& gpu_version) {\n   if (!IsInTritonNestedGemmFusion(dot)) {\n@@ -484,10 +492,14 @@ CodegenDecision IsTritonSupportedDot(\n   PrimitiveType lhs_type = lhs_shape.element_type();\n   PrimitiveType rhs_type = rhs_shape.element_type();\n \n-  if (dot.operand(0)->opcode() != HloOpcode::kFusion ||\n-      dot.operand(1)->opcode() != HloOpcode::kFusion) {\n+  bool both_operands_are_nested =\n+      dot.operand(0)->opcode() == HloOpcode::kFusion &&\n+      dot.operand(1)->opcode() == HloOpcode::kFusion;\n+  bool contraction_tile_size_is_set = IsAnnotatedWithTileSizes(dot);\n+  if (!contraction_tile_size_is_set && !both_operands_are_nested) {\n     return CodegenDecision::Forbid(\n-        \"Only operands that are fusions are supported.\");\n+        \"Only operands that are fusions are supported if the dot does not have \"\n+        \"a contraction tile size set.\");\n   }\n \n   auto types_are = [&](PrimitiveType compare1, PrimitiveType compare2) {"
        },
        {
            "sha": "805ffebfd53236ddfce80f27d213cdbee986d68d",
            "filename": "third_party/xla/xla/hlo/utils/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2FBUILD?ref=eee1c778b47f339d4eac9b5146b23e557bc302df",
            "patch": "@@ -245,6 +245,7 @@ cc_library(\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/types:span\",\n     ],\n )\n \n@@ -259,10 +260,11 @@ xla_cc_test(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/parser:hlo_parser\",\n         \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/hlo/testlib:pattern_matcher_gmock\",\n+        \"//xla/service:pattern_matcher\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n-        \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n "
        },
        {
            "sha": "d68e982b1c9c1660e9ce871ba9370a6b40b6cb08",
            "filename": "third_party/xla/xla/hlo/utils/hlo_query.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_query.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_query.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_query.cc?ref=eee1c778b47f339d4eac9b5146b23e557bc302df",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/log/check.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -217,6 +218,15 @@ HloInstruction* GetFirstInstructionWithOpcode(const HloComputation& computation,\n   return it == instructions.end() ? nullptr : *it;\n }\n \n+HloInstruction* GetFirstInstructionWithOpcode(\n+    const HloComputation& computation, absl::Span<const HloOpcode> opcodes) {\n+  auto instructions = computation.instructions();\n+  auto it = absl::c_find_if(instructions, [&](HloInstruction* instr) {\n+    return absl::c_linear_search(opcodes, instr->opcode());\n+  });\n+  return it == instructions.end() ? nullptr : *it;\n+}\n+\n bool ContainsInstrWithOpcode(const HloComputation* comp,\n                              const absl::flat_hash_set<HloOpcode>& opcodes) {\n   for (const auto* instr : comp->instructions()) {"
        },
        {
            "sha": "0f8166f73374b15d1c141f1aca7fdf7e2c14c31e",
            "filename": "third_party/xla/xla/hlo/utils/hlo_query.h",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_query.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_query.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_query.h?ref=eee1c778b47f339d4eac9b5146b23e557bc302df",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n \n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -95,6 +96,11 @@ bool IsEffectiveParameter(const HloInstruction&);\n HloInstruction* GetFirstInstructionWithOpcode(const HloComputation& computation,\n                                               HloOpcode opcode);\n \n+// Returns first HLO of the computation with one of the opcodes, otherwise\n+// nullptr.\n+HloInstruction* GetFirstInstructionWithOpcode(\n+    const HloComputation& computation, absl::Span<const HloOpcode> opcodes);\n+\n // Applies `fn` to a collection of instruction with `opcode` for a given\n // `computation`.\n template <typename Fn>"
        },
        {
            "sha": "b87575eb3352cdabf71abcb05b3f0981938fb7af",
            "filename": "third_party/xla/xla/hlo/utils/hlo_query_test.cc",
            "status": "modified",
            "additions": 63,
            "deletions": 15,
            "changes": 78,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_query_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_query_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_query_test.cc?ref=eee1c778b47f339d4eac9b5146b23e557bc302df",
            "patch": "@@ -16,7 +16,9 @@ limitations under the License.\n #include \"xla/hlo/utils/hlo_query.h\"\n \n #include <memory>\n+#include <vector>\n \n+#include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n@@ -26,12 +28,15 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/parser/hlo_parser.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/hlo/testlib/pattern_matcher_gmock.h\"\n+#include \"xla/service/pattern_matcher.h\"\n #include \"xla/util.h\"\n-#include \"tsl/platform/statusor.h\"\n \n namespace xla {\n namespace {\n \n+namespace m = ::xla::match;\n+\n using HloQueryTest = HloHardwareIndependentTestBase;\n \n template <typename Hlo>\n@@ -83,8 +88,8 @@ ENTRY main {\n   ROOT _ = (f32[32],f32[32],f32[32],f32[32],f32[32],f32[32],f32[32]) tuple(comp.0,add.0,add.1,sub.0,mul.0,mul.1,mul.2)\n })\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnUnverifiedModule(kHloString));\n+  ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                       ParseAndReturnUnverifiedModule(kHloString));\n   EXPECT_EQ(CountInstructions(*module, HloOpcode::kAdd), 2);\n   EXPECT_EQ(CountInstructions(*module, HloOpcode::kSubtract), 1);\n   EXPECT_EQ(CountInstructions(*module, HloOpcode::kMultiply), 3);\n@@ -125,8 +130,8 @@ ENTRY main {\n   ROOT _ = (f32[32],f32[32],f32[32],f32[32]) tuple(add.0,sub.0,mul.0,comp.0)\n })\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnUnverifiedModule(kHloString));\n+  ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                       ParseAndReturnUnverifiedModule(kHloString));\n   HloComputation* computation = module->GetComputationWithName(\"computation.0\");\n   EXPECT_EQ(CountInstructions(*computation, HloOpcode::kAdd), 2);\n   EXPECT_EQ(CountInstructions(*computation, HloOpcode::kSubtract), 1);\n@@ -154,8 +159,8 @@ TEST_F(HloQueryTest, GetUniqueGteTest) {\n     ROOT gte4 = f32[32]{0} get-tuple-element(param.0), index=3\n   })\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n-                          ParseAndReturnUnverifiedModule(kHloString));\n+  ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                       ParseAndReturnUnverifiedModule(kHloString));\n   HloInstruction* param = module->entry_computation()->parameter_instruction(0);\n   HloInstruction* gte1 = hlo_query::GetUniqueGteInstruction(param, /*index=*/0);\n   EXPECT_NE(gte1, nullptr);\n@@ -164,15 +169,15 @@ TEST_F(HloQueryTest, GetUniqueGteTest) {\n }\n \n TEST_F(HloQueryTest, FindComputationTest) {\n-  TF_ASSERT_OK_AND_ASSIGN(\n+  ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<HloModule> module,\n       ParseAndReturnUnverifiedModule(kConstantAdditionHloString));\n   EXPECT_NE(hlo_query::FindComputation(module.get(), \"main\"), nullptr);\n   EXPECT_EQ(hlo_query::FindComputation(module.get(), \"foo\"), nullptr);\n }\n \n TEST_F(HloQueryTest, FindInstructionUsingNameTest) {\n-  TF_ASSERT_OK_AND_ASSIGN(\n+  ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<HloModule> module,\n       ParseAndReturnUnverifiedModule(kConstantAdditionHloString));\n   const HloComputation* main = hlo_query::FindComputation(module.get(), \"main\");\n@@ -194,7 +199,7 @@ void FindInstructionsAndExpectEqual(const HloComputation* main,\n }\n \n TEST_F(HloQueryTest, FindInstructionUsingOpcodeTest) {\n-  TF_ASSERT_OK_AND_ASSIGN(\n+  ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<HloModule> module,\n       ParseAndReturnUnverifiedModule(kConstantAdditionHloString));\n   const HloComputation* main = hlo_query::FindComputation(module.get(), \"main\");\n@@ -204,7 +209,7 @@ TEST_F(HloQueryTest, FindInstructionUsingOpcodeTest) {\n }\n \n TEST_F(HloQueryTest, FindInstructionUsingOpcodeAndNameEqualTest) {\n-  TF_ASSERT_OK_AND_ASSIGN(\n+  ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<HloModule> module,\n       ParseAndReturnUnverifiedModule(kConstantAdditionHloString));\n   const HloComputation* main = hlo_query::FindComputation(module.get(), \"main\");\n@@ -215,7 +220,7 @@ TEST_F(HloQueryTest, FindInstructionUsingOpcodeAndNameEqualTest) {\n }\n \n TEST_F(HloQueryTest, FindInstructionDoesNotExistTest) {\n-  TF_ASSERT_OK_AND_ASSIGN(\n+  ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<HloModule> module,\n       ParseAndReturnUnverifiedModule(kConstantAdditionHloString));\n   const HloComputation* main = hlo_query::FindComputation(module.get(), \"main\");\n@@ -227,7 +232,7 @@ TEST_F(HloQueryTest, FindInstructionDoesNotExistTest) {\n }\n \n TEST_F(HloQueryTest, NextChannelIdForModuleWithoutChannelIdTest) {\n-  TF_ASSERT_OK_AND_ASSIGN(\n+  ASSERT_OK_AND_ASSIGN(\n       auto module, ParseAndReturnUnverifiedModule(kConstantAdditionHloString));\n   EXPECT_EQ(hlo_query::NextChannelId(*module), 1)\n       << \"module with no channel id\";\n@@ -242,7 +247,7 @@ TEST_F(HloQueryTest, NextChannelIdBasicTest) {\n         source_target_pairs={{0,1},{1,2},{2,3},{3,0}}\n     }\n     )\";\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(hlo));\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(hlo));\n   EXPECT_EQ(hlo_query::NextChannelId(*module), 9);\n }\n \n@@ -258,9 +263,52 @@ TEST_F(HloQueryTest, NextChannelIdTwoIdsTest) {\n       ROOT res = u32[] add(l,r)\n     }\n     )\";\n-  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(hlo));\n+  ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(hlo));\n   EXPECT_EQ(hlo_query::NextChannelId(*module), 10);\n }\n \n+TEST_F(HloQueryTest, GetFirstInstructionWithOpcodeTest) {\n+  ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<HloModule> module,\n+      ParseAndReturnUnverifiedModule(kConstantAdditionHloString));\n+  const HloComputation* entry = module->entry_computation();\n+\n+  EXPECT_THAT(\n+      hlo_query::GetFirstInstructionWithOpcode(*entry, HloOpcode::kConstant),\n+      GmockMatch(m::Op().WithOpcode(HloOpcode::kConstant)));\n+\n+  EXPECT_THAT(hlo_query::GetFirstInstructionWithOpcode(*entry, HloOpcode::kAdd),\n+              GmockMatch(m::Op().WithOpcode(HloOpcode::kAdd)));\n+\n+  EXPECT_EQ(\n+      hlo_query::GetFirstInstructionWithOpcode(*entry, HloOpcode::kParameter),\n+      nullptr);\n+}\n+\n+TEST_F(HloQueryTest, GetFirstInstructionWithOpcodeListTest) {\n+  ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<HloModule> module,\n+      ParseAndReturnUnverifiedModule(kConstantAdditionHloString));\n+  const HloComputation* entry = module->entry_computation();\n+\n+  std::vector<HloOpcode> constants_and_add = {HloOpcode::kConstant,\n+                                              HloOpcode::kAdd};\n+  EXPECT_THAT(\n+      hlo_query::GetFirstInstructionWithOpcode(*entry, constants_and_add),\n+      GmockMatch(\n+          m::AnyOf<HloInstruction>(m::Op().WithOpcode(HloOpcode::kConstant),\n+                                   m::Op().WithOpcode(HloOpcode::kAdd))));\n+\n+  std::vector<HloOpcode> add_and_param = {HloOpcode::kAdd,\n+                                          HloOpcode::kParameter};\n+  EXPECT_THAT(hlo_query::GetFirstInstructionWithOpcode(*entry, add_and_param),\n+              GmockMatch(m::Op().WithOpcode(HloOpcode::kAdd)));\n+\n+  std::vector<HloOpcode> param_and_tuple = {HloOpcode::kParameter,\n+                                            HloOpcode::kTuple};\n+  EXPECT_EQ(hlo_query::GetFirstInstructionWithOpcode(*entry, param_and_tuple),\n+            nullptr);\n+}\n+\n }  // namespace\n }  // namespace xla"
        },
        {
            "sha": "a7e7b5599da3755e717b453e042d593cf6b852f0",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 67,
            "deletions": 0,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=eee1c778b47f339d4eac9b5146b23e557bc302df",
            "patch": "@@ -1937,6 +1937,7 @@ cc_library(\n     srcs = [\"nest_gemm_fusion.cc\"],\n     hdrs = [\"nest_gemm_fusion.h\"],\n     deps = [\n+        \":convert_triton_gemm_config\",\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n@@ -1961,6 +1962,7 @@ cc_library(\n         \"//xla/stream_executor:device_description\",\n         \"//xla/tools:hlo_decomposer_lib\",\n         \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:status_macros\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n@@ -3368,6 +3370,71 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"convert_triton_gemm_config\",\n+    srcs = [\"convert_triton_gemm_config.cc\"],\n+    hdrs = [\"convert_triton_gemm_config.h\"],\n+    deps = [\n+        \"//xla:shape_util\",\n+        \"//xla:util\",\n+        \"//xla:xla_data_proto_cc\",\n+        \"//xla:xla_proto_cc\",\n+        \"//xla/backends/gpu/codegen/triton:support\",\n+        \"//xla/codegen/tiling:symbolic_tile\",\n+        \"//xla/codegen/tiling:symbolic_tile_analysis\",\n+        \"//xla/codegen/tiling:symbolic_tiled_hlo_instruction\",\n+        \"//xla/codegen/tiling:tiling_specification\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/pass:hlo_pass\",\n+        \"//xla/hlo/utils:hlo_query\",\n+        \"//xla/service:call_graph\",\n+        \"//xla/service:hlo_module_config\",\n+        \"//xla/service:instruction_fusion\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:ir_emission_utils\",\n+        \"//xla/service/gpu:matmul_utils\",\n+        \"//xla/service/gpu/model:block_level_parameters\",\n+        \"//xla/service/gpu/model:triton_emitter_constraints\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/tools:hlo_decomposer_lib\",\n+        \"//xla/tsl/platform:status_macros\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//mlir:IR\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"convert_triton_gemm_config_test\",\n+    srcs = [\"convert_triton_gemm_config_test.cc\"],\n+    deps = [\n+        \":convert_triton_gemm_config\",\n+        \"//xla:xla_proto_cc\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/hlo/testlib:pattern_matcher_gmock\",\n+        \"//xla/hlo/testlib:verified_hlo_module\",\n+        \"//xla/service:pattern_matcher\",\n+        \"//xla/service/gpu:backend_configs_cc\",\n+        \"//xla/service/gpu:gpu_device_info_for_tests\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@llvm-project//mlir:IR\",\n+    ],\n+)\n+\n xla_cc_test(\n     name = \"splitk_rewriter_test\",\n     srcs = [\"splitk_rewriter_test.cc\"],"
        },
        {
            "sha": "e4f5f144f2661a88ff01e9b6e3c424ed1f638cf9",
            "filename": "third_party/xla/xla/service/gpu/transforms/convert_triton_gemm_config.cc",
            "status": "added",
            "additions": 296,
            "deletions": 0,
            "changes": 296,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconvert_triton_gemm_config.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconvert_triton_gemm_config.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconvert_triton_gemm_config.cc?ref=eee1c778b47f339d4eac9b5146b23e557bc302df",
            "patch": "@@ -0,0 +1,296 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/service/gpu/transforms/convert_triton_gemm_config.h\"\n+\n+#include <cstdint>\n+#include <memory>\n+#include <utility>\n+#include <variant>\n+#include <vector>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_join.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"xla/backends/gpu/codegen/triton/support.h\"\n+#include \"xla/codegen/tiling/symbolic_tile.h\"\n+#include \"xla/codegen/tiling/symbolic_tile_analysis.h\"\n+#include \"xla/codegen/tiling/symbolic_tiled_hlo_instruction.h\"\n+#include \"xla/codegen/tiling/tiling_specification.h\"\n+#include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/utils/hlo_query.h\"\n+#include \"xla/service/call_graph.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/service/gpu/matmul_utils.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/service/gpu/model/triton_emitter_constraints.h\"\n+#include \"xla/service/hlo_module_config.h\"\n+#include \"xla/service/instruction_fusion.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/tools/hlo_decomposer.h\"\n+#include \"xla/util.h\"\n+#include \"xla/xla.pb.h\"\n+#include \"xla/xla_data.pb.h\"\n+#include \"xla/tsl/platform/status_macros.h\"\n+\n+namespace xla::gpu {\n+namespace {\n+\n+using ::mlir::MLIRContext;\n+\n+// Extracts the TritonGemmConfig from the given fusion's backend config.\n+absl::StatusOr<TritonGemmConfig> GetTritonGemmConfig(\n+    const HloFusionInstruction& fusion) {\n+  ASSIGN_OR_RETURN(auto gpu_config, fusion.backend_config<GpuBackendConfig>());\n+  const FusionBackendConfig& backend_config =\n+      gpu_config.fusion_backend_config();\n+  if (!backend_config.has_triton_gemm_config()) {\n+    return absl::InternalError(\n+        \"The fusion's backend config doesn't have a triton_gemm_config.\");\n+  }\n+  return TritonGemmConfig::FromProto(backend_config.triton_gemm_config());\n+}\n+\n+absl::Status IsDot(const HloInstruction& dot) {\n+  if (HloPredicateIsNotOp<HloOpcode::kDot, HloOpcode::kScaledDot>(&dot)) {\n+    return absl::InternalError(\n+        absl::StrCat(\"Expected a dot instruction but got \", dot.ToString()));\n+  }\n+  return absl::OkStatus();\n+}\n+\n+bool IsScaledDotWithTritonEnabled(const HloModuleConfig& module_config) {\n+  const DebugOptions& debug_options = module_config.debug_options();\n+  return debug_options.xla_gpu_experimental_scaled_dot_with_triton();\n+}\n+\n+class ConvertTritonGemmConfigVisitor : public DfsHloRewriteVisitor {\n+ public:\n+  explicit ConvertTritonGemmConfigVisitor(\n+      MLIRContext* mlir_context,\n+      const se::DeviceDescription& device_description)\n+      : mlir_context_(mlir_context), device_description_(device_description) {}\n+\n+ private:\n+  absl::Status HandleFusion(HloInstruction* instruction) override {\n+    HloFusionInstruction* fusion = Cast<HloFusionInstruction>(instruction);\n+    // Check if we target this fusion.\n+    absl::StatusOr<TritonGemmConfig> config = GetTritonGemmConfig(*fusion);\n+    if (!config.ok()) {\n+      VLOG(2) << \"Skipping fusion as it does not have a TritonGemmConfig\";\n+      return absl::OkStatus();\n+    }\n+    return RewriteFusion(fusion, *config);\n+  }\n+\n+  absl::Status RewriteFusion(HloFusionInstruction* fusion,\n+                             const TritonGemmConfig& config) {\n+    HloComputation* computation = fusion->called_computation();\n+\n+    std::vector<HloOpcode> dot_opcodes = {HloOpcode::kDot};\n+    bool scaled_dot_enabled =\n+        IsScaledDotWithTritonEnabled(fusion->GetModule()->config());\n+    if (scaled_dot_enabled) {\n+      dot_opcodes.push_back(HloOpcode::kScaledDot);\n+    }\n+    HloInstruction* dot =\n+        hlo_query::GetFirstInstructionWithOpcode(*computation, dot_opcodes);\n+    if (dot == nullptr) {\n+      VLOG(2) << \"Skipping fusion as it has no dot instruction\";\n+      return absl::OkStatus();\n+    }\n+\n+    // Annotate the dot with the contraction tile size.\n+    ASSIGN_OR_RETURN(auto tile_sizes, dot->backend_config<Tile>());\n+    tile_sizes.add_sizes(config.block_k);\n+    RETURN_IF_ERROR(dot->set_backend_config(tile_sizes));\n+\n+    // Annotate the fusion itself with the block-level parameters.\n+    ASSIGN_OR_RETURN(auto gpu_config,\n+                     fusion->backend_config<GpuBackendConfig>());\n+    FusionBackendConfig& backend_config =\n+        *gpu_config.mutable_fusion_backend_config();\n+    backend_config.clear_triton_gemm_config();\n+    backend_config.set_kind(kTritonNestedGemmFusionKind);\n+\n+    ASSIGN_OR_RETURN(BlockLevelParameters block_level_parameters,\n+                     FindBlockLevelParameters(dot, config, mlir_context_,\n+                                              device_description_));\n+\n+    *backend_config.mutable_block_level_fusion_config() =\n+        block_level_parameters.ToBlockLevelFusionConfig();\n+    RETURN_IF_ERROR(fusion->set_backend_config(gpu_config));\n+\n+    MarkAsChanged();\n+    if (CodegenDecision can_codegen_computation = IsTritonSupportedComputation(\n+            *fusion->called_computation(),\n+            device_description_.gpu_compute_capability());\n+        !can_codegen_computation) {\n+      return absl::InternalError(absl::StrCat(\n+          \"Computation of fusion \", fusion->ToString(),\n+          \" is not supported by Triton: \", can_codegen_computation.Explain()));\n+    }\n+    return absl::OkStatus();\n+  }\n+\n+  MLIRContext* mlir_context_;\n+  const se::DeviceDescription& device_description_;\n+};\n+\n+}  // namespace\n+\n+absl::StatusOr<bool> ConvertTritonGemmConfig::RunImpl(\n+    HloModule* module,\n+    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n+  bool changed = false;\n+  auto call_graph = CallGraph::Build(module, execution_threads);\n+  for (HloComputation* computation :\n+       module->MakeNonfusionComputations(execution_threads)) {\n+    ConvertTritonGemmConfigVisitor visitor(mlir_context_, device_description_);\n+    RETURN_IF_ERROR(computation->Accept(&visitor));\n+    changed |= visitor.changed();\n+  }\n+  return changed;\n+}\n+\n+absl::StatusOr<BlockLevelParameters> FindBlockLevelParameters(\n+    HloInstruction* dot, const TritonGemmConfig& config,\n+    MLIRContext* mlir_context,\n+    const se::DeviceDescription& device_description) {\n+  RETURN_IF_ERROR(IsDot(*dot));\n+  HloComputation* computation = dot->parent();\n+  VLOG(3) << \"FindOutputTileSizesForEpilogue of computation: \"\n+          << computation->ToString();\n+  SymbolicTileAnalysisOrError analysis_or =\n+      SymbolicTileAnalysis::AnalyzeComputation(\n+          *computation, mlir_context,\n+          TritonEmitterConstraints::GetBuilder(device_description));\n+\n+  if (const auto* fusion_decision = std::get_if<FusionDecision>(&analysis_or)) {\n+    std::unique_ptr<HloModule> extracted_computation_module =\n+        ExtractInstructionIntoNewModule(*computation->FusionInstruction());\n+    return absl::InternalError(absl::StrCat(\n+        \"Failed to analyze the computation (\", fusion_decision->Explain(),\n+        \"):\\n\", extracted_computation_module->ToString()));\n+  }\n+\n+  const auto& analysis = std::get<SymbolicTileAnalysis>(analysis_or);\n+  const auto& tiled_instructions = analysis.GetSymbolicTiledHloComputation();\n+  auto is_dot = [&](const auto& instr) { return instr->hlo() == dot; };\n+  auto tiled_dot_it = absl::c_find_if(tiled_instructions, is_dot);\n+  if (tiled_dot_it == tiled_instructions.end()) {\n+    return absl::InternalError(absl::StrCat(\n+        \"Couldn't find a symbolic tiled instruction for \", dot->ToString()));\n+  }\n+  const SymbolicTiledHloInstruction& tiled_dot = **tiled_dot_it;\n+\n+  auto get_tile_sizes = [&](int64_t rank) {\n+    QCHECK_GE(rank, 2) << \"Expected at least rank 2 for the dot, got \" << rank\n+                       << \" in computation \" << computation->ToString();\n+    // We always expect the shape to be [1, ..., block_m, block_n], by\n+    // construction of GemmFusions.\n+    llvm::SmallVector<int64_t> tile_sizes(rank - 2, 1);\n+    tile_sizes.append({config.block_m, config.block_n});\n+    return tile_sizes;\n+  };\n+\n+  VLOG(3) << \"FindOutputTileSizesForEpilogue: dot shape: \"\n+          << dot->shape().ToString();\n+  auto expected_dot_tile_sizes =\n+      get_tile_sizes(dot->shape().dimensions().size());\n+  VLOG(2) << \"FindOutputTileSizesForEpilogue: \" << tiled_dot.ToString()\n+          << \"\\nConstraints: \"\n+          << analysis.GetTilingSpecification().constraints().ToString()\n+          << \"Expected dot tile sizes: \"\n+          << absl::StrJoin(expected_dot_tile_sizes, \" \");\n+\n+  // Try all permutations of the dot tile sizes to see if any of them satisfy\n+  // the constraints of the analysis and map to the given config of the dot.\n+  int64_t out_rank =\n+      computation->root_instruction()->shape().dimensions().size();\n+  VLOG(3) << \"FindOutputTileSizesForEpilogue: computation root shape: \"\n+          << computation->root_instruction()->shape().ToString();\n+  llvm::SmallVector<int64_t> output_tile_sizes = get_tile_sizes(out_rank);\n+\n+  absl::c_sort(output_tile_sizes);\n+\n+  const TilingSpecification& tiling_specification =\n+      analysis.GetTilingSpecification();\n+\n+  do {\n+    VLOG(4) << \"trying output_tile_sizes = (\"\n+            << absl::StrJoin(output_tile_sizes, \",\") << \")\";\n+    Tiling::TileMapping tile_mapping;\n+    tile_mapping[dot] = {config.block_k};\n+    // If the `dot` is a root, we need to assign both the hidden parameter and\n+    // the output parameters to it.\n+    if (dot->IsRoot()) {\n+      tile_mapping[dot].insert(tile_mapping[dot].end(),\n+                               output_tile_sizes.begin(),\n+                               output_tile_sizes.end());\n+    } else {\n+      tile_mapping[dot->parent()->root_instruction()] = {\n+          output_tile_sizes.begin(), output_tile_sizes.end()};\n+    }\n+\n+    Tiling tiling(std::move(tile_mapping));\n+    ASSIGN_OR_RETURN(bool parameters_satisfy_constraints,\n+                     analysis.ParametersSatisfyConstraints(tiling));\n+    if (!parameters_satisfy_constraints) {\n+      VLOG(4) << \"Parameters don't satisfy constraints\";\n+      continue;\n+    }\n+    ASSIGN_OR_RETURN(FlatTiling flat_tiling_parameters,\n+                     tiling.Flatten(tiling_specification));\n+    llvm::SmallVector<int64_t> mapped_dot_tile_sizes =\n+        EvaluateTileSizes(tiled_dot.symbolic_tile(), flat_tiling_parameters);\n+    if (mapped_dot_tile_sizes == expected_dot_tile_sizes) {\n+      BlockLevelParameters params;\n+      params.output_tile_sizes = {std::vector<int64_t>(\n+          output_tile_sizes.begin(), output_tile_sizes.end())};\n+      params.num_warps = config.num_warps;\n+      params.num_ctas = config.num_ctas;\n+      params.num_stages = config.num_stages;\n+      params.is_tma_allowed = config.is_tma_allowed;\n+      params.is_warp_specialization_allowed =\n+          config.is_warp_specialization_allowed;\n+      return params;\n+    }\n+    VLOG(4) << \"mapped_dot_tile_sizes: \"\n+            << absl::StrJoin(mapped_dot_tile_sizes, \",\")\n+            << \" != \" << absl::StrJoin(expected_dot_tile_sizes, \",\");\n+  } while (absl::c_next_permutation(output_tile_sizes));\n+\n+  return absl::InternalError(absl::StrCat(\n+      \"Couldn't find output tile sizes that satisfy \", tiled_dot.ToString()));\n+}\n+\n+}  // namespace xla::gpu"
        },
        {
            "sha": "df5483d57ff2d6d541d7b7826e10d2f1bdafb3c7",
            "filename": "third_party/xla/xla/service/gpu/transforms/convert_triton_gemm_config.h",
            "status": "added",
            "additions": 79,
            "deletions": 0,
            "changes": 79,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconvert_triton_gemm_config.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconvert_triton_gemm_config.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconvert_triton_gemm_config.h?ref=eee1c778b47f339d4eac9b5146b23e557bc302df",
            "patch": "@@ -0,0 +1,79 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_SERVICE_GPU_TRANSFORMS_CONVERT_TRITON_GEMM_CONFIG_H_\n+#define XLA_SERVICE_GPU_TRANSFORMS_CONVERT_TRITON_GEMM_CONFIG_H_\n+\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/pass/hlo_pass_interface.h\"\n+#include \"xla/service/gpu/matmul_utils.h\"\n+#include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+\n+namespace xla::gpu {\n+\n+// Rewrites supported Triton GEMM fusions to generic Triton fusions.\n+//\n+// Fusions with kind kCustom and fusion_backend_config.kind \"__triton_gemm\" are\n+// rewritten to fusion_backend_config.kind\n+// \"__triton_nested_fusion_gemm\".\n+//\n+// While this new fusion kind is supported by generic triton emitter we want\n+// to distinguish it from \"__triton\" as we don't want other passes to modify the\n+// resulting fusions.\n+//\n+// The fusion's backend config is set to a BlockLevelFusionConfig, derived from\n+// a previously set TritonGemmConfig.\n+//\n+// The operands of the dot (including their prologues) are fused into two new\n+// nested fusions, each with their own BlockLevelFusionConfig.\n+class ConvertTritonGemmConfig : public HloModulePass {\n+ public:\n+  explicit ConvertTritonGemmConfig(\n+      const se::DeviceDescription& device_description,\n+      mlir::MLIRContext* mlir_context)\n+      : device_description_(device_description), mlir_context_(mlir_context) {}\n+\n+  absl::string_view name() const override {\n+    return \"convert_triton_gemm_config\";\n+  }\n+\n+ protected:\n+  absl::StatusOr<bool> RunImpl(\n+      HloModule* module,\n+      const absl::flat_hash_set<absl::string_view>& execution_threads) override;\n+\n+ private:\n+  const se::DeviceDescription device_description_;\n+  mlir::MLIRContext* mlir_context_;\n+};\n+\n+// Returns block level parameters based on tile sizes for the root of the\n+// analysis that satisfy the requirements of the `dot`. That is, the tile sizes\n+// need to satisfy the constraints of the analysis and map to the given `config`\n+// of the dot.\n+absl::StatusOr<BlockLevelParameters> FindBlockLevelParameters(\n+    HloInstruction* dot, const TritonGemmConfig& config,\n+    mlir::MLIRContext* mlir_context,\n+    const se::DeviceDescription& device_description);\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_SERVICE_GPU_TRANSFORMS_CONVERT_TRITON_GEMM_CONFIG_H_"
        },
        {
            "sha": "51a672f4f00e837a8470fea1ec91b3a9ae2b6315",
            "filename": "third_party/xla/xla/service/gpu/transforms/convert_triton_gemm_config_test.cc",
            "status": "added",
            "additions": 146,
            "deletions": 0,
            "changes": 146,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconvert_triton_gemm_config_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconvert_triton_gemm_config_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fconvert_triton_gemm_config_test.cc?ref=eee1c778b47f339d4eac9b5146b23e557bc302df",
            "patch": "@@ -0,0 +1,146 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/service/gpu/transforms/convert_triton_gemm_config.h\"\n+\n+#include <memory>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status_matchers.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/hlo/testlib/pattern_matcher_gmock.h\"\n+#include \"xla/hlo/testlib/verified_hlo_module.h\"\n+#include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n+#include \"xla/service/pattern_matcher.h\"\n+#include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/xla.pb.h\"\n+\n+using ::absl_testing::IsOkAndHolds;\n+using ::testing::ElementsAre;\n+\n+namespace xla::gpu {\n+namespace {\n+\n+// Wraps a matcher for a fusion instruction's output tile sizes.\n+// Proto matchers would be nice, but b/229726259 is P2.\n+MATCHER_P(ContractionTileSizesIs, matcher, \"\") {\n+  auto backend_config = arg.template backend_config<Tile>();\n+  if (!backend_config.ok()) {\n+    *result_listener << \"failed to get tile sizes: \" << backend_config.status();\n+    return false;\n+  }\n+  return ExplainMatchResult(matcher, backend_config->sizes(), result_listener);\n+}\n+\n+// Wraps a matcher for a fusion instruction's output tile sizes.\n+// Proto matchers would be nice, but b/229726259 is P2.\n+MATCHER_P(OutputTileSizesIs, matcher, \"\") {\n+  auto backend_config = arg.template backend_config<GpuBackendConfig>();\n+  if (!backend_config.ok()) {\n+    *result_listener << \"failed to get backend config: \"\n+                     << backend_config.status();\n+    return false;\n+  }\n+  FusionBackendConfig fusion_backend_config =\n+      backend_config->fusion_backend_config();\n+  if (!fusion_backend_config.has_block_level_fusion_config()) {\n+    *result_listener << \"has no block level fusion config\";\n+    return false;\n+  }\n+  if (fusion_backend_config.kind() != \"__triton_nested_gemm_fusion\") {\n+    *result_listener << \"fusion kind is not __triton_nested_gemm_fusion\";\n+    return false;\n+  }\n+  auto output_tile_sizes =\n+      fusion_backend_config.block_level_fusion_config().output_tiles(0).sizes();\n+  return ExplainMatchResult(matcher, output_tile_sizes, result_listener);\n+}\n+\n+class ConvertTritonGemmConfigTest : public HloHardwareIndependentTestBase {\n+ protected:\n+  const se::DeviceDescription device_description_{\n+      TestGpuDeviceInfo::RTXA6000DeviceInfo(\n+          se::GpuComputeCapability{se::CudaComputeCapability::Ampere()})};\n+  mlir::MLIRContext mlir_context_;\n+\n+  std::unique_ptr<VerifiedHloModule> RunConvertTritonGemmConfig(\n+      absl::string_view hlo, const bool expect_change = true) {\n+    std::unique_ptr<VerifiedHloModule> module =\n+        ParseAndReturnVerifiedModule(hlo).value();\n+    EXPECT_THAT(ConvertTritonGemmConfig(device_description_, &mlir_context_)\n+                    .Run(module.get()),\n+                IsOkAndHolds(expect_change));\n+    EXPECT_OK(verifier().Run(module.get()).status());\n+    return module;\n+  }\n+};\n+\n+TEST_F(ConvertTritonGemmConfigTest, BasicTest) {\n+  absl::string_view hlo = R\"(\n+dot {\n+  lhs = f32[8192,512] parameter(0)\n+  rhs = f32[512,512] parameter(1)\n+  ROOT  dot = f32[8192,512] dot(lhs, rhs),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+}\n+\n+ENTRY entry {\n+  p0 = f32[8192,512] parameter(0)\n+  p1 = f32[512,512] parameter(1)\n+  ROOT fusion = f32[8192,512] fusion(p0, p1),\n+    kind=kCustom, calls=dot, backend_config={\n+      \"fusion_backend_config\": {\n+        \"kind\":\"__triton_gemm\",  \"triton_gemm_config\": {\n+          \"block_m\":\"64\", \"block_n\":\"256\", \"block_k\":\"32\",\n+          \"split_k\":\"1\", \"num_stages\":\"5\", \"num_warps\":\"4\", \"num_ctas\":\"3\"\n+        }\n+      }\n+    }\n+})\";\n+\n+  std::unique_ptr<VerifiedHloModule> module = RunConvertTritonGemmConfig(hlo);\n+  const HloInstruction* fusion = nullptr;\n+  ASSERT_THAT(module->entry_computation()->root_instruction(),\n+              GmockMatch(match::Fusion(&fusion)));\n+  EXPECT_THAT(*fusion, OutputTileSizesIs(ElementsAre(64, 256)));\n+\n+  BlockLevelFusionConfig block_level_fusion_config =\n+      fusion->backend_config<GpuBackendConfig>()\n+          ->fusion_backend_config()\n+          .block_level_fusion_config();\n+  EXPECT_THAT(block_level_fusion_config.output_tiles(0).sizes(),\n+              ElementsAre(64, 256));\n+  EXPECT_THAT(block_level_fusion_config.num_warps(), 4);\n+  EXPECT_THAT(block_level_fusion_config.num_ctas(), 3);\n+  EXPECT_THAT(block_level_fusion_config.num_stages(), 5);\n+\n+  EXPECT_THAT(*fusion->fused_expression_root(),\n+              ContractionTileSizesIs(ElementsAre(32)));\n+\n+  // The old GEMM config should have been deleted.\n+  EXPECT_FALSE(fusion->backend_config<GpuBackendConfig>()\n+                   ->fusion_backend_config()\n+                   .has_triton_gemm_config());\n+}\n+\n+}  // namespace\n+}  // namespace xla::gpu"
        },
        {
            "sha": "1f26439d5f7d59632a79bc283165c1726dd1548a",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion.cc",
            "status": "modified",
            "additions": 39,
            "deletions": 154,
            "changes": 193,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.cc?ref=eee1c778b47f339d4eac9b5146b23e557bc302df",
            "patch": "@@ -54,6 +54,7 @@ limitations under the License.\n #include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n #include \"xla/service/gpu/model/triton_emitter_constraints.h\"\n+#include \"xla/service/gpu/transforms/convert_triton_gemm_config.h\"\n #include \"xla/service/instruction_fusion.h\"\n #include \"xla/service/matmul_indexing_utils.h\"\n #include \"xla/shape.h\"\n@@ -64,6 +65,7 @@ limitations under the License.\n #include \"xla/util.h\"\n #include \"xla/xla.pb.h\"\n #include \"xla/xla_data.pb.h\"\n+#include \"xla/tsl/platform/status_macros.h\"\n \n namespace xla::gpu {\n namespace {\n@@ -132,16 +134,15 @@ absl::Status FuseInstructionsForConsumer(HloInstruction& root,\n       << \"Consumer \" << consumer.ToString() << \" does not use root \"\n       << root.ToString();\n \n-  TF_ASSIGN_OR_RETURN(HloInstruction * fusion, FuseInstructionsFromRoot(root));\n+  ASSIGN_OR_RETURN(HloInstruction * fusion, FuseInstructionsFromRoot(root));\n \n-  TF_ASSIGN_OR_RETURN(auto gpu_config,\n-                      fusion->backend_config<GpuBackendConfig>());\n+  ASSIGN_OR_RETURN(auto gpu_config, fusion->backend_config<GpuBackendConfig>());\n   gpu_config.mutable_fusion_backend_config()->set_kind(\n       kTritonNestedGemmFusionKind);\n-  TF_RETURN_IF_ERROR(fusion->set_backend_config(gpu_config));\n+  RETURN_IF_ERROR(fusion->set_backend_config(gpu_config));\n \n   for (int64_t operand_index : consumer.OperandIndices(&root)) {\n-    TF_RETURN_IF_ERROR(consumer.ReplaceOperandWith(operand_index, fusion));\n+    RETURN_IF_ERROR(consumer.ReplaceOperandWith(operand_index, fusion));\n   }\n \n   return absl::OkStatus();\n@@ -164,14 +165,14 @@ absl::Status AnnotateDotOperandNestedFusionImpl(\n     absl::Span<const int64_t> contracting_dimensions,  // Must be single element\n     absl::Span<const int64_t> batch_dimensions, int64_t contracting_dim_size,\n     int64_t non_contracting_dim_size) {\n-  TF_RETURN_IF_ERROR(IsDot(dot));\n+  RETURN_IF_ERROR(IsDot(dot));\n   if (contracting_dimensions.size() != 1) {\n     return absl::InternalError(\n         absl::StrCat(\"Expected a single lhs contracting dimension but got \",\n                      contracting_dimensions.size()));\n   }\n \n-  TF_ASSIGN_OR_RETURN(\n+  ASSIGN_OR_RETURN(\n       std::vector<int64_t> non_contracting_dimensions,\n       GetNonContractingDims(dot.operand(0)->shape(), batch_dimensions,\n                             contracting_dimensions));\n@@ -198,20 +199,20 @@ absl::Status AnnotateDotOperandNestedFusionImpl(\n   block_level_parameters.is_warp_specialization_allowed =\n       config.is_warp_specialization_allowed;\n \n-  TF_ASSIGN_OR_RETURN(auto gpu_config,\n-                      nested_fusion.backend_config<GpuBackendConfig>());\n+  ASSIGN_OR_RETURN(auto gpu_config,\n+                   nested_fusion.backend_config<GpuBackendConfig>());\n   *gpu_config.mutable_fusion_backend_config()\n        ->mutable_block_level_fusion_config() =\n       block_level_parameters.ToBlockLevelFusionConfig();\n-  TF_RETURN_IF_ERROR(nested_fusion.set_backend_config(gpu_config));\n+  RETURN_IF_ERROR(nested_fusion.set_backend_config(gpu_config));\n \n   return absl::OkStatus();\n }\n \n absl::Status AnnotateDotLhsNestedFusion(HloFusionInstruction& nested_fusion,\n                                         const HloInstruction& dot,\n                                         const TritonGemmConfig& config) {\n-  TF_RETURN_IF_ERROR(IsDot(dot));\n+  RETURN_IF_ERROR(IsDot(dot));\n   const DotDimensionNumbers& dimension_numbers = dot.dot_dimension_numbers();\n   return AnnotateDotOperandNestedFusionImpl(\n       nested_fusion, dot, config,\n@@ -222,7 +223,7 @@ absl::Status AnnotateDotLhsNestedFusion(HloFusionInstruction& nested_fusion,\n absl::Status AnnotateDotRhsNestedFusion(HloFusionInstruction& nested_fusion,\n                                         const HloInstruction& dot,\n                                         const TritonGemmConfig& config) {\n-  TF_RETURN_IF_ERROR(IsDot(dot));\n+  RETURN_IF_ERROR(IsDot(dot));\n   const DotDimensionNumbers& dimension_numbers = dot.dot_dimension_numbers();\n   return AnnotateDotOperandNestedFusionImpl(\n       nested_fusion, dot, config,\n@@ -233,8 +234,7 @@ absl::Status AnnotateDotRhsNestedFusion(HloFusionInstruction& nested_fusion,\n // Extracts the TritonGemmConfig from the given fusion's backend config.\n absl::StatusOr<TritonGemmConfig> GetTritonGemmConfig(\n     const HloFusionInstruction& fusion) {\n-  TF_ASSIGN_OR_RETURN(auto gpu_config,\n-                      fusion.backend_config<GpuBackendConfig>());\n+  ASSIGN_OR_RETURN(auto gpu_config, fusion.backend_config<GpuBackendConfig>());\n   const FusionBackendConfig& backend_config =\n       gpu_config.fusion_backend_config();\n   if (!backend_config.has_triton_gemm_config()) {\n@@ -252,7 +252,7 @@ absl::Status FuseAndAnnotateConcatOperands(HloComputation* computation) {\n       continue;\n     }\n     for (HloInstruction* operand : instr->mutable_operands()) {\n-      TF_RETURN_IF_ERROR(FuseInstructionsForConsumer(*operand, *instr));\n+      RETURN_IF_ERROR(FuseInstructionsForConsumer(*operand, *instr));\n     }\n   }\n   return absl::OkStatus();\n@@ -263,28 +263,28 @@ absl::Status FuseAndAnnotateConcatOperands(HloComputation* computation) {\n absl::Status MakeNestedFusionFromGemmFusion(\n     HloFusionInstruction* fusion, HloInstruction* dot, MLIRContext* ctx,\n     const se::DeviceDescription& device_description) {\n-  TF_RETURN_IF_ERROR(IsDot(*dot));\n+  RETURN_IF_ERROR(IsDot(*dot));\n   const bool is_scaled_dot = dot->opcode() == HloOpcode::kScaledDot;\n   constexpr int lhs = 0;\n   constexpr int rhs = 1;\n-  TF_ASSIGN_OR_RETURN(TritonGemmConfig config, GetTritonGemmConfig(*fusion));\n+  ASSIGN_OR_RETURN(TritonGemmConfig config, GetTritonGemmConfig(*fusion));\n   HloComputation* computation = fusion->called_computation();\n \n   // First, create nested fusions for the operands of `concatenate` instructions\n   // if they exist.\n-  TF_RETURN_IF_ERROR(FuseAndAnnotateConcatOperands(computation));\n+  RETURN_IF_ERROR(FuseAndAnnotateConcatOperands(computation));\n \n   // Left-hand side of the dot.\n-  TF_RETURN_IF_ERROR(\n+  RETURN_IF_ERROR(\n       FuseInstructionsForConsumer(*dot->mutable_operand(lhs), *dot));\n-  TF_RETURN_IF_ERROR(AnnotateDotLhsNestedFusion(\n+  RETURN_IF_ERROR(AnnotateDotLhsNestedFusion(\n       *::xla::Cast<HloFusionInstruction>(dot->mutable_operand(lhs)), *dot,\n       config));\n \n   // Right-hand side of the dot.\n-  TF_RETURN_IF_ERROR(\n+  RETURN_IF_ERROR(\n       FuseInstructionsForConsumer(*dot->mutable_operand(rhs), *dot));\n-  TF_RETURN_IF_ERROR(AnnotateDotRhsNestedFusion(\n+  RETURN_IF_ERROR(AnnotateDotRhsNestedFusion(\n       *::xla::Cast<HloFusionInstruction>(dot->mutable_operand(rhs)), *dot,\n       config));\n \n@@ -294,38 +294,37 @@ absl::Status MakeNestedFusionFromGemmFusion(\n     constexpr int kContractingScaleFactor = 32;\n     auto scale_config = config;\n     scale_config.block_k /= kContractingScaleFactor;\n-    TF_RETURN_IF_ERROR(\n+    RETURN_IF_ERROR(\n         FuseInstructionsForConsumer(*dot->mutable_operand(kLhsScale), *dot));\n-    TF_RETURN_IF_ERROR(AnnotateDotLhsNestedFusion(\n+    RETURN_IF_ERROR(AnnotateDotLhsNestedFusion(\n         *::xla::Cast<HloFusionInstruction>(dot->mutable_operand(kLhsScale)),\n         *dot, scale_config));\n-    TF_RETURN_IF_ERROR(\n+    RETURN_IF_ERROR(\n         FuseInstructionsForConsumer(*dot->mutable_operand(kRhsScale), *dot));\n-    TF_RETURN_IF_ERROR(AnnotateDotRhsNestedFusion(\n+    RETURN_IF_ERROR(AnnotateDotRhsNestedFusion(\n         *::xla::Cast<HloFusionInstruction>(dot->mutable_operand(kRhsScale)),\n         *dot, scale_config));\n   }\n   // Delete newly unused instructions, if any.\n-  TF_ASSIGN_OR_RETURN([[maybe_unused]] bool changed,\n-                      HloDCE::RunOnComputation(\n-                          computation,\n-                          /*remove_cross_partition_collective_ops=*/false));\n+  ASSIGN_OR_RETURN([[maybe_unused]] bool changed,\n+                   HloDCE::RunOnComputation(\n+                       computation,\n+                       /*remove_cross_partition_collective_ops=*/false));\n \n   // Annotate the fusion itself.\n-  TF_ASSIGN_OR_RETURN(auto gpu_config,\n-                      fusion->backend_config<GpuBackendConfig>());\n+  ASSIGN_OR_RETURN(auto gpu_config, fusion->backend_config<GpuBackendConfig>());\n   FusionBackendConfig& backend_config =\n       *gpu_config.mutable_fusion_backend_config();\n   backend_config.clear_triton_gemm_config();\n   backend_config.set_kind(kTritonNestedGemmFusionKind);\n \n-  TF_ASSIGN_OR_RETURN(BlockLevelParameters block_level_parameters,\n-                      ::xla::gpu::detail::FindBlockLevelParameters(\n-                          dot, config, ctx, device_description));\n+  ASSIGN_OR_RETURN(\n+      BlockLevelParameters block_level_parameters,\n+      FindBlockLevelParameters(dot, config, ctx, device_description));\n \n   *backend_config.mutable_block_level_fusion_config() =\n       block_level_parameters.ToBlockLevelFusionConfig();\n-  TF_RETURN_IF_ERROR(fusion->set_backend_config(gpu_config));\n+  RETURN_IF_ERROR(fusion->set_backend_config(gpu_config));\n \n   return absl::OkStatus();\n }\n@@ -366,7 +365,7 @@ class NestGemmFusionVisitor : public DfsHloRewriteVisitor {\n   absl::Status AcceptResultingFusion(const HloFusionInstruction* fusion) {\n     const HloComputation* computation = fusion->called_computation();\n     for (const HloInstruction* instruction : computation->instructions()) {\n-      TF_RETURN_IF_ERROR(AcceptNestedInstruction(instruction));\n+      RETURN_IF_ERROR(AcceptNestedInstruction(instruction));\n     }\n     return absl::OkStatus();\n   }\n@@ -386,8 +385,8 @@ class NestGemmFusionVisitor : public DfsHloRewriteVisitor {\n       }\n     }\n \n-    TF_RETURN_IF_ERROR(MakeNestedFusionFromGemmFusion(\n-        fusion, instr, mlir_context_, device_description_));\n+    RETURN_IF_ERROR(MakeNestedFusionFromGemmFusion(fusion, instr, mlir_context_,\n+                                                   device_description_));\n \n     MarkAsChanged();\n     bool scaled_dot_enabled =\n@@ -447,7 +446,7 @@ absl::StatusOr<bool> NestGemmFusion::RunOnModule(\n        module->MakeNonfusionComputations(execution_threads)) {\n     NestGemmFusionVisitor visitor(mlir_context_, call_graph.get(),\n                                   device_description_);\n-    TF_RETURN_IF_ERROR(computation->Accept(&visitor));\n+    RETURN_IF_ERROR(computation->Accept(&visitor));\n     changed |= visitor.changed();\n   }\n   return changed;\n@@ -459,118 +458,4 @@ absl::StatusOr<bool> NestGemmFusion::RunImpl(\n   return RunOnModule(module, execution_threads);\n }\n \n-namespace detail {\n-\n-absl::StatusOr<BlockLevelParameters> FindBlockLevelParameters(\n-    HloInstruction* dot, const TritonGemmConfig& config, MLIRContext* ctx,\n-    const se::DeviceDescription& device_description) {\n-  TF_RETURN_IF_ERROR(IsDot(*dot));\n-  HloComputation* computation = dot->parent();\n-  VLOG(3) << \"FindOutputTileSizesForEpilogue of computation: \"\n-          << computation->ToString();\n-  SymbolicTileAnalysisOrError analysis_or =\n-      SymbolicTileAnalysis::AnalyzeComputation(\n-          *computation, ctx,\n-          TritonEmitterConstraints::GetBuilder(device_description));\n-\n-  if (const auto* fusion_decision = std::get_if<FusionDecision>(&analysis_or)) {\n-    std::unique_ptr<HloModule> extracted_computation_module =\n-        ExtractInstructionIntoNewModule(*computation->FusionInstruction());\n-    return absl::InternalError(absl::StrCat(\n-        \"Failed to analyze the computation (\", fusion_decision->Explain(),\n-        \"):\\n\", extracted_computation_module->ToString()));\n-  }\n-\n-  auto& analysis = std::get<SymbolicTileAnalysis>(analysis_or);\n-  const auto& tiled_instructions = analysis.GetSymbolicTiledHloComputation();\n-  auto is_dot = [&](const auto& instr) { return instr->hlo() == dot; };\n-  auto tiled_dot_it = absl::c_find_if(tiled_instructions, is_dot);\n-  if (tiled_dot_it == tiled_instructions.end()) {\n-    return absl::InternalError(absl::StrCat(\n-        \"Couldn't find a symbolic tiled instruction for \", dot->ToString()));\n-  }\n-  const SymbolicTiledHloInstruction& tiled_dot = **tiled_dot_it;\n-\n-  auto get_tile_sizes = [&](int64_t rank) {\n-    QCHECK_GE(rank, 2) << \"Expected at least rank 2 for the dot, got \" << rank\n-                       << \" in computation \" << computation->ToString();\n-    // We always expect the shape to be [1, ..., block_m, block_n], by\n-    // construction of GemmFusions.\n-    llvm::SmallVector<int64_t> tile_sizes(rank - 2, 1);\n-    tile_sizes.append({config.block_m, config.block_n});\n-    return tile_sizes;\n-  };\n-\n-  VLOG(3) << \"FindOutputTileSizesForEpilogue: dot shape: \"\n-          << dot->shape().ToString();\n-  auto expected_dot_tile_sizes =\n-      get_tile_sizes(dot->shape().dimensions().size());\n-  VLOG(2) << \"FindOutputTileSizesForEpilogue: \" << tiled_dot.ToString()\n-          << \"\\nConstraints: \"\n-          << analysis.GetTilingSpecification().constraints().ToString()\n-          << \"Expected dot tile sizes: \"\n-          << absl::StrJoin(expected_dot_tile_sizes, \" \");\n-\n-  // Try all permutations of the dot tile sizes to see if any of them satisfy\n-  // the constraints of the analysis and map to the given config of the dot.\n-  int64_t out_rank =\n-      computation->root_instruction()->shape().dimensions().size();\n-  VLOG(3) << \"FindOutputTileSizesForEpilogue: computation root shape: \"\n-          << computation->root_instruction()->shape().ToString();\n-  llvm::SmallVector<int64_t> output_tile_sizes = get_tile_sizes(out_rank);\n-\n-  absl::c_sort(output_tile_sizes);\n-\n-  const TilingSpecification& tiling_specification =\n-      analysis.GetTilingSpecification();\n-\n-  do {\n-    VLOG(4) << \"trying output_tile_sizes = (\"\n-            << absl::StrJoin(output_tile_sizes, \",\") << \")\";\n-    Tiling::TileMapping tile_mapping;\n-    tile_mapping[dot] = {config.block_k};\n-    // If the `dot` is a root, we need to assign both the hidden parameter and\n-    // the output parameters to it.\n-    if (dot->IsRoot()) {\n-      tile_mapping[dot].insert(tile_mapping[dot].end(),\n-                               output_tile_sizes.begin(),\n-                               output_tile_sizes.end());\n-    } else {\n-      tile_mapping[dot->parent()->root_instruction()] = {\n-          output_tile_sizes.begin(), output_tile_sizes.end()};\n-    }\n-\n-    Tiling tiling(std::move(tile_mapping));\n-    TF_ASSIGN_OR_RETURN(bool parameters_satisfy_constraints,\n-                        analysis.ParametersSatisfyConstraints(tiling));\n-    if (!parameters_satisfy_constraints) {\n-      VLOG(4) << \"Parameters don't satisfy constraints\";\n-      continue;\n-    }\n-    TF_ASSIGN_OR_RETURN(FlatTiling flat_tiling_parameters,\n-                        tiling.Flatten(tiling_specification));\n-    llvm::SmallVector<int64_t> mapped_dot_tile_sizes =\n-        EvaluateTileSizes(tiled_dot.symbolic_tile(), flat_tiling_parameters);\n-    if (mapped_dot_tile_sizes == expected_dot_tile_sizes) {\n-      BlockLevelParameters params;\n-      params.output_tile_sizes = {std::vector<int64_t>(\n-          output_tile_sizes.begin(), output_tile_sizes.end())};\n-      params.num_warps = config.num_warps;\n-      params.num_ctas = config.num_ctas;\n-      params.num_stages = config.num_stages;\n-      params.is_tma_allowed = config.is_tma_allowed;\n-      params.is_warp_specialization_allowed =\n-          config.is_warp_specialization_allowed;\n-      return params;\n-    }\n-    VLOG(4) << \"mapped_dot_tile_sizes: \"\n-            << absl::StrJoin(mapped_dot_tile_sizes, \",\")\n-            << \" != \" << absl::StrJoin(expected_dot_tile_sizes, \",\");\n-  } while (absl::c_next_permutation(output_tile_sizes));\n-\n-  return absl::InternalError(absl::StrCat(\n-      \"Couldn't find output tile sizes that satisfy \", tiled_dot.ToString()));\n-}\n-\n-}  // namespace detail\n }  // namespace xla::gpu"
        },
        {
            "sha": "2109f822d75a0394365b1e8ae8a20841fc35844d",
            "filename": "third_party/xla/xla/service/gpu/transforms/nest_gemm_fusion.h",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eee1c778b47f339d4eac9b5146b23e557bc302df/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fnest_gemm_fusion.h?ref=eee1c778b47f339d4eac9b5146b23e557bc302df",
            "patch": "@@ -65,24 +65,6 @@ class NestGemmFusion : public HloModulePass {\n       const absl::flat_hash_set<absl::string_view>& execution_threads);\n };\n \n-namespace detail {\n-\n-// Returns block level parameters based on tile sizes for the root of the\n-// analysis that satisfy the requirements of the `dot`. That is, the tile sizes\n-// need to satisfy the constraints of the analysis and map to the given `config`\n-// of the dot.\n-//\n-// We expose this function because using `GpuDotFusionCostModel` is only\n-// possible with `EstimateRunTimeForDotOpWithBlockParameters` method. This\n-// function can be removed once `GpuDotFusionCostModel::EstimateRunTimeForDotOp`\n-// is implemented.\n-absl::StatusOr<BlockLevelParameters> FindBlockLevelParameters(\n-    HloInstruction* dot, const TritonGemmConfig& config,\n-    mlir::MLIRContext* mlir_context,\n-    const se::DeviceDescription& device_description);\n-\n-}  // namespace detail\n-\n }  // namespace xla::gpu\n \n #endif  // XLA_SERVICE_GPU_TRANSFORMS_NEST_GEMM_FUSION_H_"
        }
    ],
    "stats": {
        "total": 915,
        "additions": 724,
        "deletions": 191
    }
}