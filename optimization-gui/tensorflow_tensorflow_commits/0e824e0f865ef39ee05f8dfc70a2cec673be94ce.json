{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Streamline the order of operations in `EmitCollectiveThunk`.\n\n1. If collective is degenerated, emit the memcpy thunk immediately.\n2. If collective is not implementable, return status.\n3. Emit collective thunk.\n\nThe current logic is the same, but more convoluted without good reason.\n\nPiperOrigin-RevId: 837814909",
    "sha": "0e824e0f865ef39ee05f8dfc70a2cec673be94ce",
    "files": [
        {
            "sha": "cdc9ae71d1f015eef00e08e3e010efda7efa9844",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 45,
            "deletions": 50,
            "changes": 95,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0e824e0f865ef39ee05f8dfc70a2cec673be94ce/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0e824e0f865ef39ee05f8dfc70a2cec673be94ce/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc?ref=0e824e0f865ef39ee05f8dfc70a2cec673be94ce",
            "patch": "@@ -1588,24 +1588,6 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCollectiveThunk(\n           << \"; partition count: \" << partition_count\n           << \"; operand count: \" << inst->operand_count();\n \n-  // A given collective op can be degenerate if across all groups\n-  // formed by it are singleton. In such a case, we don't need to do\n-  // any communication and we can just copy the input to the output.\n-  //\n-  // The only exception is RaggedAllToAll, which is not degenerate\n-  // even if all groups are singleton. In a singleton group case,\n-  // RaggedAllToAll becomes a generic equivalent of\n-  // DynamicUpdateSlice, except update size is not statically known.\n-  // This operation can not be expressed in term of standard HLO\n-  // instructions, so the best solution we have is to use NCCL thunk\n-  // even for degenerate cases.\n-  bool is_degenerate = kind != Thunk::Kind::kRaggedAllToAll &&\n-                       GetCollectiveConfig(inst, use_global_device_ids)\n-                           .IsDegenerate(replica_count, partition_count);\n-  absl::Status implementable_status = CollectiveThunkType::CheckImplementable(\n-      inst, replica_count, partition_count);\n-  bool should_use_nccl_thunk = !is_degenerate && implementable_status.ok();\n-\n   // Stash relevant information in CollectiveThunk::Buffer even if\n   // we may not generate an CollectiveThunk.\n   std::vector<CollectiveThunk::Buffer> buffers;\n@@ -1684,41 +1666,54 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCollectiveThunk(\n     }\n   }\n \n-  if (should_use_nccl_thunk) {\n-    auto thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n-        inst, ir_emitter_context_->GetNextThunkId());\n-    // The wrapper name is used when syntactic sugar is turned on.\n-    if (ir_emitter_context_->debug_options().xla_syntax_sugar_async_ops()) {\n-      thunk_info.profile_annotation = async_start->name();\n-    }\n-    std::unique_ptr<CollectiveThunkType> thunk;\n-    // TODO(b/828435206) Remove this constexpr once collective kernel thunk is\n-    // lifted out of the all reduce thunk.\n-    if constexpr (kRequiresCollectiveKernelThunk<CollectiveThunkType>) {\n-      TF_ASSIGN_OR_RETURN(\n-          auto collective_kernel_thunk,\n-          EmitCollectiveKernelThunk(ir_emitter_context_, call_graph_.get(),\n-                                    thunk_info, buffers,\n-                                    Cast<HloAllReduceInstruction>(inst),\n-                                    GetAllReduceConfigInst(inst)));\n-      thunk = std::make_unique<CollectiveThunkType>(\n-          thunk_info, inst, /*buffers=*/std::move(buffers),\n-          std::move(collective_kernel_thunk),\n-          ir_emitter_context_->debug_options().xla_gpu_use_memcpy_local_p2p());\n-    } else {\n-      thunk = std::make_unique<CollectiveThunkType>(\n-          thunk_info, inst, /*buffers=*/std::move(buffers),\n-          ir_emitter_context_->debug_options().xla_gpu_use_memcpy_local_p2p());\n-    }\n-    GetCollectivesAsyncEvents().insert({async_start, thunk->async_events()});\n-    return GetThunkSequence(std::move(thunk));\n-  }\n+  // A given collective op can be degenerate if across all groups\n+  // formed by it are singleton. In such a case, we don't need to do\n+  // any communication and we can just copy the input to the output.\n+  //\n+  // The only exception is RaggedAllToAll, which is not degenerate\n+  // even if all groups are singleton. In a singleton group case,\n+  // RaggedAllToAll becomes a generic equivalent of\n+  // DynamicUpdateSlice, except update size is not statically known.\n+  // This operation can not be expressed in term of standard HLO\n+  // instructions, so the best solution we have is to use NCCL thunk\n+  // even for degenerate cases.\n+  bool is_degenerate = kind != Thunk::Kind::kRaggedAllToAll &&\n+                       GetCollectiveConfig(inst, use_global_device_ids)\n+                           .IsDegenerate(replica_count, partition_count);\n \n-  if (!is_degenerate) {\n-    return implementable_status;\n+  if (is_degenerate) {\n+    return EmitDegeneratedCollectiveThunk(buffers, async_start, inst);\n   }\n \n-  return EmitDegeneratedCollectiveThunk(buffers, async_start, inst);\n+  TF_RETURN_IF_ERROR(CollectiveThunkType::CheckImplementable(\n+      inst, replica_count, partition_count));\n+\n+  auto thunk_info = Thunk::ThunkInfo::WithProfileAnnotation(\n+      inst, ir_emitter_context_->GetNextThunkId());\n+  // The wrapper name is used when syntactic sugar is turned on.\n+  if (ir_emitter_context_->debug_options().xla_syntax_sugar_async_ops()) {\n+    thunk_info.profile_annotation = async_start->name();\n+  }\n+  std::unique_ptr<CollectiveThunkType> thunk;\n+  // TODO(b/828435206) Remove this constexpr once collective kernel thunk is\n+  // lifted out of the all reduce thunk.\n+  if constexpr (kRequiresCollectiveKernelThunk<CollectiveThunkType>) {\n+    TF_ASSIGN_OR_RETURN(\n+        auto collective_kernel_thunk,\n+        EmitCollectiveKernelThunk(\n+            ir_emitter_context_, call_graph_.get(), thunk_info, buffers,\n+            Cast<HloAllReduceInstruction>(inst), GetAllReduceConfigInst(inst)));\n+    thunk = std::make_unique<CollectiveThunkType>(\n+        thunk_info, inst, /*buffers=*/std::move(buffers),\n+        std::move(collective_kernel_thunk),\n+        ir_emitter_context_->debug_options().xla_gpu_use_memcpy_local_p2p());\n+  } else {\n+    thunk = std::make_unique<CollectiveThunkType>(\n+        thunk_info, inst, /*buffers=*/std::move(buffers),\n+        ir_emitter_context_->debug_options().xla_gpu_use_memcpy_local_p2p());\n+  }\n+  GetCollectivesAsyncEvents().insert({async_start, thunk->async_events()});\n+  return GetThunkSequence(std::move(thunk));\n }\n \n // Find the canonical send/recv start op for one of send, recv,"
        }
    ],
    "stats": {
        "total": 95,
        "additions": 45,
        "deletions": 50
    }
}