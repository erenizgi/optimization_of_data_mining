{
    "author": "tensorflower-gardener",
    "message": "Document the effort level options avaiable in XLA.\n\nPiperOrigin-RevId: 836255708",
    "sha": "1d1f97e21aed5f89e56b05d588c3fa5372598d6b",
    "files": [
        {
            "sha": "1060f680df19f4d654a4ee13222acc56d0d80134",
            "filename": "third_party/xla/docs/_toc.yaml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1d1f97e21aed5f89e56b05d588c3fa5372598d6b/third_party%2Fxla%2Fdocs%2F_toc.yaml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1d1f97e21aed5f89e56b05d588c3fa5372598d6b/third_party%2Fxla%2Fdocs%2F_toc.yaml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2F_toc.yaml?ref=1d1f97e21aed5f89e56b05d588c3fa5372598d6b",
            "patch": "@@ -27,6 +27,8 @@ toc:\n     path: /xla/copybara\n   - title: Determinism\n     path: /xla/determinism\n+  - title: Effort Levels\n+    path: /xla/effort_levels\n   - title: Emitters\n     path: /xla/emitters\n   - title: Error Codes\n@@ -59,8 +61,6 @@ toc:\n     path: /xla/flags_guidance\n   - title: XLA Tooling\n     path: /xla/tools\n-  - title: XLA:GPU optimization levels\n-    path: /xla/gpu_optimization_levels\n - title: Debugging\n   section:\n   # This is the default tab for the Debugging section."
        },
        {
            "sha": "5628041ed01f7be8b0be1b43d73547327b37df4f",
            "filename": "third_party/xla/docs/effort_levels.md",
            "status": "added",
            "additions": 80,
            "deletions": 0,
            "changes": 80,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1d1f97e21aed5f89e56b05d588c3fa5372598d6b/third_party%2Fxla%2Fdocs%2Feffort_levels.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1d1f97e21aed5f89e56b05d588c3fa5372598d6b/third_party%2Fxla%2Fdocs%2Feffort_levels.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Feffort_levels.md?ref=1d1f97e21aed5f89e56b05d588c3fa5372598d6b",
            "patch": "@@ -0,0 +1,80 @@\n+# Effort Levels\n+\n+XLA provides options to control the amount of effort the compiler will expend to\n+\n+*   optimize for runtime performance, and\n+*   make the program \"fit in memory\" (which has a platform-dependent meaning)\n+\n+## Optimization Level\n+\n+Similar to the -O flags in gcc or clang, this field allows the user to influence\n+how much work the compiler does in optimizing for execution time. It can be set\n+via the\n+[optimization_level](https://github.com/openxla/xla/blob/f4d624b6811c28925c3006f5b779f1149b3b39ac/xla/pjrt/proto/compile_options.proto#L71)\n+field of the ExecutableBuildOptionsProto message, or the\n+[optimization_level](https://github.com/openxla/xla/blob/f4d624b6811c28925c3006f5b779f1149b3b39ac/xla/xla.proto#L1580)\n+field of the ExecutionOptions message.\n+\n+Lower optimization levels will cause various HLO passes to behave differently,\n+typically doing less work, or may disable certain HLO passes entirely. The\n+optimization level may also influence the compiler backend, such that the exact\n+effect of this field has a dependence on the target platform. However, as a\n+general guideline, the following table describes the expected overall effect of\n+each value:\n+\n+| Level     | Use Case                                                                |\n+| :-------- | :---------------------------------------------------------------------- |\n+| EFFORT_O0 | Fastest compilation, slowest runtime                                    |\n+| EFFORT_O1 | Faster compilation with reasonable runtime                              |\n+| EFFORT_O2 | Strongly prioritize runtime (suitable default for production workloads) |\n+| EFFORT_O3 | Expensive or experimental optimizations                                 |\n+\n+### Use in XLA:GPU\n+\n+In XLA:GPU, there are several passes that we disable by default because they\n+significantly increase compilation time by increasing the HLO size. For\n+convenience, we consolidate them under the optimization level option, such that\n+setting optimization_level to O1 or above will lead to the following behavior:\n+\n+*   Collectives commonly used for data-parallel communication will be pipelined.\n+    This behavior can also be steered more granularly by enabling individual\n+    flags.\n+    *   `xla_gpu_enable_pipelined_all_gather`\n+    *   `xla_gpu_enable_pipelined_all_reduce`\n+    *   `xla_gpu_enable_pipelined_reduce_scatter`\n+*   Unrolling while loops by a factor of two. Breaks down the loop-barrier\n+    potentially leading to a better compute-communication overlap and less\n+    copies.\n+    *   `xla_gpu_enable_while_loop_double_buffering`\n+*   Latency Hiding Scheduler will do most of the work to hide the communication\n+    latency.\n+    *   `xla_gpu_enable_latency_hiding_scheduler`\n+*   To maximize networking bandwidth, combiner passes will combine pipelined\n+    collectives to the maximum available memory. The optimization does not kick\n+    in if the loop is already unrolled in the input HLO.\n+    *   [all_gather_combiner](https://github.com/openxla/xla/blob/5b54d0e9cf34f4e5ab05b3752ecb390145ca5716/xla/service/gpu/transforms/collectives/all_gather_combiner.cc#L78)\n+    *   [all_reduce_combiner](https://github.com/openxla/xla/blob/5b54d0e9cf34f4e5ab05b3752ecb390145ca5716/xla/service/gpu/transforms/collectives/all_reduce_combiner.cc#L76)\n+    *   [reduce_scatter_combiner](https://github.com/openxla/xla/blob/5b54d0e9cf34f4e5ab05b3752ecb390145ca5716/xla/service/gpu/transforms/collectives/reduce_scatter_combiner.cc#L76)\n+\n+## Memory Fitting Level\n+\n+Another effort level option controls the degree to which the compiler will\n+attempt to make the resulting program \"fit in memory\", where \"fit\" and \"memory\"\n+have backend-dependent meanings (for example, in XLA:TPU, this option controls\n+the degree to which the compiler works to keep the TPU's high-bandwidth memory\n+(HBM) usage below the HBM capacity). It can be set via the\n+[memory_fitting_level](https://github.com/openxla/xla/blob/5b54d0e9cf34f4e5ab05b3752ecb390145ca5716/xla/pjrt/proto/compile_options.proto#L79)\n+field of the ExecutableBuildOptionsProto message, or the\n+[memory_fitting_level](https://github.com/openxla/xla/blob/f4d624b6811c28925c3006f5b779f1149b3b39ac/xla/xla.proto#L1588)\n+field of the ExecutionOptions message.\n+\n+As with optimization level, the exact meaning of each effort level value is\n+backend-dependent, but the following table describes the expected effect as a\n+general guideline:\n+\n+| Level     | Use Case                                                                |\n+| :-------- | :---------------------------------------------------------------------- |\n+| EFFORT_O0 | Minimal effort to fit (fail compilation as quickly as possible instead) |\n+| EFFORT_O1 | Reduced effort to fit                                                   |\n+| EFFORT_O2 | Significant effort to fit (suitable default for production workloads)   |\n+| EFFORT_O3 | Expensive or experimental algorithms to reduce memory usage             |"
        },
        {
            "sha": "67160dabcb97a260aa4f59f7524e53e428c5c0dc",
            "filename": "third_party/xla/docs/gpu_optimization_levels.md",
            "status": "removed",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/782b3a3f11b87680b13ce574fecec864dbc9c6ea/third_party%2Fxla%2Fdocs%2Fgpu_optimization_levels.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/782b3a3f11b87680b13ce574fecec864dbc9c6ea/third_party%2Fxla%2Fdocs%2Fgpu_optimization_levels.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Fgpu_optimization_levels.md?ref=782b3a3f11b87680b13ce574fecec864dbc9c6ea",
            "patch": "@@ -1,24 +0,0 @@\n-# XLA:GPU optimization Level (-On).\n-\n-There are some passes which we find beneficial to run (especially at scale), but\n-they increase the HLO size, and thus compilation time. That's why they are not\n-enabled by default. For convenience, we consolidate them under a single option.\n-\n-Setting [optimization_level](https://github.com/openxla/xla/blob/5b54d0e9cf34f4e5ab05b3752ecb390145ca5716/xla/pjrt/proto/compile_options.proto#L66-L71)\n-to [O1 or above](https://github.com/openxla/xla/blob/5b54d0e9cf34f4e5ab05b3752ecb390145ca5716/xla/xla.proto#L1481) will lead to the following behaviour:\n-\n-* Collectives commonly used for data-parallel communication will be pipelined.\n-This behavior can also be steered more granularly by enabling individual flags.\n-  * `xla_gpu_enable_pipelined_all_gather`\n-  * `xla_gpu_enable_pipelined_all_reduce`\n-  * `xla_gpu_enable_pipelined_reduce_scatter`\n-* Unrolling while loops by a factor of two. Breaks down the loop-barrier potentially leading to a better compute-communication overlap and less copies.\n-  * `xla_gpu_enable_while_loop_double_buffering`\n-* Latency Hiding Scheduler will do most the work to hide the communication latency.\n-  * `xla_gpu_enable_latency_hiding_scheduler`\n-* To maximize networking bandwidth, combiner passes will combine pipelined\n-collectives to the maximum available memory. The optimization does not kick in\n-if the loop is already unrolled in the input HLO.\n-  * [all_gather_combiner](https://github.com/openxla/xla/blob/5b54d0e9cf34f4e5ab05b3752ecb390145ca5716/xla/service/gpu/transforms/collectives/all_gather_combiner.cc#L78)\n-  * [all_reduce_combiner](https://github.com/openxla/xla/blob/5b54d0e9cf34f4e5ab05b3752ecb390145ca5716/xla/service/gpu/transforms/collectives/all_reduce_combiner.cc#L76)\n-  * [reduce_scatter_combiner](https://github.com/openxla/xla/blob/5b54d0e9cf34f4e5ab05b3752ecb390145ca5716/xla/service/gpu/transforms/collectives/reduce_scatter_combiner.cc#L76)"
        }
    ],
    "stats": {
        "total": 108,
        "additions": 82,
        "deletions": 26
    }
}