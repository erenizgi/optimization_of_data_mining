{
    "author": "felixwqp",
    "message": "For each topology(inter-partition or intra-partition),. categorize collective-permute instructions based on three patterns, ordered by latency from lowest to highest:\n- One-way: Each GPU only sends or receives data (e.g., {{0,1}, {2,3}}). This is common for pipeline parallelism and has the lowest latency.\n- Two-way-all-mutual: Devices may send and receive, but only when swapping data with the same peer (e.g., {{0,1}, {1,0}}).\n- Two-way-has-non-mutual: At least one GPU sends to one peer and receives from a different peer (e.g., {{0,1}, {1,2}}). This pattern is common in ring-based algorithms and has the highest latency.\n\nnote: If different devices in a collective exhibit different patterns, the entire collective is classified by the highest-latency pattern present, as the collective permute instruction latency is bounded by the largest latency.\n\nMore specifically, categories:\n- kIntraPartitionOneWay / kInterPartitionOneWay\n- kIntraPartitionTwoWayAllMutual / kInterPartitionTwoWayAllMutual\n- kIntraPartitionTwoWayHasNonMutual / kInterPartitionTwoWayHasNonMutual\n\nPiperOrigin-RevId: 832448003",
    "sha": "09e66286bdf27b2b63fdc665c9f28f1a73a20952",
    "files": [
        {
            "sha": "dc9e63e8b36c8aea4ca85fcba13ef5e87d921ed5",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils.cc",
            "status": "modified",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/09e66286bdf27b2b63fdc665c9f28f1a73a20952/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/09e66286bdf27b2b63fdc665c9f28f1a73a20952/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.cc?ref=09e66286bdf27b2b63fdc665c9f28f1a73a20952",
            "patch": "@@ -18,6 +18,8 @@ limitations under the License.\n #include <algorithm>\n #include <cstddef>\n #include <cstdint>\n+#include <optional>\n+#include <utility>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n@@ -134,8 +136,97 @@ bool IsNonWorldLevelCommunication(const CollectiveMetadata& pattern) {\n   return !IsSingleHost(pattern) && !IsWorldLevelCommunication(pattern);\n }\n \n+// Properties of a collective-permute instruction, categorizing its\n+// communication pattern.\n+struct CollectivePermuteProperty {\n+  std::vector<std::pair<int64_t, int64_t>> intra_partition_source_target_pairs;\n+  std::vector<std::pair<int64_t, int64_t>> inter_partition_source_target_pairs;\n+  // If true, at least one device both sends and receives data. If false, every\n+  // device involved in the collective-permute either only sends or only\n+  // receives data.\n+  bool has_devices_with_two_edges = false;\n+  // True if for every pair (s,t) in source_target_pairs, the pair (t,s) is\n+  // also present in source_target_pairs.\n+  bool is_all_mutual = false;\n+};\n+\n+// TODO(b/460155942): remove the optional wrapper once the HLO verifier stop\n+// supporting empty source-target pairs.\n+std::optional<CollectivePermuteProperty> GetCollectivePermuteProperty(\n+    const HloCollectivePermuteInstruction& instr,\n+    int64_t num_devices_per_partition) {\n+  if (instr.source_target_pairs().empty()) {\n+    return std::nullopt;\n+  }\n+\n+  CollectivePermuteProperty property;\n+  absl::flat_hash_set<int64_t> sources, targets;\n+  absl::flat_hash_set<std::pair<int64_t, int64_t>> pairs_set;\n+  absl::c_for_each(instr.source_target_pairs(),\n+                   [&](const auto& pair) { pairs_set.insert(pair); });\n+\n+  property.is_all_mutual = true;\n+\n+  for (const auto& [source, target] : instr.source_target_pairs()) {\n+    sources.insert(source);\n+    targets.insert(target);\n+\n+    bool is_intra_partition = (source / num_devices_per_partition ==\n+                               target / num_devices_per_partition);\n+\n+    if (is_intra_partition) {\n+      property.intra_partition_source_target_pairs.push_back({source, target});\n+    } else {\n+      property.inter_partition_source_target_pairs.push_back({source, target});\n+    }\n+    // If anyone of the pair (t,s) is not present in source_target_pairs, the\n+    // communication pattern is not all-mutual.\n+    if (property.is_all_mutual && !pairs_set.contains({target, source})) {\n+      property.is_all_mutual = false;\n+    }\n+  }\n+\n+  // If any source device is a target device, then it has two edges.\n+  for (int64_t source : sources) {\n+    if (targets.contains(source)) {\n+      property.has_devices_with_two_edges = true;\n+      break;\n+    }\n+  }\n+\n+  return property;\n+}\n+\n }  // namespace\n \n+CollectivePermuteCostModelType GetCollectivePermuteCostModelType(\n+    const HloCollectivePermuteInstruction& instr,\n+    int64_t num_devices_per_partition) {\n+  std::optional<CollectivePermuteProperty> property =\n+      GetCollectivePermuteProperty(instr, num_devices_per_partition);\n+  if (!property) {\n+    return CollectivePermuteCostModelType::kUnknown;\n+  }\n+\n+  if (!property->inter_partition_source_target_pairs.empty()) {\n+    if (property->has_devices_with_two_edges) {\n+      return property->is_all_mutual ? CollectivePermuteCostModelType::\n+                                           kInterPartitionTwoWayAllMutual\n+                                     : CollectivePermuteCostModelType::\n+                                           kInterPartitionTwoWayHasNonMutual;\n+    }\n+    return CollectivePermuteCostModelType::kInterPartitionOneWay;\n+  }\n+\n+  if (property->has_devices_with_two_edges) {\n+    return property->is_all_mutual\n+               ? CollectivePermuteCostModelType::kIntraPartitionTwoWayAllMutual\n+               : CollectivePermuteCostModelType::\n+                     kIntraPartitionTwoWayHasNonMutual;\n+  }\n+  return CollectivePermuteCostModelType::kIntraPartitionOneWay;\n+}\n+\n bool IsGPUSyncCollective(const HloInstruction& instr) {\n   auto backend_config = instr.backend_config<GpuBackendConfig>();\n   if (!backend_config.ok()) {"
        },
        {
            "sha": "3d0087f51ad6738a45a728f089e37cbf13ab1404",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils.h",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/09e66286bdf27b2b63fdc665c9f28f1a73a20952/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/09e66286bdf27b2b63fdc665c9f28f1a73a20952/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils.h?ref=09e66286bdf27b2b63fdc665c9f28f1a73a20952",
            "patch": "@@ -16,6 +16,11 @@ limitations under the License.\n #ifndef XLA_SERVICE_GPU_TRANSFORMS_COLLECTIVES_COLLECTIVE_OPS_UTILS_H_\n #define XLA_SERVICE_GPU_TRANSFORMS_COLLECTIVES_COLLECTIVE_OPS_UTILS_H_\n \n+#include <cstdint>\n+#include <optional>\n+#include <vector>\n+\n+#include \"absl/container/flat_hash_map.h\"\n #include \"absl/status/statusor.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -44,6 +49,37 @@ absl::StatusOr<GPUCommunicationType> CommunicationType(\n     int partition_size, const HloChannelInstruction& instr,\n     const se::GpuComputeCapability& gpu_version);\n \n+// Enum to categorize collective-permute cost models based on communication\n+// patterns. The cost model is determined by the highest-latency pattern\n+// present in any device: TwoWayHasNonMutual > TwoWayAllMutual > OneWay.\n+enum class CollectivePermuteCostModelType {\n+  // This is currently only used for CollectivePermute instructions with empty\n+  // source-target pairs.\n+  // TODO(b/460155942): Remove this field once the HLO verifier stop supporting\n+  // empty source-target pairs.\n+  kUnknown,\n+  // Intra-partition: All devices only send or only receive data.\n+  kIntraPartitionOneWay,\n+  // Intra-partition: Devices send/receive, but only with the same peer\n+  // (e.g., {{0,1},{1,0}}).\n+  kIntraPartitionTwoWayAllMutual,\n+  // Intra-partition: At least one device sends to one peer and receives from\n+  // another (e.g., {{0,1},{1,2}}).\n+  kIntraPartitionTwoWayHasNonMutual,\n+  // Inter-partition: All devices only send or only receive data.\n+  kInterPartitionOneWay,\n+  // Inter-partition: Devices send/receive, but only with the same peer.\n+  kInterPartitionTwoWayAllMutual,\n+  // Inter-partition: At least one device sends to one peer and receives from\n+  // another.\n+  kInterPartitionTwoWayHasNonMutual,\n+};\n+\n+// Returns cost model type based on collective-permute properties.\n+CollectivePermuteCostModelType GetCollectivePermuteCostModelType(\n+    const HloCollectivePermuteInstruction& instr,\n+    int64_t num_devices_per_partition);\n+\n // Returns true if instruction is a synchronous collective op.\n bool IsGPUSyncCollective(const HloInstruction& instr);\n "
        },
        {
            "sha": "3d9aea2e020140e55250cc0f6711b96e78de4405",
            "filename": "third_party/xla/xla/service/gpu/transforms/collectives/collective_ops_utils_test.cc",
            "status": "modified",
            "additions": 151,
            "deletions": 0,
            "changes": 151,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/09e66286bdf27b2b63fdc665c9f28f1a73a20952/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/09e66286bdf27b2b63fdc665c9f28f1a73a20952/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcollectives%2Fcollective_ops_utils_test.cc?ref=09e66286bdf27b2b63fdc665c9f28f1a73a20952",
            "patch": "@@ -481,4 +481,155 @@ TEST_F(CommunicationTypeTest, DetectsRailAlignedMultiPartition) {\n }\n \n }  // namespace\n+\n+TEST_F(CommunicationTypeTest, CollectivePermuteIntraPartitionOneWay) {\n+  absl::string_view kHlo = R\"(\n+    HloModule m, num_partitions=8\n+\n+    ENTRY e {\n+      p = f32[128] parameter(0)\n+      ROOT _ = f32[128] collective-permute(p),\n+        source_target_pairs={{0,1},{2,3},{4,5},{6,7}}\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(kHlo));\n+\n+  HloCollectivePermuteInstruction* instr =\n+      Cast<HloCollectivePermuteInstruction>(\n+          module->entry_computation()->root_instruction());\n+  EXPECT_EQ(GetCollectivePermuteCostModelType(*instr,\n+                                              /*num_devices_per_partition=*/8),\n+            CollectivePermuteCostModelType::kIntraPartitionOneWay);\n+}\n+\n+TEST_F(CommunicationTypeTest, CollectivePermuteIntraPartitionTwoWayMutual) {\n+  absl::string_view kHlo = R\"(\n+    HloModule m, num_partitions=4\n+\n+    ENTRY e {\n+      p = f32[128] parameter(0)\n+      ROOT _ = f32[128] collective-permute(p),\n+        source_target_pairs={{0,1},{1,0},{2,3},{3,2}}\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(kHlo));\n+\n+  HloCollectivePermuteInstruction* instr =\n+      Cast<HloCollectivePermuteInstruction>(\n+          module->entry_computation()->root_instruction());\n+  EXPECT_EQ(GetCollectivePermuteCostModelType(*instr,\n+                                              /*num_devices_per_partition=*/8),\n+            CollectivePermuteCostModelType::kIntraPartitionTwoWayAllMutual);\n+}\n+\n+TEST_F(CommunicationTypeTest, CollectivePermuteInterPartitionTwoWayMutual) {\n+  absl::string_view kHlo = R\"(\n+    HloModule m, num_partitions=16\n+\n+    ENTRY e {\n+      p = f32[128] parameter(0)\n+      ROOT _ = f32[128] collective-permute(p),\n+        source_target_pairs={{0,8},{8,0}}\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(kHlo));\n+\n+  HloCollectivePermuteInstruction* instr =\n+      Cast<HloCollectivePermuteInstruction>(\n+          module->entry_computation()->root_instruction());\n+  EXPECT_EQ(GetCollectivePermuteCostModelType(*instr,\n+                                              /*num_devices_per_partition=*/8),\n+            CollectivePermuteCostModelType::kInterPartitionTwoWayAllMutual);\n+}\n+\n+TEST_F(CommunicationTypeTest, CollectivePermuteInterPartitionOneWay) {\n+  absl::string_view kHlo = R\"(\n+    HloModule m, num_partitions=16\n+\n+    ENTRY e {\n+      p = f32[128] parameter(0)\n+      ROOT _ = f32[128] collective-permute(p),\n+        source_target_pairs={{0,8},{1,9}}\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(kHlo));\n+\n+  HloCollectivePermuteInstruction* instr =\n+      Cast<HloCollectivePermuteInstruction>(\n+          module->entry_computation()->root_instruction());\n+  EXPECT_EQ(GetCollectivePermuteCostModelType(*instr,\n+                                              /*num_devices_per_partition=*/8),\n+            CollectivePermuteCostModelType::kInterPartitionOneWay);\n+}\n+\n+TEST_F(CommunicationTypeTest,\n+       CollectivePermuteIntraPartitionTwoWayHasNonMutual) {\n+  absl::string_view kHlo = R\"(\n+    HloModule m, num_partitions=8\n+\n+    ENTRY e {\n+      p = f32[128] parameter(0)\n+      ROOT _ = f32[128] collective-permute(p),\n+        source_target_pairs={{0,1},{1,2},{2,0}}\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(kHlo));\n+\n+  HloCollectivePermuteInstruction* instr =\n+      Cast<HloCollectivePermuteInstruction>(\n+          module->entry_computation()->root_instruction());\n+  EXPECT_EQ(GetCollectivePermuteCostModelType(*instr,\n+                                              /*num_devices_per_partition=*/8),\n+            CollectivePermuteCostModelType::kIntraPartitionTwoWayHasNonMutual);\n+}\n+\n+TEST_F(CommunicationTypeTest,\n+       CollectivePermuteInterPartitionTwoWayHasNonMutual) {\n+  absl::string_view kHlo = R\"(\n+    HloModule m, num_partitions=16\n+    ENTRY e {\n+      p = f32[128] parameter(0)\n+      ROOT _ = f32[128] collective-permute(p),\n+        source_target_pairs={{0,8},{1,9},{8,2}}\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(kHlo));\n+\n+  HloCollectivePermuteInstruction* instr =\n+      Cast<HloCollectivePermuteInstruction>(\n+          module->entry_computation()->root_instruction());\n+  EXPECT_EQ(GetCollectivePermuteCostModelType(*instr,\n+                                              /*num_devices_per_partition=*/8),\n+            CollectivePermuteCostModelType::kInterPartitionTwoWayHasNonMutual);\n+}\n+\n+// TODO(b/460155942): remove once the collective-permute with empty pairs is\n+// disallowed by the HLO verifier.\n+TEST_F(CommunicationTypeTest, CollectivePermuteEmptyPairs) {\n+  absl::string_view kHlo = R\"(\n+    HloModule m, num_partitions=8\n+\n+    ENTRY e {\n+      p = f32[128] parameter(0)\n+      ROOT _ = f32[128] collective-permute(p),\n+        source_target_pairs={}\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnUnverifiedModule(kHlo));\n+\n+  HloCollectivePermuteInstruction* instr =\n+      Cast<HloCollectivePermuteInstruction>(\n+          module->entry_computation()->root_instruction());\n+  EXPECT_EQ(GetCollectivePermuteCostModelType(*instr,\n+                                              /*num_devices_per_partition=*/8),\n+            CollectivePermuteCostModelType::kUnknown);\n+}\n+\n }  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 278,
        "additions": 278,
        "deletions": 0
    }
}