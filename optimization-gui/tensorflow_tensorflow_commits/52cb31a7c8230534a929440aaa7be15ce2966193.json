{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Move all allocations to `StreamState`.\n\nPiperOrigin-RevId: 804492400",
    "sha": "52cb31a7c8230534a929440aaa7be15ce2966193",
    "files": [
        {
            "sha": "484fddb1312f6a86a4a87ed2b909d8ed886f766d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all_thunk.cc",
            "status": "modified",
            "additions": 50,
            "deletions": 66,
            "changes": 116,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/52cb31a7c8230534a929440aaa7be15ce2966193/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/52cb31a7c8230534a929440aaa7be15ce2966193/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc?ref=52cb31a7c8230534a929440aaa7be15ce2966193",
            "patch": "@@ -88,8 +88,8 @@ RaggedAllToAllConfig GetRaggedAllToAllConfig(\n // in the host memory allocated by StreamExecutor to copy data from the device\n // memory.\n absl::Status LoadRaggedTensorMetadata(\n-    se::Stream& stream, const std::vector<DeviceBufferPair>& buffers,\n-    const std::vector<int64_t*>& ragged_metadata_allocs) {\n+    se::Stream& stream, absl::Span<DeviceBufferPair const> buffers,\n+    absl::Span<int64_t* const> ragged_metadata_allocs) {\n   for (int64_t i = 0; i < kNumRaggedMetadataOperands; ++i) {\n     TF_RETURN_IF_ERROR(stream.Memcpy(ragged_metadata_allocs[i],\n                                      buffers[i + 2].source_buffer,\n@@ -146,7 +146,7 @@ absl::Status RunAllToAllOnIndexBuffer(\n absl::Status RunRaggedAllToAll(\n     int64_t ragged_row_element_size, int64_t num_total_updates,\n     const std::vector<DeviceBufferPair>& original_buffers, se::Stream& stream,\n-    Communicator* comm, const std::vector<int64_t*>& ragged_metadata_allocs,\n+    Communicator* comm, absl::Span<int64_t* const> ragged_metadata_allocs,\n     const se::DeviceMemoryBase& output_offsets_device_buffer,\n     bool use_symmetric_buffer) {\n   int device_ordinal = stream.parent()->device_ordinal();\n@@ -317,8 +317,8 @@ absl::Status RendezvousAfterKernelFinish(\n absl::Status RunMemCpyRaggedAllToAll(\n     const GpuCliqueKey& clique_key, RankId rank,\n     int64_t ragged_row_element_size, int64_t num_total_updates,\n-    const std::vector<DeviceBufferPair>& buffers, se::Stream& stream,\n-    Communicator* comm, const std::vector<int64_t*>& ragged_metadata_allocs,\n+    absl::Span<DeviceBufferPair const> buffers, se::Stream& stream,\n+    Communicator* comm, absl::Span<int64_t* const> ragged_metadata_allocs,\n     se::Event* start_event, se::Event* end_event) {\n   int device_ordinal = stream.parent()->device_ordinal();\n   VLOG(3) << \"[\" << device_ordinal << \"] Performing mem-copy-ragged-all-to-all\";\n@@ -466,60 +466,54 @@ absl::Status RaggedAllToAllStartThunk::Initialize(\n   TF_RETURN_IF_ERROR(CollectiveThunk::Initialize(params));\n   device_count_ = params.local_device_count;\n \n+  se::StreamExecutor* executor = params.executor;\n+\n   {\n-    // Allocate temp buffers in the host memory to load the sizes and offsets of\n-    // ragged tensors from device memory.\n     absl::MutexLock lock(&mutex_);\n-    if (!host_buffer_allocs_.contains(params.executor)) {\n-      std::vector<std::unique_ptr<se::MemoryAllocation>> allocs;\n-      for (int64_t i = 0; i < kNumRaggedMetadataOperands; ++i) {\n-        TF_ASSIGN_OR_RETURN(std::unique_ptr<se::MemoryAllocation> alloc,\n-                            params.executor->HostMemoryAllocate(\n-                                config_.num_total_updates * sizeof(int64_t)));\n-        allocs.push_back(std::move(alloc));\n-      }\n-      host_buffer_allocs_.emplace(params.executor, std::move(allocs));\n-    }\n-\n-    if (!device_buffer_allocs_.contains(params.executor)) {\n-      se::DeviceMemoryHandle output_offsets_device_buffer{\n-          params.executor, params.executor->Allocate(config_.num_total_updates *\n-                                                     sizeof(int64_t))};\n \n-      if (output_offsets_device_buffer.memory().is_null()) {\n-        return absl::InternalError(\"Failed to allocate output offsets buffer.\");\n-      }\n-\n-      device_buffer_allocs_.emplace(params.executor,\n-                                    std::move(output_offsets_device_buffer));\n+    // If the stream state already exists, it means that the thunk has been\n+    // initialized for this executor.\n+    if (per_stream_states_.contains(executor)) {\n+      return absl::OkStatus();\n     }\n   }\n \n-  if (is_local()) {\n-    absl::MutexLock lock(&mutex_);\n+  TF_ASSIGN_OR_RETURN(\n+      const GpuCliqueKey clique_key,\n+      GetCollectiveGpuCliqueKey(*params.collective_params, config_.config));\n+  const std::optional<RankId> rank =\n+      clique_key.rank(params.collective_params->global_device_id);\n \n-    se::StreamExecutor* executor = params.executor;\n+  auto state =\n+      std::make_unique<StreamState>(executor->device_ordinal(), rank.value());\n \n-    if (!per_stream_states_.contains(executor)) {\n-      TF_ASSIGN_OR_RETURN(\n-          const GpuCliqueKey clique_key,\n-          GetCollectiveGpuCliqueKey(*params.collective_params, config_.config));\n+  // Allocate temp buffers in the host memory to load the sizes and offsets of\n+  // ragged tensors from device memory.\n+  for (int64_t i = 0; i < kNumRaggedMetadataOperands; ++i) {\n+    TF_ASSIGN_OR_RETURN(std::unique_ptr<se::MemoryAllocation> alloc,\n+                        executor->HostMemoryAllocate(config_.num_total_updates *\n+                                                     sizeof(int64_t)));\n+    state->host_buffer_allocs.push_back(std::move(alloc));\n+  }\n \n-      const std::optional<RankId> rank =\n-          clique_key.rank(params.collective_params->global_device_id);\n+  state->output_offsets_device_buffer = se::DeviceMemoryHandle{\n+      executor,\n+      executor->Allocate(config_.num_total_updates * sizeof(int64_t))};\n \n-      TF_ASSIGN_OR_RETURN(std::unique_ptr<se::Event> start_event,\n-                          executor->CreateEvent());\n-      TF_ASSIGN_OR_RETURN(std::unique_ptr<se::Event> end_event,\n-                          executor->CreateEvent());\n+  if (state->output_offsets_device_buffer.memory().is_null()) {\n+    return absl::InternalError(\"Failed to allocate output offsets buffer.\");\n+  }\n \n-      auto state = std::make_unique<StreamState>(\n-          executor->device_ordinal(), rank.value(), std::move(start_event),\n-          std::move(end_event));\n+  if (is_local()) {\n+    TF_ASSIGN_OR_RETURN(state->start_event, executor->CreateEvent());\n+    TF_ASSIGN_OR_RETURN(state->end_event, executor->CreateEvent());\n+  }\n \n-      per_stream_states_.emplace(executor, std::move(state));\n-    }\n+  {\n+    absl::MutexLock lock(&mutex_);\n+    per_stream_states_.emplace(executor, std::move(state));\n   }\n+\n   return absl::OkStatus();\n }\n \n@@ -545,25 +539,6 @@ absl::StatusOr<bool> RaggedAllToAllStartThunk::RunCollective(\n       ConvertToDeviceBuffers(params, buffers_,\n                              config_.config.operand_element_type));\n \n-  // Get buffer allocs to load sizes and offsets of ragged tensors from device\n-  // memory.\n-  std::vector<int64_t*> ragged_metadata_allocs(kNumRaggedMetadataOperands);\n-  se::DeviceMemoryBase output_offsets_device_buffer;\n-  {\n-    absl::MutexLock lock(&mutex_);\n-    auto it = host_buffer_allocs_.find(stream.parent());\n-    CHECK(it != host_buffer_allocs_.end());\n-\n-    for (int64_t i = 0; i < kNumRaggedMetadataOperands; ++i) {\n-      ragged_metadata_allocs[i] =\n-          reinterpret_cast<int64_t*>(it->second[i]->opaque());\n-    }\n-\n-    auto jt = device_buffer_allocs_.find(stream.parent());\n-    CHECK(jt != device_buffer_allocs_.end());\n-    output_offsets_device_buffer = jt->second.memory();\n-  }\n-\n   std::optional<RankId> rank =\n       comm_handle.clique_key.rank(params.collective_params->global_device_id);\n   TF_ASSIGN_OR_RETURN(int32_t num_ranks, comm_handle.comm->NumRanks());\n@@ -578,6 +553,14 @@ absl::StatusOr<bool> RaggedAllToAllStartThunk::RunCollective(\n     state = per_stream_states_[stream.parent()].get();\n   }\n \n+  // Get buffer allocs to load sizes and offsets of ragged tensors from device\n+  // memory.\n+  absl::InlinedVector<int64_t*, 8> ragged_metadata_allocs;\n+  for (int64_t i = 0; i < kNumRaggedMetadataOperands; ++i) {\n+    ragged_metadata_allocs.push_back(\n+        reinterpret_cast<int64_t*>(state->host_buffer_allocs[i]->opaque()));\n+  }\n+\n   bool should_use_one_shot_kernel =\n       is_local() && one_shot_kernel_enabled_ && peer_access_enabled &&\n       IsRaggedAllToAllKernelSupported(num_ranks,\n@@ -604,7 +587,8 @@ absl::StatusOr<bool> RaggedAllToAllStartThunk::RunCollective(\n   TF_RETURN_IF_ERROR(RunRaggedAllToAll(\n       config_.num_row_elements, config_.num_total_updates, device_buffers,\n       stream, comm_handle.comm, ragged_metadata_allocs,\n-      output_offsets_device_buffer, config_.config.use_symmetric_buffer));\n+      state->output_offsets_device_buffer.memory(),\n+      config_.config.use_symmetric_buffer));\n   return true;\n }\n "
        },
        {
            "sha": "d173c716165807bcc5e2670c7927c24bd0e07991",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all_thunk.h",
            "status": "modified",
            "additions": 10,
            "deletions": 15,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/52cb31a7c8230534a929440aaa7be15ce2966193/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/52cb31a7c8230534a929440aaa7be15ce2966193/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.h?ref=52cb31a7c8230534a929440aaa7be15ce2966193",
            "patch": "@@ -18,11 +18,11 @@ limitations under the License.\n \n #include <cstdint>\n #include <memory>\n-#include <utility>\n #include <vector>\n \n #include \"absl/base/thread_annotations.h\"\n #include \"absl/container/flat_hash_map.h\"\n+#include \"absl/container/inlined_vector.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/synchronization/mutex.h\"\n@@ -81,6 +81,13 @@ class RaggedAllToAllStartThunk : public CollectiveThunk {\n     int device_ordinal;\n     RankId rank;\n \n+    // Host memory allocations for ragged metadata.\n+    absl::InlinedVector<std::unique_ptr<se::MemoryAllocation>, 8>\n+        host_buffer_allocs;\n+\n+    // Device memory buffer for output offsets.\n+    se::DeviceMemoryHandle output_offsets_device_buffer;\n+\n     // Event to synchronize streams on different devices at the start of the\n     // kernel.\n     std::unique_ptr<se::Event> start_event;\n@@ -89,13 +96,8 @@ class RaggedAllToAllStartThunk : public CollectiveThunk {\n     // kernel.\n     std::unique_ptr<se::Event> end_event;\n \n-    StreamState(int device_ordinal, RankId rank,\n-                std::unique_ptr<se::Event> start_event,\n-                std::unique_ptr<se::Event> end_event)\n-        : device_ordinal(device_ordinal),\n-          rank(rank),\n-          start_event(std::move(start_event)),\n-          end_event(std::move(end_event)) {}\n+    StreamState(int device_ordinal, RankId rank)\n+        : device_ordinal(device_ordinal), rank(rank) {}\n   };\n \n   bool is_local() const;\n@@ -108,13 +110,6 @@ class RaggedAllToAllStartThunk : public CollectiveThunk {\n   const bool one_shot_kernel_enabled_;\n \n   absl::Mutex mutex_;\n-  absl::flat_hash_map<se::StreamExecutor*,\n-                      std::vector<std::unique_ptr<se::MemoryAllocation>>>\n-      host_buffer_allocs_ ABSL_GUARDED_BY(mutex_);\n-\n-  absl::flat_hash_map<se::StreamExecutor*, se::DeviceMemoryHandle>\n-      device_buffer_allocs_ ABSL_GUARDED_BY(mutex_);\n-\n   absl::flat_hash_map<se::StreamExecutor*, std::unique_ptr<StreamState>>\n       per_stream_states_ ABSL_GUARDED_BY(mutex_);\n };"
        }
    ],
    "stats": {
        "total": 141,
        "additions": 60,
        "deletions": 81
    }
}