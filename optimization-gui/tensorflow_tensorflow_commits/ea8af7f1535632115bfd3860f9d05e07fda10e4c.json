{
    "author": "basioli-k",
    "message": "[XLA][codegen] Remove se::DeviceDescription argument from the shared parts of the fusion_emitter APIs.\n\nThis change introduces a `FusionEmitter` base class which currently emits xtile.\nThis is done to remove se::DeviceDescription from the fusion_emitter API in the interest of sharing the infra between CPU and GPU.\n\nThe Triton specific logic is implemented in `TritonFusionEmitter`.\n\nPiperOrigin-RevId: 830908750",
    "sha": "ea8af7f1535632115bfd3860f9d05e07fda10e4c",
    "files": [
        {
            "sha": "b2c40db85ba547fcedab83fc44daf733b01be6b4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea8af7f1535632115bfd3860f9d05e07fda10e4c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea8af7f1535632115bfd3860f9d05e07fda10e4c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=ea8af7f1535632115bfd3860f9d05e07fda10e4c",
            "patch": "@@ -446,9 +446,14 @@ cc_library(\n         \"fusion_emitter_legacy_matmul.h\",\n     ],\n     deps = [\n+        \":emitter_helpers\",\n+        \":support\",\n         \"//xla:autotuning_proto_cc\",\n+        \"//xla:util\",\n         \"//xla/codegen:emitter_loc_op_builder\",\n         \"//xla/codegen/tiling:symbolic_tile_analysis\",\n+        \"//xla/codegen/tiling:tiled_hlo_computation\",\n+        \"//xla/codegen/tiling:tiled_hlo_instruction\",\n         \"//xla/codegen/xtile/ir:xtile\",\n         \"//xla/hlo/analysis:symbolic_expr\",\n         \"//xla/hlo/ir:hlo\",\n@@ -461,6 +466,7 @@ cc_library(\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:launch_dim\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:string_view\",\n@@ -924,6 +930,7 @@ cc_library(\n         \"//xla/service/gpu:ir_emission_utils\",\n         \"//xla/service/gpu:matmul_utils\",\n         \"//xla/service/gpu/model:block_level_parameters\",\n+        \"//xla/service/gpu/model:triton_emitter_constraints\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n         \"//xla/tests:hlo_test_base\","
        },
        {
            "sha": "4fe85be555481c113f2fdf1d0274e6d51e56a72d",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 10,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea8af7f1535632115bfd3860f9d05e07fda10e4c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea8af7f1535632115bfd3860f9d05e07fda10e4c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.cc?ref=ea8af7f1535632115bfd3860f9d05e07fda10e4c",
            "patch": "@@ -317,8 +317,7 @@ Value Compare(EmitterLocOpBuilder& b, ValueRange values,\n       values[0], values[1]);\n }\n \n-Value Maximum(EmitterLocOpBuilder& b, const se::DeviceDescription& device_info,\n-              ValueRange values) {\n+Value Maximum(EmitterLocOpBuilder& b, ValueRange values) {\n   if (mlir::isa<mlir::FloatType>(mlir::getElementTypeOrSelf(values[0]))) {\n     return b.create<ma::MaximumFOp>(values);\n   }\n@@ -338,8 +337,7 @@ Value Maximum(EmitterLocOpBuilder& b, const se::DeviceDescription& device_info,\n       values[0], values[1]);\n }\n \n-Value Minimum(EmitterLocOpBuilder& b, const se::DeviceDescription& device_info,\n-              ValueRange values) {\n+Value Minimum(EmitterLocOpBuilder& b, ValueRange values) {\n   if (mlir::isa<mlir::FloatType>(mlir::getElementTypeOrSelf(values[0]))) {\n     return b.create<ma::MinimumFOp>(values);\n   }\n@@ -414,7 +412,6 @@ absl::StatusOr<Value> EmitElementwiseLibdeviceFunction(\n }\n \n absl::StatusOr<Value> EmitElementwise(EmitterLocOpBuilder& b,\n-                                      const se::DeviceDescription& device_info,\n                                       const HloInstruction& hlo,\n                                       ValueRange inputs) {\n   const bool is_integer =\n@@ -461,13 +458,11 @@ absl::StatusOr<Value> EmitElementwise(EmitterLocOpBuilder& b,\n       }\n       return b.create<ma::MulFOp>(inputs[0], inputs[1]);\n     case HloOpcode::kMaximum:\n-      return Maximum(b, device_info, inputs);\n+      return Maximum(b, inputs);\n     case HloOpcode::kMinimum:\n-      return Minimum(b, device_info, inputs);\n+      return Minimum(b, inputs);\n     case HloOpcode::kClamp:\n-      return Maximum(\n-          b, device_info,\n-          {Minimum(b, device_info, {inputs[1], inputs[2]}), inputs[0]});\n+      return Maximum(b, {Minimum(b, {inputs[1], inputs[2]}), inputs[0]});\n     case HloOpcode::kAnd:\n       return b.create<ma::AndIOp>(inputs[0], inputs[1]);\n     case HloOpcode::kOr:"
        },
        {
            "sha": "3605481a59e3130fa69628fe4e23caa9ef090eb7",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea8af7f1535632115bfd3860f9d05e07fda10e4c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea8af7f1535632115bfd3860f9d05e07fda10e4c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h?ref=ea8af7f1535632115bfd3860f9d05e07fda10e4c",
            "patch": "@@ -165,9 +165,9 @@ absl::StatusOr<mlir::Value> EmitElementwiseLibdeviceFunction(\n     const se::DeviceDescription& device_info, const HloInstruction& hlo,\n     mlir::ValueRange inputs);\n \n-absl::StatusOr<mlir::Value> EmitElementwise(\n-    EmitterLocOpBuilder& b, const se::DeviceDescription& device_info,\n-    const HloInstruction& hlo, mlir::ValueRange inputs);\n+absl::StatusOr<mlir::Value> EmitElementwise(EmitterLocOpBuilder& b,\n+                                            const HloInstruction& hlo,\n+                                            mlir::ValueRange inputs);\n \n mlir::Value Bitcast(EmitterLocOpBuilder& b, mlir::Value value, mlir::Type type);\n "
        },
        {
            "sha": "f5e73e89f79fef911460de8c9c00499a9e1c1cb5",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 70,
            "deletions": 78,
            "changes": 148,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea8af7f1535632115bfd3860f9d05e07fda10e4c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea8af7f1535632115bfd3860f9d05e07fda10e4c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=ea8af7f1535632115bfd3860f9d05e07fda10e4c",
            "patch": "@@ -344,15 +344,13 @@ TensorValue EmitParameterExtract(EmitterLocOpBuilder b,\n }\n \n absl::StatusOr<TensorValue> EmitScope(\n-    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n-    const TritonFusionAnalysis* analysis,\n+    EmitterLocOpBuilder b, const TritonFusionAnalysis* analysis,\n     absl::Span<const HloInstruction* const> instructions,\n     absl::flat_hash_map<const HloInstruction*, TensorValue>& values);\n \n absl::StatusOr<TensorValue> EmitReduce(\n     EmitterLocOpBuilder b, const TiledHloInstruction& tiled_hlo_reduce,\n-    absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values,\n-    const se::DeviceDescription& device_info) {\n+    absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values) {\n   // At the moment, we should only emit a full reduction over a single\n   // dimension using a scalar as a neutral element.\n   const HloReduceInstruction& hlo_reduce =\n@@ -429,9 +427,8 @@ absl::StatusOr<TensorValue> EmitReduce(\n \n     TF_RET_CHECK(!to_emit.empty());\n \n-    TF_ASSIGN_OR_RETURN(TensorValue result,\n-                        EmitScope(b, device_info, /*analysis=*/nullptr, to_emit,\n-                                  region_values));\n+    TF_ASSIGN_OR_RETURN(TensorValue result, EmitScope(b, /*analysis=*/nullptr,\n+                                                      to_emit, region_values));\n     b.create<stablehlo::ReturnOp>(SmallVector<Value>({result}));\n     b.setInsertionPointAfter(reduction);\n   }\n@@ -446,8 +443,7 @@ absl::StatusOr<TensorValue> EmitReduce(\n //\n // TODO(b/331413981): get rid of this special handling once this is solved.\n absl::StatusOr<TensorValue> EmitNestedFusion(\n-    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n-    const HloFusionInstruction& fusion_instruction,\n+    EmitterLocOpBuilder b, const HloFusionInstruction& fusion_instruction,\n     absl::flat_hash_map<const HloInstruction*, TensorValue>& values) {\n   // TODO(b/331402498): revisit the order of scope once we completely\n   // deprecate Triton fusion analysis.\n@@ -471,8 +467,7 @@ absl::StatusOr<TensorValue> EmitNestedFusion(\n \n   TF_RET_CHECK(to_emit.back() == fusion_computation->root_instruction());\n \n-  return EmitScope(b, device_info, /*analysis=*/nullptr, to_emit,\n-                   region_values);\n+  return EmitScope(b, /*analysis=*/nullptr, to_emit, region_values);\n }\n \n template <typename T>\n@@ -672,13 +667,11 @@ absl::StatusOr<TensorValue> EmitTiledBitcast(\n }\n \n absl::StatusOr<std::vector<TensorValue>> EmitTiledComputation(\n-    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n-    const HloFusionInstruction* fusion,\n+    EmitterLocOpBuilder b, const HloFusionInstruction* fusion,\n     const TiledHloComputation& tiled_computation,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n     absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values);\n-\n // Returns the number of iterations of the loop over the contracting\n // dimension of matrix multiplication.\n absl::StatusOr<int64_t> GetDotLoopIterationCount(\n@@ -849,8 +842,7 @@ absl::StatusOr<TensorValue> CanonicalizeDotOperand(\n }\n \n absl::StatusOr<TensorValue> EmitDot(\n-    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n-    const HloFusionInstruction* fusion,\n+    EmitterLocOpBuilder b, const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_hlo_dot,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n@@ -953,8 +945,7 @@ absl::StatusOr<TensorValue> EmitDot(\n       TF_ASSIGN_OR_RETURN(\n           std::vector<TensorValue> result,\n           EmitTiledComputation(\n-              b, device_info,\n-              ::xla::Cast<HloFusionInstruction>(tiled_fusion_operand->hlo()),\n+              b, ::xla::Cast<HloFusionInstruction>(tiled_fusion_operand->hlo()),\n               *tiled_fusion_operand->called_computation(),\n               block_level_parameters, fn, computation_index, values));\n       if (result.size() != 1) {\n@@ -1017,8 +1008,7 @@ absl::StatusOr<TensorValue> EmitDot(\n }\n \n absl::StatusOr<TensorValue> EmitScaledDot(\n-    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n-    const HloFusionInstruction* fusion,\n+    EmitterLocOpBuilder b, const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_hlo_dot,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n@@ -1086,8 +1076,7 @@ absl::StatusOr<TensorValue> EmitScaledDot(\n       TF_ASSIGN_OR_RETURN(\n           std::vector<TensorValue> result,\n           EmitTiledComputation(\n-              b, device_info,\n-              ::xla::Cast<HloFusionInstruction>(tiled_fusion_operand->hlo()),\n+              b, ::xla::Cast<HloFusionInstruction>(tiled_fusion_operand->hlo()),\n               *tiled_fusion_operand->called_computation(),\n               block_level_parameters, fn, computation_index, values));\n       if (result.size() != 1) {\n@@ -1171,8 +1160,7 @@ absl::StatusOr<TensorValue> EmitScaledDot(\n }\n \n absl::StatusOr<TensorValue> EmitConcatenate(\n-    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n-    const HloFusionInstruction* fusion,\n+    EmitterLocOpBuilder b, const HloFusionInstruction* fusion,\n     const TiledHloInstruction& tiled_concatenate,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n@@ -1265,8 +1253,7 @@ absl::StatusOr<TensorValue> EmitConcatenate(\n     TF_ASSIGN_OR_RETURN(\n         std::vector<TensorValue> result,\n         EmitTiledComputation(\n-            b, device_info,\n-            ::xla::Cast<HloFusionInstruction>(tiled_fusion_operand->hlo()),\n+            b, ::xla::Cast<HloFusionInstruction>(tiled_fusion_operand->hlo()),\n             *tiled_fusion_operand->called_computation(), block_level_parameters,\n             fn, pid, values));\n     CHECK_EQ(result.size(), 1);\n@@ -1279,8 +1266,7 @@ absl::StatusOr<TensorValue> EmitConcatenate(\n }\n \n absl::StatusOr<TensorValue> EmitPad(\n-    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n-    const TiledHloInstruction& tiled_pad,\n+    EmitterLocOpBuilder b, const TiledHloInstruction& tiled_pad,\n     absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values,\n     Value pid) {\n   // TODO(b/393299275): get rid of calls to `GetPaddedTileSizes` once tiling\n@@ -1336,8 +1322,8 @@ absl::StatusOr<TensorValue> EmitPad(\n }\n \n absl::StatusOr<TensorValue> EmitTiledHloInstruction(\n-    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n-    const HloFusionInstruction* fusion, const TiledHloInstruction& tiled_hlo,\n+    EmitterLocOpBuilder b, const HloFusionInstruction* fusion,\n+    const TiledHloInstruction& tiled_hlo,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n     absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values) {\n@@ -1387,22 +1373,22 @@ absl::StatusOr<TensorValue> EmitTiledHloInstruction(\n   }\n \n   if (hlo->opcode() == HloOpcode::kConcatenate) {\n-    return EmitConcatenate(b, device_info, fusion, tiled_hlo,\n-                           block_level_parameters, fn, pid, values);\n+    return EmitConcatenate(b, fusion, tiled_hlo, block_level_parameters, fn,\n+                           pid, values);\n   }\n \n   if (hlo->opcode() == HloOpcode::kPad) {\n-    return EmitPad(b, device_info, tiled_hlo, values, pid);\n+    return EmitPad(b, tiled_hlo, values, pid);\n   }\n \n   if (hlo->opcode() == HloOpcode::kDot) {\n-    return EmitDot(b, device_info, fusion, tiled_hlo, block_level_parameters,\n-                   fn, pid, values);\n+    return EmitDot(b, fusion, tiled_hlo, block_level_parameters, fn, pid,\n+                   values);\n   }\n \n   if (hlo->opcode() == HloOpcode::kScaledDot) {\n-    return EmitScaledDot(b, device_info, fusion, tiled_hlo,\n-                         block_level_parameters, fn, pid, values);\n+    return EmitScaledDot(b, fusion, tiled_hlo, block_level_parameters, fn, pid,\n+                         values);\n   }\n \n   if (hlo->opcode() == HloOpcode::kConstant) {\n@@ -1422,7 +1408,7 @@ absl::StatusOr<TensorValue> EmitTiledHloInstruction(\n   }\n \n   if (hlo->opcode() == HloOpcode::kReduce) {\n-    return EmitReduce(b, tiled_hlo, values, device_info);\n+    return EmitReduce(b, tiled_hlo, values);\n   }\n \n   if (hlo->IsElementwise()) {\n@@ -1432,8 +1418,7 @@ absl::StatusOr<TensorValue> EmitTiledHloInstruction(\n     for (const TiledHloInstruction* operand : tiled_hlo.operands()) {\n       operands.push_back(values[operand]);\n     }\n-    TF_ASSIGN_OR_RETURN(Value result,\n-                        EmitElementwise(b, device_info, *hlo, operands));\n+    TF_ASSIGN_OR_RETURN(Value result, EmitElementwise(b, *hlo, operands));\n     return mlir::cast<TensorValue>(result);\n   }\n \n@@ -1470,12 +1455,8 @@ absl::StatusOr<TensorValue> EmitTiledHloInstruction(\n       absl::StrCat(\"Unsupported operation \", hlo->ToString()));\n }\n \n-// Emit a sequence of instructions using compatible tiling with producers\n-// ordered before consumers in `tiled_computation`. Returns the results for the\n-// roots of `tiled_computation`.\n absl::StatusOr<std::vector<TensorValue>> EmitTiledComputation(\n-    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n-    const HloFusionInstruction* fusion,\n+    EmitterLocOpBuilder b, const HloFusionInstruction* fusion,\n     const TiledHloComputation& tiled_computation,\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, Value pid,\n@@ -1492,8 +1473,8 @@ absl::StatusOr<std::vector<TensorValue>> EmitTiledComputation(\n     }\n     TF_ASSIGN_OR_RETURN(\n         TensorValue result,\n-        EmitTiledHloInstruction(b, device_info, fusion, *tiled_hlo,\n-                                block_level_parameters, fn, pid, values));\n+        EmitTiledHloInstruction(b, fusion, *tiled_hlo, block_level_parameters,\n+                                fn, pid, values));\n     TF_RET_CHECK(values.insert({tiled_hlo, result}).second) << hlo->ToString();\n     VLOG(8) << \"Emitted \" << hlo->ToString(HloPrintOptions::ShortParsable());\n   }\n@@ -1508,8 +1489,7 @@ absl::StatusOr<std::vector<TensorValue>> EmitTiledComputation(\n // Emit sequence of instructions using compatible tiling ordered producers\n // before consumers.\n absl::StatusOr<TensorValue> EmitScope(\n-    EmitterLocOpBuilder b, const se::DeviceDescription& device_info,\n-    const TritonFusionAnalysis* analysis,\n+    EmitterLocOpBuilder b, const TritonFusionAnalysis* analysis,\n     absl::Span<const HloInstruction* const> instructions,\n     absl::flat_hash_map<const HloInstruction*, TensorValue>& values) {\n   for (const HloInstruction* hlo : instructions) {\n@@ -1538,7 +1518,7 @@ absl::StatusOr<TensorValue> EmitScope(\n         operands.push_back(values[operand]);\n       }\n       TF_ASSIGN_OR_RETURN(Value elementwise_result,\n-                          EmitElementwise(b, device_info, *hlo, operands));\n+                          EmitElementwise(b, *hlo, operands));\n       result = mlir::cast<TensorValue>(elementwise_result);\n     } else if (hlo->opcode() == HloOpcode::kTuple) {\n       TF_RET_CHECK(hlo->IsRoot()) << hlo->ToString();\n@@ -1553,9 +1533,8 @@ absl::StatusOr<TensorValue> EmitScope(\n       result = values[hlo->operand(0)];\n     } else if (hlo->opcode() == HloOpcode::kFusion) {\n       const auto* fusion_instruction = ::xla::Cast<HloFusionInstruction>(hlo);\n-      TF_ASSIGN_OR_RETURN(\n-          result,\n-          EmitNestedFusion(b, device_info, *fusion_instruction, values));\n+      TF_ASSIGN_OR_RETURN(result,\n+                          EmitNestedFusion(b, *fusion_instruction, values));\n     } else {\n       return absl::InvalidArgumentError(\n           absl::StrCat(\"Unsupported operation \", hlo->ToString()));\n@@ -1636,18 +1615,14 @@ absl::StatusOr<Tiling> TilingFromAnnotatedFusion(\n }  // namespace ir_emitter_triton_internal\n \n namespace {\n-\n using ::xla::gpu::ir_emitter_triton_internal::DumpTritonIR;\n \n-// Generate Triton IR inside 'fn', using the given block_level_parameters.\n-// TODO(b/421837868): `BlockLevelParameters` should hold all the necessary\n-// tiling information.\n-absl::Status EmitGeneric(mlir::OpBuilder builder,\n-                         const se::DeviceDescription& device_info,\n-                         const HloFusionInstruction* fusion,\n-                         xtile::EntryFuncOp fn,\n-                         const BlockLevelParameters& block_level_parameters,\n-                         SymbolicExprContext* symbolic_expr_context) {\n+absl::Status EmitGeneric(\n+    mlir::OpBuilder builder,\n+    EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder,\n+    const HloFusionInstruction* fusion, xtile::EntryFuncOp fn,\n+    const BlockLevelParameters& block_level_parameters,\n+    SymbolicExprContext* symbolic_expr_context) {\n   if (VLOG_IS_ON(6)) {\n     VLOG(6) << \"Emitting Triton IR for fusion\\n\"\n             << ExtractInstructionIntoNewModule(*fusion)->ToString();\n@@ -1656,7 +1631,8 @@ absl::Status EmitGeneric(mlir::OpBuilder builder,\n   SymbolicTileAnalysisOrError symbolic_tile_analysis_or =\n       SymbolicTileAnalysis::AnalyzeComputation(\n           *computation, symbolic_expr_context,\n-          TritonEmitterConstraints::GetBuilder(device_info));\n+          emitter_specific_constraints_builder);\n+\n   if (std::holds_alternative<FusionDecision>(symbolic_tile_analysis_or)) {\n     return Internal(\n         \"Unsupported fusion in EmitGeneric: %s\",\n@@ -1736,7 +1712,7 @@ absl::Status EmitGeneric(mlir::OpBuilder builder,\n   absl::flat_hash_map<const TiledHloInstruction*, TensorValue> values;\n   TF_ASSIGN_OR_RETURN(\n       auto results,\n-      EmitTiledComputation(b, device_info, fusion, tiled_hlo_computation,\n+      EmitTiledComputation(b, fusion, tiled_hlo_computation,\n                            block_level_parameters, fn, tile_id, values));\n \n   for (auto [root, result, arg] :\n@@ -1879,10 +1855,13 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateTritonModule(\n \n   // TODO: b/451959933 - Use reference or check pointer.\n   mlir::MLIRContext& mlir_context = *symbolic_expr_context.GetMLIRContext();\n-  TF_ASSIGN_OR_RETURN(auto triton_module,\n-                      ir_emitter_triton_internal::EmitXTileModule(\n-                          fn_name, fusion, device_info, block_level_parameters,\n-                          symbolic_expr_context));\n+\n+  TF_ASSIGN_OR_RETURN(\n+      auto triton_module,\n+      ir_emitter_triton_internal::EmitXTileModule(\n+          fn_name, TritonEmitterConstraints::GetBuilder(device_info), fusion,\n+          block_level_parameters, symbolic_expr_context,\n+          ir_emitter_triton_internal::LegacyMatmulEmitter(device_info)));\n \n   const HloComputation* hlo_computation =\n       fusion->fused_instructions_computation();\n@@ -2165,14 +2144,27 @@ std::string GetLibdevicePath(const HloModuleConfig& hlo_config,\n \n namespace ir_emitter_triton_internal {\n \n+absl::Status LegacyMatmulEmitter::Emit(\n+    EmitterLocOpBuilder& b, const HloFusionInstruction* fusion,\n+    xtile::EntryFuncOp& fn,\n+    const BlockLevelParameters& block_level_parameters) {\n+  std::string libdevice_path =\n+      GetLibdevicePath(fusion->GetModule()->config(), device_info_);\n+  TF_RETURN_IF_ERROR(EmitMatMul(b, libdevice_path, device_info_, fusion, fn,\n+                                block_level_parameters));\n+  return absl::OkStatus();\n+}\n+\n // TODO(b/447133106): Contrary to the name, this function still does a lot of\n // triton specific things. It should be migrated to use non-triton specific\n // utilities.\n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n-    absl::string_view fn_name, const HloFusionInstruction* fusion,\n-    const se::DeviceDescription& device_info,\n+    absl::string_view fn_name,\n+    EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder,\n+    const HloFusionInstruction* fusion,\n     const BlockLevelParameters& block_level_parameters,\n-    SymbolicExprContext& symbolic_expr_context) {\n+    SymbolicExprContext& symbolic_expr_context,\n+    std::optional<LegacyMatmulEmitter> legacy_matmul_emitter) {\n   mlir::MLIRContext& mlir_context = *symbolic_expr_context.GetMLIRContext();\n   LoadMlirDialectsForTriton(mlir_context);\n   const auto debug_options = fusion->GetModule()->config().debug_options();\n@@ -2228,15 +2220,15 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n             DebugOptions::GENERIC_TRITON_EMITTER_DISABLE_LEGACY_GEMM)) {\n       return Internal(\"Legacy GEMM emitter is disabled.\");\n     }\n-    std::string libdevice_path =\n-        GetLibdevicePath(fusion->GetModule()->config(), device_info);\n-    TF_RETURN_IF_ERROR(EmitMatMul(b, libdevice_path, device_info, fusion, fn,\n-                                  block_level_parameters));\n+    CHECK(legacy_matmul_emitter.has_value())\n+        << \"emit_legacy_matmul_fn is not set\";\n+    TF_RETURN_IF_ERROR(\n+        legacy_matmul_emitter->Emit(b, fusion, fn, block_level_parameters));\n   } else if (fusion_kind == kTritonFusionKind ||\n              fusion_kind == kTritonNestedGemmFusionKind ||\n              fusion_kind == kTritonScaledDotFusionKind) {\n-    TF_RETURN_IF_ERROR(EmitGeneric(b, device_info, fusion, fn,\n-                                   block_level_parameters,\n+    TF_RETURN_IF_ERROR(EmitGeneric(b, emitter_specific_constraints_builder,\n+                                   fusion, fn, block_level_parameters,\n                                    &symbolic_expr_context));\n   } else {\n     return Internal(\"Unsupported fusion kind: %s\", fusion_kind);"
        },
        {
            "sha": "8b66e41ce7ff43b94faf103407c812f67b7ec218",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.h",
            "status": "modified",
            "additions": 21,
            "deletions": 3,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea8af7f1535632115bfd3860f9d05e07fda10e4c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea8af7f1535632115bfd3860f9d05e07fda10e4c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.h?ref=ea8af7f1535632115bfd3860f9d05e07fda10e4c",
            "patch": "@@ -35,6 +35,7 @@ limitations under the License.\n #include \"xla/autotuning.pb.h\"\n #include \"xla/codegen/emitter_loc_op_builder.h\"\n #include \"xla/codegen/tiling/symbolic_tile_analysis.h\"\n+#include \"xla/codegen/xtile/ir/xtile_ops.h\"\n #include \"xla/hlo/analysis/symbolic_expr.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -145,16 +146,33 @@ absl::StatusOr<Tiling> TilingFromAnnotatedFusion(\n     const SymbolicTileAnalysis& symbolic_tile_analysis,\n     const BlockLevelParameters& block_level_parameters);\n \n+// TODO(basioli): Remove this class once the legacy matmul\n+// emitter no longer exists.\n+class LegacyMatmulEmitter {\n+ public:\n+  explicit LegacyMatmulEmitter(const se::DeviceDescription& device_info)\n+      : device_info_(device_info) {}\n+\n+  absl::Status Emit(EmitterLocOpBuilder& b, const HloFusionInstruction* fusion,\n+                    xtile::EntryFuncOp& fn,\n+                    const BlockLevelParameters& block_level_parameters);\n+\n+ private:\n+  const se::DeviceDescription& device_info_;\n+};\n+\n // This function (or its future equivalent) should emit the MLIR module in the\n // shared dialect between XLA:CPU and XLA:GPU. At the moment it is still\n // emitting GPU specific modules. It is currently exposed only for testing\n // purposes and will only be used to make sure we are properly emitting the\n // shared dialect.\n absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> EmitXTileModule(\n-    absl::string_view fn_name, const HloFusionInstruction* fusion,\n-    const se::DeviceDescription& device_info,\n+    absl::string_view fn_name,\n+    EmitterSpecificConstraintsBuilder emitter_specific_constraints_builder,\n+    const HloFusionInstruction* fusion,\n     const BlockLevelParameters& block_level_parameters,\n-    SymbolicExprContext& symbolic_expr_context);\n+    SymbolicExprContext& symbolic_expr_context,\n+    std::optional<LegacyMatmulEmitter> legacy_matmul_emitter = std::nullopt);\n \n // This function lowers the shared dialect module to Triton. It is exposed for\n // testing with the same motivation as EmitXTileModule."
        },
        {
            "sha": "605597bda175fd78f3cdb68bafad95f3ab1b5488",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/test_utils.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ea8af7f1535632115bfd3860f9d05e07fda10e4c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ea8af7f1535632115bfd3860f9d05e07fda10e4c/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc?ref=ea8af7f1535632115bfd3860f9d05e07fda10e4c",
            "patch": "@@ -53,6 +53,7 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_float_support.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n+#include \"xla/service/gpu/model/triton_emitter_constraints.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -169,8 +170,12 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> CreateXTileIrAndFileCheck(\n   TF_ASSIGN_OR_RETURN(\n       mlir::OwningOpRef<mlir::ModuleOp> xtile_dialect_module,\n       ir_emitter_triton_internal::EmitXTileModule(\n-          \"xtile_dialect_fn\", fusion, TestGpuDeviceInfo::RTXA6000DeviceInfo(),\n-          block_level_parameters, *test->symbolic_expr_context()));\n+          \"xtile_dialect_fn\",\n+          TritonEmitterConstraints::GetBuilder(\n+              TestGpuDeviceInfo::RTXA6000DeviceInfo()),\n+          fusion, block_level_parameters, *test->symbolic_expr_context(),\n+          ir_emitter_triton_internal::LegacyMatmulEmitter(\n+              TestGpuDeviceInfo::RTXA6000DeviceInfo())));\n \n   std::string out;\n   llvm::raw_string_ostream os(out);"
        }
    ],
    "stats": {
        "total": 209,
        "additions": 113,
        "deletions": 96
    }
}