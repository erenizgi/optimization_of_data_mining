{
    "author": "tensorflower-gardener",
    "message": "[XLA] Enhance `set_xla_metadata` to further support tagging gradient operations.\n\nPiperOrigin-RevId: 831469459",
    "sha": "27a2fe5c4650e965fae6d1360a8c14cec463a177",
    "files": [
        {
            "sha": "c2c11c1f49becf09996ced681c58a75af022b736",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 231,
            "deletions": 0,
            "changes": 231,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/27a2fe5c4650e965fae6d1360a8c14cec463a177/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/27a2fe5c4650e965fae6d1360a8c14cec463a177/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=27a2fe5c4650e965fae6d1360a8c14cec463a177",
            "patch": "@@ -584,6 +584,51 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.ml\n +  return %0, %1, %2 : tensor<1xf32>, tensor<2x2xi32>, tensor<3x2xcomplex<f32>>\n  }\n  \n+ // -----\n+diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n+--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n++++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir\n+@@ -27,6 +27,14 @@\n+   // CHECK-NOT: stablehlo.constant\n+   // CHECK: return %arg0\n+   return %1 : tensor<f32>\n++}\n++\n++// CHECK-LABEL: @add_cst_on_rhs_with_attrs\n++func.func @add_cst_on_rhs_with_attrs(%arg0: tensor<f32>) -> tensor<f32> {\n++  %cst = stablehlo.constant dense<1.0> : tensor<f32>\n++  // CHECK: stablehlo.add %arg0, %cst {mhlo.frontend_attributes = {foo = \"1\"}} : tensor<f32>\n++  %0 = stablehlo.add %cst, %arg0 {mhlo.frontend_attributes = {foo = \"1\"}} : tensor<f32>\n++  return %0 : tensor<f32>\n+ }\n+ \n+ // -----\n+@@ -976,6 +984,26 @@\n+   // CHECK-NOT: stablehlo.constant\n+   // CHECK: return %arg0 : tensor<f32>\n+   return %0 : tensor<f32>\n++}\n++\n++// CHECK-LABEL: @multiply_by_one_merge_attrs\n++func.func @multiply_by_one_merge_attrs(%arg0: tensor<f32>) -> tensor<f32> {\n++  %cst = stablehlo.constant dense<1.0> : tensor<f32>\n++  %0 = stablehlo.add %arg0, %arg0 {mhlo.frontend_attributes = {bar = \"1\"}} : tensor<f32>\n++  %1 = stablehlo.multiply %0, %cst {mhlo.frontend_attributes = {foo = \"1\"}} : tensor<f32>\n++  // CHECK: %[[ADD:.*]] = stablehlo.add %arg0, %arg0 {mhlo.frontend_attributes = {bar = \"1\", foo = \"1\"}} : tensor<f32>\n++  // CHECK: return %[[ADD]] : tensor<f32>\n++  return %1 : tensor<f32>\n++}\n++\n++// CHECK-LABEL: @multiply_by_one_merge_attrs_conflict\n++func.func @multiply_by_one_merge_attrs_conflict(%arg0: tensor<f32>) -> tensor<f32> {\n++  %cst = stablehlo.constant dense<1.0> : tensor<f32>\n++  %0 = stablehlo.add %arg0, %arg0 {mhlo.frontend_attributes = {bar = \"1\", foo = \"0\"}} : tensor<f32>\n++  %1 = stablehlo.multiply %0, %cst {mhlo.frontend_attributes = {foo = \"1\"}} : tensor<f32>\n++  // CHECK: %[[ADD:.*]] = stablehlo.add %arg0, %arg0 {mhlo.frontend_attributes = {bar = \"1\", foo = \"1\"}} : tensor<f32>\n++  // CHECK: return %[[ADD]] : tensor<f32>\n++  return %1 : tensor<f32>\n+ }\n+ \n  // -----\n diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir\n --- stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir\n@@ -1074,4 +1119,190 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold\n    patterns->add<FoldConvertOpPattern>(context, options, benefit);\n    patterns->add<FoldDivOpPattern>(context, options, benefit);\n    patterns->add<FoldDynamicSliceOpPattern>(context, options, benefit);\n+diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n+--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n++++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp\n+@@ -69,6 +69,54 @@\n+   });\n+ }\n+ \n++bool mergeDiscardableAttributes(ValueRange sourceValues,\n++                                ValueRange destValues) {\n++  if (sourceValues.size() != destValues.size()) return false;\n++  bool changed = false;\n++  for (auto [source, dest] : llvm::zip(sourceValues, destValues)) {\n++    if (mergeDiscardableAttributes(source, dest)) changed = true;\n++  }\n++  return changed;\n++}\n++\n++bool mergeDiscardableAttributes(Value sourceValue, Value destValue) {\n++  Operation* sourceOp = sourceValue.getDefiningOp();\n++  Operation* destOp = destValue.getDefiningOp();\n++  if (!sourceOp || !destOp) return false;\n++\n++  auto sourceAttrs = sourceOp->getDiscardableAttrDictionary();\n++  if (!sourceAttrs) return true;\n++\n++  auto destAttrs = destOp->getDiscardableAttrDictionary();\n++  if (!destAttrs) {\n++    destOp->setDiscardableAttrs(sourceAttrs);\n++    return true;\n++  }\n++\n++  NamedAttrList mergedAttrs(destAttrs);\n++  for (auto attr : sourceAttrs.getValue()) {\n++    if (attr.getName() == \"mhlo.frontend_attributes\" &&\n++        mergedAttrs.get(\"mhlo.frontend_attributes\")) {\n++      // Merge frontend attributes, prioritizing source attributes.\n++      auto destFrontendAttrs =\n++          cast<DictionaryAttr>(mergedAttrs.get(\"mhlo.frontend_attributes\"));\n++      auto sourceFrontendAttrs = cast<DictionaryAttr>(attr.getValue());\n++      NamedAttrList frontendAttrs(destFrontendAttrs);\n++      for (auto sourceAttr : sourceFrontendAttrs) {\n++        frontendAttrs.set(sourceAttr.getName(), sourceAttr.getValue());\n++      }\n++      mergedAttrs.set(\"mhlo.frontend_attributes\",\n++                      frontendAttrs.getDictionary(destOp->getContext()));\n++    } else {\n++      // Otherwise prioritize source attributes\n++      mergedAttrs.set(attr.getName(), attr.getValue());\n++    }\n++  }\n++\n++  destOp->setDiscardableAttrs(mergedAttrs.getDictionary(destOp->getContext()));\n++  return true;\n++}\n++\n+ template <typename OpType>\n+ struct SimplifyOpRewritePattern : OpRewritePattern<OpType> {\n+   SimplifyOpRewritePattern(\n+diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n+--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n++++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td\n+@@ -134,6 +134,8 @@\n+ \n+ def MergePermutations : NativeCodeCall<\"getMergedTransposePermutation($_builder, $0, $1)\">;\n+ \n++def MergeDiscardableAttributes : NativeCodeCall<\"mergeDiscardableAttributes($0, $1)\">;\n++\n+ def StableHLO_ConvertOpWithShape : NativeCodeCall<\n+     \"stablehlo::ConvertOp::create($_builder, $_loc, $0.getType(), $1)\">;\n+ \n+@@ -149,8 +151,9 @@\n+ // op(cst, X) -> op(X, cst)\n+ class CanonicalizeConstantToRhs<Op StableHLO_OpType>\n+   : Pat<(StableHLO_OpType:$op (StableHLO_ConstantOp:$lhs $value), $rhs),\n+-        (StableHLO_OpType $rhs, $lhs),\n+-        [(NotConstantOp $rhs), (CommutativeOp $op)]>;\n++        (StableHLO_OpType:$new_op $rhs, $lhs),\n++        [(NotConstantOp $rhs), (CommutativeOp $op)],\n++        [(MergeDiscardableAttributes $op, $new_op)]>;\n+ \n+ ////////\n+ // AddOp\n+@@ -161,8 +164,9 @@\n+ \n+ // Pattern: add(X, 0) -> X\n+ def AddOp_RemoveNoop\n+-  : Pat<(StableHLO_AddOp $lhs, (ConstantLikeMatcher AnyZero:$value)),\n+-        (replaceWithValue $lhs)>;\n++  : Pat<(StableHLO_AddOp:$op $lhs, (ConstantLikeMatcher AnyZero:$value)),\n++        (replaceWithValue $lhs), [],\n++        [(MergeDiscardableAttributes $op, $lhs)]>;\n+ \n+ ////////\n+ // AndOp\n+@@ -173,13 +177,15 @@\n+ \n+ // Pattern: and(X, 0) -> 0\n+ def AndOp_FoldToZero\n+-  : Pat<(StableHLO_AndOp $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n+-        (replaceWithValue $zero)>;\n++  : Pat<(StableHLO_AndOp:$op $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n++        (replaceWithValue $zero), [],\n++        [(MergeDiscardableAttributes $op, $zero)]>;\n+ \n+ // Pattern: and(X, 1) -> X\n+ def AndOp_RemoveNoop\n+-  : Pat<(StableHLO_AndOp $lhs, (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n+-        (replaceWithValue $lhs)>;\n++  : Pat<(StableHLO_AndOp:$op $lhs, (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n++        (replaceWithValue $lhs), [],\n++        [(MergeDiscardableAttributes $op, $lhs)]>;\n+ \n+ ////////\n+ // BroadcastInDimOp\n+@@ -188,7 +194,8 @@\n+ def BroadcastInDimOp_RemoveNoop\n+   : Pat<(StableHLO_BroadcastInDimOp:$op $operand, IotaDims:$dims),\n+         (replaceWithValue $operand),\n+-        [(TypesEqual $op, $operand)]>;\n++        [(TypesEqual $op, $operand)],\n++        [(MergeDiscardableAttributes $op, $operand)]>;\n+ \n+ // Pattern: broadcast_in_dim(broadcast_in_dim(X, [dimsA...]), [dimsB...])\n+ //       -> broadcast_in_dim(X, merge(dimsA, dimsB))\n+@@ -254,7 +261,8 @@\n+ def ConvertOp_RemoveNoop\n+   : Pat<(StableHLO_ConvertOp:$convert $operand),\n+         (replaceWithValue $operand),\n+-        [(TypesEqual $convert, $operand)]>;\n++        [(TypesEqual $convert, $operand)],\n++        [(MergeDiscardableAttributes $convert, $operand)]>;\n+ \n+ ////////\n+ // DynamicBroadcastInDimOp\n+@@ -441,13 +449,15 @@\n+ // Multiplication by 0. This fold is not trivial for floats in presence of NaNs,\n+ // so we currently only enable it for ints.\n+ def MulOp_FoldToZero\n+-  : Pat<(StableHLO_MulOp $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n+-        (replaceWithValue $zero)>;\n++  : Pat<(StableHLO_MulOp:$mul_op $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n++        (replaceWithValue $zero), [],\n++        [(MergeDiscardableAttributes $mul_op, $zero)]>;\n+ \n+ // Pattern: multiply(X, 1i) -> X\n+ def MulOp_RemoveNoop\n+-  : Pat<(StableHLO_MulOp $lhs, (StableHLO_ConstantOp AnyOne:$value)),\n+-        (replaceWithValue $lhs)>;\n++  : Pat<(StableHLO_MulOp:$mul_op $lhs, (StableHLO_ConstantOp AnyOne:$value)),\n++        (replaceWithValue $lhs), [],\n++        [(MergeDiscardableAttributes $mul_op, $lhs)]>;\n+ \n+ ////////\n+ // OrOp\n+@@ -457,13 +467,15 @@\n+ \n+ // Pattern: or(X, 1) -> 1\n+ def OrOp_FoldToOne\n+-  : Pat<(StableHLO_OrOp $lhs, (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n+-        (replaceWithValue $one)>;\n++  : Pat<(StableHLO_OrOp:$op $lhs, (StableHLO_ConstantOp:$one IntAllOnes:$value)),\n++        (replaceWithValue $one), [],\n++        [(MergeDiscardableAttributes $op, $one)]>;\n+ \n+ // Pattern: or(X, 0) -> X\n+ def OrOp_RemoveNoop\n+-  : Pat<(StableHLO_OrOp $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n+-        (replaceWithValue $lhs)>;\n++  : Pat<(StableHLO_OrOp:$op $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),\n++        (replaceWithValue $lhs), [],\n++        [(MergeDiscardableAttributes $op, $lhs)]>;\n+ \n+ ////////\n+ // PadOp\n+@@ -564,8 +576,9 @@\n+ \n+ // Pattern: subtract(X, 0) -> X\n+ def SubtractOp_RemoveNoop\n+-  : Pat<(StableHLO_SubtractOp $lhs, (StableHLO_ConstantOp AnyZero:$value)),\n+-        (replaceWithValue $lhs)>;\n++  : Pat<(StableHLO_SubtractOp:$op $lhs, (StableHLO_ConstantOp AnyZero:$value)),\n++        (replaceWithValue $lhs), [],\n++        [(MergeDiscardableAttributes $op, $lhs)]>;\n+ \n+ ////////\n+ // SliceOp\n "
        }
    ],
    "stats": {
        "total": 231,
        "additions": 231,
        "deletions": 0
    }
}