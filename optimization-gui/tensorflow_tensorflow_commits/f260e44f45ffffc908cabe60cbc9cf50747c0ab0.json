{
    "author": "thomasjoerg",
    "message": "[XLA:GPU] Do not drop op metadata in DotStrengthReduction.\n\nPiperOrigin-RevId: 839237849",
    "sha": "f260e44f45ffffc908cabe60cbc9cf50747c0ab0",
    "files": [
        {
            "sha": "1eaef74af3aee105e44e20951414e099af693e20",
            "filename": "third_party/xla/xla/service/gpu/transforms/dot_strength_reduction.cc",
            "status": "modified",
            "additions": 28,
            "deletions": 18,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f260e44f45ffffc908cabe60cbc9cf50747c0ab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdot_strength_reduction.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f260e44f45ffffc908cabe60cbc9cf50747c0ab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdot_strength_reduction.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdot_strength_reduction.cc?ref=f260e44f45ffffc908cabe60cbc9cf50747c0ab0",
            "patch": "@@ -45,20 +45,22 @@ namespace gpu {\n \n namespace {\n \n-HloInstruction* ConvertTo(HloInstruction* instruction, PrimitiveType type) {\n+HloInstruction* ConvertTo(HloInstruction* instruction, PrimitiveType type,\n+                          const OpMetadata* metadata) {\n   if (instruction->shape().element_type() == type) {\n     return instruction;\n   }\n   Shape new_shape = instruction->shape();\n   new_shape.set_element_type(type);\n   return instruction->parent()->AddInstruction(\n-      HloInstruction::CreateConvert(new_shape, instruction));\n+      HloInstruction::CreateConvert(new_shape, instruction), metadata);\n }\n \n // Transposes the dot operand to make dimensions in \"batch, non-contracting,\n // contracting\" order, sorted by index within each category.\n HloInstruction* PermuteDotOperandDimensions(HloInstruction* operand,\n-                                            DotOperandDims* dims) {\n+                                            DotOperandDims* dims,\n+                                            const OpMetadata* metadata) {\n   std::vector<int64_t> permutation;\n   for (auto kind : {DotOperandDims::kBatch, DotOperandDims::kNonContracting,\n                     DotOperandDims::kContracting}) {\n@@ -71,14 +73,16 @@ HloInstruction* PermuteDotOperandDimensions(HloInstruction* operand,\n   }\n   Shape new_shape = ShapeUtil::PermuteDimensions(permutation, operand->shape());\n   operand = operand->parent()->AddInstruction(\n-      HloInstruction::CreateTranspose(new_shape, operand, permutation));\n+      HloInstruction::CreateTranspose(new_shape, operand, permutation),\n+      metadata);\n   dims->Permute(permutation);\n   return operand;\n }\n \n-HloInstruction* BroadcastDimensions(\n-    HloInstruction* operand, int64_t insert_before,\n-    absl::Span<const int64_t> bounds_to_insert) {\n+HloInstruction* BroadcastDimensions(HloInstruction* operand,\n+                                    int64_t insert_before,\n+                                    absl::Span<const int64_t> bounds_to_insert,\n+                                    const OpMetadata* metadata) {\n   if (bounds_to_insert.empty()) {\n     return operand;\n   }\n@@ -93,8 +97,9 @@ HloInstruction* BroadcastDimensions(\n \n   Shape new_shape = ShapeUtil::InsertDimensionsAtIndex(\n       operand->shape(), insert_before, bounds_to_insert);\n-  return operand->parent()->AddInstruction(HloInstruction::CreateBroadcast(\n-      new_shape, operand, broadcast_dimensions));\n+  return operand->parent()->AddInstruction(\n+      HloInstruction::CreateBroadcast(new_shape, operand, broadcast_dimensions),\n+      metadata);\n }\n \n HloComputation* CreateScalarAddComputation(HloModule* module,\n@@ -113,7 +118,8 @@ HloComputation* CreateScalarAddComputation(HloModule* module,\n // Reduces the last `num_dims_to_reduce` dimensions of `instruction`.\n HloInstruction* ReduceDimensions(HloInstruction* instruction,\n                                  size_t num_dims_to_reduce,\n-                                 PrimitiveType accumulator_type) {\n+                                 PrimitiveType accumulator_type,\n+                                 const OpMetadata* metadata) {\n   if (num_dims_to_reduce == 0) {\n     return instruction;\n   }\n@@ -134,16 +140,19 @@ HloInstruction* ReduceDimensions(HloInstruction* instruction,\n   HloComputation* add_computation =\n       CreateScalarAddComputation(computation->parent(), accumulator_type);\n \n-  return computation->AddInstruction(HloInstruction::CreateReduce(\n-      reduce_shape, ConvertTo(instruction, accumulator_type), zero, reduce_dims,\n-      add_computation));\n+  return computation->AddInstruction(\n+      HloInstruction::CreateReduce(\n+          reduce_shape, ConvertTo(instruction, accumulator_type, metadata),\n+          zero, reduce_dims, add_computation),\n+      metadata);\n }\n \n }  // namespace\n \n absl::StatusOr<HloInstruction*> DotStrengthReduction::ExpandInstruction(\n     HloInstruction* instruction) {\n   HloDotInstruction* dot = Cast<HloDotInstruction>(instruction);\n+  const OpMetadata* metadata = &dot->metadata();\n   TF_ASSIGN_OR_RETURN(auto dot_dims, DotOperandDims::FromDot(dot));\n \n   std::array<HloInstruction*, 2> operands = {dot->mutable_operand(0),\n@@ -152,9 +161,9 @@ absl::StatusOr<HloInstruction*> DotStrengthReduction::ExpandInstruction(\n     DotOperandDims& our_dims = dot_dims[i];\n     DotOperandDims& other_dims = dot_dims[1 - i];\n     // Convert operands to the dot resulting type.\n-    operands[i] = ConvertTo(operands[i], dot->shape().element_type());\n+    operands[i] = ConvertTo(operands[i], dot->shape().element_type(), metadata);\n     // Ensure dimensions are in \"batch, non-contracting, contracting\" order.\n-    operands[i] = PermuteDotOperandDimensions(operands[i], &our_dims);\n+    operands[i] = PermuteDotOperandDimensions(operands[i], &our_dims, metadata);\n \n     // Both lhs and rhs will have [batch, lhs non-contracting, rhs\n     // non-contracting, contracting] dimensions.\n@@ -167,7 +176,7 @@ absl::StatusOr<HloInstruction*> DotStrengthReduction::ExpandInstruction(\n \n     operands[i] = BroadcastDimensions(\n         operands[i], insert_before,\n-        other_dims.DimensionSizes(DotOperandDims::kNonContracting));\n+        other_dims.DimensionSizes(DotOperandDims::kNonContracting), metadata);\n   }\n \n   // At this point, both operands have the same shape. Elementwise multiply.\n@@ -176,16 +185,17 @@ absl::StatusOr<HloInstruction*> DotStrengthReduction::ExpandInstruction(\n       HloInstruction * flow,\n       MakeMultiplyForDotPrecisionAlgorithm(\n           operands[0], operands[1], dot->precision_config().algorithm()));\n+  flow->set_metadata(*metadata);\n \n   // If there were any contracting dims, we need to reduce them.\n   flow = ReduceDimensions(\n       flow, dot_dims[0].DimensionCount(DotOperandDims::kContracting),\n-      GetGemmAccumulatorType(dot));\n+      GetGemmAccumulatorType(dot), metadata);\n \n   // If the output type is different from what it was before (either because\n   // reduction used a different accumulator type, or because types of operand\n   // differed the output type for multiply), convert to the output type.\n-  flow = ConvertTo(flow, dot->shape().element_type());\n+  flow = ConvertTo(flow, dot->shape().element_type(), metadata);\n   return flow;\n }\n "
        },
        {
            "sha": "6d99954b1c20a3cadf9352337382977f8558cafb",
            "filename": "third_party/xla/xla/service/gpu/transforms/dot_strength_reduction_test.cc",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f260e44f45ffffc908cabe60cbc9cf50747c0ab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdot_strength_reduction_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f260e44f45ffffc908cabe60cbc9cf50747c0ab0/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdot_strength_reduction_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fdot_strength_reduction_test.cc?ref=f260e44f45ffffc908cabe60cbc9cf50747c0ab0",
            "patch": "@@ -98,6 +98,38 @@ ENTRY test {\n   CHECK_OK(module->Verify());\n }\n \n+TEST_F(DotStrengthReductionTest, MaintainsMetadata) {\n+  const char* hlo_text = R\"(\n+HloModule test\n+\n+ENTRY test {\n+  p0 = bf16[6144]{0} parameter(0)\n+  p1 = bf16[6144,256]{1,0} parameter(1)\n+  ROOT dot = bf16[256]{0} dot(p0, p1), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name=\"test\"}\n+}\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_text));\n+  DotStrengthReduction pass{\n+      se::GpuComputeCapability(se::CudaComputeCapability::Hopper())};\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, this->RunHloPass(&pass, module.get()));\n+  EXPECT_TRUE(changed);\n+  CHECK_OK(module->Verify());\n+\n+  const char* filecheck_pattern = R\"(\n+// CHECK: bf16[256,6144]{1,0} multiply{{.*}}, metadata={op_name=\"test\"}\n+// CHECK: f32[256,6144]{1,0} convert{{.*}}, metadata={op_name=\"test\"}\n+// CHECK: f32[256]{0} reduce{{.*}}, metadata={op_name=\"test\"}\n+// CHECK: bf16[256]{0} convert{{.*}}, metadata={op_name=\"test\"}\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(bool filecheck_result,\n+                          RunFileCheck(module->ToString(), filecheck_pattern));\n+  EXPECT_TRUE(filecheck_result);\n+  CHECK_OK(module->Verify());\n+}\n+\n TEST_F(DotStrengthReductionTest, UpcastInReductionF8E4M3FN) {\n   const char* hlo_text = R\"(\n HloModule test"
        }
    ],
    "stats": {
        "total": 78,
        "additions": 60,
        "deletions": 18
    }
}