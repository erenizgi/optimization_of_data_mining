{
    "author": "tensorflower-gardener",
    "message": "[XLA:GPU] Add simple reduce check to multimem tests\n\nPiperOrigin-RevId: 825413069",
    "sha": "364a0ac855e808d20919387c6b466e13c13f309b",
    "files": [
        {
            "sha": "a0357f376326968a9f2e8155495dd4b034692658",
            "filename": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "status": "modified",
            "additions": 19,
            "deletions": 1,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/364a0ac855e808d20919387c6b466e13c13f309b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/364a0ac855e808d20919387c6b466e13c13f309b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD?ref=364a0ac855e808d20919387c6b466e13c13f309b",
            "patch": "@@ -1255,6 +1255,21 @@ xla_test(\n     ],\n )\n \n+cuda_library(\n+    name = \"cuda_executor_multigpu_test_kernels\",\n+    srcs = [\"cuda_executor_multigpu_test_kernels.cu.cc\"],\n+    hdrs = [\"cuda_executor_multigpu_test_kernels.h\"],\n+    tags = [\"cuda-only\"],\n+    deps = [\n+        \":cuda_status\",\n+        \"//xla/tsl/platform:errors\",\n+        \"@com_google_absl//absl/status\",\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+        \"@local_config_cuda//cuda:cuda_runtime\",\n+        \"@local_tsl//tsl/platform:logging\",\n+    ],\n+)\n+\n xla_test(\n     name = \"cuda_executor_multigpu_test\",\n     srcs = [\"cuda_executor_multigpu_test.cc\"],\n@@ -1268,15 +1283,18 @@ xla_test(\n     tags = [\"cuda-only\"],\n     deps = [\n         \":cuda_executor\",\n+        \":cuda_executor_multigpu_test_kernels\",\n         \":cuda_platform\",\n         \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:platform_manager\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/gpu:gpu_init\",\n+        \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n )\n@@ -1313,7 +1331,7 @@ cc_library(\n # OSX framework for device driver access\n cc_library(\n     name = \"IOKit\",\n-    linkopts = [\"-framework IOKit\"],\n+    linkopts = [\"-frameworFk IOKit\"],\n )\n \n cc_library("
        },
        {
            "sha": "c789738319ace0187f15b5aa84c1acd2bec7baa1",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/364a0ac855e808d20919387c6b466e13c13f309b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/364a0ac855e808d20919387c6b466e13c13f309b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc?ref=364a0ac855e808d20919387c6b466e13c13f309b",
            "patch": "@@ -832,6 +832,10 @@ absl::StatusOr<void*> CudaExecutor::VmmAllocateMemory(uint64_t bytes) {\n       cuMemAddressReserve(&ptr, padded_size, granularity, 0, 0)));\n   TF_RETURN_IF_ERROR(cuda::ToStatus(cuMemMap(ptr, padded_size, 0, handle, 0)));\n \n+  VLOG(3) << \"[\" << device_ordinal() << \"] VMM allocated \" << ptr\n+          << \" requested size: \" << bytes << \" padded size: \" << padded_size\n+          << \" granularity: \" << granularity;\n+\n   int device_count = 0;\n   TF_RETURN_IF_ERROR(cuda::ToStatus(cudaGetDeviceCount(&device_count)));\n   for (int peer = 0; peer < device_count; peer++) {\n@@ -1861,12 +1865,13 @@ absl::Status CudaExecutor::CudaMulticastMemory::Initialize(\n   TF_ASSIGN_OR_RETURN(CUmulticastObjectProp multicast_properties,\n                       CreateMulticastObjectProperties(num_devices_, size));\n \n+  TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(\n+      cuMulticastCreate(&handle_, &multicast_properties)));\n   VLOG(3) << \"[\" << static_cast<int>(cuda_executor->device_)\n-          << \"] Create multicast memory: \" << static_cast<uint64_t>(handle_)\n+          << \"] Created multicast memory: \" << static_cast<uint64_t>(handle_)\n           << \" size: \" << padded_size_ << \" with granularity: \" << granularity_\n           << \" for \" << num_devices_ << \" devices.\";\n-  return stream_executor::cuda::ToStatus(\n-      cuMulticastCreate(&handle_, &multicast_properties));\n+  return absl::OkStatus();\n }\n \n absl::Status CudaExecutor::CudaMulticastMemory::SubscribeDevice("
        },
        {
            "sha": "c00ff8aeb5139dad8dd6e8a4aa43dffefb75fc13",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor_multigpu_test.cc",
            "status": "modified",
            "additions": 61,
            "deletions": 12,
            "changes": 73,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/364a0ac855e808d20919387c6b466e13c13f309b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/364a0ac855e808d20919387c6b466e13c13f309b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test.cc?ref=364a0ac855e808d20919387c6b466e13c13f309b",
            "patch": "@@ -13,6 +13,7 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include <cstddef>\n #include <cstdint>\n #include <memory>\n #include <vector>\n@@ -21,12 +22,15 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/status/status.h\"\n #include \"absl/status/status_matchers.h\"\n+#include \"absl/status/statusor.h\"\n #include \"xla/stream_executor/cuda/cuda_executor.h\"\n+#include \"xla/stream_executor/cuda/cuda_executor_multigpu_test_kernels.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/gpu/gpu_init.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/platform_manager.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n \n namespace stream_executor::gpu {\n@@ -36,6 +40,37 @@ using ::absl_testing::IsOkAndHolds;\n using ::absl_testing::StatusIs;\n using ::testing::NotNull;\n \n+template <typename T>\n+absl::StatusOr<stream_executor::DeviceMemoryBase> AllocateInitializedMemory(\n+    CudaExecutor* executor, size_t size, T value) {\n+  size_t num_elements = size / sizeof(T);\n+  stream_executor::DeviceMemoryBase device_memory = executor->Allocate(\n+      size, static_cast<int64_t>(stream_executor::MemoryType::kP2P));\n+  if (device_memory.opaque() == nullptr) {\n+    return absl::InternalError(\"Failed to allocate memory.\");\n+  }\n+  std::vector<T> device_memory_vector(num_elements, value);\n+\n+  TF_RETURN_IF_ERROR(executor->SynchronousMemcpy(\n+      &device_memory, device_memory_vector.data(), size));\n+  return device_memory;\n+}\n+\n+template <typename T>\n+absl::Status CheckMemory(CudaExecutor* executor,\n+                         stream_executor::DeviceMemoryBase device_memory,\n+                         T expected_value) {\n+  size_t num_elements = device_memory.size() / sizeof(T);\n+  std::vector<T> device_memory_vector(num_elements, 0);\n+  TF_RETURN_IF_ERROR(executor->SynchronousMemcpy(\n+      device_memory_vector.data(), device_memory, device_memory.size()));\n+  for (int i = 0; i < device_memory_vector.size(); ++i) {\n+    EXPECT_EQ(device_memory_vector[i], expected_value);\n+  }\n+\n+  return absl::OkStatus();\n+}\n+\n StreamExecutor* GetGpuExecutor(int64_t device_ordinal) {\n   auto* platform =\n       PlatformManager::PlatformWithName(stream_executor::GpuPlatformName())\n@@ -109,7 +144,7 @@ TEST(CudaExecutorMultiGpuTest, CudaMulticastMemoryUsingNonVmmMemory) {\n   EXPECT_THAT(multicast_memory->SubscribeDevice(0), IsOk());\n   EXPECT_THAT(multicast_memory->SubscribeDevice(1), IsOk());\n \n-  DeviceMemoryBase device_memory = executors[0]->Allocate(1, 0);\n+  DeviceMemoryBase device_memory = executors[0]->Allocate(8, 0);\n   EXPECT_THAT(\n       multicast_memory->MapMemory(device_memory.opaque(), executors[0]),\n       StatusIs(absl::StatusCode::kInternal,\n@@ -123,27 +158,41 @@ TEST(CudaExecutorMultiGpuTest, CudaMulticastMemoryUsingVmmMemory) {\n   if (!executors[0]->is_multicast_supported()) {\n     GTEST_SKIP() << \"Test requires multicast support.\";\n   }\n-  const int64_t kNumDevices = 2;\n-  const int64_t kMemorySize = 1024;\n+  const int kNumDevices = 2;\n+  const int kNumElements = 8;\n+  const size_t kMemorySize = kNumElements * sizeof(int);\n+  const int kValue = 2;\n   std::unique_ptr<CudaExecutor::MulticastMemory> multicast_memory;\n   TF_ASSERT_OK_AND_ASSIGN(multicast_memory, executors[0]->CreateMulticastMemory(\n                                                 kMemorySize, kNumDevices));\n   EXPECT_THAT(multicast_memory->SubscribeDevice(0), IsOk());\n   EXPECT_THAT(multicast_memory->SubscribeDevice(1), IsOk());\n \n-  stream_executor::DeviceMemoryBase first_device_memory =\n-      executors[0]->Allocate(\n-          kMemorySize, static_cast<int64_t>(stream_executor::MemoryType::kP2P));\n-  EXPECT_THAT(\n-      multicast_memory->MapMemory(first_device_memory.opaque(), executors[0]),\n-      IsOkAndHolds(NotNull()));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      stream_executor::DeviceMemoryBase first_device_memory,\n+      AllocateInitializedMemory(executors[0], kMemorySize, kValue));\n \n-  stream_executor::DeviceMemoryBase second_device_memory =\n-      executors[1]->Allocate(\n-          kMemorySize, static_cast<int64_t>(stream_executor::MemoryType::kP2P));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      stream_executor::DeviceMemoryBase output_device_memory,\n+      AllocateInitializedMemory(executors[0], kMemorySize, 0));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      void* first_device_multicast_ptr,\n+      multicast_memory->MapMemory(first_device_memory.opaque(), executors[0]));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      stream_executor::DeviceMemoryBase second_device_memory,\n+      AllocateInitializedMemory(executors[1], kMemorySize, kValue));\n   EXPECT_THAT(\n       multicast_memory->MapMemory(second_device_memory.opaque(), executors[1]),\n       IsOkAndHolds(NotNull()));\n+\n+  EXPECT_THAT(\n+      MulticastReduce((int*)first_device_multicast_ptr,\n+                      (int*)output_device_memory.opaque(), kNumElements),\n+      IsOk());\n+\n+  const int kExpectedValue = kValue * kNumDevices;\n+  EXPECT_THAT(CheckMemory(executors[0], output_device_memory, kExpectedValue),\n+              IsOk());\n }\n \n }  // namespace"
        },
        {
            "sha": "fc2652cf210d944d605c593e8e09749ea06d2718",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor_multigpu_test_kernels.cu.cc",
            "status": "added",
            "additions": 51,
            "deletions": 0,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/364a0ac855e808d20919387c6b466e13c13f309b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test_kernels.cu.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/364a0ac855e808d20919387c6b466e13c13f309b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test_kernels.cu.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test_kernels.cu.cc?ref=364a0ac855e808d20919387c6b466e13c13f309b",
            "patch": "@@ -0,0 +1,51 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/stream_executor/cuda/cuda_executor_multigpu_test_kernels.h\"\n+\n+#include \"xla/stream_executor/cuda/cuda_status.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+\n+namespace stream_executor::gpu {\n+namespace {\n+\n+__global__ void MulticastReduceKernel(int* input, int* output, size_t size) {\n+#if __CUDA_ARCH__ >= 900\n+  for (int i = 0; i < size; i++) {\n+    int* multimem_element_ptr = input + i;\n+    int result = 0;\n+    asm volatile(\"multimem.ld_reduce.relaxed.sys.global.add.u32 %0, [%1];\"\n+                 : \"=r\"(result)\n+                 : \"l\"(multimem_element_ptr)\n+                 : \"memory\");\n+\n+    output[i] = result;\n+  }\n+#endif\n+}\n+}  // namespace\n+\n+__host__ absl::Status MulticastReduce(int* input, int* output, size_t size) {\n+  TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(cudaSetDevice(0)));\n+  TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(cudaDeviceSynchronize()));\n+  MulticastReduceKernel<<<1, 1, 0>>>(input, output, size);\n+  cudaError_t err = cudaGetLastError();\n+  if (err != cudaSuccess) {\n+    return absl::InternalError(\n+        absl::StrCat(\"CUDA Kernel launch failed: \", cudaGetErrorString(err)));\n+  }\n+  return stream_executor::cuda::ToStatus(cudaDeviceSynchronize());\n+}\n+}  // namespace stream_executor::gpu"
        },
        {
            "sha": "bac83b7ee8734b2deea28f10338166373a519f10",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor_multigpu_test_kernels.h",
            "status": "added",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/364a0ac855e808d20919387c6b466e13c13f309b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test_kernels.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/364a0ac855e808d20919387c6b466e13c13f309b/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test_kernels.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test_kernels.h?ref=364a0ac855e808d20919387c6b466e13c13f309b",
            "patch": "@@ -0,0 +1,25 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_STREAM_EXECUTOR_CUDA_CUDA_EXECUTOR_MULTIGPU_TEST_KERNELS_H_\n+#define XLA_STREAM_EXECUTOR_CUDA_CUDA_EXECUTOR_MULTIGPU_TEST_KERNELS_H_\n+\n+#include \"absl/status/status.h\"\n+\n+namespace stream_executor::gpu {\n+absl::Status MulticastReduce(int* input, int* output, size_t size);\n+}  // namespace stream_executor::gpu\n+\n+#endif  // XLA_STREAM_EXECUTOR_CUDA_CUDA_EXECUTOR_MULTIGPU_TEST_KERNELS_H_"
        }
    ],
    "stats": {
        "total": 180,
        "additions": 164,
        "deletions": 16
    }
}