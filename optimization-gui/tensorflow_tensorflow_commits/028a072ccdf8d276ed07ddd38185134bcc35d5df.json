{
    "author": "tensorflower-gardener",
    "message": "[Autotuner] Add IsSupported virtual method to GpuCodegenBackend.\n\nThis method is only used for Fissions to determine if a specific HloInstruction is supported by the codegen backend.\n\nPiperOrigin-RevId: 827503680",
    "sha": "028a072ccdf8d276ed07ddd38185134bcc35d5df",
    "files": [
        {
            "sha": "4434f5ec25b0c8def4a4bb7b96e9cb705c402847",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublas.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.cc?ref=028a072ccdf8d276ed07ddd38185134bcc35d5df",
            "patch": "@@ -153,5 +153,9 @@ absl::Status CublasBackend::ApplyConfig(HloInstruction& instr,\n   return absl::OkStatus();\n }\n \n+bool CublasBackend::IsSupported(const HloInstruction& instr) {\n+  return IsLegacyCublasMatmul(instr);\n+}\n+\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "3dfe16e84b2100588448e5f6f43dc20cb2d6e5db",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublas.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublas.h?ref=028a072ccdf8d276ed07ddd38185134bcc35d5df",
            "patch": "@@ -60,6 +60,9 @@ class CublasBackend : public GpuCodegenBackend {\n \n   absl::Status ApplyConfig(HloInstruction& instr,\n                            const BackendConfig& config) override;\n+\n+ private:\n+  bool IsSupported(const HloInstruction& instr) override;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "54c5b0e50a7bd69f825ccd8dd952dc28e865278c",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublaslt.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.cc?ref=028a072ccdf8d276ed07ddd38185134bcc35d5df",
            "patch": "@@ -74,12 +74,12 @@ absl::StatusOr<BlasLt::Epilogue> AsBlasLtEpilogue(\n   }\n }\n \n-bool IsSupported(const HloInstruction& instr) {\n+}  // namespace\n+\n+bool CublasLtBackend::IsSupported(const HloInstruction& instr) {\n   return IsCublasLtMatmul(instr) || IsCublasLtMatmulF8(instr);\n }\n \n-}  // namespace\n-\n absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n CublasLtBackend::GetSupportedConfigs(const HloInstruction& instr) {\n   if (!IsSupported(instr)) {"
        },
        {
            "sha": "8178e95b61cb8a68e894725e1c4eba353919470b",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cublaslt.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcublaslt.h?ref=028a072ccdf8d276ed07ddd38185134bcc35d5df",
            "patch": "@@ -59,6 +59,9 @@ class CublasLtBackend : public GpuCodegenBackend {\n \n   absl::Status ApplyConfig(HloInstruction& instr,\n                            const BackendConfig& config) override;\n+\n+ private:\n+  bool IsSupported(const HloInstruction& instr) override;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "9625194d72813572e513d318bd3c5bcb51d67958",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cudnn.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 15,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.cc?ref=028a072ccdf8d276ed07ddd38185134bcc35d5df",
            "patch": "@@ -155,20 +155,6 @@ bool IsSupportedCudnnFusion(const HloInstruction& instr,\n   return false;\n }\n \n-bool IsSupportedByCudnn(const HloInstruction& instr,\n-                        se::StreamExecutor* stream_executor,\n-                        const DebugOptions& debug_options) {\n-  if (instr.opcode() == HloOpcode::kFusion) {\n-    return IsSupportedCudnnFusion(instr, stream_executor, debug_options);\n-  }\n-\n-  if (instr.opcode() == HloOpcode::kCustomCall) {\n-    return IsCustomCallToDnnConvolution(instr);\n-  }\n-\n-  return false;\n-}\n-\n absl::StatusOr<std::vector<CudnnBackendConfig>> GetAlgorithms(\n     se::dnn::DnnSupport* dnn, se::dnn::ConvolutionKind conv_kind,\n     se::dnn::DataType input_type, se::dnn::DataType output_type,\n@@ -338,6 +324,18 @@ absl::Status ApplyConfigToCudnnCustomCall(HloInstruction& instr,\n \n }  // namespace\n \n+bool CudnnBackend::IsSupported(const HloInstruction& instr) {\n+  if (instr.opcode() == HloOpcode::kFusion) {\n+    return IsSupportedCudnnFusion(instr, stream_executor(), debug_options());\n+  }\n+\n+  if (instr.opcode() == HloOpcode::kCustomCall) {\n+    return IsCustomCallToDnnConvolution(instr);\n+  }\n+\n+  return false;\n+}\n+\n absl::StatusOr<std::unique_ptr<BackendConfig>> CudnnBackend::GetDefaultConfig(\n     const HloInstruction& instr) {\n   if (IsCustomCallToDnnConvolution(instr)) {\n@@ -358,7 +356,7 @@ absl::StatusOr<std::unique_ptr<BackendConfig>> CudnnBackend::GetDefaultConfig(\n \n absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n CudnnBackend::GetSupportedConfigs(const HloInstruction& instr) {\n-  if (!IsSupportedByCudnn(instr, stream_executor(), debug_options())) {\n+  if (!IsSupported(instr)) {\n     return std::vector<std::unique_ptr<BackendConfig>>();\n   }\n   if (instr.opcode() == HloOpcode::kFusion) {"
        },
        {
            "sha": "39867d7e39a289c8a42723e7469090c5d63bf9e2",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/cudnn.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcudnn.h?ref=028a072ccdf8d276ed07ddd38185134bcc35d5df",
            "patch": "@@ -72,6 +72,9 @@ class CudnnBackend : public GpuCodegenBackend {\n   // apply the configs with non-zero workspace size.\n   absl::Status ApplyConfig(HloInstruction& instr,\n                            const BackendConfig& config) override;\n+\n+ private:\n+  bool IsSupported(const HloInstruction& instr) override;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "be72b4e6edf99c5d5f8f01afa56bd3d963b39792",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/custom_kernel.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel.cc?ref=028a072ccdf8d276ed07ddd38185134bcc35d5df",
            "patch": "@@ -44,8 +44,7 @@ namespace se = ::stream_executor;\n \n using CustomKernelBackendConfig = AutotuneResult::CustomKernelFusionKey;\n \n-namespace {\n-bool IsSupported(const HloInstruction& instr) {\n+bool CustomKernelBackend::IsSupported(const HloInstruction& instr) {\n   if (instr.opcode() != HloOpcode::kFusion) {\n     LOG(ERROR)\n         << \"CustomKernelBackend doesn't support non-fusion instructions.\";\n@@ -61,7 +60,6 @@ bool IsSupported(const HloInstruction& instr) {\n \n   return true;\n }\n-}  // namespace\n \n absl::StatusOr<std::vector<CustomKernel>> LoadKernels(\n     const HloInstruction* fusion_instruction,"
        },
        {
            "sha": "ee6a7f3d5c3b4ffc5fe6afeb6bf2b36858cdabcb",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/custom_kernel.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fcustom_kernel.h?ref=028a072ccdf8d276ed07ddd38185134bcc35d5df",
            "patch": "@@ -48,6 +48,9 @@ class CustomKernelBackend : public GpuCodegenBackend {\n \n   absl::Status ApplyConfig(HloInstruction& instr,\n                            const BackendConfig& config) override;\n+\n+ private:\n+  bool IsSupported(const HloInstruction& instr) override;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "8722941d46c994a55aa7840a4e2ff71aa9d13169",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission.h",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.h?ref=028a072ccdf8d276ed07ddd38185134bcc35d5df",
            "patch": "@@ -65,6 +65,11 @@ class FissionBackend : public GpuCodegenBackend {\n   absl::Status ApplyConfig(HloInstruction& instr,\n                            const BackendConfig& config) override;\n \n+ private:\n+  bool IsSupported(const HloInstruction& instr) override {\n+    return instr.opcode() == HloOpcode::kFusion;\n+  }\n+\n   CublasBackend cublas_backend_;\n   CublasLtBackend cublaslt_backend_;\n   CustomKernelBackend custom_kernel_backend_;"
        },
        {
            "sha": "402752714de17bb60b6998a208216d36249b1ff8",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/gpu_codegen_backend.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fgpu_codegen_backend.h?ref=028a072ccdf8d276ed07ddd38185134bcc35d5df",
            "patch": "@@ -121,6 +121,10 @@ class GpuCodegenBackend : public CodegenBackend {\n     return hlo_module;\n   };\n \n+  virtual bool IsSupported(const HloInstruction& instr) = 0;\n+\n+  friend class FissionBackend;\n+\n   std::string name_;\n   stream_executor::StreamExecutor* stream_executor_;\n   const Compiler::TargetConfig& target_config_;"
        },
        {
            "sha": "bb29210837ae18303f9f87e990bfc0b4c60dbe0f",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/native_emitter.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.cc?ref=028a072ccdf8d276ed07ddd38185134bcc35d5df",
            "patch": "@@ -43,7 +43,7 @@ namespace gpu {\n // it has a config for another backend, and we currently don't have an easy way\n // to check that. Therefore, we only support fusions that are already set up to\n // go through the native emitter.\n-bool IsSupported(const HloInstruction& instr) {\n+bool NativeEmitterBackend::IsSupported(const HloInstruction& instr) {\n   if (instr.opcode() != HloOpcode::kFusion) {\n     return false;\n   }"
        },
        {
            "sha": "1bd6b55fea72ae5b7e9b087c37af5818f47d7c2d",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/native_emitter.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.h?ref=028a072ccdf8d276ed07ddd38185134bcc35d5df",
            "patch": "@@ -55,6 +55,9 @@ class NativeEmitterBackend : public GpuCodegenBackend {\n   // Applies a given fusion config to the instruction.\n   absl::Status ApplyConfig(HloInstruction& instr,\n                            const BackendConfig& config) override;\n+\n+ private:\n+  bool IsSupported(const HloInstruction& instr) override;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "a4be92308fef2433b57587c2ede7d9d34b83c14a",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/triton.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/028a072ccdf8d276ed07ddd38185134bcc35d5df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ftriton.h?ref=028a072ccdf8d276ed07ddd38185134bcc35d5df",
            "patch": "@@ -53,11 +53,12 @@ class TritonBackend : public GpuCodegenBackend {\n   bool CanProduceWrongResults() const override { return true; }\n \n  private:\n+  bool IsSupported(const HloInstruction& instr) override;\n+\n   absl::StatusOr<std::unique_ptr<HloModule>> RunHloPasses(\n       std::unique_ptr<HloModule> hlo_module,\n       const Compiler::CompileOptions& options) override;\n \n-  bool IsSupported(const HloInstruction& instr);\n   SymbolicExprContext* symbolic_expr_context_;\n };\n "
        }
    ],
    "stats": {
        "total": 71,
        "additions": 48,
        "deletions": 23
    }
}