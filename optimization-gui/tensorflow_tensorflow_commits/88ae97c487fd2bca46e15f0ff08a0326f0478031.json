{
    "author": "tensorflower-gardener",
    "message": "Inline all call ops to named computations, always.\n\nPiperOrigin-RevId: 799542437",
    "sha": "88ae97c487fd2bca46e15f0ff08a0326f0478031",
    "files": [
        {
            "sha": "9e16548c7e2ac060e7ad4f793420e4757b9a74be",
            "filename": "third_party/xla/xla/service/spmd/shardy/round_trip_common/import_func_calls.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 22,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fimport_func_calls.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fimport_func_calls.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fimport_func_calls.cc?ref=88ae97c487fd2bca46e15f0ff08a0326f0478031",
            "patch": "@@ -182,9 +182,6 @@ class ImportFuncCallsPass\n     for (mlir::CallGraphNode* node : llvm::reverse(rpo)) {\n       if (node->isExternal()) continue;\n       node->getCallableRegion()->walk([&](CallOp op) {\n-        if (onlyUninlineable && isInlineableCallOp(op)) {\n-          return;\n-        }\n         importCallOp(op, calleeNameToMovedRegion, rewriter, symbolTable);\n       });\n     }\n@@ -199,10 +196,8 @@ class ImportFuncCallsPass\n \n   StringRef getDescription() const override {\n     return \"Creates a pass to convert a CallOp to a NamedComputationOp with \"\n-           \"the function body inlined and the name of the callee. If \"\n-           \"onlyUninlineable is true, handle only CallOps with a \"\n-           \"backend_config or inlineable=false frontend attr. Otherwise, \"\n-           \"handle call CallOps.\";\n+           \"the function body inlined and the name of the callee. Note that \"\n+           \"the func bodies are cloned if the func is used by multiple calls.\";\n   }\n \n   void getDependentDialects(mlir::DialectRegistry& registry) const final {\n@@ -215,28 +210,16 @@ class ImportFuncCallsPass\n   ImportFuncCallsPass(ImportFuncCallsPass&&) = delete;\n   ImportFuncCallsPass& operator=(ImportFuncCallsPass&&) = delete;\n   ~ImportFuncCallsPass() override = default;\n-  ImportFuncCallsPass(bool onlyUninlineable) : ImportFuncCallsPass() {\n-    this->onlyUninlineable = onlyUninlineable;\n-  }\n-\n- protected:\n-  ::mlir::Pass::Option<bool> onlyUninlineable{\n-      *this, \"only-uninlineable\",\n-      ::llvm::cl::desc(\n-          \"Whether to convert only unlineable func calls, that is, the ones \"\n-          \"with a `backend_config` or `inlineable=false` frontend attr.\"),\n-      ::llvm::cl::init(true)};\n };\n \n }  // namespace\n \n-std::unique_ptr<mlir::Pass> createImportFuncCallsPass(bool onlyUninlineable) {\n-  return std::make_unique<ImportFuncCallsPass>(onlyUninlineable);\n+std::unique_ptr<mlir::Pass> createImportFuncCallsPass() {\n+  return std::make_unique<ImportFuncCallsPass>();\n }\n \n void registerImportFuncCallsPass() {\n-  mlir::registerPass(\n-      [] { return createImportFuncCallsPass(/*onlyUninlineable=*/true); });\n+  mlir::registerPass([] { return createImportFuncCallsPass(); });\n }\n \n }  // namespace sdy"
        },
        {
            "sha": "b7d79c7fbc6af0ca134653da2f0dfa6d539da70e",
            "filename": "third_party/xla/xla/service/spmd/shardy/round_trip_common/import_func_calls.h",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fimport_func_calls.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fimport_func_calls.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fimport_func_calls.h?ref=88ae97c487fd2bca46e15f0ff08a0326f0478031",
            "patch": "@@ -26,24 +26,11 @@ namespace sdy {\n // Creates a pass that converts a `CallOp` to a `NamedComputationOp` with the\n // function body inlined and name of the callee.\n //\n-// 1. In case `onlyIninlineable` is true (which is the default):\n-//\n-// Creates a pass that only converts a `CallOp` with a `backend_config` or\n-// `inlineable=false` frontend attr to a `NamedComputationOp` with the function\n-// body inlined and name of the callee.\n-//\n-// This pass is used to handle host offloading and GPU stream calls which are\n-// non inlined functions that require the callee to be propagated through.\n-//\n // NOTE: In case there are multiple call ops for the same callee, we will clone\n // the function body for each call op and emit a warning.\n-//\n-// 2. The case `onlyUninlineable` is false is not ready yet.\n-// TODO(enver): Support also for all func calls.\n-std::unique_ptr<mlir::Pass> createImportFuncCallsPass(bool onlyUninlineable);\n+std::unique_ptr<mlir::Pass> createImportFuncCallsPass();\n \n-// Register the xla-sdy-import-calls pass with `onlyUninlineable` is true.\n-// TODO(enver): Support also for all func calls.\n+// Register the xla-sdy-import-calls pass.\n void registerImportFuncCallsPass();\n \n }  // namespace sdy"
        },
        {
            "sha": "4985d73a38a50a37ddf5116f95035b8629bbbac3",
            "filename": "third_party/xla/xla/service/spmd/shardy/round_trip_common/pipeline_passes.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fpipeline_passes.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fpipeline_passes.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fpipeline_passes.cc?ref=88ae97c487fd2bca46e15f0ff08a0326f0478031",
            "patch": "@@ -51,12 +51,11 @@ void addCommonPreImportPasses(mlir::OpPassManager& pm,\n       mlir::stablehlo_ext::createStablehloCanonicalizeFromHloImportPass());\n }\n \n-void addCommonPostImportPasses(mlir::OpPassManager& pm, bool importFuncCalls,\n-                               bool importOnlyUninlineableFuncCalls) {\n+void addCommonPostImportPasses(mlir::OpPassManager& pm, bool importFuncCalls) {\n   pm.addPass(createImportSdyCustomCallsPass());\n   pm.addNestedPass<FuncOp>(createOpenWhileFreeVarsShardingPass());\n   if (importFuncCalls) {\n-    pm.addPass(createImportFuncCallsPass(importOnlyUninlineableFuncCalls));\n+    pm.addPass(createImportFuncCallsPass());\n   }\n }\n "
        },
        {
            "sha": "3ed66b0f28445f3e73a59bb9a26e580178ad06ba",
            "filename": "third_party/xla/xla/service/spmd/shardy/round_trip_common/pipeline_passes.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fpipeline_passes.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fpipeline_passes.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fround_trip_common%2Fpipeline_passes.h?ref=88ae97c487fd2bca46e15f0ff08a0326f0478031",
            "patch": "@@ -31,8 +31,7 @@ void addCommonPreImportPasses(mlir::OpPassManager& pm,\n // pipelines that need to be called after each pipeline converts an HLO\n // sharding/SDY sharding string into an `sdy.sharding` attribute.\n void addCommonPostImportPasses(mlir::OpPassManager& pm,\n-                               bool importFuncCalls = false,\n-                               bool importOnlyUninlineableFuncCalls = true);\n+                               bool importFuncCalls = false);\n \n }  // namespace sdy\n }  // namespace xla"
        },
        {
            "sha": "9d917bf7d4439fb77b8160e0544b5ba9447cd4db",
            "filename": "third_party/xla/xla/service/spmd/shardy/sdy_round_trip/pipelines.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 11,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fsdy_round_trip%2Fpipelines.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fsdy_round_trip%2Fpipelines.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fsdy_round_trip%2Fpipelines.cc?ref=88ae97c487fd2bca46e15f0ff08a0326f0478031",
            "patch": "@@ -61,16 +61,14 @@ void addSdyRoundTripExportPipeline(mlir::OpPassManager& pm,\n void addSdyRoundTripImportPipeline(mlir::OpPassManager& pm,\n                                    bool enableConstantImport,\n                                    bool importFuncCalls,\n-                                   bool importOnlyUninlineableFuncCalls,\n                                    bool liftAndDedupMeshes) {\n   addCommonPreImportPasses(pm, enableConstantImport);\n   pm.addPass(createSdyRoundTripImportShardyAttrsPass());\n   // TODO(b/430894772): Drop the pass and handle cloning inside shard map import\n   // pass.\n   pm.addPass(createSdyRoundTripCloneManualComputationCallsPass());\n   pm.addPass(createSdyRoundTripShardMapImportPass());\n-  addCommonPostImportPasses(pm, importFuncCalls,\n-                            importOnlyUninlineableFuncCalls);\n+  addCommonPostImportPasses(pm, importFuncCalls);\n   if (liftAndDedupMeshes) {\n     // Lift and dedup meshes required here because of sdy shardings added\n     // directly to hlo in tf2xla.\n@@ -114,21 +112,16 @@ struct SdyRoundTripImportPipelineOptions\n   Option<bool> importFuncCalls{*this, \"import-func-calls\",\n                                llvm::cl::desc(\"Import func calls.\"),\n                                llvm::cl::init(false)};\n-  // TODO(b/430894772): Drop the flag and import all func calls always.\n-  Option<bool> importOnlyUninlineableFuncCalls{\n-      *this, \"import-only-uninlineable-func-calls\",\n-      llvm::cl::desc(\"Import only unlineable func calls.\"),\n-      llvm::cl::init(true)};\n   Option<bool> liftAndDedupMeshes{*this, \"lift-and-dedup-meshes\",\n                                   llvm::cl::desc(\"Lift and dedup meshes.\"),\n                                   llvm::cl::init(false)};\n };\n \n void sdyRoundTripImportPipeline(\n     mlir::OpPassManager& pm, const SdyRoundTripImportPipelineOptions& options) {\n-  addSdyRoundTripImportPipeline(\n-      pm, options.enableConstantImport, options.importFuncCalls,\n-      options.importOnlyUninlineableFuncCalls, options.liftAndDedupMeshes);\n+  addSdyRoundTripImportPipeline(pm, options.enableConstantImport,\n+                                options.importFuncCalls,\n+                                options.liftAndDedupMeshes);\n }\n \n }  // namespace"
        },
        {
            "sha": "1503bc2339c3e5230ed0506784b91bd3f18553a9",
            "filename": "third_party/xla/xla/service/spmd/shardy/sdy_round_trip/pipelines.h",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fsdy_round_trip%2Fpipelines.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fsdy_round_trip%2Fpipelines.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fsdy_round_trip%2Fpipelines.h?ref=88ae97c487fd2bca46e15f0ff08a0326f0478031",
            "patch": "@@ -46,7 +46,6 @@ void addSdyRoundTripExportPipeline(mlir::OpPassManager& pm,\n void addSdyRoundTripImportPipeline(mlir::OpPassManager& pm,\n                                    bool enableConstantImport = true,\n                                    bool importFuncCalls = false,\n-                                   bool importOnlyUninlineableFuncCalls = true,\n                                    bool liftAndDedupMeshes = false);\n \n // Register the xla-sdy-round-trip-export-pipeline."
        },
        {
            "sha": "3adb240a7c919f87bee55a3d8730003a523998d7",
            "filename": "third_party/xla/xla/service/spmd/shardy/shardy_xla_pass.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fshardy_xla_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fshardy_xla_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fshardy_xla_pass.cc?ref=88ae97c487fd2bca46e15f0ff08a0326f0478031",
            "patch": "@@ -373,13 +373,11 @@ absl::Status runShardingPropagation(HloModule* hloModule,\n         /*allowPropagationToResults=*/\n         spanToArrayRef(\n             hloModule->config().allow_spmd_sharding_propagation_to_output()),\n-        /*importFuncCalls=*/true,\n-        /*importOnlyUninlineableFuncCalls=*/false);\n+        /*importFuncCalls=*/true);\n   } else {\n     // This branch is in production.\n     addSdyRoundTripImportPipeline(pm, /*enableConstantImport=*/true,\n                                   /*importFuncCalls=*/true,\n-                                  /*importOnlyUninlineableFuncCalls=*/false,\n                                   /*liftAndDedupMeshes=*/true);\n   }\n "
        },
        {
            "sha": "36741388f9cb1ffe21c8b2bf584300b771c0c57f",
            "filename": "third_party/xla/xla/service/spmd/shardy/stablehlo_round_trip/stablehlo_import.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fstablehlo_import.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fstablehlo_import.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fstablehlo_import.cc?ref=88ae97c487fd2bca46e15f0ff08a0326f0478031",
            "patch": "@@ -751,14 +751,12 @@ void registerStablehloImportShardingsPass() {\n void addStablehloImportPipeline(mlir::OpPassManager& pm,\n                                 ArrayRef<bool> allowPropagationToArgs,\n                                 ArrayRef<bool> allowPropagationToResults,\n-                                bool importFuncCalls,\n-                                bool importOnlyUninlineableFuncCalls) {\n+                                bool importFuncCalls) {\n   addCommonPreImportPasses(pm);\n   pm.addPass(createImportShardingsPass(allowPropagationToArgs,\n                                        allowPropagationToResults));\n   pm.addPass(createStablehloRoundTripShardMapImportPass());\n-  addCommonPostImportPasses(pm, importFuncCalls,\n-                            importOnlyUninlineableFuncCalls);\n+  addCommonPostImportPasses(pm, importFuncCalls);\n }\n \n void registerStablehloImportPipeline() {\n@@ -767,7 +765,7 @@ void registerStablehloImportPipeline() {\n       \"Run passes to import a StableHLO module with `mhlo.shardings` into the \"\n       \"SDY (Shardy) dialect.\",\n       std::bind(addStablehloImportPipeline, std::placeholders::_1,\n-                ArrayRef<bool>(), ArrayRef<bool>(), true, true));\n+                ArrayRef<bool>(), ArrayRef<bool>(), true));\n }\n \n }  // namespace sdy"
        },
        {
            "sha": "10db81bf0ce2e072c8c5ad8d8a936fa3714d288e",
            "filename": "third_party/xla/xla/service/spmd/shardy/stablehlo_round_trip/stablehlo_import.h",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fstablehlo_import.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fstablehlo_import.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fstablehlo_import.h?ref=88ae97c487fd2bca46e15f0ff08a0326f0478031",
            "patch": "@@ -76,8 +76,7 @@ void registerStablehloImportPipeline();\n void addStablehloImportPipeline(mlir::OpPassManager& pm,\n                                 mlir::ArrayRef<bool> allowPropagationToArgs,\n                                 mlir::ArrayRef<bool> allowPropagationToResults,\n-                                bool importFuncCalls = true,\n-                                bool importOnlyUninlineableFuncCalls = true);\n+                                bool importFuncCalls = true);\n \n // Creates ImportShardingsPass that converts `mhlo.sharding` to `mesh` and\n // `sdy.sharding`."
        },
        {
            "sha": "6820e89f4ca9ada30aaa97694ba4682b3523d898",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/import_func_calls.mlir",
            "status": "modified",
            "additions": 53,
            "deletions": 19,
            "changes": 72,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fimport_func_calls.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fimport_func_calls.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fimport_func_calls.mlir?ref=88ae97c487fd2bca46e15f0ff08a0326f0478031",
            "patch": "@@ -1,5 +1,5 @@\n-// RUN: sdy_opt --split-input-file %s -xla-sdy-import-func-calls='only-uninlineable=true' | FileCheck %s\n-// RUN: sdy_opt %s -split-input-file -xla-sdy-import-func-calls='only-uninlineable=true' -verify-diagnostics\n+// RUN: sdy_opt --split-input-file %s -xla-sdy-import-func-calls | FileCheck %s\n+// RUN: sdy_opt %s -split-input-file -xla-sdy-import-func-calls -verify-diagnostics\n \n sdy.mesh @mesh = #sdy.mesh<[\"x\"=2, \"y\"=2]>\n \n@@ -68,33 +68,42 @@ func.func private @baz(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32>) -> (tenso\n \n // CHECK-LABEL: func @inlineable_true\n func.func @inlineable_true(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32>) -> (tensor<8x2xi32>) {\n-  // CHECK-NEXT: %[[CALL:.*]]:2 =  call @qux(%arg0, %arg1) {mhlo.frontend_attributes = {inlineable = \"true\"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {}]>, <@mesh, [{}, {\"y\"}]>]>}\n-  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[CALL]]#0, %[[CALL]]#1 : tensor<8x2xi32>\n+  // CHECK-NEXT: %[[NC:.*]]:2 = sdy.named_computation<\"qux\">(%arg0, %arg1) out_shardings=[<@mesh, [{\"x\"}, {}]>, <@mesh, [{}, {\"y\"}]>] (%arg2: tensor<8x2xi32>, %arg3: tensor<8x2xi32>) {\n+  // CHECK-NEXT:   %[[MULT:.*]] = stablehlo.multiply %arg2, %arg3 : tensor<8x2xi32>\n+  // CHECK-NEXT:   sdy.return %[[MULT]], %arg3 : tensor<8x2xi32>, tensor<8x2xi32>\n+  // CHECK-NEXT: } {mhlo.frontend_attributes = {inlineable = \"true\"}}\n+  // CHECK-SAME: (tensor<8x2xi32>, tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<8x2xi32>)\n+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[NC]]#0, %[[NC]]#1 : tensor<8x2xi32>\n   // CHECK-NEXT: return %[[ADD]] : tensor<8x2xi32>\n   %0:2 = call @qux(%arg0, %arg1) {mhlo.frontend_attributes = {inlineable = \"true\"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {}]>, <@mesh, [{}, {\"y\"}]>]>} : (tensor<8x2xi32>, tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<8x2xi32>)\n   %1 = stablehlo.add %0#0, %0#1 : tensor<8x2xi32>\n   return %1 : tensor<8x2xi32>\n }\n \n-// CHECK: func private @qux\n+// CHECK-NOT: func private @qux\n func.func private @qux(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<8x2xi32>) {\n   %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x2xi32>\n   return %0, %arg1 : tensor<8x2xi32>, tensor<8x2xi32>\n }\n \n-// Don't import if there is no backend_config or inlineable attr.\n // CHECK-LABEL: func @no_backend_config_or_inlineable_attr\n-func.func @no_backend_config_or_inlineable_attr(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) {\n-  // CHECK-NEXT: %[[CALL:.*]] = call @quux(%arg0) : (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  // CHECK-NEXT: return %[[CALL]] : tensor<8x2xi32>\n-  %0 = call @quux(%arg0) : (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  return %0 : tensor<8x2xi32>\n+func.func @no_backend_config_or_inlineable_attr(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32>) -> (tensor<8x2xi32>) {\n+  // CHECK-NEXT: %[[NC:.*]]:2 = sdy.named_computation<\"quux\">(%arg0, %arg1) out_shardings=[<@mesh, [{\"x\"}, {}]>, <@mesh, [{}, {\"y\"}]>] (%arg2: tensor<8x2xi32>, %arg3: tensor<8x2xi32>) {\n+  // CHECK-NEXT:   %[[MULT:.*]] = stablehlo.multiply %arg2, %arg3 : tensor<8x2xi32>\n+  // CHECK-NEXT:   sdy.return %[[MULT]], %arg3 : tensor<8x2xi32>, tensor<8x2xi32>\n+  // CHECK-NEXT: }\n+  // CHECK-SAME: (tensor<8x2xi32>, tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<8x2xi32>)\n+  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[NC]]#0, %[[NC]]#1 : tensor<8x2xi32>\n+  // CHECK-NEXT: return %[[ADD]] : tensor<8x2xi32>\n+  %0:2 = call @quux(%arg0, %arg1) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {}]>, <@mesh, [{}, {\"y\"}]>]>} : (tensor<8x2xi32>, tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<8x2xi32>)\n+  %1 = stablehlo.add %0#0, %0#1 : tensor<8x2xi32>\n+  return %1 : tensor<8x2xi32>\n }\n \n-// CHECK: func private @quux\n-func.func private @quux(%arg0: tensor<8x2xi32>) -> tensor<8x2xi32> {\n-  %0 = stablehlo.multiply %arg0, %arg0 : tensor<8x2xi32>\n-  return %0 : tensor<8x2xi32>\n+// CHECK-NOT: func private @quux\n+func.func private @quux(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<8x2xi32>) {\n+  %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x2xi32>\n+  return %0, %arg1 : tensor<8x2xi32>, tensor<8x2xi32>\n }\n \n // -----\n@@ -225,29 +234,54 @@ func.func private @baz(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n // -----\n \n // CHECK-LABEL: func @non_flat_call_graph_all_inlineable\n-// CHECK-NOT: sdy.named_computation\n func.func @non_flat_call_graph_all_inlineable(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n+  // CHECK-NEXT: %[[NC1:.*]] = sdy.named_computation<\"foo\">(%arg0) (%arg1: tensor<8xf32>) {\n+  // CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg1, %arg1 : tensor<8xf32>\n+  // CHECK-NEXT:   %[[NC2:.*]] = sdy.named_computation<\"bar\">(%[[ADD]]) (%arg2: tensor<8xf32>) {\n+  // CHECK-NEXT:     %[[ABS1:.*]] = stablehlo.abs %arg2 : tensor<8xf32>\n+  // CHECK-NEXT:     sdy.return %[[ABS1]] : tensor<8xf32>\n+  // CHECK-NEXT:   }\n+  // CHECK-SAME:   {mhlo.frontend_attributes = {inlineable = \"true\"}}\n+  // CHECK-SAME:   (tensor<8xf32>) -> tensor<8xf32>\n+  // CHECK-NEXT:   sdy.return %[[NC2]] : tensor<8xf32>\n+  // CHECK-NEXT: }\n+  // CHECK-SAME: {mhlo.frontend_attributes = {inlineable = \"true\"}}\n+  // CHECK-SAME: (tensor<8xf32>) -> tensor<8xf32>\n+  // CHECK-NEXT: %[[NEGATE:.*]] = stablehlo.negate %[[NC1]] : tensor<8xf32>\n+  // CHECK-NEXT: %[[NC3:.*]] = sdy.named_computation<\"baz\">(%[[NEGATE]]) (%arg1: tensor<8xf32>) {\n+  // CHECK-NEXT:   %[[ABS2:.*]] = stablehlo.abs %arg1 : tensor<8xf32>\n+  // CHECK-NEXT:   sdy.return %[[ABS2]] : tensor<8xf32>\n+  // CHECK-NEXT: }\n+  // CHECK-SAME: {mhlo.frontend_attributes = {inlineable = \"true\"}}\n+  // CHECK-SAME: (tensor<8xf32>) -> tensor<8xf32>\n+  // CHECK-NEXT: %[[NC4:.*]] = sdy.named_computation<\"bar\">(%[[NC3]]) (%arg1: tensor<8xf32>) {\n+  // CHECK-NEXT:   %[[ABS3:.*]] = stablehlo.abs %arg1 : tensor<8xf32>\n+  // CHECK-NEXT:   sdy.return %[[ABS3]] : tensor<8xf32>\n+  // CHECK-NEXT: }\n+  // CHECK-SAME: {mhlo.frontend_attributes = {inlineable = \"true\"}}\n+  // CHECK-SAME: (tensor<8xf32>) -> tensor<8xf32>\n+  // CHECK-NEXT: return %[[NC4]] : tensor<8xf32>\n   %0 = call @foo(%arg0) {mhlo.frontend_attributes = {inlineable = \"true\"}} : (tensor<8xf32>) -> tensor<8xf32>\n   %1 = stablehlo.negate %0 : tensor<8xf32>\n   %2 = call @baz(%1) {mhlo.frontend_attributes = {inlineable = \"true\"}} : (tensor<8xf32>) -> tensor<8xf32>\n   %3 = call @bar(%2) {mhlo.frontend_attributes = {inlineable = \"true\"}} : (tensor<8xf32>) -> tensor<8xf32>\n   return %3 : tensor<8xf32>\n }\n \n-// CHECK: func private @foo\n+// CHECK-NOT: func private @foo\n func.func private @foo(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n   %0 = stablehlo.add %arg0, %arg0 : tensor<8xf32>\n   %1 = call @bar(%0) {mhlo.frontend_attributes = {inlineable = \"true\"}} : (tensor<8xf32>) -> tensor<8xf32>\n   return %1 : tensor<8xf32>\n }\n \n-// CHECK: func private @bar\n+// CHECK-NOT: func private @bar\n func.func private @bar(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n   %0 = stablehlo.abs %arg0 : tensor<8xf32>\n   return %0 : tensor<8xf32>\n }\n \n-// CHECK: func private @baz\n+// CHECK-NOT: func private @baz\n func.func private @baz(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n   %0 = stablehlo.abs %arg0 : tensor<8xf32>\n   return %0 : tensor<8xf32>"
        },
        {
            "sha": "3aba8e5d5f4c882f792829048bb706d071953940",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/import_func_calls_only_unlineable_false.mlir",
            "status": "removed",
            "additions": 0,
            "deletions": 247,
            "changes": 247,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/616d868c1cc8a03d62a31c10a213d5d931a86fd0/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fimport_func_calls_only_unlineable_false.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/616d868c1cc8a03d62a31c10a213d5d931a86fd0/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fimport_func_calls_only_unlineable_false.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fimport_func_calls_only_unlineable_false.mlir?ref=616d868c1cc8a03d62a31c10a213d5d931a86fd0",
            "patch": "@@ -1,247 +0,0 @@\n-// RUN: sdy_opt --split-input-file %s -xla-sdy-import-func-calls='only-uninlineable=false' | FileCheck %s\n-// RUN: sdy_opt %s -split-input-file -xla-sdy-import-func-calls='only-uninlineable=false' -verify-diagnostics\n-\n-sdy.mesh @mesh = #sdy.mesh<[\"x\"=2, \"y\"=2]>\n-\n-// CHECK-LABEL: func @backend_config_no_out_shardings\n-func.func @backend_config_no_out_shardings(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) {\n-  // CHECK-NEXT: %[[NC:.*]] = sdy.named_computation<\"foo\">(%arg0) (%arg1: tensor<8x2xi32>) {\n-  // CHECK-NEXT:   %[[MULT:.*]] = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}} : tensor<8x2xi32>\n-  // CHECK-NEXT:   sdy.return %[[MULT]] : tensor<8x2xi32>\n-  // CHECK-NEXT: } {mhlo.frontend_attributes = {backend_config = \"{\\22flag_configs\\22:[],\\22scoped_memory_configs\\22:[],\\22device_type\\22:\\22DEVICE_TYPE_HOST\\22,\\22used_scoped_memory_configs\\22:[]}\"},\n-  // CHECK-SAME:    random_attr = \"random_value\"}\n-  // CHECK-SAME: (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  // CHECK-NEXT: %[[MOVE_TO_HOST:.*]] = stablehlo.custom_call @MoveToHost(%[[NC]]) {backend_config = \"\"} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  // CHECK-NEXT: return %[[MOVE_TO_HOST]] : tensor<8x2xi32>\n-  %0 = call @foo(%arg0) {random_attr = \"random_value\", mhlo.frontend_attributes = {backend_config = \"{\\22flag_configs\\22:[],\\22scoped_memory_configs\\22:[],\\22device_type\\22:\\22DEVICE_TYPE_HOST\\22,\\22used_scoped_memory_configs\\22:[]}\"}} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  %1 = stablehlo.custom_call @MoveToHost(%0) {backend_config = \"\"} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  return %1 : tensor<8x2xi32>\n-}\n-\n-// CHECK-NOT: func private @foo\n-func.func private @foo(%arg0: tensor<8x2xi32>) -> tensor<8x2xi32> {\n-  %0 = stablehlo.multiply %arg0, %arg0 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}} : tensor<8x2xi32>\n-  return %0 : tensor<8x2xi32>\n-}\n-\n-// CHECK-LABEL: func @backend_config_out_shardings\n-func.func @backend_config_out_shardings(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) {\n-  // CHECK-NEXT: %[[NC:.*]] = sdy.named_computation<\"bar\">(%arg0) in_shardings=[<@mesh, [{\"x\"}, {}]>] out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n-  // CHECK-NEXT:   %[[MULT:.*]] = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}} : tensor<8x2xi32>\n-  // CHECK-NEXT:   sdy.return %[[MULT]] : tensor<8x2xi32>\n-  // CHECK-NEXT: } {mhlo.frontend_attributes = {backend_config = \"{\\22flag_configs\\22:[],\\22scoped_memory_configs\\22:[],\\22device_type\\22:\\22DEVICE_TYPE_HOST\\22,\\22used_scoped_memory_configs\\22:[]}\"},\n-  // CHECK-SAME:    random_attr = \"random_value\"}\n-  // CHECK-SAME: (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  // CHECK-NEXT: %[[MOVE_TO_HOST:.*]] = stablehlo.custom_call @MoveToHost(%[[NC]]) {backend_config = \"\"} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  // CHECK-NEXT: return %[[MOVE_TO_HOST]] : tensor<8x2xi32>\n-  %0 = call @bar(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {\"y\"}]>]>, random_attr = \"random_value\", mhlo.frontend_attributes = {backend_config = \"{\\22flag_configs\\22:[],\\22scoped_memory_configs\\22:[],\\22device_type\\22:\\22DEVICE_TYPE_HOST\\22,\\22used_scoped_memory_configs\\22:[]}\"}} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  %1 = stablehlo.custom_call @MoveToHost(%0) {backend_config = \"\"} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  return %1 : tensor<8x2xi32>\n-}\n-\n-// NOTE: we ignore any arg/result shardings on the function.\n-// CHECK-NOT: func private @bar\n-func.func private @bar(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {}]>}) -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {\"y\"}]>}) {\n-  %0 = stablehlo.multiply %arg0, %arg0 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}} : tensor<8x2xi32>\n-  return %0 : tensor<8x2xi32>\n-}\n-\n-// CHECK-LABEL: func @inlineable_false\n-func.func @inlineable_false(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32>) -> (tensor<8x2xi32>) {\n-  // CHECK-NEXT: %[[NC:.*]]:2 = sdy.named_computation<\"baz\">(%arg0, %arg1) out_shardings=[<@mesh, [{\"x\"}, {}]>, <@mesh, [{}, {\"y\"}]>] (%arg2: tensor<8x2xi32>, %arg3: tensor<8x2xi32>) {\n-  // CHECK-NEXT:   %[[MULT:.*]] = stablehlo.multiply %arg2, %arg3 : tensor<8x2xi32>\n-  // CHECK-NEXT:   sdy.return %[[MULT]], %arg3 : tensor<8x2xi32>, tensor<8x2xi32>\n-  // CHECK-NEXT: } {mhlo.frontend_attributes = {inlineable = \"false\"}}\n-  // CHECK-SAME: (tensor<8x2xi32>, tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<8x2xi32>)\n-  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[NC]]#0, %[[NC]]#1 : tensor<8x2xi32>\n-  // CHECK-NEXT: return %[[ADD]] : tensor<8x2xi32>\n-  %0:2 = call @baz(%arg0, %arg1) {mhlo.frontend_attributes = {inlineable = \"false\"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {}]>, <@mesh, [{}, {\"y\"}]>]>} : (tensor<8x2xi32>, tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<8x2xi32>)\n-  %1 = stablehlo.add %0#0, %0#1 : tensor<8x2xi32>\n-  return %1 : tensor<8x2xi32>\n-}\n-\n-// CHECK-NOT: func private @baz\n-func.func private @baz(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<8x2xi32>) {\n-  %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x2xi32>\n-  return %0, %arg1 : tensor<8x2xi32>, tensor<8x2xi32>\n-}\n-\n-// CHECK-LABEL: func @inlineable_true\n-func.func @inlineable_true(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32>) -> (tensor<8x2xi32>) {\n-  // CHECK-NEXT: %[[NC:.*]]:2 = sdy.named_computation<\"qux\">(%arg0, %arg1) out_shardings=[<@mesh, [{\"x\"}, {}]>, <@mesh, [{}, {\"y\"}]>] (%arg2: tensor<8x2xi32>, %arg3: tensor<8x2xi32>) {\n-  // CHECK-NEXT:   %[[MULT:.*]] = stablehlo.multiply %arg2, %arg3 : tensor<8x2xi32>\n-  // CHECK-NEXT:   sdy.return %[[MULT]], %arg3 : tensor<8x2xi32>, tensor<8x2xi32>\n-  // CHECK-NEXT: } {mhlo.frontend_attributes = {inlineable = \"true\"}}\n-  // CHECK-SAME: (tensor<8x2xi32>, tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<8x2xi32>)\n-  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[NC]]#0, %[[NC]]#1 : tensor<8x2xi32>\n-  // CHECK-NEXT: return %[[ADD]] : tensor<8x2xi32>\n-  %0:2 = call @qux(%arg0, %arg1) {mhlo.frontend_attributes = {inlineable = \"true\"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {}]>, <@mesh, [{}, {\"y\"}]>]>} : (tensor<8x2xi32>, tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<8x2xi32>)\n-  %1 = stablehlo.add %0#0, %0#1 : tensor<8x2xi32>\n-  return %1 : tensor<8x2xi32>\n-}\n-\n-// CHECK-NOT: func private @qux\n-func.func private @qux(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<8x2xi32>) {\n-  %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x2xi32>\n-  return %0, %arg1 : tensor<8x2xi32>, tensor<8x2xi32>\n-}\n-\n-// CHECK-LABEL: func @no_backend_config_or_inlineable_attr\n-func.func @no_backend_config_or_inlineable_attr(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32>) -> (tensor<8x2xi32>) {\n-  // CHECK-NEXT: %[[NC:.*]]:2 = sdy.named_computation<\"quux\">(%arg0, %arg1) out_shardings=[<@mesh, [{\"x\"}, {}]>, <@mesh, [{}, {\"y\"}]>] (%arg2: tensor<8x2xi32>, %arg3: tensor<8x2xi32>) {\n-  // CHECK-NEXT:   %[[MULT:.*]] = stablehlo.multiply %arg2, %arg3 : tensor<8x2xi32>\n-  // CHECK-NEXT:   sdy.return %[[MULT]], %arg3 : tensor<8x2xi32>, tensor<8x2xi32>\n-  // CHECK-NEXT: }\n-  // CHECK-SAME: (tensor<8x2xi32>, tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<8x2xi32>)\n-  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[NC]]#0, %[[NC]]#1 : tensor<8x2xi32>\n-  // CHECK-NEXT: return %[[ADD]] : tensor<8x2xi32>\n-  %0:2 = call @quux(%arg0, %arg1) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {}]>, <@mesh, [{}, {\"y\"}]>]>} : (tensor<8x2xi32>, tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<8x2xi32>)\n-  %1 = stablehlo.add %0#0, %0#1 : tensor<8x2xi32>\n-  return %1 : tensor<8x2xi32>\n-}\n-\n-// CHECK-NOT: func private @quux\n-func.func private @quux(%arg0: tensor<8x2xi32>, %arg1: tensor<8x2xi32>) -> (tensor<8x2xi32>, tensor<8x2xi32>) {\n-  %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x2xi32>\n-  return %0, %arg1 : tensor<8x2xi32>, tensor<8x2xi32>\n-}\n-\n-// -----\n-\n-sdy.mesh @mesh = #sdy.mesh<[\"x\"=2, \"y\"=2]>\n-\n-// CHECK-LABEL: func @multiple_call_ops_same_name\n-func.func @multiple_call_ops_same_name(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {\"y\"}]>}) {\n-  // CHECK-NEXT: %[[NC_0:.*]] = sdy.named_computation<\"foobar\">(%arg0) in_shardings=[<@mesh, [{\"x\"}, {}]>] out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n-  // CHECK-NEXT:   %[[MULT_0:.*]] = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}} : tensor<8x2xi32>\n-  // CHECK-NEXT:   sdy.return %[[MULT_0]] : tensor<8x2xi32>\n-  // CHECK-NEXT: } {mhlo.frontend_attributes = {backend_config = \"{\\22flag_configs\\22:[],\\22scoped_memory_configs\\22:[],\\22device_type\\22:\\22DEVICE_TYPE_HOST\\22,\\22used_scoped_memory_configs\\22:[]}\"},\n-  // CHECK-SAME:    random_attr = \"random_value\"}\n-  // CHECK-SAME: (tensor<8x2xi32>) -> tensor<8x2xi32>\n-\n-  // CHECK-NEXT: %[[NC_1:.*]] = sdy.named_computation<\"foobar\">(%[[NC_0]]) in_shardings=[<@mesh, [{\"x\"}, {}]>] out_shardings=[<@mesh, [{\"x\"}, {\"y\"}]>] (%arg1: tensor<8x2xi32>) {\n-  // CHECK-NEXT:   %[[MULT_1:.*]] = stablehlo.multiply %arg1, %arg1 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}} : tensor<8x2xi32>\n-  // CHECK-NEXT:   sdy.return %[[MULT_1]] : tensor<8x2xi32>\n-  // CHECK-NEXT: } {mhlo.frontend_attributes = {backend_config = \"{\\22flag_configs\\22:[],\\22scoped_memory_configs\\22:[],\\22device_type\\22:\\22DEVICE_TYPE_HOST\\22,\\22used_scoped_memory_configs\\22:[]}\"},\n-  // CHECK-SAME:    random_attr = \"random_value\"}\n-  // CHECK-SAME: (tensor<8x2xi32>) -> tensor<8x2xi32>\n-\n-  // CHECK-NEXT: %[[MOVE_TO_HOST:.*]] = stablehlo.custom_call @MoveToHost(%[[NC_1]]) {backend_config = \"\"} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  // CHECK-NEXT: return %[[MOVE_TO_HOST]] : tensor<8x2xi32>\n-  %0 = call @foobar(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {\"y\"}]>]>, random_attr = \"random_value\", mhlo.frontend_attributes = {backend_config = \"{\\22flag_configs\\22:[],\\22scoped_memory_configs\\22:[],\\22device_type\\22:\\22DEVICE_TYPE_HOST\\22,\\22used_scoped_memory_configs\\22:[]}\"}} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  // expected-warning @+1 {{function @foobar has multiple call ops, we need to clone the function body for each call}}\n-  %1 = call @foobar(%0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{\"x\"}, {\"y\"}]>]>, random_attr = \"random_value\", mhlo.frontend_attributes = {backend_config = \"{\\22flag_configs\\22:[],\\22scoped_memory_configs\\22:[],\\22device_type\\22:\\22DEVICE_TYPE_HOST\\22,\\22used_scoped_memory_configs\\22:[]}\"}} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  %2 = stablehlo.custom_call @MoveToHost(%1) {backend_config = \"\"} : (tensor<8x2xi32>) -> tensor<8x2xi32>\n-  return %2 : tensor<8x2xi32>\n-}\n-\n-// CHECK-NOT: func private @foobar\n-func.func private @foobar(%arg0: tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{\"x\"}, {}]>}) -> (tensor<8x2xi32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {\"y\"}]>}) {\n-  %0 = stablehlo.multiply %arg0, %arg0 {mhlo.frontend_attributes = {_xla_compute_type = \"host\"}} : tensor<8x2xi32>\n-  return %0 : tensor<8x2xi32>\n-}\n-\n-// -----\n-\n-// CHECK-LABEL: func @non_flat_call_graph_all_uninlineable\n-func.func @non_flat_call_graph_all_uninlineable(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n-  // CHECK-NEXT: %[[NC1:.*]] = sdy.named_computation<\"foo\">(%arg0) (%arg1: tensor<8xf32>) {\n-  // CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg1, %arg1 : tensor<8xf32>\n-  // CHECK-NEXT:   %[[NC2:.*]] = sdy.named_computation<\"bar\">(%[[ADD]]) (%arg2: tensor<8xf32>) {\n-  // CHECK-NEXT:     %[[ABS1:.*]] = stablehlo.abs %arg2 : tensor<8xf32>\n-  // CHECK-NEXT:     sdy.return %[[ABS1]] : tensor<8xf32>\n-  // CHECK-NEXT:   }\n-  // CHECK-SAME:   {mhlo.frontend_attributes = {inlineable = \"false\"}}\n-  // CHECK-SAME:   (tensor<8xf32>) -> tensor<8xf32>\n-  // CHECK-NEXT:   sdy.return %[[NC2]] : tensor<8xf32>\n-  // CHECK-NEXT: }\n-  // CHECK-SAME: {mhlo.frontend_attributes = {inlineable = \"false\"}}\n-  // CHECK-SAME: (tensor<8xf32>) -> tensor<8xf32>\n-  // CHECK-NEXT: %[[NEGATE:.*]] = stablehlo.negate %[[NC1]] : tensor<8xf32>\n-  // CHECK-NEXT: %[[NC3:.*]] = sdy.named_computation<\"baz\">(%[[NEGATE]]) (%arg1: tensor<8xf32>) {\n-  // CHECK-NEXT:   %[[ABS2:.*]] = stablehlo.abs %arg1 : tensor<8xf32>\n-  // CHECK-NEXT:   sdy.return %[[ABS2]] : tensor<8xf32>\n-  // CHECK-NEXT: }\n-  // CHECK-SAME: {mhlo.frontend_attributes = {inlineable = \"false\"}}\n-  // CHECK-SAME: (tensor<8xf32>) -> tensor<8xf32>\n-  // CHECK-NEXT: return %[[NC3]] : tensor<8xf32>\n-  %0 = call @foo(%arg0) {mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8xf32>) -> tensor<8xf32>\n-  %1 = stablehlo.negate %0 : tensor<8xf32>\n-  %2 = call @baz(%1) {mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8xf32>) -> tensor<8xf32>\n-  return %2 : tensor<8xf32>\n-}\n-\n-// CHECK-NOT: func private @foo\n-func.func private @foo(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n-  %0 = stablehlo.add %arg0, %arg0 : tensor<8xf32>\n-  %1 = call @bar(%0) {mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8xf32>) -> tensor<8xf32>\n-  return %1 : tensor<8xf32>\n-}\n-\n-// CHECK-NOT: func private @bar\n-func.func private @bar(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n-  %0 = stablehlo.abs %arg0 : tensor<8xf32>\n-  return %0 : tensor<8xf32>\n-}\n-\n-// CHECK-NOT: func private @baz\n-func.func private @baz(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n-  %0 = stablehlo.abs %arg0 : tensor<8xf32>\n-  return %0 : tensor<8xf32>\n-}\n-\n-// -----\n-\n-// CHECK-LABEL: func @non_flat_call_graph_all_inlineable\n-func.func @non_flat_call_graph_all_inlineable(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n-  // CHECK-NEXT: %[[NC1:.*]] = sdy.named_computation<\"foo\">(%arg0) (%arg1: tensor<8xf32>) {\n-  // CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg1, %arg1 : tensor<8xf32>\n-  // CHECK-NEXT:   %[[NC2:.*]] = sdy.named_computation<\"bar\">(%[[ADD]]) (%arg2: tensor<8xf32>) {\n-  // CHECK-NEXT:     %[[ABS1:.*]] = stablehlo.abs %arg2 : tensor<8xf32>\n-  // CHECK-NEXT:     sdy.return %[[ABS1]] : tensor<8xf32>\n-  // CHECK-NEXT:   }\n-  // CHECK-SAME:   {mhlo.frontend_attributes = {inlineable = \"true\"}}\n-  // CHECK-SAME:   (tensor<8xf32>) -> tensor<8xf32>\n-  // CHECK-NEXT:   sdy.return %[[NC2]] : tensor<8xf32>\n-  // CHECK-NEXT: }\n-  // CHECK-SAME: {mhlo.frontend_attributes = {inlineable = \"true\"}}\n-  // CHECK-SAME: (tensor<8xf32>) -> tensor<8xf32>\n-  // CHECK-NEXT: %[[NEGATE:.*]] = stablehlo.negate %[[NC1]] : tensor<8xf32>\n-  // CHECK-NEXT: %[[NC3:.*]] = sdy.named_computation<\"baz\">(%[[NEGATE]]) (%arg1: tensor<8xf32>) {\n-  // CHECK-NEXT:   %[[ABS2:.*]] = stablehlo.abs %arg1 : tensor<8xf32>\n-  // CHECK-NEXT:   sdy.return %[[ABS2]] : tensor<8xf32>\n-  // CHECK-NEXT: }\n-  // CHECK-SAME: {mhlo.frontend_attributes = {inlineable = \"true\"}}\n-  // CHECK-SAME: (tensor<8xf32>) -> tensor<8xf32>\n-  // CHECK-NEXT: %[[NC4:.*]] = sdy.named_computation<\"bar\">(%[[NC3]]) (%arg1: tensor<8xf32>) {\n-  // CHECK-NEXT:   %[[ABS3:.*]] = stablehlo.abs %arg1 : tensor<8xf32>\n-  // CHECK-NEXT:   sdy.return %[[ABS3]] : tensor<8xf32>\n-  // CHECK-NEXT: }\n-  // CHECK-SAME: {mhlo.frontend_attributes = {inlineable = \"true\"}}\n-  // CHECK-SAME: (tensor<8xf32>) -> tensor<8xf32>\n-  // CHECK-NEXT: return %[[NC4]] : tensor<8xf32>\n-  %0 = call @foo(%arg0) {mhlo.frontend_attributes = {inlineable = \"true\"}} : (tensor<8xf32>) -> tensor<8xf32>\n-  %1 = stablehlo.negate %0 : tensor<8xf32>\n-  %2 = call @baz(%1) {mhlo.frontend_attributes = {inlineable = \"true\"}} : (tensor<8xf32>) -> tensor<8xf32>\n-  %3 = call @bar(%2) {mhlo.frontend_attributes = {inlineable = \"true\"}} : (tensor<8xf32>) -> tensor<8xf32>\n-  return %3 : tensor<8xf32>\n-}\n-\n-// CHECK-NOT: func private @foo\n-func.func private @foo(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n-  %0 = stablehlo.add %arg0, %arg0 : tensor<8xf32>\n-  %1 = call @bar(%0) {mhlo.frontend_attributes = {inlineable = \"true\"}} : (tensor<8xf32>) -> tensor<8xf32>\n-  return %1 : tensor<8xf32>\n-}\n-\n-// CHECK-NOT: func private @bar\n-func.func private @bar(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n-  %0 = stablehlo.abs %arg0 : tensor<8xf32>\n-  return %0 : tensor<8xf32>\n-}\n-\n-// CHECK-NOT: func private @baz\n-func.func private @baz(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n-  %0 = stablehlo.abs %arg0 : tensor<8xf32>\n-  return %0 : tensor<8xf32>\n-}"
        },
        {
            "sha": "398b85c4a7d5c0d4dd167550cfecd1bcb23d709a",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/sdy_round_trip_import_pipeline_import_func_calls_true.mlir",
            "status": "renamed",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fsdy_round_trip_import_pipeline_import_func_calls_true.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/88ae97c487fd2bca46e15f0ff08a0326f0478031/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fsdy_round_trip_import_pipeline_import_func_calls_true.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fsdy_round_trip_import_pipeline_import_func_calls_true.mlir?ref=88ae97c487fd2bca46e15f0ff08a0326f0478031",
            "patch": "@@ -1,4 +1,4 @@\n-// RUN: sdy_opt %s --split-input-file -xla-sdy-round-trip-import-pipeline='import-func-calls=true import-only-uninlineable-func-calls=false' 2>&1 | FileCheck %s\n+// RUN: sdy_opt %s --split-input-file -xla-sdy-round-trip-import-pipeline='import-func-calls=true' 2>&1 | FileCheck %s\n \n // CHECK-LABEL: func @non_flat_call_graph_all_inlineable\n func.func @non_flat_call_graph_all_inlineable(%arg0: tensor<8xf32>) -> tensor<8xf32> {",
            "previous_filename": "third_party/xla/xla/service/spmd/shardy/test/sdy_round_trip_import_pipeline_import_all_func_calls.mlir"
        },
        {
            "sha": "0a59dcf418abb7dad70ab18bbf781571f2fd185e",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/sdy_round_trip_import_pipeline_import_only_uninlineable_func_calls.mlir",
            "status": "removed",
            "additions": 0,
            "deletions": 48,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/616d868c1cc8a03d62a31c10a213d5d931a86fd0/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fsdy_round_trip_import_pipeline_import_only_uninlineable_func_calls.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/616d868c1cc8a03d62a31c10a213d5d931a86fd0/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fsdy_round_trip_import_pipeline_import_only_uninlineable_func_calls.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fsdy_round_trip_import_pipeline_import_only_uninlineable_func_calls.mlir?ref=616d868c1cc8a03d62a31c10a213d5d931a86fd0",
            "patch": "@@ -1,48 +0,0 @@\n-// RUN: sdy_opt %s --split-input-file -xla-sdy-round-trip-import-pipeline='import-func-calls=true' 2>&1 | FileCheck %s\n-\n-// CHECK-LABEL: func @non_flat_call_graph_all_inlineable\n-// CHECK-NOT: sdy.named_computation<\"foo\">\n-func.func @non_flat_call_graph_all_inlineable(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n-  // CHECK: %0 = call @foo(%arg0)\n-  // CHECK: %1 = stablehlo.negate %0\n-  // CHECK: %2 = call @baz(%1)\n-  // CHECK: return %2 : tensor<8xf32>\n-  %0 = call @foo(%arg0) {mhlo.frontend_attributes = {inlineable = \"true\"}} : (tensor<8xf32>) -> tensor<8xf32>\n-  %1 = stablehlo.negate %0 : tensor<8xf32>\n-  %2 = call @baz(%1) {mhlo.frontend_attributes = {inlineable = \"true\"}} : (tensor<8xf32>) -> tensor<8xf32>\n-  return %2 : tensor<8xf32>\n-}\n-\n-// CHECK: func private @foo\n-func.func private @foo(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n-  %0 = stablehlo.add %arg0, %arg0 : tensor<8xf32>\n-  %1 = call @bar(%0) {mhlo.frontend_attributes = {inlineable = \"true\"}} : (tensor<8xf32>) -> tensor<8xf32>\n-  return %1 : tensor<8xf32>\n-}\n-\n-// CHECK: func private @bar\n-func.func private @bar(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n-  %0 = stablehlo.abs %arg0 : tensor<8xf32>\n-  return %0 : tensor<8xf32>\n-}\n-\n-// CHECK: func private @baz\n-func.func private @baz(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n-  %0 = stablehlo.abs %arg0 : tensor<8xf32>\n-  return %0 : tensor<8xf32>\n-}\n-\n-// -----\n-// CHECK-LABEL: func @uninlineable_call\n-func.func @uninlineable_call(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n-  // CHECK: %0 = sdy.named_computation<\"foo\">(%arg0)\n-  // CHECK: return %0 : tensor<8xf32>\n-  %0 = call @foo(%arg0) {mhlo.frontend_attributes = {inlineable = \"false\"}} : (tensor<8xf32>) -> tensor<8xf32>\n-  return %0 : tensor<8xf32>\n-}\n-\n-// CHECK-NOT: func private @foo\n-func.func private @foo(%arg0: tensor<8xf32>) -> tensor<8xf32> {\n-  %0 = stablehlo.add %arg0, %arg0 : tensor<8xf32>\n-  return %0 : tensor<8xf32>\n-}"
        }
    ],
    "stats": {
        "total": 452,
        "additions": 73,
        "deletions": 379
    }
}