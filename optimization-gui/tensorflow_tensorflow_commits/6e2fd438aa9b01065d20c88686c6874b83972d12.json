{
    "author": "ermilovmaxim",
    "message": "Reverts 08cfeaf6e86a411d0f9605a33dc528fdc93db072\n\nPiperOrigin-RevId: 829501544",
    "sha": "6e2fd438aa9b01065d20c88686c6874b83972d12",
    "files": [
        {
            "sha": "5cc5dd637b07a9eb32ca2ded395db46812abd1fd",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 62,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6e2fd438aa9b01065d20c88686c6874b83972d12/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6e2fd438aa9b01065d20c88686c6874b83972d12/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=6e2fd438aa9b01065d20c88686c6874b83972d12",
            "patch": "@@ -137,7 +137,6 @@ limitations under the License.\n #if GOOGLE_CUDA\n #include \"third_party/gpus/cuda/include/cuda.h\"\n #include \"third_party/gpus/cuda/include/cuda_runtime_api.h\"\n-#include \"third_party/gpus/cuda/nvml/include/nvml.h\"\n #include \"xla/stream_executor/gpu/gpu_cudamallocasync_allocator.h\"\n #elif TENSORFLOW_USE_ROCM\n #include \"rocm/rocm_config.h\"\n@@ -1173,14 +1172,13 @@ absl::StatusOr<DeviceTopologyPair> BuildDistributedDevices(\n     device_proto->set_core_count(desc->core_count());\n     device_proto->set_shared_memory_per_block_optin(\n         desc->shared_memory_per_block_optin());\n-#if defined(GOOGLE_CUDA) && CUDA_VERSION >= 12040\n-    if (std::stoi(compute_capability) >= 9) {\n-      auto fabric_info = GetDeviceFabricInfo(ordinal_and_device.first);\n-      if (fabric_info.ok()) {\n-        device_proto->set_fabric_uuid(*fabric_info);\n-      }\n+\n+    stream_executor::DeviceInterconnectInfo info =\n+        desc->device_interconnect_info();\n+    if (!info.cluster_uuid.empty() && !info.clique_id.empty()) {\n+      device_proto->set_fabric_uuid(\n+          absl::StrCat(info.cluster_uuid, \"/\", info.clique_id));\n     }\n-#endif  // defined(GOOGLE_CUDA) && CUDA_VERSION >= 12040\n   }\n \n   GlobalTopologyProto global_topology;\n@@ -1444,60 +1442,6 @@ std::vector<std::unique_ptr<PjRtStreamExecutorDevice>> BuildLocalDevices(\n   return devices;\n }\n \n-absl::StatusOr<std::string> GetDeviceFabricInfo(const int device_ordinal) {\n-#if defined(GOOGLE_CUDA) && CUDA_VERSION >= 12040\n-  char pciBusId[] = \"00000000:00:00.0\";\n-  cudaDeviceGetPCIBusId(pciBusId, sizeof(pciBusId), device_ordinal);\n-  nvmlDevice_t device;\n-\n-  nvmlReturn_t get_bus_id_status =\n-      nvmlDeviceGetHandleByPciBusId_v2(pciBusId, &device);\n-  // NVML library is not a part of the CUDA toolkit, so there might be a\n-  // situation when user is using CUDA 12.4 an higher, but the host NVML\n-  // version doen't have the required functions.\n-  if (get_bus_id_status == NVML_ERROR_FUNCTION_NOT_FOUND) {\n-    return absl::InternalError(\"NVML library doesn't have required functions.\");\n-  }\n-  CHECK_EQ(get_bus_id_status, NVML_SUCCESS);\n-\n-  nvmlGpuFabricInfoV_t fabricInfo = {\n-      .version = nvmlGpuFabricInfo_v2,\n-      .state = NVML_GPU_FABRIC_STATE_NOT_SUPPORTED};\n-\n-  nvmlReturn_t get_fabric_info_status =\n-      nvmlDeviceGetGpuFabricInfoV(device, &fabricInfo);\n-  if (get_fabric_info_status == NVML_ERROR_FUNCTION_NOT_FOUND) {\n-    return absl::InternalError(\"NVML library doesn't have required functions.\");\n-  }\n-  CHECK_EQ(get_fabric_info_status, NVML_SUCCESS);\n-\n-  if (fabricInfo.state == NVML_GPU_FABRIC_STATE_NOT_SUPPORTED) {\n-    std::string error_message =\n-        \"NVML doesn't support extracting fabric info or NVLink is not used by \"\n-        \"the device.\";\n-    VLOG(2) << error_message;\n-    return absl::InternalError(error_message);\n-  }\n-\n-  CHECK_EQ(sizeof(fabricInfo.clusterUuid), 16);\n-  std::string uuid_str = absl::StrFormat(\n-      \"%02x%02x%02x%02x-%02x%02x-%02x%02x-%02x%02x-%02x%02x%02x%02x%02x%02x\",\n-      fabricInfo.clusterUuid[0], fabricInfo.clusterUuid[1],\n-      fabricInfo.clusterUuid[2], fabricInfo.clusterUuid[3],\n-      fabricInfo.clusterUuid[4], fabricInfo.clusterUuid[5],\n-      fabricInfo.clusterUuid[6], fabricInfo.clusterUuid[7],\n-      fabricInfo.clusterUuid[8], fabricInfo.clusterUuid[9],\n-      fabricInfo.clusterUuid[10], fabricInfo.clusterUuid[11],\n-      fabricInfo.clusterUuid[12], fabricInfo.clusterUuid[13],\n-      fabricInfo.clusterUuid[14], fabricInfo.clusterUuid[15]);\n-  return absl::StrCat(uuid_str, \"/\", std::to_string(fabricInfo.cliqueId));\n-#else   // defined(GOOGLE_CUDA) && CUDA_VERSION >= 12040\n-  std::string error_message = \"NVML usage is not supported\";\n-  VLOG(2) << error_message;\n-  return absl::InternalError(error_message);\n-#endif  // defined(GOOGLE_CUDA) && CUDA_VERSION >= 12040\n-}\n-\n #if defined(GOOGLE_CUDA) || defined(TENSORFLOW_USE_ROCM)\n static absl::Status CheckAlignment(const BufferAllocation& allocation,\n                                    se::DeviceMemoryBase buffer, int arg_idx) {"
        },
        {
            "sha": "4bdc951adbbd5695205037f60ac3e846b2cb3325",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client_test.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 41,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6e2fd438aa9b01065d20c88686c6874b83972d12/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6e2fd438aa9b01065d20c88686c6874b83972d12/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client_test.cc?ref=6e2fd438aa9b01065d20c88686c6874b83972d12",
            "patch": "@@ -1241,47 +1241,6 @@ TEST(StreamExecutorGpuClientTest, DistributedInit) {\n   }\n }\n \n-TEST(StreamExecutorGpuClientTest, GetDeviceFabricInfo) {\n-  auto kv_store = std::make_shared<InMemoryKeyValueStore>();\n-  tsl::thread::ThreadPool thread_pool(tsl::Env::Default(),\n-                                      \"PopulateAndRetrieveFabricInfos\", 4);\n-  constexpr int num_nodes = 2;\n-  for (int node_id = 0; node_id < num_nodes; ++node_id) {\n-    thread_pool.Schedule([kv_store, node_id] {\n-      GpuClientOptions options = DefaultOptions();\n-      options.node_id = node_id;\n-      options.num_nodes = num_nodes;\n-      options.kv_store = kv_store;\n-      TF_ASSERT_OK_AND_ASSIGN(auto client, GetStreamExecutorGpuClient(options));\n-      for (const auto& device : client->addressable_devices()) {\n-        LocalDeviceState* local_device_state =\n-            tensorflow::down_cast<const PjRtStreamExecutorDevice*>(device)\n-                ->local_device_state();\n-        if (local_device_state != nullptr) {\n-          se::StreamExecutor* executor = local_device_state->executor();\n-          if (auto* cc = executor->GetDeviceDescription()\n-                             .gpu_compute_capability()\n-                             .cuda_compute_capability()) {\n-            if (cc->IsAtLeastHopper()) {\n-              auto fabric_info =\n-                  GetDeviceFabricInfo(executor->device_ordinal());\n-              if (!fabric_info.ok()) {\n-                // Only allow failures due to insufficient CUDA driver version.\n-                EXPECT_THAT(\n-                    fabric_info.status().message(),\n-                    AnyOf(HasSubstr(\"Failed to initialize NVML library.\"),\n-                          HasSubstr(\n-                              \"NVML library doesn't have required functions.\"),\n-                          HasSubstr(\"NVML usage is not supported\")));\n-              }\n-            }\n-          }\n-        }\n-      }\n-    });\n-  }\n-}\n-\n TEST(StreamExecutorGpuClientTest, GetAllocatorStatsTest) {\n   TF_ASSERT_OK_AND_ASSIGN(auto client,\n                           GetStreamExecutorGpuClient(DefaultOptions()));"
        },
        {
            "sha": "cfa17b44eb50b0a6f59e54be94fad633e914cd62",
            "filename": "third_party/xla/xla/service/gpu/model/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6e2fd438aa9b01065d20c88686c6874b83972d12/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6e2fd438aa9b01065d20c88686c6874b83972d12/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2FBUILD?ref=6e2fd438aa9b01065d20c88686c6874b83972d12",
            "patch": "@@ -8,7 +8,6 @@ load(\"//xla/tests:build_defs.bzl\", \"xla_test\")\n load(\"//xla/tsl:tsl.bzl\", \"if_google\", \"internal_visibility\")\n load(\"//xla/tsl:tsl.default.bzl\", \"get_compatible_with_portable\")\n load(\"//xla/tsl/platform:build_config.bzl\", \"tf_proto_library\")\n-load(\"//xla/tsl/platform/default:cuda_build_defs.bzl\", \"if_cuda_is_configured\")\n \n package(\n     # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n@@ -458,7 +457,6 @@ cc_library(\n     name = \"gpu_collective_performance_model\",\n     srcs = [\"gpu_collective_performance_model.cc\"],\n     hdrs = [\"gpu_collective_performance_model.h\"],\n-    local_defines = if_cuda_is_configured([\"GOOGLE_CUDA=1\"]),\n     deps = [\n         \":coalescing_analysis\",\n         \":fusion_analysis_cache\",\n@@ -494,10 +492,7 @@ cc_library(\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:IR\",\n         \"@local_tsl//tsl/platform:status\",\n-    ] + if_cuda_is_configured([\n-        \"//xla/tsl/cuda:nvml\",\n-        \"@local_config_cuda//cuda:cuda_headers\",\n-    ]),\n+    ],\n )\n \n cc_library("
        },
        {
            "sha": "674e628960b4be47b78f52473e2372f17a3f86b4",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_collective_performance_model.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 37,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6e2fd438aa9b01065d20c88686c6874b83972d12/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6e2fd438aa9b01065d20c88686c6874b83972d12/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.cc?ref=6e2fd438aa9b01065d20c88686c6874b83972d12",
            "patch": "@@ -19,7 +19,6 @@ limitations under the License.\n #include <array>\n #include <cstdint>\n #include <cstdlib>\n-#include <variant>\n #include <vector>\n \n #include \"absl/log/check.h\"\n@@ -34,10 +33,6 @@ limitations under the License.\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/util.h\"\n \n-#if GOOGLE_CUDA\n-#include \"third_party/gpus/cuda/include/cuda.h\"\n-#include \"third_party/gpus/cuda/nvml/include/nvml.h\"\n-#endif  // GOOGLE_CUDA\n namespace xla {\n namespace gpu {\n \n@@ -334,16 +329,13 @@ absl::Duration ComputeAllreduceTimeImpl(\n           /*num_blocks=*/num_channels, /*num_threads_per_block=*/num_threads);\n   total_time += compute_time_per_channel;\n \n-  uint32_t supported_p2p =\n-      GpuPerformanceWithCollectiveModel::CheckIfNvlinkSupportsP2P();\n-\n-  if (supported_p2p == 0) {\n-    VLOG(8) << \"Nvlink doesn't support p2p communication. Model will \"\n-               \"continue using default system bandwidth.\";\n-  } else {\n+  if (gpu_device_info.device_interconnect_info().active_links) {\n     VLOG(8) << \"Nvlink supports p2p communication, setting intra node \"\n                \"bandwidth to nvlink bw.\";\n     bw_intra_node = bandwidth_settings.GetNvlinkBw();\n+  } else {\n+    VLOG(8) << \"Nvlink doesn't support p2p communication. Model will \"\n+               \"continue using default system bandwidth.\";\n   }\n \n   double bus_bandwidth = bw_intra_node * num_channels;\n@@ -375,31 +367,6 @@ RocmBandwidthSettings CreateSettings(\n \n }  // namespace\n \n-/*static*/ uint32_t\n-GpuPerformanceWithCollectiveModel::CheckIfNvlinkSupportsP2P() {\n-#if GOOGLE_CUDA\n-  // We will use nvml library to detect nvlink capability\n-  // to see if it supports p2p communication.\n-  // Then gpu 0 will be used to query for nvlink capability, note that\n-  // we only look at link 0 of gpu 0 since all other links are assumed\n-  // to have the same capability.\n-  nvmlDevice_t nvml_device;\n-  nvmlReturn_t get_device_result = nvmlDeviceGetHandleByIndex(0, &nvml_device);\n-  CHECK(get_device_result == NVML_SUCCESS);\n-\n-  uint32_t supported_p2p = 0;\n-\n-  nvmlReturn_t nvlink_cap_result = nvmlDeviceGetNvLinkCapability(\n-      nvml_device, /*nvlink link number*/ 0, NVML_NVLINK_CAP_P2P_SUPPORTED,\n-      &supported_p2p);\n-  CHECK(nvlink_cap_result == NVML_SUCCESS ||\n-        nvlink_cap_result == NVML_ERROR_NOT_SUPPORTED);\n-  return supported_p2p;\n-#else\n-  return 0;\n-#endif  // GOOGLE_CUDA\n-}\n-\n /*static*/ absl::Duration\n GpuPerformanceWithCollectiveModel::ComputeAllreduceTime(\n     const HloInstruction& instr, const GpuHloCostAnalysis* cost_analysis,"
        },
        {
            "sha": "d7cf30861a75ac8106f4440673a0c217b0b75c02",
            "filename": "third_party/xla/xla/service/gpu/model/gpu_collective_performance_model.h",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6e2fd438aa9b01065d20c88686c6874b83972d12/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6e2fd438aa9b01065d20c88686c6874b83972d12/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fmodel%2Fgpu_collective_performance_model.h?ref=6e2fd438aa9b01065d20c88686c6874b83972d12",
            "patch": "@@ -35,10 +35,6 @@ class GpuPerformanceWithCollectiveModel : public GpuPerformanceModelBase {\n       const HloInstruction& instr, const GpuHloCostAnalysis* cost_analysis,\n       const se::DeviceDescription& gpu_device_info);\n \n-  // This checks if the nvlink supports direct P2P communication,\n-  // If not, we will use PCIE bandwidth to estimate latency.\n-  static uint32_t CheckIfNvlinkSupportsP2P();\n-\n  private:\n   static absl::Duration ComputeAllreduceTime(\n       const HloInstruction& instr, const GpuHloCostAnalysis* cost_analysis,"
        },
        {
            "sha": "b4dad0a94583a927e352ee099bf54362317058b9",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.cc",
            "status": "modified",
            "additions": 121,
            "deletions": 26,
            "changes": 147,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6e2fd438aa9b01065d20c88686c6874b83972d12/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6e2fd438aa9b01065d20c88686c6874b83972d12/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc?ref=6e2fd438aa9b01065d20c88686c6874b83972d12",
            "patch": "@@ -692,29 +692,38 @@ absl::StatusOr<CUmulticastObjectProp> CreateMulticastObjectProperties(\n   return multicast_properties;\n }\n \n-absl::StatusOr<int64_t> GetDevicePcieBandwidth(int device_ordinal) {\n-  nvmlDevice_t nvml_device;\n-  nvmlReturn_t result =\n-      nvmlDeviceGetHandleByIndex(device_ordinal, &nvml_device);\n-  if (result != NVML_SUCCESS) {\n-    return absl::InternalError(\n-        absl::StrCat(\"nvmlDeviceGetHandleByIndex failed with \", result));\n+absl::Status ToStatus(nvmlReturn_t result) {\n+  if (result == NVML_SUCCESS) {\n+    return absl::OkStatus();\n+  }\n+  // NVML library is not a part of the CUDA toolkit, so there might be a\n+  // situation when user is using newer CUDA, but the host NVML\n+  // version doen't have the required functions.\n+  if (result == NVML_ERROR_FUNCTION_NOT_FOUND) {\n+    return absl::InternalError(\"NVML library doesn't have required functions.\");\n   }\n+  return absl::InternalError(absl::StrFormat(\"Nvml call failed with %d(%s).\",\n+                                             result, nvmlErrorString(result)));\n+}\n+\n+// CUDA and Nvml can have different device ordering.\n+absl::StatusOr<nvmlDevice_t> GetNvmlDevice(const std::string& pci_bus_id) {\n+  nvmlDevice_t device;\n+  TF_RETURN_IF_ERROR(\n+      ToStatus(nvmlDeviceGetHandleByPciBusId_v2(pci_bus_id.c_str(), &device)));\n+  return device;\n+}\n \n+absl::StatusOr<int64_t> GetDevicePcieBandwidth(nvmlDevice_t nvml_device) {\n   // nvmlDeviceGetPcieSpeed returns wrong information. Verified with\n   // nvbandwidth.\n   unsigned int link_gen, link_width;\n-  result = nvmlDeviceGetCurrPcieLinkGeneration(nvml_device, &link_gen);\n-  if (result != NVML_SUCCESS) {\n-    return absl::InternalError(absl::StrCat(\n-        \"nvmlDeviceGetCurrPcieLinkGeneration failed with \", result));\n-  }\n+  nvmlReturn_t result =\n+      nvmlDeviceGetCurrPcieLinkGeneration(nvml_device, &link_gen);\n+  TF_RETURN_IF_ERROR(ToStatus(result));\n \n   result = nvmlDeviceGetCurrPcieLinkWidth(nvml_device, &link_width);\n-  if (result != NVML_SUCCESS) {\n-    return absl::InternalError(\n-        absl::StrCat(\"nvmlDeviceGetCurrPcieLinkWidth failed with \", result));\n-  }\n+  TF_RETURN_IF_ERROR(ToStatus(result));\n \n   // PCIe v1 single lane speed. 0.25 GB/s\n   int64_t lane_speed = 0.25 * 1024 * 1024 * 1024;\n@@ -725,6 +734,75 @@ absl::StatusOr<int64_t> GetDevicePcieBandwidth(int device_ordinal) {\n   return lane_speed * link_width;\n }\n \n+absl::StatusOr<int> GetNumberOfActiveP2PNvlinks(nvmlDevice_t nvml_device) {\n+  int p2p_links = 0;\n+\n+  constexpr int kBlackwellNvLinkCount = 18;\n+  for (unsigned int i = 0; i < kBlackwellNvLinkCount; i++) {\n+    nvmlEnableState_t is_active = NVML_FEATURE_DISABLED;\n+    nvmlReturn_t result = nvmlDeviceGetNvLinkState(nvml_device, i, &is_active);\n+    if (result == NVML_ERROR_NOT_SUPPORTED) {\n+      break;\n+    }\n+    TF_RETURN_IF_ERROR(ToStatus(result));\n+    if (is_active == NVML_FEATURE_DISABLED) {\n+      break;\n+    }\n+\n+    uint32_t supported_p2p = 0;\n+    result = nvmlDeviceGetNvLinkCapability(\n+        nvml_device, i, NVML_NVLINK_CAP_P2P_SUPPORTED, &supported_p2p);\n+    if (result != NVML_ERROR_NOT_SUPPORTED) {\n+      TF_RETURN_IF_ERROR(ToStatus(result));\n+    }\n+    if (supported_p2p) {\n+      ++p2p_links;\n+    }\n+  }\n+  return p2p_links;\n+}\n+\n+struct FabricInfo {\n+  std::string cluster_uuid;\n+  std::string clique_id;\n+};\n+\n+absl::StatusOr<FabricInfo> GetDeviceFabricInfo(nvmlDevice_t device) {\n+#if CUDA_VERSION >= 12040\n+  nvmlGpuFabricInfoV_t fabricInfo{nvmlGpuFabricInfo_v2};\n+  fabricInfo.state = NVML_GPU_FABRIC_STATE_NOT_SUPPORTED;\n+\n+  nvmlReturn_t result = nvmlDeviceGetGpuFabricInfoV(device, &fabricInfo);\n+  TF_RETURN_IF_ERROR(ToStatus(result));\n+\n+  if (fabricInfo.state == NVML_GPU_FABRIC_STATE_NOT_SUPPORTED) {\n+    std::string error_message =\n+        \"NVML doesn't support extracting fabric info or NVLink is not used by \"\n+        \"the device.\";\n+    VLOG(2) << error_message;\n+    return absl::InternalError(error_message);\n+  }\n+\n+  static_assert(sizeof(fabricInfo.clusterUuid) == 16);\n+  std::string uuid_str = absl::StrFormat(\n+      \"%02x%02x%02x%02x-%02x%02x-%02x%02x-%02x%02x-%02x%02x%02x%02x%02x%02x\",\n+      fabricInfo.clusterUuid[0], fabricInfo.clusterUuid[1],\n+      fabricInfo.clusterUuid[2], fabricInfo.clusterUuid[3],\n+      fabricInfo.clusterUuid[4], fabricInfo.clusterUuid[5],\n+      fabricInfo.clusterUuid[6], fabricInfo.clusterUuid[7],\n+      fabricInfo.clusterUuid[8], fabricInfo.clusterUuid[9],\n+      fabricInfo.clusterUuid[10], fabricInfo.clusterUuid[11],\n+      fabricInfo.clusterUuid[12], fabricInfo.clusterUuid[13],\n+      fabricInfo.clusterUuid[14], fabricInfo.clusterUuid[15]);\n+\n+  return FabricInfo{uuid_str, absl::StrCat(fabricInfo.cliqueId)};\n+#else   // CUDA_VERSION >= 12040\n+  std::string error_message = \"NVML usage is not supported\";\n+  VLOG(2) << error_message;\n+  return absl::InternalError(error_message);\n+#endif  // CUDA_VERSION >= 12040\n+}\n+\n }  // namespace\n \n // Given const GPU memory, returns a libcuda device pointer datatype, suitable\n@@ -1621,10 +1699,10 @@ CudaExecutor::CreateDeviceDescription(int device_ordinal) {\n   });\n   cudnn_version_ready.WaitForNotification();\n \n-  {\n-    std::string pci_bus_id = GetPCIBusID(device);\n-    desc.set_pci_bus_id(pci_bus_id);\n+  std::string pci_bus_id = GetPCIBusID(device);\n+  desc.set_pci_bus_id(pci_bus_id);\n \n+  {\n     // Read the NUMA node corresponding to the PCI bus ID out of sysfs.\n     std::optional<int> numa_node = ReadNumaNode(pci_bus_id, device_ordinal);\n     // If the kernel reports -1, adjust to 0; leave as -1 if no value could be\n@@ -1677,16 +1755,33 @@ CudaExecutor::CreateDeviceDescription(int device_ordinal) {\n                               int64_t{mem_bus_width_bits.value()} / 8);\n   }\n \n-  {\n-    absl::StatusOr<int64_t> status_or_bandwidth =\n-        GetDevicePcieBandwidth(device_ordinal);\n-    if (status_or_bandwidth.ok()) {\n-      desc.set_pcie_bandwidth(*status_or_bandwidth);\n+  if (absl::StatusOr<nvmlDevice_t> device = GetNvmlDevice(pci_bus_id);\n+      device.ok()) {\n+    absl::StatusOr<int64_t> bandwidth = GetDevicePcieBandwidth(*device);\n+    if (bandwidth.ok()) {\n+      desc.set_pcie_bandwidth(*bandwidth);\n     } else {\n-      LOG(ERROR) << status_or_bandwidth.status().message()\n+      LOG(ERROR) << bandwidth.status().message()\n                  << \" Assuming PCIe gen 3 x16 bandwidth.\";\n-      status_or_bandwidth = 16LL * 1024 * 1024 * 1024;\n+      bandwidth = 16LL * 1024 * 1024 * 1024;\n+    }\n+\n+    absl::StatusOr<int64_t> p2p_link_count =\n+        GetNumberOfActiveP2PNvlinks(*device);\n+    DeviceInterconnectInfo info;\n+    if (p2p_link_count.ok()) {\n+      info.active_links = *p2p_link_count;\n+    } else {\n+      LOG(ERROR) << p2p_link_count;\n+    }\n+    absl::StatusOr<FabricInfo> fabric_info = GetDeviceFabricInfo(*device);\n+    if (fabric_info.ok()) {\n+      info.cluster_uuid = fabric_info->cluster_uuid;\n+      info.clique_id = fabric_info->clique_id;\n+    } else {\n+      LOG(ERROR) << fabric_info.status();\n     }\n+    desc.set_device_interconnect_info(info);\n   }\n \n   {"
        },
        {
            "sha": "e436ee29a94fd9c4b10e9a5a2b17983ca26908d4",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6e2fd438aa9b01065d20c88686c6874b83972d12/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6e2fd438aa9b01065d20c88686c6874b83972d12/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_test.cc?ref=6e2fd438aa9b01065d20c88686c6874b83972d12",
            "patch": "@@ -72,6 +72,15 @@ TEST(CudaExecutorTest, CreateDeviceDescription) {\n \n   EXPECT_THAT(*result->gpu_compute_capability().cuda_compute_capability(),\n               ::testing::Field(\"major\", &CudaComputeCapability::major, Ge(1)));\n+\n+  DeviceInterconnectInfo info = result->device_interconnect_info();\n+  if (result->cuda_compute_capability().IsAtLeastBlackwell() &&\n+      info.active_links) {\n+    EXPECT_GE(info.active_links, 18);\n+\n+    EXPECT_THAT(info.clique_id, Not(IsEmpty()));\n+    EXPECT_THAT(info.cluster_uuid, Not(IsEmpty()));\n+  }\n }\n \n TEST(CudaExecutorTest, GetCudaKernel) {"
        },
        {
            "sha": "39962e7dda737ff17867432a3ec3b3f2d0301e66",
            "filename": "third_party/xla/xla/stream_executor/device_description.h",
            "status": "modified",
            "additions": 22,
            "deletions": 1,
            "changes": 23,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6e2fd438aa9b01065d20c88686c6874b83972d12/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_description.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6e2fd438aa9b01065d20c88686c6874b83972d12/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_description.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdevice_description.h?ref=6e2fd438aa9b01065d20c88686c6874b83972d12",
            "patch": "@@ -98,6 +98,16 @@ class GpuComputeCapability {\n       compute_capability_;\n };\n \n+// Information about NVLink/UALink.\n+struct DeviceInterconnectInfo {\n+  int active_links = 0;\n+\n+  // Uuid of the cluster to which this GPU belongs.\n+  std::string cluster_uuid;\n+  // ID of the fabric clique to which this GPU belongs.\n+  std::string clique_id;\n+};\n+\n // Data that describes the execution target of the StreamExecutor, in terms of\n // important logical parameters. These include dimensionality limits and\n // physical parameters of interest, such as number of cores present on the\n@@ -106,6 +116,8 @@ class GpuComputeCapability {\n // Thread-safe: immutable post-initialization.\n class DeviceDescription {\n  public:\n+  DeviceDescription() = default;\n+\n   // Returns the platform being run on; this value is primarily intended for\n   // printing, and comes out something like \"OpenCL 1.2\" or \"Compute Capability\n   // 3.5\".\n@@ -290,11 +302,14 @@ class DeviceDescription {\n     return 32;\n   }\n \n+  const DeviceInterconnectInfo& device_interconnect_info() const {\n+    return interconnect_info_;\n+  }\n+\n   GpuDeviceInfoProto ToGpuProto() const;\n \n   std::string ToString() const;\n \n-  DeviceDescription() = default;\n   static absl::StatusOr<DeviceDescription> FromProto(\n       const GpuDeviceInfoProto& proto);\n \n@@ -383,6 +398,10 @@ class DeviceDescription {\n   void set_fpus_per_core(int value) { fpus_per_core_ = value; }\n   void set_ecc_enabled(bool value) { ecc_enabled_ = value; }\n \n+  void set_device_interconnect_info(DeviceInterconnectInfo info) {\n+    interconnect_info_ = std::move(info);\n+  }\n+\n  private:\n   // For description of the following members, see the corresponding accessor\n   // above.\n@@ -435,6 +454,8 @@ class DeviceDescription {\n   SemanticVersion runtime_version_{0, 0, 0};\n   SemanticVersion compile_time_toolkit_version_{0, 0, 0};\n   SemanticVersion dnn_version_{0, 0, 0};\n+\n+  DeviceInterconnectInfo interconnect_info_;\n };\n \n // Returns whether the given thread_dim is acceptable given the limits described"
        }
    ],
    "stats": {
        "total": 340,
        "additions": 163,
        "deletions": 177
    }
}