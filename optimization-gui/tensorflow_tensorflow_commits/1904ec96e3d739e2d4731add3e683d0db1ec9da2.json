{
    "author": "WillFroom",
    "message": "[XLA:CPU][XTile] Register XlaTritonDialect in CPU fusion compiler.\n\nPiperOrigin-RevId: 837504168",
    "sha": "1904ec96e3d739e2d4731add3e683d0db1ec9da2",
    "files": [
        {
            "sha": "56d8c9ce8e9aec4c38572b894bec5aff72794030",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1904ec96e3d739e2d4731add3e683d0db1ec9da2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1904ec96e3d739e2d4731add3e683d0db1ec9da2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2FBUILD?ref=1904ec96e3d739e2d4731add3e683d0db1ec9da2",
            "patch": "@@ -64,5 +64,6 @@ cc_library(\n         \"@llvm-project//mlir:IR\",\n     ] + if_cuda_or_rocm_is_configured([\n         \"//xla/backends/gpu/codegen/triton:fusion_emitter\",\n+        \"//xla/backends/gpu/codegen/triton/ir:triton_xla\",\n     ]),\n )"
        },
        {
            "sha": "3e4fb469d3245336de6fdc3b71b8425af412d514",
            "filename": "third_party/xla/xla/backends/cpu/codegen/tiled/tiled_fusion_emitter.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1904ec96e3d739e2d4731add3e683d0db1ec9da2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_fusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1904ec96e3d739e2d4731add3e683d0db1ec9da2/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_fusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fcodegen%2Ftiled%2Ftiled_fusion_emitter.cc?ref=1904ec96e3d739e2d4731add3e683d0db1ec9da2",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"mlir/IR/MLIRContext.h\"\n #include \"xla/backends/cpu/codegen/kernel_api_ir_builder.h\"\n #include \"xla/backends/gpu/codegen/triton/fusion_emitter.h\"\n+#include \"xla/backends/gpu/codegen/triton/ir/triton_xla_ops.h\"\n #include \"xla/codegen/emitters/ir/xla_ops.h\"\n #include \"xla/codegen/emitters/kernel_api_builder.h\"\n #include \"xla/codegen/kernel_definition.h\"\n@@ -199,6 +200,10 @@ absl::StatusOr<KernelDefinition<MlirKernelSource>> EmitTiledFusionKernel(\n     mlir::MLIRContext& context, const HloFusionInstruction& fusion,\n     const BufferAssignment* buffer_assignment, absl::string_view name,\n     int64_t num_work_groups, absl::Span<const FlatTiling> tiling) {\n+  // TODO(willfroom): Remove this once the tiled emitter is untangled from\n+  // triton.\n+  context.loadDialect<mlir::triton::xla::XlaTritonDialect>();\n+\n   gpu::BlockLevelParameters block_level_parameters;\n   for (const auto& tile_sizes : tiling) {\n     block_level_parameters.output_tile_sizes.emplace_back(tile_sizes.begin(),"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 6,
        "deletions": 0
    }
}