{
    "author": "mtsokol",
    "message": "PR #33200: Docs: Error 0101\n\nImported from GitHub PR https://github.com/openxla/xla/pull/33200\n\nAfter #32628\n\nðŸš€ Kind of Contribution\n\nðŸ“š Documentation\n\nCopybara import of the project:\n\n--\n3a2278135c6b497ed2a7b9332192e844c473d4e2 by Mateusz SokÃ³Å‚ <mat646@gmail.com>:\n\nDoc page for Error 0101\n\n--\n11903d433e7f9b8265c3c2ec0d29608b536d92d4 by Mateusz SokÃ³Å‚ <8431159+mtsokol@users.noreply.github.com>:\n\nTweak note\n\nMerging this change closes #33200\n\nPiperOrigin-RevId: 834436478",
    "sha": "d3b24701e58fa05aadd5d84861ec0a91fc36122d",
    "files": [
        {
            "sha": "c196c9024507dacf262fe1d8a82d12f1bd54898f",
            "filename": "third_party/xla/docs/error_codes.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d3b24701e58fa05aadd5d84861ec0a91fc36122d/third_party%2Fxla%2Fdocs%2Ferror_codes.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d3b24701e58fa05aadd5d84861ec0a91fc36122d/third_party%2Fxla%2Fdocs%2Ferror_codes.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Ferror_codes.md?ref=d3b24701e58fa05aadd5d84861ec0a91fc36122d",
            "patch": "@@ -3,4 +3,5 @@\n This page is a list of all error codes emitted by the XLA compiler.\n \n -   [E0100](./errors/error_0100.md)\n+-   [E0101](./errors/error_0101.md)\n -   [E0102](./errors/error_0102.md)"
        },
        {
            "sha": "aa239d4cfce7e3152640c7cdbbbf19690b825a1e",
            "filename": "third_party/xla/docs/errors/error_0101.md",
            "status": "added",
            "additions": 66,
            "deletions": 0,
            "changes": 66,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d3b24701e58fa05aadd5d84861ec0a91fc36122d/third_party%2Fxla%2Fdocs%2Ferrors%2Ferror_0101.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d3b24701e58fa05aadd5d84861ec0a91fc36122d/third_party%2Fxla%2Fdocs%2Ferrors%2Ferror_0101.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Ferrors%2Ferror_0101.md?ref=d3b24701e58fa05aadd5d84861ec0a91fc36122d",
            "patch": "@@ -0,0 +1,66 @@\n+# Error code: 0101\n+\n+**Category:** Program allocation failure\n+\n+**Type:** Runtime\n+\n+## Error log example\n+\n+```\n+XlaRuntimeError: RESOURCE_EXHAUSTED: Error loading program 'jit_embedding_pipeline_step_fn': Attempting to reserve 29.49G at the bottom of memory. That was not possible. There are 147.64M free, 0B reserved, and 147.64M reservable. Scope: unknown..: while running replica 0 and partition 34 of a replicated computation (other replicas may have failed as well).\n+```\n+\n+## Why do these happen?\n+\n+This error indicates that the XLA runtime on a TPU device failed to load a\n+compiled XLA program executable into the TPU's HBM. It typically occurs for one\n+of the following reasons: - Program Size Exceeds Available HBM: The compiled XLA\n+program, including its instructions, static data, and any embedded constants, is\n+larger than the total amount of free HBM currently available on the specific TPU\n+core(s) where the program is being loaded. - HBM Fragmentation: While the total\n+free HBM on the device might be sufficient in aggregate, it is not available in\n+a single, contiguous block large enough to fit the entire program.\n+\n+It's important to understand how the TPU runtime prioritizes memory. Buffer\n+allocations are privileged over loaded programs. If a buffer allocation fails,\n+the runtime will evict already loaded programs from HBM to free up space. This\n+can lead to a situation where a program that loaded successfully before now\n+fails with an OOM error, because the HBM is now occupied with more data buffers.\n+\n+## How can a user fix their program when they do happen?\n+\n+-   Reduce Buffer Memory Footprint: Freeing up memory used by data buffers will\n+    leave more room for the program itself:\n+    -   Decrease Batch Size: This is one of the most effective ways to reduce\n+        the amount of memory used for activations.\n+    -   Parameter Sharding: For very large models, use model parallelism or\n+        sharding techniques (like FSDP or Megascale) to distribute the model's\n+        parameters and computation across multiple TPU cores or hosts.\n+    -   Shorten Sequence/Context Length: For models processing sequential data\n+        (e.g., NLP models), reducing the sequence length can significantly\n+        decrease memory usage.\n+    -   Buffer Donation: Use framework features (e.g., `jax.jit(...,\n+        donate_argnums=...)`) to allow XLA to reuse the memory of input buffers\n+        for storing output, reducing peak memory usage.\n+-   Reduce programâ€™s memory requirements for temporaries:\n+    -   Reduce programs memory usage for temporaries by using the\n+        `tpu_shared_memory_percent` flag. Note that this might negatively affect\n+        performance.\n+-   Optimize Execution Strategy/Reduce Serving load:\n+    -   Manage Program Loading: If you are JIT-compiling multiple functions, be\n+        aware that each function can result in a program being loaded. Try to\n+        structure your workload to minimize the number of concurrently loaded\n+        programs.\n+-   Ensure no memory leaks:\n+    -   Ensure references to `jax.Array` objects are not being held longer than\n+        intended. Holding on to `jax.Array` objects might prevent automatic\n+        de-allocation even after program compilation is completed.\n+\n+## How can a user debug these failures?\n+\n+-   Enable the `tpu_log_allocations_on_oom` flag for which the allocator will\n+    dump a detailed report of all current allocations when an OOM occurs, which\n+    can be invaluable for debugging.\n+-   Profile Your Program: Use the JAX memory profiler or the TensorFlow profiler\n+    to get a detailed view of your program's memory usage over time. This can\n+    help identify unexpected peaks in memory consumption."
        }
    ],
    "stats": {
        "total": 67,
        "additions": 67,
        "deletions": 0
    }
}