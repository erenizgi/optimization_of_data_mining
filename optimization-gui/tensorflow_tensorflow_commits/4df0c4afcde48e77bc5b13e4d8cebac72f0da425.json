{
    "author": "basioli-k",
    "message": "[XLA][codegen] Emit ReshapeToScalar as a tensor.extract op, and lower to triton specific impl.\n\nPiperOrigin-RevId: 826027204",
    "sha": "4df0c4afcde48e77bc5b13e4d8cebac72f0da425",
    "files": [
        {
            "sha": "3eeef38fb34603273991195ea1a32527dd2d2b93",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 29,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4df0c4afcde48e77bc5b13e4d8cebac72f0da425/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4df0c4afcde48e77bc5b13e4d8cebac72f0da425/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=4df0c4afcde48e77bc5b13e4d8cebac72f0da425",
            "patch": "@@ -579,37 +579,14 @@ SmallVector<Value> GetRuntimeValues(\n // Reshapes a non-0D tensor of shape [1, 1, 1, ...] to a scalar.\n ScalarOrTensor ReshapeTensorToScalar(EmitterLocOpBuilder b, Value input) {\n   auto element_type = mlir::cast<ShapedType>(input.getType()).getElementType();\n+  auto shaped_type = mlir::cast<ShapedType>(input.getType());\n \n-  // First, reshape to a 1D tensor if not already the case. This is needed\n-  // because triton::ReduceOp can only reduce 1 dimension at a time.\n-  auto single_dim_tensor = input;\n-  if (mlir::cast<ShapedType>(input.getType()).getRank() > 1) {\n-    Type output_tensor_type = mlir::RankedTensorType::get({1}, element_type);\n-    single_dim_tensor = b.create<ttir::ReshapeOp>(output_tensor_type, input,\n-                                                  /*allow_reorder=*/true);\n-  }\n-\n-  // Second, reduce to a scalar.\n-  ttir::ReduceOp reduction =\n-      b.create<ttir::ReduceOp>(single_dim_tensor, /*axis*/ 0);\n-\n-  mlir::Location loc = b.getLoc();\n-  mlir::Block* reducer = b.createBlock(\n-      &reduction->getRegion(0), /*insertPt=*/{},\n-      /*argTypes=*/{element_type, element_type}, /*locs=*/{loc, loc});\n-\n-  b.setInsertionPointToStart(reducer);\n-  Value result = mlir::isa<mlir::IntegerType>(element_type)\n-                     ? b.create<arith::AddIOp>(reducer->getArgument(0),\n-                                               reducer->getArgument(1))\n-                           .getResult()\n-                     : b.create<arith::AddFOp>(reducer->getArgument(0),\n-                                               reducer->getArgument(1))\n-                           .getResult();\n-  b.create<ttir::ReduceReturnOp>(SmallVector<Value>({result}));\n-  b.setInsertionPointAfter(reduction);\n+  SmallVector<Value> zero_indices;\n+  zero_indices.assign(shaped_type.getRank(),\n+                      b.create<mlir::arith::ConstantIndexOp>(0));\n \n-  return ScalarOrTensor(reduction.getResult().front());\n+  return ScalarOrTensor(\n+      b.create<mlir::tensor::ExtractOp>(element_type, input, zero_indices));\n }\n \n absl::StatusOr<ScalarOrTensor> EmitTiledReshape(EmitterLocOpBuilder b,"
        },
        {
            "sha": "ad548eae6ee38a01e713e06bb260a16f86fc177b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 3,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4df0c4afcde48e77bc5b13e4d8cebac72f0da425/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4df0c4afcde48e77bc5b13e4d8cebac72f0da425/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=4df0c4afcde48e77bc5b13e4d8cebac72f0da425",
            "patch": "@@ -2895,14 +2895,24 @@ ENTRY entry_computation {\n           \"num_ctas\":\"1\",\n           \"num_stages\":\"1\"}}}\n })\";\n-  TF_EXPECT_OK(\n-      CreateTritonIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto xtile_module_and_hlo_module,\n+      CreateXTileIrAndFileCheck(this, kHloText, \"triton_computation\", R\"(\n+CHECK:     tensor.extract %{{.*}}[%{{.*}}, %{{.*}}, %{{.*}}, %{{.*}}] : tensor<1x1x1x1xf32>\n+CHECK:     tensor.extract %{{.*}}[%{{.*}}] : tensor<1xf32>\n+CHECK:     xtile.insert {{.*}} : tensor<f32>\n+)\"));\n+\n+  TF_ASSERT_OK(LowerXTileIrToTritonAndFileCheck(\n+      this, xtile_module_and_hlo_module.first.get(), R\"(\n CHECK:     tt.reshape\n CHECK:     tt.reduce{{.*}}axis = 0\n CHECK-NOT: tt.reshape\n CHECK:     tt.reduce{{.*}}axis = 0\n CHECK:     xtile.insert {{.*}} : tensor<f32>\n-)\"));\n+  )\",\n+      GetFusionInstruction(*xtile_module_and_hlo_module.second,\n+                           \"triton_computation\")));\n \n   EXPECT_TRUE(RunAndCompareNoHloPasses(kHloText, kExactMatch));\n }"
        },
        {
            "sha": "a0aef797c16db40635da7c2fa7e3f55032ae1c4e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tensor_lower_to_triton.cc",
            "status": "modified",
            "additions": 68,
            "deletions": 1,
            "changes": 69,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4df0c4afcde48e77bc5b13e4d8cebac72f0da425/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftensor_lower_to_triton.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4df0c4afcde48e77bc5b13e4d8cebac72f0da425/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftensor_lower_to_triton.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftensor_lower_to_triton.cc?ref=4df0c4afcde48e77bc5b13e4d8cebac72f0da425",
            "patch": "@@ -17,9 +17,13 @@ limitations under the License.\n #include <memory>\n #include <utility>\n \n+#include \"absl/algorithm/container.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/Diagnostics.h\"\n+#include \"mlir/IR/Location.h\"\n #include \"mlir/IR/PatternMatch.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"mlir/Support/LLVM.h\"\n@@ -50,6 +54,68 @@ class LowerBitcast : public mlir::OpRewritePattern<tensor::BitcastOp> {\n   }\n };\n \n+class LowerExtractOnOneElementTensorToReshapeReduce\n+    : public mlir::OpRewritePattern<tensor::ExtractOp> {\n+ public:\n+  using OpRewritePattern::OpRewritePattern;\n+\n+ private:\n+  mlir::LogicalResult matchAndRewrite(\n+      tensor::ExtractOp op, mlir::PatternRewriter& rewriter) const override {\n+    auto input_tensor_type = op.getTensor().getType();\n+    auto input_tensor_shape = input_tensor_type.getShape();\n+\n+    if (input_tensor_shape.empty() ||\n+        !absl::c_all_of(input_tensor_shape,\n+                        [](int64_t dim) { return dim == 1; })) {\n+      return rewriter.notifyMatchFailure(\n+          op,\n+          \"Extract will only be lowered for tensors with all dimensions equal \"\n+          \"to 1.\");\n+    }\n+\n+    // First, reshape to a 1D tensor if not already the case. This is needed\n+    // because triton::ReduceOp can only reduce 1 dimension at a time.\n+    auto single_dim_tensor = op.getTensor();\n+    if (input_tensor_type.getRank() > 1) {\n+      Type output_tensor_type =\n+          mlir::RankedTensorType::get({1}, input_tensor_type.getElementType());\n+      single_dim_tensor = ttir::ReshapeOp::create(\n+          rewriter, op.getLoc(), output_tensor_type, single_dim_tensor,\n+          /*allow_reorder=*/true);\n+    }\n+\n+    // Second, reduce to a scalar.\n+    ttir::ReduceOp reduction = ttir::ReduceOp::create(\n+        rewriter, op.getLoc(), single_dim_tensor, /*axis=*/0);\n+\n+    auto element_type = input_tensor_type.getElementType();\n+    mlir::Location loc = op.getLoc();\n+    mlir::Block* reducer =\n+        rewriter.createBlock(&reduction->getRegion(0), /*insertPt=*/{},\n+                             /*argTypes=*/\n+                             {element_type, element_type},\n+                             /*locs=*/{loc, loc});\n+\n+    rewriter.setInsertionPointToStart(reducer);\n+    Value result = mlir::isa<mlir::IntegerType>(element_type)\n+                       ? arith::AddIOp::create(\n+                             rewriter, reducer->getArgument(0).getLoc(),\n+                             reducer->getArgument(0), reducer->getArgument(1))\n+                             .getResult()\n+                       : arith::AddFOp::create(\n+                             rewriter, reducer->getArgument(0).getLoc(),\n+                             reducer->getArgument(0), reducer->getArgument(1))\n+                             .getResult();\n+    ttir::ReduceReturnOp::create(rewriter, result.getLoc(),\n+                                 SmallVector<Value>({result}));\n+    rewriter.setInsertionPointAfter(reduction);\n+    rewriter.replaceOp(op, reduction);\n+\n+    return mlir::success();\n+  }\n+};\n+\n // TODO(basioli): Consider fusing this with the stablehlo lowering pass into a\n // single xtile to triton lowering pass.\n class TensorLowerToTritonPass\n@@ -58,7 +124,8 @@ class TensorLowerToTritonPass\n   void runOnOperation() override {\n     mlir::MLIRContext* mlir_context = &getContext();\n     mlir::RewritePatternSet patterns(mlir_context);\n-    patterns.add<LowerBitcast>(mlir_context);\n+    patterns.add<LowerBitcast, LowerExtractOnOneElementTensorToReshapeReduce>(\n+        mlir_context);\n \n     if (mlir::failed(\n             mlir::applyPatternsGreedily(getOperation(), std::move(patterns)))) {"
        },
        {
            "sha": "01b3e0b3c6410c3ea4e3bab606b324324569605b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/tensor_to_triton_lowering.mlir",
            "status": "modified",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4df0c4afcde48e77bc5b13e4d8cebac72f0da425/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftensor_to_triton_lowering.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4df0c4afcde48e77bc5b13e4d8cebac72f0da425/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftensor_to_triton_lowering.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftensor_to_triton_lowering.mlir?ref=4df0c4afcde48e77bc5b13e4d8cebac72f0da425",
            "patch": "@@ -11,3 +11,27 @@ func.func @lower_bitcast(%arg0: tensor<2x4x8xf32>) -> tensor<2x4x8xi32> {\n   // CHECK: return %[[RES]] : tensor<2x4x8xi32>\n   return %0 : tensor<2x4x8xi32>\n }\n+\n+// CHECK: func @lower_extract_on_one_element_tensor(%[[ARG:.*]]: tensor<1x1x1xf32>) -> f32\n+func.func @lower_extract_on_one_element_tensor(%arg0: tensor<1x1x1xf32>) -> f32 {\n+  %c0 = arith.constant 0 : index\n+  // CHECK: %[[RESHAPE:.*]] = tt.reshape %[[ARG]] allow_reorder : tensor<1x1x1xf32> -> tensor<1xf32>\n+  // CHECK: %[[RES:.*]] = \"tt.reduce\"(%[[RESHAPE]]) <{axis = 0 : i32}> ({\n+  // CHECK:   ^bb0(%[[ARG1:.*]]: f32, %[[ARG2:.*]]: f32):\n+  // CHECK:     %[[ADD:.*]] = arith.addf %[[ARG1]], %[[ARG2]] : f32\n+  // CHECK:     tt.reduce.return %[[ADD]] : f32\n+  // CHECK:   }) : (tensor<1xf32>) -> f32 \n+  %0 = tensor.extract %arg0[%c0, %c0, %c0] : tensor<1x1x1xf32>\n+  // CHECK: return %[[RES]] : f32\n+  return %0 : f32\n+}\n+\n+\n+// CHECK: func @lower_extract_on_multiple_element_tensor_falls_back_to_tensor(%[[ARG:.*]]: tensor<1x1x3xf32>) -> f32\n+func.func @lower_extract_on_multiple_element_tensor_falls_back_to_tensor(%arg0: tensor<1x1x3xf32>) -> f32 {\n+  %c0 = arith.constant 0 : index\n+  // CHECK: %[[RES:.*]] = tensor.extract %[[ARG]][%c0, %c0, %c0] : tensor<1x1x3xf32>\n+  %0 = tensor.extract %arg0[%c0, %c0, %c0] : tensor<1x1x3xf32>\n+  // CHECK: return %[[RES]] : f32\n+  return %0 : f32\n+}\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 144,
        "additions": 111,
        "deletions": 33
    }
}