{
    "author": "tensorflower-gardener",
    "message": "Integrate LLVM at llvm/llvm-project@bfee9db78577\n\nUpdates LLVM usage to match\n[bfee9db78577](https://github.com/llvm/llvm-project/commit/bfee9db78577)\n\nPiperOrigin-RevId: 820396282",
    "sha": "5863476a0520b5dc43e903314e0e41a6aae63b01",
    "files": [
        {
            "sha": "ba4c1a258447ae5930ff5d991e188b1eaeb98aed",
            "filename": "third_party/xla/third_party/llvm/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5863476a0520b5dc43e903314e0e41a6aae63b01/third_party%2Fxla%2Fthird_party%2Fllvm%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5863476a0520b5dc43e903314e0e41a6aae63b01/third_party%2Fxla%2Fthird_party%2Fllvm%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fllvm%2Fworkspace.bzl?ref=5863476a0520b5dc43e903314e0e41a6aae63b01",
            "patch": "@@ -4,8 +4,8 @@ load(\"//third_party:repo.bzl\", \"tf_http_archive\")\n \n def repo(name):\n     \"\"\"Imports LLVM.\"\"\"\n-    LLVM_COMMIT = \"267fa8dd1efce0b79ebcaa804d54542c99918df2\"\n-    LLVM_SHA256 = \"a72180219b02c46a11fa11d7ca3e5c4f57ecaa348162e010e73a59bd26623950\"\n+    LLVM_COMMIT = \"bfee9db7857757e63b64fb4d411a264690ff711a\"\n+    LLVM_SHA256 = \"b14cb659a35562d1fccee470d0bba41cf96363e1b576e113a3a795db9ad78e3e\"\n \n     tf_http_archive(\n         name = name,"
        },
        {
            "sha": "edd883f9c237e259603824cc45aa26219bd45114",
            "filename": "third_party/xla/third_party/shardy/temporary.patch",
            "status": "modified",
            "additions": 53,
            "deletions": 247,
            "changes": 300,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5863476a0520b5dc43e903314e0e41a6aae63b01/third_party%2Fxla%2Fthird_party%2Fshardy%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5863476a0520b5dc43e903314e0e41a6aae63b01/third_party%2Fxla%2Fthird_party%2Fshardy%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fshardy%2Ftemporary.patch?ref=5863476a0520b5dc43e903314e0e41a6aae63b01",
            "patch": "@@ -1,265 +1,71 @@\n-diff --git a/shardy/dialect/sdy/ir/utils.cc b/shardy/dialect/sdy/ir/utils.cc\n-index ef43fa2..54bdd21 100644\n---- a/shardy/dialect/sdy/ir/utils.cc\n-+++ b/shardy/dialect/sdy/ir/utils.cc\n-@@ -660,17 +660,5 @@ void truncateAxesByRemovingOverlaps(SmallVector<AxisRefAttr>& axes,\n-   }\n- }\n+diff --git a/docs/mpmd/mpmd_sharding_propagation_passes.md b/docs/mpmd/mpmd_sharding_propagation_passes.md\n+index a3b51cc..3b40773 100644\n+--- a/docs/mpmd/mpmd_sharding_propagation_passes.md\n++++ b/docs/mpmd/mpmd_sharding_propagation_passes.md\n+@@ -57,10 +57,3 @@ This pass is only applied to MPMD functions in global view and with a\n+ homogeneous topology.\n  \n--bool overlaps(ArrayRef<AxisRefAttr> axisRefs,\n--              ArrayRef<AxisRefAttr> otherAxisRefs) {\n--  for (AxisRefAttr axisRef : axisRefs) {\n--    for (AxisRefAttr otherAxisRef : otherAxisRefs) {\n--      if (axisRef.overlaps(otherAxisRef)) {\n--        return true;\n--      }\n--    }\n--  }\n--  return false;\n--}\n+ Precondition: all shardings are specified as op attributes and not in types.\n -\n- }  // namespace sdy\n- }  // namespace mlir\n-diff --git a/shardy/dialect/sdy/ir/utils.h b/shardy/dialect/sdy/ir/utils.h\n-index e5e9f73..b59ffb7 100644\n---- a/shardy/dialect/sdy/ir/utils.h\n-+++ b/shardy/dialect/sdy/ir/utils.h\n-@@ -572,10 +572,6 @@ std::optional<AxisRefAttr> getPrefixWithoutOverlap(\n- void truncateAxesByRemovingOverlaps(SmallVector<AxisRefAttr>& axes,\n-                                     ArrayRef<AxisRefAttr> otherAxisRefs);\n- \n--// Returns whether `axisRefs` overlaps with `otherAxisRefs`.\n--bool overlaps(ArrayRef<AxisRefAttr> axisRefs,\n--              ArrayRef<AxisRefAttr> otherAxisRefs);\n+-### `-mpmd-simplify-program`\n -\n- }  // namespace sdy\n- }  // namespace mlir\n- \n+-_Removes redundant arg/results from fragments._\n+-\n+-Simplifies a fragment or loop, its operands and results, and their\n+-corresponding block arguments and return values.\n diff --git a/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc b/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc\n-index cc2c8f5..f84811c 100644\n+index 862a2da..be8f02c 100644\n --- a/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc\n +++ b/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc\n-@@ -103,20 +103,6 @@ AxesPerFactor getCompatibleFactorShardings(\n+@@ -433,9 +433,11 @@ class FactorAxesCandidateBag {\n+         for (int64_t index = 1; index < factorIndices.size(); index++) {\n+           int64_t factorIndex = factorIndices[index];\n+           int64_t dependsOn = factorIndices[index - 1];\n+-          factorDependenciesMap\n+-              .try_emplace(factorIndex, shardingRule.getNumFactors())\n+-              .first->second.set(dependsOn);\n++          if (!factorDependenciesMap.contains(factorIndex)) {\n++            factorDependenciesMap.try_emplace(factorIndex,\n++                                              shardingRule.getNumFactors());\n++          }\n++          factorDependenciesMap[factorIndex].set(dependsOn);\n+         }\n+       }\n      }\n+diff --git a/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc b/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc\n+index dcec262..b1d7585 100644\n+--- a/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc\n++++ b/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc\n+@@ -77,7 +77,7 @@ void insertExplicitReshardsToTargetSharding(OpOperand& opOperand,\n+         rewriter, operand.getLoc(), operand,\n+         targetSharding\n+             ? targetSharding\n+-            // Since operand and target shardings are not equivalent and\n++            // Since opearand and target shardings are not equivalent and\n+             // `targetSharding` is empty, `operandSharding` is guaranteed to be\n+             // nonempty.\n+             : TensorShardingAttr::getFullyClosedLike(operandSharding));\n+@@ -327,7 +327,7 @@ void insertAllReduceOnOpIfUnreducedToReplicated(\n+       return sharding && !sharding.getUnreducedAxes().empty();\n+     };\n+     SDY_CHECK(!llvm::any_of(op->getOpOperands(), operandHasUnreducedAxes))\n+-        << \"Some operands have unreduced axes but the operation has no \"\n++        << \"Some operands has unreduced axes but the operation has no \"\n+           \"results. \";\n+     return;\n    }\n- \n--  // Detect conflict between reduction factors and output shardings.\n--  // TODO(enver): Improve the compile-time performance.\n--  for (const int64_t factorIndex : shardingRule.getReductionFactors()) {\n--    ArrayRef<AxisRefAttr> reductionSharding = commonAxesPerFactor[factorIndex];\n--    for (const TensorFactorShardings& outTensorFactorSharding :\n--         shardingProjection.getResults()) {\n--      for (const auto& [outFactorIndex, outFactorSharding] :\n--           outTensorFactorSharding.factorIndexToSharding) {\n--        if (overlaps(reductionSharding, outFactorSharding.axisRefs)) {\n--          return {};\n--        }\n--      }\n--    }\n--  }\n-   return commonAxesPerFactor;\n- }\n- \n-diff --git a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/gather_scatter.mlir b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/gather_scatter.mlir\n-index 3df8060..b55cf25 100644\n---- a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/gather_scatter.mlir\n-+++ b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/gather_scatter.mlir\n-@@ -190,17 +190,3 @@ func.func @scatter_no_reduction(\n-   } : (tensor<6x4x10x12x14xf32>, tensor<12x4x2xi64>, tensor<12x2x4x10xf32>) -> tensor<6x4x10x12x14xf32>\n-   return %0 : tensor<6x4x10x12x14xf32>\n- }\n--\n--sdy.mesh @mesh = <[\"x\"=2, \"y\"=2]>\n--// CHECK-LABEL: @gather_reduction_factor_sharding_overlaps_with_output_sharding\n--func.func @gather_reduction_factor_sharding_overlaps_with_output_sharding(%arg0: tensor<4x2x3xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {\"y\"}, {}]>}, %arg1: tensor<4x2x2x1xi32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {\"y\"}, {}, {}]>}) -> (tensor<4x2x2x3xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {\"y\"}, {}, {}]>}) {\n--  // COM: sdy.sharding_rule = #sdy.op_sharding_rule<([i, m, l], [i, j, k, n])->([i, j, k, l]) {i=4, j=2, k=2, l=3, m=2, n=1} reduction={m} need_replication={n}>\n--\n--  // CHECK-NEXT: %[[RESHARD:.*]] = sdy.reshard %arg0 <@mesh, [{}, {}, {}]>\n--  // CHECK-NEXT: %[[GATHER:.*]] = \"stablehlo.gather\"(%[[RESHARD]], %arg1)\n--  // CHECK-SAME: {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {\"y\"}, {}, {}]>]>}\n--  // CHECK-NEXT: return %[[GATHER]]\n--  %0 = \"stablehlo.gather\"(%arg0, %arg1) <{dimension_numbers = #stablehlo.gather<offset_dims = [3], collapsed_slice_dims = [1], operand_batching_dims = [0], start_indices_batching_dims = [0], start_index_map = [1], index_vector_dim = 3>, indices_are_sorted = false, slice_sizes = array<i64: 1, 1, 3>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {\"y\"}, {}, {}]>]>} : (tensor<4x2x3xf32>, tensor<4x2x2x1xi32>) -> tensor<4x2x2x3xf32>\n--  return %0 : tensor<4x2x2x3xf32>\n--}\n--\n-diff --git a/third_party/llvm/generated.patch b/third_party/llvm/generated.patch\n-index 69e643a..509398d 100644\n---- a/third_party/llvm/generated.patch\n-+++ b/third_party/llvm/generated.patch\n-@@ -1,161 +1 @@\n- Auto generated patch. Do not edit or delete it, even if empty.\n--diff -ruN --strip-trailing-cr a/clang/include/clang/Driver/Options.td b/clang/include/clang/Driver/Options.td\n----- a/clang/include/clang/Driver/Options.td\n--+++ b/clang/include/clang/Driver/Options.td\n--@@ -545,15 +545,16 @@\n--               Group<f_Group>;\n-- }\n-- \n---// Creates a BoolOption where both of the flags are prefixed with \"g\" and have\n---// the Group<g_Group>.\n--+// Creates a BoolOption where both of the flags are prefixed with \"g\".\n--+// Does *not* map to g_Group, because that is reserved for flags that are\n--+// intended to enable (or disable) debug info, which is not appropriate for a\n--+// negative boolean flag (-gno-${feature}).\n-- // Used for -cc1 frontend options. Driver-only options do not map to\n-- // CompilerInvocation.\n-- multiclass BoolGOption<string flag_base, KeyPathAndMacro kpm,\n--                        Default default, FlagDef flag1, FlagDef flag2,\n--                        BothFlags both = BothFlags<[]>> {\n---  defm NAME : BoolOption<\"g\", flag_base, kpm, default, flag1, flag2, both>,\n---              Group<g_Group>;\n--+  defm NAME : BoolOption<\"g\", flag_base, kpm, default, flag1, flag2, both>;\n-- }\n-- \n-- multiclass BoolMOption<string flag_base, KeyPathAndMacro kpm,\n--@@ -4845,8 +4846,7 @@\n--                   NegFlag<SetFalse>,\n--                   PosFlag<SetTrue, [], [],\n--                           \"Attach linkage names to C++ constructor/destructor \"\n---                          \"declarations in DWARF.\"\n---                          \"Implies -g.\">,\n--+                          \"declarations in DWARF.\">,\n--                   BothFlags<[], [ClangOption, CLOption, CC1Option]>>,\n--                   DocBrief<[{On some ABIs (e.g., Itanium), constructors and destructors may have multiple variants. Historically, when generating DWARF, Clang did not attach ``DW_AT_linkage_name`` to structor DIEs because there were multiple possible manglings (depending on the structor variant) that could be used. With ``-gstructor-decl-linkage-names``, for ABIs with structor variants, we attach a \"unified\" mangled name to structor declarations DIEs which debuggers can use to look up all the definitions for a structor declaration. E.g., a \"unified\" mangled name ``_ZN3FooC4Ev`` may have multiple definitions associated with it such as ``_ZN3FooC1Ev`` and ``_ZN3FooC2Ev``.\n-- \n--@@ -4855,7 +4855,7 @@\n--     CodeGenOpts<\"DebugKeyInstructions\">, DefaultFalse,\n--     NegFlag<SetFalse>, PosFlag<SetTrue, [], [],\n--         \"Enable Key Instructions, which reduces the jumpiness of debug stepping in optimized C/C++ code\"\n---        \" in some debuggers. DWARF only. Implies -g.\">,\n--+        \" in some debuggers. DWARF only.\">,\n--     BothFlags<[], [ClangOption, CLOption, CC1Option]>>;\n-- def headerpad__max__install__names : Joined<[\"-\"], \"headerpad_max_install_names\">;\n-- def help : Flag<[\"-\", \"--\"], \"help\">,\n--diff -ruN --strip-trailing-cr a/clang/lib/AST/DeclTemplate.cpp b/clang/lib/AST/DeclTemplate.cpp\n----- a/clang/lib/AST/DeclTemplate.cpp\n--+++ b/clang/lib/AST/DeclTemplate.cpp\n--@@ -1670,20 +1670,25 @@\n--     auto P = CTSD->getSpecializedTemplateOrPartial();\n--     TemplateParameterList *TPL;\n--     if (const auto *CTPSD =\n---            dyn_cast<ClassTemplatePartialSpecializationDecl *>(P))\n--+            dyn_cast<ClassTemplatePartialSpecializationDecl *>(P)) {\n--       TPL = CTPSD->getTemplateParameters();\n---    else\n---      TPL = cast<ClassTemplateDecl *>(P)->getTemplateParameters();\n--+      // FIXME: Obtain Args deduced for the partial specialization.\n--+      return {TPL->getParam(Index), {}};\n--+    }\n--+    TPL = cast<ClassTemplateDecl *>(P)->getTemplateParameters();\n--     return {TPL->getParam(Index), CTSD->getTemplateArgs()[Index]};\n--   }\n--   case Decl::Kind::VarTemplateSpecialization: {\n--     const auto *VTSD = cast<VarTemplateSpecializationDecl>(D);\n--     auto P = VTSD->getSpecializedTemplateOrPartial();\n--     TemplateParameterList *TPL;\n---    if (const auto *VTPSD = dyn_cast<VarTemplatePartialSpecializationDecl *>(P))\n--+    if (const auto *VTPSD =\n--+            dyn_cast<VarTemplatePartialSpecializationDecl *>(P)) {\n--       TPL = VTPSD->getTemplateParameters();\n---    else\n---      TPL = cast<VarTemplateDecl *>(P)->getTemplateParameters();\n--+      // FIXME: Obtain Args deduced for the partial specialization.\n--+      return {TPL->getParam(Index), {}};\n--+    }\n--+    TPL = cast<VarTemplateDecl *>(P)->getTemplateParameters();\n--     return {TPL->getParam(Index), VTSD->getTemplateArgs()[Index]};\n--   }\n--   case Decl::Kind::ClassTemplatePartialSpecialization:\n--diff -ruN --strip-trailing-cr a/clang/test/DebugInfo/KeyInstructions/flag.cpp b/clang/test/DebugInfo/KeyInstructions/flag.cpp\n----- a/clang/test/DebugInfo/KeyInstructions/flag.cpp\n--+++ b/clang/test/DebugInfo/KeyInstructions/flag.cpp\n--@@ -1,12 +1,15 @@\n-- // RUN: %clang -### -target x86_64 -c -gdwarf -gkey-instructions %s 2>&1 | FileCheck %s --check-prefixes=KEY-INSTRUCTIONS\n-- // RUN: %clang -### -target x86_64 -c -gdwarf -gno-key-instructions %s 2>&1 | FileCheck %s --check-prefixes=NO-KEY-INSTRUCTIONS\n--+// RUN: %clang -### -target x86_64 -c -gno-key-instructions %s 2>&1 | FileCheck %s --check-prefixes=NO-DEBUG\n-- \n-- //// Help.\n-- // RUN %clang --help | FileCheck %s --check-prefix=HELP\n---// HELP: -gkey-instructions  Enable Key Instructions, which reduces the jumpiness of debug stepping in optimized C/C++ code in some debuggers. DWARF only. Implies -g.\n--+// HELP: -gkey-instructions  Enable Key Instructions, which reduces the jumpiness of debug stepping in optimized C/C++ code in some debuggers. DWARF only.\n-- \n-- // KEY-INSTRUCTIONS: \"-gkey-instructions\"\n-- // NO-KEY-INSTRUCTIONS-NOT: key-instructions\n--+// NO-DEBUG-NOT: debug-info-kind\n--+// NO-DEBUG-NOT: dwarf\n-- \n-- //// Help hidden: flag should not be visible.\n-- // RUN: %clang --help | FileCheck %s --check-prefix=HELP\n--diff -ruN --strip-trailing-cr a/clang/test/Driver/debug-options.c b/clang/test/Driver/debug-options.c\n----- a/clang/test/Driver/debug-options.c\n--+++ b/clang/test/Driver/debug-options.c\n--@@ -268,11 +268,11 @@\n-- // RUN: %clang -### -c %s 2>&1 | FileCheck -check-prefix=NORNGBSE %s\n-- // RUN: %clang -### -c -fdebug-ranges-base-address -fno-debug-ranges-base-address %s 2>&1 | FileCheck -check-prefix=NORNGBSE %s\n-- //\n---// RUN: %clang -### -c -gomit-unreferenced-methods -fno-standalone-debug %s 2>&1 | FileCheck -check-prefix=INCTYPES %s\n--+// RUN: %clang -### -c -g -gomit-unreferenced-methods -fno-standalone-debug %s 2>&1 | FileCheck -check-prefix=INCTYPES %s\n-- // RUN: %clang -### -c %s 2>&1 | FileCheck -check-prefix=NOINCTYPES %s\n---// RUN: %clang -### -c -gomit-unreferenced-methods -fdebug-types-section -target x86_64-unknown-linux %s 2>&1 \\\n--+// RUN: %clang -### -c -g -gomit-unreferenced-methods -fdebug-types-section -target x86_64-unknown-linux %s 2>&1 \\\n-- // RUN:        | FileCheck -check-prefix=NOINCTYPES %s\n---// RUN: %clang -### -c -gomit-unreferenced-methods -fstandalone-debug %s 2>&1 | FileCheck -check-prefix=NOINCTYPES %s\n--+// RUN: %clang -### -c -g -gomit-unreferenced-methods -fstandalone-debug %s 2>&1 | FileCheck -check-prefix=NOINCTYPES %s\n-- //\n-- // RUN: %clang -### -c -glldb %s 2>&1 | FileCheck -check-prefix=NOPUB %s\n-- // RUN: %clang -### -c -glldb -gno-pubnames %s 2>&1 | FileCheck -check-prefix=NOPUB %s\n--diff -ruN --strip-trailing-cr a/clang/test/SemaTemplate/concepts.cpp b/clang/test/SemaTemplate/concepts.cpp\n----- a/clang/test/SemaTemplate/concepts.cpp\n--+++ b/clang/test/SemaTemplate/concepts.cpp\n--@@ -1476,3 +1476,20 @@\n-- // expected-error@-1 {{static assertion failed due to requirement 'requires { { &f() } -> C; }'}}\n-- \n-- }\n--+\n--+namespace GH162770 {\n--+  enum e {};\n--+  template<e> struct s {};\n--+\n--+  template<typename> struct specialized;\n--+  template<e x> struct specialized<s<x>> {\n--+    static auto make(auto) -> s<x>;\n--+  };\n--+\n--+  template<e x> struct check {\n--+    static constexpr auto m = requires { specialized<s<x>>::make(0); };\n--+  };\n--+\n--+  template<typename... Ts> auto comma = (..., Ts());\n--+  auto b = comma<check<e{}>>;\n--+} // namespace GH162770\n--diff -ruN --strip-trailing-cr a/clang/test/SemaTemplate/partial-spec-instantiate.cpp b/clang/test/SemaTemplate/partial-spec-instantiate.cpp\n----- a/clang/test/SemaTemplate/partial-spec-instantiate.cpp\n--+++ b/clang/test/SemaTemplate/partial-spec-instantiate.cpp\n--@@ -152,3 +152,16 @@\n--     ClassTemplate<>::Nested<int> instantiation;\n--   }\n-- }\n--+#if __cplusplus >= 201103L\n--+namespace GH162855 {\n--+  template <class...> using A = int;\n--+  template <class, int> struct B;\n--+  template <class...> struct C;\n--+  template <template <class, int...> class TT, long... X>\n--+  struct C<TT<int, X...>> {\n--+    template <class... Y> using l = A<B<Y, X>...>;\n--+  };\n--+  template <class> struct D;\n--+  template struct C<D<int>>;\n--+} // namespace GH162855\n--+#endif\n diff --git a/third_party/llvm/workspace.bzl b/third_party/llvm/workspace.bzl\n-index 81b4a54..ed562d4 100644\n+index ed562d4..ba4c1a2 100644\n --- a/third_party/llvm/workspace.bzl\n +++ b/third_party/llvm/workspace.bzl\n @@ -4,8 +4,8 @@ load(\"//third_party:repo.bzl\", \"tf_http_archive\")\n  \n  def repo(name):\n      \"\"\"Imports LLVM.\"\"\"\n--    LLVM_COMMIT = \"3a6b818132e3133c7d33f8f577e62503f12869b4\"\n--    LLVM_SHA256 = \"a0b3de698393e0f49d0aca3f869cc03bf0c59eba0c65f608e565278943c31958\"\n-+    LLVM_COMMIT = \"267fa8dd1efce0b79ebcaa804d54542c99918df2\"\n-+    LLVM_SHA256 = \"a72180219b02c46a11fa11d7ca3e5c4f57ecaa348162e010e73a59bd26623950\"\n+-    LLVM_COMMIT = \"267fa8dd1efce0b79ebcaa804d54542c99918df2\"\n+-    LLVM_SHA256 = \"a72180219b02c46a11fa11d7ca3e5c4f57ecaa348162e010e73a59bd26623950\"\n++    LLVM_COMMIT = \"bfee9db7857757e63b64fb4d411a264690ff711a\"\n++    LLVM_SHA256 = \"b14cb659a35562d1fccee470d0bba41cf96363e1b576e113a3a795db9ad78e3e\"\n  \n      tf_http_archive(\n          name = name,"
        },
        {
            "sha": "dd332adb67a59ae006030c7cd902730b1294ca21",
            "filename": "third_party/xla/third_party/shardy/workspace.bzl",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5863476a0520b5dc43e903314e0e41a6aae63b01/third_party%2Fxla%2Fthird_party%2Fshardy%2Fworkspace.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5863476a0520b5dc43e903314e0e41a6aae63b01/third_party%2Fxla%2Fthird_party%2Fshardy%2Fworkspace.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fshardy%2Fworkspace.bzl?ref=5863476a0520b5dc43e903314e0e41a6aae63b01",
            "patch": "@@ -3,8 +3,8 @@\n load(\"//third_party:repo.bzl\", \"tf_http_archive\", \"tf_mirror_urls\")\n \n def repo():\n-    SHARDY_COMMIT = \"5cc8d44a7428d36b18965e65d93287be66ea7faa\"\n-    SHARDY_SHA256 = \"f12259a31d64a5220b48351bf2c2409679cf789ec2bf1b7cda0e5d516e918f97\"\n+    SHARDY_COMMIT = \"8349f4b45638848d28fb7ad366c68d4ed16710ab\"\n+    SHARDY_SHA256 = \"cc7856517bec649903d302496922cef92373bb654c5f53da0014822bbff6835f\"\n \n     tf_http_archive(\n         name = \"shardy\","
        }
    ],
    "stats": {
        "total": 308,
        "additions": 57,
        "deletions": 251
    }
}