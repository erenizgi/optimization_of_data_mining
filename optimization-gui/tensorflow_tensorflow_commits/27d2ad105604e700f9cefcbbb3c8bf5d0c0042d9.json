{
    "author": "Tixxx",
    "message": "PR #33754: [NVIDIA GPU] Add pred, int8 and uint8 as supported nvshmem reduction types\n\nImported from GitHub PR https://github.com/openxla/xla/pull/33754\n\nüìù Summary of Changes\nAdd pred, int8 and uint8 as supported nvshmem reduction types\n\nüéØ Justification\nNvshmem 3.2.5 supports doing reductions for byte-sized numerics. This pr adds that missing feature.\n\nüöÄ Kind of Contribution\nPlease remove what does not apply: üêõ Bug Fix\n\nüìä Benchmark (for Performance Improvements)\nNA\nüß™ Unit Tests:\nUnit test added\n\nüß™ Execution Tests:\nUnit test also executes the module.\n\nCopybara import of the project:\n\n--\n0198f55aed8a35ff1bb2aaa90d0921cf96cdb10c by TJ Xu <tjx@nvidia.com>:\n\nAdd pred, int8 and uint8 as supported nvshmem types\n\nMerging this change closes #33754\n\nPiperOrigin-RevId: 831751332",
    "sha": "27d2ad105604e700f9cefcbbb3c8bf5d0c0042d9",
    "files": [
        {
            "sha": "12356029078e3ccfb10ffc565e8ff5f53d6def6b",
            "filename": "third_party/xla/xla/backends/gpu/collectives/nvshmem_communicator.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/27d2ad105604e700f9cefcbbb3c8bf5d0c0042d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_communicator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/27d2ad105604e700f9cefcbbb3c8bf5d0c0042d9/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_communicator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcollectives%2Fnvshmem_communicator.cc?ref=27d2ad105604e700f9cefcbbb3c8bf5d0c0042d9",
            "patch": "@@ -293,6 +293,20 @@ Future<> NvshmemCommunicator::AllReduce(\n           dest_ptr, count);\n       break;\n     }\n+    case PrimitiveType::PRED:\n+    case PrimitiveType::U8: {\n+      CALL_NVSHMEM_BITWISE_REDUCTION_DATATYPE(\n+          uint8, uint8_t, NVSHMEM_TEAM_SHARED,\n+          se::gpu::AsGpuStreamValue(stream), reduction_kind, source_ptr,\n+          dest_ptr, count);\n+      break;\n+    }\n+    case PrimitiveType::S8: {\n+      CALL_NVSHMEM_BITWISE_REDUCTION_DATATYPE(\n+          int8, int8_t, NVSHMEM_TEAM_SHARED, se::gpu::AsGpuStreamValue(stream),\n+          reduction_kind, source_ptr, dest_ptr, count);\n+      break;\n+    }\n     default:\n       return absl::InternalError(\"Invalid Nvshmem reduction type.\");\n   }"
        },
        {
            "sha": "924130dbdbde8fdad24b6a22b653c8064d76b99f",
            "filename": "third_party/xla/xla/pjrt/gpu/nvshmem_gpu_collectives_test.cc",
            "status": "modified",
            "additions": 72,
            "deletions": 0,
            "changes": 72,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/27d2ad105604e700f9cefcbbb3c8bf5d0c0042d9/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fnvshmem_gpu_collectives_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/27d2ad105604e700f9cefcbbb3c8bf5d0c0042d9/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fnvshmem_gpu_collectives_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fnvshmem_gpu_collectives_test.cc?ref=27d2ad105604e700f9cefcbbb3c8bf5d0c0042d9",
            "patch": "@@ -82,6 +82,12 @@ absl::StatusOr<std::string> GetDataTypeString(xla::PrimitiveType data_type) {\n       return \"s32\";\n     case xla::PrimitiveType::S64:\n       return \"s64\";\n+    case xla::PrimitiveType::PRED:\n+      return \"pred\";\n+    case xla::PrimitiveType::S8:\n+      return \"s8\";\n+    case xla::PrimitiveType::U8:\n+      return \"u8\";\n     default:\n       return absl::InvalidArgumentError(\"Invalida data type.\");\n   }\n@@ -127,6 +133,18 @@ TEST(NvshmemGpuCollectivesTest, NvshmemAllReduceFloat) {\n   RunNvshmemTest(PrimitiveType::F32, \"all_reduce\");\n }\n \n+TEST(NvshmemGpuCollectivesTest, NvshmemAllReducePred) {\n+  RunNvshmemTest(PrimitiveType::PRED, \"all_reduce\");\n+}\n+\n+TEST(NvshmemGpuCollectivesTest, NvshmemAllReduceInt8) {\n+  RunNvshmemTest(PrimitiveType::S8, \"all_reduce\");\n+}\n+\n+TEST(NvshmemGpuCollectivesTest, NvshmemAllReduceUint8) {\n+  RunNvshmemTest(PrimitiveType::U8, \"all_reduce\");\n+}\n+\n absl::Status NvshmemCollectiveTestBody(int rank_id, int num_ranks,\n                                        int input_data_type,\n                                        absl::string_view test_case) {\n@@ -335,6 +353,45 @@ absl::Status NvshmemCollectiveTestBody(int rank_id, int num_ranks,\n                 /*device_layout=*/nullptr));\n         break;\n       }\n+      case xla::PrimitiveType::PRED: {\n+        std::vector<uint8_t> data_array{10};\n+        TF_ASSIGN_OR_RETURN(\n+            input,\n+            client->BufferFromHostBuffer(\n+                data_array.data(), shape.element_type(), shape.dimensions(),\n+                /*byte_strides=*/std::nullopt,\n+                PjRtClient::HostBufferSemantics::kImmutableOnlyDuringCall,\n+                /*on_done_with_host_buffer=*/nullptr,\n+                *device->default_memory_space(),\n+                /*device_layout=*/nullptr));\n+        break;\n+      }\n+      case xla::PrimitiveType::S8: {\n+        std::vector<int8_t> data_array{10};\n+        TF_ASSIGN_OR_RETURN(\n+            input,\n+            client->BufferFromHostBuffer(\n+                data_array.data(), shape.element_type(), shape.dimensions(),\n+                /*byte_strides=*/std::nullopt,\n+                PjRtClient::HostBufferSemantics::kImmutableOnlyDuringCall,\n+                /*on_done_with_host_buffer=*/nullptr,\n+                *device->default_memory_space(),\n+                /*device_layout=*/nullptr));\n+        break;\n+      }\n+      case xla::PrimitiveType::U8: {\n+        std::vector<uint8_t> data_array{10};\n+        TF_ASSIGN_OR_RETURN(\n+            input,\n+            client->BufferFromHostBuffer(\n+                data_array.data(), shape.element_type(), shape.dimensions(),\n+                /*byte_strides=*/std::nullopt,\n+                PjRtClient::HostBufferSemantics::kImmutableOnlyDuringCall,\n+                /*on_done_with_host_buffer=*/nullptr,\n+                *device->default_memory_space(),\n+                /*device_layout=*/nullptr));\n+        break;\n+      }\n       default:\n         return absl::InvalidArgumentError(\"Invalida data type.\");\n     }\n@@ -480,6 +537,21 @@ absl::Status NvshmemCollectiveTestBody(int rank_id, int num_ranks,\n         TF_RET_CHECK(literal->data<int64_t>()[0] == ref_data[0]);\n         break;\n       }\n+      case xla::PrimitiveType::PRED: {\n+        std::vector<uint8_t> ref_data{20};\n+        TF_RET_CHECK(literal->data<uint8_t>()[0] == ref_data[0]);\n+        break;\n+      }\n+      case xla::PrimitiveType::S8: {\n+        std::vector<int8_t> ref_data{20};\n+        TF_RET_CHECK(literal->data<int8_t>()[0] == ref_data[0]);\n+        break;\n+      }\n+      case xla::PrimitiveType::U8: {\n+        std::vector<uint8_t> ref_data{20};\n+        TF_RET_CHECK(literal->data<uint8_t>()[0] == ref_data[0]);\n+        break;\n+      }\n       default:\n         return absl::InvalidArgumentError(\"Invalida data type.\");\n     }"
        }
    ],
    "stats": {
        "total": 86,
        "additions": 86,
        "deletions": 0
    }
}