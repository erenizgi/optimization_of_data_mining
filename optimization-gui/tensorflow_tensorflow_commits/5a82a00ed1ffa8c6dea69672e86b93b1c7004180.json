{
    "author": "mooskagh",
    "message": "[XLA:GPU] cuBLASlt rewriter incorrectly fused bias when it didn't cover the entire dimension.\n\nPiperOrigin-RevId: 806191325",
    "sha": "5a82a00ed1ffa8c6dea69672e86b93b1c7004180",
    "files": [
        {
            "sha": "cf90f6406033a84c4dc818e8cbe5e2fa8bd70862",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a82a00ed1ffa8c6dea69672e86b93b1c7004180/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a82a00ed1ffa8c6dea69672e86b93b1c7004180/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=5a82a00ed1ffa8c6dea69672e86b93b1c7004180",
            "patch": "@@ -1697,12 +1697,14 @@ xla_test(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/testlib:filecheck\",\n         \"//xla/hlo/testlib:pattern_matcher_gmock\",\n         \"//xla/hlo/testlib:test\",\n         \"//xla/service:pattern_matcher\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/strings\","
        },
        {
            "sha": "16c26d533e50e0725da1d7a045eed9b7ed8e2960",
            "filename": "third_party/xla/xla/service/gpu/transforms/cublas_gemm_rewriter_test.cc",
            "status": "modified",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a82a00ed1ffa8c6dea69672e86b93b1c7004180/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcublas_gemm_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a82a00ed1ffa8c6dea69672e86b93b1c7004180/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcublas_gemm_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fcublas_gemm_rewriter_test.cc?ref=5a82a00ed1ffa8c6dea69672e86b93b1c7004180",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"xla/error_spec.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/testlib/filecheck.h\"\n #include \"xla/hlo/testlib/pattern_matcher_gmock.h\"\n #include \"xla/hlo/testlib/test.h\"\n #include \"xla/service/gpu/transforms/gemm_rewriter.h\"\n@@ -3405,6 +3406,46 @@ ENTRY test {\n )\");\n }\n \n+TEST_F(CublasLtGemmRewriteTest, CublasLtRewriteWithBias) {\n+  // The bias has shape [7], which doesn't match the non-contracting rhs\n+  // dimension ([35]. It's reshaped from [5,7] but cuBLASlt is not aware of\n+  // that).\n+\n+  const char* hlo_text = R\"(\n+HloModule test\n+\n+ENTRY %test (x: f32[2,3,4], y: f32[4,5,7], z: f32[7]) -> f32[2,3,5,7] {\n+  %x = f32[2,3,4]{2,1,0} parameter(0)\n+  %bitcast = f32[6,4]{1,0} bitcast(%x)\n+  %y = f32[4,5,7]{2,1,0} parameter(1)\n+  %bitcast.1 = f32[4,35]{1,0} bitcast(%y)\n+  %dot = f32[6,35]{1,0} dot(%bitcast, %bitcast.1), lhs_contracting_dims={1},\n+         rhs_contracting_dims={0}, operand_precision={highest,highest}\n+  %bitcast.2 = f32[2,3,5,7]{3,2,1,0} bitcast(%dot)\n+  %z = f32[7]{0} parameter(2)\n+  %z_bcast = f32[2,3,5,7]{3,2,1,0} broadcast(%z), dimensions={3}\n+  ROOT %out = f32[2,3,5,7]{3,2,1,0} add(%bitcast.2, %z_bcast)\n+}\n+)\";\n+\n+  HloModuleConfig config;\n+  DebugOptions debug_options = GetDebugOptionsForTest();\n+  config.set_debug_options(debug_options);\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_text, config));\n+\n+  GemmRewriter pass(Capability(), GetToolkitVersion());\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, RunHloPass(&pass, module.get()));\n+  EXPECT_TRUE(changed);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(bool filecheck_result,\n+                          RunFileCheck(module->ToString(), R\"(\n+; CHECK:           custom_call_target=\"__cublas$lt$matmul\",\n+; CHECK-DAG:         \"epilogue\":\"DEFAULT\"\n+      )\"));\n+  EXPECT_TRUE(filecheck_result);\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "511df0530507d44b1ceef933219c157222fd8e4b",
            "filename": "third_party/xla/xla/service/gpu/transforms/gemm_rewriter.cc",
            "status": "modified",
            "additions": 197,
            "deletions": 199,
            "changes": 396,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5a82a00ed1ffa8c6dea69672e86b93b1c7004180/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5a82a00ed1ffa8c6dea69672e86b93b1c7004180/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fgemm_rewriter.cc?ref=5a82a00ed1ffa8c6dea69672e86b93b1c7004180",
            "patch": "@@ -82,16 +82,16 @@ namespace {\n namespace m = match;\n \n // Give this instruction a more useful name than \"custom-call.42\".\n-absl::Status SetName(HloModule *module, HloInstruction *gemm) {\n+absl::Status SetName(HloModule* module, HloInstruction* gemm) {\n   if (IsCublasLtMatmul(*gemm)) {\n     module->SetAndUniquifyInstrName(gemm, \"cublas-lt-matmul\");\n     return absl::OkStatus();\n   }\n \n   TF_ASSIGN_OR_RETURN(GpuBackendConfig gpu_config,\n                       gemm->backend_config<GpuBackendConfig>());\n-  const GemmBackendConfig &config = gpu_config.gemm_backend_config();\n-  const DotDimensionNumbers &dot_dims = config.dot_dimension_numbers();\n+  const GemmBackendConfig& config = gpu_config.gemm_backend_config();\n+  const DotDimensionNumbers& dot_dims = config.dot_dimension_numbers();\n   bool is_batch_dot = !dot_dims.lhs_batch_dimensions().empty() ||\n                       !dot_dims.rhs_batch_dimensions().empty();\n \n@@ -120,7 +120,7 @@ bool SupportsEpilogueFusion(PrimitiveType type) {\n   }\n }\n \n-bool IsF8Type(const HloInstruction *instr) {\n+bool IsF8Type(const HloInstruction* instr) {\n   return primitive_util::IsF8Type(instr->shape().element_type());\n }\n \n@@ -140,8 +140,8 @@ Shape PadShapeToMultipleOf16(const Shape old_shape,\n }\n \n // Pad the dimensions of the operands to the target shape.\n-HloInstruction *PadOperandToTargetShape(const Shape &target,\n-                                        HloInstruction *x) {\n+HloInstruction* PadOperandToTargetShape(const Shape& target,\n+                                        HloInstruction* x) {\n   if (ShapeUtil::Equal(target, x->shape()) ||\n       !ShapeUtil::SameElementType(x->shape(), target)) {\n     return x;\n@@ -156,28 +156,28 @@ HloInstruction *PadOperandToTargetShape(const Shape &target,\n     dimension->set_interior_padding(0);\n   }\n \n-  HloInstruction *zero = x->AddInstruction(HloInstruction::CreateConstant(\n+  HloInstruction* zero = x->AddInstruction(HloInstruction::CreateConstant(\n       LiteralUtil::Zero(x->shape().element_type())));\n   return x->AddInstruction(\n       HloInstruction::CreatePad(target, x, zero, padding_config));\n }\n \n // Pad the non-batch dimensions of the operands to multiples of 16 as required\n // by cuBLASLt FP8 gemms.\n-HloInstruction *PadOperandToMultipleOf16(absl::Span<const int64_t> batch_dims,\n-                                         HloInstruction *x) {\n+HloInstruction* PadOperandToMultipleOf16(absl::Span<const int64_t> batch_dims,\n+                                         HloInstruction* x) {\n   Shape padded_shape = PadShapeToMultipleOf16(x->shape(), batch_dims);\n   return PadOperandToTargetShape(padded_shape, x);\n }\n \n // Calculates the reciprocal of scalar when invert is true and converts to FP32.\n-absl::StatusOr<HloInstruction *> InvertAndConvertScalar(HloInstruction *scalar,\n-                                                        bool invert) {\n+absl::StatusOr<HloInstruction*> InvertAndConvertScalar(HloInstruction* scalar,\n+                                                       bool invert) {\n   DCHECK(ShapeUtil::IsScalar(scalar->shape()));\n \n   if (invert) {\n     Literal one_literal = LiteralUtil::One(scalar->shape().element_type());\n-    HloInstruction *one = scalar->parent()->AddInstruction(\n+    HloInstruction* one = scalar->parent()->AddInstruction(\n         HloInstruction::CreateConstant(one_literal.Clone()));\n     TF_ASSIGN_OR_RETURN(scalar, MakeBinaryHlo(HloOpcode::kDivide, one, scalar,\n                                               &scalar->metadata()));\n@@ -193,7 +193,7 @@ absl::StatusOr<HloInstruction *> InvertAndConvertScalar(HloInstruction *scalar,\n // operand_index) pairs. operand_index is the index to get to the previous\n // element in the path. I.e.,\n // path[i].first->operand(path[i].second) == path[i-1].first\n-using InstrPath = std::vector<std::pair<HloInstruction *, int>>;\n+using InstrPath = std::vector<std::pair<HloInstruction*, int>>;\n \n // From 'instr', recursively traverses operands until an FP8 instruction is\n // encountered. Only unary ops and a few types of non-unary ops are traversed.\n@@ -203,7 +203,7 @@ using InstrPath = std::vector<std::pair<HloInstruction *, int>>;\n // The intent is, given 'instr' is the operand of a dot, to find a sequence of\n // instruction that can potentially be fused into a cuBLAS LT FP8 gemm.\n std::optional<InstrPath> FindF8SubgraphRecursive(\n-    HloInstruction *instr, absl::flat_hash_set<int> &visited_instrs) {\n+    HloInstruction* instr, absl::flat_hash_set<int>& visited_instrs) {\n   // Avoid visiting the same instruction more than once.\n   if (!visited_instrs.emplace(instr->unique_id()).second) {\n     return std::nullopt;\n@@ -242,9 +242,9 @@ std::optional<InstrPath> FindF8SubgraphRecursive(\n // gemm that can be potentially pattern-matched into an FP8 cublasLT gemm.\n struct MatchedFp8Param {\n   // The FP8 input to the gemm.\n-  HloInstruction *fp8_input = nullptr;\n+  HloInstruction* fp8_input = nullptr;\n   // If nonnull, the scale for the 'x'\n-  HloInstruction *scale = nullptr;\n+  HloInstruction* scale = nullptr;\n   // Whether the scale, if present, multiplies or divides 'x'\n   bool mult_scale = false;\n   // A list of instructions from x to the dot instruction commutative with\n@@ -267,14 +267,14 @@ struct MatchedFp8Param {\n //    'commutative_ops'.\n // Steps (1) and (2) together are a dequantization, and can be fused into a\n // cublas LT matmul. Step (3) can be moved before the cublas LT matmul.\n-std::optional<MatchedFp8Param> MatchFp8Param(HloInstruction *instr) {\n+std::optional<MatchedFp8Param> MatchFp8Param(HloInstruction* instr) {\n   absl::flat_hash_set<int> visited_instrs;\n   std::optional<InstrPath> maybe_subgraph =\n       FindF8SubgraphRecursive(instr, visited_instrs);\n   if (!maybe_subgraph) {\n     return std::nullopt;\n   }\n-  InstrPath &subgraph = maybe_subgraph.value();\n+  InstrPath& subgraph = maybe_subgraph.value();\n \n   MatchedFp8Param param;\n \n@@ -312,7 +312,7 @@ std::optional<MatchedFp8Param> MatchFp8Param(HloInstruction *instr) {\n     return std::nullopt;\n   }\n \n-  auto preserves_element_type = [](const HloInstruction *instr) -> bool {\n+  auto preserves_element_type = [](const HloInstruction* instr) -> bool {\n     return ShapeUtil::SameElementType(instr->shape(),\n                                       instr->operand(0)->shape());\n   };\n@@ -353,7 +353,7 @@ std::optional<MatchedFp8Param> MatchFp8Param(HloInstruction *instr) {\n // Transposes a matrix by swapping the contracting and non-contracting\n // dimension. There must be only one contracting and only one non-contracting\n // dimension. Keeps the layout the same.\n-HloInstruction *TransposeMatrix(HloInstruction *instr, int64_t contracting_dim,\n+HloInstruction* TransposeMatrix(HloInstruction* instr, int64_t contracting_dim,\n                                 absl::Span<const int64_t> batch_dims) {\n   auto input_shape = instr->shape();\n   // Identify the dimensional order which describes a transpose of the\n@@ -404,7 +404,7 @@ HloInstruction *TransposeMatrix(HloInstruction *instr, int64_t contracting_dim,\n       ShapeUtil::PermuteDimensions(permutation, a0->shape());\n   *transpose_shape.mutable_layout() = a0->shape().layout();\n \n-  HloInstruction *normalized_transpose = instr->AddInstruction(\n+  HloInstruction* normalized_transpose = instr->AddInstruction(\n       HloInstruction::CreateTranspose(transpose_shape, a0, permutation));\n \n   Shape final_shape = ShapeUtil::PermuteDimensions(inv_perm, transpose_shape);\n@@ -426,12 +426,12 @@ HloInstruction *TransposeMatrix(HloInstruction *instr, int64_t contracting_dim,\n // constant so we can fuse it into this gemm. That would defeat the whole\n // purpose of this fusion, which is to launch fewer kernels.  So if we can,\n // we expand out this constant ourselves.\n-HloInstruction *MaybeConstantFoldBias(HloInstruction *bias) {\n+HloInstruction* MaybeConstantFoldBias(HloInstruction* bias) {\n   // This limit was not chosen carefully.\n   constexpr int kMaxMaterializeBiasBytes = 8 * 1024 * 1024;\n \n   // Don't fold broadcasts of scalars -- algsimp will just collapse it again.\n-  auto is_nonscalar = [](const HloInstruction *instr) {\n+  auto is_nonscalar = [](const HloInstruction* instr) {\n     return !ShapeUtil::IsEffectiveScalar(instr->shape());\n   };\n \n@@ -460,41 +460,41 @@ HloInstruction *MaybeConstantFoldBias(HloInstruction *bias) {\n   return bias;\n }\n \n-auto Gemm(HloInstruction **instr) {\n+auto Gemm(HloInstruction** instr) {\n   return m::CustomCall(instr, {kGemmCallTarget});\n }\n \n-auto CublasLtMatmul(HloInstruction **instr) {\n+auto CublasLtMatmul(HloInstruction** instr) {\n   return m::CustomCall(instr, {kCublasLtMatmulCallTarget});\n }\n \n-auto CublasLtMatmulF8(HloInstruction **instr) {\n+auto CublasLtMatmulF8(HloInstruction** instr) {\n   return m::CustomCall(instr, {kCublasLtMatmulF8CallTarget});\n }\n \n-auto CublasLtMatmulMaybeF8(HloInstruction **instr) {\n+auto CublasLtMatmulMaybeF8(HloInstruction** instr) {\n   return m::CustomCall(\n       instr, {kCublasLtMatmulCallTarget, kCublasLtMatmulF8CallTarget});\n }\n \n-auto GemmOrCublasLtMatmul(HloInstruction **instr) {\n+auto GemmOrCublasLtMatmul(HloInstruction** instr) {\n   return m::CustomCall(instr, {kGemmCallTarget, kCublasLtMatmulCallTarget});\n }\n \n-auto GemmOrCublasLtMatmulMaybeF8(HloInstruction **instr) {\n+auto GemmOrCublasLtMatmulMaybeF8(HloInstruction** instr) {\n   return m::CustomCall(instr, {kGemmCallTarget, kCublasLtMatmulCallTarget,\n                                kCublasLtMatmulF8CallTarget});\n }\n \n-auto BcastConstScalar(HloInstruction **instr, double value) {\n+auto BcastConstScalar(HloInstruction** instr, double value) {\n   return m::Broadcast(instr, m::ConstantScalar(value));\n }\n \n auto BcastConstScalar(double value) { return BcastConstScalar(nullptr, value); }\n \n auto BcastConstScalarNear(double value) {\n   return m::Broadcast(m::ConstantScalar().WithPredicate(\n-      [expected = value](const HloInstruction *instr) {\n+      [expected = value](const HloInstruction* instr) {\n         // Not a very robust floating-point comparison, but good enough for our\n         // purposes.\n         std::optional<double> actual =\n@@ -526,19 +526,19 @@ auto BcastConstScalarNear(double value) {\n }\n \n template <typename Pattern>\n-auto OptionalSlice(HloInstruction **optional_slice, Pattern pattern) {\n+auto OptionalSlice(HloInstruction** optional_slice, Pattern pattern) {\n   return m::AnyOf<HloInstruction>(m::Slice(optional_slice, pattern),\n                                   std::move(pattern));\n }\n \n template <typename Pattern>\n-auto OptionalConvert(HloInstruction **optional_convert, Pattern pattern) {\n+auto OptionalConvert(HloInstruction** optional_convert, Pattern pattern) {\n   return m::AnyOf<HloInstruction>(m::Convert(optional_convert, pattern),\n                                   std::move(pattern));\n }\n \n template <typename Pattern>\n-auto OptionalBitcast(HloInstruction **optional_bitcast, Pattern pattern) {\n+auto OptionalBitcast(HloInstruction** optional_bitcast, Pattern pattern) {\n   return m::AnyOf<HloInstruction>(m::Bitcast(optional_bitcast, pattern),\n                                   std::move(pattern));\n }\n@@ -577,14 +577,14 @@ auto OptionalBitcast(HloInstruction **optional_bitcast, Pattern pattern) {\n // when the output of the GEMM is requested in FP8 format.\n class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n  public:\n-  explicit GemmRewriterVisitor(const se::GpuComputeCapability &gpu_version,\n+  explicit GemmRewriterVisitor(const se::GpuComputeCapability& gpu_version,\n                                se::SemanticVersion toolkit_version,\n                                const GemmRewriterOptions options)\n       : gpu_version_(gpu_version),\n         toolkit_version_(toolkit_version),\n         options_(options) {}\n \n-  absl::Status HandleDot(HloInstruction *instr) override {\n+  absl::Status HandleDot(HloInstruction* instr) override {\n     TF_ASSIGN_OR_RETURN(\n         bool is_supported_matmul,\n         IsCublasSupportedMatMul(*instr,\n@@ -608,7 +608,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     // Create a GemmBackendConfig based on the instruction.\n     TF_ASSIGN_OR_RETURN(GpuBackendConfig gpu_backend_config,\n                         instr->backend_config<GpuBackendConfig>());\n-    GemmBackendConfig &gemm_backend_config =\n+    GemmBackendConfig& gemm_backend_config =\n         *gpu_backend_config.mutable_gemm_backend_config();\n     gemm_backend_config.set_alpha_real(1.0);\n     gemm_backend_config.set_alpha_imag(0.0);\n@@ -617,8 +617,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n         instr->dot_dimension_numbers();\n     *gemm_backend_config.mutable_precision_config() = instr->precision_config();\n \n-    HloInstruction *lhs = instr->mutable_operand(0);\n-    HloInstruction *rhs = instr->mutable_operand(1);\n+    HloInstruction* lhs = instr->mutable_operand(0);\n+    HloInstruction* rhs = instr->mutable_operand(1);\n     auto attributes = instr->frontend_attributes().map();\n     gemm_backend_config.set_grad_x(attributes[\"grad_x\"] == \"true\");\n     gemm_backend_config.set_grad_y(attributes[\"grad_y\"] == \"true\");\n@@ -650,9 +650,9 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n         std::optional<MatchedFp8Param> a, b;\n         if (supported_by_cublaslt && HloPredicateIsOp<HloOpcode::kDot>(instr) &&\n             (a = MatchFp8Param(\n-                 const_cast<HloInstruction *>(instr->operand(0)))) &&\n+                 const_cast<HloInstruction*>(instr->operand(0)))) &&\n             (b = MatchFp8Param(\n-                 const_cast<HloInstruction *>(instr->operand(1))))) {\n+                 const_cast<HloInstruction*>(instr->operand(1))))) {\n           if (IsRocm(gpu_version_) &&\n               toolkit_version_ < stream_executor::SemanticVersion{6, 2, 0} &&\n               instr->shape().element_type() != F16 &&\n@@ -685,8 +685,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n           TF_ASSIGN_OR_RETURN(\n               absl::string_view gemm_custom_call_target,\n               GetNonFp8GemmCustomCallTarget(*instr, gemm_backend_config));\n-          const Shape &output_shape = instr->shape();\n-          HloInstruction *gemm_call =\n+          const Shape& output_shape = instr->shape();\n+          HloInstruction* gemm_call =\n               instr->AddInstruction(HloInstruction::CreateCustomCall(\n                   output_shape,\n                   {instr->mutable_operand(0), instr->mutable_operand(1)},\n@@ -700,8 +700,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n   }\n \n   absl::Status TurnDotIntoConvertAndDotForBF16BF16F32(\n-      HloInstruction *instr, GemmBackendConfig &gemm_backend_config,\n-      GpuBackendConfig &gpu_backend_config) {\n+      HloInstruction* instr, GemmBackendConfig& gemm_backend_config,\n+      GpuBackendConfig& gpu_backend_config) {\n     auto lhs_shape = instr->operand(0)->shape();\n     lhs_shape.set_element_type(BF16);\n     auto lhs_convert = instr->mutable_operand(0)->AddInstruction(\n@@ -714,24 +714,24 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     TF_ASSIGN_OR_RETURN(\n         absl::string_view gemm_custom_call_target,\n         GetNonFp8GemmCustomCallTarget(*instr, gemm_backend_config));\n-    const Shape &output_shape = instr->shape();\n-    HloInstruction *gemm_call =\n+    const Shape& output_shape = instr->shape();\n+    HloInstruction* gemm_call =\n         instr->AddInstruction(HloInstruction::CreateCustomCall(\n             output_shape, {lhs_convert, rhs_convert}, gemm_custom_call_target));\n     TF_RETURN_IF_ERROR(gemm_call->set_backend_config(gpu_backend_config));\n     TF_RETURN_IF_ERROR(ReplaceInstruction(instr, gemm_call));\n     return absl::OkStatus();\n   }\n \n-  absl::Status HandleMultiply(HloInstruction *instr) override {\n+  absl::Status HandleMultiply(HloInstruction* instr) override {\n     HloInstruction *alpha, *existing_gemm;\n     if (Match(instr,\n               m::MultiplyAnyOrder(\n                   GemmOrCublasLtMatmulMaybeF8(&existing_gemm).WithOneUser(),\n                   m::Broadcast(m::ConstantScalar(&alpha)).WithOneUser()))) {\n       TF_ASSIGN_OR_RETURN(auto gpu_config,\n                           existing_gemm->backend_config<GpuBackendConfig>());\n-      GemmBackendConfig &config = *gpu_config.mutable_gemm_backend_config();\n+      GemmBackendConfig& config = *gpu_config.mutable_gemm_backend_config();\n       // Do not fuse alpha into S32 GEMM, as they only support fixed values for\n       // alpha/beta.\n       if (existing_gemm->shape().element_type() == S32) {\n@@ -749,7 +749,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n       }\n     }\n \n-    HloInstruction *d_scale;\n+    HloInstruction* d_scale;\n     if (Match(instr, m::MultiplyAnyOrder(\n                          CublasLtMatmulF8(&existing_gemm).WithOneUser(),\n                          m::Broadcast(m::Op(&d_scale)).WithOneUser()))) {\n@@ -839,7 +839,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n   }\n \n   // Fuse the scaling of an FP8 GEMM into the Custom Call.\n-  absl::Status HandleDivide(HloInstruction *instr) override {\n+  absl::Status HandleDivide(HloInstruction* instr) override {\n     HloInstruction *existing_gemm, *d_scale;\n     if (Match(instr, m::Divide(CublasLtMatmulF8(&existing_gemm).WithOneUser(),\n                                m::Broadcast(m::Op(&d_scale)).WithOneUser()))) {\n@@ -848,16 +848,16 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     return absl::OkStatus();\n   }\n \n-  absl::Status HandleAdd(HloInstruction *instr) override {\n+  absl::Status HandleAdd(HloInstruction* instr) override {\n     if (options_.bias_mode == GemmRewriterOptions::BiasMode::kNoBias) {\n       // See comments for `GemmRewriterOptions::BiasMode` for details.\n       return absl::OkStatus();\n     }\n \n     HloInstruction *bias, *existing_gemm = nullptr;\n-    HloInstruction *optional_slice = nullptr;\n-    HloInstruction *optional_convert = nullptr;\n-    HloInstruction *optional_bitcast = nullptr;\n+    HloInstruction* optional_slice = nullptr;\n+    HloInstruction* optional_convert = nullptr;\n+    HloInstruction* optional_bitcast = nullptr;\n     // Attempt to elide broadcast and fuse addition of a vector bias into\n     // GEMM, including when slicing is applied to the result.\n     if (Match(instr,\n@@ -924,7 +924,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n                       GemmOrCublasLtMatmulMaybeF8(&existing_gemm).WithOneUser())\n                       .WithOneUser(),\n                   m::Op(&bias).WithPredicate(is_not_broadcast)))) {\n-      HloInstruction *new_bitcast =\n+      HloInstruction* new_bitcast =\n           MakeBitcastHlo(bias, existing_gemm->shape(), &bias->metadata());\n       TF_ASSIGN_OR_RETURN(HloInstruction * new_add,\n                           MakeBinaryHlo(HloOpcode::kAdd, existing_gemm,\n@@ -949,7 +949,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n                   m::Op(&bias).WithPredicate(is_not_broadcast)))) {\n       TF_ASSIGN_OR_RETURN(GpuBackendConfig gpu_backend_config,\n                           existing_gemm->backend_config<GpuBackendConfig>());\n-      const GemmBackendConfig &gemm_backend_config =\n+      const GemmBackendConfig& gemm_backend_config =\n           gpu_backend_config.gemm_backend_config();\n       // check if type combination is supported here\n       TF_ASSIGN_OR_RETURN(\n@@ -976,8 +976,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n       }\n     }\n \n-    HloInstruction *optional_bitcast_matrix = nullptr;\n-    HloInstruction *optional_slice_matrix = nullptr;\n+    HloInstruction* optional_bitcast_matrix = nullptr;\n+    HloInstruction* optional_slice_matrix = nullptr;\n     if (Match(instr,\n               m::AddAnyOrder(\n                   OptionalBitcast(\n@@ -999,9 +999,9 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     return absl::OkStatus();\n   }\n \n-  absl::Status HandleMaximum(HloInstruction *instr) override {\n+  absl::Status HandleMaximum(HloInstruction* instr) override {\n     HloInstruction *existing_gemm, *zeros;\n-    HloInstruction *optional_slice_or_bitcast = nullptr;\n+    HloInstruction* optional_slice_or_bitcast = nullptr;\n     // Attempt to elide maximum and fuse ReLU activation into GEMM, including\n     // when slicing or bitcasting is applied to the result.\n     if (Match(instr,\n@@ -1022,7 +1022,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     return absl::OkStatus();\n   }\n \n-  absl::Status HandleConvert(HloInstruction *instr) override {\n+  absl::Status HandleConvert(HloInstruction* instr) override {\n     HloInstruction *clamp_lower, *clamp_upper, *existing_gemm,\n         *d_scale = nullptr, *binary = nullptr;\n     // Attempt to elide the scaling and conversion of the result of an FP8\n@@ -1049,37 +1049,37 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     return absl::OkStatus();\n   }\n \n-  static bool IsCuda(const se::GpuComputeCapability &gpu_version) {\n+  static bool IsCuda(const se::GpuComputeCapability& gpu_version) {\n     return std::holds_alternative<se::CudaComputeCapability>(gpu_version);\n   }\n \n   static absl::StatusOr<se::CudaComputeCapability> GetCudaComputeCapability(\n-      const se::GpuComputeCapability &gpu_version) {\n-    auto *cuda_cc = std::get_if<se::CudaComputeCapability>(&gpu_version);\n+      const se::GpuComputeCapability& gpu_version) {\n+    auto* cuda_cc = std::get_if<se::CudaComputeCapability>(&gpu_version);\n     if (cuda_cc == nullptr) {\n       return absl::InvalidArgumentError(\"Compute Capability is not CUDA.\");\n     }\n     return *cuda_cc;\n   }\n \n-  static bool IsRocm(const se::GpuComputeCapability &gpu_version) {\n+  static bool IsRocm(const se::GpuComputeCapability& gpu_version) {\n     return std::holds_alternative<se::RocmComputeCapability>(gpu_version);\n   }\n \n   static absl::StatusOr<se::RocmComputeCapability> GetRocmComputeCapability(\n-      const se::GpuComputeCapability &gpu_version) {\n+      const se::GpuComputeCapability& gpu_version) {\n     auto rocm_cc = std::get_if<se::RocmComputeCapability>(&gpu_version);\n     if (rocm_cc == nullptr) {\n       return absl::InvalidArgumentError(\"Compute Capability is not ROCm.\");\n     }\n     return *rocm_cc;\n   }\n \n-  absl::StatusOr<bool> CreateF8CustomCall(HloInstruction *instr,\n-                                          GpuBackendConfig &gpu_backend_config,\n+  absl::StatusOr<bool> CreateF8CustomCall(HloInstruction* instr,\n+                                          GpuBackendConfig& gpu_backend_config,\n                                           MatchedFp8Param a,\n                                           MatchedFp8Param b) {\n-    GemmBackendConfig &gemm_backend_config =\n+    GemmBackendConfig& gemm_backend_config =\n         *gpu_backend_config.mutable_gemm_backend_config();\n     se::CudaComputeCapability cuda_compute_capability;\n     if (IsCuda(gpu_version_)) {\n@@ -1198,10 +1198,10 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     // cuBLASLt FP8 GEMM kernels require the scaling factors to be in F32\n     // format. Set the factors to one when no scaling factors were captured.\n     std::array<bool, 2> mult_scale{a.mult_scale, b.mult_scale};\n-    std::array<HloInstruction *, 2> scales{a.scale, b.scale}, inv_scales,\n+    std::array<HloInstruction*, 2> scales{a.scale, b.scale}, inv_scales,\n         scales_f32;\n-    HloInstruction *one_constant = nullptr;\n-    auto one = [&one_constant, instr]() -> HloInstruction * {\n+    HloInstruction* one_constant = nullptr;\n+    auto one = [&one_constant, instr]() -> HloInstruction* {\n       if (!one_constant) {\n         one_constant = instr->AddInstruction(\n             HloInstruction::CreateConstant(LiteralUtil::One(F32)));\n@@ -1298,8 +1298,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n                  \"dimension.\";\n       return false;\n     }\n-    for (const MatchedFp8Param &param : {a, b}) {\n-      const HloInstruction *input = param.commutative_ops.empty()\n+    for (const MatchedFp8Param& param : {a, b}) {\n+      const HloInstruction* input = param.commutative_ops.empty()\n                                         ? param.fp8_input\n                                         : param.commutative_ops.back().first;\n       if (input->shape().dimensions().size() != num_batch_dims + 2) {\n@@ -1312,9 +1312,9 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n \n     // Sequentially apply the collected unary, dynamic-slice, pad and select ops\n     // to the unconverted and unscaled operands.\n-    auto shift_ops = [&instr](HloInstruction *&x, InstrPath &x_ops) -> void {\n-      for (std::pair<HloInstruction *, int> op : x_ops) {\n-        std::vector<HloInstruction *> operands = {x};\n+    auto shift_ops = [&instr](HloInstruction*& x, InstrPath& x_ops) -> void {\n+      for (std::pair<HloInstruction*, int> op : x_ops) {\n+        std::vector<HloInstruction*> operands = {x};\n         // Insert the additional operands of dynamic-slice ops.\n         if (HloPredicateIsOp<HloOpcode::kDynamicSlice>(op.first)) {\n           for (int i = 1; i < op.first->operand_count(); ++i) {\n@@ -1323,7 +1323,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n         }\n         // Convert the second operand of pad ops.\n         if (HloPredicateIsOp<HloOpcode::kPad>(op.first)) {\n-          HloInstruction *convert =\n+          HloInstruction* convert =\n               instr->AddInstruction(HloInstruction::CreateConvert(\n                   ShapeUtil::ChangeElementType(op.first->operand(1)->shape(),\n                                                x->shape().element_type()),\n@@ -1336,7 +1336,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n           operands.emplace(operands.begin(), op.first->mutable_operand(0));\n           // Convert the remaining operand.\n           int operand_idx = op.second == 2 ? 1 : 2;\n-          HloInstruction *convert =\n+          HloInstruction* convert =\n               instr->AddInstruction(HloInstruction::CreateConvert(\n                   ShapeUtil::ChangeElementType(\n                       op.first->operand(operand_idx)->shape(),\n@@ -1359,7 +1359,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n         GemmConfig gemm_config,\n         GemmConfig::For(instr, gemm_backend_config, gpu_version_));\n \n-    DotDimensionNumbers *dim_nums =\n+    DotDimensionNumbers* dim_nums =\n         gemm_backend_config.mutable_dot_dimension_numbers();\n \n     // On non-Blackwell systems, cuBLASLt FP8 GEMM kernels require the first\n@@ -1402,10 +1402,10 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     Shape new_output_shape =\n         PadShapeToMultipleOf16(instr->shape(), out_batch_dims);\n \n-    std::vector<HloInstruction *> operands_list = {\n-        a.fp8_input, b.fp8_input, scales_f32[0], scales_f32[1]};\n+    std::vector<HloInstruction*> operands_list = {a.fp8_input, b.fp8_input,\n+                                                  scales_f32[0], scales_f32[1]};\n \n-    HloInstruction *new_custom_call =\n+    HloInstruction* new_custom_call =\n         instr->AddInstruction(HloInstruction::CreateCustomCall(\n             ShapeUtil::MakeShapeWithDenseLayout(\n                 instr->shape().element_type(), new_output_shape.dimensions(),\n@@ -1415,7 +1415,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     TF_RETURN_IF_ERROR(SetName(instr->GetModule(), new_custom_call));\n \n     // Slice the result of the GEMM if the operands were padded.\n-    HloInstruction *slice = nullptr;\n+    HloInstruction* slice = nullptr;\n     if (new_output_shape.dimensions() != instr->shape().dimensions()) {\n       std::vector<int64_t> start_indices(instr->shape().dimensions().size(), 0);\n       std::vector<int64_t> strides(instr->shape().dimensions().size(), 1);\n@@ -1430,8 +1430,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     return true;\n   }\n \n-  absl::Status F8ScaleD(HloInstruction *instr, HloInstruction *existing_gemm,\n-                        HloInstruction *d_scale) {\n+  absl::Status F8ScaleD(HloInstruction* instr, HloInstruction* existing_gemm,\n+                        HloInstruction* d_scale) {\n     if (!ShapeUtil::IsScalar(d_scale->shape())) {\n       return absl::OkStatus();\n     }\n@@ -1453,7 +1453,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     // has been fused.\n     TF_ASSIGN_OR_RETURN(auto gpu_backend_config,\n                         existing_gemm->backend_config<GpuBackendConfig>());\n-    const GemmBackendConfig &config = gpu_backend_config.gemm_backend_config();\n+    const GemmBackendConfig& config = gpu_backend_config.gemm_backend_config();\n     if ((config.epilogue() != GemmBackendConfig::DEFAULT &&\n          config.epilogue() != GemmBackendConfig::RELU) ||\n         config.beta() != 0.) {\n@@ -1472,9 +1472,9 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     return absl::OkStatus();\n   }\n \n-  absl::Status F8ConvertD(HloInstruction *instr, HloInstruction *existing_gemm,\n-                          HloInstruction *d_scale, HloInstruction *clamp_lower,\n-                          HloInstruction *clamp_upper,\n+  absl::Status F8ConvertD(HloInstruction* instr, HloInstruction* existing_gemm,\n+                          HloInstruction* d_scale, HloInstruction* clamp_lower,\n+                          HloInstruction* clamp_upper,\n                           bool mult_scale = false) {\n     // TODO: add ROCm support to this fusion pattern\n     if (IsRocm(gpu_version_)) {\n@@ -1507,8 +1507,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     // maximum of the absolute value of the result of the GEMM. Since it is\n     // unknown in what form this operation will be used, it is identified in a\n     // top-down approach by inspecting the users of the GEMM.\n-    const std::vector<HloInstruction *> gemm_users = existing_gemm->users();\n-    HloInstruction *reduce_damax = nullptr;\n+    const std::vector<HloInstruction*> gemm_users = existing_gemm->users();\n+    HloInstruction* reduce_damax = nullptr;\n     if (gemm_users.size() == 2) {\n       // In the presence of a ReLU activation, the abs instruction is elided\n       // since abs(ReLU(x)) = ReLU(x).\n@@ -1536,9 +1536,9 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n             maybe_reduce->operands().size() == 2 &&\n             maybe_reduce->operand(1)->opcode() == HloOpcode::kConstant &&\n             ShapeUtil::IsScalar(maybe_reduce->operand(1)->shape())) {\n-          HloInstruction *reduce = maybe_reduce;\n-          HloComputation *reduce_comp = reduce->to_apply();\n-          HloInstruction *reduce_comp_root = reduce_comp->root_instruction();\n+          HloInstruction* reduce = maybe_reduce;\n+          HloComputation* reduce_comp = reduce->to_apply();\n+          HloInstruction* reduce_comp_root = reduce_comp->root_instruction();\n           if (reduce->operand(1)->literal().GetAsDouble({}) <= 0. &&\n               HloPredicateIsOp<HloOpcode::kMaximum>(reduce_comp_root) &&\n               reduce_comp_root->operand(0)->opcode() == HloOpcode::kParameter &&\n@@ -1556,7 +1556,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n \n     TF_ASSIGN_OR_RETURN(auto gpu_backend_config,\n                         existing_gemm->backend_config<GpuBackendConfig>());\n-    const GemmBackendConfig &gemm_backend_config =\n+    const GemmBackendConfig& gemm_backend_config =\n         gpu_backend_config.gemm_backend_config();\n \n     if (gemm_backend_config.beta() != 0.0) {\n@@ -1603,30 +1603,30 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n   }\n \n   // Adds a scalar DAmax return value to an FP8 GEMM.\n-  absl::Status F8AddDAmax(HloInstruction *instr, HloInstruction *existing_gemm,\n-                          HloInstruction *reduce_damax) {\n+  absl::Status F8AddDAmax(HloInstruction* instr, HloInstruction* existing_gemm,\n+                          HloInstruction* reduce_damax) {\n     // Change the output shape of the Custom Call to tuple(D, DAmax).\n     Shape damax_shape = ShapeUtil::MakeScalarShape(F32);\n     Shape tuple_shape =\n         ShapeUtil::MakeTupleShape({instr->shape(), damax_shape});\n-    HloInstruction *gemm_and_damax =\n+    HloInstruction* gemm_and_damax =\n         instr->AddInstruction(existing_gemm->CloneWithNewShape(tuple_shape));\n \n     TF_ASSIGN_OR_RETURN(auto gpu_config,\n                         gemm_and_damax->backend_config<GpuBackendConfig>());\n-    GemmBackendConfig &config = *gpu_config.mutable_gemm_backend_config();\n+    GemmBackendConfig& config = *gpu_config.mutable_gemm_backend_config();\n     config.set_damax_output(true);\n     TF_RETURN_IF_ERROR(gemm_and_damax->set_backend_config(gpu_config));\n \n     // Obtain D and DAmax separately from the output tuple.\n-    HloInstruction *d =\n+    HloInstruction* d =\n         instr->AddInstruction(HloInstruction::CreateGetTupleElement(\n             instr->shape(), gemm_and_damax, 0));\n-    HloInstruction *damax = instr->AddInstruction(\n+    HloInstruction* damax = instr->AddInstruction(\n         HloInstruction::CreateGetTupleElement(damax_shape, gemm_and_damax, 1));\n \n     // Convert DAmax from FP32 to the requested type and elide reduce.\n-    HloInstruction *damax_converted = instr->AddInstruction(\n+    HloInstruction* damax_converted = instr->AddInstruction(\n         HloInstruction::CreateConvert(reduce_damax->shape(), damax));\n     TF_RETURN_IF_ERROR(ReplaceInstruction(reduce_damax, damax_converted));\n     TF_RETURN_IF_ERROR(ReplaceInstruction(instr, d));\n@@ -1640,10 +1640,10 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n   // where 'gemm' is expected to be a cuBLAS custom_call. Slice is introduced\n   // when the inputs of the gemm are possibly padded. Bitcast is introduced to\n   // handle high rank input.\n-  absl::Status FuseMatrixBiasAdd(HloInstruction *instr, HloInstruction *bias,\n-                                 const HloInstruction *gemm,\n-                                 HloInstruction *bitcast = nullptr,\n-                                 HloInstruction *slice = nullptr) {\n+  absl::Status FuseMatrixBiasAdd(HloInstruction* instr, HloInstruction* bias,\n+                                 const HloInstruction* gemm,\n+                                 HloInstruction* bitcast = nullptr,\n+                                 HloInstruction* slice = nullptr) {\n     TF_RET_CHECK(Shape::Equal().IgnoreElementType()(bias->shape(),\n                                                     bitcast ? bitcast->shape()\n                                                     : slice ? slice->shape()\n@@ -1684,7 +1684,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n         // overwrite.\n         return false;\n       }\n-      const auto &in_out_alias_config =\n+      const auto& in_out_alias_config =\n           bias->GetModule()->input_output_alias_config();\n       // If the parameter is aliased, we can overwrite it.\n       // TODO(victorstone): The assumption when calling ParameterHasAlias is\n@@ -1697,7 +1697,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n                              IsCublasLtMatmul(*gemm) || can_overwrite_bias;\n \n     auto gpu_config = gemm->backend_config<GpuBackendConfig>().value();\n-    GemmBackendConfig &config = *gpu_config.mutable_gemm_backend_config();\n+    GemmBackendConfig& config = *gpu_config.mutable_gemm_backend_config();\n     // It is possible to fuse into a cublasLt matmul that already has a vector\n     // bias, but no other epilogue will commute with the matrix bias add.\n     bool supported_epilogue =\n@@ -1711,9 +1711,9 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n \n     config.set_beta(1.0);\n \n-    std::vector<HloInstruction *> operands(gemm->operands().begin(),\n-                                           gemm->operands().end());\n-    HloInstruction *maybe_constant_folded_bias = MaybeConstantFoldBias(bias);\n+    std::vector<HloInstruction*> operands(gemm->operands().begin(),\n+                                          gemm->operands().end());\n+    HloInstruction* maybe_constant_folded_bias = MaybeConstantFoldBias(bias);\n     if (bitcast) {\n       maybe_constant_folded_bias =\n           instr->AddInstruction(HloInstruction::CreateBitcast(\n@@ -1774,12 +1774,12 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n   // convert is only used for F8 matmuls as cublasLt has specific constraints\n   // on the vector bias type for such matmuls. The optional bitcast is\n   // necessary to handle high rank input cases.\n-  absl::StatusOr<bool> FuseVectorBiasAdd(HloInstruction *instr,\n-                                         HloInstruction *broadcast,\n-                                         HloInstruction *gemm,\n-                                         HloInstruction *slice = nullptr,\n-                                         HloInstruction *convert = nullptr,\n-                                         HloInstruction *bitcast = nullptr) {\n+  absl::StatusOr<bool> FuseVectorBiasAdd(HloInstruction* instr,\n+                                         HloInstruction* broadcast,\n+                                         HloInstruction* gemm,\n+                                         HloInstruction* slice = nullptr,\n+                                         HloInstruction* convert = nullptr,\n+                                         HloInstruction* bitcast = nullptr) {\n     if (!bitcast) {\n       TF_RET_CHECK(ShapeUtil::Compatible(\n           broadcast->shape(), (slice ? slice->shape() : gemm->shape())));\n@@ -1789,45 +1789,42 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n       return false;\n     }\n \n-    HloInstruction *bias = broadcast->mutable_operand(0);\n+    HloInstruction* bias = broadcast->mutable_operand(0);\n \n     TF_ASSIGN_OR_RETURN(auto gpu_config,\n                         gemm->backend_config<GpuBackendConfig>());\n-    GemmBackendConfig &config = *gpu_config.mutable_gemm_backend_config();\n+    GemmBackendConfig& config = *gpu_config.mutable_gemm_backend_config();\n     // # output column dims == # non-contracting rhs operand dims.\n-    const DotDimensionNumbers &dot_dims = config.dot_dimension_numbers();\n+    const DotDimensionNumbers& dot_dims = config.dot_dimension_numbers();\n     size_t num_col_dims = gemm->operand(1)->shape().dimensions().size() -\n                           dot_dims.rhs_batch_dimensions_size() -\n                           dot_dims.rhs_contracting_dimensions_size();\n \n     if ((gemm->user_count() != 1) ||\n-        (config.epilogue() != GemmBackendConfig::DEFAULT) ||\n-        (bias->shape().dimensions().size() != num_col_dims)) {\n+        (config.epilogue() != GemmBackendConfig::DEFAULT)) {\n       return false;\n     }\n+\n+    // By the time the GEMM is rewritten into the custom call, it's always\n+    // canonicalized, e.g. has at most one non-contracting dimension on each\n+    // operand.\n+    if (num_col_dims > 1 || bias->shape().dimensions().size() != num_col_dims) {\n+      return false;\n+    }\n+\n     // We require the bias vector to have been broadcast in the most major\n     // dimensions; i.e. its most minor physical dimensions align with most minor\n     // physical dimensions of the gemm output.\n-    absl::Span<const int64_t> broadcast_dims = broadcast->dimensions();\n-    for (size_t i = 0; i < num_col_dims; ++i) {\n-      int64_t dim =\n-          (bitcast ? bitcast : gemm)->shape().layout().minor_to_major(i);\n-\n-      // Find the corresponding dimension from the bias vector.\n-      auto it = absl::c_find(broadcast_dims, dim);\n-\n-      if (it == broadcast_dims.end()) {\n-        return false;\n-      }\n-\n-      int64_t vector_dim = it - broadcast_dims.begin();\n-      if (bias->shape().layout().minor_to_major(i) != vector_dim) {\n-        return false;\n-      }\n+    const Shape& out_gemm_shape = slice ? slice->shape() : gemm->shape();\n+    if (num_col_dims == 1 &&\n+        bias->shape().dimensions(0) !=\n+            out_gemm_shape.dimensions(\n+                out_gemm_shape.layout().minor_to_major(0))) {\n+      return false;\n     }\n \n-    std::vector<HloInstruction *> operands(gemm->operands().begin(),\n-                                           gemm->operands().end());\n+    std::vector<HloInstruction*> operands(gemm->operands().begin(),\n+                                          gemm->operands().end());\n     // When (non-trivial) matrix and vector bias co-exist for FP8 matmul, just\n     // fuse matrix bias.\n     if (gemm->custom_call_target() == kCublasLtMatmulF8CallTarget &&\n@@ -1841,7 +1838,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n         return false;\n       }\n \n-      HloInstruction *bias_f16_or_bf16 = convert->mutable_operand(0);\n+      HloInstruction* bias_f16_or_bf16 = convert->mutable_operand(0);\n       auto compatible_bias_type = [](const PrimitiveType bias_type,\n                                      const PrimitiveType output_type) {\n         if (bias_type == BF16) {\n@@ -1880,8 +1877,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     // Replace add(gemm, broadcast) with fused new_gemm.\n     operands.push_back(bias);\n     config.set_epilogue(GemmBackendConfig::BIAS);\n-    HloComputation *computation = gemm->parent();\n-    HloInstruction *result = computation->AddInstruction(\n+    HloComputation* computation = gemm->parent();\n+    HloInstruction* result = computation->AddInstruction(\n         gemm->CloneWithNewOperands(gemm->shape(), operands));\n \n     TF_RETURN_IF_ERROR(result->set_backend_config(gpu_config));\n@@ -1899,10 +1896,10 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     return true;\n   }\n \n-  absl::Status FuseReluActivation(HloInstruction *instr,\n-                                  HloInstruction *broadcast,\n-                                  HloInstruction *gemm,\n-                                  HloInstruction *slice_or_bitcast = nullptr) {\n+  absl::Status FuseReluActivation(HloInstruction* instr,\n+                                  HloInstruction* broadcast,\n+                                  HloInstruction* gemm,\n+                                  HloInstruction* slice_or_bitcast = nullptr) {\n     TF_RET_CHECK(ShapeUtil::Compatible(\n         broadcast->shape(),\n         (slice_or_bitcast ? slice_or_bitcast->shape() : gemm->shape())));\n@@ -1917,7 +1914,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n \n     TF_ASSIGN_OR_RETURN(auto gpu_config,\n                         gemm->backend_config<GpuBackendConfig>());\n-    GemmBackendConfig &config = *gpu_config.mutable_gemm_backend_config();\n+    GemmBackendConfig& config = *gpu_config.mutable_gemm_backend_config();\n     if (config.epilogue() == GemmBackendConfig::DEFAULT) {\n       config.set_epilogue(GemmBackendConfig::RELU);\n     } else if (config.epilogue() == GemmBackendConfig::BIAS) {\n@@ -1926,8 +1923,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n       return absl::OkStatus();\n     }\n \n-    HloComputation *computation = gemm->parent();\n-    HloInstruction *result = computation->AddInstruction(gemm->Clone());\n+    HloComputation* computation = gemm->parent();\n+    HloInstruction* result = computation->AddInstruction(gemm->Clone());\n     TF_RETURN_IF_ERROR(result->set_backend_config(gpu_config));\n     TF_RETURN_IF_ERROR(SetName(gemm->GetModule(), result));\n \n@@ -1940,9 +1937,9 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     return ReplaceInstruction(instr, result);\n   }\n \n-  absl::Status FuseGeluActivation(HloInstruction *multiply,\n-                                  HloInstruction *gemm,\n-                                  HloInstruction *slice_or_bitcast = nullptr) {\n+  absl::Status FuseGeluActivation(HloInstruction* multiply,\n+                                  HloInstruction* gemm,\n+                                  HloInstruction* slice_or_bitcast = nullptr) {\n     if (!SupportsEpilogueFusion(gemm->shape().element_type())) {\n       return absl::OkStatus();\n     }\n@@ -1961,7 +1958,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n \n     TF_ASSIGN_OR_RETURN(auto gpu_config,\n                         gemm->backend_config<GpuBackendConfig>());\n-    GemmBackendConfig &config = *gpu_config.mutable_gemm_backend_config();\n+    GemmBackendConfig& config = *gpu_config.mutable_gemm_backend_config();\n \n     if (config.epilogue() == GemmBackendConfig::DEFAULT) {\n       config.set_epilogue(has_aux ? GemmBackendConfig::GELU_AUX\n@@ -1986,7 +1983,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     }\n \n     if (has_aux) {\n-      HloInstruction *tuple_output =\n+      HloInstruction* tuple_output =\n           gemm->parent()->AddInstruction(std::move(output));\n       TF_RETURN_IF_ERROR(ReplaceWithNewInstruction(\n           gemm, HloInstruction::CreateGetTupleElement(tuple_output, 1)));\n@@ -1996,9 +1993,9 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     return ReplaceWithNewInstruction(multiply, std::move(output));\n   }\n \n-  absl::Status FuseSwishActivation(HloInstruction *multiply,\n-                                   HloInstruction *gemm,\n-                                   HloInstruction *slice_or_bitcast = nullptr) {\n+  absl::Status FuseSwishActivation(HloInstruction* multiply,\n+                                   HloInstruction* gemm,\n+                                   HloInstruction* slice_or_bitcast = nullptr) {\n     if (!SupportsEpilogueFusion(gemm->shape().element_type())) {\n       return absl::OkStatus();\n     }\n@@ -2017,7 +2014,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n \n     TF_ASSIGN_OR_RETURN(auto gpu_config,\n                         gemm->backend_config<GpuBackendConfig>());\n-    GemmBackendConfig &config = *gpu_config.mutable_gemm_backend_config();\n+    GemmBackendConfig& config = *gpu_config.mutable_gemm_backend_config();\n \n     if (config.epilogue() == GemmBackendConfig::DEFAULT) {\n       config.set_epilogue(GemmBackendConfig::SILU);\n@@ -2040,7 +2037,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     }\n \n     if (has_aux) {\n-      HloInstruction *tuple_output =\n+      HloInstruction* tuple_output =\n           gemm->parent()->AddInstruction(std::move(output));\n       TF_RETURN_IF_ERROR(ReplaceWithNewInstruction(\n           gemm, HloInstruction::CreateGetTupleElement(tuple_output, 1)));\n@@ -2058,8 +2055,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n   // Choose cublas or cublasLt for the target of the custom call that instr will\n   // be rewritten into.\n   absl::StatusOr<absl::string_view> GetNonFp8GemmCustomCallTarget(\n-      const HloInstruction &instr,\n-      const GemmBackendConfig &gemm_backend_config) const {\n+      const HloInstruction& instr,\n+      const GemmBackendConfig& gemm_backend_config) const {\n     if (!instr.GetModule()\n              ->config()\n              .debug_options()\n@@ -2069,8 +2066,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     }\n \n     // cublasLt is enabled, check if other internal conditions are met.\n-    const HloInstruction *lhs = instr.operand(0);\n-    const HloInstruction *rhs = instr.operand(1);\n+    const HloInstruction* lhs = instr.operand(0);\n+    const HloInstruction* rhs = instr.operand(1);\n     if (lhs->shape().element_type() == S8 ||\n         rhs->shape().element_type() == S8) {\n       return absl::string_view(kGemmCallTarget);\n@@ -2089,8 +2086,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n   }\n \n   absl::StatusOr<bool> TypesAreSupportedByLegacyCublas(\n-      const HloInstruction &instr, const GemmBackendConfig &gemm_backend_config,\n-      const HloInstruction *bias = nullptr) const {\n+      const HloInstruction& instr, const GemmBackendConfig& gemm_backend_config,\n+      const HloInstruction* bias = nullptr) const {\n     // Figure out the Atype/Btype.\n     const PrimitiveType a_dtype = instr.operand(0)->shape().element_type();\n     const PrimitiveType b_dtype = instr.operand(1)->shape().element_type();\n@@ -2179,8 +2176,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n   }\n \n   absl::StatusOr<bool> TypesAreSupportedByCublasLt(\n-      const HloInstruction &instr, const GemmBackendConfig &backend_config,\n-      const HloInstruction *bias = nullptr) const {\n+      const HloInstruction& instr, const GemmBackendConfig& backend_config,\n+      const HloInstruction* bias = nullptr) const {\n     // Figure out the Atype/Btype.\n     const PrimitiveType a_dtype = instr.operand(0)->shape().element_type();\n     const PrimitiveType b_dtype = instr.operand(1)->shape().element_type();\n@@ -2390,10 +2387,10 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n   }\n \n   absl::StatusOr<bool> GemmIsSupportedByCublasLt(\n-      const HloInstruction &instr,\n-      const GemmBackendConfig &gemm_backend_config) const {\n-    const HloInstruction *lhs = instr.operand(0);\n-    const Shape &output_shape = instr.shape();\n+      const HloInstruction& instr,\n+      const GemmBackendConfig& gemm_backend_config) const {\n+    const HloInstruction* lhs = instr.operand(0);\n+    const Shape& output_shape = instr.shape();\n \n     TF_ASSIGN_OR_RETURN(\n         bool types_are_supported_by_cublas_lt,\n@@ -2407,7 +2404,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     constexpr int64_t kMaxBatchCount = 65535;\n     // We get the batch dimension size from lhs here, but we could just as well\n     // use rhs; they are guaranteed to be the same.\n-    const auto &batch_dimensions =\n+    const auto& batch_dimensions =\n         gemm_backend_config.dot_dimension_numbers().lhs_batch_dimensions();\n     int batch_count = (batch_dimensions.empty() ? 0 : 1);\n     // All batch dimensions get flattened into a single batch dimension.\n@@ -2454,21 +2451,21 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n \n   // Turns an F8 dot with unsupported output type into an F8 dot with F32\n   // output, and converting the F32 output to unsupported output types.\n-  absl::StatusOr<HloInstruction *> TurnF8DotWithUnsupportedOutputTypeIntoF32(\n-      HloInstruction *instr) {\n+  absl::StatusOr<HloInstruction*> TurnF8DotWithUnsupportedOutputTypeIntoF32(\n+      HloInstruction* instr) {\n     Shape output_f32_shape = instr->shape();\n     output_f32_shape.set_element_type(F32);\n-    HloInstruction *f32_dot =\n+    HloInstruction* f32_dot =\n         instr->AddInstruction(instr->CloneWithNewShape(output_f32_shape));\n-    HloInstruction *convert = instr->AddInstruction(\n+    HloInstruction* convert = instr->AddInstruction(\n         HloInstruction::CreateConvert(instr->shape(), f32_dot));\n     TF_RETURN_IF_ERROR(ReplaceInstruction(instr, convert));\n     return f32_dot;\n   }\n \n   // Turns an F8 dot into an F16 dot, converting operands to F16 (or BF16) and\n   // converting the output back to F8.\n-  absl::StatusOr<HloInstruction *> TurnF8DotIntoF16Dot(HloInstruction *instr) {\n+  absl::StatusOr<HloInstruction*> TurnF8DotIntoF16Dot(HloInstruction* instr) {\n     DCHECK(IsF8Type(instr->operand(0)));\n     DCHECK(IsF8Type(instr->operand(1)));\n \n@@ -2480,7 +2477,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     for (int i = 0; i < 2; ++i) {\n       Shape operand_f16_shape = instr->operand(i)->shape();\n       operand_f16_shape.set_element_type(conv_type);\n-      HloInstruction *convert =\n+      HloInstruction* convert =\n           instr->AddInstruction(HloInstruction::CreateConvert(\n               operand_f16_shape, instr->mutable_operand(i)));\n       TF_RETURN_IF_ERROR(instr->ReplaceOperandWith(i, convert));\n@@ -2490,7 +2487,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     if (IsF8Type(instr)) {\n       Shape output_f16_shape = instr->shape();\n       output_f16_shape.set_element_type(F16);\n-      HloInstruction *f16_dot =\n+      HloInstruction* f16_dot =\n           instr->AddInstruction(instr->CloneWithNewShape(output_f16_shape));\n       HloInstruction* convert_to_f8 = instr->AddInstruction(\n           HloInstruction::CreateConvert(instr->shape(), f16_dot));\n@@ -2507,16 +2504,16 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n class GemmWorkspaceRewriteVisitor : public DfsHloRewriteVisitor {\n  public:\n   explicit GemmWorkspaceRewriteVisitor(\n-      const se::GpuComputeCapability &gpu_version)\n+      const se::GpuComputeCapability& gpu_version)\n       : gpu_version_(gpu_version) {}\n \n-  absl::Status HandleCustomCall(HloInstruction *instr) override {\n+  absl::Status HandleCustomCall(HloInstruction* instr) override {\n     bool has_aux_output = false;\n     if (instr->custom_call_target() == kCublasLtMatmulCallTarget ||\n         instr->custom_call_target() == kCublasLtMatmulF8CallTarget) {\n       TF_ASSIGN_OR_RETURN(const auto gpu_config,\n                           instr->backend_config<xla::gpu::GpuBackendConfig>());\n-      const xla::gpu::GemmBackendConfig &config =\n+      const xla::gpu::GemmBackendConfig& config =\n           gpu_config.gemm_backend_config();\n       xla::gpu::GemmBackendConfig_Epilogue epilogue = config.epilogue();\n       TF_ASSIGN_OR_RETURN(\n@@ -2538,11 +2535,11 @@ class GemmWorkspaceRewriteVisitor : public DfsHloRewriteVisitor {\n     // otherwise cuBLAS will use its own internal pool which will be competing\n     // with XLA allocator for device memory.\n     int64_t workspace = GemmConfig::kDefaultWorkspace;\n-    auto *cuda_cc = std::get_if<se::CudaComputeCapability>(&gpu_version_);\n+    auto* cuda_cc = std::get_if<se::CudaComputeCapability>(&gpu_version_);\n     if (cuda_cc != nullptr && cuda_cc->IsAtLeastHopper()) {\n       workspace = GemmConfig::kHopperWorkspace;\n     }\n-    auto *rocm_cc = std::get_if<se::RocmComputeCapability>(&gpu_version_);\n+    auto* rocm_cc = std::get_if<se::RocmComputeCapability>(&gpu_version_);\n     if (rocm_cc != nullptr) {\n       if (rocm_cc->gfx_version() == \"gfx942\") {\n         workspace = GemmConfig::kGFX942Workspace;\n@@ -2560,7 +2557,7 @@ class GemmWorkspaceRewriteVisitor : public DfsHloRewriteVisitor {\n     // sense, we should tweak it to find the minimal workspace size.\n     if (instr->custom_call_target() == kGemmCallTarget) {\n       int64_t operands_byte_size = 0;\n-      for (auto &operand : instr->operands()) {\n+      for (auto& operand : instr->operands()) {\n         operands_byte_size += ShapeUtil::ByteSizeOf(operand->shape());\n       }\n       workspace = std::min(workspace, operands_byte_size);\n@@ -2574,37 +2571,38 @@ class GemmWorkspaceRewriteVisitor : public DfsHloRewriteVisitor {\n     Shape output_shape = ShapeUtil::MakeTupleShape(output_shapes);\n \n     // Clone custom call with a new shape.\n-    HloInstruction *new_call = instr->AddInstruction(\n+    HloInstruction* new_call = instr->AddInstruction(\n         instr->CloneWithNewOperands(output_shape, instr->operands()));\n \n     // Update operand aliasing if it was a fused gemm with aliased output.\n-    auto *custom_call = xla::Cast<HloCustomCallInstruction>(new_call);\n+    auto* custom_call = xla::Cast<HloCustomCallInstruction>(new_call);\n     if (!custom_call->output_to_operand_aliasing().empty()) {\n       custom_call->set_output_to_operand_aliasing({{{0}, {2, {}}}});\n     }\n \n     if (instr->shape().IsTuple()) {\n       for (auto user : instr->users()) {\n         auto user_get_tuple =\n-            dynamic_cast<HloGetTupleElementInstruction *>(user);\n+            dynamic_cast<HloGetTupleElementInstruction*>(user);\n         TF_RET_CHECK(user_get_tuple);\n-        HloInstruction *get_output =\n+        HloInstruction* get_output =\n             instr->AddInstruction(HloInstruction::CreateGetTupleElement(\n                 new_call, user_get_tuple->tuple_index()));\n         TF_RETURN_IF_ERROR(ReplaceInstruction(user_get_tuple, get_output));\n       }\n       return absl::OkStatus();\n+    } else {\n+      HloInstruction* get_output = instr->AddInstruction(\n+          HloInstruction::CreateGetTupleElement(new_call, 0));\n+      return ReplaceInstruction(instr, get_output);\n     }\n-    HloInstruction* get_output = instr->AddInstruction(\n-        HloInstruction::CreateGetTupleElement(new_call, 0));\n-    return ReplaceInstruction(instr, get_output);\n   }\n \n  private:\n   se::GpuComputeCapability gpu_version_;\n };\n \n-absl::StatusOr<bool> RunOnComputation(HloComputation *computation,\n+absl::StatusOr<bool> RunOnComputation(HloComputation* computation,\n                                       se::GpuComputeCapability gpu_version,\n                                       se::SemanticVersion toolkit_version,\n                                       GemmRewriterOptions options) {\n@@ -2625,10 +2623,10 @@ GemmRewriter::GemmRewriter(se::GpuComputeCapability gpu_version,\n       options_(options) {}\n \n absl::StatusOr<bool> GemmRewriter::Run(\n-    HloModule *module,\n-    const absl::flat_hash_set<absl::string_view> &execution_threads) {\n+    HloModule* module,\n+    const absl::flat_hash_set<absl::string_view>& execution_threads) {\n   bool changed = false;\n-  for (HloComputation *computation :\n+  for (HloComputation* computation :\n        module->MakeNonfusionComputations(execution_threads)) {\n     TF_ASSIGN_OR_RETURN(bool result,\n                         RunOnComputation(computation, gpu_version_,"
        }
    ],
    "stats": {
        "total": 439,
        "additions": 240,
        "deletions": 199
    }
}