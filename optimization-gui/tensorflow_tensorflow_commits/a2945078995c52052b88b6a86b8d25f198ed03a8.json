{
    "author": "derdrdirk",
    "message": "[XLA:GPU] Simplify DnnSupport by removing not used methods: DoConvole, ConvolveWithAlgorithm and DoPrepareForConvolution.\n\nPiperOrigin-RevId: 840215329",
    "sha": "a2945078995c52052b88b6a86b8d25f198ed03a8",
    "files": [
        {
            "sha": "5f96a88e016daf1d2ed13c3b4b6a9e4ec2968142",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_dnn.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a2945078995c52052b88b6a86b8d25f198ed03a8/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a2945078995c52052b88b6a86b8d25f198ed03a8/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.cc?ref=a2945078995c52052b88b6a86b8d25f198ed03a8",
            "patch": "@@ -4256,20 +4256,6 @@ absl::StatusOr<CudnnGraph> GetCudnnFlashAttentionOperationGraph(\n #endif\n }\n \n-absl::Status CudnnSupport::DoPrepareForConvolution(\n-    dnn::ConvolutionKind kind, dnn::DataType element_type, Stream* stream,\n-    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n-    const dnn::FilterDescriptor& filter_descriptor,\n-    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n-    DeviceMemoryBase output_data,\n-    const dnn::ConvolutionDescriptor& convolution_descriptor,\n-    const dnn::AlgorithmConfig& algorithm_config,\n-    ScratchAllocator* scratch_allocator, dnn::AlgorithmDesc* algorithm_desc,\n-    DeviceMemory<uint8_t>* scratch_memory) {\n-  return absl::UnimplementedError(\n-      \"DoPrepareForConvolution is not implemented on CUDA platform.\");\n-}\n-\n absl::StatusOr<CudnnGraph> GetCudnnFlashAttentionF8OperationGraph(\n     dnn::DnnSupport& dnn_support,\n     const dnn::MatmulTensorDescriptor& q_descriptor,\n@@ -4996,27 +4982,6 @@ absl::StatusOr<CudnnGraph> GetCudnnFlashAttentionBackwardOperationGraph(\n #endif\n }\n \n-absl::Status CudnnSupport::DoConvolve(\n-    dnn::ConvolutionKind kind, dnn::DataType element_type,\n-    dnn::DataType output_type, Stream* stream,\n-    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n-    const dnn::FilterDescriptor& filter_descriptor,\n-    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n-    DeviceMemoryBase output_data,\n-    const dnn::ConvolutionDescriptor& convolution_descriptor,\n-    dnn::AlgorithmDesc algorithm_desc, DeviceMemory<uint8_t> scratch_memory,\n-    dnn::ProfileResult* profile_result) {\n-  TF_ASSIGN_OR_RETURN(\n-      std::unique_ptr<const dnn::ConvRunner> runner,\n-      ConvolveRunnerFromDesc(stream, algorithm_desc, kind,\n-                             /*input_type=*/element_type, output_type,\n-                             input_descriptor, filter_descriptor,\n-                             output_descriptor, convolution_descriptor));\n-\n-  return (*runner)(stream, profile_result, scratch_memory, input_data,\n-                   filter_data, output_data);\n-}\n-\n // Utility for dealing with CUDA's type-erased scaling parameters, where some\n // sets of parameters expect a void* pointing at a float while others expect\n // it to point at a double."
        },
        {
            "sha": "4051f7f48151dbe6a24d5c9b71cddd763011ee01",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_dnn.h",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a2945078995c52052b88b6a86b8d25f198ed03a8/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a2945078995c52052b88b6a86b8d25f198ed03a8/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_dnn.h?ref=a2945078995c52052b88b6a86b8d25f198ed03a8",
            "patch": "@@ -441,18 +441,6 @@ class CudnnSupport : public dnn::DnnSupport {\n       DeviceMemory<uint8_t>* reserve_space_data,\n       ScratchAllocator* workspace_allocator) override;\n \n-  absl::Status DoConvolve(\n-      dnn::ConvolutionKind kind, dnn::DataType element_type,\n-      dnn::DataType output_type, Stream* stream,\n-      const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n-      const dnn::FilterDescriptor& filter_descriptor,\n-      DeviceMemoryBase filter_data,\n-      const dnn::BatchDescriptor& output_descriptor,\n-      DeviceMemoryBase output_data,\n-      const dnn::ConvolutionDescriptor& convolution_descriptor,\n-      dnn::AlgorithmDesc algorithm_desc, DeviceMemory<uint8_t> scratch_memory,\n-      dnn::ProfileResult* output_profile_result) override;\n-\n   absl::Status DoFusedConvolve(\n       Stream* stream, dnn::DataType input_type, dnn::DataType side_input_type,\n       dnn::DataType bias_type, dnn::DataType output_type,\n@@ -673,18 +661,6 @@ class CudnnSupport : public dnn::DnnSupport {\n       DeviceMemory<uint8_t> scratch_memory, int ctc_loss_algo_id);\n \n  private:\n-  absl::Status DoPrepareForConvolution(\n-      dnn::ConvolutionKind kind, dnn::DataType element_type, Stream* stream,\n-      const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n-      const dnn::FilterDescriptor& filter_descriptor,\n-      DeviceMemoryBase filter_data,\n-      const dnn::BatchDescriptor& output_descriptor,\n-      DeviceMemoryBase output_data,\n-      const dnn::ConvolutionDescriptor& convolution_descriptor,\n-      const dnn::AlgorithmConfig& algorithm_config,\n-      ScratchAllocator* scratch_allocator, dnn::AlgorithmDesc* algorithm_desc,\n-      DeviceMemory<uint8_t>* scratch_memory) override;\n-\n   absl::Status DoPrepareForCtcLoss(\n       Stream* stream, dnn::DataType element_type,\n       const dnn::RnnStateTensorDescriptor& probs_desc,"
        },
        {
            "sha": "73e795e0d3af86fccd2df83b16b79c23d1a40cf0",
            "filename": "third_party/xla/xla/stream_executor/dnn.h",
            "status": "modified",
            "additions": 0,
            "deletions": 104,
            "changes": 104,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a2945078995c52052b88b6a86b8d25f198ed03a8/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdnn.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a2945078995c52052b88b6a86b8d25f198ed03a8/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdnn.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fdnn.h?ref=a2945078995c52052b88b6a86b8d25f198ed03a8",
            "patch": "@@ -1381,26 +1381,6 @@ class DnnSupport {\n         output_profile_result);\n   }\n \n-  template <typename ElementType, typename OutputType>\n-  absl::Status PrepareForConvolution(\n-      ConvolutionKind kind, Stream* stream,\n-      const BatchDescriptor& batch_descriptor,\n-      DeviceMemory<ElementType> input_data,\n-      const FilterDescriptor& filter_descriptor,\n-      DeviceMemory<ElementType> filter_data,\n-      const BatchDescriptor& output_descriptor,\n-      DeviceMemory<OutputType> output_data,\n-      const ConvolutionDescriptor& convolution_descriptor,\n-      const AlgorithmConfig& algorithm_config,\n-      ScratchAllocator* scratch_allocator, AlgorithmDesc* algorithm_desc,\n-      DeviceMemory<uint8_t>* scratch_memory) {\n-    return DoPrepareForConvolution(\n-        kind, ToDataType<ElementType>::value, stream, batch_descriptor,\n-        input_data, filter_descriptor, filter_data, output_descriptor,\n-        output_data, convolution_descriptor, algorithm_config,\n-        scratch_allocator, algorithm_desc, scratch_memory);\n-  }\n-\n   // cuDNN-specific input transformation that allows running int8x32\n   // convolutions faster using Tensor Core IMMA instruction.\n   virtual absl::Status CudnnReorderConvolutionFilterAndBias(\n@@ -1414,76 +1394,6 @@ class DnnSupport {\n         \"convolution implementation.\");\n   }\n \n-  // Enqueues a single-precision convolution operation onto the stream.\n-  //\n-  // Arguments (all borrowed):\n-  //  stream: borrowed pointer to the stream that the 'convolve' operation\n-  //    should be enqueued onto.\n-  //  input_descriptor: dimensions of the input layer.\n-  //  input_data: un-owned device memory region which contains the\n-  //    convolution input.\n-  //  filter_descriptor: dimensions of the convolution filter.\n-  //  convolution_descriptor: stride of the convolution filter.\n-  //  output_descriptor: dimensions of the output layer.\n-  //  output_data: un-owned device memory region in which to place the\n-  //    convolution result.\n-  //  algorithm_desc: specifies which algorithm should be used for the\n-  //    operation.\n-  //  scratch: un-owned device memory for scratch space in order to speed up\n-  //    the convolution operation.\n-  //  output_profile_result: the output profile result for this call. The\n-  //    profiling is only enabled when this is not nullptr.\n-  //\n-  // input_descriptor, filter_descriptor, convolution_descriptor and\n-  // output_descriptor together specify exactly how the convolution is aligned\n-  // with the input data:\n-  //\n-  // * (input dimensions - filter size + 1) / filter stride == output dimensions\n-  //   corresponds to dist_belief padding = VALID, i.e. the input is not padded.\n-  // * input dimensions / filter stride == output dimensions\n-  //   corresponds to dist_belief padding = SAME, i.e. input and output are the\n-  //   same size - this requires padding the input.\n-  // * (input dimensions + filter size - 1) / filter stride == output dimensions\n-  //   corresponds to dist_belief padding = FULL, i.e. the output is sized so\n-  //   that if the inverse of the filter is applied to the output in VALID mode\n-  //   the result is the same size as the input - this requires even more\n-  //   padding of the input.\n-  virtual absl::Status DoConvolve(\n-      ConvolutionKind kind, DataType element_type, DataType output_type,\n-      Stream* stream, const BatchDescriptor& input_descriptor,\n-      DeviceMemoryBase input_data, const FilterDescriptor& filter_descriptor,\n-      DeviceMemoryBase filter_data, const BatchDescriptor& output_descriptor,\n-      DeviceMemoryBase output_data,\n-      const ConvolutionDescriptor& convolution_descriptor,\n-      AlgorithmDesc algorithm_desc, DeviceMemory<uint8_t> scratch_memory,\n-      ProfileResult* output_profile_result) = 0;\n-\n-  template <typename InputType, typename OutputType>\n-  absl::Status ConvolveWithAlgorithm(\n-      Stream* stream, ConvolutionKind kind,\n-      const BatchDescriptor& input_descriptor,\n-      DeviceMemory<InputType> input_data,\n-      const FilterDescriptor& filter_descriptor,\n-      DeviceMemory<InputType> filter_data,\n-      const BatchDescriptor& output_descriptor,\n-      DeviceMemory<OutputType> output_data,\n-      const ConvolutionDescriptor& convolution_descriptor,\n-      ScratchAllocator* scratch_allocator,\n-      const AlgorithmConfig& algorithm_config,\n-      ProfileResult* output_profile_result) {\n-    DeviceMemory<uint8_t> scratch_memory;\n-    AlgorithmDesc algorithm_desc;\n-    TF_RETURN_IF_ERROR(PrepareForConvolution(\n-        kind, stream, input_descriptor, input_data, filter_descriptor,\n-        filter_data, output_descriptor, output_data, convolution_descriptor,\n-        algorithm_config, scratch_allocator, &algorithm_desc, &scratch_memory));\n-    return DoConvolve(kind, ToDataType<InputType>::value,\n-                      ToDataType<OutputType>::value, stream, input_descriptor,\n-                      input_data, filter_descriptor, filter_data,\n-                      output_descriptor, output_data, convolution_descriptor,\n-                      algorithm_desc, scratch_memory, output_profile_result);\n-  }\n-\n   virtual absl::Status GetConvolveRunners(\n       ConvolutionKind kind, DataType input_type, DataType output_type,\n       Stream* stream, const BatchDescriptor& input_descriptor,\n@@ -2079,20 +1989,6 @@ class DnnSupport {\n   static bool IsStatusOk(const absl::Status& status, bool report_error);\n \n  private:\n-  virtual absl::Status DoPrepareForConvolution(\n-      ConvolutionKind kind, DataType element_type, Stream* stream,\n-      const BatchDescriptor& batch_descriptor, DeviceMemoryBase input_data,\n-      const FilterDescriptor& filter_descriptor, DeviceMemoryBase filter_data,\n-      const BatchDescriptor& output_descriptor, DeviceMemoryBase output_data,\n-      const ConvolutionDescriptor& convolution_descriptor,\n-      const AlgorithmConfig& algorithm_config,\n-      ScratchAllocator* scratch_allocator, AlgorithmDesc* algorithm_desc,\n-      DeviceMemory<uint8_t>* scratch_memory) {\n-    *algorithm_desc = {};\n-    *scratch_memory = {};\n-    return absl::OkStatus();\n-  }\n-\n   virtual absl::Status DoPrepareForCtcLoss(\n       Stream* stream, DataType element_type,\n       const RnnStateTensorDescriptor& probs_desc,"
        },
        {
            "sha": "bed0e1df548a832af698b46fab924812d9e0d693",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_dnn.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 67,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a2945078995c52052b88b6a86b8d25f198ed03a8/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_dnn.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a2945078995c52052b88b6a86b8d25f198ed03a8/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_dnn.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_dnn.cc?ref=a2945078995c52052b88b6a86b8d25f198ed03a8",
            "patch": "@@ -3252,53 +3252,6 @@ void MIOpenDeallocatorCallback(void* ctx, void* mem) {\n   // reclaim the memory\n }\n \n-absl::Status MIOpenSupport::DoPrepareForConvolution(\n-    dnn::ConvolutionKind kind, dnn::DataType element_type, Stream* stream,\n-    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n-    const dnn::FilterDescriptor& filter_descriptor,\n-    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n-    DeviceMemoryBase output_data,\n-    const dnn::ConvolutionDescriptor& convolution_descriptor,\n-    const dnn::AlgorithmConfig& algorithm_config,\n-    ScratchAllocator* scratch_allocator, dnn::AlgorithmDesc* algorithm_desc,\n-    DeviceMemory<uint8_t>* scratch_memory) {\n-  std::optional<dnn::AlgorithmDesc> input_algo_desc =\n-      algorithm_config.algorithm();\n-\n-  assert(input_algo_desc.has_value());\n-\n-  // An algorithm has been specified.\n-  *algorithm_desc = *input_algo_desc;\n-\n-  assert(algorithm_config.scratch_size().has_value());\n-\n-  size_t scratch_memory_size = *(algorithm_config.scratch_size());\n-\n-  // allocate scratch memory\n-  if (scratch_memory_size != 0) {\n-    if (scratch_allocator == nullptr) {\n-      return absl::InternalError(\n-          \"An allocator must be specified when scratch memory is needed\");\n-    }\n-    auto allocated = scratch_allocator->AllocateBytes(scratch_memory_size);\n-    if (allocated.ok()) {\n-      *scratch_memory = allocated.value();\n-    } else {\n-      LOG(ERROR)\n-          << \"Failed to allocate scratch memory - \"\n-          << allocated.status().message() << \"\\n\"\n-          << \"\\tYou can set the env var TF_CUDNN_WORKSPACE_LIMIT_IN_MB to a \"\n-             \"larger number (e.g. 8192) to increase the max memory limit.\\n\"\n-          << \"\\tIncreasing the max memory limit might help resolve this \"\n-             \"error\";\n-      return absl::InternalError(absl::StrCat(\n-          \"Failed to allocate scratch memory of size: \", scratch_memory_size));\n-    }\n-  }\n-\n-  return absl::OkStatus();\n-}\n-\n class RocmConvRunner : public dnn::ConvRunner {\n  public:\n   RocmConvRunner(StreamExecutor* parent, MIOpenAccess* miopen, int64_t algo_id,\n@@ -3459,26 +3412,6 @@ class RocmConvRunner : public dnn::ConvRunner {\n   ScopedConvolutionDescriptor conv_desc_;\n };\n \n-absl::Status MIOpenSupport::DoConvolve(\n-    dnn::ConvolutionKind kind, dnn::DataType element_type,\n-    dnn::DataType output_type, Stream* stream,\n-    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n-    const dnn::FilterDescriptor& filter_descriptor,\n-    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n-    DeviceMemoryBase output_data,\n-    const dnn::ConvolutionDescriptor& convolution_descriptor,\n-    dnn::AlgorithmDesc algorithm_desc, DeviceMemory<uint8_t> scratch_memory,\n-    dnn::ProfileResult* output_profile_result) {\n-  TF_ASSIGN_OR_RETURN(\n-      auto runner,\n-      ConvolveRunnerFromDesc(stream, algorithm_desc, kind, element_type,\n-                             output_type, input_descriptor, filter_descriptor,\n-                             output_descriptor, convolution_descriptor));\n-\n-  return (*runner)(stream, output_profile_result, scratch_memory, input_data,\n-                   filter_data, output_data);\n-}\n-\n absl::Status MIOpenSupport::GetConvolveRunners(\n     dnn::ConvolutionKind kind, dnn::DataType input_type,\n     dnn::DataType output_type, Stream* stream,"
        },
        {
            "sha": "e793e29177c71eb5302a8b1047572ddebec0d462",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_dnn.h",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a2945078995c52052b88b6a86b8d25f198ed03a8/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_dnn.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a2945078995c52052b88b6a86b8d25f198ed03a8/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_dnn.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_dnn.h?ref=a2945078995c52052b88b6a86b8d25f198ed03a8",
            "patch": "@@ -399,18 +399,6 @@ class MIOpenSupport : public dnn::DnnSupport {\n       DeviceMemory<uint8_t>* reserve_space_data,\n       ScratchAllocator* workspace_allocator) override;\n \n-  absl::Status DoConvolve(\n-      dnn::ConvolutionKind kind, dnn::DataType element_type,\n-      dnn::DataType output_type, Stream* stream,\n-      const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n-      const dnn::FilterDescriptor& filter_descriptor,\n-      DeviceMemoryBase filter_data,\n-      const dnn::BatchDescriptor& output_descriptor,\n-      DeviceMemoryBase output_data,\n-      const dnn::ConvolutionDescriptor& convolution_descriptor,\n-      dnn::AlgorithmDesc algorithm_desc, DeviceMemory<uint8_t> scratch_memory,\n-      dnn::ProfileResult* output_profile_result) override;\n-\n   absl::Status DoFusedConvolve(\n       Stream* stream, dnn::DataType input_type, dnn::DataType side_input_type,\n       dnn::DataType bias_type, dnn::DataType output_type,\n@@ -598,18 +586,6 @@ class MIOpenSupport : public dnn::DnnSupport {\n       ScratchAllocator* workspace_allocator,\n       dnn::ProfileResult* output_profile_result);\n \n-  absl::Status DoPrepareForConvolution(\n-      dnn::ConvolutionKind kind, dnn::DataType element_type, Stream* stream,\n-      const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n-      const dnn::FilterDescriptor& filter_descriptor,\n-      DeviceMemoryBase filter_data,\n-      const dnn::BatchDescriptor& output_descriptor,\n-      DeviceMemoryBase output_data,\n-      const dnn::ConvolutionDescriptor& convolution_descriptor,\n-      const dnn::AlgorithmConfig& algorithm_config,\n-      ScratchAllocator* scratch_allocator, dnn::AlgorithmDesc* algorithm_desc,\n-      DeviceMemory<uint8_t>* scratch_memory) override;\n-\n   absl::Status DoCtcLossImpl(\n       Stream* stream, const MIOpenRnnStateTensorDescriptor& probs_desc,\n       const DeviceMemoryBase probs_data, absl::Span<const int> labels_data,"
        }
    ],
    "stats": {
        "total": 254,
        "additions": 0,
        "deletions": 254
    }
}