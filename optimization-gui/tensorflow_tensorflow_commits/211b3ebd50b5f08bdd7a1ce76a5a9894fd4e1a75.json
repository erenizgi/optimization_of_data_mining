{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Exchange events and pointers during Initialize and store them in StreamState.\n\nThis way we can use `StreamState*` as rendezvous value and store results to be used later. This helps to make code easier to follow and use the same code for sync before and after the kernel. We still need to do lightweight rendezvous for each synchronization with CUDA events.\n\nPiperOrigin-RevId: 804883580",
    "sha": "211b3ebd50b5f08bdd7a1ce76a5a9894fd4e1a75",
    "files": [
        {
            "sha": "0be97690b2c1015321e19cfe20ab87647ed13df0",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all_thunk.cc",
            "status": "modified",
            "additions": 77,
            "deletions": 126,
            "changes": 203,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/211b3ebd50b5f08bdd7a1ce76a5a9894fd4e1a75/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/211b3ebd50b5f08bdd7a1ce76a5a9894fd4e1a75/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc?ref=211b3ebd50b5f08bdd7a1ce76a5a9894fd4e1a75",
            "patch": "@@ -223,94 +223,32 @@ absl::Status RunRaggedAllToAll(\n   return absl::OkStatus();\n }\n \n-// Contains the values that are passed between host threads with rendezvous.\n-struct RendezvousValue {\n-  RankId rank;\n-  se::DeviceMemoryBase output_buffer;\n-  se::Event* start_event;\n-  se::Event* end_event;\n-\n-  bool operator<(const RendezvousValue& other) const {\n-    return rank < other.rank;\n-  }\n-};\n-\n-// Executes the rendezvous before the kernel start.\n-// Inserts CUDA events into the stream to ensure that all devices have reached\n-// the start event before the kernel starts.\n-absl::StatusOr<std::shared_ptr<std::vector<RendezvousValue>>>\n-RendezvousBeforeKernelStart(absl::string_view name,\n-                            const GpuCliqueKey& clique_key, RankId rank,\n-                            int64_t num_ranks,\n-                            const se::DeviceMemoryBase& output_buffer,\n-                            se::Stream& stream, se::Event* start_event,\n-                            se::Event* end_event) {\n-  RendezvousValue rendezvous_value;\n-  rendezvous_value.rank = rank;\n-  rendezvous_value.output_buffer = output_buffer;\n-  rendezvous_value.start_event = start_event;\n-  rendezvous_value.end_event = end_event;\n-\n-  // Record that this device has started the memcpy ragged-all-to-all. We do\n-  // this before the rendezvous to make sure that RecordEvent is called before\n+// Performs synchronization of all participating devices with CUDA events.\n+absl::Status SynchronizeWithCudaEvents(absl::string_view name,\n+                                       const GpuCliqueKey& clique_key,\n+                                       se::Stream& stream, RankId rank,\n+                                       absl::Span<se::Event* const> events) {\n+  se::Event* local_event = events[rank.value()];\n+\n+  // Record that this device has reached the synchronization point. We do this\n+  // before the rendezvous to make sure that RecordEvent is called before\n   // WaitFor on another stream.\n-  TF_RETURN_IF_ERROR(stream.RecordEvent(start_event));\n+  TF_RETURN_IF_ERROR(stream.RecordEvent(local_event));\n \n-  auto rendezvous_fn = [](absl::Span<const RendezvousValue* const> values) {\n-    std::vector<RendezvousValue> values_copy;\n-    for (const auto& value : values) {\n-      values_copy.push_back(*value);\n-    }\n-    // Sort to make sure that values are in the same order as the devices are\n-    // ordered in the communicator.\n-    absl::c_sort(values_copy);\n-    return values_copy;\n-  };\n-\n-  std::string start_rendezvous_key =\n-      absl::StrFormat(\"start %s ragged-all-to-all for rank %d, clique %s\", name,\n+  std::string rendezvous_key =\n+      absl::StrFormat(\"%s ragged-all-to-all for rank %d, clique %s\", name,\n                       rank.value(), clique_key.ToString());\n-  TF_ASSIGN_OR_RETURN(\n-      std::shared_ptr<std::vector<RendezvousValue>> rendezvous_values,\n-      Rendezvous<std::vector<RendezvousValue>>(\n-          /*name=*/\n-          start_rendezvous_key, /*key=*/clique_key,\n-          /*value=*/rendezvous_value, /*num_threads=*/num_ranks,\n-          rendezvous_fn));\n-\n-  // Wait for all devices to reach the start event. This indicates that all\n-  // output buffers are ready for transfer.\n-  for (auto& value : *rendezvous_values) {\n-    TF_RETURN_IF_ERROR(stream.WaitFor(value.start_event));\n-  }\n \n-  return rendezvous_values;\n-}\n+  // Do a rendezvous to make sure that all host threads have added a command to\n+  // record the start event on the stream.\n+  TF_RETURN_IF_ERROR(Rendezvous(rendezvous_key, clique_key,\n+                                clique_key.num_local_participants()));\n \n-// Executes the rendezvous after the kernel finish. Waits for all devices to\n-// reach the end event.\n-absl::Status RendezvousAfterKernelFinish(\n-    absl::string_view name, const GpuCliqueKey& clique_key, RankId rank,\n-    int64_t num_ranks, se::Stream& stream, se::Event* end_event,\n-    const std::shared_ptr<std::vector<RendezvousValue>>& rendezvous_values) {\n-  // Record that this device has finished the memcpy ragged-all-to-all.\n-  TF_RETURN_IF_ERROR(stream.RecordEvent(end_event));\n-\n-  // Do another rendezvous to make sure that we call RecordEvent for end_event\n-  // before WaitFor on another stream.\n-  std::string finish_rendezvous_key =\n-      absl::StrFormat(\"finish %s ragged-all-to-all for rank %d, clique %s\",\n-                      name, rank.value(), clique_key.ToString());\n-  TF_RETURN_IF_ERROR(Rendezvous(/*name=*/finish_rendezvous_key,\n-                                /*key=*/clique_key,\n-                                /*num_threads=*/num_ranks));\n-\n-  // Wait for all devices to reach the end event. This indicates that all\n-  // updates from other devices have arrived.\n-  for (auto& value : *rendezvous_values) {\n-    TF_RETURN_IF_ERROR(stream.WaitFor(value.end_event));\n+  // Wait for all devices to reach the event. This indicates that all output\n+  // buffers are ready for transfer.\n+  for (auto& event : events) {\n+    TF_RETURN_IF_ERROR(stream.WaitFor(event));\n   }\n-\n   return absl::OkStatus();\n }\n \n@@ -329,7 +267,6 @@ absl::Status RaggedAllToAllStartThunk::RunMemCpyRaggedAllToAll(\n   PrimitiveType element_type = buffers[0].element_type;\n \n   se::DeviceMemoryBase input_buffer = buffers[0].source_buffer;\n-  se::DeviceMemoryBase output_buffer = buffers[1].destination_buffer;\n \n   TF_RETURN_IF_ERROR(\n       LoadRaggedTensorMetadata(stream, buffers, ragged_metadata_allocs));\n@@ -340,11 +277,8 @@ absl::Status RaggedAllToAllStartThunk::RunMemCpyRaggedAllToAll(\n   const int64_t* send_sizes = ragged_metadata_allocs[1];\n   const int64_t* output_offsets = ragged_metadata_allocs[2];\n \n-  TF_ASSIGN_OR_RETURN(\n-      std::shared_ptr<std::vector<RendezvousValue>> rendezvous_values,\n-      RendezvousBeforeKernelStart(\n-          /*name=*/\"memcpy\", clique_key, rank, num_ranks, output_buffer, stream,\n-          state.start_event.get(), state.end_event.get()));\n+  TF_RETURN_IF_ERROR(SynchronizeWithCudaEvents(\n+      \"start memcpy\", clique_key, stream, rank, state.start_events));\n \n   // Transfer a slice of data to each peer's output buffer.\n   for (int64_t i = 0; i < num_updates_per_replica; ++i) {\n@@ -354,20 +288,17 @@ absl::Status RaggedAllToAllStartThunk::RunMemCpyRaggedAllToAll(\n           GpuCollectives::Slice(input_buffer, element_type,\n                                 input_offsets[idx] * config_.num_row_elements,\n                                 send_sizes[idx] * config_.num_row_elements);\n-      se::DeviceMemoryBase dst_slice = GpuCollectives::Slice(\n-          (*rendezvous_values)[peer].output_buffer, element_type,\n-          output_offsets[idx] * config_.num_row_elements,\n-          send_sizes[idx] * config_.num_row_elements);\n+      se::DeviceMemoryBase dst_slice =\n+          GpuCollectives::Slice(state.output_buffers[peer], element_type,\n+                                output_offsets[idx] * config_.num_row_elements,\n+                                send_sizes[idx] * config_.num_row_elements);\n       TF_RETURN_IF_ERROR(\n           stream.MemcpyD2D(&dst_slice, send_slice, send_slice.size()));\n     }\n   }\n \n-  TF_RETURN_IF_ERROR(RendezvousAfterKernelFinish(\n-      /*name=*/\"memcpy\", clique_key, rank, num_ranks, stream,\n-      state.end_event.get(), rendezvous_values));\n-\n-  return absl::OkStatus();\n+  return SynchronizeWithCudaEvents(\"finish memcpy\", clique_key, stream, rank,\n+                                   state.end_events);\n }\n \n absl::Status RaggedAllToAllStartThunk::RunOneShotRaggedAllToAll(\n@@ -384,30 +315,20 @@ absl::Status RaggedAllToAllStartThunk::RunOneShotRaggedAllToAll(\n   PrimitiveType element_type = buffers[0].element_type;\n \n   se::DeviceMemoryBase input_buffer = buffers[0].source_buffer;\n-  se::DeviceMemoryBase output_buffer = buffers[1].destination_buffer;\n \n-  TF_ASSIGN_OR_RETURN(\n-      std::shared_ptr<std::vector<RendezvousValue>> rendezvous_values,\n-      RendezvousBeforeKernelStart(\n-          /*name=*/\"one-shot\", clique_key, rank, num_ranks, output_buffer,\n-          stream, state.start_event.get(), state.end_event.get()));\n+  TF_RETURN_IF_ERROR(SynchronizeWithCudaEvents(\n+      \"start one-shot\", clique_key, stream, rank, state.start_events));\n \n   const int64_t num_updates_per_replica = config_.num_total_updates / num_ranks;\n \n-  absl::InlinedVector<se::DeviceMemoryBase, 4> output_ptrs;\n-  for (auto& value : *rendezvous_values) {\n-    output_ptrs.push_back(value.output_buffer);\n-  }\n-\n   TF_RETURN_IF_ERROR(RunRaggedAllToAllKernel(\n-      &stream, element_type, input_buffer, output_ptrs,\n+      &stream, element_type, input_buffer, state.output_buffers,\n       buffers[2].source_buffer, buffers[3].source_buffer,\n       buffers[4].source_buffer, num_ranks, num_updates_per_replica,\n       config_.num_input_rows, config_.num_row_elements));\n \n-  return RendezvousAfterKernelFinish(\n-      /*name=*/\"one-shot\", clique_key, rank, num_ranks, stream,\n-      state.end_event.get(), rendezvous_values);\n+  return SynchronizeWithCudaEvents(\"finish one-shot\", clique_key, stream, rank,\n+                                   state.end_events);\n }\n \n RaggedAllToAllStartThunk::RaggedAllToAllStartThunk(\n@@ -467,6 +388,13 @@ absl::Status RaggedAllToAllStartThunk::Initialize(\n \n   se::StreamExecutor* executor = params.executor;\n \n+  TF_ASSIGN_OR_RETURN(\n+      const GpuCliqueKey clique_key,\n+      GetCollectiveGpuCliqueKey(*params.collective_params, config_.config));\n+  const std::optional<RankId> rank =\n+      clique_key.rank(params.collective_params->global_device_id);\n+\n+  StreamState* state = nullptr;\n   {\n     absl::MutexLock lock(&mutex_);\n \n@@ -475,16 +403,12 @@ absl::Status RaggedAllToAllStartThunk::Initialize(\n     if (per_stream_states_.contains(executor)) {\n       return absl::OkStatus();\n     }\n-  }\n \n-  TF_ASSIGN_OR_RETURN(\n-      const GpuCliqueKey clique_key,\n-      GetCollectiveGpuCliqueKey(*params.collective_params, config_.config));\n-  const std::optional<RankId> rank =\n-      clique_key.rank(params.collective_params->global_device_id);\n-\n-  auto state =\n-      std::make_unique<StreamState>(executor->device_ordinal(), rank.value());\n+    auto [map_iter, was_inserted] = per_stream_states_.emplace(\n+        executor, std::make_unique<StreamState>(executor->device_ordinal(),\n+                                                rank.value()));\n+    state = map_iter->second.get();\n+  }\n \n   // Allocate temp buffers in the host memory to load the sizes and offsets of\n   // ragged tensors from device memory.\n@@ -503,14 +427,41 @@ absl::Status RaggedAllToAllStartThunk::Initialize(\n     return absl::InternalError(\"Failed to allocate output offsets buffer.\");\n   }\n \n+  state->local_output_buffer = params.buffer_allocations->GetDeviceAddress(\n+      buffers_[1].destination_buffer);\n+\n   if (is_local()) {\n-    TF_ASSIGN_OR_RETURN(state->start_event, executor->CreateEvent());\n-    TF_ASSIGN_OR_RETURN(state->end_event, executor->CreateEvent());\n+    TF_ASSIGN_OR_RETURN(state->local_start_event, executor->CreateEvent());\n+    TF_ASSIGN_OR_RETURN(state->local_end_event, executor->CreateEvent());\n   }\n \n-  {\n-    absl::MutexLock lock(&mutex_);\n-    per_stream_states_.emplace(executor, std::move(state));\n+  auto completion_fn = [](absl::Span<const StreamState* const> states)\n+      -> std::vector<const StreamState*> {\n+    std::vector<const StreamState*> copy(states.begin(), states.end());\n+    // Sort by rank for stable order.\n+    absl::c_sort(copy,\n+                 [](const StreamState* const a, const StreamState* const b) {\n+                   return a->rank < b->rank;\n+                 });\n+    return copy;\n+  };\n+  std::string rendezvous_key =\n+      absl::StrFormat(\"Initializing ragged-all-to-all for device %d, clique %s\",\n+                      state->device_ordinal, clique_key.ToString());\n+  TF_ASSIGN_OR_RETURN(\n+      std::shared_ptr<std::vector<const StreamState*>> rendezvous_values,\n+      Rendezvous<std::vector<const StreamState*>>(\n+          /*name=*/rendezvous_key, /*key=*/clique_key,\n+          /*value=*/*state,\n+          /*num_threads=*/clique_key.num_local_participants(), completion_fn));\n+\n+  state->output_buffers.reserve(rendezvous_values->size());\n+  state->start_events.reserve(rendezvous_values->size());\n+  state->end_events.reserve(rendezvous_values->size());\n+  for (auto* rendezvous_state : *rendezvous_values) {\n+    state->output_buffers.push_back(rendezvous_state->local_output_buffer);\n+    state->start_events.push_back(rendezvous_state->local_start_event.get());\n+    state->end_events.push_back(rendezvous_state->local_end_event.get());\n   }\n \n   return absl::OkStatus();"
        },
        {
            "sha": "dee156207742a27a9d440a82d825e2df53131bb6",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all_thunk.h",
            "status": "modified",
            "additions": 16,
            "deletions": 6,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/211b3ebd50b5f08bdd7a1ce76a5a9894fd4e1a75/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/211b3ebd50b5f08bdd7a1ce76a5a9894fd4e1a75/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.h?ref=211b3ebd50b5f08bdd7a1ce76a5a9894fd4e1a75",
            "patch": "@@ -25,13 +25,15 @@ limitations under the License.\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/collectives/gpu_clique_key.h\"\n #include \"xla/backends/gpu/runtime/collective_thunk.h\"\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/hlo/ir/collective_op_group_mode.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/device_memory_handle.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/memory_allocation.h\"\n@@ -89,13 +91,21 @@ class RaggedAllToAllStartThunk : public CollectiveThunk {\n     // Device memory buffer for output offsets.\n     se::DeviceMemoryHandle output_offsets_device_buffer;\n \n-    // Event to synchronize streams on different devices at the start of the\n-    // kernel.\n-    std::unique_ptr<se::Event> start_event;\n+    // Device memory buffer for outputs on the local device.\n+    se::DeviceMemoryBase local_output_buffer;\n \n-    // Event to synchronize streams on different devices at the end of the\n-    // kernel.\n-    std::unique_ptr<se::Event> end_event;\n+    // Device memory buffers for outputs on all participating devices.\n+    absl::InlinedVector<se::DeviceMemoryBase, 8> output_buffers;\n+\n+    // Events to notify before and after the kernel to synchronize with streams\n+    // on other devices.\n+    std::unique_ptr<se::Event> local_start_event;\n+    std::unique_ptr<se::Event> local_end_event;\n+\n+    // Events collected from all participating devices. Need to be waited on\n+    // before the start and after the end of the kernel.\n+    absl::InlinedVector<se::Event*, 8> start_events;\n+    absl::InlinedVector<se::Event*, 8> end_events;\n \n     StreamState(int device_ordinal, RankId rank)\n         : device_ordinal(device_ordinal), rank(rank) {}"
        }
    ],
    "stats": {
        "total": 225,
        "additions": 93,
        "deletions": 132
    }
}