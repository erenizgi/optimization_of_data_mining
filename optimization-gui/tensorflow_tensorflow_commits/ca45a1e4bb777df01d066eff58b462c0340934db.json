{
    "author": "tensorflower-gardener",
    "message": "[XLA:GPU] Allow to map slice of memory with multicast object.\n\nPiperOrigin-RevId: 826013717",
    "sha": "ca45a1e4bb777df01d066eff58b462c0340934db",
    "files": [
        {
            "sha": "a66651cff53a79af3ba6ddc624a771cbd6ee79d0",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_reduce_test.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ca45a1e4bb777df01d066eff58b462c0340934db/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ca45a1e4bb777df01d066eff58b462c0340934db/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_reduce_test.cc?ref=ca45a1e4bb777df01d066eff58b462c0340934db",
            "patch": "@@ -163,9 +163,9 @@ class AllReduceKernelTest : public ::testing::Test,\n         stream_executor::gpu::GpuExecutor* gpu_executor =\n             dynamic_cast<stream_executor::gpu::GpuExecutor*>(executors[i]);\n         TF_RET_CHECK(gpu_executor != nullptr);\n-        TF_ASSIGN_OR_RETURN(void* mapped_memory,\n-                            multicast_memory->MapMemory(\n-                                allocated_buffers[i].opaque(), gpu_executor));\n+        TF_ASSIGN_OR_RETURN(\n+            void* mapped_memory,\n+            multicast_memory->MapMemory(allocated_buffers[i], gpu_executor));\n         metadata.multicast_buffer_ptr = (uint64_t)mapped_memory;\n       } else {\n         metadata.multicast_buffer_ptr = 0;"
        },
        {
            "sha": "f5c560c71632e5f639b091a3ece3c273a1a0677f",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 5,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ca45a1e4bb777df01d066eff58b462c0340934db/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ca45a1e4bb777df01d066eff58b462c0340934db/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc?ref=ca45a1e4bb777df01d066eff58b462c0340934db",
            "patch": "@@ -806,6 +806,15 @@ CudaExecutor::RetainVmmMemoryHandle(void* ptr) {\n   return CudaExecutor::VmmMemoryHandle(static_cast<uint64_t>(handle));\n }\n \n+absl::StatusOr<size_t> CudaExecutor::GetVmmGranularity() const {\n+  CUmemAllocationProp properties =\n+      GetVmmAllocationProperties(device_, is_rdma_supported_);\n+  size_t granularity = 0;\n+  TF_RETURN_IF_ERROR(cuda::ToStatus(cuMemGetAllocationGranularity(\n+      &granularity, &properties, CU_MEM_ALLOC_GRANULARITY_RECOMMENDED)));\n+  return granularity;\n+}\n+\n absl::StatusOr<void*> CudaExecutor::VmmAllocateMemory(uint64_t bytes) {\n   if (!is_vmm_supported_) {\n     return absl::InternalError(\"VMM is not supported on this device.\");\n@@ -1893,13 +1902,13 @@ absl::Status CudaExecutor::CudaMulticastMemory::SubscribeDevice(\n }\n \n absl::StatusOr<void*> CudaExecutor::CudaMulticastMemory::MapMemory(\n-    void* device_ptr, GpuExecutor* gpu_executor) {\n+    const DeviceMemoryBase& location, GpuExecutor* gpu_executor) {\n   CudaExecutor* cuda_executor = dynamic_cast<CudaExecutor*>(gpu_executor);\n   if (cuda_executor == nullptr) {\n     return absl::InvalidArgumentError(\"GpuExecutor is not a CudaExecutor.\");\n   }\n \n-  if (device_ptr == nullptr) {\n+  if (location.is_null()) {\n     return absl::InvalidArgumentError(\"Device pointer is null.\");\n   }\n \n@@ -1914,20 +1923,26 @@ absl::StatusOr<void*> CudaExecutor::CudaMulticastMemory::MapMemory(\n \n   TF_ASSIGN_OR_RETURN(\n       stream_executor::gpu::CudaExecutor::VmmMemoryHandle memory_handle,\n-      cuda_executor->RetainVmmMemoryHandle(device_ptr));\n+      cuda_executor->RetainVmmMemoryHandle(location.opaque()));\n \n   CUmemGenericAllocationHandle retained_memory_handle =\n       static_cast<CUmemGenericAllocationHandle>(memory_handle.handle());\n \n+  TF_ASSIGN_OR_RETURN(auto base_address,\n+                      cuda_executor->GetMemoryRange(location));\n+  uint64_t offset = reinterpret_cast<uint64_t>(location.opaque()) -\n+                    reinterpret_cast<uint64_t>(base_address.opaque());\n+\n   // Bind the memory to the multicast object.\n   TF_RETURN_IF_ERROR(stream_executor::cuda::ToStatus(\n       cuMulticastBindMem(handle_, /*mcOffset=*/0, retained_memory_handle,\n-                         /*memOffset=*/0, padded_size_, /*flags=*/0)));\n+                         /*memOffset=*/offset, padded_size_, /*flags=*/0)));\n \n   VLOG(3) << \"[\" << static_cast<int>(cuda_executor->device_)\n           << \"] Mapped multicast memory: \" << static_cast<uint64_t>(handle_)\n           << \" size: \" << padded_size_ << \" with granularity: \" << granularity_\n-          << \" to address: \" << device_ptr;\n+          << \" to address: \" << location.opaque()\n+          << \" offset from base range: \" << offset;\n \n   // Map a virtual address range for the multicast memory. Multicast\n   // memory is used to reduce the data stored in the multicast object."
        },
        {
            "sha": "ac1be6efb0013f19590232de3fa06f59c023f8b7",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.h",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ca45a1e4bb777df01d066eff58b462c0340934db/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ca45a1e4bb777df01d066eff58b462c0340934db/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h?ref=ca45a1e4bb777df01d066eff58b462c0340934db",
            "patch": "@@ -17,6 +17,7 @@ limitations under the License.\n #define XLA_STREAM_EXECUTOR_CUDA_CUDA_EXECUTOR_H_\n \n #include <atomic>\n+#include <cstddef>\n #include <cstdint>\n #include <map>\n #include <memory>\n@@ -138,6 +139,11 @@ class CudaExecutor : public GpuExecutor {\n   absl::StatusOr<std::unique_ptr<MemoryAllocator>> CreateMemoryAllocator(\n       MemoryType type) override;\n \n+  // Returns the granularity which is the minimum unit of memory that can be\n+  // allocated with VMM API. In order to map the memory slices to multicast\n+  // object, the offset of the slices should be aligned with this granularity.\n+  absl::StatusOr<size_t> GetVmmGranularity() const;\n+\n   // RAII wrapper for a VMM memory handle.\n   class VmmMemoryHandle {\n    public:\n@@ -167,7 +173,7 @@ class CudaExecutor : public GpuExecutor {\n \n     absl::Status SubscribeDevice(int device_number) override;\n \n-    absl::StatusOr<void*> MapMemory(void* device_ptr,\n+    absl::StatusOr<void*> MapMemory(const DeviceMemoryBase& location,\n                                     GpuExecutor* gpu_executor) override;\n \n    private:"
        },
        {
            "sha": "c5ffbf3d4d0fbd8149114551c1c7461d633056c6",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor_multigpu_test.cc",
            "status": "modified",
            "additions": 100,
            "deletions": 17,
            "changes": 117,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ca45a1e4bb777df01d066eff58b462c0340934db/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ca45a1e4bb777df01d066eff58b462c0340934db/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test.cc?ref=ca45a1e4bb777df01d066eff58b462c0340934db",
            "patch": "@@ -42,18 +42,20 @@ using ::testing::NotNull;\n \n template <typename T>\n absl::StatusOr<stream_executor::DeviceMemoryBase> AllocateInitializedMemory(\n-    CudaExecutor* executor, size_t size, T value) {\n-  size_t num_elements = size / sizeof(T);\n+    CudaExecutor* executor, size_t size, size_t offset, T value) {\n   stream_executor::DeviceMemoryBase device_memory = executor->Allocate(\n-      size, static_cast<int64_t>(stream_executor::MemoryType::kP2P));\n+      size + offset, static_cast<int64_t>(stream_executor::MemoryType::kP2P));\n   if (device_memory.opaque() == nullptr) {\n     return absl::InternalError(\"Failed to allocate memory.\");\n   }\n-  std::vector<T> device_memory_vector(num_elements, value);\n \n+  size_t num_initialized_elements = size / sizeof(T);\n+  std::vector<T> device_memory_vector(num_initialized_elements, value);\n+\n+  auto stride_memory = device_memory.GetByteSlice(offset, size);\n   TF_RETURN_IF_ERROR(executor->SynchronousMemcpy(\n-      &device_memory, device_memory_vector.data(), size));\n-  return device_memory;\n+      &stride_memory, device_memory_vector.data(), size));\n+  return stride_memory;\n }\n \n template <typename T>\n@@ -105,10 +107,10 @@ TEST(CudaExecutorMultiGpuTest, AllDevicesMustBeSubscribedBeforeMapping) {\n   TF_ASSERT_OK_AND_ASSIGN(multicast_memory,\n                           executors[0]->CreateMulticastMemory(1024, 2));\n   EXPECT_THAT(multicast_memory->SubscribeDevice(0), IsOk());\n-  EXPECT_THAT(\n-      multicast_memory->MapMemory(reinterpret_cast<void*>(1), executors[0]),\n-      StatusIs(absl::StatusCode::kFailedPrecondition,\n-               \"All devices should be subscribed.\"));\n+  DeviceMemoryBase device_memory(reinterpret_cast<void*>(1), 1);\n+  EXPECT_THAT(multicast_memory->MapMemory(device_memory, executors[0]),\n+              StatusIs(absl::StatusCode::kFailedPrecondition,\n+                       \"All devices should be subscribed.\"));\n   ;\n }\n \n@@ -146,7 +148,7 @@ TEST(CudaExecutorMultiGpuTest, CudaMulticastMemoryUsingNonVmmMemory) {\n \n   DeviceMemoryBase device_memory = executors[0]->Allocate(8, 0);\n   EXPECT_THAT(\n-      multicast_memory->MapMemory(device_memory.opaque(), executors[0]),\n+      multicast_memory->MapMemory(device_memory, executors[0]),\n       StatusIs(absl::StatusCode::kInternal,\n                \"CUDA error: : CUDA_ERROR_INVALID_VALUE: invalid argument\"));\n }\n@@ -170,19 +172,101 @@ TEST(CudaExecutorMultiGpuTest, CudaMulticastMemoryUsingVmmMemory) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       stream_executor::DeviceMemoryBase first_device_memory,\n-      AllocateInitializedMemory(executors[0], kMemorySize, kValue));\n+      AllocateInitializedMemory(executors[0], kMemorySize, 0, kValue));\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       stream_executor::DeviceMemoryBase output_device_memory,\n-      AllocateInitializedMemory(executors[0], kMemorySize, 0));\n+      AllocateInitializedMemory(executors[0], kMemorySize, 0, 0));\n   TF_ASSERT_OK_AND_ASSIGN(\n       void* first_device_multicast_ptr,\n-      multicast_memory->MapMemory(first_device_memory.opaque(), executors[0]));\n+      multicast_memory->MapMemory(first_device_memory, executors[0]));\n   TF_ASSERT_OK_AND_ASSIGN(\n       stream_executor::DeviceMemoryBase second_device_memory,\n-      AllocateInitializedMemory(executors[1], kMemorySize, kValue));\n+      AllocateInitializedMemory(executors[1], kMemorySize, 0, kValue));\n+  EXPECT_THAT(multicast_memory->MapMemory(second_device_memory, executors[1]),\n+              IsOkAndHolds(NotNull()));\n+\n+  EXPECT_THAT(\n+      MulticastReduce((int*)first_device_multicast_ptr,\n+                      (int*)output_device_memory.opaque(), kNumElements),\n+      IsOk());\n+\n+  const int kExpectedValue = kValue * kNumDevices;\n+  EXPECT_THAT(CheckMemory(executors[0], output_device_memory, kExpectedValue),\n+              IsOk());\n+}\n+\n+TEST(CudaExecutorMultiGpuTest, CudaMulticastMemoryMapDifferentSlicesUnaligned) {\n+  std::vector<CudaExecutor*> executors = {\n+      static_cast<CudaExecutor*>(GetGpuExecutor(0)),\n+      static_cast<CudaExecutor*>(GetGpuExecutor(1))};\n+  if (!executors[0]->is_multicast_supported()) {\n+    GTEST_SKIP() << \"Test requires multicast support.\";\n+  }\n+  const int64_t kNumDevices = 2;\n+  const int64_t kNumElements = 8;\n+  const int64_t kMappedMemorySize = kNumElements * sizeof(int);\n+  const int kValue = 2;\n+  std::unique_ptr<CudaExecutor::MulticastMemory> multicast_memory;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      multicast_memory,\n+      executors[0]->CreateMulticastMemory(kMappedMemorySize, kNumDevices));\n+  EXPECT_THAT(multicast_memory->SubscribeDevice(0), IsOk());\n+  EXPECT_THAT(multicast_memory->SubscribeDevice(1), IsOk());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(size_t vmm_granularity,\n+                          executors[0]->GetVmmGranularity());\n+  // Allocate memory with unaligned offset.\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      stream_executor::DeviceMemoryBase first_device_mapped_memory,\n+      AllocateInitializedMemory(\n+          executors[0],\n+          // Add granularity to make sure that there is\n+          // enough memory after adding offset to map with multicast object.\n+          kMappedMemorySize + vmm_granularity, kMappedMemorySize, kValue));\n   EXPECT_THAT(\n-      multicast_memory->MapMemory(second_device_memory.opaque(), executors[1]),\n+      multicast_memory->MapMemory(first_device_mapped_memory, executors[0]),\n+      StatusIs(absl::StatusCode::kInternal,\n+               \"CUDA error: : CUDA_ERROR_INVALID_VALUE: invalid argument\"));\n+}\n+\n+// Slices mapping works only when offset is aligned with the VMM granularity.\n+TEST(CudaExecutorMultiGpuTest, CudaMulticastMemoryMapDifferentSlices) {\n+  std::vector<CudaExecutor*> executors = {\n+      static_cast<CudaExecutor*>(GetGpuExecutor(0)),\n+      static_cast<CudaExecutor*>(GetGpuExecutor(1))};\n+  if (!executors[0]->is_multicast_supported()) {\n+    GTEST_SKIP() << \"Test requires multicast support.\";\n+  }\n+  const int64_t kNumDevices = 2;\n+  const int64_t kNumElements = 8;\n+  const int64_t kMappedMemorySize = kNumElements * sizeof(int);\n+  const int kValue = 2;\n+  std::unique_ptr<CudaExecutor::MulticastMemory> multicast_memory;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      multicast_memory,\n+      executors[0]->CreateMulticastMemory(kMappedMemorySize, kNumDevices));\n+  EXPECT_THAT(multicast_memory->SubscribeDevice(0), IsOk());\n+  EXPECT_THAT(multicast_memory->SubscribeDevice(1), IsOk());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(size_t vmm_granularity,\n+                          executors[0]->GetVmmGranularity());\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      stream_executor::DeviceMemoryBase first_device_mapped_memory,\n+      AllocateInitializedMemory(executors[0], kMappedMemorySize,\n+                                vmm_granularity, kValue));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      stream_executor::DeviceMemoryBase output_device_memory,\n+      AllocateInitializedMemory(executors[0], kMappedMemorySize, 0, 0));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      void* first_device_multicast_ptr,\n+      multicast_memory->MapMemory(first_device_mapped_memory, executors[0]));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      stream_executor::DeviceMemoryBase second_device_mapped_memory,\n+      AllocateInitializedMemory(executors[1], kMappedMemorySize, 0, kValue));\n+  EXPECT_THAT(\n+      multicast_memory->MapMemory(second_device_mapped_memory, executors[1]),\n       IsOkAndHolds(NotNull()));\n \n   EXPECT_THAT(\n@@ -194,6 +278,5 @@ TEST(CudaExecutorMultiGpuTest, CudaMulticastMemoryUsingVmmMemory) {\n   EXPECT_THAT(CheckMemory(executors[0], output_device_memory, kExpectedValue),\n               IsOk());\n }\n-\n }  // namespace\n }  // namespace stream_executor::gpu"
        },
        {
            "sha": "4ab7c7e9983735ac0b5ab8c030fd6391dd788e7a",
            "filename": "third_party/xla/xla/stream_executor/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ca45a1e4bb777df01d066eff58b462c0340934db/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ca45a1e4bb777df01d066eff58b462c0340934db/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD?ref=ca45a1e4bb777df01d066eff58b462c0340934db",
            "patch": "@@ -168,6 +168,7 @@ cc_library(\n     name = \"gpu_executor_header\",\n     hdrs = [\"gpu_executor.h\"],\n     deps = [\n+        \"//xla/stream_executor:device_memory\",\n         \"//xla/stream_executor:platform\",\n         \"//xla/stream_executor:stream_executor_common\",\n         \"//xla/stream_executor:stream_executor_h\","
        },
        {
            "sha": "03dd5e0f410f077d88bc7377ef951020c83a21ba",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_executor.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ca45a1e4bb777df01d066eff58b462c0340934db/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ca45a1e4bb777df01d066eff58b462c0340934db/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_executor.h?ref=ca45a1e4bb777df01d066eff58b462c0340934db",
            "patch": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/synchronization/mutex.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/stream_executor/stream_executor_common.h\"\n@@ -78,7 +79,7 @@ class GpuExecutor : public StreamExecutorCommon {\n       return absl::UnimplementedError(\"SubscribeDevice is not implemented.\");\n     }\n \n-    virtual absl::StatusOr<void*> MapMemory(void* device_ptr,\n+    virtual absl::StatusOr<void*> MapMemory(const DeviceMemoryBase& location,\n                                             GpuExecutor* gpu_executor) {\n       return absl::UnimplementedError(\"MapMemory is not implemented.\");\n     }"
        }
    ],
    "stats": {
        "total": 160,
        "additions": 133,
        "deletions": 27
    }
}