{
    "author": "ermilovmaxim",
    "message": "Add Shape to TriangularSolveThunk buffer_uses\n\nModify Thunk's serialization\n\nPiperOrigin-RevId: 848323137",
    "sha": "0cebad66545e6f271086eb4b68f37fecf3b7fc9f",
    "files": [
        {
            "sha": "d1ed9d2c469f38698f4e79128a64c56520ac8d92",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0cebad66545e6f271086eb4b68f37fecf3b7fc9f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0cebad66545e6f271086eb4b68f37fecf3b7fc9f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=0cebad66545e6f271086eb4b68f37fecf3b7fc9f",
            "patch": "@@ -2408,8 +2408,10 @@ cc_library(\n     hdrs = [\"triangular_solve_thunk.h\"],\n     deps = [\n         \":make_batch_pointers\",\n+        \":shaped_slice\",\n         \":thunk\",\n         \":thunk_proto_cc\",\n+        \"//xla:shape_util\",\n         \"//xla:status_macros\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\","
        },
        {
            "sha": "f9b79285bad415ebd6b2384d558eadd2a1ff247d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.proto",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0cebad66545e6f271086eb4b68f37fecf3b7fc9f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0cebad66545e6f271086eb4b68f37fecf3b7fc9f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.proto?ref=0cebad66545e6f271086eb4b68f37fecf3b7fc9f",
            "patch": "@@ -188,15 +188,9 @@ message WaitForStreamsThunkProto {\n \n message TriangularSolveThunkProto {\n   xla.TriangularSolveOptions options = 1;\n-  xla.buffer_assignment.BufferAllocationSliceProto a_buffer = 2;\n-  xla.buffer_assignment.BufferAllocationSliceProto b_buffer = 3;\n-  xla.buffer_assignment.BufferAllocationSliceProto temp_buffer = 4;\n-  xla.PrimitiveType type = 5;\n-  int64 batch_size = 6;\n-  int64 m = 7;\n-  int64 n = 8;\n-  int64 a_batch_stride = 9;\n-  int64 b_batch_stride = 10;\n+  ShapedSliceProto a_buffer = 2;\n+  ShapedSliceProto b_buffer = 3;\n+  ShapedSliceProto temp_buffer = 4;\n }\n \n message ReplicaIdThunkProto {"
        },
        {
            "sha": "16881b25aa35ee3dd3899a000025ab326b4c1476",
            "filename": "third_party/xla/xla/backends/gpu/runtime/triangular_solve_thunk.cc",
            "status": "modified",
            "additions": 23,
            "deletions": 35,
            "changes": 58,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0cebad66545e6f271086eb4b68f37fecf3b7fc9f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ftriangular_solve_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0cebad66545e6f271086eb4b68f37fecf3b7fc9f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ftriangular_solve_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ftriangular_solve_thunk.cc?ref=0cebad66545e6f271086eb4b68f37fecf3b7fc9f",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"absl/types/span.h\"\n #include \"xla/backends/gpu/runtime/make_batch_pointers.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/service/buffer_assignment.h\"\n@@ -42,11 +43,8 @@ namespace gpu {\n \n TriangularSolveThunk::TriangularSolveThunk(\n     ThunkInfo thunk_info, const TriangularSolveOptions& options,\n-    const BufferAllocation::Slice& a_buffer,\n-    const BufferAllocation::Slice& b_buffer,\n-    const BufferAllocation::Slice& temp_buffer,  //\n-    PrimitiveType type, int64_t batch_size, int64_t m, int64_t n,\n-    int64_t a_batch_stride, int64_t b_batch_stride)\n+    const ShapedSlice& a_buffer, const ShapedSlice& b_buffer,\n+    const ShapedSlice& temp_buffer)\n     : Thunk(Kind::kTriangularSolve, thunk_info),\n       uplo_(options.lower() ? se::blas::UpperLower::kLower\n                             : se::blas::UpperLower::kUpper),\n@@ -57,12 +55,9 @@ TriangularSolveThunk::TriangularSolveThunk(\n       a_buffer_(a_buffer),\n       b_buffer_(b_buffer),\n       temp_buffer_(temp_buffer),\n-      type_(type),\n-      batch_size_(batch_size),\n-      m_(m),\n-      n_(n),\n-      a_batch_stride_(a_batch_stride),\n-      b_batch_stride_(b_batch_stride) {\n+      type_(b_buffer.shape.element_type()),\n+      m_(b_buffer.shape.dimensions(b_buffer.shape.dimensions().size() - 2)),\n+      n_(b_buffer.shape.dimensions(b_buffer.shape.dimensions().size() - 1)) {\n   transpose_a_ = [&] {\n     switch (options.transpose_a()) {\n       case TriangularSolveOptions::NO_TRANSPOSE:\n@@ -82,31 +77,30 @@ TriangularSolveThunk::TriangularSolveThunk(\n absl::Status TriangularSolveThunk::ExecuteOnStream(\n     const ExecuteParams& params) {\n   auto& buffer_allocations = *params.buffer_allocations;\n-  return RunTriangularSolve(buffer_allocations.GetDeviceAddress(a_buffer_),\n-                            buffer_allocations.GetDeviceAddress(b_buffer_),\n-                            buffer_allocations.GetDeviceAddress(temp_buffer_),\n-                            uplo_, side_, unit_diagonal_, transpose_a_, type_,\n-                            batch_size_, m_, n_, a_batch_stride_,\n-                            b_batch_stride_, params.stream);\n+  return RunTriangularSolve(\n+      buffer_allocations.GetDeviceAddress(a_buffer_.slice),\n+      buffer_allocations.GetDeviceAddress(b_buffer_.slice),\n+      buffer_allocations.GetDeviceAddress(temp_buffer_.slice), uplo_, side_,\n+      unit_diagonal_, transpose_a_, type_, batch_size(), m_, n_,\n+      a_batch_stride(), b_batch_stride(), params.stream);\n }\n \n absl::StatusOr<std::unique_ptr<TriangularSolveThunk>>\n TriangularSolveThunk::FromProto(\n     ThunkInfo thunk_info, const TriangularSolveThunkProto& proto,\n     absl::Span<const BufferAllocation> allocations) {\n-  TF_ASSIGN_OR_RETURN(\n-      BufferAllocation::Slice a_buffer,\n-      BufferAllocation::Slice::FromProto(proto.a_buffer(), allocations));\n-  TF_ASSIGN_OR_RETURN(\n-      BufferAllocation::Slice b_buffer,\n-      BufferAllocation::Slice::FromProto(proto.b_buffer(), allocations));\n-  TF_ASSIGN_OR_RETURN(\n-      BufferAllocation::Slice temp_buffer,\n-      BufferAllocation::Slice::FromProto(proto.temp_buffer(), allocations));\n+  TF_ASSIGN_OR_RETURN(ShapedSlice a_buffer,\n+                      ShapedSlice::FromProto(proto.a_buffer(), allocations));\n+  TF_ASSIGN_OR_RETURN(ShapedSlice b_buffer,\n+                      ShapedSlice::FromProto(proto.b_buffer(), allocations));\n+  TF_ASSIGN_OR_RETURN(ShapedSlice temp_buffer,\n+                      ShapedSlice::FromProto(proto.temp_buffer(), allocations));\n+\n+  if (b_buffer.shape.dimensions().size() < 2) {\n+    return absl::InvalidArgumentError(\"Unsupported shape for b\");\n+  }\n   return std::make_unique<TriangularSolveThunk>(\n-      thunk_info, proto.options(), a_buffer, b_buffer, temp_buffer,\n-      proto.type(), proto.batch_size(), proto.m(), proto.n(),\n-      proto.a_batch_stride(), proto.b_batch_stride());\n+      thunk_info, proto.options(), a_buffer, b_buffer, temp_buffer);\n }\n \n absl::StatusOr<ThunkProto> TriangularSolveThunk::ToProto() const {\n@@ -143,12 +137,6 @@ absl::StatusOr<ThunkProto> TriangularSolveThunk::ToProto() const {\n                       b_buffer_.ToProto());\n   TF_ASSIGN_OR_RETURN(*triangular_solve_thunk_proto->mutable_temp_buffer(),\n                       temp_buffer_.ToProto());\n-  triangular_solve_thunk_proto->set_type(type_);\n-  triangular_solve_thunk_proto->set_batch_size(batch_size_);\n-  triangular_solve_thunk_proto->set_m(m_);\n-  triangular_solve_thunk_proto->set_n(n_);\n-  triangular_solve_thunk_proto->set_a_batch_stride(a_batch_stride_);\n-  triangular_solve_thunk_proto->set_b_batch_stride(b_batch_stride_);\n   return proto;\n }\n "
        },
        {
            "sha": "29e701fb9a627ebe17eefcf563a60bca138e756e",
            "filename": "third_party/xla/xla/backends/gpu/runtime/triangular_solve_thunk.h",
            "status": "modified",
            "additions": 28,
            "deletions": 16,
            "changes": 44,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0cebad66545e6f271086eb4b68f37fecf3b7fc9f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ftriangular_solve_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0cebad66545e6f271086eb4b68f37fecf3b7fc9f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ftriangular_solve_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ftriangular_solve_thunk.h?ref=0cebad66545e6f271086eb4b68f37fecf3b7fc9f",
            "patch": "@@ -17,15 +17,19 @@ limitations under the License.\n #define XLA_BACKENDS_GPU_RUNTIME_TRIANGULAR_SOLVE_THUNK_H_\n \n #include <cstdint>\n+#include <functional>\n #include <memory>\n+#include <numeric>\n \n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/backends/gpu/runtime/shaped_slice.h\"\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/shape_util.h\"\n #include \"xla/stream_executor/blas.h\"\n #include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -42,12 +46,8 @@ class TriangularSolveThunk : public Thunk {\n  public:\n   TriangularSolveThunk(ThunkInfo thunk_info,\n                        const TriangularSolveOptions& options,\n-                       const BufferAllocation::Slice& a_buffer,\n-                       const BufferAllocation::Slice& b_buffer,\n-                       const BufferAllocation::Slice& temp_buffer,\n-                       PrimitiveType type, int64_t batch_size, int64_t m,\n-                       int64_t n, int64_t a_batch_stride,\n-                       int64_t b_batch_stride);\n+                       const ShapedSlice& a_buffer, const ShapedSlice& b_buffer,\n+                       const ShapedSlice& temp_buffer);\n \n   TriangularSolveThunk(const TriangularSolveThunk&) = delete;\n   TriangularSolveThunk& operator=(const TriangularSolveThunk&) = delete;\n@@ -56,10 +56,9 @@ class TriangularSolveThunk : public Thunk {\n \n   BufferUses buffer_uses() const override {\n     return {\n-        BufferUse::Read(a_buffer_),\n-        BufferUse::Write(b_buffer_),\n-        BufferUse(temp_buffer_, BufferUse::MemoryAccess::kWrite,\n-                  BufferUse::ContentValidity::kUndefined),\n+        BufferUse::Read(a_buffer_.slice, a_buffer_.shape),\n+        BufferUse::Write(b_buffer_.slice, b_buffer_.shape),\n+        BufferUse::Scratch(temp_buffer_.slice, temp_buffer_.shape),\n     };\n   };\n \n@@ -70,21 +69,34 @@ class TriangularSolveThunk : public Thunk {\n   absl::StatusOr<ThunkProto> ToProto() const override;\n \n  private:\n+  int64_t batch_size() const {\n+    return std::accumulate(b_buffer_.shape.dimensions().begin(),\n+                           b_buffer_.shape.dimensions().end() - 2, int64_t{1},\n+                           std::multiplies<int64_t>());\n+  }\n+\n+  int64_t a_batch_stride() const {\n+    int64_t elem_size = ShapeUtil::ByteSizeOfPrimitiveType(type_);\n+    return side_ == se::blas::Side::kLeft ? (m_ * m_ * elem_size)\n+                                          : (n_ * n_ * elem_size);\n+  }\n+\n+  int64_t b_batch_stride() const {\n+    return m_ * n_ * ShapeUtil::ByteSizeOfPrimitiveType(type_);\n+  }\n+\n   const se::blas::UpperLower uplo_;\n   const se::blas::Side side_;\n   const se::blas::Diagonal unit_diagonal_;\n   se::blas::Transpose transpose_a_;\n \n-  const BufferAllocation::Slice a_buffer_;\n-  const BufferAllocation::Slice b_buffer_;\n-  const BufferAllocation::Slice temp_buffer_;\n+  const ShapedSlice a_buffer_;\n+  const ShapedSlice b_buffer_;\n+  const ShapedSlice temp_buffer_;\n \n   const PrimitiveType type_;\n-  const int64_t batch_size_;\n   const int64_t m_;\n   const int64_t n_;\n-  const int64_t a_batch_stride_;\n-  const int64_t b_batch_stride_;\n };\n \n absl::Status RunTriangularSolve(se::DeviceAddressBase a_data,"
        },
        {
            "sha": "641fbc89be34cbfdd7d8b13135534b4268b3dba8",
            "filename": "third_party/xla/xla/backends/gpu/runtime/triangular_solve_thunk_test.cc",
            "status": "modified",
            "additions": 18,
            "deletions": 9,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0cebad66545e6f271086eb4b68f37fecf3b7fc9f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ftriangular_solve_thunk_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0cebad66545e6f271086eb4b68f37fecf3b7fc9f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ftriangular_solve_thunk_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Ftriangular_solve_thunk_test.cc?ref=0cebad66545e6f271086eb4b68f37fecf3b7fc9f",
            "patch": "@@ -48,15 +48,24 @@ TEST(TriangularSolveThunkTest, ProtoRoundTrip) {\n             unit_diagonal: false\n             transpose_a: TRANSPOSE\n           }\n-          a_buffer { offset: 0 size: 256 buffer_allocation_index: 0 }\n-          b_buffer { offset: 0 size: 256 buffer_allocation_index: 1 }\n-          temp_buffer { offset: 0 size: 128 buffer_allocation_index: 2 }\n-          type: F32\n-          batch_size: 1\n-          m: 32\n-          n: 32\n-          a_batch_stride: 0\n-          b_batch_stride: 1\n+          a_buffer {\n+            slice { offset: 0 size: 256 buffer_allocation_index: 0 }\n+            shape {}\n+          }\n+          b_buffer {\n+            slice { offset: 0 size: 256 buffer_allocation_index: 1 }\n+            shape {\n+              element_type: F32\n+              dimensions: 32\n+              dimensions: 32\n+              is_dynamic_dimension: false\n+              is_dynamic_dimension: false\n+            }\n+          }\n+          temp_buffer {\n+            slice { offset: 0 size: 128 buffer_allocation_index: 2 }\n+            shape {}\n+          }\n         }\n       )pb\",\n       &proto));"
        },
        {
            "sha": "f5acc8b63d0669e8bb913ed4e565a5d6beefffef",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 29,
            "changes": 40,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0cebad66545e6f271086eb4b68f37fecf3b7fc9f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0cebad66545e6f271086eb4b68f37fecf3b7fc9f/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc?ref=0cebad66545e6f271086eb4b68f37fecf3b7fc9f",
            "patch": "@@ -1129,17 +1129,10 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTriangularSolveCustomCall(\n   TF_RET_CHECK(has_fortran_layout(operands[1]->shape().layout()));\n   TF_RET_CHECK(has_fortran_layout(instr->shape().tuple_shapes(0).layout()));\n \n-  TF_ASSIGN_OR_RETURN(BufferAllocation::Slice a_slice,\n-                      GetAllocationSliceForHlo(operands[0]));\n-  TF_ASSIGN_OR_RETURN(BufferAllocation::Slice b_slice,\n-                      GetAllocationSliceForHlo(operands[1]));\n-  TF_ASSIGN_OR_RETURN(BufferAllocation::Slice result_slice,\n-                      GetAllocationSliceForHlo(instr, {0}));\n-  TF_ASSIGN_OR_RETURN(BufferAllocation::Slice temp_slice,\n-                      GetAllocationSliceForHlo(instr, {1}));\n-\n-  const Shape b_shape = operands[1]->shape();\n-  const PrimitiveType elem_ty = b_shape.element_type();\n+  ASSIGN_OR_RETURN(ShapedSlice a_slice, GetShapedSliceForHlo(operands[0]));\n+  ASSIGN_OR_RETURN(ShapedSlice b_slice, GetShapedSliceForHlo(operands[1]));\n+  ASSIGN_OR_RETURN(ShapedSlice result_slice, GetShapedSliceForHlo(instr, {0}));\n+  ASSIGN_OR_RETURN(ShapedSlice temp_slice, GetShapedSliceForHlo(instr, {1}));\n \n   TriangularSolveOptions backend_config;\n   auto& backend_config_str = instr->raw_backend_config_string();\n@@ -1152,30 +1145,19 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitTriangularSolveCustomCall(\n \n   // Triangular solve is in-place on 'b', so copy 'b' to the output\n   // if they aren't the same buffer.\n-  if (b_slice != result_slice) {\n+  if (b_slice.slice != result_slice.slice) {\n     thunks.push_back(std::make_unique<DeviceToDeviceCopyThunk>(\n         Thunk::ThunkInfo::WithProfileAnnotation(\n             instr, ir_emitter_context_->GetNextThunkId()),\n-        /*source_buffer=*/ShapedSlice{b_slice, b_shape},\n-        /*destination_buffer=*/ShapedSlice{result_slice, b_shape},\n-        /*mem_size=*/ShapeUtil::ByteSizeOf(b_shape)));\n-  }\n-\n-  int64_t m = b_shape.dimensions(b_shape.dimensions().size() - 2);\n-  int64_t n = b_shape.dimensions(b_shape.dimensions().size() - 1);\n-  int64_t batch_size = std::accumulate(\n-      b_shape.dimensions().begin(), b_shape.dimensions().end() - 2, int64_t{1},\n-      [](int64_t a, int64_t b) { return a * b; });\n-  int64_t elem_size = ShapeUtil::ByteSizeOfPrimitiveType(elem_ty);\n-  int64_t a_batch_stride =\n-      backend_config.left_side() ? m * m * elem_size : n * n * elem_size;\n-  int64_t b_batch_stride = m * n * elem_size;\n+        /*source_buffer=*/b_slice,\n+        /*destination_buffer=*/result_slice,\n+        /*mem_size=*/ShapeUtil::ByteSizeOf(b_slice.shape)));\n+  }\n+\n   thunks.push_back(std::make_unique<TriangularSolveThunk>(\n       Thunk::ThunkInfo::WithProfileAnnotation(\n           instr, ir_emitter_context_->GetNextThunkId()),\n-      backend_config,\n-      /*a_buffer=*/a_slice, /*b_buffer=*/result_slice, temp_slice, elem_ty,\n-      batch_size, m, n, a_batch_stride, b_batch_stride));\n+      backend_config, a_slice, result_slice, temp_slice));\n \n   // Elide the sequential thunk if there's no copy.\n   if (thunks.size() == 1) {"
        }
    ],
    "stats": {
        "total": 183,
        "additions": 85,
        "deletions": 98
    }
}