{
    "author": "terryysun",
    "message": "PR #34362: [GPU] Link to optimization level doc in GPU flag guidance\n\nImported from GitHub PR https://github.com/openxla/xla/pull/34362\n\nüìù Summary of Changes\nLink to optimization level doc in GPU flag guidance.\n\nüéØ Justification\nUsers can use -O1 to easily set several flags instead of setting them one by one. We should mention it in flag guidance.\n\nüöÄ Kind of Contribution\nüìö Documentation\n\nüìä Benchmark (for Performance Improvements)\nN/A\n\nüß™ Unit Tests:\nN/A\n\nüß™ Execution Tests:\nN/A\n\nCopybara import of the project:\n\n--\n831ba08529e623c9359bb92a9f0f314849d91bc0 by Terry Sun <tesun@nvidia.com>:\n\nlink to -O1 in GPU flag guidance\n\nMerging this change closes #34362\n\nPiperOrigin-RevId: 837083576",
    "sha": "de066ed27a2ee5e434095da3808efed53aef6aa7",
    "files": [
        {
            "sha": "f887b66c4fbc5083525d57519d8e21884f3108b1",
            "filename": "third_party/xla/docs/flags_guidance.md",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/de066ed27a2ee5e434095da3808efed53aef6aa7/third_party%2Fxla%2Fdocs%2Fflags_guidance.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/de066ed27a2ee5e434095da3808efed53aef6aa7/third_party%2Fxla%2Fdocs%2Fflags_guidance.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Fflags_guidance.md?ref=de066ed27a2ee5e434095da3808efed53aef6aa7",
            "patch": "@@ -70,6 +70,18 @@ Flag                                            | Type                 | Notes\n `xla_tpu_enable_ag_backward_pipelining`         | Boolean (true/false) | Pipelines all-gathers (currently megascale all-gathers) backwards through scan loops.\n \n ### GPU XLA flags\n+\n+The `-O1` optimization level enables advanced compiler passes for improved GPU\n+performance, including several categories of flags below: pipelining of\n+data-parallel collectives (`xla_gpu_enable_pipelined_all_gather`,\n+`xla_gpu_enable_pipelined_all_reduce`,\n+`xla_gpu_enable_pipelined_reduce_scatter`), while loop unrolling\n+(`xla_gpu_enable_while_loop_double_buffering`), latency hiding scheduling\n+(`xla_gpu_enable_latency_hiding_scheduler`), and SOL latency estimator on\n+Hopper/Blackwell (`xla_gpu_enable_analytical_sol_latency_estimator`). See\n+[GPU Optimization Levels](https://openxla.org/xla/gpu_optimization_levels) for\n+details.\n+\n | Flag | Type | Notes |\n | :---- | :---- | :----- |\n | `xla_gpu_enable_latency_hiding_scheduler` | Boolean (true/false) |This flag enables latency hiding schedulers to overlap asynchronous communication with computation efficiently. The default value is False. |"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 12,
        "deletions": 0
    }
}