{
    "author": "akhilgoe",
    "message": "PR #32378: [XLA:CPU][oneDNN][BugFix] Fix unsafe subtraction from unsigned values\n\nImported from GitHub PR https://github.com/openxla/xla/pull/32378\n\nA recent change updated the method used to get the size of the dimensions of a `Shape` (from `dimensions_size()` to `dimensions().size()`) in `onednn_contraction_rewriter.cc`. The new method returns an unsigned value whereas the earlier one returned a signed integer. As a result, we need to make some modifications here to avoid any unsigned underflow.\nCopybara import of the project:\n\n--\ne9707c5bb586da69564360f77345a53c166318ce by Akhil Goel <akhil.goel@intel.com>:\n\nInitial commit\n\n--\nd0b3fa5302ef535f32cb8ebb9dcf54d6f63511f5 by Akhil Goel <akhil.goel@intel.com>:\n\nAdd comment\n\nMerging this change closes #32378\n\nPiperOrigin-RevId: 839756323",
    "sha": "7131b895aefff694900cdb321465cefcb90ab1f2",
    "files": [
        {
            "sha": "facb6e644e655496a057b4a878db8152a494a9fb",
            "filename": "third_party/xla/xla/service/cpu/onednn_contraction_rewriter.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 8,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7131b895aefff694900cdb321465cefcb90ab1f2/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_contraction_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7131b895aefff694900cdb321465cefcb90ab1f2/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_contraction_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fonednn_contraction_rewriter.cc?ref=7131b895aefff694900cdb321465cefcb90ab1f2",
            "patch": "@@ -401,8 +401,14 @@ absl::StatusOr<Shape> AdjustAddendShape(const HloInstruction* contraction,\n   // If rank(new_shape) > rank(instr), extra dimensions with value = 1 can be\n   // deleted from the new_shape.\n   auto instr_shape = contraction->shape();\n-  int64_t rank_difference =\n-      new_shape.dimensions().size() - instr_shape.dimensions().size();\n+  int64_t rank_difference = 0;\n+  // Since the absl APIs return unsigned dimension sizes, compute the rank\n+  // difference only when LHS > RHS. Performing an unconditional subtraction\n+  // could cause an unsigned underflow.\n+  if (new_shape.dimensions().size() > instr_shape.dimensions().size()) {\n+    rank_difference =\n+        new_shape.dimensions().size() - instr_shape.dimensions().size();\n+  }\n   auto new_dims = new_shape.dimensions();\n   std::vector<int64_t> dims_to_delete;\n   for (int i = 0; i < rank_difference; ++i) {\n@@ -446,8 +452,8 @@ inline bool IsOperandFusible(HloInstruction* operand, HloInstruction* instr) {\n   if (operand_dims.size() > instr_dims.size()) {\n     return false;\n   }\n-  int operand_idx = operand_dims.size() - 1;\n-  int instr_idx = instr_dims.size() - 1;\n+  int operand_idx = static_cast<int>(operand_dims.size()) - 1;\n+  int instr_idx = static_cast<int>(instr_dims.size()) - 1;\n   for (; operand_idx >= 0; --operand_idx, --instr_idx) {\n     if (operand_dims[operand_idx] != 1 &&\n         operand_dims[operand_idx] != instr_dims[instr_idx]) {\n@@ -536,8 +542,8 @@ bool OneDnnContractionRewriter::ShouldRewriteDot(\n   int64_t rhs_dim_k = dot_dim_numbers.rhs_contracting_dimensions(0);\n \n   // Supported contraction is only in one of last two dimensions.\n-  if (lhs_dim_k < lhs_shape.dimensions().size() - 2 ||\n-      rhs_dim_k < rhs_shape.dimensions().size() - 2) {\n+  if (lhs_dim_k + 2 < lhs_shape.dimensions().size() ||\n+      rhs_dim_k + 2 < rhs_shape.dimensions().size()) {\n     return false;\n   }\n \n@@ -632,8 +638,8 @@ class OneDnnContractionRewriteVisitor : public DfsHloRewriteVisitor {\n     BackendConfig backend_config;\n     OneDnnMatMulConfig* matmul_config =\n         backend_config.mutable_onednn_matmul_config();\n-    bool transpose_a = (lhs_dim_k != lhs_shape.dimensions().size() - 1);\n-    bool transpose_b = (rhs_dim_k != rhs_shape.dimensions().size() - 2);\n+    bool transpose_a = (lhs_dim_k + 1 != lhs_shape.dimensions().size());\n+    bool transpose_b = (rhs_dim_k + 2 != rhs_shape.dimensions().size());\n     matmul_config->set_transpose_a(transpose_a);\n     matmul_config->set_transpose_b(transpose_b);\n     TF_RETURN_IF_ERROR(matmul_call->set_backend_config(backend_config));"
        }
    ],
    "stats": {
        "total": 22,
        "additions": 14,
        "deletions": 8
    }
}