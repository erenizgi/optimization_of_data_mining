{
    "author": "shawnwang18",
    "message": "PR #33505: [XLA:GPU] Dump command buffer contents to folder specified by --xla-dump-to through dump.h\n\nImported from GitHub PR https://github.com/openxla/xla/pull/33505\n\nüìù Summary of Changes\nThis PR migrates the command buffer content's dump through dump.h\n\nCopybara import of the project:\n\n--\n0b870f229a40435766613743d47f274e06af4763 by Shawn Wang <shawnw@nvidia.com>:\n\nDump command buffer contents to folder specified by --xla-dump-to through dump.h\n\n--\n123c2541701f4d65771b522fbb68e1e0edb7a6e4 by Shawn Wang <shawnw@nvidia.com>:\n\nclang format fix\n\nMerging this change closes #33505\n\nPiperOrigin-RevId: 828357446",
    "sha": "852c9cafebee385327d924ba0dffac07452a0678",
    "files": [
        {
            "sha": "8519a5ba53979fbe2a7af4aeba26286e00e096cd",
            "filename": "third_party/xla/xla/stream_executor/command_buffer.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/852c9cafebee385327d924ba0dffac07452a0678/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcommand_buffer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/852c9cafebee385327d924ba0dffac07452a0678/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcommand_buffer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcommand_buffer.h?ref=852c9cafebee385327d924ba0dffac07452a0678",
            "patch": "@@ -311,6 +311,8 @@ class CommandBuffer {\n   // Returns command buffer state.\n   virtual State state() const = 0;\n \n+  virtual std::string ToString() const = 0;\n+\n   //--------------------------------------------------------------------------//\n   // Command buffer tracing API\n   //--------------------------------------------------------------------------//"
        },
        {
            "sha": "078f19749226c37dc691beb81b81cd9a3a68edbd",
            "filename": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/852c9cafebee385327d924ba0dffac07452a0678/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/852c9cafebee385327d924ba0dffac07452a0678/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2FBUILD?ref=852c9cafebee385327d924ba0dffac07452a0678",
            "patch": "@@ -1559,6 +1559,7 @@ cc_library(\n         \"@com_google_absl//absl/types:span\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n         \"@local_tsl//tsl/platform:casts\",\n+        \"@local_tsl//tsl/platform:path\",\n     ],\n )\n "
        },
        {
            "sha": "010e5a38a091cdadac7e26edf5dea9d649cc38d2",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_command_buffer.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/852c9cafebee385327d924ba0dffac07452a0678/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_command_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/852c9cafebee385327d924ba0dffac07452a0678/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_command_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_command_buffer.cc?ref=852c9cafebee385327d924ba0dffac07452a0678",
            "patch": "@@ -53,6 +53,7 @@ limitations under the License.\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"tsl/platform/casts.h\"\n+#include \"tsl/platform/path.h\"\n \n namespace stream_executor::gpu {\n namespace {\n@@ -831,4 +832,25 @@ absl::Status CudaCommandBuffer::CheckCanBeUpdated() {\n   return absl::OkStatus();\n }\n \n+std::string CudaCommandBuffer::ToString() const {\n+  std::string path = tsl::io::GetTempFilename(/*extension=*/\"dot\");\n+#if CUDA_VERSION >= 12000\n+  int flags = CU_GRAPH_DEBUG_DOT_FLAGS_VERBOSE;\n+  auto dot_print_status =\n+      cuda::ToStatus(cuGraphDebugDotPrint(graph_, path.c_str(), flags),\n+                     \"Failed to print gpu graph debug file\");\n+  if (!dot_print_status.ok()) {\n+    return std::string(dot_print_status.message());\n+  }\n+  std::string dot_file_contents;\n+  auto read_status =\n+      tsl::ReadFileToString(tsl::Env::Default(), path, &dot_file_contents);\n+  if (!read_status.ok()) {\n+    return std::string(read_status.message());\n+  }\n+  return dot_file_contents;\n+#endif  // CUDA_VERSION >= 12000\n+  return \"CUDA graph debug dot print is not supported.\";\n+}\n+\n }  // namespace stream_executor::gpu"
        },
        {
            "sha": "84fb08a2cfa60d0d2d5123713b17549873a89987",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_command_buffer.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/852c9cafebee385327d924ba0dffac07452a0678/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_command_buffer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/852c9cafebee385327d924ba0dffac07452a0678/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_command_buffer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_command_buffer.h?ref=852c9cafebee385327d924ba0dffac07452a0678",
            "patch": "@@ -53,6 +53,8 @@ class CudaCommandBuffer final : public GpuCommandBuffer {\n   static absl::StatusOr<std::unique_ptr<CudaCommandBuffer>> Create(\n       Mode mode, StreamExecutor* executor, CudaContext* cuda_context);\n \n+  std::string ToString() const override;\n+\n   ~CudaCommandBuffer() override;\n \n  private:"
        },
        {
            "sha": "ac149e31c67a4c00b169479c1591823e6e73e552",
            "filename": "third_party/xla/xla/stream_executor/gpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/852c9cafebee385327d924ba0dffac07452a0678/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/852c9cafebee385327d924ba0dffac07452a0678/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2FBUILD?ref=852c9cafebee385327d924ba0dffac07452a0678",
            "patch": "@@ -137,6 +137,8 @@ cc_library(\n         \"gpu_command_buffer.h\",\n     ],\n     deps = [\n+        \"//xla:debug_options_flags\",\n+        \"//xla/service:dump\",\n         \"//xla/stream_executor:bit_pattern\",\n         \"//xla/stream_executor:command_buffer\",\n         \"//xla/stream_executor:device_memory\","
        },
        {
            "sha": "2c55443e552a9876997366f195d97f36d56313ae",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_command_buffer.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/852c9cafebee385327d924ba0dffac07452a0678/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_command_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/852c9cafebee385327d924ba0dffac07452a0678/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_command_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_command_buffer.cc?ref=852c9cafebee385327d924ba0dffac07452a0678",
            "patch": "@@ -33,6 +33,8 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/debug_options_flags.h\"\n+#include \"xla/service/dump.h\"\n #include \"xla/stream_executor/bit_pattern.h\"\n #include \"xla/stream_executor/command_buffer.h\"\n #include \"xla/stream_executor/device_memory.h\"\n@@ -571,14 +573,10 @@ absl::Status GpuCommandBuffer::Finalize() {\n   // Maybe dump created GPU graph to a dot file for debugging.\n   if (state_ == State::kCreate &&\n       (VLOG_IS_ON(10) || (VLOG_IS_ON(9) && mode_ == Mode::kPrimary))) {\n-    std::string path = tsl::io::GetTempFilename(/*extension=*/\"dot\");\n-    TF_RETURN_IF_ERROR(WriteGraphToDotFile(path));\n-    if (VLOG_IS_ON(100) || (VLOG_IS_ON(90) && mode_ == Mode::kPrimary)) {\n-      std::string dot_file_contents;\n-      TF_RETURN_IF_ERROR(\n-          tsl::ReadFileToString(tsl::Env::Default(), path, &dot_file_contents));\n-      VLOG(90) << \"Contents of \" << path << \" is:\\n\" << dot_file_contents;\n-    }\n+    xla::DebugOptions debug_options = xla::GetDebugOptionsFromFlags();\n+    std::string contents = ToString();\n+    std::string filename = absl::StrFormat(\"gpu_command_buffer_%p.dot\", this);\n+    xla::DumpToFileInDir(debug_options, filename, contents);\n   }\n \n   size_t num_commands = commands_.size();"
        },
        {
            "sha": "9f462f338977f28e05a7e2f08a6e0a670305ded7",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_command_buffer.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/852c9cafebee385327d924ba0dffac07452a0678/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_command_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/852c9cafebee385327d924ba0dffac07452a0678/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_command_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_command_buffer.cc?ref=852c9cafebee385327d924ba0dffac07452a0678",
            "patch": "@@ -463,4 +463,8 @@ absl::Status RocmCommandBuffer::CheckCanBeUpdated() {\n   return absl::OkStatus();\n }\n \n+std::string RocmCommandBuffer::ToString() const {\n+  return \"ROCM graph debug dot print is not supported.\";\n+}\n+\n }  // namespace stream_executor::gpu"
        },
        {
            "sha": "9f680dff3ffbb4cd167f190ab58ddac5310af6ab",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_command_buffer.h",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/852c9cafebee385327d924ba0dffac07452a0678/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_command_buffer.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/852c9cafebee385327d924ba0dffac07452a0678/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_command_buffer.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_command_buffer.h?ref=852c9cafebee385327d924ba0dffac07452a0678",
            "patch": "@@ -46,6 +46,8 @@ class RocmCommandBuffer : public GpuCommandBuffer {\n   static absl::StatusOr<std::unique_ptr<RocmCommandBuffer>> Create(\n       Mode mode, StreamExecutor* executor);\n \n+  std::string ToString() const override;\n+\n   ~RocmCommandBuffer() override;\n \n  private:"
        }
    ],
    "stats": {
        "total": 49,
        "additions": 41,
        "deletions": 8
    }
}