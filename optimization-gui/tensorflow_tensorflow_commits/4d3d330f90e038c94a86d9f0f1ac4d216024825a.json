{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 845598510",
    "sha": "4d3d330f90e038c94a86d9f0f1ac4d216024825a",
    "files": [
        {
            "sha": "73214c817eaf046395bd87349ae9d54fa07379a2",
            "filename": "tensorflow/core/tpu/kernels/tpu_reshard_variables_op_util.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 8,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4d3d330f90e038c94a86d9f0f1ac4d216024825a/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_reshard_variables_op_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4d3d330f90e038c94a86d9f0f1ac4d216024825a/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_reshard_variables_op_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_reshard_variables_op_util.cc?ref=4d3d330f90e038c94a86d9f0f1ac4d216024825a",
            "patch": "@@ -99,7 +99,7 @@ absl::Status GetComputationCacheEntry(\n }\n \n // Builds an InputBuffers object that describes the inputs to the computation.\n-absl::StatusOr<xla::ShapeTree<xla::MaybeOwningDeviceMemory>> BuildInputBuffers(\n+absl::StatusOr<xla::ShapeTree<xla::MaybeOwningDeviceAddress>> BuildInputBuffers(\n     OpKernelContext* context, const std::vector<VariableInfo>& variables,\n     const xla::Shape& input_host_shape, xla::Backend* backend,\n     int device_ordinal, se::Stream* stream) {\n@@ -150,10 +150,11 @@ absl::StatusOr<xla::ShapeTree<xla::MaybeOwningDeviceMemory>> BuildInputBuffers(\n         validate_shape(variables[i].index(), *variables[i].var()->tensor()));\n   }\n \n-  se::DeviceMemoryAllocator* const allocator = backend->memory_allocator();\n+  stream_executor::DeviceAddressAllocator* const allocator =\n+      backend->memory_allocator();\n   xla::TransferManager* const transfer_manager = backend->transfer_manager();\n \n-  xla::ShapeTree<xla::MaybeOwningDeviceMemory> input_buffers(\n+  xla::ShapeTree<xla::MaybeOwningDeviceAddress> input_buffers(\n       transfer_manager->HostShapeToDeviceShape(input_host_shape));\n \n   // Allocates a buffer for the root tuple.\n@@ -165,15 +166,17 @@ absl::StatusOr<xla::ShapeTree<xla::MaybeOwningDeviceMemory>> BuildInputBuffers(\n   auto set_input_buffers_helper = [&](int arg_index, xla::ShapedBuffer* buffers,\n                                       bool owning = false) {\n     buffers->buffers().ForEachMutableElement(\n-        [&](const xla::ShapeIndex& index, se::DeviceMemoryBase* buffer) {\n+        [&](const xla::ShapeIndex& index,\n+            stream_executor::DeviceAddressBase* buffer) {\n           xla::ShapeIndex in_index = {arg_index};\n           for (int64_t j : index) {\n             in_index.push_back(j);\n           }\n           if (owning) {\n             *input_buffers.mutable_element(in_index) =\n-                se::OwningDeviceMemory(*buffer, device_ordinal, allocator);\n-            *buffer = se::DeviceMemoryBase();\n+                stream_executor::ScopedDeviceAddress<uint8_t>(\n+                    *buffer, device_ordinal, allocator);\n+            *buffer = stream_executor::DeviceAddressBase();\n           } else {\n             *input_buffers.mutable_element(in_index) = *buffer;\n           }\n@@ -268,7 +271,8 @@ absl::Status UpdateOutputVariables(\n   TF_RET_CHECK(result_buffers.on_host_shape().IsTuple());\n   TF_RET_CHECK(!xla::ShapeUtil::IsNestedTuple(result_buffers.on_host_shape()));\n \n-  se::DeviceMemoryAllocator* const allocator = backend->memory_allocator();\n+  stream_executor::DeviceAddressAllocator* const allocator =\n+      backend->memory_allocator();\n \n   auto output_buffers = result_buffers.release();\n   const xla::Shape& output_host_shape = output_buffers.on_host_shape();\n@@ -285,7 +289,8 @@ absl::Status UpdateOutputVariables(\n       xla::ScopedShapedBuffer shaped_buffer(host_shape, device_shape, allocator,\n                                             device_ordinal);\n       shaped_buffer.buffers().ForEachMutableElement(\n-          [&](const xla::ShapeIndex& index, se::DeviceMemoryBase* buffer) {\n+          [&](const xla::ShapeIndex& index,\n+              stream_executor::DeviceAddressBase* buffer) {\n             xla::ShapeIndex out_index = {i};\n             for (int64_t j : index) {\n               out_index.push_back(j);"
        },
        {
            "sha": "ab44f7788fbf50ca312af0f56974dea2afd8996f",
            "filename": "tensorflow/core/tpu/kernels/tpu_reshard_variables_op_util.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/4d3d330f90e038c94a86d9f0f1ac4d216024825a/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_reshard_variables_op_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/4d3d330f90e038c94a86d9f0f1ac4d216024825a/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_reshard_variables_op_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Fkernels%2Ftpu_reshard_variables_op_util.h?ref=4d3d330f90e038c94a86d9f0f1ac4d216024825a",
            "patch": "@@ -42,7 +42,7 @@ absl::Status GetComputationCacheEntry(\n     std::unique_ptr<tpu::CompilationCacheEntryRef>* entry,\n     tpu::CompilationCacheFetchTarget fetch_target);\n \n-absl::StatusOr<xla::ShapeTree<xla::MaybeOwningDeviceMemory>> BuildInputBuffers(\n+absl::StatusOr<xla::ShapeTree<xla::MaybeOwningDeviceAddress>> BuildInputBuffers(\n     OpKernelContext* context, const std::vector<VariableInfo>& variables,\n     const xla::Shape& input_host_shape, xla::Backend* backend,\n     int device_ordinal, se::Stream* stream);"
        }
    ],
    "stats": {
        "total": 23,
        "additions": 14,
        "deletions": 9
    }
}