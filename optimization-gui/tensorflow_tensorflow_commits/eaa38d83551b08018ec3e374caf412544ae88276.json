{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Fix dispatch decomposition when replica groups are shuffled.\n\nWhen replica groups for for singe host are not contiguous, we need an extra step to reorder offsets and sizes metadata operands. We need a similar for combine ragged-all-to-all that I'll do in the following change.\n\nPiperOrigin-RevId: 842830042",
    "sha": "eaa38d83551b08018ec3e374caf412544ae88276",
    "files": [
        {
            "sha": "1a95a47a2258238018d9370ccdabd9846a529036",
            "filename": "third_party/xla/xla/service/gpu/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eaa38d83551b08018ec3e374caf412544ae88276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eaa38d83551b08018ec3e374caf412544ae88276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2FBUILD?ref=eaa38d83551b08018ec3e374caf412544ae88276",
            "patch": "@@ -3106,6 +3106,7 @@ cc_library(\n     srcs = [\"ragged_all_to_all_multi_host_decomposer.cc\"],\n     hdrs = [\"ragged_all_to_all_multi_host_decomposer.h\"],\n     deps = [\n+        \"//xla:array\",\n         \"//xla:literal_util\",\n         \"//xla:shape_util\",\n         \"//xla:util\","
        },
        {
            "sha": "a3d967d7b4cb40e8cd235e626029ea13a260e048",
            "filename": "third_party/xla/xla/service/gpu/transforms/ragged_all_to_all_multi_host_decomposer.cc",
            "status": "modified",
            "additions": 116,
            "deletions": 7,
            "changes": 123,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eaa38d83551b08018ec3e374caf412544ae88276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eaa38d83551b08018ec3e374caf412544ae88276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer.cc?ref=eaa38d83551b08018ec3e374caf412544ae88276",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/array.h\"\n #include \"xla/hlo/ir/dfs_hlo_visitor.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n@@ -50,6 +51,90 @@ namespace gpu {\n \n using hlo_query::NextChannelId;\n \n+// Returns a permutation of the devices in the replica group such that devices\n+// on the same host are next to each other. The order of the devices within a\n+// host is preserved.\n+absl::InlinedVector<int64_t, 8> FindPermutation(\n+    const ReplicaGroup& replica_group, int64_t num_devices_per_host) {\n+  int64_t num_devices_in_replica = replica_group.replica_ids_size();\n+\n+  absl::InlinedVector<int64_t, 8> permutation(num_devices_in_replica);\n+  absl::c_iota(permutation, 0);\n+\n+  absl::c_stable_sort(permutation, [&](int64_t i, int64_t j) {\n+    int64_t host_i = replica_group.replica_ids(i) / num_devices_per_host;\n+    int64_t host_j = replica_group.replica_ids(j) / num_devices_per_host;\n+    return host_i < host_j;\n+    return replica_group.replica_ids(i) < replica_group.replica_ids(j);\n+  });\n+  return permutation;\n+}\n+\n+// Returns a permutation of the devices in the replica groups such that devices\n+// on the same host are next to each other. Returns std::nullopt if the\n+// permutation is not the same for all replica groups.\n+std::optional<absl::InlinedVector<int64_t, 8>> FindReplicaGroupsPermutation(\n+    absl::Span<ReplicaGroup const> replica_groups,\n+    int64_t num_devices_per_host) {\n+  absl::InlinedVector<int64_t, 8> permutation =\n+      FindPermutation(replica_groups[0], num_devices_per_host);\n+\n+  // Check that all replica groups have the same permutation. Operand\n+  // permutation doesn't not depend on the device id, so if permutations are\n+  // different, we can't rewrite the ragged-all-to-all.\n+  for (int64_t i = 1; i < replica_groups.size(); ++i) {\n+    auto replica_group_permutation =\n+        FindPermutation(replica_groups[i], num_devices_per_host);\n+    if (replica_group_permutation != permutation) {\n+      return std::nullopt;\n+    }\n+  }\n+\n+  return permutation;\n+}\n+\n+// Shuffle values in the hlo instruction based on the permutation.\n+HloInstruction* ShuffleMetadataOperandValues(\n+    HloInstruction* hlo, absl::Span<int64_t const> permutation) {\n+  // If the permutation is already sorted, then we don't need to shuffle.\n+  if (absl::c_is_sorted(permutation)) {\n+    return hlo;\n+  }\n+\n+  HloComputation* computation = hlo->parent();\n+\n+  const Shape& shape = hlo->shape();\n+  CHECK_EQ(shape.dimensions().size(), 1);\n+\n+  int64_t num_elements = shape.dimensions(0);\n+  int64_t num_replicas = permutation.size();\n+  int64_t num_elements_per_replica = num_elements / permutation.size();\n+\n+  Array<int64_t> permutation_array({num_replicas, 1});\n+  for (int64_t i = 0; i < permutation.size(); ++i) {\n+    permutation_array(i, 0) = num_elements_per_replica * permutation[i];\n+  }\n+\n+  auto permutation_constant =\n+      computation->AddInstruction(HloInstruction::CreateConstant(\n+          LiteralUtil::CreateFromArray(permutation_array)));\n+\n+  Shape new_shape = ShapeUtil::MakeShape(\n+      shape.element_type(), {num_replicas, num_elements_per_replica});\n+\n+  hlo = computation->AddInstruction(\n+      HloInstruction::CreateGather(new_shape, hlo, permutation_constant,\n+                                   HloGatherInstruction::MakeGatherDimNumbers(\n+                                       /*offset_dims=*/{1},\n+                                       /*collapsed_slice_dims=*/{},\n+                                       /*start_index_map=*/{0},\n+                                       /*index_vector_dim=*/1),\n+                                   /*slice_sizes=*/{num_elements_per_replica},\n+                                   /*indices_are_sorted=*/false));\n+\n+  return computation->AddInstruction(HloInstruction::CreateReshape(shape, hlo));\n+}\n+\n // Corrects the offsets in the local metadata to account for the number of input\n // rows in the combined ragged tensor.\n HloInstruction* CorrectOffsets(int64_t offset, HloInstruction* local_metadata,\n@@ -83,13 +168,16 @@ HloInstruction* CorrectOffsets(int64_t offset, HloInstruction* local_metadata,\n absl::InlinedVector<HloInstruction*, 4> GetIntraHostMetadata(\n     HloRaggedAllToAllInstruction* ragged_all_to_all,\n     HloComputation* computation, absl::Span<ReplicaGroup const> replica_groups,\n-    int64_t num_hosts, int64_t num_devices_in_replica) {\n+    absl::Span<int64_t const> replica_groups_permutation, int64_t num_hosts,\n+    int64_t num_devices_in_replica) {\n   int64_t num_devices_in_replica_per_host = num_devices_in_replica / num_hosts;\n \n   absl::InlinedVector<HloInstruction*, 4> metadata_operands;\n   metadata_operands.reserve(4);\n   for (int i = 2; i < 6; ++i) {\n     metadata_operands.push_back(ragged_all_to_all->mutable_operand(i));\n+    metadata_operands.back() = ShuffleMetadataOperandValues(\n+        metadata_operands.back(), replica_groups_permutation);\n   }\n \n   Shape metadata_operand_shape = metadata_operands[0]->shape();\n@@ -179,7 +267,8 @@ absl::StatusOr<bool> DecomposeDispatchRaggedAllToAll(\n     HloRaggedAllToAllInstruction* ragged_all_to_all,\n     HloComputation* computation,\n     absl::Span<ReplicaGroup const> inter_host_replica_groups,\n-    absl::Span<ReplicaGroup const> intra_host_replica_groups, int64_t num_hosts,\n+    absl::Span<ReplicaGroup const> intra_host_replica_groups,\n+    absl::Span<int64_t const> replica_groups_permutation, int64_t num_hosts,\n     int64_t num_devices_in_replica) {\n   HloInstruction* input_operand = ragged_all_to_all->mutable_operand(0);\n \n@@ -208,9 +297,9 @@ absl::StatusOr<bool> DecomposeDispatchRaggedAllToAll(\n           ragged_all_to_all->channel_id().has_value()));\n \n   absl::InlinedVector<HloInstruction*, 4> intra_host_metadata =\n-      GetIntraHostMetadata(ragged_all_to_all, computation,\n-                           inter_host_replica_groups, num_hosts,\n-                           num_devices_in_replica);\n+      GetIntraHostMetadata(\n+          ragged_all_to_all, computation, inter_host_replica_groups,\n+          replica_groups_permutation, num_hosts, num_devices_in_replica);\n \n   HloInstruction* new_ragged_all_to_all =\n       computation->AddInstruction(HloInstruction::CreateRaggedAllToAll(\n@@ -219,7 +308,7 @@ absl::StatusOr<bool> DecomposeDispatchRaggedAllToAll(\n           {all_gather_input, ragged_all_to_all->mutable_operand(1),\n            intra_host_metadata[0], intra_host_metadata[1],\n            intra_host_metadata[2], intra_host_metadata[3]},\n-          /*replica_groups=*/intra_host_replica_groups,\n+          /*device_list=*/CollectiveDeviceList(intra_host_replica_groups),\n           /*channel_id=*/ragged_all_to_all->channel_id()));\n \n   TF_RETURN_IF_ERROR(computation->ReplaceInstruction(ragged_all_to_all,\n@@ -430,6 +519,25 @@ absl::StatusOr<bool> DecomposeRaggedAllToAll(\n     return false;\n   }\n \n+  // Offsets and sizes in metadata operands are stored in the order of replica\n+  // groups. For example, if the replica groups are:\n+  //   {{0, 2, 4, 6, 1, 3, 5, 7}}\n+  // Then the offsets and sizes are stored in the order of\n+  //   [0, 2, 4, 6, 1, 3, 5, 7]\n+  // In the decomposition, we want to exchange all the intra-host metadata\n+  // between hosts. To do that we want to group the metadata by hosts. We\n+  // compute permutation that need to be performed on the metadata operand and\n+  // use gather to move values. After the shuffle, offsets and sizes will be\n+  // ordered as:\n+  //   [0, 2, 1, 3, 4, 6, 5, 7]\n+  auto replica_groups_permutation = FindReplicaGroupsPermutation(\n+      replica_groups, fast_interconnect_slice_size);\n+  // Empty value means that we can not find such permutation and the\n+  // ragged-all-to-all can not be decomposed.\n+  if (!replica_groups_permutation.has_value()) {\n+    return false;\n+  }\n+\n   // Decompose the replica groups into inter-host and intra-host replica groups.\n   // For example, if the original replica groups were:\n   //   {{0, 2, 4, 6, 8, 10, 12, 14}, {1, 3, 5, 7, 9, 11, 13, 15}}\n@@ -488,7 +596,8 @@ absl::StatusOr<bool> DecomposeRaggedAllToAll(\n \n   return DecomposeDispatchRaggedAllToAll(\n       ragged_all_to_all, computation, inter_host_replica_groups,\n-      intra_host_replica_groups, num_hosts, num_devices_in_replica);\n+      intra_host_replica_groups, *replica_groups_permutation, num_hosts,\n+      num_devices_in_replica);\n }\n \n absl::StatusOr<bool> RaggedAllToAllMultiHostDecomposer::RunImpl("
        },
        {
            "sha": "e0df20ff42a18944a156ab03af265ebbc13b9270",
            "filename": "third_party/xla/xla/service/gpu/transforms/ragged_all_to_all_multi_host_decomposer_test.cc",
            "status": "modified",
            "additions": 36,
            "deletions": 2,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eaa38d83551b08018ec3e374caf412544ae88276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eaa38d83551b08018ec3e374caf412544ae88276/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fragged_all_to_all_multi_host_decomposer_test.cc?ref=eaa38d83551b08018ec3e374caf412544ae88276",
            "patch": "@@ -68,8 +68,7 @@ ENTRY main {\n   )\"));\n }\n \n-TEST_F(RaggedAllToAllDecomposerTest,\n-       SimpleRaggedAllToAllCrossPartitionIsSupported) {\n+TEST_F(RaggedAllToAllDecomposerTest, DispatchRaggedAllToAllIsDecomposed) {\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n HloModule module, num_partitions=16\n \n@@ -102,6 +101,41 @@ ENTRY main {\n   )\"));\n }\n \n+TEST_F(RaggedAllToAllDecomposerTest,\n+       DispatchRaggedAllToAllWithShuffledReplicaGroupsIsDecomposed) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n+HloModule module, num_partitions=16\n+\n+ENTRY main {\n+  input = bf16[128] parameter(0)\n+  output = bf16[256] parameter(1)\n+  input_offsets = s64[32] parameter(2)\n+  send_sizes = s64[32] parameter(3)\n+  output_offsets = s64[32] parameter(4)\n+  recv_sizes = s64[32] parameter(5)\n+  ROOT ra2a = bf16[256] ragged-all-to-all(input, output, input_offsets,\n+    send_sizes, output_offsets, recv_sizes),\n+    replica_groups={{0,4,8,12,1,5,9,13,2,6,10,14,3,7,11,15}}\n+}\n+)\"));\n+\n+  RaggedAllToAllMultiHostDecomposer decomposer(\n+      /*fast_interconnect_slice_size=*/8);\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, decomposer.Run(module.get(), {}));\n+\n+  EXPECT_TRUE(changed);\n+  EXPECT_OK(VerifyHloModule(module.get(), true, true));\n+  EXPECT_OK(HloDCE().Run(module.get()));\n+  EXPECT_OK(HloCSE(true).Run(module.get()));\n+\n+  EXPECT_TRUE(*RunFileCheck(module->ToString(), R\"(\n+    // CHECK: all-gather{{.*}}, replica_groups={{[{]}}{0,8},{4,12},{1,9},{5,13},{2,10},{6,14},{3,11},{7,15}{{[}]}}\n+    // CHECK-COUNT-4: s64[16,2]{1,0} gather\n+    // CHECK: all-to-all{{.*}}, replica_groups={{[{]}}{0,8},{4,12},{1,9},{5,13},{2,10},{6,14},{3,11},{7,15}{{[}]}}\n+    // CHECK: ragged-all-to-all{{.*}}, replica_groups={{[{]}}{0,4,1,5,2,6,3,7},{8,12,9,13,10,14,11,15}{{[}]}}\n+  )\"));\n+}\n+\n TEST_F(RaggedAllToAllDecomposerTest, SingleHostRaggedAllToAllIsNotDecomposed) {\n   TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(R\"(\n HloModule module"
        },
        {
            "sha": "f74275d7933bc3ba4794b625bf905aaad6d88909",
            "filename": "third_party/xla/xla/tests/ragged_all_to_all_e2e_test.cc",
            "status": "modified",
            "additions": 60,
            "deletions": 0,
            "changes": 60,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/eaa38d83551b08018ec3e374caf412544ae88276/third_party%2Fxla%2Fxla%2Ftests%2Fragged_all_to_all_e2e_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/eaa38d83551b08018ec3e374caf412544ae88276/third_party%2Fxla%2Fxla%2Ftests%2Fragged_all_to_all_e2e_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fragged_all_to_all_e2e_test.cc?ref=eaa38d83551b08018ec3e374caf412544ae88276",
            "patch": "@@ -931,6 +931,66 @@ TEST_P(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_2GPUs_SliceSize1) {\n   }\n }\n \n+TEST_P(RaggedAllToAllMultiHostDecomposerTest,\n+       RaggedAllToAll_8GPUs_SliceSize4_ShuffledReplicaGroups) {\n+  auto [num_input_rows, num_output_rows] = GetParam();\n+\n+  if (num_input_rows > num_output_rows) {\n+    // TODO(b/445380264): Fix decomposer for combine ragged-all-to-all.\n+    GTEST_SKIP()\n+        << \"The test will currently fail for combine ragged-all-to-all (when \"\n+           \"input is larger than output).\";\n+  }\n+\n+  std::string kModuleReplicatedStr =\n+      absl::Substitute(R\"(\n+  HloModule module\n+\n+  ENTRY entry {\n+    input = f32[$0,5,32] parameter(0)\n+    output = f32[$1,5,32] parameter(1)\n+    input_offsets = s32[32] parameter(2)\n+    send_sizes = s32[32] parameter(3)\n+    output_offsets = s32[32] parameter(4)\n+    recv_sizes = s32[32] parameter(5)\n+    ROOT ra2a = f32[$1,5,32] ragged-all-to-all(input, output,\n+      input_offsets, send_sizes, output_offsets, recv_sizes),\n+      replica_groups={{0,2,4,6,1,3,5,7}}\n+  })\",\n+                       num_input_rows, num_output_rows);\n+\n+  const int64_t kNumReplicas = 8;\n+  const int64_t kNumUpdatesPerReplica = 4;\n+  if (hlo_runner_->device_count() < kNumReplicas) {\n+    GTEST_SKIP() << \"Test requires at least \" << kNumReplicas << \" devices (\"\n+                 << hlo_runner_->device_count() << \" available)\";\n+  }\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(\n+                                           kModuleReplicatedStr, kNumReplicas));\n+\n+  module->mutable_config()\n+      .mutable_debug_options()\n+      .set_xla_gpu_unsupported_override_fast_interconnect_slice_size(4);\n+\n+  Array<int64_t> input_sizes(\n+      {kNumReplicas, kNumReplicas, kNumUpdatesPerReplica});\n+  input_sizes.FillRandomUniform(0, 10);\n+\n+  TF_ASSERT_OK(CreateRandomTestData(module.get(), input_sizes));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      ExecutionResult execution_result,\n+      ExecuteReplicated(std::move(module), GetInputLiteralPtrs()));\n+\n+  const std::vector<Literal>& results = execution_result.results;\n+  ASSERT_EQ(results.size(), kNumReplicas);\n+\n+  for (int i = 0; i < kNumReplicas; ++i) {\n+    EXPECT_TRUE(LiteralTestUtil::Equal(expected_outputs_[i], results[i]));\n+  }\n+}\n+\n TEST_P(RaggedAllToAllMultiHostDecomposerTest, RaggedAllToAll_8GPUs_SliceSize4) {\n   auto [num_input_rows, num_output_rows] = GetParam();\n "
        }
    ],
    "stats": {
        "total": 222,
        "additions": 213,
        "deletions": 9
    }
}