{
    "author": "derdrdirk",
    "message": "[Autotuner] Find dot instructions in nested computations for fusion autotuning.\n\nThe fission autotuner previously only searched for dot instructions in the entry computation of an HLO module. This caused it to miss dot operations located in nested computations, such as the body of a while loop, preventing the autotuner from applying configurations to them.\n\nPiperOrigin-RevId: 822037141",
    "sha": "1aff85868d3f37c24aca2cc050c8d6627ea27ea6",
    "files": [
        {
            "sha": "6cd9fd685fc7dfe976a6d9b0cc345c32075843cb",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 13,
            "changes": 32,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1aff85868d3f37c24aca2cc050c8d6627ea27ea6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1aff85868d3f37c24aca2cc050c8d6627ea27ea6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission.cc?ref=1aff85868d3f37c24aca2cc050c8d6627ea27ea6",
            "patch": "@@ -85,21 +85,27 @@ absl::Status FissionToCublas(HloModule* hlo_module,\n       .mutable_debug_options()\n       .set_xla_gpu_enable_cublaslt(rewrite_to_cublaslt);\n \n-  HloInstruction* dot = hlo_query::GetFirstInstructionWithOpcode(\n-      *hlo_module->entry_computation(), HloOpcode::kDot);\n-\n-  if (dot == nullptr) {\n-    return absl::InvalidArgumentError(\n-        \"No dot instruction found in the fusion.\");\n+  HloInstruction* dot = nullptr;\n+  bool has_dot = false;\n+  for (HloComputation* computation : hlo_module->computations()) {\n+    dot =\n+        hlo_query::GetFirstInstructionWithOpcode(*computation, HloOpcode::kDot);\n+    if (dot != nullptr) {\n+      // Substitute algorithms, which are not supported by cuBLAS for the check,\n+      // but don't use cuBlas in the end. This assumes that the substituting\n+      // algorithm has result which are close enough for the check in this file.\n+      if (dot->precision_config().algorithm() ==\n+          PrecisionConfig::ALG_DOT_TF32_TF32_F32_X3) {\n+        dot->mutable_precision_config()->set_algorithm(\n+            PrecisionConfig::ALG_DOT_F32_F32_F32);\n+      }\n+      has_dot = true;\n+    }\n   }\n \n-  // Substitute algorithms, which are not supported by cuBLAS for the check, but\n-  // don't use cuBlas in the end. This assumes that the substituting algorithm\n-  // has result which are close enough for the check in this file.\n-  if (dot->precision_config().algorithm() ==\n-      PrecisionConfig::ALG_DOT_TF32_TF32_F32_X3) {\n-    dot->mutable_precision_config()->set_algorithm(\n-        PrecisionConfig::ALG_DOT_F32_F32_F32);\n+  if (!has_dot) {\n+    return absl::InvalidArgumentError(\n+        \"Fission to cuBLAS failed because no dot instruction found.\");\n   }\n \n   bool is_rewritten_to_cublas_custom_call = false;"
        },
        {
            "sha": "ed8307dfb14b7d90caa77acccc08105b02858eb7",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_test.cc",
            "status": "modified",
            "additions": 74,
            "deletions": 0,
            "changes": 74,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1aff85868d3f37c24aca2cc050c8d6627ea27ea6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1aff85868d3f37c24aca2cc050c8d6627ea27ea6/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_test.cc?ref=1aff85868d3f37c24aca2cc050c8d6627ea27ea6",
            "patch": "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"xla/autotuning.pb.h\"\n #include \"xla/backends/autotuner/codegen_backend.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/testlib/filecheck.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/service/compiler.h\"\n@@ -185,6 +186,79 @@ TEST_F(FissionBackendTest, ApplyCustomKernelConfigToFusionInstruction) {\n               IsOkAndHolds(true));\n }\n \n+TEST_F(FissionBackendTest, ApplyCublasConfigToFusionInWhileBody) {\n+  const char kWhileHlo[] = R\"(\n+HloModule module\n+\n+fusion_computation {\n+  fp0 = bf16[1024,1024]{1,0} parameter(0)\n+  convert0 = f32[1024,1024]{1,0} convert(fp0)\n+  fp1 = s8[1024,1024]{1,0} parameter(1)\n+  convert1 = f32[1024,1024]{1,0} convert(fp1)\n+  ROOT dot = f32[1024,1024]{1,0} dot(convert0, convert1),\n+      lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+}\n+\n+while_cond {\n+  cond_param = (s32[], f32[1024,1024]{1,0}, bf16[1024,1024]{1,0}, s8[1024,1024]{1,0}) parameter(0)\n+  count = s32[] get-tuple-element(cond_param), index=0\n+  limit = s32[] constant(1)\n+  ROOT result = pred[] compare(count, limit), direction=LT\n+}\n+\n+while_body {\n+  body_param = (s32[], f32[1024,1024]{1,0}, bf16[1024,1024]{1,0}, s8[1024,1024]{1,0}) parameter(0)\n+  count = s32[] get-tuple-element(body_param), index=0\n+  p0_body = bf16[1024,1024]{1,0} get-tuple-element(body_param), index=2\n+  p1_body = s8[1024,1024]{1,0} get-tuple-element(body_param), index=3\n+  fusion = f32[1024,1024]{1,0} fusion(p0_body, p1_body),\n+    kind=kCustom, calls=fusion_computation,\n+    backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\"}}\n+  one = s32[] constant(1)\n+  new_count = s32[] add(count, one)\n+  ROOT result = (s32[], f32[1024,1024]{1,0}, bf16[1024,1024]{1,0}, s8[1024,1024]{1,0}) tuple(new_count, fusion, p0_body, p1_body)\n+}\n+\n+ENTRY main {\n+  p0 = bf16[1024,1024]{1,0} parameter(0)\n+  p1 = s8[1024,1024]{1,0} parameter(1)\n+  c0 = s32[] constant(0)\n+  init_f32 = f32[1024,1024]{1,0} broadcast(f32[] constant(0.0)), dimensions={}\n+  while_init = (s32[], f32[1024,1024]{1,0}, bf16[1024,1024]{1,0}, s8[1024,1024]{1,0}) tuple(c0, init_f32, p0, p1)\n+  while_result = (s32[], f32[1024,1024]{1,0}, bf16[1024,1024]{1,0}, s8[1024,1024]{1,0}) while(while_init),\n+    body=while_body, condition=while_cond\n+  ROOT result = f32[1024,1024]{1,0} get-tuple-element(while_result), index=1\n+}\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> hlo_module,\n+                          ParseAndReturnVerifiedModule(kWhileHlo));\n+  hlo_module->mutable_config()\n+      .mutable_debug_options()\n+      .set_xla_gpu_enable_cublaslt(false);\n+\n+  HloInstruction* while_instr =\n+      hlo_module->entry_computation()->root_instruction()->mutable_operand(0);\n+  ASSERT_EQ(while_instr->opcode(), HloOpcode::kWhile);\n+  HloComputation* body_computation = while_instr->while_body();\n+  HloInstruction* fusion_instr =\n+      body_computation->root_instruction()->mutable_operand(1);\n+  ASSERT_EQ(fusion_instr->opcode(), HloOpcode::kFusion);\n+\n+  AutotuneResult::GemmKey config;\n+  config.set_algorithm(3);\n+  google::protobuf::Any any;\n+  any.PackFrom(config);\n+  TF_EXPECT_OK(backend_.ApplyConfig(*fusion_instr, any));\n+  EXPECT_THAT(\n+      RunFileCheck(\n+          hlo_module->ToString(),\n+          \"CHECK: while_body\"\n+          \"\\nCHECK: custom-call({{.*}}), custom_call_target=\\\"__cublas$gemm\\\"\"\n+          \"\\nCHECK: \\\"selected_algorithm\\\":\\\"3\\\"\"),\n+      IsOkAndHolds(true));\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 106,
        "additions": 93,
        "deletions": 13
    }
}