{
    "author": "GleasonK",
    "message": "Add broadcasting APIs with support for bounded dynamism\n\nPiperOrigin-RevId: 829534337",
    "sha": "e095ceec54c6e06c8134eb1254f536964d3e83cf",
    "files": [
        {
            "sha": "9a63b163f7fbd05714df76d7217b1ce4307e6594",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 805,
            "deletions": 0,
            "changes": 805,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e095ceec54c6e06c8134eb1254f536964d3e83cf/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e095ceec54c6e06c8134eb1254f536964d3e83cf/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=e095ceec54c6e06c8134eb1254f536964d3e83cf",
            "patch": "@@ -1,3 +1,31 @@\n+diff --ruN a/stablehlo/BUILD.bazel b/stablehlo/BUILD.bazel\n+--- stablehlo/BUILD.bazel\n++++ stablehlo/BUILD.bazel\n+@@ -1105,6 +1105,24 @@\n+     tblgen = \"@llvm-project//mlir:mlir-tblgen\",\n+     td_file = \"stablehlo/transforms/Passes.td\",\n+     deps = [\"@llvm-project//mlir:PassBaseTdFiles\"],\n++)\n++\n++cc_library(\n++    name = \"stablehlo_broadcast_lowering\",\n++    srcs = [\n++        \"stablehlo/transforms/StablehloBroadcastLowering.cpp\",\n++    ],\n++    hdrs = [\n++        \"stablehlo/transforms/StablehloBroadcastLowering.h\",\n++    ],\n++    strip_include_prefix = \".\",\n++    deps = [\n++        \":stablehlo_ops\",\n++        \"@llvm-project//llvm:Support\",\n++        \"@llvm-project//mlir:IR\",\n++        \"@llvm-project//mlir:ShapeDialect\",\n++        \"@llvm-project//mlir:Support\",\n++    ],\n+ )\n+ \n+ cc_library(\n diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/dialect/StablehloOps.cpp\n --- stablehlo/stablehlo/dialect/StablehloOps.cpp\n +++ stablehlo/stablehlo/dialect/StablehloOps.cpp\n@@ -73,4 +101,781 @@ diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.c\n  #include \"mlir/IR/BuiltinAttributes.h\"\n  #include \"mlir/IR/BuiltinOps.h\"\n  #include \"mlir/IR/DialectRegistry.h\"\n+diff --ruN a/stablehlo/stablehlo/tests/BUILD.bazel b/stablehlo/stablehlo/tests/BUILD.bazel\n+--- stablehlo/stablehlo/tests/BUILD.bazel\n++++ stablehlo/stablehlo/tests/BUILD.bazel\n+@@ -102,6 +102,8 @@\n+     deps = [\n+         \":test_utils_inc_gen\",\n+         \"//:stablehlo_assembly_format\",\n++        \"//:stablehlo_broadcast_lowering\",\n++        \"//:stablehlo_ops\",\n+         \"@llvm-project//llvm:Support\",\n+         \"@llvm-project//mlir:FuncDialect\",\n+         \"@llvm-project//mlir:IR\",\n+diff --ruN a/stablehlo/stablehlo/tests/TestUtils.cpp b/stablehlo/stablehlo/tests/TestUtils.cpp\n+--- stablehlo/stablehlo/tests/TestUtils.cpp\n++++ stablehlo/stablehlo/tests/TestUtils.cpp\n+@@ -19,6 +19,7 @@\n+ #include <utility>\n+ \n+ #include \"llvm/ADT/STLExtras.h\"\n++#include \"llvm/ADT/SmallVector.h\"\n+ #include \"llvm/Support/Casting.h\"\n+ #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+ #include \"mlir/Dialect/Shape/IR/Shape.h\"\n+@@ -35,11 +36,35 @@\n+ #include \"mlir/Support/LLVM.h\"\n+ #include \"mlir/Support/LogicalResult.h\"\n+ #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n++#include \"stablehlo/dialect/StablehloOps.h\"\n++#include \"stablehlo/transforms/StablehloBroadcastLowering.h\"\n++#include \"third_party/llvm/llvm-project/mlir/include/mlir/IR/TypeRange.h\"\n+ \n+ namespace mlir {\n+ namespace hlo {\n+ \n+ namespace {\n++\n++struct BroadcastValuesPattern : public RewritePattern {\n++  explicit BroadcastValuesPattern(MLIRContext* context)\n++      : RewritePattern(\"hlo_test_broadcast.numpy_broadcast\", 1, context) {}\n++  LogicalResult matchAndRewrite(Operation* op,\n++                                PatternRewriter& rewriter) const override {\n++    // Process all operands\n++    SmallVector<Value> operands = llvm::to_vector(op->getOperands());\n++    auto broadcastedOperands =\n++        stablehlo::numpyBroadcastIfNeeded(rewriter, operands);\n++    if (failed(broadcastedOperands)) return failure();\n++\n++    // Replace with custom call to avoid pattern reapplication\n++    auto customCall = stablehlo::CustomCallOp::create(\n++        rewriter, op->getLoc(), op->getResultTypes(), *broadcastedOperands);\n++    customCall.setCallTargetName(\"numpy_broadcasted\");\n++    customCall.setHasSideEffect(true);\n++    rewriter.replaceOp(op, customCall);\n++    return success();\n++  }\n++};\n+ \n+ struct InferReturnTypesPattern : public RewritePattern {\n+   explicit InferReturnTypesPattern(MLIRContext *context)\n+@@ -137,36 +162,55 @@\n+   }\n+ };\n+ \n++#define GEN_PASS_DEF_HLOTESTBROADCASTPASS\n+ #define GEN_PASS_DEF_HLOTESTINFERPASS\n+ #define GEN_PASS_DEF_HLOTESTSPECULATABILITYPASS\n+ #include \"stablehlo/tests/TestUtils.h.inc\"\n+ \n++struct HloTestBroadcastPass\n++    : public impl::HloTestBroadcastPassBase<HloTestBroadcastPass> {\n++  LogicalResult initialize(MLIRContext* context) override {\n++    RewritePatternSet patterns(context);\n++    patterns.add<BroadcastValuesPattern>(context);\n++    patterns_ = std::move(patterns);\n++    return success();\n++  }\n++\n++  void runOnOperation() override {\n++    if (failed(applyPatternsGreedily(getOperation(), std::move(patterns_))))\n++      return signalPassFailure();\n++  }\n++\n++ private:\n++  FrozenRewritePatternSet patterns_;\n++};\n++\n+ struct HloTestInferPass : public impl::HloTestInferPassBase<HloTestInferPass> {\n+   LogicalResult initialize(MLIRContext *context) override {\n+-    RewritePatternSet patterns_(context);\n+-    patterns_.add<InferReturnTypesPattern>(context);\n+-    patterns_.add<ReifyReturnTypeShapesPattern>(context);\n+-    patterns = std::move(patterns_);\n++    RewritePatternSet patterns(context);\n++    patterns.add<InferReturnTypesPattern>(context);\n++    patterns.add<ReifyReturnTypeShapesPattern>(context);\n++    patterns_ = std::move(patterns);\n+     return success();\n+   }\n+ \n+   void runOnOperation() override {\n+-    if (failed(applyPatternsGreedily(getOperation(), std::move(patterns))))\n++    if (failed(applyPatternsGreedily(getOperation(), std::move(patterns_))))\n+       return signalPassFailure();\n+   }\n+ \n+  private:\n+-  FrozenRewritePatternSet patterns;\n++  FrozenRewritePatternSet patterns_;\n+ };\n+ \n+ struct HloTestSpeculatabilityPass\n+     : public impl::HloTestSpeculatabilityPassBase<HloTestSpeculatabilityPass> {\n+   LogicalResult initialize(MLIRContext *context) override {\n+-    RewritePatternSet patterns_(context);\n+-    patterns_.add<IsSpeculatablePattern>(context);\n+-    patterns_.add<IsNotSpeculatablePattern>(context);\n+-    patterns_.add<IsRecursivelySpeculatablePattern>(context);\n+-    patterns = std::move(patterns_);\n++    RewritePatternSet patterns(context);\n++    patterns.add<IsSpeculatablePattern>(context);\n++    patterns.add<IsNotSpeculatablePattern>(context);\n++    patterns.add<IsRecursivelySpeculatablePattern>(context);\n++    patterns_ = std::move(patterns);\n+     return success();\n+   }\n+ \n+@@ -175,11 +219,11 @@\n+     config.setMaxIterations(1)\n+         .setUseTopDownTraversal(true)\n+         .setRegionSimplificationLevel(GreedySimplifyRegionLevel::Disabled);\n+-    (void)applyPatternsGreedily(getOperation(), std::move(patterns));\n++    (void)applyPatternsGreedily(getOperation(), std::move(patterns_));\n+   }\n+ \n+  private:\n+-  FrozenRewritePatternSet patterns;\n++  FrozenRewritePatternSet patterns_;\n+ };\n+ \n+ #define GEN_PASS_REGISTRATION\n+diff --ruN a/stablehlo/stablehlo/tests/TestUtils.td b/stablehlo/stablehlo/tests/TestUtils.td\n+--- stablehlo/stablehlo/tests/TestUtils.td\n++++ stablehlo/stablehlo/tests/TestUtils.td\n+@@ -16,6 +16,11 @@\n+ \n+ include \"mlir/Pass/PassBase.td\"\n+ \n++def HloTestBroadcastPass : Pass<\"hlo-test-broadcast\", \"func::FuncOp\"> {\n++  let summary = \"Uses test ops to invoke BroadcastUtils methods.\";\n++  let dependentDialects = [\"stablehlo::StablehloDialect\"];\n++}\n++\n+ def HloTestInferPass : Pass<\"hlo-test-infer\", \"func::FuncOp\"> {\n+   let summary = \"Uses test ops to invoke InferShapedTypeOpInterface methods.\";\n+   let dependentDialects = [\"shape::ShapeDialect\"];\n+diff --ruN a/stablehlo/stablehlo/tests/ops_broadcasting.mlir b/stablehlo/stablehlo/tests/ops_broadcasting.mlir\n+--- stablehlo/stablehlo/tests/ops_broadcasting.mlir\n++++ stablehlo/stablehlo/tests/ops_broadcasting.mlir\n+@@ -0,0 +1,249 @@\n++// RUN: stablehlo-opt %s --hlo-test-broadcast --split-input-file --allow-unregistered-dialect | FileCheck %s\n++\n++/////////\n++// Scalar broadcast tests.\n++\n++// [] x [1] => [1]\n++// CHECK-LABEL: func @scalar_broadcast_scalar_x_1\n++func.func @scalar_broadcast_scalar_x_1(%arg0: tensor<f64>, %arg1: tensor<1xf64>) -> !stablehlo.token {\n++  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f64>) -> tensor<1xf64>\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST]], %arg1)\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<f64>, tensor<1xf64>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// -----\n++\n++// [1] x [] => [1]\n++// CHECK-LABEL: func @scalar_broadcast_1_x_scalar\n++func.func @scalar_broadcast_1_x_scalar(%arg0: tensor<1xf64>, %arg1: tensor<f64>) -> !stablehlo.token {\n++  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f64>) -> tensor<1xf64>\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %[[RHS_BCAST]])\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<1xf64>, tensor<f64>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// -----\n++\n++// [] x [10] => [10]\n++// CHECK-LABEL: func @scalar_broadcast_scalar_x_10\n++func.func @scalar_broadcast_scalar_x_10(%arg0: tensor<f64>, %arg1: tensor<10xf64>) -> !stablehlo.token {\n++  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f64>) -> tensor<10xf64>\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST]], %arg1)\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<f64>, tensor<10xf64>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// -----\n++\n++// [<=10] x [] => [<=10]\n++// CHECK-LABEL: func @scalar_broadcast_b10_x_scalar\n++func.func @scalar_broadcast_b10_x_scalar(%arg0: tensor<?xf64, #stablehlo.bounds<10>>, %arg1: tensor<f64>) -> !stablehlo.token {\n++  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f64>) -> tensor<10xf64>\n++  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 0\n++  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST]], %[[DIM_SIZE]], dim = 0\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %[[RHS_BCAST_DYN]])\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<?xf64, #stablehlo.bounds<10>>, tensor<f64>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// -----\n++\n++// [] x [<=10] => [<=10]\n++// CHECK-LABEL: func @scalar_broadcast_scalar_x_b10\n++func.func @scalar_broadcast_scalar_x_b10(%arg0: tensor<f64>, %arg1: tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token {\n++  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f64>) -> tensor<10xf64>\n++  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg1, dim = 0\n++  // CHECK: %[[LHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[LHS_BCAST]], %[[DIM_SIZE]], dim = 0\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST_DYN]], %arg1)\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<f64>, tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// -----\n++\n++// [] x [1, <=10, 1] => [1, <=10, 1]\n++// CHECK-LABEL: func @scalar_broadcast_scalar_x_1_b10_1\n++func.func @scalar_broadcast_scalar_x_1_b10_1(%arg0: tensor<f64>, %arg1: tensor<1x?x1xf64, #stablehlo.bounds<?, 10, ?>>) -> !stablehlo.token {\n++  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f64>) -> tensor<1x10x1xf64>\n++  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg1, dim = 1\n++  // CHECK: %[[LHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[LHS_BCAST]], %[[DIM_SIZE]], dim = 1\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST_DYN]], %arg1)\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<f64>, tensor<1x?x1xf64, #stablehlo.bounds<?, 10, ?>>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// [10, 1, <=5] x [] => [10, 1, <=5]\n++// CHECK-LABEL: func @scalar_broadcast_10_1_b5_x_scalar\n++func.func @scalar_broadcast_10_1_b5_x_scalar(%arg0: tensor<10x1x?xf64, #stablehlo.bounds<?, ?, 5>>, %arg1: tensor<f64>) -> !stablehlo.token {\n++  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f64>) -> tensor<10x1x5xf64>\n++  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 2\n++  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST]], %[[DIM_SIZE]], dim = 2\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %[[RHS_BCAST_DYN]])\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<10x1x?xf64, #stablehlo.bounds<?, ?, 5>>, tensor<f64>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++//////\n++// 1-D SCALAR TESTS\n++\n++// [1] x [1] => [1]\n++// [1] x [10] => [1]\n++// [<=10] x [1] => [<=10]\n++// [1] x [<=10] => [<=10]\n++// [1] x [1, <=10, 1] => [1, <=10, 1]\n++\n++\n++// [1] x [1] => [1]\n++// CHECK-LABEL: func @single_dim_scalar_1_x_1\n++func.func @single_dim_scalar_1_x_1(%arg0: tensor<1xf64>, %arg1: tensor<1xf64>) -> !stablehlo.token {\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %arg1)\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<1xf64>, tensor<1xf64>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// -----\n++\n++// [1] x [10] => [10]\n++// CHECK-LABEL: func @single_dim_scalar_1_x_10\n++func.func @single_dim_scalar_1_x_10(%arg0: tensor<1xf64>, %arg1: tensor<10xf64>) -> !stablehlo.token {\n++  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0] : (tensor<1xf64>) -> tensor<10xf64>\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST]], %arg1)\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<1xf64>, tensor<10xf64>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// -----\n++\n++// [<=10] x [1] => [<=10]\n++// CHECK-LABEL: func @single_dim_scalar_b10_x_1\n++func.func @single_dim_scalar_b10_x_1(%arg0: tensor<?xf64, #stablehlo.bounds<10>>, %arg1: tensor<1xf64>) -> !stablehlo.token {\n++  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<1xf64>) -> tensor<10xf64>\n++  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 0\n++  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST]], %[[DIM_SIZE]], dim = 0\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %[[RHS_BCAST_DYN]])\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<?xf64, #stablehlo.bounds<10>>, tensor<1xf64>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// -----\n++\n++// [1] x [<=10] => [<=10]\n++// CHECK-LABEL: func @single_dim_scalar_1_x_b10\n++func.func @single_dim_scalar_1_x_b10(%arg0: tensor<1xf64>, %arg1: tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token {\n++  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0] : (tensor<1xf64>) -> tensor<10xf64>\n++  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg1, dim = 0\n++  // CHECK: %[[LHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[LHS_BCAST]], %[[DIM_SIZE]], dim = 0\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST_DYN]], %arg1)\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<1xf64>, tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// [<=10] x [<=10] => [<=10] // PT layer must ensure these are identical!\n++// CHECK-LABEL: func @single_dim_scalar_b10_x_b10\n++func.func @single_dim_scalar_b10_x_b10(%arg0: tensor<?xf64, #stablehlo.bounds<10>>, %arg1: tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token {\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %arg1)\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<?xf64, #stablehlo.bounds<10>>, tensor<?xf64, #stablehlo.bounds<10>>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// -----\n++\n++// [1] x [1, <=10, 1] => [1, <=10, 1]\n++// CHECK-LABEL: func @single_dim_scalar_1_x_1_b10_1\n++func.func @single_dim_scalar_1_x_1_b10_1(%arg0: tensor<1xf64>, %arg1: tensor<1x?x1xf64, #stablehlo.bounds<?, 10, ?>>) -> !stablehlo.token {\n++  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [2] : (tensor<1xf64>) -> tensor<1x10x1xf64>\n++  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg1, dim = 1\n++  // CHECK: %[[LHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[LHS_BCAST]], %[[DIM_SIZE]], dim = 1\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST_DYN]], %arg1)\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<1xf64>, tensor<1x?x1xf64, #stablehlo.bounds<?, 10, ?>>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// -----\n++\n++// [10, 1, <=5] x [1] => [10, 1, <=5]\n++// CHECK-LABEL: func @single_dim_scalar_10_1_b5_x_1\n++func.func @single_dim_scalar_10_1_b5_x_1(%arg0: tensor<10x1x?xf64, #stablehlo.bounds<?, ?, 5>>, %arg1: tensor<1xf64>) -> !stablehlo.token {\n++  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [2] : (tensor<1xf64>) -> tensor<10x1x5xf64>\n++  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 2\n++  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST]], %[[DIM_SIZE]], dim = 2\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %[[RHS_BCAST_DYN]])\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<10x1x?xf64, #stablehlo.bounds<?, ?, 5>>, tensor<1xf64>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++\n++//////\n++// N-D Tests\n++\n++// [1, 2] x [1, 2] => [1, 2]\n++// CHECK-LABEL: func @tensor_no_broadcast_match\n++func.func @tensor_no_broadcast_match(%arg0: tensor<1x2xf64>, %arg1: tensor<1x2xf64>) -> !stablehlo.token {\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %arg1)\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<1x2xf64>, tensor<1x2xf64>) ->  !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// [10, 1] x [1, 1] => [10, 1]\n++// CHECK-LABEL: func @tensor_broadcast_10_1_x_1_1\n++func.func @tensor_broadcast_10_1_x_1_1(%arg0: tensor<10x1xf64>, %arg1: tensor<1x1xf64>) -> !stablehlo.token {\n++  // CHECK: %[[RHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<1x1xf64>) -> tensor<10x1xf64>\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%arg0, %[[RHS_BCAST]])\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<10x1xf64>, tensor<1x1xf64>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// -----\n++\n++// [<=10, 1] x [1, 10] => [<=10, 10]\n++// CHECK-LABEL: func @tensor_broadcast_b10_1_x_1_10\n++func.func @tensor_broadcast_b10_1_x_1_10(%arg0: tensor<?x1xf64, #stablehlo.bounds<10, ?>>, %arg1: tensor<1x10xf64>) -> !stablehlo.token {\n++  // CHECK: %[[LHS_BCAST:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0, 1] : (tensor<?x1xf64, #stablehlo.bounds<10, ?>>) -> tensor<?x10xf64, #stablehlo.bounds<10, ?>>\n++  // CHECK: %[[RHS_BCAST_STATIC:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<1x10xf64>) -> tensor<10x10xf64>\n++  // CHECK: %[[DIM_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 0\n++  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST_STATIC]], %[[DIM_SIZE]], dim = 0\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST]], %[[RHS_BCAST_DYN]])\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (tensor<?x1xf64, #stablehlo.bounds<10, ?>>, tensor<1x10xf64>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// -----\n++\n++// [<=10, 1] x [1, <=10] => [<=10, <=10]\n++// CHECK-LABEL: func @tensor_broadcast_b10_1_x_1_b10\n++func.func @tensor_broadcast_b10_1_x_1_b10(\n++  %arg0: tensor<?x1xf64, #stablehlo.bounds<10, ?>>,\n++  %arg1: tensor<1x?xf64, #stablehlo.bounds<?, 10>>\n++) -> !stablehlo.token {\n++  // CHECK: %[[LHS_BCAST_STATIC:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0, 1] : (tensor<?x1xf64, #stablehlo.bounds<10, ?>>) -> tensor<?x10xf64, #stablehlo.bounds<10, ?>>\n++  // CHECK: %[[ARG1_DIM1_SIZE:.+]] = stablehlo.get_dimension_size %arg1, dim = 1\n++  // CHECK: %[[LHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[LHS_BCAST_STATIC]], %[[ARG1_DIM1_SIZE]], dim = 1\n++  // CHECK: %[[RHS_BCAST_STATIC:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<1x?xf64, #stablehlo.bounds<?, 10>>) -> tensor<10x?xf64, #stablehlo.bounds<?, 10>>\n++  // CHECK: %[[ARG0_DIM0_SIZE:.+]] = stablehlo.get_dimension_size %arg0, dim = 0\n++  // CHECK: %[[RHS_BCAST_DYN:.+]] = stablehlo.set_dimension_size %[[RHS_BCAST_STATIC]], %[[ARG0_DIM0_SIZE]], dim = 0\n++  // CHECK-NEXT: stablehlo.custom_call @numpy_broadcasted(%[[LHS_BCAST_DYN]], %[[RHS_BCAST_DYN]])\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1) : (\n++    tensor<?x1xf64, #stablehlo.bounds<10, ?>>,\n++    tensor<1x?xf64, #stablehlo.bounds<?, 10>>\n++  ) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n++// -----\n++\n++//////\n++// N-ary broadcast tests.\n++\n++\n++// [<=10, 1] x [1, <=10] x [1] => [<=10, <=10]\n++// CHECK-LABEL: func @nary_broadcast_b10_1_x_1_b10_x_1\n++func.func @nary_broadcast_b10_1_x_1_b10_x_1(\n++  %arg0: tensor<?x1xf64, #stablehlo.bounds<10, ?>>,\n++  %arg1: tensor<1x?xf64, #stablehlo.bounds<?, 10>>,\n++  %arg2: tensor<1xf64>\n++) -> !stablehlo.token {\n++  %0 = \"hlo_test_broadcast.numpy_broadcast\"(%arg0, %arg1, %arg2) : (tensor<?x1xf64, #stablehlo.bounds<10, ?>>, tensor<1x?xf64, #stablehlo.bounds<?, 10>>, tensor<1xf64>) -> !stablehlo.token\n++  return %0 : !stablehlo.token\n++}\n++\n+diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n+--- stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n++++ stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n+@@ -0,0 +1,293 @@\n++/* Copyright 2025 The StableHLO Authors.\n++\n++Licensed under the Apache License, Version 2.0 (the \"License\");\n++you may not use this file except in compliance with the License.\n++You may obtain a copy of the License at\n++\n++    http://www.apache.org/licenses/LICENSE-2.0\n++\n++Unless required by applicable law or agreed to in writing, software\n++distributed under the License is distributed on an \"AS IS\" BASIS,\n++WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n++See the License for the specific language governing permissions and\n++limitations under the License.\n++==============================================================================*/\n++\n++#include \"stablehlo/transforms/StablehloBroadcastLowering.h\"\n++\n++#include <algorithm>\n++#include <cassert>\n++#include <cstddef>\n++#include <cstdint>\n++#include <string>\n++#include <utility>\n++\n++#include \"llvm/ADT/STLExtras.h\"\n++#include \"llvm/Support/Debug.h\"\n++#include \"llvm/Support/raw_ostream.h\"\n++#include \"mlir/IR/Builders.h\"\n++#include \"mlir/IR/BuiltinTypeInterfaces.h\"\n++#include \"mlir/IR/BuiltinTypes.h\"\n++#include \"mlir/IR/Diagnostics.h\"\n++#include \"mlir/IR/Location.h\"\n++#include \"mlir/IR/Types.h\"\n++#include \"mlir/IR/Value.h\"\n++#include \"mlir/Support/LLVM.h\"\n++#include \"stablehlo/dialect/StablehloOps.h\"\n++#include \"third_party/llvm/llvm-project/llvm/include/llvm/ADT/Sequence.h\"\n++#include \"third_party/llvm/llvm-project/llvm/include/llvm/ADT/SmallVector.h\"\n++\n++#define DEBUG_TYPE \"stablehlo-broadcast-lowering\"\n++\n++namespace mlir {\n++namespace stablehlo {\n++\n++/////\n++// Bounded dynamism broadcasting\n++\n++namespace {\n++\n++DimensionInfo getDimensionInfo(Value op, mlir::RankedTensorType tensorType,\n++                               TypeExtensionsAttr encoding,\n++                               int64_t dim) {\n++  if (!encoding || !mlir::ShapedType::isDynamic(tensorType.getDimSize(dim)))\n++    return DimensionInfo{tensorType.getDimSize(dim)};\n++\n++  return DimensionInfo{\n++      encoding.getBounds()[dim],\n++      op,\n++      dim,\n++  };\n++}\n++\n++FailureOr<Dimensions> getDimensions(Value op) {\n++  // Get tensor type\n++  mlir::RankedTensorType tensor_type = dyn_cast<RankedTensorType>(op.getType());\n++  if (!tensor_type)\n++    return emitError(op.getLoc(), \"expected ranked tensor type\");\n++\n++  auto encoding =\n++      mlir::dyn_cast_if_present<mlir::stablehlo::TypeExtensionsAttr>(\n++          tensor_type.getEncoding());\n++\n++  Dimensions dimensions;\n++  dimensions.reserve(tensor_type.getRank());\n++  for (size_t idx = 0; idx < tensor_type.getRank(); ++idx) {\n++    auto dimInfo = getDimensionInfo(op, tensor_type, encoding, idx);\n++    dimensions.push_back(dimInfo);\n++  }\n++  return dimensions;\n++}\n++\n++FailureOr<Dimensions> getNumpyBroadcastShapeWithBounds(\n++    const Dimensions& a, const Dimensions& b) {\n++  LLVM_DEBUG(llvm::dbgs() << \"[getNumpyBroadcastShapeWithBounds] inputs: \"\n++                          << toString(a) << \" * \" << toString(b));\n++  size_t max_rank = std::max(a.size(), b.size());\n++  Dimensions result(max_rank);\n++\n++  // Iterate from right to left (NumPy-style broadcasting)\n++  for (int i = 1; i <= max_rank; ++i) {\n++    size_t a_idx = a.size() - i;\n++    size_t b_idx = b.size() - i;\n++    size_t res_idx = max_rank - i;\n++\n++    // Get DimensionInfo for the current index, padding with size 1 if out of\n++    // bounds.\n++    DimensionInfo dim_a =\n++        (a_idx >= 0 && a_idx < a.size()) ? a[a_idx] : DimensionInfo{1};\n++    DimensionInfo dim_b =\n++        (b_idx >= 0 && b_idx < b.size()) ? b[b_idx] : DimensionInfo{1};\n++\n++    // Short circuit on size 1 dimensions.\n++    if (dim_a.size == 1) {\n++      result[res_idx] = dim_b;\n++      continue;\n++    }\n++    if (dim_b.size == 1) {\n++      result[res_idx] = dim_a;\n++      continue;\n++    }\n++\n++    // If both LHS and RHS are not 1, dim size must match.\n++    if (dim_a.size != dim_b.size) {\n++      return emitError(a[a_idx].boundOp.value().getLoc(),\n++                       \"incompatible shapes for broadcasting \")\n++             << dim_a.size << \" and \" << dim_b.size;\n++    }\n++\n++    // If bounded both must be bounded\n++    if (dim_a.boundOp.has_value() != dim_b.boundOp.has_value()) {\n++      return emitError(a[a_idx].boundOp.value().getLoc(),\n++                       \"cannot mix bounded and static dimensions in broadcast\");\n++    }\n++\n++    // LHS and RHS match, populate with one of the dimensions.\n++    result[res_idx] = dim_a;\n++  }\n++\n++  LLVM_DEBUG(llvm::dbgs() << \"[getNumpyBroadcastShapeWithBounds] result: \"\n++                          << toString(result));\n++  return result;\n++}\n++\n++mlir::RankedTensorType getRankedTensorType(const Dimensions& dims,\n++                                           mlir::Type element_type) {\n++  mlir::SmallVector<int64_t> shape;\n++  mlir::SmallVector<int64_t> bounds;\n++  shape.reserve(dims.size());\n++  for (const DimensionInfo& dim : dims) {\n++    if (dim.boundOp.has_value()) {\n++      shape.push_back(mlir::ShapedType::kDynamic);\n++      bounds.push_back(dim.size);\n++    } else {\n++      shape.push_back(dim.size);\n++      bounds.push_back(mlir::ShapedType::kDynamic);\n++    }\n++  }\n++  mlir::stablehlo::TypeExtensionsAttr encoding;\n++  if (!llvm::all_of(\n++          bounds, [](int64_t b) { return b == mlir::ShapedType::kDynamic; })) {\n++    encoding = mlir::stablehlo::TypeExtensionsAttr::get(\n++        element_type.getContext(), bounds);\n++  }\n++  return mlir::RankedTensorType::get(shape, element_type, encoding);\n++}\n++\n++}  // namespace\n++\n++\n++FailureOr<Dimensions> getNumpyBroadcastShape(ArrayRef<Value> ops) {\n++  if (ops.empty()) return failure();\n++\n++  Value first = ops[0];\n++  auto bcastShapeOrFail = getDimensions(first);\n++  if (failed(bcastShapeOrFail)) return failure();\n++  Dimensions bcastShape = std::move(*bcastShapeOrFail);\n++\n++  for (int i = 1; i < ops.size(); ++i) {\n++    Value currOp = ops[i];\n++    auto dims = getDimensions(currOp);\n++    if (failed(dims)) return failure();\n++    auto currBcastShapeOrFail =\n++        getNumpyBroadcastShapeWithBounds(bcastShape, *dims);\n++    if (failed(currBcastShapeOrFail)) return failure();\n++    bcastShape = std::move(*currBcastShapeOrFail);\n++  }\n++  return std::move(bcastShape);\n++}\n++\n++std::string toString(const Dimensions& dims) {\n++  std::string result;\n++  llvm::raw_string_ostream os(result);\n++  os << \"tensor<\";\n++  llvm::interleave(\n++      dims, os,\n++      [&](const DimensionInfo& dim) {\n++        os << (dim.boundOp.has_value() ? \"b\" : \"\") << dim.size;\n++      },\n++      \"x\");\n++  os << \">\";\n++  return result;\n++}\n++\n++FailureOr<SmallVector<Value>> numpyBroadcastIfNeeded(OpBuilder& builder,\n++                                                     ArrayRef<Value> operands) {\n++  // Figure out the broadcast shape\n++  auto bcastShapeOrFail = getNumpyBroadcastShape(operands);\n++  if (failed(bcastShapeOrFail)) return failure();\n++  Dimensions bcastShape = std::move(*bcastShapeOrFail);\n++\n++  // Apply to all operands\n++  SmallVector<Value> broadcastedOperands;\n++  for (auto operand : operands) {\n++    auto bcastOperand = numpyBroadcastIfNeeded(builder, operand, bcastShape);\n++    if (failed(bcastOperand)) return failure();\n++    broadcastedOperands.push_back(*bcastOperand);\n++  }\n++  return std::move(broadcastedOperands);\n++}\n++\n++FailureOr<Value> numpyBroadcastIfNeeded(OpBuilder& builder, Value input,\n++                                        const Dimensions& shape) {\n++  LLVM_DEBUG(llvm::dbgs() << \"[BroadcastIfNeeded] input: \" << input\n++                          << \" shape: \" << toString(shape));\n++  auto loc = input.getLoc();\n++  mlir::RankedTensorType input_type =\n++      dyn_cast<RankedTensorType>(input.getType());\n++  if (!input_type) return emitError(input.getLoc(), \"expected tensor type\");\n++  mlir::RankedTensorType output_type =\n++      getRankedTensorType(shape, input_type.getElementType());\n++\n++  // Short circuit if no broadcasting is needed.\n++  if (input_type == output_type) return input;\n++\n++  int64_t input_rank = input_type.getRank();\n++  int64_t output_rank = output_type.getRank();\n++  if (input_rank > output_rank)\n++    return emitError(loc, \"input rank must be <= output rank, got \")\n++           << input_rank << \" vs \" << output_rank;\n++\n++  size_t rank_diff = output_rank - input_rank;\n++  SmallVector<int64_t> bcast_dims;\n++  bcast_dims.reserve(input_rank);\n++\n++  auto inputShapeOrFail = getDimensions(input);\n++  if (failed(inputShapeOrFail)) return failure();\n++  Dimensions inputShape = std::move(*inputShapeOrFail);\n++\n++  // Construct broadcast dimensions.\n++  auto broadcastDimensions = llvm::to_vector(\n++      llvm::seq<int64_t>(output_rank - input_rank, output_rank));\n++\n++  // Construct the result type of the broadcast\n++  //  - If input is static and target shape is static, use static shape.\n++  //  - If input has bounded dim, target shape must be bounded, use bounded dim.\n++  //  - If input is not bounded, but target shape is bounded, broadcast to\n++  //    the padded shape then call SetDimensionSize to make dynamic.\n++  auto bcastShape = shape;\n++  for (size_t i = 0; i < input_rank; ++i) {\n++    int64_t input_dim_size = inputShape[i].size;\n++    int64_t result_idx = i + rank_diff;\n++    int64_t result_dim_size = shape[result_idx].size;\n++    if (input_dim_size != 1 && input_dim_size != result_dim_size)\n++      return emitError(loc, \"Cannot broadcast input: \")\n++             << input_type << \" to target shape \" << toString(shape);\n++\n++    if (!inputShape[i].boundOp.has_value() &&\n++        shape[result_idx].boundOp.has_value()) {\n++      // Use padded shape in broadcast.\n++      bcastShape[result_idx] = DimensionInfo{shape[result_idx].size};\n++    }\n++    bcast_dims.push_back(result_idx);\n++  }\n++\n++  // Broadcast to padded size for remaining dimensions.\n++  for (size_t i = input_rank; i < shape.size(); ++i) {\n++    bcastShape[i] = DimensionInfo{shape[i].size};\n++  }\n++\n++  // Insert broadcast ops\n++  mlir::RankedTensorType bcast_type =\n++      getRankedTensorType(bcastShape, input_type.getElementType());\n++  Value bcast_op = stablehlo::BroadcastInDimOp::create(\n++      builder, loc, bcast_type, input, broadcastDimensions);\n++  if (bcast_op.getType() == output_type) return bcast_op;\n++\n++  // Mark the padded broadcast as dynamic where the result is bounded.\n++  // Inserts `GetDimSize(boundOp)->SetDimSize(inputBcast)` for any bounded\n++  // dimensions that required broadcasting.\n++  for (size_t i = 0; i < shape.size(); ++i) {\n++    if (!bcastShape[i].boundOp.has_value() && shape[i].boundOp.has_value()) {\n++      Value boundOp = shape[i].boundOp.value();\n++      auto dim_size = stablehlo::GetDimensionSizeOp::create(\n++          builder, loc, boundOp, shape[i].boundOpDim);\n++      bcast_op = stablehlo::SetDimensionSizeOp::create(builder, loc, bcast_op,\n++                                                       dim_size, i);\n++    }\n++  }\n++  return bcast_op;\n++}\n++\n++}  // namespace stablehlo\n++}  // namespace mlir\n+diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h b/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n+--- stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n++++ stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h\n+@@ -0,0 +1,68 @@\n++/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n++   Copyright 2022 The StableHLO Authors.\n++\n++Licensed under the Apache License, Version 2.0 (the \"License\");\n++you may not use this file except in compliance with the License.\n++You may obtain a copy of the License at\n++\n++    http://www.apache.org/licenses/LICENSE-2.0\n++\n++Unless required by applicable law or agreed to in writing, software\n++distributed under the License is distributed on an \"AS IS\" BASIS,\n++WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n++See the License for the specific language governing permissions and\n++limitations under the License.\n++==============================================================================*/\n++\n++\n++#ifndef STABLEHLO_TRANSFORMS_OPBROADCASTUTILS_H_\n++#define STABLEHLO_TRANSFORMS_OPBROADCASTUTILS_H_\n++\n++#include <cstdint>\n++#include <optional>\n++#include <string>\n++\n++#include \"mlir/IR/Builders.h\"\n++#include \"mlir/IR/Value.h\"\n++#include \"mlir/Support/LLVM.h\"\n++\n++namespace mlir {\n++namespace stablehlo {\n++\n++///////\n++// Numpy broadcasting with support for bounded dynamism.\n++\n++// Struct that represents a dim size of a tensor and possible dynamic value to\n++// match. If dimension is not dynamic, bound_op is set to std::nullopt. If\n++// dimension is bounded, the resulting dimension should be padded to `size` then\n++// marked dynamic using:\n++//   runtime_size = get_dimension_size(bound_op, dim=bound_op_dim)\n++//   T = set_dimension_size(T, dim=bound_op_dim, runtime_size)\n++//\n++struct DimensionInfo {\n++  int64_t size;\n++  std::optional<Value> boundOp = std::nullopt;\n++  int64_t boundOpDim = -1;\n++};\n++\n++using Dimensions = SmallVector<DimensionInfo>;\n++std::string toString(const Dimensions& dims);\n++\n++// Returns the common shape these ops would broadcast to, or an error if the\n++// ops are not broadcastable.\n++FailureOr<Dimensions> getNumpyBroadcastShape(ArrayRef<Value> ops);\n++\n++// Apply numpy broadcasting to the given operands, returning an error if any\n++// operands are not broadcastable.\n++FailureOr<SmallVector<Value>> numpyBroadcastIfNeeded(OpBuilder& builder,\n++                                                     ArrayRef<Value> operands);\n++\n++// Apply numpy broadcasting to the given operand, returning an error if the\n++// operand is not broadcastable.\n++FailureOr<Value> numpyBroadcastIfNeeded(OpBuilder& builder, Value input,\n++                                        const Dimensions& shape);\n++\n++}  // namespace stablehlo\n++}  // namespace mlir\n++\n++#endif  // STABLEHLO_TRANSFORMS_OPBROADCASTUTILS_H_\n "
        }
    ],
    "stats": {
        "total": 805,
        "additions": 805,
        "deletions": 0
    }
}