{
    "author": "KanishAnand",
    "message": "This is a mechanical change to replace `tile_assignment().dimensions()` by `dimensions()`.\nThis is 4/N cl's to privatize `tile_assignment()` method.\n\nPiperOrigin-RevId: 840296625",
    "sha": "9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
    "files": [
        {
            "sha": "f93b62d0b4a19802f5777d8f4b2c6a41d623249c",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding.cc?ref=9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
            "patch": "@@ -1090,8 +1090,7 @@ void EnumerateAll1DPartitionReshape(const HloInstruction* ins,\n       }\n \n       if (cluster_env.IsDeviceMesh1D() &&\n-          VectorGreaterThanOneElementCount(\n-              input_spec->tile_assignment().dimensions()) > 1) {\n+          VectorGreaterThanOneElementCount(input_spec->dimensions()) > 1) {\n         continue;\n       }\n \n@@ -1459,7 +1458,7 @@ void TrimOrGenerateStrategiesBasedOnExistingSharding(\n           ShardingIsConsistent(existing_sharding, strategy.output_sharding,\n                                strict) ||\n           (VectorGreaterThanOneElementCount(\n-               strategy.output_sharding.tile_assignment().dimensions()) == 1 &&\n+               strategy.output_sharding.dimensions()) == 1 &&\n            spmd::ShardingIsComplete(\n                strategy.output_sharding,\n                cluster_env.original_device_mesh_.num_elements()))) {\n@@ -2028,11 +2027,11 @@ void CheckHloSharding(\n           }\n           const std::vector<int64_t> ins_sharded_dims =\n               VectorGreaterThanOneElementIndices(\n-                  ins->sharding().tile_assignment().dimensions(),\n+                  ins->sharding().dimensions(),\n                   ins->sharding().ReplicateOnLastTileDim());\n           const std::vector<int64_t> op_sharded_dims =\n               VectorGreaterThanOneElementIndices(\n-                  op->sharding().tile_assignment().dimensions(),\n+                  op->sharding().dimensions(),\n                   op->sharding().ReplicateOnLastTileDim());\n           bool not_consistent = false;\n           if (ins_sharded_dims.size() != op_sharded_dims.size()) {"
        },
        {
            "sha": "34e28ef5bf75a54566a8e5e7a7361316e444c04e",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding_strategy.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_strategy.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_strategy.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_strategy.cc?ref=9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
            "patch": "@@ -651,9 +651,8 @@ BuildStrategyAndCost(\n               // actually equal to the number of devices in the original\n               // mesh). Below, we use the correct mesh depending on the number\n               // of elements in the 1D sharding.\n-              bool is_1d_sharding =\n-                  VectorGreaterThanOneElementCount(\n-                      input_spec.tile_assignment().dimensions()) == 1;\n+              bool is_1d_sharding = VectorGreaterThanOneElementCount(\n+                                        input_spec.dimensions()) == 1;\n               if (is_1d_sharding &&\n                   input_spec.TotalNumTiles() ==\n                       cluster_env.device_mesh_1d_.num_elements()) {"
        },
        {
            "sha": "d5b99476932f57f012e22f27d92cf48efc42a898",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding_strategy.h",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_strategy.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_strategy.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_strategy.h?ref=9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
            "patch": "@@ -93,13 +93,11 @@ struct InputShardings {\n         absl::StrAppend(&str, \"[R],\");\n       } else {\n         if (s->ReplicateOnLastTileDim()) {\n-          absl::StrAppend(\n-              &str, \"[\", absl::StrJoin(s->tile_assignment().dimensions(), \", \"),\n-              \"]last_tile_dim_replicate,\");\n+          absl::StrAppend(&str, \"[\", absl::StrJoin(s->dimensions(), \", \"),\n+                          \"]last_tile_dim_replicate,\");\n         } else {\n-          absl::StrAppend(\n-              &str, \"[\", absl::StrJoin(s->tile_assignment().dimensions(), \", \"),\n-              \"],\");\n+          absl::StrAppend(&str, \"[\", absl::StrJoin(s->dimensions(), \", \"),\n+                          \"],\");\n         }\n       }\n     }"
        },
        {
            "sha": "aaf75b62f733631a3728bb6902806cd6880ac260",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_test.cc?ref=9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
            "patch": "@@ -190,7 +190,7 @@ ENTRY %elementwise {\n     ASSERT_NE(root, nullptr);\n     EXPECT_EQ(root->sharding().NumTiles(), expected_num_tiles);\n     EXPECT_EQ(VectorGreaterThanOneElementCount(\n-                  root->sharding().tile_assignment().dimensions(),\n+                  root->sharding().dimensions(),\n                   root->sharding().ReplicateOnLastTileDim()),\n               expected_sharded_dimensions);\n   }\n@@ -227,8 +227,7 @@ ENTRY %elementwise {\n     ASSERT_NE(root, nullptr);\n     EXPECT_EQ(root->sharding().ReplicateOnLastTileDim(),\n               expected_last_dim_replicate);\n-    EXPECT_THAT(root->sharding().tile_assignment().dimensions(),\n-                ElementsAreArray(expected_tile));\n+    EXPECT_THAT(root->sharding().dimensions(), ElementsAreArray(expected_tile));\n   }\n \n   AliasInfo alias_info_;"
        },
        {
            "sha": "e9c7e6f3934f1372cae0b3d9e7db3919940af575",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding_util.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_util.cc?ref=9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
            "patch": "@@ -2440,9 +2440,9 @@ std::vector<std::vector<int64_t>> InferMeshShapesToTry(\n       return;\n     }\n     tiled_shardings.push_back(sharding);\n-    max_tile_dimensions = std::max(\n-        max_tile_dimensions, VectorGreaterThanOneElementCount(\n-                                 sharding.tile_assignment().dimensions()));\n+    max_tile_dimensions =\n+        std::max(max_tile_dimensions,\n+                 VectorGreaterThanOneElementCount(sharding.dimensions()));\n     if (sharding.tile_assignment().iota().has_value()) {\n       max_reshape_dims =\n           std::max(max_reshape_dims,\n@@ -2461,8 +2461,8 @@ std::vector<std::vector<int64_t>> InferMeshShapesToTry(\n \n   std::vector<HloSharding> tiled_shardings_with_all_dims_in_order;\n   for (const HloSharding& sharding : tiled_shardings) {\n-    if (VectorGreaterThanOneElementCount(\n-            sharding.tile_assignment().dimensions()) != max_tile_dimensions ||\n+    if (VectorGreaterThanOneElementCount(sharding.dimensions()) !=\n+            max_tile_dimensions ||\n         !sharding.tile_assignment().iota().has_value()) {\n       continue;\n     }\n@@ -2475,8 +2475,8 @@ std::vector<std::vector<int64_t>> InferMeshShapesToTry(\n   absl::flat_hash_set<std::vector<int64_t>> mesh_shape_candidates;\n   if (!tiled_shardings_with_all_dims_in_order.empty()) {\n     for (const HloSharding& sharding : tiled_shardings_with_all_dims_in_order) {\n-      mesh_shape_candidates.insert(VectorGreaterThanOneElements(\n-          sharding.tile_assignment().dimensions()));\n+      mesh_shape_candidates.insert(\n+          VectorGreaterThanOneElements(sharding.dimensions()));\n     }\n   } else {\n     for (const HloSharding& sharding : tiled_shardings) {"
        },
        {
            "sha": "c465a8c0e5cb8c74ea89da97e6639c4b8a7531b5",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding_util.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fauto_sharding_util.h?ref=9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
            "patch": "@@ -331,7 +331,7 @@ inline std::string ToStringSimple(const HloSharding& spec) {\n   if (spec.IsReplicated()) {\n     return \"R\";\n   }\n-  return ToString(spec.tile_assignment().dimensions());\n+  return ToString(spec.dimensions());\n }\n \n // Insert a copy of the operand to force the sharding of the operand."
        },
        {
            "sha": "ef30645d9c55f9326e1dcd5b8cc69c1d0d76be48",
            "filename": "third_party/xla/xla/hlo/experimental/auto_sharding/cluster_environment.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fcluster_environment.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fcluster_environment.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fexperimental%2Fauto_sharding%2Fcluster_environment.cc?ref=9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
            "patch": "@@ -320,8 +320,7 @@ double ClusterEnvironment::ReshardingCost(const Shape& shape,\n \n   auto get_tensor_dim_to_mesh_dim = [&](int64_t rank,\n                                         const HloSharding& sharding) {\n-    if (VectorGreaterThanOneElementCount(\n-            sharding.tile_assignment().dimensions()) == 1 &&\n+    if (VectorGreaterThanOneElementCount(sharding.dimensions()) == 1 &&\n         VectorGreaterThanOneElementCount(device_mesh_.dimensions()) > 1) {\n       // sharding is 1D and device_mesh is 2D or 3D\n       return GetTensorDimToMeshDimNoCrash(\n@@ -340,8 +339,8 @@ double ClusterEnvironment::ReshardingCost(const Shape& shape,\n       get_tensor_dim_to_mesh_dim(dst_rank, dst_spec);\n \n   if (!src_tensor_dim_to_mesh_dim_or.ok() && dst_spec.IsReplicated()) {\n-    auto equivalent_src_spec = HloSharding::IotaTile(\n-        src_spec.tile_assignment().dimensions(), src_spec.metadata());\n+    auto equivalent_src_spec =\n+        HloSharding::IotaTile(src_spec.dimensions(), src_spec.metadata());\n     if (auto equivalent_src_tensor_dim_to_mesh_dim_or =\n             get_tensor_dim_to_mesh_dim(src_rank, equivalent_src_spec);\n         equivalent_src_tensor_dim_to_mesh_dim_or.ok()) {"
        },
        {
            "sha": "026e93eff722e5448aa870917affd70f2e15b538",
            "filename": "third_party/xla/xla/hlo/ir/hlo_sharding.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 15,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.cc?ref=9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
            "patch": "@@ -642,14 +642,12 @@ absl::Status HloSharding::EachTile(\n   CHECK(!IsUnknown());\n   CHECK(!maximal_);\n \n-  // At the high-level, tile_assignment_dims[i] describes the number of ways the\n-  // shape is partitioned along i-th dimension. Note that\n-  // tile_assignment_dims[i] with i >= dims.size() encodes other information\n-  // such as subgroups to express partial replication/sharding and other\n-  // semantics.  They do not participate in determining the tile origin and\n-  // shape.\n-  const absl::Span<const int64_t> tile_assignment_dims =\n-      tile_assignment().dimensions();\n+  // At the high-level, sharding_dims[i] describes the number of ways the shape\n+  // is partitioned along i-th dimension. Note that sharding_dims[i] with i >=\n+  // dims.size() encodes other information such as subgroups to express partial\n+  // replication/sharding and other semantics.  They do not participate in\n+  // determining the tile origin and shape.\n+  const absl::Span<const int64_t> sharding_dims = dimensions();\n   const int num_devices = tile_assignment().array().num_elements();\n \n   if (dims.size() != TiledDataRank()) {\n@@ -660,21 +658,21 @@ absl::Status HloSharding::EachTile(\n   absl::InlinedVector<int64_t, 6> tile_dims;\n   tile_dims.reserve(dims.size());\n   for (int64_t i = 0; i < dims.size(); ++i) {\n-    tile_dims.push_back(CeilOfRatio(dims[i], tile_assignment_dims[i]));\n+    tile_dims.push_back(CeilOfRatio(dims[i], sharding_dims[i]));\n   }\n \n   const int64_t replication_dim = SubgroupReplicationDim();\n   int64_t num_replicas;\n   if (replication_dim == -1) {\n     num_replicas = 1;\n   } else {\n-    num_replicas = tile_assignment_dims[replication_dim];\n+    num_replicas = sharding_dims[replication_dim];\n   }\n \n-  // Enumerate over all indices of tiles. For instance, if tile_assignment_dims\n-  // is [3, 2], iterate over [[0, 0], [0, 1], [1, 0], [1, 1], [2, 0], [2, 1]].\n-  // If tile_assignment_dims includes replication, we only enumerate over the\n-  // sharding portion, and copy the same indices multiple times.\n+  // Enumerate over all indices of tiles. For instance, if sharding_dims is [3,\n+  // 2], iterate over [[0, 0], [0, 1], [1, 0], [1, 1], [2, 0], [2, 1]]. If\n+  // sharding_dims includes replication, we only enumerate over the sharding\n+  // portion, and copy the same indices multiple times.\n   absl::InlinedVector<int64_t, 6> unique_tile_index(dims.size());\n   absl::InlinedVector<int64_t, 6> tile_offset(dims.size());\n   absl::InlinedVector<int64_t, 6> tile_limit(dims.size());\n@@ -698,7 +696,7 @@ absl::Status HloSharding::EachTile(\n       f(device_id, tile_offset, tile_limit);\n       ++flat_tile_index;\n     }\n-  } while (NextIndex(&unique_tile_index, tile_assignment_dims));\n+  } while (NextIndex(&unique_tile_index, sharding_dims));\n   return absl::OkStatus();\n }\n "
        },
        {
            "sha": "ae2b33299ffe1c8c021a1a737c3480b6a2023f5f",
            "filename": "third_party/xla/xla/hlo/ir/hlo_sharding.h",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fir%2Fhlo_sharding.h?ref=9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
            "patch": "@@ -526,6 +526,11 @@ class HloSharding {\n     return tile_assignment().dim(dim_index);\n   }\n \n+  // Returns all sharding dimensions.\n+  absl::Span<const int64_t> dimensions() const {\n+    return tile_assignment().dimensions();\n+  }\n+\n   // Returns the total number of devices used by sharding.\n   int64_t num_devices() const { return tile_assignment().num_elements(); }\n "
        },
        {
            "sha": "69149aca17d996811be73592624f6075afb5ef3c",
            "filename": "third_party/xla/xla/hlo/utils/hlo_sharding_util.cc",
            "status": "modified",
            "additions": 28,
            "deletions": 40,
            "changes": 68,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Futils%2Fhlo_sharding_util.cc?ref=9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
            "patch": "@@ -227,9 +227,8 @@ bool IsSubTilingOrEqualSharding(const Shape& potential_sharded_shape,\n   DimensionVector base_tile(tiled_data_rank);\n   bool shortcut = true;\n   int64_t diff_dim_counter = 0;\n-  DimensionVector reshape_dims(\n-      potential_subsharding.tile_assignment().dimensions().begin(),\n-      potential_subsharding.tile_assignment().dimensions().end());\n+  DimensionVector reshape_dims(potential_subsharding.dimensions().begin(),\n+                               potential_subsharding.dimensions().end());\n   for (int64_t i = 0; i < tiled_data_rank; ++i) {\n     const auto shape_i = potential_sharded_shape.dimensions(i);\n     const auto p_tile_dim_i = potential_subsharding.dimension(i);\n@@ -287,7 +286,7 @@ bool IsSubTilingOrEqualSharding(const Shape& potential_sharded_shape,\n     auto reshaped_ta = potential_subsharding.tile_assignment()\n                            .Reshape(reshape_dims)\n                            .Transpose(perm)\n-                           .Reshape(sharding.tile_assignment().dimensions());\n+                           .Reshape(sharding.dimensions());\n     return HloSharding::PartialTile(reshaped_ta).tile_assignment() ==\n            sharding.tile_assignment();\n   }\n@@ -812,9 +811,8 @@ HloSharding MoveAndMergeShardingTiles(const HloSharding& sharding,\n   // target_dim and the swapped source_dim.\n \n   // Step 1. Adding a dummy dim of size 1 after the target_dim.\n-  std::vector<int64_t> ta_dims_1(\n-      sharding.tile_assignment().dimensions().begin(),\n-      sharding.tile_assignment().dimensions().end());\n+  std::vector<int64_t> ta_dims_1(sharding.dimensions().begin(),\n+                                 sharding.dimensions().end());\n   ta_dims_1.insert(ta_dims_1.begin() + target_dim + 1, 1);\n   TileAssignment new_tile_assignment =\n       sharding.tile_assignment().Reshape(ta_dims_1);\n@@ -1070,9 +1068,8 @@ HloSharding PropagateShardingThroughReshape(const Shape& source_shape,\n           perm.push_back(i);\n         }\n \n-        DimensionVector reshape_dims(\n-            reshaped->tile_assignment().dimensions().begin(),\n-            reshaped->tile_assignment().dimensions().end());\n+        DimensionVector reshape_dims(reshaped->dimensions().begin(),\n+                                     reshaped->dimensions().end());\n         CHECK_EQ(sharding.num_devices() % Product(reshape_dims), 0);\n         int64_t num_replicated_dims =\n             sharding.num_devices() / Product(reshape_dims);\n@@ -1113,7 +1110,7 @@ HloSharding ReverseSharding(const HloSharding& sharding,\n     return sharding;\n   }\n \n-  Array<int64_t> new_tile_assignment(sharding.tile_assignment().dimensions());\n+  Array<int64_t> new_tile_assignment(sharding.dimensions());\n   new_tile_assignment.Each(\n       [&](absl::Span<const int64_t> indices, int64_t* device) {\n         std::vector<int64_t> original_indices(indices.begin(), indices.end());\n@@ -1616,9 +1613,8 @@ HloSharding PartiallyReplicateTiledShardingOnDims(\n   });\n   auto new_tile =\n       TransposeSharding(sharding, dim_permutation).tile_assignment();\n-  DimensionVector new_tile_shape(\n-      sharding.tile_assignment().dimensions().begin(),\n-      sharding.tile_assignment().dimensions().end());\n+  DimensionVector new_tile_shape(sharding.dimensions().begin(),\n+                                 sharding.dimensions().end());\n   for (int64_t dim : valid_dims_to_replicate) {\n     new_tile_shape[dim] = 1;\n   }\n@@ -2250,19 +2246,17 @@ GroupedSharding GroupShardingOnReplicatedDim(\n     int64_t data_rank, absl::Span<const int64_t> replicable_dims) {\n   // 1. Try group sharding on partially replicated dim.\n   if (sharding.ReplicateOnLastTileDim() &&\n-      sharding.tile_assignment().dimensions().back() % num_groups == 0) {\n+      sharding.dimensions().back() % num_groups == 0) {\n     absl::InlinedVector<int64_t, 1> group_dim_shards = {\n-        sharding.tile_assignment().dimensions().back() / num_groups};\n+        sharding.dimensions().back() / num_groups};\n     return GroupShardingOnDims(sharding, {sharding.num_dimensions() - 1},\n                                group_dim_shards);\n   }\n \n   // 2. Try borrow dimensions from replicable_dims in order, and group sharding.\n   if (sharding.IsTiled()) {\n     const int64_t reps_on_last_tile_dim =\n-        sharding.ReplicateOnLastTileDim()\n-            ? sharding.tile_assignment().dimensions().back()\n-            : 1;\n+        sharding.ReplicateOnLastTileDim() ? sharding.dimensions().back() : 1;\n \n     const int64_t max_replicable_dimensions =\n         absl::c_accumulate(replicable_dims, reps_on_last_tile_dim,\n@@ -2274,9 +2268,8 @@ GroupedSharding GroupShardingOnReplicatedDim(\n         num_groups % reps_on_last_tile_dim == 0) {\n       auto tile_assignment = [&]() -> std::optional<TileAssignment> {\n         int dimensions_to_borrow = num_groups / reps_on_last_tile_dim;\n-        DimensionVector tile_dims(\n-            sharding.tile_assignment().dimensions().begin(),\n-            sharding.tile_assignment().dimensions().end());\n+        DimensionVector tile_dims(sharding.dimensions().begin(),\n+                                  sharding.dimensions().end());\n         if (!sharding.ReplicateOnLastTileDim()) {\n           tile_dims.push_back(1);\n         }\n@@ -2352,26 +2345,23 @@ PartialReplicatedGroupShardingWithAssignedDeviceGroups(\n     const HloSharding& sharding, int64_t num_shards,\n     const DeviceGroupTileAssignment& device_groups) {\n   if (!sharding.ReplicateOnLastTileDim() ||\n-      sharding.tile_assignment().dimensions().back() %\n-              device_groups.num_groups() !=\n-          0) {\n+      sharding.dimensions().back() % device_groups.num_groups() != 0) {\n     VLOG(5) << \"Failed because not partial replicated or not divisible\";\n     return std::nullopt;\n   }\n   std::vector<DimensionVector> device_to_index(\n-      Product(sharding.tile_assignment().dimensions()),\n+      Product(sharding.dimensions()),\n       DimensionVector(sharding.num_dimensions()));\n   sharding.tile_assignment().Each(\n       [&device_to_index](absl::Span<const int64_t> indices, int64_t device) {\n         device_to_index[device].assign(indices.begin(), indices.end());\n       });\n-  DimensionVector grouped_tiling_dims(\n-      sharding.tile_assignment().dimensions().begin(),\n-      sharding.tile_assignment().dimensions().end());\n+  DimensionVector grouped_tiling_dims(sharding.dimensions().begin(),\n+                                      sharding.dimensions().end());\n   grouped_tiling_dims.back() /= device_groups.num_groups();\n   std::optional<HloSharding> final_sharding;\n   const int64_t shard_size_on_replicated_dim =\n-      sharding.tile_assignment().dimensions().back() / num_shards;\n+      sharding.dimensions().back() / num_shards;\n \n   for (int64_t group_idx = 0; group_idx < device_groups.num_groups();\n        ++group_idx) {\n@@ -2450,9 +2440,8 @@ HloSharding UngroupSharding(const GroupedSharding& grouped_sharding) {\n     subgroup_types = std::vector<OpSharding::Type>(subgroup_dim_size,\n                                                    OpSharding::REPLICATED);\n     if (!grouped_sharding.sharding.IsTileMaximal()) {\n-      tiling_dims.assign(\n-          grouped_sharding.sharding.tile_assignment().dimensions().begin(),\n-          grouped_sharding.sharding.tile_assignment().dimensions().end());\n+      tiling_dims.assign(grouped_sharding.sharding.dimensions().begin(),\n+                         grouped_sharding.sharding.dimensions().end());\n     }\n     for (int i = 0; i < grouped_sharding.group_dims.size(); i++) {\n       subgroup_types[grouped_sharding.group_dims[i] -\n@@ -2463,9 +2452,8 @@ HloSharding UngroupSharding(const GroupedSharding& grouped_sharding) {\n   } else if (!grouped_sharding.sharding.IsTileMaximal()) {\n     // Handles tile replicated.\n     partial_sharding = grouped_sharding.sharding.ReplicateOnLastTileDim();\n-    tiling_dims.assign(\n-        grouped_sharding.sharding.tile_assignment().dimensions().begin(),\n-        grouped_sharding.sharding.tile_assignment().dimensions().end());\n+    tiling_dims.assign(grouped_sharding.sharding.dimensions().begin(),\n+                       grouped_sharding.sharding.dimensions().end());\n     if (absl::c_linear_search(grouped_sharding.group_dims,\n                               tiling_dims.size())) {\n       tiling_dims.push_back(1);\n@@ -2611,8 +2599,8 @@ HloSharding SplitShardingDimension(const HloSharding& sharding,\n   CHECK_GT(sharding.TiledDataRank(), dimension);\n   CHECK_EQ(sharding.dimension(dimension) % new_dim_size, 0)\n       << \"dim size \" << new_dim_size;\n-  DimensionVector dimensions(sharding.tile_assignment().dimensions().begin(),\n-                             sharding.tile_assignment().dimensions().end());\n+  DimensionVector dimensions(sharding.dimensions().begin(),\n+                             sharding.dimensions().end());\n   int64_t current_dimension = dimensions[dimension];\n   dimensions.insert(dimensions.begin() + dimension + 1,\n                     current_dimension / new_dim_size);\n@@ -2627,8 +2615,8 @@ HloSharding SplitShardingDimension(const HloSharding& sharding,\n HloSharding MergeShardingDimension(const HloSharding& sharding,\n                                    int64_t dimension) {\n   CHECK_GT(sharding.TiledDataRank(), dimension);\n-  DimensionVector dimensions(sharding.tile_assignment().dimensions().begin(),\n-                             sharding.tile_assignment().dimensions().end());\n+  DimensionVector dimensions(sharding.dimensions().begin(),\n+                             sharding.dimensions().end());\n   dimensions[dimension] *= dimensions[dimension + 1];\n   dimensions.erase(dimensions.begin() + dimension + 1);\n   auto new_tile_assignment = sharding.tile_assignment().Reshape(dimensions);"
        },
        {
            "sha": "8b7886e6a369f19c67f3847a3baa4d044856e95c",
            "filename": "third_party/xla/xla/python/custom_call_batch_partitioner.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fpython%2Fcustom_call_batch_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fpython%2Fcustom_call_batch_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fcustom_call_batch_partitioner.cc?ref=9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
            "patch": "@@ -72,9 +72,8 @@ HloSharding GetBatchSharding(const HloSharding& sharding,\n     return batch_sharding;\n   }\n \n-  std::vector<int64_t> dimensions(\n-      batch_sharding.tile_assignment().dimensions().begin(),\n-      batch_sharding.tile_assignment().dimensions().end());\n+  std::vector<int64_t> dimensions(batch_sharding.dimensions().begin(),\n+                                  batch_sharding.dimensions().end());\n   dimensions.erase(\n       dimensions.begin() + batch_sharding.TiledDataRank() - num_replicate_dims,\n       dimensions.begin() + batch_sharding.TiledDataRank());\n@@ -96,9 +95,8 @@ HloSharding InsertNonBatchSharding(const HloSharding& sharding,\n   if (num_replicate_dims == 0) {\n     return sharding;\n   }\n-  std::vector<int64_t> dimensions(\n-      sharding.tile_assignment().dimensions().begin(),\n-      sharding.tile_assignment().dimensions().end());\n+  std::vector<int64_t> dimensions(sharding.dimensions().begin(),\n+                                  sharding.dimensions().end());\n   for (int64_t i = 0; i < num_replicate_dims; ++i) {\n     dimensions.insert(dimensions.begin() + sharding.TiledDataRank(), 1);\n   }"
        },
        {
            "sha": "880fd3922a37fc7b080f42cfa26f2ba3d8f3e1c4",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/xla_sharding.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fxla_sharding.cc?ref=9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
            "patch": "@@ -144,13 +144,12 @@ absl::StatusOr<Shape> HloSharding::GetShardShape(const Shape& shape) const {\n         \"HloSharding %d\",\n         shape.dims().size(), xla_hlo_sharding_.TiledDataRank());\n   }\n-  const absl::Span<const int64_t> tile_assignment_dims =\n-      xla_hlo_sharding_.tile_assignment().dimensions();\n+  const absl::Span<const int64_t> sharding_dims =\n+      xla_hlo_sharding_.dimensions();\n   Shape::Dimensions tile_shape;\n   tile_shape.reserve(shape.dims().size());\n   for (int64_t i = 0; i < shape.dims().size(); ++i) {\n-    tile_shape.push_back(\n-        xla::CeilOfRatio(shape.dims()[i], tile_assignment_dims[i]));\n+    tile_shape.push_back(xla::CeilOfRatio(shape.dims()[i], sharding_dims[i]));\n   }\n   return Shape(std::move(tile_shape));\n }"
        },
        {
            "sha": "b07524c8891f651473b4fa538f8fc5702c51342c",
            "filename": "third_party/xla/xla/service/sharding_propagation.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fservice%2Fsharding_propagation.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fservice%2Fsharding_propagation.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fsharding_propagation.cc?ref=9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
            "patch": "@@ -778,9 +778,8 @@ bool RefineManualAutoShardingFromAuto(\n   // We are also merging the non-manual sharding into the manual sharding. To\n   // leverage existing merging implementation, we treat the manual dim as a\n   // data dim, and add it right before the replication dim.\n-  std::vector<int64_t> partial_manual_shape(\n-      partial_rep.tile_assignment().dimensions().begin(),\n-      partial_rep.tile_assignment().dimensions().end());\n+  std::vector<int64_t> partial_manual_shape(partial_rep.dimensions().begin(),\n+                                            partial_rep.dimensions().end());\n   partial_manual_shape.insert(partial_manual_shape.begin() + data_rank, 1);\n   auto partial_tiling_for_manual =\n       partial_rep.tile_assignment().Reshape(partial_manual_shape);"
        },
        {
            "sha": "7dd17a745e623130eeadf85a4c2eafa95c9a234d",
            "filename": "third_party/xla/xla/service/spmd/custom_call_handler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fcustom_call_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fcustom_call_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fcustom_call_handler.cc?ref=9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
            "patch": "@@ -130,9 +130,8 @@ absl::Status SpmdPartitioningVisitor::HandleCustomCallTopK(\n     partition_state = CreatePerGroupPartitioningState(\n         partitioned_input.state(), sharding_grouped.device_groups,\n         partitioned_input.state().b);\n-    std::vector<int64_t> reshape_dimensions(\n-        sharding.tile_assignment().dimensions().begin(),\n-        sharding.tile_assignment().dimensions().end());\n+    std::vector<int64_t> reshape_dimensions(sharding.dimensions().begin(),\n+                                            sharding.dimensions().end());\n     reshape_dimensions.push_back(reshape_dimensions.back());\n     reshape_dimensions[sort_dim] = 1;\n     auto reshape_tile_assignment ="
        },
        {
            "sha": "ddc9012283166d612726885ca649a8c0b5c8c484",
            "filename": "third_party/xla/xla/service/spmd/dot_handler.cc",
            "status": "modified",
            "additions": 20,
            "deletions": 31,
            "changes": 51,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fdot_handler.cc?ref=9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
            "patch": "@@ -903,8 +903,7 @@ absl::StatusOr<HloInstruction*> EmitWindowedDotGeneral(\n                          ? &*lhs_sharding_transposed_to_match_output\n                          : &*rhs_sharding_transposed_to_match_output;\n   }\n-  CHECK_EQ(Product(slice_sharding->tile_assignment().dimensions()),\n-           num_partitions);\n+  CHECK_EQ(Product(slice_sharding->dimensions()), num_partitions);\n   int64_t slice_sharding_dim = -1;\n   for (int64_t i = 0; i < slice_sharding->num_dimensions(); ++i) {\n     if (slice_sharding->dimension(i) > 1) {\n@@ -2151,18 +2150,15 @@ absl::StatusOr<HloInstruction*> PartitionDotGroupOnBatchImpl(\n   auto lhs_sharding_dims_adjusted_to_output =\n       lhs.sharding().IsReplicated()\n           ? std::vector<int64_t>(lhs.base_shape().dimensions().size(), 1)\n-          : std::vector<int64_t>(\n-                lhs.sharding().tile_assignment().dimensions().begin(),\n-                lhs.sharding().tile_assignment().dimensions().end());\n+          : std::vector<int64_t>(lhs.sharding().dimensions().begin(),\n+                                 lhs.sharding().dimensions().end());\n   auto rhs_sharding_dims_adjusted_to_output =\n       rhs.sharding().IsReplicated()\n           ? std::vector<int64_t>(rhs.base_shape().dimensions().size(), 1)\n-          : std::vector<int64_t>(\n-                rhs.sharding().tile_assignment().dimensions().begin(),\n-                rhs.sharding().tile_assignment().dimensions().end());\n+          : std::vector<int64_t>(rhs.sharding().dimensions().begin(),\n+                                 rhs.sharding().dimensions().end());\n   std::vector<int64_t> output_sharding_dims_adjusted_to_lhs(\n-      output_sharding.tile_assignment().dimensions().begin(),\n-      output_sharding.tile_assignment().dimensions().end());\n+      output_sharding.dimensions().begin(), output_sharding.dimensions().end());\n   bool lhs_rhs_dims_matching = true;\n   for (const auto& dim : dims_mapping.batch_dims) {\n     lhs_dims.push_back(dim.lhs);\n@@ -2367,8 +2363,8 @@ GroupedSharding GetNonContractingPartitionGroupedShardingForMatchedOperand(\n     const HloSharding& output_sharding,\n     absl::Span<const DotConvolutionDimsInfo::DimNums> partitioned_dims) {\n   std::vector<int64_t> matching_sharding_dims(\n-      matching_sharding.tile_assignment().dimensions().begin(),\n-      matching_sharding.tile_assignment().dimensions().end());\n+      matching_sharding.dimensions().begin(),\n+      matching_sharding.dimensions().end());\n   std::vector<int64_t> matching_dims;\n   std::vector<int64_t> output_dims;\n   // Make sure the partitioning on matching's non-contracting dimensions\n@@ -2416,13 +2412,13 @@ GetNonContractingPartitionGroupedShardingForOtherOperand(\n   std::vector<int64_t> other_group_dims;\n   // Try to match on the replicated dimensions first.\n   if (other_sharding.ReplicateOnLastTileDim() &&\n-      other_sharding.tile_assignment().dimensions().back() % group_count == 0) {\n+      other_sharding.dimensions().back() % group_count == 0) {\n     // Try to aggressively match the replicated dimension with the current\n     // output device groups. If fails then try find a dimension to swap instead\n     // of reordering the mesh with collective permutes that can create weird\n     // patterns. If that fails also do the traditional replication matching.\n     for (int64_t i = other_sharding.num_dimensions() - 1; i >= 0; --i) {\n-      if (other_sharding.tile_assignment().dimensions()[i] % group_count == 0) {\n+      if (other_sharding.dimensions()[i] % group_count == 0) {\n         std::vector<int64_t> perm(other_sharding.num_dimensions(), 0);\n         absl::c_iota(perm, 0);\n         std::swap(perm[i], perm[other_sharding.num_dimensions() - 1]);\n@@ -2433,8 +2429,7 @@ GetNonContractingPartitionGroupedShardingForOtherOperand(\n         if (auto grouped_sharding = hlo_sharding_util::\n                 PartialReplicatedGroupShardingWithAssignedDeviceGroups(\n                     sharding_to_match,\n-                    sharding_to_match.tile_assignment().dimensions().back() /\n-                        group_count,\n+                    sharding_to_match.dimensions().back() / group_count,\n                     output_grouped.device_groups)) {\n           return grouped_sharding.value();\n         }\n@@ -2457,9 +2452,7 @@ GetNonContractingPartitionGroupedShardingForOtherOperand(\n     } else if (other_sharding.ReplicateOnLastTileDim() &&\n                // Match grouping non-matching replicated dimension at a lower\n                // priority than finding matched dimensions as it usually pro\n-               other_sharding.tile_assignment().dimensions().back() %\n-                       group_count ==\n-                   0) {\n+               other_sharding.dimensions().back() % group_count == 0) {\n       other_group_dims.push_back(other_sharding.num_dimensions() - 1);\n     } else if (may_replicate_other_contracting_dims &&\n                (!may_replicate_other_non_contracting_dims ||\n@@ -2480,7 +2473,7 @@ GetNonContractingPartitionGroupedShardingForOtherOperand(\n   if (other_group_dims.size() == 1 &&\n       other_group_dims[0] == other_sharding.num_dimensions() - 1) {\n     std::vector<int64_t> group_dim_shards = {\n-        other_sharding.tile_assignment().dimensions().back() / group_count};\n+        other_sharding.dimensions().back() / group_count};\n     return AlignGroupsWith(\n         hlo_sharding_util::GroupShardingOnDims(\n             other_sharding, {other_group_dims[0]}, group_dim_shards),\n@@ -2641,8 +2634,7 @@ GetDotGroupPartitionContractingOutputShardings(\n   // non-contracting dimensions of the operands.\n   if (output_sharding.IsTiled() &&\n       (!output_sharding.ReplicateOnLastTileDim() ||\n-       output_sharding.tile_assignment().dimensions().back() % group_count !=\n-           0)) {\n+       output_sharding.dimensions().back() % group_count != 0)) {\n     DotDimensionIndexMapping indices_map = ComputeDimensionIndexMapping(\n         dims_mapping, lhs_grouped.data_rank, rhs_grouped.data_rank,\n         output_sharding.TiledDataRank());\n@@ -2685,10 +2677,9 @@ GetDotGroupPartitionContractingOutputShardings(\n \n   std::vector<int64_t> output_slice_dims;\n   if (output_sharding.ReplicateOnLastTileDim() &&\n-      output_sharding.tile_assignment().dimensions().back() % group_count ==\n-          0) {\n+      output_sharding.dimensions().back() % group_count == 0) {\n     std::vector<int64_t> group_dim_shards = {\n-        output_sharding.tile_assignment().dimensions().back() / group_count};\n+        output_sharding.dimensions().back() / group_count};\n     auto grouped = AlignGroupsWith(\n         hlo_sharding_util::GroupShardingOnDims(\n             output_sharding, {output_sharding.num_dimensions() - 1},\n@@ -2761,12 +2752,10 @@ GetDotGroupPartitionContractingLhsRhsShardings(\n         partitioned_contracting_dims) {\n   HloSharding lhs_sharding = lhs.sharding();\n   HloSharding rhs_sharding = rhs.sharding();\n-  std::vector<int64_t> lhs_tile_shape(\n-      lhs_sharding.tile_assignment().dimensions().begin(),\n-      lhs_sharding.tile_assignment().dimensions().end());\n-  std::vector<int64_t> rhs_tile_shape(\n-      rhs_sharding.tile_assignment().dimensions().begin(),\n-      rhs_sharding.tile_assignment().dimensions().end());\n+  std::vector<int64_t> lhs_tile_shape(lhs_sharding.dimensions().begin(),\n+                                      lhs_sharding.dimensions().end());\n+  std::vector<int64_t> rhs_tile_shape(rhs_sharding.dimensions().begin(),\n+                                      rhs_sharding.dimensions().end());\n   if (ShapeUtil::ByteSizeOf(lhs.hlo()->shape()) >\n       ShapeUtil::ByteSizeOf(rhs.hlo()->shape())) {\n     for (const auto& dim : partitioned_contracting_dims) {"
        },
        {
            "sha": "c34038157c59da3e1aa0b92b6e645d326dbb94a0",
            "filename": "third_party/xla/xla/service/spmd/fft_handler.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Ffft_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Ffft_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Ffft_handler.cc?ref=9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
            "patch": "@@ -363,8 +363,7 @@ absl::Status SpmdPartitioningVisitor::HandleFft(HloInstruction* hlo) {\n \n   // Support partition at the last dimension only.\n   if (!hlo->has_sharding() ||\n-      hlo->sharding().tile_assignment().dimensions().back() !=\n-          num_partitions_) {\n+      hlo->sharding().dimensions().back() != num_partitions_) {\n     return DefaultAction(hlo);\n   }\n "
        },
        {
            "sha": "cf665909a9ff50a6d8d97fe4684999253616fbd0",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 22,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
            "patch": "@@ -1186,8 +1186,7 @@ PartitionedHlo::ReshardAsWindowedInput(const Window& window,\n         hlo_, shard_window, get_dynamic_slice_offset_on_output_if_needed()});\n   }\n   if (target.ReplicateOnLastTileDim()) {\n-    trimmed_target_sharding_tile_shape.push_back(\n-        target.tile_assignment().dimensions().back());\n+    trimmed_target_sharding_tile_shape.push_back(target.dimensions().back());\n   }\n   std::optional<HloSharding> trimmed_target;\n   const HloSharding* halo_exchange_target = &target;\n@@ -1567,8 +1566,7 @@ PartitionedHlo::ReshardFromPartialReplicateWithDynamicSlice(\n \n   // Add another dimension in tiling_dim_factors if target is partial replicate.\n   if (target.ReplicateOnLastTileDim()) {\n-    tiling_dim_factors.emplace_back(\n-        target.tile_assignment().dimensions().back());\n+    tiling_dim_factors.emplace_back(target.dimensions().back());\n   }\n \n   // 2. Get the padded_hlo, do right halo exchange if needed.\n@@ -2047,8 +2045,8 @@ PatternMatchMergeOrSplitSharding(const Shape& base_shape,\n   }\n   if ((source.HasPartialReplication() ^ target.HasPartialReplication()) ||\n       (source.HasPartialReplication() &&\n-       source.tile_assignment().dimensions()[source.TiledDataRank()] !=\n-           target.tile_assignment().dimensions()[target.TiledDataRank()])) {\n+       source.dimensions()[source.TiledDataRank()] !=\n+           target.dimensions()[target.TiledDataRank()])) {\n     return std::nullopt;\n   }\n \n@@ -2227,8 +2225,7 @@ std::optional<PartitionedHlo> PartitionedHlo::TryComplexReshardHandling(\n     return final_reshard;\n   }\n   if (is_source_partially_replicated && !is_target_partially_replicated) {\n-    const int64_t partial_repl_amount =\n-        sharding().tile_assignment().dimensions().back();\n+    const int64_t partial_repl_amount = sharding().dimensions().back();\n     int64_t first_different_dimension = -1;\n     // Trying to match conditions like [..,X,..,Z,..,Y] last_tile_dim_replicate\n     // to [..,Y,..,Z,..,X,..], where Y in the source is partially replicated,\n@@ -2288,8 +2285,7 @@ PartitionedHlo::ReshardPartialReplicateWithAllToAll(\n   // sharding={devices=[1,2,3]0,1,2,3,4,5 last_tile_dim_replicate}\n   // to sharding={devices=[2,3]0,1,2,3,4,5}, where\n   // the last tile dim will be sharded after all-to-all.\n-  const int num_replicas =\n-      partial_replicate_sharding.tile_assignment().dimensions().back();\n+  const int num_replicas = partial_replicate_sharding.dimensions().back();\n   if (((tile_sharding.num_dimensions() + 1) !=\n        partial_replicate_sharding.num_dimensions()) ||\n       (partial_replicate_sharding.dimension(0) != 1)) {\n@@ -2317,18 +2313,17 @@ PartitionedHlo::ReshardPartialReplicateWithAllToAll(\n   // Check if core assignments for source and the target are the same.\n   auto reshape_tile_assignment =\n       partial_replicate_sharding.tile_assignment().Reshape(\n-          tile_sharding.tile_assignment().dimensions());\n+          tile_sharding.dimensions());\n   if (reshape_tile_assignment != tile_sharding.tile_assignment()) {\n     return std::nullopt;\n   }\n \n-  std::vector<int64_t> tmp_tile_assignment_dimensions(\n-      tile_sharding.tile_assignment().dimensions().begin(),\n-      tile_sharding.tile_assignment().dimensions().end());\n-  tmp_tile_assignment_dimensions[to_replicate_dim] = 1;\n-  tmp_tile_assignment_dimensions.push_back(num_replicas);\n+  std::vector<int64_t> tmp_sharding_dimensions(\n+      tile_sharding.dimensions().begin(), tile_sharding.dimensions().end());\n+  tmp_sharding_dimensions[to_replicate_dim] = 1;\n+  tmp_sharding_dimensions.push_back(num_replicas);\n   auto tmp_tile_assignment =\n-      tile_sharding.tile_assignment().Reshape(tmp_tile_assignment_dimensions);\n+      tile_sharding.tile_assignment().Reshape(tmp_sharding_dimensions);\n   auto tmp_partial_replicate_sharding =\n       HloSharding::PartialTile(tmp_tile_assignment);\n \n@@ -3241,17 +3236,15 @@ absl::Status SpmdPartitioningVisitor::HandleReshape(HloInstruction* hlo) {\n       return replicate();\n     }\n     // Fix potential device ordering mismatch in tile assignment.\n-    auto new_input_tile_assignment = sharding.tile_assignment().Reshape(\n-        operand.sharding().tile_assignment().dimensions());\n+    auto new_input_tile_assignment =\n+        sharding.tile_assignment().Reshape(operand.sharding().dimensions());\n     auto aligned_sharding =\n         sharding.ReplicateOnLastTileDim()\n             ? HloSharding::PartialTile(new_input_tile_assignment)\n             : HloSharding::Tile(new_input_tile_assignment);\n     operand = operand.Reshard(aligned_sharding);\n     auto replication_count =\n-        sharding.ReplicateOnLastTileDim()\n-            ? sharding.tile_assignment().dimensions().back()\n-            : 1;\n+        sharding.ReplicateOnLastTileDim() ? sharding.dimensions().back() : 1;\n \n     int64_t input_dim_size = operand.base_shape().dimensions(input_sharded_dim);\n     int64_t output_dim_size = base_shape.dimensions(output_sharded_dim);"
        },
        {
            "sha": "0fd2a3cdbd59314aaf2e039756f92290736ec530",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_util.cc",
            "status": "modified",
            "additions": 30,
            "deletions": 35,
            "changes": 65,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f6b534d3aa56c0e1afc6ec7b0908745352245b0/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc?ref=9f6b534d3aa56c0e1afc6ec7b0908745352245b0",
            "patch": "@@ -411,7 +411,7 @@ std::vector<HloInstruction*> MakePartitionOffsets(\n std::vector<HloInstruction*> MakeTiledPartitionOrdinals(\n     const HloSharding& sharding, HloInstruction* partition_id, SpmdBuilder* b) {\n   CHECK(!sharding.IsTileMaximal());\n-  auto dimensions = sharding.tile_assignment().dimensions();\n+  auto dimensions = sharding.dimensions();\n   if (sharding.ReplicateOnLastTileDim()) {\n     dimensions.remove_suffix(1);\n   }\n@@ -719,8 +719,8 @@ std::optional<HloSharding> PartialReplicateReshardCompatibleSharding(\n   }\n \n   const std::vector<int64_t> shape_dims(\n-      target_sharding.tile_assignment().dimensions().begin(),\n-      target_sharding.tile_assignment().dimensions().begin() + rank);\n+      target_sharding.dimensions().begin(),\n+      target_sharding.dimensions().begin() + rank);\n   if (hlo_sharding_util::IsSubTilingOrEqualSharding(\n           ShapeUtil::MakeShape(F32, shape_dims), target_sharding,\n           partial_sharding)) {\n@@ -731,8 +731,8 @@ std::optional<HloSharding> PartialReplicateReshardCompatibleSharding(\n   // decompose partial_sharding on the last tile dimension (replicated one) and\n   // move the decomposed tile dimensions to the expanded tile dimensions.\n   std::vector<int64_t> reshape_dimensions(\n-      partial_sharding.tile_assignment().dimensions().begin(),\n-      partial_sharding.tile_assignment().dimensions().begin() + rank);\n+      partial_sharding.dimensions().begin(),\n+      partial_sharding.dimensions().begin() + rank);\n   reshape_dimensions.insert(reshape_dimensions.end(), expand_tile_sizes.begin(),\n                             expand_tile_sizes.end());\n \n@@ -746,16 +746,14 @@ std::optional<HloSharding> PartialReplicateReshardCompatibleSharding(\n   }\n \n   if (target_sharding.ReplicateOnLastTileDim()) {\n-    reshape_dimensions.push_back(\n-        target_sharding.tile_assignment().dimensions().back());\n+    reshape_dimensions.push_back(target_sharding.dimensions().back());\n     perm.push_back(reshape_dimensions.size() - 1);\n   }\n \n-  auto transpose_tile_assignment =\n-      partial_sharding.tile_assignment()\n-          .Reshape(reshape_dimensions)\n-          .Transpose(perm)\n-          .Reshape(target_sharding.tile_assignment().dimensions());\n+  auto transpose_tile_assignment = partial_sharding.tile_assignment()\n+                                       .Reshape(reshape_dimensions)\n+                                       .Transpose(perm)\n+                                       .Reshape(target_sharding.dimensions());\n \n   return target_sharding.ReplicateOnLastTileDim()\n              ? HloSharding::PartialTile(transpose_tile_assignment)\n@@ -2235,8 +2233,7 @@ GetReshardAllToAllSourceTargetDims(const HloSharding& source,\n bool CanReshardWithCollectivePermute(const HloSharding& source,\n                                      const HloSharding& target) {\n   return !source.IsTileMaximal() && !target.IsTileMaximal() &&\n-         source.tile_assignment().dimensions() ==\n-             target.tile_assignment().dimensions() &&\n+         source.dimensions() == target.dimensions() &&\n          source.ReplicateOnLastTileDim() == target.ReplicateOnLastTileDim() &&\n          source.tile_assignment() != target.tile_assignment();\n }\n@@ -2581,9 +2578,8 @@ GatherScatterOperandsShardedAcrossParallelDims(\n     if (!to_adjust->ReplicateOnLastTileDim()) {\n       return std::nullopt;\n     }\n-    std::vector<int64_t> new_tile_assignment_dims(\n-        to_adjust->tile_assignment().dimensions().begin(),\n-        to_adjust->tile_assignment().dimensions().end());\n+    std::vector<int64_t> new_sharding_dims(to_adjust->dimensions().begin(),\n+                                           to_adjust->dimensions().end());\n     for (int i = 0; i < to_adjust_dims.size(); ++i) {\n       int64_t target_dim = target->dimension(target_dims[i]);\n       int64_t to_adjust_dim = to_adjust->dimension(to_adjust_dims[i]);\n@@ -2595,20 +2591,20 @@ GatherScatterOperandsShardedAcrossParallelDims(\n       }\n       int64_t ratio = target_dim / to_adjust_dim;\n       if (target_dim % to_adjust_dim != 0 ||\n-          new_tile_assignment_dims.back() % ratio != 0) {\n+          new_sharding_dims.back() % ratio != 0) {\n         return std::nullopt;\n       }\n-      new_tile_assignment_dims[to_adjust_dims[i]] *= ratio;\n-      new_tile_assignment_dims.back() /= ratio;\n+      new_sharding_dims[to_adjust_dims[i]] *= ratio;\n+      new_sharding_dims.back() /= ratio;\n     }\n-    CHECK_GE(new_tile_assignment_dims.back(), 1);\n+    CHECK_GE(new_sharding_dims.back(), 1);\n     bool to_partially_replicate = true;\n-    if (new_tile_assignment_dims.back() == 1) {\n-      new_tile_assignment_dims.pop_back();\n+    if (new_sharding_dims.back() == 1) {\n+      new_sharding_dims.pop_back();\n       to_partially_replicate = false;\n     }\n     auto new_tile_assignment =\n-        to_adjust->tile_assignment().Reshape(new_tile_assignment_dims);\n+        to_adjust->tile_assignment().Reshape(new_sharding_dims);\n     if (to_partially_replicate) {\n       *to_adjust =\n           AlignShardingOnDims(HloSharding::PartialTile(new_tile_assignment),\n@@ -2620,8 +2616,8 @@ GatherScatterOperandsShardedAcrossParallelDims(\n   }\n   // Make sure that the parallel dimensions are aligned.\n   std::vector<int64_t> operand_shard_tile_dims(\n-      new_operand_shard.tile_assignment().dimensions().begin(),\n-      new_operand_shard.tile_assignment().dimensions().end());\n+      new_operand_shard.dimensions().begin(),\n+      new_operand_shard.dimensions().end());\n   for (int i = 0; i < indices_parallel_dims.size(); ++i) {\n     operand_shard_tile_dims[operand_parallel_dims[i]] =\n         new_index_shard.dimension(indices_parallel_dims[i]);\n@@ -2846,31 +2842,30 @@ HloInstruction* PadDataFromWindowReshard(\n \n std::vector<std::vector<int64_t>> GetPartitionGroupsForReplication(\n     const HloSharding& sharding, absl::Span<const int64_t> replication_dims) {\n-  absl::Span<const int64_t> tile_assignment_dims =\n-      sharding.tile_assignment().dimensions();\n-  DCHECK_GE(tile_assignment_dims.size(), replication_dims.size());\n+  absl::Span<const int64_t> sharding_dims = sharding.dimensions();\n+  DCHECK_GE(sharding_dims.size(), replication_dims.size());\n   int64_t group_size = 1;\n   for (int64_t i : replication_dims) {\n-    DCHECK_LT(i, tile_assignment_dims.size());\n-    group_size *= tile_assignment_dims[i];\n+    DCHECK_LT(i, sharding_dims.size());\n+    group_size *= sharding_dims[i];\n   }\n   std::vector<int64_t> non_replication_indices;\n-  non_replication_indices.reserve(tile_assignment_dims.size() -\n+  non_replication_indices.reserve(sharding_dims.size() -\n                                   replication_dims.size());\n-  for (int64_t i = 0; i < tile_assignment_dims.size(); ++i) {\n+  for (int64_t i = 0; i < sharding_dims.size(); ++i) {\n     if (!absl::c_linear_search(replication_dims, i)) {\n       non_replication_indices.push_back(i);\n     }\n   }\n   DCHECK_EQ(replication_dims.size() + non_replication_indices.size(),\n-            tile_assignment_dims.size());\n+            sharding_dims.size());\n   std::vector<int64_t> non_replication_strides(non_replication_indices.size());\n   if (!non_replication_strides.empty()) {\n     non_replication_strides.back() = 1;\n     for (int64_t i = non_replication_indices.size() - 1; i > 0; --i) {\n       non_replication_strides[i - 1] =\n           non_replication_strides[i] *\n-          tile_assignment_dims[non_replication_indices[i]];\n+          sharding_dims[non_replication_indices[i]];\n     }\n   }\n "
        }
    ],
    "stats": {
        "total": 336,
        "additions": 146,
        "deletions": 190
    }
}