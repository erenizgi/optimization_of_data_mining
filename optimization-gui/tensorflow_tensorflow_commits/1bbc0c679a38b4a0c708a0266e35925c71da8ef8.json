{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 846053555",
    "sha": "1bbc0c679a38b4a0c708a0266e35925c71da8ef8",
    "files": [
        {
            "sha": "95044439f5b894dfce25ff3ed0a3f454e521d202",
            "filename": "tensorflow/core/tpu/tpu_embedding_optimization_parameters_utils.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1bbc0c679a38b4a0c708a0266e35925c71da8ef8/tensorflow%2Fcore%2Ftpu%2Ftpu_embedding_optimization_parameters_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1bbc0c679a38b4a0c708a0266e35925c71da8ef8/tensorflow%2Fcore%2Ftpu%2Ftpu_embedding_optimization_parameters_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Ftpu_embedding_optimization_parameters_utils.cc?ref=1bbc0c679a38b4a0c708a0266e35925c71da8ef8",
            "patch": "@@ -256,7 +256,7 @@ absl::Status UseGradientAccumulation(const OptimizationParameters& params,\n     }\n     case GradientAccumulationSupport::kNotSupported: {\n       if (raw_gradient_accumulation_status) {\n-        return errors::InvalidArgument(strings::Printf(\n+        return errors::InvalidArgument(absl::StrFormat(\n             \"Optimization algorithm %s does not support gradient accumulation \"\n             \"but parameters specify it.\",\n             GetOptimizationAlgorithmName(params.parameters_case()).c_str()));"
        },
        {
            "sha": "251cde239bcf6ce18918ab0c273d746a594ae58a",
            "filename": "tensorflow/core/tpu/tpu_execute.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1bbc0c679a38b4a0c708a0266e35925c71da8ef8/tensorflow%2Fcore%2Ftpu%2Ftpu_execute.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1bbc0c679a38b4a0c708a0266e35925c71da8ef8/tensorflow%2Fcore%2Ftpu%2Ftpu_execute.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftpu%2Ftpu_execute.cc?ref=1bbc0c679a38b4a0c708a0266e35925c71da8ef8",
            "patch": "@@ -115,16 +115,16 @@ absl::Status FixTupleTableAsync(se::Stream* stream,\n         if (!element_shape.IsTuple()) {\n           return absl::OkStatus();\n         }\n-        std::vector<se::DeviceMemoryBase> elements;\n+        std::vector<stream_executor::DeviceAddressBase> elements;\n         xla::ShapeIndex element_index = index;\n         element_index.push_back(0);\n         for (int i = 0; i < element_shape.tuple_shapes().size(); ++i) {\n           // Gather all children of the tuple element.\n           element_index.back() = i;\n-          elements.push_back(mem->Buffer(element_index).AsDeviceMemoryBase());\n+          elements.push_back(mem->Buffer(element_index).AsDeviceAddress());\n         }\n-        se::DeviceMemoryBase tuple_table_addr =\n-            mem->Buffer(index).AsDeviceMemoryBase();\n+        stream_executor::DeviceAddressBase tuple_table_addr =\n+            mem->Buffer(index).AsDeviceAddress();\n         return transfer_manager->WriteSingleTupleIndexTable(\n             stream, elements, element_shape, &tuple_table_addr);\n       });\n@@ -160,7 +160,7 @@ bool DynamicShapeIsCompatible(const xla::Shape& dynamic_shape,\n // Metadata contains the sizes of shape without padding, eventually\n // representing the size of valid data.\n absl::Status UpdateDynamicInputs(\n-    se::Stream* stream, se::DeviceMemoryAllocator* allocator,\n+    se::Stream* stream, stream_executor::DeviceAddressAllocator* allocator,\n     std::vector<xla::ExecutionInput>* runtime_inputs,\n     const std::vector<xla::Shape>& compile_time_shapes) {\n   TF_RET_CHECK(runtime_inputs->size() == compile_time_shapes.size());\n@@ -193,14 +193,15 @@ absl::Status UpdateDynamicInputs(\n           TF_RET_CHECK(\n               DynamicShapeIsCompatible(runtime_shape, compile_time_shape));\n \n-          xla::MaybeOwningDeviceMemory* mutable_input_mem =\n+          xla::MaybeOwningDeviceAddress* mutable_input_mem =\n               runtime_input.MutableBuffer(index);\n           auto padded_data = std::make_shared<std::vector<int8_t>>(\n               ShapeSizeCompact(compile_time_shape), -1);\n           auto raw_input_runtime = std::make_shared<std::vector<uint32_t>>(\n               ShapeSizeCompact(runtime_shape) / sizeof(uint32_t));\n           TF_RETURN_IF_ERROR(stream->MemcpyD2H(\n-              se::DeviceMemory<int8_t>(mutable_input_mem->AsDeviceMemoryBase()),\n+              stream_executor::DeviceAddress<int8_t>(\n+                  mutable_input_mem->AsDeviceAddress()),\n               absl::MakeSpan(absl::bit_cast<int8_t*>(raw_input_runtime->data()),\n                              ShapeSizeCompactRaw(runtime_shape))));\n           TF_RETURN_IF_ERROR(stream->DoHostCallbackWithStatus(\n@@ -239,7 +240,7 @@ absl::Status UpdateDynamicInputs(\n               allocator->Allocate(stream->parent()->device_ordinal(),\n                                   ShapeSizeCompact(compile_time_shape)));\n           auto typed_new_input_memory =\n-              se::DeviceMemory<int8_t>(new_input.cref());\n+              stream_executor::DeviceAddress<int8_t>(new_input.cref());\n           TF_RETURN_IF_ERROR(\n               stream->MemcpyH2D<int8_t>(*padded_data, &typed_new_input_memory));\n \n@@ -249,7 +250,7 @@ absl::Status UpdateDynamicInputs(\n           // Modify the memory location in the input shape tree to point to the\n           // new input.\n           *mutable_input_mem =\n-              xla::MaybeOwningDeviceMemory(std::move(new_input));\n+              xla::MaybeOwningDeviceAddress(std::move(new_input));\n           element_modified = true;\n           return absl::OkStatus();\n         }));\n@@ -499,7 +500,7 @@ absl::StatusOr<xla::ExecutionOutput> TPUExecute(\n   VLOG(1) << \"TPUExecute: Adding \" << device_memory_addrs_count\n           << \" TPUEmbedding memory addresses to HLO parameters.\";\n   for (int i = 0; i < device_memory_addrs_count; ++i) {\n-    xla::ShapeTree<xla::MaybeOwningDeviceMemory> tree(\n+    xla::ShapeTree<xla::MaybeOwningDeviceAddress> tree(\n         xla::ShapeUtil::MakeOpaqueShape());\n     const SE_DeviceAddressBase& addr = device_memory_addrs[i];\n     VLOG(2) << absl::StrFormat(\"Device memory addr[%i] = {%p, %llu, %llu}\", i,"
        }
    ],
    "stats": {
        "total": 23,
        "additions": 12,
        "deletions": 11
    }
}