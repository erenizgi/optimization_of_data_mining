{
    "author": "ezhulenev",
    "message": "[stream_executor] Move MemoryType to separate header and rename to MemorySpace\n\nPiperOrigin-RevId: 843533783",
    "sha": "ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
    "files": [
        {
            "sha": "b7336030ff9e37e056438ad3af5059e844389155",
            "filename": "third_party/xla/xla/stream_executor/BUILD",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2FBUILD?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -255,6 +255,7 @@ cc_library(\n         \":kernel_spec\",\n         \":memory_allocation\",\n         \":memory_allocator\",\n+        \":memory_space\",\n         \":module_spec\",\n         \":platform\",\n         \":stream\",\n@@ -484,9 +485,11 @@ cc_library(\n         \":kernel_spec\",\n         \":memory_allocation\",\n         \":memory_allocator\",\n+        \":memory_space\",\n         \":module_spec\",\n         \":platform\",\n         \":stream\",\n+        \":tensor_map\",\n         \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/lib/gtl:int_type\",\n         \"@com_google_absl//absl/base:core_headers\",\n@@ -518,6 +521,12 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"memory_space\",\n+    hdrs = [\"memory_space.h\"],\n+    deps = [\"@com_google_absl//absl/base:core_headers\"],\n+)\n+\n cc_library(\n     name = \"generic_memory_allocator\",\n     hdrs = [\"generic_memory_allocator.h\"],"
        },
        {
            "sha": "39e49d407abe47bb0515ef64d0bbac68093b10f4",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -1016,8 +1016,8 @@ absl::Status CollectiveMemoryDeallocate(StreamExecutor* executor,\n }\n \n absl::StatusOr<std::unique_ptr<MemoryAllocator>>\n-CudaExecutor::CreateMemoryAllocator(MemoryType type) {\n-  if (type == MemoryType::kUnified) {\n+CudaExecutor::CreateMemoryAllocator(MemorySpace type) {\n+  if (type == MemorySpace::kUnified) {\n     return std::make_unique<GenericMemoryAllocator>(\n         [this](uint64_t size)\n             -> absl::StatusOr<std::unique_ptr<MemoryAllocation>> {\n@@ -1049,7 +1049,7 @@ CudaExecutor::CreateMemoryAllocator(MemoryType type) {\n         });\n   }\n \n-  if (type == MemoryType::kCollective) {\n+  if (type == MemorySpace::kCollective) {\n     return std::make_unique<GenericMemoryAllocator>(\n         [this](uint64_t size)\n             -> absl::StatusOr<std::unique_ptr<MemoryAllocation>> {\n@@ -1073,7 +1073,7 @@ CudaExecutor::CreateMemoryAllocator(MemoryType type) {\n         });\n   }\n \n-  if (type == MemoryType::kHost) {\n+  if (type == MemorySpace::kHost) {\n     return std::make_unique<GenericMemoryAllocator>([this](uint64_t size) {\n       return AllocateHostMemory(cuda_context_, numa_node_, size);\n     });\n@@ -1408,7 +1408,7 @@ DeviceAddressBase CudaExecutor::Allocate(uint64_t size, int64_t memory_space) {\n       << \"CudaExecutor::Allocate size: \" << size\n       << \" memory_space: \" << memory_space;\n \n-  if (memory_space == static_cast<int64_t>(MemoryType::kCollective)) {\n+  if (memory_space == static_cast<int64_t>(MemorySpace::kCollective)) {\n     auto result = CollectiveMemoryAllocate(this, size);\n     if (!result.ok()) {\n       XLA_LOG_DEVICE(ERROR, device_ordinal())\n@@ -1419,7 +1419,7 @@ DeviceAddressBase CudaExecutor::Allocate(uint64_t size, int64_t memory_space) {\n     return DeviceAddressBase(result.value(), size);\n   }\n \n-  if (memory_space == static_cast<int64_t>(MemoryType::kHost)) {\n+  if (memory_space == static_cast<int64_t>(MemorySpace::kHost)) {\n     auto result = HostAllocate(cuda_context_, numa_node_, size);\n     if (!result.ok()) {\n       XLA_LOG_DEVICE(ERROR, device_ordinal())\n@@ -1431,7 +1431,7 @@ DeviceAddressBase CudaExecutor::Allocate(uint64_t size, int64_t memory_space) {\n     return DeviceAddressBase(result.value(), size);\n   }\n \n-  if (memory_space == static_cast<int64_t>(MemoryType::kP2P) &&\n+  if (memory_space == static_cast<int64_t>(MemorySpace::kP2P) &&\n       is_vmm_supported_) {\n     auto device_buf_base = VmmAllocateMemory(size);\n \n@@ -1445,8 +1445,8 @@ DeviceAddressBase CudaExecutor::Allocate(uint64_t size, int64_t memory_space) {\n     return DeviceAddressBase(nullptr, 0);\n   }\n \n-  CHECK(memory_space == static_cast<int64_t>(MemoryType::kDevice) ||\n-        memory_space == static_cast<int64_t>(MemoryType::kP2P));\n+  CHECK(memory_space == static_cast<int64_t>(MemorySpace::kDevice) ||\n+        memory_space == static_cast<int64_t>(MemorySpace::kP2P));\n \n   auto device_buf_base = DeviceAllocate(cuda_context_, size);\n   XLA_VLOG_DEVICE(1, device_ordinal())\n@@ -1469,7 +1469,7 @@ void CudaExecutor::Deallocate(DeviceAddressBase* mem) {\n     return;\n   }\n   auto memory_space = status_or_memory_space.value();\n-  if (memory_space == MemoryType::kHost) {\n+  if (memory_space == MemorySpace::kHost) {\n     HostDeallocate(cuda_context_, numa_node_, mem->opaque(), mem->size());\n   } else {\n     // Memory space is always kDevice here, so the only way to check if the\n@@ -1899,25 +1899,25 @@ CudaExecutor::CreateDeviceDescription(int device_ordinal) {\n   return std::make_unique<DeviceDescription>(std::move(desc));\n }\n \n-absl::StatusOr<MemoryType> CudaExecutor::GetPointerMemorySpace(\n+absl::StatusOr<MemorySpace> CudaExecutor::GetPointerMemorySpace(\n     const void* ptr) {\n   CUdeviceptr pointer = reinterpret_cast<CUdeviceptr>(const_cast<void*>(ptr));\n   unsigned int is_managed;\n   TF_RETURN_IF_ERROR(cuda::ToStatus(cuPointerGetAttribute(\n       &is_managed, CU_POINTER_ATTRIBUTE_IS_MANAGED, pointer)));\n \n   if (is_managed) {\n-    return MemoryType::kUnified;\n+    return MemorySpace::kUnified;\n   }\n \n   unsigned int value;\n   TF_RETURN_IF_ERROR(cuda::ToStatus(cuPointerGetAttribute(\n       &value, CU_POINTER_ATTRIBUTE_MEMORY_TYPE, pointer)));\n   switch (value) {\n     case CU_MEMORYTYPE_DEVICE:\n-      return MemoryType::kDevice;\n+      return MemorySpace::kDevice;\n     case CU_MEMORYTYPE_HOST:\n-      return MemoryType::kHost;\n+      return MemorySpace::kHost;\n     default:\n       return absl::InternalError(\n           absl::StrCat(\"unknown memory space provided by CUDA API: \", value));"
        },
        {
            "sha": "ff9c0c3d49a165fab2575d7aa5c1a43ab20b8b50",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -114,7 +114,7 @@ class CudaExecutor : public GpuExecutor {\n   bool HostMemoryRegister(void* location, uint64_t size) override;\n   bool HostMemoryUnregister(void* location) override;\n \n-  absl::StatusOr<MemoryType> GetPointerMemorySpace(const void* ptr) override;\n+  absl::StatusOr<MemorySpace> GetPointerMemorySpace(const void* ptr) override;\n \n   Stream* FindAllocatedStream(void* gpu_stream) override {\n     absl::MutexLock lock(alive_gpu_streams_mu_);\n@@ -138,7 +138,7 @@ class CudaExecutor : public GpuExecutor {\n   absl::StatusOr<TensorMap> CreateTensorMap(const TmaDescriptor& tma_desc,\n                                             void* global_address) override;\n   absl::StatusOr<std::unique_ptr<MemoryAllocator>> CreateMemoryAllocator(\n-      MemoryType type) override;\n+      MemorySpace type) override;\n \n   // Returns the granularity which is the minimum unit of memory that can be\n   // allocated with VMM API. In order to map the memory slices to multicast"
        },
        {
            "sha": "9ad1336dc5343face88d3d1840f58158bc01fd65",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor_multigpu_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_multigpu_test.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -45,7 +45,7 @@ template <typename T>\n absl::StatusOr<stream_executor::DeviceAddressBase> AllocateInitializedMemory(\n     CudaExecutor* executor, size_t size, size_t offset, T value) {\n   stream_executor::DeviceAddressBase device_memory = executor->Allocate(\n-      size + offset, static_cast<int64_t>(stream_executor::MemoryType::kP2P));\n+      size + offset, static_cast<int64_t>(stream_executor::MemorySpace::kP2P));\n   if (device_memory.opaque() == nullptr) {\n     return absl::InternalError(\"Failed to allocate memory.\");\n   }"
        },
        {
            "sha": "8b6c6ea3491fe984acce6d4aa4e1b03ee93578e6",
            "filename": "third_party/xla/xla/stream_executor/cuda/cuda_executor_test.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fcuda%2Fcuda_executor_test.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -115,7 +115,7 @@ TEST(CudaExecutorTest, CreateUnifiedMemoryAllocatorWorks) {\n                           platform->ExecutorForDevice(0));\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<MemoryAllocator> allocator,\n-      executor->CreateMemoryAllocator(MemoryType::kUnified));\n+      executor->CreateMemoryAllocator(MemorySpace::kUnified));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<MemoryAllocation> allocation,\n                           allocator->Allocate(1024));\n   EXPECT_NE(allocation->opaque(), nullptr);\n@@ -128,7 +128,7 @@ TEST(CudaExecutorTest, CreateHostMemoryAllocatorWorks) {\n   TF_ASSERT_OK_AND_ASSIGN(StreamExecutor * executor,\n                           platform->ExecutorForDevice(0));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<MemoryAllocator> allocator,\n-                          executor->CreateMemoryAllocator(MemoryType::kHost));\n+                          executor->CreateMemoryAllocator(MemorySpace::kHost));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<MemoryAllocation> allocation,\n                           allocator->Allocate(1024));\n   EXPECT_NE(allocation->opaque(), nullptr);\n@@ -142,7 +142,7 @@ TEST(CudaExecutorTest, CreateCollectiveMemoryAllocatorWorks) {\n                           platform->ExecutorForDevice(0));\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<MemoryAllocator> allocator,\n-      executor->CreateMemoryAllocator(MemoryType::kCollective));\n+      executor->CreateMemoryAllocator(MemorySpace::kCollective));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<MemoryAllocation> allocation,\n                           allocator->Allocate(1024));\n   EXPECT_NE(allocation->opaque(), nullptr);\n@@ -158,7 +158,7 @@ TEST(CudaExecutorTest,\n                           platform->ExecutorForDevice(0));\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<MemoryAllocator> allocator,\n-      executor->CreateMemoryAllocator(MemoryType::kCollective));\n+      executor->CreateMemoryAllocator(MemorySpace::kCollective));\n   constexpr uint64_t kTooBig = 1125899906842624;  // 1 PiB\n   EXPECT_THAT(\n       allocator->Allocate(kTooBig),\n@@ -173,7 +173,7 @@ TEST(CudaExecutorTest, CreateUnsupportedMemoryAllocatorsFail) {\n                           PlatformManager::PlatformWithName(\"CUDA\"));\n   TF_ASSERT_OK_AND_ASSIGN(StreamExecutor * executor,\n                           platform->ExecutorForDevice(0));\n-  EXPECT_THAT(executor->CreateMemoryAllocator(MemoryType::kDevice),\n+  EXPECT_THAT(executor->CreateMemoryAllocator(MemorySpace::kDevice),\n               Not(absl_testing::IsOk()));\n }\n \n@@ -185,12 +185,12 @@ TEST(CudaExecutorTest, GetPointerMemorySpaceWorksWithUnifiedMemory) {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       auto unified_memory_allocator,\n-      executor->CreateMemoryAllocator(MemoryType::kUnified));\n+      executor->CreateMemoryAllocator(MemorySpace::kUnified));\n \n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<MemoryAllocation> allocation,\n                           unified_memory_allocator->Allocate(256));\n   EXPECT_THAT(executor->GetPointerMemorySpace(allocation->opaque()),\n-              absl_testing::IsOkAndHolds(MemoryType::kUnified));\n+              absl_testing::IsOkAndHolds(MemorySpace::kUnified));\n }\n \n TEST(CudaExecutorTest, GetPointerMemorySpaceWorksWithHostMemory) {\n@@ -202,7 +202,7 @@ TEST(CudaExecutorTest, GetPointerMemorySpaceWorksWithHostMemory) {\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<MemoryAllocation> allocation,\n                           executor->HostMemoryAllocate(256));\n   EXPECT_THAT(executor->GetPointerMemorySpace(allocation->opaque()),\n-              absl_testing::IsOkAndHolds(MemoryType::kHost));\n+              absl_testing::IsOkAndHolds(MemorySpace::kHost));\n }\n \n TEST(CudaExecutorTest, GetPointerMemorySpaceWorksWithDeviceAddress) {\n@@ -214,7 +214,7 @@ TEST(CudaExecutorTest, GetPointerMemorySpaceWorksWithDeviceAddress) {\n   DeviceAddressBase allocation = executor->Allocate(256);\n   EXPECT_NE(allocation.opaque(), nullptr);\n   EXPECT_THAT(executor->GetPointerMemorySpace(allocation.opaque()),\n-              absl_testing::IsOkAndHolds(MemoryType::kDevice));\n+              absl_testing::IsOkAndHolds(MemorySpace::kDevice));\n }\n \n TEST(CudaExecutorTest, AllocateMemoryWithVmmApi) {\n@@ -226,12 +226,12 @@ TEST(CudaExecutorTest, AllocateMemoryWithVmmApi) {\n   auto cuda_executor = dynamic_cast<CudaExecutor*>(executor);\n   ASSERT_NE(cuda_executor, nullptr);\n   DeviceAddressBase ptr =\n-      cuda_executor->Allocate(1024, static_cast<int>(MemoryType::kP2P));\n+      cuda_executor->Allocate(1024, static_cast<int>(MemorySpace::kP2P));\n \n   EXPECT_NE(ptr.opaque(), nullptr);\n   EXPECT_EQ(ptr.size(), 1024);\n   EXPECT_THAT(executor->GetPointerMemorySpace(ptr.opaque()),\n-              absl_testing::IsOkAndHolds(MemoryType::kDevice));\n+              absl_testing::IsOkAndHolds(MemorySpace::kDevice));\n \n   TF_ASSERT_OK_AND_ASSIGN(CudaExecutor::VmmMemoryHandle handle,\n                           cuda_executor->RetainVmmMemoryHandle(ptr.opaque()));\n@@ -248,7 +248,7 @@ TEST(CudaExecutorTest,\n   auto cuda_executor = dynamic_cast<CudaExecutor*>(executor);\n   ASSERT_NE(cuda_executor, nullptr);\n   DeviceAddressBase ptr =\n-      cuda_executor->Allocate(1024, static_cast<int>(MemoryType::kDevice));\n+      cuda_executor->Allocate(1024, static_cast<int>(MemorySpace::kDevice));\n \n   EXPECT_NE(ptr.opaque(), nullptr);\n   EXPECT_EQ(ptr.size(), 1024);"
        },
        {
            "sha": "fabea8c509c04f49935053e2d0f340c7842c3713",
            "filename": "third_party/xla/xla/stream_executor/gpu/gpu_executor_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_executor_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_executor_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fgpu%2Fgpu_executor_test.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -44,17 +44,17 @@ TEST_F(GetPointerMemorySpaceTest, Host) {\n   TF_ASSERT_OK_AND_ASSIGN(auto host_ptr, executor->HostMemoryAllocate(64));\n   TF_ASSERT_OK_AND_ASSIGN(auto memory_space,\n                           executor->GetPointerMemorySpace(host_ptr->opaque()));\n-  EXPECT_EQ(memory_space, MemoryType::kHost);\n+  EXPECT_EQ(memory_space, MemorySpace::kHost);\n }\n \n TEST_F(GetPointerMemorySpaceTest, HostAllocatedWithMemoryKind) {\n   StreamExecutor* executor = GetPlatform()->ExecutorForDevice(0).value();\n   DeviceAddressBase host_ptr = executor->Allocate(\n-      64, static_cast<int64_t>(stream_executor::MemoryType::kHost));\n+      64, static_cast<int64_t>(stream_executor::MemorySpace::kHost));\n   EXPECT_FALSE(host_ptr.is_null());\n-  TF_ASSERT_OK_AND_ASSIGN(MemoryType memory_space,\n+  TF_ASSERT_OK_AND_ASSIGN(MemorySpace memory_space,\n                           executor->GetPointerMemorySpace(host_ptr.opaque()));\n-  EXPECT_EQ(memory_space, MemoryType::kHost);\n+  EXPECT_EQ(memory_space, MemorySpace::kHost);\n   executor->Deallocate(&host_ptr);\n }\n \n@@ -64,7 +64,7 @@ TEST_F(GetPointerMemorySpaceTest, Device) {\n   ASSERT_NE(mem, nullptr);\n   TF_ASSERT_OK_AND_ASSIGN(auto memory_space,\n                           executor->GetPointerMemorySpace(mem.opaque()));\n-  EXPECT_EQ(memory_space, MemoryType::kDevice);\n+  EXPECT_EQ(memory_space, MemorySpace::kDevice);\n   executor->Deallocate(&mem);\n }\n "
        },
        {
            "sha": "151d7b51306d2434746a35dad5a4144f8ae683cf",
            "filename": "third_party/xla/xla/stream_executor/host/host_executor.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2Fhost_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2Fhost_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2Fhost_executor.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -148,8 +148,8 @@ absl::StatusOr<std::unique_ptr<Stream>> HostExecutor::CreateStream(\n }\n \n absl::StatusOr<std::unique_ptr<MemoryAllocator>>\n-HostExecutor::CreateMemoryAllocator(MemoryType type) {\n-  if (type == MemoryType::kHost) {\n+HostExecutor::CreateMemoryAllocator(MemorySpace type) {\n+  if (type == MemorySpace::kHost) {\n     return std::make_unique<GenericMemoryAllocator>(\n         [](uint64_t size) -> absl::StatusOr<std::unique_ptr<MemoryAllocation>> {\n           void* ptr = new char[size];"
        },
        {
            "sha": "69e40a59a880bf53ed681d296b865d36c883df49",
            "filename": "third_party/xla/xla/stream_executor/host/host_executor.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2Fhost_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2Fhost_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fhost%2Fhost_executor.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -101,7 +101,7 @@ class HostExecutor : public StreamExecutorCommon {\n   absl::StatusOr<std::unique_ptr<Stream>> CreateStream(\n       std::optional<std::variant<StreamPriority, int>> priority) override;\n   absl::StatusOr<std::unique_ptr<MemoryAllocator>> CreateMemoryAllocator(\n-      MemoryType type) override;\n+      MemorySpace type) override;\n \n  private:\n   int device_ordinal_;"
        },
        {
            "sha": "c49e1b17c61e4998a857efa76570ec86b7a8d926",
            "filename": "third_party/xla/xla/stream_executor/integrations/stream_executor_allocator.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Fstream_executor_allocator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Fstream_executor_allocator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Fstream_executor_allocator.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -31,24 +31,24 @@ limitations under the License.\n namespace stream_executor {\n \n StreamExecutorAllocator::StreamExecutorAllocator(\n-    std::unique_ptr<MemoryAllocator> memory_allocator, MemoryType memory_type,\n+    std::unique_ptr<MemoryAllocator> memory_allocator, MemorySpace memory_type,\n     int index, const std::vector<Visitor>& alloc_visitors,\n     const std::vector<Visitor>& free_visitors)\n     : tsl::SubAllocator(alloc_visitors, free_visitors),\n       memory_allocator_(std::move(memory_allocator)),\n       memory_type_(memory_type),\n       index_(index) {}\n \n-// Converts MemoryType to a human-readable string for allocation error messages\n-static absl::string_view MemoryTypeToString(MemoryType type) {\n+// Converts MemorySpace to a human-readable string for allocation error messages\n+static absl::string_view MemorySpaceToString(MemorySpace type) {\n   switch (type) {\n-    case MemoryType::kDevice:\n+    case MemorySpace::kDevice:\n       return \"device\";\n-    case MemoryType::kUnified:\n+    case MemorySpace::kUnified:\n       return \"unified\";\n-    case MemoryType::kHost:\n+    case MemorySpace::kHost:\n       return \"pinned host\";\n-    case MemoryType::kCollective:\n+    case MemorySpace::kCollective:\n       return \"collective\";\n     default:\n       return \"unknown\";\n@@ -64,7 +64,7 @@ void* StreamExecutorAllocator::Alloc(size_t alignment, size_t num_bytes,\n   if (num_bytes > 0) {\n     auto allocation = memory_allocator_->Allocate(num_bytes);\n     if (!allocation.ok()) {\n-      LOG(WARNING) << \"could not allocate \" << MemoryTypeToString(memory_type_)\n+      LOG(WARNING) << \"could not allocate \" << MemorySpaceToString(memory_type_)\n                    << \" of size: \" << num_bytes;\n       *bytes_received = 0;\n       return nullptr;\n@@ -95,7 +95,7 @@ void StreamExecutorAllocator::Free(void* ptr, size_t num_bytes) {\n bool StreamExecutorAllocator::SupportsCoalescing() const { return false; }\n \n tsl::AllocatorMemoryType StreamExecutorAllocator::GetMemoryType() const {\n-  if (memory_type_ == MemoryType::kHost) {\n+  if (memory_type_ == MemorySpace::kHost) {\n     return tsl::AllocatorMemoryType::kHostPinned;\n   } else {\n     return tsl::AllocatorMemoryType::kDevice;"
        },
        {
            "sha": "8b104ca784c66ec9c9c892c11e7fe1358d4e6bac",
            "filename": "third_party/xla/xla/stream_executor/integrations/stream_executor_allocator.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Fstream_executor_allocator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Fstream_executor_allocator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Fstream_executor_allocator.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -35,7 +35,7 @@ namespace stream_executor {\n class StreamExecutorAllocator : public tsl::SubAllocator {\n  public:\n   StreamExecutorAllocator(std::unique_ptr<MemoryAllocator> memory_allocator,\n-                          MemoryType memory_type, int index,\n+                          MemorySpace memory_type, int index,\n                           const std::vector<Visitor>& alloc_visitors = {},\n                           const std::vector<Visitor>& free_visitors = {});\n \n@@ -48,7 +48,7 @@ class StreamExecutorAllocator : public tsl::SubAllocator {\n \n  private:\n   std::unique_ptr<MemoryAllocator> memory_allocator_;\n-  MemoryType memory_type_;\n+  MemorySpace memory_type_;\n   int index_;\n \n   StreamExecutorAllocator(const StreamExecutorAllocator&) = delete;"
        },
        {
            "sha": "8a40b3b8c796c1a90c1c81dae8f1ea7cb3803a7b",
            "filename": "third_party/xla/xla/stream_executor/integrations/stream_executor_allocator_test.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Fstream_executor_allocator_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Fstream_executor_allocator_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fintegrations%2Fstream_executor_allocator_test.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -39,7 +39,7 @@ TEST(StreamExecutorAllocatorTest, NoMemoryReturnsNullptr) {\n       });\n \n   StreamExecutorAllocator stream_executor_allocator(\n-      std::move(allocator), MemoryType::kHost, /*index=*/0,\n+      std::move(allocator), MemorySpace::kHost, /*index=*/0,\n       /*alloc_visitors=*/{},\n       /*free_visitors=*/{});\n   size_t bytes_received = 0;\n@@ -55,7 +55,7 @@ TEST(StreamExecutorAllocatorTest, DoesntSupportCoalescing) {\n         return absl::InternalError(\"Failed to allocate memory\");\n       });\n   StreamExecutorAllocator stream_executor_allocator(\n-      std::move(allocator), MemoryType::kHost, /*index=*/0,\n+      std::move(allocator), MemorySpace::kHost, /*index=*/0,\n       /*alloc_visitors=*/{},\n       /*free_visitors=*/{});\n   EXPECT_FALSE(stream_executor_allocator.SupportsCoalescing());\n@@ -67,7 +67,7 @@ TEST(StreamExecutorAllocatorTest, GetMemoryTypeReturnsHostPinnedForHostMemory) {\n         return absl::InternalError(\"Failed to allocate memory\");\n       });\n   StreamExecutorAllocator stream_executor_allocator(\n-      std::move(allocator), MemoryType::kHost, /*index=*/0,\n+      std::move(allocator), MemorySpace::kHost, /*index=*/0,\n       /*alloc_visitors=*/{},\n       /*free_visitors=*/{});\n   EXPECT_EQ(tsl::AllocatorMemoryType::kHostPinned,\n@@ -80,7 +80,7 @@ TEST(StreamExecutorAllocatorTest, GetMemoryTypeReturnsDeviceForDeviceAddress) {\n         return absl::InternalError(\"Failed to allocate memory\");\n       });\n   StreamExecutorAllocator stream_executor_allocator(\n-      std::move(allocator), MemoryType::kDevice, /*index=*/0,\n+      std::move(allocator), MemorySpace::kDevice, /*index=*/0,\n       /*alloc_visitors=*/{},\n       /*free_visitors=*/{});\n   EXPECT_EQ(tsl::AllocatorMemoryType::kDevice,\n@@ -122,7 +122,7 @@ TEST(StreamExecutorAllocatorTest,\n     free_visitor_called = true;\n   };\n   StreamExecutorAllocator stream_executor_allocator(\n-      std::move(allocator), MemoryType::kDevice, /*index=*/0, {alloc_visitor},\n+      std::move(allocator), MemorySpace::kDevice, /*index=*/0, {alloc_visitor},\n       {free_visitor});\n   EXPECT_FALSE(free_visitor_called);\n   EXPECT_FALSE(alloc_visitor_called);"
        },
        {
            "sha": "3aed5aac37ea44d2c3b3ca637829ce5232bd45d5",
            "filename": "third_party/xla/xla/stream_executor/memory_allocator.h",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fmemory_allocator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fmemory_allocator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fmemory_allocator.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -24,7 +24,12 @@ limitations under the License.\n \n namespace stream_executor {\n \n-// This class defines the interface for memory allocators.\n+// A base class for stream executor memory allocators.\n+//\n+// Memory allocators are responsible allocating physical memory for a given\n+// stream executor, this physical memory might reside in different memory spaces\n+// such as device memory, unified memory, host memory, etc. See MemoryAllocation\n+// documentation for more details.\n class MemoryAllocator {\n  public:\n   virtual ~MemoryAllocator() = default;"
        },
        {
            "sha": "251dc9bbd5b3393ba809bc9fb7d4291fa0351c47",
            "filename": "third_party/xla/xla/stream_executor/memory_space.h",
            "status": "added",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fmemory_space.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fmemory_space.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fmemory_space.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -0,0 +1,38 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_STREAM_EXECUTOR_MEMORY_SPACE_H_\n+#define XLA_STREAM_EXECUTOR_MEMORY_SPACE_H_\n+\n+#include <cstdint>\n+\n+#include \"absl/base/macros.h\"\n+\n+namespace stream_executor {\n+\n+// Identifies the memory space where a physical allocation resides.\n+enum class MemorySpace : uint8_t {\n+  kDevice = 0,\n+  kUnified,\n+  kCollective,\n+  kP2P,\n+  kHost = 5,\n+};\n+\n+using MemoryType ABSL_DEPRECATE_AND_INLINE() = MemorySpace;\n+\n+}  // namespace stream_executor\n+\n+#endif  // XLA_STREAM_EXECUTOR_MEMORY_SPACE_H_"
        },
        {
            "sha": "589a590e65c36e31c04210df7577ec646e14a5d5",
            "filename": "third_party/xla/xla/stream_executor/mock_stream_executor.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fmock_stream_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fmock_stream_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fmock_stream_executor.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -40,6 +40,7 @@ limitations under the License.\n #include \"xla/stream_executor/kernel_spec.h\"\n #include \"xla/stream_executor/memory_allocation.h\"\n #include \"xla/stream_executor/memory_allocator.h\"\n+#include \"xla/stream_executor/memory_space.h\"\n #include \"xla/stream_executor/module_spec.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream.h\"\n@@ -112,7 +113,7 @@ class MockStreamExecutor : public StreamExecutor {\n               CreateEventBasedTimer, (Stream * stream, bool use_delay_kernel),\n               (override));\n   MOCK_METHOD(absl::StatusOr<std::unique_ptr<MemoryAllocator>>,\n-              CreateMemoryAllocator, (MemoryType type), (override));\n+              CreateMemoryAllocator, (MemorySpace memory_space), (override));\n };\n \n }  // namespace stream_executor"
        },
        {
            "sha": "7d3d2c8ebaf4596b79af3c4cf1a00509cd8f738f",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_executor.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -779,20 +779,20 @@ absl::StatusOr<ModuleHandle> RocmExecutor::LoadModuleFromHsaco(\n }\n \n DeviceAddressBase RocmExecutor::Allocate(uint64_t size, int64_t memory_space) {\n-  switch (static_cast<MemoryType>(memory_space)) {\n-    case MemoryType::kCollective:\n-    case MemoryType::kDevice:\n+  switch (static_cast<MemorySpace>(memory_space)) {\n+    case MemorySpace::kCollective:\n+    case MemorySpace::kDevice:\n       return DeviceAddressBase(\n           DeviceAllocate(rocm_context_, size, /*is_fine_grained*/ false), size);\n-    case MemoryType::kP2P:\n+    case MemorySpace::kP2P:\n       // On the ROCm platform, differences in cache design (e.g., coherence\n       // protocol) can cause cache coherence issues for some archs (e.g., MI200)\n       // when using normal device memory. To avoid these problems, we use\n       // fine-grained memory in P2P communication for all archs to make sure of\n       // the correctness.\n       return DeviceAddressBase(\n           DeviceAllocate(rocm_context_, size, /*is_fine_grained*/ true), size);\n-    case MemoryType::kHost:\n+    case MemorySpace::kHost:\n       if (auto result = HostAllocate(rocm_context_, size); result.ok()) {\n         return DeviceAddressBase(*result, size);\n       }\n@@ -811,9 +811,9 @@ void RocmExecutor::Deallocate(DeviceAddressBase* mem) {\n }\n \n absl::StatusOr<std::unique_ptr<MemoryAllocator>>\n-RocmExecutor::CreateMemoryAllocator(MemoryType type) {\n+RocmExecutor::CreateMemoryAllocator(MemorySpace type) {\n   switch (type) {\n-    case MemoryType::kUnified:\n+    case MemorySpace::kUnified:\n       return std::make_unique<GenericMemoryAllocator>(\n           [this](uint64_t size)\n               -> absl::StatusOr<std::unique_ptr<MemoryAllocation>> {\n@@ -841,7 +841,7 @@ RocmExecutor::CreateMemoryAllocator(MemoryType type) {\n                   }\n                 });\n           });\n-    case MemoryType::kCollective:\n+    case MemorySpace::kCollective:\n       return std::make_unique<GenericMemoryAllocator>(\n           [](uint64_t size)\n               -> absl::StatusOr<std::unique_ptr<MemoryAllocation>> {\n@@ -868,7 +868,7 @@ RocmExecutor::CreateMemoryAllocator(MemoryType type) {\n                   }\n                 });\n           });\n-    case MemoryType::kHost:\n+    case MemorySpace::kHost:\n       return std::make_unique<GenericMemoryAllocator>([this](uint64_t size) {\n         return AllocateHostMemory(rocm_context_, size);\n       });\n@@ -1242,7 +1242,7 @@ RocmExecutor::CreateDeviceDescription(int device_ordinal) {\n   return std::make_unique<DeviceDescription>(std::move(desc));\n }\n \n-absl::StatusOr<MemoryType> RocmExecutor::GetPointerMemorySpace(\n+absl::StatusOr<MemorySpace> RocmExecutor::GetPointerMemorySpace(\n     const void* ptr) {\n   hipDeviceptr_t pointer =\n       reinterpret_cast<hipDeviceptr_t>(const_cast<void*>(ptr));\n@@ -1252,9 +1252,9 @@ absl::StatusOr<MemoryType> RocmExecutor::GetPointerMemorySpace(\n   if (result == hipSuccess) {\n     switch (value) {\n       case hipMemoryTypeDevice:\n-        return MemoryType::kDevice;\n+        return MemorySpace::kDevice;\n       case hipMemoryTypeHost:\n-        return MemoryType::kHost;\n+        return MemorySpace::kHost;\n       default:\n         return absl::InternalError(\n             absl::StrCat(\"unknown memory space provided by ROCM API: \", value));"
        },
        {
            "sha": "cbf064795206c64802c91c1ed3fed9cc10ef153d",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_executor.h",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -113,7 +113,7 @@ class RocmExecutor : public GpuExecutor {\n   bool HostMemoryRegister(void* location, uint64_t size) override;\n   bool HostMemoryUnregister(void* location) override;\n \n-  absl::StatusOr<MemoryType> GetPointerMemorySpace(const void* ptr) override;\n+  absl::StatusOr<MemorySpace> GetPointerMemorySpace(const void* ptr) override;\n \n   Stream* FindAllocatedStream(void* gpu_stream) override {\n     absl::MutexLock lock(alive_gpu_streams_mu_);\n@@ -131,7 +131,7 @@ class RocmExecutor : public GpuExecutor {\n   // associated with this executor. Otherwise a NotFound error is returned.\n   absl::StatusOr<const RocmKernel*> GetRocmKernel(const Kernel* kernel);\n   absl::StatusOr<std::unique_ptr<MemoryAllocator>> CreateMemoryAllocator(\n-      MemoryType type) override;\n+      MemorySpace type) override;\n \n   int GetGpuStreamPriority(StreamPriority priority) override;\n "
        },
        {
            "sha": "60e1e72cd4b657a9cdc664cbb0b8bdc8185682cb",
            "filename": "third_party/xla/xla/stream_executor/rocm/rocm_executor_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Frocm%2Frocm_executor_test.cc?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -90,7 +90,7 @@ TEST(RocmExecutorTest, CreateUnifiedMemoryAllocatorWorks) {\n                           platform->ExecutorForDevice(0));\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<MemoryAllocator> allocator,\n-      executor->CreateMemoryAllocator(MemoryType::kUnified));\n+      executor->CreateMemoryAllocator(MemorySpace::kUnified));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<MemoryAllocation> allocation,\n                           allocator->Allocate(1024));\n   EXPECT_NE(allocation->opaque(), nullptr);\n@@ -104,7 +104,7 @@ TEST(RocmExecutorTest, CreateHostMemoryAllocatorWorks) {\n   TF_ASSERT_OK_AND_ASSIGN(StreamExecutor * executor,\n                           platform->ExecutorForDevice(0));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<MemoryAllocator> allocator,\n-                          executor->CreateMemoryAllocator(MemoryType::kHost));\n+                          executor->CreateMemoryAllocator(MemorySpace::kHost));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<MemoryAllocation> allocation,\n                           allocator->Allocate(1024));\n   EXPECT_NE(allocation->opaque(), nullptr);\n@@ -119,7 +119,7 @@ TEST(RocmExecutorTest, CreateCollectiveMemoryAllocatorWorks) {\n                           platform->ExecutorForDevice(0));\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<MemoryAllocator> allocator,\n-      executor->CreateMemoryAllocator(MemoryType::kCollective));\n+      executor->CreateMemoryAllocator(MemorySpace::kCollective));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<MemoryAllocation> allocation,\n                           allocator->Allocate(1024));\n   EXPECT_NE(allocation->opaque(), nullptr);\n@@ -132,7 +132,7 @@ TEST(RocmExecutorTest, CreateUnsupportedMemoryAllocatorsFail) {\n                           PlatformManager::PlatformWithName(\"ROCM\"));\n   TF_ASSERT_OK_AND_ASSIGN(StreamExecutor * executor,\n                           platform->ExecutorForDevice(0));\n-  EXPECT_THAT(executor->CreateMemoryAllocator(MemoryType::kDevice),\n+  EXPECT_THAT(executor->CreateMemoryAllocator(MemorySpace::kDevice),\n               Not(absl_testing::IsOk()));\n }\n "
        },
        {
            "sha": "045d30aefe1aba61b56cf23df48743b9739cafba",
            "filename": "third_party/xla/xla/stream_executor/stream_executor.h",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fstream_executor.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/ff5a8e6365ce800ea0aab07373ee7c08acfa77e3/third_party%2Fxla%2Fxla%2Fstream_executor%2Fstream_executor.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fstream_executor%2Fstream_executor.h?ref=ff5a8e6365ce800ea0aab07373ee7c08acfa77e3",
            "patch": "@@ -46,19 +46,18 @@ limitations under the License.\n #include \"xla/stream_executor/kernel_spec.h\"\n #include \"xla/stream_executor/memory_allocation.h\"\n #include \"xla/stream_executor/memory_allocator.h\"\n+#include \"xla/stream_executor/memory_space.h\"\n #include \"xla/stream_executor/module_spec.h\"\n #include \"xla/stream_executor/platform.h\"\n #include \"xla/stream_executor/stream.h\"\n+#include \"xla/stream_executor/tensor_map.h\"\n #include \"xla/tsl/lib/gtl/int_type.h\"\n \n // TODO(ezhulenev): Remove this once transitive dependencies are fixed.\n #include \"xla/stream_executor/device_memory.h\"\n \n namespace stream_executor {\n \n-// Identifies the memory space where an allocation resides.\n-enum class MemoryType { kDevice = 0, kUnified, kCollective, kP2P, kHost = 5 };\n-\n /// The StreamExecutor is a single-device abstraction for:\n //\n // * Loading/launching data-parallel-kernels\n@@ -109,7 +108,7 @@ class StreamExecutor {\n \n   // Creates a MemoryAllocator for the given type.\n   virtual absl::StatusOr<std::unique_ptr<MemoryAllocator>>\n-  CreateMemoryAllocator(MemoryType type) {\n+  CreateMemoryAllocator(MemorySpace memory_space) {\n     return absl::UnimplementedError(\"Not Implemented\");\n   }\n \n@@ -179,7 +178,7 @@ class StreamExecutor {\n       uint64_t size) = 0;\n \n   // Returns the memory space of the given pointer.\n-  virtual absl::StatusOr<MemoryType> GetPointerMemorySpace(const void* ptr) {\n+  virtual absl::StatusOr<MemorySpace> GetPointerMemorySpace(const void* ptr) {\n     return absl::UnimplementedError(\"Not implemented\");\n   }\n "
        }
    ],
    "stats": {
        "total": 208,
        "additions": 130,
        "deletions": 78
    }
}