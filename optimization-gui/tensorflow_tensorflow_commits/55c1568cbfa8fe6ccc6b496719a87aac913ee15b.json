{
    "author": "tensorflower-gardener",
    "message": "Fixes a bug in which rematerialization that does not change the peak instruction would cause peak priority remat to hit a dead end. Correctly propagates wether the peak memory or peak instruction has changed inside a subpass, and skips dead instructions during block selection, to avoid premature termination of block selection. Increases the VLOG level of a peak priority mode.\n\nPiperOrigin-RevId: 802579399",
    "sha": "55c1568cbfa8fe6ccc6b496719a87aac913ee15b",
    "files": [
        {
            "sha": "f5dacc832824cf4489f55c070fc4444f47508da2",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55c1568cbfa8fe6ccc6b496719a87aac913ee15b/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55c1568cbfa8fe6ccc6b496719a87aac913ee15b/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2FBUILD?ref=55c1568cbfa8fe6ccc6b496719a87aac913ee15b",
            "patch": "@@ -949,7 +949,6 @@ xla_cc_test(\n         \"//xla/service:buffer_value\",\n         \"//xla/service:hlo_cost_analysis\",\n         \"//xla/tsl/lib/core:status_test_util\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\","
        },
        {
            "sha": "22ee78c284f79960b194a9a1d40b44c6566ae73a",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc",
            "status": "modified",
            "additions": 28,
            "deletions": 13,
            "changes": 41,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55c1568cbfa8fe6ccc6b496719a87aac913ee15b/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fhlo_rematerialization.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55c1568cbfa8fe6ccc6b496719a87aac913ee15b/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fhlo_rematerialization.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fhlo_rematerialization.cc?ref=55c1568cbfa8fe6ccc6b496719a87aac913ee15b",
            "patch": "@@ -1591,6 +1591,11 @@ MemoryUsageTracker::PickRematerializationCandidates(\n   for (auto* start_item = instruction_list.first_skip_node();\n        start_item != nullptr;\n        start_item = instruction_list.next_skip_node(start_item)) {\n+    if (start_item->instruction->IsDead()) {\n+      // This should only happen in peak priority mode because it does not run a\n+      // DCE pass to remove dead instructions in between certain remat calls.\n+      continue;\n+    }\n     std::vector<HloRematItem*> block =\n         GetInitialBlock(instruction_list, *this, start_item, min_block_size);\n     if (block.size() < min_block_size) {\n@@ -2505,8 +2510,7 @@ RematPeakAggressively(\n               << max_block_size;\n     }\n     if (instructions_added.remat_count > 0) {\n-      VLOG(2) << \"Instructions were rematerialized, readjusting schedule\";\n-\n+      VLOG(2) << \"Instructions were rematerialized\";\n       // Found a valid block. Reset to start looking for single\n       // instructions again.\n       remat->UpdateMaxRematerializedBlockSize(max_block_size);\n@@ -2673,20 +2677,28 @@ HloRematerialization::PeakPrioritySubPass(\n \n   // Update peak memory used by computation.\n   computation_peak_memory_.at(computation) = peak_memory_during_remat;\n+  RematSubpassResult remat_subpass_result{\n+      // NOLINTNEXTLINE (-Wpre-c++20-compat-pedantic)\n+      .status = RematSubpassStatus::kUnchanged,\n+      .peak_memory_during_remat = peak_memory_during_remat,\n+      .peak_memory_instruction = peak_memory_instruction,\n+  };\n   if (module_changed_in_this_subpass && over_memory_limit) {\n-    return RematSubpassResult::kChangedButOverMemoryLimit;\n+    remat_subpass_result.status =\n+        RematSubpassStatus::kChangedButOverMemoryLimit;\n   }\n   if (module_changed_in_this_subpass && !over_memory_limit) {\n-    return RematSubpassResult::kChangedAndUnderMemoryLimit;\n+    remat_subpass_result.status =\n+        RematSubpassStatus::kChangedAndUnderMemoryLimit;\n   }\n-  return RematSubpassResult::kUnchanged;\n+  return remat_subpass_result;\n }\n \n absl::StatusOr<bool> HloRematerialization::RematerializeComputationPeakPriority(\n     HloComputation* computation, HloSchedule* schedule,\n     int64_t memory_limit_bytes, int64_t min_remat_size,\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n-  VLOG(2) << \"Rematerializing Using Peak Priority\";\n+  VLOG(1) << \"Rematerializing Using Peak Priority\";\n   // If memory limit is zero, cost savings estimates don't work because the cost\n   // is defined as memory_limit_bytes / memory_reduced. Bounds it to a large\n   // enough value for cost differences to be comparable.\n@@ -2736,19 +2748,22 @@ absl::StatusOr<bool> HloRematerialization::RematerializeComputationPeakPriority(\n       &remat_move_instructions,\n       &execution_threads};\n \n-  RematSubpassResult remat_subpass_result =\n-      RematSubpassResult::kChangedButOverMemoryLimit;\n+  RematSubpassStatus remat_subpass_status;\n   bool changed = false;\n-  while (remat_subpass_result ==\n-         RematSubpassResult::kChangedButOverMemoryLimit) {\n+  do {\n     TF_ASSIGN_OR_RETURN(\n-        remat_subpass_result,\n+        RematSubpassResult remat_subpass_result,\n         PeakPrioritySubPass(peak_memory_instruction, rematerialization_state,\n                             computation, call_graph_node, min_remat_size,\n                             peak_memory_during_remat, memory_limit_bytes,\n                             execution_threads));\n-    changed |= (remat_subpass_result != RematSubpassResult::kUnchanged);\n-  }\n+    changed |= (remat_subpass_result.status != RematSubpassStatus::kUnchanged);\n+    remat_subpass_status = remat_subpass_result.status;\n+    peak_memory_during_remat = remat_subpass_result.peak_memory_during_remat;\n+    peak_memory_instruction = remat_subpass_result.peak_memory_instruction;\n+  } while (remat_subpass_status ==\n+           RematSubpassStatus::kChangedButOverMemoryLimit);\n+\n   rematerialized_computations_.insert(computation);\n   return changed;\n }"
        },
        {
            "sha": "9ef2558bfabe7349d69af294e8b9c088ec2bde96",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.h",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55c1568cbfa8fe6ccc6b496719a87aac913ee15b/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fhlo_rematerialization.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55c1568cbfa8fe6ccc6b496719a87aac913ee15b/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fhlo_rematerialization.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fhlo_rematerialization.h?ref=55c1568cbfa8fe6ccc6b496719a87aac913ee15b",
            "patch": "@@ -257,12 +257,18 @@ class HloRematerialization : public HloModulePass {\n     int64_t remat_instructions_count;\n   };\n \n-  enum class RematSubpassResult : char {\n+  enum class RematSubpassStatus : char {\n     kUnchanged,\n     kChangedButOverMemoryLimit,\n     kChangedAndUnderMemoryLimit,\n   };\n \n+  struct RematSubpassResult {\n+    RematSubpassStatus status = RematSubpassStatus::kUnchanged;\n+    int64_t peak_memory_during_remat = 0;\n+    const HloInstruction* peak_memory_instruction = nullptr;\n+  };\n+\n   // Holds the memory usage and instruction at a given program point (usually\n   // the peak memory).\n   struct MemoryUsageAndInstruction {"
        },
        {
            "sha": "0df6009712e004a769307cb35c05c2019969786d",
            "filename": "third_party/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization_test.cc",
            "status": "modified",
            "additions": 84,
            "deletions": 2,
            "changes": 86,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/55c1568cbfa8fe6ccc6b496719a87aac913ee15b/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fhlo_rematerialization_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/55c1568cbfa8fe6ccc6b496719a87aac913ee15b/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fhlo_rematerialization_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftransforms%2Fsimplifiers%2Fhlo_rematerialization_test.cc?ref=55c1568cbfa8fe6ccc6b496719a87aac913ee15b",
            "patch": "@@ -47,7 +47,6 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/lib/core/status_test_util.h\"\n-#include \"xla/tsl/platform/status_matchers.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n@@ -57,15 +56,17 @@ namespace {\n \n namespace op = xla::testing::opcode_matchers;\n \n+using ::absl_testing::IsOkAndHolds;\n using ::testing::_;\n using ::testing::Contains;\n using ::testing::ElementsAre;\n using ::testing::Eq;\n using ::testing::HasSubstr;\n using ::testing::Not;\n using ::testing::Pair;\n+using ::testing::Property;\n+using ::testing::StrEq;\n using ::testing::UnorderedElementsAre;\n-using tsl::testing::IsOkAndHolds;\n \n class AsyncRematerializationTest : public RematerializationTestBase {\n  protected:\n@@ -1907,5 +1908,86 @@ ENTRY %entry (param.0: f32[], param.1: f32[]) -> f32[1024] {\n   }\n }\n \n+TEST_F(RecomputeAndCompressHloRematerializationTest,\n+       PeakFirstRematerializesAtSamePeak) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(R\"(\n+HloModule fusion, is_scheduled=true\n+\n+%call_convoluted (param_0: f32[1024], param_1: f32[1024]) -> f32[1024] {\n+  %constant_source_8 = f32[] constant(8)\n+  %constant_source_8_user = f32[1024]{0} broadcast(%constant_source_8), dimensions={}\n+  %param_0 = f32[1024]{0} parameter(0)\n+  %constant_source_8_user_2 = f32[1024]{0} broadcast(%constant_source_8), dimensions={}\n+  %param_1 = f32[1024]{0} parameter(1)\n+  %res_param_add = f32[1024]{0} add(%param_0, %param_1)\n+  %constant.anon = f32[] constant(1)\n+  %constant_0 = f32[16384]{0} broadcast(%constant.anon), dimensions={}\n+  %op_1 = f32[16384]{0} tanh(%constant_0)\n+  %op_2 = f32[16384]{0} tanh(%op_1)\n+  %op_3 = f32[16384]{0} tanh(%op_2)\n+  %op_4 = f32[16384]{0} tanh(%op_3)\n+  %tan_res = f32[1024]{0} slice(%op_4), slice={[0:1024]}\n+  %res_1 = f32[1024]{0} add(%res_param_add, %tan_res)\n+  %res_3 = f32[1024]{0} add(%constant_source_8_user, %res_1)\n+  %res_3_2 = f32[1024]{0} add(%constant_source_8_user_2, %res_3)\n+  %constant_x = f32[1024]{0} broadcast(%constant_source_8), dimensions={}\n+  %constant_x_and_res_param_add = f32[1024]{0} add(%constant_x, %res_param_add)\n+  %res_4 = f32[1024]{0} add(%res_3_2, %constant_x_and_res_param_add)\n+  ROOT %res = f32[1024]{0} add(%res_3, %res_4)\n+}\n+\n+%call_comp (p: f32[1024], p_2: f32[1024]) -> f32[1024] {\n+  %p = f32[1024]{0} parameter(0)\n+  %p_2 = f32[1024]{0} parameter(1)\n+  %call_convoluted = f32[1024]{0} call(%p, %p_2), to_apply=%call_convoluted\n+  ROOT %n = f32[1024]{0} negate(%call_convoluted)\n+}\n+\n+%add_mul_comp (p0: f32[], p1: f32[]) -> f32[1024] {\n+  %p0 = f32[] parameter(0)\n+  %p1 = f32[] parameter(1)\n+  %p0_bcast = f32[1024]{0} broadcast(%p0), dimensions={}\n+  %p1_bcast = f32[1024]{0} broadcast(%p1), dimensions={}\n+  %res_comp = f32[1024]{0} call(%p0_bcast, %p1_bcast), to_apply=%call_comp\n+  ROOT %res_mul = f32[1024]{0} multiply(%res_comp, %res_comp)\n+}\n+\n+ENTRY %entry (param.0: f32[], param.1: f32[]) -> f32[1024] {\n+  %param.0 = f32[] parameter(0)\n+  %param.1 = f32[] parameter(1)\n+  %res = f32[1024]{0} call(%param.0, %param.1), to_apply=%add_mul_comp\n+  ROOT %res_2 = f32[1024]{0} negate(%res)\n+}\n+)\"));\n+\n+  // Rematerialize with a low memory limit and min_remat_size.\n+  EXPECT_THAT(RunHloRematerialization(\n+                  /*memory_limit_bytes=*/0, module.get(),\n+                  /*min_remat_size=*/0,\n+                  HloRematerialization::RematAlgorithm::kPeakPriority),\n+              IsOkAndHolds(true));\n+\n+  const std::vector<HloInstruction*>& call_convoluted_instructions =\n+      module->schedule()\n+          .sequence(module->GetComputationWithName(\"call_convoluted\"))\n+          .instructions();\n+\n+  EXPECT_THAT(call_convoluted_instructions,\n+              AllOf(\n+                  // Should remat a large instruction.\n+                  Not(Contains(Property(&HloInstruction::name,\n+                                        StrEq(\"constant_source_8_user\")))),\n+                  // Should not remat after a peak\n+                  Not(Contains(Property(&HloInstruction::name,\n+                                        StrEq(\"constant_x.remat2\")))),\n+                  // Should remat both constant_source_8_user even with them\n+                  // being associated with the same peak.\n+                  Contains(Property(&HloInstruction::name,\n+                                    StrEq(\"constant_source_8_user.remat\"))),\n+                  Contains(Property(&HloInstruction::name,\n+                                    StrEq(\"constant_source_8_user_2.remat\")))));\n+}\n+\n }  // namespace\n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 136,
        "additions": 119,
        "deletions": 17
    }
}