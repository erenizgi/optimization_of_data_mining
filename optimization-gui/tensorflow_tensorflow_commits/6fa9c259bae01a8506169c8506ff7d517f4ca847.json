{
    "author": "pschuh",
    "message": "Remove unneeded BufferFromHostBuffer implementation.\n\nPiperOrigin-RevId: 809126959",
    "sha": "6fa9c259bae01a8506169c8506ff7d517f4ca847",
    "files": [
        {
            "sha": "6e9d2e3074fc4a46f7f6f40eb9d53dc618463ce9",
            "filename": "third_party/xla/xla/pjrt/pjrt_stream_executor_client.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 226,
            "changes": 226,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6fa9c259bae01a8506169c8506ff7d517f4ca847/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6fa9c259bae01a8506169c8506ff7d517f4ca847/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc?ref=6fa9c259bae01a8506169c8506ff7d517f4ca847",
            "patch": "@@ -1070,232 +1070,6 @@ PjRtStreamExecutorClient::LinearizeHostBufferInto(\n   return definition_event;\n }\n \n-// BufferFromHostBuffer() is used to create a buffer either for a device, or\n-// for a host memory, depending on `memory_space`. The memory copy is needed\n-// for both cases, either from the unpinned host memory to device, or from\n-// the unpinned host memory to the pinned host memory.\n-absl::StatusOr<std::unique_ptr<PjRtBuffer>>\n-PjRtStreamExecutorClient::BufferFromHostBufferInternal(\n-    const void* data, PrimitiveType type, absl::Span<int64_t const> dims,\n-    std::optional<absl::Span<int64_t const>> byte_strides,\n-    HostBufferSemantics host_buffer_semantics,\n-    absl::AnyInvocable<void() &&> on_done_with_host_buffer, PjRtDevice* device,\n-    const Layout* device_layout, PjRtMemorySpace* memory_space) {\n-  tsl::profiler::TraceMe traceme(\n-      \"PjRtStreamExecutorClient::BufferFromHostBuffer\");\n-  Shape device_shape = ShapeUtil::MakeShape(type, dims);\n-  VLOG(1) << \"PjRtStreamExecutorClient::BufferFromHostBuffer: shape: \"\n-          << device_shape.ToString() << \" device: \" << device->DebugString();\n-  TF_ASSIGN_OR_RETURN(LocalDeviceState * local_device,\n-                      tensorflow::down_cast<PjRtStreamExecutorDevice*>(device)\n-                          ->GetLocalDeviceState());\n-\n-  absl::InlinedVector<int64_t, 4> tmp_strides;\n-  if (!byte_strides) {\n-    tmp_strides.resize(dims.size());\n-    TF_RETURN_IF_ERROR(\n-        ShapeUtil::ByteStrides(device_shape, absl::MakeSpan(tmp_strides)));\n-    byte_strides = tmp_strides;\n-  }\n-  int64_t size = ShapeUtil::ByteSizeOf(device_shape);\n-\n-  TransferManager* transfer_manager = client()->backend().transfer_manager();\n-  if (device_layout != nullptr) {\n-    *(device_shape.mutable_layout()) = *device_layout;\n-  } else {\n-    TF_ASSIGN_OR_RETURN(\n-        device_shape,\n-        transfer_manager->ChooseCompactLayoutForShape(device_shape));\n-  }\n-  absl::InlinedVector<int64_t, 4> shape_strides(\n-      device_shape.dimensions().size());\n-  TF_RETURN_IF_ERROR(\n-      ShapeUtil::ByteStrides(device_shape, absl::MakeSpan(shape_strides)));\n-  bool host_and_device_strides_equal =\n-      (size == 0 || *byte_strides == shape_strides);\n-\n-  TF_ASSIGN_OR_RETURN(\n-      std::unique_ptr<PjRtStreamExecutorBuffer> py_buffer,\n-      AllocateDestinationBuffer(device_shape, device, local_device,\n-                                local_device->host_to_device_stream(),\n-                                /*is_uninitialized_create=*/false, this,\n-                                /*definition_event=*/nullptr, memory_space));\n-\n-  PjRtStreamExecutorBuffer::ScopedHold device_buffer(\n-      py_buffer->GetBufferWithUsageHold());\n-  CHECK(device_buffer.ok());\n-\n-  std::shared_ptr<TransposePlan> transpose;\n-  if (!host_and_device_strides_equal) {\n-    absl::InlinedVector<int64_t, 4> permutation(dims.size());\n-    absl::c_reverse_copy(device_shape.layout().minor_to_major(),\n-                         permutation.begin());\n-    TransposePlan::Options options;\n-    options.elem_size_in_bytes = primitive_util::ByteWidth(type);\n-    options.dims = dims;\n-    options.permutation = permutation;\n-    options.input_layout = TransposePlan::Striding{*byte_strides};\n-    absl::MutexLock lock(&transpose_mu_);\n-    TF_ASSIGN_OR_RETURN(transpose, transpose_cache_.GetOrCreate(options));\n-  }\n-\n-  bool should_pack = primitive_util::IsSubByteNonPredType(type) &&\n-                     transfer_manager->PackSubbyteTypes();\n-  int64_t packed_size;\n-  if (should_pack) {\n-    packed_size =\n-        CeilOfRatio<int64_t>(size, 8 / primitive_util::BitWidth(type));\n-  } else {\n-    packed_size = size;\n-  }\n-\n-  // If necessary, allocate a host-side buffer for staging host-to-device\n-  // transfers. On GPU this is a buffer in pinned memory.\n-  std::shared_ptr<void> staging_buffer;\n-  bool must_use_staging_buffer =\n-      host_buffer_semantics == HostBufferSemantics::kImmutableOnlyDuringCall ||\n-      !host_and_device_strides_equal || packed_size != size;\n-  // Allocating multigigabyte pinned buffers can be very slow. In that case,\n-  // using a staging buffer is probably worse than not using one.\n-  // TODO(phawkins): add chunking for transfers.\n-  if (must_use_staging_buffer || (!IsDmaMapped(data, packed_size) &&\n-                                  (should_stage_host_to_device_transfers() &&\n-                                   packed_size < (int64_t{1} << 30)))) {\n-    void* ptr = host_memory_allocator()->AllocateRaw(\n-        tsl::Allocator::kAllocatorAlignment, transpose ? size : packed_size);\n-    staging_buffer = std::shared_ptr<void>(\n-        ptr, [host_memory_allocator = host_memory_allocator()](void* ptr) {\n-          host_memory_allocator->DeallocateRaw(ptr);\n-        });\n-  }\n-\n-  // Copy the buffer into a staging buffer before returning control to the\n-  // caller if the caller only guaranteed that the buffer is valid for the\n-  // duration of the call. Otherwise, we stage (if necessary) on a separate\n-  // thread.\n-  if (host_buffer_semantics == HostBufferSemantics::kImmutableOnlyDuringCall) {\n-    if (transpose) {\n-      transpose->Execute(data, staging_buffer.get());\n-      if (should_pack) {\n-        primitive_util::PackIntN(\n-            type,\n-            absl::MakeConstSpan(static_cast<const char*>(staging_buffer.get()),\n-                                size),\n-            absl::MakeSpan(static_cast<char*>(staging_buffer.get()),\n-                           packed_size));\n-      }\n-    } else {\n-      if (should_pack) {\n-        primitive_util::PackIntN(\n-            type, absl::MakeConstSpan(static_cast<const char*>(data), size),\n-            absl::MakeSpan(static_cast<char*>(staging_buffer.get()),\n-                           packed_size));\n-      } else {\n-        std::memcpy(staging_buffer.get(), data, size);\n-      }\n-    }\n-    if (on_done_with_host_buffer) {\n-      std::move(on_done_with_host_buffer)();\n-      on_done_with_host_buffer = nullptr;\n-    }\n-  }\n-\n-  BufferSequencingEventRef event = device_buffer->definition_events()[0];\n-\n-  // The host to device transfer is performed on a thread pool, mostly because\n-  // it includes linearization that may be slow. It is OK to capture the\n-  // py_buffer pointer because the py_buffer can't be deleted until all the\n-  // usage holds have gone away.\n-  // TODO(misard) assess if it would be preferable to introduce a heuristic to\n-  // put the transfer into the calling thread for small literals.\n-  auto transfer_h2d =\n-      [this, local_client = client(), local_device, data, size, type,\n-       packed_size, event, device_memory_owned = device_buffer->device_memory(),\n-       device_shape, should_pack, py_buffer{py_buffer.get()},\n-       on_device_shape{py_buffer->on_device_shape()},\n-       staging_buffer{std::move(staging_buffer)},\n-       on_done_with_host_buffer =\n-           on_done_with_host_buffer\n-               ? std::make_shared<absl::AnyInvocable<void() &&>>(\n-                     std::move(on_done_with_host_buffer))\n-               : nullptr,\n-       host_buffer_semantics, transpose{std::move(transpose)}]() mutable {\n-        // This function uses TF_CHECK_OK and value() since we have no way\n-        // to report failures from a callback. However, the operations here are\n-        // unlikely to fail and not recoverable even if we were to fail: DMAs to\n-        // memory that has already been allocated, and a possible Event\n-        // allocation.\n-\n-        se::DeviceMemoryBase device_memory = device_memory_owned->mem();\n-\n-        // If applicable on the backend, stage the transfer via host memory\n-        // allocated via the host_memory_allocator. On GPU, this is pinned\n-        // memory.\n-        if (staging_buffer) {\n-          // If we didn't already copy the input buffer into the staging buffer,\n-          // do so now.\n-          if (host_buffer_semantics !=\n-              HostBufferSemantics::kImmutableOnlyDuringCall) {\n-            if (transpose) {\n-              transpose->Execute(data, staging_buffer.get());\n-              if (should_pack) {\n-                primitive_util::PackIntN(\n-                    type,\n-                    absl::MakeConstSpan(\n-                        static_cast<const char*>(staging_buffer.get()), size),\n-                    absl::MakeSpan(static_cast<char*>(staging_buffer.get()),\n-                                   packed_size));\n-              }\n-            } else {\n-              if (should_pack) {\n-                primitive_util::PackIntN(\n-                    type,\n-                    absl::MakeConstSpan(static_cast<const char*>(data), size),\n-                    absl::MakeSpan(static_cast<char*>(staging_buffer.get()),\n-                                   packed_size));\n-              } else {\n-                std::memcpy(staging_buffer.get(), data, size);\n-              }\n-            }\n-          }\n-          TF_CHECK_OK(local_device->host_to_device_stream()->Memcpy(\n-              &device_memory, staging_buffer.get(), packed_size));\n-        } else {\n-          TF_CHECK_OK(local_device->host_to_device_stream()->Memcpy(\n-              &device_memory, data, packed_size));\n-        }\n-\n-        TF_CHECK_OK(AddDestinationBufferSynchronization(\n-            this, local_device, event, local_device->host_to_device_stream()));\n-\n-        event.AndThen([device_memory_owned = std::move(device_memory_owned),\n-                       staging_buffer{std::move(staging_buffer)},\n-                       on_done_with_host_buffer{\n-                           std::move(on_done_with_host_buffer)}]() mutable {\n-          if (on_done_with_host_buffer) {\n-            std::move (*on_done_with_host_buffer)();\n-          }\n-        });\n-      };\n-  thread_pool()->Schedule(WrapClosureAsCopyable(std::move(transfer_h2d)));\n-  RecordUsage(std::move(device_buffer), local_device, local_device, event,\n-              local_device->host_to_device_stream());\n-  return std::unique_ptr<PjRtBuffer>(std::move(py_buffer));\n-}\n-\n-absl::StatusOr<std::unique_ptr<PjRtBuffer>>\n-PjRtStreamExecutorClient::BufferFromHostBuffer(\n-    const void* data, PrimitiveType type, absl::Span<int64_t const> dims,\n-    std::optional<absl::Span<int64_t const>> byte_strides,\n-    HostBufferSemantics host_buffer_semantics,\n-    absl::AnyInvocable<void() &&> on_done_with_host_buffer,\n-    PjRtMemorySpace* memory_space, const Layout* device_layout) {\n-  return BufferFromHostBufferInternal(\n-      data, type, dims, byte_strides, host_buffer_semantics,\n-      std::move(on_done_with_host_buffer), memory_space->devices()[0],\n-      device_layout, memory_space);\n-}\n-\n absl::StatusOr<std::unique_ptr<PjRtBuffer>>\n PjRtStreamExecutorClient::CreateUninitializedBuffer(\n     const Shape& shape, PjRtMemorySpace* memory_space) {"
        },
        {
            "sha": "ff035b628eb671181b2d68bbb3a35931041dd32b",
            "filename": "third_party/xla/xla/pjrt/pjrt_stream_executor_client.h",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6fa9c259bae01a8506169c8506ff7d517f4ca847/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6fa9c259bae01a8506169c8506ff7d517f4ca847/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.h?ref=6fa9c259bae01a8506169c8506ff7d517f4ca847",
            "patch": "@@ -331,13 +331,6 @@ class PjRtStreamExecutorClient : public CommonPjRtClient {\n   absl::StatusOr<std::unique_ptr<PjRtBuffer>> CreateErrorBuffer(\n       absl::Status error, const Shape& shape, PjRtMemorySpace* memory) override;\n \n-  absl::StatusOr<std::unique_ptr<PjRtBuffer>> BufferFromHostBuffer(\n-      const void* data, PrimitiveType type, absl::Span<int64_t const> dims,\n-      std::optional<absl::Span<int64_t const>> byte_strides,\n-      HostBufferSemantics host_buffer_semantics,\n-      absl::AnyInvocable<void() &&> on_done_with_host_buffer,\n-      PjRtMemorySpace* memory_space, const Layout* device_layout) override;\n-\n   using PjRtClient::BufferFromHostLiteral;\n   absl::StatusOr<std::unique_ptr<PjRtBuffer>> BufferFromHostLiteral(\n       const LiteralSlice& literal, PjRtMemorySpace* memory_space,\n@@ -517,14 +510,6 @@ class PjRtStreamExecutorClient : public CommonPjRtClient {\n       std::vector<std::unique_ptr<LocalExecutable>> local_executables,\n       CompileOptions compile_options, bool dump);\n \n-  absl::StatusOr<std::unique_ptr<PjRtBuffer>> BufferFromHostBufferInternal(\n-      const void* data, PrimitiveType type, absl::Span<int64_t const> dims,\n-      std::optional<absl::Span<int64_t const>> byte_strides,\n-      HostBufferSemantics host_buffer_semantics,\n-      absl::AnyInvocable<void() &&> on_done_with_host_buffer,\n-      PjRtDevice* device, const Layout* device_layout,\n-      PjRtMemorySpace* memory_space);\n-\n   const PjRtPlatformId platform_id_;\n   const std::string platform_name_;\n   LocalClient* client_;"
        }
    ],
    "stats": {
        "total": 241,
        "additions": 0,
        "deletions": 241
    }
}