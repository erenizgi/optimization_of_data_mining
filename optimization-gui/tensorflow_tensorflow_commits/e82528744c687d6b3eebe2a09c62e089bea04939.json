{
    "author": "loislo",
    "message": "[XLA:GPU] Move TritonScaledDotGemmTest to fusion_emitter_device_test.cc.\n\nThis change relocates the `TritonScaledDotGemmTest` suite, including its parameterized tests for FP8 scaled dot products, from `fusion_emitter_device_legacy_port_test.cc` to `fusion_emitter_device_test.cc`. The test class is updated to inherit from `TritonEmitterTest` and includes necessary adjustments to debug options and compute capability checks.\n\nPiperOrigin-RevId: 811323345",
    "sha": "e82528744c687d6b3eebe2a09c62e089bea04939",
    "files": [
        {
            "sha": "cb24d19245848af60d811c1bd673b381a88f594e",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e82528744c687d6b3eebe2a09c62e089bea04939/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e82528744c687d6b3eebe2a09c62e089bea04939/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=e82528744c687d6b3eebe2a09c62e089bea04939",
            "patch": "@@ -832,12 +832,11 @@ xla_test(\n         \"//xla/tsl/lib/core:status_test_util\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n-        \"//xla/tsl/platform:status_matchers\",\n         \"//xla/tsl/platform:statusor\",\n         \"//xla/tsl/platform:test\",\n-        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest\",\n         \"@eigen_archive//:eigen3\","
        },
        {
            "sha": "65184593908afb5fe3366d84d3ebdb0415043399",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_legacy_port_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 238,
            "changes": 239,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e82528744c687d6b3eebe2a09c62e089bea04939/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e82528744c687d6b3eebe2a09c62e089bea04939/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_legacy_port_test.cc?ref=e82528744c687d6b3eebe2a09c62e089bea04939",
            "patch": "@@ -15,7 +15,6 @@ limitations under the License.\n \n #include <cstdlib>\n #include <memory>\n-#include <ostream>\n #include <string>\n #include <utility>\n #include <variant>\n@@ -27,7 +26,6 @@ limitations under the License.\n #include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n-#include \"absl/strings/str_replace.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/strings/substitute.h\"\n #include \"llvm/IR/LLVMContext.h\"\n@@ -69,7 +67,6 @@ namespace gpu {\n namespace {\n \n namespace m = ::xla::match;\n-using tsl::testing::StatusIs;\n \n struct ModuleAndNestedFusionMetadata {\n   std::unique_ptr<VerifiedHloModule> module;\n@@ -3234,241 +3231,7 @@ ENTRY e {\n ; CHECK-SAME: __triton_nested_gemm_fusion\n   )\");\n }\n-}  // namespace\n-\n-struct ScaleDotTestParams {\n-  std::string lhs_type;\n-  std::string lhs_scale_type;\n-  std::string rhs_type;\n-  std::string rhs_scale_type;\n-  std::string output_type;\n-  std::string expected_triton_type;\n-\n-  std::string PrepareHloText(absl::string_view hlo_template) const {\n-    return absl::StrReplaceAll(hlo_template,\n-                               {{\"$lhs_type\", lhs_type},\n-                                {\"$lhs_scale_type\", lhs_scale_type},\n-                                {\"$rhs_type\", rhs_type},\n-                                {\"$rhs_scale_type\", rhs_scale_type},\n-                                {\"$output_type\", output_type}});\n-  }\n-  static std::string ToString(\n-      const ::testing::TestParamInfo<ScaleDotTestParams>& info) {\n-    const ScaleDotTestParams& params = info.param;\n-    auto name = absl::StrCat(params.lhs_type, \"_\", params.lhs_scale_type, \"_\",\n-                             params.rhs_type, \"_\", params.rhs_scale_type, \"_\",\n-                             params.output_type);\n-    absl::StrReplaceAll({{\"[\", \"_\"}, {\"]\", \"_\"}, {\",\", \"x\"}}, &name);\n-    return name;\n-  }\n-};\n-\n-std::ostream& operator<<(std::ostream& stream, const ScaleDotTestParams& tc) {\n-  return stream << \"{\\n\\tlhs_type:\" << tc.lhs_type\n-                << \",\\n\\tlhs_scale_type:\" << tc.lhs_scale_type\n-                << \",\\n\\trhs_type:\" << tc.rhs_type\n-                << \",\\n\\trhs_scale_type:\" << tc.rhs_scale_type\n-                << \",\\n\\toutput_type:\" << tc.output_type << \"\\n}\";\n-}\n-\n-class TritonScaledDotGemmTest\n-    : public TritonGemmTest,\n-      public ::testing::WithParamInterface<ScaleDotTestParams> {};\n-\n-TEST_P(TritonScaledDotGemmTest,\n-       FP8ScaledDotCompilesToPtxIntrinsicsWhenAvailable) {\n-  const ScaleDotTestParams& params = GetParam();\n-  constexpr absl::string_view kHloTextTemplate = R\"hlo(\n-HloModule m\n-flhs (p0: $lhs_type) -> $lhs_type {\n-  ROOT p0 = $lhs_type{1,0} parameter(0)\n-}\n-flhs_scale (p0: $lhs_scale_type) -> $lhs_scale_type {\n-  ROOT p0 = $lhs_scale_type{1,0} parameter(0)\n-}\n-frhs (p0: $rhs_type) -> $rhs_type {\n-  ROOT p0 = $rhs_type{1,0} parameter(0)\n-}\n-frhs_scale (p0: $rhs_scale_type) -> $rhs_scale_type {\n-  ROOT p0 = $rhs_scale_type{1,0} parameter(0)\n-}\n-\n-triton_dot {\n-  lhs = $lhs_type parameter(0)\n-  lhs1 = $lhs_type{1,0} fusion(lhs),\n-    kind=kCustom,\n-    calls=flhs,\n-    backend_config={\n-      \"fusion_backend_config\":{\n-        \"kind\":\"__triton_nested_gemm_fusion\",\n-        \"block_level_fusion_config\":{\n-          \"output_tiles\":[{\"sizes\":[\"128\",\"128\"]}],\n-          \"num_warps\":\"4\",\n-          \"num_stages\":\"1\",\n-          \"num_ctas\":\"1\",\n-        }\n-      }\n-    }\n-  lhs_scale = $lhs_scale_type parameter(1)\n-  lhs_scale1 = $lhs_scale_type{1,0} fusion(lhs_scale),\n-    kind=kCustom,\n-    calls=flhs_scale,\n-    backend_config={\n-      \"fusion_backend_config\":{\n-        \"kind\":\"__triton_nested_gemm_fusion\",\n-        \"block_level_fusion_config\":{\n-          \"output_tiles\":[{\"sizes\":[\"128\",\"128\"]}],\n-          \"num_warps\":\"4\",\n-          \"num_stages\":\"1\",\n-          \"num_ctas\":\"1\",\n-        }\n-      }\n-    }\n-  rhs = $rhs_type parameter(2)\n-  rhs1 = $rhs_type{1,0} fusion(rhs),\n-    kind=kCustom,\n-    calls=frhs,\n-    backend_config={\n-      \"fusion_backend_config\":{\n-        \"kind\":\"__triton_nested_gemm_fusion\",\n-        \"block_level_fusion_config\":{\n-          \"output_tiles\":[{\"sizes\":[\"128\",\"256\"]}],\n-          \"num_warps\":\"4\",\n-          \"num_stages\":\"1\",\n-          \"num_ctas\":\"1\",\n-        }\n-      }\n-    }\n-  rhs_scale = $rhs_scale_type parameter(3)\n-  rhs_scale1 = $rhs_scale_type{1,0} fusion(rhs_scale),\n-    kind=kCustom,\n-    calls=frhs_scale,\n-    backend_config={\n-      \"fusion_backend_config\":{\n-        \"kind\":\"__triton_nested_gemm_fusion\",\n-        \"block_level_fusion_config\":{\n-          \"output_tiles\":[{\"sizes\":[\"128\", \"256\"]}],\n-          \"num_warps\":\"4\",\n-          \"num_stages\":\"1\",\n-          \"num_ctas\":\"1\",\n-        }\n-      }\n-    }\n-  ROOT _ = $output_type{1,0} scaled-dot(lhs1, lhs_scale1, rhs1, rhs_scale1),\n-    lhs_contracting_dims={1},\n-    rhs_contracting_dims={0}\n-}\n-\n-ENTRY e {\n-  p0 = $lhs_type{1,0} parameter(0)\n-  p1 = $lhs_scale_type{1,0} parameter(1)\n-  p2 = $rhs_type{1,0} parameter(2)\n-  p3 = $rhs_scale_type{1,0} parameter(3)\n-  ROOT _ = $output_type{1,0} fusion(p0, p1, p2, p3),\n-    kind=kCustom,\n-    calls=triton_dot,\n-    backend_config={\n-      \"fusion_backend_config\": {\n-        kind: \"__triton_scaled_dot_fusion\",\n-        \"block_level_fusion_config\":{\n-          \"output_tiles\":[{\"sizes\":[\"128\", \"256\"]}],\n-          \"num_warps\":\"4\",\n-          \"num_stages\":\"1\",\n-          \"num_ctas\":\"1\"\n-        }\n-      }\n-    }\n-}\n-)hlo\";\n-\n-  auto hlo_text = params.PrepareHloText(kHloTextTemplate);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n-                          ParseAndReturnVerifiedModule(hlo_text));\n-\n-  auto debug_options = module->config().debug_options();\n-  debug_options.set_xla_gpu_experimental_scaled_dot_with_triton(true);\n-  module->mutable_config().set_debug_options(debug_options);\n-\n-  constexpr absl::string_view kExpectedTritonIrTmpl = R\"(\n-      CHECK: tt.dot_scaled\n-      CHECK: tensor<128x128x$triton_type>, tensor<128x4xi8>\n-      CHECK: tensor<128x256x$triton_type>, tensor<256x4xi8>\n-      CHECK: -> tensor<128x256xf32>\n-  )\";\n-  auto expected_triton_ir = absl::StrReplaceAll(\n-      kExpectedTritonIrTmpl, {{\"$triton_type\", params.expected_triton_type}});\n-  EXPECT_THAT(\n-      CreateTritonIrAndFileCheck(*module->GetComputationWithName(\"triton_dot\"),\n-                                 /*block_level_parameters=*/\n-                                 {\n-                                     {{128, 256}},\n-                                     4,\n-                                     1,\n-                                     1,\n-                                     true,\n-                                 },\n-                                 expected_triton_ir),\n-      absl_testing::IsOk());\n-  if (GetCudaComputeCapability().IsAtLeastBlackwell()) {\n-    CompileAndOptionallyVerifyPtx(\n-        std::move(module), R\"(CHECK: mxf8f6f4.block_scale.scale_vec::1X)\");\n-  }\n-}\n-\n-TEST_P(TritonScaledDotGemmTest, FP8ScaledDotGetsFusedAndExecutesCorrectly) {\n-  const ScaleDotTestParams& params = GetParam();\n-  if (!GetCudaComputeCapability().IsAtLeastBlackwell()) {\n-    GTEST_SKIP() << \"Skipping test for pre-Blackwell GPUs.\";\n-  }\n-  constexpr absl::string_view kHloTextTemplate = R\"hlo(\n-HloModule FP8ScaledDotGetsFused\n-\n-ENTRY e {\n-  lhs = $lhs_type parameter(0)\n-  lhs_scale = $lhs_scale_type parameter(1)\n-  rhs = $rhs_type parameter(2)\n-  rhs_scale = $rhs_scale_type parameter(3)\n-  ROOT _ = $output_type{1,0} scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n-    lhs_contracting_dims={1},\n-    rhs_contracting_dims={0}\n-}\n-)hlo\";\n-\n-  auto hlo_text = params.PrepareHloText(kHloTextTemplate);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n-                          ParseAndReturnVerifiedModule(hlo_text));\n-\n-  auto debug_options = module->config().debug_options();\n-  debug_options.set_xla_gpu_experimental_scaled_dot_with_triton(true);\n-  debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n-      DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n-  module->mutable_config().set_debug_options(debug_options);\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto optimized_module,\n-                          GetOptimizedModule(std::move(module)));\n-  EXPECT_TRUE(*RunFileCheck(optimized_module->ToString(), R\"(\n-    CHECK: fusion\n-    CHECK: ROOT {{.*}} scaled-dot\n-    CHECK: ENTRY\n-    CHECK: __triton_nested_gemm_fusion\n-  )\"));\n-  EXPECT_TRUE(RunAndCompareNoHloPasses(\n-      std::move(optimized_module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n-}\n-\n-INSTANTIATE_TEST_SUITE_P(\n-    TritonScaledDotGemmTest, TritonScaledDotGemmTest,\n-    ::testing::Values(ScaleDotTestParams{\"f8e4m3fn[128,128]\",\n-                                         \"f8e8m0fnu[128,4]\",\n-                                         \"f8e4m3fn[128,256]\",\n-                                         \"f8e8m0fnu[4,256]\", \"bf16[128,256]\",\n-                                         \"f8E4M3FN\"},\n-                      ScaleDotTestParams{\"f8e5m2[128,128]\", \"f8e8m0fnu[128,4]\",\n-                                         \"f8e5m2[128,256]\", \"f8e8m0fnu[4,256]\",\n-                                         \"bf16[128,256]\", \"f8E5M2\"}),\n-    ScaleDotTestParams::ToString);\n \n+}  // namespace\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "7ef2b38c1f5690c84d34dc44197e560cebacddd3",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter_device_test.cc",
            "status": "modified",
            "additions": 243,
            "deletions": 0,
            "changes": 243,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e82528744c687d6b3eebe2a09c62e089bea04939/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e82528744c687d6b3eebe2a09c62e089bea04939/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter_device_test.cc?ref=e82528744c687d6b3eebe2a09c62e089bea04939",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n #include <array>\n #include <cstdint>\n #include <memory>\n+#include <ostream>\n #include <string>\n #include <tuple>\n #include <utility>\n@@ -26,6 +27,7 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_join.h\"\n #include \"absl/strings/str_replace.h\"\n@@ -3865,6 +3867,247 @@ ENTRY entry {\n       kHloText, ErrorSpec{/*aabs=*/1e-4, /*arel=*/1e-6}));\n }\n \n+struct ScaleDotTestParams {\n+  std::string lhs_type;\n+  std::string lhs_scale_type;\n+  std::string rhs_type;\n+  std::string rhs_scale_type;\n+  std::string output_type;\n+  std::string expected_triton_type;\n+\n+  std::string PrepareHloText(absl::string_view hlo_template) const {\n+    return absl::StrReplaceAll(hlo_template,\n+                               {{\"$lhs_type\", lhs_type},\n+                                {\"$lhs_scale_type\", lhs_scale_type},\n+                                {\"$rhs_type\", rhs_type},\n+                                {\"$rhs_scale_type\", rhs_scale_type},\n+                                {\"$output_type\", output_type}});\n+  }\n+  static std::string ToString(\n+      const ::testing::TestParamInfo<ScaleDotTestParams>& info) {\n+    const ScaleDotTestParams& params = info.param;\n+    auto name = absl::StrCat(params.lhs_type, \"_\", params.lhs_scale_type, \"_\",\n+                             params.rhs_type, \"_\", params.rhs_scale_type, \"_\",\n+                             params.output_type);\n+    absl::StrReplaceAll({{\"[\", \"_\"}, {\"]\", \"_\"}, {\",\", \"x\"}}, &name);\n+    return name;\n+  }\n+};\n+\n+std::ostream& operator<<(std::ostream& stream, const ScaleDotTestParams& tc) {\n+  return stream << \"{\\n\\tlhs_type:\" << tc.lhs_type\n+                << \",\\n\\tlhs_scale_type:\" << tc.lhs_scale_type\n+                << \",\\n\\trhs_type:\" << tc.rhs_type\n+                << \",\\n\\trhs_scale_type:\" << tc.rhs_scale_type\n+                << \",\\n\\toutput_type:\" << tc.output_type << \"\\n}\";\n+}\n+\n+class TritonScaledDotGemmTest\n+    : public TritonEmitterTest,\n+      public ::testing::WithParamInterface<ScaleDotTestParams> {\n+ public:\n+  DebugOptions GetDebugOptionsForTest() const override {\n+    DebugOptions debug_options = TritonEmitterTest::GetDebugOptionsForTest();\n+    debug_options.set_xla_gpu_experimental_scaled_dot_with_triton(true);\n+    debug_options.set_xla_gpu_autotune_level(0);\n+    debug_options.set_xla_gpu_cublas_fallback(false);\n+    debug_options.add_xla_gpu_unsupported_generic_triton_emitter_features(\n+        DebugOptions::GENERIC_TRITON_EMITTER_ENABLE_NESTED_GEMM);\n+    return debug_options;\n+  }\n+  stream_executor::CudaComputeCapability GetCudaComputeCapability() {\n+    return backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .cuda_compute_capability();\n+  }\n+};\n+\n+TEST_P(TritonScaledDotGemmTest,\n+       FP8ScaledDotCompilesToPtxIntrinsicsWhenAvailable) {\n+  const ScaleDotTestParams& params = GetParam();\n+  constexpr absl::string_view kHloTextTemplate = R\"hlo(\n+HloModule m\n+flhs (p0: $lhs_type) -> $lhs_type {\n+  ROOT p0 = $lhs_type{1,0} parameter(0)\n+}\n+flhs_scale (p0: $lhs_scale_type) -> $lhs_scale_type {\n+  ROOT p0 = $lhs_scale_type{1,0} parameter(0)\n+}\n+frhs (p0: $rhs_type) -> $rhs_type {\n+  ROOT p0 = $rhs_type{1,0} parameter(0)\n+}\n+frhs_scale (p0: $rhs_scale_type) -> $rhs_scale_type {\n+  ROOT p0 = $rhs_scale_type{1,0} parameter(0)\n+}\n+\n+triton_dot {\n+  lhs = $lhs_type parameter(0)\n+  lhs1 = $lhs_type{1,0} fusion(lhs),\n+    kind=kCustom,\n+    calls=flhs,\n+    backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"128\",\"128\"]}],\n+          \"num_warps\":\"4\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\",\n+        }\n+      }\n+    }\n+  lhs_scale = $lhs_scale_type parameter(1)\n+  lhs_scale1 = $lhs_scale_type{1,0} fusion(lhs_scale),\n+    kind=kCustom,\n+    calls=flhs_scale,\n+    backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"128\",\"128\"]}],\n+          \"num_warps\":\"4\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\",\n+        }\n+      }\n+    }\n+  rhs = $rhs_type parameter(2)\n+  rhs1 = $rhs_type{1,0} fusion(rhs),\n+    kind=kCustom,\n+    calls=frhs,\n+    backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"128\",\"256\"]}],\n+          \"num_warps\":\"4\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\",\n+        }\n+      }\n+    }\n+  rhs_scale = $rhs_scale_type parameter(3)\n+  rhs_scale1 = $rhs_scale_type{1,0} fusion(rhs_scale),\n+    kind=kCustom,\n+    calls=frhs_scale,\n+    backend_config={\n+      \"fusion_backend_config\":{\n+        \"kind\":\"__triton_nested_gemm_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"128\", \"256\"]}],\n+          \"num_warps\":\"4\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\",\n+        }\n+      }\n+    }\n+  ROOT _ = $output_type{1,0} scaled-dot(lhs1, lhs_scale1, rhs1, rhs_scale1),\n+    lhs_contracting_dims={1},\n+    rhs_contracting_dims={0}\n+}\n+\n+ENTRY e {\n+  p0 = $lhs_type{1,0} parameter(0)\n+  p1 = $lhs_scale_type{1,0} parameter(1)\n+  p2 = $rhs_type{1,0} parameter(2)\n+  p3 = $rhs_scale_type{1,0} parameter(3)\n+  ROOT _ = $output_type{1,0} fusion(p0, p1, p2, p3),\n+    kind=kCustom,\n+    calls=triton_dot,\n+    backend_config={\n+      \"fusion_backend_config\": {\n+        kind: \"__triton_scaled_dot_fusion\",\n+        \"block_level_fusion_config\":{\n+          \"output_tiles\":[{\"sizes\":[\"128\", \"256\"]}],\n+          \"num_warps\":\"4\",\n+          \"num_stages\":\"1\",\n+          \"num_ctas\":\"1\"\n+        }\n+      }\n+    }\n+}\n+)hlo\";\n+\n+  auto hlo_text = params.PrepareHloText(kHloTextTemplate);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_text));\n+\n+  constexpr absl::string_view kExpectedTritonIrTmpl = R\"(\n+      CHECK: tt.dot_scaled\n+      CHECK: tensor<128x128x$triton_type>, tensor<128x4xi8>\n+      CHECK: tensor<128x256x$triton_type>, tensor<256x4xi8>\n+      CHECK: -> tensor<128x256xf32>\n+  )\";\n+  auto expected_triton_ir = absl::StrReplaceAll(\n+      kExpectedTritonIrTmpl, {{\"$triton_type\", params.expected_triton_type}});\n+  EXPECT_THAT(\n+      CreateTritonIrAndFileCheck(*module->GetComputationWithName(\"triton_dot\"),\n+                                 /*block_level_parameters=*/\n+                                 {\n+                                     {{128, 256}},\n+                                     4,\n+                                     1,\n+                                     1,\n+                                     true,\n+                                 },\n+                                 expected_triton_ir),\n+      absl_testing::IsOk());\n+  if (GetCudaComputeCapability().IsAtLeastBlackwell()) {\n+    CompileAndOptionallyVerifyPtx(\n+        std::move(module), R\"(CHECK: mxf8f6f4.block_scale.scale_vec::1X)\");\n+  }\n+}\n+\n+TEST_P(TritonScaledDotGemmTest, FP8ScaledDotGetsFusedAndExecutesCorrectly) {\n+  const ScaleDotTestParams& params = GetParam();\n+  if (!GetCudaComputeCapability().IsAtLeastBlackwell()) {\n+    GTEST_SKIP() << \"Skipping test for pre-Blackwell GPUs.\";\n+  }\n+  constexpr absl::string_view kHloTextTemplate = R\"hlo(\n+HloModule FP8ScaledDotGetsFused\n+\n+ENTRY e {\n+  lhs = $lhs_type parameter(0)\n+  lhs_scale = $lhs_scale_type parameter(1)\n+  rhs = $rhs_type parameter(2)\n+  rhs_scale = $rhs_scale_type parameter(3)\n+  ROOT _ = $output_type{1,0} scaled-dot(lhs, lhs_scale, rhs, rhs_scale),\n+    lhs_contracting_dims={1},\n+    rhs_contracting_dims={0}\n+}\n+)hlo\";\n+\n+  auto hlo_text = params.PrepareHloText(kHloTextTemplate);\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo_text));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto optimized_module,\n+                          GetOptimizedModule(std::move(module)));\n+  EXPECT_TRUE(*RunFileCheck(optimized_module->ToString(), R\"(\n+    CHECK: fusion\n+    CHECK: ROOT {{.*}} scaled-dot\n+    CHECK: ENTRY\n+    CHECK: __triton_nested_gemm_fusion\n+  )\"));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(\n+      std::move(optimized_module), ErrorSpec{/*aabs=*/1e-3, /*arel=*/1e-3}));\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    TritonScaledDotGemmTest, TritonScaledDotGemmTest,\n+    ::testing::Values(ScaleDotTestParams{\"f8e4m3fn[128,128]\",\n+                                         \"f8e8m0fnu[128,4]\",\n+                                         \"f8e4m3fn[128,256]\",\n+                                         \"f8e8m0fnu[4,256]\", \"bf16[128,256]\",\n+                                         \"f8E4M3FN\"},\n+                      ScaleDotTestParams{\"f8e5m2[128,128]\", \"f8e8m0fnu[128,4]\",\n+                                         \"f8e5m2[128,256]\", \"f8e8m0fnu[4,256]\",\n+                                         \"bf16[128,256]\", \"f8E5M2\"}),\n+    ScaleDotTestParams::ToString);\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 485,
        "additions": 245,
        "deletions": 240
    }
}