{
    "author": "akuegel",
    "message": "[XLA:GPU] Increment autotuning cache key version and adapt autotune db.\n\nA recent change has affected the autotuning key, so we need to update the\nversion.\n\nPiperOrigin-RevId: 845661452",
    "sha": "3ffe7a698b2a2f078ec0a2f6a91f08e8424a07c8",
    "files": [
        {
            "sha": "1c5cf3a2298d40e23cf025015e1673faf6c5a713",
            "filename": "third_party/xla/xla/service/gpu/autotuning/autotune_cache_key.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3ffe7a698b2a2f078ec0a2f6a91f08e8424a07c8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3ffe7a698b2a2f078ec0a2f6a91f08e8424a07c8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fautotuning%2Fautotune_cache_key.h?ref=3ffe7a698b2a2f078ec0a2f6a91f08e8424a07c8",
            "patch": "@@ -32,7 +32,7 @@ class AutotuneCacheKey {\n   // Tie a version to the cache key in order to invalidate the cache when\n   // necessary. This should be incremented on triton upgrades or any other\n   // changes that may affect the autotuning results.\n-  static constexpr int kCurrentVersion = 20;\n+  static constexpr int kCurrentVersion = 21;\n \n   AutotuneCacheKey(const se::DeviceDescription& device_description,\n                    const HloInstruction& instruction,"
        },
        {
            "sha": "67d67b9594af5fe673ce08c76875026783c4c1d3",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler_test_autotune_db.textproto",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3ffe7a698b2a2f078ec0a2f6a91f08e8424a07c8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3ffe7a698b2a2f078ec0a2f6a91f08e8424a07c8/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler_test_autotune_db.textproto?ref=3ffe7a698b2a2f078ec0a2f6a91f08e8424a07c8",
            "patch": "@@ -27,7 +27,7 @@ results {\n }\n results {\n   device: \"CUDA: 8.0, Cores: 108, GPU clock: 1.41 GHz, Memory bandwidth: 1555 GB/s, L2 cache: 40 MB, DNN version: 1.2.3\"\n-  hlo: \"(bf16[128,1024,1024]{2,1,0}, s8[4194304]{0}) custom-call(bf16[128,1024,1024]{2,1,0}, bf16[128,1024,1024]{2,1,0}), custom_call_target=\\\"__cublas$gemm\\\", backend_config={\\\"force_earliest_schedule\\\":false,\\\"gemm_backend_config\\\":{\\\"alpha_imag\\\":0,\\\"alpha_real\\\":1,\\\"beta\\\":0,\\\"damax_output\\\":false,\\\"dot_dimension_numbers\\\":{\\\"lhs_batch_dimensions\\\":[\\\"0\\\"],\\\"lhs_contracting_dimensions\\\":[\\\"2\\\"],\\\"rhs_batch_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\",\\\"grad_x\\\":false,\\\"grad_y\\\":false,\\\"lhs_stride\\\":\\\"1048576\\\",\\\"precision_config\\\":{\\\"algorithm\\\":\\\"ALG_UNSET\\\",\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"rhs_stride\\\":\\\"1048576\\\"},\\\"operation_queue_id\\\":\\\"0\\\",\\\"wait_on_operation_queues\\\":[]}\"\n+  hlo: \"(bf16[128,1024,1024]{2,1,0}, s8[33554432]{0}) custom-call(bf16[128,1024,1024]{2,1,0}, bf16[128,1024,1024]{2,1,0}), custom_call_target=\\\"__cublas$gemm\\\", backend_config={\\\"force_earliest_schedule\\\":false,\\\"gemm_backend_config\\\":{\\\"alpha_imag\\\":0,\\\"alpha_real\\\":1,\\\"autotune_workspace_size\\\":\\\"0\\\",\\\"beta\\\":0,\\\"damax_output\\\":false,\\\"dot_dimension_numbers\\\":{\\\"lhs_batch_dimensions\\\":[\\\"0\\\"],\\\"lhs_contracting_dimensions\\\":[\\\"2\\\"],\\\"rhs_batch_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\",\\\"grad_x\\\":false,\\\"grad_y\\\":false,\\\"lhs_stride\\\":\\\"1048576\\\",\\\"precision_config\\\":{\\\"algorithm\\\":\\\"ALG_UNSET\\\",\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"rhs_stride\\\":\\\"1048576\\\"},\\\"operation_queue_id\\\":\\\"0\\\",\\\"wait_on_operation_queues\\\":[]}\"\n   result {\n     run_time {\n       nanos: 1\n@@ -51,7 +51,7 @@ results {\n }\n results {\n   device: \"CUDA: 8.0, Cores: 108, GPU clock: 1.41 GHz, Memory bandwidth: 2039 GB/s, L2 cache: 40 MB, DNN version: 1.2.3\"\n-  hlo: \"(bf16[128,1024,1024]{2,1,0}, s8[4194304]{0}) custom-call(bf16[128,1024,1024]{2,1,0}, bf16[128,1024,1024]{2,1,0}), custom_call_target=\\\"__cublas$gemm\\\", backend_config={\\\"force_earliest_schedule\\\":false,\\\"gemm_backend_config\\\":{\\\"alpha_imag\\\":0,\\\"alpha_real\\\":1,\\\"beta\\\":0,\\\"damax_output\\\":false,\\\"dot_dimension_numbers\\\":{\\\"lhs_batch_dimensions\\\":[\\\"0\\\"],\\\"lhs_contracting_dimensions\\\":[\\\"2\\\"],\\\"rhs_batch_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\",\\\"grad_x\\\":false,\\\"grad_y\\\":false,\\\"lhs_stride\\\":\\\"1048576\\\",\\\"precision_config\\\":{\\\"algorithm\\\":\\\"ALG_UNSET\\\",\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"rhs_stride\\\":\\\"1048576\\\"},\\\"operation_queue_id\\\":\\\"0\\\",\\\"wait_on_operation_queues\\\":[]}\"\n+  hlo: \"(bf16[128,1024,1024]{2,1,0}, s8[33554432]{0}) custom-call(bf16[128,1024,1024]{2,1,0}, bf16[128,1024,1024]{2,1,0}), custom_call_target=\\\"__cublas$gemm\\\", backend_config={\\\"force_earliest_schedule\\\":false,\\\"gemm_backend_config\\\":{\\\"alpha_imag\\\":0,\\\"alpha_real\\\":1,\\\"autotune_workspace_size\\\":\\\"0\\\",\\\"beta\\\":0,\\\"damax_output\\\":false,\\\"dot_dimension_numbers\\\":{\\\"lhs_batch_dimensions\\\":[\\\"0\\\"],\\\"lhs_contracting_dimensions\\\":[\\\"2\\\"],\\\"rhs_batch_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\",\\\"grad_x\\\":false,\\\"grad_y\\\":false,\\\"lhs_stride\\\":\\\"1048576\\\",\\\"precision_config\\\":{\\\"algorithm\\\":\\\"ALG_UNSET\\\",\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"rhs_stride\\\":\\\"1048576\\\"},\\\"operation_queue_id\\\":\\\"0\\\",\\\"wait_on_operation_queues\\\":[]}\"\n   result {\n     run_time {\n       nanos: 1\n@@ -75,7 +75,7 @@ results {\n }\n results {\n   device: \"CUDA: 9.0, Cores: 132, GPU clock: 1.98 GHz, Memory bandwidth: 3352 GB/s, L2 cache: 50 MB, DNN version: 1.2.3\"\n-  hlo: \"(bf16[128,1024,1024]{2,1,0}, s8[33554432]{0}) custom-call(bf16[128,1024,1024]{2,1,0}, bf16[128,1024,1024]{2,1,0}), custom_call_target=\\\"__cublas$gemm\\\", backend_config={\\\"device_type\\\":\\\"DEVICE_TYPE_INVALID\\\",\\\"force_earliest_schedule\\\":false,\\\"gemm_backend_config\\\":{\\\"alpha_imag\\\":0,\\\"alpha_real\\\":1,\\\"beta\\\":0,\\\"damax_output\\\":false,\\\"dot_dimension_numbers\\\":{\\\"lhs_batch_dimensions\\\":[\\\"0\\\"],\\\"lhs_contracting_dimensions\\\":[\\\"2\\\"],\\\"rhs_batch_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\",\\\"grad_x\\\":false,\\\"grad_y\\\":false,\\\"lhs_stride\\\":\\\"1048576\\\",\\\"precision_config\\\":{\\\"algorithm\\\":\\\"ALG_UNSET\\\",\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"rhs_stride\\\":\\\"1048576\\\"},\\\"operation_queue_id\\\":\\\"0\\\",\\\"reification_cost\\\":[],\\\"wait_on_operation_queues\\\":[]}\"\n+  hlo: \"(bf16[128,1024,1024]{2,1,0}, s8[33554432]{0}) custom-call(bf16[128,1024,1024]{2,1,0}, bf16[128,1024,1024]{2,1,0}), custom_call_target=\\\"__cublas$gemm\\\", backend_config={\\\"device_type\\\":\\\"DEVICE_TYPE_INVALID\\\",\\\"force_earliest_schedule\\\":false,\\\"gemm_backend_config\\\":{\\\"alpha_imag\\\":0,\\\"alpha_real\\\":1,\\\"autotune_workspace_size\\\":\\\"0\\\",\\\"beta\\\":0,\\\"damax_output\\\":false,\\\"dot_dimension_numbers\\\":{\\\"lhs_batch_dimensions\\\":[\\\"0\\\"],\\\"lhs_contracting_dimensions\\\":[\\\"2\\\"],\\\"rhs_batch_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\",\\\"grad_x\\\":false,\\\"grad_y\\\":false,\\\"lhs_stride\\\":\\\"1048576\\\",\\\"precision_config\\\":{\\\"algorithm\\\":\\\"ALG_UNSET\\\",\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"rhs_stride\\\":\\\"1048576\\\"},\\\"operation_queue_id\\\":\\\"0\\\",\\\"reification_cost\\\":[],\\\"wait_on_operation_queues\\\":[]}\"\n   result {\n     gemm {\n       algorithm: -1\n@@ -87,7 +87,7 @@ results {\n }\n results {\n   device: \"CUDA: 9.0, Cores: 132, GPU clock: 1.98 GHz, Memory bandwidth: 3352 GB/s, L2 cache: 50 MB, DNN version: 1.2.3\"\n-  hlo: \"(bf16[12288,16384]{1,0}, s8[33554432]{0}) custom-call(f8e4m3fn[12288,4096]{1,0}, f8e4m3fn[4096,16384]{0,1}, f32[], f32[]), custom_call_target=\\\"__cublas$lt$matmul$f8\\\", backend_config={\\\"device_type\\\":\\\"DEVICE_TYPE_INVALID\\\",\\\"force_earliest_schedule\\\":false,\\\"gemm_backend_config\\\":{\\\"alpha_imag\\\":0,\\\"alpha_real\\\":0.95703125,\\\"beta\\\":0,\\\"damax_output\\\":false,\\\"dot_dimension_numbers\\\":{\\\"lhs_batch_dimensions\\\":[],\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_batch_dimensions\\\":[],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\",\\\"grad_x\\\":false,\\\"grad_y\\\":false,\\\"lhs_stride\\\":\\\"50331648\\\",\\\"precision_config\\\":{\\\"algorithm\\\":\\\"ALG_UNSET\\\",\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"rhs_stride\\\":\\\"67108864\\\"},\\\"operation_queue_id\\\":\\\"0\\\",\\\"reification_cost\\\":[],\\\"wait_on_operation_queues\\\":[]}\"\n+  hlo: \"(bf16[12288,16384]{1,0}, s8[33554432]{0}) custom-call(f8e4m3fn[12288,4096]{1,0}, f8e4m3fn[4096,16384]{0,1}, f32[], f32[]), custom_call_target=\\\"__cublas$lt$matmul$f8\\\", backend_config={\\\"device_type\\\":\\\"DEVICE_TYPE_INVALID\\\",\\\"force_earliest_schedule\\\":false,\\\"gemm_backend_config\\\":{\\\"alpha_imag\\\":0,\\\"alpha_real\\\":0.95703125,\\\"autotune_workspace_size\\\":\\\"0\\\",\\\"beta\\\":0,\\\"damax_output\\\":false,\\\"dot_dimension_numbers\\\":{\\\"lhs_batch_dimensions\\\":[],\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_batch_dimensions\\\":[],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\",\\\"grad_x\\\":false,\\\"grad_y\\\":false,\\\"lhs_stride\\\":\\\"50331648\\\",\\\"precision_config\\\":{\\\"algorithm\\\":\\\"ALG_UNSET\\\",\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"rhs_stride\\\":\\\"67108864\\\"},\\\"operation_queue_id\\\":\\\"0\\\",\\\"reification_cost\\\":[],\\\"wait_on_operation_queues\\\":[]}\"\n   result {\n     gemm {\n     }\n@@ -192,4 +192,4 @@ results {\n       }\n     }\n   }\n-}\n\\ No newline at end of file\n+}"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 6,
        "deletions": 6
    }
}