{
    "author": "vwbaker",
    "message": "Add option to only use default config to the BlockLevelEmitter backend.\n\nThis allows us to be able to autotune the BlockLevelEmitter as a general backend using the config given to us by the CostModel. Having this option allows us to autotune it against different backends without destroying compilation time.\n\nPiperOrigin-RevId: 798200644",
    "sha": "a1b6992838ffd8ccbdf543083852761ae6c7b2cc",
    "files": [
        {
            "sha": "63d032944562f4a8301980165cabc741a3468a72",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a1b6992838ffd8ccbdf543083852761ae6c7b2cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a1b6992838ffd8ccbdf543083852761ae6c7b2cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.cc?ref=a1b6992838ffd8ccbdf543083852761ae6c7b2cc",
            "patch": "@@ -242,6 +242,17 @@ void ExtendConfigsWithTma(\n \n absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n BlockLevelEmitterBackend::GetSupportedConfigs(const HloInstruction& instr) {\n+  // When use_default_config_ is true, we only return a single config for the\n+  // autotuner to use. It is expected that the default config exists already\n+  // in the HLO fusion and therefore fails if a default config cannot be\n+  // constructed.\n+  if (use_default_config_) {\n+    TF_ASSIGN_OR_RETURN(auto config, GetDefaultConfig(instr));\n+    std::vector<std::unique_ptr<BackendConfig>> configs;\n+    configs.push_back(std::move(config));\n+    return configs;\n+  }\n+\n   if (!IsSupported(instr)) {\n     return std::vector<std::unique_ptr<BackendConfig>>();\n   }"
        },
        {
            "sha": "ed40e420e976039e2bf2c159f1dfb89cee8d79a0",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter.h",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a1b6992838ffd8ccbdf543083852761ae6c7b2cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a1b6992838ffd8ccbdf543083852761ae6c7b2cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter.h?ref=a1b6992838ffd8ccbdf543083852761ae6c7b2cc",
            "patch": "@@ -42,9 +42,10 @@ class BlockLevelEmitterBackend : public GpuCodegenBackend {\n   explicit BlockLevelEmitterBackend(\n       stream_executor::StreamExecutor* absl_nonnull stream_executor,\n       const DebugOptions* absl_nonnull debug_options,\n-      Compiler* absl_nonnull compiler)\n+      Compiler* absl_nonnull compiler, bool use_default_config = false)\n       : GpuCodegenBackend(\"BlockLevelEmitter\", stream_executor, debug_options,\n-                          compiler) {}\n+                          compiler),\n+        use_default_config_(use_default_config) {}\n \n   // Returns all supported block-level tiling configurations for the given\n   // instruction.\n@@ -61,6 +62,14 @@ class BlockLevelEmitterBackend : public GpuCodegenBackend {\n \n   // Determines whether the given HLO instruction is supported by this backend.\n   bool IsSupported(const HloInstruction& instr);\n+\n+ private:\n+  // If true, the backend will return a single default configuration in\n+  // GetSupportedConfigs instead of generating all supported configurations.\n+  // This is useful to autotune between different backends without increasing\n+  // compile time by too much. It will use the default config, likely already\n+  // assigned by the cost model.\n+  bool use_default_config_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "4dbf510fbc9438964a4d85c5a452988ffa26b6e1",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/block_level_emitter_test.cc",
            "status": "modified",
            "additions": 52,
            "deletions": 0,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a1b6992838ffd8ccbdf543083852761ae6c7b2cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a1b6992838ffd8ccbdf543083852761ae6c7b2cc/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fblock_level_emitter_test.cc?ref=a1b6992838ffd8ccbdf543083852761ae6c7b2cc",
            "patch": "@@ -531,5 +531,57 @@ ENTRY %main {\n   EXPECT_THAT(executable, absl_testing::IsOk());\n }\n \n+TEST_F(TritonBlockLevelFusionEmitterBackendTest, UseDefaultConfigFlag) {\n+  auto backend = BlockLevelEmitterBackend(\n+      PlatformUtil::GetDefaultPlatform().value()->ExecutorForDevice(0).value(),\n+      &debug_options_, &compiler_, /*use_default_config=*/true);\n+  // Parse an HLO module containing a kCustom Triton fusion with a backend\n+  // config that includes block-level tiling parameters.\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(R\"(\n+HloModule m\n+%wrapped_transpose_computation {\n+  %param_0 = f32[16,64]{1,0} parameter(0)\n+  ROOT %transpose.3.1 = f32[64,16]{1,0} transpose(%param_0), dimensions={1,0}\n+}\n+\n+ENTRY %main {\n+  %p0 = f32[16,64]{1,0} parameter(0), metadata={op_name=\"a\"}\n+  ROOT %wrapped_transpose = f32[64,16]{1,0} fusion(%p0), kind=kCustom,\n+  calls=%wrapped_transpose_computation,\n+  metadata={op_name=\"a\"},\n+  backend_config={\n+  \"fusion_backend_config\": {\n+    \"kind\": \"__triton\",\n+    \"block_level_fusion_config\": {\n+      \"output_tiles\": [\n+        {\"sizes\": [\"4\",\"16\"]}\n+      ],\n+      \"num_warps\": \"2\",\n+      \"num_ctas\": 1,\n+      \"num_stages\": 1\n+    }\n+  }}\n+}\n+)\"));\n+  // Call GetSupportedConfigs on the root instruction (the fusion op).`\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<std::unique_ptr<BackendConfig>> configs,\n+      backend.GetSupportedConfigs(\n+          *(module->entry_computation()->root_instruction())));\n+  // With the use_default_config flag set to true, we expect a single config\n+  // to be returned.\n+  ASSERT_EQ(configs.size(), 1);\n+  // We expect this config to be equal to the one in the HLO instruction.\n+  BlockLevelFusionConfig block_level_fusion_config;\n+  ASSERT_TRUE(configs[0]->UnpackTo(&block_level_fusion_config));\n+  EXPECT_THAT(block_level_fusion_config, EqualsProto(R\"pb(\n+                output_tiles { sizes: 4 sizes: 16 }\n+                num_warps: 2\n+                num_ctas: 1\n+                num_stages: 1\n+              )pb\"));\n+}\n+\n }  // namespace gpu\n }  // namespace xla"
        }
    ],
    "stats": {
        "total": 76,
        "additions": 74,
        "deletions": 2
    }
}