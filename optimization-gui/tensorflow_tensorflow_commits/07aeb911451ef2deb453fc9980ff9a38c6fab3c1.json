{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 814985615",
    "sha": "07aeb911451ef2deb453fc9980ff9a38c6fab3c1",
    "files": [
        {
            "sha": "3b0aed13cae6334079e7034484e376d71fc8cdf4",
            "filename": "tensorflow/core/kernels/data/flat_map_dataset_op.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Fflat_map_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Fflat_map_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fflat_map_dataset_op.cc?ref=07aeb911451ef2deb453fc9980ff9a38c6fab3c1",
            "patch": "@@ -440,7 +440,7 @@ class FlatMapDatasetOp::Dataset : public DatasetBase {\n               writer->WriteScalar(prefix(), kInputsSize, inputs_.size()));\n           for (int i = 0; i < inputs_.size(); i++) {\n             TF_RETURN_IF_ERROR(writer->WriteTensor(\n-                prefix(), strings::StrCat(kInputs, \"[\", i, \"]\"), inputs_[i]));\n+                prefix(), absl::StrCat(kInputs, \"[\", i, \"]\"), inputs_[i]));\n           }\n           TF_RETURN_IF_ERROR(SaveInput(ctx, writer, current_element_iterator_));\n         }\n@@ -558,7 +558,7 @@ class FlatMapDatasetOp::Dataset : public DatasetBase {\n       for (int i = 0; i < inputs_size; i++) {\n         inputs_.emplace_back();\n         TF_RETURN_IF_ERROR(reader->ReadTensor(\n-            ctx->flr(), prefix(), strings::StrCat(kInputs, \"[\", i, \"]\"),\n+            ctx->flr(), prefix(), absl::StrCat(kInputs, \"[\", i, \"]\"),\n             &inputs_.back()));\n       }\n "
        },
        {
            "sha": "fd44136d9e60109e59cbcfa4e5bfe4b6ebbec972",
            "filename": "tensorflow/core/kernels/data/interleave_dataset_op.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Finterleave_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Finterleave_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Finterleave_dataset_op.cc?ref=07aeb911451ef2deb453fc9980ff9a38c6fab3c1",
            "patch": "@@ -394,7 +394,7 @@ class InterleaveDatasetOp::Dataset : public DatasetBase {\n       for (int idx = 0; idx < current_elements_.size(); idx++) {\n         TF_RETURN_IF_ERROR(writer->WriteScalar(\n             prefix(),\n-            strings::StrCat(kCurrentElementsUninitialized, \"[\", idx, \"]\"),\n+            absl::StrCat(kCurrentElementsUninitialized, \"[\", idx, \"]\"),\n             !current_elements_[idx]));\n \n         if (!current_elements_[idx]) {\n@@ -405,16 +405,15 @@ class InterleaveDatasetOp::Dataset : public DatasetBase {\n               SaveInput(ctx, writer, current_elements_[idx]->iterator));\n           const auto& args = current_elements_[idx]->args;\n           TF_RETURN_IF_ERROR(writer->WriteScalar(\n-              prefix(), strings::StrCat(kArgsSize, \"[\", idx, \"]\"),\n-              args.size()));\n+              prefix(), absl::StrCat(kArgsSize, \"[\", idx, \"]\"), args.size()));\n           for (int i = 0; i < args.size(); i++) {\n             TF_RETURN_IF_ERROR(writer->WriteTensor(\n                 prefix(), strings::StrCat(kArgsList, \"[\", idx, \"][\", i, \"]\"),\n                 args[i]));\n           }\n         } else {\n           TF_RETURN_IF_ERROR(writer->WriteScalar(\n-              prefix(), strings::StrCat(kInputElementIndices, \"[\", idx, \"]\"),\n+              prefix(), absl::StrCat(kInputElementIndices, \"[\", idx, \"]\"),\n               current_elements_[idx]->input_element_index));\n         }\n       }\n@@ -430,15 +429,14 @@ class InterleaveDatasetOp::Dataset : public DatasetBase {\n         int64_t current_element_uninitialized;\n         TF_RETURN_IF_ERROR(reader.ReadScalar(\n             prefix(),\n-            strings::StrCat(kCurrentElementsUninitialized, \"[\", cycle_idx, \"]\"),\n+            absl::StrCat(kCurrentElementsUninitialized, \"[\", cycle_idx, \"]\"),\n             &current_element_uninitialized));\n \n         if (!current_element_uninitialized) {\n           int64_t input_element_index;\n \n           TF_RETURN_IF_ERROR(reader.ReadScalar(\n-              prefix(),\n-              strings::StrCat(kInputElementIndices, \"[\", cycle_idx, \"]\"),\n+              prefix(), absl::StrCat(kInputElementIndices, \"[\", cycle_idx, \"]\"),\n               &input_element_index));\n \n           input_offsets.push_back(\n@@ -586,7 +584,7 @@ class InterleaveDatasetOp::Dataset : public DatasetBase {\n         int64_t current_element_uninitialized;\n         TF_RETURN_IF_ERROR(reader->ReadScalar(\n             prefix(),\n-            strings::StrCat(kCurrentElementsUninitialized, \"[\", idx, \"]\"),\n+            absl::StrCat(kCurrentElementsUninitialized, \"[\", idx, \"]\"),\n             &current_element_uninitialized));\n         if (!current_element_uninitialized) {\n           if (!ctx->symbolic_checkpoint()) {\n@@ -597,8 +595,7 @@ class InterleaveDatasetOp::Dataset : public DatasetBase {\n             std::vector<Tensor> current_element_args;\n \n             TF_RETURN_IF_ERROR(reader->ReadScalar(\n-                prefix(), strings::StrCat(kArgsSize, \"[\", idx, \"]\"),\n-                &args_size));\n+                prefix(), absl::StrCat(kArgsSize, \"[\", idx, \"]\"), &args_size));\n             current_element_args.resize(args_size);\n \n             for (int i = 0; i < args_size; i++) {"
        },
        {
            "sha": "13b0cb5524546ad6402e8a2617f260e27a286fea",
            "filename": "tensorflow/core/kernels/data/map_defun_op_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Fmap_defun_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Fmap_defun_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fmap_defun_op_test.cc?ref=07aeb911451ef2deb453fc9980ff9a38c6fab3c1",
            "patch": "@@ -52,12 +52,11 @@ class MapDefunOpParams : public DatasetParams {\n \n     input_names->reserve(arguments_.size() + captured_inputs_.size());\n     for (int i = 0; i < arguments_.size(); ++i) {\n-      input_names->emplace_back(\n-          strings::StrCat(MapDefunOp::kArguments, \"_\", i));\n+      input_names->emplace_back(absl::StrCat(MapDefunOp::kArguments, \"_\", i));\n     }\n     for (int i = 0; i < captured_inputs_.size(); ++i) {\n       input_names->emplace_back(\n-          strings::StrCat(MapDefunOp::kCapturedInputs, \"_\", i));\n+          absl::StrCat(MapDefunOp::kCapturedInputs, \"_\", i));\n     }\n     return absl::OkStatus();\n   }"
        },
        {
            "sha": "d105421996a64a1b1437215bbc8573b07282254f",
            "filename": "tensorflow/core/kernels/data/model_dataset_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Fmodel_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Fmodel_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fmodel_dataset_op.cc?ref=07aeb911451ef2deb453fc9980ff9a38c6fab3c1",
            "patch": "@@ -77,7 +77,7 @@ class ModelDatasetOp::Dataset : public DatasetBase {\n   std::unique_ptr<IteratorBase> MakeIteratorInternal(\n       const string& prefix) const override {\n     return std::make_unique<Iterator>(\n-        Iterator::Params{this, strings::StrCat(prefix, \"::Model\")});\n+        Iterator::Params{this, absl::StrCat(prefix, \"::Model\")});\n   }\n \n   const DataTypeVector& output_dtypes() const override {"
        },
        {
            "sha": "5ac49719ad76deb389ba5b23d2ec96324694a434",
            "filename": "tensorflow/core/kernels/data/multi_device_iterator_ops.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Fmulti_device_iterator_ops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Fmulti_device_iterator_ops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fmulti_device_iterator_ops.cc?ref=07aeb911451ef2deb453fc9980ff9a38c6fab3c1",
            "patch": "@@ -105,8 +105,8 @@ class MultiDeviceIterator : public ResourceBase {\n   }\n \n   string DebugString() const override {\n-    return strings::StrCat(\"MultiDeviceIterator for \", devices_.size(),\n-                           \" devices\");\n+    return absl::StrCat(\"MultiDeviceIterator for \", devices_.size(),\n+                        \" devices\");\n   }\n \n   absl::Status Init(std::unique_ptr<IteratorBase> iterator,\n@@ -532,8 +532,8 @@ class MultiDeviceIteratorHandleOp : public OpKernel {\n         MultiDeviceIterator* resource;\n \n         if (name_ == ResourceHandle::ANONYMOUS_NAME) {\n-          unique_name = strings::StrCat(\"_AnonymousMultiDeviceIterator\",\n-                                        current_id_.fetch_add(1));\n+          unique_name = absl::StrCat(\"_AnonymousMultiDeviceIterator\",\n+                                     current_id_.fetch_add(1));\n           container_name = kAnonymousMultiDeviceIterator;\n           resource = new MultiDeviceIterator(\n               context->env(), output_types_, output_shapes_, devices_,"
        },
        {
            "sha": "91853d3c05b7f003c4c9687fc3c7bd47dd64b316",
            "filename": "tensorflow/core/kernels/data/optional_ops_util.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Foptional_ops_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Foptional_ops_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Foptional_ops_util.h?ref=07aeb911451ef2deb453fc9980ff9a38c6fab3c1",
            "patch": "@@ -83,14 +83,14 @@ class OptionalVariant {\n \n   string DebugString() const {\n     if (values_) {\n-      return strings::StrCat(\"OptionalVariant<\", \"values: (\",\n-                             absl::StrJoin(*values_, \", \",\n-                                           [](string* s, const Tensor& elem) {\n-                                             *s = elem.DebugString();\n-                                           }),\n-                             \")>\");\n+      return absl::StrCat(\"OptionalVariant<\", \"values: (\",\n+                          absl::StrJoin(*values_, \", \",\n+                                        [](string* s, const Tensor& elem) {\n+                                          *s = elem.DebugString();\n+                                        }),\n+                          \")>\");\n     } else {\n-      return strings::StrCat(\"OptionalVariant<None>\");\n+      return absl::StrCat(\"OptionalVariant<None>\");\n     }\n   }\n "
        },
        {
            "sha": "e8b39498a8d764160bdf968ab11042b9da3dfe00",
            "filename": "tensorflow/core/kernels/data/padded_batch_dataset_op_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Fpadded_batch_dataset_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Fpadded_batch_dataset_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fpadded_batch_dataset_op_test.cc?ref=07aeb911451ef2deb453fc9980ff9a38c6fab3c1",
            "patch": "@@ -68,12 +68,12 @@ class PaddedBatchDatasetParams : public DatasetParams {\n     // Create the input names for the input padded_shapes.\n     for (int i = 0; i < num_padded_shapes_; ++i) {\n       input_names->emplace_back(\n-          strings::StrCat(PaddedBatchDatasetOp::kPaddedShapes, \"_\", i));\n+          absl::StrCat(PaddedBatchDatasetOp::kPaddedShapes, \"_\", i));\n     }\n     // Create the input names for the input padding_values.\n     for (int j = 0; j < padded_values_.size(); ++j) {\n       input_names->emplace_back(\n-          strings::StrCat(PaddedBatchDatasetOp::kPaddingValues, \"_\", j));\n+          absl::StrCat(PaddedBatchDatasetOp::kPaddingValues, \"_\", j));\n     }\n     input_names->push_back(PaddedBatchDatasetOp::kDropRemainder);\n     return absl::OkStatus();"
        },
        {
            "sha": "65779d4e53ef99e24cda86148964a3d4166e3aa9",
            "filename": "tensorflow/core/kernels/data/parallel_batch_dataset_op.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 15,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Fparallel_batch_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Fparallel_batch_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fparallel_batch_dataset_op.cc?ref=07aeb911451ef2deb453fc9980ff9a38c6fab3c1",
            "patch": "@@ -555,22 +555,22 @@ class ParallelBatchDatasetOp::Dataset : public DatasetBase {\n         TF_EXCLUSIVE_LOCKS_REQUIRED(*mu_) {\n       batch_results_.push_back(std::make_shared<BatchResult>(ctx));\n       std::shared_ptr<BatchResult> result = batch_results_.back();\n-      string batch_prefix = strings::StrCat(kBatchResults, \"_\", index);\n+      string batch_prefix = absl::StrCat(kBatchResults, \"_\", index);\n       mutex_lock l(result->mu);\n       result->end_of_input = reader->Contains(\n-          prefix(), strings::StrCat(batch_prefix, \"_\", kEndOfInput));\n+          prefix(), absl::StrCat(batch_prefix, \"_\", kEndOfInput));\n       TF_RETURN_IF_ERROR(reader->ReadScalar(\n-          prefix(), strings::StrCat(batch_prefix, \"_\", kNumElements),\n+          prefix(), absl::StrCat(batch_prefix, \"_\", kNumElements),\n           &result->num_elements));\n       result->call_finished = reader->Contains(\n-          prefix(), strings::StrCat(batch_prefix, \"_\", kCallFinished));\n+          prefix(), absl::StrCat(batch_prefix, \"_\", kCallFinished));\n       result->output_allocated = reader->Contains(\n-          prefix(), strings::StrCat(batch_prefix, \"_\", kOutputAllocated));\n+          prefix(), absl::StrCat(batch_prefix, \"_\", kOutputAllocated));\n \n       TF_RETURN_IF_ERROR(ReadBatch(ctx, reader, dataset()->batch_size_,\n                                    prefix(), batch_prefix, &result->output));\n       TF_RETURN_IF_ERROR(ReadStatus(prefix(),\n-                                    strings::StrCat(batch_prefix, \"_\", kStatus),\n+                                    absl::StrCat(batch_prefix, \"_\", kStatus),\n                                     reader, &result->status));\n       if (result->output_allocated) {\n         RecordBufferEnqueue(ctx, result->output);\n@@ -581,31 +581,30 @@ class ParallelBatchDatasetOp::Dataset : public DatasetBase {\n     absl::Status WriteBatchResult(IteratorStateWriter* writer, size_t index)\n         TF_EXCLUSIVE_LOCKS_REQUIRED(*mu_) {\n       std::shared_ptr<BatchResult> result = batch_results_[index];\n-      string batch_prefix = strings::StrCat(kBatchResults, \"_\", index);\n+      string batch_prefix = absl::StrCat(kBatchResults, \"_\", index);\n       mutex_lock l(result->mu);\n       if (result->end_of_input) {\n         TF_RETURN_IF_ERROR(writer->WriteScalar(\n-            prefix(), strings::StrCat(batch_prefix, \"_\", kEndOfInput), \"\"));\n+            prefix(), absl::StrCat(batch_prefix, \"_\", kEndOfInput), \"\"));\n       }\n       TF_RETURN_IF_ERROR(writer->WriteScalar(\n-          prefix(), strings::StrCat(batch_prefix, \"_\", kNumElements),\n+          prefix(), absl::StrCat(batch_prefix, \"_\", kNumElements),\n           result->num_elements));\n       if (result->call_finished) {\n         TF_RETURN_IF_ERROR(writer->WriteScalar(\n-            prefix(), strings::StrCat(batch_prefix, \"_\", kCallFinished), \"\"));\n+            prefix(), absl::StrCat(batch_prefix, \"_\", kCallFinished), \"\"));\n       }\n       if (result->output_allocated) {\n         TF_RETURN_IF_ERROR(writer->WriteScalar(\n-            prefix(), strings::StrCat(batch_prefix, \"_\", kOutputAllocated),\n-            \"\"));\n+            prefix(), absl::StrCat(batch_prefix, \"_\", kOutputAllocated), \"\"));\n       }\n \n       TF_RETURN_IF_ERROR(WriteBatch(dataset()->batch_size_,\n                                     result->num_elements, prefix(),\n                                     batch_prefix, writer, &result->output));\n-      TF_RETURN_IF_ERROR(\n-          WriteStatus(prefix(), strings::StrCat(batch_prefix, \"_\", kStatus),\n-                      result->status, writer));\n+      TF_RETURN_IF_ERROR(WriteStatus(prefix(),\n+                                     absl::StrCat(batch_prefix, \"_\", kStatus),\n+                                     result->status, writer));\n       return absl::OkStatus();\n     }\n "
        },
        {
            "sha": "51d3728fda153a1263916667bad669b3336f823a",
            "filename": "tensorflow/core/kernels/data/parallel_interleave_dataset_op.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Fparallel_interleave_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Fparallel_interleave_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fparallel_interleave_dataset_op.cc?ref=07aeb911451ef2deb453fc9980ff9a38c6fab3c1",
            "patch": "@@ -1587,9 +1587,9 @@ class ParallelInterleaveDatasetOp::Dataset : public DatasetBase {\n \n     std::string DebugString() TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n       std::string result;\n-      result.append(strings::StrCat(\"Cycle index: \", cycle_index_, \"\\n\"));\n-      result.append(strings::StrCat(\"Block index: \", block_index_, \"\\n\"));\n-      result.append(strings::StrCat(\"End of input: \", end_of_input_, \"\\n\"));\n+      result.append(absl::StrCat(\"Cycle index: \", cycle_index_, \"\\n\"));\n+      result.append(absl::StrCat(\"Block index: \", block_index_, \"\\n\"));\n+      result.append(absl::StrCat(\"End of input: \", end_of_input_, \"\\n\"));\n       {\n         result.append(\"Current elements:\\n\");\n         for (int i = 0; i < current_elements_.size(); ++i) {"
        },
        {
            "sha": "1938b6732a499e7c7b13d65db8eb5f5fc106586d",
            "filename": "tensorflow/core/kernels/data/reduce_dataset_op_test.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Freduce_dataset_op_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Freduce_dataset_op_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Freduce_dataset_op_test.cc?ref=07aeb911451ef2deb453fc9980ff9a38c6fab3c1",
            "patch": "@@ -61,10 +61,10 @@ class ReduceDatasetParams : public DatasetParams {\n     input_names->clear();\n     input_names->emplace_back(\"input_dataset\");\n     for (int i = 0; i < initial_state_.size(); ++i) {\n-      input_names->emplace_back(strings::StrCat(\"initial_state_\", i));\n+      input_names->emplace_back(absl::StrCat(\"initial_state_\", i));\n     }\n     for (int i = 0; i < other_arguments_.size(); ++i) {\n-      input_names->emplace_back(strings::StrCat(\"other_arguments_\", i));\n+      input_names->emplace_back(absl::StrCat(\"other_arguments_\", i));\n     }\n     return absl::OkStatus();\n   }"
        },
        {
            "sha": "791e910afcbdac3aefe1f690e3dcd1499881655b",
            "filename": "tensorflow/core/kernels/data/repeat_dataset_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Frepeat_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Frepeat_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Frepeat_dataset_op.cc?ref=07aeb911451ef2deb453fc9980ff9a38c6fab3c1",
            "patch": "@@ -58,7 +58,7 @@ constexpr char kUninitialized[] = \"uninitialized\";\n constexpr int64_t kKnownRatio = 1;\n \n std::string nested_prefix(const std::string& prefix, int64_t epoch) {\n-  return strings::StrCat(prefix, \"[\", epoch, \"]\");\n+  return absl::StrCat(prefix, \"[\", epoch, \"]\");\n }\n \n // Returns whether `dataset` has an input dataset of the given type. This check"
        },
        {
            "sha": "97aed153888fc80fd25aa0e923fce82e92c10737",
            "filename": "tensorflow/core/kernels/data/sparse_tensor_slice_dataset_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Fsparse_tensor_slice_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Fsparse_tensor_slice_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fsparse_tensor_slice_dataset_op.cc?ref=07aeb911451ef2deb453fc9980ff9a38c6fab3c1",
            "patch": "@@ -42,7 +42,7 @@ class Dataset : public DatasetBase {\n   std::unique_ptr<IteratorBase> MakeIteratorInternal(\n       const string& prefix) const override {\n     return std::make_unique<Iterator>(typename Iterator::Params{\n-        this, strings::StrCat(prefix, \"::SparseTensorSlice\")});\n+        this, absl::StrCat(prefix, \"::SparseTensorSlice\")});\n   }\n \n   const DataTypeVector& output_dtypes() const override { return dtypes_; }"
        },
        {
            "sha": "b381f28def6ea46f0635920b7c03cc92c9b2a6b9",
            "filename": "tensorflow/core/kernels/data/zip_dataset_op.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Fzip_dataset_op.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/07aeb911451ef2deb453fc9980ff9a38c6fab3c1/tensorflow%2Fcore%2Fkernels%2Fdata%2Fzip_dataset_op.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fkernels%2Fdata%2Fzip_dataset_op.cc?ref=07aeb911451ef2deb453fc9980ff9a38c6fab3c1",
            "patch": "@@ -185,7 +185,7 @@ class ZipDatasetOp::Dataset : public DatasetBase {\n       input_impls_.resize(dataset()->inputs_.size());\n       for (size_t i = 0; i < input_impls_.size(); ++i) {\n         TF_RETURN_IF_ERROR(dataset()->inputs_[i]->MakeIterator(\n-            &input_contexts_[i], this, strings::StrCat(prefix(), \"[\", i, \"]\"),\n+            &input_contexts_[i], this, absl::StrCat(prefix(), \"[\", i, \"]\"),\n             &input_impls_[i]));\n         ctx->MergeCheckpoint(input_contexts_[i].checkpoint());\n       }"
        }
    ],
    "stats": {
        "total": 99,
        "additions": 47,
        "deletions": 52
    }
}