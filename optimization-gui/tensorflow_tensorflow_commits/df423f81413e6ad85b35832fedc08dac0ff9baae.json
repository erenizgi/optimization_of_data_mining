{
    "author": "pifon2a",
    "message": "[XLA:GPU] Move LLVMIR emitters out of ThunkEmitter and remove IREmitter base class.\n\nPiperOrigin-RevId: 837424830",
    "sha": "df423f81413e6ad85b35832fedc08dac0ff9baae",
    "files": [
        {
            "sha": "a9d64d9310e0abe7b60b135c39eb746b942e741e",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 16,
            "deletions": 5,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/df423f81413e6ad85b35832fedc08dac0ff9baae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/df423f81413e6ad85b35832fedc08dac0ff9baae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=df423f81413e6ad85b35832fedc08dac0ff9baae",
            "patch": "@@ -419,11 +419,9 @@ cc_library(\n \n cc_library(\n     name = \"ir_emitter_context\",\n-    srcs = [\"ir_emitter_context.cc\"],\n     hdrs = [\"ir_emitter_context.h\"],\n     deps = [\n         \":execution_stream_assignment\",\n-        \":gpu_constants\",\n         \":gpu_executable\",\n         \":ir_emission_utils\",\n         \":kernel_reuse_cache\",\n@@ -435,13 +433,11 @@ cc_library(\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:call_inliner\",\n         \"//xla/service:name_uniquer\",\n+        \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor/cuda:cuda_compute_capability\",\n-        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/strings:string_view\",\n-        \"@llvm-project//llvm:Support\",\n-        \"@llvm-project//llvm:TargetParser\",\n         \"@llvm-project//llvm:ir_headers\",\n         \"@llvm-project//mlir:IR\",\n     ],\n@@ -624,20 +620,32 @@ cc_library(\n         \":hlo_to_ir_bindings\",\n         \":ir_emission_utils\",\n         \":ir_emitter_context\",\n+        \":launch_dimensions\",\n+        \":parallel_loop_emitter\",\n         \"//xla:literal\",\n         \"//xla:shape_util\",\n         \"//xla:status_macros\",\n         \"//xla:util\",\n+        \"//xla/backends/gpu/codegen:fusion_emitter\",\n+        \"//xla/backends/gpu/runtime:kernel_thunk\",\n+        \"//xla/backends/gpu/runtime:thunk\",\n+        \"//xla/backends/gpu/runtime:thunk_id\",\n         \"//xla/codegen/emitters:computation_fingerprint\",\n+        \"//xla/codegen/emitters:kernel_arguments\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/service:buffer_assignment\",\n         \"//xla/service:elemental_ir_emitter\",\n         \"//xla/service/llvm_ir:buffer_assignment_util\",\n         \"//xla/service/llvm_ir:fused_ir_emitter\",\n         \"//xla/service/llvm_ir:ir_array\",\n         \"//xla/service/llvm_ir:ir_builder_mixin\",\n+        \"//xla/service/llvm_ir:kernel_support_library\",\n         \"//xla/service/llvm_ir:llvm_util\",\n         \"//xla/service/llvm_ir:loop_emitter\",\n+        \"//xla/service/llvm_ir:sort_util\",\n         \"//xla/service/llvm_ir:tuple_ops\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor/gpu:tma_metadata\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n@@ -646,6 +654,7 @@ cc_library(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Core\",\n         \"@llvm-project//llvm:Support\",\n@@ -900,6 +909,7 @@ cc_library(\n     deps = [\n         \":backend_configs_cc\",\n         \":ir_emission_utils_proto_cc\",\n+        \":launch_dimensions\",\n         \":target_util\",\n         \"//xla:literal\",\n         \"//xla:permutation_util\",\n@@ -925,6 +935,7 @@ cc_library(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Core\",\n         \"@llvm-project//llvm:Support\","
        },
        {
            "sha": "a749ef0923f87b3b7fee60d0f608d5b988881572",
            "filename": "third_party/xla/xla/service/gpu/ir_emission_utils.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/df423f81413e6ad85b35832fedc08dac0ff9baae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/df423f81413e6ad85b35832fedc08dac0ff9baae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emission_utils.cc?ref=df423f81413e6ad85b35832fedc08dac0ff9baae",
            "patch": "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/strings/escaping.h\"\n #include \"absl/strings/match.h\"\n+#include \"absl/strings/str_format.h\"\n #include \"absl/strings/str_join.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/strings/substitute.h\"\n@@ -57,6 +58,7 @@ limitations under the License.\n #include \"xla/primitive_util.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n+#include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/service/gpu/target_util.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/service/matmul_indexing_utils.h\"\n@@ -759,5 +761,6 @@ DenseDataIntermediate DenseDataIntermediate::FromProto(\n   return DenseDataIntermediate::Own(\n       std::vector<uint8_t>(data.begin(), data.end()));\n }\n+\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "defdd95413c2b5fb0cb819d749d6f109f50a84ae",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter.cc",
            "status": "modified",
            "additions": 669,
            "deletions": 0,
            "changes": 669,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/df423f81413e6ad85b35832fedc08dac0ff9baae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/df423f81413e6ad85b35832fedc08dac0ff9baae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter.cc?ref=df423f81413e6ad85b35832fedc08dac0ff9baae",
            "patch": "@@ -15,29 +15,56 @@ limitations under the License.\n \n #include \"xla/service/gpu/ir_emitter.h\"\n \n+#include <algorithm>\n #include <cstdint>\n+#include <memory>\n+#include <optional>\n+#include <string>\n #include <utility>\n #include <vector>\n \n // IWYU pragma: no_include \"llvm/IR/Intrinsics.gen.inc\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/strings/str_join.h\"\n #include \"absl/types/span.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/IR/BasicBlock.h\"\n #include \"llvm/IR/DerivedTypes.h\"\n #include \"llvm/IR/IRBuilder.h\"\n #include \"llvm/IR/Instructions.h\"\n #include \"llvm/IR/Module.h\"\n+#include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n+#include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n+#include \"xla/backends/gpu/runtime/thunk_id.h\"\n+#include \"xla/codegen/emitters/kernel_arguments.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/primitive_util.h\"\n+#include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/elemental_ir_emitter.h\"\n+#include \"xla/service/gpu/gpu_constants.h\"\n+#include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/ir_emitter_context.h\"\n #include \"xla/service/gpu/ir_emitter_nested.h\"\n+#include \"xla/service/gpu/launch_dimensions.h\"\n+#include \"xla/service/gpu/parallel_loop_emitter.h\"\n #include \"xla/service/llvm_ir/fused_ir_emitter.h\"\n #include \"xla/service/llvm_ir/ir_array.h\"\n+#include \"xla/service/llvm_ir/kernel_support_library.h\"\n #include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/service/llvm_ir/loop_emitter.h\"\n+#include \"xla/service/llvm_ir/sort_util.h\"\n #include \"xla/service/llvm_ir/tuple_ops.h\"\n+#include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n+#include \"xla/status_macros.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/gpu/tma_metadata.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n \n namespace xla {\n@@ -210,6 +237,11 @@ std::vector<llvm_ir::IrArray> IrEmitter::ConstructIrArrayForOutputs(\n   return output_arrays;\n }\n \n+absl::Status IrEmitter::EmitTargetElementLoop(\n+    const HloInstruction& hlo, const llvm_ir::ElementGenerator& body_emitter) {\n+  return Internal(\"This should be unreachable\");\n+}\n+\n void IrEmitter::BindFusionArguments(const HloInstruction* fusion,\n                                     FusedIrEmitter* fused_emitter) {\n   for (int i = 0; i < fusion->operand_count(); i++) {\n@@ -223,5 +255,642 @@ void IrEmitter::BindFusionArguments(const HloInstruction* fusion,\n   }\n }\n \n+namespace {\n+\n+struct KernelThunkInfo {\n+  std::vector<llvm_ir::IrArray> ir_arrays;\n+  std::unique_ptr<Thunk> thunk;\n+};\n+\n+absl::StatusOr<KernelThunkInfo> BuildKernelThunkForNonFusionOp(\n+    llvm::Module* llvm_module, const HloInstruction* hlo,\n+    const BufferAssignment& buffer_assignment, ThunkId thunk_id,\n+    const se::DeviceDescription& gpu_device_info,\n+    const std::string& sanitized_kernel_name,\n+\n+    IrEmitter& ir_emitter, const LaunchDimensions& launch_dimensions) {\n+  std::string suggested_kernel_name(hlo->name());\n+\n+  TF_ASSIGN_OR_RETURN(auto kernel_arguments,\n+                      emitters::KernelArguments::Create(\n+                          buffer_assignment, GetDefaultBufferAlignment(), hlo));\n+\n+  VLOG(3) << \"Generating (without reuse check): \" << suggested_kernel_name;\n+\n+  TF_ASSIGN_OR_RETURN(\n+      llvm::Function * kernel,\n+      BuildKernelPrototype(llvm_module, gpu_device_info, suggested_kernel_name,\n+                           sanitized_kernel_name, kernel_arguments,\n+                           launch_dimensions, ir_emitter.builder()));\n+\n+  auto thunk = std::make_unique<KernelThunk>(\n+      Thunk::ThunkInfo::WithProfileAnnotation(hlo, thunk_id),\n+      kernel->getName().str(), kernel_arguments, launch_dimensions,\n+      /*cluster_dim=*/std::nullopt,\n+      /*shmem_bytes=*/0,\n+      /*tma_metadata=*/se::gpu::TmaMetadata());\n+\n+  std::vector<llvm_ir::IrArray> ir_arrays;\n+  ir_arrays.reserve(kernel_arguments.args().size());\n+  for (const auto& [kernel_argument, llvm_arg] :\n+       llvm::zip(kernel_arguments.args(), kernel->args())) {\n+    llvm::Type* ir_type =\n+        llvm_ir::ShapeToIrType(kernel_argument.shape(), llvm_arg.getContext());\n+    llvm_ir::IrArray ir_array(&llvm_arg, ir_type, kernel_argument.shape());\n+\n+    if (!kernel_argument.written()) {\n+      ir_array.MarkInvariantOverWholeProgram(&llvm_arg.getContext());\n+    }\n+    ir_arrays.push_back(ir_array);\n+  }\n+  return {KernelThunkInfo{ir_arrays, std::move(thunk)}};\n+}\n+\n+llvm::Value* CreateLoad(llvm::Value* address, llvm::Type* data_type,\n+                        int alignment_bytes, llvm::IRBuilderBase* b) {\n+  int data_bytes = data_type->getPrimitiveSizeInBits() /\n+                   primitive_util::BitWidth(PrimitiveType::U8);\n+  if (alignment_bytes == 0) {\n+    return b->CreateLoad(data_type, address);\n+  }\n+\n+  int alignment_bitwidth =\n+      alignment_bytes * primitive_util::BitWidth(PrimitiveType::U8);\n+\n+  llvm::Value* output = llvm::ConstantInt::get(data_type, 0);\n+  for (int offset_bytes = 0; offset_bytes < data_bytes;\n+       offset_bytes += alignment_bytes) {\n+    llvm::Value* offset_address = b->CreateConstInBoundsGEP1_32(\n+        b->getInt8Ty(), address, offset_bytes, \"offset_address\");\n+    llvm::Value* partial_value = b->CreateLoad(b->getIntNTy(alignment_bitwidth),\n+                                               offset_address, \"partial_value\");\n+    llvm::Value* zextd =\n+        b->CreateZExt(partial_value, output->getType(), \"partial_value_zextd\");\n+    llvm::Value* shifted = b->CreateShl(\n+        zextd, llvm::ConstantInt::get(b->getInt32Ty(), offset_bytes),\n+        \"partial_input_shifted\");\n+    output = b->CreateAdd(output, shifted, \"output_updated\");\n+  }\n+  return output;\n+}\n+\n+void CreateStore(llvm::Value* data, llvm::Value* address, int alignment_bytes,\n+                 llvm::IRBuilderBase* b) {\n+  int data_bytes = data->getType()->getPrimitiveSizeInBits() /\n+                   primitive_util::BitWidth(PrimitiveType::U8);\n+  CHECK_GE(data_bytes, alignment_bytes);\n+  if (alignment_bytes == 0) {\n+    b->CreateStore(data, address);\n+    return;\n+  }\n+\n+  int alignment_bitwidth =\n+      alignment_bytes * primitive_util::BitWidth(PrimitiveType::U8);\n+\n+  for (int offset_bytes = 0; offset_bytes < data_bytes;\n+       offset_bytes += alignment_bytes) {\n+    llvm::Value* offset_address = b->CreateConstInBoundsGEP1_32(\n+        b->getInt8Ty(), address, offset_bytes, \"offset_address\");\n+    llvm::Value* shifted_partial = b->CreateTrunc(\n+        b->CreateLShr(data,\n+                      llvm::ConstantInt::get(b->getInt32Ty(), offset_bytes)),\n+        b->getIntNTy(alignment_bitwidth), \"truncated_value\");\n+    b->CreateStore(shifted_partial, offset_address);\n+  }\n+}\n+\n+}  // namespace\n+\n+absl::StatusOr<ThunkSequence> EmitBitonicSortLLVMIR(\n+    const HloSortInstruction* sort, llvm::Module* llvm_module,\n+    IrEmitterContext* ir_emitter_context) {\n+  std::string op_name(sort->name());\n+\n+  // Copy of the main context with the local module.\n+  IrEmitterContext local_ir_emitter_context(\n+      &ir_emitter_context->hlo_module(),\n+      &ir_emitter_context->buffer_assignment(),\n+      &ir_emitter_context->execution_stream_assignment(),\n+      std::string(ir_emitter_context->platform_name()),\n+      ir_emitter_context->gpu_device_info(), ir_emitter_context->mlir_context(),\n+      llvm_module, ir_emitter_context->llvm_module_constants(),\n+      ir_emitter_context->emit_kernels());\n+\n+  IrEmitter ir_emitter(&local_ir_emitter_context, /*nested=*/false);\n+\n+  int64_t dimension_to_sort = sort->sort_dimension();\n+  const Shape& keys_shape = sort->operand(0)->shape();\n+  uint64_t dimension_to_sort_bound = keys_shape.dimensions(dimension_to_sort);\n+  int64_t num_stages = Log2Ceiling(dimension_to_sort_bound);\n+  VLOG(2) << op_name << \" requires \" << num_stages << \" stages.\";\n+  CHECK_GE(1ULL << num_stages, dimension_to_sort_bound);\n+  CHECK_LT(1ULL << (num_stages - 1), dimension_to_sort_bound);\n+\n+  // Naive C++ code for the outer loops:\n+  //\n+  // for (int64_t stage = 0; stage <\n+  // Log2Ceiling(dimension_to_sort_bound);\n+  //     ++stage) {\n+  //   int64_t first_xor_mask = (1LL << (stage + 1)) - 1;\n+  //   SortInPlace(first_xor_mask);\n+  //   for (int64_t mask = stage - 1; mask >= 0; --mask) {\n+  //     int64_t later_xor_mask = 1LL << mask;\n+  //     SortInPlace(later_xor_mask);\n+  //   }\n+  // }\n+  //\n+  // This follows the alternative representation of the algorithm\n+  // described on Wikipedia:\n+  // https://en.wikipedia.org/wiki/Bitonic_sorter\n+  //\n+  // Each mask specifies how to derive from one position in the\n+  // array the position with which it should be compared (we\n+  // calculate the xor of the position with the mask). As an\n+  // optimization, we can move the 'mask' loop to inside the\n+  // sorting/comparison loop if the comparisons happen within a\n+  // small block of the array. To make this work, we collect all\n+  // consecutive masks that are smaller than our chosen power of 2\n+  // tile size, and pass them to SortInPlace. Each block then\n+  // processes one tile of data.\n+\n+  const uint64_t kUnrollFactor = 4;\n+  // Determine the total element size of all sort operands. We need to choose a\n+  // tile size such that we have enough shared memory to store a tile of\n+  // elements from each operand.\n+  uint64_t total_element_size = 0;\n+  for (int64_t i = 0; i < sort->operand_count(); ++i) {\n+    total_element_size += ShapeUtil::ByteSizeOfPrimitiveType(\n+        sort->operand(i)->shape().element_type());\n+  }\n+  const uint64_t kMaxSharedMemoryPerBlock =\n+      ir_emitter_context->gpu_device_info().shared_memory_per_block();\n+  uint64_t max_tile_size_fitting_into_shared_memory =\n+      kMaxSharedMemoryPerBlock / total_element_size;\n+  const uint64_t kMaxThreadsPerBlock =\n+      ir_emitter_context->gpu_device_info().threads_per_block_limit();\n+  // Choose the tile size based on actual amount of elements to sort, the amount\n+  // of shared memory available, and the maximum number of threads per block.\n+  uint64_t tile_size =\n+      std::min(std::min(kMaxThreadsPerBlock * kUnrollFactor,\n+                        max_tile_size_fitting_into_shared_memory),\n+               uint64_t{1} << num_stages);\n+  // The tile size needs to be a power of 2.\n+  tile_size = uint64_t{1} << Log2Floor(tile_size);\n+\n+  // If we cannot combine several xor masks together, we don't use\n+  // tiling, so we calculate the standard launch dimensions for the\n+  // shape. However we only need to iterate through ~half of the\n+  // dimension to sort (rounded up to the next highest power of 2),\n+  // because each iteration compares one pair of elements.\n+  Shape standard_iteration_shape = keys_shape;\n+  uint64_t standard_num_iterations_in_sort_dim = 1ULL << (num_stages - 1);\n+  standard_iteration_shape.set_dimensions(\n+      dimension_to_sort,\n+      CeilOfRatio(standard_num_iterations_in_sort_dim, kUnrollFactor));\n+\n+  LaunchDimensions standard_launch_dimensions = CalculateLaunchDimensions(\n+      standard_iteration_shape, ir_emitter_context->gpu_device_info());\n+\n+  // Calculate the launch dimensions for the case where we use\n+  // tiling. We split the dimension that should be sorted into tiles\n+  // of size 'tile_size'. This means we first need to round\n+  // 'dimension_to_sort_bound' up to be a multiple of the tile size.\n+  uint64_t rounded_bound = RoundUpTo(dimension_to_sort_bound, tile_size);\n+  Shape iteration_shape = keys_shape;\n+\n+  // We iterate through the element pairs that should be compared.\n+  uint64_t num_iterations_in_sort_dim =\n+      CeilOfRatio(rounded_bound, kUnrollFactor);\n+  iteration_shape.set_dimensions(dimension_to_sort, num_iterations_in_sort_dim);\n+  uint64_t num_iterations = ShapeUtil::ElementsIn(iteration_shape);\n+\n+  // For correctness reasons we need exactly `tile_size` / `kUnrollFactor` many\n+  // threads per block. Each thread is responsible for copying\n+  // exactly `kUnrollFactor` many adjacent elements into shared memory, and then\n+  // does `kUnrollFactor` / 2 many comparisons of two elements taken from shared\n+  // memory.\n+  const uint64_t kThreadsPerBlock =\n+      std::max(uint64_t{1}, tile_size / kUnrollFactor);\n+\n+  uint64_t num_blocks = CeilOfRatio(num_iterations, kThreadsPerBlock);\n+  LaunchDimensions tiled_launch_dimensions(num_blocks, kThreadsPerBlock);\n+  VLOG(2) << absl::StreamFormat(\"%s launch dims: %d blocks, %d threads/block\",\n+                                op_name, num_blocks, kThreadsPerBlock);\n+  ThunkSequence thunks;\n+  auto emit_kernel = [&](absl::Span<const int64_t> xor_masks) {\n+    VLOG(2) << absl::StreamFormat(\n+        \"%s uses kernel for xor masks [%s]\", op_name,\n+        absl::StrJoin(xor_masks, \", \", [](std::string* out, int64_t xor_mask) {\n+          absl::StrAppendFormat(out, \"0x%x\", xor_mask);\n+        }));\n+    LaunchDimensions launch_dimensions = xor_masks.size() > 1\n+                                             ? tiled_launch_dimensions\n+                                             : standard_launch_dimensions;\n+    TF_ASSIGN_OR_RETURN(\n+        KernelThunkInfo kernel_thunk_info,\n+        BuildKernelThunkForNonFusionOp(\n+            llvm_module, sort, ir_emitter_context->buffer_assignment(),\n+            ir_emitter_context->GetNextThunkId(),\n+            ir_emitter_context->gpu_device_info(),\n+            ir_emitter_context->GetSanitizedUniqueName(op_name), ir_emitter,\n+            launch_dimensions));\n+    thunks.push_back(std::move(kernel_thunk_info.thunk));\n+\n+    // The first `operand_count()` elements of `ir_arrays` are the input\n+    // operands and the rest are the output arrays. Inputs are aliases with\n+    // outputs, so we need to pass only the outputs to the in-place sort kernel.\n+    auto output_arrays_span =\n+        absl::Span<const llvm_ir::IrArray>(kernel_thunk_info.ir_arrays)\n+            .subspan(sort->operand_count());\n+\n+    auto* comparator = sort->called_computations().front();\n+    auto* builder = ir_emitter.builder();\n+    return llvm_ir::EmitSortInPlace(\n+        dimension_to_sort, output_arrays_span, llvm_ir::IrName(op_name),\n+        xor_masks, ir_emitter.builder(), launch_dimensions,\n+        xor_masks.size() > 1 ? num_iterations_in_sort_dim\n+                             : standard_num_iterations_in_sort_dim,\n+        tile_size, kUnrollFactor,\n+        [&](absl::Span<llvm::Value* const> operands, llvm::Value* output) {\n+          return CallNestedComputation(builder, local_ir_emitter_context,\n+                                       llvm_module, *comparator, operands,\n+                                       output);\n+        });\n+  };\n+  std::vector<int64_t> xor_masks;\n+  for (int64_t stage = 0; stage < num_stages; ++stage) {\n+    for (int64_t mask = stage; mask >= 0; --mask) {\n+      int64_t xor_mask;\n+      if (mask == stage) {\n+        xor_mask = (1LL << (stage + 1)) - 1;\n+      } else {\n+        xor_mask = 1LL << mask;\n+      }\n+      if (xor_mask >= tile_size) {\n+        if (!xor_masks.empty()) {\n+          TF_RETURN_IF_ERROR(emit_kernel(xor_masks));\n+          xor_masks.clear();\n+        }\n+        TF_RETURN_IF_ERROR(emit_kernel({xor_mask}));\n+      } else {\n+        xor_masks.push_back(xor_mask);\n+      }\n+    }\n+  }\n+  if (!xor_masks.empty()) {\n+    TF_RETURN_IF_ERROR(emit_kernel(xor_masks));\n+  }\n+  return thunks;\n+}\n+\n+// Input = {dynamic array(with dynamic dimension meta data at the\n+// end)} Output = {static array, dynamic_dim0, dynamic_dim1}\n+absl::StatusOr<ThunkSequence> EmitPadToStaticLLVMIR(\n+    const HloCustomCallInstruction* hlo, llvm::Module* llvm_module,\n+    IrEmitterContext* ir_emitter_context) {\n+  std::string ir_name = std::string(hlo->name());\n+\n+  // Copy of the main context with the local module.\n+  IrEmitterContext local_ir_emitter_context(\n+      &ir_emitter_context->hlo_module(),\n+      &ir_emitter_context->buffer_assignment(),\n+      &ir_emitter_context->execution_stream_assignment(),\n+      std::string(ir_emitter_context->platform_name()),\n+      ir_emitter_context->gpu_device_info(), ir_emitter_context->mlir_context(),\n+      llvm_module, ir_emitter_context->llvm_module_constants(),\n+      ir_emitter_context->emit_kernels());\n+\n+  IrEmitter ir_emitter(&local_ir_emitter_context, /*nested=*/false);\n+\n+  constexpr int kUnrollFactor = 1;\n+  const Shape& input_shape = hlo->operand(0)->shape();\n+\n+  LaunchDimensions launch_dimensions = CalculateLaunchDimensions(\n+      input_shape, ir_emitter_context->gpu_device_info(), {kUnrollFactor});\n+\n+  TF_ASSIGN_OR_RETURN(\n+      KernelThunkInfo kernel_thunk_info,\n+      BuildKernelThunkForNonFusionOp(\n+          llvm_module, hlo, ir_emitter_context->buffer_assignment(),\n+          ir_emitter_context->GetNextThunkId(),\n+          ir_emitter_context->gpu_device_info(),\n+          ir_emitter_context->GetSanitizedUniqueName(ir_name), ir_emitter,\n+          launch_dimensions));\n+  ThunkSequence thunk_sequence;\n+  thunk_sequence.push_back(std::move(kernel_thunk_info.thunk));\n+\n+  const llvm_ir::IrArray& source_array = kernel_thunk_info.ir_arrays[0];\n+  const llvm_ir::IrArray& output_array = kernel_thunk_info.ir_arrays[1];\n+  auto output_dim_arrays =\n+      absl::Span<const llvm_ir::IrArray>(kernel_thunk_info.ir_arrays)\n+          .subspan(2);\n+\n+  llvm::Type* index_ty = GetIndexTypeForKernel(\n+      hlo, launch_dimensions.launch_bound(), ir_emitter.builder());\n+\n+  // pseudo code for PadToStatic on a 2d array\n+  //   int* source_array = args[0];\n+  //   int* dest_array = args[1];\n+  llvm::Value* source_buffer = source_array.GetBasePointer();\n+\n+  // TODO(jurahul): input_shape here is the static shape of the\n+  // input (which has a dynamic shape in XLA). Currently, we are\n+  // mapping that to a static shaped memref. When we change that to\n+  // a more appropriate representation in MLIR, fix this code to\n+  // correctly deduce the static shape backing the dynamically\n+  // shaped memref.\n+  int64_t raw_data_size = ShapeUtil::ByteSizeOf(input_shape);\n+\n+  //   int* dyn_dim0_size = source_array + meta_data_offset;\n+  //   int* dyn_dim1_size = source_array + meta_data_offset +\n+  //   sizeof(int);\n+  std::vector<llvm::Value*> dynamic_dims;\n+  int alignment = raw_data_size % sizeof(int32_t);\n+  std::vector<ShapeUtil::IndexedShape> output_shapes =\n+      ShapeUtil::GetLeafShapes(hlo->shape());\n+\n+  for (int64_t i = 1; i < output_shapes.size(); ++i) {\n+    // Dynamic size of each dimension is attached at the end of the\n+    // source array(operand(0)). We need to extract these value.\n+    const Shape& dim_shape = output_shapes[i].shape;\n+    TF_RET_CHECK(Shape::Equal()(dim_shape, ShapeUtil::MakeScalarShape(S32)));\n+\n+    const int64_t dim_index = i - 1;\n+    llvm::Value* metadata = ir_emitter.builder()->CreateConstInBoundsGEP1_32(\n+        ir_emitter.builder()->getInt8Ty(), source_buffer,\n+        raw_data_size + dim_index * sizeof(int32_t));\n+    llvm::Value* dyn_dim_size =\n+        CreateLoad(metadata, ir_emitter.builder()->getInt32Ty(), alignment,\n+                   ir_emitter.builder());\n+    dynamic_dims.push_back(dyn_dim_size);\n+  }\n+\n+  // only one thread need to store the dynamic index\n+  //   int thread_id = GetThreadId();\n+  //   int block_id = GetBlockId();\n+  //   if (thread_id == 0 && block_id == 0) {\n+  //     *output[1] = *dyn_dim0_size;\n+  //     *output[2] = *dyn_dim1_size;\n+  //   }\n+  KernelSupportLibrary{ir_emitter.builder()}.If(\n+      \"is_thread_0\", IsBlock0Thread0(ir_emitter.builder()), [&] {\n+        for (int64_t i = 1; i < output_shapes.size(); ++i) {\n+          const int64_t dim_index = i - 1;\n+          llvm::Value* dest_dim_size_address =\n+              output_dim_arrays[dim_index].GetBasePointer();\n+          // output[i] stores dynamic_dim_(i-1)\n+          CreateStore(dynamic_dims[dim_index], dest_dim_size_address, alignment,\n+                      ir_emitter.builder());\n+        }\n+      });\n+\n+  //     int dyn_element_total = 1;\n+  //     dyn_element_total *= *dyn_dim0_size;\n+  //     dyn_element_total *= *dyn_dim1_size;\n+  llvm::Value* dyn_element_total = llvm::ConstantInt::get(index_ty, 1);\n+  for (llvm::Value* dynamic_dim : dynamic_dims) {\n+    dyn_element_total = ir_emitter.builder()->CreateMul(\n+        dyn_element_total,\n+        ir_emitter.builder()->CreateIntCast(dynamic_dim,\n+                                            dyn_element_total->getType(),\n+                                            /*isSigned=*/true),\n+        /*Name=*/\"dyn_element_total_pad\");\n+  }\n+\n+  //   linear_index = block_id * threads_per_block + thread_id;\n+  //   if (linear_index < max_num_element) {\n+  //     Index static_index =\n+  //         delinerized(linerized_index, static_dim0_size,\n+  //         static_dim1_size);\n+  //     if (linerized_index < dyn_element_total) {\n+  //       Index dyn_index =\n+  //           delinerized(linerized_index, *dyn_dim0_size,\n+  //           *dyn_dim1_size);\n+  //       dest_array[dyn_index.dim0][dyn_index.dim1] =\n+  //           source_array[static_index.dim0][static_index.dim1];\n+  //     }\n+  //   }\n+  llvm_ir::BodyEmitter body_generator =\n+      [&](const llvm_ir::IrArray::Index& array_index) -> absl::Status {\n+    llvm::Value* linearIndex =\n+        array_index.Linearize(input_shape.dimensions(), ir_emitter.builder());\n+    auto if_in_dyn_bounds = llvm_ir::EmitIfThenElse(\n+        ir_emitter.builder()->CreateICmpULT(linearIndex, dyn_element_total),\n+        llvm_ir::IrName(ir_name, \"in_dyn_bounds\"), ir_emitter.builder(), false);\n+    // Set IR builder insertion point to the body of the if\n+    // structure.\n+    llvm_ir::SetToFirstInsertPoint(if_in_dyn_bounds.true_block,\n+                                   ir_emitter.builder());\n+    llvm_ir::IrArray::Index dyn_index(linearIndex, input_shape,\n+                                      absl::MakeSpan(dynamic_dims),\n+                                      ir_emitter.builder());\n+    output_array.EmitWriteArrayElement(\n+        dyn_index,\n+        source_array.EmitReadArrayElement(array_index, ir_emitter.builder(),\n+                                          /*name=*/\"\"),\n+        ir_emitter.builder(),\n+        /*use_linear_index=*/false);\n+    return absl::OkStatus();\n+  };\n+\n+  const Shape& data_shape = hlo->shape().tuple_shapes(0);\n+  TF_RETURN_IF_ERROR(ParallelLoopEmitter(body_generator, data_shape,\n+                                         launch_dimensions,\n+                                         ir_emitter.builder(), {kUnrollFactor})\n+                         .EmitLoop(ir_name, index_ty));\n+  return thunk_sequence;\n+}\n+\n+// Input = {dynamic array(with dynamic dimension meta data at the\n+// end)} Output = {static array, dynamic_dim0, dynamic_dim1}\n+absl::StatusOr<ThunkSequence> EmitSliceToDynamicLLVMIR(\n+    const HloCustomCallInstruction* hlo, llvm::Module* llvm_module,\n+    IrEmitterContext* ir_emitter_context) {\n+  std::string ir_name = std::string(hlo->name());\n+\n+  // Copy of the main context with the local module.\n+  IrEmitterContext local_ir_emitter_context(\n+      &ir_emitter_context->hlo_module(),\n+      &ir_emitter_context->buffer_assignment(),\n+      &ir_emitter_context->execution_stream_assignment(),\n+      std::string(ir_emitter_context->platform_name()),\n+      ir_emitter_context->gpu_device_info(), ir_emitter_context->mlir_context(),\n+      llvm_module, ir_emitter_context->llvm_module_constants(),\n+      ir_emitter_context->emit_kernels());\n+\n+  IrEmitter ir_emitter(&local_ir_emitter_context, /*nested=*/false);\n+  // TODO(jurahul): Create an op to represent SliceToDynamic.\n+  constexpr int kUnrollFactor = 1;\n+  const Shape& input_shape = hlo->operand(0)->shape();\n+\n+  LaunchDimensions launch_dimensions = CalculateLaunchDimensions(\n+      input_shape, ir_emitter_context->gpu_device_info(), {kUnrollFactor});\n+  llvm::Type* index_ty = GetIndexTypeForKernel(\n+      hlo, launch_dimensions.launch_bound(), ir_emitter.builder());\n+  TF_ASSIGN_OR_RETURN(\n+      KernelThunkInfo kernel_thunk_info,\n+      BuildKernelThunkForNonFusionOp(\n+          llvm_module, hlo, ir_emitter_context->buffer_assignment(),\n+          ir_emitter_context->GetNextThunkId(),\n+          ir_emitter_context->gpu_device_info(),\n+          ir_emitter_context->GetSanitizedUniqueName(ir_name), ir_emitter,\n+          launch_dimensions));\n+  ThunkSequence thunk_sequence;\n+  thunk_sequence.push_back(std::move(kernel_thunk_info.thunk));\n+\n+  const Shape& data_shape = ShapeUtil::MakeStaticShape(hlo->shape());\n+  TF_RET_CHECK(data_shape.IsArray());\n+\n+  // TODO(jurahul): data_shape here is the static shape of the\n+  // output (which has a dynamic shape in XLA). Currently, we are\n+  // mapping that to a static shaped memref. When we change that to\n+  // a more appropriate representation in MLIR, fix this code to\n+  // correctly deduce the static shape backing the dynamically\n+  // shaped memref.\n+\n+  // calculate the location where metadata needs to be inserted\n+  //   int* dyn_dim0_size = dest_array + meta_data_offset;\n+  //   int* dyn_dim1_size = dest_array + meta_data_offset +\n+  //   sizeof(int);\n+  int32_t raw_data_size = ShapeUtil::ByteSizeOf(data_shape);\n+\n+  // pseudo code for sliceToDynamic on a 2d array\n+  //   int* source_array = args[0];\n+  //   int* dest_array = args.back();\n+  const auto& ir_arrays = kernel_thunk_info.ir_arrays;\n+  const llvm_ir::IrArray& data_array = ir_arrays.back();\n+  llvm::Value* dest_buffer = data_array.GetBasePointer();\n+\n+  // Load dynamic dimensions from memory.\n+  std::vector<llvm::Value*> dynamic_dims;\n+  int alignment = raw_data_size % sizeof(int32_t);\n+  for (int64_t i = 1; i < hlo->operand_count(); ++i) {\n+    llvm::Value* source_buffer = ir_arrays[i].GetBasePointer();\n+    llvm::Type* source_buffer_pointee_type = ir_arrays[i].GetBasePointeeType();\n+    llvm::LoadInst* dyn_dim_size = ir_emitter.builder()->CreateLoad(\n+        source_buffer_pointee_type, source_buffer, \"dyn_dim_size\");\n+    dynamic_dims.push_back(dyn_dim_size);\n+  }\n+\n+  // only one thread need to store the dynamic index\n+  //   int thread_id = GetThreadId();\n+  //   int block_id = GetBlockId();\n+  //   if (thread_id == 0 && block_id == 0) {\n+  //     *dyn_dim0_size = *output[1];\n+  //     *dyn_dim1_size = *output[2];\n+  //   }\n+  KernelSupportLibrary{ir_emitter.builder()}.If(\n+      \"is_thread_0\", IsBlock0Thread0(ir_emitter.builder()), [&] {\n+        for (int64_t i = 1; i < hlo->operand_count(); ++i) {\n+          const int64_t dim_index = i - 1;\n+          llvm::Value* metadata =\n+              ir_emitter.builder()->CreateConstInBoundsGEP1_32(\n+                  ir_emitter.builder()->getInt8Ty(), dest_buffer,\n+                  raw_data_size + dim_index * sizeof(int32_t));\n+          // output[i] stores dynamic_dim_(i-1)\n+          CreateStore(dynamic_dims[dim_index], metadata, alignment,\n+                      ir_emitter.builder());\n+        }\n+      });\n+\n+  //     int dyn_element_total = 1;\n+  //     dyn_element_total *= dyn_dim0_size;\n+  //     dyn_element_total *= dyn_dim1_size;\n+  llvm::Value* dyn_element_total = llvm::ConstantInt::get(index_ty, 1);\n+  for (llvm::Value* dynamic_dim : dynamic_dims) {\n+    dyn_element_total = ir_emitter.builder()->CreateMul(\n+        dyn_element_total,\n+        ir_emitter.builder()->CreateIntCast(dynamic_dim,\n+                                            dyn_element_total->getType(),\n+                                            /*isSigned=*/true),\n+        /*Name=*/\"dyn_element_total_slice\");\n+  }\n+\n+  //   linear_index = block_id * threads_per_block + thread_id;\n+  //   if (linear_index < max_num_element) {\n+  //     Index static_index =\n+  //         delinerized(linerized_index, static_dim0_size,\n+  //         static_dim1_size);\n+  //     if (linerized_index < dyn_element_total) {\n+  //       Index dyn_index =\n+  //           delinerized(linerized_index, *dyn_dim0_size,\n+  //           *dyn_dim1_size);\n+  //       dest_array[static_index.dim0][static_index.di] =\n+  //           source_array[dyn_index.dim0][dyn_index.dim1];\n+  //     }\n+  //   }\n+  llvm_ir::BodyEmitter body_generator =\n+      [&](const llvm_ir::IrArray::Index& array_index) -> absl::Status {\n+    llvm::Value* linearIndex =\n+        array_index.Linearize(input_shape.dimensions(), ir_emitter.builder());\n+    auto if_in_dyn_bounds = llvm_ir::EmitIfThenElse(\n+        ir_emitter.builder()->CreateICmpULT(linearIndex, dyn_element_total),\n+        llvm_ir::IrName(ir_name, \"in_dyn_bounds\"), ir_emitter.builder(), false);\n+    // Set IR builder insertion point to the body of the if\n+    // structure.\n+    llvm_ir::SetToFirstInsertPoint(if_in_dyn_bounds.true_block,\n+                                   ir_emitter.builder());\n+    llvm_ir::IrArray::Index dyn_index(linearIndex, input_shape,\n+                                      absl::MakeSpan(dynamic_dims),\n+                                      ir_emitter.builder());\n+\n+    data_array.EmitWriteArrayElement(\n+        array_index,\n+        ir_arrays[0].EmitReadArrayElement(dyn_index, ir_emitter.builder(),\n+                                          /*name=*/\"\",\n+                                          /*use_linear_index=*/false),\n+        ir_emitter.builder());\n+    return absl::OkStatus();\n+  };\n+\n+  TF_RETURN_IF_ERROR(ParallelLoopEmitter(body_generator, data_shape,\n+                                         launch_dimensions,\n+                                         ir_emitter.builder(), {kUnrollFactor})\n+                         .EmitLoop(ir_name, index_ty));\n+  return thunk_sequence;\n+}\n+\n+absl::StatusOr<ThunkSequence> EmitRngGetAndUpdateStateLLVMIR(\n+    const HloRngGetAndUpdateStateInstruction* hlo, llvm::Module* llvm_module,\n+    IrEmitterContext* ir_emitter_context) {\n+  std::string ir_name = std::string(hlo->name());\n+\n+  // Copy of the main context with the local module.\n+  IrEmitterContext local_ir_emitter_context(\n+      &ir_emitter_context->hlo_module(),\n+      &ir_emitter_context->buffer_assignment(),\n+      &ir_emitter_context->execution_stream_assignment(),\n+      std::string(ir_emitter_context->platform_name()),\n+      ir_emitter_context->gpu_device_info(), ir_emitter_context->mlir_context(),\n+      llvm_module, ir_emitter_context->llvm_module_constants(),\n+      ir_emitter_context->emit_kernels());\n+\n+  IrEmitter ir_emitter(&local_ir_emitter_context, /*nested=*/false);\n+\n+  auto& b = *ir_emitter.builder();\n+  // Emit a kernel to increment the global state for Philox RNG\n+  // algorithm.\n+  TF_ASSIGN_OR_RETURN(\n+      KernelThunkInfo kernel_thunk_info,\n+      BuildKernelThunkForNonFusionOp(\n+          llvm_module, hlo, ir_emitter_context->buffer_assignment(),\n+          ir_emitter_context->GetNextThunkId(),\n+          ir_emitter_context->gpu_device_info(),\n+          ir_emitter_context->GetSanitizedUniqueName(ir_name), ir_emitter,\n+          LaunchDimensions()));\n+  ThunkSequence thunk_sequence;\n+  thunk_sequence.push_back(std::move(kernel_thunk_info.thunk));\n+\n+  auto& ir_arrays = kernel_thunk_info.ir_arrays;\n+  llvm::Value* old_state =\n+      llvm_ir::RngGetAndUpdateState(hlo->delta(), llvm_module, &b);\n+  llvm::Value* output_address = ir_arrays[0].EmitArrayElementAddress(\n+      llvm_ir::IrArray::Index(\n+          /*linear=*/b.getInt64(0), hlo->shape(), &b),\n+      &b, \"rng_state_address\");\n+  b.CreateStore(old_state, output_address);\n+  return thunk_sequence;\n+}\n+\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "a5d57e55e05a9e80ae8b8e33f85d0dfd98f5472b",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter.h",
            "status": "modified",
            "additions": 25,
            "deletions": 8,
            "changes": 33,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/df423f81413e6ad85b35832fedc08dac0ff9baae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/df423f81413e6ad85b35832fedc08dac0ff9baae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter.h?ref=df423f81413e6ad85b35832fedc08dac0ff9baae",
            "patch": "@@ -19,12 +19,14 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n #include \"llvm/IR/Function.h\"\n #include \"llvm/IR/IRBuilder.h\"\n #include \"llvm/IR/Value.h\"\n-#include \"llvm/Support/AtomicOrdering.h\"\n+#include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/gpu/hlo_to_ir_bindings.h\"\n #include \"xla/service/gpu/ir_emitter_context.h\"\n #include \"xla/service/llvm_ir/fused_ir_emitter.h\"\n@@ -59,6 +61,11 @@ class IrEmitter : public DfsHloVisitorWithDefault,\n   IrEmitter(const IrEmitter&) = delete;\n   IrEmitter& operator=(const IrEmitter&) = delete;\n \n+  // Constructs an IrEmitter with the given IrEmitter context.\n+  // ir_emitter_context is owned by the caller and should outlive the IrEmitter\n+  // object.\n+  explicit IrEmitter(IrEmitterContext* ir_emitter_context, bool is_nested);\n+\n   absl::Status DefaultAction(HloInstruction* hlo) override;\n   absl::Status HandleConstant(HloInstruction* constant) override;\n   absl::Status HandleGetTupleElement(\n@@ -89,11 +96,6 @@ class IrEmitter : public DfsHloVisitorWithDefault,\n   llvm::IRBuilderBase* builder() { return &b_; }\n \n  protected:\n-  // Constructs an IrEmitter with the given IrEmitter context.\n-  // ir_emitter_context is owned by the caller and should outlive the IrEmitter\n-  // object.\n-  explicit IrEmitter(IrEmitterContext* ir_emitter_context, bool is_nested);\n-\n   // Helper for calling HloToIrBindings::GetIrArray.\n   //\n   // Gets the IrArray which contains inst.  This array has metadata that makes\n@@ -121,8 +123,7 @@ class IrEmitter : public DfsHloVisitorWithDefault,\n   // nested loops (e.g. one for each dimension of the `hlo`'s shape). The body\n   // of the inner-most loop is provided by the body_emitter function.\n   virtual absl::Status EmitTargetElementLoop(\n-      const HloInstruction& hlo,\n-      const llvm_ir::ElementGenerator& body_emitter) = 0;\n+      const HloInstruction& hlo, const llvm_ir::ElementGenerator& body_emitter);\n \n   IrEmitterContext* ir_emitter_context_;\n   llvm::Module* module_;\n@@ -139,6 +140,22 @@ class IrEmitter : public DfsHloVisitorWithDefault,\n                            FusedIrEmitter* fused_emitter);\n };\n \n+absl::StatusOr<ThunkSequence> EmitBitonicSortLLVMIR(\n+    const HloSortInstruction* sort, llvm::Module* llvm_module,\n+    IrEmitterContext* ir_emitter_context);\n+\n+absl::StatusOr<ThunkSequence> EmitPadToStaticLLVMIR(\n+    const HloCustomCallInstruction* hlo, llvm::Module* llvm_module,\n+    IrEmitterContext* ir_emitter_context);\n+\n+absl::StatusOr<ThunkSequence> EmitSliceToDynamicLLVMIR(\n+    const HloCustomCallInstruction* hlo, llvm::Module* llvm_module,\n+    IrEmitterContext* ir_emitter_context);\n+\n+absl::StatusOr<ThunkSequence> EmitRngGetAndUpdateStateLLVMIR(\n+    const HloRngGetAndUpdateStateInstruction* hlo, llvm::Module* llvm_module,\n+    IrEmitterContext* ir_emitter_context);\n+\n }  // namespace gpu\n }  // namespace xla\n "
        },
        {
            "sha": "f32c6bf3a4f069e461027c5c1fd8ce636faafb6d",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_context.cc",
            "status": "removed",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f150d49dd508aee2585dfc3c4f69f3bac11fd434/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f150d49dd508aee2585dfc3c4f69f3bac11fd434/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.cc?ref=f150d49dd508aee2585dfc3c4f69f3bac11fd434",
            "patch": "@@ -1,43 +0,0 @@\n-/* Copyright 2022 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"xla/service/gpu/ir_emitter_context.h\"\n-\n-#include <algorithm>\n-#include <cstdint>\n-#include <utility>\n-#include <vector>\n-\n-#include \"absl/algorithm/container.h\"\n-#include \"absl/strings/string_view.h\"\n-#include \"llvm/ADT/ArrayRef.h\"\n-#include \"llvm/IR/Constant.h\"\n-#include \"llvm/IR/Constants.h\"\n-#include \"llvm/IR/DerivedTypes.h\"\n-#include \"llvm/IR/GlobalValue.h\"\n-#include \"llvm/IR/GlobalVariable.h\"\n-#include \"llvm/IR/IRBuilder.h\"\n-#include \"llvm/Support/Alignment.h\"\n-#include \"llvm/TargetParser/Triple.h\"\n-#include \"xla/service/gpu/gpu_constants.h\"\n-#include \"xla/service/gpu/gpu_executable.h\"\n-#include \"xla/service/gpu/ir_emission_utils.h\"\n-\n-namespace xla {\n-namespace gpu {\n-\n-\n-}  // namespace gpu\n-}  // namespace xla"
        },
        {
            "sha": "238ac1408dde74a1d48048d94a5e193e41ec2f4a",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_context.h",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/df423f81413e6ad85b35832fedc08dac0ff9baae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/df423f81413e6ad85b35832fedc08dac0ff9baae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_context.h?ref=df423f81413e6ad85b35832fedc08dac0ff9baae",
            "patch": "@@ -41,6 +41,7 @@ limitations under the License.\n #include \"xla/service/gpu/gpu_executable.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/kernel_reuse_cache.h\"\n+#include \"xla/service/llvm_ir/llvm_util.h\"\n #include \"xla/service/name_uniquer.h\"\n #include \"xla/stream_executor/cuda/cuda_compute_capability.h\"\n #include \"xla/stream_executor/device_description.h\"\n@@ -141,6 +142,11 @@ class IrEmitterContext {\n \n   ThunkId GetNextThunkId() { return thunk_id_generator_.GetNextThunkId(); }\n \n+  std::string GetSanitizedUniqueName(const std::string& suggested_name) {\n+    return name_uniquer_.GetUniqueName(\n+        llvm_ir::SanitizeFunctionName(suggested_name));\n+  }\n+\n  private:\n   const HloModule* hlo_module_;\n   const BufferAssignment* buffer_assignment_;"
        },
        {
            "sha": "bfccf3aa4db3fb63e90c5ca437576721e20c1d79",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_nested.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/df423f81413e6ad85b35832fedc08dac0ff9baae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_nested.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/df423f81413e6ad85b35832fedc08dac0ff9baae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_nested.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_nested.cc?ref=df423f81413e6ad85b35832fedc08dac0ff9baae",
            "patch": "@@ -285,8 +285,7 @@ absl::Status IrEmitterNested::EmitConstants(const HloComputation& computation) {\n         ShapeUtil::ByteSizeOfPrimitiveType(literal.shape().element_type()),\n         global_name, /*allocation_idx=*/-1,\n         DenseDataIntermediate::Alias(\n-            absl::MakeSpan(base, base + literal.size_bytes())),\n-        &b_);\n+            absl::MakeSpan(base, base + literal.size_bytes())));\n     ir_emitter_context_->constants().push_back(std::move(info));\n   }\n   return absl::OkStatus();\n@@ -341,21 +340,22 @@ absl::Status CallNestedComputation(llvm::IRBuilderBase* builder,\n GpuExecutable::ConstantInfo AppendGlobalConstant(\n     llvm::Module* module, int64_t num_elements, int64_t bytes_per_element,\n     absl::string_view symbol_name, int allocation_idx,\n-    DenseDataIntermediate content, llvm::IRBuilderBase* b) {\n+    DenseDataIntermediate content) {\n   // LLVM and PTXAS don't deal well with large constants, so we only emit very\n   // small constants directly in LLVM IR.  Larger constants are emitted with\n   // zero initializers in LLVM IR and are later overwritten when the PTX/CUBIN\n   // is loaded.\n   bool should_emit_initializer = num_elements <= 1;\n \n+  llvm::IRBuilder<> b(module->getContext());\n   // Ptxas has issues if the constant allocation is smaller than 64 bytes.\n   // TODO(b/253259975): Remove when fixed ptxas version is submitted.\n   constexpr int64_t kMinConstAllocationInBytes = 64;\n   bool needs_padding =\n       num_elements * bytes_per_element < kMinConstAllocationInBytes;\n \n   llvm::ArrayType* global_type = llvm::ArrayType::get(\n-      b->getInt8Ty(),\n+      b.getInt8Ty(),\n       std::max(num_elements * bytes_per_element, kMinConstAllocationInBytes));\n \n   GpuExecutable::ConstantInfo info;"
        },
        {
            "sha": "5f53c25cdd8042e952e384b77ab1a41dfc326f53",
            "filename": "third_party/xla/xla/service/gpu/ir_emitter_nested.h",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/df423f81413e6ad85b35832fedc08dac0ff9baae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_nested.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/df423f81413e6ad85b35832fedc08dac0ff9baae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_nested.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fir_emitter_nested.h?ref=df423f81413e6ad85b35832fedc08dac0ff9baae",
            "patch": "@@ -56,10 +56,12 @@ absl::Status CallNestedComputation(llvm::IRBuilderBase* builder,\n \n // Emit a constant with a given number of element, given byte size of the\n // element, given symbol name and content.\n-GpuExecutable::ConstantInfo AppendGlobalConstant(\n-    llvm::Module* module, int64_t num_elements, int64_t bytes_per_element,\n-    absl::string_view symbol_name, int allocation_idx,\n-    DenseDataIntermediate content, llvm::IRBuilderBase* b);\n+GpuExecutable::ConstantInfo AppendGlobalConstant(llvm::Module* module,\n+                                                 int64_t num_elements,\n+                                                 int64_t bytes_per_element,\n+                                                 absl::string_view symbol_name,\n+                                                 int allocation_idx,\n+                                                 DenseDataIntermediate content);\n \n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "001e40f25a7b05252f198d3bc9163b1caef5084c",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 37,
            "deletions": 553,
            "changes": 590,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/df423f81413e6ad85b35832fedc08dac0ff9baae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/df423f81413e6ad85b35832fedc08dac0ff9baae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc?ref=df423f81413e6ad85b35832fedc08dac0ff9baae",
            "patch": "@@ -15,20 +15,6 @@ limitations under the License.\n \n #include \"xla/service/gpu/thunk_emitter.h\"\n \n-#include <algorithm>\n-#include <cassert>\n-#include <cstdint>\n-#include <cstring>\n-#include <functional>\n-#include <memory>\n-#include <numeric>\n-#include <optional>\n-#include <string>\n-#include <tuple>\n-#include <type_traits>\n-#include <utility>\n-#include <vector>\n-\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/container/inlined_vector.h\"\n@@ -38,27 +24,18 @@ limitations under the License.\n #include \"absl/strings/escaping.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n-#include \"absl/strings/str_join.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/APInt.h\"\n-#include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/StringRef.h\"\n-#include \"llvm/IR/Argument.h\"\n-#include \"llvm/IR/Attributes.h\"\n #include \"llvm/IR/BasicBlock.h\"\n-#include \"llvm/IR/Constants.h\"\n-#include \"llvm/IR/DerivedTypes.h\"\n #include \"llvm/IR/Function.h\"\n-#include \"llvm/IR/IRBuilder.h\"\n #include \"llvm/IR/Instructions.h\"\n #include \"llvm/IR/LLVMContext.h\"\n #include \"llvm/IR/Module.h\"\n #include \"llvm/Linker/Linker.h\"\n-#include \"llvm/Support/Casting.h\"\n #include \"llvm/TargetParser/Triple.h\"\n #include \"mlir/AsmParser/AsmParser.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Dialect/MemRef/Transforms/Passes.h\"\n #include \"mlir/IR/Attributes.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n@@ -141,7 +118,6 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/call_graph.h\"\n #include \"xla/service/collective_ops_utils.h\"\n-#include \"xla/service/global_device_id.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/custom_kernel_emitter.h\"\n@@ -160,23 +136,15 @@ limitations under the License.\n #include \"xla/service/gpu/launch_dimensions.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/service/gpu/model/block_level_parameters.h\"\n-#include \"xla/service/gpu/parallel_loop_emitter.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n #include \"xla/service/gpu/triton_call.h\"\n #include \"xla/service/llvm_ir/buffer_assignment_util.h\"\n-#include \"xla/service/llvm_ir/ir_array.h\"\n-#include \"xla/service/llvm_ir/kernel_support_library.h\"\n-#include \"xla/service/llvm_ir/llvm_util.h\"\n-#include \"xla/service/llvm_ir/loop_emitter.h\"\n-#include \"xla/service/llvm_ir/sort_util.h\"\n-#include \"xla/service/name_uniquer.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/device_description.h\"\n #include \"xla/stream_executor/gpu/gpu_blas_lt.h\"\n-#include \"xla/stream_executor/gpu/tma_metadata.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tools/hlo_decomposer.h\"\n@@ -191,8 +159,8 @@ limitations under the License.\n \n namespace xla {\n namespace gpu {\n-\n namespace {\n+\n // TODO: move into a host_execute specific file.\n bool IsHostExecuteCustomCall(const HloInstruction& hlo) {\n   return hlo.opcode() == HloOpcode::kCustomCall &&\n@@ -247,7 +215,7 @@ std::unique_ptr<llvm::Module> CreateLocalLLVMModule(\n }  // namespace\n \n ThunkEmitter::ThunkEmitter(IrEmitterContext* ir_emitter_context)\n-    : IrEmitter(ir_emitter_context, /*is_nested=*/false),\n+    : ir_emitter_context_(ir_emitter_context),\n       send_recv_events_(std::make_shared<HostSendRecvAsyncEvents>()),\n       copy_events_(std::make_shared<CopyThunk::AsyncEvents>()),\n       nvshmem_buffer_addresses_(std::make_shared<NvshmemBufferAddresses>()),\n@@ -274,7 +242,7 @@ absl::Status ThunkEmitter::EmitConstant(const HloConstantInstruction* instr) {\n \n   GpuExecutable::ConstantInfo info = AppendGlobalConstant(\n       ir_emitter_context_->llvm_module_constants(), num_elements, element_bytes,\n-      global_name, slice.index(), std::move(content), &b_);\n+      global_name, slice.index(), std::move(content));\n   ir_emitter_context_->constants().push_back(std::move(info));\n   return absl::OkStatus();\n }\n@@ -305,60 +273,6 @@ absl::Status ThunkEmitter::EmitConditional(const HloInstruction* instr) {\n   return absl::OkStatus();\n }\n \n-llvm::Value* ThunkEmitter::CreateLoad(llvm::Value* address,\n-                                      llvm::Type* data_type,\n-                                      int alignment_bytes) {\n-  int data_bytes = data_type->getPrimitiveSizeInBits() /\n-                   primitive_util::BitWidth(PrimitiveType::U8);\n-  if (alignment_bytes == 0) {\n-    return b_.CreateLoad(data_type, address);\n-  }\n-\n-  int alignment_bitwidth =\n-      alignment_bytes * primitive_util::BitWidth(PrimitiveType::U8);\n-\n-  llvm::Value* output = llvm::ConstantInt::get(data_type, 0);\n-  for (int offset_bytes = 0; offset_bytes < data_bytes;\n-       offset_bytes += alignment_bytes) {\n-    llvm::Value* offset_address = b_.CreateConstInBoundsGEP1_32(\n-        b_.getInt8Ty(), address, offset_bytes, \"offset_address\");\n-    llvm::Value* partial_value = b_.CreateLoad(b_.getIntNTy(alignment_bitwidth),\n-                                               offset_address, \"partial_value\");\n-    llvm::Value* zextd =\n-        b_.CreateZExt(partial_value, output->getType(), \"partial_value_zextd\");\n-    llvm::Value* shifted = b_.CreateShl(\n-        zextd, llvm::ConstantInt::get(b_.getInt32Ty(), offset_bytes),\n-        \"partial_input_shifted\");\n-    output = b_.CreateAdd(output, shifted, \"output_updated\");\n-  }\n-  return output;\n-}\n-\n-void ThunkEmitter::CreateStore(llvm::Value* data, llvm::Value* address,\n-                               int alignment_bytes) {\n-  int data_bytes = data->getType()->getPrimitiveSizeInBits() /\n-                   primitive_util::BitWidth(PrimitiveType::U8);\n-  CHECK_GE(data_bytes, alignment_bytes);\n-  if (alignment_bytes == 0) {\n-    b_.CreateStore(data, address);\n-    return;\n-  }\n-\n-  int alignment_bitwidth =\n-      alignment_bytes * primitive_util::BitWidth(PrimitiveType::U8);\n-\n-  for (int offset_bytes = 0; offset_bytes < data_bytes;\n-       offset_bytes += alignment_bytes) {\n-    llvm::Value* offset_address = b_.CreateConstInBoundsGEP1_32(\n-        b_.getInt8Ty(), address, offset_bytes, \"offset_address\");\n-    llvm::Value* shifted_partial = b_.CreateTrunc(\n-        b_.CreateLShr(data,\n-                      llvm::ConstantInt::get(b_.getInt32Ty(), offset_bytes)),\n-        b_.getIntNTy(alignment_bitwidth), \"truncated_value\");\n-    b_.CreateStore(shifted_partial, offset_address);\n-  }\n-}\n-\n // Input = {dynamic array(with dynamic dimension meta data at the\n // end)} Output = {static array, dynamic_dim0, dynamic_dim1}\n absl::Status ThunkEmitter::EmitPadToStatic(\n@@ -367,127 +281,12 @@ absl::Status ThunkEmitter::EmitPadToStatic(\n   auto local_llvm_module =\n       CreateLocalLLVMModule(ir_name, ir_emitter_context_->llvm_module());\n \n-  constexpr int kUnrollFactor = 1;\n-  const Shape& input_shape = instr->operand(0)->shape();\n-\n-  LaunchDimensions launch_dimensions = CalculateLaunchDimensions(\n-      input_shape, ir_emitter_context_->gpu_device_info(), {kUnrollFactor});\n-  TF_ASSIGN_OR_RETURN(std::vector<llvm_ir::IrArray> ir_arrays,\n-                      BuildKernelThunkForNonFusionOp(local_llvm_module.get(),\n-                                                     instr, launch_dimensions));\n-\n-  const llvm_ir::IrArray& source_array = ir_arrays[0];\n-  const llvm_ir::IrArray& output_array = ir_arrays[1];\n-  auto output_dim_arrays =\n-      absl::Span<const llvm_ir::IrArray>(ir_arrays).subspan(2);\n-\n-  llvm::Type* index_ty =\n-      GetIndexTypeForKernel(instr, launch_dimensions.launch_bound(), &b_);\n-\n-  // pseudo code for PadToStatic on a 2d array\n-  //   int* source_array = args[0];\n-  //   int* dest_array = args[1];\n-  llvm::Value* source_buffer = source_array.GetBasePointer();\n-\n-  // TODO(jurahul): input_shape here is the static shape of the\n-  // input (which has a dynamic shape in XLA). Currently, we are\n-  // mapping that to a static shaped memref. When we change that to\n-  // a more appropriate representation in MLIR, fix this code to\n-  // correctly deduce the static shape backing the dynamically\n-  // shaped memref.\n-  int64_t raw_data_size = ShapeUtil::ByteSizeOf(input_shape);\n-\n-  //   int* dyn_dim0_size = source_array + meta_data_offset;\n-  //   int* dyn_dim1_size = source_array + meta_data_offset +\n-  //   sizeof(int);\n-  std::vector<llvm::Value*> dynamic_dims;\n-  int alignment = raw_data_size % sizeof(int32_t);\n-  std::vector<ShapeUtil::IndexedShape> output_shapes =\n-      ShapeUtil::GetLeafShapes(instr->shape());\n-\n-  for (int64_t i = 1; i < output_shapes.size(); ++i) {\n-    // Dynamic size of each dimension is attached at the end of the\n-    // source array(operand(0)). We need to extract these value.\n-    const Shape& dim_shape = output_shapes[i].shape;\n-    TF_RET_CHECK(Shape::Equal()(dim_shape, ShapeUtil::MakeScalarShape(S32)));\n-\n-    const int64_t dim_index = i - 1;\n-    llvm::Value* metadata = b_.CreateConstInBoundsGEP1_32(\n-        b_.getInt8Ty(), source_buffer,\n-        raw_data_size + dim_index * sizeof(int32_t));\n-    llvm::Value* dyn_dim_size =\n-        CreateLoad(metadata, b_.getInt32Ty(), alignment);\n-    dynamic_dims.push_back(dyn_dim_size);\n-  }\n-\n-  // only one thread need to store the dynamic index\n-  //   int thread_id = GetThreadId();\n-  //   int block_id = GetBlockId();\n-  //   if (thread_id == 0 && block_id == 0) {\n-  //     *output[1] = *dyn_dim0_size;\n-  //     *output[2] = *dyn_dim1_size;\n-  //   }\n-  KernelSupportLibrary{&b_}.If(\"is_thread_0\", IsBlock0Thread0(&b_), [&] {\n-    for (int64_t i = 1; i < output_shapes.size(); ++i) {\n-      const int64_t dim_index = i - 1;\n-      llvm::Value* dest_dim_size_address =\n-          output_dim_arrays[dim_index].GetBasePointer();\n-      // output[i] stores dynamic_dim_(i-1)\n-      CreateStore(dynamic_dims[dim_index], dest_dim_size_address, alignment);\n-    }\n-  });\n-\n-  //     int dyn_element_total = 1;\n-  //     dyn_element_total *= *dyn_dim0_size;\n-  //     dyn_element_total *= *dyn_dim1_size;\n-  llvm::Value* dyn_element_total = llvm::ConstantInt::get(index_ty, 1);\n-  for (llvm::Value* dynamic_dim : dynamic_dims) {\n-    dyn_element_total =\n-        b_.CreateMul(dyn_element_total,\n-                     b_.CreateIntCast(dynamic_dim, dyn_element_total->getType(),\n-                                      /*isSigned=*/true),\n-                     /*Name=*/\"dyn_element_total_pad\");\n-  }\n-\n-  //   linear_index = block_id * threads_per_block + thread_id;\n-  //   if (linear_index < max_num_element) {\n-  //     Index static_index =\n-  //         delinerized(linerized_index, static_dim0_size,\n-  //         static_dim1_size);\n-  //     if (linerized_index < dyn_element_total) {\n-  //       Index dyn_index =\n-  //           delinerized(linerized_index, *dyn_dim0_size,\n-  //           *dyn_dim1_size);\n-  //       dest_array[dyn_index.dim0][dyn_index.dim1] =\n-  //           source_array[static_index.dim0][static_index.dim1];\n-  //     }\n-  //   }\n-  llvm_ir::BodyEmitter body_generator =\n-      [&](const llvm_ir::IrArray::Index& array_index) -> absl::Status {\n-    llvm::Value* linearIndex =\n-        array_index.Linearize(input_shape.dimensions(), &b_);\n-    auto if_in_dyn_bounds = llvm_ir::EmitIfThenElse(\n-        b_.CreateICmpULT(linearIndex, dyn_element_total),\n-        llvm_ir::IrName(ir_name, \"in_dyn_bounds\"), &b_, false);\n-    // Set IR builder insertion point to the body of the if\n-    // structure.\n-    llvm_ir::SetToFirstInsertPoint(if_in_dyn_bounds.true_block, &b_);\n-    llvm_ir::IrArray::Index dyn_index(linearIndex, input_shape,\n-                                      absl::MakeSpan(dynamic_dims), &b_);\n-    output_array.EmitWriteArrayElement(\n-        dyn_index,\n-        source_array.EmitReadArrayElement(array_index, &b_,\n-                                          /*name=*/\"\"),\n-        &b_,\n-        /*use_linear_index=*/false);\n-    return absl::OkStatus();\n-  };\n-\n-  const Shape& data_shape = instr->shape().tuple_shapes(0);\n-  TF_RETURN_IF_ERROR(ParallelLoopEmitter(body_generator, data_shape,\n-                                         launch_dimensions, &b_,\n-                                         {kUnrollFactor})\n-                         .EmitLoop(ir_name, index_ty));\n+  TF_ASSIGN_OR_RETURN(auto thunk_sequence,\n+                      EmitPadToStaticLLVMIR(instr, local_llvm_module.get(),\n+                                            ir_emitter_context_));\n+  for (auto& thunk : thunk_sequence) {\n+    AddThunkToThunkSequence(std::move(thunk));\n+  }\n   CHECK(!llvm::Linker::linkModules(*ir_emitter_context_->llvm_module(),\n                                    std::move(local_llvm_module),\n                                    llvm::Linker::Flags::OverrideFromSrc));\n@@ -501,120 +300,12 @@ absl::Status ThunkEmitter::EmitSliceToDynamic(\n   std::string ir_name = std::string(instr->name());\n   auto local_llvm_module =\n       CreateLocalLLVMModule(ir_name, ir_emitter_context_->llvm_module());\n-\n-  // TODO(jurahul): Create an op to represent SliceToDynamic.\n-  constexpr int kUnrollFactor = 1;\n-  const Shape& input_shape = instr->operand(0)->shape();\n-\n-  LaunchDimensions launch_dimensions = CalculateLaunchDimensions(\n-      input_shape, ir_emitter_context_->gpu_device_info(), {kUnrollFactor});\n-  llvm::Type* index_ty =\n-      GetIndexTypeForKernel(instr, launch_dimensions.launch_bound(), &b_);\n-  TF_ASSIGN_OR_RETURN(std::vector<llvm_ir::IrArray> ir_arrays,\n-                      BuildKernelThunkForNonFusionOp(local_llvm_module.get(),\n-                                                     instr, launch_dimensions));\n-\n-  const Shape& data_shape = ShapeUtil::MakeStaticShape(instr->shape());\n-  TF_RET_CHECK(data_shape.IsArray());\n-\n-  // TODO(jurahul): data_shape here is the static shape of the\n-  // output (which has a dynamic shape in XLA). Currently, we are\n-  // mapping that to a static shaped memref. When we change that to\n-  // a more appropriate representation in MLIR, fix this code to\n-  // correctly deduce the static shape backing the dynamically\n-  // shaped memref.\n-\n-  // calculate the location where metadata needs to be inserted\n-  //   int* dyn_dim0_size = dest_array + meta_data_offset;\n-  //   int* dyn_dim1_size = dest_array + meta_data_offset +\n-  //   sizeof(int);\n-  int32_t raw_data_size = ShapeUtil::ByteSizeOf(data_shape);\n-\n-  // pseudo code for sliceToDynamic on a 2d array\n-  //   int* source_array = args[0];\n-  //   int* dest_array = args.back();\n-  const llvm_ir::IrArray& data_array = ir_arrays.back();\n-  llvm::Value* dest_buffer = data_array.GetBasePointer();\n-\n-  // Load dynamic dimensions from memory.\n-  std::vector<llvm::Value*> dynamic_dims;\n-  int alignment = raw_data_size % sizeof(int32_t);\n-  for (int64_t i = 1; i < instr->operand_count(); ++i) {\n-    llvm::Value* source_buffer = ir_arrays[i].GetBasePointer();\n-    llvm::Type* source_buffer_pointee_type = ir_arrays[i].GetBasePointeeType();\n-    llvm::LoadInst* dyn_dim_size =\n-        Load(source_buffer_pointee_type, source_buffer, \"dyn_dim_size\");\n-    dynamic_dims.push_back(dyn_dim_size);\n-  }\n-\n-  // only one thread need to store the dynamic index\n-  //   int thread_id = GetThreadId();\n-  //   int block_id = GetBlockId();\n-  //   if (thread_id == 0 && block_id == 0) {\n-  //     *dyn_dim0_size = *output[1];\n-  //     *dyn_dim1_size = *output[2];\n-  //   }\n-  KernelSupportLibrary{&b_}.If(\"is_thread_0\", IsBlock0Thread0(&b_), [&] {\n-    for (int64_t i = 1; i < instr->operand_count(); ++i) {\n-      const int64_t dim_index = i - 1;\n-      llvm::Value* metadata = b_.CreateConstInBoundsGEP1_32(\n-          b_.getInt8Ty(), dest_buffer,\n-          raw_data_size + dim_index * sizeof(int32_t));\n-      // output[i] stores dynamic_dim_(i-1)\n-      CreateStore(dynamic_dims[dim_index], metadata, alignment);\n-    }\n-  });\n-\n-  //     int dyn_element_total = 1;\n-  //     dyn_element_total *= dyn_dim0_size;\n-  //     dyn_element_total *= dyn_dim1_size;\n-  llvm::Value* dyn_element_total = llvm::ConstantInt::get(index_ty, 1);\n-  for (llvm::Value* dynamic_dim : dynamic_dims) {\n-    dyn_element_total =\n-        b_.CreateMul(dyn_element_total,\n-                     b_.CreateIntCast(dynamic_dim, dyn_element_total->getType(),\n-                                      /*isSigned=*/true),\n-                     /*Name=*/\"dyn_element_total_slice\");\n-  }\n-\n-  //   linear_index = block_id * threads_per_block + thread_id;\n-  //   if (linear_index < max_num_element) {\n-  //     Index static_index =\n-  //         delinerized(linerized_index, static_dim0_size,\n-  //         static_dim1_size);\n-  //     if (linerized_index < dyn_element_total) {\n-  //       Index dyn_index =\n-  //           delinerized(linerized_index, *dyn_dim0_size,\n-  //           *dyn_dim1_size);\n-  //       dest_array[static_index.dim0][static_index.di] =\n-  //           source_array[dyn_index.dim0][dyn_index.dim1];\n-  //     }\n-  //   }\n-  llvm_ir::BodyEmitter body_generator =\n-      [&](const llvm_ir::IrArray::Index& array_index) -> absl::Status {\n-    llvm::Value* linearIndex =\n-        array_index.Linearize(input_shape.dimensions(), &b_);\n-    auto if_in_dyn_bounds = llvm_ir::EmitIfThenElse(\n-        b_.CreateICmpULT(linearIndex, dyn_element_total),\n-        llvm_ir::IrName(ir_name, \"in_dyn_bounds\"), &b_, false);\n-    // Set IR builder insertion point to the body of the if\n-    // structure.\n-    llvm_ir::SetToFirstInsertPoint(if_in_dyn_bounds.true_block, &b_);\n-    llvm_ir::IrArray::Index dyn_index(linearIndex, input_shape,\n-                                      absl::MakeSpan(dynamic_dims), &b_);\n-\n-    data_array.EmitWriteArrayElement(\n-        array_index,\n-        ir_arrays[0].EmitReadArrayElement(dyn_index, &b_, /*name=*/\"\",\n-                                          /*use_linear_index=*/false),\n-        &b_);\n-    return absl::OkStatus();\n-  };\n-\n-  TF_RETURN_IF_ERROR(ParallelLoopEmitter(body_generator, data_shape,\n-                                         launch_dimensions, &b_,\n-                                         {kUnrollFactor})\n-                         .EmitLoop(ir_name, index_ty));\n+  TF_ASSIGN_OR_RETURN(auto thunk_sequence,\n+                      EmitSliceToDynamicLLVMIR(instr, local_llvm_module.get(),\n+                                               ir_emitter_context_));\n+  for (auto& thunk : thunk_sequence) {\n+    AddThunkToThunkSequence(std::move(thunk));\n+  }\n   CHECK(!llvm::Linker::linkModules(*ir_emitter_context_->llvm_module(),\n                                    std::move(local_llvm_module),\n                                    llvm::Linker::Flags::OverrideFromSrc));\n@@ -673,7 +364,6 @@ absl::Status ThunkEmitter::EmitCommandBufferThunk(const HloInstruction* instr) {\n       std::move(thunk_sequence),\n       ir_emitter_context_->debug_options()\n           .xla_enable_command_buffers_during_profiling()));\n-\n   return absl::OkStatus();\n }\n \n@@ -1687,25 +1377,25 @@ absl::Status ThunkEmitter::EmitWhile(const HloInstruction* instr) {\n \n absl::Status ThunkEmitter::EmitRngGetAndUpdateState(\n     const HloRngGetAndUpdateStateInstruction* instr) {\n-  // Emit a kernel to increment the global state for Philox RNG\n-  // algorithm.\n-  TF_ASSIGN_OR_RETURN(auto ir_arrays, BuildKernelThunkForNonFusionOp(\n-                                          ir_emitter_context_->llvm_module(),\n-                                          instr, LaunchDimensions()));\n-  llvm::Value* old_state =\n-      llvm_ir::RngGetAndUpdateState(instr->delta(), module_, &b_);\n-  llvm::Value* output_address = ir_arrays[0].EmitArrayElementAddress(\n-      llvm_ir::IrArray::Index(\n-          /*linear=*/b_.getInt64(0), instr->shape(), &b_),\n-      &b_, \"rng_state_address\");\n-  Store(old_state, output_address);\n+  std::string ir_name = std::string(instr->name());\n+  auto local_llvm_module =\n+      CreateLocalLLVMModule(ir_name, ir_emitter_context_->llvm_module());\n+\n+  TF_ASSIGN_OR_RETURN(auto thunk_sequence,\n+                      EmitRngGetAndUpdateStateLLVMIR(\n+                          instr, local_llvm_module.get(), ir_emitter_context_));\n+  for (auto& thunk : thunk_sequence) {\n+    AddThunkToThunkSequence(std::move(thunk));\n+  }\n+  CHECK(!llvm::Linker::linkModules(*ir_emitter_context_->llvm_module(),\n+                                   std::move(local_llvm_module),\n+                                   llvm::Linker::Flags::OverrideFromSrc));\n   return absl::OkStatus();\n }\n \n absl::Status ThunkEmitter::EmitSort(const HloSortInstruction* sort) {\n   std::string op_name(sort->name());\n   const Shape& keys_shape = sort->operand(0)->shape();\n-  int64_t dimension_to_sort = sort->sort_dimension();\n   for (int64_t i = 0; i < sort->operand_count(); ++i) {\n     ShapeIndex shape_index =\n         sort->operand_count() > 1 ? ShapeIndex({i}) : ShapeIndex({});\n@@ -1743,173 +1433,18 @@ absl::Status ThunkEmitter::EmitSort(const HloSortInstruction* sort) {\n     }\n   }\n \n-  uint64_t dimension_to_sort_bound = keys_shape.dimensions(dimension_to_sort);\n-  int64_t num_stages = Log2Ceiling(dimension_to_sort_bound);\n-  VLOG(2) << op_name << \" requires \" << num_stages << \" stages.\";\n-  CHECK_GE(1ULL << num_stages, dimension_to_sort_bound);\n-  CHECK_LT(1ULL << (num_stages - 1), dimension_to_sort_bound);\n-\n-  // Naive C++ code for the outer loops:\n-  //\n-  // for (int64_t stage = 0; stage <\n-  // Log2Ceiling(dimension_to_sort_bound);\n-  //     ++stage) {\n-  //   int64_t first_xor_mask = (1LL << (stage + 1)) - 1;\n-  //   SortInPlace(first_xor_mask);\n-  //   for (int64_t mask = stage - 1; mask >= 0; --mask) {\n-  //     int64_t later_xor_mask = 1LL << mask;\n-  //     SortInPlace(later_xor_mask);\n-  //   }\n-  // }\n-  //\n-  // This follows the alternative representation of the algorithm\n-  // described on Wikipedia:\n-  // https://en.wikipedia.org/wiki/Bitonic_sorter\n-  //\n-  // Each mask specifies how to derive from one position in the\n-  // array the position with which it should be compared (we\n-  // calculate the xor of the position with the mask). As an\n-  // optimization, we can move the 'mask' loop to inside the\n-  // sorting/comparison loop if the comparisons happen within a\n-  // small block of the array. To make this work, we collect all\n-  // consecutive masks that are smaller than our chosen power of 2\n-  // tile size, and pass them to SortInPlace. Each block then\n-  // processes one tile of data.\n-\n-  const uint64_t kUnrollFactor = 4;\n-  // Determine the total element size of all sort operands. We need to choose a\n-  // tile size such that we have enough shared memory to store a tile of\n-  // elements from each operand.\n-  uint64_t total_element_size = 0;\n-  for (int64_t i = 0; i < sort->operand_count(); ++i) {\n-    total_element_size += ShapeUtil::ByteSizeOfPrimitiveType(\n-        sort->operand(i)->shape().element_type());\n-  }\n-  const uint64_t kMaxSharedMemoryPerBlock =\n-      ir_emitter_context_->gpu_device_info().shared_memory_per_block();\n-  uint64_t max_tile_size_fitting_into_shared_memory =\n-      kMaxSharedMemoryPerBlock / total_element_size;\n-  const uint64_t kMaxThreadsPerBlock =\n-      ir_emitter_context_->gpu_device_info().threads_per_block_limit();\n-  // Choose the tile size based on actual amount of elements to sort, the amount\n-  // of shared memory available, and the maximum number of threads per block.\n-  uint64_t tile_size =\n-      std::min(std::min(kMaxThreadsPerBlock * kUnrollFactor,\n-                        max_tile_size_fitting_into_shared_memory),\n-               uint64_t{1} << num_stages);\n-  // The tile size needs to be a power of 2.\n-  tile_size = uint64_t{1} << Log2Floor(tile_size);\n-\n-  // If we cannot combine several xor masks together, we don't use\n-  // tiling, so we calculate the standard launch dimensions for the\n-  // shape. However we only need to iterate through ~half of the\n-  // dimension to sort (rounded up to the next highest power of 2),\n-  // because each iteration compares one pair of elements.\n-  Shape standard_iteration_shape = keys_shape;\n-  uint64_t standard_num_iterations_in_sort_dim = 1ULL << (num_stages - 1);\n-  standard_iteration_shape.set_dimensions(\n-      dimension_to_sort,\n-      CeilOfRatio(standard_num_iterations_in_sort_dim, kUnrollFactor));\n-\n-  LaunchDimensions standard_launch_dimensions = CalculateLaunchDimensions(\n-      standard_iteration_shape, ir_emitter_context_->gpu_device_info());\n-\n-  // Calculate the launch dimensions for the case where we use\n-  // tiling. We split the dimension that should be sorted into tiles\n-  // of size 'tile_size'. This means we first need to round\n-  // 'dimension_to_sort_bound' up to be a multiple of the tile size.\n-  uint64_t rounded_bound = RoundUpTo(dimension_to_sort_bound, tile_size);\n-  Shape iteration_shape = keys_shape;\n-\n-  // We iterate through the element pairs that should be compared.\n-  uint64_t num_iterations_in_sort_dim =\n-      CeilOfRatio(rounded_bound, kUnrollFactor);\n-  iteration_shape.set_dimensions(dimension_to_sort, num_iterations_in_sort_dim);\n-  uint64_t num_iterations = ShapeUtil::ElementsIn(iteration_shape);\n-\n-  // For correctness reasons we need exactly `tile_size` / `kUnrollFactor` many\n-  // threads per block. Each thread is responsible for copying\n-  // exactly `kUnrollFactor` many adjacent elements into shared memory, and then\n-  // does `kUnrollFactor` / 2 many comparisons of two elements taken from shared\n-  // memory.\n-  const uint64_t kThreadsPerBlock =\n-      std::max(uint64_t{1}, tile_size / kUnrollFactor);\n-\n-  uint64_t num_blocks = CeilOfRatio(num_iterations, kThreadsPerBlock);\n-  LaunchDimensions tiled_launch_dimensions(num_blocks, kThreadsPerBlock);\n-  VLOG(2) << absl::StreamFormat(\"%s launch dims: %d blocks, %d threads/block\",\n-                                op_name, num_blocks, kThreadsPerBlock);\n   auto local_llvm_module =\n       CreateLocalLLVMModule(op_name, ir_emitter_context_->llvm_module());\n-  // Copy of the main context with the local module.\n-  IrEmitterContext local_ir_emitter_context(\n-      &ir_emitter_context_->hlo_module(),\n-      &ir_emitter_context_->buffer_assignment(),\n-      &ir_emitter_context_->execution_stream_assignment(),\n-      std::string(ir_emitter_context_->platform_name()),\n-      ir_emitter_context_->gpu_device_info(),\n-      ir_emitter_context_->mlir_context(), local_llvm_module.get(),\n-      ir_emitter_context_->llvm_module_constants(),\n-      ir_emitter_context_->emit_kernels());\n-  auto emit_kernel = [&](absl::Span<const int64_t> xor_masks) {\n-    VLOG(2) << absl::StreamFormat(\n-        \"%s uses kernel for xor masks [%s]\", op_name,\n-        absl::StrJoin(xor_masks, \", \", [](std::string* out, int64_t xor_mask) {\n-          absl::StrAppendFormat(out, \"0x%x\", xor_mask);\n-        }));\n-    LaunchDimensions launch_dimensions = xor_masks.size() > 1\n-                                             ? tiled_launch_dimensions\n-                                             : standard_launch_dimensions;\n-    TF_ASSIGN_OR_RETURN(std::vector<llvm_ir::IrArray> ir_arrays,\n-                        BuildKernelThunkForNonFusionOp(\n-                            local_llvm_module.get(), sort, launch_dimensions));\n-\n-    // The first `operand_count()` elements of `ir_arrays` are the input\n-    // operands and the rest are the output arrays. Inputs are aliases with\n-    // outputs, so we need to pass only the outputs to the in-place sort kernel.\n-    auto output_arrays_span =\n-        absl::Span<const llvm_ir::IrArray>(ir_arrays).subspan(\n-            sort->operand_count());\n-\n-    auto* comparator = sort->called_computations().front();\n-    return llvm_ir::EmitSortInPlace(\n-        dimension_to_sort, output_arrays_span, llvm_ir::IrName(op_name),\n-        xor_masks, &b_, launch_dimensions,\n-        xor_masks.size() > 1 ? num_iterations_in_sort_dim\n-                             : standard_num_iterations_in_sort_dim,\n-        tile_size, kUnrollFactor,\n-        [&](absl::Span<llvm::Value* const> operands, llvm::Value* output) {\n-          return CallNestedComputation(&b_, local_ir_emitter_context,\n-                                       local_llvm_module.get(), *comparator,\n-                                       operands, output);\n-        });\n-  };\n-  std::vector<int64_t> xor_masks;\n-  for (int64_t stage = 0; stage < num_stages; ++stage) {\n-    for (int64_t mask = stage; mask >= 0; --mask) {\n-      int64_t xor_mask;\n-      if (mask == stage) {\n-        xor_mask = (1LL << (stage + 1)) - 1;\n-      } else {\n-        xor_mask = 1LL << mask;\n-      }\n-      if (xor_mask >= tile_size) {\n-        if (!xor_masks.empty()) {\n-          TF_RETURN_IF_ERROR(emit_kernel(xor_masks));\n-          xor_masks.clear();\n-        }\n-        TF_RETURN_IF_ERROR(emit_kernel({xor_mask}));\n-      } else {\n-        xor_masks.push_back(xor_mask);\n-      }\n-    }\n-  }\n-  if (!xor_masks.empty()) {\n-    TF_RETURN_IF_ERROR(emit_kernel(xor_masks));\n+\n+  TF_ASSIGN_OR_RETURN(ThunkSequence thunks,\n+                      EmitBitonicSortLLVMIR(sort, local_llvm_module.get(),\n+                                            ir_emitter_context_));\n+  for (auto& thunk : thunks) {\n+    AddThunkToThunkSequence(std::move(thunk));\n   }\n-  TF_RET_CHECK(!llvm::Linker::linkModules(\n-      *ir_emitter_context_->llvm_module(), std::move(local_llvm_module),\n-      llvm::Linker::Flags::OverrideFromSrc));\n+  llvm::Linker::linkModules(*ir_emitter_context_->llvm_module(),\n+                            std::move(local_llvm_module),\n+                            llvm::Linker::Flags::OverrideFromSrc);\n   return absl::OkStatus();\n }\n \n@@ -2615,52 +2150,6 @@ absl::Status ThunkEmitter::EmitOutfeed(const HloOutfeedInstruction* instr) {\n   return absl::OkStatus();\n }\n \n-absl::StatusOr<std::vector<llvm_ir::IrArray>>\n-ThunkEmitter::BuildKernelThunkForNonFusionOp(\n-    llvm::Module* llvm_module, const HloInstruction* instr,\n-    const LaunchDimensions& launch_dimensions) {\n-  std::string suggested_kernel_name(instr->name());\n-\n-  TF_ASSIGN_OR_RETURN(auto kernel_arguments,\n-                      emitters::KernelArguments::Create(\n-                          ir_emitter_context_->buffer_assignment(),\n-                          GetDefaultBufferAlignment(), instr));\n-\n-  VLOG(3) << \"Generating (without reuse check): \" << suggested_kernel_name;\n-\n-  TF_ASSIGN_OR_RETURN(\n-      llvm::Function * kernel,\n-      BuildKernelPrototype(\n-          llvm_module, ir_emitter_context_->gpu_device_info(),\n-          suggested_kernel_name,\n-          GetSanitizedUniqueName(*ir_emitter_context_, suggested_kernel_name),\n-          kernel_arguments, launch_dimensions, &b_));\n-\n-  AddThunkToThunkSequence(std::make_unique<KernelThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(\n-          instr, ir_emitter_context_->GetNextThunkId()),\n-      kernel->getName().str(), kernel_arguments, launch_dimensions,\n-      /*cluster_dim=*/std::nullopt,\n-      /*shmem_bytes=*/0,\n-      /*tma_metadata=*/se::gpu::TmaMetadata()));\n-\n-  std::vector<llvm_ir::IrArray> ir_arrays;\n-  ir_arrays.reserve(kernel_arguments.args().size());\n-  for (const auto& [kernel_argument, llvm_arg] :\n-       llvm::zip(kernel_arguments.args(), kernel->args())) {\n-    llvm::Type* ir_type =\n-        llvm_ir::ShapeToIrType(kernel_argument.shape(), llvm_arg.getContext());\n-    llvm_ir::IrArray ir_array(&llvm_arg, ir_type, kernel_argument.shape());\n-\n-    if (!kernel_argument.written()) {\n-      ir_array.MarkInvariantOverWholeProgram(&llvm_arg.getContext());\n-    }\n-\n-    ir_arrays.push_back(ir_array);\n-  }\n-\n-  return ir_arrays;\n-}\n \n absl::StatusOr<std::unique_ptr<Thunk>> ThunkEmitter::BuildWhileThunk(\n     const HloInstruction* instr, const Thunk::ThunkInfo& thunk_info,\n@@ -2693,11 +2182,6 @@ absl::StatusOr<std::unique_ptr<Thunk>> ThunkEmitter::BuildWhileThunk(\n       ir_emitter_body->ConsumeThunkSequence(body_thunk_info), trip_count));\n }\n \n-absl::Status ThunkEmitter::EmitTargetElementLoop(\n-    const HloInstruction& hlo, const llvm_ir::ElementGenerator& body_emitter) {\n-  return Internal(\"This should be unreachable\");\n-}\n-\n static absl::flat_hash_map<std::string, std::string> ConvertFrontendAttributes(\n     const FrontendAttributes& attrs) {\n   absl::flat_hash_map<std::string, std::string> result;"
        },
        {
            "sha": "7d4610082a4377799dbac60be369cb74d28ed5f5",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.h",
            "status": "modified",
            "additions": 2,
            "deletions": 33,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/df423f81413e6ad85b35832fedc08dac0ff9baae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/df423f81413e6ad85b35832fedc08dac0ff9baae/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.h?ref=df423f81413e6ad85b35832fedc08dac0ff9baae",
            "patch": "@@ -72,7 +72,7 @@ namespace gpu {\n //    within a kernel function using FusedIrEmitter.  (FusedIrEmitter is not\n //    really an IrEmitter, but is more an \"IR generator generator\".)\n //\n-class ThunkEmitter : public IrEmitter {\n+class ThunkEmitter {\n  public:\n   absl::string_view platform_name() const {\n     return ir_emitter_context_->platform_name();\n@@ -191,10 +191,6 @@ class ThunkEmitter : public IrEmitter {\n \n   absl::Status EmitCollectiveGroupStartThunk(const HloInstruction* instr);\n \n-  absl::Status EmitTargetElementLoop(\n-      const HloInstruction& hlo,\n-      const llvm_ir::ElementGenerator& body_emitter) override;\n-\n   // Add a owning Thunk object to the thunk sequence.\n   void AddThunkToThunkSequence(std::unique_ptr<Thunk> thunk) {\n     if (emit_group_thunks_) {\n@@ -204,30 +200,6 @@ class ThunkEmitter : public IrEmitter {\n     thunk_sequence_.emplace_back(std::move(thunk));\n   }\n \n-  // Load data from potentially unaligned address. If address is offset by\n-  // `alignment_bytes`, data is read in the unit of `alignment_bytes` to avoid\n-  // memory read misalignment in CUDA; otherwise, the entire data are loaded\n-  // from the given memory address.\n-  //\n-  //   address: the memory address to load data from.\n-  //   data_type: the type of data to load.\n-  //   alignment_bytes: the number of bytes required to align. The number of\n-  //     bytes of the data_type must be divisible by alignment_bytes.\n-  llvm::Value* CreateLoad(llvm::Value* address, llvm::Type* data_type,\n-                          int alignment_bytes);\n-\n-  // Store data at a potentially unaligned address. If the address is offset by\n-  // `alignment_bytes`, data is stored in the unit of `alignment_bytes` to avoid\n-  // memory write misalignment in CUDA; otherwise, the entire data is stored at\n-  // the given memory address.\n-  //\n-  //   data: the data to be stored.\n-  //   address: the memory address to store data.\n-  //   alignment_bytes: the number of bytes required to align. The number of\n-  //     bytes of the data_type must be divisible by alignment_bytes.\n-  void CreateStore(llvm::Value* data, llvm::Value* address,\n-                   int alignment_bytes);\n-\n   // Input = {static array, dynamic_dim0, dynamic_dim1}\n   // Output = {dynamic array(with dynamic dimension meta data at the end)}\n   // For a tensor with static dimension [2][<=5] and dynamic dimension [2][3]\n@@ -320,10 +292,6 @@ class ThunkEmitter : public IrEmitter {\n   //   ```\n   absl::Status EmitSliceToDynamic(const HloCustomCallInstruction* instr);\n \n-  absl::StatusOr<std::vector<llvm_ir::IrArray>> BuildKernelThunkForNonFusionOp(\n-      llvm::Module* llvm_module, const HloInstruction* instr,\n-      const LaunchDimensions& launch_dimensions);\n-\n   // Returns a WhileThunk that invokes thunk sequences for 'condition' and\n   // 'body' sub-computations of while instruction.\n   absl::StatusOr<std::unique_ptr<Thunk>> BuildWhileThunk(\n@@ -343,6 +311,7 @@ class ThunkEmitter : public IrEmitter {\n   GetInstructionToHostExecuteAsyncEvents() {\n     return ir_emitter_context_->instruction_to_host_execute_async_events();\n   }\n+  IrEmitterContext* ir_emitter_context_;\n \n   // The thunk sequence this IrEmitter generates for the input computation.\n   ThunkSequence thunk_sequence_;"
        }
    ],
    "stats": {
        "total": 1418,
        "additions": 768,
        "deletions": 650
    }
}