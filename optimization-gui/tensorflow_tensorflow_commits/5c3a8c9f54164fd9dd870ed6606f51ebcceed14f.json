{
    "author": "mtsokol",
    "message": "PR #31688: Update `operation_semantics.md`\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31688\n\nüìù Summary of Changes\n\nThis is a follow-up after https://github.com/openxla/xla/pull/31366 which further updates `operation_semantics.md` to match https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h source.\n\n@GleasonK I noticed that for ops Gather and Scatter, [`xla_builder.h`](https://github.com/openxla/xla/blob/bd9e70df3d34fcdb663ccf8e7f4c450903bbf773/xla/hlo/builder/xla_builder.h#L1080) declarations are now way different than [`operation_semantics.md`](https://github.com/openxla/xla/blob/main/docs/operation_semantics.md#scatter) contents and [StableHLO definitions](https://openxla.org/stablehlo/spec#inputs_80). Should md file also match `xla_builder.h` version here?\n\nüéØ Justification\nDocumentation update\n\nüöÄ Kind of Contribution\n‚ôªÔ∏è Cleanup, üìö Documentation\n\nCopybara import of the project:\n\n--\n2186fa1e600e7c0228f120102d99e7b9b97975d3 by Mateusz Sok√≥≈Ç <mat646@gmail.com>:\n\nUpdate `operation_semantics.md`\n\nMerging this change closes #31688\n\nPiperOrigin-RevId: 813656587",
    "sha": "5c3a8c9f54164fd9dd870ed6606f51ebcceed14f",
    "files": [
        {
            "sha": "326e1049c68e7b3310b43186b4368b88f83c2fce",
            "filename": "third_party/xla/docs/operation_semantics.md",
            "status": "modified",
            "additions": 101,
            "deletions": 94,
            "changes": 195,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5c3a8c9f54164fd9dd870ed6606f51ebcceed14f/third_party%2Fxla%2Fdocs%2Foperation_semantics.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5c3a8c9f54164fd9dd870ed6606f51ebcceed14f/third_party%2Fxla%2Fdocs%2Foperation_semantics.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Foperation_semantics.md?ref=5c3a8c9f54164fd9dd870ed6606f51ebcceed14f",
            "patch": "@@ -110,20 +110,20 @@ See also\n \n Performs concatenation across replicas.\n \n-**`AllGather(operand, all_gather_dimension, shard_count, replica_group_ids,\n+**`AllGather(operand, all_gather_dimension, shard_count, replica_groups,\n channel_id, layout, use_global_device_ids)`**\n \n | Arguments               | Type                 | Semantics                   |\n-| :---------------------- | :------------------- | :-------------------------- |\n+| ----------------------- | -------------------- | --------------------------- |\n | `operand`               | `XlaOp`              | Array to concatenate across |\n :                         :                      : replicas                    :\n | `all_gather_dimension`  | `int64`              | Concatenation dimension     |\n | `shard_count`           | `int64`              | The size of each replica    |\n :                         :                      : group                       :\n | `replica_groups`        | vector of vectors of | Groups between which the    |\n :                         : `int64`              : concatenation is performed  :\n-| `channel_id`            | optional `int64`     | Optional channel ID for     |\n-:                         :                      : cross-module communication  :\n+| `channel_id`            | optional             | Optional channel ID for     |\n+:                         : `ChannelHandle`      : cross-module communication  :\n | `layout`                | optional `Layout`    | Creates a layout pattern    |\n :                         :                      : that will capture the       :\n :                         :                      : matched layout in the       :\n@@ -149,11 +149,11 @@ channel_id, layout, use_global_device_ids)`**\n     instead of a replica id. This enables more flexible grouping of devices if\n     this all-reduce is both cross-partition and cross-replica.\n \n-The output shape is the input shape with the `all_gather_dim` made `shard_count`\n-times larger. For example, if there are two replicas and the operand has the\n-value `[1.0, 2.5]` and `[3.0, 5.25]` respectively on the two replicas, then the\n-output value from this op where `all_gather_dim` is `0` will be `[1.0, 2.5, 3.0,\n-5.25]` on both replicas.\n+The output shape is the input shape with the `all_gather_dimension` made\n+`shard_count` times larger. For example, if there are two replicas and the\n+operand has the value `[1.0, 2.5]` and `[3.0, 5.25]` respectively on the two\n+replicas, then the output value from this op where `all_gather_dim` is `0` will\n+be `[1.0, 2.5, 3.0,5.25]` on both replicas.\n \n The API of `AllGather` is internally decomposed into 2 HLO instructions\n (`AllGatherStart` and `AllGatherDone`).\n@@ -181,21 +181,21 @@ Performs a custom computation across replicas.\n **`AllReduce(operand, computation, replica_groups, channel_id,\n shape_with_layout, use_global_device_ids)`**\n \n-| Arguments               | Type                 | Semantics                  |\n-| :---------------------- | :------------------- | :------------------------- |\n-| `operand`               | `XlaOp`              | Array or a non-empty tuple |\n-:                         :                      : of arrays to reduce across :\n-:                         :                      : replicas                   :\n-| `computation`           | `XlaComputation`     | Reduction computation      |\n-| `replica_groups`        | vector of vectors of | Groups between which the   |\n-:                         : `int64`              : reductions are performed   :\n-| `channel_id`            | optional `int64`     | Optional channel ID for    |\n-:                         :                      : cross-module communication :\n-| `shape_with_layout`     | optional `Shape`     | Defines the layout of the  |\n-:                         :                      : data transferred           :\n-| `use_global_device_ids` | optional `bool`      | Returns true if the ids in |\n-:                         :                      : the ReplicaGroup config    :\n-:                         :                      : represent a global id      :\n+| Arguments               | Type                  | Semantics                  |\n+| ----------------------- | --------------------- | -------------------------- |\n+| `operand`               | `XlaOp`               | Array or a non-empty tuple |\n+:                         :                       : of arrays to reduce across :\n+:                         :                       : replicas                   :\n+| `computation`           | `XlaComputation`      | Reduction computation      |\n+| `replica_groups`        | `ReplicaGroup` vector | Groups between which the   |\n+:                         :                       : reductions are performed   :\n+| `channel_id`            | optional              | Optional channel ID for    |\n+:                         : `ChannelHandle`       : cross-module communication :\n+| `shape_with_layout`     | optional `Shape`      | Defines the layout of the  |\n+:                         :                       : data transferred           :\n+| `use_global_device_ids` | optional `bool`       | Returns true if the ids in |\n+:                         :                       : the ReplicaGroup config    :\n+:                         :                       : represent a global id      :\n \n -   When `operand` is a tuple of arrays, the all-reduce is performed on each\n     element of the tuple.\n@@ -899,17 +899,17 @@ See also\n \n Invokes a computation with the given arguments.\n \n-**`Call(computation, args...)`**\n+**`Call(computation, operands...)`**\n \n | Arguments     | Type                   | Semantics                           |\n | ------------- | ---------------------- | ----------------------------------- |\n | `computation` | `XlaComputation`       | computation of type `T_0, T_1, ..., |\n :               :                        : T_{N-1} -> S` with N parameters of  :\n :               :                        : arbitrary type                      :\n-| `args`        | sequence of N `XlaOp`s | N arguments of arbitrary type       |\n+| `operands`    | sequence of N `XlaOp`s | N arguments of arbitrary type       |\n \n-The arity and types of the `args` must match the parameters of the\n-`computation`. It is allowed to have no `args`.\n+The arity and types of the `operands` must match the parameters of the\n+`computation`. It is allowed to have no `operands`.\n \n ### CompositeCall\n \n@@ -941,19 +941,19 @@ frontend_attributes = {\n }\n ```\n \n-**`Call(computation, args..., name, attributes, version)`**\n+**`CompositeCall(computation, operands..., name, attributes, version)`**\n \n-| Arguments       | Type              | Semantics                              |\n-| --------------- | ----------------- | -------------------------------------- |\n-| `inputs`        | `XlaOp`           | variadic number of values              |\n-| `name`          | `string`          | name of the composite                  |\n-| `attributes`    | optional `string` | optional stringified dictionary of     |\n-:                 :                   : attributes                             :\n-| `decomposition` | `XlaComputation`  | computation of type `T_0, T_1, ...,    |\n-:                 :                   : T_{N-1} -> S` with N parameters of     :\n-:                 :                   : arbitrary type                         :\n-| `version`       | `int64`.          | number to version updates to semantics |\n-:                 :                   : of the composite op                    :\n+| Arguments     | Type                   | Semantics                           |\n+| ------------- | ---------------------- | ----------------------------------- |\n+| `computation` | `XlaComputation`       | computation of type `T_0, T_1, ..., |\n+:               :                        : T_{N-1} -> S` with N parameters of  :\n+:               :                        : arbitrary type                      :\n+| `operands`    | sequence of N `XlaOp`s | variadic number of values           |\n+| `name`        | `string`               | name of the composite               |\n+| `attributes`  | optional `string`      | optional stringified dictionary of  |\n+:               :                        : attributes                          :\n+| `version`     | optional `int64`       | number to version updates to        |\n+:               :                        : semantics of the composite op       :\n \n An op‚Äôs `decomposition` isn‚Äôt a field called, but instead appears as a to_apply\n attribute that points to the function which contains the lower-level\n@@ -1193,20 +1193,23 @@ replicas.\n \n **`CollectivePermute(operand, source_target_pairs, channel_id, inplace)`**\n \n-| Arguments             | Type                    | Semantics                  |\n-| :-------------------- | :---------------------- | :------------------------- |\n-| `operand`             | `XlaOp`                 | n dimensional input array  |\n-| `source_target_pairs` | `<int64, int64>` vector | A list of                  |\n-:                       :                         : (source_replica_id,        :\n-:                       :                         : target_replica_id) pairs.  :\n-:                       :                         : For each pair, the operand :\n-:                       :                         : is sent from source        :\n-:                       :                         : replica to target replica. :\n-| `channel_id`          | optional `int64`        | Optional channel ID for    |\n-:                       :                         : cross-module communication :\n-| `inplace`             | bool                    | Optional inplace           |\n-\n-Note that there are the following restrictions on the `source_target_pair`:\n+| Arguments             | Type                     | Semantics                 |\n+| --------------------- | ------------------------ | ------------------------- |\n+| `operand`             | `XlaOp`                  | n dimensional input array |\n+| `source_target_pairs` | `<int64, int64>` vector  | A list of                 |\n+:                       :                          : (source_replica_id,       :\n+:                       :                          : target_replica_id) pairs. :\n+:                       :                          : For each pair, the        :\n+:                       :                          : operand is sent from      :\n+:                       :                          : source replica to target  :\n+:                       :                          : replica.                  :\n+| `channel_id`          | optional `ChannelHandle` | Optional channel ID for   |\n+:                       :                          : cross-module              :\n+:                       :                          : communication             :\n+| `inpace`              | optional `bool`          | flag whether permutation  |\n+:                       :                          : should be done inplace    :\n+\n+Note that there are the following restrictions on the `source_target_pairs`:\n \n -   Any two pairs should not have the same target replica id, and they should\n     not have the same source replica id.\n@@ -1724,7 +1727,7 @@ false_computation)`**\n | `false_computation` | `XlaComputation` | XlaComputation of type $T_1 \\to S$ |\n \n Executes `true_computation` if `predicate` is `true`, `false_computation` if\n-`pred` is `false`, and returns the result.\n+`predicate` is `false`, and returns the result.\n \n The `true_computation` must take in a single argument of type $T_0$ and will\n be invoked with `true_operand` which must be of the same type. The\n@@ -1735,7 +1738,7 @@ returned value of `true_computation` and `false_computation` must be the same.\n <!-- mdformat on -->\n \n Note that only one of `true_computation` and `false_computation` will be\n-executed depending on the value of `pred`.\n+executed depending on the value of `predicate`.\n \n **`Conditional(branch_index, branch_computations, branch_operands)`**\n \n@@ -2485,7 +2488,7 @@ dimension: [start, start + size). The shape of `start_indices` must be\n 1-dimensional, with dimension size equal to the number of dimensions of\n `operand`.\n \n-**`DynamicSlice(operand, start_indices, size_indices)`**\n+**`DynamicSlice(operand, start_indices, slice_sizes)`**\n \n | Arguments       | Type                  | Semantics                          |\n | --------------- | --------------------- | ---------------------------------- |\n@@ -2507,7 +2510,7 @@ The effective slice indices are computed by applying the following\n transformation for each index `i` in `[1, N)` before performing the slice:\n \n ```cpp\n-start_indices[i] = clamp(start_indices[i], 0, operand.dimension_size[i] - size_indices[i])\n+start_indices[i] = clamp(start_indices[i], 0, operand.dimension_size[i] - slice_sizes[i])\n ```\n \n This ensures that the extracted slice is always in-bounds with respect to the\n@@ -3077,14 +3080,16 @@ For StableHLO information see\n See also\n [`XlaBuilder::Infeed`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n \n-**`Infeed(shape)`**\n+**`Infeed(shape, config)`**\n \n-| Argument | Type    | Semantics                                             |\n-| -------- | ------- | ----------------------------------------------------- |\n-| `shape`  | `Shape` | Shape of the data read from the Infeed interface. The |\n-:          :         : layout field of the shape must be set to match the    :\n-:          :         : layout of the data sent to the device; otherwise its  :\n-:          :         : behavior is undefined.                                :\n+| Argument | Type              | Semantics                                     |\n+| -------- | ----------------- | --------------------------------------------- |\n+| `shape`  | `Shape`           | Shape of the data read from the Infeed        |\n+:          :                   : interface. The layout field of the shape must :\n+:          :                   : be set to match the layout of the data sent   :\n+:          :                   : to the device; otherwise its behavior is      :\n+:          :                   : undefined.                                    :\n+| `config` | optional `string` | Configuration of the op.                      |\n \n Reads a single data item from the implicit Infeed streaming interface of the\n device, interpreting the data as the given shape and its layout, and returns a\n@@ -3272,14 +3277,16 @@ See also\n \n **`Map(operands..., computation, dimensions)`**\n \n-| Arguments     | Type                   | Semantics                          |\n-| ------------- | ---------------------- | ---------------------------------- |\n-| `operands`    | sequence of N `XlaOp`s | N arrays of types T_0..T_{N-1}     |\n-| `computation` | `XlaComputation`       | computation of type `T_0, T_1, .., |\n-:               :                        : T_{N + M -1} -> S` with N          :\n-:               :                        : parameters of type T and M of      :\n-:               :                        : arbitrary type                     :\n-| `dimensions`  | `int64` array          | array of map dimensions            |\n+| Arguments         | Type                   | Semantics                      |\n+| ----------------- | ---------------------- | ------------------------------ |\n+| `operands`        | sequence of N `XlaOp`s | N arrays of types T_0..T_{N-1} |\n+| `computation`     | `XlaComputation`       | Computation of type `T_0, T_1, |\n+:                   :                        : .., T_{N + M -1} -> S` with N  :\n+:                   :                        : parameters of type T and M of  :\n+:                   :                        : arbitrary type.                :\n+| `dimensions`      | `int64` array          | Array of map dimensions        |\n+| `static_operands` | sequence of N `XlaOp`s | Static ops for the map         |\n+:                   :                        : operation                      :\n \n Applies a scalar function over the given `operands` arrays, producing an array\n of the same dimensions where each element is the result of the mapped function\n@@ -3961,22 +3968,22 @@ shard.\n **`ReduceScatter(operand, computation, scatter_dimension, shard_count,\n replica_groups, channel_id, layout, use_global_device_ids)`**\n \n-| Arguments               | Type                 | Semantics                  |\n-| :---------------------- | :------------------- | :------------------------- |\n-| `operand`               | `XlaOp`              | Array or a non-empty tuple |\n-:                         :                      : of arrays to reduce across :\n-:                         :                      : replicas.                  :\n-| `computation`           | `XlaComputation`     | Reduction computation      |\n-| `scatter_dimension`     | `int64`              | Dimension to scatter.      |\n-| `shard_count`           | `int64`              | Number of blocks to split  |\n-:                         :                      : `scatter_dimension`        :\n-| `replica_groups`        | vector of vectors of | Groups between which the   |\n-:                         : `int64`              : reductions are performed   :\n-| `channel_id`            | optional `int64`     | Optional channel ID for    |\n-:                         :                      : cross-module communication :\n-| `layout`                | optional `Layout`    | user-specified memory      |\n-:                         :                      : layout                     :\n-| `use_global_device_ids` | optional `bool`      | user-specified flag        |\n+| Arguments               | Type                  | Semantics                  |\n+| ----------------------- | --------------------- | -------------------------- |\n+| `operand`               | `XlaOp`               | Array or a non-empty tuple |\n+:                         :                       : of arrays to reduce across :\n+:                         :                       : replicas.                  :\n+| `computation`           | `XlaComputation`      | Reduction computation      |\n+| `scatter_dimension`     | `int64`               | Dimension to scatter.      |\n+| `shard_count`           | `int64`               | Number of blocks to split  |\n+:                         :                       : `scatter_dimension`        :\n+| `replica_groups`        | `ReplicaGroup` vector | Groups between which the   |\n+:                         :                       : reductions are performed   :\n+| `channel_id`            | optional              | Optional channel ID for    |\n+:                         : `ChannelHandle`       : cross-module communication :\n+| `layout`                | optional `Layout`     | user-specified memory      |\n+:                         :                       : layout                     :\n+| `use_global_device_ids` | optional `bool`       | user-specified flag        |\n \n -   When `operand` is a tuple of arrays, the reduce-scatter is performed on each\n     element of the tuple.\n@@ -4839,12 +4846,12 @@ communication primitives in HLO. These ops typically appear in HLO dumps as part\n of low-level input/output or cross-device transfer, but they are not intended to\n be constructed manually by end users.\n \n-**`Send(operand, channel_handle)`**\n+**`Send(operand, handle)`**\n \n-Arguments        | Type            | Semantics\n----------------- | --------------- | -----------------------------------------\n-`operand`        | `XlaOp`         | data to send (array of type T)\n-`channel_handle` | `ChannelHandle` | unique identifier for each send/recv pair\n+Arguments | Type            | Semantics\n+--------- | --------------- | -----------------------------------------\n+`operand` | `XlaOp`         | data to send (array of type T)\n+`handle`  | `ChannelHandle` | unique identifier for each send/recv pair\n \n Sends the given operand data to a [`Recv`](#recv) instruction in another\n computation that shares the same channel handle. Does not return any data."
        }
    ],
    "stats": {
        "total": 195,
        "additions": 101,
        "deletions": 94
    }
}